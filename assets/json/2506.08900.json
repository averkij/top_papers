{
    "paper_title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis",
    "authors": [
        "José Morano",
        "Botond Fazekas",
        "Emese Sükei",
        "Ronald Fecso",
        "Taha Emre",
        "Markus Gumpinger",
        "Georg Faustmann",
        "Marzieh Oghbaie",
        "Ursula Schmidt-Erfurth",
        "Hrvoje Bogunović"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 0 0 9 8 0 . 6 0 5 2 : r MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis Jose Morano1,2,*, Botond Fazekas1,2, Emese Sukei3, Ronald Fecso1,2, Taha Emre1,2, Markus Gumpinger3, Georg Faustmann1,2, Marzieh Oghbaie1,2, Ursula Schmidt-Erfurth3, and Hrvoje Bogunovic1,2,* 1Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria 2Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria 3OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria *jose.moranosanchez@meduniwien.ac.at, hrvoje.bogunovic@meduniwien.ac.at Abstract Artificial intelligence (AI) has become fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on single imaging modality. In this context, we propose MIRAGE, novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE."
        },
        {
            "title": "Introduction",
            "content": "The prevalence of ocular and systemic diseases affecting the eye represents significant public health concern, with an estimated 1 billion individuals worldwide affected by visual impairment or blindness1. Major causes of vision loss include retinal diseases such as age-related macular degeneration (AMD) and glaucoma, and systemic conditions such as hypertension and diabetes, which frequently leads to diabetic retinopathy (DR). The aging of the population and the increasing prevalence of hypertension and diabetes are expected to further worsen the burden of retinal diseases. In this context, early detection and accurate diagnosis and characterization are crucial for timely intervention and adequate management to prevent or slow vision loss. Diagnosis and monitoring of retinal diseases rely on imaging modalities such as color fundus photography (CFP) and optical coherence tomography (OCT) with scanning laser ophthalmoscopy (SLO) (also referred to as near-infrared imaging). While CFP and SLO provide 2D en-face views of the retina, 3D OCT provides cross-sectional images (B-scans) for detailed 3D analysis of retinal layers and their thickness and integrity2, 3. As result, OCT has become the gold standard for the diagnosis of diseases such as AMD, glaucoma and diabetic macular edema (DME) as well as the primary modality in the management of patients undergoing 1 treatment with anti-vascular endothelial growth factor (anti-VEGF) drugs4, 5. However, the interpretation of these detailed OCT images is complex, time-consuming, and requires specialized expertise. Furthermore, manual qualitative analysis is inherently subjective and prone to interand intra-observer variability, which affects diagnostic accuracy and reliability6, 7. Several artificial intelligence (AI) methods, especially based on deep learning (DL), have been developed for the automated or semi-automated analysis of retinal OCT images811. AI has proven particularly valuable in detecting and quantifying OCT biomarkers in AMD, the leading cause of vision loss in the developed world, to support patient management12, 13. However, their performance often depends on the availability of large, manually annotated datasets, which are crucial for training large DL models. Creating these datasets is particularly challenging in the medical domain due to the sensitive nature of medical data, the high acquisition costs, and the required expertise. As result, datasets tend to be small and homogeneous, limiting the generalizability and robustness of AI models and thus their translation into clinical practice1416. Self-supervised learning (SSL)17 offers promising solution to these problems by enabling models to learn meaningful representations from the data without expert annotation. SSL methods train models on unlabeled datasets using pretext tasks such as masked autoencoding (MAE)18, contrastive learning19, 20, and self-distillation21. While most SSL methods are unimodal, recent research highlights the exciting potential of multimodal approaches to significantly improve the performance of DL models in downstream tasks20, 22, 23. successful example in computer vision is MultiMAE23, which extends the concept of MAE to multimodal image data. In the context of retinal imaging, multimodal reconstruction2426, which aims to learn representations by reconstructing one modality from another, and contrastive learning using multimodal pairs (imagetext27, imageimage28) have shown superior performance compared to unimodal approaches. Advances in SSL and scalable network architectures such as Vision Transformers (ViTs)29 have facilitated the development of foundation models (FMs)30, 31, large DL models that are pretrained on vast datasets and can be applied to diverse tasks with minimal tuning. In the field of computer vision, the DINOv2 model32 has established new benchmark for SSL methods in downstream tasks such as classification, detection, and segmentation. In ophthalmology, FMs such as RETFound33 (which includes two separate models for OCT and CFP), FLAIR27 (for CFP), VisionFM34 (including separate models for five modalities), EyeFound35, and EyeCLIP36 (both multimodal, focusing mainly on CFP and fluorescein angiography) have shown superior performance in diagnostic and prognostic tasks compared to models pretrained on ImageNet37 (IN) and other datasets of natural images. However, most of these models either focus on single retinal imaging modality27, 33, 34 or naively mix multiple unpaired modalities35, 38 during training, completely ignoring the relationship between them. Importantly, although VisionFM was presented as multimodal FM, it has different (and separately trained) encoders for each image modality. This strategy, also used in RETFound, results in zoo of unimodal models, but not in truly multimodal FM. Other models, such as EyeCLIP, although trained on partially paired multimodal data, still mixes arbitrary non-paired modalities for training, and relies on CLIP-based contrastive learning approach to learn joint representation space of the fewer paired cases. This CLIP-based approach strongly focuses on global features, and does not exploit the more fine-grained relations between the modalities, which are crucial for pixel-level tasks such as segmentation39. Furthermore, the lack of evaluation on segmentation tasks of models such as RETFound, FLAIR, and EyeCLIP further limits their potential adoption in real-world clinical settings, where segmentation is one of the most useful applications of AI in retina9. On the other hand, FMs for medical image segmentation4043, most of them based on SAM44, are trained on large medical image datasets containing heterogeneous, unpaired image modalities (such as X-ray, magnetic resonance imaging, etc.) and some are not even trained on any OCT scans, such as MedSAM-2D43. Thus, they lack the necessary specialization to perform well in fully automated OCT segmentation setting45. 2 Figure 1: Overview of the proposed model (MIRAGE) and other general (DINOv2) and domain-specific (MedSAM, RETFound) foundation models. In contrast to existing unimodal foundation models, our approach utilizes multimodal self-supervised learning to train Vision Transformer on large dataset of paired multimodal retinal images, including optical coherence tomography (OCT), scanning laser ophthalmoscopy (SLO), and automatically generated labels for retinal layers. We evaluated the model on comprehensive benchmark consisting of 19 tasks from 14 publicly available datasets and two private datasets, covering both OCT and SLO classification and segmentation tasks. Statistical significance was calculated using the Wilcoxon signed-rank test across all datasets. Our foundation model, MIRAGE, significantly outperforms state-of-the-art foundation models across all task types. 3 In this context, the main contribution of this work is twofold. First, we introduce MIRAGE (Figure 1), the first multimodal foundation model for comprehensive analysis of retinal OCT/SLO images, and second, we propose comprehensive evaluation benchmark for validating foundation models in retinal OCT/SLO imaging, including several classification and segmentation tasks. MIRAGE was trained on large dataset using paired multimodal MAE approach, which allows the model to effectively exploit the complementary information from different modalities and process any of them at inference time. We thoroughly evaluate MIRAGE on the proposed benchmark and compare it with state-of-the-art (SOTA) SSL methods and foundation models, including DINOv232, RETFound33, and MedSAM42. Our benchmark results show the superior performance of MIRAGE on both classification and segmentation tasks, showcasing its generalizability and robustness. These findings show the excellent suitability of MIRAGE as foundation for the development of AI systems for OCT and SLO image analysis. Both MIRAGE and the proposed evaluation benchmark, including code and data splits, have been made available to the academic community to facilitate reproducibility and monitoring of research progress in the field."
        },
        {
            "title": "MIRAGE development data",
            "content": "MIRAGE was pretrained on large in-house dataset of multimodal retinal images, the Vienna Imaging Biomarker Eye Study46 (VIBES) registry of the Macula Clinic at the Medical University of Vienna, including OCT and SLO. For each sample, we additionally generated labels of retinal layers using an automatic method47, 48. The model was trained using multimodal MAE pretext task, which aims to reconstruct all input modalities from masked versions of the same images using shared ViT encoder. total of 261 184 paired OCTSLOLayers samples were used for training. The complete information about the pretraining dataset can be found in the section MethodsPretraining dataset."
        },
        {
            "title": "Ocular disease diagnosis",
            "content": "We evaluated MIRAGE on several diagnostic and staging tasks involving different ocular diseases, OCT devices, and imaging modalities from datasets acquired at different institutions worldwide. In particular, there are 9 tasks based on the same number of datasets (8 public and 1 in-house): Duke iAMD49, GAMMA50, Harvard Glaucoma51, Kermany52, 53, Noor Eye Hospital54, OCTDL55, OCTID56, and OLIVES57 and OPTIMA9C58 (in-house). OLIVES and OPTIMA9C are the only datasets that include SLO images. All tasks are different and feature different diseases or disease stages. Section MethodsBenchmark datasets contains further information about these datasets. For each task, we compared MIRAGE to three other selected SOTA models, all based on ViT: supervised ImageNet pretraining (SL-IN)29, DINOv232, and RETFound33. The models were tuned and evaluated on each dataset using linear probing (LP), where all the parameters of the model are frozen except for final linear layer. In this way, it is possible to determine how discriminative (i.e., how meaningful) the extracted features are for the classification task, and thus how effective the pretraining method was in learning useful data representations. Figure 2 shows the performance of the OCTand SLO-based models using linear probing in terms of the receiver operating characteristic curve (AUROC) value for each dataset. The average performances across all OCT and SLO classification datasets are presented in Table 1, including the AUROC, average precision (AP), and balanced accuracy (BAcc) values. Full quantitative results are presented in Supplementary Tables 1 and 2 for OCT and SLO tasks, respectively (all extended results are presented in Supplementary Note 1). 4 Figure 2: Comparison of MIRAGE and other SOTA models for ocular disease diagnosis. OCT diagnosis. SLO diagnosis. For each classification task, we trained the models (all based on ViTLarge) with linear probing using five different random seeds (determining the shuffling of the training data and the augmentation). We then evaluated these models on the hold-out test set and obtained five replicas from which the statistics were derived. The error bars show the standard deviation. In each case, the performance of the two best models was compared to check for statistically significant differences. The p-value is calculated using the one-tailed Students t-test and is indicated in the figure. MIRAGE outperforms all other models in all but one dataset, with statistically significant differences in 7 out of 11 cases. Table 1: Average performance of MIRAGE and other SOTA models for ocular disease diagnosis with linear probing. All models are based on the ViT-Large architecture. The best results are underlined. In each case, the performance of the best (underlined) and second best models across all datasets was compared to determine if there were statistically significant differences in average performance. To this end, the Wilcoxon signed-rank test was used. The results are presented in the table, with the significance level indicated by asterisks (*: < 0.5, **: < 0.01, ***: < 0.001). Modality Model AUROC AP BAcc OCT SLO 91.69 10.64 DINOv2 92.67 9.51 SL-IN RETFound 94.44 6.66 MIRAGE 95.59 5.80*** 87.69 12.23 88.63 11.21 91.35 8.15 92.99 6.39*** 77.91 15.10 80.44 12.86 81.50 11.48 84.01 10.51*** DINOv2 SL-IN MIRAGE 82.52 9.68 82.59 9.52 83.74 9.67* 72.39 2.70 74.85 2.28 75.39 2.05 55.64 3.48 58.79 6.21 61.13 5.65** The results on OCT-based tasks show that MIRAGE outperformed competing models in all but one task, with statistically significant differences in 6 out of 9 cases when evaluated with the one-tailed Students t-test. The performance of our model was particularly high on datasets involving the diagnosis or staging of AMD and complex multi-class classification tasks. For example, on the Duke iAMD dataset (for iAMD detection), MIRAGE achieved an AUROC of 99.52%, outperforming the second-best model, DINOv2, by significant difference of 0.39 percentage points (pp) (p < 0.01). The same trend was observed for glaucoma staging in the small GAMMA dataset, for which MIRAGE outperformed RETFound by 5.20 pp (p < 0.01). Smaller but still significant differences were also found for Kermany (choroidal neovascularization [CNV], drusen, and DME detection), Noor Eye Hospital (AMD and DME detection), OCTDL (6 disease detection), and OPTIMA9C (4 disease detection and AMD staging). In Harvard Glaucoma, for glaucoma detection, and OCTID (4 disease detection), MIRAGE also outperformed the other approaches, but no statistically significant differences were found. On the other hand, MIRAGE showed consistently lower performance than other SOTA approaches in OLIVES, for DME and diabetic retinopathy (DR) discrimination. These results can be attributed to the distribution of diseases in the pretraining dataset (see Table 8 in Methods). While our in-house pretraining dataset contains high prevalence of AMD, other diseases, such as diabetes (which was highly represented in the RETFound pretraining dataset), are scarcely represented. In particular, at least 85% of the OCT scans used to train RETFound were from the Moorfields diabetic image dataset (MEH-MIDAS), while less than 2% of the scans in our dataset were from diabetic patients. Further analysis of the results (Supplementary Table 3) shows that MIRAGE performs equally well on the first scans of the patients, which are treatment-naıve. Performance declines on the later scans, which are more likely to be affected by treatment, which introduces data shift to which our model is less robust. Notwithstanding, the overall performance of MIRAGE on OCT-based tasks was still significantly better than that of the other models, with an average AUROC of 95.59% across all OCT tasks, outperforming the second-best model, RETFound, by significant margin of 1.15 pp (p < 0.001). Similar differences were observed for the other metrics, with MIRAGE achieving an average AP of 92.99% (+1.64 pp, < 0.001) and BAcc of 84.01% (+2.51 pp, < 0.001). For the SLO tasks, MIRAGE significantly outperformed SL-IN and DINOv2 on the OPTIMA9C dataset, with an AUROC of 93.40% and more than 1.5 pp difference (p < 0.001). Our model also outperformed the state of the art on the OLIVES dataset, but the differences were not statistically significant. On average, MIRAGE achieved an AUROC of 83.74% (+1.15 pp over the second model, < 0.05), AP of 75.39% (+0.54 pp), and BAcc of 61.13% (+2.34 pp, < 0.01). Consequently, our MIRAGE model demonstrated significantly superior performance in both OCTand SLO-based diagnosis and staging. Cross-dataset OCT classification performance To further assess the generalization capabilities of MIRAGE, we evaluated its performance in cross-dataset scenarios and compared it to the performance of the other SOTA models used in the previous experiments. The results of this evaluation reflect how well the models can adapt to new, unseen data, which is key indicator of their robustness and potential applicability in real-world clinical settings. For classification, we evaluated the models tuned on the Noor Eye Hospital54 dataset (with AMD and DME classes) on combined external dataset consisting of the UMN59 (AMD and DME) and Duke Srinivasan60 (DME and iAMD) datasets. In addition, we performed the inverse evaluation, tuning the models on the UMN + Duke Srinivasan dataset and evaluating them on the Noor Eye Hospital dataset. As in the previous experiments, we used linear probing to tune the models for specific classification task. The results of this evaluation, shown in Figure 3 and detailed in Supplementary Table 4, demonstrate the superior performance of our model over the other SOTA models, especially DINOv2 and RETFound. Significant performance improvements were observed for Noor Eye Hospital, with MIRAGE outperforming SL-IN by 4.74 pp (p < 0.01) in terms of AUROC. Smaller improvements were observed for UMN + Duke Srinivasan, with MIRAGE outperforming SL-IN by 2.26 pp. These results highlight the robustness and adaptability of our model in handling domain shifts and generalizing effectively to new datasets. Figure 3: Cross-dataset evaluation results for OCT classification tasks. For each task, we trained the models (all based on ViT-Large) with linear probing using five different random seeds. We then evaluated these models on the full external dataset and obtained five replicas from which the statistics were derived. The error bars show the standard deviation, while the colored bars show the mean AUROC. The performance of the two best models was compared using the one-tailed Students t-test, with the resulting p-values indicated in the figure. MIRAGE outperforms all other models in both cases, with significant differences on Noor Eye Hospital dataset."
        },
        {
            "title": "Segmentation of retinal lesions and layers",
            "content": "We utilized four public and one in-house diverse datasets to validate the adaptability of our model to lesion and layer segmentation tasks in both OCT and SLO images. Specifically, we used the Duke DME61, AROI62, RETOUCH63, and GOALS64 datasets for lesion and layer segmentation in OCT, and the SGA dataset65 (in-house) for the segmentation of geographic atrophy (GA) in SLO. For adapting the model to the segmentation tasks, we used decoder-only fine-tuning strategy, where ConvNeXt-based66 decoder was trained on top of the pretrained ViT encoder23, which is kept frozen during downstream fine-tuning. The ConvNeXt decoder was 7 Figure 4: Performance comparison of MIRAGE and SOTA FMs at the retinal lesion and layer segmentation tasks. OCT segmentation. SLO segmentation. The error bars show the patient-wise standard deviation, while the colored bars show the mean Dice score. The dagger symbol () indicates that the patient information was not available, so the standard deviation is not shown. In each case, the performance of the two best models was compared to see if there were statistically significant differences. The p-value is calculated using the one-tailed Students t-test and is indicated in the figure. MIRAGE outperforms all other FMs on all datasets, with significant differences in all cases for which the standard deviation was available. chosen based on previous work23 and preliminary experiments (Supplementary Table 5) showing that it is well suited for medical image segmentation. This efficient strategy allows us to evaluate how well the different pretrained encoders (and thus the corresponding pretraining approaches) are suited for OCT and SLO segmentation tasks while achieving accurate segmentations. In addition to comparing our model with the DINOv2 and RETFound models previously used for classification, we also compare it with MedSAM42, state-of-the-art ViT-based medical image segmentation FM. All models were trained simultaneously for layer and lesion segmentation in datasets where both tasks were available (AROI and Duke DME). However, the results are reported separately for more detailed analysis. Figure 4 shows the performance of the models on the OCT and SLO segmentation tasks in terms of the average Dice score (calculated at the patient level) for each dataset. The average performance of the models in terms of Dice score, intersection over union (IoU), and 95th percentile Hausdorff distance (HD95) across all OCT and SLO datasets is summarized Table 2: Average performance of MIRAGE and SOTA FMs on segmentation tasks. Average and standard deviation were calculated across all different datasets. No standard deviation was calculated for SLO segmentation tasks because only one dataset was available. The performance of the two best models was compared using the Wilcoxon signed-rank test, with the resulting p-values indicated in the table by asterisks (*p < 0.05). The best results are underlined. Modality Model Dice IoU 63.00 19.99 DINOv2 RETFound 60.05 18.15 MedSAM 71.36 18.37 78.46 14.26* MIRAGE 52.68 21.29 47.73 19.58 60.86 21.98 68.24 18.40* HD95 42.67 22.80 57.20 40.57 26.47 20.34 19.61 16.87* 61.34 24.56 DINOv2 MedSAM 72.20 24.05 MIRAGE 75.31 22.16*** 49.13 23.88 62.22 25.58 65.81 24.14*** 210.02 85.59 182.22 86.21 164.42 95.35** OCT SLO in Table 2. HD95 is distance-based metric measured in pixels, with lower values indicating better performance. The detailed results for each dataset are presented in the Supplementary Table 6. Since the official evaluation server of the RETOUCH challenge was used to obtain the results, and it provides only the average Dice and absolute volume difference (AVD) values, RETOUCH was excluded from the calculation of the average metrics except for the Dice score. In all tasks, our model significantly outperformed the other SOTA FM models, including the specialized medical image segmentation FM MedSAM. The greatest improvements were observed for the segmentation of retinal lesions in OCT datasets, where MIRAGE achieved Dice scores of 69.72%, 52.18%, and 79.60% on Duke DME, AROI, and RETOUCH, respectively, outperforming the second best FM, MedSAM, by 13.92 pp (p < 0.001), 9.51 pp (p < 0.001), and 10.20 pp, respectively. The same trend was observed for the other metrics (see Supplementary Table 6). No statistical analysis was performed for RETOUCH because the sample-wise results were not available, as the results were obtained using the official evaluation server of the challenge. The performance of both DINOv2 and RETFound was overall significantly lower than that of MIRAGE and MedSAM, and remained generally low on all datasets. MIRAGE was also the best performing model for layer segmentation, achieving Dice scores of 83.02%, 93.79%, and 92.46% on Duke DME, AROI, and GOALS, respectively. In all cases, the differences with respect to the second best model, MedSAM, were greater than 2 pp, with significant differences (p < 0.001) in all cases for which the patient-wise results were available (see Figure 4a). Across all OCT segmentation tasks, MIRAGE achieved an average Dice score of 78.46%, significantly outperforming MedSAM, the second-best model, which achieved Dice score of 71.36% (p < 0.05). Significant differences were also observed for the other metrics. In SLO image segmentation (Figure 4b), similar results were observed, where MIRAGE outperformed MedSAM by 3.11 pp (p < 0.001), achieving an average Dice score of 75.31% in GA segmentation. Our model also outperformed MedSAM and the other FMs by significant margin in terms of IoU, and HD95. Cross-dataset OCT segmentation performance For cross-dataset segmentation evaluation, all models trained on the AROI dataset, which includes segmentation of four retinal layers and three lesions, were tested on the large Duke iAMD dataset49. The three layer classes in the Duke iAMD dataset are composites of the layers present in AROI. The results, shown in Figure 5 and detailed in Supplementary Table 6, demonstrate the superior performance of our model over the other SOTA models, with significant improvements (p < 0.001) of 13.55, 12.22, and 15.29 pp in terms of Dice score over DINOv2, RETFound, and 9 Figure 5: Cross-dataset evaluation results for OCT segmentation. The error bars show the patient-wise standard deviation, while the colored bars show the mean Dice score. The performance of the two best models was compared to see if there were statistically significant differences. The p-value is calculated using the one-tailed Students t-test and is indicated in the figure. MIRAGE significantly outperforms all other FMs. MedSAM, respectively. These results underscore the robustness and generalization capabilities of our model compared to existing general and segmentation-specific FMs, such as MedSAM, which was trained for medical image segmentation tasks. In fact, while MedSAM was the second best model on the AROI dataset, it achieved the worst performance in the cross-dataset evaluation on Duke iAMD. These results highlight the limitations of in-dataset evaluation and the importance of cross-dataset evaluation for effective model validation."
        },
        {
            "title": "Segmentation performance comparison with specialist models",
            "content": "To further assess the relative segmentation capabilities of our model beyond FMs, we compared the performance of MIRAGE with that of specialist models on the same datasets. Specifically, we evaluated SwinUNETR-V267, MedNeXt68, TransUNet69, and nnUNet version 270, 71 (from now on referred to as nnUNet). SwinUNETR-V2, MedNeXt, and TransUNet are state-of-theart models for medical image segmentation based on the Swin Transformer72, ConvNeXt66, and Transformer73 architectures, respectively. On the other hand, nnUNet is fully automated deep learning framework, winner of many medical image segmentation challenges70, based on the successful U-Net74 convolutional neural network (CNN) architecture7578. For more fair comparison, we also trained MIRAGE on the segmentation tasks using full fine-tuning (FFT), the same strategy used by the competing models. FFT involves training the entire model (both the ViT encoder and the ConvNeXt decoder) on the target dataset. While this results in more computationally expensive training process, it usually leads to better performance, especially when the target data differs substantially from the pretraining data. The results of this comparison are shown in Figure 6. In particular, Figure 6a shows the performance of the models on the OCT segmentation tasks; Figure 6b, the cross-dataset evaluation results for OCT segmentation; and Figure 6c, the performance on the SLO segmentation tasks. The average performance of the models in terms of the Dice score, IoU, and HD95 across all OCT and SLO datasets is shown in Table 3. Detailed results for each dataset are presented in Supplementary Table 7. In general, MIRAGE performed similarly to specialist models on the OCT segmentation tasks, with the only exceptions being AROI (lesions) and the RETOUCH datasets (see Figure 6a). In the former, nnUNet outperformed MIRAGE by 2.48 pp (p < 0.05) and MIRAGE FFT by 10.81 pp (p < 0.01) in terms of Dice score, while in the latter, MIRAGE outperformed nnUNet by 6.67 pp. On the other hand, MIRAGE significantly outperformed all specialist models on the cross-dataset OCT segmentation task (Figure 6b), with Dice scores of 91.06% and 91.29% for MIRAGE and MIRAGEFFT, respectively, compared to 61.21% for nnUNet 10 Figure 6: Performance comparison of MIRAGE and specialist models for retinal lesion and layer segmentation. OCT segmentation. Cross-dataset OCT segmentation. SLO segmentation. MIRAGE was evaluated using both linear probing and full fine tuning (FFT). The error bars show the patient-wise standard deviation, when available, while the colored bars show the mean Dice score. The dagger symbol () indicates that the patient information was not available, so the standard deviation is not shown. In each case, the performance of the two best models was compared to see if there were statistically significant differences. The p-value is calculated using the one-tailed Students t-test and is indicated in the figure. MIRAGE performs similarly to the specialist models on most in-domain tasks, while significantly outperforming them on the cross-dataset task. and around 47% for the other specialist models. This corresponds to difference of about 30 pp between MIRAGE and the best specialist model. These results demonstrate the very limited generalizability of nnUNet and other specialist models to new datasets, and the superior generalization capabilities of our model. Small improvements were observed when using the FFT strategy, with the largest differences seen in AROI (lesions), where the Dice score increased by 8.33 pp, from 52.18% to 60.51%. This improvement, combined with small increases in other datasets such as Duke DME (layers) and AROI (layers), resulted in an average Dice score of 80.10% across all OCT segmentation tasks, surpassing the second best model, nnUNet, by 1.18 pp, and MIRAGE (with decoder-only tuning) by 1.64 pp. Improvements were also observed in terms of IoU and HD95. In the segmentation of GA in SLO (Figure 6b), TransUNet achieved the best performance, with Dice score of 82.12%, followed by nnUNet with 79.41% and MIRAGEFFT with 79.36%. It also outperformed the other models in terms of IoU and HD95. In this case, it is important to note that only one binary segmentation task was available for evaluation, limiting the scope of the comparison and the conclusions that can be drawn. In addition, this model performed 11 Table 3: Average performance of MIRAGE and specialist models on segmentation tasks. The standard deviation was calculated across all different datasets. No standard deviation was calculated for SLO segmentation tasks because only one dataset was available. The best results are underlined. Modality Model Dice IoU OCT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet OCT cross-dataset nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT"
        },
        {
            "title": "SLO",
            "content": "74.23 17.26 76.22 15.48 77.47 14.28 78.92 12.49 78.46 14.26 80.10 11.98 46.88 1.72 47.05 1.47 46.88 1.59 61.21 15.33 91.06 2.43 91.29 2.14 74.51 23.51 77.22 19.62 82.12 15.92 79.41 19.26 75.31 22.16 79.36 17.07 64.02 22.09 65.73 19.78 67.58 18.84 70.39 17.06 68.24 18.40 70.24 16.52 43.90 2.70 44.05 2.34 43.76 2.61 55.35 12.98 84.62 3.38 84.92 3.15 65.09 24.88 67.87 22.19 73.69 18.65 71.33 21.25 65.81 24.14 70.03 19. HD95 36.04 40.01 27.73 27.80 21.31 18.88 20.71 19.41 19.61 16.87 19.26 17.45 201.81 24.71 203.56 29.73 194.12 36.45 97.95 60.82 4.62 4.52 3.94 2.27 169.03 94.67 157.15 84.21 127.19 57.98 136.56 74.10 164.42 95.35 136.16 43.48 consistently worse than MIRAGE, MIRAGEFFT, and nnUNet on the OCT datasets. The results presented in this section show that our model is not only superior to all other SOTA foundation models in terms of segmentation performance (see Table 2), but also competitive with specialized models on the same segmentation tasks. In addition, the results of the cross-dataset evaluation show that our model significantly outperforms the specialist models under domain shifts, highlighting its greater generalization capabilities and potential for real-world applications."
        },
        {
            "title": "Qualitative analysis of segmentation results",
            "content": "In addition to the quantitative evaluations presented in the previous sections, we also examined the qualitative performance of the models on the segmentation tasks by visualizing the segmentations produced by each model. Figure 7 shows examples of segmentations produced by MIRAGE and the SOTA models RETFound (for OCT tasks), DINOv2 (for SLO tasks), MedSAM, and nnUNet on the different OCT and SLO segmentation datasets. The qualitative results are consistent with the quantitative results, showing that our model produces precise and accurate segmentations, equal to or better than nnUNet, and significantly better than the other SOTA models, here represented by RETFound, DINOv2 and MedSAM. The higher quality of the segmentations produced by MIRAGE is particularly evident in the cross-dataset setting (Figure 7, first row highlighted in green), where the models were evaluated on Duke iAMD but trained on AROI. In this cross-dataset scenario, MIRAGE demonstrates superior generalization capabilities, producing segmentations that are more accurate than those produced by the other models. In particular, nnUNet produces an incomplete segmentation of the layers, misclassifying some of the pixels (in red in Figure 7) as background, while RETFound and MedSAM, while correctly distinguishing the layers from the background in most cases, assign large areas of the image (in violet) to the wrong layer class. For the other datasets, the 12 Figure 7: Examples of segmentations from different models. The examples belong, from top to bottom, to the following datasets: Duke iAMD (cross-dataset evaluation), AROI, Duke DME, GOALS, RETOUCH, and SGA. True positives are depicted in the grayscale value of the class; false background pixels, in red; false lesion or layer pixels, in cyan; and true but wrongly classified lesion or layer pixels, in violet. The results show that our model produces precise and accurate segmentations, rarely missing or misclassifying lesions and layers. The quality of our segmentations is appreciably higher than that of nnUNet in the cross-dataset evaluation (first row), and similar in the other datasets. Compared to MedSAM and RETFound, the differences are more pronounced, as these models often misclassify pathological regions. 13 differences between MIRAGE and nnUNet are subtle, with both models producing generally accurate segmentations. On the other hand, RETFound shows poor overall performance, with many layers or lesions incorrectly classified as background and assigned to the wrong layer or lesion type, especially in regions with pathological signs. MedSAM, while generally more accurate than RETFound, shares some of the same problems, with the exception of the GOALS and SGA datasets, where the segmentations are nearly as accurate as those produced by MIRAGE and nnUNet. Effect of pseudo-labeling We evaluated the impact of using retinal layer pseudo-labels during pretraining by comparing the downstream performance of ViT model pretrained using only OCT images with that of model trained using OCT images and pseudo-labels of retinal layers (OCT+Layers). The models were pretrained using the MultiMAE approach23 and evaluated on the OCT classification and segmentation tasks using linear probing (LP). While this strategy is suboptimal for segmentation32, it was chosen in this case to minimize the impact of the decoder on the results and focus on the pseudo-labeling effect. In particular, we follow the approach proposed in DINOv232, which consists of training linear layer to classify each patch token and then upsampling the resulting map to full resolution. The results of this analysis are presented in Table 4, while the detailed results are presented in the Supplementary Tables 8 and 9 for classification and segmentation tasks, respectively. As shown in the table, the model pretrained with OCT images and pseudo-labels for the retinal layers (OCT+Layers) significantly outperformed the model pretrained with OCT images alone on the OCT classification and segmentation tasks. These results demonstrate the positive impact of using pseudo-labels during pretraining. Table 4: Effect of pseudo-labeling. Comparison of the performance of ViT-Base model trained on OCT alone, and OCT with pseudo-labels for the retinal layers (OCT+Layers) using the MultiMAE approach23 on OCT classification and segmentation tasks with linear probing. The best results are underlined. In each case, the performance of the two models across all datasets for every metric was compared using the Wilcoxon signed-rank test (*: < 0.05, **: < 0.01, ***: < 0.001)."
        },
        {
            "title": "Classification",
            "content": "Segmentation (LP)"
        },
        {
            "title": "Dice",
            "content": "HD95 OCT OCT+Layers 93.75 7.99 95.44 6.00*** 79.77 12.76 82.81 12.62*** 66.37 13.68 69.07 14.21* 40.83 23.58 34.33 17. Effectiveness of domain-specific multimodal pretraining We evaluated the effect of domain-specific and multimodal pretraining by comparing the performance of MIRAGE, pretrained on our multimodal VIBES dataset, with that of domain-specific unimodal models pretrained on unimodal retinal imaging data (VIBES-OCT and VIBES-SLO) and general multimodal models pretrained on multimodal data from ImageNet (Multi-IN). The comparison was made for ViT models pretrained with MAE18 and MultiMAE23 on unimodal and multimodal pretraining setups, respectively, and evaluated on the OCT and SLO classification and segmentation tasks using linear probing (LP). Similar to the previous experiment, we use this strategy to minimize the impact of the decoder on the results and focus on the pretraining effect. Since the only available MultiMAE-pretrained model is the ViT-Base, and for the sake of computational efficiency, we used the ViT-Base architecture for this analysis. By comparing the results of the models pretrained with single modality with those of the multimodal MIRAGE, we were able to effectively measure the importance of integrating the 14 Table 5: Effect of domain-specific and multimodal pretraining. Comparison of the impact of domain-specific and multimodal pretraining on the performance of ViT-Base model in classification and segmentation tasks with linear probing. The best results are underlined. In each case, the performance of the two best models across all datasets was compared to determine if there were statistically significant differences in average performance. For this, the Wilcoxon signed-rank test was used. The results are presented in the table, with the significance level indicated by asterisks: **: < 0.01, ***: < 0.001. Tuning Model Pretraining Classification Segmentation (LP) modality dataset AUROC BAcc Dice HD95 OCT SLO 91.01 9.60 MultiMAE Multi-IN MAE-OCT VIBES-OCT 93.75 7.99 MIRAGE VIBES 94.52 6.97*** 75.73 13.06 79.77 12.76 81.84 11.24*** 41.09 26.13 66.37 13.68 69.63 14.60* 80.46 47.68 40.83 23.58 34.33 21.42 83.52 6.44 MultiMAE Multi-IN MAE-SLO VIBES-SLO 74.67 10.11 85.66 7.19** MIRAGE VIBES 56.13 9.72 51.61 15.42 61.55 10.28** 68.69 70.63 72.24 187.07 174.96 166.25 different imaging modalities during pretraining. The average results of this analysis are presented in Table 5, while the detailed results are presented in the Supplementary Tables 10 and 11 for classification and segmentation tasks, respectively. As shown in the table, the models pretrained with domain-specific data (OCT, SLO, and layers) significantly outperformed those pretrained with single modality (OCT or SLO) or multimodal data from ImageNet. Specifically, our model achieved the highest scores across all metrics for OCT-based models, with an average AUROC of 94.52%, BAcc of 81.84%, and segmentation Dice of 69.63% and HD95 of 34.33 pixels, with significant improvements over MultiMAE and MAE-OCT. Similarly, our model also showed significantly superior performance on SLO classification tasks, achieving an average AUROC of 85.66%, BAcc of 61.55%, and consistently higher segmentation performance, with Dice of 72.24% and HD95 of 166.25 pixels. These results highlight the substantial benefit of pretraining with domain-specific multimodal data on downstream performance regardless of the modality (OCT/SLO) and the type of task (classification/segmentation)."
        },
        {
            "title": "Impact of model capacity",
            "content": "We explored the impact of model capacity on the performance of MIRAGE by comparing the ViT-Base ( 86M parameters) and ViT-Large ( 307M parameters) architectures across all tasks. The average results of this analysis are presented in Table 6, along with the results of the best SOTA FMs in each case: RETFound for OCT classification, SL-IN for SLO classification, and MedSAM for OCT and SLO segmentation. The detailed results of the ViT-Base version of MIRAGE for each dataset are presented in the Supplementary Tables 12 and 13 for classification and segmentation, respectively. In classification tasks, the performance of the ViT-Large model was significantly superior to that of the ViT-Base model in OCT, while ViT-Base performed better in SLO. For instance, the ViT-Large model achieved an AUROC of 95.59% for OCT classification, significantly higher than the 94.52% achieved by the ViT-Base model. For SLO classification, the differences were not significant. In segmentation tasks, the ViT-Large and ViT-Base models achieved similar performance, although the ViT-Large model performed slightly better in most cases in terms of Dice score. In all cases, MIRAGE-Base outperformed the SOTA models, namely RETFound, SL-IN (both based on ViT-Large) and MedSAM (based on ViT-Base), for classification and segmentation, respectively. These results demonstrate that the effectiveness of MIRAGE is not dependent on model capacity, as both the ViT-Base and ViT-Large models achieved satisfactory performance in most tasks. Furthermore, they show that, although the ViT-Large model generally outperforms 15 the ViT-Base model (especially in OCT classification tasks), the performance difference is usually small. Thus, our MIRAGE-Base model may be an appropriate choice in scenarios where computational resources are limited. Table 6: Effect of model capacity on downstream performance. Average performance of MIRAGE based on ViT-Base or ViT-Large across all OCT and SLO classification and segmentation tasks. The best SOTA results are also shown in gray for reference: RETFound for OCT classification, SL-IN for SLO classification, and MedSAM for OCT and SLO segmentation. The best results are underlined. The Wilcoxon signed-rank test was used to compare the performance of the two models in each task, with the resulting p-values shown in the table (*: < 0.5, **: < 0.01, ***: < 0.001). Modality Model Classification Segmentation OCT SLO AUROC BAcc Dice HD95 SOTA MIRAGE-Base MIRAGE-Large 94.44 6.66 94.52 6.97 95.59 5.80*** 81.50 11.48 81.84 11.24 84.01 10.51*** 71.36 18.37 77.30 16.84 78.46 14.26 26.47 20.34 20.96 19.17 19.61 16.87 SOTA MIRAGE-Base MIRAGE-Large 82.59 9.52 85.66 7.19 83.74 9.67 58.79 6.21 61.55 10.28 61.13 5.65 72.20 74.47 75. 182.22 161.13 164."
        },
        {
            "title": "Discussion",
            "content": "This work introduces MIRAGE, robust multimodal foundation model (FM) for comprehensive retinal image analysis, and extensively evaluates its generalization capabilities across range of diagnostic, staging, and segmentation tasks. MIRAGE is based on the ViT architecture29 and is trained using SSL23, 26 on large dataset of paired OCT and SLO images, along with automatically generated pseudo-labels of retinal layers47, 48. The model can be adapted, with minimal tuning, to various retinal imaging tasks, including both classification and segmentation in OCT and SLO images. Our extensive evaluations demonstrate the superiority of MIRAGE over existing FMs and its potential for research and clinical applications. Unlike existing FMs, which are either strictly unimodal33, 34, naively mix multiple modalities in the same batch35, 38, 42 (ignoring multimodal complementarity), or focus on image-level contrastive learning on partially paired datasets36, MIRAGE was pretrained using fully-paired dataset of multimodal retinal images in multimodal MAE setting23, 26. This pretext task involves reconstructing all input multimodal images from their highly masked versions, requiring the model to infer masked information from the limited visible image patches of the same image and the other modalities. To allow the model to process any subset of the input modalities in inference, we use sample strategy that randomly and non-uniformly selects the patches from the different modalities. Thus, during training, the patches received by the model for given sample can all be from the same modality (and none from the others) or mix from different modalities. This pretraining strategy has several advantages. First, it allows MIRAGE to be used for both OCT and SLO image analysis, since the model is pretrained with both modalities simultaneously. This contrasts with existing FMs for ophthalmology, such as RETFound33 and VisionFM34, which feature different models for each modality. Second, it makes the model more robust to domain shifts, as it learns more general features that are not specific to single modality, but common to all input modalities. Finally, through the multimodal reconstruction task, the different modalities provide fine-grained, complementary supervisory signals, as lesions or other pathological characteristics are often visible in both modalities, but in different manners. In this way, the model can learn to associate the appearance of pathological signs in one modality with the abnormal patterns in the other modalities, ultimately improving detection. 16 All of these factors make MIRAGE the first truly multimodal FM for retinal image analysis and contribute to the learning of robust multimodal representations that are generalizable to wide range of tasks. To evaluate the generalization capabilities and the quality of the learned representations of MIRAGE, we conducted extensive evaluations on number of clinically relevant tasks. These tasks include diagnosis and staging from OCT and SLO images and, unlike previous works27, 33, retinal lesion and layer segmentation. For the classification of diseases and stages in OCT and SLO images, MIRAGE was compared to other SOTA FMs, including RETFound33 and DINOv221, as well as models trained using supervised learning on the ImageNet-21k dataset of natural images (SL-IN)29. RETFound is unimodal FM trained with MAE18 on private dataset of 700k OCT images. DINOv2 was pretrained on private dataset of 142M natural images using an SSL approach based on self-distillation. On the other hand, the ImageNet-21k dataset contains 14M images with 21k categories of natural objects. Similar to previous work on FMs27, 32, 42, 44, 79, 80, all models were evaluated using linear probing on the target datasets. This evaluation strategy involves optimizing single linear classifier (only about 2k parameters) on top of the frozen pretrained encoder. This allows fair comparison of the quality of the representations learned by the models during pretraining without the need for extensive finetuning. The results on 11 different datasets (10 of which are public) (Figure 2 and Table 1) show that MIRAGE significantly outperforms the other models in both OCT and SLO diagnostic and staging tasks. Among the different tasks, MIRAGE was particularly effective in diagnosing and staging AMD (see, for example, the results on Duke iAMD or 9C from Figure 2), the most prevalent retinal disease worldwide (affecting 8.7% of the global population)81. On the other hand, the lowest relative performance of MIRAGE was observed in the discrimination of diabetes-related lesions (see the results on the OLIVES dataset from Figure 2). These results are consistent with the distribution of the diseases in the pretraining dataset, which contains larger proportion of AMD samples (> 50%) compared to other diseases such as diabetes (< 2%). In contrast, at least 85% of the OCT scans used to train RETFound were from the Moorfields diabetic image dataset (MEH-MIDAS). Further analysis of the results (Supplementary Table 3) showed that, while MIRAGE was less robust than RETFound to the data shift introduced by patient treatment in this dataset, it achieved similar performance on treatment-naıve baseline scans of the patients, underscoring the importance of pretraining data in the generalization capabilities of models to very specialized tasks. However, an additional cross-dataset evaluation (Figure 3) shows that MIRAGE is also more robust to general dataset shifts than the other models. In contrast to previous work27, 33, we also evaluated the performance of MIRAGE in retinal lesion and layer segmentation. We believe that evaluating FMs in these tasks is essential, as they represent the most common use cases of AI in clinical practice, due to high demand for OCT biomarker detection and quantification. In addition, such evaluations can provide meaningful insights into the capabilities of FMs in capturing the internal structure of the data and finegrained but relevant details. Our model was again compared to DINOv2 and RETFound, but even more importantly to MedSAM42, FM for medical image segmentation. Similarly to the classification tasks, the encoder of each model was frozen, and only the initial linear projection layers and convolutional decoder were fine-tuned on the target datasets. This adds up to about 12M parameters, which is less than half the total parameters of U-Net model ( 31M parameters)74, the most commonly used model for medical image segmentation70, 78. The results on five public and one private dataset (Figure 4 and Table 2) show that MIRAGE significantly outperforms the other FMs in both lesion and layer segmentation tasks in OCT and SLO images. The differences were especially pronounced in the cross-dataset evaluation (Figure 5). To further contextualize these results, we also compared MIRAGE to the specialized segmentation models nnUNet70 (version 2), SwinUNETR67, MedNeXt68, and TransUNet69. As shown in Figure 6 and Table 3, MIRAGE performs on par with the specialist models on the 17 within-dataset evaluation, and significantly outperforms them in the cross-dataset evaluation. The nnUNet framework was specifically designed to maximize performance on single dataset by tuning both the architecture and the hyperparameters, as well as the data augmentation strategies and other training settings, to that specific dataset. While this approach can lead to high performance on the target dataset, it often comes at the cost of generalization to new datasets, as demonstrated by the results of this evaluation. While the other specialist models are not as tuned in detail as nnUNet for the specific tasks, they are still designed to maximize performance on existing medical image segmentation benchmarks, which are usually well-curated and homogeneous, with similar training and testing distributions. Our results show that these models are not well suited for real-world clinical applications, where the ability to generalize to new, unseen data is essential. In contrast, our model, pretrained on diverse set of multimodal data, demonstrates superior generalization capabilities, highlighting its potential for real-world clinical applications. The qualitative analysis of the segmentations (Figure 7) is consistent with the previous quantitative results. While nnUNet, the most performant and generalizable specialist model, generally performs well, it fails more often in the presence of pathological signs, such as large fluid pockets. MIRAGE, on the other hand, is more robust to these changes, resulting in more accurate segmentations. The differences with the other FMs are more pronounced, and the higher quality of the segmentations produced by MIRAGE is evident in the visualizations. These results demonstrate the greater robustness of MIRAGE over both existing FMs and highly specialized models in retinal lesion and layer segmentation tasks, and therefore its greater potential for clinical applications. To further investigate the benefits of using retinal layer pseudo-labels during pretraining, we compared the performance of model pretrained with OCT images alone to that of model pretrained with OCT images and pseudo-labels for the retinal layers. The results of this analysis  (Table 4)  show that the model pretrained with OCT images and pseudo-labels for the retinal layers significantly outperformed the model pretrained with OCT images alone in both OCT classification and segmentation tasks. These results demonstrate the positive impact of using pseudo-labels during pretraining. Similarly, to further verify the benefits of domain-specific multimodal pretraining, we compared the downstream performance of MIRAGE to equivalent models pretrained on only one modality (i.e., OCT or SLO) and to models pretrained on multimodal natural images. The results  (Table 5)  show that MIRAGE significantly outperforms the other models in most tasks, with significant improvements on average. This demonstrates the potential of multimodal pretraining on paired imaging data for the development of robust and generalizable medical FMs. Previous work has shown that the performance of FMs can be significantly improved by increasing the model capacity21, 29, 32, 44. For this reason, most FMs are based on high-capacity models such as the ViT-Large architecture33, 80, 82. However, large models such as ViT-Large are computationally expensive, making them impractical or unusable for low-resource settings or real-time applications. To mitigate this problem, we have also developed smaller version of MIRAGE based on the ViT-Base architecture, which has about 72% fewer parameters than the ViT-Large model. Our results  (Table 6)  show that, while the performance of the ViT-Large version of MIRAGE is generally better than the ViT-Base version (with significant improvements in OCT classification), the differences are not substantial, and the ViT-Base version still outperforms the other FMs by most metrics. This suggests that the ViT-Base version of MIRAGE may be more practical choice for low-resource settings, while still providing state-of-the-art performance in retinal image analysis. In recent years, the development of FMs has become central topic in AI research in general21, 32, 44, 83, and in medicine in particular27, 33, 80, 84, 85. Pivotal works such as RETFound33 and GMAI84 have demonstrated the potential of FMs for medical image analysis and for democratizing access to medical AI. The development of MIRAGE further extends this line of research by demonstrating the benefits of multimodal pretraining on paired medical image data for the development of more robust and generalizable FMs. Our MIRAGE model can be easily adapted to various retinal imaging tasks, including classification and segmentation in both OCT and SLO images, with minimal tuning. Through the use of efficient tuning strategies and our proposed evaluation benchmark, we showed the superior performance of MIRAGE in wide spectrum of downstream retinal image analysis tasks. However, the applications of MIRAGE are not limited to these tasks. Similar to other FMs, MIRAGE could be effectively used as backbone in any method that requires the extraction of high-level features from OCT or SLO images. More than performant classification or segmentation method, MIRAGE is general tool for retinal image analysis with wide range of applications. In this sense, we believe that integrating domain-specific FMs like MIRAGE into AI-based systems could significantly improve their accuracy and generalizability, ultimately leading to higher-quality AI models for healthcare. This contrasts with the development of performant but non-generalizable models that are often developed, and which can lead to skepticism about the benefits of AI and limit its adoption33. In line with previous work33, 42, we have made MIRAGE publicly available for research, with the goal of improving reproducibility and accelerating the progress of AI in retinal image analysis. While this work has comprehensively validated MIRAGE in several tasks, there are still limitations and challenges that need to be explored in future work. First, all the data used to develop MIRAGE is from single center (Medical University of Vienna, Austria), and most of the samples are from patients with AMD, with smaller number of samples from other diseases, such as diabetes. Therefore, it is important to pursue larger dataset by including retinal images across the world with more diverse and realistic data distribution. Second, the pretraining of MIRAGE and the evaluation of its OCT diagnostic performance were limited to the central 2D B-scans of the OCT volumes. This was done, in line with previous work33, to avoid the high computational cost of processing the entire 3D OCT derived from the use of ViT models. However, this is limitation, as the use of 3D information is essential in clinical practice, and previous work58, 8688 has shown that it can significantly improve the performance of models in retinal image analysis tasks. Thus, research on developing efficient ways to incorporate 3D information into the pretraining and evaluation of FMs is an important direction for future work, that could ultimately lead to the development of more accurate FMs. Third, retinal layer segmentations for pretraining were generated using graph-theoretic approach and image processing techniques47, 48. While this algorithm is OCT device-agnostic, and guarantees the topological correctness of the segmentations, its accurate performance is limited to healthy images. For this reason, layer segmentations are considered as weak labels or pseduo-labels. It is likely that the segmentation performance on pathological images could be improved by using more sophisticated segmentation algorithms, which could even incorporate the segmentation of pathological structures such as fluid pockets. Such segmentations would likely further improve the representation learning of MIRAGE and its downstream performance, especially on highly pathological segmentation datasets. In particular, we plan to investigate the use of topology-aware deep learning algorithms such as SD-LayerNet89. Fourth, while MIRAGE has shown strong performance in segmentation tasks, especially in the cross-dataset evaluation, it is slightly less performant than some task-specific state-of-the-art models89, 90. This is likely due to the fact that these methods are specifically designed for the segmentation task at hand, while MIRAGE was tuned using general and straightforward semantic segmentation setup. This was done to minimize the impact of modules other than the pretrained encoder on the results, and to focus primarily on the evaluation of the pretraining strategy. Nevertheless, MIRAGE is not inherently incompatible with more sophisticated segmentation methods and architectures, and it is likely that its performance could be improved by using any of these innovations. Exploring this question represents an interesting area for future work. Fifth, while the linear probing and decoder-only fine-tuning strategies allowed us to effectively evaluate MIRAGE and SOTA FMs and achieve strong performance in most tasks, more sophisticated adapters or tuning strategies 19 could improve downstream performance. common example is Low-Rank Adapters (LoRA)91, which enables the adaptation of large models to downstream tasks by tuning small set of modules interleaved with the original architecture. This approach has been shown to improve the performance of large models in OCT medical image analysis45. In addition, it is very likely that fully fine-tuning the models could improve downstream classification performance if enough data is available33, 92, as is the case for segmentation. Unfortunately, there are not as many large-scale datasets available for OCT classification as there are for other imaging modalities, such as natural images, and, despite successful advances in the field, it is not guaranteed that full fine-tuning would not lead to overfitting on small datasets. Investigating how to adapt MIRAGE to maximize performance while maintaining generalizability represents an interesting area for future work. Finally, the current version of MIRAGE only uses image information from the OCT and SLO modalities, and pseudo-labels of retinal layers. The inclusion of color fundus photography (CFP), which is considered to be more informative and much more widely used than SLO (especially in developing countries), would make MIRAGE even more useful. In this study, it was not possible to include CFP since, to the best of our knowledge, no large dataset of paired OCTCFP data is available (either publicly or in our clinic), and this pairing is essential for multimodal pretraining. This is because, unlike OCT and SLO images (which are commonly acquired together by modern OCT devices), CFP images are usually acquired using separate imaging device or not at all when fundus is examined primarily with slit lamp, making it difficult to collect paired OCTCFP data. In addition, incorporating data such as clinical notes or demographic information during pretraining could provide further meaningful feedback to the model, ultimately improving representations. In this sense, we believe that MIRAGE could open the path towards more effective vision language models (VLMs) for OCT. While generalist large language models (LLMs) have demonstrated strong language capabilities for ophthalmology, they have also demonstrated limited vision capabilities for feature detection93, 94. Therefore, incorporating robust and generalizable vision encoder such as MIRAGE could improve the feature detection and thus overall performance. These limitations and future directions represent interesting research opportunities in the field and areas for improvement of the proposed FM. In conclusion, we proposed MIRAGE, the first multimodal vision foundation model for OCT and SLO image analysis. To demonstrate its effectiveness and efficiency in adapting to various healthcare applications, we evaluated MIRAGE and other FMs on newly proposed benchmark. Unlike previous evaluation approaches, our benchmark includes segmentation tasks in addition to diagnosis and staging, and is composed of heterogeneous set of 19 tasks from 16 datasets. The results on our benchmark show significant performance improvements of MIRAGE over existing FMs in detecting and staging ocular diseases and segmenting retinal lesions and layers in OCT and SLO images. MIRAGE was found to be particularly robust to dataset shifts and generalizable to different types of tasks and modalities, with the largest differences over existing FMs found in OCT segmentation. In light of these results, we believe that MIRAGE can serve as an effective multimodal foundation model for retinal image analysis, with potential applications in research and clinical practice. In addition, we hope that our benchmark will help to better assess the capability of FMs for OCT, facilitating future comparisons and tracking progress in the field."
        },
        {
            "title": "Pretraining dataset",
            "content": "The pretraining dataset, VIBES46, described in detail in Tables 7 and 8, consists of 261 184 samples from 42 082 patients and 75 653 unique eyes. This was obtained after filtering out samples with very poor SLO quality from the original dataset of 350 005 samples. Each sample consists 20 Table 7: Details of the pretraining dataset. Our VIBES46 pretraining dataset consists of 261 184 samples (OCT and SLO scan pairs) from 42 082 patients and 75 653 unique eyes. Age at scan* Gender* Vendor # B-scans Field of view (mm2) 0-20 20-40 40-60 60-80 80-103 Male (0.8%) (5.6%) Female (16.0%) (52.8%) (24.8%) (41.2%) (58.8%) Cirrus Spectralis (54.4%) (45.6%) 128 25 49 19 200 97 Other 6 6 (49.8%) 6 5 (15.0%) 6 4 (14.5%) 4 4 (5.1%) 9 8 (4.6%) 6 7 (4.0%) (7.0%) Other (83.8%) (5.1%) (1.7%) (1.1%) (1.0%) (1.0%) (6.3%) *Estimate based on the information available for 54.8% of the samples. of triplet of paired OCT and SLO images and pseudo-labels of retinal layers. Pseudo-labels of retinal layers were generated specifically for the purpose of this study using an automated segmentation algorithm47, 48. All scans were acquired between April 2007 and April 2021 at the Macula Clinic, Department of Ophthalmology and Optometry, Medical University of Vienna (MedUni Wien). The MedUni Wien Ethics Committee approved the post hoc analysis of the dataset (EK-Nr: 2095/2018), and the requirement for informed consent was waived due to the retrospective nature of the study and the de-identification of data, which was performed in accordance with institutional policies. The work adhered to the tenets of the Declaration of Helsinki and MedUni Wien standards of good scientific practice. Images were acquired with Cirrus (Carl Zeiss Meditec, Dublin, CA, USA) and Spectralis devices (Heidelberg Engineering, Heidelberg, Germany). The image resolution for Cirrus Bscans is always 512 1024 pixels (height width). For Spectralis, the B-scan height is always 496 pixels, but the width varies between 512, 768, and 1024 pixels. Following previous work33, only the central B-scans of the 3D OCT volumes were used for the analysis. The dataset includes wide range of retinal diseases, lesions, and conditions (see Table 8), with the most common being cataract, choroidal neovascularization (CNV), age-related macular degeneration (AMD), retinal vascular occlusion (RVO), and glaucoma."
        },
        {
            "title": "Benchmark datasets",
            "content": "The evaluation benchmark consists of 14 publicly available datasets, including 9 for classification, 4 for segmentation, and one for both classification and segmentation. To make the multimodal evaluation even more comprehensive, we also incorporated private dataset for classification (including both OCT and SLO images) and private dataset for lesion segmentation on SLO images. This adds up to total of 16 datasets and 19 different tasks. The selected retinal disease classification datasets, listed in Table 9, contain images from both normal eyes and from patients exhibiting retinal signs of various diseases and conditions, including glaucoma, DME, early, intermediate, and advanced AMD ([e,i,a]AMD), RVO, geographic atrophy (GA), macular hole (MH), central serous retinopathy (CSR), epiretinal membrane (ERM), retinal artery occlusion (RAO), vitreomacular interface disease (VID), Stargardt disease, and CNV. The scans were acquired using diverse set of OCT devices: Spectralis, Cirrus, Triton (Topcon, Tokyo, Japan), RTVue XR (Optovue, Fremont, USA) or Bioptigen (Leica Microsystems, Wetzlar, Germany). In addition, the datasets come from clinics in 6 different countries: USA, China, Austria, Iran, Russia, and India. As it was done for the pretraining dataset and in previous work33, we restricted the analysis to the central B-scans of the OCT volumes. We also compiled five retinal layer and lesion segmentation datasets to benchmark our model on retinal lesion and layer segmentation tasks (see Table 10). In addition to the public datasets, we included private dataset with SLO images for the segmentation of geographic atrophy (GA). 21 Table 8: Distribution of diseases, lesions, and conditions in the pretraining dataset. Estimate based on diagnostic labels available for 30.8% of the samples. Diagnostic label Cataract(a)/lens(opacity) Choroidal neovascularization Age-related macular degeneration Other % 57. 35.58 33.69 19.67 Various (nevus, melanoma, endophthalmitis, retinitis, etc.) 13.56 Anti-VEGF therapy Unhealthy retina Healthy retina restricted Healthy retina Retinal vascular occlusion Glaucoma Chorioretinopathia centralis serosa Epiretinal membrane Fibrosis, scar Refractive anomalies Vitreous body Geographic atrophy Retinal surgery Laser intervention Fundus hypertonicus Cystic macular edema Macular hemorrhage Disciform macula degeneration (Junius-Khunt disease) Myopic CNV Pigment epithelial detachment Diabetes Macular telangiectasia Pattern dystrophy Macular hole, lamellar hole, NH defect Cortisone treatment Irvine Gass syndrome RPE tear Drusen papilla Coat, Best, and Stargardt diseases, Terson syndrome Subretinal fluid 8.47 8.09 7.89 7.75 7.44 6. 4.61 4.48 4.17 4.01 3.50 3. 3.32 3.03 2.93 2.62 2.50 1. 1.74 1.71 1.69 1.39 1.34 1. 1.09 1.01 0.99 0.33 0.33 0. Table 9: Details of the downstream classification datasets. Modality # Samples # Patients Classes (# Samples) Acquisition device Dataset Duke iAMD49 Duke Srinivasan GAMMA50 Harvard Glaucoma51 Kermany52, 53 Noor Eye Hospital54 OCTDL55 OCTID OLIVES57 UMN59 OCT OCT OCT OCT OCT OCT OCT OCT OCT, SLO OCT OPTIMA9C58 (in-house) OCT, SLO 383 45 100 1 000 109 148 2 064 572 1590 54 383 Control (115), iAMD (268) Cirrus 45 Control (15), DME (15), iAMD (15) Bioptigen 100 Control (50), Early glaucoma (26), Triton Intermediate/advanced glaucoma (24) 1 000 Control (557), Glaucoma (443) 4,686 Control (51 390), DME (11 598), CNV (37 455), Drusen (8 866) 148 Control (50), DME (50), AMD (48) 821 Control (332), AMD (1 231), DME (147), ERM (155), RAO (22), RVO (101), VID (76) Cirrus Spectralis Spectralis RTVue XR - Control (206), MH (102), AMD (55), Cirrus CSR (102), DR (107) 96 DME (931), DR (659) 54 DME (30), AMD (24) 3652 Control (183), RVO (763), Stargardt (130), DME (1 091), iAMD (1 128), GA (452), CNV1 (99), CNV2 (83), CNV3 (276) Spectralis Spectralis Spectralis (61%) Cirrus (38%) Triton (1%) Table 10: Details of the downstream segmentation datasets. Dataset Modality # Samples # Patients Classes (excluding background) Devices (# B-scans/sample) AROI62 OCT 24 (128*) 24 Layers: ILMIPL/INL, IPL/INLRPE, Cirrus RPEBM, Below BM Lesions: Cyst, PED, SRF Duke DME61 OCT 10 (11) 10 Layers: ILM, RNFL, GCL, IPL, INL, Spectralis OPL, ONL, ISM, OS, RPE Lesions: Fluid Duke iAMD49 OCT 384 (100) 384 Layers: ILMInner RPEDC, Cirrus GOALS64 RETOUCH63 OCT OCT Inner RPEDCOuter BM, Below BM 100 - Layers: RNFL, GCIPL, Choroid Triton 112 (128) 112 Lesions: IRF, SRF, PED Spectralis Cirrus Triton Spectralis SGA65 (in-house) 965 *Only an average of 47.3 B-scans were annotated. SLO 100 Lesions: GA These datasets cover wide range of retinal lesions and layers, some of which are subsets or combinations of others. In particular, the following lesions are included: cystoid edema (cyst), pigment epithelial detachment (PED), subretinal fluid (SRF), intraretinal fluid (IRF), and GA. The datasets also include the following retinal layers: inner limiting membrane (ILM), retinal nerve fiber layer (RNFL), retinal pigment epithelium (RPE), retinal pigment epithelium-drusen complex (RPEDC), ganglion cell layer (GCL), inner plexiform layer (IPL), inner nuclear layer (INL), outer plexiform layer (OPL), outer nuclear layer (ONL), external limiting membrane (ELM), inner segment myoid (ISM), outer segment (OS), ganglion cell-inner plexiform layer complex (GCIPL), Bruchs membrane (BM), choroid, and choroid-sclera interface (below BM). The scans were acquired with Cirrus, Spectralis, and Triton scanners at clinics in 5 different countries: USA, China, Austria, Croatia, and the Netherlands. In contrast to the classification datasets, all the B-scans in the segmentation datasets are used for training and evaluating the models. This was done following standard practices in the literature, where layer and lesion segmentation tasks are usually performed B-scan-wise, and not on the full volumes75, 76, 95. More details on all these datasets are listed in the Supplementary Note 2, with illustrative 23 examples in Supplementary Figures 1 and 2. When training the models on the tasks listed above, we followed the standard train-test splits for the public datasets when available. In cases where no standard splits were provided, we performed random split into training, validation, and test sets using patient and label stratification to ensure balanced representation."
        },
        {
            "title": "MIRAGE approach",
            "content": "The proposed framework for training and testing our foundation model, based on multimodal MAE approach23, is illustrated in Figure 8. The model architecture is based on ViT29 encoder and modality-specific linear projection layers and Transformer decoders. The ViT architecture was chosen because of its wide adoption in the literature (especially for foundation models18, 23, 32, 33, 42) and its demonstrated effectiveness in variety of medical image classification33 and segmentation tasks42. The modality-specific linear projection layers consist of flattening operation followed by linear layer. The decoders are shallow transformer networks consisting of linear projection layer followed by cross-attention layer, multi-layer perceptron Figure 8: Overview of the approach used for training and tuning our multimodal foundation model. Multimodal pretraining. Classification tuning. Segmentation tuning. The approach consists of training Vision Transformer (ViT) model on large dataset of multimodal retinal images (including OCT, SLO, and pseudo-labels of retinal layers) by reconstructing the input images from masked or partial view of them. Black arrows represent cross-attention operations between all encoded tokens and the modality tokens. The reconstruction loss = LO + LN + LL on the masked tokens is used as the objective function. In fine-tuning, the model can be trained on downstream tasks by replacing the decoders used for pretraining with task-specific heads. Moreover, it accepts both OCT and SLO images as input during inference. 24 (MLP), two Transformer blocks73, and linear projection layer followed by reshape operation to reconstruct the patches. The cross-attention operation (represented by black arrows in Figure 8) is used to allow the model to leverage information from the other modalities. The pretraining pretext task consists of reconstructing the input OCT, SLO, and retinal layer segmentation images from masked or partial versions of themselves. For this purpose, the images are first divided into modality-specific patches, and then the proportion of non-masked patches per modality is determined by sampling from symmetric Dirichlet distribution23 with concentration parameter α = 1. Then, non-masked tokens are sampled uniformly at random without replacement. This sampling ensures that the model receives both unimodal inputs (from one modality) and multimodal inputs (from two or three modalities) during training. Thus, it can handle any subset of the input modalities during downstream tasks. In Supplementary Note 3, we provide more details on this masking strategy and the selection of α. After the masking process, the unmasked visible patches are projected onto tokens using the modality-specific linear projection layers, concatenated, and passed through the encoder to generate the latent features. An additional global token with learned embedding is added to the input sequence to provide global context for the model, similar to the CLS token in ViT29. Patch tokens are then fed into the modality-specific decoders to reconstruct the previously masked patches. reconstruction loss that measures the difference between the original and reconstructed masked patches was used as the objective function. Following prior work18, 23, the loss is defined as the L2 distance for image patches and as cross-entropy loss (CE) for pseudo-labels of retinal layers. The total training loss is the sum of the losses for each modality so that = LO + LN + LL = ˆxO, xO2 + ˆxN , xN 2 + CE(ˆxL, xL) , (1) where the subscripts O, , and denote OCT, SLO, and retinal layers pseudo-labels, and ˆx and are the predicted and ground truth patches, respectively. Retinal layer segmentations for the pretraining dataset were generated using graphtheoretic approach and image processing techniques47, 48. This algorithm is OCT device-agnostic and guarantees the topological correctness of the segmentations. In this way, the model can effectively learn, during pretraining, the specific features of the retinal layers as well as the general, physical structure of the retina and the arrangement of these layers within that structure. To adapt the model to downstream tasks, the shallow transformer decoders are replaced with task-specific heads (see Figure 8, bottom). For classification, they are replaced by single linear layer followed by softmax activation function. For segmentation, they are replaced by simple convolutional segmentation head based on ConvNeXt66."
        },
        {
            "title": "Network architecture",
            "content": "The ViT backbone used in the experiments, shown in Figure 9, is the same as the one proposed by Dosovitskiy et al.29. The model consists of linear projection layer followed by positional embedding addition and stack of Transformer blocks, each containing multi-head selfattention mechanism and feed-forward neural network, with layer normalization and residual connection around each sub-block. The ViT-Base model has = 12 Transformer blocks, embedding dimension = 768, and = 12 attention heads (86M parameters). The ViT-Large model has = 24 Transformer blocks, embedding dimension = 1024, and = 16 attention heads (307M parameters). For each modality, different linear projection layers are used to project the input patches into the embedding space of the ViT backbone. The linear projection layers are implemented as flatten operation followed by fully-connected layer with p2 input features and output features, where is the patch size and is the embedding dimension of the ViT backbone (768 and 1024 for ViT-Base and ViT-Large, respectively). The rest of the linear projection layers 25 Figure 9: Overview of the ViT encoder. The linear projection layer projects the input patches into embeddings of dimension d, which are then passed through stack of Transformer blocks containing attention heads. in the model are implemented in the same way, only varying the number of input and output features when necessary. Projected patches from the different modalities are concatenated into sequence of tokens and given as input to the same Transformer encoder, as in the original ViT model described above. The modality-specific decoders, depicted in Figure 10, were implemented as shallow transformer networks18. Each decoder consists of linear projection layer followed by crossattention layer, multi-layer perceptron (MLP), two Transformer blocks and linear projection layer followed by reshape operation to reconstruct the patches. The cross-attention is performed between all encoded tokens (K, V) and the modality tokens (Q), allowing the decoder to integrate multimodal information. Figure 10: Overview of the modality-specific decoders. Modality-specific features are fed into the transformer-based decoders along with the rest of the encoded tokens through cross-attention operations. For the classification experiments, we used linear classifier head on top of the frozen ViT backbone to predict the class labels. In particular, we used global average pooling in the token dimension (excluding the global token) followed by linear layer with input features and output classes, where is the embedding dimension of the ViT backbone and is the number of classes. Finally, we applied softmax activation function to obtain the class probabilities. The segmentation head used in the segmentation experiments consists of the following operations (see Figure 11 for an overview). First, linear projection is applied to the output tokens of the encoder to increase their dimensionality to = 6144. Then, the tokens are reshaped to form feature map of size H/4 W/4 D/8. Finally, 4 ConvNeXt blocks66 are applied to this feature map before it is upsampled to full resolution (H ) using bilinear interpolation. 26 Figure 11: Overview of the ConvNeXt-based segmentation decoder. The output tokens of the encoder are projected to higher dimension and reshaped into feature map, which is then processed by series of ConvNeXt blocks and upsampled to full resolution."
        },
        {
            "title": "Implementation details",
            "content": "All experiments were implemented using PyTorch96 (pytorch.org) and the timm library (huggi ngface.co/docs/timm). For the evaluation, the scikit-learn (scikit-learn.org) library was used. Following previous work33, we used the Vision Transformer (ViT)29 architecture pretrained on ImageNet using MAE18 as the backbone of our model. The main experiments were conducted using the ViT-Large architecture, while version of MIRAGE based on the ViTBase architecture was trained to study the impact of domain-specific multimodal pretraining and model capacity. The pseudo-labels of retinal layers were generated using the Iowa Reference Algorithms 3.6 (iibi.uiowa.edu/oct-reference) (Retinal Image Analysis Lab, Iowa Institute for Biomedical Imaging, Iowa City, IA)47, 48. During model pretraining, the AdamW97 optimizer with base learning rate of 104 and weight decay of 0.05 was used. The learning rate follows the linear scaling rule98: lr = base lr batchsize/256. Model weights were initialized from ViT model pretrained with MAE18 on ImageNet37. The pretraining lasted 1600 epochs, with 40-epoch warm-up starting with learning rate of 106, which was then decayed to 0 using cosine decay. Training was performed with batch size of 256 on single A100 GPU (80 GB) with automatic mixed precision enabled. The input images were resized offline to 512512, and data augmentation was used to artificially increase the size of the dataset. In particular, we applied random affine transformations, slight intensity shifts, and horizontal flipping. The number of selected patches was fixed to 49 in the case of single modality, and 98 for two or three modalities, with patch size of 32 32 pixels. The proportion of tokens per modality λm was determined by sampling from symmetric Dirichlet distribution with α = 1, ensuring that the sum of λm across all modalities equals 1. In Supplementary Note 4, we provide our pre-experimental results confirming the effective functioning of the pretraining approach with the chosen hyperparameters. For the classification tasks, modality-specific decoders were replaced with single linear layer followed by softmax activation function. In the experiments, these are the only parameters of the model that are trained, while the encoder remains frozen. We train the models for maximum of 1000 epochs using the AdamW optimizer with learning rate of 103 and weight decay of 102. Early stopping was implemented from epoch 20, with patience of 20 epochs, saving model checkpoint if the balanced accuracy (BAcc) on the validation set exceeded the previous best or matched it with lower loss. The minimum improvement required was 0.1 percentage points. The batch size is configured to be 25% of the training set size, with maximum of 64. Similar to RETFound33, we performed label smoothing with smoothing 27 factor of 0.1. For each dataset, five random seeds were used to ensure the robustness of the results. Random augmentation was applied to the input images during training, including random horizontal flipping, affine transformations, and slight intensity shifts. For MIRAGE, min-max normalization was applied to the input images, while for the other models, they were standardized using ImageNet statistics. For the segmentation tasks, the modality-specific decoders were replaced by different modules depending on the experiment. For the state-of-the-art comparison, to maximize performance, we used convolutional segmentation head based on ConvNeXt66, as proposed by Bachmann et al.23. However, preliminary experiments showed that other segmentation heads such as DPT99 can be used with similar performance (Supplementary Table 5). In contrast, to analyze the impact of domain-specific multimodal pretraining, we used linear probing strategy. In particular, following DINOv232, we used single 1 1 convolutional layer followed by bicubic upsampling. This tuning setting, while less effective than the ConvNeXt head, allows us to minimize the impact of the added modules and focus on the effect of the pretraining approach. The best models were selected based on the validation set performance and evaluated on the test set. The models were then tuned for 200 epochs using the AdamW optimizer with 104 learning rate and batch size of 4. In the default tuning setup, the ViT encoder was frozen, and only the linear projection layers and the segmentation decoder were trained. In the full fine-tuning setup, the encoder was also trained. Random horizontal flipping and random cropping were applied to the input images during training. The size of the cropped images was set to 1024 1024 pixels. Similar to the classification tasks, images were normalized using min-max normalization for MIRAGE and ImageNet statistics for the other models. For the specialist models, nnUNet70, 71 was used as is, but fixing the training, validation, and test splits to ensure consistency with the other models. On the other hand, SwinUNETR-V267, MedNeXt68 and TransUNet69 were trained using similar setup to MIRAGE, with the same data augmentation, optimizer, hyperparameters, and model selection criteria. In Supplementary Note 5, we provide detailed analysis of the computational efficiency of MIRAGE and its different architectural variants (pretraining, classification, and segmentation) and comparison with other FMs in terms of pretraining efficiency. The results of this analysis are shown in Supplementary Tables 14 and 15."
        },
        {
            "title": "Classification evaluation procedure",
            "content": "Model performance on the classification tasks was evaluated using the area under the receiver operating characteristic curve (AUROC), average precision (AP), and balanced accuracy (BAcc) metrics. AUROC is calculated using the one-vs-all strategy, where the AUC of each class is computed against the rest100, 101. The overall AUROC is then calculated by averaging these AUCs using weighted average, where the weight is the number of positive samples in each class. The AP summarizes precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight. Similar to the AUROC, the total AP is calculated by averaging the APs of each class, weighted by the number of positive samples for each label. Finally, the BAcc is defined as the average of recall obtained on each class, as described by Mosley102. This combination of metrics allows us to obtain comprehensive understanding of model performance, especially in the context of class imbalance. For each task, the models were trained with five different random seeds to ensure robustness and reliability of the results. This approach helps to capture the inherent variability in the training process, as different random seeds influence the initialization of the shuffling and augmentation of the training data. The mean and standard deviation of the performance over the five replicas from the five seeds are then reported. To assess the statistical significance of the performance difference between the best and second-best models in each task, we used the one-tailed Students test33. On the other hand, 28 to compare the performance of the models across different tasks, we used the Wilcoxon signedrank test103."
        },
        {
            "title": "Segmentation evaluation procedure",
            "content": "To evaluate the accuracy of the predicted regions, we followed standard practices in the literature42, 61, 63, 64, 104 and calculated the volume Dice coefficient, volume intersection over union (IoU), and 95th percentile of the Hausdorff distance (HD95). Dice coefficient is measure of the overlap between the predicted and manual segmentations, with value of 1 indicating perfect overlap and 0 indicating no overlap. It is calculated as the intersection of the predicted and manual segmentations divided by the sum of their volumes. IoU is similar metric that is calculated as the intersection of the segmentations divided by their union. Finally, HD95 is measure of the maximum distance between the predicted and manual segmentations, with the 95th percentile of the distances used to reduce the impact of outliers. HD95 is measured in pixels. Since the evaluation on RETOUCH is performed using the official evaluation server, only the average absolute volume difference (AVD) is available apart from the Dice coefficient. Therefore, for RETOUCH, we report only these two metrics. The AVD is calculated as the absolute difference between the predicted and ground truth lesion volumes, in cubic millimeters (mm3), averaged across all lesions. In the segmentation tasks, the models were trained to segment all classes simultaneously, including lesions and layers in datasets where both types of segmentation were available. All models were trained with single seed, as the training process was stable and little variability was observed between different runs. In each dataset, we computed the metrics for every class for each B-scan; then, the B-scans were grouped by patient, and the averages for each lesion were computed, which were ultimately averaged to get the final performance for that patient. The mean and standard deviation of the performance of the model were calculated at the patient level. Similar to the classification tasks, the statistical significance of the performance difference between the best and second best models in each task was assessed using the one-tailed Students test, while the Wilcoxon signed-rank test was used to compare the performance of the models across different tasks."
        },
        {
            "title": "Data availability",
            "content": "The pretraining data used in this study come from an in-house dataset, while the data for the evaluation come from 14 publicly available datasets and two in-house datasets. Publicly available data can be obtained from the original sources, with the exception of GOALS, which is no longer available but has been uploaded to our GitHub repository. The links to the datasets are as follows: Duke iAMD49 (people.duke.edu/sf59/RPEDC_Ophth_2013_da taset.htm); Duke Srinivasan60 (people.duke.edu/sf59/Srinivasan_BOE_2014_dataset.ht m); GAMMA50 (gamma.grand-challenge.org/); Harvard Glaucoma51 (github.com/HarvardOphthalmology-AI-Lab/Harvard-GDP); Kermany52, 53 (data.mendeley.com/datasets/rscb jbr9sj/2); Noor Eye Hospital54 (hrabbani.site123.me/available-datasets/dataset-fo r-oct-classification-50-normal-48-amd-50-dme); OCTDL55 (data.mendeley.com/data sets/sncdhf53xc/4); OCTID56 (borealisdata.ca/dataverse/OCTID); OLIVES57 (zenodo.o rg/records/7105232); UMN59 (people.ece.umn.edu/users/parhi/.DATA/); AROI62 (ipg. fer.hr/ipg/resources/oct_image_database); Duke DME61 (people.duke.edu/sf59/Chi u_BOE_2014_dataset.htm); GOALS64 (github.com/j-morano/MIRAGE); RETOUCH63 (ret ouch.grand-challenge.org/). Notwithstanding, to facilitate reproducibility, we also provide these datasets with the same splits used in this study, except for Kermany52, in our GitHub repository: https://github.com/j-morano/MIRAGE. The Kermany dataset with the original 29 split, which is the same we used in this work, can be accessed at the original URL provided by the authors and listed above. Due to privacy concerns, the in-house VIBES46, OPTIMA9C58, and SGA65 datasets cannot be made publicly available."
        },
        {
            "title": "Code availability",
            "content": "The underlying code for this study, the weights of the pretrained models, and the full detailed results are available on GitHub and can be accessed via the following link: https://github.c om/j-morano/MIRAGE."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by Austrian Federal Ministry of Economy, Energy and Tourism, the National Foundation for Research, Technology and Development, the Christian Doppler Research Association, Heidelberg Engineering, and Austrian Science Fund (FWF) grant 10.55776/FG9. For open access purposes, the author has applied CC BY public copyright license to any author-accepted manuscript version arising from this submission."
        },
        {
            "title": "Author contributions",
            "content": "J.M. conceived the methodology, designed and executed all experiments, conducted all subsequent statistical analyses, and drafted the manuscript. B.F. helped design the experiments and contributed to the validation of the proposed method and the writing. E.S. contributed to the preparation of the benchmark data, the implementation of the downstream experiments, and the writing. R.F. contributed to the implementation of the segmentation baselines. T.E. helped with the design of the experiments. M.G. and G.F. contributed to the data preparation and analysis. M.O. contributed to the analysis of the results and writing the supplementary material. U.S. contributed to the collection and preparation of the pretraining dataset. H.B. contributed to the design of the experiments, analysis of the results, and made substantial revisions and edits of the draft manuscript. All authors contributed to the manuscript."
        },
        {
            "title": "Competing interests",
            "content": "The authors declare no competing interests."
        },
        {
            "title": "Supplementary Information",
            "content": "Supplementary Note 1: Extended results In the following, we provide the extended tables of the experimental results, showing not only the average performance across all datasets, but also the results for each individual dataset. For the classification tasks, we report the area under the receiver operating characteristic curve (AUROC), average precision (AP), and balanced accuracy (BAcc). For the segmentation tasks, we report the Dice score, intersection over union (IoU), and 95th percentile of the Hausdorff distance (HD95) for all datasets except RETOUCH. For the RETOUCH dataset, we used the official evaluation server of the challenge to compute the metrics, which include the Dice score and absolute volume difference (AVD). To facilitate the comparison of the results, we highlight the best results in bold and underline the second best results. In addition, we have color-coded the best results according to the model. The color code is as follows (excluding models with no best results): MIRAGE (blue), RETFound33 (red), DINOv232 (green), SLIN29 (black), nnUNet70 (magenta), MIRAGEFFT (cyan), MultiMAE23 (teal), MAEOCT and MAE-SLO (salmon), MIRAGE-Base (blue-gray), OCT+Layers (lime), and TransUNet (dark gray). In all cases, the one-tailed Students t-test was used to assess the statistical significance of the performance difference between the best (in bold) and second best (underlined) models in each dataset. The Wilcoxon signed-rank test was used to assess the statistical significance of the performance differences across all datasets. Statistical significance is indicated by asterisks in the tables: *p < 0.05, **p < 0.01, ***p < 0.001. In those cases where in-house datasets were used for the evaluation, we provide the average performance across both all the datasets and only public datasets (excluding in-house datasets). 31 Supplementary Table 1: Performance of MIRAGE and state-of-the-art foundation models SL-IN29, DINOv232, and RETFound33 on the different datasets for the diagnosis and staging of ocular diseases on OCT. All models are based on the ViT-Large architecture and were tuned using linear probing. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Dataset Model AUROC AP BAcc Duke iAMD GAMMA 97.63 0.23 SL-IN 99.13 0.34 DINOv2 RETFound 98.24 0.66 MIRAGE 99.52 0.23** 74.83 1.67 SL-IN 71.99 2.62 DINOv2 RETFound 82.30 2.27 MIRAGE 87.50 0.64** Harvard Glaucoma SL-IN 75.11 0.90 72.29 2.31 DINOv2 RETFound 82.14 0.68 MIRAGE 82.75 0. Kermany 98.65 0.04 SL-IN 98.67 0.06 DINOv2 RETFound 98.92 0.07 MIRAGE 97.56 0.35 99.23 0.30 98.69 0.50 99.52 0.23** 64.02 1.89 60.87 3.75 73.27 3.65 81.00 1.89* 74.19 0.92 72.60 2.48 81.53 0.71 82.54 0.68 95.71 0.12 95.87 0.11 96.50 0. 94.49 0.45 95.53 0.80 96.03 1.72 96.71 1.42 54.72 2.72 45.83 4.48 59.17 2.72 63.61 4.16* 69.32 1.02 64.80 1.94 74.14 1.28 76.15 1.25 86.15 0.68 86.48 0.39 85.89 0.51 99.53 0.01*** 98.39 0.04*** 91.40 0.25*** Noor Eye Hospital 97.03 0.78 SL-IN 97.53 0.67 DINOv2 RETFound 95.77 0.87 MIRAGE 98.63 0.40** 95.06 1.21 96.19 0.86 94.24 0.96 97.69 0.68** 98.27 0.15*** 93.32 0.27** 84.67 2.67 90.00 2.11 86.00 2.49 92.67 3.27 79.69 1.39 77.59 1.95 74.30 1.39 76.93 2. 89.54 0.81 85.04 2.49 88.16 2.04 87.53 1.58 95.79 0.54 88.64 0.39 95.81 0.27 93.14 0.27 69.63 0.83 67.27 1.22 74.02 0.64 91.24 0.21 88.26 1.06 91.72 0.66 95.71 0.44 95.96 0.83 96.81 0.65 96.54 0.84 97.48 0.11 93.46 0.22 97.68 0.25 94.36 0. 86.65 0.32 86.81 0.14 91.76 0.32 99.01 0.02*** 93.52 0.16*** 77.95 0.45*** 95.59 5.80*** 92.99 6.39*** 84.01 10.51*** 88.63 11.21 87.69 12.23 91.35 8.15 80.44 12.86 77.91 15.10 81.50 11.48 88.87 11.87 87.80 12.96 91.30 8. 81.80 13.02 79.24 15.50 82.44 11.84 95.17 6.02*** 92.92 6.77*** 84.77 10.91*** OCTDL OCTID OLIVES OPTIMA9C (in-house) Average Average (public only, excluding in-house) 96.86 0.16 SL-IN 95.22 0.42 DINOv2 RETFound 96.96 0.28 MIRAGE 98.72 0.11 SL-IN 98.81 0.20 DINOv2 RETFound 98.98 0.28 MIRAGE 99.07 0.28 97.89 0.09 94.13 0. SL-IN DINOv2 RETFound 98.05 0.15* MIRAGE 96.06 0.09 97.33 0.07 SL-IN 97.44 0.08 DINOv2 RETFound 98.62 0.08 MIRAGE 92.67 9.51 SL-IN 91.69 10.64 DINOv2 RETFound 94.44 6.66 MIRAGE 92.09 9.93 SL-IN 90.97 11.07 DINOv2 RETFound 93.92 6.89 MIRAGE Supplementary Table 2: Performance of MIRAGE and state-of-the-art foundation models SL-IN29, DINOv232, and RETFound33 on the different datasets for the diagnosis of ocular diseases on SLO. All models are based on the ViT-Large architecture and were tuned using linear probing. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001)."
        },
        {
            "title": "AUROC",
            "content": "AP"
        },
        {
            "title": "BAcc",
            "content": "OPTIMA9C SL-IN DINOv2 MIRAGE 93.40 0.14*** 77.38 0.47* 55.55 0.45** 75.99 0.45 73.22 0.44 52.96 1.42 52.67 1.45 91.96 0.15 91.89 0."
        },
        {
            "title": "Average",
            "content": "SL-IN DINOv2 MIRAGE 74.08 0.52 73.22 2.34 73.14 3.38 SL-IN DINOv2 MIRAGE 83.74 9.67* 82.59 9.52 82.52 9.68 73.71 2.76 71.55 3.61 73.40 0.52 74.85 2.28 72.39 2.70 75.39 2. 64.61 2.70 58.61 2.12 66.72 1.14 58.79 6.21 55.64 3.48 61.13 5.65** Supplementary Table 3: Performance of MIRAGE and RETFound33 models on the first and last scans of the OLIVES dataset (DME/DR discrimination) under linear probing. OLIVES data comes from two clinical studies where patients are treated throughout the process. As shown in the original paper57, treatment produces meaningful changes in the retina, and causes domain shift in the data for the same patient, which is reflected in the performance of the models on the last scans. The lower robustness of MIRAGE to this domain shift can be explained by the much lower number of samples from diabetic patients used in the pretraining of the model. In particular, less than 2% of the samples from our VIBES dataset are from diabetic patients, while 85% of the samples from the dataset used to train RETFound are from the Moorfields diabetic image dataset (MEH-MIDAS)33."
        },
        {
            "title": "AUROC",
            "content": "AP"
        },
        {
            "title": "BAcc",
            "content": "OLIVES (first scans) RETFound 98.80 0."
        },
        {
            "title": "MIRAGE",
            "content": "95.00 0.00 98.92 0.67 100.00 0.00 100.00 0.00 95.00 0.00 OLIVES (last scans) RETFound 94.00 3."
        },
        {
            "title": "MIRAGE",
            "content": "80.20 3.54 94.71 3.25 82.76 3.43 84.00 5.83 70.00 4.47 Supplementary Table 4: Performance of MIRAGE and state-of-the-art foundation models SL-IN29, DINOv232, and RETFound33 on the cross-dataset evaluation setting for OCT diagnosis. All models are based on the ViT-Large architecture and were tuned using linear probing. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Dataset Model AUROC AP BAcc Noor Eye Hospital trained on UMN + Duke Srinivasan 90.05 0.99 SL-IN 83.59 1.23 DINOv2 RETFound 81.63 2.97 MIRAGE 82.65 1.82 73.49 1.96 66.57 4. 94.79 0.74** 92.27 0.74*** UMN + Duke Srinivasan SL-IN trained on Noor Eye Hospital 86.23 0.72 78.76 1.01 DINOv2 RETFound 79.63 1.06 MIRAGE 88.59 3.48 80.74 0.77 72.73 1.06 72.81 1.53 85.69 4.78 66.72 5.01 66.77 3.80 47.39 1.40 64.98 1.70 65.01 1.83 68.60 1.82 68.74 2.87 79.81 2.37** Supplementary Table 5: Performance of MIRAGE with different decoders on different downstream segmentation tasks: Segmenter105, DPT99, and ConvNeXt23, 66. In all cases, the encoder is based on the ViT-Base architecture and is kept frozen during tuning. Statistical significance between the best (bold) and second best (underlined) models across all SLO and OCT datasets was assessed using the Wilcoxon signed-rank test. However, no statistical significance was found (p > 0.05). MIRAGE performs particularly well with ConvNeXt and DPT decoders, but still offers adequate performance with the more lightweight Segmenter decoder. Modality Dataset Model Dice IoU HD95 SLO SGA (lesions) Segmenter DPT ConvNeXt 71.97 22.70 77.07 18.89 67.64 21.61 153.31 57.12 74.47 22.11 169.34 78.47 161.13 88.94 61.54 23.88 64.79 24.23 Duke DME (layers) OCT Duke DME (lesions) Segmenter DPT ConvNeXt 82.86 3.60 72.39 4.52 82.68 3.40 57.53 5.50 70.96 4.83 71.23 5.12 19.45 7.29 8.23 4.23 8.57 4. Segmenter DPT ConvNeXt 69.06 9.15 62.38 9.26 63.92 9.48 45.87 10.00 47.54 10.15 53.36 11.00 49.32 30.38 50.32 18.13 40.46 11.58 GOALS (layers) 82.87 Segmenter DPT 92.01 ConvNeXt 92. 71.45 85.33 86.79 14.20 4.89 4.87 OCT & SLO Average 72.40 7.25 Segmenter 78.92 10.17 DPT ConvNeXt 79.82 9.00 59.10 9.17 67.87 13.49 69.04 12.08 55.97 63.17 63.33 62.75 51.72 60. 34 Supplementary Table 6: Performance of MIRAGE and state-of-the-art foundation models DINOv232, RETFound33, and MedSAM42 on the different datasets for the segmentation of retinal lesions and layers. Mean and standard deviation values are calculated across the patients in each dataset, except for the GOALS and RETOUCH datasets, where patient information is not available. All models are based on the ViT-Large architecture, except for MedSAM, which is only available in the ViT-Base version. The models were fine-tuned using both the decoder-only fine-tuning strategy. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). RETOUCH results were obtained using the official evaluation server of the challenge, which only includes the Dice score and absolute volume difference (AVD), which is reported in the HD95 column. Only the Dice score is included for the calculation of the average performance across all datasets. Modality Dataset Model Dice IoU HD95 / AVD AROI (layers) AROI (lesions) Duke DME (layers) OCT Duke DME (lesions) 85.60 2.08 DINOv2 RETFound 85.23 1.51 MedSAM 90.85 2.13 MIRAGE 93.79 0.85*** 28.82 8.88 DINOv2 RETFound 28.09 8.39 MedSAM 38.26 8.86 MIRAGE 52.18 16.63** 76.44 5.90 DINOv2 RETFound 75.32 4.80 MedSAM 79.06 4.47 MIRAGE 83.02 2.93** 52.27 12.63 DINOv2 RETFound 51.53 9.53 MedSAM 60.21 9.74 MIRAGE 69.72 9.16* 75.90 2.99 75.46 2.04 84.01 2.95 88.67 1.42*** 19.64 6.66 19.42 6.32 27.96 7.08 40.89 13.70** 63.03 7.64 61.47 6.00 66.14 6.05 71.41 4.18** 36.15 11.34 35.16 8.91 43.60 9.58 54.14 11.14* GOALS (layers) RETOUCH (lesions) DINOv2 81.28 RETFound 62.79 MedSAM 90.36 92.46 MIRAGE DINOv2 53.60 RETFound 57.32 MedSAM 69.40 79.60 MIRAGE Average 63.00 19.99 DINOv2 RETFound 60.05 18.15 MedSAM 71.36 18.37 MIRAGE 78.46 14.26* OCT cross-dataset 77.51 8.36 Duke iAMD (layers) RETFound 78.84 7.59 MedSAM 75.77 11.54 trained on AROI MIRAGE DINOv2 91.06 2.43*** 68.68 47.16 82.57 86. - - - - 52.68 21.29 47.73 19.58 60.86 21.98 68.24 18.40* 66.17 10.26 67.56 9.49 66.55 12.22 84.62 3.38*** 49.13 23.88 62.22 25.58 13.97 9.07 8.21 2.30 7.80 4.99 3.25 1.43** 69.69 32.14 67.91 34.61 46.65 15.64 37.56 12.17* 21.38 11.53 19.60 12.12 12.01 5.16 9.81 4.77* 66.96 43.42 68.51 25.25 55.52 27.47 42.45 22.88 41.34 121.79 10.38 4.96 0.13 0.11 0.06 0.03 42.67 22.80 57.20 40.57 26.47 20.34 19.61 16.87* 60.93 59.86 53.87 53.22 39.98 31.38 4.62 4.52*** 210.02 85.59 182.22 86.21 SLO SGA (lesions) 61.34 24.56 DINOv2 MedSAM 72.20 24.05 MIRAGE 75.31 22.16*** 65.81 24.14*** 164.42 95.35** 35 Supplementary Table 7: Performance of MIRAGE and state-of-the-art specialist segmentation models SwinUNETR-V267, MedNeXt68, TransUNet69, and nnUNet70 on the different datasets for the segmentation of retinal lesions and layers. Mean and standard deviation values are calculated across the patients in each dataset, except for the GOALS and RETOUCH datasets, where patient information is not available. MIRAGE was trained using both the decoder-only fine-tuning and full fine-tuning (FFT) strategies. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Modality Dataset Model Dice IoU HD95 / AVD AROI (layers) AROI (lesions) Duke DME (layers) OCT Duke DME (lesions) GOALS (layers) RETOUCH (lesions) Average SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT SwinUNETR-V2 MedNeXt OCT cross-dataset Duke iAMD (layers) TransUNet trained on AROI nnUNet MIRAGE MIRAGEFFT 94.00 1.42 94.07 1.45 94.63 1.27 95.05 0.79 93.79 0.85 95.02 0. 48.98 13.64 49.70 14.62 54.30 16.23 62.99 21.28 52.18 16.63 60.51 18.79 81.90 3.10 80.87 3.48 81.52 3.25 83.49 2.55 83.02 2.93 83.44 3.00 54.44 11.16 63.69 8.96 65.12 10.24 66.05 10.70 69.72 9.16 70.04 7.99 89.06 2.34 89.19 2.29 90.10 2.11 90.81 1.34 88.67 1.42 90.73 1.30 38.20 10.40 39.22 11.92 43.50 12.94 51.87 19.16 40.89 13.70 48.74 15.82 69.88 4.26 68.44 4.76 69.36 4.42 72.15 3.53 71.41 4.18 72.08 4. 38.04 10.56 47.24 9.85 48.97 11.28 50.05 11.55 54.14 11.14 54.36 9.66 91.77 91.55 92.38 93.04 92.46 92.01 74.29 77.43 76.87 72.93 79.60 79.61 84.92 84.54 85.97 87.08 86.08 85.30 - - - - - - 74.23 17.26 76.22 15.48 77.47 14.28 78.92 12.49 78.46 14.26 80.10 11. 64.02 22.09 65.73 19.78 67.58 18.84 70.39 17.06 68.24 18.40 70.24 16.52 3.66 1.82 3.60 1.68 2.74 1.18 2.33 0.79 3.25 1.43 2.18 0.47 41.27 13.17 37.85 16.77 35.82 13.57 49.65 30.74 37.56 12.17 42.09 14.84 15.14 5.79 12.51 5.73 9.81 5.44 9.31 4.06 9.81 4.77 7.85 3.54 111.72 98.36 77.86 53.93 51.15 18.93 38.11 12.22 42.45 22.88 38.86 22.73 8.41 6.86 7.04 4.13 4.96 5.32 0.06 0.04 0.05 0.06 0.03 0.04 36.04 40.01 27.73 27.80 21.31 18.88 20.71 19.41 19.61 16.87 19.26 17. 46.88 1.72 47.05 1.47 46.88 1.59 61.21 15.33 91.06 2.43 91.29 2.14*** 84.92 3.15*** 3.94 2.27*** 201.81 24.71 203.56 29.73 194.12 36.45 97.95 60.82 4.62 4.52 43.90 2.70 44.05 2.34 43.76 2.61 55.35 12.98 84.62 3.38 SLO SGA (lesions) SwinUNETR-V2 MedNeXt TransUNet nnUNet MIRAGE MIRAGEFFT 74.51 23.51 77.22 19.62 82.12 15.92* 79.41 19.26 75.31 22.16 79.36 17.07 65.09 24.88 67.87 22.19 73.69 18.65* 71.33 21.25 65.81 24.14 70.03 19.92 169.03 94.67 157.15 84.21 127.19 57.98 136.56 74.10 164.42 95.35 136.16 43.48 36 Supplementary Table 8: Performance of ViT-Base model on the downstream classification tasks with linear probing pretrained with and without retinal layer pseudo-labels on our VIBES46 dataset. Statistical significance between the best (bold) and second best models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Dataset Modalities AUROC AP BAcc Duke iAMD GAMMA Harvard Glaucoma Kermany OCT OCT+Layers 99.58 0.03*** 99.60 0.03*** 95.72 1.03** 93.08 0.80 99.01 0.05 98.89 0.06 OCT OCT+Layers 84.86 0.36*** 81.51 0.18 73.62 2.34 72.98 0.73 52.22 1.67 52.50 2.22 OCT OCT+Layers 83.92 0.98*** 83.87 0.87*** 77.17 1.89** 69.67 0.81 77.02 0. 76.61 0.95 OCT OCT+Layers 98.93 0.01*** 96.66 0.07*** 87.60 0.42*** 84.20 0.39 95.84 0.22 98.64 0.07 Noor Eye Hospital OCT OCT+Layers 99.60 0.50** 97.50 0.75 96.02 1.08 99.27 0.91** 86.00 3.89 94.67 1.63** OCTDL OCTID OLIVES OPTIMA9C Average OCT OCT+Layers 98.41 0.10** 96.65 0.63 90.76 1.32 94.12 0.16** 73.96 2.09 80.92 0.78** OCT OCT+Layers 98.60 0.04*** 95.10 0.08 94.95 0.22 98.10 0.07 84.41 0.32 85.78 0.46*** OCT OCT+Layers 96.85 0.03** 96.28 0.21 95.84 0.09* 94.66 1.08 96.67 0.41* 92.81 2.14 OCT OCT+Layers 99.02 0.01*** 93.51 0.10** 98.82 0.04 93.26 0. 77.68 0.55 78.15 0.73 OCT OCT+Layers 95.44 6.00*** 92.17 8.07*** 82.81 12.62*** 79.77 12.76 90.73 8.59 93.75 7.99 Supplementary Table 9: Performance of ViT-Base model on the downstream segmentation tasks with linear probing pretrained with and without retinal layer pseudo-labels on our VIBES46 dataset. Statistical significance between the best (bold) and second best models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Modality Dataset Model Dice IoU HD95 / AVD AROI (layers) AROI (lesions) OCT OCT+Layers 89.47 1.76*** 81.65 2.55*** 9.09 4.25* 13.52 4.93 73.58 4.03 84.12 2.89 OCT OCT+Layers 47.27 18. 42.54 17.83 30.29 14.28 34.84 14.65 72.68 40.85 56.10 28.64* OCT Duke DME (layers) Duke DME (lesions) OCT OCT+Layers 72.99 4.15 72.23 3.81 57.38 4.62 58.37 5.02* OCT OCT+Layers 57.42 16.29 56.73 11.92 41.63 15.00 40.34 11. GOALS (layers) RETOUCH (lesions) OCT 77.45 OCT+Layers 81.20 OCT 64.49 OCT+Layers 66.78 63.74 68.80 - - Average OCT OCT+Layers 69.07 14.21* 66.37 13.68 53.32 15.51 56.80 17.42 OCT cross-dataset Duke iAMD (layers) OCT trained on AROI 55.40 6.90 39.18 6.09 OCT+Layers 65.91 3.91*** 49.65 3.86*** 23.47 11.61 27.88 14.52 65.07 41.03 52.89 28.38 29.41 25.70 0.09 0.06 40.83 23.58 34.33 17. 214.22 19.67*** 222.92 15.67 37 Supplementary Table 10: Performance of ViT-Base model on the downstream classification tasks with linear probing for different pretraining strategies. The strategies include MultiMAE23, pretrained on ImageNet using multimodal data, MAE-OCT and MAE-SLO, trained using MAE18 on the OCT or SLO images of our VIBES46 dataset, respectively, and MIRAGE, our proposed model based on multimodal MAE trained on our multimodal VIBES dataset. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Tuning modality Dataset Model AUROC AP BAcc Duke iAMD GAMMA MultiMAE 94.86 2.41 MAE-OCT 98.89 0.06 MIRAGE 99.05 0.09* 95.91 1.85 99.01 0.05 99.18 0.07* 88.08 1.87 93.08 0.80 94.68 1.26* MultiMAE 74.64 1.40 MAE-OCT 81.51 0.18 MIRAGE 61.20 1.60 73.62 2.34 55.83 2.22 52.22 1.67 58.89 4. 85.52 0.23*** 74.26 0.57 Harvard Glaucoma Kermany MultiMAE 72.06 1.43 MAE-OCT 76.61 0.95 MIRAGE 78.57 0.18** 70.90 1.57 77.02 0.69 78.95 0.19** 70.48 0.40* 65.63 2.21 69.67 0.81 MultiMAE 97.66 0.02 MAE-OCT 98.64 0.07 MIRAGE 98.77 0.03* 93.22 0.06 95.84 0.22 96.23 0.09* 79.44 0.93 84.20 0.39 85.84 0.32*** OCT Noor Eye Hospital OCTDL OCTID OLIVES OPTIMA9C Average OPTIMA9C SLO OLIVES Average MultiMAE 96.73 0.54 MAE-OCT 97.50 0.75 MIRAGE 98.53 0. MultiMAE 93.28 0.54 MAE-OCT 96.65 0.63 MIRAGE 97.51 0.14* MultiMAE 97.15 0.17 MAE-OCT 98.10 0.07 MIRAGE 98.81 0.24** MultiMAE 97.03 0.31 MAE-OCT 96.85 0.03 95.00 0.13 MIRAGE MultiMAE 95.64 0.25 MAE-OCT 99.02 0.01** MIRAGE 98.91 0.05 MultiMAE 91.01 9.60 MAE-OCT 93.75 7.99 MIRAGE 95.81 0.46 96.02 1.08 97.52 0.95 85.56 0.68 90.76 1.32 92.61 0.32* 89.43 0.50 94.95 0.22 95.90 0.82* 95.10 1.04 95.84 0.09 91.83 0. 81.93 0.68 93.51 0.10 93.58 0.05 85.45 11.59 90.73 8.59 89.33 1.33 86.00 3.89 93.33 4.22 67.28 2.10 73.96 2.09 77.83 0.89* 75.51 0.85 84.41 0.32 85.83 1.41 96.70 0.39 96.67 0.41 91.11 0. 63.71 2.96 77.68 0.55 78.54 0.10* 75.73 13.06 79.77 12.76 94.52 6.97*** 91.12 8.15** 81.84 11.24*** MultiMAE 89.88 0.95 MAE-SLO 84.77 0.48 MIRAGE 92.83 0.25** 70.19 2.22 59.67 1.02 75.60 0.50** 51.30 0.69* 46.60 2.30 36.23 1.46 MultiMAE 77.17 1.14 MAE-SLO 64.57 0.38 MIRAGE 78.49 0.74 76.03 1.21 62.19 0.49 77.49 0.78 65.65 1.51 66.98 0.54 71.79 0.98*** MultiMAE 83.52 6.44 MAE-SLO 74.67 10.11 MIRAGE 85.66 7.19** 73.11 3.42 60.93 1.49 76.54 1.15** 61.55 10.28** 56.13 9.72 51.61 15.42 38 Supplementary Table 11: Performance of ViT-Base model on the downstream segmentation tasks with linear probing for different pretraining strategies The strategies include MultiMAE23, pretrained on ImageNet using multimodal data, MAE-OCT, and MAE-SLO, both trained using MAE18 on the OCT or SLO images of our VIBES46 dataset, respectively, and MIRAGE, our proposed model based on multimodal MAE trained on our multimodal VIBES dataset. Statistical significance between the best (bold) and second best (underlined) models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Modality Dataset Model Dice IoU HD95 / AVD AROI (layers) AROI (lesions) Duke DME (layers) MultiMAE 74.01 3.16 MAE-OCT 84.12 2.89 MIRAGE 60.28 3.93 73.58 4.03 32.46 13.69 13.52 4.93 89.73 1.48*** 81.93 2.32*** 8.59 3.52** MultiMAE 17.72 7.38 MAE-OCT 42.54 17.83 MIRAGE 43.84 14.84 11.23 5.02 30.29 14.28 31.92 11.52 MultiMAE 55.88 8.49 MAE-OCT 72.23 3.81 MIRAGE 40.97 8.14 57.38 4.62 75.03 3.85*** 60.80 4.91*** 17.44 9.93** 80.69 28.04 72.68 40.85 69.78 69. 38.03 23.22 23.47 11.61 OCT Duke DME (lesions) MultiMAE 1.29 1.78 MAE-OCT 57.42 16.29 62.68 7.72 MIRAGE 0.65 0.91 41.63 15.00 46.01 8.23 GOALS (layers) RETOUCH (lesions) MultiMAE 65.30 MAE-OCT 77.45 80.52 MIRAGE MultiMAE 32.34 MAE-OCT 64.49 65.99 MIRAGE 48.97 63.74 68.07 - - - Average MultiMAE 41.09 26.13 MAE-OCT 66.37 13.68 MIRAGE 69.63 14.60* 32.42 22.72 53.32 15.51 57.75 17.35* OCT cross-dataset Duke iAMD (layers) MultiMAE 33.97 11.20 MAE-OCT 55.40 6.90 trained on AROI MIRAGE 21.32 8.22 39.18 6. 63.32 4.19*** 46.76 4.03*** 165.52 77.51 65.07 41.03 43.54 24.19 85.58 29.41 32.27 0.16 0.09 0.07 80.46 47.68 40.83 23.58 34.33 21.42 226.25 27.87 214.22 19.67*** 217.92 16.78 SLO SGA (lesions) MultiMAE 68.69 23.43 MAE-SLO 70.63 22.54 MIRAGE 72.24 21.68 57.76 24.21 59.79 23.36 61.79 23.55 187.07 88.52 174.96 78.10 166.25 73.90 39 Supplementary Table 12: Performance of the ViT-Large and ViT-Base versions of MIRAGE on the downstream classification tasks using linear probing. Statistical significance between the best (bold) and second best models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Modality Dataset Model AUROC AP BAcc Duke iAMD GAMMA Harvard Glaucoma Kermany MIRAGE-B 99.05 0.09 MIRAGE-L 99.52 0.23* 99.18 0.07 99.52 0.23* 94.68 1.26 96.71 1. MIRAGE-B 85.52 0.23 MIRAGE-L 87.50 0.64** 74.26 0.57 81.00 1.89*** 63.61 4.16 58.89 4.27 MIRAGE-B 78.57 0.18 MIRAGE-L 82.75 0.65*** 82.54 0.68*** 76.15 1.25*** 70.48 0.40 78.95 0. MIRAGE-B 98.77 0.03 MIRAGE-L 99.53 0.01*** 98.39 0.04*** 91.40 0.25*** 85.84 0.32 96.23 0.09 Noor Eye Hospital MIRAGE-B 98.53 0.50 MIRAGE-L 98.63 0.40 97.52 0.95 97.69 0. 93.33 4.22 92.67 3.27 OCT OCTDL OCTID OLIVES OPTIMA9C (in-house) Average MIRAGE-B 97.51 0.14 MIRAGE-L 98.27 0.15*** 93.32 0.27** 92.61 0.32 77.83 0.89 76.93 2.94 MIRAGE-B 98.81 0.24 MIRAGE-L 99.07 0.28 95.90 0.82 96.54 0. 85.83 1.41 87.53 1.58 MIRAGE-B 95.00 0.13 MIRAGE-L 96.06 0.09*** 94.36 0.24*** 93.14 0.27*** 91.11 0.07 91.83 0.12 MIRAGE-B 98.91 0.05 MIRAGE-L 99.01 0.02* 93.58 0.05 93.52 0. 78.54 0.10* 77.95 0.45 MIRAGE-B 94.52 6.97 MIRAGE-L 95.59 5.80*** 92.99 6.39*** 84.01 10.51*** 81.84 11.24 91.12 8.15 Average (public only, no in-house) MIRAGE-L 95.17 6.02*** 92.92 6.77*** 84.77 10.91*** MIRAGE-B 93.97 7. 82.25 11.86 90.81 8.60 OPTIMA9C (in-house) MIRAGE-B 92.83 0.25 MIRAGE-L 93.40 0.14** 75.60 0.50 77.38 0.47*** 55.55 0.45*** 51.30 0. SLO OLIVES Average MIRAGE-B 78.49 0.74*** 77.49 0.78*** 71.79 0.98** MIRAGE-L 74.08 0.52 66.72 1.14 73.40 0. MIRAGE-B 85.66 7.19 MIRAGE-L 83.74 9.67 76.54 1.15 75.39 2.05 61.55 10.28 61.13 5.65 40 Supplementary Table 13: Performance of the ViT-Large and ViT-Base versions of MIRAGE on the downstream segmentation tasks for OCT and SLO datasets using decoder-only finetuning and full fine-tuning (FFT). Mean and standard deviation values are calculated across the patients in each dataset, except for the GOALS and RETOUCH datasets, where patient information is not available. Statistical significance between the best (bold) and second best models in each and across all datasets was assessed using the one-tailed Students t-test and Wilcoxon signed-rank test, respectively (*p < 0.05, **p < 0.01, ***p < 0.001). Modality Dataset Model Dice IoU HD95 / AVD AROI (layers) AROI (lesions) MIRAGE-B 94.04 1.18 MIRAGE-L 93.79 0.85 89.12 1.93 88.67 1.42 3.39 1.12 3.25 1.43 MIRAGE-B 49.12 16.58 MIRAGE-L 52.18 16.63 38.11 13.67 40.89 13.70 38.64 14.99 37.56 12. Duke DME (layers) MIRAGE-B 82.86 3.60 MIRAGE-L 83.02 2.93 71.23 5.12 71.41 4.18 8.57 4.01 9.81 4.77 Duke DME (lesions) MIRAGE-B 69.06 9.15 MIRAGE-L 69.72 9. 53.36 11.00 54.14 11.14 49.32 30.38 42.45 22.88 GOALS (layers) RETOUCH (lesions) MIRAGE-B 92.87 MIRAGE-L 92.46 MIRAGE-B 75.87 MIRAGE-L 79. 86.79 86.08 - - Average MIRAGE-B 77.30 15.37 MIRAGE-L 78.46 14.26 67.72 19.58 68.24 18.40 4.87 4.96 0.06 0.03 20.96 19.17 19.61 16. Duke iAMD (layers) MIRAGE-B 90.14 3.33 trained on AROI MIRAGE-L 91.06 2.43*** 84.62 3.38*** 4.62 4.52*** 83.30 4.27 12.29 10.04 OCT OCT cross-dataset SLO SGA (lesions) MIRAGE-B 74.47 22.11 MIRAGE-L 75.31 22.16 64.79 24.23 65.81 24.14 161.13 88.94 164.42 95.35 Supplementary Note 2: Benchmark datasets This section provides brief description of the 16 datasets used in the evaluation benchmark. B-scan dimensions are always in the format [B-scan height (A-scan depth) B-scan width (# A-scans)] pixels. The number of B-scans per volume is indicated in each case. The field of view (FOV) always refers to the retinal en-face view. The datasets are publicly available unless otherwise noted, and links to the datasets are provided."
        },
        {
            "title": "Classification datasets",
            "content": "In the following, we provide brief description of the 11 classification datasets used in the evaluation benchmark. For all datasets that provide full OCT volumes, we limited our analysis to the central B-scan, as done in previous work33. Examples of images from each dataset and the corresponding ground truth labels are shown in Supplementary Figure 1. Duke iAMD49. The Duke intermediate age-related macular degeneration (Duke iAMD) dataset includes 38,400 SD-OCT B-scans from 269 iAMD patients and 115 healthy subjects centered on 5 mm diameter at the fovea. The images were acquired using Bioptigen SDOCT system (Research Triangle Park, NC) at four different clinics in the USA. Each volume has retinal FOV of 6.76.7 mm2, and consists of 100 B-scans with dimensions of 5121000 pixels. In addition to the diagnosis, the dataset includes expert-annotated segmentation maps for three key retinal regions: the inner limiting membrane (ILM) to the inner retinal pigment epithelium (RPE) detachment complex (RPEDC), the inner RPEDC to the outer Bruchs membrane (BM), and below the BM. For the classification evaluation, we focused on the detection of iAMD, while for the segmentation evaluation, we focused on the segmentation of all the aforementioned layers. Link: https://people.duke.edu/sf59/RPEDC_Ophth_2013_dataset.htm Duke Srinivasan60. This dataset presented by Srinivasan et al.60 consists of OCT volumes acquired from 45 patients. The scans were categorized according to the diagnosis of the patients: healthy (15 samples), dry AMD (15), and diabetic macular edema (DME) (15). The scans were acquired using Spectralis device (Heidelberg Engineering, Heidelberg, Germany) at Duke University, Harvard University, and the University of Michigan, USA. The FOV and OCT dimensions are very heterogeneous; for more information, see Table 1 from the original publication60. For the purposes of this study, we focused on the detection of AMD and DME. Link: https://people.duke.edu/sf59/Srinivasan_BOE_2014_dataset.htm GAMMA50. This dataset originates from the Glaucoma grAding from Multi-Modality imAges (GAMMA) challenge50, held in conjunction with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2023. It was provided by the Sun Yat-sen Ophthalmic Center, China, and contains pairs of color fundus and OCT images from randomly selected subjects with and without glaucoma. In particular, the public dataset includes 100 samples from the same number of patients. OCT volumes were acquired using Topcon DRI OCT Triton device (Topcon, Tokyo, Japan). All volumes are centered on the macula with FOV of 3 3 mm. Each volume has 25 B-scans with dimensions of 992 512 pixels. The dataset also includes glaucoma stages, foveal coordinates, and cup and optic disc segmentation masks. There are 26 samples with early glaucoma, 24 with moderate or advanced glaucoma, and 50 normal samples. In this study, we focus on the detection and staging of glaucoma. Link: https://gamma.grand-challenge.org/) Harvard Glaucoma51. This dataset from the Mass Eye and Ear of Harvard Medical School, USA, consists of 1 000 samples from 1 000 patients. Each sample includes the diagnostic label Supplementary Figure 1: Example images from classification datasets. OCT and/or SLO images are shown for each dataset along with the corresponding ground truth labels. 43 indicating the presence or absence of glaucoma (557 and 443, respectively), an OCT volume centered on the optic nerve head, retinal nerve fiber layer thickness (RNFLT) map, and other demographic and clinical information such as visual field test results and patient age. The OCT volumes and RNFLT maps were acquired using Cirrus (Carl Zeiss Meditec, Jena, Germany) device. Each volume has FOV of 6 6mm2 and 200 B-scans of 200 300 pixels. Link: https://github.com/Harvard-Ophthalmology-AI-Lab/Harvard-GDP Kermany52, 53. This dataset, presented by Kermany et al.52, 53, consists of 109 309 OCT and 5 862 chest X-ray images. The OCT images, from 4 686 patients, were acquired using an Spectralis device at different clinics in California, USA, Shanghai, China, and Beijing, China. Each sample was then assigned to one of the following four classes: choroidal neovascularization (CNV) (37 455 samples), DME (11 598), drusen (8 866), and control (51 390). The FOVs and dimensions of the OCT scans vary considerably across the dataset52. Link: https://data .mendeley.com/datasets/rscbjbr9sj/2 Noor Eye Hospital54. This dataset from the Noor Eye Hospital in Tehran, Iran, consists of 148 OCT volumes from the same number of patients. Every volume is categorized into one of three classes, depending on the diagnosis: control (50 samples), dry AMD (48), and DME (50). Scans are centered on the macula and were acquired using Spectralis device with different FOVs, resolutions, and number of B-scans. Link: https://hrabbani.site123.me/availabledatasets/dataset-for-oct-classification-50-normal-48-amd-50-dme OCTDL55. The Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) is public dataset from the Ural Federal University Named afer the first president of Russia B. N. Yeltsin, Russia. The dataset consists of 2 064 B-scans from 821 patients labeled based on disease type and retinal pathology. Scans were acquired using an Optovue Avanti RTVue XR (Fremont, CA, USA) device centered on the fovea with different field of views (FOVs) and image resolutions. Specialists classified the images into one of the following classes: control (332 samples), AMD (1 231), DME (147), epiretinal membrane (ERM) (155), retinal artery occlusion (RAO) (22), retinal vein occlusion (RVO) (101), and vitreomacular interface disease (VID) (76). Link: https://data.mendeley.com/datasets/sncdhf53xc/ OCTID56. The Optical Coherence Tomography Image Database (OCTID) is public dataset of OCT B-scans from the Sankara Nethralaya (SN) Eye Hospital, India. The images were acquired using Cirrus HD-OCT machine with raster scan protocol with 2 mm scan length and an image resolution of 512 1024 pixels. Images were labeled based on the diagnosis of retinal clinical experts at the SN hospital. Specifically, the selected 572 B-scans were categorized into the following classes: control (206 samples), macular hole (MH) (102), AMD (55), central serous retinopathy (CSR) (102), and diabetic retinopathy (DR) (107). In each volumetric scan, fovea-centered image was selected by an experienced clinical optometrist. The images were then resized to 500x750 pixels. Link: https://borealisdata.ca/dataverse/OCTID OLIVES57. The Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) is longitudinal dataset consisting of 78 189 OCT B-scans (1590 volumes) and scanning laser ophthalmoscopy (SLO) (referred to as near-infrared fundus images) from 96 patients with DR or DME. The OLIVES dataset is derived from the PRIME106 and TREX-DME107 clinical studies run at the Retina Consultants of Texas, USA. Every volume has around 49 B-scans, with an image resolution of 496 504 pixels. It includes 16 biomarker labels, 4 clinical labels, and disease diagnosis for either DME (931 samples) or DR (659 samples). The images were acquired using Spectralis devices. Link: https://zenodo.org/records/7105232 44 UMN59. The University of Minnesota (UMN) dataset, collected by the University of Minnesota Ophthalmology Clinic, USA, consists of 54 OCT volumes from 30 patients with DME and 24 patients with AMD. All scans were acquired using Spectralis device centered on the macula. Each volume contains 25 B-scans with an image resolution of 496 1 024 pixels. Link: https://people.ece.umn.edu/users/parhi/.DATA/ OPTIMA9C58. This private dataset comes from the imaging data collection at OPTIMA Lab, Medical University of Vienna, Austria. The dataset contains 4 205 treatment-naive OCT volumes and their corresponding SLO images from 3 652 patients. The OCT volumes are baseline scans from randomized, multicenter clinical trials and include data from several manufacturers: Spectralis, Cirrus, and Triton. The dataset consists of nine classes based on clinical study data: iAMD (clinical studies NCT01790802 and NCT00891735, Observational study 1108); three types of CNV (NCT02307682, NCT01780935, NCT01972789); geographic atrophy (GA) (NCT02503332, Observational study 265); RVO (NCT01599650, NCT01535261); DME (NCT01331681, NCT01627249); Stargardt disease (Observational study 3109); and healthy samples (NCT03465124 and the fellow eyes from the clinical studies NCT00891735, NCT01780935, NCT01948830, NCT01599650, and NCT01535261). Thus, the classes and the number of samples per class are as follows: control (183), RVO (763), Stargardt (130), DME (1 091), iAMD (1 128), GA (452), CNV1 (99), CNV2 (83), and CNV3 (276). OPTIMA9C is notable for its variation in spatial resolution and disease severity, making it more challenging and unique than other datasets. Image widths range from 200 to 1536 pixels, with heights varying from 480 to 1024 pixels, while the number of slices per volume ranges from 25 to 261 slices, with an average of 81. For privacy reasons, the dataset cannot be made publicly available."
        },
        {
            "title": "Segmentation datasets",
            "content": "The segmentation datasets used in the evaluation benchmark are briefly described below. In contrast to the classification datasets, all the B-scans in the segmentation datasets are used for training and evaluating the models. This was done following standard practices in the literature, where layer and lesion segmentation tasks are usually performed B-scan-wise, and not on the full volumes75, 76, 95. Examples of images from each dataset and the corresponding segmentation labels are shown in Supplementary Figure 2. AROI62. The Annotated Retinal Optical coherence tomography Images (AROI) dataset comprises 3 200 B-scans collected from 25 patients diagnosed with neovascular AMD. Macular SDOCT volumes were acquired with the Zeiss Cirrus HD OCT 4000 device at the Sestre milosrdnice University Hospital Center, Croatia. Each OCT volume consisted of 128 B-scans with resolution of 1024 512 pixels (pixel size 1.96 11.74µm). Retinal fluids and layers were annotated for 1 136 B-scans. In our study, one of the OCT volumes was discarded (patient14) due to inconsistencies in the annotations, resulting in 3 072 B-scans from 24 patients. Of the fluids, the following were annotated: pigment epithelium detachment (PED), subretinal fluid and subretinal hyperreflective material (both labeled as SRF), and intraretinal fluid (IRF) (named cyst in the dataset). Four layer boundaries were also annotated: internal limiting membrane (ILM), inner plexiform layer/inner nuclear layer (IPL/INL), retinal pigment epithelium (RPE), and Bruchs membrane (BM). Translating the annotations from boundaries to layers results in the following labels: ILMIPL/INL, IPL/INLRPE, RPEBM, below the BM. Link: https://ipg.fer.hr/ipg/resources/oct_image_database Duke DME61. The Duke Diabetic Macular Edema (Duke DME) dataset, provided by Duke University, USA, consists of 110 annotated OCT B-scan images from 10 patients with severe DME. Each patient features 11 B-scans centered on the fovea with dimensions of 496536 pixels. 45 Supplementary Figure 2: Example images from segmentation datasets. OCT and/or SLO images are shown for each dataset along with their corresponding segmentation masks. Scans were acquired using Spectralis device. The dataset includes detailed annotations of eight retinal layer boundaries: ILM, retinal nerve fiber layer (RNFL), ganglion cell layer (GCL), IPL, INL, outer plexiform layer (OPL), outer nuclear layer (ONL), inner segments of photoreceptors (ISM), outer segments of photoreceptors (OS), and RPE. It also includes annotations for fluid regions. Link: https://people.duke.edu/sf59/Chiu_BOE_2014_dataset.htm Duke iAMD49. See the description in the Classification Datasets section above. GOALS64. The Glaucoma OCT Analysis and Layer Segmentation (GOALS) dataset is part of the GOALS Challenge, held in conjunction with MICCAI 2022. The full dataset comprises 300 circumpapillary OCT B-scans randomly selected from glaucoma study cohorts collected at the Zhongshan Ophthalmic Center, China. However, only the 100 B-scans from the training subset were publicly available at the time of preparing the evaluation benchmark, so we used this subset in our study. The number of patients is unknown, as no information is provided in the dataset description and no anonymized patient identifiers are included in the data. FOV information is not provided either. All scans were acquired using Topcon DRI Swept Source OCT system. All B-scans have dimension of 8001100 pixels. Each B-scan is annotated with the following three layers: RNFL, GCLIPL (GCIPL), and choroid. Since the dataset is not publicly available at the time of writing, we will upload it to our repository: https://github.com/j-morano/MIRAGE. RETOUCH63. The retinal OCT fluid detection and segmentation benchmark and challenge (RETOUCH) dataset originates from the MICCAI 2017 retinal OCT fluid challenge. Annotations and scans came from different clinical centers: Medical University of Vienna, Austria, Erasmus University Medical Center, Netherlands, and Radboud University Medical Center, Netherlands. The dataset includes 70 OCT volumes, with half of the patients diagnosed with macular edema secondary to AMD and the other half with edema secondary to RVO. Each B-scan is labeled with three retinal fluid types: IRF, subretinal fluid (SRF), and PED. The training data consists of volumes from three OCT systems: 24 from Cirrus (Model 5000), 22 from Triton (Models T-1000/T-2000), and 24 from Spectralis. Cirrus scans consist of 128 Bscans of 1024 512 pixels, Triton scans consist of 128 B-scans 512 650 or 512 885 pixels, and Spectralis scans consist of 49 B-scans 512 496 pixels. There is at least one fluid lesion in each volume. For this dataset, the results are obtained using the official evaluation script provided by the challenge organizers. Link: https://retouch.grand-challenge.org/ SGA65. This dataset for the segmentation of geographic atrophy (SGA) is private dataset consisting of 965 samples consisting of OCT volumes as well as SLO and fundus autofluorescence (FAF) images. All samples come from 100 patients (184 eyes) diagnosed with GA who were part of clinical study on natural GA progression conducted at the Medical University of Vienna, Austria65. The SLO and FAF images were acquired with Spectralis device centered on the macula with FOV of 6 6 mm2 and resolution of 1024 1024 pixels. OCT and SLO images were co-registered by the device, while FAF and SLO images were registered with an in-house image registration pipeline based on aligning retinal vessel segmentation110. All samples have GA en face masks annotated by retinal expert on FAF images. In this work, we used only the SLO images and the corresponding GA masks. For privacy reasons, the dataset cannot be made publicly available. Supplementary Note 3: Masking strategy During pretraining, following MultiMAE23, we sample the number of non-masked tokens for each modality from symmetric Dirichlet distribution with concentration parameter α = 1. This value results in diverse sampling across the different modalities. Below, we provide an analysis of the effect of different values of α on the token distribution across three modalities: OCT, SLO, and Layers. In Supplementary Figure 3, we show 3D visualization of the number of non-masked tokens per modality for 2000 samples when using different values of α. In the plot, each point represents sample, and its coordinates correspond to the number of non-masked tokens for each modality. The number of non-masked tokens per modality out of the fixed total (98) was sampled from Dirichlet distribution with the specified α value. When α is set to 0.1, most samples are concentrated near the edges of the simplex, meaning that in most cases nearly all tokens are from single modality. This leads to frequent unimodal processing, potentially causing the model to ignore multimodal interactions. For α = 1, samples are spread across the entire simplex, indicating that the model is more likely to receive tokens from multiple modalities. However, as shown in the plot, it is still possible for the model to receive tokens from only one or two modalities. In this way, the model can learn to process each modality independently while still leveraging multimodal information. At α = 100, almost all samples are tightly clustered around the center of the simplex, indicating that the number of tokens per modality is roughly equal (32 tokens per modality). While this configuration ensures balanced multimodal learning, it may limit the ability of the model to process samples where only one modality is available. To ensure that the model can effectively leverage the multimodal information while still being able to process each modality independently, we set α to 1, which provides good balance between modality specialization and multimodal learning. In our training approach, once the number of non-masked tokens per modality has been sampled, the non-masked tokens are sampled uniformly at random without replacement and used as input to the model. Supplementary Figure 3: 3D scatter plots of token allocations across OCT, SLO, and Layers for different Dirichlet concentration parameters (α = 0.1, 1, and 100). Each point represents sample, with the coordinates corresponding to the number of non-masked tokens for each modality. total of 2000 samples are shown for each α value, with the number of total tokens fixed at 98. Lower α values (left) result in sparse, modality-dominant allocations, while higher α values (right) enforce more uniform token distributions. 48 Supplementary Note 4: Pre-experimental results To comprehensively assess the effectiveness of the proposed multimodal pretraining approach, we conducted series of analyses evaluating the quality of the learned representations of MIRAGE without fine-tuning the model on downstream tasks. In particular, we performed three different analyses: training loss, reconstruction visualization, and feature visualization. Loss curves. To analyze the convergence behavior of the models during pretraining, we show in Supplementary Figure 4 the loss curves for the three training losses (one for each modality) of MIRAGE during the pretraining stage, as well as the total loss and the learning rate. The curves show that the model converges well to values close to zero for all losses, indicating that the model is able to reconstruct the input patches effectively. Supplementary Figure 4: Loss curves of MIARGE during pretraining. All training losses for the three modalities (OCT, SLO, and Layers) are significantly minimized during the pretraining stage. Qualitative reconstruction results. To further validate the effectiveness of our paired multimodal pretraining strategy, we show in Supplementary Figure 5 the predictions of MIRAGE on our internal dataset for different masking ratios of the input modalities. In the figure, it can be observed that the model is able to reconstruct the input patches from the different modalities with relatively high fidelity, even when large portion of the input modality is masked. For instance, we show that the model is able to predict the segmented layers from the OCT B-scans and the SLO, even when all patches from the input Layers modality are masked. This is also true in the reverse direction, where the model is able to reconstruct the OCT B-scans from the segmented layers and SLO images. Additionally, although the SLO reconstructions are of lower quality compared to OCT and Layers, the model is still able to reconstruct the coarse lesions from the OCT B-scans and the segmented layers. These visualizations suggest that the model effectively learns the relationships between the different modalities during the pretraining phase. Feature visualization. To provide qualitative understanding of the representations learned by MIRAGE, we visualize the embeddings produced by it, DINOv2, and RETFound for the 49 Supplementary Figure 5: MIRAGE predicitons on our internal dataset. For each sample (composed of an OCT B-scan, an SLO image, and layer segmentation) we show the predictions with different masking proportions for each modality. The visualizations show that the model is able to reconstruct masked patches from the different modalities, suggesting that the learned representations encode meaningful multimodal information. OCTID and Kermany datasets. Both models were used out-of-the-box without any fine-tuning. To project the embeddings into 2D space, we used the UMAP algorithm111. The results are shown in Supplementary Figure 6. As shown in the figure, the embeddings produced by MIRAGE are well separated and roughly cluster the samples according to their classes. While the embeddings produced by DINOv2 and RETFound also show good separation for OCTID, they are less distinct for the Kermany dataset. This indicates that MIRAGE has learned meaningful data representations during the pretraining stage. Supplementary Figure 6: Visualization of feature embeddings from different foundation models. These plots show the embeddings produced by DINOv2, RETFound, and MIRAGE for the OCTID and Kermany datasets after projection to 2D using UMAP. MIRAGE embeddings show better separation and clustering of samples compared to DINOv2 and RETFound, suggesting better representation learning. 51 Supplementary Note 5: Computational efficiency To evaluate the computational efficiency of the proposed MIRAGE model, we provide, in Supplementary Table 14, the number of parameters, FLOPs, and Mult-Adds operations for the different configurations of the model, i.e., for pretraining (with three modality-specific linear projection layers [LPL] and Transformer-based decoders), for classification (with only one LPL and linear probing head) and for segmentation (with one LPL and ConvNeXt-based segmentation head). Both classification and segmentation configurations are common to all the adapted foundation models, which also share the same encoder architecture. The only difference is the encoder size. Thus, for RETFound and DINOv2, that use ViT-Large encoder, the Large configuration is used, and for MedSAM, which uses ViT-Base, the Base. In addition, we provide, in Supplementary Table 15, the computational resources and amount of data required to pretrain MIRAGE compared to other foundation models. In particular, we show the training time, number of epochs, batch size, samples and learning approach used for pretraining along with the hardware. All models except MedSAM and MultiMAE, which use ViT-Base, use ViT-Large encoders. As shown in the table, MIRAGE requires fewer computational resources and less data compared to other medical foundation models such as RETFound and MedSAM. Supplementary Table 14: Computational efficiency of models for pretraining, classification, and segmentation tasks. The table shows the number of parameters, FLOPs and Mult-Adds operations of the models. While the values for pretraining are only for MIRAGE, the values for classification and segmentation are the same for all adapted foundation models, depending on the task and the size of the encoder. For example, RETFound and DINOv2 use ViT-Large encoder, so the Large configuration is used. In contrast, MedSAM uses ViT-Base encoder, so the Base configuration is used. Input size is fixed to 512 512 for the calculations. Configuration Size # Parameters FLOPs Mult-Adds (G) (M) (G)"
        },
        {
            "title": "Large\nBase",
            "content": "318.23 99.01 309.40 90.37 315.52 96.16 236.85 69.26 155.89 44.11 396.08 171. 1.92 1.30 1.14 0.57 2.71 2.22 Supplementary Table 15: Computational resources and data requirements for pretraining MIRAGE compared to other foundation models. The table shows the training time, the number of epochs, the batch size, the number of samples, the data type and the learning method (fully supervised [FS] or self-supervised [SS]) used for pretraining along with the used hardware. Method Training time # Epochs Batch size # Samples Data Method Hardware SL-IN DINOv2 MultiMAE MedSAM RETFound MIRAGE 23 days >41 days - - 14 days 25 days 90 - 1600 150 800 1600 4096 2048 2048 160 1792 14.2M RGB natural images, Classes RGB natural images 142M RGB, Depth, Seg. masks 1.28M 10 modalities, Seg. masks 1.57M OCT 736k OCT, SLO, Layers 261k FS 10x TPUv3 SS 160x V100 (32GB) 8x A100 (80GB) SS 20x A100 (80GB) FS 8x A100 (40GB) SS 1x A100 (80GB) SS"
        },
        {
            "title": "References",
            "content": "[1] Burton, M. J. et al. The lancet global health commission on global eye health: vision beyond 2020. The Lancet Global Health 9, e489e551 (2021). [2] Kanski, J. J. & Bowling, B. Clinical Ophthalmology: Systematic Approach (Elsevier Health Sciences, 2011), seventh edn. [3] Mohammadpour, M. Diagnostics in Ocular Imaging: Cornea, Retina, Glaucoma and Orbit (Springer Nature, 2020). [4] Schmidt-Erfurth, U. & Waldstein, S. M. paradigm shift in imaging biomarkers in neovascular age-related macular degeneration. Progress in Retinal and Eye Research 50, 124 (2016). [5] Schuman, J. S., Fujimoto, J. G., Duker, J. & Ishikawa, H. Optical coherence tomography of ocular diseases (CRC Press, 2024). [6] Bemme, S. et al. Reliability of subjective assessment of spectral-domain OCT pathologic features by multiple raters in retinal vein occlusion. Ophthalmology Science 1, 100031 (2021). [7] Schmitz-Valckenberg, S. et al. Interreader agreement and longitudinal progression of incomplete/complete retinal pigment epithelium and outer retinal atrophy in age-related macular degeneration. Ophthalmology Retina 7, 10591068 (2023). [8] De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature Medicine 24, 13421350 (2018). [9] Schmidt-Erfurth, U., Sadeghipour, A., Gerendas, B. S., Waldstein, S. M. & Bogunovic, H. Artificial intelligence in retina. Progress in retinal and eye research 67, 129 (2018). [10] Yim, J. et al. Predicting conversion to wet age-related macular degeneration using deep learning. Nature Medicine 18 (2020). [11] Ting, D. S. W. et al. Artificial intelligence and deep learning in ophthalmology. British Journal of Ophthalmology 103, 167175 (2019). [12] Schmidt-Erfurth, U. et al. AI-based monitoring of retinal fluid in disease activity and under therapy. Progress in Retinal and Eye Research 86, 100972 (2022). [13] Reiter, G. S. et al. AI in the clinical management of GA: novel therapeutic universe requires novel tools. Progress in Retinal and Eye Research 103, 101305 (2024). [14] Paschali, M., Conjeti, S., Navarro, F. & Navab, N. Generalizability vs. robustness: investigating medical imaging networks using adversarial examples. In Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, 493501 (Springer, 2018). [15] Hemelings, R. et al. generalizable deep learning regression model for automated glaucoma screening from fundus images. NPJ digital medicine 6, 112 (2023). [16] Yang, Y., Zhang, H., Gichoya, J. W., Katabi, D. & Ghassemi, M. The limits of fair medical imaging AI in real-world generalization. Nature Medicine 111 (2024). [17] Ericsson, L., Gouk, H., Loy, C. C. & Hospedales, T. M. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Signal Processing Magazine 39, 4262 (2022). 53 [18] He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1597915988 (2022). [19] Le-Khac, P. H., Healy, G. & Smeaton, A. F. Contrastive representation learning: framework and review. Ieee Access 8, 193907193934 (2020). [20] Radford, A. et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, 87488763 (PMLR, 2021). [21] Caron, M. et al. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 96509660 (2021). [22] Girdhar, R. et al. ImageBind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1518015190 (2023). [23] Bachmann, R., Mizrahi, D., Atanov, A. & Zamir, A. MultiMAE: Multi-modal multi-task masked autoencoders. In Proceedings of the European Conference on Computer Vision, 348367 (Springer, 2022). [24] Morano, J., Hervella, A. S., Barreira, N., Novo, J. & Rouco, J. Multimodal transfer learning-based approaches for retinal vascular segmentation. In ECAI 2020, 18661873 (IOS Press, 2020). [25] Li, X., Jia, M., Islam, M. T., Yu, L. & Xing, L. Self-supervised feature learning via exploiting multi-modal data for retinal disease diagnosis. IEEE Transactions on Medical Imaging 39, 40234033 (2020). [26] Morano, J. et al. Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation. In Greenspan, H. et al. (eds.) Proceedings of the International Conference on Medical Image Computing and ComputerAssisted Intervention, 589599 (Springer Nature Switzerland, Cham, 2023). [27] Silva-Rodrıguez, J., Chakor, H., Kobbi, R., Dolz, J. & Ben Ayed, I. foundation language-image model of the retina (FLAIR): encoding expert knowledge in text supervision. Medical Image Analysis 99, 103357 (2025). [28] Sukei, E. et al. Multi-modal representation learning in retinal imaging using selfsupervised learning for enhanced clinical predictions. Scientific Reports 14, 26802 (2024). [29] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition In Proceedings of the International Conference on Learning Representations at scale. (2021). [30] Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). [31] Tu, T. et al. Towards generalist biomedical AI. NEJM AI 1, AIoa2300138 (2024). [32] Oquab, M. et al. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research (2024). [33] Zhou, Y. et al. foundation model for generalizable disease detection from retinal images. Nature 622, 156163 (2023). 54 [34] Qiu, J. et al. Development and validation of multimodal multitask vision foundation model for generalist ophthalmic artificial intelligence. NEJM AI 1, AIoa2300221 (2024). [35] Shi, D. et al. EyeFound: multimodal generalist foundation model for ophthalmic imaging. arXiv preprint arXiv:2405.11338 (2024). [36] Shi, D. et al. EyeCLIP: visual-language foundation model for multi-modal ophthalmic image analysis. arXiv preprint arXiv:2409.06644 (2024). [37] Deng, J. et al. ImageNet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 248255 (IEEE, 2009). [38] Cai, Z., Lin, L., He, H., Cheng, P. & Tang, X. Uni4Eye++: general masked image modeling multi-modal pre-training framework for ophthalmic image classification and segmentation. IEEE Transactions on Medical Imaging (2024). [39] Dong, X. et al. MaskCLIP: Masked self-distillation advances contrastive language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1099511005 (2023). [40] Mazurowski, M. A. et al. Segment anything model for medical image analysis: an experimental study. Medical Image Analysis 89, 102918 (2023). [41] Huang, Y. et al. Segment anything model for medical images? Medical Image Analysis 92, 103061 (2024). [42] Ma, J. et al. Segment anything in medical images. Nature Communications 15, 654 (2024). [43] Zhu, J., Qi, Y. & Wu, J. Medical SAM 2: Segment medical images as video via Segment Anything Model 2. arXiv preprint arXiv:2408.00874 (2024). [44] Kirillov, A. et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 40154026 (2023). [45] Fazekas, B., Morano, J., Lachinov, D., Aresta, G. & Bogunovic, H. Adapting segment anything model (SAM) for retinal OCT. In Antony, B. et al. (eds.) Ophthalmic Medical Image Analysis, 92101 (Springer Nature Switzerland, Cham, 2023). [46] Gerendas, B. S. et al. Validation of an automated fluid algorithm on real-world data of neovascular age-related macular degeneration over five years. RETINA 42 (2022). [47] Garvin, M. K. et al. Intraretinal layer segmentation of macular optical coherence tomography images using optimal 3-d graph search. IEEE Transactions on Medical Imaging 27, 14951505 (2008). [48] Antony, B. et al. Automated 3-d method for the correction of axial artifacts in spectraldomain optical coherence tomography images. Biomedical Optics Express 2, 2403 (2011). [49] Farsiu, S. et al. Quantitative classification of eyes with and without intermediate agerelated macular degeneration using optical coherence tomography. Ophthalmology 121, 162172 (2014). URL https://people.duke.edu/sf59/RPEDC_Ophth_2013_dataset .htm. [50] Wu, J. et al. GAMMA challenge: Glaucoma grading from multi-modality images. Medical Image Analysis 90, 102938 (2023). URL https://gamma.grand-challenge.org/. 55 [51] Luo, Y., Shi, M., Tian, Y., Elze, T. & Wang, M. Harvard glaucoma detection and progression: multimodal multitask dataset and generalization-reinforced semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2047120482 (2023). URL https://ophai.hms.harvard.edu/datasets/harvardgdp1000/. [52] Kermany, D. Labeled optical coherence tomography (OCT) and chest X-ray images for classification (2018). URL https://data.mendeley.com/datasets/rscbjbr9sj/2. [53] Kermany, D. S. et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell 172, 11221131.e9 (2018). [54] Rasti, R., Rabbani, H., Mehridehnavi, A. & Hajizadeh, F. Macular OCT classification using multi-scale convolutional neural network ensemble. IEEE Transactions on Medical Imaging 37, 10241034 (2017). URL https://hrabbani.site123.me/available-data sets/dataset-for-oct-classification-50-normal-48-amd-50-dme. [55] Kulyabin, M. et al. OCTDL: Optical coherence tomography dataset for image-based deep learning methods. Scientific Data 11, 365 (2024). URL https://dx.doi.org/10.21227/ fpvs-8n55. [56] Gholami, P., Roy, P., Parthasarathy, M. K. & Lakshminarayanan, V. OCTID: Optical coherence tomography image database. Computers & Electrical Engineering 81, 106532 (2020). URL https://borealisdata.ca/dataverse/OCTID. [57] Prabhushankar, M. et al. OLIVES dataset: Ophthalmic labels for investigating visual In Advances in Neural Information Processing Systems (2022). URL eye semantics. https://zenodo.org/records/7105232. [58] Oghbaie, M., Araujo, T., Schmidt-Erfurth, U. & Bogunovic, H. VLFATRollout: Fully transformer-based classifier for retinal OCT volumes. Computerized Medical Imaging and Graphics 118, 102452 (2024). [59] Rashno, A. et al. Fully automated segmentation of fluid/cyst regions in optical coherence tomography images with diabetic macular edema using neutrosophic sets and graph algorithms. IEEE Transactions on Biomedical Engineering 65, 9891001 (2018). URL https://people.ece.umn.edu/users/parhi/.DATA/. [60] Srinivasan, P. P. et al. Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images. Biomedical Optics Express 5, 35683577 (2014). URL https://people.duke.edu/sf59/Sriniva san_BOE_2014_dataset.htm. [61] Chiu, S. J. et al. Kernel regression based segmentation of optical coherence tomography images with diabetic macular edema. Biomedical Optics Express 6, 11721194 (2015). URL https://people.duke.edu/sf59/Chiu_BOE_2014_dataset.htm. [62] Melinˇsˇcak, M., Radmiloviˇc, M., Vatavuk, Z. & Lonˇcaric, S. AROI: Annotated retinal OCT images database. In Proceedings of the International Convention on Information, Communication and Electronic Technology, 371376 (2021). URL https://ipg.fer.hr/ ipg/resources/oct_image_database. [63] Bogunovic, H. et al. RETOUCH: The retinal OCT fluid detection and segmentation benchmark and challenge. IEEE Transactions on Medical Imaging 38, 18581874 (2019). URL https://retouch.grand-challenge.org/. 56 [64] Fang, H. et al. Dataset and evaluation algorithm design for GOALS challenge. In Antony, B. et al. (eds.) Ophthalmic Medical Image Analysis, 135142 (Springer International Publishing, Cham, 2022). [65] Bui, P. T. A. et al. Fundus autofluorescence and optical coherence tomography biomarkers associated with the progression of geographic atrophy secondary to age-related macular degeneration. Eye 36, 20132019 (2022). [66] Liu, Z. et al. convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1197611986 (2022). [67] He, Y. et al. SwinUNETR-V2: Stronger swin transformers with stagewise convolutions for 3D medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 416426 (Springer, 2023). [68] Roy, S. et al. MedNeXt: Transformer-driven scaling of convnets for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 405415 (Springer, 2023). [69] Chen, J. et al. TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers. Medical Image Analysis 97, 103280 (2024). [70] Isensee, F., Jaeger, P. F., Kohl, S. A. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203211 (2021). [71] Isensee, F. et al. nnU-Net revisited: call for rigorous validation in 3D medical image segmentation. In International Conference on Medical Image Computing and ComputerAssisted Intervention, 488498 (Springer, 2024). [72] Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 1001210022 (2021). [73] Vaswani, A. et al. Attention is all you need. Advances in Neural Information Processing Systems 30 (2017). [74] Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical In Proceedings of the international conference on Medical Image image segmentation. Computing and Computer-Assisted Intervention, 234241 (Springer, 2015). [75] He, Y. et al. Structured layer surface segmentation for retina OCT using fully convolutional regression networks. Medical Image Analysis 68, 101856 (2021). [76] Fazekas, B. et al. Segmentation of Bruchs membrane in retinal OCT with AMD using anatomical priors and uncertainty quantification. IEEE Journal of Biomedical and Health Informatics 27, 4152 (2023). [77] Morano, J., Aresta, G. & Bogunovic, H. RRWNet: Recursive refinement network for effective retinal artery/vein segmentation and classification. Expert Systems with Applications 256, 124970 (2024). [78] Azad, R. et al. Medical image segmentation review: The success of U-Net. IEEE Transactions on Pattern Analysis and Machine Intelligence 120 (2024). [79] Singh, A. et al. FLAVA: foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1563815650 (2022). 57 [80] Chen, R. J. et al. Towards general-purpose foundation model for computational pathology. Nature Medicine 30, 850862 (2024). [81] Guymer, R. H. & Campbell, T. G. Age-related macular degeneration. The Lancet 401, 14591472 (2023). [82] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [83] Brown, T. et al. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. & Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, 18771901 (Curran Associates, Inc., 2020). [84] Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259265 (2023). [85] Zhang, S. & Metaxas, D. On the challenges and perspectives of foundation models for medical image analysis. Medical Image Analysis 91, 102996 (2024). [86] George, Y. et al. Attention-guided 3D-CNN framework for glaucoma detection and IEEE journal of biomedical structural-functional association using volumetric images. and health informatics 24, 34213430 (2020). [87] Lin, A. C., Lee, C. S., Blazes, M., Lee, A. Y. & Gorin, M. B. Assessing the clinical utility of expanded macular OCTs using machine learning. Translational Vision Science and Technology 10, 3232 (2021). [88] Liu, H. et al. Simultaneous alignment and surface regression using hybrid 2D3D networks for 3D coherent layer segmentation of retinal OCT images with full and sparse annotations. Medical Image Analysis 91, 103019 (2024). [89] Fazekas, B. et al. SD-LayerNet: Robust and label-efficient retinal layer segmentation via anatomical priors. Computer Methods and Programs in Biomedicine 261, 108586 (2025). [90] Li, J. et al. Multi-scale GCN-assisted two-stage network for joint segmentation of retinal layers and discs in peripapillary OCT images. Biomed. Opt. Express 12, 22042220 (2021). [91] Hu, E. J. et al. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [92] Steiner, A. P. et al. How to train your ViT? data, augmentation, and regularization in vision transformers. Transactions on Machine Learning Research (2022). [93] Jeong, D., Garg, S., Lipton, Z. C. & Oberst, M. Medical adaptation of large language and vision-language models: Are we making progress? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 1214312170 (2024). [94] Antaki, F., Chopra, R. & Keane, P. A. Vision-Language models for feature detection of macular diseases on optical coherence tomography. JAMA Ophthalmol 142, 573576 (2024). [95] Roy, A. G. et al. ReLayNet: Retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks. Biomedical Optics Express 8, 36273642 (2017). [96] Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019). 58 [97] Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations (2018). [98] Goyal, P. et al. Accurate, large minibatch SGD: training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677 (2017). [99] Ranftl, R., Bochkovskiy, A. & Koltun, V. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, 1217912188 (2021). [100] Provost, F. & Domingos, P. Well-trained PETs: Improving probability estimation trees. CeDER Working Paper IS-00-04, Stern School of Business, New York University (2000). [101] Fawcett, T. An introduction to ROC analysis. Pattern recognition letters 27, 861874 (2006). [102] Mosley, L. balanced approach to the multi-class imbalance problem (2013). [103] Demˇsar, J. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine learning research 7, 130 (2006). [104] Heimann, T. et al. Comparison and evaluation of methods for liver segmentation from CT datasets. IEEE transactions on medical imaging 28, 12511265 (2009). [105] Strudel, R., Garcia, R., Laptev, I. & Schmid, C. Segmenter: Transformer for semantic In Proceedings of the IEEE/CVF international conference on computer segmentation. vision, 72627272 (2021). [106] Yu, H. J. et al. Real-time photographicand fluorescein angiographic-guided management of diabetic retinopathy: Randomized PRIME trial outcomes. American Journal of Ophthalmology 226, 126136 (2021). [107] Payne, J. F. et al. Randomized trial of treat and extend Ranibizumab with and without navigated laser for diabetic macular edema: TREX-DME 1 year outcomes. Ophthalmology 124, 7481 (2017). [108] Schlanitz, F. G. et al. Drusen volume development over time and its relevance to the course of age-related macular degeneration. British Journal of Ophthalmology 101, 198 203 (2017). [109] Ritter, M. et al. Deep learning based quantification of photoreceptor and retinal pigment epithelium degeneration as predictive factors in stargardt disease. Investigative Ophthalmology & Visual Science 65, 37773777 (2024). [110] Arikan, M., Sadeghipour, A., Gerendas, B., Told, R. & Schmidt-Erfurt, U. Deep learning In Suzuki, K. et al. (eds.) Interbased multi-modal registration for retinal imaging. pretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support, 7582 (Springer International Publishing, Cham, 2019). [111] McInnes, L., Healy, J. & Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018)."
        }
    ],
    "affiliations": [
        "Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria",
        "Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria",
        "OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria"
    ]
}