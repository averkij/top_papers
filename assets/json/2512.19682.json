{
    "paper_title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "authors": [
        "Jiacheng Guo",
        "Ling Yang",
        "Peter Chen",
        "Qixin Xiao",
        "Yinjie Wang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Ke Shen",
        "Mengdi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $Î±$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities."
        },
        {
            "title": "Start",
            "content": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators 2025-12-23 Jiacheng Guo1*, Ling Yang1,2*, Peter Chen3*, Qixin Xiao4*, Yinjie Wang5, Xinzhe Juan4, Jiahao Qiu1, Ke Shen2, Mengdi Wang1 2ByteDance Seed 3Columbia University 1Princeton University 4University of Michigan 5University of Chicago *Equal Contribution Corresponding Authors Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, framework that establishes difficulty-aligned co-evolutionary game between an agent and scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates Data-Evolving Paradigm: the simulator acts as dynamic curriculum policy, continuously generating tasks specifically tailored to the agents zone of proximal development. This process is guided by simple but effective ğ›¼-Curriculum Reward, which aligns task difficulty with the agents current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3 less data. By shifting from static supervision to adaptive simulation, GenEnv provides data-efficient pathway for scaling agent capabilities. Our codes are available at https://github.com/Gen-Verse/GenEnv 5 2 0 2 2 ] . [ 1 2 8 6 9 1 . 2 1 5 2 : r Figure 1 GenEnvs cross-benchmark gains and data efficiency. (a) We compare GenEnv (7B) against representative baselines (Qwen2.5-7B, ReSearch, SearchR1, ToRL) and larger open models (e.g., Qwen3-14B, GPT-OSS-20B). Blue callouts report the absolute improvement of GenEnv over Qwen2.5-7B on each benchmark. (b) Validation data-efficiency comparison on BFCL: GenEnv surpasses RandomEnv and Static Augmentation, and outperforms Gemini-based offline augmentation even with 3.3 more synthetic data. Together, the figure shows that difficulty-aligned adaptive simulation can outperform stronger static augmentation baselines under comparable training settings. GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Figure 2 comparison between the traditional training paradigm and our proposed GenEnv framework. The traditional approach (top) relies on high-cost interaction with the real world to create static dataset, leading to inefficient training and poor generalization. GenEnv (bottom) creates co-evolutionary loop where an Environment LLM generates adaptive tasks for the Agent LLM, enabling low-cost simulation, an adaptive curriculum, and improved efficiency. 1. Introduction Training capable Large Language Model (LLM) agents for complex, interactive tasks like web navigation or tool use is constrained by significant bottleneck: the high cost of data collection through real-world interaction (Ning et al., 2025; Shinn et al., 2023; Wang et al., 2025a,b; Yao et al., 2023). Each step an agent takes in real-world environment can be slow, expensive, and difficult to parallelize. For instance, web agent that navigates an e-commerce site may fail when buttons label changes from Add to Cart to Add to Basket (Gur et al., 2023), but discovering such failure modes requires extensive and costly real-world exploration. This fragility highlights key limitation in how these agents are commonly trained. central driver of this issue is the reliance on static, pre-collected datasets of expert trajectories (Levine et al., 2020; Pomerleau, 1991; Samadi et al., 2024). Such datasets, no matter how large, represent fixed snapshot of the world and struggle to capture the wide range of variations an agent will encounter in open-ended environments (Levine et al., 2020). Increasing the dataset size alone does not resolve this limitation: the bottleneck often lies not just in data volume, but in the static and costly nature of its collection and its inability to adapt as the agent improves. The challenge of insufficient and static data has led to significant interest in synthetic data generation. However, despite progress, these methods often produce large but ultimately static corpus that can fail to adapt to the agents evolving requirements (Ding et al., 2024; Ye et al., 2024). This can result in inefficient training that still does not effectively target the agents specific weaknesses. The high cost of interaction and data collection remains core problem. To address this, we approach the problem differently by proposing GenEnv, framework built on 2 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators leveraging an LLM as scalable environment simulator to reduce interaction costs. Instead of relying on slow and expensive real-world feedback, our framework trains the agent almost entirely within simulated environment that can generate diverse and relevant training scenarios at substantially lower cost. As illustrated in Figure 2, this contrasts with traditional methods that evolve model on static data. In our approach, generative environment model is trained to produce an adaptive curriculum of tasks, creating challenges tailored to the agents performance. This leads to our primary research question: Can an LLM-based environment simulator provide scalable, low-cost alternative to real-world interaction for effectively training capable agents? In this simulation-centric process, the agent learns to overcome challenges generated by the simulator. The agents performance provides natural reward signal that guides the simulators curriculum generation, allowing both to improve in self-contained training loop. Throughout the paper, we use agent and Agent Policy ğœ‹agent interchangeably, and environment simulator and Environment Policy ğœ‹env interchangeably. As previewed in Figure 1, GenEnv delivers consistent gains over strong 7B baselines across five benchmarks and achieves higher accuracy than Gemini-based offline augmentation while using less synthetic data, highlighting the advantage of difficulty-aligned, adaptive simulation over static scaling of data. Our contributions are: The Data-Evolving Paradigm: We propose co-evolutionary framework where the training data distribution adapts dynamically to the agents learning progress, breaking the reliance on static corpora. Difficulty-Aligned Simulation: We introduce the ğ›¼-Curriculum Reward, mechanism that rewards the simulator for generating tasks within the agents target success zone (akin to the zone of proximal development (Vygotsky, 1978)), ensuring an efficient automated curriculum. Data Efficiency: On our benchmarks, GenEnv matches or surpasses Gemini 2.5 Pro-based static augmentation pipelines while using 3.3 less synthetic data, suggesting that an adaptive simulator can be more valuable than simply scaling the teacher model. 2. GenEnv: Difficulty-Aligned Co-Evolution GenEnv views agent training as two-player curriculum game rather than single-player optimization problem. We maintain two policies: an Agent Policy ğœ‹agent (the agent) and an Environment Policy ğœ‹env (the environment simulator). Unlike standard RL where the environment is fixed, GenEnv enables both to co-evolve: ğœ‹agent learns to solve tasks sampled from the current simulator. ğœ‹env is rewarded for generating tasks whose difficulty is aligned with the agents current capabilitytargeting the zone of proximal development where learning is most effective (Vygotsky, 1978). 2.1. Data-Evolving Paradigm: From Static Corpora to Adaptive Simulation Standard training minimizes loss (ğœƒ) over static distribution Dstatic, where ğœƒ denotes the parameters of the agent. In contrast, GenEnv implements Data-Evolving Paradigm. The training data Dğ‘¡ is generated on-the-fly by ğœ‹env, conditioned on the agents historical performance. This creates feedback loop (Figure 3) where the simulator seeks not to defeat the agent, but to find its breaking points to facilitate learning. 3 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Figure 3 The GenEnv Co-Evolutionary Loop. (1) The Environment Policy generates tasks. (2) The Agent Policy attempts them. (3) The environment reward (difficulty alignment) updates the simulator, while the agent reward (task success) updates the agent. 2.2. Rewards: Agent vs. Environment (with explicit equation references) 2.2.1. Agent Task Reward (ğ‘…agent) Each environment-generated task induces target action/trajectory ğ‘ (e.g., sequence of tool calls or final answer), and the agent produces prediction ğ‘. We distinguish structured action space Astruct (e.g., executable API calls) from free-form answers (e.g., natural language). For structured actions we can rely on exact execution; for unstructured ones we use soft similarity score. We define the agent reward (used to update ğœ‹agent) as: ğ‘…agent(ğ‘, ğ‘) = ğ•€(ğ‘ = ğ‘) ğ•€(ğ‘ Astruct) + sim(ğ‘, ğ‘) ğ•€(ğ‘ Astruct), (1) where sim(ğ‘, ğ‘) [0, 1] is task-dependent (e.g., normalized token-F1 or embedding similarity). In all benchmarks we scale ğ‘…agent into [0, 1]. 2.2.2. Environment Difficulty-Alignment Reward (ğ‘…env) The core innovation for ğœ‹env is difficulty-aligned reward that targets success-rate band around desired ğ›¼ (0, 1) (we use ğ›¼ = 0.5). For each generated batch of ğ‘› task variations, after the agent attempts them we compute the empirical success rate: Ë†ğ‘ = ğ‘˜ ğ‘› , (2) where ğ‘˜ is the number of successes under ğ‘…agent (Eq. equation 1). We then assign the environment reward (used to update ğœ‹env): ğ‘…env( Ë†ğ‘) = exp (cid:16) ğ›½( Ë†ğ‘ ğ›¼)2(cid:17) , (3) where ğ›½ > 0 controls sharpness. This bell-shaped reward peaks when the agents performance matches ğ›¼, discouraging tasks that are already mastered (Ë†ğ‘ 1) or hopeless (Ë†ğ‘ 0). We additionally apply 4 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators difficulty filter: task batches with Ë†ğ‘ ğ›¼ > ğ‘˜min (we use ğ‘˜min = 0.1) are excluded from environment updates to prevent overfitting to transient spikes. 2.3. Data Structures and How New Training Data Is Produced recurring confusion in co-evolution papers is where the training data actually comes from. We therefore make the data flow explicit. What the environment generates. At epoch ğ‘¡, the environment policy ğœ‹env generates task batch Tğ‘¡. Concretely, Tğ‘¡ contains ğ‘› task instances (often multiple variations of the same seed), where each instance includes: (i) task prompt/context (including tool specs, constraints, and goal), (ii) an evaluation specification (e.g., executable checker / exact-match target / reference answer), and optionally (iii) structured ground truth target action ğ‘ when the benchmark provides it (e.g., tool-call arguments). What the agent produces. The agent interacts with each task instance and yields an interaction trace (rollout) that we denote by an element ğ‘’ Eğ‘¡. Each trace records at minimum: ğ‘’ = (task, trajectory, ğ‘, ğ‘Ÿ), where trajectory can include intermediate reasoning text and tool calls, ğ‘ is the final output/action, and ğ‘Ÿ = ğ‘…agent(ğ‘, ğ‘) is computed via Eq. equation 1 (or its benchmark-specific instantiation). Two growing datasets: one for the agent, one for the environment. We maintain two pools that grow online: Agent training pool Dtrain: stores valid interaction traces from Eğ‘¡. trace is valid if it is well-formed and evaluable for the benchmark (e.g., tool calls parse and execute; outputs follow required schema; checker runs without error). We append these valid tuples to Dtrain so the agent can (i) learn from fresh on-policy experiences and (ii) retain mastery of earlier curricula by continuing to sample from the accumulated pool. Environment SFT pool Denv: stores environment generations used to train ğœ‹env via RWR. Each record is supervised pair of the form (env-conditioning context generated task instance), where the conditioning context includes the seed prompt plus any summary signals (e.g., recent success statistics) that ğœ‹env conditions on. We weight each record by monotone function of the environment reward in Eq. equation 3 (e.g., exp(ğœ†ğ‘…env( Ë†ğ‘)), where ğœ† = 1.0 is temperature hyperparameter). How this produces data-evolving training set. At every epoch, both Tğ‘¡ and Eğ‘¡ are newly generated; thus Dtrain is not fixed offline corpus but an evolving mixture of (a) base data, (b) previously collected valid traces, and (c) newly collected on-policy traces. Meanwhile, Denv evolves toward generating tasks whose empirical success rate stays near ğ›¼ (Eq. equation 3), which in turn shifts the difficulty distribution of future Tğ‘¡+1. This closes the loop: new data is produced as byproduct of interaction, and is then explicitly aggregated into training pools for subsequent updates. GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators 2.4. Two-Player Curriculum RL: Optimization Loop Player 1 Update (Agent). The Agent Policy ğœ‹agent is updated to maximize ğ”¼[ğ‘…agent] (Eq. equation 1). In our experiments, we instantiate this using Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Player 2 Update (Environment). The Environment Policy ğœ‹env is updated to maximize ğ”¼[ğ‘…env] (Eq. equation 3). We implement this via Reward-Weighted Regression (RWR): from each environmentgenerated batch, we compute Ë†ğ‘ (Eq. equation 2) from agent rollouts and assign ğ‘…env( Ë†ğ‘). We then construct weighted SFT set of environment generations and fine-tune ğœ‹env toward higher-reward generations. For stability, we regularize updates with KL penalty to the initial simulator and cap per-step updates by maximum KL threshold. Algorithm 1 GenEnv Co-Evolutionary Loop (with explicit equation references) Initialize: Agent ğœ‹agent, Environment ğœ‹env, Agent pool Dtrain, Env pool Denv. for epoch ğ‘¡ = 1, . . . , ğ‘‡ do Phase 1: Online Generation & Interaction Environment generates task batch Tğ‘¡ ğœ‹env(). Agent ğœ‹agent rolls out on Tğ‘¡ to obtain traces Eğ‘¡ and per-trajectory agent rewards ğ‘…agent via equation 1. Compute batch success Ë†ğ‘ via equation 2 and assign environment reward ğ‘…env( Ë†ğ‘) via equation 3. Phase 2: Dual Update (Two Players, Two Objectives) Update agent ğœ‹agent via GRPO to maximize ğ”¼[ğ‘…agent] (equation 1). Filter out batches with Ë†ğ‘ ğ›¼ > ğ‘˜min for environment updates. Build weighted env SFT set Dğ‘¡ Update environment ğœ‹env via RWR on Dğ‘¡ Phase 3: Aggregation (How New Data Enters Training) Extract valid traces from Eğ‘¡ (e.g., parseable/executable/checker-passed) and append to agent env from Tğ‘¡ with weights exp(ğœ†ğ‘…env( Ë†ğ‘)) (equation 3). env to maximize ğ”¼[ğ‘…env] (equation 3). pool: Dtrain Dtrain Valid(Eğ‘¡). Append weighted environment generations to env pool: Denv Denv Dğ‘¡ env. end for 3. Theoretical Analysis of Difficulty-Aligned GenEnv In this section we provide simple theoretical analysis of the difficulty-aligned co-evolution mechanism in GenEnv. Our goal is not to fully characterize the dynamics of large LLMs, but to clarify why (1) tasks whose success rate is close to the target band ğ›¼ carry the strongest learning signal for the Agent Policy ğœ‹agent, and (2) the ğ›¼-Curriculum Reward ğ‘…env provides statistically consistent signal for the Environment Policy ğœ‹env to rank task types by how well their difficulty matches the current agent. 3.1. Intermediate Difficulty Maximizes Agent Learning Signal We first consider stylized bandit setting in which the Agent Policy ğœ‹agent interacts with single environment-generated task type ğœ (e.g., family of API-calling problems of similar difficulty). The outcome of each attempt is scalar reward ğ‘Ÿ {0, 1}, where ğ‘Ÿ = 1 denotes success on the task and ğ‘Ÿ = 0 denotes failure.1 For fixed Agent Policy parameterization ğœƒ, let ğ‘(ğœ) = Pr(ğ‘Ÿ = 1 ğœ, ğœƒ) denote 1The analysis extends to ğ‘…agent [0, 1] by rescaling; we use the binary case for clarity. 6 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators the success probability on task type ğœ. The Agent Policy is updated with REINFORCE-style estimator ğ‘”(ğœ, ğ‘Ÿ) = (ğ‘Ÿ ğ‘(ğœ))ğœƒ log ğœ‹ğœƒ(ğ‘ ğœ), (4) where ğ‘ is the sampled action (e.g., rollout trajectory of tool-calling sequence) and ğ‘(ğœ) is baseline (e.g., an estimate of the expected reward on ğœ). The quantity ğ”¼[ğ‘”(ğœ, ğ‘Ÿ)2] can be viewed as measuring how strong the stochastic gradient signal is for this task type. Given the trust-region KL constraint and the gradient-clipping bias in GRPO-style policy updates, it is expected that the squared norm of the score function remains within trust-region bound. This behavior has been discussed in recent analyses of one-step policy updates (e.g., Chen et al. (2025c, Theorem 3.2)) as well as in the literature on trust-region policy optimization (Schulman et al., 2015). Accordingly, we establish our theoretical analysis under the reasonable assumption that the squared norm of the score function ğœƒ log ğœ‹ğœƒ(ğ‘ ğœ) does not vary too dramatically when conditioned on the binary outcome ğ‘Ÿ. Assumption 1 (Bounded score variation). For fixed task type ğœ, there exist constants 0 < ğ‘min ğ‘max < such that ğ‘min ğ”¼ 2 (cid:12) (cid:104)(cid:13) (cid:13)ğœƒ log ğœ‹ğœƒ(ğ‘ ğœ)(cid:13) (cid:12) (cid:13) (cid:12) (cid:105) ğ‘Ÿ ğ‘max for both ğ‘Ÿ = 0 and ğ‘Ÿ = 1. We take the baseline to be the on-task expected reward, ğ‘(ğœ) = ğ”¼[ğ‘Ÿ ğœ] = ğ‘(ğœ), which is the variance minimizer in the standard REINFORCE analysis. Under these conditions we obtain the following result. Proposition 1 (Intermediate difficulty maximizes gradient signal). Suppose Assumption 1 holds and the baseline is chosen as ğ‘(ğœ) = ğ‘(ğœ). Then there exist positive constants ğ¶min and ğ¶max, independent of ğ‘(ğœ), such that ğ¶min ğ‘(ğœ) (cid:0)1 ğ‘(ğœ)(cid:1) ğ”¼(cid:2)ğ‘”(ğœ, ğ‘Ÿ)2(cid:3) ğ¶max ğ‘(ğœ) (cid:0)1 ğ‘(ğœ)(cid:1) . (5) In particular, up to constant factors, the expected squared gradient norm is proportional to ğ‘(ğœ) (cid:0)1 ğ‘(ğœ)(cid:1), which is maximized when ğ‘(ğœ) = 1/2, i.e., for tasks of intermediate difficulty. Proof sketch. With ğ‘Ÿ {0, 1} and ğ‘(ğœ) = ğ‘(ğœ), we have ğ”¼[(ğ‘Ÿ ğ‘(ğœ))2 ğœ] = Var(ğ‘Ÿ ğœ) := ğ‘(ğœ)(1 ğ‘(ğœ)). Using the law of total expectation and Assumption 1, we can factor out the variation coming from the score function up to multiplicative constants, which yields equation 5. The function ğ‘(1 ğ‘) is concave quadratic on [0, 1] with unique maximum at ğ‘ = 1/2. full proof is given in Appendix A.1. 2 -Curriculum reward). Considering the case that ğ›¼ = 1 , we have the identity ğ‘(1 ğ‘) = Remark 1 ( 1 (cid:1) 2. Thus, maximizing the variance term ğ‘(1 ğ‘) is exactly equivalent to minimizing the 4 (cid:0) ğ‘ 1 1 squared distance to the target success rate ğ›¼ = 1 . In GenEnv, the Environment Policy does not observe 2 the true success probability ğ‘(ğœ) but only an empirical estimate Ë†ğ‘(ğœ) from finite number of rollouts. The ğ›¼-Curriculum Reward takes the form 2 ğ‘…env( Ë†ğ‘(ğœ)) = exp(cid:0) ğ›½( Ë†ğ‘(ğœ) ğ›¼)2(cid:1), (6) which is monotone transformation of ( Ë†ğ‘(ğœ) ğ›¼)2 and therefore encourages the simulator to propose tasks whose empirical success rate stays close to ğ›¼. Proposition 1 then suggests that, in expectation, this aligns the simulator with task types that provide the strongest learning signal for ğœ‹agent. 7 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators 3.2. Ranking Consistency of the ğ›¼-Curriculum Reward We next show that, despite relying on noisy empirical success rates, the ğ›¼-Curriculum Reward provides statistically consistent signal for ranking task types by how well their difficulty matches the target band. The argument is based on standard concentration inequalities. Consider two task types ğœ1 and ğœ2. For fixed Agent Policy ğœ‹agent, let ğ‘ğ‘– = ğ‘(ğœğ‘–) denote the true success probability on ğœğ‘–, and define their distances to the target band ğ›¼ as ğ‘– {1, 2}. Î”ğ‘– = ğ‘ğ‘– ğ›¼, (7) Without loss of generality, assume Î”1 < Î”2, i.e., ğœ1 is closer to the target difficulty than ğœ2. For each ğœğ‘– we run ğ‘›ğ‘– independent rollouts and compute the empirical success rate Ë†ğ‘ğ‘– = ğ‘˜ğ‘–/ğ‘›ğ‘–, where ğ‘˜ğ‘– is the number of successes. The Environment Policy receives the reward ğ‘…env( Ë†ğ‘ğ‘–) = exp (cid:0) ğ›½( Ë†ğ‘ğ‘– ğ›¼)2(cid:1) . (8) Since the exponential is monotone, ranking tasks by ğ‘…env is equivalent to ranking them by their squared distance ( Ë†ğ‘ğ‘– ğ›¼)2 to the target band. The following theorem shows that the mis-ranking probability decays exponentially in the minimum number of rollouts. Theorem 1 (Ranking consistency of ğ‘…env). Let ğ‘› = min{ğ‘›1, ğ‘›2} and Î”1 < Î”2 as above. Define ğ›¿ = (Î”2 Î”1)/3 > 0. Then Pr (cid:0)ğ‘…env( Ë†ğ‘1) ğ‘…env( Ë†ğ‘2)(cid:1) 4 exp (cid:18) 2 9 (Î”2 Î”1)2 ğ‘› (cid:19) . (9) In particular, as ğ‘› the reward ranking is consistent: tasks whose true success probability lies closer to the target band ğ›¼ receive higher ğ›¼-Curriculum Reward with probability approaching 1 at an exponential rate. Proof sketch. Because the exponential is monotone, ğ‘…env( Ë†ğ‘1) ğ‘…env( Ë†ğ‘2) is equivalent to Ë†ğ‘1 ğ›¼ Ë†ğ‘2 ğ›¼. We show that if both empirical estimates Ë†ğ‘ğ‘– lie within ğ›¿ of their true means ğ‘ğ‘–, then necessarily Ë†ğ‘1 ğ›¼ < Ë†ğ‘2 ğ›¼ and hence ğ‘…env( Ë†ğ‘1) > ğ‘…env( Ë†ğ‘2). Thus mis-ranking can only occur when at least one empirical mean deviates from its expectation by more than ğ›¿, which can be bounded using Hoeffdings inequality for Bernoulli random variables. detailed proof is provided in Appendix A.1. Implications for GenEnv. Theorem 1 shows that, even though the Environment Policy only observes noisy empirical success rates Ë†ğ‘ğ‘– derived from finite number of rollouts, the ğ›¼-Curriculum Reward is statistically consistent proxy for task difficulty. As we increase the rollout budget per task type, the environment LLM can more reliably identify and up-weight task families whose difficulty lies in the target zone of proximal development. This provides formal justification for the empirical convergence behaviour observed in Figure 7, where the agents success rate on simulated tasks concentrates around band centered at ğ›¼ = 0.5. Together, Proposition 1 and Theorem 1 clarify why aligning the simulator with an intermediate success rate both maximizes learning signal for the agent and yields stable, difficulty-calibrated curriculum. 4. Experiments 4.1. Experimental Setup Backbone Models. Unless otherwise specified, all 7B agents and simulators are initialized from Qwen2.5-7B-Instruct (Yang and Qwen Team, 2024). For large-scale baselines in Table 1, we include GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Table 1 Main Results. Comparison on five benchmarks. Models are grouped by size: Large Scale Models (> 10B) are sorted by size descending, followed by 7B Models. Bold numbers indicate the best performance within each group. GenEnv (7B) significantly outperforms other 7B baselines and even surpasses the average performance of several 72B/405B models on this suite. Model ALFWorld BFCL API-Bank Bamboogle TravelPlanner Average Large Scale Models Llama 3.1 405B GPT-OSS 120B Qwen 2.5 72B Llama 3.1 70B Qwen 3 32B GPT-OSS 20B Qwen 3 14B 7B Models ReSearch SearchR1 Qwen 2.5 7B ToRL GenEnv (Ours) 65.3 60.4 63.5 60.1 52.3 53.6 37.8 18.7 16.1 14.2 8.0 54.5 5.5 21.9 35.3 13.4 33.8 24.4 29.4 5.0 5.0 7.0 0.0 41. 74.4 53.6 54.9 64.3 63.8 41.2 66.7 65.3 63.3 61.6 54.1 79.1 77.6 29.6 69.6 76.8 71.2 33.6 76.0 68.0 67.2 68.0 34.4 76.0 16.5 14.7 20.5 17.6 22.5 14.9 14.7 16.4 16.1 14.3 14.8 16. 47.9 36.0 48.8 46.4 48.7 33.5 44.9 34.7 33.5 33.0 22.3 53.6 Llama 3.1 models (Grattafiori et al., 2024), GPT-OSS open-weight models (OpenAI, 2025), and Qwen3 models (Yang and Qwen Team, 2025). These models are evaluated using the same tool-calling interface and prompt templates as our 7B baselines for fairness. Benchmarks. We evaluate across 5 diverse benchmarks that span tool use, embodied interaction, and real-world planning. API-Bank (Li et al., 2023) measures function-calling and tool-augmented reasoning, and Bamboogle is compositional multi-hop QA benchmark built on top of the framework from Press et al. (2023); for both, we follow the evaluation protocols from ToRL (Li et al., 2025b). ALFWorld (Shridhar et al., 2021) aligns textual instructions with embodied environments; we utilize the official validation set, where multi-turn tasks are decomposed into single steps for evaluation. BFCL follows the Berkeley Function-Calling Leaderboard setup (Patil et al., 2025); specifically, we evaluate on the long-context subset treating each turn independently. TravelPlanner (Xie et al., 2024) captures end-to-end planning and tool use in realistic travel scenarios; we report the average of four metrics: CS Micro (%), CS Macro (%), HD Micro (%), and HD Macro (%). The Average column in Table 1 is the unweighted mean of these per-benchmark success metrics. Baselines & Variants. We compare against standard instructed models (Qwen2.5-7B-Instruct) and specialized search-and-planning agents, including ReSearch, SearchR1, and ToRL (Jin et al., 2025; Li et al., 2025b). These methods represent strong model-evolving pipelines that either improve search policies or alignment rewards on largely static datasets. To strictly evaluate our Data-Evolving contribution, we define: GenEnv-Random: The simulator generates new tasks every epoch but is not trained via ğ‘…env. This isolates the effect of dynamic data vs. aligned curriculum. GenEnv-Static: The simulator generates large batch of synthetic data once before training. Gemini-Offline (2x / 3.3x): High-quality synthetic data generated offline by Gemini 2.5 Pro (approx. 1.76x and 3.27x the training set size). This represents strong teacher-distillation 9 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Figure 4 Training dynamics of GenEnv. From left to right: (a) training step-wise reward (critic/score/mean); (b) validation score across epochs; (c) batch-level ground-truth accuracy; and (d) per-epoch average reward. The curves show that GenEnv trains stably without reward collapse or divergence, with both reward and accuracy improving smoothly over time. baseline. Training Configuration. For the Agent Policy (ğœ‹agent), we train for 10 epochs using GRPO with batch size 64 and maximum sequence length 9,000 tokens (prompt + response). The Environment Policy (ğœ‹env) is updated at the same epoch frequency via RWR with batch size 64. We use the same optimizer family (AdamW) for both policies, with learning rates detailed in Appendix A.2. All methods are trained on the same base dataset, and for GenEnv variants the additional data comes solely from the simulator. Evaluation Protocol. All modelsincluding large modelsare evaluated in unified tool-calling framework. We use identical system prompts, tool specifications, and decoding settings for all models on given benchmark. We do not attach additional multi-agent orchestration or human-in-the-loop corrections to any method, to focus the comparison on training and data regimes. 10 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Figure 5 Emergent curriculum in GenEnv. Across training epochs, the environment simulator gradually increases task complexity (a), reflected by longer task descriptions; the agent correspondingly produces longer reasoning chains (b) as it learns to solve harder tasks; and its success rate (c) remains within controlled band despite rising difficulty. Together these curves show that GenEnv induces an emergent curriculum in which task difficulty and agent capability co-evolve in stable manner. Summary of Results. Across all benchmarks, GenEnv improves the 7B base agent significantly, with gains up to +40.3% on ALFWorld and +20.4% on API-Bank compared to strong baselines. 4.2. RQ1: Does GenEnv Improve Downstream Task Performance? Table 1 presents the main comparison. GenEnv consistently outperforms both general-purpose models and specialized RL agents across all five benchmarks. Notably, on ALFWorld, which requires long-horizon planning, GenEnv achieves 54.5% accuracy compared to 14.2% for the base model, demonstrating the power of the simulator in generating diverse embodied scenarios that are costly to collect in the real world. On API-Bank and BFCL, GenEnv achieves 79.1% and 41.8% success respectively, markedly improving over other 7B baselines that rely on static data or non-adaptive exploration. Beyond absolute performance, GenEnv also closes much of the gap to substantially larger models. The average score of GenEnv (53.6) is competitive withand in many cases exceedsthat of 14B72B models that do not benefit from difficulty-aligned simulator. This supports our central claim that how data is generated and aligned with the agent matters as much as, or more than, simply scaling model size or collecting larger static datasets. In the remainder of the section, we investigate how the co-evolutionary process shapes the curriculum, data efficiency, and difficulty calibration of the environment. We also verify that the co-evolutionary training process itself is well behaved. Figure 4 plots the training dynamics of GenEnv: the per-step GRPO reward and batch-level ground-truth accuracy both increase steadily, and the validation score improves monotonically before saturating, without signs of reward hacking or instability. This suggests that our difficulty-aligned simulator can be optimized jointly with the agent using standard policy gradients, without introducing pathological oscillations during training. 4.3. RQ2: Does GenEnv Learn to Tackle Harder Tasks Over Time? We next investigate whether the environment simulator actually learns curriculum, rather than simply generating random variations. To this end, we use the average length of the agents required 11 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators response as proxy for reasoning complexity and task difficulty, and track how it evolves throughout training. Figure 5 plots this average response length together with the agents success rate on simulated tasks across epochs. Finding. The required response length increases from 137 to 204 tokens (+49%) by epoch 6. This confirms that ğœ‹env learns to generate progressively more complex reasoning challenges as ğœ‹agent becomes more capable, creating an emergent curriculum without manual design. At the same time, the agents success rate does not collapse; instead, it grows in tandem with task complexity, suggesting that the simulator is adapting difficulty in controlled way rather than simply making tasks arbitrarily harder. This pattern is consistent with our design of the ğ›¼-Curriculum Reward: as the agent improves on given family of tasks, their success rate on that family moves away from the target band around ğ›¼, reducing its contribution to ğ‘…env and encouraging the simulator to propose harder variations. The observed increase in response length indicates that these harder tasks require more extensive multi-step reasoning and tool use, rather than superficial changes to surface form. Qualitatively, we observe that later tasks tend to involve more complex compositions of tools and deeper chains of intermediate subgoals. Finally, the fact that curriculum emerges without any hand-specified difficulty schedule supports our broader view of GenEnv as data-evolving system: the environment learns where the agents breaking points are and adapts task generation accordingly. This contrasts with conventional curriculum learning, which typically relies on fixed heuristics or manually designed difficulty levels. Here, difficulty is inferred directly from the agents behaviour via ğ‘…env( Ë†ğ‘). 4.4. RQ3: Is GenEnv More Data-Efficient Than Gemini-Based Augmentation? (a) Method comparison. GenEnv outperforms both static Gemini-based augmentation and the GenEnvRandom variant, showing that how data is generated and aligned with the agent matters more than simply adding more offline synthetic data. Figure 6 Static vs. difficulty-aligned simulation. (b) Data efficiency. GenEnv achieves higher validation performance while using substantially fewer synthetic samples than Gemini-based offline augmentation, indicating that difficulty-aligned, on-policy simulation provides more learning signal per example than untargeted teacher-generated data. key question is whether GenEnv is simply benefiting from more data or from better-targeted data. We compare GenEnv (which generates data on-the-fly) against offline augmentation using Gemini 2.5 Pro, under comparable training budgets. Gemini-Offline (2x / 3.3x) corresponds to large static corpora generated before training, while GenEnv continuously adapts task generation as the agent evolves. GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Finding. As shown in Figure 6b, GenEnv (using 1x original data + dynamic simulation) reaches validation score of 0.458. This outperforms Gemini-Offline (3.3x) (0.438), which uses 3.3x more synthetic data generated by much stronger model. This confirms that targeted, difficulty-aligned data generation can be structurally superior to massive but untargeted augmentation. Furthermore, GenEnv outperforms GenEnv-Random by 12.3%, indicating that the ğ‘…env optimization is critical. These results highlight two distinct effects. First, merely adding more synthetic trajectories from powerful teacher model quickly encounters diminishing returns: once the static dataset ceases to match the agents current weaknesses, additional examples provide limited new learning signal. Second, the comparison between GenEnv and GenEnv-Random controls for the presence of simulator: both generate trajectories online, but only GenEnv trains the simulator to target the ğ›¼ band. The performance gap between these two variants isolates the benefit of difficulty alignment itself, rather than just the benefit of having generative environment. From practical standpoint, these findings suggest shift in how we invest computational and annotation budget. Instead of paying for ever larger static datasets created by stronger teachers, it may be more effective to invest in moderately sized simulator that co-evolves with the student agent. This is especially attractive in domains where collecting real trajectories is expensive or slow, as the simulator can keep generating fresh, on-policy data without requiring repeated human involvement. 4.5. RQ4: Does the Environment Reward Produce Well-Calibrated Difficulty? Figure 7 Difficulty calibration via the ğ›¼-Curriculum Reward. As training progresses, the agents success rate on simulator-generated tasks converges to the target difficulty band (centered at ğ›¼ = 0.5), demonstrating that the environment policy reliably adapts task difficulty to match the agents current capability. This empirically verifies the theoretical ranking consistency of ğ‘…env and shows that the simulator self-calibrates to maintain tasks in the zone of proximal development. Finally, we verify if the ğ›¼-Curriculum Reward successfully calibrates task difficulty. During training, we track the agents success rate on simulated tasks generated by ğœ‹env and examine whether it converges to the intended band around ğ›¼. Figure 7 summarizes this trajectory over training epochs. Finding. Figure 7 shows the agents success rate on generated tasks during training. Starting from 0.138, the success rate converges towards band centered at the target difficulty ğ›¼ = 0.5, remaining within range of approximately [0.4, 0.6] for most of training. This suggests that ğœ‹env is actively 13 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Figure 8 Problem-solving behavior during training. (a) GenEnv consistently increases the proportion of fully solved tasks per batch, surpassing the RandomEnv variant; (b) the rate of unsolved tasks decreases substantially faster under GenEnv. These trends show that difficulty-aligned simulation not only improves average performance but also accelerates the elimination of failure modes compared to unguided task generation. optimizing for the zone of proximal development, avoiding the collapse into trivial or impossible tasks that plagues random generation. This behaviour is precisely what our theoretical analysis predicts. Theorem 1 shows that, given enough rollouts, ğ‘…env( Ë†ğ‘) provides consistent ranking signal that favours task types with success probabilities closest to ğ›¼. Empirically, we see this mechanism in action: early in training, when most tasks are either too hard or too easy, the simulator updates quickly reshape the distribution towards intermediate difficulty. Later, updates become smaller as the success rate stabilizes near the target band, leading to self-calibrated curriculum. We also observe that the calibrated difficulty band coexists with improved downstream performance on real benchmarks. That is, the simulator does not merely keep the agent at 50% success on synthetic tasks; rather, it continually moves the frontier of what intermediate difficulty means as the agent learns. This reinforces our view of GenEnv as genuinely co-evolving system, in which both the agent and the task distribution adapt in lockstep. 5. Related Work 5.1. Large Language Model Agents Recent advancements have demonstrated the ability of LLMs to function as autonomous agents. Pioneering works like ReAct, Reflexion, and Voyager have shown that combining chain-of-thought reasoning with action generation and memory evaluation enables agents to tackle complex tasks (Guo et al., 2025; Shinn et al., 2023; Wang et al., 2023; Yao et al., 2023; Zou et al., 2025). More recent efforts such as KnowAgent and MemBench further refine planning and memory capabilities in agentic settings (Tan et al., 2025; Zhu et al., 2024). Others, such as Toolformer and WebGPT, have focused on augmenting LLMs with external tools and web-browsing capabilities, expanding their operational scope (Nakano et al., 2021; Schick et al., 2023). While these models showcase strong performance, their training paradigms primarily rely on imitation learning from static, pre-collected GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators datasets of expert trajectories (Nakano et al., 2021; Wang et al., 2023). This dependency on expert data forms significant bottleneck, limiting the agents ability to explore and discover strategies beyond the provided demonstrations (Shinn et al., 2023). Our work addresses this limitation by creating dynamic learning environment that does not solely depend on fixed dataset. 5.2. Trajectory Synthesis for Agent Training Recent efforts have focused on generating synthetic trajectories to address the limitations of fixed expert data for training LLM agents (Yu et al., 2025). Some methods focus on offline synthetic data generation, creating novel trajectories to increase diversity and coverage where expert data is sparse (Ding et al., 2024; Ye et al., 2024). Other approaches leverage LLMs to generate self-reflective trajectories, incorporating reflections and corrections to learn from errors (Chen, 2025), or to provide stepwise guidance from teacher model toward correcting mistakes (Chen et al., 2025b). For web agents, scalable synthesis pipelines use web tutorials or exploration-driven methods to produce large-scale synthetic datasets of multimodal trajectories (Pahuja et al., 2025; Yuan et al., 2024). Other works introduce iterative self-training for reflection (Wang et al., 2025a; Yuan et al., 2025), fine-tuning on massive interaction trajectories (Zhang et al., 2025a), step-level calibration (Luo et al., 2025), and simulators for online exploration to generate high-quality feedback-driven data (Hoang et al., 2025). Beyond these, several recent frameworks further advance autonomous and adaptive trajectory synthesis. Wang et al. (2025c) propose scalable LLM-based digital environment that models user-interface transitions as structured trajectories, combining simulation and targeted scaling to expose agents to increasingly complex states. Zhao et al. (2025) build upon exploration-driven task generation, using two-stage pipeline that first explores application environments and then synthesizes executable trajectories, yielding tens of thousands of realistic multimodal interaction traces. In the data-science domain, Zhang et al. (2025b) integrate curriculum-based agentic training with data-grounded trajectory synthesis framework to produce high-fidelity analytical workflows, allowing smaller models to outperform larger workflow-based agents. Complementarily, Liu et al. (2025) challenge the assumption that more data is always betterdemonstrating that strategically curated and high-quality trajectories can induce stronger agentic reasoning from only handful of demonstrations. Chen et al. (2025a) also demonstrates that noisy preference data could be utilized into improve agent alignment. Finally, Sun et al. (2025) introduce an implicit-feedback paradigm where agents learn from their own early interactions before formal reinforcement learning, leveraging self-reflection and world-modeling to bootstrap generalization from suboptimal actions. Taken together, these developments indicate trend toward adaptive, self-improving trajectory synthesis: instead of relying on static or expert-curated data, agents increasingly generate, evaluate, and refine their own experiencesclosing the loop between simulation, exploration, and learning. This motivates our approach, which leverages dynamic environment simulator to generate these adaptive experiences on-demand, directly addressing the limitations of static datasets. 5.3. Environment Simulation Simulators have long been cornerstone of reinforcement learning, especially in domains such as robotics where interacting with the real world is costly (Todorov et al., 2012). In the context of LLM agents, environment simulation has emerged as key mechanism for generating training data and evaluating agent capabilities. Wang et al. (2025c) demonstrate how an LLM-powered digital world simulator can generate structured user-interface states and transitions; its targeted scaling strategy produces diverse, high-impact tasks and yields agents that rival those trained on real 15 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators UIs. Complementary simulation frameworks move beyond UI tasks: Zhou et al. (2025) introduce scalable closed-loop simulator that samples multi-step tasks from toolrelationship graph, simulates interactions with configurable user and environment archetypes, and evaluates procedural alignment and success. Experiments reveal that environment reliability and user archetypes are dominant factors in agent performance. Beyond task-centric environments, Li et al. (2025a) integrate LLM-driven agents with realistic societal environment and large-scale simulation engine, generating social lives for over ten thousand agents and millions of interactions among agents and their surroundings. These works highlight the importance of faithful and diverse environment simulation in creating high-quality training data and benchmarks. Our environment LLM diverges from traditional simulators: rather than predicting state transitions or user responses for fixed task, it generates entire tasks and goals conditioned on the agents recent performance, with an explicit objective to match target difficulty band. This casts environment design itself as learnable policy with its own reward signal. 6. Conclusion We presented GenEnv, framework that shifts agent training from static, model-evolving process to dynamic, data-evolving game. By establishing difficulty-aligned co-evolutionary loop between an Agent Policy and an Environment Policy, GenEnv achieves superior performance and data efficiency on diverse suite of agent benchmarks. Our results suggest that future agent training systems should move beyond larger static datasets toward adaptive, self-calibrating simulation environments. Beyond the particular instantiation studied here, we believe that difficulty-aligned simulators can serve as general recipe for training robust LLM agents in domains where real-world exploration is costly or risky. 16 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"
        },
        {
            "title": "References",
            "content": "P. Chen, X. Chen, W. Yin, and T. Lin. Compo: Preference alignment via comparison oracles. In NeurIPS, 2025a. P. Chen, X. Li, Z. Li, X. Chen, and T. Lin. Stepwise guided policy optimization: Coloring your incorrect reasoning in grpo. In The 5th Workshop on Mathematical Reasoning and AI at NeurIPS 2025, 2025b. P. Chen, X. Li, Z. Li, W. Yin, X. Chen, and T. Lin. Exploration vs exploitation: Rethinking rlvr through clipping, entropy, and spurious reward. arXiv preprint arXiv:2512.16912, 2025c. Y. Chen. Training LLM-based agents with synthetic self-reflected trajectories and partial masking. arXiv preprint arXiv:2505.20023, 2025. B. Ding, C. Qin, R. Liu, L. Bing, S. Joty, Q. Li, and C. Xiao. Data augmentation using large language models: Data perspectives, learning paradigms and challenges. arXiv preprint arXiv:2403.02990, 2024. A. Grattafiori et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. J. Guo, Z. Li, J. Qiu, Y. Wu, and M. Wang. On the role of preference variance in preference optimization. arXiv preprint arXiv:2510.13022, 2025. I. Gur, O. Huang, A. Kim, A. Anderson, G. Fast, N. Kushman, B. Zitkovich, D. Aiken, M. G. Luong, et al. Understanding HTML with large language models. arXiv preprint arXiv:2210.03945, 2023. T. Q. Hoang, K.-H. Huang, S. Kokane, J. Zhang, Z. Liu, M. Zhu, J. Grigsby, T. Lan, M. S. Ryoo, C.-S. Wu, S. Heinecke, H. Wang, S. Savarese, C. Xiong, and J. C. Niebles. LAM SIMULATOR: Advancing data generation for large action model training via online exploration and trajectory feedback. Findings of the Association for Computational Linguistics: ACL 2025, 2025. B. Jin, H. Zeng, Z. Yue, D. Wang, H. Zamani, and J. Han. Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. URL https://arxiv.org/abs/2503.09516. S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. C. Li, R. Wang, X. Zhao, B. Xu, F. Liu, R. Yang, and X. Zhang. Agentsociety: Large-scale simulation of LLM-driven generative agents advances understanding of human behaviors and society. arXiv preprint arXiv:2502.08691, 2025a. URL https://arxiv.org/abs/2502.08691. M. Li, Y. Chen, Z. Zhang, et al. comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://aclanthology.org/2023.emnlp-main.187. API-Bank benchmark. Z. Li, T. Zou, et al. ToRL: Scaling tool-integrated reinforcement learning. arXiv:2503.23383, 2025b. URL https://arxiv.org/abs/2503.23383. arXiv preprint Z. Liu, H. Yuan, Q. Xu, X. Jin, and H. Ren. LIMI: Less is more for agency. arXiv preprint arXiv:2509.17567, 2025. J. Luo et al. STeCa: Step-level trajectory calibration for LLM agent learning. arXiv preprint arXiv:2503.21460, 2025. GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2021. L. Ning, Z. Liang, Z. Jiang, H. Qu, Y. Ding, W. Fan, X.-y. Wei, S. Lin, H. Liu, P. S. Yu, and Q. Li. survey of webagents: Towards next-generation AI agents for web automation with large foundation models. arXiv preprint arXiv:2503.23350, 2025. OpenAI. GPT-OSS: Open-weight models for reasoning and agents. https://openai.com/index/ introducing-gpt-oss/, 2025. OpenAI blog post. V. Pahuja, Y. Lu, C. Rosset, B. Gou, A. Mitra, S. Whitehead, Y. Su, and A. H. Awadallah. Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. Findings of the Association for Computational Linguistics: ACL 2025, 2025. S. G. Patil, Z. Zhang, X. Li, et al. The berkeley function-calling leaderboard (BFCL). In International Conference on Machine Learning (ICML), 2025. URL https://gorilla.cs.berkeley.edu/blogs/ 8_berkeley_function_calling_leaderboard.html. Berkeley Function-Calling Leaderboard. D. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):8897, 1991. O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, 2023. URL https://arxiv.org/abs/2210.03350. A. Samadi, S. K. S. Ghasemipour, V. Caggiano, M. Dangelmaier, and S. Jha. Good data is all imitation learning needs. arXiv preprint arXiv:2409.17605, 2024. T. Schick, J. Dwivedi-Yu, R. DessÃ¬, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR, 2015. Z. Shao et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://arxiv.org/abs/2402.03300. Introduces GRPO (Group Relative Policy Optimization). N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://arxiv.org/abs/2303.11366. M. Shridhar, X. Yuan, M.-A. CÃ´tÃ©, Y. Bisk, A. Trischler, and M. Hausknecht. ALFWorld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768. J. Sun, Y. Ren, H. Wang, Y. Tang, J. Liu, and Z. Yuan. Agent learning via early experience. arXiv preprint arXiv:2510.08558, 2025. H. Tan, Z. Zhang, C. Ma, X. Chen, Q. Dai, and Z. Dong. MemBench: Towards more comprehensive evaluation on the memory of LLM-based agents. In Findings of the Association for Computational Linguistics: ACL 2025, 2025. URL https://aclanthology.org/2025.findings-acl.989. 18 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators E. Todorov, T. Erez, and Y. Tassa. MuJoCo: physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 50265033, 2012. L. S. Vygotsky. Mind in Society: The Development of Higher Psychological Processes. Harvard University Press, Cambridge, MA, 1978. G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An open-ended embodied agent with large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://arxiv.org/abs/2305.16291. Y. Wang, L. Yang, Y. Tian, K. Shen, and M. Wang. Co-evolving LLM coder and unit tester via reinforcement learning. arXiv preprint arXiv:2506.03136, 2025a. Y. Wang, L. Yang, Y. Tian, K. Shen, and M. Wang. Cure: Co-evolving coders and unit testers via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. Y. Wang, D. Yin, Y. Cui, R. Zheng, Z. Li, et al. UI-Simulator: LLMs as scalable, general-purpose simulators for evolving digital agent training. arXiv preprint arXiv:2510.14969, 2025c. URL https://arxiv.org/abs/2510.14969. J. Xie, K. Zhang, J. Chen, T. Zhu, R. Lou, Y. Tian, Y. Xiao, and Y. Su. Travelplanner: benchmark for real-world planning with language agents. In International Conference on Machine Learning (ICML), 2024. URL https://arxiv.org/abs/2402.01622. A. Yang and Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. A. Yang and Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. URL https://arxiv.org/abs/2210.03629. J. Ye, X. Gao, K. Zhang, G. Gao, Y. Li, and X. Liu. LLM-DA: Data augmentation via large language models for few-shot named entity recognition. arXiv preprint arXiv:2402.14568, 2024. Z. Yu, L. Yang, J. Zou, S. Yan, and M. Wang. Demystifying reinforcement learning in agentic reasoning. arXiv preprint arXiv:2510.11701, 2025. S. Yuan, Z. Chen, Z. Xi, J. Ye, Z. Du, and J. Chen. Agent-R: Training language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025. S. Yuan et al. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint, 2024. J. Zhang et al. Agentbank: Towards generalized LLM agents via fine-tuning on 50000+ interaction trajectories. arXiv preprint, 2025a. R. Zhang, H. Li, J. Liu, M. Chen, and X. Wang. Deepanalyze: Agentic large language models for autonomous data science. arXiv preprint arXiv:2510.16872, 2025b. T. Zhao, Q. Huang, H. Liu, J. He, F. Peng, P. Li, and Y. Liu. Scaling synthetic task generation for agents via exploration. arXiv preprint arXiv:2509.25047, 2025. 19 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators J. Zhou, Y. Lee, K. Huang, Y. Zhao, R. Chen, and H. Zhang. Faithful simulation of useragent environment interactions for reliable evaluation of LLM agents. In Proceedings of the 2025 International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/ pdf/b590327f99064f537038a6254f0d993d2671ad29.pdf. OpenReview preprint. Y. Zhu, S. Qiao, Y. Ou, S. Deng, N. Zhang, et al. Knowagent: Knowledge-augmented planning for LLM-based agents. arXiv preprint arXiv:2403.03101, 2024. URL https://arxiv.org/abs/ 2403.03101. J. Zou, X. Yang, R. Qiu, G. Li, K. Tieu, P. Lu, K. Shen, H. Tong, Y. Choi, J. He, et al. Latent collaboration in multi-agent systems. arXiv preprint arXiv:2511.20639, 2025. 20 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators A. Appendix A.1. Proofs for Section 3 In this appendix we provide detailed proofs for the theoretical results stated in Section 3. A.1.1. Proof of Proposition 1 Recall that for fixed task type ğœ and parameter vector ğœƒ, the Agent Policy update uses the REINFORCEstyle estimator ğ‘”(ğœ, ğ‘Ÿ) = (ğ‘Ÿ ğ‘(ğœ))ğœƒ log ğœ‹ğœƒ(ğ‘ ğœ), and we choose the baseline to be ğ‘(ğœ) = ğ”¼[ğ‘Ÿ ğœ] = ğ‘(ğœ). Let ğ‘† = ğœƒ log ğœ‹ğœƒ(ğ‘ ğœ) for brevity. Conditioned on ğœ, the reward ğ‘Ÿ is Bernoulli with success probability ğ‘(ğœ), so ğ”¼[ğ‘Ÿ ğœ] = ğ‘(ğœ) and Var(ğ‘Ÿ ğœ) = ğ‘(ğœ)(1 ğ‘(ğœ)). We have ğ”¼(cid:2)ğ‘”(ğœ, ğ‘Ÿ)2 ğœ(cid:3) = ğ”¼(cid:2)(ğ‘Ÿ ğ‘(ğœ))2 ğ‘†2 ğœ(cid:3) = Pr(ğ‘Ÿ ğœ) (ğ‘Ÿ ğ‘(ğœ))2 ğ”¼(cid:2)ğ‘†2 ğœ, ğ‘Ÿ(cid:3) . By Assumption 1, for both ğ‘Ÿ = 0 and ğ‘Ÿ = 1 we have ğ‘Ÿ {0,1} ğ‘min ğ”¼(cid:2)ğ‘†2 ğœ, ğ‘Ÿ(cid:3) ğ‘max. Therefore and similarly ğ”¼(cid:2)ğ‘”(ğœ, ğ‘Ÿ)2 ğœ(cid:3) ğ‘min ğ‘Ÿ {0,1} Pr(ğ‘Ÿ ğœ)(ğ‘Ÿ ğ‘(ğœ))2 = ğ‘min ğ”¼(cid:2)(ğ‘Ÿ ğ‘(ğœ))2 ğœ(cid:3) = ğ‘min Var(ğ‘Ÿ ğœ) = ğ‘min ğ‘(ğœ) (cid:0)1 ğ‘(ğœ)(cid:1), ğ”¼(cid:2)ğ‘”(ğœ, ğ‘Ÿ)2 ğœ(cid:3) ğ‘max Pr(ğ‘Ÿ ğœ) (ğ‘Ÿ ğ‘(ğœ)) ğ‘Ÿ {0,1} = ğ‘max ğ‘(ğœ) (cid:0)1 ğ‘(ğœ)(cid:1) . (10) (11) (12) (13) (14) (15) (16) (17) Taking ğ¶min = ğ‘min and ğ¶max = ğ‘max yields the bounds in Equation equation 5. Since ğ‘(1 ğ‘) is concave quadratic on [0, 1] with unique maximum at ğ‘ = 1/2, the expected squared gradient norm is maximized (up to constant factors) for tasks with ğ‘(ğœ) = 1/2, i.e., tasks of intermediate difficulty. This proves Proposition 1. A.1.2. Proof of Theorem 1 We restate the setting for clarity. For ğ‘– {1, 2}, the true success probability on task type ğœğ‘– is ğ‘ğ‘–, and we define Î”ğ‘– = ğ‘ğ‘– ğ›¼ with Î”1 < Î”2. From ğ‘›ğ‘– independent rollouts we obtain the empirical success rate Ë†ğ‘ğ‘– = ğ‘˜ğ‘–/ğ‘›ğ‘–, where ğ‘˜ğ‘– is the number of successes. The ğ›¼-Curriculum Reward is ğ‘…env( Ë†ğ‘ğ‘–) = exp (cid:0) ğ›½( Ë†ğ‘ğ‘– ğ›¼)2(cid:1) . 21 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Since the exponential function is strictly monotone decreasing in ( Ë†ğ‘ğ‘– ğ›¼)2, we have ğ‘…env( Ë†ğ‘1) ğ‘…env( Ë†ğ‘2) Ë†ğ‘1 ğ›¼ Ë†ğ‘2 ğ›¼. Thus the event that the Environment Policy mis-ranks the two tasks (i.e., gives ğœ1 no larger reward than ğœ2) is exactly the event Let ğ‘› = min{ğ‘›1, ğ‘›2} and define Emis = { Ë†ğ‘1 ğ›¼ Ë†ğ‘2 ğ›¼ }. ğ›¿ = Î”2 Î”1 3 > 0. Consider the event Egood = (cid:8) Ë†ğ‘1 ğ‘1 ğ›¿ and Ë†ğ‘2 ğ‘2 ğ›¿ (cid:9). We claim that on Egood we must have Ë†ğ‘1 ğ›¼ < Ë†ğ‘2 ğ›¼, and hence ğ‘…env( Ë†ğ‘1) > ğ‘…env( Ë†ğ‘2). Indeed, by the triangle inequality, Ë†ğ‘1 ğ›¼ ğ‘1 ğ›¼ + Ë†ğ‘1 ğ‘1 Î”1 + ğ›¿, Ë†ğ‘2 ğ›¼ ğ‘2 ğ›¼ Ë†ğ‘2 ğ‘2 Î”2 ğ›¿. (18) (19) By the choice of ğ›¿, we have Î”1 + ğ›¿ = Î”1 + Î”2 Î”1 3 = 2Î”1 + Î”2 3 , Î”2 ğ›¿ = Î”2 Î”2 Î”1 = Î”1 + 2Î”2 3 , and since Î”1 < Î”2, it follows that Î”1 + ğ›¿ = 2Î”1 + Î”2 < Î”1 + 2Î”2 3 = Î”2 ğ›¿. Therefore, Ë†ğ‘1 ğ›¼ Î”1 + ğ›¿ < Î”2 ğ›¿ Ë†ğ‘2 ğ›¼ on Egood, which implies ğ‘…env( Ë†ğ‘1) > ğ‘…env( Ë†ğ‘2). Consequently, Emis can only occur on the complement event ğ‘ good, and hence Pr(Emis) Pr(E ğ‘ good ) Pr (cid:0) Ë†ğ‘1 ğ‘1 > ğ›¿(cid:1) + Pr (cid:0) Ë†ğ‘2 ğ‘2 > ğ›¿(cid:1), where the last inequality is union bound. For each ğ‘– {1, 2}, Ë†ğ‘ğ‘– is the empirical mean of ğ‘›ğ‘– i.i.d. Bernoulli random variables with mean ğ‘ğ‘–. By Hoeffdings inequality, Pr (cid:0) Ë†ğ‘ğ‘– ğ‘ğ‘– > ğ›¿(cid:1) 2 exp(2ğ‘›ğ‘–ğ›¿2) 2 exp(2ğ‘›ğ›¿2), where ğ‘› = min{ğ‘›1, ğ‘›2}. Therefore Pr(Emis) 4 exp(2ğ‘›ğ›¿2) (cid:32) = 4 exp 2ğ‘› (cid:19) 2(cid:33) (cid:18) Î”2 Î”1 3 = 4 exp (cid:18) 2 (Î”2 Î”1)2 ğ‘› (cid:19) , which establishes the desired exponential bound and completes the proof of Theorem 1. (20) (21) (22) 22 GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators Table 2 Hyperparameters for GenEnv training. Parameter Agent Policy (ğœ‹agent) Environment Policy (ğœ‹env) Learning Rate Batch Size Method Total Epochs Target Difficulty ğ›¼ Difficulty Filter ğ‘˜min RWR Temperature ğœ† Optimizer (AdamW) 1 106 64 5 107 64 Policy Optimization GRPO 10 N/A N/A N/A Reward-Weighted Update (RWR) 10 0.5 0.1 1.0 A.2. Hyperparameter Details Table 2 lists the key hyperparameters used in our experiments. Note that for the Agent (ğœ‹agent), we employ GRPO, while the Environment (ğœ‹env) uses Reward-Weighted Regression (RWR). A.3. Environment Baseline Implementation Details Here we detail the specific implementation of the environment variants used in Section 4: GenEnv-Random: We use the same base model (Qwen2.5-7B-Instruct) for the environment. The autoenv.disable_env_training flag is set to True. It generates 4 variations per prompt per epoch dynamically, but the model weights are never updated based on ğ‘…env( Ë†ğ‘). GenEnv-Static: We use the generate_static_augmentation.py script to pre-generate 5 variations for each of the 544 original training samples, resulting in fixed dataset of 3,264 samples. The agent is trained using standard PPO for 10 epochs. Gemini-Offline Baselines: We prompted Gemini 2.5 Pro to generate variations of the training data. Due to API constraints and filtering, the Gemini-2x setting resulted in 957 samples (1.76x) and Gemini-4x resulted in 1,777 samples (3.27x). These represent high-quality, but static, external data augmentation."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Columbia University",
        "Princeton University",
        "University of Chicago",
        "University of Michigan"
    ]
}