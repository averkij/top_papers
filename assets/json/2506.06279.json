{
    "paper_title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "authors": [
        "Shi Liu",
        "Weijie Su",
        "Xizhou Zhu",
        "Wenhai Wang",
        "Jifeng Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/."
        },
        {
            "title": "Start",
            "content": "CoMemo: LVLMs Need Image Context with Image Memory Shi Liu * 1 Weijie Su * (cid:0) 1 Xizhou Zhu 2 1 Wenhai Wang 3 1 Jifeng Dai 2 1 5 2 0 2 6 ] . [ 1 9 7 2 6 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - dual-path architecture that combines Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemos superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/. 1. Introduction Recent advances in large language models (LLMs) have demonstrated unprecedented generative capabilities (Ouyang et al., 2022; Touvron et al., 2023), primarily driven by the exponential scaling of training data and model *Equal contribution Project lead 1Shanghai Artificial Intelligence Laboratory 2Tsinghua University 3The Chinese University of Hong Kong. Correspondence to: Weijie Su <suweijie@pjlab.org.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). parameters. Building upon this foundation, large visionlanguage models (LVLMs) have emerged as powerful multimodal systems that align visual representations with LLM embedding spaces to enable cross-modal reasoning (Liu et al., 2024b; Alayrac et al., 2022). Current methodologies for visual information injection predominantly follow two architectural paradigms. The first paradigm (referred to as LVLM-X, e.g., Flamingo (Alayrac et al., 2022)) employs cross-attention mechanisms to integrate visual features into textual representations. While this approach offers flexible modality interaction, recent studies (Laurencon et al., 2024) have revealed its suboptimal performance compared to alternative approaches when using identical LLM backbones and training data - finding corroborated by our studies in Figure 1. The second paradigm (referred to as LVLM-S, e.g., LLaVA (Liu et al., 2024b)) aligns visual tokens into text token embeddings space and then performs autoregressive processing. This paradigm is more compatible with LLM architectures; however, the preservation intrinsic mechanisms such as attention bimodality (Xiao et al., 2023; Liu et al., 2025a) and the linearly increasing position encoding leads to critical limitations: (1) the lost in the middle (Liu et al., 2024c; Song et al., 2024) phenomenon degrades performance with increasing context length, and (2) positional encoding sparsity induces remote decay and 2d-dimensional lost in high-resolution image processing. In this paper, we propose novel framework for LVLM, named CoMemo. Our key idea is to introduce an additional image-processing mechanism that is unaffected by context, without modifying the internal mechanisms of the LLM. Specifically, we concatenate image tokens with text tokens as the input sequence for fully autoregressive processing, while simultaneously feeding the image tokens into mixin layer for cross-attention computation. Crossattention retrieves image information based on text, avoiding the issue of image neglect that can occur in causal selfattention. However, simply combining these two structures, as in NVLM-H (Dai et al., 2024), does not work well. To address this, we first investigated balanced scheme for visual representation inputs. Then, we introduced threestage training technique to prevent overreliance on the crossattention path, effectively transforming it into memory CoMemo: LVLMs Need Image Context with Image Memory (a) LVLM-S. (b) LVLM-X. (c) Ours. Figure 1: Evaluation results of three architectures with same training data and model size (2B). Please refer to Tables 2 to 4 for details. Figure 2: Comparison three types of architectures for LVLMs. Method (a) use image encoder to align visual features with the LLMs continuous token representation space. Method (b) employs mixin layer with cross-attention to update LLMs hidden states based on visual features. And Method (c) contrust dual-path structure to enable the model to focus more on visual content during generation. path. Meanwhile, the fully autoregressive process serves as the primary path for introducing image context, referred to as the context path. The positional encoding scheme in LVLMs typically adopts RoPE from LLMs, treating each image patch token as an individual token for encoding. However, this approach results in highly sparse positional encodings for dynamic highresolution image patch tokens. Such sparse encoding can lead to remote decay issues in positional encoding, and the one-dimensional incremental encoding scheme also loses the two-dimensional information of the image. To address this, we propose novel positional encoding scheme based on the dynamic high-resolution method. Specifically, in the dynamic high-resolution approach, the image is divided into multiple image tiles and single image thumbnail. We treat the image thumbnail as part of the input sequence for standard sequential positional encoding, while mapping the image tiles to the image thumbnail index based on their two-dimensional positional relationships. To fully evaluate the impact of the CoMemo architecture, we collected set of multimodal benchmarks and categorized them into seven evaluation tasks: Caption, Long-generation, Multi-image, Long-context, Math, General VQA, and OCRrelated tasks. The results demonstrates CoMemos superiority over LVLM-X/S baselines under same data and model settings. Our framework achieves 17.2%, 7.0% and 5.6% relative improvement on Caption, Long-Generation and LongContext tasks respectively, with consistent gains across various benchmarks. 2. Design Thinking for CoMemo 2.1. Why LVLMs Tend to lose in the middle? Previous studies have shown that both LLMs and LVLMs exhibit the Lost in the middle phenomenon (Liu et al., Figure 3: Average gradients and attention weights assigned to tokens at corresponding positions. We computed the average over 1,000 samples. 2024c; Song et al., 2024), as illustrated in Figure 8a. This refers to their struggle to capture key information placed in the middle of the input. However, this phenomenon arises from the causal self-attention and in-context mechanisms inherent in these models. Both LLMs and LVLMs are trained using the next-token prediction paradigm, which heavily relies on contextual tokens during prediction. As shown in Figure 3, we plot the gradients of each input token during training and the attention weights assigned to each token during inference for the InternVL2 model. To handle sequences of varying lengths, we map each sequence into 100 bins based on the position of each token. significant portion of the gradient for the current predicted token is backpropagated to nearby tokens. As result, models trained in this way tend to allocate most of their attention to adjacent tokens during inference, while the initial token, which has larger gradient and attention, acts as an attention sink to release redundant attention (Xiao et al., 2023). This results in the effective capture of key information at the be2 CoMemo: LVLMs Need Image Context with Image Memory Figure 4: Remote decay estimation for InternVL2-2B. The relative distance refers to the difference between absolute position IDs. In RoPE, the position ID of each input token increments by 1 with the input sequence. Figure 5: Balancing experiments. Experiment settings are described in Section 2.3. 1k, 2k and 4k means pretrain steps. All scores are evaluated after fine-tuning the pretrained checkpoint corresponding to the x-axis. ginning of the sequence, as well as nearby tokens benefiting from contextual attention. However, key information placed in the middle of the sequence is more likely to be lost. As the context length increases, the middle segment, which is at higher risk of being lost, also extends. Finding 1: The attention distribution mechanism in causal self-attention is the cause of the Lost in the middle phenomenon. This mechanism also limits the performance of LVLMs in long-context scenarios. 2.2. Remote Decay in LVLMs with DHR Dynamic high resolution significantly enhances the performance of visual tasks by introducing more visual context, especially in tasks that require high resolution, such as OCR-related evaluations. However, this benefit introduces critical trade-off: excessive visual context exacerbates the remote decay issue in Rotary Position Embedding (RoPE), ultimately limiting model effectiveness in long-context scenarios. RoPE implements relative position encoding through absolute encoding mechanisms, formally expressed as: (Rmq)(Rnk) = Re d/21 (cid:88) q[2i:2i+1]k [2i:2i+1]ei(mn)θi , i=0 (1) where m, denote token positions, the embedding dimension, and θi follows the sinusoidal position encoding scheme. This formulation inherently inherits the remote decay characteristics of sinusoidal encodings. RoPE exhibits the remote decay property where the relative size between tokens decreases as the relative distance in3 creases (Su, 2021), as shown in Figure 4. While standard InternVL2 processes 256 image tokens per image, activating DHR with dynamic number of 6 expands this to 1,792 tokensa 7 increase that further reduces the influence of image tokens during generation. Finding 2: The context expansion from DHR fundamentally aggravates image neglect. 2.3. The Balance Between tow Pathways CoMemo faces critical challenge in balancing two visual processing pathways: the cross-attention path and the fully autoregressive path. Through systematic experiments with different high-resolution allocation strategies and training strategies, we identify two key balancing principles. In the mixin layers, we incorporate gating mechanism that utilizes gate values to reflect the influence of the crossattention path during the decoding process. Therefore, we choose to evaluate the dependency of LVLM on these two paths from this perspective. Our analysis averages the gates values of mixin layers to quantify pathway preference, with performance evaluated across captioning, general VQA, and OCR tasks. Balance in DHR Allocation. We compare three distinct DHR allocation strategies: (1) allocating DHR information exclusively to the fully autoregressive path, termed DHR-S, (2) assigning it solely to the cross-attention path, termed DHR-X, which corresponds to the allocation mechanism of NVLM-H (Dai et al., 2024), and (3) distributing it to both pathways, termed DHR-B. In unilateral allocation scenarios, the counterpart pathway receives only thumbnail resolution information. As shown in Figure 5, DHR allocation significantly influences pathway specialization. When DHR is exclusively allocated to one pathway, the model shows pronounced bias toward that pathway. In contrast, dual-pathway allocation results in more stable and balanced CoMemo: LVLMs Need Image Context with Image Memory model behavior. Balance in training steps. Our findings reveal that pretraining steps profoundly affects the equilibrium between the two mechanisms. Although fine-tuning involves extended training (9,000 steps), the visual processing paradigm is largely established during pretraining. In Figure 5, the DHR-B configuration demonstrates that insufficient pretraining steps result in suboptimal alignment, leading to compromised fine-tuning performance. Conversely, excessive pretraining induces over-reliance on specific pathways. While fine-tuning stage attempts to mitigate this imbalance, it ultimately fails to fully rectify the entrenched dependency in DHR-B-4k in Figure 5. This phenomenon arises because during pretraining, only the memory branch and projector parameters are trainable. The projectors limited function of mapping image representations into the text space provides minimal gains in visual comprehension. Consequently, prolonged pretraining naturally reinforces reliance on the cross-attention branch. Finding 3: CoMemo exhibits fundamental balancing challenge between dual visual processing pathways. Finding 4: CoMemos dual-path visual processing paradigm establishes during pretraining. 3. Method 3.1. RoPE-DHR Standard RoPE implementations in LLMs employ continuous incremental positional encoding scheme, which is inherited by LVLMs. While effective for sequential text data, this approach faces significant limitations when processing high-resolution visual inputs: (1) remote decay due to extended context lengths, and (2) dimensional collapse of 2D spatial relationships. To address these limitations, we propose RoPE-DHR, novel position encoding method that employs hierarchical strategy. We first processes the thumbnail patch tokens using conventional RoPE to generate base position IDs. For highresolution tiles, we establish geometric correspondence by mapping each tile patchs coordinates (xtile, ytile) to its corresponding thumbnail patch index ithumb. This mapping is defined as: ithumb = (xtile Wtile Worig +wbtile, ytile Horig Hthumb +hbtile) (2) where (Worig, Horig) and (Wtile, Htile) denote the original and tile dimensions respectively. The terms wbtile and hbtile 4 represents the start position biases for the tile in the width and height dimensions.This mapping preserves 2D spatial relationships while maintaining compatibility with existing RoPE implementations. In Figure 6, we visualize the raw image,the thumnail and their corresponding patch token position IDs using color bar. Crucially, our method decouples positional encoding from absolute sequence positions through: (1) length reduction: prevent the sparse position encoding for DHR by compression the position length in global perspective. (2) geometry reserve: tile patches inherit positional context from their thumbnail anchors. 3.2. Architecture of CoMemo While LLaVA-like architectures demonstrate effective visual-language alignment, they exhibit tendency to disregard visual information when processing lengthy contextual inputs or generating extended responses. To address this limitation, we propose the CoMemo architecture, which is based on three key structures: Dual-stream Structure. CoMemo maintains tow visual processing streams: (1) context path serves as the primary processing stream where image representations are treated as special tokens, concatenated with text tokens to form the input sequence for autoregressive modeling. (2) memory path establishes an auxiliary processing stream where image representations interact with the input sequence through cross-attention mechanisms. The two paths maintain identical image representations, ensuring feature consistency while enabling complementary processing and the balance between in tow path as we disccussed in Section 2.3. Position-aware Cross-attention. Existing LVLM-X typically employ absolute positional encoding for image patch tokens during encoding (Dubey et al., 2024), with some variants introducing additional positional semantics through specialized tokens (Dai et al., 2024). However, these approaches provide only unidirectional positional awareness. To address this limitation, we implement RoPE in crossmodal attention, establishing bidirectional positional awareness: query positions (poss) correspond to input sequence token ordering and key positions (posi) align with visual token indices in the input sequence. The attention mask employs bidirectional visibility constraints, similar to mPlugowl3 (Ye et al., 2024), where image tokens are visible to their corresponding sequence positions while maintaining bidirectional attention. Memory Mixin Strategy As shown in Figure 7, CoMemo mixin layers are interleaved with standard transformer blocks at 1:4 ratio. Each memory layer performs: (1) Gated Cross-attention: Modulates visual influence through learnable attention gates (attn gate). (2) Adaptive FeedCoMemo: LVLMs Need Image Context with Image Memory Algorithm 1 Mixin Layers Require: hs (sequence hidden states), hi (image hidden states), attn gate , ffw gate , poss (sequence position IDs), posi (image position IDs) Ensure: Updated hs 1: hs hs + tanh(attn gate) cross attn(q = hs, kv = hi, poss = poss, posi = posi) 2: hs hs + tanh(ffw gate) ffw(hs) 3: Return: hs Figure 6: The computation process of Rope-DHR. The colors are assigned based on mapping of position IDs in RoPE. Figure 7: Framework of CoMemo. Both paths share the same encoder and projector. The pixel shuffle technique is adopted from InternVL series (Team, 2024; Chen et al., 2024c). forward: Enhances feature transformation via gated nonlinearity (ffw gate). During autoregressive decoding, CoMemo requires only single-step computation between the current decoding token and the cached visual memory states, eliminating the need for key-value caches. This approach circumvents the issue of increasing key-value cache size as the sequence lengthens. The orthogonal design of the architecture ensures compatibility with existing LLaVA variants. 3.3. Training Stages Traditional LVLMs training consists of two phases: pretraining and fine-tuning. During the pretraining phase, we selectively update the projector module and memory architecture parameters while keeping other components frozen. This phase prioritizes cross-modal representation alignment and dynamic equilibrium between dual visual processing pathways. critical challenge emerges during pretraining stemming from asymmetric parameter updates: (1) Insufficient training iterations lead to suboptimal projector learning; (2) Prolonged training induces excessive dependency on the memory pathway for LVLM decoding. This imbalance originates from the disparate update strategies - full fine-tuning of memory parameters versus partial tuning of the context projector. Consequently, the LVLM exhibits inherent bias towards memory-based information retrieval to achieve loss minimization. To mitigate this optimization bias, we propose three-stage training strategy. In the first stage, we tune the parameters 5 of projector and mixin layers. In the next stage, we freeze the gate parameters. This design aims to enable the model to learn representation alignment and balance the dual-path structure in the first stage. After certain number of training steps to prevent over-reliance on the cross-attention path, we freeze the corresponding gate control parameters and continue training until the alignment structure is sufficiently learned. The subsequent fine-tuning stage adopts full-parameter training paradigm. During this stage, all model parameters become trainable with the training objective shifted towards instruction-following. 4. Experiments 4.1. Setup To ensure fair comparison of the capabilities across various architectures, all three of our models utilize the same pretraining and fine-tuning datasets. Each architecture adopts InternLM-1.8B as the LLM backbone and InternViT-300M as the image encoder. The hidden dimensions of each architectures network, along with hyperparameters such as learning rate and weight decay during training, are also kept consistent. We use the same training data as InternVL-2. Therefore, LVLM-S in our experiments essentially represents InternVL-2. Training for all architectures is divided into two phases: pretraining and fine-tuning. For specific settings, please refer to the appendix. CoMemo: LVLMs Need Image Context with Image Memory Table 1: Comparison with other LVLMs of different architectures. * indicates results obtained from our own experiments. Other results are sourced from official reports or third-party evaluation leaderboards. Model Params. Caption Long-Generation Multi-Image Long-Context Math General VQA OCR-Related C - - 90* - - 79.1* 114* 108.2 98. c - - 72* - - 65.4* 80.2* 86.3 78.5 C - - - 94.2* - - 60.0* 108.6* - 78.8 . A L - - 64.2* 80.9 66.1 62.9* - 52.5 66. M 70.2 - - - - 28.7* 33.6* - 38.7 L 68.0 54. 50.3 39.8 41.2 38.2* 44.0 44.4 43.5 n - 62.7 63.1 - - 48.3* 54.8 - 50. M 65.4 64.3 - 53.1 54.5 50.2* 54.5 55.1 51.3 Closed-Source GPT-4o GPT-4V LVLM-X mPlug-owl3 LLama3.2-V LVLM-S MiniCPM-V-2 InternVL-2 InternVL-2.5 Qwen2-VL CoMemo - - 8B 11B 2.8B 2B 2B 2B 2B N - - - - - - 27.0* 22.2* - 34.2 e i 60.3 53.0 50.0* - - 52.1* 50.1* - 54.6 i a 63.8 58.1 - 51.5 39 48.0* 51.3 43.0 i h - - - - 15.4 16.4* 13.5 19.7 17.0 e M - - - 77.3 62.9 73.1* 74.7 - 74.2 - 1926.6 - 1820. 1808.2 1869* 2138.2 2326.8 1904 M - 38.7 - - - 31.3* 40 - 36.0 2 84.6 78.2 73.4 91.1 64.8 74.3* 74.9 74.7 74.2 r 85.7 78. - 83.4 59.6 75.6* 79.2 73.5 73.6 t 77.4 78.0 - 73.1 - 74.2* 74.3 79.7 72. Table 2: The results on Generation and Math benchmarks. The test settings are described in Section 4.3. The highest scores are highlighted in bold. Table 3: The results on Multi-image and Long-context benchmarks. The test settings are described in Section 4.4. The highest scores are highlighted in bold. Model Caption Long Generation Math COCO Flickr No-Caps LLaVABench MMDU MathVista MathVision Model Multi-Image Long-Context BLINK Mantis MMT MM-NIAH-M MM-NIAH-T MileBench LVLM-X LVLM-S Ours 84.9 79.1 98.6 68.9 65.4 78.5 62.9 60.0 78.8 50.8 62.9 66.9 31.5 28.7 38. 44.2 48.0 50.0 15.8 16.4 17.0 LVLM-X LVLM-S Ours 41.5 38.2 43.5 46.5 48.3 50.6 47.8 50.2 51. 39.51 26.7 33.7 29.51 27.3 34.6 53.21 52.1 54.6 4.2. Comparison with Other LVLMs As shown in Table 1, we benchmark CoMemo against leading open-source and proprietary LVLMs. It is important to note that our primary objective is architectural ablation rather than absolute performance maximization. The comparison with other models serves to demonstrate that our model was trained on large-scale dataset and achieves toptier performance, rather than being tested on toy datasets. 4.3. Generation and Math Benchmark Contemporary multimodal benchmarks primarily rely on constrained response formats (e.g., multiple-choice questions and Yes/No queries) to facilitate evaluation. However, such approaches inadequately assess open-ended reasoning capabilities, as free-form responses introduce higher diversity and complexity. To comprehensively evaluate model capabilities across different granularities, we collected several benchmarks for open-ended evaluation: Caption Generation: Evaluates the models ability to generate concise image descriptions (10-15 words) using CIDEr scores on COCO (Lin et al., 2014a), Flickr30k (Plummer et al., 2015), and NoCaps (Agrawal et al., 2019) datasets. lengthy textual context (avg. 6.4k tokens) and multiple images (ranging from 2 to 20 images). Math: Measures model reasoning ability on math diagrams and visual math problems using MathVision (Wang et al., 2024a) and MathVista (Lu et al., 2023). During evaluation, we asked the model to provide step-by-step reasoning and extracted the final answer to calculate accuracy. As shown in 2, our analysis reveals three key findings. First, LVLM-Xs continuous attention mechanism demonstrates superiority in pure visual captioning tasks, achieving +4% average improvement. Second, LVLM-Ss causal attention architecture achieves better performance in knowledgeintensive scenarios, demonstrating enhanced contextual reasoning capabilities from its LLM backbone. Our proposed CoMemo combines the advantages of both approaches, outperforming the original architectures across various tasks. This supports our hypothesis that dual-path attention allocation effectively integrates the benefits of both architectures: maintaining visual grounding while enabling complex reasoning. 4.4. Multi-image and Long-context Benchmark Long Generation: Evaluates extended inference using LLaVABench (Liu et al., 2024b) and MMDU (Liu et al., 2024e). LLaVABench focuses on single-image context reasoning, involving few hundred context tokens, while MMDU is multi-image analysis with To systematically evaluate multimodal long-context understanding, we establish two complementary evaluation dimensions: 1LVLM-Xs single image token compression reduces average context length by 50% (e.g., 32k16k). 6 CoMemo: LVLMs Need Image Context with Image Memory Multi-image: When DHR is enabled, each image contributes approximately 2k tokens to the context length, thereby necessitating extended context capacity for multi-image analysis. We evaluate performance on the BLINK (Fu et al., 2025), Mantis (Jiang et al., 2024), and MMT (Ying et al., 2024) datasets. Long-context: Tests information extraction in long context scenarios. We select MM-NIAH (Wang et al., 2024c) evaluation that detects image/text needles within hybrid long contexts. And MileBench (Song et al., 2024) progressively challenging tasks with 2109 images. These benchmarks systematically quantify long-context capabilities from both textual token and visual token perspectives. As shown in Table 3, our proposed architecture achieves the best performance in scenarios involving multiple images and long contexts. Specifically, MM-Niah-T represents the needle that is the key information placed in the text, while MM-NIAH-M represents the needle placed in the image. In the evaluation of MM-NIAH-T, the memory structure stores image data that is unrelated to the needle, redundant information. Nevertheless, our model still achieves the best performance. This not only demonstrates that compressing the image token position space through RoPE-DHR enhances the models ability to understand long sequence texts but also indicates that the Memory structure does not cause the model to overfocus on image information, thereby preserving its ability to retrieve and reason with language effectively. In the MileBench evaluation, due to the potential for excessive image tokens leading to long context sequences and out-of-memory issues, we did not enable DHR settings. Therefore, in this evaluation, each image in the input sequence has only single image thumbnail. In this scenario, our architectures positional encoding is the same as LVLM-S, primarily reflecting the role of the memory structure. Despite this, our architecture still achieved 2.5% improvement over LVLM-S. The MileBench benchmark also includes needle in haystack tasks for both text and image scenarios. In Figure 8, we visualize the average results for these two types of NIAH tasks in MileBench. As mentioned earlier, we observed that the attention mechanism of LLMs and LVLMs exhibits bimodal distribution, where LVLMs tend to focus more on the beginning and the most recent tokens, leading to the Lost in the middle phenomenon. This means that when the needle is placed in the middle of the sequence, the models performance on NIAH tasks deteriorates. Our architecture, however, addresses this issue by continuously focusing on the middle image information during the token generation process, effectively mitigating this problem. (a) LVLM-S. (b) CoMemo Figure 8: Heatmap of results for the NIAH evaluation on MileBench benchmark. The depth percentage indicates the position of the target information (needle) relative to the entire sequence. Table 4: The results on General VQA and OCR-related benchmarks. The test settings are described in Section 4.5. The highest scores are highlighted in bold. Model LVLM-X LVLM-S Ours General VQA OCR-Related MMBench MME MMVP AI2D ChartQA TextVQA 69.6 73.1 74.2 1812 1869 1904 27.3 31.3 36.0 69.9 74.3 74. 69.4 75.6 73.6 68.8 74.2 72.6 4.5. General VQA and OCR Benchmark To comprehensively evaluate CoMemos multimodal understanding capabilities, we conduct extensive experiments on conventional vision-language benchmarks covering two main categories: General VQA: Assessing visual perception and reasoning abilities through single-image question answering, including MMBench (Liu et al., 2025b), MME (Fu et al., 2023), and MMVP (Zhong et al., 2023). OCR-Related: Requiring fine-grained information extraction from diagrams and charts, evaluated on AI2D (Hiippala et al., 2021), TextVQA (Singh et al., 2019b), and ChartQA (Masry et al., 2022b). The input sequences in these two types of benchmarks are relatively short, and responses typically consisting of single words or short phrases. In contrast, CoMemo incorporates memory structure that alleviates image neglect by introducing an additional image focus mechanism. Additionally, RoPE-DHR addresses image neglect by compressing image information to reduce the long-range decay caused by positional encoding. While these techniques are not specifically tailored for the benchmarks mentioned above, our architecture still performs competitively when compared to 7 CoMemo: LVLMs Need Image Context with Image Memory LVLM-S. As shown in Table 4, our architecture performs slightly better than LVLM-S on some general multimodal benchmarks. However, in tasks such as text and OCR, which require high-resolution image information, traditional approaches typically rely on more granular image representations. This approach contrasts with the philosophy underlying our approach, which focuses on improving the models long-context and long-generation capabilities. 4.6. Training and Inference Efficiency Comparison Table 5: Training efficiency comparison across different 2B parameter models. Model LVLM-X LVLM-S CoMemo Batch size 1024 1024 1024 # of A100 GPUs 64 64 64 Train steps/s 0.123 0.105 0.096 Train samples/s 15.71 13.4 12. Table 6: Inference speed comparison across different 2B parameter models (lower is better). Model Batch size # of A100 GPUs LVLM-X LVLM-S CoMemo 1 1 8 8 8 Caption COCO (sec) 260 270 280 MMBench (sec) 76 90 110 MMNIAH (min) 22 25 30 TextVQA (sec) 88 100 120 To comprehensively compare the training and inference efficiency of three architectures, we measured their sample throughput during training and inference times across multiple benchmarks. As shown in Table 5 and Table 6, we report the training and inference efficiency for each architecture. The results demonstrate that CoMemo achieves latency comparable to LVLM-S. Although LVLM-X exhibits higher efficiency due to the use of fewer image tokens, its performance is significantly inferior to both CoMemo and LVLM-S. 4.7. Ablation Study Ablation on Components of CoMemo. We conducted complete ablation study on components using 2B-scale model, as shown in Variants 1 to 5 in Table 7. When only RoPE-DHR is introduced, the compressed position encoding significantly improves performance on both longgeneration and long-context tasks (Variant 2). When only the memory path was introduced, it addressed issues related to neglecting image information, leading to some improvement in the models performance (Variant 3). However, since the image tokens in the memory path lack positional information, during cross-attention computation, there is no positional correspondence between image tokens and text tokens. Moreover, the lack of distinguishing features between image tiles, thumnails, and multiple images hindered the models capabilities in different dimensions. Therefore, 8 after incorporating RoPE-DHR, the models capabilities in different dimensions were further enhanced (Variant 5). However, since RoPE-DHR is essentially compressionoriented encoding, it may affect scenarios like OCR that require fine-grained information. Ablation on compression ratio of RoPE-DHR. In the main experiments, RoPE-DHR uses shared position IDs for image tokens in the thumbnail and their corresponding subimage tokens. This approach effectively compresses the position encoding. Therefore, we propose variant, RoPEDHR without compression, where the position encodings for subimage tokens corresponding to two-dimensional position increment by 1 relative to the thumbnail image tokens, while the position IDs between thumbnail image tokens increment based on the number of tokens at their corresponding positions, rather than by 1. The experimental results are shown in Table 7 in Variants 4. It can be observed that, without compression, the model outperforms the LVLM-S architecture across all dimensions. Ablation on different scale models. To verify that CoMemo adheres to the scaling law, we select InternLM7B as the language model for experiments at the 7B scale. As shown in Table 7 in Variant 6 and 7, at the 8B scale, CoMemos average performance across all dimensions remains superior to the LVLM-S architecture. The CoMemo architecture continues to deliver outstanding performance in both Cation and Long-context tasks. As language models scale up, the impact of compressed position encoding becomes more pronounced in OCR tasks. Consistency on Different Datasets. We also conducted dataset-switching experiments using the open-source InternVL-1.2 1.2M fine-tuning dataset (OpenGVLab, 2024) during the SFT stage, while keeping the pretraining data consistent. As shown in Variants 8 to 9 in Table 7, even with the changes in the dataset, our CoMemo consistently outperforms the LVLM-S architecture across various dimensions. 5. Related Works 5.1. Mainstream LVLMs and Their Architectures Contemporary LVLMs typically employ pre-trained language models as decoders, utilizing two dominant strategies for visual-text alignment: (1) cross-attention mechanisms and (2) joint projector-autoregressive architectures. Fully Autoregressive LVLMs: LLaVA (Liu et al., 2024b) pioneered this approach by projecting image representations into the LLMs space and jointly decoding them with text tokens. Subsequent models like VILA-v1.5 (Lin et al., 2024) and LLaVA-Next (Liu et al., 2024a) built on this architecture, with LLaVA-Next introducing dynamic highresolution techniques for improved performance. Other adCoMemo: LVLMs Need Image Context with Image Memory Table 7: Ablation Study. The test settings are described in Section 4.7. NC means to use RoPE-DHR without compression. Name Variant 1 Variant 2 Variant 3 Variant 4 Variant 5 Variant 6 Variant 7 Variant 8 Variant 9 Params. 2B 2B 2B 2B 2B 8B 8B 2B 2B Memory RoPE-DHR NC Datasets Ours Ours Ours Ours Ours Ours Ours 1.2M 1.2M Overall 55.4 56.7 59.0 59.5 60.5 65.4 68.0 51.1 54.6 Caption 68.1 67.2 82.8 79.7 85.3 77.7 92.1 79.7 89.5 Long-Generation 45.8 51.4 49.6 51.9 52.8 61.0 61.4 36.1 38.4 Multi-Image 45.5 47.5 46.0 48.5 48.5 57.4 57.7 44.1 46.2 Long-Context 39.5 42.0 43.0 43.1 44.4 45.3 50.8 28.7 31.5 Math 32.2 31.7 32.2 34.7 33.5 38.7 38.9 28.8 28. General VQA 65.9 69.2 66.7 67.3 68.4 78.3 78.1 58.5 62.3 OCR-Related 74.7 72.9 75.6 74.8 73.4 82.3 79.1 61.8 63.5 vancements include Qwen2-VL (Wang et al., 2024b), which introduced M-RoPE, and InternVL-2.5 (Chen et al., 2024c), which used pixel shuffle to reduce token count after DHR. DeepSeek-VL2 (Wu et al., 2024) employed pretrained MoE-based LLM. However, this alignment approach inherits the LLMs generation mechanism, which can lead to issues such as image neglect or the lost in the middle problem. Cross-Attention-Based LVLMs: Flamingo (Alayrac et al., 2022) is an early example of LVLMs using cross-attention mechanisms. Later models, like Idefics (Laurencon et al., 2024) and LLaMa-3.2-Vision (Dubey et al., 2024), adopted its mixin layer design, which introduces cross-attention and gating mechanisms. EVLM (Chen et al., 2024b) experimented with using intermediate Vision Transformer representations as inputs to the mixin layer. mPlug-owl3 (Ye et al., 2024) added adaptive gating and hyper-layer fusion to combine cross-attention and self-attention. This approach enables visual understanding while maintaining the LLMs frozen language ability, as seen in LLaMa-3.2-Vision. However, in these models, image representations are aligned directly to the LLMs hidden state, whereas LLaVA-like methods align them with the text token space, better leveraging autoregressive decoding capabilities and improving performance. 5.2. Position Encoding Schemes in LVLMs Most LVLMs use position encoding methods inherited from LLMs, primarily RoPE. In these models, each image patch token is treated like text token and assigned position IDs for RoPE computation. However, several advancements address specific challenges. MiniCPM-V (Yao et al., 2024) introduced an absolute position encoding for each image tile in the context of DHR, while LLaMA-3.2-V (Yao et al., 2024) designed encodings for both image tiles and patch tokens. NVLM (Yao et al., 2024), also leveraging DHR, added special tokens before each tile to convey positional information. While effective for predefined DHR ratios, these methods lack scalability. In contrast, Qwen-VL2 (Wang et al., 2024b) introduced M-RoPE, multi-dimensional position encoding extending RoPE to three channels (temporal, height, width) for images and videos. However, this position encoding requires customized ViT and thus cannot be applied to LVLMs employing DHR. Our proposed RoPE-DHR, based on 1D principles, offers 2D-aware encoding scheme that addresses these challenges without the extra computational burden. 6. Conclusion We present CoMemo, novel architecture for Large VisionLanguage Models specifically designed for long-form generation and extended context understanding. Our approach features dual-path image processing mechanism and introduces RoPE-DHR to alleviate remote decay in DHR scenarios while restoring critical two-dimensional spatial information. These innovations significantly enhance model performance across multiple tasks including image captioning, long-form generation, long-context understanding, multiimage analysis, and general visual question answering. We hope this work will contribute to the advancement of the vision-language modeling community."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Large Vision-Language Model. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgments",
            "content": "The work is supported by the National Key R&D Program of China (NO. 2022ZD0161301), by the National Natural Science Foundation of China (U24A20325, 62321005, 62376134)."
        },
        {
            "title": "References",
            "content": "Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 89488957, 2019. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, 9 CoMemo: LVLMs Need Image Context with Image Memory M., et al. Flamingo: visual language model for fewshot learning. Advances in neural information processing systems, 35:2371623736, 2022. Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi, Y., and Hajishirzi, H. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. Ben Abacha, A., Hasan, S. A., Datla, V. V., DemnerFushman, D., and Muller, H. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes. 9-12 September 2019, 2019. Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. Scene text visual question answering. pp. 42914301, 2019. Byeon, M., Park, B., Kim, H., Lee, S., Baek, Image-text pair https://github.com/kakaobrain/ Coyo-700m: W., and Kim, S. dataset. coyo-dataset, 2022. Cao, J. and Xiao, J. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In COLING, pp. 15111520, 2022. Carter, J. Textocr-gpt4v. https://huggingface. co/datasets/jimmycarter/textocr-gpt4v, 2024. Chang, S., Palzer, D., Li, J., Fosler-Lussier, E., and Xiao, N. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. Chen, G. H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang, Z., Chen, Z., Li, J., Wan, X., and Wang, B. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a. Chen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., and Liang, X. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Chen, K., Shen, D., Zhong, H., Zhong, H., Xia, K., Xu, D., Yuan, W., Hu, Y., Wen, B., Zhang, T., et al. Evlm: An efficient vision-language model for visual understanding. arXiv preprint arXiv:2407.14177, 2024b. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models 10 with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Chi, Z., Huang, H., Xu, H.-D., Yu, H., Yin, W., and Mao, X.-L. Complicated table structure recognition. arXiv preprint arXiv:1908.04729, 2019. Chng, C. K., Liu, Y., Sun, Y., Ng, C. C., Luo, C., Ni, Z., Fang, C., Zhang, S., Han, J., Ding, E., et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In ICDAR, pp. 15711576. IEEE, 2019. Clark, C. and Gardner, M. Simple and effective multiarXiv preprint paragraph reading comprehension. arXiv:1710.10723, 2017. Dai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J., Rintamaki, T., Shoeybi, M., Catanzaro, B., and Ping, W. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. Davis, B., Morse, B., Cohen, S., Price, B., and Tensmeyer, C. Deep visual template-free form parsing. In ICDAR, pp. 134141. IEEE, 2019. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Fu, X., Hu, Y., Li, B., Feng, Y., Wang, H., Lin, X., Roth, D., Smith, N. A., Ma, W.-C., and Krishna, R. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2025. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. pp. 69046913, 2017. Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. 35:2641826431, 2022. Guo, H., Qin, X., Liu, J., Han, J., Liu, J., and Ding, E. Eaten: Entity-aware attention for single shot visual text extraction. In ICDAR, pp. 254259. IEEE, 2019. He, M., Liu, Y., Yang, Z., Zhang, S., Luo, C., Gao, F., Zheng, Q., Wang, Y., Zhang, X., and Jin, L. Icpr2018 contest on robust reading for multi-type web images. pp. 712. IEEE, 2018. CoMemo: LVLMs Need Image Context with Image Memory He, X., Zhang, Y., Mou, L., Xing, E., and Xie, P. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Referitgame: Referring to objects in photographs of natural scenes. pp. 787798, 2014. Hiippala, T., Alikhani, M., Haverinen, J., Kalliokoski, T., Logacheva, E., Orekhova, S., Tuomainen, A., Stone, M., and Bateman, J. A. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. Hosu, V., Lin, H., Sziranyi, T., and Saupe, D. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. 29:40414056, 2020. Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. Hu, X., Gu, L., An, Q., Zhang, M., Liu, L., Kobayashi, K., Harada, T., Summers, R., and Zhu, Y. Medical-diff-vqa: large-scale medical dataset for difference visual question answering on chest x-ray images, 2023. Huang, Q., Xiong, Y., Rao, A., Wang, J., and Lin, D. Movienet: holistic dataset for movie understanding. pp. 709727. Springer, 2020. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., and Jawahar, C. Icdar2019 competition on scanned receipt ocr and information extraction. In ICDAR, pp. 15161520. IEEE, 2019. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. pp. 67006709, 2019. Jiang, D., He, X., Zeng, H., Wei, C., Ku, M., Liu, Q., and Chen, W. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. pp. 29012910, 2017. Kafle, K., Price, B., Cohen, S., and Kanan, C. Dvqa: Understanding data visualizations via question answering. pp. 56485656, 2018. Kahou, S. E., Michalski, V., Atkinson, A., Kadar, A., Trischler, A., and Bengio, Y. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. Kantharaj, S., Leong, R. T. K., Lin, X., Masry, A., Thakkar, M., Hoque, E., and Joty, S. Chart-to-text: largescale benchmark for chart summarization. arXiv preprint arXiv:2203.06486, 2022. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. pp. 235251, 2016. Kembhavi, A., Seo, M., Schwenk, D., Choi, J., Farhadi, A., and Hajishirzi, H. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. pp. 49995007, 2017. Kuang, J., Hua, W., Liang, D., Yang, M., Jiang, D., Ren, B., and Bai, X. Visual information extraction in the wild: practical dataset and end-to-end solution. In ICDAR, pp. 3653. Springer, 2023. LAION. Laion-gpt4v dataset. https://huggingface. co/datasets/laion/gpt4v-dataset, 2023. Lau, J. J., Gayen, S., Ben Abacha, A., and DemnerFushman, D. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. Laurencon, H., Tronchon, L., Cord, M., and Sanh, V. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. Lerner, P., Ferret, O., Guinaudeau, C., Le Borgne, H., Besancon, R., Moreno, J. G., and Lovon Melgarejo, J. Viquae, dataset for knowledge-based visual question answering about named entities. In SIGIR, pp. 31083120, 2022. Li, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou, C., Liu, W., Yang, Y., Xiong, X., et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. arXiv preprint arXiv:2408.07246, 2024. Li, Z., Wang, X., Stengel-Eskin, E., Kortylewski, A., Ma, W., Van Durme, B., and Yuille, A. L. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. pp. 1496314973, 2023. Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and Han, S. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26689 26699, 2024. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014a. 11 CoMemo: LVLMs Need Image Context with Image Memory Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. pp. 740755. Springer, 2014b. Liu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y., and Wu, X.- M. Slake: semantically-labeled knowledge-enhanced dataset for medical visual question answering. In ISBI, pp. 16501654. IEEE, 2021. Liu, C.-L., Yin, F., Wang, D.-H., and Wang, Q.-F. Casia online and offline chinese handwriting databases. In ICDAR, pp. 3741. IEEE, 2011. Liu, F., Emerson, G., and Collier, N. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023a. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. Mitigating hallucination in large multi-modal models via robust instruction tuning. 2023b. Liu, F., Wang, X., Yao, W., Chen, J., Song, K., Cho, S., Yacoob, Y., and Yu, D. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774, 2023c. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024c. Liu, S., Zheng, K., and Chen, W. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In European Conference on Computer Vision, pp. 125140. Springer, 2025a. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2025b. Liu, Z., Chu, T., Zang, Y., Wei, X., Dong, X., Zhang, P., Liang, Z., Xiong, Y., Qiao, Y., Lin, D., et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833, 2024d. 12 Liu, Z., Chu, T., Zang, Y., Wei, X., Dong, X., Zhang, P., Liang, Z., Xiong, Y., Qiao, Y., Lin, D., et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833, 2024e. Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. 35:25072521, 2022a. Lu, P., Qiu, L., Chang, K.-W., Wu, Y. N., Zhu, S.-C., Rajpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022b. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Mao, H., Cheung, M., and She, J. Deepart: Learning joint representations of visual arts. pp. 11831191, 2017. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Okvqa: visual question answering benchmark requiring external knowledge. pp. 31953204, 2019. Marti, U.-V. and Bunke, H. The iam-database: an english sentence database for offline handwriting recognition. International journal on document analysis and recognition, 5:3946, 2002. Masry, A., Do, X. L., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about In ACL, pp. charts with visual and logical reasoning. 22632279, 2022a. Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022b. Masry, A., Kavehzadeh, P., Do, X. L., Hoque, E., and Joty, S. Unichart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., and Jawahar, C. Infographicvqa. pp. 16971706, 2022. CoMemo: LVLMs Need Image Context with Image Memory Methani, N., Ganguly, P., Khapra, M. M., and Kumar, P. Plotqa: Reasoning over scientific plots. pp. 15271536, 2020. OpenGVLab. Jan 2024. OpenGVLab/InternVL-Chat-V1-2. Opengvlab/internvl-chat-v1-2-sft-data, URL https://huggingface.co/ Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 2641 2649, 2015. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. 35:2527825294, 2022a. Schuhmann, C., Kopf, A., Vencu, R., Coombes, T., and Beaumont, R. Laion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/ laion-coco/, 2022b. Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., and Malcolm, C. Solving geometry problems: Combining text and diagram interpretation. pp. 14661476, 2015. Shah, S., Mishra, A., Yadati, N., and Talukdar, P. P. Kvqa: Knowledge-aware visual question answering. volume 33, pp. 88768884, 2019. Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., and Sun, J. Objects365: large-scale, high-quality dataset for object detection. pp. 84308439, 2019. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. pp. 83178326, 2019a. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019b. Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., and Hassner, T. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. pp. 88028812, 2021. Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., and Wang, B. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. Su, J. Transformer roadmap: 2. rotary position embedding, Mar 2021. URL https://spaces.ac.cn/ archives/8265. Sun, Y., Ni, Z., Chng, C.-K., Liu, Y., Luo, C., Ng, C. C., Han, J., Ding, E., Liu, J., Karatzas, D., et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In ICDAR, pp. 15571562. IEEE, 2019. Team, O. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. https://internvl.github.io/blog/ URL 2024-07-02-InternVL-2.0/. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Veit, A., Matera, T., Neumann, L., Matas, J., and Belongie, S. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., and Lin, D. V3det: Vast vocabulary visual detection dataset. pp. 1984419854, 2023a. Wang, K., Pan, J., Shi, W., Lu, Z., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with mathvision dataset. arXiv preprint arXiv:2402.14804, 2024a. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing, L., Chen, Z., Li, H., Zhu, X., Cao, Z., et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907, 2023b. Wang, W., Zhang, S., Ren, Y., Duan, Y., Li, T., Liu, S., Hu, M., Chen, Z., Zhang, K., Lu, L., et al. Needle in 13 CoMemo: LVLMs Need Image Context with Image Memory multimodal haystack. arXiv preprint arXiv:2406.07230, 2024c. for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023a. Wang, X., Liu, Y., Shen, C., Ng, C. C., Luo, C., Jin, L., Chan, C. S., Hengel, A. v. d., and Wang, L. On the general value of evidence, and bilingual scene-text visual question answering. pp. 1012610135, 2020. Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023b. Zheng, X., Burdick, D., Popa, L., Zhong, X., and Wang, N. X. R. Global table extractor (gte): framework for joint table identification and cell structure recognition using visual context. pp. 697706, 2021. Zhong, Y., Liang, L., Zharkov, I., and Neumann, U. Mmvp: Motion-matrix-based video prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 42734283, 2023. Wu, C. Pmc-casereport. https://huggingface. co/datasets/chaoyi-wu/PMC-CaseReport, 2023. Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., et al. Deepseek-vl2: Mixtureof-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Ye, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. Ying, K., Meng, F., Wang, J., Li, Z., Lin, H., Yang, Y., Zhang, H., Zhang, W., Lin, Y., Liu, S., et al. Mmtbench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yuan, T., Zhu, Z., Xu, K., Li, C., Mu, T., and Hu, S. large chinese text dataset in the wild. 34(3):509521, 2019. Yuan, Y., Liu, X., Dikubab, W., Liu, H., Ji, Z., Wu, Z., and Bai, X. Syntax-aware network for handwritten mathematical expression recognition. arXiv preprint arXiv:2203.01601, 2022. Zhang, R., Zhou, Y., Jiang, Q., Song, Q., Li, N., Zhou, K., Wang, L., Wang, D., Liao, M., Yang, M., et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In ICDAR, pp. 15771581. IEEE, 2019. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., and Xie, W. Pmc-vqa: Visual instruction tuning CoMemo: LVLMs Need Image Context with Image Memory A. Theoretical Derivation of Remote Decay Definition A.1 (RoPE-induced Inner Product). Given query vector Cd and key vector Cd, their inner product under Rotary Position Embedding (RoPE) is defined as: (Rmq)(Rnk) Re d/21 (cid:88) q[2i:2i+1]k [2i:2i+1]ei(mn)θi where q[2i:2i+1] denotes the i-th 2D subvector of q, θi is the preset rotational frequency, and represents complex conjugation. i=0 Building on Definition A.1, we establish the following core lemma: Lemma A.2 (Abel Transform Representation). Let hi q[2i:2i+1]k conditions hd/2 = 0 and S0 = 0. Then: [2i:2i+1] and Sj (cid:80)j1 i=0 ei(mn)θi, with boundary d/21 (cid:88) i= hiei(mn)θi = d/21 (cid:88) i=0 hi(Si+1 Si) = d/21 (cid:88) i= Si+1(hi+1 hi) Proof. By Abel summation by parts: d/21 (cid:88) i=0 hi(Si+1 Si) = d/21 (cid:88) i=0 hiSi+1 d/21 (cid:88) i=0 hiSi = d/2 (cid:88) i=1 hi1Si d/21 (cid:88) i=0 hiSi (index shift) = = d/21 (cid:88) i=0 d/21 (cid:88) i=0 Si(hi hi1) + hd/21Sd/2 h1S0 Si+1(hi+1 hi) (using boundary conditions) Theorem A.3 (Decay Rate Bound). Under the assumptions of Lemma A.2, the absolute value of the inner product satisfies: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) d/21 (cid:88) i=0 hiei(mn)θi (cid:18) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) max 0id/21 (cid:19) d/21 (cid:88) hi+1 hi Si+1 i=0 Proof. From Lemma A.2 and the triangle inequality: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) d/21 (cid:88) i=0 hiei(mn)θi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) d/21 (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) d/21 (cid:88) i=0 Si+1(hi+1 hi) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Si+1 hi+1 hi i=0 (cid:16) max hi+1 hi (cid:17) d/21 (cid:88) i=0 Si+1 15 CoMemo: LVLMs Need Image Context with Image Memory B. Training Details The hyperparameters used for pretraining and finetuning across the three architectures are listed in Table 5. We observed that the 2B model, with its smaller hidden dimension, requires less extensive training, making Phase 2 optional. Therefore, in the ablation experiments in the main text, we only used Pretraining Phase 1 for the 2B model, while the 8B model utilized both pretraining phases. Table 5: Hyperparameters for Training and Inference Parameter Pretraining Phase 1 Pretraining Phase 2 Finetuning 8192 12 AdamW 1 104 0. 8192 Max sequence length 12 Max tile/image AdamW Optimizer 1 104 Learning rate 0.01 Weight decay β1, β2 = 0.9, 0.999 Optimizer momentum Learning rate schedule Constant with warmup Constant with warmup 0.03 Warmup ratio 2000 Training steps 1024 Batch size Number of mixin layers 4 Trainable weights 0.03 2000 1024 4 8192 12 AdamW 4 105 0.01 Cosine decay 0.03 9000 1024 4 All β1, β2 = 0.9, 0.999 β1, β2 = 0.9, 0.999 LVLM-S: MLP LVLM-X: Mixin layers + MLP CoMemo: Mixin layers + MLP (Freeze gate in Phase 2) C. Detailed Expereiment Results We provide the detailed ablation study results in Table 9. Table 9: Detailed results on albation study. Model Caption Long-Generation Multi-Image Long-Context Math General VQA OCR-Related c O s - . A L Variant 1 79.1 65.4 60.0 62.9 Variant 2 74.1 63.5 64.0 64.0 Variant 3 98.0 74.4 75.7 65.2 Variant 4 90.2 67.2 81.9 64.2 Variant 5 98.6 78.5 78.8 66.9 Variant 6 90.1 73.0 69.9 73.7 Variant 7 102.0 82.2 92.2 76.2 Variant 8 92.0 61.7 85.6 42 Variant 9 100.5 71.5 96.6 44. D. Dataset Details M 28.7 38.8 34.1 39.6 38.7 48.2 46.5 30.2 32.0 - M n A M B 38.2 48.3 50.2 27.0 42.7 48.8 51.1 30.2 37 51.6 49.4 31.9 42.7 52.1 50.6 30.6 43.5 50.6 51.3 34.2 49.2 65.0 57.9 29.8 49.7 65.4 58.0 38.2 40.5 44.2 47.8 15.6 40.6 48.8 49.2 17.5 e i 52.1 53.8 54.2 55.6 54.6 60.8 63.4 41.9 45.6 s t n M i a Q h 2 M M A x 48 16.5 73.1 1869 31.3 74.3 75.6 74.2 48.1 15.2 72.8 1899 40 73.4 72.5 72.9 50.1 14.3 73.1 1834 35.3 75.9 76.2 74.6 49.9 19.4 74.0 1826 36.7 74.2 76.1 74.2 50.0 17.0 74.2 1904 36 74.2 73.6 72.6 58.3 19.1 79.1 2210 45.3 83.5 84.6 78.7 59.7 18.1 79.8 2182 45.3 83.5 77.2 76.6 39.9 17.7 63.8 1674 28 63.2 63.8 58.6 44.2 13.2 67.6 1759 31.3 63.3 64.7 62.6 The data used in the pre-training stage are listed in Table 6. And datasets used for instruction tuning are listed in Table 7. 16 CoMemo: LVLMs Need Image Context with Image Memory Table 6: Summary of datasets used in the pretraining stage. task Short Caption OCR Detection Conversation Image-text instruction data dataset Laion (en&zh) (Schuhmann et al., 2022a), COYO (Byeon et al., 2022), COCO (Lin et al., 2014b) Wukong-OCR (Gu et al., 2022), LaionCOCO-OCR (Schuhmann et al., 2022b) GRIT (Peng et al., 2023), Objects365 (Shao et al., 2019) All-Seeing (en&zh) (Wang et al., 2023b) (see Table 7) Table 7: Summary of datasets used in the instruction tuning stage. task General QA Science Medical Chart Mathematics Knowledge OCR Document Grounding Conversation Detection dataset VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), OKVQA (Marino et al., 2019), VSR (Liu et al., 2023a) AI2D (Kembhavi et al., 2016), ScienceQA (Lu et al., 2022a), Chemistry Data (Li et al., 2024) TQA (Kembhavi et al., 2017) PMC-VQA (Zhang et al., 2023a), VQA-RAD (Lau et al., 2018), VQA-Med (Ben Abacha et al., 2019) Medical-Diff-VQA (Hu et al., 2023), PathVQA (He et al., 2020), SLAKE (Liu et al., 2021), PMC-CaseReport (Wu, 2023) ChartQA (Masry et al., 2022a), LRV-Instruction (Liu et al., 2023b), PlotQA (Methani et al., 2020) Unichart (Masry et al., 2023), MMC-Inst (Liu et al., 2023c), DVQA (Kafle et al., 2018) TableMWP (Lu et al., 2022b), FigureQA (Kahou et al., 2017), MapQA (Chang et al., 2022) SciTSR (Chi et al., 2019), Fintabnet (Zheng et al., 2021) CLEVR (Johnson et al., 2017), MetaMath (Yu et al., 2023), GeoQA+ (Cao & Xiao, 2022) Geometry3k (Lu et al., 2021), GeoS (Seo et al., 2015), Unigeo (Chen et al., 2022) Super-CLEVR (Li et al., 2023), MathQA (Amini et al., 2019) Art500k (Mao et al., 2017), MovieNet (Huang et al., 2020), KonIQ-10k (Hosu et al., 2020) KVQA (Shah et al., 2019), ViQuAE (Lerner et al., 2022) InfoVQA (Mathew et al., 2022), TextVQA (Singh et al., 2019a), ArT (Chng et al., 2019) CASIA (Liu et al., 2011), Chart-to-text (Kantharaj et al., 2022), COCO-text (Veit et al., 2016) CTW (Yuan et al., 2019), EATEN (Guo et al., 2019), ICDAR2019-LSVT (Sun et al., 2019) ICPR MTWI (He et al., 2018), NAF (Davis et al., 2019), ReCTS (Zhang et al., 2019) TextOCR (Singh et al., 2021), LLaVAR (Zhang et al., 2023b), HME-100k (Yuan et al., 2022) POIE (Kuang et al., 2023), SROIE (Huang et al., 2019), ST-VQA (Biten et al., 2019) EST-VQA (Wang et al., 2020), IAM (Marti & Bunke, 2002) DocVQA (Clark & Gardner, 2017), DocReason25k (Hu et al., 2024) RefCOCO (Kazemzadeh et al., 2014), RefCOCO+ (Kazemzadeh et al., 2014), RefCOCOg (Kazemzadeh et al., 2014) RD-BoxCoT (Chen et al., 2023) ALLaVA (Chen et al., 2024a), LAION-GPT4V (LAION, 2023) MMDU (Liu et al., 2024d), TextOCR-GPT4V (Carter, 2024) Objects365 (Shao et al., 2019), V3Det (Wang et al., 2023a)"
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}