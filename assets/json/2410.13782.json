{
    "paper_title": "DPLM-2: A Multimodal Diffusion Protein Language Model",
    "authors": [
        "Xinyou Wang",
        "Zaixiang Zheng",
        "Fei Ye",
        "Dongyu Xue",
        "Shujian Huang",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 2 8 7 3 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DPLM-2: MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL Xinyou Wang Zaixiang Zheng Fei Ye Dongyu Xue Shujian Huang Quanquan Gu Dept. of Computer Science, Nanjing University ByteDance Research wangxinyou@smail.nju.edu.cn, zhengzaixiang, quanquan.gu { Project Page: https://bytedance.github.io/dplm/dplm-2 @bytedance.com }"
        },
        {
            "title": "ABSTRACT",
            "content": "Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks."
        },
        {
            "title": "1\nProteins are macromolecules that execute crucial roles in every living organism. They are character-\nized by their amino acid sequences and three-dimensional structure, where the sequence determines\nthe structure, which in turn governs the protein’s function. Generative modeling for proteins has\nmade significant strides in recent years. Among them, diffusion models (Ho et al., 2020; Song et al.,\n2020) exhibit great success in protein structure-based generative modeling (Watson et al., 2023;\nYim et al., 2023). Meanwhile, large-scale protein language models (Rives et al., 2019; Lin et al.,\n2022), trained on evolutionary-scale sequence database, have become one of the most important\ncornerstones in sequence-based foundation models for protein sequence representation learning and\ngeneration. Remarkably, DPLM (Wang et al., 2024), a discrete diffusion (Austin et al., 2021) based\nprotein language models, has exhibited the state-of-the-art performance in both sequence generation\nand understanding, addressing a wide range of sequence-oriented applications.\nMany protein engineering applications, e.g., motif-scaffolding (Watson et al., 2023; Yim et al., 2024)\nand antibody design (Jin et al., 2021; Kong et al., 2022; Zhou et al., 2024), require jointly determine\nboth structure and sequence. However, the aforementioned approaches mostly employ generative\nmodels for one modality (either sequence or structure) and resort to separate models (Jumper et al.,\n2021; Dauparas et al., 2022) for the other. This highlights the pressing need for multimodal protein",
            "content": "This work was done during Xinyous internship at ByteDance Research. Project Lead. Corresponding Author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of GVP-based encoder to yield invariant backbone geometric features, lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structuresequence mixed-modal input and output. generative models that can integrate both sequence and structure, enabling more comprehensive understanding of protein behaviors and functions. This, therefore, raises the following question: Can we build multimodal protein foundation model to simultaneously model, understand, and generate both sequences and structures? To pursue this goal, Multiflow (Campbell et al., 2024) is recent effort for structure-sequence co-generation that incorporates sequences into structure-based generative models using multimodal flow matching. Despite its impressive structure generation capability, Multiflow exhibits suboptimal performance in co-generating structurally-compatible sequences and consequently resorts to instancelevel knowledge distillation from ProteinMPNN (Dauparas et al., 2022). Furthermore, it completely falls short in protein folding for given sequences, showing Mulitflows inadequacy in sequence understanding. We argue that this bottleneck arises from the absence (co-)evolutionary inductive bias derived from massive pre-training from sequence database, as prior studies have demonstrated that the evolutionarily-informed representations learned by pre-trained protein language models implicitly capture structural information enables direct structure prediction (Lin et al., 2022). As consequence, the limitation in sequence understanding and generation renders Multiflow inadequate as multimodal protein generative foundation. Inspired by the connection between evolutionary knowledge and spatial interactions, we deem that sequence-based generative language models like DPLM, with their strong sequence generation and predictive abilities, hold great promise as foundation for multimodal learning for proteins. Despite its exciting potential, this approach presents two key challenges: (1) language models cannot directly handle continuous data like structure; and (2) language models heavily necessitate sufficient scale of data and compute resources while structure data is much smaller compared to sequence databases."
        },
        {
            "title": "Preprint",
            "content": "In this paper, we address the aforementioned questions by introducing DPLM-2, multimodal protein foundation model that advances the state-of-the-art discrete diffusion-based protein language model (i.e., DPLM) to accommodate both sequences and structures. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We present several key recipes to facilitate multimodal learning in DPLM-2: (1) the core difficulty lies in enabling the language model to learn structural information, which is challenging and remains elusive, for which we develop lookup-free quantization (LFQ, Yu et al., 2023) structure tokenizer to convert 3D coordinates to discrete tokens and vice versa (Fig. 1A, 3.3); (2) we implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based DPLM (Fig. 1B, 3.2); and (3) we also address the exposure bias problem in discrete diffusion for sequence learning (Ranzato et al., 2016; Bengio et al., 2015) by self-mixup training strategy that leads to enhanced generation quality and diversity. We highlight our main contributions and findings as follows: (i) We present DPLM-2, multimodal protein generative language model that aims to simultaneously model, understand and generate protein structure and sequence. We show that it can be fairly efficient and effective to obtain mulitmodal protein model with moderate amount of high-quality data, decent structure tokenizer and publicly-accessible sequence-only pre-trained language models. (ii) As mulitmodal generative model, DPLM-2 enables unconditional co-generation of designable and diverse proteins that guarantees consistency between structure and sequence (Fig. 1C(1)). Our empirical evaluation shows that DPLM-2 attains competitive co-generation performance compared to structure-based generative approaches, while the proteins generated by DPLM-2 have better alignment with the characteristics of natural proteins in secondary structure statistics (4.1). (iii) In addition, DPLM-2 supports various conditional generation tasks by its multimodal nature, ranging from (sequence-conditioned) folding (Fig. 1C(3), 4.2), (structure-conditioned) inverse-folding (Fig. 1C(4), 4.3), to more successful motif-scaffolding given multimodal motif conditioning (Fig. 1C(5), 4.4). (iv) Last but not least, we demonstrate that the structure-aware protein representation learned by DPLM-2 brings additional benefit for range of protein predictive tasks (Fig. 1C(2), 4.5). Concurrent work. During the development of DPLM-2, we became aware of the recently proposed multimodal generative protein language model, ESM3 (Hayes et al., 2024), which also jointly models tokenized structure and sequence using generative masked language model. While both models aim for similar goals, DPLM-2 differs from ESM3 in several key aspects: (1) Multimodal protein generation: DPLM-2 treats structure and sequence modalities equally by design and emphasizes the simultaneous co-generation of compatible protein sequence and structure, whereas ESM3 is sequence-first model (other modalities are subject to dropout during training) and generates in cascaded modality-by-modality manner. (2) Data and compute efficiency: ESM3 seeks to perform mulimodal pre-training from scratch using huge amount of synthetic data, with modal size ranging from 1.4B to 98B. With strict license and absence of training infrastructure, this prohibits community from replicating for customized purposes. In contrast, DPLM-2 leverages much smaller datasets (PDB + SwissProt) and builds on open-source, pre-trained sequence-based DPLM (150M/650M/3B), which leverages DPLMs learned evolutionary knowledge and inherits strong sequence understanding and generation capabilities. We are also committed to open-source our models, training and inference code to democratize multimodal generative protein LMs to benefit the community. Overall, we believe DPLM-2 provides unique contributions to the community."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 GENERATIVE MODELING FOR PROTEIN The aim of generative protein modeling is to estimate the underlying distribution prot q(prot) of the protein data of our interest by learning probabilistic model pθ(prot). Here prot = (r1, r2, . . . , rL) denotes protein with residues, where each residue ri = (si, xi) is represented by two major modalities, i.e., si { is cat- } 1, ..., 20 = , and egorical variable for its amino acid type in } { 3 is the real-value Cartesian coordinates of its residue xi RNatoms 0, 1 Table 1: Generative tasks w.r.t. structure & sequence. task folding inv-folding seq. gen. struct. gen. objective pθ(xs) pθ(sx) pθ(s) pθ(x) seq-struct co-gen. pθ(s, x)"
        },
        {
            "title": "Preprint",
            "content": "atoms (we only consider backbone atoms herein, i.e., [N, Cα, C, O] with Natoms = 4). Namely, pθ(prot) = pθ(s1, s2, . . . , sL, x1, x2, . . . , xL) = pθ(s, x) As result, most of protein tasks can be viewed as specifying their input conditioning and output between these two modalities (Tab. 1), including (1) sequence-conditioned structure prediction (folding, Jumper et al., 2021; Lin et al., 2022; Huguet et al., 2024), (2) structure-conditioned sequence generation (inverse folding or fixed-backbone design, Dauparas et al., 2022; Hsu et al., 2022; Zheng et al., 2023b), (3) sequence learning or generation (Rives et al., 2019; Nijkamp et al., 2022; Alamdari et al., 2023; Wang et al., 2024), (4) structure generation (Yim et al., 2023; Watson et al., 2023; Ingraham et al., 2023), and (5) sequence-structure co-generation (Jin et al., 2021; Shi et al., 2022; Campbell et al., 2024). These further enable various conditional applications by allowing single or mixed-modal conditioning for partial generation, e.g., motif-scaffolding and antibody design."
        },
        {
            "title": "2.2 DIFFUSION PROTEIN LANGUAGE MODEL (DPLM)",
            "content": "Language models (LMs), typically parameterized by Transformers (Vaswani et al., 2017) have become the de facto choice dominating different domains with scalable and performing expressiveness (OpenAI, 2023). Among them, protein LMs have been serving as one of the AI foundation for protein sequence learning (Rives et al., 2019; Lin et al., 2022) and generation (Nijkamp et al., 2022; Alamdari et al., 2023). Diffusion protein language model (DPLM, Wang et al., 2024), in particular, shows excelling performance in both generation and representation learning of protein sequences. DPLM is grounded in absorbing discrete diffusion framework (Austin et al., 2021; Zheng et al., 2023a), which is characterized by forward and backward Markov process. Let Cat(x; p) be categorical distribution on protein sequence parameterized by vector on ( 1)-dimensional probability simplex. The forward process of discrete diffusion defines Markov process governed by the transition kernel q(x(t) q(x(0)) x(t); βtx(t 1)) = Cat βt)qnoise into stationary distribution x(T ) qnoise. For absorbing diffusion, qnoise is the point mass with all of the probability on the absorbing (mask) state. The learned backward process pθ(x(t x(t)) reversely denoises the x(T ) towards the data distribution x(0), which is typically optimized by the variational bound of the log-likelihood (Ho et al., 2020): pθ(x(0:T )) that gradually perturb the data x(0) 1) + (1 x(t 1) (cid:0) (cid:1) Eq(x(0)) log pθ(x(0)) Eq(x(0:T )) log = Eq(x(0)) (cid:2) (cid:104) x(1)) + log pθ(x(0) (cid:3) (cid:20) t=2 q(x(1:T ) KL q(x(t x(0)) 1) (cid:21) x(t), x(0)) (cid:80) (cid:2) pθ(x(t 1) x(t)) +const., (cid:3)(cid:105) where Jt is the learning objective. The learning objective of discrete diffusion can be further simplified into reweighted cross-entropies (Zheng et al., 2023a), resembling masked language modeling at arbitrary noise levels: (cid:124) (cid:125) Jt = Eq(x(0)) = Eq(x(0)) λ(t) KL q(x(t (cid:2) 1 (cid:104) pθ(x(t 1) 1) x(t), x(0)) log pθ(x(0) Lbi(t) x(t)) (cid:105) x(t)) , (cid:3) (1) where λ(t) is weighting coefficient induced from the specific noising schedule. For inference, DPLM is able to generate amino acid sequences by the reverse iterative denoising process of discrete diffusion (Hoogeboom et al., 2021; Austin et al., 2021) from the following distribution, (cid:80) 1) pθ(x(t x(t)) = x(t), x(0)pθ(x(0) 1) is sampled x(t)), then less noisy x(t Specifically, at time t, it first generates x(0) from pθ( x(t), x(0) = x(0)). Within absorbing diffusion, the generation process can be viewed as an by q( iterative mask-predict approach. For sequence representation for predictive tasks, it can be obtained by simply letting DPLM take the sequence as input. x(0) q(x(t x(t)). (cid:80) 1)"
        },
        {
            "title": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL",
            "content": "3.1 OVERVIEW Fig. 1 illustrates DPLM-2s overall architecture. DPLM-2 is built on the state-of-the-art sequencebased generative protein LM, i.e., DPLM (Wang et al., 2024), using discrete diffusion probabilistic 4 (cid:123)(cid:122)"
        },
        {
            "title": "Preprint",
            "content": "RL framework to concurrently model both protein sequences and their corresponding structures. To facilitate structure learning in language models, we introduce token-based representation for protein 3, the 3D coordinates of the protein backbone Nbackb structure via tokenizer that converts into discrete structure token sequence, denoted as = (z1, z2, . . . , zL) Z, where each token zi represents local structural element of the i-th residue. Given tokenized structure, DPLM-2 processes mulitmodal input by concatenating the structure token sequence with the corresponding amino acid sequence for the same protein. Notably, there exists position-byposition correspondence between and s, where zi and si refer to the two modalities of the i-th residue, respectively. To reinforce this correspondence, we assign identical position encodings to both zi and si, thereby ensuring that structural and sequence information is aligned at the residue level. To train DPLM-2, we leverage high-quality dataset comprising 20K clustered experimental structures from the Protein Data Bank (PDB) (Berman et al., 2000) and 200K predicted structures from the AFDB SwissProt split (Varadi et al., 2022), with length < 512. During training, DPLM-2 is tasked with denoising the input sequence across spectrum of noise levels, ranging from fully noisy to completely clean. The multimodal training objective of DPLM-2 is derived from Eq. (1) as, 0, 1 } { , (cid:105) Jt = Eq(x(0),s(0)),z(0) tokenize(x(0)) λ(t) 1 Lbi(t) log pθ(z(0) , s(0) z(t), s(t)) (cid:104) (cid:80) ) + log pθ(si 1) 1), s(t ) = log pθ(zi ) by assuming conditional independence. By learnwhere log pθ(zi, si ing pθ(z(t z(t), s(t)), the model enables the simultaneous generation of highly correlated protein structures and sequences. This eliminates the need for cascaded generation paradigm, allowing us to derive both the proteins structure and sequence in single step. To further enhance DPLM-2s ability to differentiate between structure and sequence, noising level for each modality is subjected to distinct scheduler, denoted as tz and ts, respectively. This facilitates more comprehensive understanding of the relationships between protein sequences and their corresponding structures. This design also allows us to explore arbitrary combinations of (tz, ts), thus providing flexible sampling options, including sampling from the marginals of each modality and conditionals between them for various applications (Fig. 1C). Furthermore, we also identify the exposure bias issue in discrete diffusion for sequence learning (Ranzato et al., 2016; Bengio et al., 2015), and mitigate this by proposing self-mixup strategy inspired by scheduled sampling, which improves both generation quality and diversity (see A.1). 3.2 EFFICIENT WARM-UP FROM PRE-TRAINED SEQUENCE-BASED DPLM Protein sequences encode critical evolutionary information, reflecting co-evolutionary processes where residue pairs mutate together and often interact in 3D space, offering insights for predicting protein folding (Melnyk et al., 2022b). Lin et al. (2022) further showed that protein language models trained on large-scale evolutionary data implicitly capture this information, which can facilitate structure prediction. Motivated by the link between evolutionary knowledge and structural interactions, we propose to built DPLM-2 with an efficient warmup from pre-trained sequence-based DPLM, to make the most of established evolutionary information for protein structure modeling, Since our structure dataset is significantly smaller than UniRef50 sequence database (200K vs. 45M), enabling efficient fine-tuning of the pre-trained model. we want to keep the sequence knowledge intact and reduce the risk of catastrophic forgetting, we apply LoRA (Hu et al., 2021) to limit too much deviation to the original parameters. This approach not only lowers training costs compared to starting from scratch but also effectively transfers valuable evolutionary information. 3.3 LEARNING STRUCTURE TOKENIZATION The core difficulty of achieving mulimodal protein LM lies in enabling the language model to learn structural information, which is challenging and remains elusive, Tokenizing continuous data modalities into discrete representations (Van Den Oord et al., 2017) has gained attraction across domains like image synthesis due to its ability to capture compact, meaningful information, enabling effective compression and efficient generation, especially with sequence-based models like Transformers. Recent efforts have applied this approach to protein structure coordinates (Van Kempen et al., 2024; Liu et al., 2023; Gao et al., 2024; Lu et al., 2024). This allows language models to better learn the composition of local structural elements. However, how to learn an effective structure tokenizer remains an active research question."
        },
        {
            "title": "Preprint",
            "content": "Structure tokenization under typical VQVAE (Van Den Oord et al., 2017) framework can be summarized as follows: x, Z} Nbackb 0, 1, . . . , decoder encoder given finite-size codebook Figure 2: Reconstruction and secondary structure correspondence of structure tokenizers. quantizer where (1) structure encoder encodes backbone 3 into invariant RL 3D coordinates dquant , (2) quantizer converts RL features into of discrete tokens where zi { ; and (3) structure decoder reconstructs 3D coordinates from the discrete tokens. We utilize GVPbased (Jing et al., 2020) structure encoder from pre-trained GVP-Transformer (Hsu et al., 2022) and IPA-based (Jumper et al., 2021) structure decoder. In terms of quantizer, our preliminary experiment showed that conventional VQ-VAE pretty much struggles in training. To mitigate this, we instead adopts Lookup-Free Quantizer (LFQ) from the currently best visual tokenizer (Yu et al., 2023) to protein structure tokenization. Specifically, the latent space of LFQ is decomposed as the Cartesian product of single-dimensional binary variables, as = . Given Ck, where log2 Z, each dimension (indexed by k) of the quantized RL the encoded feature = encoder(x) representation quant(ei) is obtained from: quant(ei)[k] = As such, with LFQ, the token indices for = 1 0 ei[k] > 0 } { { z1, z2, ..., zi, ..., zL} is given by: { log2 2k zi k=1 The LFQ-based structure tokenizer is trained on the same structure dataset as mentioned before, using combination of reconstruction, commitment, and entropy regularization losses, similar to standard VQ-VAE. Here FAPE loss (Jumper et al., 2021) is used as the primary reconstruction loss. Evaluation. As shown in Fig. 2A, LFQ significantly outperforms VQ-VAE regarding reconstruction accuracy while training of LFQ is much faster than VQ-VAE (2 vs. 15 days on 8 A100s). Increasing codebook size leads to improved reconstruction while codebook size of 8192 achieves the best compression-reconstruction trade-off. Meanwhile in Fig. 2B, we observe strong correlation between structure tokens and secondary structures. For instance, lot of structure tokens concentrated at the alpha helix and beta sheet vertices, while some tokens lie between regions. This suggests that structure tokens the fine-grained structural elements in backbone local environment. Ci,k = sign(ei[k]) = zi = index(quant(ei)) = ei[k] > 0 } { log2 k=1 Ck = 1, 1 } zi[k] + 1 . } { 11 (cid:80) z. ,"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate DPLM-2 on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation, 4.1), and variety of conditional tasks, such as folding (4.2), inverse folding (4.3) and motif-scaffolding (4.4), and series of protein predictive tasks (4.5). 4.1 UNCONDITIONAL PROTEIN GENERATION The goal of unconditional protein generation is to produce both the 3D structure and amino acid sequence. Typically, this is done using cascaded approach: either generating the structure first and then use another model to predict the sequence, or vice versa. Here, we focus on generating structure and sequence simultaneously. We evaluate DPLM-2 on both cascaded and simultaneous generation across three tasks: unconditional structure generation, unconditional sequence generation, and structure-sequence co-generation. Following Multiflow (Campbell et al., 2024), we evaluate the generated proteins in terms of quality, novelty and diversity. Quality is measured through designability (structures ability to fold into valid sequence) and foldability (sequences ability to fold into reasonable structure). Designability is assessed by folding the generated sequence with ESMFold (Lin et al., 2022), then using sc-TMscore and sc-RMSD with the co-generated structure to evaluate similarity. Foldability is evaluated via ESMFold, with pLDDT > 70 considered plausible. Novelty is assessed by comparing generated structures to known ones in PDB using TMScore (pdb-TM), with lower values indicating greater novelty. Diversity is measured by calculating pairwise TMscore (inner-TM), where lower scores indicate more dissimilarity. The number of clusters identified by FoldSeek (van Kempen et al., 2023) also quantifies diversity, normalized by the total number of structures."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Evaluation of DPLM-2 on unconditional structure-sequence co-generation. Here for designability of co-generated proteins, we use ESMFold to obtain refolded structure of DPLM-2generated sequence and measure the structural similarity between DPLM-2-generated structure and the refolded structure, which aims to measure the compatibility of the co-generated structure and sequence pairs. 4.1.1 DPLM-2 ENABLES HIGH-QUALITY, DIVERSE AND NOVEL PROTEIN SEQUENCE AND STRUCTURE GENERATION Tab. 2 and Fig. 3 present the results of DPLM-2 for unconditional protein generation. We highlight our key findings in the following aspects: (1) DPLM-2 can generate diverse and highly-plausible protein with simultaneous structuresequence co-generation. We sampled 100 proteins for each length in 100, 200, 300, 400, and 500. Fig. 3A/B demonstrates that DPLM-2 can sample sequence and structures with high designability across various lengths, with most sc-TM values exceeding 0.9, with diverse structure clusters. Fig. 3D shows that the novelty of sampled proteins, measured by pdb-TM, generally increases with longer protein lengths. In addition, DPLM-2 can generate with both modalities simultaneously or modality-by-modality. As shown in Tab. 2, the co-generation performance exhibit highest scTM, suggesting that co-modeling indeed benefits protein generation. (2) DPLM-2 can attains competitive performance with strong baselines on co-generation, as well as backbone-only and sequence-only generation, respectively. As shown in Tab. 2, DPLM-2 achieves the strong sc-TM compared to strong baselines, approaching the quality of native structures from PDB. We notice that ESM3-Open (Hayes et al., 2024), which runs in sequence-then-structure order, fails short of unconditional generation. Compared to MultiFlow (Campbell et al., 2024), DPLM-2 achieves comparable co-generation quality. Notably, as also reported in Campbell et al. (2024), Multiflow falls short of sequence generation when directly trained from structures with native sequences, resulting in greatly degraded co-generation performance without data distillation from external inverse folding models (ProteinMPNN). For reference, we also provide the result of Multiflow retrained using our training data, where its co-generation performance remains unsatisfying and lags behind DPLM-2, which suggests that DPLM-2 has advantages of directly and effectively learning from complex structure-sequence joint distribution. Moreover, DPLM-2 can also only produce single modality if needed, where it matches the best competitive models in these settings respectively. These results demonstrate DPLM-2s effectiveness as mulitmodal generative model. (3) DPLM-2 generates longer proteins beyond training data. As DPLM-2 is trained with 512 length cutoff, we are curious about its length extrapolation, and evaluate sampled proteins at lengths of [600, 700, 800, 900, 1000]. As shown in Fig. 3F, notably, for proteins exceeding the maximum training length of 512, the pLDDT scores of sequences sampled by DPLM-2 are close to those of DPLM. This suggests that DPLM-2 largely retains its sequence generation capability inherited from sequence pre-training in DPLM, leading to its capability of length extrapolation."
        },
        {
            "title": "Preprint",
            "content": "(4) Case study. Fig. 3H shows some generated samples of DPLM-2 up to 700 residues, while in Fig. 3I we showcase that we can manipulate DPLM-2 to design symmetric oligomers by forcing to duplicate the predicted tokens with repetitive structure and sequence patterns. Table 2: Benchmarking comparison of unconditional protein generation, in terms of structuresequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively. Quality Novelty Diversity ) ) scRMSD ( pLDDT ( ) avg. pdb-TM ( ) MaxCluster ( ) avg. inner-TM ( ) scTM ( struct) Structure-sequence co-generation. 4.623 Native PDB protein 0.624 ESM3-Open (1.4B, seq 0.930 MultiFlow w/ distillation (official ckpt) *MultiFlow w/o distillation 0.750 *MultiFlow (retrained on our training data) 0.871 0.907 DPLM-2 (650M, seq 0.921 DPLM-2 (650M, struct 0.925 DPLM-2 (650M, co-generation) struct) seq) 5.688 0.904 0.232 24.180 0.098 3.208 0.163 9.306 0.934 6.580 6.337 0.117 4.969 0.098 3.899 0. 0.129 24.109 4.741 8.499 6.258 9.403 6.735 3.723 79.447 65.861 62.624 82.246 81.910 82.686 Unconditional backbone generation. (sequence predicted by ProteinMPNN) Native PDB struct. (seq. from PMPNN) FrameDiff FoldFlow RFDiffusion DPLM-2 (650M) Unconditional sequence generation. (structures predicted by ESMFold) EvoDiff DPLM (650M) DPLM-2 (650M) 0.000 0.864 0.000 3.919 0.000 7.965 0.000 1.969 4.451 0. 0.000 0.000 0.000 0.000 5.261 0.969 0.818 0.540 0.914 0.945 35.846 83.252 82.246 0.660 0.704 0.653 0.637 0.640 0.668 0.566 0.657 0.637 0.432 0.541 0. 0.000 0.000 0.410 0.468 0.195 0.195 0.204 0.000 0.000 0.000 0.195 0.106 0.187 0.199 0.594 0.679 0. 0.282 0.465 0.411 0.363 0.679 0.366 0.515 0.589 0.167 0.152 0.270 0.288 0.279 0.000 0.000 0.000 0.000 0.288 0.070 0.222 0.268 0.540 0. 0.651 0.575 0.545 0.782 0.252 0.762 0.598 0.575 0.990 0.735 0.700 Figure 4: Analysis regarding secondary structure of generated proteins. (A) Statistics of averaged proportions of secondary structures for proteins from different methods and PDB; (B) Secondary structure vs. designability; (C) Samples of Multiflow, PDB and DPLM-2, as well as their secondary structure distributions. 4.1.2 DPLM-2 GENERATES PROTEINS THAT RESEMBLES NATURAL PROTEINS To further analyze the properties of different model, we examine their secondary structure distribution against natural proteins from PDB. Proteins sampled by DPLM-2 have secondary structures most similar to natural proteins. As seen in Fig. 4A, structure-based models like RFDiffusion and MultiFlow generate proteins with"
        },
        {
            "title": "Preprint",
            "content": "more helices and fewer sheets and loops than natural proteins in PDB. Protein language models like ESM3 and DPLM-2 show no strong bias towards alpha helices, but ESM3 tends to generate more loops. Among the methods, DPLM-2 produces the most natural-like secondary structure proportions, closely matching PDB proteins. In Fig. 4C, proteins generated by MultiFlow contain many helices and become more globular as length increases, exhibiting idealized secondary structures. In contrast, proteins generated from DPLM-2 resembles natural ones have more balanced structures, with fewer helices and more beta sheets and loops. On the other hands, simplex plots in Fig. 4C shows that while MultiFlows proteins are clustered in helix-rich regions, DPLM-2s proteins span wider area similar to natural proteins, while it rarely samples proteins composed mostly of sheets and loops, which do occur in nature. Additionally, Fig. 4B shows that the loop ratio has significant impact on designability, where higher proportion of loops will increase scRMSD, as loops are highly flexible. Thus, proteins with long loops, which DPLM-2 often generates, tend to have relatively high scRMSD, aligning with the results in Tab. 2."
        },
        {
            "title": "4.1.3 ABLATION STUDY",
            "content": "In DPLM-2 training, we start with warmup from the sequence-based pre-trained DPLM to exploit established evolutionary information and augment the data with high-quality AlphaFold-predicted structures from SwissProt (around 200K) and clustered PDB structures. This section evaluates the effects of sequence pre-training and data augmentation on unconditional protein generation. Table 3: Ablation study on the sequence pre-training and training data augmentation. sequence pre-training synthetic structures length length 200 length 300 length 400 length 500 scTM clusters scTM clusters scTM clusters scTM clusters scTM clusters 0.9241 0.9610 0.8988 0.9348 20 26 27 35 0.8674 0.9349 0.9182 0.9428 34 47 15 0.7667 0.9169 0.9343 0.9232 33 38 13 48 0.5016 0.8643 0.8518 0.9260 25 35 21 40 0.4511 0.7673 0.8288 0.9012 25 52 31 Tab. 3 demonstrates that sequence pre-training and data augmentation can significantly improve the designability and diversity, especially in generating long proteins (length > 300). We hypothesize that the limited number of long proteins in PDB leads to insufficient training. In contrast, sequence pretraining, which includes evolutionary data, is essential and can be transferred to improve protein structure modeling and generation quality. Additionally, this evolutionary information boosts sampling diversity. While increasing the amount of training data improves designability, it is less effective in enhancing diversity compared to sequence pretraining. By combining both strategies, we achieve the best overall performance, which forms the core of our training strategy. 4.2 FORWARD FOLDING (SEQUENCE-CONDITIONED STRUCTURE PREDICTION) RMSD RMSD Models TMscore PDB date split CAMEO 2022 Table 4: Structure prediction performance comparison between DPLM-2 and different baseline approaches on CAMEO 2022 datasets. : PVQD results are quoted from Liu et al. (2023). The goal of folding is to predict the 3D structure for the given amino acid sequence (Jumper et al., 2021). As mulitmodal generative model, DPLM-2 spontaneously enables protein structure prediction task (see Fig. 1C-3) given sequence as conditioning. We assess DPLM-2 on CAMEO 2022 and PDB data split used by Multiflow (Campbell et al., 2024). We utilize RMSD and TMscore between predicted and ground truth structure for evaluation, while DPLM-2 adopts argmax decoding for 100 sampling iterations. Tab. 4 indicates that DPLM-2 can perform sufficiently good folding in zeroshot manner. Performance can be improved after further supervised fine-tuning (SFT) using folding objective (maxθ log pθ(z s)). Overall, DPLM-2 can outperform or on par with the strong baselines, while achieving close performance with ESMFold. Furthermore, We observe that DPLM-2 with larger model scales can attain better results than smaller ones. We suggest that DPLM-2 benefits from the evolutionary information inherited from DPLM pre-trained on the vast number of protein sequences, which can be transferred and leveraged into structure modeling. 3.99/2.03 4.08/1.95 17.84/17.96 0.50/0.46 15.64/16.08 0.53/0.49 0.87/0. 9.22/7.64 w/ folding SFT 7.66/4.37 7.37/4.89 w/ folding SFT 6.21/3.78 6.34/3.65 w/ folding SFT 5.71/3.23 8.35/5.60 6.00/3.41 5.67/3.33 3.40/1.78 4.54/2.54 3.15/1.69 0.76/0.82 0.83/0.88 0.83/0.88 0.89/0.94 0.86/0.92 0.90/0.95 0.75/0.81 0.80/0.86 0.79/0.86 0.84/0.89 0.83/0.89 0.85/0.90 ESMFold PVQD MultiFlow ESM3 0.85/0.93 0.81/0. 2.84/1.19 0.93/0.97 DPLM-2 (150M) DPLM-2 (650M) DPLM-2 (3B) TMscore 0.85/0.92 4.94/2.28 6.33/2."
        },
        {
            "title": "Preprint",
            "content": "4.3 INVERSE FOLDING (STRUCTURE-CONDITIONED SEQUENCE GENERATION) Table 5: Comparison on inverse folding task. The goal of inverse folding is to find an amino acid sequence that can fold to given backbone structure. For evaluation, we employ amino acid recovery (AAR) for sequence evaluation, and we also assess the structure by self-consistency TM-score (scTM) between the native structure and the ESMFold-predicted structure of the generated sequence. DPLM-2 can generate reasonable sequences that fold into the given structures. Tab. 5 presents that DPLM-2 can outperform or be on par with other cogeneration models (MultiFlow, ESM3). As the model size increases, the performance in terms of sequence recovery (AAR) and structural consistency (scTM) improves, revealing the same scaling law observed in the folding task. We suggest that multimodal training effectively aligns the structure and sequence into the same space, such that DPLM-2 can yield the corresponding sequence without additional training. DPLM-2 (150M) 45.22/46.12 0.87/0.93 48.83/47.96 0.89/0.95 DPLM-2 (650M) 49.01/50.10 0.88/0.93 54.80/53/07 0.91/0.96 52.36/53.72 0.89/0.95 61.67/57.91 0.92/0.96 DPLM-2 (3B) 32.28/33.58 0.87/0.94 37.74/37.59 0.94/0.96 47.06/46.24 0.90/0.95 49.50/49.42 0.94/0. MultiFlow ESM3 CAMEO 2022 PDB date split Models scTM scTM AAR AAR 4.4 SCAFFOLDING WITH MIXED-MODAL MOTIF CONDITIONING The objective of motif-scaffolding is to generate suitable scaffold to preserve the structure of the given motif and maintain its original function. We follow the experimental setting of Yim et al. (2024), with 24 motif-scaffolding problems and we sample 100 scaffolds for each motif, where we (1) first determine the length of scaffold, and then (2) keep the motif segment unchanged and sample the scaffold part conditioned on the motif. The scaffold length is sampled from range provided by Yim et al. (2024), and when there are multiple motifs, the order of motif segments is consistent with Yim et al. (2024). We provide the 3D structure and sequence of motif as input of DPLM-2. As multimodal model, we evaluate DPLM-2 using sequence-based, structure-based, and co-generation approaches. scaffold is considered successful if it satisfies both criteria (1) overall designablity, which is successful when pLDDT > 70 (for sequence-based models) or scTM > 0.8, and (2) motif-preseving, which is deemed successful when the predicted motif structure matches the native one with motif-RMSD <1 A. Fig. 5 reveals that DPLM-2 is capable of generate reasonable scaffolds for the given functional motifs. In sequencebased, structure-based and co-generation evaluation, DPLM-2 can outperform or be on par with the corresponding approaches in most cases, solving more motif problem and achieving higher average success rate. We compared to sequence-based method, DPLM-2 shows better performance since it allows structural input of motif, which is important for preserving motifs structure hence the functions. Remarkably, DPLM-2 attains comparable performance with RFDiffusion when only generating scaffold structure, while achieve better performance when simultaneously designing scaffold sequence and structure, outperforming ESM3. Despite not experimentally verified, these results suggest that with DPLM-2, mulitmodal conditioning and generation could lead to more successful conditional protein design. Figure 5: Evaluation of motifscaffolding w.r.t. success rate and num. of solved problems. Table 6: Performance on various protein predictive downstream tasks. quoted from Su et al. (2023). : benchmarked results are Models Thermostability HumanPPI Metal Ion Binding EC GO BP CC MF Spearmans ρ Acc (%) Acc (%) Fmax Fmax Fmax Fmax DeepLoc Subcellular Acc (%) Binary Acc (%) SaProt (650M) MIF-ST (Yang et al., 2022b) ESM2 (650M) DPLM (650M) DPLM-2 (650M) 0.724 0.694 0.691 0.695 0.714 86.41 75.54 84.78 86. 84.44 0.884 0.803 0.866 0.875 0.878 0.678 0.627 0.676 0. 0.680 0.356 0.239 0.344 0.357 0.414 0.248 0.402 0.409 0. 0.411 85.57 78.96 83.68 84.56 82.98 93.55 91.76 92.28 93. 93.64 75.75 75.08 71.88 75.15 74."
        },
        {
            "title": "4.5 EVALUATION OF PROTEIN REPRESENTATION LEARNING",
            "content": "Table 7: Performance without large-scale sequence pre-training. Directly access to structure information is supposed to benefit downstream protein predictive tasks. To inspect this, we evaluate DPLM-2 on variety of protein predictive tasks utilizing the dataset provided by SaProt (Su et al., 2023), where we provide tokenized protein structure tokens along with the protein sequences to DPLM-2. DPLM-2 can perform multimodal representation learning by leveraging both structure and sequence information. Tab. 6 presents that DPLM-2 shows further improvement compared to sequence-only methods (ESM2, DPLM) on some tasks, indicating that DPLM-2 can leverage protein structures to generate better representations containing multimodal information for downstream tasks. However, we find that DPLM2 falls behind the state-of-the-art structure-aware protein LM, i.e., SaProt, in most tasks and even lags behind DPLM in certain tasks. We hypothesize this is because the strutcure training data of DPLM-2, consisting of PDB and SwissProt, is smaller and differs from UniRef50, which DPLM is pretrained on, potentially causing catastrophic forgetting and suboptimal representation. To test this, we conducted an experiment on the DeepLoc subcellular task, where DPLM-2 underperforms compared to DPLM. As shown in Tab. 7, without large-scale sequence pretraining, DPLM-2 outperforms DPLM significantly, suggesting that: (1) Incorporating structure information enhances performance over sequence-only models. (2) Smaller datasets can lead to catastrophic forgetting, diminishing the benefits of large-scale pretraining. As result, to further improve the predictive performance, one deserving direction is to exploit larger-scale predicted structures in our future work. DeepLoc Subcellular Acc (%) DPLM-2 (650M) DPLM (650M) Models 63.49 66."
        },
        {
            "title": "5 DISCUSSIONS\nIn this paper, we introduce DPLM-2, a multimodal diffusion protein language model that understands,\ngenerates and reasons over protein structure and sequence, aiming to severe as a mulimodal foundation\nfor protein. Despite promising performance spanning protein co-generation, folding, inverse folding\nand conditional motif-scaffolding with mulimodal input and output, there remains several limitations\ndeserving to be addressed. (1) Structure data: Our findings indicate that while structure awareness\nmay help with predictive tasks, the limited structure data constrains DPLM-2’s ability to learn robust\nrepresentations. It is also important to account for longer protein chains and multimers in future\nstudies. (2) Trade-off of discrete latent representation: Tokenizing structure into discrete symbols\nfacilitates multimodal protein language models and co-generation but may come at the cost of losing\nfine-grained structural details and control, such as precise atomic positions and inter-atomic distances.\nFuture work should aim to also integrate the strengths of data-space structure-based generative models\ninto sequence-based mulitimodal language models to maximize the best of both worlds.",
            "content": "ACKNOWLEDGEMENT We would like to thank Dr. Hang Li for insightful discussions on the project and feedback on the manuscript that help shape this study. We thank Yi Zhou, Jing Yuan, Yilai Li, Yuning Shen, Wesley Hsieh and Daiheng Zhang for their valuable comments. REFERENCES Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin Yang. Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, pp. 202309, 2023. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, volume 34, pp. 1798117993, 2021. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Corinna Cortes, Neil D. (eds.), Advances Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 11711179, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ e995f98d56967d946471af29d7bf99f1-Abstract.html."
        },
        {
            "title": "Preprint",
            "content": "Helen Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady Bhat, Helge Weissig, Ilya Shindyalov, and Philip Bourne. The protein data bank. Nucleic acids research, 28(1): 235242, 2000. Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):21022110, 2022. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. Alexander Chu, Jinho Kim, Lucy Cheng, Gina El Nesr, Minkai Xu, Richard Shuai, and Po-Ssu Huang. An all-atom protein generative model. Proceedings of the National Academy of Sciences, 121(27):e2311500121, 2024. Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert Ragotte, Lukas Milles, Basile IM Wicky, Alexis Courbet, Rob de Haas, Neville Bethel, et al. Robust deep learningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):71127127, 2021. Noelia Ferruz, Steffen Schmidt, and Birte Hocker. Protgpt2 is deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022. Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, and Stan Li. Foldtoken: Learning protein language via vector quantization and beyond. arXiv preprint arXiv:2403.09673, 2024. Tomas Hayes, Roshan Rao, Halil Akin, Nicholas Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with language model. bioRxiv, pp. 202407, 2024. Liang He, Shizhuo Zhang, Lijun Wu, Huanhuan Xia, Fusong Ju, He Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, Pan Deng, et al. Pre-training co-evolutionary protein representation via pairwise masked language model. arXiv preprint arXiv:2110.15527, 2021. Jonathan Ho, Ajay Jain, models. 2020. 4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Advances URL and Pieter Abbeel. Denoising diffusion probabilistic in Neural 33:68406851, https://proceedings.neurips.cc/paper/2020/file/ Information Processing Systems, Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 89468970. PMLR, 1723 Jul 2022. URL https://proceedings. mlr.press/v162/hsu22a.html. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, et al. Sequence-augmented se (3)-flow matching for conditional protein backbone generation. arXiv preprint arXiv:2405.20313, 2024."
        },
        {
            "title": "Preprint",
            "content": "John Ingraham, Max Baranov, Zak Costello, Karl Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana Lord, Christopher Ng-Thow-Hing, Erik Van Vlack, et al. Illuminating protein space with programmable generative model. Nature, pp. 19, 2023. Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi Jaakkola. Iterative refinement graph neural network for antibody sequence-structure co-design. In International Conference on Learning Representations, 2021. Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2020. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. arXiv preprint arXiv:2208.06073, 2022. Jin Sub Lee, Jisun Kim, and Philip Kim. Proteinsgm: Score-based generative modeling for de novo protein design. bioRxiv, pp. 202207, 2022. Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022. Haiyan Liu, Yufeng Liu, and Linghui Chen. Diffusion in quantized vector space generates nonidealized protein structures and predicts conformational distributions. bioRxiv, pp. 202311, 2023. Amy Lu, Haoran Zhang, Marzyeh Ghassemi, and Alan Moses. Self-supervised contrastive learning of protein representations by mutual information maximization. BioRxiv, pp. 202009, 2020. Amy Lu, Wilson Yan, Kevin Yang, Vladimir Gligorijevic, Kyunghyun Cho, Pieter Abbeel, Richard Bonneau, and Nathan Frey. Tokenized and continuous embedding compressions of protein sequence and structure. bioRxiv, pp. 202408, 2024. Ali Madani, Ben Krause, Eric Greene, Subu Subramanian, Benjamin Mohr, James Holton, Jose Luis Olmos Jr, Caiming Xiong, Zachary Sun, Richard Socher, et al. Deep neural language modeling enables functional protein generation across families. bioRxiv, pp. 202107, 2021. Matthew McDermott, Brendan Yap, Harry Hsu, Di Jin, and Peter Szolovits. Adversarial contrastive pre-training for protein sequences. arXiv preprint arXiv:2102.00466, 2021. Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. In Advances in Neural Information Processing Systems, pp. 2928729303, 2021. Igor Melnyk, Vijil Chenthamarakshan, Pin-Yu Chen, Payel Das, Amit Dhurandhar, Inkit Padhi, and Devleena Das. Reprogramming large pretrained language models for antibody sequence infilling. arXiv preprint arXiv:2210.07144, 2022a. Igor Melnyk, Aurelie Lozano, Payel Das, and Vijil Chenthamarakshan. Alphafold distillation for improved inverse protein folding. arXiv preprint arXiv:2210.03488, 2022b."
        },
        {
            "title": "Preprint",
            "content": "Seonwoo Min, Seunghyun Park, Siwon Kim, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. Pre-training of deep bidirectional protein sequence representations with structural information. IEEE Access, 9:123912123926, 2021. Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, and Anna Ritz. Transforming the language of life: transformer neural networks for protein prediction tasks. In Proceedings of the 11th ACM international conference on bioinformatics, computational biology and health informatics, pp. 18, 2020. Erik Nijkamp, Jeffrey Ruffolo, Eli Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517, 2022. Esmaeil Nourani, Ehsaneddin Asgari, Alice McHardy, and Mohammad RK Mofrad. Tripletprot: deep representation learning of proteins based on siamese networks. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(6):37443753, 2021. OpenAI. Gpt-4 technical report, 2023. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06732. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS, 2019. doi: 10.1101/622803. URL https://www.biorxiv.org/content/10.1101/622803v4. Chence Shi, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, and Jian Tang. Protein sequence and structure co-design with equivariant translation. arXiv preprint arXiv:2210.08761, 2022. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek. Udsmprot: universal deep sequence models for protein classification. Bioinformatics, 36(8):24012409, 2020. Pascal Sturmfels, Jesse Vig, Ali Madani, and Nazneen Fatema Rajani. Profile prediction: An alignment-based pre-training task for protein sequence models. arXiv preprint arXiv:2012.00195, 2020. Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. Saprot: Protein language modeling with structure-aware vocabulary. bioRxiv, pp. 202310, 2023. Brian Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. arXiv preprint arXiv:2206.04119, 2022. Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, Aybar Acar, and Tunca Dogan. Learning functional properties of proteins with language models. Nature Machine Intelligence, 4 (3):227245, 2022. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Michel van Kempen, Stephanie Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron LM Gilchrist, Johannes Soding, and Martin Steinegger. Fast and accurate protein structure search with foldseek. Nature Biotechnology, pp. 14, 2023."
        },
        {
            "title": "Preprint",
            "content": "Michel Van Kempen, Stephanie Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron LM Gilchrist, Johannes Soding, and Martin Steinegger. Fast and accurate protein structure search with foldseek. Nature biotechnology, 42(2):243246, 2024. Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, et al. Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic acids research, 50(D1):D439D444, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, volume 30, pp. 59986008, 2017. Robert Verkuil, Ori Kabeli, Yilun Du, Basile IM Wicky, Lukas Milles, Justas Dauparas, David Baker, Sergey Ovchinnikov, Tom Sercu, and Alexander Rives. Language models generalize beyond natural proteins. bioRxiv, pp. 202212, 2022. Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567, 2024. Joseph Watson, David Juergens, Nathaniel Bennett, Brian Trippe, Jason Yim, Helen Eisenach, Woody Ahern, Andrew Borst, Robert Ragotte, Lukas Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023. Kevin Wu, Kevin Yang, Rianne van den Berg, James Zou, Alex Lu, and Ava Amini. Protein structure generation via folding diffusion. arXiv preprint arXiv:2209.15611, 2022a. Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction from primary sequence. BioRxiv, pp. 202207, 2022b. Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large-scale pretrain language model. arXiv preprint arXiv:2108.07435, 2021. Kevin Yang, Alex Lu, and Nicolo Fusi. Convolutions are competitive with transformers for protein sequence pretraining. bioRxiv, pp. 202205, 2022a. Kevin Yang, Niccol`o Zanichelli, and Hugh Yeh. Masked inverse folding with sequence transfer for protein representation learning. bioRxiv, pp. 202205, 2022b. Jason Yim, Brian Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023. Jason Yim, Andrew Campbell, Emile Mathieu, Andrew YK Foong, Michael Gastegger, Jose JimenezImproved Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan Veeling, Frank Noe, et al. motif-scaffolding with se (3) flow matching. arXiv preprint arXiv:2401.04082, 2024. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats In The Twelfth International Conference on diffusion-tokenizer is key to visual generation. Learning Representations, 2023. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023a. Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei YE, and Quanquan Gu. Structure-informed language models are protein designers. In International Conference on Machine Learning, 2023b. Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, and Quanquan Gu. Antigen-specific antibody design via direct energy-based preference optimization. Advances in neural information processing systems, 2024."
        },
        {
            "title": "Preprint",
            "content": "A DPLM-2 TRAINING A.1 TACKLING EXPOSURE BIAS IN DISCRETE DIFFUSION WITH SELF-MIXUP TRAINING"
        },
        {
            "title": "STRATEGY",
            "content": "We find that discrete diffusion training will face the exposure bias problem (Ranzato et al., 2016; Bengio et al., 2015), which means mismatch between training and inference. The model is trained to denoise given the ground-truth context during training. However, during inference, the model needs to denoise based on the predicted tokens, which may not be correct and inconsistent with the always-accurate context during training. This may lead to error accumulation and negatively impact the generation performance. To address this issue, we propose self-mixup training paradigm for discrete diffusion model, enhancing the consistency between training and inference. During training, we perform an additional forward pass, allowing the model to first make predictions and then denoise based on those predictions. Tab. 8 shows that the self-mixup training strategy effectively enhances the diversity of samples. We attribute this to the model producing more accurate logits during inference, leading to more diverse reasonable sampling paths instead of converging on the sampling paths with the highest probability, which results in more diverse proteins. Table 8: Ablation study on the self-mixup training strategy. Mixup strategy length 100 length length 300 length 400 length 500 scTM clusters scTM clusters scTM clusters scTM clusters scTM clusters 0.9237 0.8812 44 62 0.9180 0.8820 53 62 0.9147 0. 48 59 0.9059 0.9099 42 54 0.8896 0.8845 33 38 A.2 DATASET The training set of DPLM-2 is composed by experimental data, i.e., PDB (Berman et al., 2000), and high quality synthetic data, i.e., SwissProt (Varadi et al., 2022). We filter the SwissProt data by pLDDT > 85. After filtering, the overall training set contains approximately 200,000 proteins. We limit the maximum length of the training set to 512. For proteins longer than 512, we randomly crop it to 512. We crop the low pLDDT (pLDDT < 50) segments located at the both ends of proteins in the SwissProt dataset. These segments are typically non-structural and may negatively impact the training results. Moreover, we find that the length distribution of the training set is not balanced, where the number of proteins with length less than 100 is relatively small, leading to suboptimal diversity among the short proteins. Therefore, during training, we randomly crop long proteins to short proteins with probability of 50% for each batch to improve the diversity. A.3 HYPERPARAMETER We train all models using AdamW optimizer (Kingma & Ba, 2015) with β1 = 0.9 and β2 = 0.95. We use weight decay of 0.01 and gradient clipping of 0.5. We employ 2K warmup steps until reaching the maximum learning rate, and utilize linear decay scheduler to decay LR to 10% of the maximum learning rate by the end of training. The maximum learning rate is 1e-4, and the overall training step is 100,000. We utilize the pretrained DPLM as the parameter initialization, and the diffusion timestep is set to 500. We train 150M DPLM-2 with 8 A100 GPUs for 3 days, while 650M with 16 A100 GPUs for 3 days and 3B with 16 A100 GPUs for week."
        },
        {
            "title": "B STRUCTURE TOKENIZER",
            "content": "The core difficulty of achieving mulimodal protein LM lies in enabling the language model to learn structural information, which is challenging and remains elusive, Tokenizing continuous data modalities into discrete representations (Van Den Oord et al., 2017) has gained attraction across domains like image synthesis due to its ability to capture compact, meaningful information, enabling effective compression and efficient generation, especially with sequence-based models like Transformers. Recent efforts have applied this approach to protein structure coordinates (Van Kempen et al., 2024; Liu et al., 2023; Gao et al., 2024; Lu et al., 2024). B.1 DATASET Our structure tokenizers are trained using the same structure data as our mulitmodal language model, containing both experimental and high-quality structures, totaling 200K proteins."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: scaffolding. Sequence-based, structure-based and co-generation evaluation pipeline of motifB.2 MODEL ARCHITECTURE As shown in Fig. 1A, the structure tokenizer in this paper consists of structure encoder, quantizer, and structure decoder. The encoder is based on pre-trained GVP-Transformer (Hsu et al., 2022), with its parameters frozen during training. It transforms backbone structures into geometric features, which are projected onto latent embedding using an MLP layer. For the quantizer, we adopt lookup-free quantizer from state-of-the-art video tokenizer (Yu et al., 2023), where the latent dimension is set to log2 as the codebook size. The structure decoder follows the IPA-based modules from AlphaFold2 (Jumper et al., 2021), using 4 EvoFormer layers without MSA row attention, following ESMFold (Lin et al., 2022), to generate atomic positions from the structure tokens. , with B.3 TRAINING The structure tokenizer is trained using standard VQ-VAE framework, with the objective including reconstruction loss, codebook commitment loss, and entropy regularization loss to ensure effective codebook utilization. For the reconstruction loss, we adopt the FAPE loss, violation loss, and distogram loss from AlphaFold2, measuring the difference between predicted and native structures. To further enhance the training, we introduce sequence prediction head on top of the structure decoders final representation and minimize the cross-entropy against the native sequence."
        },
        {
            "title": "C MOTIF SCAFFOLDING",
            "content": "C.1 EVALUATION PIPELINE We evaluate DPLM-2 in sequence-based, structure-based and co-generation ways. The overall illustration is shown in Fig. 6. We focus on the two aspects: overall quality and motif part consistency. The assessment of overall quality varies across different approaches. Specifically, (1) For sequence-based method, we only take the generated sequence and utilize ESMFold to obtain the predicted structure, and the pLDDT score provided by ESMFold is used to assess overall quality. (2) For structure-based method, we only take the generated structure, and then leverage ProteinMPNN to predict the sequence, followed by ESMFold to predict the structure, where overall quality is assessed by scTM. (3) For co-generation method, we take both the generated structure and sequence, and predict structure given generated sequence with ESMFold, where scTM is calculated between generated structure and ESMFold predicted structure to evaluate overall quality. Considering that the ground truth motif structure is given, we only utilize the ESMFold predicted structure to calculate motif-RMSD. C.2 RESULT OF EACH PROBLEM Tab. 9 presents the result of each motif-scaffolding problem. DPLM-2 achieves the best average success rate in each evaluation. Compared with ESM3, DPLM-2 shows better results in 12 problems in co-generation evaluation and 10 problems in sequence-based evaluation. Meanwhile, DPLM-2 outperforms RFDiffusion in 14 problems in structure-based evaluation. This demonstrates that DPLM-2 can achieve strong performance under various evaluation methods."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Motif-scaffolding results of each problem. * means best result from 8 samples. sequence-based structure-based co-generation EvoDiff DPLM ESM3 DPLM2 *RFDiffusion *DPLM2 ESM3 DPLM2 *DPLM2 1BCF 1PRW 1QJG 1YCR 2KL8 3IXT 4JHW 4ZYP 5IUS 5TPN 5TRV long 5TRV med 5TRV short 5WN9 5YUI 6E6R long 6E6R med 6E6R short 6EXZ long 6EXZ med 6EXZ short 7MRX long 7MRX med 7MRX short pass rate avg. success rate 0.00 0.61 0.00 0.02 0.04 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.03 0.07 0.00 0.00 0.00 0.00 0.00 0.00 7/24 0. 0.00 0.83 0.00 0.38 0.08 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.94 0.87 0.01 0.00 0.00 0.02 0.31 0.34 0.89 0.96 0.02 0.41 0.11 0.18 0.00 0.03 0.00 0.03 0.19 0.16 0.01 0.02 0.00 0.07 0.24 0.09 0.32 0.31 0.31 0.36 0.65 0.68 0.01 0.86 0.03 0.77 0.47 0.67 0.00 0.16 0.00 0.00 0.00 0.03 0.07 0.00 0.00 0.91 0.93 0.86 0.61 0.66 0.66 0.23 0.28 0.26 1.00 0.08 0.00 0.74 0.88 0.25 0.00 0.40 0.02 0.61 0.37 0.24 0.04 0.00 0.02 0.86 0.89 0.39 0.76 0.49 0.39 0.09 0.11 0.02 0.07 0.96 0.00 0.93 0.94 0.77 0.00 0.51 0.00 0.06 0.08 0.07 0.10 0.20 0.00 0.92 0.88 0.78 0.63 0.63 0.41 0.32 0.31 0.41 0.23 0.54 0.03 0.18 0.11 0.02 0.00 0.08 0.00 0.01 0.19 0.16 0.01 0.00 0.00 0.04 0.14 0.06 0.13 0.31 0.28 0.37 0.59 0. 0.01 0.84 0.02 0.53 0.57 0.41 0.00 0.10 0.00 0.00 0.00 0.02 0.03 0.00 0.00 0.78 0.77 0.64 0.44 0.55 0.58 0.20 0.22 0.24 0.05 0.95 0.05 0.98 1.00 0.73 0.00 0.64 0.00 0.00 0.07 0.19 0.11 0.00 0.00 1.00 0.97 0.99 0.95 0.96 0.87 0.73 0.70 0.88 11/24 0.19 21/24 0.25 18/24 0.35 20/24 0. 20/24 0.42 20/24 0.18 18/24 0.29 19/24 0.53 We also find that taking the best result from 8 samples can bring significant improvement compared to 1 sample, especially in terms of success rate. In the co-generation evaluation, DPLM2 with sampling 8 times improves the success rate of most of the problems by large margin. We hypothesize that sampling eight times largely alleviates errors caused by randomness in the sampling process, thereby producing more suitable scaffold for the given motif."
        },
        {
            "title": "D RELATED WORK",
            "content": "D.1 PROTEIN LANGUAGE MODELS There is growing interest in developing protein LMs at the scale of evolution, such as the series of ESM (Rives et al., 2019; Lin et al., 2022), TAPE (Rao et al., 2019), ProtTrans (Elnaggar et al., 2021), PRoBERTa (Nambiar et al., 2020), PMLM (He et al., 2021), ProteinLM (Xiao et al., 2021), PLUS (Min et al., 2021), Adversarial Masked LMs (McDermott et al., 2021), ProteinBERT (Brandes et al., 2022), CARP (Yang et al., 2022a) in masked language modeling (MLM) paradigm, ProtGPT2 (Ferruz et al., 2022) in causal language modeling paradigm, and several others (Melnyk et al., 2022a; Madani et al., 2021; Unsal et al., 2022; Nourani et al., 2021; Lu et al., 2020; Sturmfels et al., 2020; Strodthoff et al., 2020). These protein language models exhibit remarkable generalization ability on various downstream tasks and be able to capture evolutionary information about secondary and tertiary structures from sequences alone. Meanwhile, recent study shows these models potency in revealing protein structures (Lin et al., 2022), predicting the effect of sequence variation on function (Meier et al., 2021), antibody infilling (Melnyk et al., 2022a) and many other general purposes (Rives et al., 2019). Simultaneously, Verkuil et al. (2022) demonstrate that the large scale protein LMs can generate de novo proteins by generalizing beyond natural proteins, both theoretically and experimentally validating their hypothesis in exhaustive detail, in which protein LMs demonstrate competency in designing protein structure despite being exclusively trained on sequences. D.2 PROTEIN STRUCTURE GENERATIVE MODELS Diffusion models have become popular tools in structural biology for protein generation, and their utility has been demonstrated across range of generative tasks in recent years. Trippe et al. (2022), along with others, have introduced several diffusion model variants, each with its unique approach. For instance, while some models focus on generating the protein backbone by diffusing over protein coordinates, others, such as those proposed by Wu et al. (2022b), target inter-residue angles. Lin & AlQuraishi (2023) and Yim et al. (2023) have developed models that handle both the position and orientation of residue frames. RFDiffusion (Watson et al., 2023) is model that assists in designing protein structures for specific functions, such as enzymes. It is versatile in protein"
        },
        {
            "title": "Preprint",
            "content": "design and has been used to create therapeutic proteins, with some designs being confirmed in the laboratory. ProteinSGM (Lee et al., 2022) is model that uses 2D matrices, which represent the distances and angles between protein parts, to create 3D protein structures for novel protein designs. FoldingDiff (Wu et al., 2022a) is model that generates protein sequences expected to fold into specific structure. These sequences are verified with prediction tools, although they have not been experimentally confirmed yet. Chroma (Ingraham et al., 2023) is model designed for creating large proteins and protein complexes, considering various constraints like distances and symmetry. It transforms collapsed polymer into protein backbone and sequence more quickly than older methods, thereby allowing for the efficient generation of large structures. Multiflow (Campbell et al., 2024) develop mulitmodal flow matching for protein structure-sequence co-generation (Jin et al., 2021; Shi et al., 2022). ProtPardelle (Chu et al., 2024) propose an all-atom generative approach for co-design."
        }
    ],
    "affiliations": [
        "ByteDance Research",
        "Dept. of Computer Science, Nanjing University"
    ]
}