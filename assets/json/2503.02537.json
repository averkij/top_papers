{
    "paper_title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
    "authors": [
        "Zhen Yang",
        "Guibao Shen",
        "Liang Hou",
        "Mushui Liu",
        "Luozhou Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires a few lines of code to unlock the model's high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts a simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 3 5 2 0 . 3 0 5 2 : r RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification Zhen Yang1* Guibao Shen1* Liang Hou2 Mushui Liu4 Luozhou Wang1 Xin Tao2 Pengfei Wan2 Di Zhang2 Ying-Cong Chen1,3 1HKUST(GZ) 2Kuaishou Technology 3HKUST 4Zhejiang University {zheny.cs,sgbsiat,wileewang97,jiangsutx}@gmail.com, lms@zju.edu.cn {houliang06,wanpengfei,zhangdi08}@kuaishou.com, yingcongchen@ust.hk Figure 1. Generated images of our method. Our training-free method can enable diffusion models (SDXL in the figure) to generate images at resolutions higher than their original training resolution. ZOOM IN for closer look."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires few lines of code to unlock the models high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency. The project page can be found here. 1. Introduction The development of diffusion models [6, 11, 26, 29, 33, 36, 40, 42, 58] has made the generative artificial intelligence community more prosperous, which has also improved the performance of large number of visual tasks, such as image editing [1, 4, 8, 24, 38, 39, 52, 54] and custom generation [2, 9, 12, 28, 44, 51]. Although the models perform well on these tasks, due to the lack of train- *Equal contribution. Corresponding author. Figure 2. The visualization results of predicted x0 at different time step t, abbreviated as pt x0 . The figure visualizes the process of how pt x0 changes with the sampling steps, where the x-axis represents the timestep in the sampling process. The 11 images are evenly extracted from 50 images. It can be observed that in the first half of the process, pt x0 is mainly responsible for global structure generation, while the second half is mainly responsible for local detail generation. ergy rectification. Specifically, Noise Refresh enhances the resolution of predicted x0 at certain sampling steps in the diffusion model and introduces new noise to align with the current step. Gradually applying noise refresh enables coarse-to-fine high-resolution image generation. Noise Refresh preserves the original sampling steps to ensure efficient performance. Consequently, our method achieves significant speed advantage compared to approaches like DiffuseHigh[25] and AP-LDM[5], which rely on multiround editing, as well as Signal-to-Noise Ratio (SNR) correction schemes such as FreCas[57] and MegaFusion[53], all of which introduce extra sampling steps. Furthermore, we observe an energy decay phenomenon in Noise Refresh and mitigate this issue by increasing the classifier-free guidance (CFG)[17] hyperparameter for energy rectification. As demonstrated in Fig. 5, our approach achieves superior results through simple and efficient framework. Concretely, we observe in Fig. 2 and Fig. 3 that predicted x0 primarily generates structure in early sampling steps and details in later sampling steps. Since lowresolution image details are lost during resizing, they contribute little to high-resolution images. Thus, in later sampling stages, we progressively replace low-resolution images with high-resolution ones. This preserves structural information from low resolutions to reduce repetitive patterns while skipping unnecessary low-resolution detail generation, enabling an efficient noise refresh algorithm. However, as shown in Fig. 7, using Noise Refresh alone introduces blurriness. To analyze the source of the issue, we introduce latent average energy to measure energy changes during sampling. As illustrated in Fig. 4a, we observe that Noise Refresh induces energy decay. We further discover that CFG (Classifier-Free Guidance) can control the latent average energy, as demonstrated in Fig.4b. Therefore, we can achieve energy rectification simply by increasing the value of the CFG hyperparameter, ensuring that RectifiedHR achieves high speed, superior quality, and straightforward logic. In general, our main contributions are as follows: Figure 3. The trend of predicted x0 at different time step t, abbreviated as pt x0 , on 100 random prompts. (a) The trend of the average CLIP Score between pt x0 and the prompt over different timesteps. The x-axis represents the sampling timestep, and the y-axis represents the average CLIP Score. (b) Average MSE between pt x0 and pt1 x0 . The x-axis represents the sampling timestep, and the y-axis represents the Average MSE. It can be observed that after approximately 30 steps, the trend of change in pt x0 slows down. ing in high-resolution data, existing methods show performance drop when generating images beyond the resolution of the training data. Training on high-resolution images is costly, so unleashing the models capability to generate high-resolution images without additional training has become important. Currently, many methods have explored training-free These methods have high-resolution generation task. attempted variety of complex techniques, including schemes based on sliding window denoising [2, 10, 27, 30, 31], modifying the model network structure [14, 20, 22, 56], adjusting classifier-free guidance [13, 21]. Some methods design suitable prompts for high-resolution image generation using vision language models [35, 47]. Others leverage the idea of image editing capabilities [37] to enrich the details of low-resolution images [5, 25, 53, 57]. However, these methods either require redundant sampling steps, leading to efficiency issues, suffer from repetitive patterns due to insufficient global information exchange, or have complex implementation logic. This raises an important question: Can we create method that is efficient and has simple implementation logic? In this work, we propose RectifiedHR, simple and efficient framework for high-resolution image generation. Our approach consists of two modules: noise refresh and enWe introduce relative latent energy analysis, and to the best of our knowledge, we are the first to discover the energy decay phenomenon during the high-resolution gen2 (a) The energy decay phenomenon of our noise refresh sampling process compared to the original sampling process on 100 random prompts. (b) The process of how the average latent energy changes with the timestep when 1024 1024 resolution images are generated from 100 random prompts under different classifier-free guidance hyperparameters. Figure 4. (a) The x-axis represents the timestep of the sampling process, and the y-axis represents the average latent energy. The blue line shows the average latent energy of the original sampling process generating 1024 1024-resolution images over the sampling process. The red line represents our noise refresh sampling process, where noise refresh is performed at the 30th and 40th sampling timesteps, and the resolution gradually increases from 1024 1024 to 2048 2048, and then to 3072 3072. It can be observed that noise refresh will cause the relative latent energy to show significant decay. From the left images, it can be observed that after energy rectification, the image details have become more prominent. (b) The x-axis represents the timestep, the y-axis represents the average latent energy, and ω is the hyperparameter for classifier-free guidance. It can be observed that the relative latent energy increases with the increase of ω. From the right figures, it can be observed how the images change as ω increases. eration process. We design novel training-free high-resolution image generation pipeline, which primarily includes noise refresh and energy rectification operations. This pipeline requires fewer lines of theoretical code to implement and is highly efficient. We compare RectifiedHR with large number of existing baselines and demonstrate the efficiency and effectiveness of our approach. 2. Related Work 2.1. Text-guided image generation With the scaling of models, data volume and computational resources, text-guided image generation models have witnessed unprecedented growth, leading to the emergence of numerous diffusion models, including LDM [42], SDXL [40], PixArt [6, 7], HunyuanDiT [29], LuminaNext [58], FLUX [26], SD3 [11] and LCM [36]. These models establish connections between Gaussian noise and high-quality images through various training and sampling methods, such as DDPM [18], SGM [50], EDM [23], DDIM [49], flow matching [32] and rectified flow [34]. However, due to the lack of training on high-resolution data, these models exhibit optimal performance only at specific resolutions and fall short when generating images with higher resolution. Consequently, exploring the potential of diffusion models for high-resolution image generation in training-free manner has become crucial in the vision-generation community. Our approach focuses primarily on achieving efficient highresolution image generation with minimal modifications to existing pipelines. 2.2. Training-free high-resolution image generation Due to the domain gap between different resolutions, directly using diffusion models for high-resolution image generation can result in pattern repetition and poor semantic structure. MultiDiffusion [2] proposes sliding window denoising scheme to achieve panoramic image generation. However, this method suffers from serious pattern repetition issues, as it mainly considers the aggregation of local information. Improved methods based on the sliding window denoising scheme include SyncDiffusion [27], Demofusion [10], AccDiffusion [31], and CutDiffusion [30]. Specifically, SyncDiffusion introduces global information using the gradient of perceptual loss from the predicted denoised images at each denoising step as guidance. Demofusion uses progressive upscaling, skip residual, and dilated sampling mechanisms to achieve higher-resolution image generation. AccDiffusions patch-content-aware prompts mechanism and CutDiffusions coarse-to-fine mechanism can solve pattern repetition issues. But they all have complex implementation logic and suffer from efficiency issues. ScaleCrafter [14], FouriScale [20], HiDiffusion [56], and Attn-SF[22] modify the network structure of the diffusion model, which may lead to suboptimal performance. Additionally, ScaleCrafter, FouriScale, and HiDiffusion can not generalize to other UNet[43]-free diffusion models beFigure 5. Overview of our method. (a) the original sampling process and its pseudocode. (b) The sampling process and pseudocode of our method. The orange parts of the pseudocode and modules correspond to Noise Refresh, while the purple parts represent Energy Rectification. ϵ is Gaussian random noise and its shape changes according to the shape of pt x0resize. Other symbols in the pseudocode can be found in Sec.3.1. cause of their highly structure-related methods. ResMaster [47] and HiPrompt [35] introduce multi-modal models to regenerate prompts to enrich image details, but the introduction of multi-modal models is too heavy, resulting in further efficient problems. Upscale Guidance [21] and ElasticDiffusion [13] all propose to add global denoising information and local denoising information to classifier-free guidance [17]. However, their global information requires heavy computational complexity compared to our progressive resolution increase approach. DiffuseHigh [25] and AP-LDM [5] utilize SDEdits [37] detail enhancement capability, gradually adding details from low-resolution images to high-resolution images. Compared to these methods, our approach does not increase the sampling steps and is therefore more efficient, while also having simple implementation. The most related concurrent works to ours are FreCas [57] and MegaFusion [53]. Compared to rescale noise methods like FreCas and MegaFusion, our method only modifies fewer lines of the sampling formula, resulting in simpler implementation logic while also avoiding the need for additional sampling steps. Compared to FreCas, we discover the energy decay issue and only need to adjust the classifier-free guidance parameter to rectify the energy to achieve better results. 3. Method 3.1. Preliminaries The diffusion models establish connection between Gaussian noise and images, enabling the generation of an image by randomly sampling noise. In this paper, we default to using SDXL [40] for our experiments and we assume the sampling steps to be 50 steps, with the denoising process defaulting from step 0 to step 49. We define Io as the real RGB image. During the training process, SDXL first adopts VAE encoder E() to transform the RGB image into lower-dimensional latent space, and we refer to the transformed latent as x0. Then, by applying the forward diffusion formula as follows: xt = αtx0 + 1 αtϵ, (1) we add varying degrees of noise to x0 to obtain different xt, where αt is time-related scheduler parameter to control the noise strength and ϵ is random sampled Gaussian noise. The neural network ˆϵ(xt, t, c) parameterized by θ is optimized to predict the noise added to x0 by the following training objective: (cid:104) Ext,t,c min θ ϵ ˆϵ (xt, t, c)2 2 (cid:105) , (2) where is the condition signal for generation (text prompt for T2I task). During the sampling process, random noise is sampled in the latent space, and then the diffusion model transforms the random noise into an image in gradually denoise manner. Finally, the latent is passed through VAEs decoder D() to reconstruct generated RGB image. The objective of high-resolution generation is to generate images with resolutions beyond the training datasets, e.g. resolutions more than 1024 1024 in our setting. 4 Classifier-free guidance for diffusion models. Currently, classifier-free guidance(CFG) [17] is widely used to enhance the quality of generated images by incorporating unconditional output at each denoising step. The classifierfree guidance formula is as follows: ϵ(xt, t) = ˆϵ(xt, t, ) + ω [ˆϵ(xt, t, c) ˆϵ(xt, t, )], (3) where ω is the hyperparameter of classifier-free guidance, ˆϵ(xt, t, ) and ˆϵ(xt, t, c) represent the predicted noises of the unconditional branch and conditional branch respectively, we refer to ϵ(xt, t) as the predicted noise after applying classifier-free guidance. Sampling process for diffusion models. In this paper, we use the DDIM sampler[49] by default. The deterministic sampling formula for DDIM is as follows: xt1 = αt1 (cid:18) xt 1 αtϵ(xt, t) (cid:19) αt (cid:123)(cid:122) predicted x0pt +(cid:112)1 αt1 ϵ(xt, t), x0 (cid:125) (cid:124) As illustrated in Eq. 4, in time step t, we first predict the noise ϵ(xt, t) by the pre-trained neural network ˆϵ(). Then, we can compute predicted x0 at time step called pt x0. Finally, xt1 could be derived from ϵ(xt, t) and pt x0 by the diffusion process in Eq. 4. 3.2. Noise refresh As shown in the changes of pt x0 with timestep in Fig. 2, we can observe that during the first half of the denoising process, the global structural information of pt x0 undergoes significant changes. However, in the latter half, the global structure remains largely unchanged, with the primary focus shifting toward the generation of detailed local information. As shown in Fig. 3, we further conduct experiments on the generation of pt x0 on 100 random prompts from LAION-5B [46] and analyze the CLIP Score [15] and Mean Squared Errors (MSE). In Fig. 3b, it is observed that after 30 steps of denoising, the CLIP score of pt x0 with prompt increases slowly. From Fig. 3a, we find that after 30 steps of denoising, the MSE between pt x0 changes very little. Therefore, we consider that the latter stages of the denoising process are primarily responsible for generating finer details. Furthermore, we find that in the last half of the sampling process, the global structure of pt x0 remains largely unchanged. Therefore, we can convert pt x0 to the RGB space for resizing. In addition, the latter sampling stage is mainly responsible for the generation of local details. Based on the above two points, we can generate high-resolution images with rich details. The formula for increasing the resolution of the pt x0 and pt1 x0 is as follows: x0 resize = E(resize(D(pt pt x0 ))), (5) 5 where represents VAEs Encoder, and resize refers to the operation of increasing the size of the RGB image. We adopt the bilinear interpolation as the default resize operation. Directly changing the size of xt can cause Signal-toNoise Ratio (SNR) mismatch issues [19, 21]. Therefore, we have updated the sampling formula to refresh new noise. The updated sampling formula is as follows: xt1 = αt1pt x0 resize + (cid:112)1 αt1ϵ (6) ϵ represents random Gaussian noise that shares the same shape as pt x0 resize. We refer to this process as Noise Refresh. As illustrated in Fig. 5b, the noise refresh operation is applied to several specific time points Ti during the sampling process. To automate the selection of these time steps , we propose the following selection formula: (4) Ti = (Tmax Tmin) ( )M + Tmin, (7) Tmax and Tmin define the range of sampling timesteps to use noise refresh. denotes the number of noise refresh that need to be performed. The range of is all integers between 1 and . is hyperparameter that can be adjusted to obtain different strategies to select Ti. 3.3. Energy rectification Although the image resolution increases after using noise refresh, we find that the generated images exhibit significant blurring if we do not conduct further process, as shown in the fourth column in Fig. 7. To analyze the cause of this phenomenon, we introduce relative latent energy formula as follows: Ex2 = (cid:80)C i=1 (cid:80)H j= (cid:80)W k=1 x2 tijk , (8) xt represents the latent variable at time t, where C, H, and denote the dimensions of the channel, height, and width of latent, respectively. The definition is very similar to the energy definition of an image, and is used to indicate the average energy of each element of latent vector. To analyze the issue of image blurring, we conduct an average energy experiment on 100 random prompts. As illustrated in Fig. 4a, we first compare the relative latent energy differences between the noise refresh sampling process and the original sampling process. We observe significant energy decay phenomenon in the noise refresh sampling process, which is the reason why the naive implementation method produces noticeably blurred images. Subsequently, we conduct an experiment to analyze the impact of the hyperparameter ω in classifier-free guidance on latent energy. As shown in Fig. 4b, we find that as the classifierfree guidance parameter ω increases, the energy exhibits Figure 6. Qualitative comparison of our method with other training-free methods using three LAION-5Bs prompts. gradually increasing trend. Therefore, we can address the issue of energy decay and improve the quality of generated images by increasing ω to enhance the energy in the noise refresh sampling scheme. As demonstrated in the fourth column and fifth column in Fig. 7, after the energy is rectified with larger classifier-free guidance hyperparameter ω, the blurry issue has been well addressed and the generated image shows remarkable clarity. We refer to this process of correcting energy decay as Energy Rectification. As shown in Fig. 5b, the energy rectification operation is applied to the sampling process after noise refresh. To more automatically select ω in the classifier-free guidance, we propose the following selection formula: ωi = (ωmax ωmin) ( )M + ωmin, (9) ωmax and ωmin represent the range of ω in classifier-free guidance during sampling process. denotes the number of noise refresh that needs to be performed. The range of is all integers between 1 and . is hyperparameter that can be adjusted to obtain different strategies to select ωi. 4. Results 4.1. Evaluation Setup We conduct experiments using SDXL [40] with 50 sampling steps as our base model, which is able to generate images at 1024 1024 resolution by default. Following the previous work, we randomly sample 1,000 prompts from the laion-5B [46] dataset as conditions to generate images. We compare our method with the following state-of-the-art: Demofusion [10], DiffuseHigh [25], HiDiffusion[56], CutDiffusion [30], ElasticDiffusion [13], AP-LDM [5], FreCas [57], SDXL+BSRGAN [55] FouriScale [20], ScaleCrafter 6 Methods FouriScale ScaleCrafter HiDiffusion CutDiffusion ElasticDiffusion AP-LDM AccDiffusion DiffuseHigh FreCas DemoFusion SDXL+BSRGAN Ours FouriScale ScaleCrafter HiDiffusion CutDiffusion ElasticDiffusion AP-LDM AccDiffusion DiffuseHigh FreCas DemoFusion SDXL+BSRGAN Ours FIDr KIDr 0.010 71.344 0.007 64.236 0.007 63.674 0.007 59.152 0.010 56.639 0.004 51.083 0.002 48.143 0.003 49.748 0.003 49.129 0.002 47.079 0.002 47.452 0.002 48.361 0.046 135.111 0.028 110.094 0.024 93.515 0.055 130.207 0.056 101.313 0.005 51.274 0.005 54.918 0.003 48.861 0.003 49.764 0.003 48.983 0.002 47.923 0.003 48.684 ISr 15.957 15.952 16.876 17.109 15.326 18.867 18.466 19.537 20.274 19.533 20.260 20.616 9.481 10.098 11.878 9.334 9.406 18.676 17.444 19.716 18.656 18.225 19.815 20. FIDc KIDc 0.014 53.990 0.010 45.861 0.008 41.930 0.008 38.004 0.014 37.649 0.006 29.193 0.008 32.747 0.004 27.667 0.004 27.002 0.004 26.441 0.004 25.827 0.003 25.347 0.057 129.895 0.043 112.105 0.058 120.170 0.055 113.033 0.089 111.102 0.012 41.615 0.023 60.362 0.010 40.267 0.010 39.047 0.010 38.136 0.014 41.126 0.009 35.718 ISc 20.625 22.252 23.165 23.444 19.867 25.331 24.778 27.876 29.843 27.843 27.155 28.126 9.792 11.421 11.272 10.961 7.627 20.126 16.370 21.550 21.700 20.786 19.231 20.819 CLIP 31.157 31.803 31.711 32.573 32.301 33.601 33.153 33.436 33.700 33.748 33.867 33.756 26.891 27.809 27.853 26.734 27.725 33.632 32.438 33.390 33.237 33.311 33.874 33.415 Time 59s 35s 18s 53s 150s 25s 111s 37s 14s 79s 6s 13s 489s 528s 71s 193s 400s 153s 826s 190s 74s 605s 6s 37s 8 4 0 2 8 4 0 2 6 9 0 4 6 9 0 Table 1. Comparison to SOTA methods in 2048 2048 and 4096 4096 resolution. The bold numbers denote the best performance and the underlined numbers denote the second best performance. and denote the higher the better and the lower the better respectively. We test the inference time on the same machine using single NVIDIA A800 GPU. [14], and AccDiffusion [31]. Except for SDXL+BSRGAN, which requires to use the trained BSRGAN model, other methods are training-free. We fix inference steps and set In addition, we remove the negative prompts as empty. additional tricks such as FreeU [48] for fair comparison. Quantitatively, we mainly generate high-resolution images at target resolutions of 2048 2048 (4x of the original resolution) and 4096 4096 (16x of the original resolution). We employ four widely used quantitative evaluation metrics: FID (Frechet Inception Distance) [16], KID (Kernel Inception Distance) [3], IS (Inception Score) [45], and CLIP Score [41]. Specifically, FIDr, KIDr, and ISr require resizing images to 299x299 before calculation. However, this kind of evaluation is not reasonable for high-resolution image generation. Following the approach of previous works [10, 31], we randomly crop 10 patches of 1024x1024 (1x) from each generated high-resolution image to further calculate FIDs, KIDc, and ISc. For the 2048 2048 resolution scene, we set Tmin at 40, Tmax at 50, at 1, ωmin at 30, ωmax at 30, in Eq. 7 at 2 and in Eq. 9 at 1. For the 4096 4096 resolution scene, we set Tmin at 40, Tmax at 50, at 2, ωmin at 36.8, ωmax at 50, in Eq. 7 at 0.5 and in Eq. 9 at 0.5. Unless otherwise specified, all of our evaluation experiments are conducted on NVIDIAA800 GPUs. 4.2. Quantitative Results As demonstrated in Tab. 1, our method achieves the best performance in 4 out of 8 metrics and the second best on 3 out of 8 metrics in 2048 2048 resolution scene, and achieves the best performance in 3 out of 8 metrics and the second best on 3 out of 8 metrics in 4096 4096 resolution scene, showing the effectiveness of RectifiedHR in solving the high-resolution generation task. Considering that our method requires the least time to generate sample among all the training-free baselines in both 2048 2048 resolution and 4096 4096 resolution, it still produces high-resolution images with highly competitive performance, demonstrating the efficiency of our method. Although the SDXL+BSRGAN method is faster than ours, in cropped metrics, which are specifically designed for highresolution generation, our method demonstrates superior performance compared to SDXL+BSRGAN at an acceptable additional time cost. In addition, we add the qualitative results for comparison with SDXL+BSRGAN in Fig. 8. 7 Figure 8. Qualitative Comparison between our method and SDXL+BSRGAN. pare the qualitative results. The experimental setup is consistent with Sec. 4.1. As shown in Fig. 8, we find once the data generated by SDXL exceeds the domain of the original image, such as distorted faces, BSRGAN lacks the ability to correct these errors, leading to performance decline. Moreover, we consider our method and BSRGAN are not mutually exclusive. Some traditional super-resolution models, similar to BSRGAN, can help our training-free high-resolution image generation method add more details. 4.5. Ablation Study Our method comprises two components: (i) noise refresh and (ii) energy rectification. To validate the effectiveness of these components, we perform experiments on all possible combinations, as illustrated in Fig. 7. The first and second rows in Fig. 7 represent images generated directly at resolutions of 1024 1024 and 2048 2048, respectively. It can be observed that when the 1024 1024 image is enlarged, there are local blurring phenomena. At the same time, it is evident that the 2048 2048 image in the second row of Fig. 7 exhibits repeated patterns and also suffers from blurring issues due to energy decay. The third row does not use noise refresh; instead, it only adds energy rectification in the last 15 steps of direct inference. Compared to the second row, although the repeated pattern problem is not resolved, the image becomes clearer. The fourth row introduces noise refresh but does not use energy rectification. It can be seen that noise refresh solves the repeated pattern problems found in the second and third rows, but there are still some blurring phenomena. The fifth row represents our method, which can be seen to solve both the repeated pattern problem and to make the details clearer. 5. Conclusion And Future Work We propose an efficient and simple method called RectifiedHR for higher-resolution image generation. Specifically, we present relative latent energy analysis and, to the best of our knowledge, are the first to identify the energy decay phenomenon during the high-resolution image generation process. We develop novel training-free and easy pipeline for high-resolution image generation, which primarily consists of noise refresh and energy rectification 8 Figure 7. Ablation studies on our method. 4.3. Qualitative Results To clearly observe the differences between our method and other baselines, we select three prompts from the LAION5B dataset to conduct qualitative comparison experiments on FourwiScale, ScaleCrafter, DemoFusion, and HiDiffusion. In Fig. 6a, b, and c, we can observe that FouriScale exhibits abnormal high-frequency information, which masks the main subjects in the images, leading to decrease in the quality of the generated images. In column of Fig. 6, there is noticeable edge blurring effect, which might be due to the window denoising mechanism of DemoFusion. From Fig. 6a, it can be seen that the car structures generated by ScaleCrafter are poor and also tend to produce blurred edges similar to Demofusion. It is evident that our method generates images with more reasonable edges and more accurate structures in Fig. 6. 4.4. Comparison with the super-resolution model. further To the SDXL+BSRGAN and our method, we further comdifferences compare between the operations. Extensive comparisons show that RectifiedHR outperforms existing methods in terms of effectiveness and efficiency. However, our method has certain limitations; we have not yet adapted it to tasks beyond image generation. In the future, we plan to use our method in more tasks, such as image editing, video generation, and custom generation."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In ECCV, pages 707723. Springer, 2022. 1 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 1, 2, 3 [3] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 7 [4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. 1 [5] Boyuan Cao, Jiaxin Ye, Yujie Wei, and Hongming Shan. Ap-ldm: Attentive and progressive latent diffusion model for training-free high-resolution image generation. arXiv preprint arXiv:2410.06055, 2024. 2, 4, 6 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1, [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2025. 3 [8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. 2022. 1 [9] Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and Chunhua Shen. Freecustom: Tuningfree customized image generation for multi-concept composition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90899098, 2024. 1 [10] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising highIn Proceedings of resolution image generation with no. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024. 2, 3, 6, 7 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 3 [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [13] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66036612, 2024. 2, 4, 6 [14] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 7 [15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. 2021. 5 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4, [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [19] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 5 [20] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. In European Conference on Computer Vision, pages 196212. Springer, 2025. 2, 3, 6 [21] Juno Hwang, Yong-Hyun Park, and Junghyo Jo. Upsample guidance: Scale up diffusion models without training. arXiv preprint arXiv:2404.01709, 2024. 2, 4, 5 [22] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Trainingfree diffusion model adaptation for variable-sized text-toimage synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. 2, 3 [23] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, pages 60076017, 2023. 1 [25] Younghyun Kim, Geunmin Hwang, Junyu Zhang, and Eunbyung Park. Diffusehigh: Training-free progressive highresolution image synthesis through structure guidance. arXiv preprint arXiv:2406.18459, 2024. 2, 4, 6 9 [26] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. 1, 3 [27] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. Advances in Neural Information Processing Systems, 36:5064850660, 2023. 2, [28] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. 1 [29] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 1, 3 [30] Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, and Rongrong Ji. Cutdiffusion: simple, fast, cheap, and strong diffusion extrapolation method. arXiv preprint arXiv:2404.15141, 2024. 2, 3, 6 [31] Zhihang Lin, Mingbao Lin, Meng Zhao, and Rongrong Ji. Accdiffusion: An accurate method for higher-resolution imIn European Conference on Computer Viage generation. sion, pages 3853. Springer, 2025. 2, 3, 7 [32] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [33] Mushui Liu, Yuhang Ma, Yang Zhen, Jun Dan, Yunlong Yu, Zeng Zhao, Zhipeng Hu, Bai Liu, and Changjie Fan. Llm4gen: Leveraging semantic representation of llms for text-to-image generation. arXiv preprint arXiv:2407.00737, 2024. [34] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [35] Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, et al. Hiprompt: Tuning-free higher-resolution genarXiv preprint eration with hierarchical mllm prompts. arXiv:2409.02919, 2024. 2, 4 [36] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 3 [37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2, 4 [38] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. 2023. 1 [39] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In CVPR, pages 60386047, 2023. 1 Robin Rombach. Sdxl: els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 3, 4, Improving latent diffusion modarXiv preprint [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. 1 [45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 5, 6 [47] Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. Resmaster: Mastering highresolution image generation via structural and fine-grained guidance. arXiv preprint arXiv:2406.16476, 2024. 2, 4 [48] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. In Proceedings of Freeu: Free lunch in diffusion u-net. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47334743, 2024. 7 [49] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3, 5 and Stefano Ermon. arXiv preprint [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [51] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalizaIn ACM SIGGRAPH 2023 Conference Proceedings, tion. 2023. 1 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and [52] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. 10 image-to-image translation. 2023. In CVPR, pages 19211930, [53] Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, and Yanfeng Wang. Megafusion: Extend diffusion models towards higher-resolution image generation without further tuning. arXiv preprint arXiv:2408.11001, 2024. 2, 4 [54] Zhen Yang, Ganggui Ding, Wen Wang, Hao Chen, Bohan Zhuang, and Chunhua Shen. Object-aware inverarXiv preprint sion and reassembly for image editing. arXiv:2310.12149, 2023. 1 [55] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind In Proceedings of the IEEE/CVF image super-resolution. International Conference on Computer Vision, pages 4791 4800, 2021. 6 [56] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, and Jiajun Liang. Hidiffusion: Unlocking high-resolution creativity and efficiency in low-resolution trained diffusion models. arXiv preprint arXiv:2311.17528, 2023. 2, 3, 6 [57] Zhengqiang Zhang, Ruihuang Li, and Lei Zhang. Frecas: Efficient higher-resolution image generation via frequencyaware cascaded sampling. arXiv preprint arXiv:2410.18410, 2024. 2, 4, [58] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 1,"
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "Kuaishou Technology",
        "Zhejiang University"
    ]
}