{
    "paper_title": "Scaling Zero-Shot Reference-to-Video Generation",
    "authors": [
        "Zijian Zhou",
        "Shikun Liu",
        "Haozhe Liu",
        "Haonan Qiu",
        "Zhaochong An",
        "Weiming Ren",
        "Zhiheng Liu",
        "Xiaoke Huang",
        "Kam Woh Ng",
        "Tian Xie",
        "Xiao Han",
        "Yuren Cong",
        "Hang Li",
        "Chuyan Zhu",
        "Aditya Patel",
        "Tao Xiang",
        "Sen He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 0 9 6 0 . 2 1 5 2 : r Scaling Zero-Shot Reference-to-Video Generation Zijian Zhou1,2,, Shikun Liu1, Haozhe Liu1, Haonan Qiu1, Zhaochong An1, Weiming Ren1, Zhiheng Liu1, Xiaoke Huang1, Kam Woh Ng1, Tian Xie1, Xiao Han1, Yuren Cong1, Hang Li1, Chuyan Zhu1, Aditya Patel1, Tao Xiang1, Sen He1 1Meta AI, 2Kings College London Work done at Meta Reference-to-video (R2V) generation aims to synthesize videos that align with text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs masked training strategy and tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across varying number of references and achieves superior performance on the OpenS2VEval benchmark compared to methods trained with R2V data. Date: December 9, 2025 Website: https://franciszzj.github.io/Saber/"
        },
        {
            "title": "1 Introduction",
            "content": "Reference-to-video (R2V) generation synthesizes videos that align with given text prompt while preserving the identity and appearance of subjects in reference images. This task represents crucial step toward personalized video generation, enabling applications such as customized storytelling (Rahman et al., 2023; Wang et al., 2024; Zhou et al., 2024b) and virtual avatars (Guo et al., 2024a; Yuan et al., 2025b; Gao et al., 2025a; Shen et al., 2025). Despite recent progress in text-to-video (T2V) and image-to-video (I2V) generation (Hong et al., 2023; Yang et al., 2025; Liu et al., 2024; HaCohen et al., 2024; Kong et al., 2024; Wan et al., 2025; Gao et al., 2025b; Zhang et al., 2025), R2V remains uniquely challenging as it must simultaneously ensure semantic alignment with text and maintain high-fidelity subject identity from the reference images. Existing R2V methods (Zhou et al., 2024a; Liu et al., 2025; Jiang et al., 2025; Deng et al., 2025; Fei et al., 2025; Hu et al., 2025b; Xue et al., 2025; Li et al., 2025) typically rely on constructing explicit R2V datasets (e.g., OpenS2V-5M (Yuan et al., 2025a) and Phantom-Data (Chen et al., 2025b)) that contain triplets of reference images, videos, and text prompts. Building such datasets involves complex pipelines for data collection, annotation, clustering and filtering, which are costly and difficult to scale. Moreover, the limited diversity of reference images in these datasets restrict generalization, making it difficult to handle unseen subject categories. We propose Saber, scalable, zero-shot framework that bypasses this data bottleneck. Saber is trained solely on large-scale video-text pairs, the same data paradigm used for T2V and I2V models. This design allows Saber to leverage abundant video-text datasets (Chen et al., 2024b; Wang et al., 2025; Team), completely eliminating the need for bespoke R2V data construction. To this end, our method introduces masked training strategy that uses randomly sampled and partially masked video frames as reference images during training, where the randomness of masking provides diverse reference conditions and improves generalization across subject categories. This process compels the model to learn identityand appearance-consistent representations from the reference context, effectively simulating the R2V task without R2V data. This strategy is complemented by tailored attention mechanism, guided by attention masks, which directs the model to focus on reference-aware features while suppressing 1 Figure 1 Saber is zero-shot reference-to-video method trained only on video-text pairs. It preserves identity and appearance while coherently integrating single/multiple references into videos guided by text prompts. background noise. To further enhance visual fidelity and mitigate the copy-paste artifacts which is common in reference-to-video generation Liu et al. (2025); Fei et al. (2025); Hu et al. (2025a), we integrate series of spatial mask augmentations, effectively improving the visual quality of the generated videos. Sabers design is inherently scalable. It naturally supports varying number of reference images (see Fig. 1 and Fig. 4), without additional data preparation or modification to the training pipeline, allowing for richer, multi-subject customization. The stochasticity of the masked training strategy also allows Saber to robustly handle multiple reference views of the same subject (see Fig. 7). We evaluate Saber on the OpenS2V-Eval (Yuan et al., 2025a) benchmark, where it consistently outperforms models (Zhou et al., 2024a; Liu et al., 2025; Jiang et al., 2025; Deng et al., 2025; Fei et al., 2025; Hu et al., 2025b; Li et al., 2025) that were explicitly trained on R2V data. In addition, by simply adjusting the masking ratio during training, Saber can adapt to references depicting either foreground subjects or background scenes (see Fig. 1). Our contributions are summarized as follows: We introduce Saber, the first zero-shot R2V framework that eliminates the need for explicit R2V data through masked training on video-text pairs. Saber surpasses previous R2V-data-trained methods on OpenS2V-Eval (Yuan et al., 2025a) and demonstrates strong generalization and scalability, paving the way for future research in scaling reference-tovideo generation."
        },
        {
            "title": "2.1 Video Generation",
            "content": "The rapid progress of diffusion models (Rombach et al., 2022) has greatly advanced video generation. Early methods (Blattmann et al., 2023; Guo et al., 2024b; Chen et al., 2024a) extended pre-trained text-toimage models (Rombach et al., 2022; Podell et al., 2023) with temporal modules to synthesize videos. More recently, large-scale models based on Diffusion Transformer (Peebles and Xie, 2023) and trained on massive video-text datasets (Chen et al., 2024b; Wang et al., 2025) have achieved state-of-the-art, high-fidelity video generation (Yang et al., 2025; Kong et al., 2024; Chen et al., 2025a; Wan et al., 2025; Gao et al., 2025b; Zhang et al., 2025). Despite these advances, existing methods mainly focus on text-to-video and image-to-video tasks. While fine-grained, subject-driven control, as required by reference-to-video generation, remains significant challenge, the complex, costly data construction for R2V datasets (Yuan et al., 2025a; Liu et al., 2025) makes the large-scale training seen in T2V and I2V infeasible."
        },
        {
            "title": "2.2 Reference-to-Video Generation",
            "content": "Building on the progress of text-to-video and image-to-video models (Hong et al., 2023; Yang et al., 2025; Kong et al., 2024; HaCohen et al., 2024; Wan et al., 2025), reference-to-video generation (Yuan et al., 2025b; Jiang et al., 2025; Liu et al., 2025; Hu et al., 2025a,b; Li et al., 2025) has seen significant advancement. Early studies (Gao et al., 2025a; Yuan et al., 2025b; Shen et al., 2025) mainly focused on human reference images, termed identitypreserving video generation, where facial or body features are injected into models to maintain identity consistency. Later, reference images extended from humans to various objects and backgrounds (Liu et al., 2025; Jiang et al., 2025; Hu et al., 2025a), allowing more flexible control. Some works (Liu et al., 2025; Yuan et al., 2025a) also refer to this task as subject-consistent or subject-to-video generation, which is equivalent to reference-to-video generation. Representative works include Phantom (Liu et al., 2025), which learns cross-modal alignment with joint text-image injection model using image-video-text triplet data. VACE (Jiang et al., 2025) introduces context adapter to process reference images and enable temporal-spatial feature interaction within unified framework. SkyReels-A2 (Fei et al., 2025) builds an image-text joint embedding model to inject multi-element representations, balancing consistency and coherence. HunyuanCustom (Hu et al., 2025a) employs LLaVAbased (Liu et al., 2023) fusion module and an image ID enhancement module to strengthen multimodal understanding and identity consistency. MAGREF (Deng et al., 2025) uses region-aware masking and pixelwise concatenation for effective multi-reference interaction. PolyVivid (Hu et al., 2025b) adds 3D-RoPE enhancement and attention-inherited identity injection to reduce identity drift. BindWeave (Li et al., 2025) leverages an MLLM (Bai et al., 2025) to link complex prompts with visual subjects, improving video generation quality. However, critical limitation unites these approaches: these methods rely on explicit reference image-videotext triplet datasets, which are costly and difficult to construct. Datasets such as OpenS2V-5M (Yuan et al., 2025a) and Phantom-Data (Chen et al., 2025b) require complex construction pipelines, including candidate extraction, low-quality sample filtering, sample clustering, cross-pair matching, and expensive API calls for reference image generation. Such processes result in uncontrolled data quality, poor scalability, and high construction complexity. In contrast, we propose zero-shot R2V framework trained solely on video-text pairs, achieving strong performance on public benchmarks."
        },
        {
            "title": "3.1 Video Generation Models",
            "content": "Video generation models (Hong et al., 2023; Yang et al., 2025; Kong et al., 2024; HaCohen et al., 2024; Wan et al., 2025) have achieved remarkable progress and gained broad attention. Among these, the Wan Video series (e.g., Wan2.1 (Wan et al., 2025)) is one of the most popular open-source frameworks. Our method builds on the Wan2.1-14B model (Wan et al., 2025), which consists of variational autoencoder (VAE) (Kingma 3 and Welling, 2013), transformer backbone (Vaswani et al., 2017; Peebles and Xie, 2023) and text encoder (i.e., umt5-xxl (Chung et al., 2023)). The VAE encodes videos into temporally and spatially compressed latents z0 and decodes them back to pixel space, reducing token count and computation. Wan2.1 trains the diffusion model Ψ using Flow Matching (FM) (Lipman et al., 2022), where the forward process linearly interpolates between data and noise. For time step [0, 1], Gaussian noise ϵ (0, I) is added to z0 to obtain zt, following zt = (1 t)z0 + tϵ. The model is optimized to predict the target velocity with the following objective: LFM = Ez0,ϵ,t,c (cid:104) (z0 ϵ) Ψ θ(zt, t, c)2 2 (cid:105) , (1) where θ denotes the learnable parameters of the diffusion model Ψ, and represents the condition features derived from the given text prompt and reference images."
        },
        {
            "title": "3.2 Task Definition and Notations",
            "content": "Given reference images {Ik RHkWk3}K video {I RHW3}F images while following the instructions in text prompt P. Here, Hk the corresponding k-th reference image Ik width of the generated video. and text prompt P, the reference-to-video method generates whose subjects preserve the identities and appearances of those in the reference denote the height and width of , while F, H, and represent the number of frames, height, and and Wk k=1 ="
        },
        {
            "title": "4 Method",
            "content": "Our goal is to train diffusion model Ψ while following the provided text prompt and appearance of subjects in the given reference images {Ik}K P. Previous methods (Liu et al., 2025; Jiang et al., 2025; Hu et al., 2025a,b; Li et al., 2025) rely on reference image-video-text triplets, which are costly and hard to scale. In contrast, Saber achieves R2V capabilities using only video-text pairs, the same data paradigm used for T2V and I2V training. capable of generating videos {I }F that preserve the identity k=1 =1 θ Our core idea of Saber is to simulate the R2V task by replacing the explicitly collected reference images with randomly masked frames during training. This masked training strategy is supported by two key components to enhance robustness and visual quality: i) series of mask augmentations designed to mitigate copy-paste artifacts, and ii) tailored attention mechanism that guides the model to focus on relevant reference features. We first introduce the construction of masked frames in Sec. 4.1, including mask generation and augmentation. Next, we present the models architecture design in Sec. 4.2, detailing the input format and transformer-based attention mechanism. Finally, Sec. 4.3 describes the zero-shot R2V inference process."
        },
        {
            "title": "4.1 Masked Frames as Reference",
            "content": "A standard R2V model learns to extract identity and appearance features from reference images {Ik}K k=1 and inject them into the generated video {I }F . However, existing R2V datasets (Yuan et al., 2025a; Chen et al., 2025b) mainly consist of humans and common objects, leading to limited subject diversity and poor generalization. To address this, instead of relying on pre-collected reference images, we use randomly masked frames as dynamic substitutes during training. This strategy naturally introduces highly diverse reference samples, allowing the model to learn more effective subject integration and achieve stronger generalization. =1 As shown in Fig. 2, for each k-th reference image Ik randomly sampled from the video, we first use mask generator to produce binary mask Mk {0, 1}HW. To mitigate the copy-paste issue (Liu et al., 2025) in R2V tasks, we perform mask augmentation to disrupt spatial correspondence between the masked references and their corresponding video frames. Specifically, we apply an identical set of spatial augmentations to both Ik . This process is repeated to create the full set of masked frames {ˆIk}K is then obtained as ˆIk = Ik Mk that serve as the reference condition. . The masked frame ˆIk k=1 , producing Ik and Mk and Mk 4 Mask Generator. We randomly select one mask type from predefined shape categories (e.g., ellipse, Fourier blob, convex/concave polygon, etc.) to generate binary mask {0, 1}HW with target foreground area ratio [rmin, rmax]. Specifically, we first randomly select foreground center. To ensure that the generated mask meets the desired foreground area ratio r, we define continuous scale parameter for each shape category, where the masks foreground area increases monotonically with the scale. bisection search over the scale is then performed to satisfy the area ratio constraint. When pixel discretization prevents an exact match, small topology-preserving adjustments are applied: growth dilates background boundary pixels, while shrinkage erases background boundary pixels. This design ensures controllable foreground area ratios while maintaining diversity in mask shapes. Several mask examples are illustrated in Fig. 2 Top. Figure 2 Masked reference generation. Given video, the mask generator produces diverse random masks, which are then applied to each randomly sampled video frame with mask augmentation. Mask Augmentation. We apply random affine transformations, including rotation, scaling, shear, translation, , ensuring the masked region remains fully and optional horizontal flip, to both the image Ik inside the frame. Transformation parameters are uniformly sampled within predefined ranges and validated to avoid boundary overflow. The same affine transformation is applied to the image and mask using bilinear and nearest-neighbor interpolation, respectively. and its mask Mk The reference code of the mask generator and augmentation are provided in the supplementary materials."
        },
        {
            "title": "4.2 Model Design",
            "content": "as reference images, we detail our model design for the R2V task. After obtaining the masked frames {ˆIk}K We adopt simple yet effective input format by concatenating reference images along the temporal dimension at the end of the target video frames in latent space. This allows the model to manage the interaction between the target video latents and reference latents through the attention mechanism in each transformer block. k=1 =1 and P, and the masked frames {ˆIk}K Input Format. Given video-text pair {I }F we use the VAE to encode the video from pixel space into latent space, obtaining z0 = {z ˆf Rhwd} ˆF ˆf =1 Here, ˆF = (F 1)/4 + 1, where 4 is the temporal compression ratio of the Wan2.1 VAE (Wan et al., 2025), h, w, and denote the height, width, and feature dimension of the video latent, respectively. We obtain zt with time step following Sec. 3.1. For the reference images, we individually encode each ˆIk using the VAE to obtain zref = {zk Rhwd}K is resized to match the latent space resolution, . Accordingly, each Mk producing mref = {mk {0, 1}hw4}K , where 0 indicates non-reference and 1 indicates reference region. The transformer input zin is defined as in Eq. 2: as reference images, k=1 k=1 k=1 . zin = cat cat[ zref zt cat[ mzero mref cat[ zref zzero ]temporal ]temporal ]temporal , channel (2) where cat[]temporal and cat[]channel denote concatenation along the temporal and channel dimensions, respectively. zzero is the VAE-encoded latent of zero-value video, and mzero is an all-zero mask, both shaped 5 to match the temporal dimensions of the video part. Note that zref remains noise-free to preserve accurate conditioning. Attention Mechanism. After obtaining the transformer input zin, we encode the text prompt into text features zP and jointly feed them, along with the time step t, into the transformer. Each transformer block consists of self-attention, cross-attention, and feedforward (FFN) modules. In self-attention, the video and reference parts of zin interact with each other. To avoid attending to non-reference regions, each Mk is resized to match the flattened latent shape to form an attention mask, where video tokens are bidirectionally attended, and only valid reference regions are attended in the reference part. The self-attention output is then passed to the cross-attention module to interact with zP. Here, video tokens are guided by the text prompt, while reference tokens learn their semantic alignment, enabling the integration of reference image information under textual constraints. The FFN module refines the results, with the time step injected into the latents for the control of the time step. After multiple transformer blocks, the transformer model outputs the predicted latent zt1 . The model design is shown in Fig. 3."
        },
        {
            "title": "4.3 Zero-Shot Inference",
            "content": "Figure 3 Model design overview. Masked frames serve as reference images and are concatenated to the video tokens in latent space. Self-attention enables interaction between video and reference tokens under the attention mask, while cross-attention incorporates text guidance for semantic alignment. The VAE, text encoder, and timestep components are omitted for clarity. In this section, we present the approach that enables the model trained with masked frames to perform zero-shot R2V inference. During inference, for each reference image Ik , we first use pre-trained object segmenter (Ren et al., 2024; Zheng et al., 2024) to extract the foreground subject region mask Mk . We then normalize the reference image Ik to the range of [1, 1] and fill the masked background regions with zeros (color gray). Notably, this segmentation step is flexible. If reference image is intended to provide background scene rather than foreground subject, segmentation is skipped. In this case, we use the full, unmasked reference image and an all-ones mask Mk Both Ik from its original size (Hk, Wk) to fit within the target video size (H, W) while preserving the aspect ratio and pads the remaining area with zeros to produce centered reference image of size (H, W). Finally, the processed reference image and mask are fed into the model following the input format in Sec. 4.2 and are used for prediction following the Wan inference pipeline (Wan et al., 2025). are processed by resize-and-padding operation, which scales Ik , treating the entire image as the reference region. and Mk Table 1 Quantitative results on the OpenS2V-Eval (Yuan et al., 2025a) benchmark. Saber outperforms both closed-source and explicitly trained R2V methods, achieving the highest overall score in zero-shot setting. It also attains the best NexusScore for subject consistency and competitive performance on GmeScore and NaturalScore. Method Total Score Aesthetics MotionSmoothness MotionAmplitude FaceSim GmeScore NexusScore NaturalScore Pika2.1 (Team, 2025b) Vidu2.0 (Team, 2025c) Kling1.6 (Team, 2025a) SkyReels-A2 (Fei et al., 2025) MAGREF (Deng et al., 2025) Phantom-14B (Liu et al., 2025) VACE-14B (Jiang et al., 2025) BindWeave (Li et al., 2025) 51.88% 51.95% 56.23% 52.25% 52.51% 56.77% 57.55% 57.61% 46.88% 41.48% 44.59% 39.41% 45.02% 46.39% 47.21% 45.55% Saber (Ours) 57.91% 42.42%"
        },
        {
            "title": "5 Experiments",
            "content": "Closed-source commercial R2V methods Explicit R2V data-based training methods 87.06% 90.45% 86.93% 87.93% 93.17% 96.31% 94.97% 95.90% 96.12% Zero-shot R2V methods 24.71% 13.52% 41.60% 25.60% 21.81% 33.42% 15.02% 13.91% 21.12% 30.38% 35.11% 40.10% 45.95% 30.83% 51.46% 55.09% 53.71% 69.19% 67.57% 66.20% 64.54% 70.47% 70.65% 67.27% 67.79% 45.40% 43.37% 45.89% 43.75% 43.04% 37.43% 44.08% 46.84% 63.32% 65.88% 74.59% 60.32% 66.90% 69.35% 67.04% 66.85% 49.89% 67.50% 47.22% 72.55%"
        },
        {
            "title": "5.1 Datasets, Metrics and Implementation Details\nDatasets. Benefiting from the masked training strategy, Saber is trained exclusively on video-text pair datasets,\nenabling the use of data from T2V and I2V sources. Specifically, we employ the ShutterStock Video (Team)\ndataset and generate captions for all video clips using Qwen2.5-VL-Instruct (Bai et al., 2025), thus constructing\nthe corresponding video-text pairs for training.",
            "content": "Metrics. To ensure fair comparison, we adopt the OpenS2V-Eval (Yuan et al., 2025a) benchmark and follow its official protocol for fine-grained evaluation of reference-to-video generation. The benchmark contains 180 prompts across seven categories, spanning single-reference (face, human, entity) and multi-reference (multi-face, multi-human, human-entity) scenarios. We report automated metrics where higher scores indicate better performance, including Aesthetics for visual quality, MotionSmoothness for temporal coherence, MotionAmplitude for motion magnitude, and FaceSim for identity preservation. In addition, we use three OpenS2V-Eval metrics, NexusScore, NaturalScore, and GmeScore, which measure subject consistency, naturalness, and text-video alignment, respectively. Implementation Details. Saber is finetuned from the Wan2.1-14B (Wan et al., 2025) model using our proposed masked training strategy on video-text pair datasets. For the mask generator, we adopt probabilistic sampling strategy for the foreground area ratio r: with 10% probability, we set [0, 0.1] to simulate minimal and no reference information, enabling the model to handle varying numbers of reference images; with 80% probability, we set [0.1, 0.5] to represent typical primary subjects; and with the remaining 10% probability, we set [0.5, 1.0] to help the model learn from large reference images or background scenes. For mask augmentation, we randomly apply rotation within [10, 10] degree, scaling in the range [0.8, 2.0], horizontal flipping with 50% probability, and shearing within [10, 10] degree. We found these augmentations to be empirically effective at overcoming copy-paste artifacts. We train our model with the objective defined in Eq. 1, using the AdamW optimizer with 1e5 learning rate and global batch size of 64. During inference, we use BiRefNet (Zheng et al., 2024) to segment the foreground subjects from the reference images. Following the standard setting of Wan2.1 (Wan et al., 2025), we generate videos with 50 denoising steps and CFG (Ho and Salimans, 2022) guidance scale of 5.0."
        },
        {
            "title": "5.2 Quantitative Results",
            "content": "We follow Yuan et al. (2025a) and conduct comprehensive evaluation on OpenS2V-Eval (Yuan et al., 2025a) benchmark, with the results presented in Tab. 1. The table compares three types of methods: closed-source commercial R2V methods, explicitly trained R2V methods, and our zero-shot R2V approach. Compared with the closed-source commercial method Kling1.6 (Team, 2025a), our model achieves 1.68% higher total score. Among methods trained on explicit R2V datasets, our method surpasses Phantom (Liu et al., 2025) by 1.14%, VACE (Jiang et al., 2025) by 0.36%, and BindWeave (Li et al., 2025) by 0.30%. While these methods rely on 7 Figure 4 Qualitative comparison with existing R2V methods. We compare Saber with Kling1.6 (Team, 2025a), Phantom (Liu et al., 2025), and VACE (Jiang et al., 2025) across four scenarios: single/multiple human and object references. Saber accurately preserves subject identity and appearance, integrates multiple references coherently, and generates smoother, more visually consistent videos. costly explicit R2V datasets that are difficult to scale, our approach uses only text-video pairs with masked training strategy, achieving the best overall performance in zero-shot setting. Among all sub-metrics, NexusScore best represents R2V performance by measuring subject consistency. Saber achieves the highest NexusScore, exceeding Phantom by 9.79%, VACE by 3.14%, and BindWeave by 0.36%. This shows that the masked training strategy effectively learns subject features from video-text pairs in zero-shot setting, outperforming all R2V-data-based models. Our method also achieves competitive results on GmeScore (text-video alignment) and NaturalScore (video naturalness)."
        },
        {
            "title": "5.3 Qualitative Results",
            "content": "We further conduct qualitative comparison between Saber and other methods (Kling1.6 (Team, 2025a), Phantom (Liu et al., 2025) and VACE (Jiang et al., 2025)) across various visual scenarios, as shown in Fig. 4. i) In the top-left (single human reference), both Kling1.6 and Phantom fail to embed the reference subject into the generated video, leading to inconsistent facial appearances. VACE suffers from copy-paste issue, directly overlaying the face from the reference image. In contrast, Saber generates video with consistent and text-aligned facial identity. ii) In the bottom-left (single object reference), Kling1.6 produces bowl with an incorrect leg structure, while Phantom and VACE fail to capture both the shape and appearance of the bowl from the reference. In contrast, our method, benefiting from the rich diversity of masked frames during masked training, accurately integrates the bowls shape and appearance into the generated video. iii) In the top-right (multiple human references), Kling1.6 embeds only one subject, Phantom duplicates the same identity twice, and VACE fails to inject facial information from the references. In contrast, Saber incorporates 8 Table 2 Ablation study. i) Masked training outperforms training on the OpenS2V-5M (Yuan et al., 2025a) (w/o masked training), demonstrating the advantage of the masked training strategy. ii) Using only single mask type reduces total score, while combining all types performs best, showing the importance of mask diversity. iii) Fixing the foreground area ratio (r = 0.3) leads to further drop, indicating limited mask variation harms generalization. Method Total Score GmeScore NexusScore NaturalScore Saber w/o masked training ellipse only fourier only polygon only fixed = 0.3 57.91% 56.24% 54.56% 56.33% 56.49% 51.73% 67.50% 67.27% 67.98% 67.25% 67.41% 67.12% 47.22% 45.33% 40.28% 44.82% 45.24% 39.20% 72.55% 70.19% 72.54% 72.46% 72.21% 69.55% all three subjects and generates coherent, natural video. iv) In the bottom-right (multiple object references), Kling1.6 produces incorrect patterns on the cup, while Phantom and VACE generate correct cup textures but omit the ashtray, with VACE also showing temporal discontinuity. Saber, on the other hand, generates all referenced objects correctly and maintains smooth, consistent video quality."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "We conduct series of ablation studies to analyze the key components of Saber, including the masked training strategy, mask generator, mask augmentation, and attention mask in the attention mechanism. The Effect of Masked Training. To evaluate the effect of masked training, we finetune our model on the OpenS2V-5M (Yuan et al., 2025a) dataset using the same architecture. As shown in Tab. 2, masked training improves the total score by +1.67%, indicating that it strengthens subject representation learning and reduces overfitting to specific reference cues. In Tab. 2, we first analyze the mask type by training the model using only one The Effect of Mask Generator. type at time. Using only ellipse, Fourier, or polygon masks reduces total score by 3.35%, 1.58%, and 1.42%, respectively, whereas combining all types yields the best results, showing that mask diversity is crucial for masked training. We also fix the foreground area ratio to 0.3, which leads to 6.18% drop, indicating that restricting mask variation limits generalization. The Effect of Mask Augmentation. We evaluate the effect of mask augmentation by training the model with and without it. As shown in Fig. 5, without augmentation, the model exhibits severe copypaste artifacts, directly placing the T-shirt upright on the rock. With augmentation (rotation, scaling, flipping, and shearing), the T-shirt naturally lies on the rock surface, resulting in more realistic and coherent compositions. This demonstrates that geometric diversity in masking is crucial for natural video generation. Figure 5 Effect of mask augmentation. Without mask augmentation, the model shows copy-paste artifacts by directly copying reference content. Applying augmentation enables more natural and coherent video generation. The Effect of the Attention Mask. Finally, we examine the role of the attention mask in our attention mechanism, which constrains reference-video token interaction. As shown in Fig. 6, removing the attention mask introduces visible gray artifacts around the subject regions, as the model fails to correctly extract subjects from masked reference images (e.g., gray areas behind sunglasses or clocks). Incorporating the attention mask effectively resolves these issues, leading to cleaner subject separation, smoother blending, and improved overall video quality. 9 Figure 6 Effect of the attention mask. Removing the attention mask introduces gray artifacts around subjects, while applying it ensures clean separation from the gray background and smoother, more natural video results."
        },
        {
            "title": "5.5 Emergent Abilities",
            "content": "In this section, we explore several interesting capabilities of Saber that emerged from its training strategy, demonstrating robustness beyond the standard R2V task. Single Subject Multiple Views. We test Sabers ability to handle multiple reference images corresponding to different views of the same subject. As shown in Fig. 7, we use the front, side, and back views of robot as reference inputs to Saber. The results show that Saber successfully understands that all reference images depict the same subject (the robot) and integrates the appearance feature from different views into single coherent video subject. Despite the robots complex surface details and wiring structure, Saber accurately captures and synthesizes these visual characteristics into the generated video. Figure 7 Qualitative results on multiple reference images of the same subject. Given the front, side, and back views of robot as reference, Saber correctly recognizes them as the same subject and integrates multi-view appearance features into coherent video, accurately preserving fine structural and surface details. Figure 8 Qualitative results of cross-modal alignment between reference images and text prompts. By swapping subject descriptions in the prompts (e.g., clothing color or subject positions), Saber accurately reflects the corresponding visual changes, demonstrating robust alignment between reference images and textual descriptions through its attention mechanisms. Cross-modal Alignment. We also evaluate the models alignment between reference images and text prompts by swapping subject descriptions and observing the corresponding video changes. As shown in Fig. 8, when altering prompts such as man wearing blue shirt seated (Type 1) and man wearing black vest seated (Type 2), Saber correctly aligns subjects with their descriptions. Similarly, when swapping the positions of man and woman in the second case, the model accurately reflects the change. This demonstrates that, as described in Sec. 4.2, the interaction between video and reference tokens through self-attention, followed by cross-attention with text prompt features, enables robust reference image-text alignment."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Saber, scalable zero-shot framework for reference-to-video generation that eliminates the need for explicitly R2V datasets. Trained solely on large-scale video-text pairs, Saber leverages masked training strategy, tailored attention mechanism, and mask augmentation to achieve identity-consistent, natural, and coherent video generation. It further scales to multiple references, supporting both multi-identity and multi-view inputs without additional data preparation or changes to the training pipeline. Extensive experiments on the OpenS2V-Eval benchmark demonstrate that Saber consistently outperforms methods trained on explicit R2V data. These results show that effective R2V models can be trained without dedicated datasets, paving the way for future research in scalable and generalizable reference-to-video generation. Limitations. While Saber achieves strong zero-shot performance and scalability, several limitations remain. First, R2V generation may collapse when the number of reference images increases significantly (e.g., 12), resulting in fragmented compositions where references are combined without coherent understanding. Second, Saber primarily focuses on identity preservation and visual coherence, while fine-grained motion control and temporal consistency under complex prompts remain challenging. Future work can explore more effective integration of numerous reference images into unified video generation, as well as adaptive guidance to further improve controllability and realism in reference-to-video generation."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In CVPR, 2024a. Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. In CVPR, 2025a. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In CVPR, 2024b. Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, et al. Phantom-data: Towards general subject-consistent video generation dataset. arXiv preprint arXiv:2506.18851, 2025b. Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. Jiayi Gao, Changcheng Hua, Qingchao Chen, Yuxin Peng, and Yang Liu. Identity-preserving text-to-video generation via training-free prompt, image, and guidance enhancement. arXiv preprint arXiv:2509.01362, 2025a. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025b. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024a. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024b. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025a. Teng Hu, Zhentao Yu, Zhengguang Zhou, Jiangning Zhang, Yuan Zhou, Qinglin Lu, and Ran Yi. Polyvivid: Vivid multi-subject video generation with cross-modal interaction and enhancement. In NeurIPS, 2025b. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In ICCV, 2025. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, and Zehuan Yuan. Bindweave: Subject-consistent video generation via cross-modal integration. arXiv preprint arXiv:2510.00438, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. In ICCV, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR, 2023. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, and Bo Zheng. Identity-preserving image-to-video generation via reward-guided optimization. arXiv preprint arXiv:2510.14255, 2025. Kling Team. Kling1.6 elements to video. https://app.klingai.com/global/image-to-video/multi-id/new, 2025a. Pika Team. Pika2.1 consistent character video. https://pollo.ai/consistent-character-video, 2025b. ShutterStock Team. Shutterstock video dataset. https://www.shutterstock.com. Vidu Team. Vidu2.0 reference to video. https://www.vidu.com/ai-reference-to-video, 2025c. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 12 Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In CVPR, 2025. Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, and Alex Kot. Evolving storytelling: benchmarks and methods for new character customization with diffusion models. In ACM MM, 2024. Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, and Chen Li. Stand-in: lightweight and plug-and-play identity control for video generation. arXiv preprint arXiv:2508.07901, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2v-nexus: detailed benchmark and million-scale dataset for subject-to-video generation. arXiv preprint arXiv:2505.20292, 2025a. Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. In CVPR, 2025b. Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. arXiv preprint arXiv:2401.03407, 2024. Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Nanxuan Zhao, Jing Shi, and Tong Sun. Sugar: Subject-driven video customization in zero-shot manner. arXiv preprint arXiv:2412.10533, 2024a. Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. In NeurIPS, 2024b."
        }
    ],
    "affiliations": [
        "Kings College London",
        "Meta AI"
    ]
}