{
    "paper_title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "authors": [
        "Ruining Li",
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Andrea Vedaldi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 7 7 6 2 2 . 3 0 5 2 : r DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness Ruining Li Chuanxia Zheng Christian Rupprecht Andrea Vedaldi Visual Geometry Group, University of Oxford {ruining, cxzheng, chrisr, vedaldi}@robots.ox.ac.uk ruiningli.com/dso Image-to-3D (TRELLIS) Image-to-3D with DSO (ours) Input images Real 3D prints Figure 1. Top-left: state-of-the-art image-to-3D model like TRELLIS often fails to reconstruct 3D objects that can stand under gravity, even when prompted with images of stable objects (e.g., bottom-left). Top-right: Our method, DSO, improves the image-to-3D model via Direct Simulation Optimization, significantly increasing the likelihood that generated 3D objects can stand, both in physical simulation and in real life when 3D printed (bottom-right). The method incurs no additional cost at test time and thus generate such objects in seconds."
        },
        {
            "title": "Abstract",
            "content": "Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be selfsupporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), framework to use the feedback from (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct dataset of 3D objects labeled with stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feedforward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs. 1. Introduction Given single image of an object that is stable under gravity, we consider the problem of reconstructing it in 3D. Recent image-to-3D reconstructors [24, 40, 44, 45, 55, 67, 76, 8587, 89, 95, 101, 112, 113] have focused on improving the quality of the objects 3D geometry and appearance, but not necessarily their physical soundness. As shown in Fig. 2, when prompted with an image of an object which is stable, state-of-the-art generators like TRELLIS [101] and Hunyuan3D 2.0 [87] often fail to produce stable object. The failure rate is 15% even for objects seen during training and increases significantly for new objects, such as 1 the clock and the motorcycles in Fig. 1. Stability is common property of natural and man-made objects and is important in many applications, such as fabrication and simulation [35, 59]. It is, therefore, important to be able to reconstruct 3D objects that satisfy this property. Works that have explored the generation of physically sound 3D objects [56, 61, 104] have focused on specific object categories, such as furniture. More recent methods like Atlas3D [7] and PhysComp [21] tackle broader range of object categories. Both methods optimize 3D model, either from scratch [7] or from the output of an off-the-shelf 3D generator [21], using differentiable physics-based losses that reward stability. To compute these losses, they require differentiable simulators such as [26, 52], which, despite continuous improvements, remain slower and numerically less stable than non-differentiable simulators like [53, 88]. Because of this, Atlas3D and PhysComp are slow and susceptible to local optima and numerical instability. In this paper, we thus aim to improve feed-forward 3D generator so that it directly outputs physically stable objects without having to correct them at test time. naive approach is to use losses similar to those proposed by Atlas3D and PhysComp for feed-forward training instead of test-time optimization, but this would still require differentiable simulator. Instead, inspired by works on aligning generative models with human preferences [71, 90], we introduce Direct Simulation Optimization (DSO). This simple and effective approach fine-tunes 3D generator by aligning it with the preference provided automatically by an off-the-shelf physics simulator. With this, we explore three research questions: (1) how to use this simulation preference dataset to efficiently fine-tune 3D generator; (2) how to construct such dataset without requiring ground-truth 3D data; and (3) whether the fine-tuned generator generalizes well, outputting physically sound 3D objects from image prompts unseen during training. Our motivation for looking at reward optimization is that stability, like many other physical attributes of an object, is discrete: either an object is stable, or it collapses under gravity. Stability does not distinguish between unstable states regardless of how close they are to becoming stable and is thus difficult to optimize using techniques like gradient descent. In contrast, it is easy to tell if an object is stable or not by using physics simulator. Hence, we reformulate the problem as reward-based learning task, where we reward stable outputs and penalize unstable ones. Inspired by direct preference optimization (DPO) [71], we propose an alternative objective, direct reward optimization (DRO), for aligning diffusion models with external preferences. Notably, DRO does not require pairwise preference data for training. Our second contribution is to show that we can derive the reward signals solely from generated data, which can remove the need to collect many stable 3D objects for training at scale. We do so by generating new 3D assets using the 3D generator itself. These generated 3D assets are then evaluated within physics simulator, classifying them as stable or unstable. This process allows us to construct fully automated self-improving pipeline, where the model is trained on its own output assessed by physics simulator rather than relying on large dataset of 3D objects. We show that when integrated with either DPO or DRO as the objective function, our Direct Simulation Optimization framework can steer the output of the 3D generator to align with physical soundness. The final model surpasses previous approaches for physically stable 3D generation on existing evaluation benchmarks [21]. It operates in feedforward manner at test time, outperforming heavily engineered solutions like [7, 21] that perform test-time optimization, both in terms of speed and probability of generating stable object as output. The model also generalizes well to images collected in the wild  (Fig. 1)  . Our experiments show that, in our setting, the proposed DRO objective achieves faster convergence and superior alignment compared to DPO, suggesting that it may be better candidate for diffusion alignment in general. While our study focuses on stability under gravity, there is nothing special about this particular attribute; the reward-based approach and the selfimproving optimization strategy can work, in principle, for any physical attributes that can be assessed via simulator. 2. Related Work 3D generation and reconstruction. Early 3D generators used generative adversarial networks (GANs) [20] and various 3D representations such as point clouds [28, 41], voxel grids [98, 103, 115], view sets [60, 66], NeRF [4, 5, 13, 63, 75], SDF [17], and 3D Gaussian mixtures [97]. However, GANs are difficult to train on large scale in an open world setting. This explains why recent methods have switched to diffusion models [23, 79] while still using the same 3D representations [9, 51, 58, 62, 77, 83], improving training stability and scalability. Others have trained neural networks [6, 8, 27, 29, 30, 39, 82, 84, 99, 100, 106, 107] to directly regress 3D models from 2D images. Authors have also explored scaling 3D reconstruction models [24, 85, 94] on Objaverse [11, 12], improving generalization. DreamFusion [68] and SJC [91] use large-scale image/video generators for 3D generation using score distillation [29, 40, 55, 68, 91, 93, 116]. The works of [18, 22, 36, 45, 47, 48, 54, 76, 86, 96, 113] fine-tune these models for generalizable 3D generation. More recently, authors have introduced latent 3D representations [10, 101, 111] whose distribution can be modeled effectively by denoising diffusion or rectified flow [1, 42, 46]. CLAY [112] and TRELLIS [101] are among the 3D generators trained in this manner, producing superior results com2 pared to methods that piggyback on 2D generation. These advances have significantly improved the quality of the geometry and appearance of the generated 3D assets, but not necessarily their physical soundness. This limitation reduces their utility in downstream applications like fabrication and simulation. In contrast, we propose 3D generation approach that explicitly optimizes physical soundness in the guise of stability under gravity. Physically-sound 3D generation. Early studies have explored methods to predict physical properties from images and videos, like mass [81], shadows [92], materials [109], occlusions [110], and support [78]. While effective in predicting specific physical parameters, these methods do not immediately generalize to 3D reconstruction. Recent works like Physdiff [108], PhysGaussian [102], and PIE-NeRF [15] extended physics-based rendering [26] to NeRF [57] and 3D Gaussian Splatting [32]. These methods focus on modeling the motion of objects instead of their stability under gravity. Like our work, PhysDeepSDF [56], PhyScene [104], and PhyRecon [61] incorporate explicit physical constraints in 3D reconstruction. However, these methods focus on specific object categories, such as furniture. More related to our work, Atlas3D [7] and PhysComp [21] are not restricted to specific categories; instead, they resort to test-time optimization using carefully designed differentiable, physics-based losses. We solve similar problem, but in feed-forward manner using reward-based optimization without requiring to optimize fragile and slow physics-based losses at test time. Preference alignment in generative models. The Direct Simulation Optimization (DSO) framework we propose can be trained using direct preference optimization (DPO) [71], technique originally developed for fine-tuning large language models. Diffusion-DPO [90] first extended DPO to vision diffusion models, enabling direct optimization of human preferences, and was further extended by [16, 43]. While various preference alignment approaches exist [2, 14, 34, 65, 69], DPO has the distinct advantage of not requiring to consult an oracle to compute the reward signal during training, and eschews reward modeling. In this paper, inspired by DPO, we also propose an alternative objective named direct reward optimization (DRO), which does not require pairwise preference data to align the generator. 3. Method Given pre-trained diffusion-based 3D generator pref that takes single image as input and generates 3D assets x0 pref(x0I), our goal is to learn new model pθ that produces more physically sound generations than pref. We assume access to an oracle that, given sample x0, can output o(x0) {0, 1}, indicating whether x0 is physically sound. In this paper, we focus on stability under gravity, Figure 2. State-of-the-art 3D generators cannot robustly produce stable objects. Even when taking images of stable objects in their training set as input, TRELLIS [101] and Hunyuan3D-2.0 [87] generate about 30% and 15% unstable assets respectively. where is computed by physical simulator to determine whether 3D model x0 is self-supporting or not. 3.1. Challenges of Optimizing Physical Soundness To improve the physical soundness of the generated samples, one approach would be to fine-tune the model with the following objective: max θ EII,x0pθ(x0I) [o(x0)] βDKL [pθ(x0I)pref(x0I)] , (1) where is (the empirical distribution corresponding to) dataset of image prompts, and β is hyperparameter trading-off the two terms. The first term encourages the generated object x0 from pθ(x0I) to be physically sound, while the second term constrains the distribution to remain close to the base model to ensure that the generated geometry remains faithful to the input image I. One challenge in optimizing Eq. (1) is that the oracle is non-differentiable. One approach to address this issue is to reframe the denoising process as multi-step Markov decision process (MDP) [2] and optimize Eq. (1) using reinforcement learning (RL) [73, 74]. However, in our setting, evaluating is computationally expensive due to the need to run physical simulation and the overhead introduced by decoding latent 3D representations x0 into simulationready asset. The decoding process of state-of-the-art 3D generators involves querying dense 3D grid points and extracting 3D mesh with marching cubes [87, 112], and may even require inference of another geometry generator [101]. These factors make optimization of Eq. (1) via RL computationally prohibitive. 3.2. Formulation as Reward Optimization We aim to reformulate the objective function to be easier to optimize, specifically eliminating the need to evaluate during training, while still preserving the intended goals 3 Figure 3. Overview of Direct Simulation Optimization (DSO). Left: Starting from set of (potentially synthetic) image prompts, we task the base model pref to generate 3D models. Each model is augmented with binary stability label through physics-based simulation (Sec. 3.3). Middle: Using this dataset, we fine-tune the base model by reinforcing stable samples and discouraging unstable ones. Our objective formulation enables efficient training via gradient descent without pairwise preferences (Sec. 3.2). Right: At test time, the finetuned model can generate self-supporting objects when conditioned on (out-of-distribution) images of stable objects captured in the wild. of Eq. (1). This is analogous to the goal of text-to-image diffusion model alignment in Diffusion-DPO [90]. In both cases, the reward signal (i.e., evaluation of by simulation or by collecting human preferences for [90]) is hard to obtain in scalable way at training time. Following Diffusion-DPO [90], we can re-parameterize o(x0) using the optimal reverse diffusion process, modeled by θ(x0:T ), that maximizes (a lower bound of) Eq. (1): there exists choice of β that leads to the same optimum θ as with the original oracle in Eq. (1). Since we aim to use stochastic gradient descent, which is local and continuous, to optimize L, we may as well choose and in way that within the training compute budget, the sum of log Z(I) and the expectation over x1:T is bounded within ( β ). By doing so, we can remove the absolute value in Eq. (3) and get rid of the terms independent of pθ: β , (cid:21) + β log Z(I) arg min = arg minEII,x0XI ,x1:T pθ(x1:T x0,I) (cid:20) (4) (1 2o(x0)) log (cid:21) pθ(x0:T I) pref(x0:T I) o(x0) = βEpθ(x1:T x0,I) (cid:20) log θ(x0:T I) pref(x0:T I) (2) for any supp(I). Z(I) is normalizing term independent of pθ. The derivation follows [90] and is given in detail in Appendix A. Direct Reward Optimization (DRO). Given an image dataset and 3D models XI corresponding to each image I, we can then pre-compute o(x0) for each 3D model x0 XI to supervise pθ using Eq. (2), via an L1 loss: :=EII,x0XI o(x0) β (cid:18) (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) (cid:20) Epθ(x1:T x0,I) log (cid:21) pθ(x0:T I) pref(x0:T I) + log Z(I) (cid:21) . (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (3) However, despite being function of the trainable parameters θ, it is intractable because neither Z(I) nor the expectation over x1:T can be computed efficiently. To address this issue, we notice that the absolute value of o(x0) is arbitrary, i.e., we could have used another oracle o(x0) {l, u} which evaluates to for unstable x0 and for stable x0 as long as < in Eq. (1). In this setting, 4 To make sampling tractable, we approximate the reverse process pθ(x1:T x0, I) with the forward process q(x1:T x0), following [90]. With some algebra, this yields: LDRO = EII,x0XI ,tU (0,T ),xtq(xtx0) (cid:21) , w(t)(1 2o(x0))ϵ ϵθ(xt, t)2 2 (cid:20) (5) where ϵ (0, I) is draw from q(xtx0) and w(t) is weighting function. LDRO directly encourages the model to improve at denoising samples x0 with high reward (i.e., o(x0) = 1) and to denoise less well samples x0 with low reward (i.e., o(x0) = 0). We hence dub it direct reward optimization (DRO). Different from the DPO formulation [90] (which we briefly review next), fine-tuning with LDRO does not require pairwise preference data and does not query the base model ϵref during training, potentially applicable to more alignment settings than DPO. Direct Preference Optimization (DPO). Alternatively, assuming XI contains both stable and unstable models, we can use the objective introduced in [90], which relies on pairwise preference data and minimizes contrastive loss: (cid:20) 0 ,xl 0)X 2 II,(xw 0 , xl LDPO := log sigmoid(r(xw 0 ) r(xl 0)) (6) where (xw 0) is pair of physically sound and unsound 3D models corresponding to the same image (i.e., o(xw 0) = 1), and is reward model introduced to derive the loss from the Bradley-Terry model [3]. Following the derivation in [90], this simplifies to: 0 ) = 1 o(xl LDPO = II,(xw 0)X 2 ,tU (0,T ),xw q(xw xw 0 ),xl tq(xl txl 0) 0 ,xl (cid:18) log sigmoid βT w(t)( ϵw ϵθ(xw (cid:0)ϵl ϵθ(xl , t)2 2 ϵw ϵref(xw , t)2 2 t, t)2 2 ϵl ϵref(xl t, t)2 2 (cid:19) (cid:1) (7) where ϵw, ϵl (0, I) are two independent random draws. Please refer to [90] for details. 3.3. DSO with Generated Data 1 with Eq. (5) We can now fine-tune the generator pθ or Eq. (7) as the objective using stochastic gradient descent. The final cornerstone of our framework, Direct Simulation Optimization (DSO), is to obtain set of images and their corresponding 3D models XII. Procuring large number of stable 3D objects for training at scale is challenging, especially if we want multiple different objects for single image prompt as in Eq. (7). Instead, we propose scheme that leverages the 3D models generated by the generator pref itself. As illustrated in Fig. 3, we first curate large, diverse image dataset I. These images can be either renderings of existing 3D datasets such as [11, 12], or synthetic images generated by 2D generator such as [33, 72]. We then task the base model pθ to create 3D models XI , taking individual images as input. These 3D models, subsequently augmented with physical soundness scores via physics-based simulation, are used to fine-tune the model for enhanced physical soundness, achieving self-improvement without relying on 3D ground truths. 4. Experiments We evaluate DSO on the task of generating physically stable 3D models under gravity and compare it to prior works 1While our presentation in Sec. 3.2 focuses on DDPM-formulated diffusion model with discrete timesteps [23], the same approach can be readily adapted to rectified flow models [1, 42, 46] and other diffusion formulations [31, 80], as their differences primarily lie in the noise schedule and loss weighting [19]. 5 that use test-time optimization of physically-based losses (Sec. 4.2). We test the ability to generate stable 3D objects while also retaining the fidelity of the 3D reconstruction (as it would be trivial to make all objects stable by making them, e.g., cubes). We discuss the effect of DSO on the generated geometry in Sec. 4.3 and DSOs scaling behavior in Sec. 4.5. In Sec. 4.6, we demonstrate how DSO can be adapted to leverage exclusively synthetic 2D images instead of renderings of ground-truth 3D models. (cid:21) 4.1. Experiment Details Model and data. We apply our method DSO to finetune TRELLIS [101], state-of-the-art image-to-3D generator, and measure its ability to consistently generate self-supporting 3D models before and after optimization. TRELLIS contains two rectified flow transformers: the first is tasked with generating the coarse geometry of the 3D object, and the second its fine-grained details. In our experiments, we fine-tune the linear layers of the first transformer only, as we found that stability is mostly controlled by the coarse geometry. We use LoRA [25] to reduce the number of parameters to optimize. We select TRELLIS because it is state-of-the-art 3D generator and it is available as open source, but our method is not specific to this model. For the training data, we first generate large number of 3D models with TRELLIS, conditioned on Objaverse [12] renderings. We exclude objects from Objaverse with unstable ground-truth shapes and filter out low-quality ones following [101]. In addition, we include only objects categorized by GObjaverse [70] as Human-Shape, Animals, or Daily-Used, as these categories often feature two-legged shapes and tall, slender structures, making them more challenging to stabilize under gravity. We render 6 images for each of the remaining 13k objects and generate 4 different models per image, yielding 312k 3D models in total. We then use the MuJoCo [88] simulator to conduct physical simulations for each model, starting from an upright pose on flat ground. We use its tilting angle at the final equilibrium state to determine its stability, based on hard cut-off of 20: model x0 is considered stable (i.e., o(x0) = 1) if its tilting angle is below 20 and unstable otherwise. During training, we sample models for an image prompt uniformly at random. Training. We use AdamW [49] to fine-tune the base model using LoRA [25] (rank 64) with batch size 48 on 4 NVIDIA A100 GPUs. We train two separate models, optimizing them using LDRO (Eq. (5)) for 4, 000 steps and using LDPO (Eq. (7)) for 8, 000 steps, respectively. The β in Eq. (7) is set to 500. More training details can be found in Appendix B. Evaluation. We evaluate on the dataset as in [21], which consists of 100 Objaverse [12] objects from plants, animals, and characters. We exclude the 35 objects whose groundMethod Stability Geometry % Stable (% Output) Rot. CD F-Score Full evaluation set (65 objects) TRELLIS [101] Atlas3D [7] TRELLIS + DSO (w/ LDPO) 95.1 (100) 0.0480 TRELLIS + DSO (w/ LDRO) 99.0 (100) 1.88 0.0440 85.1 (100) 14.14 0.0485 69.4 (95.4) 32.86 5.42 73.12 73.62 76.17 Partial evaluation set (11 unstable objects) 54.5 (100) 39.18 0.0529 TRELLIS [101] 80.3 (46.2) 18.14 0.0698 TRELLIS + PhysComp [21] TRELLIS + DSO (w/ LDPO) 82.6 (100) 16.83 0.0509 TRELLIS + DSO (w/ LDRO) 95.5 (100) 5.58 0.0520 72.48 53.73 73.07 73.61 Table 1. Quantitative Results. DSO fine-tuned models (using either LDRO or LDPO) significantly outperform baseline methods Atlas3D [7] and PhysComp [21] in both physical stability and geometric quality. Beyond improving the physical soundness of the base model TRELLIS [101], DSO also slightly improves its geometric fidelity without requiring ground-truth 3D supervision. truth shape is not self-supporting and render 12 images for each of the remaining objects, resulting in final set of 65 objects and 780 images. These objects are removed from our training set. Metrics. For quantitative results, we report the following stability measures: % Output counts the frequency of successfully outputting 3D object, regardless of its stability; % Stable counts the percentage of stable assets among those generated; Rotation angle (Rot. in short) measures the average tilting angle of generated objects at their equilibrium state. In addition, to evaluate the mesh geometry, we report Chamfer Distance (CD) and F-Score (with threshold 0.05 [44, 55, 94]). Following common practices [44, 55, 94], we scale the meshes to fit within the unit cube and align the generated meshes optimally with the ground truths using Iterated Closest Point (ICP) before computing CD and F-Score. Baselines. In addition to our base model TRELLIS [101], two baseline methods designed to genwe consider Atlas3D [7] and erate self-supporting 3D objects: PhysComp [21]. Atlas3D is text-to-3D framework that combines score distillation sampling [68] with physically based loss terms, primarily the magnitude of the object orientation change at equilibrium, computed via differentiable simulation. PhysComp takes (volumetric) tetrahedral mesh as input and applies test-time optimization to improve its physical soundness, including its stability under gravity. This is achieved by encouraging the projection of the center of mass to be within the convex hull of the contact points. For [7] and [21], we use their official implementation. For the text-conditioned Atlas3D, we prompt it 6 using captions of our multi-view renderings, obtained with GPT-4V [64]. We generate one asset per object in the evaluation set. For PhysComp, we task it to optimize the 3D models generated by TRELLIS. Since the optimization on our hardware (24-core CPU with 668 GiB RAM in total) takes significantly longer (on average 15 minutes) than the 80 seconds reported by the authors, we only run it on an 11object subset whose renderings lead TRELLIS to generate unstable 3D models, amounting to 11 12 = 132 runs. As the optimization time varies dramatically with mesh complexity, we set strict time budget of 30 minutes per run. 4.2. Results and Analysis Quantitative results. Table 1 reports the quantitative results evaluated for both baselines and our method. Notably, our DSO fine-tuned TRELLIS (using either LDRO or LDPO) outperforms all baselines on both physical stability and geometry fidelity without any test-time optimization. Qualitative results. Figure 4 presents qualitative comparisons with baselines, highlighting cases where our base model TRELLIS [101] fails to generate self-supporting assets. Atlas3D [7], inheriting the limitations of SDS-based approaches [68], often suffers from over-saturation and over-smoothness (a, b, c). While incorporating physicsbased stability loss, its optimization remains unreliable (a) and can introduce structural artifacts such as extraneous limbs (b, c). PhysComp [21], which refines TRELLIS outputs, does not preserve texture and can distort the original shape (a), compromising faithfulness to the input image. The method struggles to stabilize meshes in challenging scenarios (a) and frequently suffers from numerical instabilities, sometimes failing to generate outputs entirely (c). In contrast, our final model leverages the strong geometric prior of TRELLIS while significantly enhancing physical stability without introducing additional computational overhead at test time. Analysis. Our results yield several observations: (1) Differentiable simulation often suffers from numerical issues, as reflected by the lower % Output of [7] and [21], due to the need for differentiable ODE solving. DSO circumvents this requirement by framing physical soundness optimization as reward learning task (Sec. 3.2) and augmenting 3D models with simulation feedback before training. (2) Unlike visual quality, physical stability demands high accuracy, especially in the contact region. While existing efforts to align vision generators [16, 43, 69, 90, 114] focus on enhancing visual quality, we show that alignment can also substantially improve accuracy-sensitive metrics. (3) For our task, LDRO proves to be more effective objective than LDPO (Tab. 1) and could also be beneficial in other diffusion alignment settings, especially when access to pairwise preference data is limited. Figure 4. Qualitative Comparison with baseline methods. Our model can more reliably generate 3D assets that are stable under gravity and faithful to the conditioning images. Method Stability Geometry % Stable Rot. CD F-Score TRELLIS [101] TRELLIS + SFT TRELLIS + DSO w/ LDPO TRELLIS + DSO w/ LDRO 85.1 89.5 95.1 99.0 14.14 0.0485 10.22 0.0440 5.42 0.0480 1.88 0.0440 73.12 76.17 73.62 76. Table 2. Comparison with Supervised Fine-tuning (SFT). SFT yields faithful geometry, but its samples are less physically stable. (quantified by the tilting angle at equilibrium), as shown in Fig. 5. The correlation is not statistically significant, suggesting that improving physical soundness does not need to compromise geometric quality. If anything, there is very slight positive correlation between the two. 4.4. Comparison with Supervised Fine-tuning To further assess the effectiveness of DSO, we also compare it with supervised fine-tuning (SFT) in Tab. 2. For SFT, we fine-tune TRELLIS on the stable subset of our constructed dataset (i.e., {x0 o(x0) = 1}, consisting of 72k objects out of the 312k generated in total), using the rectified flow objective [1, 42, 46] with the same hyperparameter configuration as our main training runs for 8,000 steps. While SFT yields better geometry, its samples are less physically sound. This suggests the model prioritizes geometry over physical plausibility, making fine-tuning 3D generators solely on physically stable objects less effective for aligning physical soundness. In contrast, by exposing Figure 5. (Lack of) Correlation between geometry reconstruction quality (Chamfer Distance) and stability (rotation angle). 4.3. Physical Soundness vs. Geometry Quality Eq. (1) and the other losses in Sec. 3 imply potential tradeoff between physical soundness and geometric quality, controlled by the parameter β. However, in Tab. 1, DSO finetuned on TRELLIS not only enhances physical stability but also improves geometric fidelity. This outcome is somewhat surprising, given that TRELLIS was explicitly trained on (at least some) objects in the evaluation with geometric losses (e.g., occupancy), whereas our DSO does not directly supervise the base model with ground-truth geometry. To investigate this, we generate 800 distinct 3D assets using TRELLIS and analyze the relationship between their geometric quality (measured by CD) and physical stability 7 (a) Scaling with training compute. Longer training enhances the physical stability further, yet excessive training degrades geometric quality. (b) Scaling with training data. Smaller ( 1 achieve comparable results for physical alignment. 16 of the full dataset) data Figure 6. Scaling Behaviors of DSO with training compute (left) and data (right). the model to both stable and unstable objects, DSO encourages the model to better focus on physical properties. Method Synth. Loss 4.5. Scaling Behaviors We study how DSO scales when optimizing LDPO. Scaling with training compute. Fig. 6a illustrates the progression of evaluation metrics throughout training. While longer training further enhances the physical stability measure, excessive training with DSO significantly degrades geometric quality. In particular, the model eventually cheats by generating flat structure beneath the 3D asset as base to prevent it from toppling over. Scaling with training data. In Fig. 6b, we analyze the impact of training data size on model performance. We train 6 models with identical hyperparameters as our main training run, progressively reducing the amount of data exposed to each model. The smallest dataset used is only 1 64 of the full dataset, constructed as described in Sec. 4.1. While training on extremely small datasets leads to model collapse, we find that using just 1 16 of the full dataset (equivalent to 19.2k synthetic 3D models with simulation feedback) already produces results comparable to our main training run. This suggests that aligning state-of-the-art 3D generators with physical soundness requires only modest amount of preference data. This is promising for aligning other physical properties, such as 3D scene decomposition [61, 104, 105] and part articulation [37, 38, 50], for which obtaining positive samples may be more challenging due to their rarity. 4.6. DSO without Real Data Our training objective does not rely on ground-truth 3D data for supervision. Nevertheless, in our main experiments presented in Sec. 4.2, we used Objaverse renderings as prompts to construct preference dataset. Here, we show that access to Objaverse models is not necessary. We substitute the renderings with object-centric synthetic images to condition the base model TRELLIS to generate 3D models. We then TRELLIS [101] TRELLIS + DSO LDPO LDPO TRELLIS + DSO TRELLIS + DSO LDRO LDRO TRELLIS + DSO Stability Geometry % Stable Rot. CD F-Score 85.1 93.5 95.1 97.6 99.0 14.14 0.0485 73.12 6.92 0.0483 73.40 5.42 0.0480 73.62 3.17 0.0455 76.05 1.88 0.0440 76.17 Table 3. DSO can be trained solely on synthetic data. The resulting models achieve greater physical soundness than the base model. evaluate the physical stability of these generated models using simulation feedback, assigning binary preference label which we use for DSO fine-tuning. In more detail, we task GPT-4 [64] to generate 1,000 diverse prompts of detailed object descriptions and use them to prompt FLUX [33], an open-source text-to-image model, to generate synthetic images. We then obtain in total 64k generated 3D assets, on which we conduct physical simulation as detailed in Sec. 4.1. The performance of the model trained on this dataset is reported in Tab. 3. Despite the larger domain gap, the fine-tuned model generalizes well to the evaluation images and is more likely than the base model TRELLIS to generate stable assets under gravity. Without relying on physically sound 3D datasets, this self-improving scheme can be extended to enhance other physical attributes that can be evaluated via simulation for 3D generators. 5. Conclusion We presented DSO, new framework to generate physically sound 3D objects by leveraging feedback from physics simulator. We use dataset of 3D objects labeled with stability scores obtained from the simulator, potentially starting from entirely synthetic images. We fine-tune the base generator using objective DPO or DRO, the latter of which we introduced. The resulting feed-forward generator is much faster and more reliable in producing stable objects compared to test-time optimization methods. 8 in part Acknowledgments. This work is supported by Toshiba Research Studentship, EPSRC SYN3D EP/Z001811/1, and ERC-CoG UNION 101001212. We thank Minghao Guo and Bohan Wang for providing us with the evaluation set in their work [21], and Mariem Mezghanni for insightful discussions during the early stages of this project. We also thank Zeren Jiang, Minghao Chen, Jinghao Zhou and Gabrijel Boduljak for helpful suggestions."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. 2, 5, 7 [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3 [3] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952. 5 [4] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 2 [5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In CVPR, 2021. 2 [6] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from imIn age pairs for scalable generalizable 3d reconstruction. CVPR, 2024. [7] Yunuo Chen, Tianyi Xie, Zeshun Zong, Xuan Li, Feng Gao, Yin Yang, Ying Nian Wu, and Chenfanfu Jiang. Atlas3d: Physically constrained self-supporting text-to-3d for simulation and fabrication. In NeurIPS, 2024. 2, 3, 6, 14 [8] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, 2024. 2 [9] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. In NeurIPS, 2024. 2 [10] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. 2 [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: universe of 10M+ 3D objects. In NeurIPS, 2023. 2, 5 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, 9 Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 2, 5 [13] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for 3d-aware image generation. In CVPR, 2022. 2 [14] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK: Reinforcement learning for fine-tuning text-toimage diffusion models. In NeurIPS, 2023. 3 [15] Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pie-nerf: Physics-based interactive elastodynamics with nerf. In CVPR, 2024. 3 [16] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. 3, [17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. NeurIPS, 2022. 2 [18] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in NeurIPS, 2024. 2 [19] Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin P. Murphy, and Tim Salimans. Diffusion meets flow matching: Two sides of the same coin. 2024. 5 [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 2 [21] Minghao Guo, Bohan Wang, Pingchuan Ma, Tianyuan Zhang, Crystal Owens, Chuang Gan, Josh Tenenbaum, Kaiming He, and Wojciech Matusik. Physically compatible 3d object modeling from single image. NeurIPS, 2024. 2, 3, 5, 6, 9, 14 [22] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. In ECCV, 2024. 2 [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 5 [24] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single imIn The Eleventh International Conference on age to 3d. Learning Representations (ICLR), 2024. 1, 2 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. In Lora: Low-rank adaptation of large language models. ICLR, 2022. [26] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fredo Durand. Difftaichi: Differentiable programming for physical simulation. In International Conference on Learning Representations (ICLR), 2019. 2, 3 [27] Zixuan Huang, Varun Jampani, Anh Thai, Yuanzhen Li, Stefan Stojanov, and James Rehg. Shapeclipper: Scalable 3d shape learning from single-view images via geometric and clip-based consistency. In CVPR, 2023. 2 [28] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. Pf-net: Point fractal network for 3d point cloud completion. In CVPR, 2020. 2 [29] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3D: Learning articulated 3d animals by distilling 2d diffusion. In 3DV, 2024. 2 [30] Angjoo Kanazawa, Shubham Tulsiani, Alexei Efros, and Jitendra Malik. Learning category-specific mesh reconstruction from image collections. In ECCV, 2018. 2 [31] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. NeurIPS, 2022. 5 [32] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), 2023. 3 [33] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5, [34] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textto-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. 3 [35] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning (CoRL). PMLR, 2023. 2 [36] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3D: Fast text-to-3D with sparse-view generation and large reconstruction model. In ICLR, 2024. 2 [37] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dragapart: Learning part-level motion prior for articulated objects. In ECCV, 2024. 8 [38] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Puppet-master: Scaling interactive video generation as motion prior for part-level dynamics. arXiv preprint arXiv:2408.04631, 2024. 8 [39] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, and Jiajun Wu. Learning the 3d fauna of the web. In CVPR, 2024. [40] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: Highresolution text-to-3d content creation. In CVPR, 2023. 1, 2 [41] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning efficient point cloud generation for dense 3d object reconstruction. In proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 32, 2018. 2 [42] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 2, 5, 7 [43] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 3, 6, [44] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. NeurIPS, 2023. 1, 6 [45] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 1, 2 [46] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 5, 7 [47] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2 [48] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In CVPR, 2024. 2 [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [50] Rundong Luo, Haoran Geng, Congyue Deng, Puhao Li, Zan Wang, Baoxiong Jia, Leonidas Guibas, and Siyuan Huang. Physpart: Physically plausible part completion for interactable objects. arXiv preprint arXiv:2408.13724, 2024. 8 [51] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, 2021. 2 [52] Miles Macklin. Warp: high-performance python framehttps : / / work for gpu simulation and graphics. github.com/nvidia/warp, 2022. NVIDIA GPU Technology Conference (GTC). 2 [53] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel High performance gpu-based State. arXiv preprint physics simulation for robot arXiv:2108.10470, 2021. 2 Isaac gym: learning. [54] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. In ICLR, 2024. 2 [55] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from single image. In CVPR, 2023. 1, 2, 6 [56] Mariem Mezghanni, Theo Bodrito, Malika Boulkenafed, and Maks Ovsjanikov. Physical simulation layer for accurate 3d modeling. In CVPR, 2022. 2, 3 [57] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 3 [58] Norman Muller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Nießner. Diffrf: Rendering-guided 3d radiance field diffusion. In CVPR, 2023. 2 [59] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. ArXiv, 2024. 2 [60] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of 3d representations from natural images. In ICCV, 2019. 2 [61] Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, and Siyuan Huang. Phyrecon: Physically plausible neural scene reconstruction. 2024. 2, 3, 8 [62] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2 [63] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, 2021. [64] OpenAI. Gpt-4 technical arXiv:2303.08774, 2023. 6, 8 report. arXiv preprint [65] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. 3 [66] Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, and Alexander Berg. Transformation-grounded image generation network for novel 3d view synthesis. In CVPR, 2017. 2 [67] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [68] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, 6 [69] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffuarXiv preprint sion alignment via reward gradients. arXiv:2407.08737, 2024. 3, 6 [70] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In CVPR, 2024. 5 [71] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. 2, 3 [72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 5 [73] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015. 3 [74] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [75] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in NeurIPS, 33, 2020. 2 [76] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3D generation. In ICLR, 2024. 1, 2 [77] Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In CVPR, 2023. 2 [78] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV. Springer, 2012. 3 [79] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML. pmlr, 2015. 2 [80] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In ICLR, 2021. 5 [81] Trevor Standley, Ozan Sener, Dawn Chen, and Silvio image2mass: Estimating the mass of an object Savarese. from its image. In CoRL, 2017. 3 [82] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. In 3DV, 2025. 2 [83] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion: (0-)image-conditioned 3d generative models from 2d data. In ICCV, 2023. [84] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR, 2024. 2 [85] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV. Springer, 2025. 1, 2 [86] Tencent Hunyuan3D Team. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation, 2024. 2 [87] Tencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. 1, 3 [88] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012. 2, 5, 14 [89] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In ECCV, 2024. [90] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caim11 rgb image. arXiv preprint arXiv:2502.12894, 2025. 8 [106] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. ShelfIn CVPR, 2021. supervised mesh prediction in the wild. 2 [107] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 2 [108] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In ICCV, 2023. [109] Albert Zhai, Yuan Shen, Emily Chen, Gloria Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang. Physical property understanding from languageembedded feature fields. In CVPR, 2024. 3 [110] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. What does stable diffusion know about the 3d scene? In NeurIPS, 2024. 3 [111] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM TOG, 2023. 2 [112] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM TOG, 2024. 1, 2, 3 [113] Chuanxia Zheng and Andrea Vedaldi. Free3d: Consistent novel view synthesis without 3d representation. In CVPR, 2024. 1, 2 [114] Zhenglin Zhou, Xiaobo Xia, Fan Ma, Hehe Fan, Yi Yang, and Tat-Seng Chua. Dreamdpo: Aligning text-to-3d generation with human preferences via direct preference optimization. arXiv preprint arXiv:2502.04370, 2025. 6 [115] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, and Bill Freeman. Visual object networks: Image generation with disentangled 3d representations. NeurIPS, 2018. [116] Thomas Hanwen Zhu, Ruining Li, and Tomas Jakab. Dreamhoi: Subject-driven generation of 3d humanobject interactions with diffusion priors. arXiv preprint arXiv:2409.08278, 2024. 2 ing Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. 2, 3, 4, 5, 6, 13 [91] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2023. 2 [92] Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann Heng. Single-stage instance shadow detection with bidirectional relation learning. In CVPR, 2021. 3 [93] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 2023. 2 [94] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In ECCV, 2024. 2, [95] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In ICLR, 2023. 1 [96] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023. 2 [97] Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, and Jan Eric Lenssen. latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction. In ECCV, 2024. 2 [98] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. NeurIPS, 2016. 2 [99] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning articulated 3d animals in the wild. In CVPR, 2023. 2 [100] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Unsupervised learning of probably symmetric deformable 3d objects from images in the wild. In CVPR, 2020. 2 [101] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 1, 2, 3, 5, 6, 7, 8, 13, [102] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In CVPR, 2024. 3 [103] Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, and Niki Trigoni. 3d object reconstruction from single depth view with adversarial learning. In ICCVW, 2017. 2 [104] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In CVPR, 2024. 2, 3, 8 [105] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an 12 A. Details of the Derivations From Eq. (1) to Eq. (2). As in [90], we introduce latent oracle defined on the whole denoising chain x0:T , such that: Then, starting from Eq. (1), we have: o(x0) = Epθ(x1:T x0) [O(x0:T )] . max θ max θ = max θ EII,x0pθ(x0I) [o(x0)] βDKL [pθ(x0I)pref(x0I)] EII,x0pθ(x0I) [o(x0)] βDKL [pθ(x0:T I)pref(x0:T I)] EII,x0:T pθ(x0:T I) [O(x0:T )] βDKL [pθ(x0:T I)pref(x0:T I)] =β max θ EII,x0:T pθ(x0:T I) (cid:20) log Z(I) log pθ(x0:T I) pref(x0:T I) exp(O(x0:T )/β)/Z(I) (cid:21) , where Z(I) = (cid:80) x0:T pref(x0:T I) exp(O(x0:T )/β) is normalizing factor independent of θ. Since (8) (9) EII,x0:T pθ(x0:T I) (cid:20) log pθ(x0:T I) pref(x0:T I) exp(O(x0:T )/β)/Z(I) (cid:21) with equality if and only if the two distributions are identical, the optimal unique closed-form solution: = DKL [pθ(x0:T I)pref(x0:T I) exp(O(x0:T )/β)/Z(I)] 0 (10) θ(x0:T I) of the right-hand side of Eq. (9) has Therefore, θ(x0:T I) = pref(x0:T I) exp(O(x0:T )/β)/Z(I). O(x0:T ) = β log Z(I) + β log θ(x0:T I) pref(x0:T I) (11) (12) for any supp(I). We can then obtain Eq. (2) by plugging Eq. (12) into Eq. (8). From Eq. (4) to Eq. (5). Since sampling from pθ(x1:T x0, I) is intractable, we follow [90] and replace it with q(x1:T x0): LDRO := min EII,x0XI ,x1:T q(x1:T x0) (cid:20) (1 2o(x0)) log (cid:21) pθ(x0:T I) pref(x0:T I) = min EII,x0XI ,x1:T q(x1:T x0) (1 2o(x0)) (cid:34) (cid:88) t=1 log pθ(xt1xt, I) pref(xt1xt, I) (cid:35) = min EII,x0XI ,tU (0,T ),xtq(xtx0),xt1q(xt1x0,xt) = min EII,x0XI ,tU (0,T ),xtq(xtx0) (cid:20) (1 2o(x0)) (cid:18) (cid:20) (1 2o(x0)) log pθ(xt1xt, I) pref(xt1xt, I) (cid:21) (13) DKL [q(xt1xt, x0)pθ(xt1xt, I)] DKL [q(xt1xt, x0)pref(xt1xt, I)] (cid:19)(cid:21) . Recall that for diffusion models pθ and pref, the distributions q(xt1xt, x0), pθ(xt1xt, I) and pref(xt1xt, I) are all Gaussian. Therefore, the KL divergence on the right-hand side of Eq. (13) can be re-parameterized analytically using ϵθ. After some algebra, and removing all terms independent of θ, this yields Eq. (5). B. Additional Training Details All hyperparameters are listed in Tab. 4. We did not extensively tune these parameters: the LoRA parameters and the β used in LDPO follow [43], and the rectified flow noise level sampling uses the distribution from TRELLIS [101]. 13 Loss formulation LDRO LDPO Optimization Optimizer Learning rate Learning rate warmup Weight decay Effective batch size Training iterations Precision LoRA Rank α Dropout AdamW 5 106 Linear 2, 000 iterations 0.01 48 4, 000 bf16 AdamW 5 106 Linear 2, 000 iterations 0.01 48 8, 000 bf 64 128 0 64 128 0 Method Alarm clock Motorcycle % Stable Rot. % Stable Rot. TRELLIS [101] TRELLIS + DSO 67.5 85.0 14.14 5.58 44.4 58. 46.53 36.75 Table 5. DSO enhances the models ability to generate assets that remain stable under gravity from in-the-wild images of stable objects. Miscellaneous Rectified flow sampling LogitNorm(1, 1) LogitNorm(1, 1) β in LDPO 500 Table 4. DSO training details and hyperparameter settings. Figure 7. DSO fine-tuned TRELLIS (ours) is more likely to generate physically sound 3D objects when conditioned on real-world images of challenging categories. C. Additional Evaluation Details For evaluation, the 3D models are generated by TRELLIS [101] and DSO fine-tuned TRELLIS using the default setting: 12 sampling steps in stage 1 with classifier-free guidance 7.5 and 12 sampling steps in stage 2 with classifier-free guidance 3. Under this setting, generating one model takes 10 seconds on average on an NVIDIA A100 GPU. By contrast, Atlas3D [7] takes 2 hours to generate model using SDS and PhysComp [21] takes on average 15 minutes to optimize one model output by TRELLIS on our hardware. We use Mujoco [88] for rigid body simulation for evaluation. The 3D models are assumed to be rigid and uniform in density. We run the simulation for 10 seconds, at which almost all objects have reached the steady state. D. Additional Results on In-the-Wild Images To assess the generalization of DSO fine-tuned models in generating physically sound 3D objects from real-world images, we curate set of 30 CC-licensed images for each category: stable alarm clocks and motorcycles supported by kickstands. We select these two categories because the base model, TRELLIS, struggles to generate physically stable versions of these objects. The results are reported in Tab. 5, with randomly sampled examples visualized in Fig. 7. As is evident, DSO enhances the models ability to generate assets that remain stable under gravity from in-the-wild images of stable objects. E. Limitations and Future Work DSOs self-improving scheme relies on the base model generating at least some positive samples, and hence may be less effective for base models where such samples are rare. DSO opens up new possibilities for integrating physical constraints into generative models, enhancing their applicability in real-world scenarios where adherence to such constraints is crucial."
        }
    ],
    "affiliations": [
        "Visual Geometry Group, University of Oxford"
    ]
}