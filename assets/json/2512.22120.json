{
    "paper_title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "authors": [
        "Shuoshuo Zhang",
        "Yizhen Zhang",
        "Jingjing Fu",
        "Lei Song",
        "Jiang Bian",
        "Yujiu Yang",
        "Rui Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types."
        },
        {
            "title": "Start",
            "content": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning Shuoshuo Zhangϕπ* Yizhen Zhangϕ* Jingjing Fuπ Lei Songπ Jiang Bianπ Yujiu Yangϕ Rui Wangπ π Microsoft Research ϕ Tsinghua University zss24@mails.tsinghua.edu.cn, yang.yujiu@sz.tsinghua.edu.cn, ruiwa@microsoft.com 5 2 0 2 6 2 ] . [ 1 0 2 1 2 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large visionlanguage models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across In this padomains, and incur high inference-time cost. per, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception BiPS first applies KL-consistency during training. constraint between the original image and an evidencepreserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types. Code is available at https://github.com/zss02/BiPS. 1. Introduction Large visionlanguage models (VLMs) are increasingly serving as unified interface for both visual and languagebased reasoning [1, 7]. Among real-world applications, visual question answering (VQA) is widely deployed, highimpact task: system must parse natural-language query, localize the pertinent visual evidence, and produce an answer whose reasoning remains tied to that evidence. Despite rapid progress, the perceptual capability of VLMs, including the identification, localization, and accurate reading *Equal contribution. Work done during internship at Microsoft. Corresponding author. Figure 1. Illustration of different paradigms. Within each colored box, the top row illustrates the training stage and the bottom row shows the inference stage. Prior approaches are limited by shape-rigid inference-time tools and domain-specific solutions that generalize poorly. of fine-grained visual cues is still bottleneck [2, 19, 33]. If perception slips, downstream reasoning can rely on incomplete or misleading cues, yielding plausible but evidencemismatched answers and degrading VQA performance. To mitigate this perceptual bottleneck, complementary line of work augments VLMs with external visual tools (e.g., cropping, masking, segmentation) that produce evidence-focused intermediate visual cues at inference time [11, 13, 23, 31, 40, 51, 55]. Recent efforts further collect step-by-step visual chain-of-thought traces, where the model is guided by intermediate boxes, tool-use trajectories, or auxiliary images and then trained to reproduce these visual reasoning steps [6, 11, 31, 48, 55]. In practice, such mechanisms improve grounding and answer accuracy across tasks such as chart understanding, image-based math problems, and natural-image VQA with sparse cues. Despite their utility, these approaches face three practical limitations. (i) Shape rigidity. Focused regions are typically rectangular crops or coarse masks, which miss irregular or fragmented evidence, such as thin polylines and intersections in charts, lesion contours in medical images, or nonconvex polygons in geometry diagrams. (ii) Scenariospecific solutions. Both custom tools and training pipelines that teach models to emit task-tailored latent visual tokens at inference time are tightly coupled to particular layouts or domains, limiting generalization. (iii) Inference-time overhead. Whether implemented via external tools or learned visual hints (e.g., boxes, masks, latent visual tokens), generating intermediate cues at inference introduces extra steps and computation and increases the risk of cascading errors. As illustrated in Fig. 1, we take different route: programmatically generating perfect, ground-truth visual cues not as inference-time crutches, but as training signals. Rather than teaching the model to output specific visual cues, we use these cues to shape the models internal policy, biasing it toward grounding its answers in visually supported content. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), two-stage training-time approach integrated into the Group Relative Policy Optimization (GRPO) framework [32], which shapes VLMs perception by pulling predictions toward an evidence-preserving view and pushing them away from an evidence-ablated view. BiPS first builds question-conditioned evidencepreserving view that keeps regions needed to answer the query while masking distractors, and applies consistency constraint based on the KullbackLeibler (KL) divergence to align the models predictions on this view with those on the original image, encouraging coarse but complete coverage of supporting pixels. It then constructs complementary evidence-ablated view that finely removes critical pixels so the answer changes or becomes unanswerable, and adds KL-based separation term that pushes predictions on the original and ablated views apart, discouraging textonly shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. This bidirectional shaping yields visually grounded decisions while requiring no extra annotations or customized parsers at inference. Realizing BiPS requires precise question-conditioned supervision in the form of paired views, including an evidence-preserving view and an evidence-ablated view for each image and question pair. Naive pixel-level masking or cropping is common, but it remains coarse and shape-constrained. Complex, multi-panel charts naturally carry the fine-grained evidence (e.g., thin polylines, layered marks, and small symbols), making charts rich source of training signals. We therefore build programmatic data pipeline for chart data that generates the required evidenceIn this work, we preserving and evidence-ablated views. instantiate the pipeline using ECD [47], synthetic corpus of complex multi-panel figures paired with executable rendering code. Because each figure is generated by code, every object (marks, layers, axes, legends) has explicit provenance, which enables exact edits to synthesize the two complementary views. This pipeline yields 13K synthetic training examples. Fine-tuned solely on this set, BiPS already generalizes well: across eight benchmarks spanning realworld figure datasets (e.g., CharXiv [41], ChartQAPro [25]) and out-of-domain general VQA (e.g., MathVista [21], MMStar [3]), average accuracy improves by +7.3% over the base model (Qwen2.5-VL-7B [1]). Adding 39k mathspecialized examples with standard GRPO further increases the average gain to 8.2%. The contributions are summarized as follows: We turn inference-time visual cues into training signals that internalize perception. BiPS applies KL consistency term toward an evidence-preserving view and KL separation term from an evidence-ablated view. These signals teach the model to capture visual evidence, including fine and irregular details, yielding evidencefaithful predictions without test-time overhead. We build programmatic data construction pipeline that uses executable chart scripts to synthesize precise paired views (preserve and ablate) without human labeling. Fine-tuned solely on chart-derived cues, BiPS extends well to real-world figures and general VQA, indicating strong out-of-domain generalization. Experiments show that BiPS delivers substantial, consistent performance gains, with strong cross-domain generalization and high data efficiency. Trained on only 13K chart samples, it boosts Qwen2.5-VL-7B by 7.3% on average across eight widely used chart and general VQA benchmarks (e.g., CharXiv, MathVista, MMStar), surpassing specialized models trained on vastly larger datasets. Adding 39K math-focused samples with GRPO further increases the average improvement to 8.2%. 2. Related Works 2.1. Multimodal Reasoning Recent advances in reinforcement learning (RL) for language models have sparked growing efforts to extend RL to multimodal reasoning [22, 52]. Recent studies apply RL to strengthen visual understanding or reasoning across domains, including single-image [5, 16, 28, 38, 46] reasoning for spatial perception, multi-image reasoning [53] for crossscene consistency, video reasoning [8] for temporal causality, and chart reasoning [4] for quantitative alignment. Complementary to advances in language reasoning, growing body of work improves VLMs by injecting intermediate visual cues at inference time. Typical Figure 2. Overview of the Bi-directional Perceptual Shaping (BiPS) framework. BiPS employs two-stage training curriculum built on the GRPO framework. Stage 1 (Consistency Stage) minimizes the KL-divergence (Lcons) between the original policy (πθ) and the policy on an evidence-preserving view (πθ). Stage 2 (Separation Stage) maximizes the KL-divergence (Lsep) between the original policy (πθ) and the policy on an evidence-ablated view, forcing the model to ground its reasoning in visual evidence. strategies highlight question-relevant content via bounding boxes [31], question-conditioned crops [23], or masks that suppress distractors [55], biasing predictions toward the appropriate visual evidence. Building on this idea, subsequent methods design task-aware tools or specialized modules for tighter evidence localization, such as chart-specific annotation tools [11, 51] and visual sketchpad [13]. Pushing this paradigm further, recent studies supervise models with labeled evidence and train them to reproduce coordinates, masks, or tool selections so that downstream answers can be conditioned on the predicted evidence [6, 11, 30, 31, 34, 35, 43, 55, 56]. However, these designs inherit several limitations: rectangular or coarse masks struggle with fine-grained or irregular structures, task-specific engineering reduces generalization, and multi-step pipelines introduce nontrivial inference overhead. recent work removes the need for explicit intermediate images by encouraging models to reason through latent internal processes [48], but such approach remain confined to specific tasks. ChiP [10] and PAPO [42] inject Gaussian noise or random masks as negative perturbations that penalize reliance on incorrect images to avoid visual hallucinations and text-only shortcuts, yet these methods overlook informative detailed signals inside the figure. 2.2. Chart Understanding And Reasoning Unlike general multimodal reasoning on natural scenes [9] or geometric diagrams [20, 21, 39], chart understanding targets structured quantitative graphics where numerical relations are encoded by axes and marks, demanding precise value perception and visualnumerical correspondence. Early chart-domain studies mainly focused on lowlevel perception tasks such as chart element detection and text extraction [24, 29], while recent benchmarks shift toward reasoning-centric tasks [25, 36, 41] that require interpreting implicit patterns, making comparisons, and executing multi-step computations over visualized data. To enhance such reasoning, subsequent studies convert charts into structured symbolic programs or code for executable reasoning [18, 45, 49, 54], while others explore multimodal feedback, reflective learning, and reinforcement-based optimization [4, 15, 26] to improve reliability. Nonetheless, chart reasoning still faces challenges such as limited chartto-code accuracy [24, 44], weak generalization to diverse layouts, and difficulty handling high-frequency curve fluctuations or subtle oscillatory patterns, which often cause quantitative misalignment. Addressing these challenges decomplex, multi-panel charts along with their executable rendering code. Charts serve as an ideal substrate for this purpose: their elements (marks, axes, legends) are semantically structured, fine-grained, and explicitly linked to the underlying code. This enables editing at the code level rather than pixels, yielding semantically precise and fine-grained control over visual evidence. As shown in Figure 3, our pipeline consists of the following three main stages. Question Reformulation and Validation. The original datasets reasoning questions are open-ended and difficult to verify by predefined rules. Following typical RLVR setting [28, 32], we refine this data by employing an auxiliary LLM arbitrator, GPT5-mini, to convert the original questions into multiple-choice format. This arbitrator is provided with the charts source code and metadata to ensure that the reformulated question remains answerable and that the ground-truth option is correct. This step provides verifiable supervision compatible with downstream RL training and ensures the overall quality of our base question set. Difficulty Filtering. To focus the training on non-trivial examples, we filter out questions that are too easy for the base model (Qwen2.5-VL-7B-Instruct [1]). We perform 8 rollouts for each validated question. Any question that the base model answers correctly in all rollouts is considered easy and is discarded from our training set, allowing our method to concentrate on more challenging reasoning tasks. Code Editing and Counterpart Rendering. This is the core stage where we generate the paired visual counterparts required for BiPS training. For each filtered question and its corresponding chart-rendering code C, we employ the LLM arbitrator guided by structured prompts to identify and modify the relevant code components. Evidence-Preserving View (Ipres): To construct the minimal evidence view used in the Consistency Stage, the arbitrator removes code segments unrelated to answering and executes the remaining script to render an image containing only the necessary visual elements. Evidence-Ablated View (Iabl): To create the complementary view for Separation Stage, the arbitrator identifies code segments that provide the key evidence and removes them while keeping general contextual structures such as axes, legends, and layout. The resulting image omits fine-grained cues yet preserves the global context. This programmatic process produces large-scale dataset of (I, q, Ipres, Iabl) tuples, providing semantically precise and well-aligned supervision for BiPS. After this pipeline, we obtained final set of 13K high-quality training samples (Detailed data statistics are available in Appendix). Figure 3. Overview of our data generation pipeline. This pipeline programmatically edits chart source code to generate the paired Evidence-Preserving (Ipres) and Evidence-Ablated (Iabl) views used for bi-directional training. mands finer-grained perception and robust visual reasoning. 3. Method To address the core perceptual failures of VLMs, namely their tendency to be distracted by irrelevant visual information and their inability to focus on fine-grained evidence, we propose BiPS. As illustrated in Figure 2, BiPS is implemented as two-stage training curriculum consisting of Consistency Stage followed by Separation Stage. The Consistency Stage guides the model to maintain consistent predictions when only evidence-relevant regions are retained, whereas the Separation Stage drives the model to diverge when the critical regions are removed. Both stages are implemented as bidirectional KL-based objectives, optimized within the GRPO [32] framework. 3.1. Programmatic Data Construction Pipeline To enable BiPS to learn evidence-aware grounding, we require training supervision that precisely distinguishes relevant from irrelevant visual content. Such supervision takes the form of paired visual viewsan evidence-preserving view that retains the regions needed to answer the question, and an evidence-ablated view where critical regions are removed. While masking or cropping images is common way to approximate such pairs, these operations are often coarse and shape-limited (e.g., rectangles), failing to capture fine-grained or irregular evidence. We overcome this limitation by constructing the paired views programmatically in the code domain. Our data generation pipeline builds upon ECD [47], which provides 3.2. Bi-directional KL Constraints Our core methodology comprises two complementary KL constraints operating on the paired views constructed in Section 3.1. The first constraint provides the primary positive guidance for evidence localization, while the second provides negative-space regularization to ensure that this localization is robust and visually grounded. 3.2.1. Focusing via Consistency To teach the model to ignore distractions and focus on the correct region, we use Consistency constraint, which enforces that the models policy on the full image should be consistent with its policy on the evidence-preserving view Ipres. We apply this constraint by minimizing the KLdivergence between these two distributions: Lcons = E(I,q,r) (cid:104) I(r=1) min (cid:0)πθ( I, q) (cid:13)"
        },
        {
            "title": "DKL",
            "content": "(cid:16) ccons, (cid:13) sg(cid:2)πθ( Ipres, q)(cid:3)(cid:1)(cid:17)(cid:105) conflicting gradients. We therefore devise two-stage curriculum that decouples these objectives. Base GRPO Training Objective. Before detailing the stages, we define the base GRPO [32] objective, which extends Proximal Policy Optimization (PPO) by normalizing rewards across rollouts within the same group to stabilize training. The training objective is: LGRPO = E(I,q) (cid:104) min (cid:0)rt(θ)At, clip(rt(θ), 1 ϵ, 1 + ϵ)At (cid:1) γDKL (cid:0)πθπref (cid:1)(cid:105) . (3) Here, At denotes the group-relative advantage, ϵ is the clipping threshold, and γ controls the KL penalty strength. (1) . Stage 1: Consistency Stage. In the first stage, we train the model on the primary task of evidence localization using the consistency constraint: Here πθ is the models answer distribution; πθ denotes target distribution computed on the evidence-preserving view. sg[] indicates stop-gradient so that the Ipres branch serves as fixed target; I(r=1) restricts supervision to validated correct samples; and ccons clips the KL term for stability."
        },
        {
            "title": "The forward direction DKL",
            "content": "(cid:1) pulls probability mass on toward evidence-supported answers on Ipres, encouraging the policy to base decisions on preserved evidence and treat extraneous regions as redundant. (cid:0)πθ πθ 3.2.2. Robustness via Separation On its own, Lcons is insufficient as it is susceptible to shortcut learning. model can perfectly minimize Lcons by ignoring both visual inputs and relying solely on the textual question to produce the same answer. To ensure the models focus is truly visually grounded, we introduce complementary Separation constraint, which acts as regularizer, forcing the model to learn that the visual signal is indispensable. It achieves this by enforcing that the models policy on the full image must be divergent from its policy on the evidence-ablated View Iabl. We maximize the KL-divergence between these distributions: Lsep = E(I,q) (cid:104) (cid:16) min csep, (cid:0)πθ( I, q) (cid:13)"
        },
        {
            "title": "DKL",
            "content": "(cid:13) sg(cid:2)πθ( Iabl, q)(cid:3)(cid:1)(cid:17)(cid:105) . (2) where csep is clipping hyperparameter. This objective penalizes similarity between the two policies until the divergence exceeds csep, breaking text-only shortcuts and promoting fine-grained grounding. 3.3. Coarse-to-Fine Training Curriculum Optimizing Lcons (an attractive force) and Lsep (a repulsive force) simultaneously can be challenging due to potentially LStage1 = LGRPO + α Lcons (4) where α is the consistency constraint coefficient. This stage establishes the foundational, coarse-grained skill of what to focus on. Stage 2: Separation Stage. Building on the Stage 1 checkpoint, we now introduce the separation constraint using view that removes fine-grained visual evidence, ensuring the learned focus is robust and truly visually grounded: LStage2 = LGRPO β Lsep (5) where β is the separation constraint coefficient. This coarse-to-fine curriculum first applies the positive signal (Lcons) and then the regularizer (Lsep) to ensure the learned policy is both accurate and grounded. We demonstrate the superiority of this curriculum over joint training and the reversed order in our ablation studies (Sec. 4.3). 4. Experiments 4.1. Experiment Setting Implementation. We use Qwen2.5-VL-7B [1] as the base model. Stage 1 is trained for 5 epochs on 7K samples containing evidence-preserving views Ipres. Subsequently, Stage 2 trains for 3 epochs on 13K samples including evidence-ablated views Iabl, producing our BiPS-Chart model. To further enhance general reasoning capabilities, we re-optimize the Stage 2 checkpoint for 3 epochs on 39K samples from ViRL39k [38] using standard GRPO, yielding the final BiPS-General. All models are optimized with AdamW (lr = 1 106) on 8H100 GPUs. We set the constraint coefficients to α = 0.01 and β = 0.02, and the clipping thresholds to ccons = 1.0 and csep = 0.2. Table 1. Evaluation on chart understanding and general perception & reasoning benchmarks. Models with are chart-specialized; are math-specialized; denotes our models. Blue numbers denote chart-related data, and red numbers denote math-related models with or perception-related data. Avg. is the arithmetic mean over available metrics. Models trained on MathVision (marked with ) omit their scores on MathVision. Best scores are in bold, and second-best scores are underlined. Model Data Chart Understanding and Reasoning General Perception and Reasoning CharXiv ChartQAPro ChartMuseum Evochart MathVista MathVision MathVerse-VO MMStar Closed-Source Models GPT-4o Claude-3.7-Sonnet Open-Source General Models Qwen2.5-VL-7B InternVL3-8B Multimodal Reasoning Models - - - - ChartLlama-13B ChartGemma-3B TinyChart-3B R1-OneVision-7B Vision-R1-7B DeepEyes-7B BigCharts-R1-7B Chart-R1-7B BiPS-Chart-7B over base model BiPS-General-7B over base model 160K 163K 1.3M 67K + 98K 73K + 137K 14K + 33K 1.7M 258K 13K 13K+39K 47.1 64.2 42.5 37.6 14.2 12.5 8. 33.8 42.5 42.9 41.3 46.2 37.7 36.6 36.9 6.8 13.3 36.1 39. 38.1 - 44.0 49.4 6.9 50.6 8.1 51.9 15. 51.8 15.2 42.2 60.3 26.8 28.2 12.2 12. 27.2 28.5 28.1 - 31.7 33.5 6. 34.0 7.2 49.8 70.7 52.0 55.0 9.5 30.6 25. 35.1 59.9 65.6 - 64.7 68.2 16. 68.7 16.7 63.8 74.5 68.2 70.4 64.1 73.2 70.8 - 67.5 73.5 5. 75.0 6.8 31.2 58.6 25.2 26.3 29.9 - 26.5 - 20.6 27.2 2. 28.6 3.4 40.6 52.0 41.1 33.9 40.0 47.7 44.9 - 28.1 44.4 3. 45.3 4.2 Avg. 47.2 44.3 44.6 39.8 47.5 65.1 68. 62.1 68.2 52.2 64. 63.0 - 61.1 45.5 64.9 2.8 65.7 3. 51.6 7.3 52.5 8.2 Benchmark. We evaluate our models on comprehensive suite of benchmarks spanning two categories. To assess chart understanding and reasoning capabilities, we report scores on CharXiv [41], ChartQAPro [25], ChartMuseum [36], Evochart [14] and ECD-Bench [47]. For general perception and reasoning, We use MathVista [21], MathVision [39], MathVerse-VO [50], and MMStar [3]. Baseline. We evaluate our method against diverse baseincluding closed-source models like GPT-4o [17] lines, and Claude-3.7-Sonnet as high-level SOTA references. Our open-source comparisons include general models such as InternVL3-8B [57] and Qwen2.5-VL-7B [1]. We also compare against specialized reasoning models, including chart-focused systems (ChartLlama [12], ChartGemma [27], Chart-R1 [4], BigCharts-R1 [26]) and broader multimodal reasoners (R1-OneVision [46], Vision-R1 [16], DeepEyes [56]) that handle both chart and math reasoning. 4.2. Main Results Table 1 reports performance across diverse set of chart understanding and general multimodal reasoning benchmarks. Overall, our approach delivers substantial improvements over the baseline Qwen2.5-VL-7B, boosting the average score by +7.3 points (from 44.3 to 51.6) with BiPSChart-7B and by total of +8.2 points (to 52.5) with BiPSGeneral-7B. This reflects clear and consistent gains in both chart-centric and general reasoning performance. BiPS-Chart-7B first illustrates the power of our method, achieving significant gains on both chart-specific and general out-of-domain benchmarks. Trained on only 13K chart samples, it delivers substantial gains on challenging chart reasoning benchmarks. On ChartXiv, performance increases from 42.5 (Qwen2.5-VL-7B) to 49.4 (+6.9), and on Evochart from 52.0 to 68.2 (+16.2). Crucially, BiPSChart-7B also exhibits strong OOD generalization, boosting performance on unseen general reasoning tasks such as MathVista (+5.3) and MMStar (+2.8). These improvements surpass other chart-specialized models that rely on far larger training sets, including TinyChart-3B, BigChartsR1-7B, and Chart-R1-7B, which are trained on hundreds of thousands to millions of chart examples but achieve lower scores on these key benchmarks. This comparison underscores that BiPS, by enhancing the models core visual perception capabilities, enables more data-efficient acquisition of chart reasoning and promotes robust generalization across diverse chart types. Building on these gains, BiPS-General-7B incorporates 39K math-focused samples to further enhance the models reasoning capacity. This integration leads to consistent imTable 2. Ablation study on the components of BiPS. We analyze the contribution of the consistency constraint (Lcons) and the separation constraint (Lsep) when added to the GRPO baseline. Table 4. Impact of the Counterpart Generation Strategy. We compare our programmatic code-editing pipeline against common baseline that uses random masking."
        },
        {
            "title": "ChartMuseum",
            "content": "Qwen2.5-VL-7B GRPO"
        },
        {
            "title": "GRPO with Lcons\nGRPO with Lsep\nOurs",
            "content": "42.5 44.3 47.2 47.7 49.4 19.0 35.6 36.3 38.3 39.9 26.0 30.8 31.3 31.8 33. Table 3. Analysis of the Training Curriculum. We compare our two-stage curriculum against alternative optimization strategies: Joint Training (optimizing Lcons and Lsep simultaneously) and Reversed Order (Stage 1: Lsep, Stage 2: Lcons)."
        },
        {
            "title": "Joint Training\nReversed Order\nOurs",
            "content": "46.4 46.8 49.4 36.7 39.2 39.9 31.5 31.3 33.5 provements across chart and general benchmarks. BiPSGeneral-7B reaches 50.6 on ChartXiv, 68.7 on Evochart, 75.0 on MathVista, and 65.7 on MMStar, surpassing both the baseline and the chart-only BiPS-Chart-7B model. This demonstrates that the introduction of math-specific data further compounds these gains by explicitly strengthening the models abstract and numerical reasoning abilities. 4.3. Ablation Study Impact of Bi-directional KL Constraints. Table 2 presents the component-wise ablation of BiPS, where individual constraints are added on top of the GRPO baseline. Adding either constraint consistently improves performance across benchmarks. Integrating the consistency constraint Lcons leads to notable gains on CharXiv (+2.9%), indicating that the coarse-grained focusing stage effectively guides the model toward relevant visual evidence. Incorporating the separation constraint Lsep yields more improvements on ECD-Bench (+2.7%) and CharXiv (+3.4%), showing that fine-grained visual grounding suppresses shortcut reliance and strengthens visual reasoning. Combining both stages achieves the best overall results (e.g., 39.9% on ECD-Bench and 49.4% on CharXiv), demonstrating that coarse-grained focusing and fine-grained grounding work synergistically to build robust and well-aligned perceptual capability. Analysis of the Training Curriculum. We evaluate the Coarse-to-Fine Training Curriculum against two alternatives: Joint Training (optimize both constraints simultaneously) and Reversed Order (apply Lsep before Lcons). As shown in Table 3, our curriculum attains the best results"
        },
        {
            "title": "Random Masking\nOurs",
            "content": "44.8 49.4 37.6 39.9 31.8 33.5 Figure 4. Accuracy on CharXiv with respect to the weighting coefficients of the consistency α and separation β constraints. across all three benchmarks. Specifically, both two-stage variants outperform Joint Training on CharXiv and ECDBench (e.g., ours is +3.0% / +3.2% over Joint), suggesting that simultaneously optimizing the guidance (Lcons) and regularization (Lsep) objectives can introduce competing gradient directions and higher update variance under the GRPO policy optimization. Decoupling them into successive stages allows the model to first form stable, evidencealigned policy before enforcing the grounding constraint, effectively reducing such interference. The training order also matters: our coarse-to-fine schedule consistently surpasses the reversed variant (49.4% vs. 46.8% on CharXiv). We attribute this to the fact that establishing coarse-grained perceptual focus first provides well-anchored representation aligned with relevant evidence, upon which finegrained grounding can add discriminative margin and suppress residual shortcuts. In contrast, applying the grounding constraint too early may over-regularize before reliable focus is established, forcing divergence along under-defined directions and leading to slower or unstable convergence. Impact of Counterpart Generation Strategy. We compare our programmatic code-editing strategy against random masking baseline. Following prior work [37, 42], this baseline randomly masks out 60% of the image patches to create the alternative view. As shown in Table 4, our programmatic code-editing strategy significantly outperforms the random masking baseline across all benchmarks. These improvements highlight the importance of generating seFigure 5. Case study on ChartXiv comparing Qwen2.5-VL-7B and our BiPS-Chart. BiPS yields more visually grounded answers. mantically faithful counterparts for the bi-directional KL constraints. Programmatic editing explicitly isolates the key-evidence and question-unrelated code components, ensuring that Lsep is computed on pure distraction view. In contrast, random masking drops patches blindly and fails to produce clean distraction view: if it mostly hides irrelevant regions, the example becomes easier and the prediction gets closer to Ipres, making KL maximization counterproductive; when it also hides task-critical evidence, the masked input becomes mixture of missing-evidence and residual distractions rather than pure distractor, so the KL is computed against an ill-defined target and yields noisy gradients. By ensuring clean, semantically controlled counterparts, our programmatic generation provides precise supervision for both Lcons and Lsep, leading to more stable optimization and stronger perceptual grounding. Effect of KL Constraint Coefficient. Figure 4 shows the performance impact of varying the consistency (α) and separation (β) constraint coefficients. Both objectives outperform the baseline (coefficient=0) over wide range, confirming they provide complementary gains without requiring sensitive tuning. Peak accuracy is observed at the moderate coefficients α = 0.01 and β = 0.02, each obtained with the other fixed to 0. Conversely, large coefficients (e.g., 0.08) degrade performance. We attribute this to the auxiliary consistency and separation losses dominating the GRPO objective and increasing update variance. Case Study. Figure 5 presents two representative cases from ChartXiv [41]. In both cases, Qwen2.5-VL-7B produces plausible but incorrect answers by relying on textual or statistical cues rather than interpreting the visual structure. In the intersection-count example, it hallucinates extra curve crossings without actually tracing the plotted functions, while in the multi-plot accuracy example it overfits to frequently occurring numerical patterns (e.g., 94.6%) instead of reasoning over per-panel maxima. In contrast, BiPS yields correct and visually grounded responses. This improvement suggests the bidirectional perceptual shaping encourages the model to attend to the structural alignment and cross-plot relationships that are critical for reasoning, rather than inferring from superficial patterns or numeric priors. Additional cases can be found in the Appendix. 5. Conclusion In this paper, we proposed BiPS, training-time framework that turns question-conditioned visual cues into perceptual shaping signals for VLMs. BiPS imposes two complementary KL constraints that pull predictions on the original image toward an evidence-preserving view and push them away from an evidence-ablated view. These signals encourage coarse coverage of supporting regions and fine-grained reliance on visual evidence, improving perception without generating any visual cues at inference. BiPS establishes new paradigm for multimodal reasoning with three benefits: 1) strong generalization and data efficiency, where using only 13K chart samples yields significant, wide-ranging gains on both chart-specific benchmarks and general VQA; 2) improved fine-grained perception, as demonstrated on chart benchmarks whose complex layouts and thin polylines demand precise visual interpretation; and 3) inference efficiency with no additional test-time overhead."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 4, 5, 6 [2] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 38363845, 2025. 1 [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 2, 6 [4] Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Yufeng Zhong, and Lin Ma. Chart-r1: Chain-of-thought supervision and reinforcement for advanced chart reasoner. arXiv preprint arXiv:2507.15509, 2025. 2, 3, 6 [5] Shuang Chen, Yue Guo, Zhao yu Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. ArXiv, abs/2506.04207, 2025. 2 [6] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. ArXiv, abs/2506.05331, 2025. 1, 3 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 1 [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. ArXiv, abs/2503.21776, 2025. 2 [9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. 3 [10] Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, and See-Kiong Ng. Chip: Cross-modal hierarchical direct preference optimization for multimodal llms. arXiv preprint arXiv:2501.16629, 2025. [11] Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. 1, 3 [12] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. 6 [13] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 1, 3 [14] Muye Huang, Han Lai, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, and Jun Liu. Evochart: benchmark and self-training approach towards real-world chart understandIn Proceedings of the AAAI Conference on Artificial ing. Intelligence, pages 36803688, 2025. 6 [15] Muye Huang, Lingling Zhang, Jie Ma, Han Lai, Fangzhi Xu, Yifei Li, Wenjun Wu, Yaqiang Wu, and Jun Liu. Chartsketcher: Reasoning with multimodal feedback and reflection for chart understanding. ArXiv, abs/2505.19076, 2025. 3 [16] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2, 6 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [18] Caijun Jia, Nan Xu, Jingxuan Wei, Qingli Wang, Lei Wang, Bihui Yu, and Junnan Zhu. Chartreasoner: Code-driven modality bridging for long-chain reasoning in chart question answering. ArXiv, abs/2506.10116, 2025. [19] Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, and Junxian He. On the perception bottleneck of vlms for chart understanding. arXiv preprint arXiv:2503.18435, 2025. 1 [20] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symIn Annual Meeting of the Association for bolic reasoning. Computational Linguistics, 2021. 3 [21] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 2, 3, 6 [22] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Xinzhe Ni, Zicheng Lin, Songtao Jiang, Yiyao Yu, Chufan Shi, Ruihang Chu, Jin Zeng, et al. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. 2 [23] Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, and Zhiding Yu. Argus: Vision-centric reasoning with grounded chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1426814280, 2025. 1, [24] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. ArXiv, abs/2203.10244, 2022. 3 [25] Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. 2, 3, 6 [26] Ahmed Masry, Abhay Puri, Masoud Hashemi, Juan Rodriguez, Megh Thakkar, Khyati Mahajan, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Alexandre Piche, Dzmitry Bahdanau, et al. Bigcharts-r1: Enhanced chart reasonarXiv preprint ing with visual reinforcement finetuning. arXiv:2508.09804, 2025. 3, 6 [27] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 625643, 2025. 6 [28] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 2, 4 [29] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 15161525, 2019. 3 [30] Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025. [31] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 1, 3 [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4, 5 [33] Mong Yuan Sim, Wei Emma Zhang, Xiang Dai, and Biaoyan Fang. Can vlms actually see and read? survey on modality collapse in vision-language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2445224470, 2025. 1 [34] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. ArXiv, abs/2505.15966, 2025. 3 [35] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. [36] Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, et al. Chartmuseum: Testing visual reasoning capabilities of large vision-language models. arXiv preprint arXiv:2505.13444, 2025. 3, 6 [37] Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024. 7 [38] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 2, 5 [39] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 3, 6 [40] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. 1 [41] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. 2, 3, 6, [42] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. 3, 7 [43] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. 3 [44] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts. ArXiv, abs/2312.15915, 2023. 3 [45] Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, et al. Chartmimic: Evaluating lmms cross-modal arXiv reasoning capability via chart-to-code generation. preprint arXiv:2406.09961, 2024. 3 [46] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. 2, 6 [47] Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, and Liang Zheng. Effective training data synthesis for improving mllm chart In Proceedings of the IEEE/CVF Internaunderstanding. tional Conference on Computer Vision, pages 26532663, 2025. 2, 4, 6, [48] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens, 2025. 1, 3 [49] Liang Zhang, Anwen Hu, Haiyang Xu, Mingshi Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with program-of-thoughts learning and visual token merging. In Conference on Empirical Methods in Natural Language Processing, 2024. 3 [50] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. 6 [51] Shuoshuo Zhang, Zijian Li, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Yujiu Yang, and Rui Wang. Pixelcraft: multi-agent system for high-fidelity arXiv preprint visual reasoning on structured images. arXiv:2509.25185, 2025. 1, 3 [52] Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, and Yujiu Yang. Generative universal verifier as multimodal meta-reasoner. arXiv preprint arXiv:2510.13804, 2025. 2 [53] Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, et al. Perl: Permutation-enhanced reinforcement learning for interleaved vision-language reasoning. arXiv preprint arXiv:2506.14907, 2025. 2 [54] Xuanle Zhao, Xianzhen Luo, Qi Shi, Chi Chen, Shuo Wang, Wanxiang Che, Zhiyuan Liu, and Maosong Sun. Chartcoder: Advancing multimodal large language model for chart-tocode generation. ArXiv, abs/2501.06598, 2025. [55] Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, and Xianyuan Instruction-guided visual masking. Advances in Zhan. neural information processing systems, 37:126004126031, 2024. 1, 3 [56] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. DeepIncentivizing thinking with images via reinforceeyes: ment learning. ArXiv, abs/2505.14362, 2025. 3, 6 [57] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 6 See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implementation Details 6.1. Data Statistics We provide quantitative breakdown of the data generation pipeline described in the main paper. The construction of our training dataset involves rigorous filtering and synthesis process to ensure the quality and difficulty of the reasoning tasks. The statistics for each stage are summarized in Table 5 and detailed below: Stage 1: Sampling and Reformulation. We initially randomly sampled 50,000 raw chart-code pairs from the ECD [47] dataset. After the Question Reformulation and Validation phase, where the LLM arbitrator (GPT-5-mini) converted open-ended questions into verified multiplechoice formats, approximately 30K valid samples were retained. Stage 2: Difficulty Filtering. To ensure the model learns from non-trivial examples, we filtered the dataset using the base model (Qwen2.5-VL-7B-Instruct). Approximately 10K easy samples that answered correctly in all 8 rollouts, were discarded, leaving roughly 20K challenging samples. Stage 3: Code Editing. In this final stage, we performed programmatic editing to generate visual counterparts. We successfully generated the Evidence-Ablated View (Iabl) for 13K samples. Within this subset, we further successfully synthesized the Evidence-Preserving View (Ipres) for approximately 7K instances. Consequently, the final high-quality dataset used for BiPS training comprises 13K samples. Table 5. Statistics of the Data Generation Pipeline. The table tracks the number of samples retained after each processing stage."
        },
        {
            "title": "Sample Count",
            "content": "Initial Sampling (from ECD) After Reformulation & Validation After Difficulty Filtering Final Training Set (Success in Iabl Generation) subset containing Ipres 50K 30K 20K 13K 7K 6.2. Training Details We detail the hyperparameter configurations for RL training in Table 6. Specifically, we employ the AdamW optimizer with learning rate of 1 106 and keep the vision tower unfrozen. The reward is composed of 0.1 for correct formatting and 0.9 for the correct prediction. Table 6. Hyperparameters for Reinforcement Learning."
        },
        {
            "title": "Value",
            "content": "256 1 106 AdamW False 2,048 0.01 8 0.85 7. Additional Results 7.1. Case Study As shown in Figure 6, beyond in-domain chart benchmarks, we further evaluate cross-domain transfer on visual counting. The baseline fails due to incomplete object-level reasoning, whereas our model explicitly tracks and subtracts objects to arrive at the correct answer. The stronger performance in this setting indicates improved cross-domain generalization. 7.2. Comparison with Standard GRPO To verify that the improvements stem from our specific perceptual shaping pipeline rather than merely applying RL to the data, we compare BiPS-General against baseline trained with standard GRPO. This baseline is fine-tuned on the exact same combined dataset (programmatic samples + ViRL39k) but treats all data uniformly without the proposed two-stage perceptual shaping curriculum. Analysis. As shown in Table 7, while standard GRPO yields substantial gains over the base model, BiPS-General consistently outperforms it across all benchmarks. Notably, on complex chart reasoning tasks like CharXiv, our method surpasses the standard GRPO baseline by significant margin (+5.2). This performance gap highlights critical insight: simply optimizing reasoning via RL is insufficient if the underlying visual grounding is flawed. By explicitly shaping the models perception through our programmatic curriculum before the general reasoning stage, BiPS ensures that the RL process operates on high-fidelity visual signals, thereby amplifying the effectiveness of the optimization. Figure 6. Cross-domain case on visual counting. The baseline fails due to incomplete object reasoning, whereas BiPS correctly tracks and subtracts objects to obtain the right answer. Table 7. Comparison between standard GRPO and our BiPS-General on the same data mixture. Our method consistently outperforms the standard GRPO baseline across all benchmarks."
        },
        {
            "title": "CharXiv ChartQAPro ChartMuseum EvoChart MathVista MathVision MathVerse MMStar",
            "content": "Qwen2.5-VL-7B GRPO BiPS-General 42.5 45.4 50.6 36.6 50.2 51.8 26.8 32.9 34.0 52.0 68.0 68.7 68.2 74.3 75. 25.2 27.3 28.6 41.1 42.6 45.3 62.1 64.6 65.7 8. Prompts"
        },
        {
            "title": "Question Reformulation and Validation",
            "content": "Ensure options are plausible but only one is correct. Include at least 3 options, preferably 4. Distractors should reflect realistic misconcepSystem: You are an expert in data analysis and tions. question generation. Task: You will analyze chart-related questionanswer pair for correctness and potentially rewrite it as multiple-choice question. Given: Chart metadata and code snippets problem/question about the chart An answer to that problem Your task: 1. Analyze correctness: Determine if both the question and answer are factually correct based on the chart data. 2. Generate output: If correct: Rewrite as multiple-choice question with 34 options. If incorrect: Explain the error(s) without rewriting. If uncertain: Explain what information is missing or unclear. Guidelines: Keep questions clear and unambiguous. Use data directly from provided metadata/- code. User: Chart Metadata: {code} Original Problem: {question} Provided Answer: {answer} Please analyze the correctness of this questionanswer pair and generate the appropriate output according to the format specified. Evidence-Preserving View System: You are an expert in chart code editing and data visualization. You will receive chart code (Matplotlib/Seaborn/Plotly/Altair) and question. Your goal is to minimize edits while removing irrelevant elements and preserving layout: figure size, subplot grid, spacing, suptitle, legend order/length, trace order, color assignment, and axis links. Editing Principles: Preserve all layout structure; if hiding content, keep axes and series positions. Never change plotting library; only minimal imports for placeholders are allowed. Keep legend/trace counts; for removed series use placeholders (e.g., NaNs, transparent marks, or legendonly). Be careful not to let the model derive the answer directly from the remaining elements; keep the necessary distractors. Maintain axis limits when feasible to avoid scale drift. Decision Rules: Subplot-specific questions: Keep only referenced subplots; blank others but preserve axes. Legend/category-specific questions: Keep only mentioned categories; others become placeholders. Series/trace-specific questions: Keep only targeted lines/bars/points; blank others. Global comparison or vague questions: Do not edit. If uncertain, set should edit = false. Post-edit requirements: Subplot grid unchanged; all axes preserved. Legend length and order unchanged. Placeholders inserted for every removed subplot/series. Axis ranges preserved when appropriate. Output must be JSON only. User: Chart Code: {code} Question: {problem str} Please determine whether the chart code should be edited to remove irrelevant elements according to the rules above. Evidence-Ablated View System: You are an expert in chart code obfuscation for evaluation/red-teaming. Given chart code (Matplotlib/Seaborn/Plotly/Altair) and question, your task is to make the question unanswerable by removing/blanking decisive evidence while preserving layout. Objectives (priority order): 1. Ensure unanswerability: blank all chart elements that allow definitive answer. 2. Preserve layout: keep figure size, subplot grid, spacing, legend structure, series order, and color assignments. 3. Minimize edits: hide or blank evidence without refactoring or adding new content. Decisive Evidence (to be blanked): Any subplot targeted or compared by the question. Legend/categories mentioned or implied by the question/options. Series/traces/marks revealing values, trends, peaks, ranks, or comparisons. Numeric cues: labels, annotations, thresholds, reference lines. Axes information that allows inference once geometry is hidden. If unsure, blanking). treat as decisive (favor overBlanking Tactics (by library): Matplotlib: Replace data with NaNs, set invisible while keeping legend handles, or use dummy Line2D. For bars/scatter: empty/- NaN arrays or alpha=0. For entire subplots: ax.cla(); ax.set axis off(). Plotly: Keep trace but hide geometry via visible=legendonly or empty x/y while keeping showlegend=True. Altair: Keep encodings and legend domain; blank via opacity=0 or empty filters. Decision Rules: Options provided: referenced elements. blank all optionComparisons/ranking/extremes: blank all compared candidates. Single-target lookup: blank the targets geometry and any revealing annotation. Global comparisons: blank decisive evidence across all involved candidates. Trend/correlation: blank scatter points and trend/regression lines. Threshold questions: blank values and relevant threshold lines. Post-edit Requirements: Subplot grid unchanged; axes preserved. Legend length/order preserved (dummy placeholders allowed). Placeholders inserted for all blanked series/- subplots. Axis ranges preserved when applicable; code must run. Remaining visuals must not allow human to answer the question. User: Chart Code: {code} Question: {problem str} Your task: Make the question unanswerable by blanking all decisive evidence while preserving layout."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Tsinghua University"
    ]
}