{
    "paper_title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "authors": [
        "Enyi Jiang",
        "Yibo Jacky Zhang",
        "Yinglun Xu",
        "Andreas Haupt",
        "Nancy Amato",
        "Sanmi Koyejo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 3 8 0 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Latent Adversarial Regularization for Offline Preference Optimization",
            "content": "Enyi Jiang 1 2 Yibo Jacky Zhang 1 Yinglun Xu 2 Andreas Haupt 1 Nancy Amato 2 Sanmi Koyejo"
        },
        {
            "title": "Abstract",
            "content": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because tokenspace similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of policy model and reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead. 1. Introduction Learning from human feedback is essential for aligning large language models (LLMs) with human preferences (Leike et al., 2018; Ouyang et al., 2022). Most modern methods are done via pairwise preference optimization (PO): given pairwise preference data, we update policy model πθ while constraining it to remain close to reference model πref. This regularization constraint is crucial for improving 1Department of Computer Science, Stanford University 2Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign. Correspondence to: Enyi Jiang <enyij2@illinois.edu, enyij@stanford.edu>. Preprint. January 30, 2026. generalization and reducing reward hacking, and it is most commonly implemented via KL regularizer (Ouyang et al., 2022; Rafailov et al., 2023b; Xiong et al., 2024). Recent work has also explored alternative divergence measures, e.g., χ2-divergence and other -divergences (Wang et al., 2023a; Huang et al., 2025). However, limitation of these regularizers is that they operate purely in token space. Two sentences, for example, may be distant under token-level divergences while remaining semantically very similar (Hi there vs Good morning to you) and vice versa (Hi there vs Hit there), as illustrated in Figure 2. As result, token-space divergences can be coarse proxy for the actual behavioral similarity. Motivated by the limitation of token space optimization, recent work begun to explore the use of latent spaces instead of token spaces. For example, studies on continuous latent reasoning suggest that optimization in latent representations can lead to improved reasoning capabilities (Hao et al., 2025; Zhu et al., 2025) compared to methods that purely operate in token space. This paper investigates regularization performed in the latent space: we show that latent regularization can provide structural alignment feedback, which is absent from tokenlevel constraints. Concretely, given preference dataset, we consider the distributions of internal representations produced by the policy model and the reference model, and penalize their divergence. Latent representations are lowerdimensional than token distributions and often encode dense, structured information about semantics and reasoning state. This makes latent regularization potentially better way for semantic alignment than explicit token-level constraints. The immediate challenge is that, unlike token probabilities, latent representations do not admit an explicit probability density, making standard divergence measures computational challenge. To address this, we adopt technique inspired by GANs (Goodfellow et al., 2014): we introduce discriminators that distinguish representations generated by the policy from those generated by the reference model. We show that optimizing the policy against such discriminators is equivalent to minimizing latent-space divergence, yielding an efficient adversarial regularizer that can be added to existing preference optimization objectives. To fully leverage paired preference signals, we move beyond the standard Latent Adversarial Regularization for Offline Preference Optimization Figure 1. Comparison between DPO and GANPO. Offline preference optimization methods (e.g., DPO) optimize an implicit reward defined by preference data. GANPO augments this objective with latent-space discriminator, whose adversarial interaction induces regularization between the latent representation distributions of the policy model and the reference model. models internal representation space, yielding improved robustness to stochastic sampling noise and distributional shifts. Moreover, GANPO maintains comparable performance on downstream tasks with modest additional computational overhead. 2. Preliminaries 2.1. Preference Optimization In modern language-model alignment, preference learning is commonly instantiated through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) by optimizing KL-regularized reward objective. For parameter β > 0, this objective is given by max πθ r(πθ) β DKL(πθ πref ), (1) where r(πθ) := ExD,yπθ(x)[r(x, y)] denotes the expected reward of the policy over the data distribution D, and DKL(πθ πref ) := ExD[DKL(πθ(x) πref (x))] is the expected KL between the policy and reference distribution. In many practical settings, the available supervision is offline and pairwise: dataset consisting of preference triplets (x, yw, yl), representing prompt x, chosen response yw, and rejected response yl. Preference optimization methods working with such offline datasets are referred to as offline preference optimization (OPO). standard approach of OPO is Direct Preference Optimization (DPO) (Rafailov et al., 2023b). DPO uses the analytical solution to the KL-regularized RLHF objective (equation 1) to remove the explicit reward function. Instead, the reward r(x, y) is implicitly reparameterized in terms of the optimal policy πθ and the reference policy πref. To model human preferences, DPO incorporates this formulation into the Bradley-Terry model (Bradley & Terry, 1952), resulting in tractable objective that directly optimizes the policy: LDPO(πθ; πref) = E(x,yw,yl)D (cid:20) (cid:18) πθ(yw x) πref(yw x) β log πθ(yl x) πref(yl x) (cid:19)(cid:21) , (2) log σ β log 2 Figure 2. Latent space vs token space. Anchor (Hi there.) is the reference point for distance measurements. Semantically similar paraphrases exhibit large token-level variation yet remain close in latent space, while semantically different phrases show smaller token changes but larger latent space differences. binary GAN formulation and introduce quad representation framework. Specifically, we design contrastive objective by training two discriminators that jointly distinguish high-quality and low-quality representations, while retaining the original offline preference optimization objective. This formulation enables the policy model to learn from pairwise preference dataset while receiving dense structural feedback through latent-space optimization. In this work, we focus on offline preference optimization (OPO)-style methods. Our contributions are summarized as follows: We propose GANPO (generative adversarial network preference optimization), the first latent-space regularization for language-model preference optimization, which introduces an efficient plug-and-play adversarial regularizer that can be added to existing preference optimization objectives. Experiments across diverse model architectures and tasks demonstrate consistent improvements by plugging GANPO into OPO-style methods on AlpacaEval-2.0. We conduct extensive experimental studies demonstrating that GANPO better preserves the geometry of the Latent Adversarial Regularization for Offline Preference Optimization where σ() stands for the sigmoid function. DPO implicitly incorporates the KL regularization with strength β > 0. 2.2. Generative Adversarial Networks Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is framework for learning generative models through an adversarial training process. This framework formulates minimax, two-player game between two components: generator πG and discriminator D. The generator πG aims to produce samples similar to those from the real distribution πreal. The discriminator D(x) (0, 1) is trained to distinguish between real samples drawn from the data distribution πreal and fake samples produced by the generator πG. The adversarial interaction between these two models is formalized as the following minimax optimization problem in standard GANs: min max Exπreal[log D(x)] + ExπG [log(1 D(x))]. (3) Our method integrates these two approaches in principled way to exploit latent space geometry, as detailed next. 3. Latent Adversarial Regularization In this section, we introduce latent space adversarial regularization, leading to the proposed method GANPO. 3.1. Latent-Space Regularization Offline preference optimization, e.g., DPO, uses loss function (equation 2) that implicitly regularizes the deviation of learned policy πθ from reference policy πref using token-space KL divergence of DKL(πθ πref ). However, token-space divergences can be coarse proxy for the actual behavioral similarity, as they may assign large divergences to semantically similar outputs. Unlike standard reward models that rely on the token verified reward, our discriminator aligns the global semantic structure by operating on the representation of the entire sequence. To explore regularization in latent space, we consider prompt-response pairs (x, y) drawn from data distribution, e.g., samples from preference dataset. Let hθ be the corresponding latent representation produced by the policy model πθ, and href the corresponding representation from πref . Specifically, we designate the LLMs final-layer hidden state representations as the latent space representations. The induced representation distributions are respectively denoted by hθ pθ and href pref . distributions pθ and pref , we augment the standard OPO loss in plug-and-play manner as follows, so the original loss can be from any existing policy optimization methods. LOPO(πθ; πref) + λ D(pθ pref ). (4) min πθ 3.2. Generative Adversarial Formulation While choosing the divergence in equation (4) to be the DKL is conceptually natural, it typically requires densities of latent representations, which are generally intractable. Fortunately, some divergences enjoy variational form. For example, let := 1 2 (pθ + pref ) denote their average mixture, the Jensen-Shannon divergence (JSD) has dual formulation (Goodfellow et al., 2014) expressed as follows. 2DJSD(pθ pref ) := DKL(pθ p) + DKL(pref p) = log 4 + sup (cid:2) log D(href )(cid:3) + Ehθ (cid:2) log (cid:0)1 D(hθ)(cid:1)(cid:3)(cid:3) , (cid:2)Ehref where the supremum over is taken over every function mapping from the space of latent representations to [0, 1]. This is precisely the objective for the discriminator in the standard GANs (equation 3), where href corresponds to the real samples and hθ corresponds to fake samples. The relativistic average GANs objective. The standard GANs are notoriously unstable in training. To improve optimization stability, we adopt the relativistic average version of the GANs objective (Jolicoeur-Martineau, 2018). It estimates the probability that real sample is more realistic than the average of the fake samples in the current batch. Let Cϕ be the scalar logit from the discriminator parameterized by ϕ. Given batch of samples, define the average baselines as mθ := Ehpθ [Cϕ(h)], The relativistic discriminator Dϕ is defined to be (cid:1) σ(cid:0)Cϕ(h) mθ σ(cid:0)Cϕ(h) mref mref := Ehpref [Cϕ(h)]. if pref , if pθ. Dϕ(h) := (cid:40) (cid:1) where σ is the sigmoid function. The regularization we use in GANPO is therefore implemented by relativistic average GANs (RaGANs), where the divergence is formulated as follows. DRa(pθ pref ) (cid:104) Ehref (cid:2) log (cid:0)1 Dϕ(hθ)(cid:1)(cid:3)(cid:105) (cid:2) log Dϕ(href )(cid:3) + Ehθ = sup . ϕ (5) The general form of latent space regularized OPO. Given divergence function of the two latent representation It is shown in Jolicoeur-Martineau (2020) that, under the same type of assumptions required for the dual formulation of the DJSD, DRa is well-defined divergence. I.e., it 3 Latent Adversarial Regularization for Offline Preference Optimization satisfies: (i) DRa(pθ pref ) 0; and (ii) DRa(pθ pref ) = 0 pθ = pref . detailed theoretical analysis of this claim is provided in Appendix A. To further simplify the divergence and prepare for implementation, we can interpret Dϕ(h) (0, 1) to be the discriminator predicting the probability that comes from the reference distribution, where we assign labels 1 to href and = 0 to hθ. Then the discriminator objective in equation (5) is exactly binary cross-entropy (BCE) term for this binary classification problem. In general, for two generic distributions of representations with random samples h1 (label 1) and h2 (label 0), denote BCEϕ(h1, h2) := Eh1 (cid:104) log Dϕ(h1) (cid:105) Eh2 (cid:16) (cid:104) log 1 Dϕ(h2) (cid:17)(cid:105) . Therefore, the relativistic average divergence (equation 5) can be simply written as DRa(pθ pref ) = sup ϕ BCEϕ(href , hθ) = inf ϕ BCEϕ(href , hθ). (6) Up to this point, we have defined the latent-space regularization using the relativistic average divergence DRa. This divergence is reparameterized through the variational formulation in (6), where ϕ can be interpreted as discriminator trained to minimize the BCE loss. This leads to the GAN preference optimization algorithm, as presented next. 4. GAN Preference Optimization Before implementing GANPO, design choice must be made: which data are used to extract latent representations. Preference alignment is typically implemented with datasets consisting of chosen and rejected pairs (yw, yl). To fully utilize the paired nature of preference data (x, yw, yl), we move beyond the standard binary GAN objective to quad representation space. 4.1. GANPO Loss Functions Given samples (x, yw, yl) from preference dataset, we implement GANPO on quad-tuple of latent representations at each training step, defined as follows. The latent representations are the last layer of hidden outputs from the model inferences. 1. h+ 2. 3. h+ 4. ref from the ref on the chosen response (x, yw). ref from the ref on the chosen response (x, yl). θ from the policy on the chosen response (x, yw). θ from the policy on the chosen response (x, yl). Given four representations, we treat the reference models positive and negative representations as anchors in latent space. To effectively exploit signals from both two models, we adopt relativistic average discriminator, which enables the discriminator to reason about representations in comparative manner rather than in isolation. Concretely, we introduce two discriminators that respectively model the distributions of good and bad latent representations, since two manifolds might be topologically distinct, and single discriminator cannot separate multiple distributions simultaneously. As formalized in Eq. (6), both discriminators are trained using relativistic binary cross-entropy objectives, while the policy model is regularized against these discriminators to align its latent representations accordingly. This design allows the model to simultaneously distinguish highand low-quality responses and to receive dense structural feedback during optimization. θ a) Generator Optimization: The policy model θ minimizes the latent-space regularization DRa(p+ ref ) and DRa(p ref ). Given trained discriminators ϕpos and ϕneg, according to (6), the latent space adversarial regularization loss for the policy model θ is: ref , h+ θ ) (cid:125) BCEϕneg (h (cid:124) Ladv := BCEϕpos(h+ ref , θ ) (cid:125) θ p+ (7) (cid:124) (cid:123)(cid:122) Mimic Good (cid:123)(cid:122) Align Bad We note that in the actual implementation, we view the (cid:2) log (cid:0)1 moving average mθ as constant. Thus, only Ehθ Dϕ(hθ)(cid:1)(cid:3) in the BCE loss is relevant for the optimization of the policy model θ. b) Positive Discriminator (ϕpos). The positive discriminaref , h+ tor minimizes BCEϕpos(h+ θ ), i.e., it aims to give the reference good representation to receive higher score than the policys good representation. To better utilize the latent space geometry, we ask it to also distinguish between the policys good representation and the reference bad representation, i.e., minimizing BCEϕpos(h+ ref ). This design allows the discriminator to capture fine-grained preference structure beyond simple real-fake classification. We have Lϕpos := BCEϕpos(h+ ref , h+ θ ) (cid:124) (cid:125) (cid:123)(cid:122) Ref Good > Policy Good θ , (cid:123)(cid:122) Policy Good > Ref Bad + BCEϕpos(h+ θ , ref ) (cid:125) (8) (cid:124) c) The Negative Discriminator (ϕneg): Similarly, we have Lϕneg := BCEϕneg (h (cid:124) ref , (cid:123)(cid:122) Ref Bad > Policy Bad θ )) (cid:125) + BCEϕneg (h θ , h+ (cid:123)(cid:122) Policy Bad > Ref Good ref ) (cid:125) (cid:124) (9) Thus, the generator (policy model) and the discriminators are optimized alternately, as detailed in Algorithm 1. 4.2. Design Choices: from GAN Perspectives Having motivated GANPO from regularization perspective, we now discuss it from purely GAN-based viewpoint. 4 Latent Adversarial Regularization for Offline Preference Optimization Algorithm 1 GAN Preference Optimization (GANPO) 1: Input: Preference dataset = {(x, yw, yl)}, Policy πθ, Reference πref, Discriminators ϕpos and ϕneg. 2: Hyperparameters: Learning rate η, Adversarial weight λ, moving average decay rate α. 3: Initialize: Global running means µpos 0, µneg 0. 4: for each training step = 1, . . . , do 5: 6: 7: 8: 1. Data Sampling Sample batch = {(x, yw, yl)} D. 2. Feature Extraction (Latent Space) Get last hidden states from Policy πθ (with gradients) and Reference πref (frozen): 9: 10: 11: 12: 13: 14: 15: 16: h+ θ , h+ ref , θ Forward(πθ, B) ref Forward(πref, B) 3. Discriminator Optimization (Relativistic) Compute raw logits = Cϕ(h) for all four h. Update global running means via moving average: µpos αµpos + (1 α)Mean(s+ ref) µneg αµneg + (1 α)Mean(s ref) Compute Discriminators losses Lϕpos and Lϕneg with Eq 8 & Eq 9 Update Discriminators: 17: 18: 19: 20: 21: 22: 23: 24: 25: end for ϕpos ϕpos ηϕpos(Lϕpos) ϕneg ϕneg ηϕneg (Lϕneg ) 4. Generator Optimization Compute Offline PO (e.g., DPO) Loss LOPO. Compute Generator Loss Ladv using Eq. 7 Update Generator: θ θ ηθ(LOPO + λLadv) The structure-preserving adversarial game. Viewing preference alignment through GAN-style lens highlights fundamental limitations of offline optimization methods such as DPO. Because DPO operates solely on fixed preference dataset, the policy is trained to separate preferred and rejected responses within the data distribution. This disconnect encourages spurious correlations, most notably between implicit reward and response length, leading to verbosity rather than genuine semantic improvement (Liu et al., 2024b). GANPO mitigates this issue by introducing an adversarial discriminator that operates directly on latent representations, providing dense structural feedback while framing preference alignment as zero-sum game between the generator and the discriminator, in which both components are jointly strengthened through adversarial optimization. This adversarial signal acts as geometry-preserving regularizer, constraining the policy to remain aligned with the reference manifold of high-quality responses even under distributional shift. We discuss more on this in Section 5.2. Design choice: the definition of real data. From GAN perspective, the real data in GANPO consists of represen5 tations generated by the reference model, which the policy model aims to match. Alternatively, one might ask what would happen if the real data were representations obtained from stronger external teacher model. While πref may be sub-optimal in text generation, its latent manifold represents the well-formed structure of natural language acquired during pre-training. We view the adversarial loss not as correctness objective (handled by DPO), but as syntax/manifold constraint to prevent mode-collapse. Further, we argue that anchoring to the reference model offers two key advantages over anchoring to teacher model. a) Manifold consistency for training stability. strong teacher model often lies on distributional manifold that is too dissimilar from the policy, causing the discriminator to learn superficial stylistic differences rather than meaningful structural distinctions, which leads to rapid saturation. With reference-anchored training, we ensure meaningful distributional overlap, forcing the discriminator to learn semantic distinctions and provide dense, informative gradients. b) Computational efficiency. Sampling from an external Teacher at each training step is prohibitively expensive. In contrast, πref is usually required for preference optimization (e.g., DPO), enabling fully offline, self-contained adversarial training loop with small additional overhead. 5. Experiment 5.1. Experimental setup Datasets and setup. Our models are trained on the UltraFeedback dataset (Cui et al., 2024), large-scale preference dataset for instruction-following and dialogue alignment. We follow standard preference optimization training protocols, with minimal modifications to support the GANPO framework. Our evaluation focuses on four aspects: (1) Assessing GANPOs effectiveness on general instructionfollowing using AlpacaEval-style metrics; (2) Examining its robustness across model architectures and scales, with experiments on Gemma2-2B-it (Team et al., 2024) and Llama3-8B-Instruct (AI@Meta, 2024) models; (3) Providing analysis and insights of the structural regularization of GANPO; and (4) Evaluating whether GANPO preserves or improves performance on downstream tasks beyond preference alignment. Full experimental details are provided in Appendix B. Baselines. We compare against DPO (Rafailov et al., 2023b), which aligns models via contrastive objective relative to fixed reference policy, and SimPO (Meng et al., 2024), which removes the reference model for simpler and more efficient objective. GANPO is plug-and-play extension to both methods, retaining their original preference losses while adding structural supervision in the latent space, requiring no changes to the underlying training pipeline. Latent Adversarial Regularization for Offline Preference Optimization Gemma2-2B-it Llama3-8B-Instruct Method DPO GANPO (DPO) SimPO GANPO (SimPO) Disc. N/A Transformer N/A Transformer Win 22.76 24.17 30.66 31. LC-Win 27.79 29.69 36.03 36.74 Len 1668 1664 1740 Disc. N/A Transformer N/A Transformer Win 33.90 35.23 44.09 46. LC-Win 32.34 33.87 48.31 50.48 Len 2041 2043 1836 Table 1. AlpacaEval 2.0 (weighted alpaca eval gpt4 turbo) results. GANPO yields consistent gains in raw and lengthcontrolled win rates over DPO and SimPO across model scales, without increasing response length. (a) AlpacaEval. GANPO widens the performance gap over DPO as entropy increases (T 1.0), demonstrating better quality retention under stochastic sampling. (b) IFEval Strict Accuracy. While DPO suffers from rapid structural degradation as temperature rises, GANPO exhibits resilience, maintaining high instruction adherence even in high-noise regimes. Figure 3. Robustness against entropy. Comparison of model performance across varying sampling temperatures (T [0.0, 1.5]). Unlike DPO, which relies heavily on greedy decoding for peak performance and collapses under noise, GANPO acts as structural regularizer, effectively preserving both preference alignment and constraint satisfaction during high-entropy generation. 5.2. Results and Analysis distinct regimes: Preference alignment on open-ended instructions. Table 1 shows that GANPO consistently improves over its non-adversarial counterparts across both model scales on AlpacaEval-2.0. On Gemma2-2B-it, GANPO yields clear gain in both raw and length-controlled win rates over DPO (+1.41% LC-Win) and SimPO (+0.71% LC-Win), while maintaining comparable response lengths. Similar trends hold for Llama3-8B-Instruct, where GANPO improves LCWin rates around 1.5%-2.0% over DPO and SimPO. These results indicate that adversarial regularization provides benefits to preference optimization, improving alignment quality without relying on increased verbosity. Structural regularization under stochastic decoding. To examine the impact of the structured regularization imposed by GANPO under increasingly stochastic decoding, we stress-test the Gemma2-2B-it model across spectrum of sampling temperatures (T [0.0, 1.5]). Higher temperatures induce greater diversity but simultaneously amplify exposure bias and structural instability, serving as proxy for out-of-distribution robustness. Our evaluation spans two (1) AlpacaEval: We evaluate general response quality using the Skywork-Reward-V2-Llama-3-8B-it model (Liu et al., 2025) as an oracle judge. As shown in Figure 3a, GANPO consistently achieves higher winrate and reward scores than DPO across wide range of temperatures. Crucially, the winrate gap widens in high-entropy regimes (T 1.0). This divergence indicates that GANPO is more robust under highentropy generation, where DPOs token-level optimization becomes increasingly brittle. (2) IFEval (Zhou et al., 2023b): To assess the stability of instruction following under noise, we measure the strict prompt-level accuracy with structured outputs on IFEval (Figure 3b). Here, the contrast is stark: DPO suffers from more severe structural collapse as temperature increases (dropping nearly 20% in accuracy from = 0.5 to = 1.0), indicating that its adherence to constraints heavily relies on greedy decoding. In contrast, GANPO demonstrates stronger resilience, retaining good strict accuracy under stochastic sampling. Together, GANPO moves beyond surface-level alignment Latent Adversarial Regularization for Offline Preference Optimization and learns structurally robust manifold. As result, preference alignment and constraint adherence remain stable even when generation trajectories deviate from the optimal path, where purely likelihood-based methods tend to degrade. The effectiveness of D. To evaluate whether the trained discriminator can reliably distinguish highand low-quality representations, we train Gemma2-2B-it reward model on the UltraFeedback dataset as proxy reward and compare its scores against the gold standard Skywork-Reward-V2Llama-3-8B-it model (Liu et al., 2025). We conduct stress tests under high-entropy generation by sampling single response (N = 1) from large candidate pool (k = 1024) at elevated temperatures (T = 1.5 and = 2.0). In Figure 4, under these out-of-distribution conditions, the learned reward model exhibits severe reward hacking, collapsing to weak (r = 0.14) or even negative correlations (r = 0.50) with the oracle. In contrast, the discriminator maintains strong positive correlation (r = 0.59 and = 0.52), demonstrating robustness to distributional shift. These results suggest that the discriminator acts as an effective structural regularizer in latent space, capturing semantic properties rather than surface-level token patterns. Downstream evaluation on multiple benchmarks. Beyond AlpacaEval, GANPO does not degrade, and in several cases improves performance on downstream benchmarks, including math, reasoning, and factuality tasks  (Table 2)  . This suggests that the adversarial objective does not overfit to the preference dataset by sacrificing the performance on other tasks. Instead, it acts as form of structured regularization, encouraging representations that generalize beyond the alignment setting. Method Base DPO GANPO GSM8K (Math) MMLU (Knowledge) ANLI (R3) (Reasoning) TruthfulQA (Factuality) 46.32 48.37 48.67 56.73 57.02 56. 47.75 47.92 48.25 53.11 55.28 55.67 Table 2. Downstream evaluation on Gemma2-2B-it model. Architectures of D. Across all experiments, we observe that Transformer-based discriminator consistently outperforms simpler alternatives, such as fixed MSE critics or shallow MLPs, as shown in Table 3. This architectural choice enables the discriminator to provide holistic, sequence-level feedback, capturing long-range dependencies and structural properties of text that scalar or local critics fail to model. As reflected in the AlpacaEval results, stronger discriminators translate directly into higher win rates, underscoring the importance of expressive discriminator architectures for effective GAN preference optimization. 5.3. Discussion and Limitations Limitations. Unlike DPO and SimPO, which are parameterefficient and essentially reward-free in architecture, Discriminator Win Rate LC-Win Rate Avg Length N/A MSE (fixed D) MLP Transformer 22.76 22.96 22.66 24.17 27.79 27.03 27.52 29. 1668 1684 1671 1664 Table 3. Effect of Ds architecture on alignment performance. GANPO requires maintaining and updating discriminator alongside the policy. The computational overhead is illustrated in Table 4. The adversarial game introduces additional complexity in hyperparameter tuning compared to the stability of supervised objectives with modest additional cost. Also, our reference-anchored training stabilizes the discriminator by using the SFT model to define the target manifold. This effectively bounds the exploration space. If the SFT model is fundamentally misaligned or possesses defective latent structure, GANPO may struggle to diverge sufficiently to find globally optimal policy, effectively inheriting the topological flaws of its anchor. Lastly, we acknowledge that latent space and token space regularization may be complementary and require more investigation. Future Work. Standard alignment methods often struggle with strict syntactic constraints (e.g., valid JSON, compilable code). Future work should explore augmenting the discriminator with symbolic feedback, injecting compiler signals or logical verifiers directly into the latent loss, to enforce syntax as differentiable manifold constraint. Also, GANPO currently operates in an offline setting. Extending this to an online Self-Play framework where the model generates its own rollouts to be critiqued by an evolving discriminator could bridge the gap between offline efficiency and the performance benefits of online methods like PPO. Further, since GANPO operates on the representation space rather than discrete tokens, it is inherently modalityagnostic. Adapting this framework to Vision-Language Models (VLMs) could provide powerful method for aligning cross-modal generation, where structural consistency between text and image representations is critical. 6. Related Work Preference optimization objectives. Online preference optimization methods are often complex and difficult to stabilize (Zheng et al., 2023; Santacroce et al., 2023), motivating the development of simpler and more efficient offline alternatives. DPO (Rafailov et al., 2023a) is representative approach, but its lack of an explicit reward model limits its ability to leverage samples from the optimal policy. Prior work addresses this by augmenting preference data using SFT-generated responses or rejection sampling (Zhao et al., 2023; Liu et al., 2024a), and by extending DPO to iterative or self-improving training schemes (Dong et al., 2024; Kim et al., 2024; Rosset et al., 2024; Xiong et al., 2024; Yuan et al., 2024). For latent-space optimization, Hao 7 Latent Adversarial Regularization for Offline Preference Optimization (a) = 1.5 (b) = 2.0 Figure 4. Comparison of discriminator-based scoring and learned reward models under high-entropy generation. At elevated sampling temperatures (T = 1.5 and = 2.0), the learned reward model exhibits severe reward hacking, including correlation collapse and inversion with respect to the oracle. In contrast, the discriminator maintains strong positive correlation, demonstrating robustness to out-of-distribution generations and providing stable structural supervision in latent space. et al. (2025); Zhu et al. (2025) show that latent representations can lead to improved reasoning capabilities. In this work, we focus on offline alignment with latent-space regularization. We compare GANPO against DPO (Rafailov et al., 2023a) and SimPO (Meng et al., 2024), showing that GANPO consistently outperforms both with modest additional computational cost. GANs. GANs formulate learning as minimax game between generator and discriminator (Goodfellow et al., 2014), and GAN variants have been extensively studied for stabilizing adversarial distribution matching and improving training dynamics (Zhu et al., 2017; Gulrajani et al., 2017; Jolicoeur-Martineau, 2018; 2020). In natural language generation, GAN-based methods have been explored empirically (Zhang et al., 2016; 2017), where discriminator distinguishes generated text from human-written samples, and TextGAIL (Wu et al., 2021) adapts adversarial imitation learning to optimize language models as response policies. More recently, minimax formulations have been proposed for preference learning, such as the Adversarial Preference Optimization framework, in which the LLM and the reward model update alternatively via minmax game in an online manner (Cheng et al., 2023). Orthogonal to previous work, we introduce GAN-style adversarial regularizer operating in latent space, designed to complement offline preference optimization and mitigate exposure bias and structural degradation in LLM generation. Reinforcement learning from human feedback. Reinforcement Learning from Human Feedback (RLHF) is widely adopted paradigm for aligning large language models (LLMs) with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). classical RLHF pipeline typically consists of three stages: supervised fine-tuning (Zhou et al., 2023a; Taori et al., 2023; Geng et al., 2023; Conover et al., 2023; Kopf et al., 2023; Ding et al., 2023; Wang et al., 2024; Chen et al., 2024a; Xia et al., 2024), reward model training (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024b; Lightman et al., 2023; Havrilla et al., 2024; Lambert et al., 2024), and policy optimization, most commonly via Proximal Policy Optimization (Schulman et al., 2017; Anthony et al., 2017). Also, recent work has highlighted systemic challenges throughout the RLHF pipeline (Casper et al., 2023). Moreover, RLHF has been shown to induce unintended biases, such as excessive verbosity and length-based reward hacking (Dubois et al., 2024; Singhal et al., 2023; Wang et al., 2023b). In contrast, GANPO is fully offline alignment method orthogonal to RLHF, as it requires no online rollouts or reinforcement learning objectives. By introducing adversarial structural feedback in the latent space, GANPO shows great potential in future LLM alignment. 7. Conclusion We proposed GANPO, framework that augments preference learning with adversarial regularization to address the structural degradation inherent in offline methods like DPO. By leveraging Latent-Space Alignment and DualContrastive Objective, GANPO enables the discriminator to provide differentiable feedback that guides the policy toward high-quality, structurally sound modes. Furthermore, our Reference-Anchored Training ensures that this process remains stable and computationally efficient. Our experiments demonstrate that GANPO significantly outperforms stateof-the-art baselines like DPO and SimPO, particularly in controlling verbosity and maintaining structural coherence. This work validates the hypothesis that adversarial feedback, when applied to latent representations, serves as crucial regularizer for aligning LLMs with human preferences. Latent Adversarial Regularization for Offline Preference Optimization 8. Impact Statement This work advances preference optimization methods for large language models. The proposed approach improves the performance and robustness of offline alignment by introducing adversarial structural regularization. We do not anticipate societal or ethical impacts beyond those commonly associated with large language model training and alignment."
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. Chen, L., Li, S., Yan, J., Wang, H., Gunaratna, K., Yadav, V., Tang, Z., Srinivasan, V., Zhou, T., Huang, H., and Jin, H. AlpaGasus: Training better Alpaca with fewer data. In ICLR, 2024a. URL https://openreview.net/ forum?id=FdVXgSJhvz. Chen, L., Zhu, C., Soselia, D., Chen, J., Zhou, T., Goldstein, T., Huang, H., Shoeybi, M., and Catanzaro, B. ODIN: Disentangled reward mitigates hacking in RLHF. arXiv preprint arXiv:2402.07319, 2024b. Cheng, P., Yang, Y., Li, J., Dai, Y., and Du, N. Adversarial preference optimization. CoRR, 2023. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free dolly: Introducing the worlds first truly 9 open instruction-tuned LLM, 2023. URL https: //www.databricks.com/blog/2023/04/12/ dolly-first-open-commercially-viable-instruction-tuned-llm. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. UltraFeedback: Boosting language models with high-quality feedback. In ICML, 2024. Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. RLHF workflow: From reward modeling to online RLHF. arXiv preprint arXiv:2405.07863, 2024. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled AlpacaEval: simple way to debias automatic evaluators. ArXiv, abs/2404.04475, 2024. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. Koala: dialogue model for academic research. Blog post, April, 1:6, 2023. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J. E., and Tian, Y. Training large language models to In Second Conreason in continuous latent space. ference on Language Modeling, 2025. URL https: //openreview.net/forum?id=Itxz7S4Ip3. Havrilla, A., Raparthy, S., Nalmpantis, C., Dwivedi-Yu, J., Zhuravinskyi, M., Hambro, E., and Railneau, R. GLoRe: When, where, and how to improve LLM reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024. Huang, A., Zhan, W., Xie, T., Lee, J. D., Sun, W., Krishnamurthy, A., and Foster, D. J. Correcting the mythos of KL-regularization: Direct alignment without overoptimization via chi-squared preference optimization. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=hXm0Wu2U9K. Latent Adversarial Regularization for Offline Preference Optimization Jolicoeur-Martineau, A. The relativistic discriminator: key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Jolicoeur-Martineau, A. On relativistic f-divergences. In International Conference on Machine Learning, pp. 4931 4939. PMLR, 2020. Kim, D., Kim, Y., Song, W., Kim, H., Kim, Y., Kim, S., and Park, C. sDPO: Dont use your data all at once. ArXiv, abs/2403.19270, 2024. Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z. R., Stevens, K., Barhoum, A., Nguyen, D. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Lambert, N., Pyatkin, V., Morrison, J. D., Miranda, L. J. V., Lin, B. Y., Chandu, K. R., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith, N. A., and Hajishirzi, H. RewardBench: Evaluating reward models for language modeling. ArXiv, abs/2403.13787, 2024. Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, C. Y., Zeng, L., Xiao, Y., He, J., Liu, J., Wang, C., Yan, R., Shen, W., Zhang, F., Xu, J., Liu, Y., and Zhou, Y. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025. Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. Statistical rejection sampling In The Twelfth Inimproves preference optimization. ternational Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=xbjSwwrQOe. Liu, W., Bai, Y., Han, C., Weng, R., Xu, J., Cao, X., Wang, J., and Cai, X. Length desensitization in direct preference optimization. arXiv preprint arXiv:2409.06411, 2024b. Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. In NeurIPS, 2022. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023a. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023b. Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. Direct nash optimization: Teaching language models to self-improve with general preferences. ArXiv, abs/2404.03715, 2024. Santacroce, M., Lu, Y., Yu, H., Li, Y., and Shen, Y. Efficient RLHF: Reducing the memory usage of PPO. arXiv preprint arXiv:2309.00754, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Singhal, P., Goyal, T., Xu, J., and Durrett, G. long way to go: Investigating length correlations in RLHF. arXiv preprint arXiv:2310.03716, 2023. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, 2023. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Wang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023a. Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. OpenChat: Advancing open-source language models with mixed-quality data. In ICLR, 2024. URL https: //openreview.net/forum?id=AOJyfhWYHf. 10 Latent Adversarial Regularization for Offline Preference Optimization Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 22232232, 2017. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. Wu, Q., Li, L., and Yu, Z. Textgail: Generative adversarial imitation learning for text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1406714075, 2021. Xia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D. LESS: Selecting influential data for targeted instruction tuning. In ICML, 2024. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint. In Forty-first International Conference on Machine Learning, 2024. Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Zhang, Y., Gan, Z., and Carin, L. Generating text via adIn NIPS workshop on Adversarial versarial training. Training, volume 21, pp. 2132. Academia. edu, 2016. Zhang, Y., Gan, Z., Fan, K., Chen, Z., Henao, R., Shen, D., and Carin, L. Adversarial feature matching for text generation. In International conference on machine learning, pp. 40064015. PMLR, 2017. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. SLiC-HF: Sequence likelihood calibration with human feedback. ArXiv, abs/2305.10425, 2023. Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et al. Secrets of RLHF in large language models part I: PPO. arXiv preprint arXiv:2307.04964, 2023. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. LIMA: Less is more for alignment. NeurIPS, 2023a. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023b. Zhu, H., Hao, S., Hu, Z., Jiao, J., Russell, S., and Tian, Y. Reasoning by superposition: theoretical perspective on chain of continuous thought. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=UdOEZgWJLc. 11 Latent Adversarial Regularization for Offline Preference Optimization A. Relativistic Average Divergence Consider two probability distributions p, with support . divergence between two probability distributions is defined as follows. Definition A.1 (Statistical Divergence). Let denote the space of all probability distributions with support . function : is divergence if for all p, M: 1. D(p q) 0; 2. D(p q) = 0 = q. It is shown that the following divergence is well-defined. Proposition A.2 (Relativistic Average Divergence (Jolicoeur-Martineau, 2020)). Let : be concave function such that (0) = 0, is differentiable at 0, (0) = 0, supx (x) > 0, arg supx (x) > 0. Ley p, be two distributions with common support . Then, Df Ra(p q) := sup C:X Eyq [f (C(y) ExpC(x))] + Exp [f (EyqC(y) C(x))] is divergence. In the main paper (equation 5), we define the following term. DRa(pθ pref ) := sup ϕ (cid:104) Ehref (cid:2) log σ(cid:0)Cϕ(h) mθ (cid:1)(cid:3) + Ehθ (cid:2) log (cid:0)1 σ(cid:0)Cϕ(h) mref (cid:1)(cid:1)(cid:3)(cid:105) , (10) where Cϕ is the scalar logit from the discriminator parameterized by ϕ; denotes latent representation; σ denotes the sigmoid function; and the average baselines are mθ := Ehpθ [Cϕ(h)], mref := Ehpref [Cϕ(h)]. We can prove that equation 10 is indeed well-defined divergence given that Cϕ is taken over all functions mapping from the representation space R. Proposition A.3. The following term is divergence between pθ and pref . DRa(pθ pref ) := sup C:HR (cid:104) Ehref (cid:2) log σ(cid:0)C(h) mθ (cid:1)(cid:3) + Ehθ (cid:2) log (cid:0)1 σ(cid:0)C(h) mref (cid:1)(cid:1)(cid:3)(cid:105) . Proof. Define function : as We can see that DRa = Df Ra as follows. (x) = log σ(x) + log 2. First, it is straightforward to verify that is strictly concave; everywhere differentiable; (0) = log 2 + log 2 = 0; (0) = 1 2 == 0; supx (x) = log 2 > 0; arg supx (x) = + > 0. Thus, satisfies the conditions specified by Proposition A.2. To conclude the proof, we use identity 1 σ(x) = σ(x), and replace the log-sigmoid function by : DRa(pθ pref ) := sup (cid:104) Ehref (cid:2) log σ(cid:0)C(h) mθ (cid:1)(cid:3) + Ehθ C:HR (cid:104) = sup C:HR Ehref (cid:2) log σ(cid:0)C(h) mθ (cid:1)(cid:3) + Ehθ (cid:2) log (cid:0)1 σ(cid:0)C(h) mref (cid:2) log σ(cid:0)mref C(h)(cid:1)(cid:1)(cid:3)(cid:105) (cid:1)(cid:1)(cid:3)(cid:105) . (cid:104) = sup C:HR Ehref (cid:2)f (cid:0)C(h) mθ (cid:1)(cid:3) + Ehθ (cid:2)f (cid:0)mref C(h)(cid:1)(cid:1)(cid:3)(cid:105) = Df Ra(pθ pref ). Therefore, by Proposition A.2, DRa(pθ pref ) is well-defined divergence. 12 Latent Adversarial Regularization for Offline Preference Optimization B. Implementation Details and Hyperparameters Training details. For the preference optimization, we use batch size of 128 and train the models for 1 epoch. Additionally, we set the max sequence length to be 2048 and apply cosine learning rate schedule with 10% warmup steps on the preference optimization dataset. As for the learning rates of the generator, for Gemma2-2B-it experiments, we use the learning rate of 5.0e7; for Llama-3-8B-Ins experiments, we use the learning rate of 1e6. For training the discriminators, we use half of the learning rate of the generator for the discriminator training. Further, we set adversarial weight λ = 1 and moving average decay rate α = 0.9 for all experiments. For DPO training, we use β = 0.1 for all experiments; for SimPO training, we follow the setup as in Meng et al. (2024). For the generation stage, we use temperature of 0.7 for the Gemma2-2B-it setting and temperature of 0.9 for Llama3-8B-Instruct settings. Transformer architecture (Figure 10). We use transformer architecture (2 layers for Gemma2-2B-it experiments and 4 layers for Llama3-8B-Instruct experiments). The discriminator operates directly on continuous latent representations Input hidden states are first projected to lower-dimensional space via spectrally produced by the policy model. normalized linear layer to stabilize adversarial training. lightweight Transformer encoder with learned positional embeddings then models global and long-range dependencies across the sequence using pre-layer normalization. Sequencelevel representations are obtained via masked mean pooling, ensuring robustness to variable-length inputs. Finally, spectrally normalized MLP head maps the pooled representation to scalar score. This design enables the discriminator to capture holistic, structural properties of generation trajectories while remaining computationally efficient and stable. Computation environment. All the training experiments in this paper were conducted on 2H200 or 4xA100 GPUs. C. Computational Cost Analysis Table 4 shows that GANPO introduces only modest computational overhead compared to its corresponding DPO and SimPO baselines. On Gemma2-2B-it, GANPO (DPO) increases training time by less than 4% while using identical hardware, and exhibits similar scaling behavior on Llama-3-8B-Instruct. Although GANPO (SimPO) incurs larger overhead, it remains within the same GPU budget and does not require additional rollout generation or external teacher queries. Overall, these results demonstrate that adversarial regularization in GANPO can be incorporated into standard preference optimization pipelines with small additional cost, making it practical and scalable alternative to purely offline alignment methods. Method Gemma2-2B-it Llama-3-8B-Instruct Time GPU Time GPU DPO GANPO (DPO) SimPO GANPO (SimPO) 2h 31m 50s 2h 37m 24s 2h 11m 22s 3h 17m 26s 4 A100 4 A100 2 H200 2 H200 4h 20m 13s 4h 34m 08s 3h 10m 40s 4h 15m 52s 4 A100 4 A100 4 A100 4 A100 Table 4. Training time and hardware comparison between GANPO and its corresponding DPO/SimPO baselines. GANPO introduces only modest computational overhead while providing consistent alignment improvements. D. Additional Empirical Visualizations D.1. Win-Rate versus Response Length Figure 5 analyzes win rates across different response length buckets. DPO shows clear degradation in performance as responses become longer, consistent with prior observations that offline preference optimization tends to exploit lengthrelated artifacts. In contrast, GANPO maintains stable and consistently higher win rates for medium and long responses, suggesting that adversarial structural regularization mitigates verbosity bias and improves preference alignment beyond token-level heuristics. D.2. Margins Analysis Across training, GANPO consistently achieves larger preference margins than both DPO and SimPO. We show it happens for both different models and OOP objectives (Figure 6, Figure 7, Figure 8, and Figure 9), indicating clearer separation 13 Latent Adversarial Regularization for Offline Preference Optimization Figure 5. Win rate as function of response length for DPO and GANPO. Figure 6. Evolution of reward margins during training on Gemma2-2B-it, comparing DPO and GANPO. between preferred and rejected responses. These margins increase steadily over optimization, suggesting more stable and effective preference learning dynamics. In contrast to purely likelihood-based objectives, the adversarial component in GANPO provides structured feedback in latent space, which helps reinforce robust preference separation rather than relying on surface-level token correlations. When combined with SimPO, GANPO also shows margin growth, demonstrating that adversarial structural regularization complements existing offline preference objectives by strengthening latent alignment without destabilizing training. 14 Latent Adversarial Regularization for Offline Preference Optimization Figure 7. Reward margin comparison between SimPO and GANPO on Gemma2-2B-it. Figure 8. Evolution of reward margins during training on Llama-3-8B-Instruct, comparing DPO and GANPO. Figure 9. Reward margin comparison between SimPO and GANPO on Llama-3-8B-Instruct. 15 Latent Adversarial Regularization for Offline Preference Optimization ) self.transformer = nn.TransformerEncoder( encoder_layer, num_layers=num_layers spectral_norm(nn.Linear(hidden_dim, hidden_dim)), nn.GELU(), spectral_norm(nn.Linear(hidden_dim, 1)) # Dimension of LLM hidden states # Internal dimension for Discriminator # Shallow Transformer depth # Number of attention heads # Max supported sequence length self.project_in = spectral_norm(nn.Linear(input_dim, hidden_dim)) torch.randn(1, max_seq_len, hidden_dim) * 0.02 encoder_layer = nn.TransformerEncoderLayer( ) ) ): super().__init__() self.head = nn.Sequential( self.pos_embedding = nn.Parameter( d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4, dropout=dropout, activation=gelu, batch_first=True, norm_first=True def __init__( self, input_dim=4096, hidden_dim=512, num_layers=2, num_heads=8, max_seq_len=2048, dropout=0. 1 import torch 2 import torch.nn as nn 3 from torch.nn.utils import spectral_norm 4 5 class TransformerDiscriminator(nn.Module): 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 src_key_padding_mask = None if mask is not None: = self.project_in(hidden_states) return self.head(pooled_output) self.apply(self._init_weights) pooled_output = x.mean(dim=1) if isinstance(m, nn.Linear): def _init_weights(self, m): if mask is not None: else: else: ) torch.nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) def forward(self, hidden_states, mask=None): batch_size, seq_len, _ = hidden_states.size() if seq_len <= self.pos_embedding.size(1): = + self.pos_embedding[:, :seq_len, :] = + self.pos_embedding[:, :self.pos_embedding.size(1), :] src_key_padding_mask = (mask == 0) = self.transformer(x, src_key_padding_mask=src_key_padding_mask) mask_expanded = mask.unsqueeze(-1).expand_as(x) sum_embeddings = torch.sum(x * mask_expanded, dim=1) sum_mask = mask.sum(dim=1, keepdim=True).clamp(min=1e-9) pooled_output = sum_embeddings / sum_mask Figure 10. PyTorch implementation of the Transformer Discriminator."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Stanford University",
        "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
    ]
}