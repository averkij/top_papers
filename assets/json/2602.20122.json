{
    "paper_title": "NanoKnow: How to Know What Your Language Model Knows",
    "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow."
        },
        {
            "title": "Start",
            "content": "NanoKnow: How to Know What Your Language Model Knows Nour Jedidi University of Waterloo Waterloo, ON, Canada njedidi@uwaterloo.ca Lingwei Gu University of Waterloo Waterloo, ON, Canada lingwei.gu@uwaterloo.ca Jimmy Lin University of Waterloo Waterloo, ON, Canada jimmylin@uwaterloo.ca 6 2 0 2 3 2 ] . [ 1 2 2 1 0 2 . 2 0 6 2 : r Abstract How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often black box unknown or inaccessible. The recent release of nanochat family of small LLMs with fully open pretraining data addresses this as it provides transparent view into where models parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochats pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnows utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pretraining data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) have demonstrated remarkable ca-\npabilities across a wide range of tasks, yet it is unclear how they\nknow what they know. While LLMs ultimately express their knowl-\nedge through their outputs at inference time, how and where this\nknowledge is acquired remains an open question.",
            "content": "Knowledge expressed by LLMs can originate from various, potentially entangled, sources. There exists knowledge stored within their parameters [19], which can be probed via closed-book question answering [8], but this only tells us what LLMs know, not necessarily how that knowledge was acquired. For example, did this knowledge come from memorization of its pre-training data [3] or is the model performing sort of multi-hop reasoning over facts encoded within its parameters [27]? Alternatively, external knowledge can be injected into the LLM using retrieval-augmented generation (RAG), but in this case does the models output solely represent facts present in the external context or does the output represent latent interaction between the external context and the LLMs parametric knowledge [28]? Ultimately, answering these questions requires understanding the models pre-training data, but this has been difficult as such data is often unknown or inaccessible [13]. Equal Contribution Recently, this changed with developments in fully open LLMs, making the understanding of the pre-training data now possible. notable example is the release of nanochat [10], which, by being pre-trained on the open FineWeb-Edu corpus 100-billion-token collection of educational web content [16] provides completely transparent and self-contained environment for tracing the information an LLM has seen. Such transparency allows us to answer questions like: does seeing facts more often make it easier to recall? When does RAG actually make difference? However, transparency of data is only the first step. Before we can answer these questions systematically, we require resource which can not only identify questions an LLM has seen the answer to during pre-training but also questions beyond its knowledge. Such resource is necessary step toward properly disentangling and understanding the various sources of knowledge LLMs rely on when producing their outputs. To address this, we release NanoKnow, benchmark dataset of questions from Natural Questions (NQ) [11] and SQuAD [18] projected onto the FineWeb-Edu corpus. NanoKnow partitions each dataset into two splits supported (questions for which the answer exists in the pre-training data) and unsupported (questions for which the answer does not exist in the pre-training data) enabling controlled evaluation of knowledge in LLMs, like nanochat, which were pre-trained entirely on FineWeb-Edu. To generate these relevance judgments, NanoKnow was built in three stages. In the first stage, we build searchable BM25 index over the corpus using Anserini [26] and retrieve candidate documents for each question. Next, we check for exact match answer strings across the retrieved documents. In the last stage, we use LLM-based verification to filter out coincidental matches, keeping only documents that genuinely answer the questions. With NanoKnow in hand, we run comprehensive experiments using eight nanochat checkpoints across three different model scales. Our various experiments demonstrate the value of NanoKnow as tool for confidently disentangling and evaluating the contributions of different knowledge sources underlying an LLMs outputs. Using NanoKnow, we were able to confirm and replicate range of results across the literature [2, 3, 6, 9, 14, 22], highlighting its reliability: (1) Closed-book question answering effectiveness is highly related to answer frequency in the pre-training corpus. We found clear increase in nanochats accuracy when it has seen the answer more often. (2) Integrating external evidence mitigates this dependence on memorization, but even with external evidence, nanochat is more effective on questions with higher answer frequency in the pre-training corpus. (3) Even when provided the oracle answer document, nanochat was more accurate on supported versus unsupported questions, demonstrating that parametric knowledge can complement external knowledge. Figure 1: An overview of how NanoKnow was built. If any of the retrieved passages from the FineWeb-Edu corpus are deemed to properly answer the question, we label the question as supported. Otherwise, the question is considered unsupported. (4) Despite nanochat having seen the answer to question, it is negatively impacted by distractor documents (i.e., non-relevant documents). We found clear decline in accuracy based on where it is positioned with respect to distractors as well as how many distractors are present. We hope NanoKnow provides foundation for future explorations in understanding how LLMs know what they know."
        },
        {
            "title": "2.1 Projection Results\nWe apply NanoKnow to the shuffled version of the FineWeb-Edu\ncorpus released by Karpathy [10].1 FineWeb-Edu comes as 1,823\nparquet shards, each containing thousands of web documents. The\ntotal corpus size is about 171GB and contains 97,230,848 documents.\nAfter indexing with Anserini [26], the index size is about 326GB.\nThe FineWeb-Edu corpus was chosen in our experiments primarily\ndue to the recent release of nanochat [10], a family of small LLMs\nthat utilized it for pre-training.",
            "content": "We project the following QA benchmarks onto FineWeb-Edu: Natural Questions (NQ) [11]: Open-domain questions from Google search queries. We use the validation set (3,610 questions). Each question has one or more short answers. SQuAD [18]: Reading comprehension questions where answers are spans from Wikipedia passages. We use the validation set (10,570 questions)."
        },
        {
            "title": "2.1.1 Relevance Judgments. From these QA benchmarks, we build\nrelevance judgments that split each dataset in two:\n• Supported: Questions where the answer appears in FineWeb-",
            "content": "Edu in relevant context. Unsupported: Questions where the answer does not appear in any retrieved document or only shows up in unrelated contexts. 1https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle Table 1 shows examples of each from the NQ dataset. For each supported question, the relevance judgments record which documents contain the answer. These two splits allow us to compare model effectiveness on questions it saw during pre-training versus questions which are outside the models knowledge. Table 2 shows the projection results. The String Match column shows the percentage of questions which have an answer in at least one retrieved document. LLM Verified is how many survive after filtering out coincidental matches. The overlap is high for both benchmarks. For NQ, 73.9% of questions have the answer string in retrieved document; after LLM verification, 66.2% are confirmed to be supported. SQuAD is even higher at 70.9% verified. This is not surprising: SQuAD answers come from Wikipedia, and FineWebEdu has lot of Wikipedia-derived content. About 11% of string matches turn out to be coincidental. The LLM verification process catches most of these."
        },
        {
            "title": "2.1.2 Relevance Judgments Format and Corpus Access. For each\nquestion with a match in the corpus, we store relevance judgments\nlinking the question to its answer locations in FineWeb-Edu. Each\nentry contains the question ID, question and answer text, docu-\nment ID (shard and row offset, e.g., shard_00151_20323), and the\ncharacter offset where the answer appears.",
            "content": "Each document gets unique ID that encodes its location in the corpus. The format is shard_XXXXX_YYYYY, where XXXXX is the zero-padded shard number and YYYYY is the row offset within that shard. For example, shard_00151_20323 refers to row 20,323 in shard 151. This encoding lets us trace any retrieved document back to its exact location in the original parquet files. The document ID encoding enables efficient corpus access. Since each ID contains the shard number and row offset, we can fetch the original document directly from the parquet files without scanning. For example, using DuckDB, given shard_00151_20323, we parse the shard number (151) and row offset (20,323), then run single-row query on the corresponding parquet file. This gives submillisecond latency for document retrieval, which matters when running thousands of RAG experiments. The character offsets let us extract answer passages of any length. For our experiments, we pull 256 words before and after the answer match, but researchers can adjust this window as needed. Since the Table 1: Examples of supported and unsupported NQ questions. Supported example includes the matching passage from FineWeb-Edu. Question When was the last time anyone was on the moon? What is the main artery that takes blood from the heart to the body? Why does kerosene rise up in the wick of lantern? Answer Supported December 1972 The aorta Capillary action Who sang ran all the way home? Unsupported The Impalas Who plays Gram on The Young and the Restless? Max Shippee Love Yourself by Justin Bieber is about who? Rihanna Evidence No one has walked on the Moon since December 1972. Arteries begin with the aorta, the large artery leaving the heart. ...the kerosene burns, capillary action in the wick draws more kerosene up from the fuel tank. Table 2: Projection rates for NQ and SQuAD on FineWeb-Edu. The reported percentage represents how many questions are supported after the string matching (String Match) and subsequent LLM-based verification (LLM Verified) steps."
        },
        {
            "title": "Dataset\nNQ\nSQuAD",
            "content": "Samples 3,610 10,570 String Match LLM Verified 66.2% 70.9% 73.9% 78.9% relevance judgments store both the document ID and the character offset, the context can be efficiently retrieved by DuckDB without scanning the full corpus. Validation. To check that our unsupported labels are accurate, we prompt the official d32 nanochat checkpoints with unsupported questions in closed-book setting.2 We found an accuracy of 1.5% and 0.8% for SQuAD and NQ when evaluating using the answer string matching approach we discuss in Section 2.2. Released Artifacts. We release the following to support reproducibility and future research: Qrels: Relevance judgments mapping questions in NQ and SQuAD to FineWeb-Edu documents. For each data split (supported or unsupported), we provide file with corresponding question IDs. Lucene Index: Pre-built index over FineWeb-Edu (326GB).3 Evaluation Code: Scripts to reproduce all experiments, including LLM-Judge prompts and evaluation metrics. All artifacts are available at https://github.com/castorini/NanoKnow."
        },
        {
            "title": "2.2 Pipeline\nWe now discuss how we projected NQ and SQuAD onto FineWeb-\nEdu. A high-level overview of the pipeline is shown in Figure 1.",
            "content": "2karpathy/nanochat-d32 3https://huggingface.co/datasets/LingweiGu/NanoKnow-Fineweb-Edu-Index System: You verify whether answer knowledge exists in text. Check if test Q&A pairs appear in the pre-training corpus. Be STRICT. Only mark TRUE if the context DIRECTLY answers the question. User: QUESTION: {question} ANSWER FOUND IN TEXT: {matched_answer} CONTEXT WHERE ANSWER APPEARS: {context} Does the CONTEXT directly answer the QUESTION using the ANSWER? Respond with: TRUE: [reason] or COINCIDENTAL: [reason] Figure 2: Prompt used for LLM-based verification. Given question-answer pair from NQ or SQuAD, we project it using the following three steps: Step 1: BM25 Retrieval. Using BM25, we first search the index to retrieve documents that may contain the answer. We retrieve the top 100 candidate documents, leveraging Pyserini [12] for retrieval. Step 2: Answer String Matching. Next, we check if any retrieved document contains the answer. We lowercase everything and strip extra whitespace, then look for the answer as substring. If it shows up, we flag the question as candidate match. This is fast, but returns many false positives. For example, for the question What is the best bakery in Paris? the word Paris might appear in document about the song Paris and not Paris, France. Another step is needed to filter these out. Step 3: LLM-Based Verification. To address the false positives, we leverage an LLM to separate real matches from coincidental ones. For each candidate, we extract context window around where the answer appears: 256 words before the match and 256 words after, giving roughly 512 words total. We then send this context to the LLM along with the question and ask it to classify the match. We leverage Qwen3-8B [25] as the LLM for this, using greedy decoding (i.e., temperature of 0). The prompt used is in Figure 2."
        },
        {
            "title": "3 Experimental Setup\nNanoKnow now allows us to answer many interesting questions\nregarding how pre-training data shapes what knowledge LLMs rely\non. We study a subset of these questions:",
            "content": "Does seeing the answer to question more often in pre-training improve closed-book QA accuracy? What if we integrate external evidence? How does closed-book QA compare to open-book QA on supported questions? How does an LLMs open-book QA accuracy differ on supported versus unsupported questions? On supported questions, how do distractors (i.e., non-relevant information) influence an LLMs QA accuracy? To answer these questions, we make use of the nanochat [10] family of models, which were entirely pre-trained on the FineWeb-Edu corpus discussed in Section 2. We consider three nanochat model sizes: d20 ( 561M parameters); d32 ( 1.9B parameters); and d34 Figure 3: Distribution of NanoKnows supported questions by answer frequency in FineWeb-Edu for NQ and SQuAD. ( 2.2B parameters). To ensure the robustness of our results to any variations in how models were trained, each evaluation is run with multiple open-source checkpoints for each model scale. Across experiments, we use any of the following three prompting setups. The first is Closed-Book, where nanochat is only prompted with the question. The second is w/ FineWeb context, where nanochat is reminded with the oracle answer passage from its pre-training data; and, lastly, for SQuAD we also evaluate nanochat with the original context, w/ Original Context, where nanochat is provided the original answer context from SQuAD.4 As FineWeb-Edu documents are very long, for these experiments, we only consider the surrounding context window of 200 words around the first matched answer (approximately 100 words before and 100 words after the answer). To evaluate the accuracy of responses generated by nanochat, we use two approaches. The first is an exact match (EM) evaluation which checks if any of the predefined correct answers exactly appear in the models output. If there is match, the answer is deemed correct; otherwise the answer is deemed incorrect. The next method we consider is an LLM-Judge, which given nanochats output and the predefined correct answers, classifies nanochats output as correct or not. For this, we leverage Qwen3-14B [25]."
        },
        {
            "title": "4 Results\n4.1 Impact of Answer Frequency in Pre-training\nWe begin by measuring how closed-book and open-book QA ac-\ncuracy are impacted by how often the answer was replicated (i.e.,\n“seen”) during pre-training. To study this, we measure how accuracy\nchanges with answer frequency in the pre-training corpus.",
            "content": "To measure frequency, for given question, we count the number of FineWeb-Edu documents in which the answer was found and verified by the LLM in step 3 of Section 2.2. We then categorize questions into four frequency buckets: Rare (15 verified documents), Low (620), Medium (2150), and High (51+). Figure 3 shows the distribution of supported questions across these buckets; the majority of questions fall in the Rare and Low frequency buckets for both NQ and SQuAD. The relationship between answer frequency and nanochats accuracy is shown in Figure 4. We find clear increase in closed-book 4As NQ is an open-domain QA task, there does not exist singular, default original context, thus for simplicity we do not consider it in our experiments. Figure 4: Influence of pre-training data answer frequency on nanochats accuracy. Solid lines show the closed-book prompt setup; dashed lines show w/ FineWeb context. QA effectiveness on both NQ and SQuAD as the answer frequency increases, with accuracy more than doubling for questions with high answer frequency versus rare answer frequency. However, interestingly we did not find this to be the case for d20 (omitted from the plot), suggesting that at smaller parameter counts the LLM does not have the capacity to memorize information. When integrating external evidence (i.e., open-book QA), there is also general increase in accuracy as answer frequency in the pre-training data increases. However, the rate of this improvement, especially for SQuAD, is much lower than in the closed-book setting, demonstrating that RAG can help mitigate this dependence on pretraining frequency."
        },
        {
            "title": "4.2 Closed-Book QA vs. Open-Book QA\nWe next examine how much improvement external knowledge\nprovides over the LLM’s parametric knowledge. The results of this\nexperiment can be found in Table 3.",
            "content": "As expected, we see clear upward trend in closed-book accuracy for both SQuAD and NQ as model size increases, demonstrating that larger nanochat checkpoints indeed memorize more of their training data [3, 21]. In particular, for SQuAD, the closed-book LLMJudge accuracy improves by 21.7 points (19.7%) when comparing the best nanochat-d20 (row 2) to the best nanochat-d34 checkpoint (row 6). With NQ, accuracy improves by 26.9 points (19.2%). Comparing the Closed-Book versus w/ FineWeb Context columns, we find that all nanochat checkpoints see large jump in Table 3: Comparing closed-book versus open-book QA over the SQuAD and NQ supported splits of NanoKnow. Closed-Book w/ FineWeb Context w/ Original Context Closed-Book w/ FineWeb Context SQuAD NQ Model Checkpoint Model Size EM LLM-Judge EM LLM-Judge EM LLM-Judge EM LLM-Judge EM LLM-Judge 1 2 3 4 5 6 7 8 sampathchanda/nanochat-d20 shu127/nanochat-d20 pankajmathur/nanochat-d20 karpathy/nanochat-d32 Antigma/nanochat-d renatocastro33/nanochat-d34-sft victoremnm/nanochat-d34-sft pankajmathur/nanochat-d34-finetuned 561M 1.9B 2.2B 0.004 0.003 0.005 0.114 0. 0.167 0.167 0.141 0.002 0.011 0.008 0.173 0.169 0.228 0.227 0.210 0.021 0.016 0.022 0.465 0. 0.512 0.512 0.476 0.019 0.013 0.020 0.540 0.551 0.587 0.587 0.569 0.086 0.042 0.056 0.672 0. 0.721 0.721 0.670 0.081 0.040 0.052 0.736 0.740 0.779 0.777 0.749 0.004 0.003 0.003 0.196 0. 0.250 0.250 0.239 0.008 0.005 0.014 0.224 0.226 0.283 0.277 0.271 0.022 0.021 0.028 0.468 0. 0.503 0.503 0.479 0.018 0.016 0.023 0.476 0.492 0.528 0.523 0.522 Table 4: QA accuracy for supported versus unsupported questions on SQuAD (w/ Original Context). Model Checkpoint Model Size sampathchanda/nanochat-d20 shu127/nanochat-d20 pankajmathur/nanochat-d20 karpathy/nanochat-d32 Antigma/nanochat-d32 renatocastro33/nanochat-d34-sft victoremnm/nanochat-d34-sft pankajmathur/nanochat-d34-finetuned 561M 1.9B 2.2B Supported EM LLM-Judge Unsupported EM LLM-Judge 0.086 0.042 0.056 0.672 0.686 0.721 0.721 0. 0.081 0.040 0.052 0.736 0.740 0.779 0.777 0.749 0.069 0.032 0.051 0.554 0.553 0.610 0.610 0. 0.068 0.031 0.054 0.688 0.680 0.737 0.733 0.702 accuracy when provided an answer passage from its pre-training data as additional context. On average, the relative improvement of incorporating FineWeb context decreases as the nanochat model size increases. For example, on NQ, we see an average LLM-Judge accuracy improvement of 2.4, 2.14, and 1.89, for the d20, d32, and d34 model scales, respectively. With SQuAD, the average LLMJudge accuracy improvement is 4.4, 3.2, and 2.6, for d20, d32, and d34. This result suggests that smaller models benefit more from open-book QA versus larger models. Lastly, we find that each of the model checkpoints is more accurate on SQuAD when utilizing the original context versus the FineWeb context. This makes sense since the original context is tailored specifically to answer the question; in other words, the FineWeb context is like textbook, whereas the original context is the answer booklet."
        },
        {
            "title": "4.4 Influence of Distractors\nLastly, we are interested in understanding how nanochat is in-\nfluenced by distractors (i.e., non-relevant contexts). For this ex-\nperiment, we follow the setup in Cuconasu et al. [6]. We prompt",
            "content": "nanochat using three different placements of the answer contexts and the distractor: Far in which the answer context is placed furthest away from the question; Mid, in which the answer context is placed in the middle of the prompt, in between different distractor contexts; and Near, in which the answer context is placed closest to the question. We additionally consider setting in which nanochat is only prompted with distractor context (Distractor only). We compare all distractor setups to the closed-book and w/ FineWeb context (Answer only) settings shown in Table 3. For this experiment, we focus on Antigma/nanochat-d32, the strongest d32 checkpoint. The results are shown in Table 5. Beginning with comparing closed-book (row 1) to the distractor only setting (row 2), we find that prompting nanochat with nonrelevant context does worse than utilizing the models parametric knowledge, with the LLM-Judge accuracy dropping by 1.5 and 3.2 points on SQuAD and NQ, respectively. The negative influence of the distractor context on nanochats effectiveness is further confirmed when comparing the answer only setting (row 3) to each of the answer + distractor settings (rows 4 to 6). When prompted with answer and distractor documents, as might be expected in practical RAG setting, nanochat is consistently less accurate than when only prompted with the correct answer context, across all prompt setups (far, mid, near). Furthermore, nanochat is also less accurate when prompted with more distractors. Using the Far prompt on SQuAD as representative case, the LLM-Judge accuracy drops from 0.478 (1 distractor) to 0.367 (4 distractors). Lastly, the results show that nanochat is most accurate when the answer context is closest to the question. But notably, being closer to the question is only helpful when there are no distractors between the answer and the question, as nanochat has lost in the middle effect [14], where it is least effective when the answer context is placed between distractors."
        },
        {
            "title": "5 Related Work",
            "content": "Tracing an LLMs capabilities to its pre-training data. LLMs pick up wide range of factual knowledge during their pre-training, but identifying where in the pre-training data the LLM learned that knowledge remains an open research question. Much of the research in this area fits directly under the umbrella of training data attribution methods [1, 4, 20], which try to find the pre-training data that can explain models output. This has been commonly done via gradient-based or representation-based methods [20]. Other works, Table 5: Influence of distractors on nanochats (Antigma/nanochat-d32) effectiveness on supported questions. denotes the answer document, denotes the distractor document, and denotes the question. Far: [A, D, Q] Mid: [D, A, D, Q] Near: [D, A, Q]"
        },
        {
            "title": "SQuAD",
            "content": "NQ"
        },
        {
            "title": "SQuAD",
            "content": "NQ"
        },
        {
            "title": "SQuAD",
            "content": "NQ Setting 1 Closed-Book 2 Distractor only 3 Answer only 4 Answer + 1 Distractor 5 Answer + 2 Distractors 6 Answer + 4 Distractors EM LLM-Judge EM LLM-Judge EM LLM-Judge EM LLM-Judge EM LLM-Judge EM LLM-Judge 0.122 0.091 0.483 0.411 0.363 0.287 0.169 0.154 0.551 0.478 0.438 0. 0.198 0.152 0.516 0.452 0.414 0.357 0.226 0.194 0.492 0.447 0.422 0.378 0.122 0.091 0.483 0.352 0. 0.169 0.154 0.551 0.428 0.363 0.198 0.152 0.516 N/A 0.387 0.334 0.226 0.194 0. 0.406 0.368 0.122 0.091 0.483 0.428 0.397 0.369 0.169 0.154 0.551 0.501 0.480 0.457 0.198 0.152 0. 0.456 0.433 0.417 0.226 0.194 0.492 0.457 0.448 0.432 such as FASTTRACK [5] and OLMoTrace [13] take more retrievaloriented approach, leveraging semantic clustering or lexical overlap to match the LLMs outputs to training examples. In fact, it was shown by Akyürek et al. [1] that even simple approach like BM25 can serve as strong baseline for tracing models outputs back to training examples. There have also been other works that have proposed methods for mapping task specific data back to pre-training data priori, with the goal of understanding how knowledge contained in, or properties of, the pre-training data links to capabilities of LLMs on specific downstream tasks. For example, Kandpal et al. [9] proposed an approach which counts how often specific question and answer entities appear in documents in the pre-training corpus, with the aim of studying how the number of relevant documents to question relates to its answering accuracy similar to our experiment in Section 4.1. More recently, Wang et al. [22], proposed method to measure an LLMs memorization versus generalization by mapping its output distribution to the task-specific pre-training data frequency. Interplay of parametric versus external knowledge. It has been shown in previous works [9] and further demonstrated in our experiments that LLMs struggle to recall knowledge which is less frequent in its pre-training data. Even more so, parametric knowledge can become obsolete as time goes on. To address these shortcomings, RAG has been proposed as an approach to feed the LLM external knowledge at inference time to guide its generated output. With this, various research [7, 15, 17, 23] has focused on understanding this interplay between parametric knowledge within the LLMs and external knowledge provided to the LLM; see Xu et al. [24] for survey on the topic. We note, however, that large chunk of these works have focused on evaluating this interplay for LLMs in which the pre-training data is unknown. This makes it difficult to properly disentangle the effects of parametric and external knowledge since the knowledge the LLM knows is unclear. NanoKnow provides benchmark in which the interplay of the different knowledge sources can be explored confidently."
        },
        {
            "title": "6 Conclusion\nIn this paper, we set out to answer a simple question: how do LLMs\nknow what they know? Towards this goal, we introduced NanoKnow,\na benchmark dataset that identifies questions which nanochat –",
            "content": "or any LLM entirely pre-trained on FineWeb-Edu has seen the answer to during pre-training, along with the pipeline to produce it. By projecting Natural Questions and SQuAD onto FineWeb-Edu, we found that over 66% and 71% of questions, respectively, have verifiable supported answers. NanoKnow now allows us to answer many interesting questions regarding how pre-training data shapes what language models know. Using NanoKnow, we ran controlled experiments across various nanochat checkpoints, providing clear picture of how parametric knowledge and external knowledge interact. What model knows is largely function of frequency: answers seen often are recalled reliably, while rare answers require external help. RAG closes this gap, improving accuracy where parametric knowledge is weakest. Even with RAG, models perform better on questions they have already seen, showing that parametric knowledge and external knowledge complement each other. At the same time, external knowledge is fragile: distractors degrade accuracy, particularly when the answer is buried in the middle of the context, highlighting the importance of retrieval precision in practical RAG systems. Taken together, these findings replicate wide range of results in the literature, further underscoring the reliability of NanoKnow. While we built NanoKnow on FineWeb-Edu, the methodology used to create it can be extended to any open corpus. Indexing is one-time cost, and projecting new benchmarks afterward is fast. This opens the door to questions we have not yet explored thoroughly: how does the topical composition of pre-training data influence downstream capabilities? Can answer frequency information guide more effective data curation? We leave these directions to future work and release NanoKnow for the community to conduct fair and controlled studies of how training data shapes what LLMs can and cannot do. Acknowledgments This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada. References [1] Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. 2022. Towards Tracing Knowledge in Language Models Back to the Training Data. In Findings of the Association for Computational Linguistics: EMNLP 2022. 24292446. [2] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: Suite for Analyzing Large Language Models Across Training and Scaling. In International Conference on Machine Learning. 23972430. [3] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying Memorization Across Neural Language Models. In The Eleventh International Conference on Learning Representations. [4] Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, and Ian Tenney. 2025. Scalable Influence and Fact Tracing for Large Language Model Pretraining. In The Thirteenth International Conference on Learning Representations. [5] Si Chen, Feiyang Kang, Ning Yu, and Ruoxi Jia. 2024. FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation. In Findings of the Association for Computational Linguistics: EMNLP 2024. 58215836. [6] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024). 719729. [7] Mehrdad Farahani and Richard Johansson. 2024. Deciphering the Interplay of Parametric and Non-Parametric Memory in Retrieval-Augmented Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP). 1696616977. [8] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Language Models Know? Transactions of the Association for Computational Linguistics 8 (2020), 423438. [9] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large Language Models Struggle to Learn Long-Tail Knowledge. In International Conference on Machine Learning. 1569615707. [10] Andrej Karpathy. 2025. nanochat: The Best ChatGPT That $100 Can Buy. https: //github.com/karpathy/nanochat [12] [13] [11] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452466. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021). 23562362. Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, Yen-Sung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Yu-Yen Cheng, Karen Farley, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, and Jesse Dodge. 2025. OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). 178188. [14] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics 12 (2024), 157173. [15] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 98029822. [16] Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In Proceedings of the 38th International Conference on Neural Information Processing Systems. [17] Cheng Qian, Xinran Zhao, and Tongshuang Wu. 2024. \"Merge Conflicts!\" Exploring the Impacts of External Knowledge Distractors to Parametric Knowledge Graphs. In First Conference on Language Modeling. [18] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). 23832392. [19] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack Into the Parameters of Language Model?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 54185426. [20] Weiwei Sun, Haokun Liu, Nikhil Kandpal, Colin Raffel, and Yiming Yang. 2025. Enhancing Training Data Attribution with Representational Optimization. arXiv preprint arXiv:2505.18513 (2025). [21] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. In Proceedings of the 36th International Conference on Neural Information Processing Systems. [22] Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, and William Yang Wang. 2025. Generalization v.s. Memorization: Tracing Language Models Capabilities Back to Pretraining Data. In The Thirteenth International Conference on Learning Representations. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts. In The Twelfth International Conference on Learning Representations. [24] Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge Conflicts for LLMs: Survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP). 85418565. [23] [25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025). [26] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017). 12531256. [27] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024. Do Large Language Models Latently Perform Multi-Hop Reasoning?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1021010229. Jun Zhao, Yongzhuo Yang, Xiang Hu, Jingqi Tong, Yi Lu, Wei Wu, Tao Gui, Qi Zhang, and Xuanjing Huang. 2025. Understanding Parametric and Contextual Knowledge Reconciliation within Large Language Models. In Proceedings of the 39th International Conference on Neural Information Processing Systems. [28]"
        }
    ],
    "affiliations": [
        "University of Waterloo"
    ]
}