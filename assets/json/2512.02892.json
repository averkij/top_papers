{
    "paper_title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
    "authors": [
        "Amr Mohamed",
        "Yang Zhang",
        "Michalis Vazirgiannis",
        "Guokan Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\\times$ speedups while retaining $99.8$-$100\\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\\%$ performance retention, with up to $2.34\\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 9 8 2 0 . 2 1 5 2 : r FAST-DECODING DIFFUSION LANGUAGE MODELS VIA PROGRESS-AWARE CONFIDENCE SCHEDULES Amr Mohamed1,2, Yang Zhang2, Michalis Vazirgiannis1,2, Guokan Shang1 1MBZUAI, 2Ecole Polytechnique"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion large language models (dLLMs) offer promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, training-free, model-agnostic earlyexit algorithm that aggregates full-span logit margins and halts decoding once smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.84.0 speedups while retaining 99.8100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1100% performance retention, with up to 2.34 under more aggressive settings. Using conservative speed metric that heavily penalizes quality loss (QPS,γ=4), we show that SchED is robust and clearly outperforms prior confidence-based earlyexit methods, which break down on long-form generation. An entropy analysis of the models token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have evolved rapidly in recent years, but the dominant decoding paradigm remains autoregressive (AR), which is inherently sequential and constrains opportunities for parallel generation and global context use (Brown et al., 2020; Yin et al., 2024; Zhang et al., 2025b). In response, Diffusion Large Language Models (dLLMs) have emerged as credible alternative to AR decoding, offering compelling advantages such as parallel refinement, flexible infilling, and bidirectional attention over the partially generated sequence (Zou et al., 2023). This paradigm has matured rapidly, with efforts to train powerful base and instruct models from scratch (Ye et al., 2025), adapt existing AR checkpoints (Gong et al., 2024b; Nie et al., 2025), and push capabilities into complex domains like reasoning and planning, where they can outperform AR models on certain tasks (Phuong et al., 2025; Gong et al., 2024a). Recent systems demonstrate that dLLMs can be scaled and engineered with many of the same ingredients that power AR LLMs, including sparse mixture-of-experts (Huang et al., 2025), long-context extensions (Wang et al., 2025c), high-throughput inference pipelines for code generation (Song et al., 2025; Pal et al., 2025), and principled scaling analyses for masked diffusion objectives (Li et al., 2024). Collectively, these results position dLLMs as practical family of foundation-model architectures with distinct decoding affordances. Despite this promise, decoding efficiency remains central bottleneck for dLLMs. dLLM generation proceeds proceeds via reverse-diffusion chain with many refinement steps. In addition, practitioners must choose step budgets and transfer schedules priori, often conservatively, to avoid quality loss. This leads to unnecessary computation on easy inputs and, when heuristic decoding parameter choices (e.g., step budgets) are too extreme, unstable behavior across tasks. growing body of work addresses this bottleneck through training-free early-commit methods, which exploit Correspondence: amr.mohamed@mbzuai.ac.ae 1 the empirical observation that predictions often stabilize well before the final diffusion step (Pengxiang et al., 2025). Dynamic policies modulate exploration versus acceleration across the chain, using signals like historical logits or local determinism to reduce redundant steps (Wei et al., 2025; Wang et al., 2025b; Or-El et al., 2025). Other lines of work focus on error correction and refinement, enabling models to revise their own outputs by re-masking low-confidence tokens (Zhang et al., 2025a; Su et al., 2025). Fewand one-step routes also address this bottleneck by transferring ideas from continuous diffusion via curriculum, consistency distillation, or flow matching (Sahoo et al., 2025; Chen et al., 2025; Meister et al., 2024). Orthogonal efforts reduce per-step latency with caching adapted to bidirectional refinement (Yuan et al., 2025b; Wang et al., 2025a) and with speculative mechanisms that draft and verify tokens in parallel (Sohoni et al., 2025; Yuan et al., 2025a). While impactful, these approaches often require additional training, introduce auxiliary models, or rely on complex heuristics, leaving room for simple, training-free, and architecture-agnostic early-exit principle. We revisit diffusion decoding as when-to-stop problem and introduce SchED, schedule-based early-exit mechanism that is both training-free and model-agnostic. Concretely, we aggregate tokenlevel confidence (top-2 logit margins) over an answer region and compare it against monotonic, continuous threshold that relaxes smoothly with normalized diffusion progress. By decoupling the confidence target from step count and making the threshold smooth function of progress, the sampler exits as soon as predictions are stable, while avoiding the brittleness of hard phase changes or fixed budgets. SchED composes with standard transfer schedules (single-suffix or block diffusion) and requires no changes to model training (Nie et al., 2025; Ye et al., 2025). We evaluate SchED across two diffusion LLM families (a single-block Dream decoder and blockdiffusion LLaDA decoder), each in base and instruction-tuned variants, on ten diverse benchmarks covering multiple-choice, math, long-form QA/summarization, and translation. On instructiontuned models, SchED retains 99.8100% of baseline quality on average while yielding 3.84.0 speedups and outperforming recent training-free early-commit methods on conservative qualitypenalized speed metric; on base models, it delivers smaller but consistent gains at near-parity quality (5). Together, these results show that SchED can substantially reduce diffusion decoding costs without sacrificing quality, and that instruction tuning lowers the confidence barrier for early exit in QA-style tasks, making it particularly effective in that regime. This paper makes the following contributions: 1. Training-free, schedule-based early exit. We introduce SchED (Schedule-based Early Decoding) for diffusion language models, which thresholds full-span logit-margin aggregate against smooth, progress-dependent schedule (linear, cosine, exponential), enabling stable, architecture-agnostic stopping without retraining. 2. Strong efficiency at near-parity quality. On instruction-tuned models, SchED retains 99.8100% of baseline performance while achieving 3.84 speedups. On base models, it delivers 99.1100% of baseline accuracy with 1.041.14 speedups in conservative settings, and up to 2.34 under more extreme schedules. 3. Principled qualityspeed trade-off metric. We propose QualityPenalized Speed (QPS) (Eq. 14), which conservatively penalizes accuracy drops. With γ=4, SCHED schedules achieve QPS values of 1.012.03 on Dream Base and 3.244.30 on Dream Instruct, outperforming prior early-commit methods  (Table 3)  . 4. Mechanistic explanation via entropy-based analysis. Analyzing predictive-entropy trajectories over the answer span shows that instruction tuning accelerates confidence stabilization, aligning with progress-aware thresholds and explaining early exits without quality loss  (Fig. 1)  . SchED builds on the vanilla dLLM denoising process by enforcing smooth, progress-dependent confidence threshold over the generated span, exploiting the bidirectional, parallel strengths of dLLMs to achieve end-to-end efficiency without retraining (Zou et al., 2023; Nie et al., 2025; Ye et al., 2025; Gong et al., 2024a). Code is publicly available 1. 1https://github.com/amr-mohamedd/SchED.git"
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion LLMs. Recent work establishes diffusion language models (dLLMs) as viable alternative to autoregressive (AR) models, training either from scratch or by adapting AR checkpoints. LLaDA trains bidirectional Transformer with forward masking process and reverse denoising chain, demonstrating strong pretraining and SFT performance (Nie et al., 2025). Several lines adapt AR models into diffusion ones for fair, scalable comparisons across sizes and tasks (Gong et al., 2024b), while Dream-7B advances recipe and refinement design and releases base/instruct variants (Ye et al., 2025). Architectural advancements have also been integrated, with models like LLaDAMoE demonstrating the successful application of sparse Mixture-of-Experts to dLLMs, achieving competitive performance with significantly reduced active parameters (Huang et al., 2025). The capabilities of dLLMs are also expanding to long-context scenarios; The work like UltraLLaDA shows that techniques such as Rotary Positional Embedding (RoPE) scaling can be adapted to extend context windows to 128K tokens (Wang et al., 2025c). Seed Diffusion emphasizes high-throughput inference for code models (Song et al., 2025). Moreover, formal studies on the scaling laws of masked diffusion models have further established their viability, demonstrating scaling rate comparable to autoregressive models (Li et al., 2024). Survey and perspective papers summarize the landscape, strengths (parallelism, infilling, global context), and open questions (Zou et al., 2023). Sampling efficiency and early commit. central bottleneck for dLLMs is the number of refinement steps. Prophet exploits the empirical observation that answers often stabilize well before the final step and proposes training-free early-commit rule based on top-2 logit gaps (Pengxiang et al., 2025). Complementary dynamic policies include SlowFast Sampling, which alternates exploratory vs. accelerated phases guided by certainty/convergence/position principles and can combine with caching (Wei et al., 2025); Duo transfers continuous-diffusion techniques (curriculum learning, consistency distillation) to discrete diffusion to enable few-step sampling (Sahoo et al., 2025); and DLM-One studies one-step generation via score distillation in continuous spaces (Chen et al., 2025). Orthogonal to reducing step counts, other efforts focus on reducing per-step latency by developing novel caching mechanisms like dKV-Cache and d²Cache, which adapt key-value caching to the bidirectional nature of dLLMs (Yuan et al., 2025b; Wang et al., 2025a). Furthermore, speculative decoding has been adapted for the diffusion framework, using the models own predictions to draft and verify multiple tokens in parallel, as seen in Spiffy and Self Speculative Decoding (Sohoni et al., 2025; Yuan et al., 2025a). Our work also treats decoding as when-to-stop problem, but (unlike Prophet) we introduce smooth confidence schedule that relaxes thresholds over progress, yielding stable, training-free early exit that is agnostic to architecture and training."
        },
        {
            "title": "3 METHODS",
            "content": "In this section, we formalize discrete diffusion language models and introduce SchED, our schedulebased early decoding mechanism. We first review the forward and reverse masking processes and the standard training objective, then describe our confidence aggregation scheme, progress-dependent threshold schedules, and the resulting early-exit decoding algorithm. 3.1 PRELIMINARIES: DISCRETE DIFFUSION LANGUAGE MODELS Setup and notation. Let denote the vocabulary and [mask] special placeholder token. Given prompt prefix, generation proceeds over reverse-diffusion steps {1, . . . , } on sequences xt (V {[mask]})L. At each step, given the current noised sequence xt and prompt xprompt, the model produces logits and per-position categorical distributions Lt = fθ(xprompt, xt, t) RLV, pt,i( xprompt, xt) = softmax(cid:0)Lt,i (cid:1) V. (1) (2) We write {1, . . . , L} for the answer region used later for confidence aggregation, and define normalized diffusion progress = t/T . 3 Forward (masking) process q. We model discrete diffusion as progressive masking process that corrupts clean sequence x0 over steps. Let βt [0, 1) be step-dependent masking rate and define the per-step transition q(xt xt1) = (cid:89) i=1 (cid:2)(1 βt) δ(xt,i = xt1,i) + βt δ(xt,i = [mask])(cid:3). (3) Writing αt = (cid:81)t becomes s=1(1 βs) for the token survival probability after steps, the t-step marginal q(xt x0, t) = (cid:89) i=1 (cid:2)αt δ(xt,i = x0,i) + (1 αt) δ(xt,i = [mask])(cid:3). (4) Reverse (denoising) process pθ. Diffusion language models define learned reverse chain that progressively unmasks tokens from xT (fully masked) to x0 (fully decoded) by factoring over individual positions: pθ(xt1 xprompt, xt) = (cid:89) pθ(xt1,i xprompt, xt). (5) i=1 Each per-position term is then parameterized as categorical distribution based on the models current output: pθ(xt1,i xprompt, xt) = Cat(cid:0)xt1,i; pt,i( xprompt, xt)(cid:1), where pt,i( xprompt, xt) denotes the models categorical prediction at position given the partially masked sequence xt and prompt context xprompt at timestep t. (6) Masked diffusion with partial unmasking (transfer schedule). To control generation granularity, only subset of masked positions are updated at each step. Let Mt = {i : xt,i = [mask]} and let πt(i) {0, 1} indicate whether position is selected for update under transfer schedule. Given the models predictive distribution pt,i( xprompt, xt) at timestep t, the step operator is xt1,i = (cid:26)xt,i, ˆxt,i, if / Mt or πt(i) = 0, if Mt and πt(i) = 1, (7) where ˆxt,i is obtained either deterministically or stochastically, ˆxt,i = arg max pt,i (cid:0)v xprompt, xt (cid:1) or ˆxt,i Cat(cid:0)pt,i( xprompt, xt)(cid:1). Block-diffusion variants (e.g., LLaDA) choose πt over contiguous token blocks, whereas singleblock variants (e.g., Dream) often select the entire suffix or fixed proportion. Cat(cid:0)pt,i( xprompt, xt)(cid:1). Training objective. Diffusion language models are typically trained to invert the forward masking process via masked-token objective over uniformly sampled timesteps: L(θ) = Ex0D EtU {1:T } Extq(x0,t) (cid:34) (cid:88) iMt log pt,i (cid:0)x0,i xprompt, xt (cid:1) (cid:35) . (8) Here xt is obtained by applying the forward kernel at timestep t, and pt,i(x0,i xprompt, xt) is (cid:0)x0,i xprompt, xt, t(cid:1) at position given the shorthand for the models predictive probability pθ partially noised context. This formulation encourages the model to predict the original tokens at masked sites given partially noised contexts, aligning pθ with the forward corruption kernel q. 3. SchED: SCHEDULE-BASED EARLY DECODING We propose SchED, confidenceand progress-aware earlyexit algorithm for diffusion decoding. At each step, SchED thresholds the models aggregated token confidence against smooth, nonincreasing function of progress p, i.e., (gt τ (p)). This design builds on the assumption that pertoken confidence typically rises as denoising proceeds, allowing generation to terminate precisely 4 when predictions stabilize rather than at fixed budget. The complete, model-agnostic procedure is summarized in Algorithm 1. answer region {1, . . . , L}; aggregator Agg; threshold schedule τ (p; τhigh, τlow); transfer schedule πt (or block policy for block diffusion). Algorithm 1 SchED: Schedule-Based Early Decoding for Diffusion Language Models Require: model ; prompt tokens xprompt; generation length Lgen; max steps ; 1: 2: 3: Ensure: completed sequence 4: Initialize [ xprompt; [mask] Lgen ] 5: for = 1 to do Lt (x) 6: t,i L(2) Compute token-level margins gt,i = L(1) Aggregate confidence gt Agg(cid:0){gt,i : A}(cid:1) t/T if gt τ (p; τhigh, τlow) then t,i for 7: Fill all remaining [mask] tokens with current argmax predictions and return Model logits at step Normalized diffusion progress end if Identify masked positions Mt = {i : xi = [mask]} Select positions St { Mt : πt(i) = 1 } Set xSt arg maxv Lt,St,v 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: return If no early exit occurred within steps SchED tracks how model confidence evolves throughout denoising and terminates the process once confidence exceeds progress-dependent threshold, preventing redundant refinement after predictions have stabilized. Confidence measurement. Following the work of Pengxiang et al. (2025), we quantify model confidence using token-level logit margins, which capture how decisively the model prefers its top prediction over alternatives. region-level confidence score is obtained by aggregating top-2 margins over the entire answer region (i.e., the full model response span): gt = Agg(cid:0){ gt,i : }(cid:1), with Agg = mean by default, (9) so that gt = 1/A (cid:80) for every A. All thresholds (τhigh, τlow) are expressed in logit units. iA gt,i in our experiments. Importantly, gt,i uses the current logits at step Early-exit trigger. An early exit is triggered when the aggregated confidence surpasses smooth, progress-dependent threshold schedule τ (p): gt τ (p), (10) where τ : [0, 1] R0 is nonincreasing function of the generation progress = t/T . This formulation ensures that the confidence requirement for stopping is highest at the beginning of generation and gradually relaxes as denoising proceeds, allowing the model to terminate decoding once its predictions have stabilized. Smooth threshold schedules. The threshold function τ (p) controls when the sampler terminates. We parameterize τ using pair (τhigh, τlow), which specify the initial and final confidence thresholds, and an optional slope parameter > 0. We explore three families of schedules: Linear: Cosine: Exponential: τlin(p) = τhigh + (τlow τhigh) p, 2 (τhigh τlow)(cid:0)1 + cos(πp)(cid:1), τcos(p) = τlow + 1 τexp(p) = τlow + (τhigh τlow) ekp, with > 0. (11) (12) (13) Each schedule defines smooth, nonincreasing trajectory from τhigh at = 0 to τlow at = 1, offering different degrees of early-exit aggressiveness and stability control while avoiding the brittleness of fixed thresholds or discrete rules."
        },
        {
            "title": "4 EXPERIMENTAL SETTINGS",
            "content": "We evaluate SchED across diverse suite of multiple-choice and long-form generation tasks, assessing its ability to accelerate diffusion decoding while preserving output quality. Specifically, we compare its denoising efficiency against two baselines: (i) standard diffusion sampling without early exit and (ii) Prophet (Pengxiang et al., 2025). Unless otherwise stated, we fix the upper threshold at (τhigh = 7.5) and, for each schedule family, evaluate two lower-threshold settings: relaxed (τlow = 0) and more conservative (τlow = 2.5), yielding smooth, monotonic relaxation over diffusion progress in both regimes. SchED is model-agnostic and can be applied to any dLLM without architectural or trainModels. ing modifications. To demonstrate its generality, we evaluate on two representative dLLM families that employ distinct decoding paradigms: the Dream family (Ye et al., 2025), which performs fullsequence refinement using single-block decoder, and the LLaDA family (Nie et al., 2025), which adopts block-diffusion strategy that denoises contiguous token segments. Each family is evaluated in both base and instruction-tuned variants, and we apply the low-confidence remasking strategy across all models. Benchmarks. We evaluate on GPQA (Rein et al., 2023), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020) , PIQA (Bisk et al., 2020), and Winogrande (Sakaguchi et al., 2020) . For long-context evaluation, we use tasks from the LongBench suite (Bai et al., 2023), specifically LongBenchHotpotQA (Yang et al., 2018) and LongBenchMultiNews (Fabbri et al., 2019). For translation, we use WMT14 EnFr (Bojar et al., 2014) (5-shot) and WMT16 EnDe (Bojar et al., 2016) (5-shot). Each benchmark includes runs for the baseline, Prophet, and the full set of linear, cosine, and exponential schedules across both model variants. Evaluation metrics and framework. For multiple-choice (MCQ) benchmarks, GPQA, HellaSwag, MMLU, PIQA, and Winogrande, we report accuracy. For GSM8K we also report accuracy based on the generated final answer. For HotpotQA, we report token-level F1-score; for MultiNews, we report ROUGE (Lin, 2004); and for translation (WMT14 EnFr, WMT16 EnDe), we report CHRF. All evaluations are conducted using the Language Model Evaluation Harness (Gao et al., 2024). Efficiency metric for qualityspeed trade-offs. Reporting quality and speedup separately can obscure Pareto differences between methods. To jointly capture both aspects, we define the QualityPenalized Speed (QPS) metric: QPSγ = speedup (cid:16) score baseline score (cid:17)γ , (14) where the exponent γ 1 controls how strongly quality degradation is penalized. Higher values of γ emphasize fidelity over raw acceleration; in our experiments, we use γ=4 to provide conservative and interpretable efficiency comparisons."
        },
        {
            "title": "5 RESULTS",
            "content": "Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average 28.57 (1.00) 79.15 (1.00) 73.39 (1.00) 88.68 (1.00) 75.61 (1.00) 77.10 (1.00) 8.67 (1.00) 21.24 (1.00) 52.99 (1.00) Baseline 28.79 (1.13) 79.15 (1.00) 73.37 (1.03) 88.68 (1.20) 75.61 (1.00) 77.03 (1.07) 8.68 (1.03) 21.21 (1.06) 52.99 (1.11) Prophet 28.12 (1.23) 79.15 (1.02) 73.40 (1.09) 88.57 (1.22) 75.61 (1.01) 76.95 (1.20) 8.74 (1.12) 21.28 (1.13) 52.79 (1.23) Cosine (7.5, 0) 28.79 (1.11) 79.15 (1.00) 73.37 (1.02) 88.68 (1.14) 75.61 (1.00) 77.03 (1.07) 8.68 (1.02) 21.21 (1.05) 52.99 (1.10) Cosine (7.5, 2.5) 29.02 (1.19) 79.15 (1.00) 73.40 (1.06) 88.57 (1.22) 75.61 (1.00) 76.95 (1.13) 8.66 (1.08) 21.26 (1.10) 52.87 (1.18) Linear (7.5, 0) 28.79 (1.10) 79.15 (1.00) 73.37 (1.02) 88.68 (1.03) 75.61 (1.00) 77.03 (1.06) 8.67 (1.02) 21.21 (1.05) 53.00 (1.09) Linear (7.5, 2.5) 24.33 (1.19) 79.13 (1.00) 73.50 (1.07) 88.47 (1.27) 75.77 (1.00) 77.63 (1.11) 8.17 (1.07) 20.94 (1.10) 53.16 (1.18) Exp-k=2 (7.5, 0) 24.33 (1.10) 79.13 (1.00) 73.49 (1.02) 88.63 (1.06) 75.77 (1.00) 77.71 (1.06) 8.16 (1.03) 20.88 (1.05) 53.26 (1.09) Exp-k=2 (7.5, 2.5) Exp-k=16 (7.5, 0) 24.78 (2.04) 77.14 (2.50) 73.05 (2.50) 87.92 (2.50) 72.85 (2.50) 62.77 (3.05) 15.75 (2.78) 21.74 (2.75) 52.87 (1.44) Exp-k=16 (7.5, 2.5) 23.88 (1.14) 79.13 (1.00) 73.53 (1.18) 88.47 (1.36) 75.77 (1.00) 77.63 (1.08) 8.22 (1.07) 20.92 (1.07) 53.26 (1.13) 55.31 (1.00) 47.70 (1.00) 55.31 (1.07) 47.57 (1.06) 55.02 (1.14) 45.64 (1.20) 55.31 (1.06) 47.64 (1.05) 46.43 (1.15) 55.19 (1.11) 47.67 (1.04) 55.32 (1.04) 54.79 (1.11) 46.81 (1.13) 54.90 (1.04) 47.60 (1.04) 53.36 (2.34) 44.75 (1.39) 54.83 (1.11) 47.51 (1.07) Table 1: Dream Base: benchmark scores with speedups (). Two thresholdsconservative (7.5, 2.5) and relaxed (7.5, 0). Column best in bold, second-best in italic. Tables 1 and 2 report results for Dream Base and Dream Instruct, respectively. 6 Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average Baseline Prophet Cosine (7.5, 0) Cosine (7.5, 2.5) Linear (7.5, 0) Linear (7.5, 2.5) Exp-k=2 (7.5, 0) Exp-k=2 (7.5, 2.5) Exp-k=16 (7.5, 0) Exp-k=16 (7.5, 2.5) 30.58 (17.16) 79.68 (2.13) 79.31 (2.50) 72.78 (1.55) 84.49 (2.29) 72.61 (2.10) 26.58 (4.33) 24.26 (7.21) 30.36 (1.00) 79.91 (1.00) 80.69 (1.00) 73.02 (1.00) 85.80 (1.00) 74.66 (1.00) 27.51 (1.00) 24.39 (1.00) 58.20 (1.00) 28.79 (16.45) 26.69 (7.81) 78.73 (1.14) 72.33 (1.01) 72.58 (1.03) 66.54 (1.02) 27.87 (4.51) 2.77 (31.21) 20.17 (23.93) 15.28 (27.87) 41.18 (11.60) 55.82 (2.39) 58.06 (3.87) 30.58 (15.96) 78.54 (2.11) 80.81 (1.56) 73.06 (1.14) 85.96 (1.32) 74.11 (1.61) 27.46 (3.24) 24.39 (6.60) 30.58 (15.93) 79.53 (2.08) 80.75 (1.43) 73.08 (1.11) 85.91 (1.23) 74.27 (1.28) 27.47 (3.14) 24.38 (6.59) 55.82 (2.34) 58.16 (3.79) 30.58 (16.08) 79.00 (2.11) 80.81 (1.56) 73.14 (1.14) 85.96 (1.32) 74.11 (1.49) 27.48 (3.26) 24.39 (3.02) 55.82 (2.38) 58.11 (3.89) 55.82 (2.33) 58.16 (3.79) 30.58 (16.01) 79.76 (2.08) 80.76 (1.43) 73.09 (1.10) 85.85 (1.22) 74.27 (1.27) 27.47 (3.17) 24.38 (6.59) 55.76 (2.40) 30.58 (16.32) 79.45 (2.14) 80.77 (1.62) 73.11 (1.21) 86.16 (1.54) 74.35 (1.67) 26.71 (3.47) 24.56 (6.66) 58.14 (3.97) 30.13 (16.18) 80.21 (2.10) 80.69 (1.54) 73.25 (1.11) 86.24 (1.23) 74.51 (1.53) 26.91 (3.23) 24.60 (6.57) 55.76 (2.33) 58.21 (3.85) 51.12 (5.44) 29.46 (18.64) 32.98 (3.73) 79.31 (5.00) 58.81 (2.50) 84.55 (5.00) 73.80 (5.00) 26.20 (5.79) 23.99 (17.38) 53.53 (5.56) 55.76 (2.38) 57.59 (4.48) 49.86 (2.81) 49.85 (2.77) 49.86 (2.80) 49.85 (2.76) 49.84 (2.76) 49.84 (2.71) 49.72 (3.17) 49.84 (2.74) 49.85 (1.00) 55.82 (1.00) Table 2: Dream Instruct: benchmark scores with speedups (). Two thresholdsconservative (7.5, 2.5) and relaxed (7.5, 0). Column best in bold, second-best in italic. Dream Base. With conservative smooth schedules (τhigh, τlow) = (7.5, 2.5), linear, cosine, and exp-k=2, we observe steady 1.041.14 average speedups at near-parity quality, with marginal task gains (e.g., WMT14 EnFr: +0.27)) under exp-k=2 (7.5, 2.5). Fast-decaying exponentials (large k) with τlow = 2.5 remain close to parity, but do not increase the mean speed beyond 1.1; setting τlow = 0 yields 2.34 average speed (exp-k=16 ), with task-specific gains that are most pronounced on HotpotQA (+7.1); however, at an overall quality cost of 2%. Prophet offers 1.07 average speed with near-parity accuracy, providing limited practical benefits relative to smooth schedules. Dream Instruct. Under the same conservative thresholds, smooth schedules deliver 3.84.0 average speedups at near parity: table averages cluster around the baseline (e.g., 58.0658.22 vs. 58.20). Notably, the large-k (fast-decaying) exponential with (7.5, 2.5) attains 4.48, while remaining close to parity (57.59, 1%)). Less conservative settings with τlow = 0 also preserve translation near baseline with 2.32.8 speed (e.g., cosine and linear). For the large-k exponential, (k=16, τlow=0) delivers the most pronounced speedups while remaining near parity on most benchmarks; however, it exhibits large degradations on HellaSwag and PIQA. LLaDA exhibits similar trends to those observed on Dream: conservative schedules provide reliable speedups at near-parity quality on the base model; the instruction-tuned variant yields higher speedups. Fast-decaying settings risk over-commitment that degrades math and long-form. While Prophet provides high speedups, it underperforms on long-form generation. Detailed LLaDA Base and Instruct results are in Tables 7 and 8; additional schedule ablations for Dream and LLaDA appear in Appendix B. Efficiency results. Table 3 reports QualityPenalized Speed (QPS; γ=4). On Dream Base, smooth schedules concentrate in the 1.011.12 range, reflecting the near-parity averages in Table 1 (e.g., 55.0255.32 vs. baseline 55.31) coupled with modest mean speedups ( 1.041.14). The highest base-model efficiency is achieved by the rapidly decaing exponential, Exp-k=16, (7.5, 0), with 2.03, driven by large average speedup (2.34), and average-score drop of 1.95. Prophet attains 1.07, consistent with its near-baseline averages and limited acceleration. Method Dream Base Dream Instruct Cosine (7.5, 0) Cosine (7.5, 2.5) Linear (7.5, 0) Linear (7.5, 2.5) Exp-k=2 (7.5, 0) Exp-k=2 (7.5, 2.5) Exp-k=16 (7.5, 0) Exp-k=16 (7.5, 2.5) Prophet 1.12 1.06 1.10 1.04 1.07 1.01 2.03 1.07 1.07 3.83 3.78 3.87 3.78 3.95 3.85 3.24 4.30 2.91 Table 3: QPS (γ=4) for all SchED variants and Prophet. Higher is better. On Dream Instruct, QPS values increase substantially in line with the larger mean speedups observed in Table 2 ( 3.84.0 for conservative smooth schedules) while maintaining average scores close to baseline. The best overall result, 4.30, is obtained by Exp-k=16, (7.5, 2.5), which combines near-parity average score (57.59) with pronounced 4.48 mean speedup. Other smooth schedules lie tightly in the 3.783.95 band. Prophet trails at 2.91, reflecting weaker average scores and failures on long-form despite notable raw speed. Overall, SchED consistently yields higher QPS4 than Prophet; the top settings pair near-parity average performance with substantial step reductions, which explains their leading efficiency. Figure 1: Mean predictive entropy across diffusion steps for five input types (GPQA, GSM8K, HotpotQA, MMLU, WMT14 ENFR). Curves show per-token entropy (nats/token) averaged over evaluation samples; shaded bands denote one standard deviation across samples. Both Dream Base and Dream Instruct exhibit monotonically decreasing entropy as denoising progresses."
        },
        {
            "title": "6 ANALYSIS",
            "content": "To better understand the differences in speedup values between base and instruct models, and the variation in speedups across benchmarks, we analyze how predictive uncertainty evolves over the reverse-diffusion chain. We track per-token predictive entropy on the generated region at each step t. Using the per-position distributions pt,i state xt, the mean per-token entropy is Ht = (cid:32) 1 (cid:88) iS (cid:88) vV pt,i(vxprompt, xt) log pt,i(vxprompt, xt) (cid:33) . (15) where pt,i(v) denotes the models step-t predictive distribution at position i, computed from logits Lt,i given xprompt and xt (Eq. 6). Results are presented in Figure 1. Dream Instruct starts the denoising phase with higher entropy on math-heavy prompts (GPQA and GSM8K) and decays more slowly in the earliest steps than on MMLU, HotpotQA, and WMT14, yet the trajectories converge to similarly low final entropies. Notably, the instruct curves show rapid early drop followed by nearly uniform per-step decreases, stepwise profile consistent with Dreams masked diffusion decoding, where at each reverse-diffusion step the model updates high-confidence tokens in parallel and thereby reduces uncertainty in approximately regular increments across the chain. CARTs context-adaptive token-level noise rescheduling further sharpens this behavior by encouraging more confident predictions at positions with stronger local context, particularly near the prompt. By contrast, Dream Base shows more overlapping trajectories across benchmarks and higher residual entropy overall, suggesting less decisive posteriors for QA-style inputs. This aligns with the smaller speedups obtained by schedule-based early exit on Base models (Tables 12)."
        },
        {
            "title": "7 DISCUSSION",
            "content": "The findings reported in Section 5 and Section 6 demonstrate that progress-aware scheduling strategy effectively translates model confidence into computational savings, with an unusually high degree of robustness across tasks and model families. For instruction-tuned variants, SchED attains speedups of up to approximately 17, while preserving full baseline accuracy and achieves average accelerations of up to about 4, with overall accuracies essentially indistinguishable from the baseline, whereas base models exhibit more moderate yet consistently positive improvements. The entropy patterns in Figure 1 elucidate this discrepancy. Instruction tuning steers the model toward confident, QA-oriented completions, leading to rapid reduction in uncertainty over the answer span. For Dream Instruct, this reduction follows characteristic profile: an initial, pronounced decline, followed by approximately uniform stepwise decreases. By contrast, Dream Base retains higher entropy with flatter and more overlapping trajectories across benchmarks, postponing the 8 point at which aggregated confidence exceeds the stopping criterion and thereby constraining the attainable speedups. The stopping criterion is central to the methods stability. SchED compares sequence-level logitmargin aggregate to smooth, nonincreasing function of normalized diffusion progress = t/T , imposing stringent requirement at early steps and gradually relaxing it thereafter. This construction links the decision to terminate directly to the evolution of model certainty, dampens transient spikes, and triggers exit when predictions have effectively converged rather than at predetermined step count; using the entire generated span for aggregation further stabilizes the decision relative to short-prefix estimates, at the expense of modest additional computation. Empirically, conservative cosine and linear schedules maintain performance very close to the baseline while substantially reducing the number of refinement steps, and the QPS metric with γ = 4 indicates that these smooth schedules consistently achieve high qualityefficiency trade-offs. In contrast, rapidly decaying exponential schedules (large k, τlow = 0) yield greater acceleration but incur noticeable accuracy losses. Prophets degradation on long-form generation arises from computing confidence over fixed, localized region and employing discrete commit rule, which allows localized confidence spikes to induce premature termination while later portions of the output remain underresolved, leading to reduced ROUGE and F1. SchED mitigates these issues by aggregating over the full generated sequence, making the threshold explicitly progress-dependent, and avoiding auxiliary suffix prompts that could artificially inflate early certainty. Across tasks, the behavior of the stopping criterion is strongly input dependent. Math-oriented benchmarks exhibit distinct profiles: GPQA, although mathematically challenging, is multiple choice, so the presence of explicit answer options enables the model to reach relatively high confidence earlier in the diffusion process, even at nontrivial difficulty. By contrast, GSM8K also involves mathematical reasoning but requires free-form solutions; here confidence must build over longer span, and the entropy trajectories indicate that additional refinement steps are needed before the schedules threshold is reliably exceeded. Long-form generation tasks benefit most from smoothing and full-span aggregation, since evaluation quality depends on maintaining coherent, accurate content throughout the response rather than producing an early, locally confident fragment. Translation occupies an intermediate regime: schedules keep CHRF close to baseline while still reducing the number of steps, reflecting the combination of bidirectional refinement and comparatively tight lexical and semantic constraints on the target. These regularities are not specific to single model family: LLaDA exhibits analogous behavior, with conservative smooth schedules improving efficiency at near-parity quality on base variants and yielding substantially larger gains on instruction-tuned models. In aggregate, progress-aware thresholds provide consistent acceleration, and the choice of schedule should be aligned with the tolerated error budget. Conservative configurations are appropriate when preserving fidelity is critical, whereas more rapidly decaying schedules are suitable for fault-tolerant or latency-sensitive applications that can accommodate modest degradation, trade-off succinctly captured by the qualitypenalized speed metric."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this work, we introduced SchED, training-free, architecture-agnostic early-exit mechanism for diffusion language models that compares sequence-level confidence statistic to smooth, progressconditioned threshold. By explicitly tying the stopping decision to the stabilization of model predictions, the method achieves consistent reductions in denoising steps across both Dream and LLaDA, with particularly large gains on instruction-tuned variants where uncertainty decays rapidly, while preserving performance, as corroborated by the entropy analysis and the qualitypenalized speed metric. SchED integrates with existing transfer schedules without modifying the underlying models. The resulting family of schedules spans spectrum of qualityefficiency trade-offs: more conservative configurations are suitable when high fidelity is required, whereas rapidly relaxing schedules are appropriate for latency-sensitive or fault-tolerant scenarios. Promising directions for further study include learning schedule parameters, adapting aggregation strategies to task structure, designing domain-aware thresholds, and combining the approach with speculative or cache-based denoising. Overall, formulating diffusion decoding as stopping-time problem yields simple and robust primitive for deploying dLLMs under simultaneous accuracy and throughput constraints."
        },
        {
            "title": "LIMITATIONS",
            "content": "While SchED provides training-free mechanism for accelerating diffusion decoding, our study has some limitations. SchED shows an explicit qualityefficiency trade-off through the choice of schedule family and hyperparameters (τhigh, τlow, k). Conservative schedules (e.g., linear or cosine with higher final thresholds) yield near-parity quality but moderate speedups, whereas more extreme, rapidly decaying schedules can deliver larger accelerations at the cost of marginal average degradation. In our experiments, this trade-off is resolved by selecting small set of global configurations per model and regime; in practice, the right schedule is task-dependent. Additionally, SchED operates purely at inference time and treats the schedule as an external control signal. We do not investigate joint training of models and schedules, learned aggregation functions beyond averaging over the answer span, or tighter integration with complementary acceleration techniques, such as speculative or cache-based denoising. These directions could further improve robustness and efficiency but are left for future work."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Yazid Janati for his insights and feedback throughout the development of this work."
        },
        {
            "title": "REFERENCES",
            "content": "Yushi Bai, Xin Lv, Jiajie Jin, Yidong Zhang, Hongbo Zhang, Beichen Zheng, Yikai Chen, JianGuang Qian, Zeyu Chen, Wen-Han Liu, et al. LongBench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Yonatan Bisk, Rowan Zellers, Ronan Lebras, Chandra Bhagavatula, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 74327439, 2020. Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 1258, 2014. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings of the 2016 conference on machine translation (wmt16). In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pp. 131198, 2016. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tianqi Chen, Shujian Zhang, and Mingyuan Zhou. DLM-one: Diffusion language models for onestep sequence generation. arXiv [cs.CL], May 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Alexander Fabbri, Greg Durrett, Daniel Farcas, and Viv Ng. Multi-News: large-scale arXiv preprint multi-document summarization dataset and abstractive hierarchical model. arXiv:1906.01749, 2019. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Chen Gong, Hantian Li, Denny Zhou, and Quoc Le. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024a. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. arXiv [cs.CL], October 2024b. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Ippolito, Jared Kaplan, Jana Scheurer, Sandeep Datta, Cameron Foote, et al. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Jun-Jie Huang, Zixuan Li, Han-Ning Wu, Zixun Lan, Weizhen Qi, Hangbo Bao, Zewen Chi, WenHong Li, Jun-Cheng Chen, Li-Yuan Wang, Wei-Nan Zhang, and Ting Liu. Llada-moe: sparse moe diffusion language model. arXiv preprint arXiv:2509.24389, 2025. Xiang Lisa Li, Jack Rae, Yiding Jiang, Yixiao Li, Hui-Ling Zhen, Grzegorz Gorny, Piotr Nawrot, Tim GJ Rudner, Jean-Baptiste Lespiau, Jonathan Richard Schwarz, Arthur Mensch, Roman Ring, Lorenzo Noci, Trevor Cai, Sebastian Borgeaud, Jeff Stanway, Charlie Chen, David Ding, Daniel Autobiography, Danila de Jesus, Susannah Young, Samuel Smith, Laurent Sifre, Aidan Gomez, Yann Dauphin, and Bogdan Damoc. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. Clara Meister, Ayan Pal, Emma Strubell, Mike Rabbat, and Samar Singh. Fs-dfm: Fast and accurate long text generation with few-step diffusion language models. arXiv preprint arXiv:2509.20624, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv [cs.CL], February 2025. Friedrich Or-El, Zheng-Hao Liu, Chen-Lin Zhang, Hong-Yi Yuan, and Wei-Qiang Zhang. arXiv preprint Accelerating diffusion llm inference via local determinism propagation. arXiv:2510.07081, 2025. Ayan Pal, Meet Shah, Emma Strubell, Berkin Gunel, Clara Meister, Ameet Pokle, Yilun Zhou, Mike Rabbat, and Samar Singh. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Li Pengxiang, Zhou Yefan, Muhtar Dilxat, Yin Lu, Yan Shilin, Shen Li, Liang Yi, Vosoughi Soroush, and Liu Shiwei. Diffusion language models know the answer before decoding. arXiv [cs.CL], August 2025. Mary Phuong, Chen Gong, Hantian Li, Denny Zhou, and Quoc Le. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. David Rein, Stas Raichuk, Orion Selesnick, Armel Fotso, Jarno Textor, Daniel Lindner, Yafang Du, Tsvi August, Dan Gutman, Mor Geva, et al. GPQA: graduate-level google-proof Q&A benchmark. arXiv preprint arXiv:2311.12022, 2023. Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion duality. arXiv [cs.LG], June 2025. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 87328740, 2020. Nimit Sohoni, Chien-Yu Lin, Aniruddha Nrusimha, and Christopher Re. Spiffy: Multiplying diffusion llm acceleration via lossless speculative decoding. arXiv preprint arXiv:2509.18085, 2025. 11 Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv [cs.CL], August 2025. Yixuan Su, Cheng Zhu, Wangchunshu Zhou, Jing Chen, and Graham Neubig. Dont settle too early: Self-reflective remasking for diffusion language models. arXiv preprint arXiv:2509.23653, 2025. Chuhan Wang, Hong-Ning Dai, Yi-Chen Zhang, Chun-Wei Tsung, Xin-Hao Li, Wei-Guo Zheng, and Ming-Li Song. d2cache: Accelerating diffusion-based llms via dual adaptive caching. arXiv preprint arXiv:2509.23094, 2025a. Chuhan Wang, Yi-Chen Zhang, Xin-Hao Li, Hong-Ning Dai, and Ming-Li Song. Creditdecoding: Accelerating parallel decoding in diffusion large language models with trace credits. arXiv preprint arXiv:2510.06133, 2025b. Ming-Hao Wang, Yun-Zhu Song, Jun-Jie Huang, Zixuan Li, Wen-Hong Li, Jia-Jun Zhang, and Cheng-Chung Hsu. Ultrallada: Scaling the context length to 128k for diffusion large language models. arXiv preprint arXiv:2510.10481, 2025c. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating diffusion large language models with SlowFast sampling: The three golden principles. arXiv [cs.CL], June 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, 2018. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7B: Diffusion large language models. arXiv [cs.CL], August 2025. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. Hong-Yi Yuan, Zheng-Hao Liu, Chen-Lin Zhang, and Wei-Qiang Zhang. Self speculative decoding for diffusion large language models. arXiv preprint arXiv:2510.04147, 2025a. Hongyi Yuan, Zheng-Hao Liu, Chen-Lin Zhang, Xu-Ran Wang, Yu-Hong-Vey Wong, and Jia-Wei Liu. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025b. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Chen-Lin Zhang, Zheng-Hao Liu, Hong-Yi Yuan, and Wei-Qiang Zhang. Finish first, perfect later: Test-time token-level cross-validation for diffusion large language models. arXiv preprint arXiv:2510.05090, 2025a. Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip Yu, et al. survey on parallel text generation: From parallel decoding to diffusion language models. arXiv preprint arXiv:2508.08712, 2025b. Hao Zou, Zae Myung Kim, and Dongyeop Kang. survey of diffusion models in natural language processing. arXiv [cs.CL], May 2023."
        },
        {
            "title": "A GENERATION CONFIGURATIONS",
            "content": "Table 4 summarizes the decoding hyperparameters used across benchmarks. For each task we specify the maximum number of reverse-diffusion steps (the step budget over which progress p=t/T is normalized), the generation length (the maximum number of tokens in the generated answer region A), the number of in-context examples (shots), andfor the block-diffusion variant LLaDAthe block size used by the transfer schedule πt (Dream uses single-block/suffix refinement and thus does not require block-size setting). Short multiple-choice (MCQ) tasks are evaluated with compact budgets (T =5) and very short generation lengths, whereas math, translation, and long-form tasks employ substantially larger step budgets (256512) and longer generation lengths, making them more sensitive to fast-decaying earlyexit schedules. All MCQ benchmarks (MMLU, HellaSwag, PIQA, Winogrande) are evaluated in generative mode with full decoding of the answer region, not via likelihood/ranking of options; the model must produce the final answer tokens in A, and accuracy is computed from the generated outputs. For LLaDA, we use block size of 5 tokens on short MCQ benchmarks and 32 tokens on tasks that require generation lengths of 32 tokens or more."
        },
        {
            "title": "Shots LLaDA Block",
            "content": "MMLU (generative) HellaSwag (generative) PIQA (generative) Winogrande (generative) GPQA (n-shot) GSM8K WMT14 EnFr WMT16 EnDe LongBench MultiNews LongBench HotpotQA 5 5 5 5 128 256 256 256 512 32 5 5 5 5 128 256 256 256 512 32 5 5 5 5 8 8 5 5 0 0 5 5 5 5 32 32 32 32 32 32 Table 4: Decoding hyperparameters per benchmark. Generation length denotes the maximum number of tokens in the generated answer region A. LLaDA Block applies only to the block-diffusion decoder; Dream uses single-block transfer schedule."
        },
        {
            "title": "B ADDITIONAL SchED VARIANTS AND LLaDA RESULTS",
            "content": "This appendix primarily details the full LLaDA resultsboth Base and Instructunder the same evaluation protocol and schedule families as the main paper (Tables 78). In addition, we report one new set of variants for Dream: intermediatecurvature exponential schedules with {4, 8}, each evaluated under the conservative threshold (τhigh, τlow) = (7.5, 2.5) and the relaxed (7.5, 0) (Tables 56). All other schedules (linear, cosine, and exponential with {2, 16}) are already covered in the main text. Dream. On Dream Instruct, the intermediate exponentials are highly competitive with the best smooth schedules. In particular, Expk=4 with (7.5, 2.5) attains the highest overall average score 58.22 at an average speedup of 3.97  (Table 6)  . Expk=8 with (7.5, 2.5) matches the MMLU peak (73.53) while preserving translation near baseline (WMT14 EnFr 55.76, WMT16 EnDe 49.84) and pushing the mean speedup to 4.23. At the task level, the intermediate schedules frequently sit on or near the column bests: e.g., Expk=8 (7.5, 2.5) yields the top GPQA (30.80) and shares the MMLU peak, while Expk=4 (7.5, 2.5) is within rounding of the GSM8K best (80.14 vs. 80.21 for Expk=2). As intended, their average speedups typically fall between the relaxed k=2 exponential (e.g., 58.14 at 3.97 for (7.5, 0)) and the highcurvature k=16 exponential (57.59 at 4.48 for (7.5, 2.5)). On Dream Base  (Table 5)  , the same pattern holds at smaller scale. Expk=8 (7.5, 2.5) reaches the best MMLU (73.53) and ties the top WMT14 EnFr (53.26) while delivering mean 1.10 13 speedup; Expk=4 (7.5, 2.5) is similarly competitive on WMT14 (53.24) and PIQA (second-best 88.63) with 1.07 speed. These sit neatly between the relaxed k=2 exponential (mean 1.11) and the highcurvature k=16 exponential (mean 1.11 under (7.5, 2.5), or much larger 2.34 under (7.5, 0) with the expected quality trade-off). Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average 28.57 (1.00) 79.15 (1.00) 73.39 (1.00) 88.68 (1.00) 75.61 (1.00) 77.10 (1.00) 8.67 (1.00) 21.24 (1.00) 52.99 (1.00) Baseline 28.79 (1.13) 79.15 (1.00) 73.37 (1.03) 88.68 (1.20) 75.61 (1.00) 77.03 (1.07) 8.68 (1.03) 21.21 (1.06) 52.99 (1.11) Prophet 28.79 (1.11) 79.15 (1.00) 73.37 (1.02) 88.68 (1.14) 75.61 (1.00) 77.03 (1.07) 8.68 (1.02) 21.21 (1.05) 52.99 (1.10) Cosine (7.5, 2.5) 28.12 (1.23) 79.15 (1.02) 73.40 (1.09) 88.57 (1.22) 75.61 (1.01) 76.95 (1.20) 8.74 (1.12) 21.28 (1.13) 52.79 (1.23) Cosine (7.5, 0) 28.79 (1.10) 79.15 (1.00) 73.37 (1.02) 88.68 (1.03) 75.61 (1.00) 77.03 (1.06) 8.67 (1.02) 21.21 (1.05) 53.00 (1.09) Linear (7.5, 2.5) 29.02 (1.19) 79.15 (1.00) 73.40 (1.06) 88.57 (1.22) 75.61 (1.00) 76.95 (1.13) 8.66 (1.08) 21.26 (1.10) 52.87 (1.18) Linear (7.5, 0) 24.33 (1.10) 79.13 (1.00) 73.49 (1.02) 88.63 (1.06) 75.77 (1.00) 77.71 (1.06) 8.16 (1.03) 20.88 (1.05) 53.26 (1.09) Exp-k=2 (7.5, 2.5) 24.33 (1.19) 79.13 (1.00) 73.50 (1.07) 88.47 (1.27) 75.77 (1.00) 77.63 (1.11) 8.17 (1.07) 20.94 (1.10) 53.16 (1.18) Exp-k=2 (7.5, 0) 23.66 (1.13) 79.13 (1.00) 73.49 (1.06) 88.63 (1.20) 75.77 (1.00) 77.63 (1.07) 8.16 (1.05) 20.90 (1.06) 53.24 (1.12) Exp-k=4 (7.5, 2.5) 23.44 (1.33) 79.13 (1.15) 73.53 (1.51) 88.47 (1.62) 75.77 (1.31) 77.33 (1.47) 8.89 (1.35) 21.03 (1.30) 52.97 (1.32) Exp-k=4 (7.5, 0) 23.66 (1.14) 79.13 (1.00) 73.53 (1.15) 88.52 (1.29) 75.77 (1.00) 77.63 (1.08) 8.22 (1.07) 20.92 (1.07) 53.26 (1.13) Exp-k=8 (7.5, 2.5) 24.78 (1.60) 78.99 (1.56) 73.33 (2.39) 87.98 (2.42) 73.56 (2.39) 72.71 (2.20) 14.02 (2.17) 21.57 (1.91) 52.91 (1.40) Exp-k=8 (7.5, 0) Exp-k=16 (7.5, 2.5) 23.88 (1.14) 79.13 (1.00) 73.53 (1.18) 88.47 (1.36) 75.77 (1.00) 79.83 (2.12)* 8.22 (1.07) 20.92 (1.07) 53.26 (1.13) 24.78 (2.04) 77.14 (2.50) 73.05 (2.50) 87.92 (2.50) 72.85 (2.50) 62.77 (3.05) 15.75 (2.78) 21.74 (2.75) 52.87 (1.44) Exp-k=16 (7.5, 0) 47.70 (1.00) 55.31 (1.00) 47.57 (1.06) 55.31 (1.07) 47.64 (1.05) 55.31 (1.06) 45.64 (1.20) 55.02 (1.14) 47.67 (1.04) 55.32 (1.04) 46.43 (1.15) 55.19 (1.11) 47.60 (1.04) 54.90 (1.04) 46.81 (1.13) 54.79 (1.11) 47.51 (1.05) 54.81 (1.07) 44.95 (1.30) 54.55 (1.37) 47.51 (1.06) 54.81 (1.10) 44.78 (1.37) 54.46 (1.94) 47.51 (1.07) 54.83 (1.11) 44.75 (1.39) 53.36 (2.34) * GSM8K speedup was unchanged from your source row; value kept where it originally appeared. Table 5: Dream Base. Highest accuracy per column in bold; second-highest in italics. The rightmost column reports mean accuracy with mean speedup in parentheses. Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average 30.36 (1.00) 80.69 (1.00) 73.02 (1.00) 85.80 (1.00) 74.66 (1.00) 79.91 (1.00) 27.51 (1.00) 24.39 (1.00) 58.20 (1.00) Baseline 28.79 (16.45) 78.73 (1.14) 72.33 (1.01) 72.58 (1.03) 66.54 (1.02) 26.69 (7.81) 27.87 (4.51) 2.77 (31.21) 20.17 (23.93) 15.28 (27.87) 41.18 (11.60) Prophet 30.58 (15.93) 80.75 (1.43) 73.08 (1.11) 85.91 (1.23) 74.27 (1.28) 79.53 (2.08) 27.47 (3.14) 24.38 (6.59) 58.16 (3.79) Cosine (7.5, 2.5) 30.58 (15.96) 80.81 (1.56) 73.06 (1.14) 85.96 (1.32) 74.11 (1.61) 78.54 (2.11) 27.46 (3.24) 24.39 (6.60) 58.06 (3.87) Cosine (7.5, 0) 30.58 (16.01) 80.76 (1.43) 73.09 (1.10) 85.85 (1.22) 74.27 (1.27) 79.76 (2.08) 27.47 (3.17) 24.38 (6.59) 58.16 (3.79) Linear (7.5, 2.5) 30.58 (16.08) 80.81 (1.56) 73.14 (1.14) 85.96 (1.32) 74.11 (1.49) 79.00 (2.11) 27.48 (3.26) 24.39 (3.02) 58.11 (3.89) Linear (7.5, 0) 58.21 (3.85) 30.13 (16.18) 80.69 (1.54) 73.25 (1.11) 86.24 (1.23) 74.51 (1.53) 80.21 (2.10) 26.91 (3.23) 24.60 (6.57) Exp-k=2 (7.5, 2.5) 30.58 (16.32) 80.77 (1.62) 73.11 (1.21) 86.16 (1.54) 74.35 (1.67) 79.45 (2.14) 26.71 (3.47) 24.56 (6.66) 58.14 (3.97) Exp-k=2 (7.5, 0) 30.58 (16.38) 80.78 (1.70) 73.10 (1.14) 86.29 (2.01) 74.27 (1.49) 80.14 (2.12) 26.86 (3.47) 24.57 (6.67) 58.22 (3.97) Exp-k=4 (7.5, 2.5) 30.58 (16.62) 79.31 (2.50) 70.57 (1.64) 84.82 (2.51) 74.66 (2.54) 71.27 (2.25) 26.58 (4.03) 24.03 (5.59) 56.57 (4.44) Exp-k=4 (7.5, 0) 57.97 (4.23) 30.80 (16.69) 79.48 (2.48) 73.53 (1.24) 86.13 (2.37) 73.64 (1.72) 79.83 (2.12) 26.65 (3.82) 24.29 (5.50) Exp-k=8 (7.5, 2.5) 52.48 (4.94) Exp-k=8 (7.5, 0) 30.36 (17.03) 79.31 (4.21) 58.86 (1.64) 84.55 (4.76) 74.03 (4.64) 43.67 (2.94) 26.28 (4.77) 20.86 (9.53) Exp-k=16 (7.5, 2.5) 30.58 (17.16) 79.31 (2.50) 72.78 (1.55) 84.49 (2.29) 72.61 (2.10) 79.68 (2.13) 26.58 (4.33) 24.26 (7.21) 57.59 (4.48) 51.12 (5.44) Exp-k=16 (7.5, 0) 55.82 (2.34) 55.82 (2.39) 55.82 (2.33) 55.82 (2.38) 55.76 (2.33) 55.76 (2.40) 55.76 (2.36) 55.78 (2.94) 55.76 (2.37) 55.27 (4.15) 55.76 (2.38) 29.46 (18.64) 79.31 (5.00) 58.81 (2.50) 84.55 (5.00) 73.80 (5.00) 32.98 (3.73) 26.20 (5.79) 23.99 (17.38) 53.53 (5.56) 49.85 (2.77) 49.86 (2.81) 49.85 (2.76) 49.86 (2.80) 49.84 (2.71) 49.84 (2.76) 49.84 (2.73) 49.84 (2.85) 49.84 (2.74) 49.86 (3.05) 49.84 (2.74) 49.72 (3.17) 49.85 (1.00) 55.82 (1.00) Table 6: Dream Instruct. Highest accuracy per column in bold; second-highest in italics. The rightmost column reports mean accuracy with mean speedup in parentheses. LLaDA. Results on LLaDA align closely with the Dream trends for both base and instructiontuned variants. For LLaDA Instruct  (Table 8)  , Expk=4 (7.5, 2.5) ties for the highest overall average (53.17) while achieving 2.13 speedup; moving to Expk=8 (7.5, 2.5) increases the speed to 2.55 with small average dip (52.55), and relaxing to (7.5, 0) further boosts speed (e.g., Winogrande 78.69 at 3.99) at the cost of broader quality dropsmirroring the Dream trade-off. For LLaDA Base  (Table 7)  , the curvaturespeed relationship is monotone while quality degrades gradually with curvature: averages move from 49.58 at 7.07 for Expk=2 (7.5, 2.5) to 49.30 at 7.56 for k=4, 47.79 at 8.47 for k=8, and 47.48 at 10.87 for k=16. In short, the intermediatecurvature exponentials again occupy the expected middle ground between the gradualdecay and highcurvature regimes, providing convenient knob to trade bit more speed for small, predictable loss in fidelity. Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average 26.12 (1.00) 86.20 (1.00) 58.16 (1.00) 81.72 (1.00) 77.98 (1.00) 47.76 (1.00) 9.08 (1.00) 52.99 (1.00) Baseline 31.03 (15.32) 86.17 (1.24) 58.00 (1.25) 81.56 (2.05) 77.98 (1.25) 16.07 (7.84) 14.94 (3.19) 18.83 (32.48) 49.58 (17.53) 45.15 (17.98) 47.93 (10.01) Prophet 27.23 (1.97) 86.16 (1.17) 58.00 (1.24) 81.77 (2.04) 77.98 (1.23) 39.12 (1.97) 9.78 (1.60) 52.41 (1.68) 25.00 (1.80) Cosine (7.5, 2.5) 27.90 (2.24) 86.08 (1.49) 58.00 (1.24) 81.50 (2.12) 77.98 (1.54) 42.68 (2.24) 11.12 (1.88) 25.23 (2.07) 52.96 (1.90) Cosine (7.5, 0) Exp-k=16 (7.5, 2.5) 30.80 (4.47) 85.98 (1.67) 58.00 (1.24) 80.30 (4.69) 77.98 (1.65) 30.40 (3.75) 12.93 (2.84) 24.77 (3.38) 51.98 (2.98) 28.79 (8.67) 86.01 (5.00) 58.00 (1.24) 79.98 (5.00) 77.66 (5.00) 4.93 (8.58) 20.98 (7.13) 20.74 (8.55) 47.53 (6.49) Exp-k=16 (7.5, 0) 26.34 (2.24) 86.16 (1.07) 58.00 (1.24) 81.56 (2.12) 77.98 (1.14) 42.38 (2.19) 9.83 (1.65) 52.65 (1.76) Exp-k=2 (7.5, 2.5) 25.11 (1.92) 53.17 (2.21) 29.02 (2.85) 86.03 (1.58) 58.00 (1.24) 81.23 (2.45) 77.98 (1.64) 43.52 (2.75) 11.65 (2.20) 25.30 (2.47) Exp-k=2 (7.5, 0) 28.79 (2.86) 86.06 (1.41) 58.00 (1.24) 81.18 (2.42) 77.98 (1.43) 43.75 (2.73) 11.57 (2.08) 25.37 (2.40) 53.17 (2.13) Exp-k=4 (7.5, 2.5) 27.46 (4.09) 85.96 (2.34) 58.00 (1.24) 81.28 (2.48) 78.53 (3.17) 31.01 (3.72) 13.59 (3.17) 24.72 (3.52) 51.85 (3.05) Exp-k=4 (7.5, 0) 28.57 (3.70) 86.05 (1.62) 58.00 (1.24) 81.28 (2.95) 77.98 (1.58) 37.23 (3.32) 12.50 (2.57) 25.10 (3.00) 52.55 (2.55) Exp-k=8 (7.5, 2.5) 28.35 (5.94) 85.95 (3.37) 58.00 (1.24) 79.98 (5.00) 78.69 (3.99) 17.36 (5.46) 17.04 (5.74) 23.45 (9.53) 50.12 (4.49) Exp-k=8 (7.5, 0) 28.12 (1.98) 86.16 (1.07) 58.00 (1.24) 81.61 (2.07) 77.98 (1.14) 39.12 (1.96) 9.68 (1.53) 52.47 (1.65) Linear (7.5, 2.5) 24.98 (1.76) 28.57 (2.35) 86.08 (1.41) 58.00 (1.24) 81.61 (2.14) 77.98 (1.42) 42.84 (2.32) 11.05 (1.86) 25.25 (2.09) 53.05 (1.91) Linear (7.5, 0) 56.68 (1.87) 56.66 (2.10) 56.40 (3.04) 47.03 (7.79) 56.68 (2.00) 56.62 (2.45) 56.64 (2.39) 56.08 (3.33) 56.53 (2.80) 53.58 (4.95) 56.68 (1.85) 56.66 (2.13) 62.41 (1.87) 62.41 (2.10) 62.19 (3.09) 51.15 (7.93) 62.41 (1.99) 62.38 (2.46) 62.40 (2.39) 61.82 (3.37) 62.30 (2.81) 58.77 (5.00) 62.41 (1.85) 62.41 (2.13) 62.39 (1.00) 56.67 (1.00) 23.85 (1.00) Table 7: LLaDA Base with intermediate-curvature exponentials (k {4, 8}) and explicit thresholds. Highest accuracy per column in bold; second-highest in italics. The rightmost column reports mean accuracy with mean speedup in parentheses. 14 Method GPQA HellaSwag MMLU PIQA Winogrande GSM8K HotpotQA MultiNews WMT14 En-Fr WMT16 En-De Average 24.33 (1.00) 74.28 (1.00) 63.19 (1.00) 82.86 (1.00) 76.72 (1.00) 51.93 (1.00) 10.65 (1.00) 26.79 (1.00) 52.28 (1.00) Baseline 23.44 (5.97) 74.74 (1.93) 63.91 (1.63) 82.70 (1.81) 76.16 (1.45) 27.60 (18.62) 9.37 (3.31) 19.25 (37.98) 44.32 (26.05) 45.71 (15.26) 46.15 (11.52) Prophet 26.80 (2.35) 55.39 (24.65) 47.93 (29.64) 49.74 (6.83) 24.78 (1.89) 74.69 (1.92) 63.86 (1.64) 82.70 (1.78) 76.16 (1.45) 37.30 (2.25) Cosine (7.5, 2.5) 26.68 (2.63) 55.29 (24.66) 47.87 (29.65) 49.74 (7.06) 25.22 (2.18) 74.69 (1.93) 63.86 (1.64) 82.70 (1.78) 76.16 (1.57) 36.85 (2.50) Cosine (7.5, 0) Exp-k=16 (7.5, 2.5) 24.33 (4.19) 74.63 (2.22) 63.86 (1.64) 81.56 (2.82) 76.09 (2.10) 33.59 (5.74) 22.37 (8.57) 37.47 (35.10) 33.91 (37.22) 47.48 (10.87) 25.00 (9.26) 73.51 (5.00) 63.86 (1.64) 79.00 (5.00) 73.80 (5.00) 27.98 (11.45) 7.61 (9.17) 16.78 (17.38) 33.27 (44.67) 29.45 (45.10) 45.22 (20.66) Exp-k=16 (7.5, 0) 24.78 (2.12) 74.69 (1.92) 63.86 (1.64) 82.70 (1.78) 76.16 (1.46) 36.92 (2.60) 26.48 (2.91) 54.83 (24.76) 47.73 (29.69) 49.58 (7.07) Exp-k=2 (7.5, 2.5) 24.78 (2.71) 74.69 (1.94) 63.86 (1.64) 82.59 (2.09) 76.16 (1.67) 35.94 (3.22) 25.81 (3.73) 52.43 (25.23) 46.36 (29.89) 49.19 (7.52) Exp-k=2 (7.5, 0) 24.78 (2.68) 74.69 (1.93) 63.86 (1.64) 82.75 (2.01) 76.16 (1.49) 36.01 (3.29) 25.70 (3.88) 51.04 (25.69) 45.61 (30.10) 49.30 (7.56) Exp-k=4 (7.5, 2.5) 24.55 (3.92) 75.24 (2.50) 63.86 (1.64) 80.47 (2.51) 74.66 (2.54) 33.74 (4.57) 24.03 (5.59) 45.49 (27.83) 41.09 (31.67) 47.83 (8.77) Exp-k=4 (7.5, 0) 25.22 (3.44) 74.66 (2.07) 63.86 (1.64) 82.15 (2.37) 76.16 (1.72) 34.42 (4.33) 24.29 (5.50) 44.02 (28.83) 39.90 (32.36) 47.79 (8.47) Exp-k=8 (7.5, 2.5) 25.22 (5.91) 74.65 (4.21) 63.86 (1.64) 79.27 (4.76) 74.03 (4.64) 30.33 (7.03) 20.86 (9.53) 38.49 (33.37) 34.79 (36.03) 45.46 (11.19) Exp-k=8 (7.5, 0) 26.73 (2.42) 55.36 (24.66) 47.92 (29.65) 49.88 (6.85) 25.22 (1.88) 74.69 (1.92) 63.86 (1.64) 82.70 (1.78) 76.16 (1.45) 37.38 (2.31) Linear (7.5, 2.5) 24.78 (2.27) 74.69 (1.92) 63.86 (1.64) 82.70 (1.79) 76.16 (1.49) 36.47 (2.67) 26.64 (3.02) 54.90 (24.73) 47.74 (29.68) 49.58 (7.10) Linear (7.5, 0) 9.63 (1.92) 9.30 (2.56) 9.38 (2.49) 9.29 (3.80) 9.29 (3.31) 9.14 (5.74) 9.99 (1.74) 9.27 (2.10) 9.81 (1.76) 9.16 (2.07) 9.18 (4.10) 54.26 (1.00) 60.80 (1.00) Table 8: LLaDA Instruct with intermediate-curvature exponentials (k {4, 8}) and explicit thresholds. Highest accuracy per column in bold; second-highest in italics. The rightmost column reports mean accuracy with mean speedup in parentheses."
        }
    ],
    "affiliations": [
        "Ecole Polytechnique",
        "MBZUAI"
    ]
}