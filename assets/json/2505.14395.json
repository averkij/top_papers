{
    "paper_title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language",
    "authors": [
        "Seyoung Song",
        "Seogyeong Jeong",
        "Eunsu Kim",
        "Jiho Jin",
        "Dongkwan Kim",
        "Jay Shin",
        "Alice Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages."
        },
        {
            "title": "Start",
            "content": "MUG-Eval: Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language * Eunsu Kim"
        },
        {
            "title": "Jiho Jin",
            "content": "*"
        },
        {
            "title": "Dongkwan Kim",
            "content": ""
        },
        {
            "title": "Alice Oh",
            "content": ""
        },
        {
            "title": "KAIST",
            "content": ""
        },
        {
            "title": "Trillion Labs",
            "content": "{seyoung.song, sg.jeong28, kes0317, jinjh0123, dongkwan.kim}@kaist.ac.kr jay@trillionlabs.co, alice.oh@kaist.edu 5 2 0 2 0 2 ] . [ 1 5 9 3 4 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, novel framework that evaluates LLMs multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks (r > 0.75) while enabling standardized comparisons across languages and models. Our framework provides robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across multiple languages, but evaluating their multilingual generation abilities remains significant challenge. These challenges are particularly pronounced for low-resource languages, which often lack robust natural language processing tools, comprehensive reference corpora, or established benchmarks. Consequently, evaluation resources for these lowresource languages predominantly derive from massively multilingual evaluation benchmarks (Hasan *Equal contributions. Figure 1: General concept of MUG-Eval. Two instances of the same LLM engage in self-communication in the target language to complete information-gap tasks. Model outputs are evaluated using algorithmic methods (e.g., string matching or code testing), without requiring language-specific tools or LLMs-as-judges. Task success rate serves as proxy for measuring the models multilingual generation capability. et al., 2021; Goyal et al., 2022; Bandarkar et al., 2024; Adelani et al., 2024, inter alia). Extending and evaluating natural language generation tasks presents considerable complexity, especially in the absence of language-specific resources. Recent approaches (Holtermann et al., 2024; Pombal et al., 2025) have employed LLMs-asjudges, but they face an inherent limitationthe reliability of judgments depends directly on the evaluator LLMs performance in the target language. While this limitation may be less pronounced for high-resource languages (Pombal et al., 2025), the applicability of such approaches to low-resource languages remains questionable and has not been rigorously validated. Conventional evaluation approaches for generation capabilities often reFeature Global-MMLU Belebele Flores-101 XL-Sum MultiQ MUG-Eval Evaluates generation (not comprehension) Metrics comparable across languages No LLMs-as-Judges required Native speaker annotation is optional # of languages supported 42 122 47 137 2,102 Table 1: Positioning of MUG-Eval among multilingual evaluation benchmarks. MUG-Eval uniquely combines: (1) evaluation of generation capability (not just comprehension), (2) cross-linguistically comparable metrics, and (3) objective scoring without LLMs-as-judges, and (4) reduced dependency on cross-lingual annotation. Tested on 30 languages, MUG-Eval currently supports 2,102 languages via GlotLID (Kargaran et al., 2023), with the potential to scale further as more advanced language identification tools develop. Benchmarks referenced are MultiQ (Holtermann et al., 2024), Flores-101 (Goyal et al., 2022), XL-Sum (Hasan et al., 2021), GlobalMMLU (Singh et al., 2024a), and Belebele (Bandarkar et al., 2024). quire human-annotated ground truth data, such as BLEU (Papineni et al., 2002) for machine translation or ROUGE (Lin, 2004) for summarization. Overall, there exists gap in methodologies that offer both reliability and scalability for quantifying LLM generation performance across diverse languages. In this paper, we propose MUG-Eval, framework for evaluating the multilingual generation capabilities of LLMs, particularly for languages where direct evaluation proves challenging or infeasible. Our methodology creates informationgap scenarios that require successful communication in the target language to complete tasks, such as providing hidden information to one agent while another must discover it through questioning. We implement three tasks in MUG-Eval by adapting existing benchmarks (Zhang et al., 2024; Bandarkar et al., 2024; Muennighoff et al., 2024) into conversational and multilingual settingsEasy Twenty Questions, MCQ Conversation, and Code Reconstructionwhere task completion rates serve as proxies for different aspects of generation ability: reasoning, instruction following, and programming (3.1). Our approach builds on the insight from Muennighoff et al. (2024): instead of directly assessing LLM-generated text quality, we can indirectly measure how well the LLM comprehends what it has itself generated. We evaluate 8 LLMs across 30 languages from high-, mid-, and low-resource categories as defined by Singh et al. (2024b). Our experiments demonstrate that MUG-Eval has strong discriminative power, enabling precise comparisons both across languages and across models (4.1). The framework shows high internal consistency among its three tasks and correlates strongly (Pearsons > 0.75) with established benchmarks including Belebele (Bandarkar et al., 2024), MultiQ (Holtermann et al., 2024), and Global-MMLU (Singh et al., 2024a) (5.1). Additionally, our analysis of MCQ Conversation reveals that when native-language references are unavailable, English is not always the optimal substitute language, particularly for lowresource languages (5.2). Our primary contribution lies in proposing MUG-Eval1, novel language-agnostic framework for evaluating multilingual generation in large language models through self-comprehension tasks, without relying on language-specific NLP tools or human annotations. To demonstrate the utility and effectiveness of this framework, we structure the paper as follows. We begin by reviewing the landscape of multilingual generation evaluation, identifying critical gaps in existing methodologies that motivate our approach (2). We then present the design of MUG-Eval, introducing three conversational tasks that recast generation evaluation as communication-based task (3). We evaluate eight large language models in 30 linguistically diverse languages, demonstrating strong correlations with established benchmarks while offering unprecedented scalability (4). Through detailed analysis, we uncover cross-linguistic performance patterns and validate the effectiveness of MUG-Eval as robust, language-agnostic evaluation framework (5), and conclude with directions for future work in multilingual LLM evaluation (6)."
        },
        {
            "title": "2 Related Work",
            "content": "Reference-based metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and 1Code and dataset available at https://github.com/ seyoungsong/mugeval. Figure 2: Overview of evaluation tasks. Two instances of the same LLM engage in self-communication in the target language to complete information-gap tasks: (1) Easy Twenty Questionsguessing hidden word, (2) MCQ Conversationfinding the answer through passage-based dialogue, and (3) Code Reconstructionexplaining and reconstructing codes. chrF (Popovic, 2015) assess generation quality by comparing outputs against reference texts, usually requiring human-annotated target texts as ground truth. These metrics are widely adopted in benchmarks such as MEGA (Ahuja et al., 2023), GlotEval (Luo et al., 2025), Multi-IF (He et al., 2024), and BenchMAX (Huang et al., 2025). However, such reference-based approaches are limited by their reliance on high-quality parallel data, which is scarce in many languages. Moreover, they struggle in cross-lingual comparisons due to their sensitivity to lexical and syntactic features. To address these limitations, reference-free methodsparticularly those using LLMs as evaluatorshave gained attention (Dang et al., 2024; Holtermann et al., 2024; Pombal et al., 2025). Nonetheless, Hada et al. (2024) highlights the instability and reduced reliability of LLM evaluators in low-resource or non-Latin script languages, raising concerns about fairness and generalizability. An emerging line of work evaluates generation quality through downstream utility, assessing how well generated content supports task completion. Recent benchmarks explore the generation-comprehension link through interactive information-gap tasks that require mutual understanding. These include clarifying question generation (Gan et al., 2024), reference games (Gul and Artzi, 2024; Eisenstein et al., 2023), bidirectional code understanding (Muennighoff et al., 2024), and multi-turn interactive benchmarks such as HumanEvalComm (Wu and Fard, 2025), telephone-game simulations (Perez et al., 2025), and 20Q (Zhang et al., 2024). Drawing inspiration from 20Q (Zhang et al., 2024) and HumanEvalExplain (Muennighoff et al., 2024), our framework builds on tasks that inherently require both comprehension and generation, foregrounding successful communication as the central evaluation criterion. Designed to be language-agnostic, reference-free, and LLMindependent, it offers more equitable and scalable multilingual evaluation across an unlimited spectrum of languages. 3 MUG-Eval: Language-Agnostic"
        },
        {
            "title": "Evaluation Framework",
            "content": "MUG-Eval consists of three tasks adapted from existing benchmarks (Zhang et al., 2024; Bandarkar et al., 2024; Muennighoff et al., 2024) to evaluate multilingual generation capabilities. Each task requires two LLM instances with distinct roles to communicate in target language, with task completion rates serving as the primary evaluation metric. This section provides detailed descriptions of each task and evaluation procedures. Additional details, including prompts and generation parameters, are provided in the Appendix B.2."
        },
        {
            "title": "3.1 Tasks",
            "content": "Easy Twenty Questions. This task evaluates reasoning and strategic questioning abilities through word-guessing game. Drawing from the Things dataset (Zhang et al., 2024), we translate 140 English words into 30 languages using Google Translate. One model instance (answerer) receives hidden word from this set, while another (questioner) must identify it from list of 100 candidates. The questioner poses up to 20 yes/no questions in the target language, to which the answerer responds only with yes, no, or maybe in English. The predefined candidate list ensures consistent evaluation across languages, mitigating lexical diversity from affecting task difficulty or scoring mechanisms. MCQ Conversation. We transform the Belebele benchmark (Bandarkar et al., 2024)a reading comprehension dataset spanning 122 languagesinto conversational task. From the original dataset of 900 samples, we separate the reading passages from their corresponding questions and answer choices. Similar to the previous task, the answerer instance accesses only the passage, while the questioner sees the question and four answer options. To discover the correct answer, the questioner may ask up to 10 yes/no questions in the target language, receiving yes, no, or maybe responses in English, similar to the previous task. This design tests multi-turn instruction-following capabilities. Code Reconstruction. This task adapts HumanEvalExplain (Muennighoff et al., 2024) to assess code generation abilities across languages, not only in English. Using 164 Python function samples with corresponding unit tests, one model instance (describer) generates natural language explanation of the code in the target language. Another instance (rebuilder) then reconstructs the original function from this description and the function declaration snippet. Success is measured by whether the reconstructed code passes all unit tests."
        },
        {
            "title": "3.2 Evaluation Metrics",
            "content": "Task completion rate serves as our primary metric, calculated as the ratio of successfully completed tasks. We use exact string matching for word or choice predictions, with responses prompted to appear within double brackets and extracted via regular expressions. We employ GlotLID (Kargaran et al., 2023) to ensure the models responses are in the target language. Tasks fail when models: (1) produce question or description in wrong languages, (2) produce invalid responses, or (3) violate task-specific constraints such as including more than 20 consecutive source code characters in explanations."
        },
        {
            "title": "4 Experiments",
            "content": "Models. We evaluate eight multilingual large language models to assess their generation capabilities across diverse languages. Our selection includes four open-weight models: Llama 3.3-70B (Llama Team, 2024), Llama 3.1-8B, Qwen2.5-72B (Qwen Team, 2024), and Qwen2.5-7B, alongside four closed-source models: GPT-4o (OpenAI, 2024), GPT-4o-mini, Gemini 2.5 Flash (Google, 2025), and Gemini 2.0 Flash (Google, 2024). All models are accessed via API endpoints, with GPT-4o variants served through Azure OpenAI Services and the remaining models through OpenRouter. Detailed model information is provided in the Appendix B.1. Languages. We test our framework on 30 languages grouped by resource availability following Singh et al. (2024b)s classification, with 10 languages selected from each resource category. We include high-resource languages Arabic (arb), Chinese (zho), English (eng), French (fra), German (deu), Hindi (hin), Italian (ita), Japanese (jpn), Portuguese (por), and Spanish (spa); midresource languages Bengali (ben), Greek (ell), Hebrew (heb), Indonesian (ind), Korean (kor), Lithuanian (lit), Malay (zsm), Romanian (ron), Thai (tha), and Ukrainian (ukr); and low-resource languages Amharic (amh), Hausa (hau), Igbo (ibo), Kyrgyz (kir), Nepali (npi), Sinhala (sin), Somali (som), Swahili (swh), Telugu (tel), and Yoruba (yor). This selection covers diverse language families and writing systems, including Latin, Cyrillic, and Devanagari scripts, ensuring comprehensive evaluation across typologically distinct languages. Detailes language information is provided in the Appendix A.1. Figure 3: Accuracy of 8 LLMs across three tasks in 30 languages. Languages are grouped by resource level and sorted by average performance within each group. Results show that Code Reconstruction is the easiest task, followed by MCQ Conversation and Easy Twenty Questions. The gap is minor between high and mid-resource languages, but substantial between mid and low. Larger models consistently outperform smaller ones within the same language family, and tasks exhibits distinct ceiling effect. Model GPT-4o Gemini-2.0-flash Gemini-2.5-flash Qwen2.5-72B GPT-4o-mini Llama-3.3-70B Qwen2.5-7B Llama-3.1-8B All 62.21 51.93 62.26 35.17 31.95 33.79 7.90 8.45 Easy Twenty Questions ENG High Mid Low All MCQ Conversation ENG High Mid Low All Code Reconstruction ENG High Mid 75.71 51.43 72.14 72.14 53.57 53.57 20.71 25.00 72.64 56.07 70.57 53.86 44.29 44.14 14.50 12.64 69.21 55.57 66.36 40.64 35.93 40.36 6.64 7.71 44.79 44.14 49.86 11.00 15.64 16.86 2.57 5. 70.14 66.72 62.90 61.90 59.83 61.15 37.33 28.94 85.56 86.22 85.89 80.33 78.78 81.33 64.67 44.22 77.31 73.33 68.90 76.61 70.11 70.04 46.48 33.46 74.33 69.74 65.74 72.44 65.91 68.29 40.33 30.23 58.78 57.08 54.07 36.63 43.48 45.12 25.17 23.13 83.43 86.79 77.05 73.68 75.02 58.03 40.47 31. 88.41 89.02 90.85 84.76 87.80 75.61 75.00 51.83 89.02 89.21 74.63 87.56 82.50 68.05 56.28 46.10 86.59 89.45 84.39 84.15 80.12 65.61 46.22 36.16 Low 74.70 81.71 72.13 49.33 62.44 40.43 18.90 13.60 Table 2: Average accuracy (%) of 8 LLMs across three tasks, grouped by language resource categories. The best and the second-best performances within each task and resource category are bolded and underlined, respectively. consistent performance degradation is observed as the language resource level decreases from high (including English) to low."
        },
        {
            "title": "4.1 Results",
            "content": "one. Table 2 summarizes overall accuracy, and Figure 3 visualizes trends by language and task. Full results are provided in Appendix C.1. How difficult is MUG-Eval? Average accuracy scores across tasks vary depending on the model and the resource level of the language. Code Reconstruction emerged as the easiest task, followed by MCQ Conversation, while Easy Twenty Questions is the most challenging. This may be due to the number of interaction turns: multi-turn tasks are more error-prone as mistakes accumulate. This pattern aligns with average turn counts  (Table 8)  : Easy Twenty Questions requires the most turns, MCQ Conversation fewer, and Code Reconstruction only Performance varies across resource levels and models. The performance gap between highand mid-resource language groups is relatively small compared to the much larger gap observed between midand low-resource groups. Additionally, larger models consistently outperform smaller ones within the same model family. Despite some variation in task-wise rankings, overall trends of task ranking remain stable across models. Complementary ceiling effects exists across tasks. Code Reconstruction and MCQ Conversation saturate near the toparound 0.9 and 0.8, respectively. In contrast, Easy Twenty Questions exhibits saturation toward the lower end, with many scores concentrated near zeroespecially in lowresource languages and smaller models. MCQ Conversation shows lower saturation than its original benchmark, Belebele (0.8 vs. 0.95; see Figure 4), likely due to its split-agent design, which can produce ambiguous question generations, leading to unsolvable cases. These differing saturation patterns enhance the discriminative power of MUG-Eval. Easier tasks are more effective at separating weaker models and low-resource languages, while the harder task better distinguishes stronger models and high-resource languages. Together, they ensure that MUG-Eval maintains discriminative power across the full performance spectrum."
        },
        {
            "title": "5.1 Comparative Analysis",
            "content": "granularity. Overall, all three tasks in MUG-Eval shows greater discriminative capability than the three existing benchmarks. How consistent is performance across different tasks? Figure 5 compares performance correlations across six tasks, including the three introduced in MUG-Eval. Pearson correlation coefficients are all above 0.75, indicating strong consistency between task accuracy. Spearmans rank correlation coefficients exceed 0.75 in all cases, suggesting positive correlations in rank ordering. The reason why the correlations are not perfect is likely due to the distinct capabilities each task targets. Easy Twenty Questions primarily evaluates generation in aspect of reasoning, MCQ Conversation focuses on instruction following, Code Reconstruction assesses coding under information asymmetry. These differences account for the variation observed across tasks despite overall similarity. Figure 4: Score distributions across six evaluation tasks, demonstrating varying discriminative powers. Notably, MCQ Conversation, derived from the Belebele task, exhibits greater statistical dispersion, indicating greater ability to distinguish between models than the original Belebele benchmark. Figure 5: Correlation analysis between MUG-Eval tasks and existing multilingual benchmarks. Heatmaps show Pearsons (left) and Spearmans ρ (right) correlation coefficients between three MUG-Eval tasks and three established benchmarks. All correlations exceed 0.75, demonstrating strong consistency between MUG-Eval and existing evaluation methods, validating its effectiveness as multilingual evaluation framework. Which tasks best distinguish between models? Figure 4 presents violin plots of accuracy scores for six tasks, including the three introduced in MUGEval. Easy Twenty Questions exhibited broad distribution of scores, indicating strong discriminative power and the ability to distinguish models with varying capabilities. In contrast, Code Reconstruction showed much narrower range, suggesting limited differentiation in few models. Notably, MUGEvals MCQ Conversation demonstrated substantially greater discriminative power compared to the original Belebele task, highlighting its usefulness in evaluating multilingual understanding with finer Validation against established benchmarks. Figure 5 also compares performance correlations across six tasks, including the three introduced in MUG-Eval. While neither Pearsons nor Spearmans coefficients indicate perfect alignment between the three tasks in MUG-Eval and existing benchmarks, the figure demonstrates high degree of correlation. This suggests that MUG-Eval produces reliable results in terms of both accuracy and ranking, despite its low cost by the absence of human-annotated datasets. The detailed visualization result on Pearsons is provided in Appendix C.2."
        },
        {
            "title": "Substitution Analysis",
            "content": "Figure 6: MCQ Conversation accuracy comparison across 30 languages for GPT-4o using passages in: (1) the target language, (2) English, and (3) five fixed highresource languages (averaged), and (4)an optimized subset of up to five high-resource languages most similar to the target language. Results demonstrate that highresource language substitution more closely approximates native language performance than using English alone, especially for low-resource languages. The original MCQ Conversation task assumes that the answerer receives passage written in the target language. This raises practical question: if such passage is unavailable, can an English passage be used instead without significantly affecting performance? Would using passages from other high-resource languages yield better substitute? To investigate this, three experimental settings were compared: (1) using the original target language passage, (2) using an English passage, and (3) using passages in five high-resource languagesEnglish, Chinese, Arabic, Japanese, and Hindi. Two models, GPT-4o and GPT-4o-mini, were evaluated, with GPT-4o result presented in Figure 6. The result on the other model(GPT-4omini) is provided in the Appendix C.3. On average, performance based on the five highresource language passages more closely approximated that of the target-language baseline than when using English alone. This indicates that incorporating diverse high-resource languages may provide better alternative when native-language passages are unavailable. To further validate the applicability of MCQConversation, we conducted an evaluation to assess whether replacing native-language passages with those in five high-resource languages maintains consistent performance patterns across languages. The correlation between results using original target-language passages and those using the high-resource substitutes was 0.60 for Pearson (based on raw scores) and 0.71 for Spearman (based on rank-order consistency). Given that MUG-Eval is ultimately designed for cross-lingual comparisons, the higher Spearman correlation suggests that relative language rankings are preserved without native-language input. To deepen the analysis, we identified the highresource language combination that best approximates the native passage for each target language. MCQ Conversation was executed across all target languages using the five high-resource passages across two models: GPT-4o and GPT-4o-mini. For each case, the L2 distance between the performance with the substituted passage and that on the original native-language passage was calculated. The combination of high-resource language that minimizes this distance is reported in Table 7 and plotted in Figure 6. Results show that for highand mid-resource languages, the best-performing combination typically includes English. However, for low-resource languages, combinations excluding English usually performed better. This indicates that English is not always the optimal substitute, especially for low-resource languages. The details about the best combinations on each language is provided in Appendix C.4."
        },
        {
            "title": "5.3 Qualitative Error Analysis",
            "content": "Setup. To validate that task completion rates reflect genuine language capabilities, we analyzed failure patterns across tasks and languages. The authors manually annotated 160 GPT-4o conversation logs, sampling 20 success and failure cases each for Easy Twenty Questions and MCQ Conversation in English and Korean. Initial classification was performed using Gemini-2.5-flash, then manually corrected by two authors proficient in both languages. Findings. Figure 7 reveals systematic taskspecific error patterns that validate our framework design. Easy Twenty Questions failed primarily and language groups. Specifically, we measured (1) token count, (2) sequence length, (3) language fidelity, (4) instruction-following of the Answerer, and (5) interaction length. full description of these statistics is provided in Appendix D. We summarize key findings below: Token Count and Sequence Length: Output length varied by language resource level, with English being the shortest and low-resource languages generally producing the longest outputs. Language Fidelity: Although slightly lower in low-resource languages, fidelity scores remained similarly high across all groups. Answerer Instruction-Following and Interaction Length: These metrics were largely consistent across language resource groups and models. On average, Easy Twenty Questions involved 14.3 turns, and MCQ Conversation 4.0."
        },
        {
            "title": "6 Conclusion",
            "content": "A fundamental limitation in multilingual evaluation is the reliance on ground-truth references or LLM-based judgments, which are often unreliable or infeasible for low-resource languages. To address this, we introduce MUG-Eval, languageagnostic evaluation framework based on three conversational task completion between LLMs that assess both generation and comprehension. We evaluate 8 LLMs across 30 languages using MUG-Eval. Our framework demonstrates strong internal consistency and aligns well with established multilingual benchmarks, while remaining reference-free and cost-effective. Our results highlight few implications. First, MUG-Eval enables fine-grained performance comparisons even in lowresource settings due to its task diversity and saturation characteristics. Second, we find that substituting native-language passages with English often degrades performanceespecially for low-resource languagesunderscoring the need for evaluation methods that go beyond English-centric assumptions."
        },
        {
            "title": "Limitations",
            "content": "Figure 7: Attribution of errors by conversational role. Bars show the percentage of failures caused by Questioner (green), Answerer (blue), or Both roles (purple). due to questioner errors, reflecting strategic question generation challenges, while MCQ Conversation showed predominantly answerer errors, indicating passage comprehension difficulties. These patterns remained consistent across languages, confirming that failures stem from genuine communicative challenges rather than external factors. Success cases showed minimal errors in both roles, while rare successful cases with conversational errors reflected expected random chance. The LLMbased initial annotation achieved 78.8% accuracy (62.5% for failure cases, 95.0% for success cases). Representative Error Case. In the MCQConversation task, Questioner errors often stemmed from failures to faithfully incorporate all relevant information from the original query when generating questions. Key semantic or lexical elements were frequently omitted, resulting in questions that lacked sufficient grounding in the passageultimately leading to unanswerable or misleading queries. In contrast, Answerer errors primarily reflected incorrect inference from the passage. Detailed examples of representative error cases are provided in Appendix C.5. In the Easy Twenty Questions task, Questioner errors were typically caused by ineffective information-seeking strategies, such as asking insufficiently discriminative questions within the 20turn limit or making premature guesses despite the presence of multiple plausible candidates. Most Answerer errors in this task were due to hallucinated responses, where the model generated logically incorrect yes/no/maybe answers."
        },
        {
            "title": "5.4 Generation Statistics",
            "content": "While running the experiments, we collected detailed generation statistics, averaged over models MUG-Eval measures whether communication succeeds, but not how well it succeedsa model generating minimal functional text scores identically to one producing sophisticated, nuanced output, as long as both complete the task. This limitation poses challenges for applications requiring natural, culturally appropriate, or stylistically rich text generation. Furthermore, comparing linguistic quality across languages remains fundamentally difficult because notions of richness and quality vary significantly across linguistic and cultural contexts, making it challenging to establish universal crosslinguistic metrics. While this trade-off enables our language-agnostic and objective evaluation approach, it remains limitation for comprehensively assessing generation quality. While MUG-Evals reliability is supported by its strong correlations with existing benchmarks, comprehensive human evaluation has not yet been conducted. Our qualitative error analysis of 160 conversation logs (5.3) provided initial validation of failure patterns and confirmed that task failures stem from genuine communicative challenges rather than external factors. However, broader human validation across all 30 languages would provide deeper insights into the frameworks fairness across different languages and enable more detailed qualitative analysis of model performance patterns. Given the conversational nature of MUG-Evals tasks, human evaluation could reveal which specific conversational aspects challenge different models, particularly since performance varies significantly depending on conversational roles. Despite MUG-Evals language-agnostic design, certain implementation aspects remain Englishcentric. The difficulty of accurately translating prompts into all target languages, especially lowresource ones, necessitated using English for instructional prompts in the conversational scenarios. Additionally, the Code Reconstruction task employs Latin script for code, with variable and function names following English naming conventions. These factors may introduce systematic biases against non-Latin script languages and lowresource language contexts, potentially affecting the frameworks cross-linguistic validity."
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge using ChatGPT1 and Claude2 for writing and coding assistance, and Perplexity3 and OpenScholar (Asai et al., 2024) for literature 1https://chatgpt.com 2https://claude.ai 3https://perplexity.ai search."
        },
        {
            "title": "References",
            "content": "David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Haonan Gao, and En-Shiun Annie Lee. 2024. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226245, St. Julians, Malta. Association for Computational Linguistics. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 42324267, Singapore. Association for Computational Linguistics. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, and 6 others. 2024. Openscholar: Synthesizing scientific literature with retrievalaugmented lms. arXiv preprint arXiv:2411.14199. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand. Association for Computational Linguistics. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. arXiv preprint arXiv:2412.04261. Jacob Eisenstein, Vinodkumar Prabhakaran, Clara Rivera, Dorottya Demszky, and Devyani Sharma. 2023. Md3: The multi-dialect dataset of dialogues. In Interspeech 2023, pages 40594063. Yujian Gan, Changling Li, Jinxia Xie, Luou Wen, Matthew Purver, and Massimo Poesio. 2024. Clarqllm: benchmark for models clarifying and requesting information in task-oriented dialog. arXiv preprint arXiv:2409.06097. Google. 2024. Introducing gemini 2.0: our new ai model for the agentic era. Google. 2025. Gemini 2.5: Our most intelligent ai model. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Mustafa Omer Gul and Yoav Artzi. 2024. CoGen: Learning from feedback with coupled comprehension and generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1296612982, Miami, Florida, USA. Association for Computational Linguistics. Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2024. Are large language model-based evaluators the solution to scaling up multilingual evaluation? In Findings of the Association for Computational Linguistics: EACL 2024, pages 10511070, St. Julians, Malta. Association for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 46934703, Online. Association for Computational Linguistics. Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, and Sinong Wang. 2024. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553. Carolin Holtermann, Paul Röttger, Timm Dill, and Anne Lauscher. 2024. Evaluating the elementary multilingual capabilities of large language models with MultiQ. In Findings of the Association for Computational Linguistics: ACL 2024, pages 44764494, Bangkok, Thailand. Association for Computational Linguistics. Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, and Fei Yuan. 2025. Benchmax: comprehensive multilingual evaluation suite for large language models. arXiv preprint arXiv:2502.07346. Amir Hossein Kargaran, Ayyoob Imani, François Yvon, and Hinrich Schuetze. 2023. GlotLID: Language identification for low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 61556218, Singapore. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Llama Team. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Hengyu Luo, Zihao Li, Joseph Attieh, Sawal Devkota, Ona de Gibert, Shaoxiong Ji, Peiqin Lin, Bhavani Sai Praneeth Varma Mantina, Ananda Sreenidhi, Raúl Vázquez, Mengjie Wang, Samea Yusofi, and Jörg Tiedemann. 2025. Gloteval: test suite for massively multilingual evaluation of large language models. arXiv preprint arXiv:2504.04155. Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2024. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations. OpenAI. 2024. Gpt-4o contributions. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Jérémy Perez, Grgur Kovaˇc, Corentin Léger, Cédric Colas, Gaia Molinaro, Maxime Derex, Pierre-Yves Oudeyer, and Clément Moulin-Frier. 2025. When LLMs play the telephone game: Cultural attractors as conceptual tools to evaluate LLMs in multi-turn settings. In The Thirteenth International Conference on Learning Representations. José Pombal, Dongkeun Yoon, Patrick Fernandes, Ian Wu, Seungone Kim, Ricardo Rei, Graham Neubig, and André F. T. Martins. 2025. M-prometheus: suite of open multilingual llm judges. arXiv preprint arXiv:2504.04953. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Qwen Team. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Sebastian Ruder, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, and 5 others. 2024a. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, and 14 others. 2024b. Aya dataset: An open-access collection for multilingual instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11521 11567, Bangkok, Thailand. Association for Computational Linguistics. Jie Jw Wu and Fatemeh H. Fard. 2025. Humanevalcomm: Benchmarking the communication competence of code generation for llms and llm agent. ACM Trans. Softw. Eng. Methodol. Just Accepted. Yizhe Zhang, Jiarui Lu, and Navdeep Jaitly. 2024. Probing the multi-turn planning capabilities of LLMs via 20 question games. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1495 1516, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Data Preparation",
            "content": "A.1 Languages Throughout this paper, we evaluated LLMs across 30 languages: 10 high-resource, 10 mid-resource, and 10 low-resource languages. The resource classification follows the categorization defined by Singh et al. (2024b). ISO Code Language Script Resources Indonesian Latin Arabic Latin Latin Latin Devanagari Latin Japanese arb_Arab Arabic deu_Latn German eng_Latn English fra_Latn French hin_Deva Hindi ita_Latn Italian jpn_Jpan Japanese por_Latn Portuguese Latin spa_Latn Spanish Latin zho_Hans Chinese Simplified Han ben_Beng Bengali Bengali ell_Grek Greek Greek heb_Hebr Hebrew Hebrew ind_Latn kor_Hang Korean lit_Latn Lithuanian Latin ron_Latn Romanian Latin tha_Thai Thai Thai ukr_Cyrl Ukrainian Cyrillic zsm_Latn Malay Latin amh_Ethi Amharic Ethiopic hau_Latn Hausa Latin ibo_Latn Latin kir_Cyrl Kyrgyz Cyrillic npi_Deva Nepali Devanagari sin_Sinh Sinhala Sinhala som_Latn Somali Latin swh_Latn Swahili Latin tel_Telu Telugu Telugu yor_Latn Yoruba Latin Hangul Igbo High High High High High High High High High High Mid Mid Mid Mid Mid Mid Mid Mid Mid Mid Low Low Low Low Low Low Low Low Low Low Table 3: All 30 languages used in this paper with each languages corresponding ISO codes, scripts, and resource classifications defined by Singh et al. (2024b) A.2 Datasets Easy Twenty Questions. We began with 200 English words from the dev and test sets of the Things1 dataset (Zhang et al., 2024). We translated these words into all 30 target languages using Google Translate2. To ensure consistency and quality, we 1https://github.com/apple/ ml-entity-deduction-arena 2https://translate.google.com applied several filtering steps: we removed words where Latin characters persisted in non-Latin script languages, eliminated duplicates within each language, and filtered out remaining loan words to ensure semantic consistency across all languages. This filtering process yielded final set of 140 words that maintained equivalence across all 30 languages. For each target word in each language, we randomly sampled 99 additional words from the same language to create candidate pool of 100 words. The composition of these candidate pools and their ordering were kept consistent across all languages to ensure fair comparison. Table 4 provides example target words used in the Easy Twenty Questions task. Other tasks and benchmarks. We utilized datasets available on Hugging Face for Belebele3, HumanEvalExplain4, Global-MMLU5, and MultiQ6. Our experiments included the same 30 languages for Belebele and MultiQ that we used in our framework, while Global-MMLU experiments covered 29 languages (excluding Thai). For GlobalMMLU, we specifically used only the CulturallyAgnostic (CA) subset to ensure fair cross-lingual comparability across all evaluated languages."
        },
        {
            "title": "B Experimental Setup",
            "content": "B.1 Models We conduct our evaluation by selecting recent LLMs, accessing with APIs. This information is summarized in Table 5. B.2 Generations The tasks used in our evaluation were configured with different generation parameters, such as temperature, token limits, and thresholds for fidelity scoring. Details for each task are provided in Table 6. Generation settings. We modified several benchmark settings to ensure fair multilingual comparison. Key adjustments included explicitly prompting models to use the target language, rather than assuming responses would match the question language. For Code Reconstruction, we removed code description length limits since consistent length 3https://hf.co/datasets/facebook/belebele 4https://hf.co/datasets/bigcode/humanevalpack 5https://hf.co/datasets/CohereLabs/ Global-MMLU 6https://hf.co/datasets/caro-holt/MultiQ 2024), and Global-MMLU (Singh et al., 2024a) (for section 5.1). Each table outlines the rolespecific prompts that we provided to two separate model instances. For Easy Twenty Questions and MCQ Conversation, the instances act as questioner and an answerer; for Code Reconstruction, they act as describer and rebuilder. The prompt for Easy Twenty Questions is provided in Table 9, MCQ Conversation is in Table 10, and Code Reconstruction is in Table 11. The prompt for the preexisting three tasks are provided in Table 12."
        },
        {
            "title": "Analysis",
            "content": "This section presents comprehensive breakdown of our experimental results, including task-specific performance and its cross-lingual comparisons across multiple models. We also provide visualizations of task-wise correlations and additional evaluation results not included in the main paper. C.1 Results on all languages on all models Table 15, 16 present the evaluation results for all eight models across 30 languages and three tasks. For each model, we report task-wise accuracy scores across all languages, along with their corresponding Z-scores. To account for varying task difficulties and enable unified language ranking per model, we compute Z-scores that aggregate performance across the three tasks. Each tasks scores are standardized independently, using the global mean and standard deviation computed over all models and languages for that task. This ensures that task-specific differences in difficulty are normalized appropriately. We then compute the average Z-score across the three tasks per language, allowing for relative performance comparisons across languages within each model. Z-score above 0 indicates that the models accuracy on that language is above the global average, while negative score suggests below-average performance. These aggregated Z-scores provide normalized basis for ranking languages within each model and allow for interpretable comparisons. C.2 Visualizations of task-wise correlations We present set of 6 6 scatter plots in Figure 9, visualizing pairwise correlations between the six tasks. Each plot compares the accuracy scores of two tasks across all 30 languages for 8 models, resulting in one point per language per model. Table 4: Example target words used in the Easy Twenty Questions task. Words were sourced from the Things dataset and translated into 30 languages via Google Translate. constraints across different scripts isnt feasible. We use 5-shot prompting for Global-MMLU and zero-shot for Belebele. Prompts. We provide prompts used for the three main tasks introduced in Section 3.1, as well as for established benchmarks which are Belebele (Bandarkar et al., 2024), MultiQ (Holtermann et al., Model Model Identifier API Provider GPT-4o GPT-4o-mini gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 Azure OpenAI Service Gemini-2.5-flash Gemini-2.0-flash Qwen2.5-72B Qwen2.5-7B Llama-3.3-70B Llama-3.1-8B gemini-2.5-flash-preview-04-17 gemini-2.0-flash-001 Qwen/Qwen2.5-72B-Instruct Qwen/Qwen2.5-7B-Instruct meta-llama/Llama-3.3-70B-Instruct meta-llama/Llama-3.1-8B-Instruct OpenRouter Table 5: Model identifiers and API providers used in experiments Name Temperature Max Tokens Fidelity Threshold Easy Twenty Questions 0.7 MCQ conversation 0.7 Questioner: 1024 Answerer: 128 Questioner: 2048 Answerer: 256 Code reconstruction Global MMLU Belebele MultiQ Describer: 0.7 Rebuilder: 0.2 0.0 0.7 0.0 2048 32 2048 Model: 256 Judge: 32 Language: 0.7 Answer: 0.9 Language: 0.9 Answer: 0. Language: 0.9 N/A N/A Language: 0.9 Table 6: Task-specific generation settings used in the evaluation Each point in scatter plot represents the performance of particular language on two different tasks, with the xand y-axes indicating the accuracy scores for each task. These visualizations help identify trends and clusters, revealing how performance on one task relates to another across languages. These scatter plots serve as visual counterpart to the Pearson correlation coefficients (r) reported in Figure 5, offering an intuitive understanding of inter-task relationships observed in our experiments. C.3 Additional plot about language resource flexibility on MCQ Conversation Following up on the analysis in Section 5.2, we conducted the same experiment with GPT-4o-mini under identical settings. Figure 8 presents the MCQ Conversation accuracy across 30 languages when passages are provided in four different conditions: (1) the target language, (2) English, (3) fixed set of five highresource languages (averaged), and (4) selection of up to five high-resource languages that are most similar to the target language. The overall trend is consistent with that of GPT-4o (Figure 6). Figure 8: MCQ Conversation accuracy comparison across 30 languages for GPT-4o-mini, using passages in: (1) the target language, (2) English, (3) fixed set of five high-resource languages (averaged), and (4) selection of up to five high-resource languages most similar to the target language, with scores averaged. C.4 Additional analysis about language C.5 Human analysis case on resource flexibility on MCQ Conversation MCQ-Conversation Errors As described in Section 5.3, we conducted qualitative error analysis for both the Easy Twenty Questions and MCQ-Conversation tasks. Specifically, we examined which conversational agentthe Questioner or the Answererwas primarily responsible for task failure in each case. Tables 13 and 14 provide illustrative examples of typical errors for each role, along with our analysis of the underlying issues."
        },
        {
            "title": "D Generation Statistics",
            "content": "As stated in Section 5.4, we report detailed generation statistics in Table 8, averaged over models and language groups. Specifically, we measured the following: Token Count and Sequence Length: The number of tokens (# Token) and total character count (# Char) are computed from outputs generated in the target language by the questioner or the describer. The number of tokens were computed using the tokenizer associated with each model used in the experiments. Language Fidelity: Fidelity is measured as the percentage of questioner or describer outputs identified by GlotLID as matching the target language. Instruction-Following of the Answerer: Answer(A I-F) Instruction-Following is defined for Easy Twenty Questions and MCQ Conversation as the proportion of answerer responses that strictly follow the output format (yes, no, and maybe). Interaction Length: The number of question turns per interaction (# Turn) is reported for Easy Twenty Questions and MCQ Conversation, both of which are multi-turn tasks. To complement the substitution analysis in Section 5.2, Table 7 lists, for each of the 30 target languages, the subset of high-resource languages (selected from English, Chinese, Japanese, Hindi, and Arabic) that most closely approximates the original target-language passage in terms of MCQ Conversation accuracy. The optimal subset for each target language was determined by selecting the combination (up to five languages) that minimizes the L2 distance from the original accuracy, as described in Section 5.2. When the target language itself was one of the five high-resource languages, it was excluded from its own substitution set. These exclusions are marked with in the corresponding table entries. ISO Code Language Resources ENG ZHO ARB JPN HIN spa_Latn Spanish arb_Arab Arabic deu_Latn German fra_Latn French ita_Latn Italian por_Latn Portuguese zho_Hans Chinese eng_Latn English jpn_Jpan Japanese hin_Deva Hindi zsm_Latn Malay lit_Latn Lithuanian kor_Hang Korean ben_Beng Bengali ron_Latn Romanian ukr_Cyrl Ukrainian ell_Grek Greek heb_Hebr Hebrew ind_Latn tha_Thai Thai Indonesian sin_Sinh Sinhala npi_Deva Nepali kir_Cyrl Kyrgyz amh_Ethi Amharic swh_Latn Swahili hau_Latn Hausa ibo_Latn som_Latn Somali tel_Telu Telugu yor_Latn Yoruba Igbo High High High High High High High High High High Mid Mid Mid Mid Mid Mid Mid Mid Mid Mid Low Low Low Low Low Low Low Low Low Low Table 7: Optimal subsets of high-resource languages (selected from English, Chinese, Japanese, Hindi, and Arabic) for approximating the native-language passage performance in the MCQ Conversation task. For each target language, the listed subset scores the lowest L2 distance from the original accuracy. If the target language is one of the five high-resource options, it is excluded from its own substitution set, denoted with # Token Easy Twenty Questions # Char. Fidelity I-F # Turn # Token MCQ Conversation # Char. Fidelity I-F Code Reconstruction # Turn # Token # Char. Fidelity u L o All ENG High Mid Low GPT-4o Gemini-2.0-flash Gemini-2.5-flash Qwen2.5-72B GPT-4o-mini Llama-3.3-70B Qwen2.5-7B Llama-3.1-8B 29.95 11.19 16.19 18.88 54.77 14.80 9.81 9.70 57.47 14.28 38.33 61.22 33.83 52.12 45.28 44.47 41.97 69.87 38.52 22.60 24.23 78.58 34.67 82.68 81.07 54.97 95.00 96.52 95.78 95.59 93.61 97.16 94.49 95.28 96.48 97.64 91.52 93.78 93. 99.57 100.00 99.30 99.53 99.88 100.00 99.99 99.90 100.00 100.00 99.93 99.83 96.89 14.33 14.32 14.25 14.32 14.42 13.96 15.59 14.02 14.24 16.00 11.07 16.50 13.25 49.07 23.22 30.24 38.16 78.70 27.10 44.15 55.10 61.42 47.45 46.92 77.60 87. 103.85 111.43 94.63 92.54 124.31 71.54 110.25 178.68 98.82 81.39 82.72 128.97 138.48 98.32 99.88 97.76 98.93 98.28 99.80 99.06 91.88 99.85 99.85 99.86 97.16 99.12 99.50 99.95 99.37 99.72 99.40 100.00 100.00 99.88 100.00 100.00 98.85 99.92 97. 3.99 4.05 3.98 3.95 4.04 4.02 4.21 3.95 3.94 4.08 4.00 3.30 4.40 181.04 93.50 113.36 147.26 282.50 123.68 124.75 117.67 288.51 124.70 139.93 256.50 272.59 374.30 412.86 341.29 344.32 437.31 345.46 332.00 296.46 494.24 351.47 256.66 443.95 474. 97.72 99.63 97.91 98.64 96.61 99.91 99.85 96.48 99.46 99.98 99.83 92.84 93.39 Table 8: Average token count (# Token), character-level sequence length (# Character), GlotLID-based language fidelity (Fidelity), instruction-following rate of the answerer (A I-F), and average number of question turns (# Turn) are computed per task, model, and language group. Figure 9: Correlation matrix showing relationships between MUG-Eval tasks and existing multilingual benchmarks. Each cell displays Pearsons correlation coefficient (r) with 95% confidence intervals, with points colored by language resource level. Role Type Prompt Questioner Initial Instruction You will be solving an entity deduction game by asking questions about hidden item in {lang_full}. Your goal is to identify exactly one correct entity from list of 100 items through strategic questioning, using as few questions as possible. You can ask yes/no/maybe questions in {lang_full}, one at time. Each question must be concise and answerable only with \"Yes,\" \"No,\" or \"Maybe.\" Do not ask for hints or the answer directly. Among the list, exactly one is correct. You have maximum of 20 questions before making your final choice, but fewer questions are better. When you have determined the correct entity, provide your final answer using double brackets: [[entity]]. The entity must be exactly as it appears in the Entity List (do not translate to another language). <Entity List> {entity_list} <Language> {lang_full} Answerer Instruction Begin with your first question. \"Answer the following question about the hidden entity: {entity}. Respond only with \"\"Yes,\"\" \"\"No,\"\" or \"\"Maybe\"\" (including the period) without providing explanations or additional words. Do not use \"\"Maybe\"\" if the answer can be definitively \"\"Yes\"\" or \"\"No.\"\" Your responses must be in English. Never mention the entity in your response. If the query contains two or more questions asked simultaneously, consider it invalid and respond with \"\"Maybe.\"\" <Entity> {entity} <Language> {lang_f ull} <Question> {question} <Options> - Yes. - No. - Maybe. Questioner Final Instruction After 20 Turns You have now used all available questions. Based on the responses youve received, provide your final guess of the entity. Even if you cannot determine the entity with confidence, provide your best guess based on the information gathered. Indicate your answer in double brackets: [[entity]]. The entity must be exactly as it appears in the Entity List (do not translate to another language). Table 9: Prompt design for the Easy Twenty Questions task. The questioner and answerer are separately prompted with role-specific instructions to simulate Twenty Questions game. The prompts include task rules, language constraints, response formatting requirements, and structured input fields (e.g., {entity_list}, {lang_full}, {question}) Role Type Prompt Questioner Initial Instruction You will be solving multiple-choice question by asking questions about hidden passage. am serving as an intermediary between you and person who has this passage. You can ask me questions about the passage content, and will relay these to the person. They will respond with only yes, no, or maybe, which will then share with you. Your questions must be in {lang_full} and you can only ask one question at time. Do not ask for hints or request the passage directly. Among the four answer choices provided, exactly one is correct. You must ask exactly 4 questions (one corresponding to each answer choice) before making your final decision. After receiving all four responses, provide your final answer in {lang_full}, indicating the correct number choice with double brackets: [[X]] <Query> {query} <Choices> (1) {c1} (2) {c2} (3) {c3} (4) {c4} <Language> {lang_full} Answerer Instruction Answer the following question based solely on the information contained in the passage. Do not use any external knowledge or make assumptions beyond what is explicitly stated in the text. Your response must be in English and limited to exactly one of these three answers: \"Yes.\", \"No.\", or \"Maybe.\" (including the period). Do not provide any explanation or additional words. If the query contains two or more questions asked simultaneously, consider it invalid and respond with \"Maybe.\" <Passage> passage <Language> lang_full <Question> question <Options> - Yes. - No. - Maybe. Questioner Final Instruction After 10 Turns You have now used all available questions. Based on the responses youve received, please provide your final answer in lang_full. Indicate your choice by including the number of the correct answer in double brackets: [[X]] Table 10: Prompt design for the MCQ Conversation task. The questioner and answerer are assigned separate prompts to simulate collaborative multiple-choice reasoning task. The questioner asks yes/no questions based on hidden passage, while the answerer responds with constrained answers. Prompts include language and formatting instructions, as well as structured fields such as {query}, {passage}, and {lang_full}. Role Type Prompt Describer Describer Instruction for the English Language Instruction for Non-English Languages Provide concise natural language description of the code in {lang_full}. <Code> {code} <Language> {lang_full} Provide concise natural language description of the code in lang_full. Variable names or technical terms in English are allowed, but there shouldnt be any phrases or full sentences written in English or any language except lang_full. <Code> code <Language> lang_full Rebuilder Instruction \"Write functional code in {code_lang_f ull} according to the description written in {lang_f ull}. Start your code with the given declaration. Write only the code without any additional descriptions, comments, or delimiters. <Language> {lang_f ull} <Description> {description} <Declaration> {declaration} Table 11: Prompt design for the Code Reconstruction task. One model instance (the describer) generates natural language description of given code snippet in the target language. Another instance (the rebuilder) reconstructs the original function from this description and given declaration. The prompts specify language requirements and restrict the output format to code-only. Benchmark Type Prompt MultiQ Evaluatee Please provide an answer to the following question in {lang_full}. <Question> {question} <Language> {lang_full} MultiQ Classifier (LLM-as-aJudge) Determine if the answer to the following question is correct. The question is in English and the answer is in {lang_full}. Respond only with yes or no - do not include explanations or additional words. <Question> {question_en} <Language> {lang_full} <Answer> {model_pred} {question} A. {option_a} B. {option_b} C. {option_c} D. {option_d} Answer: Given the following passage, query, and answer choices, output the number corresponding to the correct answer in double brackets: [[X]] GlobalMMLU Belebele - - <Language> {lang_full} <Passage> {passage} <Query> query <Choices> (1) {c1} (2) {c2} (3) {c3} (4) {c4} Table 12: Prompt design for the pre-existing benchmark tasks used in our evaluation. For MultiQ, we include both evaluatee prompts and classification prompts for LLM-as-a-Judge. Global-MMLU and Belebele use simpler one-shot prompts formatted according to their original task definitions. Prompts include structured input fields such as {question}, {lang_full}, and {choices}"
        },
        {
            "title": "Passage",
            "content": "Victoria Falls is small city in western Zimbabwe, across the border from Livingston, Zambia and Botswana. This town is located right next to the waterfalls, and they are the towns main attraction, and this popular tourist attraction also offers many opportunities for adventurers and tourists alike to stay longer. During the rainy season (November to March), waterfalls increase in volume, and the waterfalls become more dramatic. Crossing bridge or walking down winding path near the waterfall will cause your clothes to get wet. On the other hand, the amount of water is so large that the actual size of the waterfall is obscured by the sheer volume! 빅토리아 폴스는 리빙스턴, 잠비아와 보츠와나 근처의 국경 건너편의 짐바브웨 서쪽에 있는 작은 도시입니다. 이 마을은 폭포 바로 옆에 위치해 있고, 그 폭포들이 마을의 주요 명소이며, 또한 이 유명한 관광지는 모험가들과 관광객들 모두에게 더 오래 머물 수 있는 많은 기회를 제공한다. 우기(11월부터 3월까지)에는 수량이 증가하며, 폭포가 더 극적으 로 변합니다. 다리를 건너거나 폭포 근처의 구불구불한 길을 따라 걸어가면 옷이 젖게 됩니다. 반면에, 물의 양이 너무 많아서 실제 보이는 폭포의 크기는 - 엄청난 수량으로 인해 - 가려진다!"
        },
        {
            "title": "Choices",
            "content": "If want an open view , when should visit the waterfall? 탁 트인 전망을 원한다면 언제 폭포를 방문해야 합니까? 1. From November to March 11월부터 3월까지 2. During the tourist season 관광철에 3. When the water volume is high 수량이 많을 때 4. When its not rainy season 장마철이 아닐 때 Conversation Questioner: Is the period from November to March suitable for visiting the waterfall? 폭포를 방문하기에 11월부터 3월까지가 적합한가요? Answerer: Yes. Questioner: Is the tourist season suitable for visiting the waterfall? 폭포를 방문하기에 관광철이 적합한가요? Answerer: Maybe. Questioner: Is it suitable to visit the waterfall when the water volume is high? 폭포를 방문하기에 수량이 많을 때가 적합한가요? Answerer: Maybe. Questioner: Is it suitable to visit the waterfall when it is not the rainy season? 폭포를 방문하기에 장마철이 아닐 때가 적합한가요? Answerer: Maybe."
        },
        {
            "title": "Error\nAnalysis",
            "content": "Each questions from the Questioner does not ask whether the visiting timing is suitable if the visitor wants an open view . As result, each question becomes an ambiguous question that is difficult to answer based on the passage. Table 13: Human analysis of case where the Questioner made an erroneous output in MCQ-conversation, ran by gpt-4o-2024-08-06. The original dataset and conversation are in Korean, with the Korean text shown in gray."
        },
        {
            "title": "Choices",
            "content": "During the 18th century Cambodia found itself squeezed between two powerful neighbors, Thailand and Vietnam. The Thais invaded Cambodia several times in the 18th century and in 1772 they destroyed Phnom Phen. In the last years of the 18th century the Vietnamese also invaded Cambodia. Which of the following statements accurately describes the invasions of Cambodia? 1. The Vietnamese invaded Cambodia several times 2. The Thais and Vietnamese invaded Cambodia in 1772 3. The Vietnamese destroyed Phnom Phen 4. The Thais invaded Cambodia several times Conversation Questioner: Is it true that the Vietnamese invaded Cambodia several times? Answerer: No. Questioner: Did the Thais and Vietnamese invade Cambodia in 1772? Answerer: Yes. Questioner: Is it true that the Vietnamese destroyed Phnom Penh? Answerer: No. Questioner: Did the Thais invade Cambodia several times? Answerer: Yes."
        },
        {
            "title": "Error\nAnalysis",
            "content": "The answerer gave an incorrect response to the second question. While there is mention that the Thais invaded in 1772, there is no information indicating that the Vietnamese invaded Cambodia in 1772. However, the answerer responded Yes. to the second question. Table 14: Human analysis of case where the Answerer made an erroneous output in MCQ-conversation, ran by gpt-4o-2024-08-06. resource lang high high mid high high high mid high mid mid high high high mid mid mid high mid mid mid low low low low low low low low low low eng zho ron ita por spa ell fra ukr heb arb deu jpn zsm ind kor hin tha ben lit kir npi swh tel sin som hau yor ibo amh resource lang high high mid mid mid high mid high high mid mid mid low high high low mid high mid mid high low low low high low low low low low eng zho ukr ind heb arb zsm hin jpn kor tha ron sin spa ita tel lit deu ell ben fra amh npi swh por ibo kir som yor hau gpt-4o-2024-08-06 Total avg. 1.37 1.33 1.29 1.26 1.24 1.18 1.17 1.16 1.16 1.13 1.12 1.12 1.08 1.06 1.05 1.05 1.04 1.03 0.84 0.81 0.77 0.72 0.68 0.56 0.35 0.16 0.1 -0.01 -0.07 -0. E-20Q (Acc) 1.62 (75.7) 1.94 (83.6) 1.7 (77.9) 1.73 (78.6) 1.47 (72.1) 1.29 (67.9) 1.35 (69.3) 1.32 (68.6) 1.62 (75.7) 1.76 (79.3) 1.29 (67.9) 1.32 (68.6) 1.5 (72.9) 1.08 (62.9) 1.17 (65) 1.35 (69.3) 1.41 (70.7) 1.41 (70.7) 1.35 (69.3) 0.67 (52.9) 1.05 (62.1) 1.05 (62.1) 0.2 (41.4) 0.7 (53.6) 0.34 (45) 0.17 (40.7) -0.28 (30) 0.2 (41.4) 0.08 (38.6) -0.16 (32.9) MCQ-C (Acc) 1.56 (85.6) 1.01 (75.2) 1.14 (77.7) 1.07 (76.3) 1.21 (79.1) 1.16 (78.1) 1.08 (76.6) 1.18 (78.4) 1.09 (76.7) 0.93 (73.8) 1.11 (77.1) 1.27 (80.2) 0.95 (74.1) 1.12 (77.3) 1.1 (77) 1.07 (76.4) 0.67 (68.9) 0.83 (71.8) 0.24 (60.7) 1.02 (75.4) 0.68 (69) 0.36 (63) 0.94 (73.9) 0.3 (61.8) 0.4 (63.7) -0.01 (55.9) 0.12 (58.4) -0.49 (46.8) -0.57 (45.3) -0.32 (50) gemini-2.5-flash-preview Total avg. 1.36 1.15 1.12 1.11 1.07 1.02 1.02 0.98 0.92 0.87 0.84 0.81 0.8 0.74 0.74 0.63 0.56 0.54 0.53 0.43 0.41 0.36 0.3 0.3 0.3 0.27 -0.04 -0.04 -0.06 -0.19 E-20Q (Acc) 1.47 (72.1) 1.59 (75) 1.29 (67.9) 1.32 (68.6) 1.29 (67.9) 1.32 (68.6) 1.11 (63.6) 1.62 (75.7) 1.05 (62.1) 1.23 (66.4) 1.35 (69.3) 1.32 (68.6) 1.2 (65.7) 1.17 (65) 1.53 (73.6) 1.11 (63.6) 0.85 (57.1) 1.56 (74.3) 1.08 (62.9) 1.44 (71.4) 1.53 (73.6) 0.31 (44.3) 1.26 (67.1) 0.11 (39.3) 1.2 (65.7) 0.4 (46.4) 1.05 (62.1) 0.17 (40.7) 0.11 (39.3) -0.28 (30) MCQ-C (Acc) 1.57 (85.9) 0.85 (72.2) 1.02 (75.3) 1.19 (78.6) 1 (75.1) 0.96 (74.2) 1.22 (79.2) 0.55 (66.6) 0.82 (71.7) 0.64 (68.2) 0.11 (58.1) 0.12 (58.3) 0.31 (61.9) 0.55 (66.4) 0.74 (70.1) 0.34 (62.6) 1 (75) 0.56 (66.7) -0.15 (53.3) -1.05 (36.2) 0.3 (61.8) 0.47 (65) -1.1 (35.3) 0.36 (62.9) -0.14 (53.4) -0.36 (49.3) 0.18 (59.4) -0.28 (50.9) -0.53 (46.1) -0.47 (47.2) CR (Acc) 0.94 (88.4) 1.04 (90.9) 1.02 (90.2) 0.99 (89.6) 1.04 (90.9) 1.09 (92.1) 1.07 (91.5) 0.99 (89.6) 0.79 (84.8) 0.69 (82.3) 0.97 (89) 0.76 (84.1) 0.79 (84.8) 0.97 (89) 0.86 (86.6) 0.71 (82.9) 1.04 (90.9) 0.86 (86.6) 0.94 (88.4) 0.74 (83.5) 0.59 (79.9) 0.74 (83.5) 0.92 (87.8) 0.69 (82.3) 0.31 (73.2) 0.33 (73.8) 0.46 (76.8) 0.28 (72.6) 0.28 (72.6) -0.89 (44.5) CR (Acc) 1.04 (90.9) 1.02 (90.2) 1.07 (91.5) 0.81 (85.4) 0.92 (87.8) 0.79 (84.8) 0.71 (82.9) 0.76 (84.1) 0.89 (87.2) 0.74 (83.5) 1.07 (91.5) 0.99 (89.6) 0.89 (87.2) 0.51 (78) -0.05 (64.6) 0.43 (76.2) -0.15 (62.2) -0.51 (53.7) 0.66 (81.7) 0.92 (87.8) -0.61 (51.2) 0.31 (73.2) 0.74 (83.5) 0.43 (76.2) -0.18 (61.6) 0.76 (84.1) -1.34 (33.5) -0.02 (65.2) 0.26 (72) 0.18 (70.1) resource lang high high mid high high high high high mid high mid high mid mid high mid mid mid mid mid low low low low low low low low low low eng zho ukr spa fra por deu jpn ron hin ell ita kor zsm arb tha ind ben heb lit npi kir swh tel hau som sin ibo yor amh resource lang high high high mid high mid high mid mid mid high high high mid mid mid mid high high mid low low low low low low low low low low eng fra jpn ukr zho ron por lit zsm ind hin deu spa kor ell heb tha ita arb ben sin kir tel swh amh hau som yor ibo npi gpt-4o-mini-2024-07-18 E-20Q Total (Acc) avg. 0.7 (53.6) 0.94 0.76 (55) 0.77 0.55 (50) 0.68 0.37 (45.7) 0.66 0.2 (41.4) 0.59 0.14 (40) 0.58 0.46 (47.9) 0.57 0.4 (46.4) 0.54 0.14 (40) 0.51 0.7 (53.6) 0.46 0.2 (41.4) 0.45 -0.31 (29.3) 0.39 -0.01 (36.4) 0.37 -0.25 (30.7) 0.34 -0.28 (30) 0.33 -0.07 (35) 0.29 -0.57 (22.9) 0.29 0.25 (42.9) 0.28 -0.22 (31.4) 0.22 -0.34 (28.6) 0.17 -0.13 (33.6) 0 -0.19 (32.1) -0.1 -0.96 (13.6) -0.16 -0.66 (20.7) -0.28 -1.02 (12.1) -0.53 -1.17 (8.6) -0.61 -1.05 (11.4) -0.61 -0.93 (14.3) -0.77 -1.2 (7.9) -0.92 -1.43 (2.1) -1.61 MCQ-C (Acc) 1.2 (78.8) 0.8 (71.3) 0.67 (68.8) 0.86 (72.3) 0.82 (71.7) 0.75 (70.3) 0.75 (70.2) 0.65 (68.3) 0.62 (67.8) 0.18 (59.6) 0.58 (67.1) 0.81 (71.4) 0.48 (65.1) 0.6 (67.6) 0.58 (67.1) 0.43 (64.2) 0.82 (71.7) 0.06 (57.2) 0.48 (65.1) 0.45 (64.6) -0.2 (52.3) -0.15 (53.3) -0.02 (55.7) -0.39 (48.7) -0.5 (46.7) -0.8 (41) -0.52 (46.3) -1.27 (32.1) -1.02 (36.8) -1.81 (21.9) gemini-2.0-flash-001 E-20Q (Acc) 0.61 (51.4) 0.88 (57.9) 1.2 (65.7) 0.96 (60) 0.91 (58.6) 0.67 (52.9) 0.64 (52.1) 0.58 (50.7) 0.91 (58.6) 0.67 (52.9) 0.99 (60.7) 0.58 (50.7) 0.91 (58.6) 0.82 (56.4) 0.91 (58.6) 0.7 (53.6) 0.52 (49.3) 0.49 (48.6) 0.82 (56.4) 1.08 (62.9) 0.96 (60) 0.61 (51.4) 0.7 (53.6) -0.07 (35) 0.76 (55) -0.43 (26.4) -0.04 (35.7) -0.13 (33.6) 0.11 (39.3) 0.61 (51.4) MCQ-C (Acc) 1.59 (86.2) 1.13 (77.6) 1.02 (75.4) 1.05 (76) 0.87 (72.7) 1.19 (78.6) 1.04 (75.9) 1.06 (76.1) 0.85 (72.1) 0.99 (74.9) 0.64 (68.2) 1.03 (75.6) 0.66 (68.7) 0.63 (68) 0.46 (64.8) 0.63 (68.1) 0.54 (66.3) 0.58 (67.1) 0.52 (66) -0.19 (52.6) 0.36 (63) 0.55 (66.6) 0.36 (63) 0.8 (71.2) -0.16 (53) 0.28 (61.3) 0.04 (56.9) -0.23 (51.7) -0.33 (49.8) -1.15 (34.3) Total avg. 1.06 1.04 1 0.99 0.99 0.96 0.93 0.91 0.88 0.88 0.87 0.86 0.84 0.8 0.79 0.74 0.72 0.69 0.68 0.61 0.6 0.57 0.57 0.5 0.34 0.26 0.23 0.15 0.11 0.08 CR (Acc) 0.92 (87.8) 0.74 (83.5) 0.81 (85.4) 0.74 (83.5) 0.76 (84.1) 0.86 (86.6) 0.51 (78) 0.56 (79.3) 0.79 (84.8) 0.51 (78) 0.59 (79.9) 0.66 (81.7) 0.64 (81.1) 0.66 (81.7) 0.69 (82.3) 0.51 (78) 0.61 (80.5) 0.53 (78.7) 0.41 (75.6) 0.41 (75.6) 0.33 (73.8) 0.03 (66.5) 0.51 (78) 0.21 (70.7) -0.07 (64) 0.13 (68.9) -0.28 (59.1) -0.12 (62.8) -0.53 (53) -1.6 (27.4) CR (Acc) 0.97 (89) 1.12 (92.7) 0.79 (84.8) 0.97 (89) 1.19 (94.5) 1.02 (90.2) 1.09 (92.1) 1.09 (92.1) 0.89 (87.2) 0.97 (89) 0.97 (89) 0.97 (89) 0.94 (88.4) 0.97 (89) 1.02 (90.2) 0.89 (87.2) 1.09 (92.1) 0.99 (89.6) 0.71 (82.9) 0.94 (88.4) 0.46 (76.8) 0.56 (79.3) 0.64 (81.1) 0.76 (84.1) 0.43 (76.2) 0.92 (87.8) 0.69 (82.3) 0.81 (85.4) 0.56 (79.3) 0.79 (84.8) Table 15: Results for each task on MUG-Eval across 30 languages, evaluated using gpt-4o-2024-08-06, gpt-4omini-2024-07-18, gemini-2.5-flash-preview, and gemini-2.0-flash-001. Accuracy was normalized using Z-scores and averaged across tasks. Languages were then ranked by their averaged Z-score. resource lang high high high mid high mid mid mid high high high mid mid mid high mid mid high high low low mid low low low low low low low low eng zho fra ind spa ron ukr zsm por ita deu ell heb tha arb lit ben jpn hin tel swh kor npi kir sin hau ibo som yor amh resource lang high high high high high high high mid high mid mid mid high mid high mid mid mid mid mid low low low low low low low low low low eng zho fra deu arb jpn por ron spa zsm ukr ind ita kor hin tha ell ben heb lit npi kir tel swh sin hau som ibo yor amh llama-3.3-70b-instruct Total avg. 0.81 0.79 0.56 0.55 0.54 0.53 0.51 0.5 0.45 0.4 0.36 0.24 0.2 0.14 0.11 0.1 0.1 -0.08 -0.15 -0.23 -0.24 -0.25 -0.38 -0.54 -0.58 -0.86 -1.08 -1.26 -1.35 -1.68 E-20Q (Acc) 0.7 (53.6) 1.11 (63.6) 0.61 (51.4) 0.73 (54.3) 0.43 (47.1) 0.61 (51.4) 0.55 (50) 0.73 (54.3) 0.14 (40) 0.25 (42.9) 0.14 (40) 0.05 (37.9) 0.28 (43.6) 0.17 (40.7) 0.43 (47.1) -0.31 (29.3) 0.23 (42.1) -1.02 (12.1) 0.28 (43.6) -0.54 (23.6) -0.43 (26.4) -1.52 (0) -0.13 (33.6) -0.84 (16.4) -0.78 (17.9) -0.96 (13.6) -0.87 (15.7) -1.2 (7.9) -1.11 (10) -1.37 (3.6) MCQ-C (Acc) 1.33 (81.3) 0.94 (73.9) 0.96 (74.3) 0.99 (74.9) 0.9 (73.1) 0.76 (70.6) 0.8 (71.3) 0.87 (72.6) 0.92 (73.4) 0.77 (70.8) 0.8 (71.2) 0.51 (65.8) 0.37 (63.1) 0.46 (64.9) 0.6 (67.4) 0.66 (68.7) 0.18 (59.4) 0.67 (68.9) -0.53 (46) 0.15 (58.9) 0.37 (63.1) 0.82 (71.7) -0.43 (48) -0.04 (55.4) 0.18 (59.4) -0.5 (46.7) -1.04 (36.4) -1.23 (32.9) -1.36 (30.3) -1.91 (20) qwen2.5-72b-instruct Total avg. 1.18 1.17 1.04 1.02 0.86 0.85 0.84 0.81 0.77 0.75 0.72 0.71 0.66 0.66 0.61 0.6 0.54 0.45 0.42 0.3 -0.07 -0.24 -0.51 -0.8 -1 -1.19 -1.27 -1.29 -1.4 -1.49 E-20Q (Acc) 1.47 (72.1) 1.29 (67.9) 0.96 (60) 0.94 (59.3) 0.58 (50.7) 0.82 (56.4) 0.4 (46.4) 0.52 (49.3) 0.14 (40) 0.4 (46.4) 0.43 (47.1) 0.05 (37.9) 0.02 (37.1) 0.28 (43.6) 0.49 (48.6) 0.43 (47.1) 0.05 (37.9) -0.1 (34.3) 0.11 (39.3) -0.54 (23.6) -0.19 (32.1) -0.57 (22.9) -0.72 (19.3) -1.2 (7.9) -1.31 (5) -1.25 (6.4) -1.31 (5) -1.22 (7.1) -1.4 (2.9) -1.46 (1.4) MCQ-C (Acc) 1.28 (80.3) 1.14 (77.8) 1.23 (79.4) 1.2 (78.8) 1.11 (77.1) 0.96 (74.3) 1.18 (78.4) 0.97 (74.4) 1.16 (78.1) 0.97 (74.4) 0.99 (74.8) 1.13 (77.4) 1.04 (75.9) 1 (75) 0.52 (65.9) 0.62 (67.8) 0.82 (71.6) 0.67 (68.8) 0.73 (69.9) 0.75 (70.3) -0.21 (52.1) -0.33 (49.8) -0.67 (43.4) -0.83 (40.4) -0.75 (42) -1.41 (29.3) -1.44 (28.8) -1.64 (25) -1.55 (26.8) -1.45 (28.7) CR (Acc) 0.41 (75.6) 0.31 (73.2) 0.1 (68.3) -0.07 (64) 0.28 (72.6) 0.21 (70.7) 0.18 (70.1) -0.1 (63.4) 0.31 (73.2) 0.18 (70.1) 0.15 (69.5) 0.15 (69.5) -0.05 (64.6) -0.2 (61) -0.71 (48.8) -0.05 (64.6) -0.1 (63.4) 0.1 (68.3) -0.2 (61) -0.3 (58.5) -0.66 (50) -0.05 (64.6) -0.58 (51.8) -0.73 (48.2) -1.14 (38.4) -1.11 (39) -1.34 (33.5) -1.37 (32.9) -1.57 (28) -1.75 (23.8) CR (Acc) 0.79 (84.8) 1.07 (91.5) 0.92 (87.8) 0.94 (88.4) 0.89 (87.2) 0.76 (84.1) 0.94 (88.4) 0.94 (88.4) 1.02 (90.2) 0.89 (87.2) 0.74 (83.5) 0.94 (88.4) 0.92 (87.8) 0.69 (82.3) 0.81 (85.4) 0.76 (84.1) 0.76 (84.1) 0.79 (84.8) 0.43 (76.2) 0.69 (82.3) 0.21 (70.7) 0.18 (70.1) -0.15 (62.2) -0.38 (56.7) -0.94 (43.3) -0.91 (43.9) -1.04 (40.9) -1.01 (41.5) -1.24 (36) -1.57 (28) resource lang high high high high mid high high high mid mid high mid mid high mid mid mid high mid mid low low low low low low low low low low eng spa ita por ind fra deu zho ukr ron jpn zsm kor hin tha ell lit arb heb ben ibo swh som hau tel kir yor sin amh npi resource lang high high high high high mid high mid mid mid mid high high mid mid high mid mid high mid low low low low low low low low low low eng zho spa fra deu ind ita ukr kor zsm ron arb por lit ell jpn tha heb hin ben tel som hau swh npi kir yor sin ibo amh llama-3.1-8b-instruct E-20Q (Acc) -0.49 (25) -0.84 (16.4) -0.99 (12.9) -1.2 (7.9) -1.05 (11.4) -0.96 (13.6) -1.08 (10.7) -1.05 (11.4) -1.08 (10.7) -1.17 (8.6) -1.14 (9.3) -1.14 (9.3) -1.22 (7.1) -0.93 (14.3) -1.14 (9.3) -1.28 (5.7) -1.31 (5) -1.31 (5) -1.37 (3.6) -1.25 (6.4) -1.14 (9.3) -1.43 (2.1) -1.28 (5.7) -1.2 (7.9) -1.31 (5) -1.37 (3.6) -1.11 (10) -1.4 (2.9) -1.46 (1.4) -1.43 (2.1) MCQ-C (Acc) -0.63 (44.2) -0.87 (39.6) -1.19 (33.6) -1.24 (32.6) -1.13 (34.8) -1.55 (26.8) -1.61 (25.7) -1.23 (32.9) -1.27 (32) -1.4 (29.6) -1.26 (32.2) -1.21 (33.2) -1.15 (34.3) -1.22 (33) -1.3 (31.4) -1.56 (26.7) -1.29 (31.7) -1.16 (34.1) -1.6 (25.9) -1.76 (22.8) -1.71 (23.7) -1.54 (26.9) -1.57 (26.4) -1.66 (24.7) -1.87 (20.8) -1.53 (27.1) -1.77 (22.6) -1.78 (22.4) -1.95 (19.2) -2.04 (17.6) Total avg. -0.56 -0.62 -0.85 -0.96 -0.97 -0.99 -1.1 -1.12 -1.13 -1.14 -1.17 -1.21 -1.21 -1.24 -1.26 -1.3 -1.35 -1.43 -1.49 -1.6 -1.61 -1.63 -1.64 -1.65 -1.7 -1.75 -1.76 -1.83 -1.9 -1.96 qwen2.5-7b-instruct E-20Q (Acc) -0.66 (20.7) -0.49 (25) -1.08 (10.7) -0.75 (18.6) -0.75 (18.6) -1.11 (10) -1.05 (11.4) -1.05 (11.4) -1.2 (7.9) -1.02 (12.1) -1.31 (5) -1.08 (10.7) -1.02 (12.1) -1.37 (3.6) -1.25 (6.4) -0.93 (14.3) -1.4 (2.9) -1.43 (2.1) -1.4 (2.9) -1.31 (5) -1.4 (2.9) -1.4 (2.9) -1.4 (2.9) -1.37 (3.6) -1.37 (3.6) -1.37 (3.6) -1.43 (2.1) -1.46 (1.4) -1.46 (1.4) -1.46 (1.4) MCQ-C (Acc) 0.45 (64.7) -0.27 (51) 0.06 (57.2) 0.06 (57.2) -0.11 (54) -0.02 (55.8) -0.54 (45.9) -0.43 (47.9) -0.54 (45.9) -0.59 (44.9) -0.79 (41.1) -0.45 (47.6) -2.01 (18.1) -1.12 (34.9) -0.91 (38.9) -0.8 (41) -1.49 (28) -0.87 (39.7) -1.48 (28.1) -1.57 (26.3) -1.48 (28.1) -1.68 (24.3) -1.66 (24.8) -1.59 (26) -1.6 (25.8) -1.45 (28.7) -1.83 (21.6) -1.7 (24) -1.6 (25.8) -1.77 (22.7) Total avg. 0.06 -0.21 -0.28 -0.29 -0.35 -0.41 -0.55 -0.59 -0.67 -0.69 -0.78 -0.89 -0.91 -1.19 -1.2 -1.22 -1.29 -1.36 -1.45 -1.47 -1.52 -1.62 -1.62 -1.63 -1.64 -1.65 -1.74 -1.74 -1.74 -1.78 CR (Acc) -0.58 (51.8) -0.15 (62.2) -0.38 (56.7) -0.43 (55.5) -0.73 (48.2) -0.45 (54.9) -0.61 (51.2) -1.09 (39.6) -1.04 (40.9) -0.86 (45.1) -1.11 (39) -1.29 (34.8) -1.27 (35.4) -1.57 (28) -1.34 (33.5) -1.06 (40.2) -1.44 (31.1) -1.82 (22) -1.49 (29.9) -1.8 (22.6) -1.98 (18.3) -1.9 (20.1) -2.08 (15.9) -2.1 (15.2) -1.93 (19.5) -2.33 (9.8) -2.41 (7.9) -2.31 (10.4) -2.28 (11) -2.41 (7.9) CR (Acc) 0.38 (75) 0.13 (68.9) 0.18 (70.1) -0.18 (61.6) -0.2 (61) -0.1 (63.4) -0.05 (64.6) -0.3 (58.5) -0.28 (59.1) -0.45 (54.9) -0.23 (60.4) -1.14 (38.4) 0.31 (73.2) -1.06 (40.2) -1.44 (31.1) -1.93 (19.5) -0.99 (42.1) -1.77 (23.2) -1.47 (30.5) -1.52 (29.3) -1.67 (25.6) -1.77 (23.2) -1.8 (22.6) -1.93 (19.5) -1.95 (18.9) -2.13 (14.6) -1.95 (18.9) -2.05 (16.5) -2.15 (14) -2.1 (15.2) Table 16: Results for each task on MUG-Eval across 30 languages, evaluated using llama-3.3-70b-instruct, llama3.1-8b-instruct, qwen2.5-72b-instruct and qwen2.5-7b-instruct. Accuracy was normalized using Z-scores and averaged across tasks. Languages were then ranked by their averaged Z-score."
        }
    ],
    "affiliations": [
        "KAIST",
        "Trillion Labs"
    ]
}