{
    "paper_title": "Animate-X: Universal Character Image Animation with Enhanced Motion Representation",
    "authors": [
        "Shuai Tan",
        "Biao Gong",
        "Xiang Wang",
        "Shiwei Zhang",
        "Dandan Zheng",
        "Ruobing Zheng",
        "Kecheng Zheng",
        "Jingdong Chen",
        "Ming Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 6 0 3 0 1 . 0 1 4 2 : r Preprint. Under review. ANIMATE-X: UNIVERSAL CHARACTER IMAGE ANIMATION WITH ENHANCED MOTION REPRESENTATION Shuai Tan1, Biao Gong1, Xiang Wang2, Shiwei Zhang2, Dandan Zheng1, Ruobing Zheng1, Kecheng Zheng1, Jingdong Chen1, Ming Yang1 1Ant Group {tanshuai2001,a.biao.gong}@gmail.com, {xiaolao.wx,zhangjin.zsw}@alibaba-inc.com,{yuandan.zdd, zhengruobing.zrb,zhengkecheng.zkc,jingdongchen.cjd,m.yang}@antgroup.com 2Alibaba Group Project Page: https://lucaria-academy.github.io/Animate-X/ Figure 1: Animations produced by Animate-X which extends beyond human to anthropomorphic characters with various body structures, e.g., without limbs, from games, animations, and posters."
        },
        {
            "title": "ABSTRACT",
            "content": "Character image animation, which generates high-quality videos from reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce new Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods. Work done during internship at Ant Group. Project lead and corresponding author. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "Character image animation Yang et al. (2018); Zablotskaia et al. (2019b) is compelling and challenging task that aims to generate lifelike, high-quality videos from reference image and target pose sequence. modern image animation method shall ideally balance the identity preservation and motion consistency, which contributes to the promise of broad utilization Hu et al. (2023); Xu et al. (2023a); Chang et al. (2023a); Jiang et al. (2022). The phenomenal successes of GAN Goodfellow et al. (2014); Yu et al. (2023); Zhang et al. (2022b) and generative diffusion models Ho et al. (2022; 2020); Guo et al. (2023) have reshaped the performance of character animation generation. Nevertheless, most existing methods only apply to the human-specific character domain. In practice, the concept of character encompasses much broader concept than human, including anthropomorphic figures in cartoons and games, collectively referred to as X, which are often more desirable in gaming, film, short videos, etc. The difficulty in extending current models to these domains can be attributed to two main factors: (1) the predominantly human-centered nature of available datasets, and (2) the limited generalization capabilities of current motion representations. The limitations are clearly evidenced for non-human characters in Fig. 5. To replicate the given poses, the diffusion models trained on human dance video datasets tend to introduce unrelated human characteristics which may not make sense to reference figures, resulting in abnormal distortions. In other words, these models treat identity preservation and motion consistency as conflicting goals and struggle to balance them, while motion control often prevails. This issue is particularly pronounced for non-human anthropomorphic characters, whose body structures often differ from human anatomysuch as disproportionately large heads or the absence of arms, as shown in Fig. 1. The primary cause is that the motion representations extracted merely from pose conditions are hard to generalize to broad range of common cartoon characters with unique physical characteristics, leading to their excessive sacrifices in identity preservation in favor of strict pose consistency, which is an unsensible trade-off between these conflicting goals. To address this issue, the natural approach is to enhance the flexibility of motion representations without discarding current pose condition, which can prevent the model from making unsensible trade-offs between overly precise poses and low fidelity to reference images. To this end, we identify two key limitations of existing methods. First, the simple 2D pose skeletons, constructed by connecting sparse keypoints, lack of image-level details and therefore cannot capture the essence of the reference video, such as motion-induced deformations (e.g., body part overlap and occlusion) and overall motion patterns. Second, the self-driven reconstruction strategy aligns reference and pose skeletons by body shape, simplifying animation but ignoring shape differences during inference. These inspire us to design the new Pose Indicator from both implicit and explicit perspectives. In this paper, we propose Animate-X for animating any character X. Sparked by generative diffusion models Rombach et al. (2022), we employ 3D-UNet Blattmann et al. (2023) as the denoising network and provide it with motion feature and figure identity as condition. To fully capture the gist of motion from the driving video, we introduce the Pose Indicator, which consists of the Implicit Pose Indicator (IPI) and the Explicit Pose Indicator (EPI). Specifically, IPI extracts implicit motionrelated features with the assistance of CLIP image feature, isolating essential motion patterns and relations that cannot be directly represented by the pose skeletons from the driving video. Meanwhile, EPI enhances the representation and understanding of the pose encoder by simulating real-world misalignments between the reference image and driven poses during training, strengthening the ability to generate explicit pose features. With the combined power of implicit and explicit features, Animate-X demonstrates strong character generalization and pose robustness, enabling general character animation even though it is trained solely on human datasets. Moreover, we introduce new Animated Anthropomorphic Benchmark (A2Bench), which includes 500 anthropomorphic characters along with corresponding dance videos, to evaluate the performance of Animate-X on other types of characters. Extensive experiments on both public human animation datasets and A2Bench demonstrate that Animate-X outperforms state-of-the-art methods in preserving identity and maintaining motion consistency in animating X. Main contributions summarized as follows: We present Animate-X, which facilitates image-conditioned pose-guided video generation with high generalizability, particularly for attractive anthropomorphic characters. To the best of our knowledge, this is the first work to animate generic cartoon images without the need for strict pose alignment. Preprint. Under review. The rethinking about the motion inspire us to propose Pose Indicator, which extracts motion representation suitable for anthropomorphic characters in both implicit and explicit manner, enhancing the robustness of Animate-X. Since the popular datasets only contain human video with limited character diversity, we present new A2Bench, specifically for evaluating performance on anthropomorphic characters. Extensive experiments demonstrate that our Animate-X outperforms the competing methods quantitatively and qualitatively on both A2Bench and current human animation benchmark."
        },
        {
            "title": "2.1 DIFFUSION MODELS FOR IMAGE/VIDEO GENERATION",
            "content": "In recent years, diffusion models Song et al. (2021); Ho et al. (2020) have demonstrated strong generative capabilities, pushing image generation technique towards daily productivity tool Nichol et al. (2022); Ramesh et al. (2022); Mou et al. (2023); Huang et al. (2023); Zhang et al. (2023a); Liu et al. (2023). Pioneering works such as DALL-E 2 Ramesh et al. (2022) and Imagen Saharia et al. (2022) have showcased the extraordinary potential of diffusion models for high-quality image synthesis. Notable contributions, including Stable Diffusion Rombach et al. (2022), have well balanced scalability and efficiency, making diffusion-based image generation accessible and versatile across various applications. On the video generation front, diffusion models are making amazing progress Singer et al. (2023); Wang et al. (2023a; 2024c); Wu et al. (2023); Chai et al. (2023); Ceylan et al. (2023); Guo et al. (2023); Zhou et al. (2022); An et al. (2023); Xing et al. (2023); Qing et al. (2023); Yuan et al. (2023); Tan et al. (2024d); Gong et al. (2024). These methods joint spatio-temporal modeling to generate realistic motion dynamics and ensure temporal consistency, marking substantial step forward in generative models for video content. In this work, we aim to tackle the character-centered image animation task, dedicated of conditional video generation. Our approach enables the transformation of static images into dynamic animations by conditioning on desired motion. This innovation bridges the gap between image and video generation, highlights the versatility and adaptability of diffusion models in creating engaging visual narratives. 2.2 POSE-GUIDED CHARACTER MOTION TRANSFER Character image animation aims to transfer motion from the source character to the target identity Zhang et al. (2024); Chang et al. (2023b), which has experienced an impressive journey to improve animation quality and versatility. Early works Li et al. (2019); Siarohin et al. (2019b; 2021b); Zhao & Zhang (2022b); Tan et al. (2024a); Wang et al. (2022); Tan et al. (2024c;b; 2023) predominantly utilize Generative Adversarial Networks (GANs) to generate animated human images. However, these GAN-based models are often confronted by the emergence of various artifacts in the generated outputs. With the advent of diffusion models, researchers Shen et al. (2024); Zhu et al. (2024) explored how to go beyond GANs. One effort is Disco Wang et al. (2023b), which leverages ControlNet Zhang et al. (2023b) to facilitate human dance generation, demonstrating the potential of diffusion models in generating dynamic human poses. Following this, MagicAnimate Xu et al. (2023b) and Animate Anyone Hu et al. (2023) introduce transformer-based temporal attention modules Vaswani (2017), enhancing the temporal consistency of animations and resulting in more smooth movement transitions. Sparked by the linear time efficiency of Mamba Gu & Dao (2023); Gu et al. (2021) conceptually merges the merits of parallelism and non-locality, Unianimate Wang et al. (2024b) resorts to it resorts to Mamba for efficient temporal modeling. While these approaches have improved the realism of the animations, notable limitation remains: most current methods require strict alignment between reference image and driving video. This restricts their applicability in the scenarios where poses cannot be easily extracted, such as anthropomorphic characters, often resulting in bizarre and unsatisfactory outputs. In contrast, our approach adopts robust and flexible motion representation to mitigate the dependence on pose alignment. This enables the generation of high-quality animations even in cases where previous methods struggle with non-alignable poses. In this manner, our method enhances the versatility and applicability of character image animation across broad range of contexts (X character). 3 Preprint. Under review. φ and latent feature Figure 2: (a) The overview of our Animate-X. Given reference image r, we first extract CLIP image feature via CLIP image encoder Φ and VAE encoder E. The proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI) produce motion feature fi and pose feature fe, respectively. fe is concatenated with the noised input ϵ along the channel dimension, then further concatenated with along the temporal dimension. This serves as the input to the diffusion model ϵθ for progressive denoising. During the denoising process, φ and fi provide appearance condition from and motion condition from 1:F . At last, VAE decoder is adopted to map the generated latent representation z0 to the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator."
        },
        {
            "title": "3 METHOD",
            "content": "In this work, we aim to generate an animated video that maintains consistency in identity with reference image and body movement with driving video 1:F . Different from previous works, our primary objective is to animate general characters beyond human, particularly like anthropomorphic ones, which has broader applications in entertainment industry. 3.1 PRELIMINARIES OF LATENT DIFFUSION MODEL diffusion model (DM) operates by learning probabilistic process that models data generation through noise. To mitigate the heavy computational load of traditional pixel-based diffusion models in high-dimensional RGB spaces, latent diffusion models (LDMs) Rombach et al. (2022) propose to shift the process into lower-dimensional latent space using pre-trained variational autoencoder (VAE) Kingma (2013). It encodes the input data into compressed latent representation z0. Gaussian noise is then incrementally added to this latent representation over several steps, reducing computational requirements while maintaining the generative capabilities of the model. The process can be formalized as: q(ztzt1) = (zt; (cid:112)1 βtzt1, βtI), (1) where βt (0, 1) represents the noise schedule. As 1, 2, ..., increases, the cumulative noise applied to the original z0 intensifies, causing zt to progressively resemble random Gaussian noise. 4 Preprint. Under review. Compared to the forward diffusion process, the reverse denoising process pθ aims to reconstruct the clean sample z0 from the noisy input zt. We represent the denoising step p(zt 1zt) as follows: pθ(zt1zt) = (zt1; µθ(zt, t), Σθ(zt, t)), (2) in which µθ(zt, t) refers to the estimated target of the reverse diffusion process and the process typically is achieved by diffusion model ϵθ with the parameters θ. To model the temporal dimension, the denoising model ϵθ is commonly built on 3D-UNet architecture Blattmann et al. (2023) in video generation methods Hu et al. (2023); Wang et al. (2023c). Given the input conditional guidance c, they usually use an L2 loss to reduce the difference between the predicted noise and the ground-truth noise during the optimization process: = Eθ ϵ ϵθ(zt, t, c)2(cid:105) (cid:104) (3) once the reversed denoising stage is complete, the predicted clean latent is passed through the VAE decoder to reconstruct the predicted video in pixel space."
        },
        {
            "title": "3.2 POSE INDICATOR",
            "content": "To extract motion representations, previous works typically detect the pose keypoints via DW1:F and further visualize them as pose image p, Pose Yang et al. (2023) from the driven video which are trained using self-driven reconstruction strategy. However, it brings several limitations as mentioned in Sec. 1: (1) The sole pose skeletons lack image-level details and are therefore unable to capture the essence of the reference video, such as motion-induced deformations and overall motion patterns. (2) The self-driven reconstruction training strategy naturally aligns the reference and pose images in terms of body shape, which simplifies the animation task by overlooking likely body shape differences between the reference image and the pose image during inference. Both limitations weaken the model to develop deep, holistic motion understanding, leading to inadequate motion representation. To address these issues, we propose Pose Indicator, which consists of Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI). φ = Φ(I Implicit Pose Indicator (IPI). To extract unified motion representations from the driving video in the first limitation, we resort to the CLIP image feature 1:F ) extracted by CLIP Image Encoder. CLIP utilizes contrastive learning to align the embeddings of related images and texts, which may include descriptions of appearance, movement, spatial relationships and etc. Therefore, the CLIP image feature is actually highly entangled representation, containing motion patterns and relations helpful to animation generation. As presented in Fig. 2 (a), we introduce lightweight extractor which is composed of stacked layers of cross-attention and feed-forward networks (FFN). In cross attention layer, we employ φ as the keys (K) and values (V ). Consequently, the challenge becomes designing an appropriate query (Q), which should act as guidance for motion extraction. Considering that the keypoints pd extracted by DWPose provide direct description of the motion, we design transformer-based encoder to obtain the embedding qp, which is regarded as an ideal candidate for Q. Nevertheless, motion modeling using sole sparse keypoints is overly simplistic, resulting in the loss of underlying motion patterns. To this end, we draw inspiration from query transformer architecture Awadalla et al. (2023); Jaegle et al. (2021) and initialize learnable query vector ql to complement sparse keypoints. Subsequently, we feed the merged query qm = qp + ql and φ into and get the implicit pose indicator fi, which contains the essential representation of motion that cannot be represented by the simple 2D pose skeletons. Explicit Pose Indicator (EPI). To deal with the second limitation in the training strategy, we propose EPI, designed to train the model to handle misaligned input pairs during inference. The key insight lies in simulating misalignments between reference image and pose images during training while ensuring the motion remains consistent with the given driving video 1:F . Therefore, we explore two pose transformation schemes: Pose Realignment and Pose Rescale. As shown in Fig. 2 (b), in the pose realignment scheme, we first establish pose pool containing pose images from the training set. In each training step, we first sample the reference image and the driving pose following previous works. Additionally, we randomly select an align anchor pose anchor from the pose pool. This anchor serves as reference for aligning the driving pose, producing the aligned pose realign. However, since the characters we aim to animate are often anthropomorphic characters, whose shapes can significantly differ from human, such as varying head-to-shoulder ratios, extremely short legs, or even the absence of arms (as shown in Fig. 1 and Fig. 5), relying solely 5 Preprint. Under review. on pose realignment is insufficient to capture these variations for simulation. Therefore, we further introduce Pose Rescale. Specifically, we define set of keypoint rescaling operations, including modifying the length of the body, legs, arms, neck, and shoulders, altering face size, even adding or removing specific body parts and etc. These transformations are stored in rescale pool. After obtaining the realigned poses realign, we apply random selection of transformations from this pool with certain probability on them, generating the final transformed poses (additional examples of transformations are provided in the Appendix A). Note that we set the probability of λ [0, 1] to apply the pose transformation, and with probability of 1 λ, the pose image remains unchanged. Subsequently, is encoded to the explicit feature fe via Pose Encoder."
        },
        {
            "title": "3.3 FRAMEWORK AND IMPLEMENT DETAILS",
            "content": "In light of the success of previous works Hu et al. (2023); Zhang et al. (2024), Animate-X follows the main framework, which consists of several encoders for feature extraction and 3D-UNet Wang et al. (2023a;c); Blattmann et al. (2023) for video generation. As shown in Fig. 2, given reference image r, we employ the pretrained CLIP Image Encoder Φ Radford et al. (2021) to extract appearance feature φ from r. To reduce the parameters of the framework and facilitate appearance alignment, we exclude the Reference Net presented in most of the previous works Hu et al. (2023); Zhang et al. (2024); Zhu et al. (2024). Instead, VAE encoder is utilized to extract the latent from r, which is then directly used as part of the input for the denoising network representation 1:F , we detect the pose keypoints pd and ϵθ following Wang et al. (2024b). For the driven video CLIP feature via DWPose Yang et al. (2023) and CLIP Image Encoder Φ. Subsequently, IPI and EPI introduced in Sec. 3.2 extract the implicit latent fi and explicit latent fe, respectively. The explicit fe is first concatenated with the noised latent ϵ to obtain the fused features along the channel dimension, which is further stacked with along the temporal dimension, resulting in combined features fmerge. Then, the combined features are fed into the video diffusion model ϵθ for jointly appearance alignment and motion modeling. The diffusion model ϵθ comprises multiple stacked layers of Spatial Attention, Motion Attention and Temporal Attention. The Spatial Attention receives inputs from fmerge and and fuses the identity condition from with the motion condition from through cross-attention (CA), producing an intermediate representation x. To further enhance motion consistency, the implicit representation fi is fed into the Motion Attention module, along with in the form of residual connection, resulting in the representation = + CA(x, fi). Inpsired by the linear time efficiency of Mamba Gu & Dao (2023) in long sequence processing, we employ it as Temporal Attention module to maintain the temporal consistency. Training and Inference. To improve the models robustness against pose and reference image misalignments, we adopt two key training schemes. First, we set high transformation probability λ (over 98%) in the EPI, enabling the model to handle wide range of misalignment scenarios. Second, we apply random dropout to the input conditions at predefined rate Wang et al. (2024b). After that, while the reference image and driven video are from the same human dancing video during training, in the inference phase (Fig. 9 (b)), Animate-X can handle an arbitrary reference image and driven video, which may differ in appearance. 3.4 A2BE The main task of our Animate-X is to animate an anthropomorphic character with vivid and smooth motions. However, current publicly available datasets Jafarian & Park (2021); Zablotskaia et al. (2019a) primarily focus on human animation and fall short in capturing broad range of anthropomorphic characters and corresponding dancing videos. This gap makes these datasets and benchmarks unsuitable for quantitatively evaluating different methods in anthropomorphic character animation. Figure 3: Examples from our A2Bench. Preprint. Under review. Method Moore-AnimateAnyone Corporation (2024) MimicMotion Zhang et al. (2024) (ArXiv24) ControlNeXt Peng et al. (2024) (ArXiv24) MusePose Tong et al. (2024) (ArXiv24) Unianimate Wang et al. (2024b) (ArXiv24) Animate-X Table 1: Quantitative comparisons with SOTAs on A2Bench with the rescaled pose setting. PSNR* means using the modified metric Wang et al. (2024a) to avoid numerical overflow. LPIPS FID FID-VID FVD 1367.84 2250.13 1652.09 1760.46 1156.36 703.87 PSNR* SSIM 0.299 0.318 0.379 0.397 0.398 0.452 L1 1.58E-04 1.51E-04 1.38E-04 1.27E-04 1.24E-04 1.02E-04 50.97 122.92 68.15 100.91 48.47 26. 75.11 129.40 81.05 114.15 61.03 32.23 0.626 0.622 0.572 0.549 0.532 0.430 9.86 10.18 10.88 11.05 11.82 13.60 Method FOMM Siarohin et al. (2019a) (NeurIPS19) MRAA Siarohin et al. (2021a) (CVPR21) LIA Wang et al. (2022) (ICLR22) DreamPose Karras et al. (2023) (ICCV23) MagicAnimate Xu et al. (2023a) (CVPR24) Moore-AnimateAnyone Corporation (2024) (CVPR24) MimicMotion Zhang et al. (2024) (ArXiv24) ControlNeXt Peng et al. (2024) (ArXiv24) MusePose Tong et al. (2024) (ArXiv24) Animate-X PSNR* SSIM L1 10.49 12.62 13.78 7.76 11.90 11.56 12.66 12.82 12.92 14.10 0.363 1.47E-04 0.420 1.09E-04 0.445 9.70E-05 0.305 2.28E-04 0.396 1.17E-04 0.360 1.27E-04 0.407 1.07E-04 0.421 1.02E-04 0.438 9.90E-05 0.463 8.92E-05 LPIPS FID FID-VID FVD 2535.12 3094.68 1813.28 183.18 161.57 105. 147.82 196.87 78.51 0.613 0.556 0.497 0.534 0.523 0.532 0.497 0.472 0.470 0.425 277.64 117.09 37.82 96.46 46.66 80.22 31.58 315.58 117.54 59.80 61.77 59.41 87.97 33.15 4324.42 2021.93 1117.29 1368.83 1152.96 1401.96 849. Table 2: Quantitative comparisons with existing methods on A2Bench in the self-driven setting. Underline means the second best result. To bridge this gap, we propose the Animated Anthropomorphic character Benchmark (A2Bench) to comprehensively evaluate the performance of different methods. Specifically, we first provide prompt template to GPT-4 OpenAI (2024) and leverage it to generate 500 prompts, each of which contains textual description of an anthropomorphic character. Please refer to Appendix B.2 for details. Inspired by the powerful image generation capability of KLing AI Technology (2024), we feed the produced prompts into its Text-To-Image module, which synthesizes the corresponding anthropomorphic character images according to the given text prompts. Subsequently, the ImageTo-Video module is employed to further make the characters in the images dance vividly. For each prompt, we repeat the process for 4 times and filter the most satisfactory image-video pairs as the output corresponding to this prompt. In this manner, we collect 500 anthropomorphic characters and the corresponding dance videos, as shown in Fig. 3. Please refer to Appendix for details."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Dataset. We collect approximately 9,000 human videos from the internet and supplement this with TikTok dataset Jafarian & Park (2021) and Fashion dataset Zablotskaia et al. (2019a) for training. Following previous works Hu et al. (2023); Zablotskaia et al. (2019a); Jafarian & Park (2021), we use 10 and 100 videos for both qualitative and quantitative comparisons from TikTok and Fashion dataset, respectively. We additionally experimented on 100 image-video pairs selected from the newly proposed A2Bench introduced in Sec 3.4. Please note that, to ensure fair comparison, the data in the A2Bench are not included in the training set to train our model. The data are only used to evaluate the quantitative results and provide interesting reference image cases. Figure 4: The illustration of comparison settings. Evaluation Metrics. We assess the results using evaluation metrics in Appendix B.1, including PSNR Hore & Ziou (2010), SSIM Wang et al. (2004), L1, LPIPS Zhang et al. (2018), which are widely-used image metrics for measuring the visual quality of the generated results. In addition, we introduce FID Heusel et al. (2017), FID-VID Balaji et al. (2019) and FVD Unterthiner et al. (2018) to quantify the discrepancy between the generated video distribution and the real video distribution. 4.2 EXPERIMENTAL RESULTS Quantitative Results. Since our Animate-X primarily focuses on animating the anthropomorphic characters, very few of which, if not none, can be extracted the pose skeleton accurately by 7 Preprint. Under review. Figure 5: Qualitative comparisons with state-of-the-art methods. DWPose Yang et al. (2023). It naturally leads to misalignment of the input reference image with the driving pose images. To compute quantitative results in this case, we set up new comparison setting. For each case in A2Bench (i.e., reference image and pose a, as shown in Fig. 4), we randomly select one humans pose image and align the anthropomorphic characters pose retains the movements of but has the same body shape to it, such that the aligned pose pa (fat/thin, tall/short, etc.) as pb. Ultimately, we take the anthropomorphic character Ia and the aligned driving pose image pa as inputs to the model, generating results that allow it to calculate quantitative metrics with the original anthropomorphic character dancing video in A2Bench. In this setting, we compare our method with Animate Anyone Hu et al. (2023), Unianimate Wang et al. (2024b), MimicMotion Zhang et al. (2024), ControlNeXt Peng et al. (2024) and MusePose Tong et al. (2024), which also use pose images (e.g., in Fig. 4) as input. The results of Animate Anyone Hu et al. (2023) are obtained by leveraging the publicly available reproduced code Corporation (2024). Tab. 1 presents the quantitative results, where Animate-X markedly surpasses all comparative methods in terms of all metrics. It is worth noting that, we do not use A2Bench as training data to avoid overfitting and ensure fair comparisons, in line with other comparative methods. Following previous works which evaluate quantitative results in self-driven and reconstruction manner, we additionally compare our method with (a) GAN-based image animate works: FOMM Siarohin et al. (2019a), MRAA Siarohin et al. (2021a), LIA Wang et al. (2022). (b) Diffusion model-based image animate works: DreamPose Karras et al. (2023), MagicAnimate Xu et al. (2023a) and present the results in Tab. 2, which indicates that our method achieves the best performance across all the metrics. Moreover, we provide the quantitative results on the human dataset (TikTok and Fashion) in Tab. 7 and Tab. 8, respectively. Please refer to Appendix D.2 for details. Animate-X reaches the comparable score to Unianimate and exceeds other SOTA methods, which demonstrates the superiority of Animate-X on both anthropomorphic and human benchmarks. Qualitative Results. Qualitative comparisons of anthropomorphic animation are shown in Fig. 5. We observe that GAN-based LIA Wang et al. (2022) does not generalize well, which can only work on specific dataset like Siarohin et al. (2019b). Benefiting from the powerful generative capabilities of the diffusion model, Animate Anyone Hu et al. (2023) renders higher resolution image, but the identity of the image changes and do not generate an accurate reference pose motion. Although MusePose Tong et al. (2024), Unianimate Wang et al. (2024b) and MimicMotion Zhang et al. (2024) improve the accuracy of the motion transfer, these methods generate unseen person, which is not 8 Preprint. Under review. Figure 6: Qualitative comparisons with Unianimate in terms of long video generation. Method Identity preservation Temporal consistency Visual quality Moore-AA MimicMotion ControlNeXt MusePose Unianimate 60.4% 19.8% 27.0% 14.8% 24.9% 17.2% 52.0% 36.9% 40.4% 31.3% 43.9% 40.3% 43.0% 81.1% 79.3% Animate-X 98.5% 93.4% 95.8% Table 3: User study results. the desired result. ControlNeXt combines the advantages of the above two types of methods, so maintains the consistency of identity and motion transfer to some extent, yet the results are somewhat unnatural and unsatisfactory, e.g., the ears of the rabbit and the legs of the banana in Fig. 5. In contrast, Animate-X ensures both identity and consistency with the reference image while generating expressive and exaggerated figure motion, rather than simply adopting quasi-static motion of the target character. Further, we present some long video comparisons in Fig. 6. Unianimate generates woman out of thin air who dances according to the given pose images. Animate-X animates the reference image in cute way while preserving appearance and temporal continuity, and it does not generate parts that do not originally exist. In summary, Animate-X excels in maintaining appearance and producing precise, vivid animations with high temporal consistency. Please refer to Appendix D.1 for details. User Study. To estimate the quality of our method and SOTAs from human perspectives, we conduct blind user study with 10 participants. Specifically, we randomly select 10 characters from A2Bench and collect 10 driving video from the website. For each of 6 methods tested, 10 animation clips are generated, resulting in total of 60 clips. Each participant is presented two results generated by different methods for the same set of inputs and asked to choose which one is better in terms of visual quality, identity preservation, and temporal consistency. This process is repeated 6 2 times. The results are summarized in Tab. 3, where our method noticeably outperforms other methods in all aspects, demonstrating its superiority and effectiveness. Details in Appendix C. 4.3 ABLATION STUDY Ablation on Implicit Pose Indicator. To analyze the contributions of Implicit Pose Indicator, we remove it from Animate-X as w/o IPI and compare it with Baseline and Animate-X. From the first row of Fig. 7, we observe that Baseline generates person whose appearance is appreciably distinct from the reference image. With the help of EPI, this problem is mildly mitigated. However, due to the absence of IPI, compared to Ours, there are still strange things and human-like hands 9 Preprint. Under review. Figure 7: Visualization of ablation study on IPI and EPI. appearing, as indicated by the blue circle. For more detailed analysis about the structure of IPI, we set up several variants: (1) remove IPI: w/o IPI. (2) remove learnable query: w/o LQ. (3) remove DWPose query: w/o DQ. The quantitative results are shown in Tab. 4. It can be seen that removing the entire IPI presents the worst performance. By modifying the IPI module, although it improves on the w/o IPI, it still falls short of the final result of Animate-X, which suggests that our current IPI structure is the most reasonable and achieves the best performance. L1 PSNR* SSIM 64.31 42.74 62.34 13.30 13.48 13. Method w/o IPI w/o LQ w/o DQ 0.433 1.35E-04 0.454 32.56 0.445 1.76E-04 0.454 28.24 0.445 1.01E-04 0.456 30.33 LPIPS FID FID-VID FVD 893.31 754.37 913.33 Ablation on Explicit Pose Indicator. We demonstrate the visual results of ablating EPI setting in the second row of Fig. 7 by removing EPI. Without EPI, although the appearance of the panda is preserved thanks to IPI, the model incorrectly treats the pandas ears as arms and forcibly stretches the legs to match the length of the legs in the pose image indicated by red circles. In contrast, these issues are completely resolved by the assistance of EPI. We further conduct more detailed ablation experiments for different pairs of pose transformations by (1) removing the entire EPI: w/o EPI. (3) remove Pose Realignment: w/o Realignment. (2) removing Pose Rescale: w/o Rescale; From the results displayed in Tab. 4, we found that Pose Realignment contributes the most. It suggests that simulating misalignment case in inference is the the key factor. w/o EPI 12.63 w/o Realign 12.27 w/o Rescale 13.23 Animate-X 13.60 0.403 1.80E-04 0.509 42.17 0.433 1.17E-04 0.434 34.60 0.438 1.21E-04 0.464 27. Table 4: Quantitative results of ablation study. 0.452 1.02E-04 0.430 26.11 948.25 860.25 721.11 58.17 49.33 35.95 703.87 32. In summary, we can draw conclusions: (1) IPI facilitates the preservation of appearance and prevents the generation of content that does not exist in the reference image like human arms. (2) EPI prevents the forced alignment of pose image that is not naturally aligned with the reference image during animation, thus avoiding the unintended animation of parts that should remain static like the pandas ears shown in Fig. 7. Please refer to Appendix D.4 for details."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this study, we present Animate-X, novel approach to character animation capable of generalizing across different types of characters named X. To address the imbalance between identity preservation and movement consistency caused by the insufficient motion representation, we introduce the Pose Indicator, which leverages both implicit and explicit features to enhance the motion understanding of the model. In this way, Animate-X demonstrates strong generalization and robustness, achieving general character animation. The proposed framework showcases significant improvements over state-of-the-art methods in terms of identity preservation and motion consistency, as evidenced by experiments on both public datasets and the newly introduced A2Bench, which features anthropomorphic characters. Limitation and ethical considerations see Appendix E. 10 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latentshift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional GAN with discriminative filter generation for text-to-video synthesis. In IJCAI, volume 1, pp. 2, 2019. Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, and Fahad Shahbaz Khan. Person image synthesis via denoising diffusion model. In CVPR, pp. 59685976, 2023. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pp. 2256322575, 2023. Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour Hadid. Face anti-spoofing based on color texture analysis. In 2015 IEEE international conference on image processing (ICIP), pp. 26362640. IEEE, 2015. Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In ICCV, pp. 2320623217, 2023. Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In ICCV, pp. 2304023050, 2023. Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. arXiv preprint arXiv:2311.12052, 2023a. Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023b. Moore Threads Corporation. Moore-AnimateAnyone. 2024. URL https://github.com/ MooreThreads/Moore-AnimateAnyone. Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, and Yu Liu. Check locate rectify: training-free layout calibration system for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66246634, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Preprint. Under review. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33: 68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pp. 23662369. IEEE, 2010. Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. ICML, 2023. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 46514664. PMLR, 2021. Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In CVPR, pp. 1275312762, 2021. Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human: Text-driven controllable human image generation. ACM Transactions on Graphics, 41(4):111, 2022. Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion video synthesis with stable diffusion. In ICCV, pp. 2268022690, 2023. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Yining Li, Chen Huang, and Chen Change Loy. Dense intrinsic appearance flow for human In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern pose transfer. Recognition, pp. 36933702, 2019. Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, and Lei Zhang. Survey on leveraging pre-trained generative adversarial networks for image editing and restoration. Science China Information Sciences, 66(5):151101, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, pp. 1678416804, 2022. OpenAI. Chatgpt-4o. 2024. URL https://chat.openai.com/chat. Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. arXiv preprint arXiv:2312.04483, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. 12 Preprint. Under review. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Yurui Ren, Xiaoqing Fan, Ge Li, Shan Liu, and Thomas Li. Neural texture extraction and distribution for controllable person image synthesis. In CVPR, pp. 1353513544, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, and Yang Wei. Advancing pose-guided image synthesis with progressive conditional diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=rHzapPnCgT. Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. NeurIPS, 32, 2019a. Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019b. Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In CVPR, pp. 1365313662, 2021a. Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1365313662, 2021b. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. ICLR, 2023. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. Shuai Tan, Bin Ji, and Ye Pan. Emmn: Emotional motion memory network for audio-driven emoIn Proceedings of the IEEE/CVF International Conference on tional talking face generation. Computer Vision, pp. 2214622156, 2023. Shuai Tan, Bin Ji, Mengxiao Bi, and Ye Pan. Edtalk: Efficient disentanglement for emotional talking head synthesis. arXiv preprint arXiv:2404.01647, 2024a. Shuai Tan, Bin Ji, Yu Ding, and Ye Pan. Say anything with any style. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 50885096, 2024b. Shuai Tan, Bin Ji, and Ye Pan. Flowvqtalker: High-quality emotional talking face generation through normalizing flow and quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2631726327, 2024c. Shuai Tan, Bin Ji, and Ye Pan. Style2talker: High-resolution talking head generation with emotion style and art style. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 50795087, 2024d. Kuaishou Technology. Kling ai. 2024. URL https://klingai.kuaishou.com. Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu, and Wenjiang Zhou. Musepose: pose-driven image-to-video framework for virtual human generation. arxiv, 2024. Preprint. Under review. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. arXiv e-prints, pp. arXiv2307, 2023b. Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. In ICLR, 2024a. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 2023c. Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024b. Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation with text-free videos. In CVPR, 2024c. Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022. Zezheng Wang, Zitong Yu, Chenxu Zhao, Xiangyu Zhu, Yunxiao Qin, Qiusheng Zhou, Feng Zhou, and Zhen Lei. Deep spatial gradient and temporal depth learning for face anti-spoofing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5042 5051, 2020. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600 612, 2004. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pp. 76237633, 2023. Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video generation. arXiv preprint arXiv:2308.09710, 2023. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. arXiv preprint arXiv:2311.16498, 2023a. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In arXiv, 2023b. Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, and Dahua Lin. Pose guided human video generation. In ECCV, pp. 201216, 2018. Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In ICCV, pp. 42104220, 2023. Wing-Yin Yu, Lai-Man Po, Ray CC Cheung, Yuzhi Zhao, Yu Xue, and Kun Li. Bidirectionally deformable motion modulation for video-based human pose transfer. In ICCV, pp. 75027512, 2023. 14 Preprint. Under review. Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, and Guoying Zhao. Searching central difference convolutional networks for face anti-spoofing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5295 5305, 2020. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: Instructing video diffusion models with human feedback. arXiv preprint arXiv:2312.12490, 2023. Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019a. Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pp. 38363847, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023b. Pengze Zhang, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Exploring dual-task correlation for pose guided person image generation. In CVPR, pp. 77137722, 2022a. Pengze Zhang, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Exploring dual-task correlation for pose guided person image generation. In CVPR, pp. 77137722, 2022b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pp. 586595, 2018. Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In CVPR, pp. 36573666, 2022a. Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36573666, 2022b. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision (ECCV), 2024. 15 Preprint. Under review."
        },
        {
            "title": "A NETWORK DETAILS",
            "content": "Due to space constraints in the main paper, we only present brief overview of the EPI process. Here, in Fig. 8, we provide more detailed explanation of the pose transformation in EPI, along with additional case examples. First, we sample driving pose and then randomly select an anchor pose anchor from the pose pool (two examples are shown in Fig. 8). The driving pose is aligned to the anchor pose anchor, resulting in the aligned pose realign. Next, we apply several rescaling operations randomly chosen from the rescale pool to further modify the aligned pose realign. By combining different rescaling options, we can obtain multiple transformed poses n. However, it is important to note that in each training step, only one anchor pose p anchor and one rescaling combination are selected, so only one transformed pose is used for training. As shown retains the same motion as the sampled pose but has body in the Fig. 8, the transformed pose shape similar to the anchor pose anchor. This simulates scenarios during inference where there are body shape differences between the reference image and the driving pose, enabling the model to generalize to such cases. In the experiments, we use the visual encoder of the multi-modal CLIP-Huge model Radford et al. (2021) in Stable Diffusion v2.1 Rombach et al. (2022) to encode the CLIP embedding of the reference image and driving videos. The pose encoder, composed of several convolutional layers, follows similar structure to the STC-encoder in VideoComposer Wang et al. (2023c). For model initialization, we employ pre-trained video generation model Wang et al. (2024c), as done in previous approaches Xu et al. (2023a); Hu et al. (2023); Zhu et al. (2024); Wang et al. (2024b). The experiments are carried out using 8 NVIDIA A100 GPUs. During training, videos are resized to spatial resolution of 768512 pixels, and we feed the model with uniformly sampled video segments of 32 frames to ensure temporal consistency. We use the AdamW optimizer Loshchilov & Hutter (2017) with learning rates of 5e-7 for the implicit pose indicator and 5e-5 for other modules. For noise sampling, DDPM Ho et al. (2020) with 1000 steps is applied during training. In the inference phase, we adjust the length of the driving pose to align roughly with the reference pose and used the DDIM sampler Song et al. (2021) with 50 steps for faster sampling. Figure 8: More example for EPI. 16 Preprint. Under review. Figure 9: The difference of training and inference pipeline. During training, the reference image and the driven video come from the same video, while in the inference pipeline, the reference image and the driven video can be from any sources and appreciably different."
        },
        {
            "title": "B BENCHMARK DETAILS",
            "content": "B.1 EVALUATION METRIC We employ several evaluation metrics to quantitatively assess our results, including PSNR, SSIM, L1, LPIPS, FID, FID-VID and FVD. The detailed metrics are introduced as follows: PSNR is measure used to evaluate the quality of reconstructed images compared to the original ones. It is expressed in decibels (dB) and higher values indicate better quality. PSNR is commonly used in image compression and restoration fields. SSIM assesses the similarity between two images based on their luminance, contrast, and structural information. It considers perceptual phenomena affecting human vision and thus provides better correlation with perceived image quality than PSNR. The L1 metric refers to the mean absolute difference between the corresponding pixel values of two images. It quantifies the average magnitude of errors in predictions without considering their direction, making it useful for measuring the extent of differences. LPIPS is perceptual distance metric based on deep learning. It evaluates the similarity between images by analyzing the feature representations of image patches and tends to align well with human visual perception, making it suitable for tasks like image generation. FID is used to assess the quality of images generated by generative models (like GANs) by comparing the distribution of generated images to that of real images in feature space (extracted by pretrained CNN). Lower FID values suggest that the generated images are more similar to real images. FID-VID extends the FID metric to video data. It measures the quality of generated videos by comparing the distribution of generated video features to real video features, providing insights into the temporal aspects of video generation. FVD is another metric for evaluating video generation, similar to FID. It measures the distance between the feature distributions of real and generated videos, taking both spatial and temporal dimensions into account. Lower FVD indicates that generated videos are closer to real ones regarding visual quality and dynamics. 17 Preprint. Under review. Figure 10: Detailed pipeline for building A2Bench based on large-scale pretrained models, including Open-ChatGPT 4o and KLing AI. B.2 DATA DETAILS The detailed process for constructing A2Bench is outlined in Fig. 10. We initially provide GPT4o with template that clearly specifies the demand to generate anthropomorphized images. The images were required to be cute, with arms and legs, standing, dancing, and of high quality. To allow for variety of image outputs, we left the fields for object, season, province, and specific 18 Preprint. Under review. location empty. For the key factor influencing diversity and relevance, i.e., object, we provide selectable range, such as everyday items, furniture, fruits, and natural creatures. To help GPT-4o better understand our intent, we additionally provide two examples, where the prompts had already been proven to generate satisfactory images by text-to-image module of KLing AI. Thanks to the text understanding and generation capabilities of GPT-4o, we collect 500 prompts for image generation. We then fed these 500 prompts into the text-to-image module of Keling AI, obtaining corresponding anthropomorphic characters images. Based on these images, we further generate videos of them dancing using the image-to-video module of Keling AI. In this way, we collect 500 pairs of images and videos of anthropomorphic characters, forming our A2Bench. Since most current animation methods Wang et al. (2024b); Hu et al. (2023); Zhang et al. (2024) take pose image sequence as motion source, we also provide our A2Bench with additional pose images. To achieve this, we employ DWPose Yang et al. (2023) to extract pose sequences from the videos. However, since DWPose is trained on human data, it does not accurately extract every pose in the dancing video of the anthropomorphic character, so after extraction, we manually screen 100 videos with accurate poses, and view them as test videos for calculating quantitative metrics. Fig. 3 displays several examples, which include anthropomorphic characters of plants, animals, food, furniture, etc. For images and videos where pose extraction is not feasible, we take them as key sources of reference images in our qualitative demonstrations. This will inspire the community to animate wider range of interesting cases. We also anticipate that these data could serve as an important resource for future pose extraction algorithms tailored to anthropomorphic datasets, making them accessible for broader use."
        },
        {
            "title": "C USER STUDY",
            "content": "In Fig. 11, we present examples shown to participants for evaluation in our user study. To obtain genuine feedback reflective of practical applications, the ten participants in our user study experiment come from diverse academic backgrounds. Since many of them do not major in computer vision, we provide detailed explanations for each question to assist their judgments. Identity Preservation: By comparing the reference image with the two generated videos by different methods, determine which videos character more closely resembles the character in the image. Temporal Consistency: Evaluate the motion changes of the character within the video and compare which video exhibits more coherent movement. Visual Quality: Compared to the previous two questions, this one involves more subjective judgment. Participants should assess the videos comprehensively based on visual content (e.g., flashes, distortions, afterimages), motion effects (e.g., smoothness, physical logic), and overall plausibility."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "D.1 MORE QUALITATIVE RESULTS In the main paper, we present qualitative comparison results between our method and the state-ofthe-art (SOTA) methods under cross-driven setting on human-like character, where our approach demonstrates outstanding performance. Considering that the other methods are primarily self-driven and trained on human characters, making them more suitable for inference in such settings, we additionally provide comparison results under self-reconstruction setting on Tiktok and Abench. As shown in Fig. 14, when there is appreciably difference between the reference pose and the reference image, the GAN-based LIA Wang et al. (2022) produces noticeable artifacts. Thanks to the powerful generative capabilities of diffusion models, diffusion-based models generate higher-quality results. However, MusePose Tong et al. (2024) and MimicMotion Zhang et al. (2024) generate awkward arms and blurry hands, respectively, while ControlNeXt Peng et al. (2024) synthesizes incorrect movements. Only Unianimate Wang et al. (2024b) can obtain results comparable to ours. Yet, when the reference image is non-human character, even in self-driven setting with the same training strategy as Unianimate, their results still show distorted heads. Fig. 15 provides results of 19 Preprint. Under review. Figure 11: Visualization of cases in the user study more comparison results, including MRAA Siarohin et al. (2021a), MagicAnimate Xu et al. (2023a) and Moore-AnimateAnyone Corporation (2024). In contrast, our method consistently generates satisfactory results for both human and anthropomorphic characters, demonstrating its ability to drive character and highlighting its strong generalization and robustness. D.2 MORE QUANTITATIVE RESULTS Tab. 7 and Tab. 8 presents the quantitative results on TikTok Jafarian & Park (2021) and Fashion Zablotskaia et al. (2019a) dataset, which suggests the superiority of methods over the comparison SOTA methods. Only Unianimate achieves comparable performance; however, our method is applicable to wider range of characters and various unaligned pose inputs, as demonstrated in 20 Preprint. Under review. Figure 12: Visualization of the robustness of Animate-X. Tab. 1. This addresses the main issue that this paper aims to solve: developing universal character image animation model. D.3 ROBUSTNESS Our method demonstrates robustness to both input character and pose variations. On the one hand, as shown in Fig. 1, our approach successfully handles inputs from diverse subjects, including characters vastly different from humans, such as those without limbs, as well as game characters or those generated by other models. Despite these variations, our method consistently produces satisfactory results without crashing, showcasing its robustness to the input reference images. On 21 Preprint. Under review. Method w/o IPI w/o LQ w/o DQ PA KV w/o EPI w/o Add w/o Drop w/o BS w/o NF w/o AL w/o Rescalings w/o Realign Animate-X PSNR* 13.30 13.48 13.39 13.25 13.34 SSIM 0.433 0.445 0.445 0.436 0.443 12.63 13.28 13.36 13.27 13.41 13.04 13.23 12.27 13.60 0.403 0.442 0.441 0.443 0.446 0.429 0.438 0. 0.452 L1 1.35E-04 1.76E-04 1.01E-04 1.11E-04 1.17E-04 1.80E-04 1.56E-04 1.94E-04 1.08E-04 1.82E-04 1.04E-04 1.21E-04 1.17E-04 1.02E-04 LPIPS 0.454 0.454 0.456 0.464 0.459 0.509 0.459 0.458 0.461 0.455 0.474 0.464 0. 0.430 FID 32.56 28.24 30.33 27.63 26.75 42.17 34.24 26.65 29.60 29.21 27.17 27.64 34.60 26.11 FID-VID 64.31 42.74 62.34 46.54 42.14 58.17 52.94 44.55 56.56 56.48 33.97 35.95 49. 32.23 FVD 893.31 754.37 913.33 785.36 785.69 948.25 804.37 764.52 850.17 878.11 765.69 721.11 860.25 703.87 Table 5: Quantitative results of ablation study. the other hand, as illustrated in Fig. 12, even when the pose images exhibit body part omissions (highlighted by the red circles), our method correctly interprets the intended motion and generates coherent results for the reference images. This highlights the robustness of our approach to different pose images. D.4 MORE ABLATION STUDY In the main paper, we present the results of the primary ablation experiments for IPI and EPI. In this section, we supplement those results with additional ablation experiments to further demonstrate the contribution of each individual module. Ablation on Implicit Pose Indicator. For more detailed analysis about the structure of IPI, we set up several variants: (1) remove IPI: w/o IPI. (2) remove learnable query: w/o LQ. (3) remove DWPose query: w/o DQ. (4) set IPI and spatial Attention to Parallel: PA. (5) set CLIP features as and DWPose as K,V in IPI: KV Q. The quantitative results are shown in Tab. 5. It can be seen that removing the entire IPI presents the worst performance. By modifying the IPI module, although it improves on the w/o IPI, it still falls short of the final result of Animate-X, which suggests that our current IPI structure is the most reasonable and achieves the best performance. Since IPI is embedded in Animate-X in the form of residual connection, i.e., = + αIP I(x), we also explore the impact of the weight α of IPI on performance as illustrated in Fig. 13, as α increases from 0 to 1, all metrics show stable improvement despite some fluctuations. The best performance is achieved when α is set to 1, so we empirically set α to 1 in the final configuration. Ablation on Explicit Pose Indicator. We conduct more detailed ablation experiments for different pairs of pose transformations by (1) removing the entire EPI: w/o EPI; (2)&(3) removing adding and dropping parts; canceling the change of the length of (4) body and should: w/o BS; (5) neck and face: w/o NF; (6) arm and leg: w/o AL; (7) removing all rescaling process: w/o Rescalings; (8) remove another person pose alignment: w/o Realign. From the results displayed in Tab. 5, we found that each pose transformation contributes compared to w/o EPI, with aligned transformations with another persons pose contributing the most. It suggests that maintaining the overall integrity of the pose while allowing for some variations is the most important factor, and EPI also learns the overall integrity of the pose. The final result indicates that all the transformations together achieve the best performance. To explore the effect of different probabilities λ of using pose transformation for EPI on the model performance, we set λ as 100%, 98%, 95%, 90% and 80% for the ablation experiments on two datasets. The results presented in Tab. 6 suggest that high λ performs better on A2Bench, i.e., it performs better when the reference image and pose image are not aligned, but harms performance on the TikTok dataset, i.e., when the reference image and pose image are strictly aligned. In contrast, 22 Preprint. Under review. Figure 13: Ablation study on the weight α of Implicit Pose Indicator. To better visualize the impact of α on performance, we normalize all the values to the range of 0 to 1. A2Bench TikTok Jafarian & Park (2021) Method 100% 98% 95% 90% 80% SSIM FID FID-VID FVD SSIM FID FID-VID FVD 0.452 0.448 0.447 0.444 0.442 26.11 26.93 27.46 27.15 29.13 32.23 37.67 39.21 38.03 47. 703.87 775.24 785.55 775.38 803.97 0.802 0.797 0.804 0.806 0.802 55.26 55.81 52.72 52.81 54.51 17.47 16.28 14.61 14.82 14.42 138.36 129.48 124.92 139.01 133.78 Table 6: Quantitative results for different probabilities of using pose transformation. Method FOMM Siarohin et al. (2019a) (NeurIPS19) MRAA Siarohin et al. (2021a) (CVPR21) TPS Zhao & Zhang (2022a) (CVPR22) DreamPose Karras et al. (2023) (ICCV23) DisCo Wang et al. (2024a) (CVPR24) MagicAnimate Xu et al. (2023a) (CVPR24) Animate Anyone Hu et al. (2023) (CVPR24) Champ Zhu et al. (2024) (ECCV24) Unianimate Wang et al. (2024b) (ArXiv24) MusePose Tong et al. (2024) (ArXiv24) MimicMotion Zhang et al. (2024) (ArXiv24) ControlNeXt Peng et al. (2024) (ArXiv24) Animate-X L1 3.61E-04 3.21E-04 3.23E-04 6.88E-04 3.78E-04 3.13E-04 - 2.94E-04 2.66E-04 3.86E-04 5.85E-04 6.20E-04 2.70E-04 PSNR - - - PSNR* 17.26 18.14 18. SSIM 0.648 0.672 0.673 LPIPS 0.335 0.296 0.299 28.11 29.03 29.16 29.56 29.91 30.77 - - - 30.78 12.82 16.55 - - - 20.58 17.67 14.44 13.83 20.77 0.511 0.668 0.714 0.718 0.802 0.811 0.744 0.601 0.615 0.806 0.442 0.292 0.239 0.285 0.234 0.231 0.297 0.414 0.416 0. FVD 405.22 284.82 306.17 551.02 292.80 179.07 171.90 160.82 148.06 215.72 232.95 326.57 139.01 Table 7: Quantitative comparisons with existing methods on TikTok dataset. relatively low λ, e.g., 90%, would be in this case perform better. It is reasonable that in the case of strict alignment, we expect the pose to provide strictly accurate motion source, and thus need to reduce the percentage λ of pose transformation. However, in the non-strictly aligned case, we expect the pose image to provide an approximate motion trend, so we need to increase λ. 23 Preprint. Under review. Method MRAA Siarohin et al. (2021a) (CVPR21) TPS Zhao & Zhang (2022a) (CVPR22) DPTN Zhang et al. (2022a) (CVPR22) NTED Ren et al. (2022) (CVPR22) PIDM Bhunia et al. (2023) (CVPR23) DBMM Yu et al. (2023) (ICCV23) DreamPose Karras et al. (2023) (ICCV23) DreamPose w/o Finetune Karras et al. (2023) (ICCV23) Animate Anyone Hu et al. (2023) (CVPR24) Unianimate Wang et al. (2024b) (ArXiv24) MimicMotion Zhang et al. (2024) (ArXiv24) Animate-X PSNR - - - - - - - 34.75 38.49 37.92 - 36.73 PSNR* - - 24.00 22.03 - 24.07 - - - 27.56 27.06 27. SSIM 0.749 0.746 0.907 0.890 0.713 0.918 0.885 0.879 0.931 0.940 0.928 0.940 LPIPS 0.212 0.213 0.060 0.073 0.288 0.048 0.068 0.111 0.044 0.031 0.036 0.030 FVD 253.6 247.5 215.1 278.9 1197.4 168.3 238.7 279.6 81.6 68.1 118.48 79. Table 8: Quantitative comparisons with existing methods on the Fashion dataset. w/o Finetune represents the method without additional finetuning on the fashion dataset. Figure 14: Visualization comparison on TikTok dataset and A2Bench."
        },
        {
            "title": "E DISCUSSION",
            "content": "E.1 LIMITATION AND FUTURE WORK Although our method has made remarkable progress, it still has certain limitations. Firstly, its ability to model hands and faces remains insufficient, limitation commonly faced by most current generative models. While our IPI leverages CLIP features to extract implicit information such as motion patterns from the driving video, mitigating the reliance on potentially inaccurate hand and face detection by DWPose, there is still gap between our results and the desired realism. Secondly, due to the multiple denoising steps in the diffusion process, even though we replace the transformer with more efficient Mamba model for temporal modeling, Animate-X still cannot achieve real24 Preprint. Under review. Figure 15: Comparison with more SOTAs on A2Bench. time animation. In future work, we aim to address these two limitations. Additionally, we will focus on studying interactions between the character and the surrounding environment, such as the background, as key task to resolve. E.2 ETHICAL CONSIDERATIONS Our approach focuses on generating high-quality character animation videos, which can be applied in diverse fields such as gaming, virtual reality, and cinematic production. By providing body movement, our method enables animators to create more lifelike and dynamic characters. However, the potential misuse of this technology, particularly in creating misleading or harmful content on digital platforms, is concern. While greatly progress has been made in detecting manipulated animations Boulkenafet et al. (2015); Wang et al. (2020); Yu et al. (2020), challenges remain in accurately identifying increasingly sophisticated forgeries. We believe that our animation results can contribute to the development of better detection techniques, ensuring the responsible use of animation technology across different domains."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Ant Group"
    ]
}