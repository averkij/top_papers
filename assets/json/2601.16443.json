{
    "paper_title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
    "authors": [
        "Kanishk Gandhi",
        "Shivam Garg",
        "Noah D. Goodman",
        "Dimitris Papailiopoulos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 1 3 4 4 6 1 . 1 0 6 2 : r Preprint. Endless Terminals: Scaling RL Environments for Terminal Agents Kanishk Gandhi Stanford University Shivam Garg Microsoft Research Noah D. Goodman Stanford University Dimitris Papailiopoulos Microsoft Research UW-Madison Abstract Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires scalable pipeline, not just dataset. We introduce Endless Terminals, fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinkersft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale. 1 Figure 1: Endless Terminals. Tasks are procedurally generated through four phases: (I) task description generation, (II) container setup with iterative validation, (III) completion test generation, and (IV) solution-based filtering using o3. The pipeline yields 3255 verified tasks for training terminal agents with PPO. Part of the work was done during summer internship at Microsoft Research 1Code available at https://github.com/kanishkg/endless-terminals 1 Preprint."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning is hungry for environments. Successes of RL in improving language model reasoning, from mathematical problem solving to code generation have relied on access to large set of diverse, and automatically verifiable tasks. Yet for training LLM agents to perform multi-turn computer tasks in terminal, no such scalable environment exists. Real world terminal use requires agents to reason across multiple interactions, recover from errors, and interactively execute sequences of commands that transform the stat of system. Manual curation of such environments is expensive, and existing benchmarks offer at most hundreds of tasks, far too few to support robust RL training (Team, 2025b; Prime Intellect, 2025; Team, 2025a). Prior approaches to building capable terminal agents have largely sidestepped this environment bottleneck. Some rely on fixed evaluation benchmarks repurposed for training, risking overfitting to narrow task distributions. Others distill behavior from stronger proprietary models through supervised finetuning (Guha et al., 2025), inheriting the ceiling of the teacher and requiring expensive API access. third line of work (Team, 2025a; Lin et al., 2018) uses human curated datasets of coding or shell tasks, but the annotation cost limits scale and diversity. What remains missing is fully autonomous pipeline that can generate an endless stream of terminal tasks: complete with initial environments, task specifications, and verification tests, with minimal human supervision. We introduce Endless Terminals, procedural generation pipeline that synthesizes terminaluse tasks without human annotation or distillation. The pipeline operates in four stages (see Fig. 2): 1) generating diverse task descriptions by sampling across categories, complexity levels, and scenario contexts; 2) building containerized environments and validating them with automatically generated prerequisite tests; 3) generating completion tests that verify the expected end state; and 4) filtering tasks by sampling solutions from capable model to ensure solvability. We train models with minimal agent architecture, simple interaction loop where the model reasons, executes commands, and observes the output every turn, with no retrieval, tool use, or multi-agent scaffolding. Vanilla PPO (Schulman et al., 2017) with 16 turns on Endless Terminals yields substantial gains: on our held-out dev set, Llama-3.2-3B (Grattafiori et al., 2024) improves from 4.0% to 18.2%, Qwen-2.5-7B (Qwen et al., 2024) from 10.7% to 53.3%, and Qwen-3-8B-openthinker-sft (Yang et al., 2025; Team, 2025a) from 42.6% to 59.0%. These gains transfer to difficult, human curated benchmarks. On TerminalBench 2.0 (Team, 2025b), Llama-3.2-3B improves from 0.0% to 2.2%, Qwen-2.5-7B from 2.2% to 3.4%, and Qwen-3-8B-Open-Thoughts from 1.1% to 6.7%, in each case outperforming other finetuned versions of the corresponding models on TerminalBench. These results demonstrate that an autonomous pipeline to scale environments can lead to improvements on human curated benchmarks and that simple RL setups succeed if we can scale them up."
        },
        {
            "title": "2 Related Work",
            "content": "Agentic Scaffolds. Prior work develops scaffolds for LLM agents with custom tools, retrieval, context management, and multi-agent coordination. SWE-agent (Yang et al., 2024) provides specialized commands for code navigation and editing. OpenHands (Wang et al., 2024) combines an agent with tools for bash execution, file editing, and browser interaction. Terminus (Team, 2025b) takes simpler approach, giving the agent only an interactive tmux session controlled via keystrokes. Our scaffold is simpler still: the agent reasons and emits commands, with the full history of prior thoughts, actions and shell outputs in context. Supervised Finetuning and Distillation One approach to building capable agents is to curate high quality datasets for supervised finetuning. Guha et al. (2025) (OpenThoughts) distill reasoning traces from frontier models, showing that data quality drives downstream improvement with RL. Gandhi et al. (2025) identify cognitive behaviors: verification, backtracking, subgoal setting that enable self-improvement via RL, and show these can be 2 Preprint. Figure 2: Overview of the Endless Terminals procedural generation pipeline. Phase generates diverse task descriptions by sampling across categories, complexity levels, and scenario contexts, producing both task description and privileged ground truth data for verification. Phase II creates containerized environments and validates them with self written prerequisite tests. Phase III generates completion tests that verify the expected end state. Phase IV filters tasks by sampling 16 solutions from o3, retaining only tasks where at least one solution succeeds. Some example tasks include shell configuration, log analysis, and checksum generation. instilled. Olmo et al. (2025) demonstrate how midtraining on targeted data mixes to elicit capabilities like math and coding aids improvement with RL. Finally, OpenThinker-Agent (Team, 2025a) applies this recipe to terminal tasks, distilling traces from strong teachers. These approaches are complementary: SFT can provide warm start for RL training. Benchmarks and Interactive Environments. Progress on coding agents has been driven by human curated benchmarks with execution based evaluation, including SWEBench (Jimenez et al., 2023) for GitHub issue resolution and TerminalBench 2.0 (Team, 2025b) for terminal tasks. These benchmarks require multiturn interaction where agents issue commands and observe outputs before deciding the next action, format shared by InterCode (Yang et al., 2023) for interactive coding, and WebArena (Zhou et al., 2023) for web navigation. Our procedurally generated tasks follow this multiturn format, with agents interacting with persistent shell. Synthetic Environment Generation Some recent works construct environments for training agents on verifiable tasks. SWEGym (Pan et al., 2024) provides 2438 Python tasks with executable tests, but relies on existing GitHub issues rather than procedural generation. In single turn domains, Poesia et al. (2024) propose models that posit and solve increasingly difficult problems training them through self-play. OpenThoughts Agent (Team, 2025a) is closest to our work, introducing terminal-use tasks for SFT and RL. However, their RL dataset involves human generated queries and commands from NL2Bash (Lin et al., 2018), and doesnt yield gains on benchmarks like Terminal Bench 2.0, or an autonomously generated endless terminals dev set. Endless Terminals generates tasks fully autonomously at 3 Preprint. arbitrary scale, and vanilla PPO yields substantial improvements that transfer to held out benchmarks."
        },
        {
            "title": "3 Procedural Generation of Tasks",
            "content": "Our procedural generation pipeline  (Fig. 2)  for EndlessTerminals consists of four stages: 1) generating task descriptions, 2) setting up an environment while validating it with selfwritten tests, 3) generating tests to verify completion of task, and 4) generating several solutions from strong model to ensure the validity of task. Each stage builds on the previous, with automatic verification ensuring validity at every step. Generating Task Descriptions We prompt (Fig. 2, Phase I) language model to generate task descriptions paired with privileged ground truth information. To ensure diversity, each prompt samples randomly from three dimensions: task categories (like file management, text processing, log analysis, git operations, database queries, security scanning, among others), complexity levels (from single commands to multi-step sequences), and scenario contexts (like developer organizing files, DevOps engineer debugging logs, data analyst processing CSVs). The model outputs task instruction, written as request someone might pose to an AI assistant, along with separate privileged information section containing exact file contents, paths, and expected states that automated tests will use for verification. The privileged information section is never revealed to the agent interacting with the environment. Setting up and validating containers Given the task description and ground truth, we generate two files (Fig. 2, Phase II): an initial state test file that validates the container before the agent begins, and an Apptainer container definition or Dockerfile that establishes the required environment. The initial state tests verify prerequisites for the task: the presence of specific files, directories, running processes, or cloned repositories. For container generation, we employ an iterative refinement loop: the model generates container definition, we build it and run the initial tests inside, and if tests fail, we feed the failure output back to the model for correction. This continues for up to = 3 rounds or until tests pass. Tasks that cannot produce valid container are discarded. Final Test Generation. We generate second test file that validates the system state after successful task completion. Given the task description, ground truth, and initial state tests, the model produces tests verifying expected outcomes, for example checking for the created files with correct contents, modified configurations, computed results (Fig. 2, Phase III). These tests use paths, instructions and explicit data derived from the privileged ground truth. We verify that these tests do not pass in the initial state, ensuring they meaningfully assess task completion rather than trivially succeeding. Solution Based Filtering. To ensure tasks are solvable, we sample = 16 solution attempts from capable model (o3 (OpenAI, 2025)) using our agent framework (Fig. 2, Phase IV). Each attempt consists of an interactive session where the agent issues commands, observes outputs, and continues until declaring completion or exhausting its action budget. We retain tasks where at least one solution succeeds (pass@16 > 0) and discard the rest (see Fig. 5, right). This filtering removes under specified or impossible tasks while confirming that retained tasks are achievable by strong models. The complete pipeline processes tasks in parallel, failed tasks are discarded automatically. This approach gives us controlled, automated and procedural way to generate tasks in target domain while remaining automatically verifiable."
        },
        {
            "title": "Interacting with the Terminal",
            "content": "Interaction Loop. At each turn, the model receives the conversation history, including its own previous reasoning and the shell outputs from prior commands, and pro4 Preprint. duces either command to execute or signal that the task is complete. We use minimal XML tags to structure outputs: <command>...</command> wraps shell commands, and <command>done</command> indicates task completion. The model can include arbitrary reasoning before its command, which becomes part of the conversation history visible in subsequent turns. This means that the model can reference prior reasoning, correct mistakes, or build on partial progress. Shell Environment. For the shell, we support two types of containers: Docker and Apptainer. For Docker, we use the harbor framework. For Apptainer, we maintain persistent interactive shell session using pseudo-terminal (PTY). The agent connects to an Apptainer container instance that remains alive across all turns of an episode, preserving filesystem state, environment variables, and running processes between commands. Each command executes in this persistent context. We capture both stdout and stderr, along with the exit code, and return structured observation: whether the command succeeded or failed, followed by the output. This feedback is appended to the conversation as the next user message, and the loop continues. Minimal Scaffolding. The system prompt contains only basic instructions: output one command per turn, use non-interactive flags, verify solutions before declaring completion. So, tools like vim, htop etc. cannot be used by the agent. Episode Termination. An episode ends when the agent emits the done action, reaches maximum number of turns (16 while training) or tokens (16k while training). We execute the held-out final tests inside the container to determine success. So, we get minimal loop between the model and the environment. The model reasons, acts, and observes, the conversation history accumulates, and the model reasons again with full context of what it has tried."
        },
        {
            "title": "5 Experiments",
            "content": "Task Generation. Our pipeline produces 3255 tasks in Apptainer format, of which approximately 2500 are also converted to Harbor (Shaw, 2025) format. We use the Apptainer pipeline for all experiments. Solvability filtering discards roughly half of all generated candidates, those where o3 (OpenAI, 2025) fails all 16 attempts, removing underspecified or invalid tasks from the training distribution. Endless Terminals produces diverse distribution of solvable tasks. Fig. 4 shows the composition of our generated dataset. Task categories span file operations (the largest category), log management, data processing, text processing, scripting, archiving and compression and database operations. Solution lengths (Fig. 5, left) vary considerably, with most tasks requiring between 1,000 and 4,000 characters of interaction, though the distribution has long tail extending beyond 10,000 characters for more complex tasks. The distribution of the success rate of solutions sampled from o3 (Fig. 5, right) shows that roughly half the tasks are solved by all 16 attempts, with the remainder spanning range of difficulties: though all are, by construction, within reach of current frontier models. Training Setup. We train our agent with Proximal Policy Optimization (PPO) (Schulman et al., 2017). Our implementation uses SkyRL (Griggs et al., 2025) For each training batch, we sample 16 rollouts per prompt for up to 16 turns, with maximum of 2048 tokens generated per turn and total context window of 16k tokens for the full conversation history. We use temperature of 0.6 for both training and evaluation. We treat each complete episode as single reward signal: the agent receives reward 1 if the final tests pass and 0 otherwise, with no intermediate rewards. For the PPO objective, we use clipping bounds ϵlow = 0.2 and ϵhigh = 0.28 (Yu et al., 2025), with sequence level loss averaging. We do not use KL penalty since anecdotally we found that this hurt performance. To reduce the time for collecting rollouts, we add 5 minute environment 5 Preprint. Figure 3: Training and evaluation results. Top row: Reward curves during PPO training on Endless Terminals for (left) Llama-3.2-3B, (center) Qwen2.5-7B, and (right) Qwen3-8Bopenthinker-sft, showing consistent improvement across all models. Bottom row: Pass rates on (left) our development set, (center) OpenThinker development set , and (right) TerminalBench 2.0 . Models trained with our RL approach (+RL Ours) outperform base models and alternative finetuned variants across all evaluations. Here, RL (OpenThinker) denotes RL training in OpenThoughts-Agent (Team, 2025a). Results on Terminal Bench 2.0 are averaged over 5 runs for our methods. timeout. At inference time, we allow the model to interact for 64 turns, collapsing history when the context limit is reached by add the history of previous commands to the first user message. We train three models: Llama-3.2-3B-Instruct, Qwen2.5-7B-Instruct, and Qwen3-8Bopenthinker-sft as our base models. Qwen3-8B-openthinker-sft is finetuned on 15000 traces from two sources: NL2Bash (Lin et al., 2018), synthetically generated tasks for shell command formatting, and InferredBugs (Jin et al., 2023), collection of C# and Java bugs converted into interactive tasks. Traces are distilled from GLM-4.6 (Zeng et al., 2025). Llama-3.2-3b and Qwen2.5-7b were trained on 4 A100s for about 2 days. Qwen3-8b-openthoughts-sft was trained on 8 B200s for about 8 hours. Endless Terminals yields consistent improvement across models. Training with PPO on procedurally generated tasks produces steady gains regardless of model size or starting capability. As shown in Fig. 3 (top row), reward increases throughout three base training for all modelsLlama-3.2-3B, Qwen2.5-7B, and Qwen3-8B-openthinker-sft. On our heldout development set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.57B from 10.7% to 53.3%, and Qwen3-8Bopenthinker-sft from 42.6% to 59.0% This demonstrates that our procedural generation pipeline provides reliable training signal. On the OpenThinker development set, we observe similar trends: Llama-3.2-3B imFigure 4: Distribution of tasks in Endless Terminals. Left: Task categories, with file operations and log management comprising the largest shares. Preprint. Figure 5: Distribution of Generated Solutions. (left) Distribution of solution lengths (in characters, log scale), showing most tasks require 1000-4000 characters of interaction with long tail for complex tasks. (right) Distribution of pass rates across 16 o3 solution attempts, showing roughly half of tasks are solved by all attempts while the remainder span range of difficulties. Figure 6: Analysis of=uccesses on TerminalBench 2.0. (left) Success rates (pass@5) decline with task difficulty and (right) vary substantially across categories, with software engineering tasks showing the best performance. proves from 0.0% to 1.0%, Qwen2.5-7B from 3.9% to 8.5%, and Qwen3-8B-openthinker-sft from 9.7% to 10.2%. Gains on the OpenThinker development set are smaller, as this benchmark includes general software engineering tasks like issue resolution rather than purely terminal based tasks. Simple RL succeeds when environments scale. Our setup uses vanilla PPO with binary episode level rewards, no intermediate shaping, no KL penalty, and minimal agent architecture without retrieval or multi-agent scaffolding. The gains come not from algorithmic sophistication but from scaling the environments, Endless Terminals provides the diverse, automatically verifiable tasks that RL requires. Gains transfer to held out human-curated benchmarks. The improvements from training on Endless Terminals generalize beyond our procedurally generated distribution. On TerminalBench 2.0, human curated benchmark the models never see during training, we observe substantial improvements: llama-3.2-3b improves from 0.0% to 2.2%, qwen-2.5-7b from 2.2% to 3.4%, and qwen-3-8b-open-thoughts-sft from 1.1% to 6.7%. In each case our RL trained models outperform other versions of the same base architectures (Fig. 3, bottom right), including models trained with alternative RL recipes (Team, 2025a) with agentic scaffolds like Terminus-2 (Team, 2025b). Finally, our tasks were generated before the release of TerminalBench 2.0, ensuring no data leakage. 7 Preprint. Failure Modes. We analyze the tasks where our best model, qwen-3-8b-openthoguhts-sft achieved zero success on TerminalBench 2.0. For context, Claude Sonnet 4.5 with Terminus-2 achieves 42.8% on this benchmark with 200 turn limit and an agentic scaffold, compared to our 6.7% with 64 turns and no agentic scaffold. Pass@5 rates (at least one success in 5 attempts) decrease with task difficulty: 25% (1/4) easy, 14.5% (8/55) medium, and 10% (3/30) hard tasks  (Fig. 6)  . We identify two primary failure modes  (Fig. 7)  : 1) loop failures (Pipis et al., 2025), where the model becomes stuck repeating the same command sequence, accounting for 39% of failures (30 tasks), and 2) turn exhaustion, where the model hits the turn limit (we train with 16 turns and evaluate with 64 turns with sliding window), affecting 26% of failures (20 tasks). These categories overlap, with 11 tasks exhibiting both behaviors. The remaining failures (49%) terminate early with incorrect solutions, often in specialized domains like cryptanalysis, machine learning model extraction, and bioinformatics where the model lacks domain knowledge. By category (pass@5), the model performs best on software-engineering (6/26), followed by data-science (1/8), optimization (1/2), scientific-computing (1/8), systemadministration (1/9), data-processing (1/4), and security (1/8), while achieving zero success on mathematics (0/4), machine-learning (0/3), and model-training (0/4) tasks. To understand loop failures, we measure command diversity: the ratio of unique commands to total commands issued after the first error. Successful tasks exhibit significantly higher command diversity (0.49 on average) compared to failed tasks with loop behavior (0.18 on average), indicating that successful runs explore alternative approaches after errors while failed runs repeat commands. Figure 7: Failure Analysis. Failure modes are dominated by loop behaviors (39%) and turn exhaustion (26%), with remaining failures terminating early on specialized domains like cryptanalysis and bioinformatics (right). Distillation and RL are complementary. Our results show that starting from stronger base amplifies gains from RL. Qwen3-8B-openthinker-sft, which was first finetuned on 15,000 distilled traces from NL2Bash and InferredBugs, achieves the highest final performance after RL training (6.7% on TerminalBench 2.0). This suggests that SFT provides warm start that RL can build upon more effectively (Gandhi et al., 2025; Guha et al., 2025)."
        },
        {
            "title": "6 Discussion",
            "content": "We introduced Endless Terminals, procedural generation pipeline that synthesizes terminal-use tasks without human annotation or distillation from stronger models. The pipeline operates autonomously across four stagestask description generation, container setup with self, validation, completion test generation, and solution based filtering, producing 3,255 valid tasks spanning tasks like file operations, log management, data processing, text processing, etc. Training with PPO on these tasks yields consistent improvements across model scales, and these gains transfer to TerminalBench 2.0., human curated benchmark. Our results demonstrate that simple RL setups can succeed when environments scale. Our failure mode analysis on TerminalBench 2.0 revealed two patterns: loop failures, where models repeat the same command sequences (39% of failures), and turn exhaustion (26%). Successful tasks exhibit significantly higher command diversity after the first error (0.49 vs 0.18 for looping failures), suggesting that exploring alternative approaches is crucial. Performance also varies by domain: software engineering tasks achieve 23% success while mathematics, machine learning, and model training show zero success. This gap may reflect insufficient coverage of these domains in our procedural generation pipeline. 8 Preprint. Our approach has several limitations worth noting. First, the procedurally generated tasks tend to resemble competitive programming problems more than the messy, underspecified requests that users actually pose to AI assistants. Real terminal use often involves ambiguous goals, implicit context, and might require clarifying questions. These are difficult to capture in automatically generated specifications without sacrificing verifiability. However, better user modeling (Shaikh et al., 2025; Weston & Foerster, 2025; Gandhi et al., 2026) might enable building of such fuzzy environments. Conditioning the generation prompt to produce more naturalistic requests while maintaining sufficient specification for verification remains an open challenge. Our filter for solvability introduces capability ceiling. We filter using pass@16 from o3, retaining only tasks where at least one solution succeeds. This process that discards roughly half of all generated candidates. This filtering confirms that retained tasks are solvable and removes potentially underspecified or invalid tasks but also tasks that are beyond the capabilities of o3. This means that our pipeline cannot generate tasks beyond the frontier models capability. As stronger models emerge, this ceiling will rise, but the dependence on frontier validator limits our ability to train agents on truly novel problems. Self-play approaches, where models iteratively generate tasks just beyond their current capability and learn to solve them, could adaptively scale difficulty without relying on fixed frontier validator (Poesia et al., 2024; Zhao et al., 2025). Incorporating humans in the loop, either to validate generated tasks or to provide naturalistic task descriptions, could improve both task quality and diversity beyond what purely synthetic generation achieves, albeit at the increasing the cost of generating tasks, making the pipeline less scalable. Testing RL with richer agentic scaffolds (retrieval, multi-agent coordination, tool use) may yield further gains. Partial rewards based on the number of test cases passed, rather than binary episode-level rewards, could provide denser training signal and accelerate learning (Sun et al., 2025). Finally, learning world models of terminal dynamics (Copet et al., 2025) or distilling environment dynamics into reasoning-based experience models (Chen et al., 2025) could enable more sample-efficient training by allowing agents to plan and simulate outcomes through imagined rollouts before executing commands. These directions, task verification, richer scaffolds, denser reward signals, and learned world models, highlight that building capable terminal agents remains multifaceted challenge. Endless Terminals represents one cog in this greater effort, demonstrating that scalable environment generation can unlock substantial gains even with simple RL setups. We hope this work encourages the community to invest in automated task generation pipelines alongside algorithmic advances, richer scaffolds, and improved training objectives."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank Vaish Shrivastava, Sahaj Agarwal, Vasilis Kontonis, Ahmed Awadallah, Corby Rossett and Shital Shah for discussions and their support. This work was started during KGs internship at Microsoft Research. KG was supported by an HAI-SAP Grant and an NSF Expeditions grant during his time at Stanford."
        },
        {
            "title": "References",
            "content": "Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, et al. Scaling agent learning via experience synthesis. arXiv preprint arXiv:2511.03773, 2025. Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 9 Preprint. Kanishk Gandhi, Agam Bhatia, and Noah Goodman. Learning to simulate human dialogue. arXiv preprint arXiv:2601.04436, 2026. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tyler Griggs, Sumanth Hegde, Eric Tang, Shu Liu, Shiyi Cao, Dacheng Li, Charlie Ruan, Philipp Moritz, Kourosh Hakhamaneshi, Richard Liaw, Akshay Malik, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Evolving skyrl into highly-modular rl framework, 2025. Notion Blog. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https: //arxiv.org/abs/2506.04178. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. Inferfix: End-to-end program repair with llms. In Proceedings of the 31st ACM joint european software engineering conference and symposium on the foundations of software engineering, pp. 16461656, 2023. Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael Ernst. Nl2bash: corpus and semantic parser for natural language interface to the linux operating system. arXiv preprint arXiv:1802.08979, 2018. Team Olmo, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. OpenAI. OpenAI o3 and o4-mini system card. o3-o4-mini-system-card/, April 2025. https://openai.com/index/ Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. Charilaos Pipis, Shivam Garg, Vasilis Kontonis, Vaishnavi Shrivastava, Akshay Krishnamurthy, and Dimitris Papailiopoulos. Wait, wait, wait... why do reasoning models loop? arXiv preprint arXiv:2512.12895, 2025. Gabriel Poesia, David Broman, Nick Haber, and Noah Goodman. Learning formal mathematics from intrinsic motivation. Advances in Neural Information Processing Systems, 37: 4303243057, 2024. Prime Intellect. https://app.primeintellect.ai/dashboard/ environments, 2025. community hub for discovering and sharing environments for RL training and downstream evaluation. Accessed: 2025-01-17. Environments hub. Yang Qwen, Baosong Yang, Zhang, Hui, Zheng, Yu, Chengpeng Li, Liu, Huang, Wei, et al. Qwen2. 5 technical report. arXiv preprint, 2024. 10 Preprint. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Omar Shaikh, Shardul Sapkota, Shan Rizvi, Eric Horvitz, Joon Sung Park, Diyi Yang, and Michael Bernstein. Creating general user models from computer use. In Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology, pp. 123, 2025. Alex Shaw. Harbor Framework, 2025. URL https://github.com/laude-institute/harbor. Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, and Dawn Song. Rl grokking recipe: How does rl unlock and transfer new algorithms in llms? arXiv preprint arXiv:2509.21016, 2025. OpenThoughts-Agent Team. OpenThoughts-Agent. https://open-thoughts.ai/agent, December 2025a. The Terminal-Bench Team. Terminal-bench: benchmark for ai agents in terminal environments, Apr 2025b. URL https://github.com/laude-institute/terminal-bench. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Jason Weston and Jakob Foerster. Ai & human co-improvement for safer co-superintelligence. arXiv preprint arXiv:2512.05356, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/2405.15793. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Stanford University",
        "UW-Madison"
    ]
}