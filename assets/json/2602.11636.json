{
    "paper_title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
    "authors": [
        "Changti Wu",
        "Jiahuai Mao",
        "Yuzhuo Miao",
        "Shijie Lian",
        "Bin Yu",
        "Xiaopeng Lin",
        "Cong Huang",
        "Lei Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at \\href{https://github.com/ChangtiWu/ScalSelect}{ScalSelect}."
        },
        {
            "title": "Start",
            "content": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning Changti Wu1,2, Jiahuai Mao3, Yuzhuo Miao4,2, Shijie Lian5,2, Bin Yu4,2, Xiaopeng Lin6,7, Cong Huang2,7, Lei Zhang1, Kai Chen2,7 1East China Normal University 2Zhongguancun Academy 3The Hong Kong Polytechnic University 4Harbin Institute of Technology 5Huazhong University of Science and Technology 6The Hong Kong University of Science and Technology (Guangzhou) 7Zhongguancun Institute of Artificial Intelligence 6 2 0 2 2 1 ] . [ 1 6 3 6 1 1 . 2 0 6 2 : r Abstract Large-scale Visual Instruction Tuning (VIT) has become key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at ScalSelect. CCS Concepts Computing methodologies Artificial intelligence. Keywords Data selection, Visual instruction tuning, Vision-language models"
        },
        {
            "title": "1 Introduction\nLarge-scale Visual Instruction Tuning (VIT) [25, 27, 54] has be-\ncome a cornerstone for advancing the capabilities of modern vision-\nlanguage models (VLMs) [12, 20, 52]. By leveraging massive collec-\ntions of image-instruction pairs, recent VLMs have demonstrated\nstrong performance across a wide range of multimodal tasks, includ-\ning visual question answering, optical character recognition (OCR),\ndiagram understanding, and multimodal reasoning [21, 26, 31, 44,\n45, 47]. However, as the scale of visual instruction datasets continues\nto grow, full fine-tuning on the entire dataset becomes increasingly\nexpensive and computationally inefficient [40]. In practice, such",
            "content": "Corresponding author. datasets often contain substantial redundancy, where many samples are highly overlapping in content or supervision and contribute limited marginal gains while incurring significant computational cost. This motivates the need for effective data selection methods that can identify compact subset of samples, enabling efficient training while maintaining the competitive performance of full fine-tuning. To address data selection, number of existing approaches adopt training-based or gradient-based strategies. Training-based methods [5, 32, 34, 46] typically rely on warm-up training or training proxy model to estimate sample importance, while gradient-based methods [35, 50] require backpropagation through the target model to measure the influence of samples. Although effective in certain settings, these approaches inevitably introduce additional training procedures and computational overhead, partially conflicting with the original goal of data selection. Moreover, the reliance on proxy models or auxiliary training stages complicates deployment and limits scalability, especially for large-scale multimodal datasets. As result, there has been growing interest in training-free data selection methods that aim to avoid the high computational cost associated with training or backpropagation while still preserving model performance. Existing training-free multimodal data selection methods [3, 14, 17, 42] often rely on the similarity-related metrics between sample representations. Despite efficiency advantages, they often suffer from limitations in how sample representations are constructed. Many approaches either rely on external proxy models or datasets, such as CLIP-based visual representations, or aggregate all visual tokens from the target model to form sample representation. These strategies are largely instruction-agnostic: they treat visual content independently of the accompanying textual instruction. However, in visual instruction tuning, the semantic meaning of sample is largely conditioned on the instruction. The same image may require attention to entirely different visual regions under different instructions. By indiscriminately aggregating all visual tokens without considering instruction relevance, existing methods risk mixing instruction-relevant and irrelevant information, resulting in sample representations failing to distinguish such cases effectively. Another fundamental limitation shared by many prior methods lies in their local perspective on data selection. Most existing approaches [3, 14, 17, 32, 42] rely on pairwise comparisons between samples, such as similarity computation, clustering, or influence. While these local criteria encourage selecting samples that are different from one another, they do not explicitly account for the global structure of the dataset. Selecting samples that are locally diverse does not necessarily ensure that the overall representation space is well preserved. Furthermore, pairwise comparisons inevitably incur at least quadratic time complexity, posing significant scalability bottleneck for large datasets. In this work, we propose fundamentally different perspective on multimodal data selection by adopting global, subspaceoriented view. Instead of reasoning about relationships between individual samples, we focus on the contribution of each sample to the representation space of the entire dataset, i.e., the space spanned by the full dataset representations. This perspective builds on the observation that, under large-scale VIT, the induced representation space is often highly redundant and approximately low-rank, with most meaningful variability concentrated in small number of dominant directions. Accordingly, we aim to select subset of samples whose representations can best span and approximate the dominant subspace of the original representation space. From this perspective, data selection can be viewed as form of subspace compression in representation space: the goal is to identify compact subset that preserves the datasets essential global structure under low-rank view, such that training on the subset remains close to training on the full data. This global formulation eliminates the need for pairwise comparisons and enables scalable data selection. Based on this insight, we introduce ScalSelect, scalable and training-free multimodal data selection method. ScalSelect constructs sample representations directly from the target model by leveraging instruction-conditioned attention signals in the first transformer layer of the LLM, capturing instruction-relevant visual information while suppressing irrelevant content. To select informative samples, ScalSelect estimates the dominant representation subspace and scores samples by their contributions to this subspace, selecting compact subset with linear-time scalability with respect to the number of samples and without proxy models or datasets. Extensive experiments across multiple VLM architectures, datasets, selection budgets, and evaluation benchmarks show that ScalSelect can match full-data training closely using only small fraction of the data. Under 16% selection budget, ScalSelect achieves over 97.50% of the performance of full-data training on LLaVA-Vicuna7B across two datasets, and even surpasses full-data training on Qwen3-VL models in our experiments. Our contributions can be summarized as follows: Instruction-Conditioned Early Representation is introduced to construct sample representations by extracting instructionrelevant visual features from the first LLM transformer layer of the target VLM, enabling instruction-aware modeling. Subspace-Aware Global Selection is proposed as global data selection perspective that prioritizes preserving the dominant representation subspace rather than relying on local, sample-to-sample relationships. ScalSelect is developed as scalable training-free selection method with linear-time complexity, avoiding pairwise comparisons, proxy models, and additional training procedures. Extensive empirical studies, including cross-model and crossdataset evaluations, selection budget scaling, ablation studies, and representation-level analyses, systematically validate the effectiveness, robustness, and key design choices of ScalSelect."
        },
        {
            "title": "2 Related Work\n2.1 Vision-Language Models and Visual",
            "content": "Instruction Tuning. Vision-language models (VLMs) have made significant progress by combining pretrained large language models with visual encoders, enabling models to perform various visual-language tasks. Early works, such as CLIP [39] and ALIGN [15], pioneered large-scale contrastive learning to align visual features with textual representations in shared latent space. Recently, visual instruction tuning (VIT) [25, 27, 54] has advanced VLMs by fine-tuning them with large-scale image-instruction pairs, enabling models to follow natural language instructions for visual inputs. Pioneering models like LLaVA [27], LLaVA-1.5 [25], and MiniGPT-4 [55] established the feasibility of following complex human intentions by VIT. This trajectory has been further advanced by diverse architectures, including Qwen3-VL [2], LLaVA-NeXT [26], and DeepSeek-VL2 [47]. Moreover, specialized models have pushed the boundaries in niche domains [4, 6, 22], such as DeepSeek-OCR [44] for document intelligence and LLaVA-Med [18] for biomedicine. However, VIT faces challenges in terms of computational efficiency, as full fine-tuning on large datasets is expensive, and many samples in these datasets contribute redundantly to performance. This has led to the exploration of multimodal data selection to reduce computational costs while largely preserving model performance."
        },
        {
            "title": "2.2 Multimodal Data Selection for Visual",
            "content": "Instruction Tuning. Multimodal data selection aims to identify informative subsets of image-instruction pairs for visual instruction tuning (VIT), reducing training cost while retaining most of the performance of full fine-tuning. Early approaches primarily rely on simple heuristics, such as random sampling, instruction length, or perplexity-based filtering [38], which are computationally cheap but largely task agnostic. More recent work has explored training-based [5, 32, 34, 46] and gradient-based [35, 50] selection methods, which estimate sample importance via warm-up training, proxy models, or gradient backpropagation through the target model. While effective in some settings, these approaches introduce additional training stages, incur substantial computational overhead, and suffer from limited scalability on large multimodal datasets. To overcome these issues, growing line of research focuses on training-free data selection methods [3, 14, 17, 30, 42, 51] that operate on pretrained representations without explicit training or backpropagation. However, existing training-free methods rely on external proxy encoders, construct instruction-agnostic sample representations, or depend on pairwise similarity computations, leading to representation mismatch, limited instruction awareness, and quadratic complexity. For example, COINCIDE [17] selects samples based on clustering and similarity but relies on proxy encoders and pairwise comparisons. PRISM [3] depends on similarity computations with quadratic complexity. RDS+ [14] achieves linear-time selection but assumes access to an auxiliary proxy dataset that matches the evaluation distribution, which may not hold in practice. These limitations highlight the need for more scalable and representation-faithful training-free data selection strategies for visual instruction tuning. Figure 1: Overview of ScalSelect. (Left) For each sample, the target VLM extracts instruction-conditioned early representations by aggregating the visual tokens that are most attended by the user instruction tokens in the first layer of the LLM, yielding compact sample representation. (Right) The representations of all samples are stacked into representation matrix, from which ScalSelect identifies the dominant low-rank subspace of the full representation space (the space spanned by the full dataset representations). Each sample is scored according to its contribution to this dominant subspace, producing an importance score distribution from which compact subset of informative samples is selected."
        },
        {
            "title": "Representation",
            "content": "A growing body of representation learning related work [10, 23, 41, 53] suggests that the early transformer layers of the LLM play critical role in cross-modal alignment. For example, Align-KD [10] empirically shows that the first LLM layer primarily aligns text and visual hidden states into shared representation space, while the final layer projects representations into the output space. Other works [23, 53] reveal that, in the early LLM layers, visual information is predominantly injected into textual representations, and visual tokens receive the highest attention weights, after which their attention rapidly diminishes to negligible levels. Taken together, these empirical observations suggest consistent layer-wise 3 tendency in VLMs: early LLM layers tend to extract instructionrelevant visual information, the middle layers are more involved in text-dominant reasoning, and the final layer primarily maps the fused representation to the output space. In addition, for the same image, different instructions can induce the model to attend to entirely different visual regions and semantic attributes. Consequently, treating visual representations independently of the instruction, as adopted by many existing data selection methods, fails to account for the diversity introduced by varying textual instructions. This motivates us to condition sample representations on the input instruction when characterizing their contribution to the global representation space. Motivated by these findings, we construct our sample representations from the first transformer layer of the LLM, where crossmodal alignment is most prominent, visual tokens receive the highest attention, and visual representations are already conditioned on the input instruction. Specifically, let the original dataset to be selected be denoted as ğ· = {ğ‘ 1, . . . , ğ‘ ğ‘ }, where each sample , ğ´ğ‘‡ğ‘– ğ‘– } consists of the visual input ğ‘‰ğ‘– and ğ‘ ğ‘– = {ğ‘‰ğ‘–, ğ‘ˆ 1 the ğ‘‡ğ‘– -turn conversation. Each conversation round {ğ‘ˆ ğ‘¡ ğ‘– } corresponds to the user instruction ğ‘ˆ ğ‘¡ ğ‘– and the assistant response ğ´ğ‘¡ ğ‘– at turn ğ‘¡ {1, . . . ,ğ‘‡ğ‘– }. For each sample ğ‘ ğ‘– ğ·, the visual input is encoded and projected into the LLM hidden space, while the text input is tokenized and embedded into the same space: ğ‘– , . . . , ğ‘ˆ ğ‘‡ğ‘– ğ‘– , ğ´1 ğ‘– , ğ´ğ‘¡ ğ‘– {ğ‘£ 1 {ğ‘¢ğ‘¡,1 ğ‘– {ğ‘ğ‘¡,1 ğ‘– ğ‘– , . . . , ğ‘£ ğ‘ğ‘£ ğ‘– , . . . , ğ‘¢ğ‘¡,ğ‘ ğ‘¡ ğ‘– , . . . , ğ‘ğ‘¡,ğ‘ ğ‘¡ ğ‘– ğ‘ ğ‘¢ } Proj(VE(ğ‘‰ğ‘– )), } Emb(Tok(ğ‘ˆ ğ‘¡ } Emb(Tok(ğ´ğ‘¡ ğ‘– )), ğ‘– )), (1) where VE() denotes the visual encoder, Proj() denotes the projector, Tok() and Emb() denote the tokenizer and embedding layers of the LLM, respectively. {ğ‘£ ğ‘— ğ‘– }ğ‘ğ‘£ ğ‘—=1 denotes the sequence of visual ğ‘ ğ‘¡ ğ‘ ğ‘¡ token embeddings, while {ğ‘¢ğ‘¡,ğ‘— ğ‘—=1 and {ğ‘ğ‘¡,ğ‘— ğ‘ ğ‘¢ ğ‘– } ğ‘– } ğ‘—=1 denote the token embeddings of the user instruction and assistant response at turn ğ‘¡, respectively. At the first transformer layer of the LLM, multi-head self-attention is applied over the concatenated sequence of visual and textual tokens. Let ğ‘„ (1), ğ¾ (1) Rğ¿ğ‘‘â„ denote the query and key matrices at this layer, where ğ¿ is the total number of tokens in the input sequence and ğ‘‘â„ is the hidden dimension of each attention head. For single attention head, the attention matrix is computed as (1) = softmax (cid:32) ğ‘„ (1)ğ¾ (1) ğ‘‘â„ (cid:33) , (2) where (1) Rğ¿ğ¿ encodes post-softmax attention scores between all token pairs, and the entry (1) [ğ‘, ğ‘] represents the attention score from the ğ‘-th token to the ğ‘-th token in the input sequence. In the multi-head setting, we average the attention matrices across all heads. Based on the head-averaged attention matrix, we compute the aggregated attention score received by each visual token ğ›¼ ğ‘— ğ‘– (ğ‘— {1, . . . , ğ‘ğ‘£ }) from all user instruction tokens: ğ›¼ ğ‘— ğ‘– = ğ‘‡ğ‘– ğ‘ ğ‘¡ ğ‘¢ ğ‘¡ = ğ‘š=1 Attn(1) (ğ‘¢ğ‘¡,ğ‘š ğ‘– , ğ‘£ ğ‘— ğ‘– ), (3) where Attn(1) (, ) denotes the attention score from user instruction token to visual token in the head-averaged attention matrix of the first transformer layer. We then sort the visual tokens in descending order of their aggregated attention scores and select tokens from top to bottom until their cumulative attention reaches threshold ğœ (0, 1) of the total attention. For each sample ğ‘ ğ‘– ğ·, the selected visual tokens form the set of the most attended visual tokens, denoted as Vğ‘– : Vğ‘– = arg minV ğ‘– {ğ‘£1 ğ‘– ,...,ğ‘£ğ‘ğ‘£ ğ‘– } ğ‘– s.t. ğ›¼ ğ‘— ğ‘– ğœ ğ‘£ ğ‘— ğ‘– ğ‘– ğ‘ğ‘£ ğ‘—=1 ğ›¼ ğ‘— ğ‘– . (4) We find that the selection remains stable when the attention-mass threshold ğœ varies within reasonable range (e.g., 0.850.95; see Appendix B). We therefore use ğœ = 0.9 by default for good trade-off between compactness and coverage. Finally, we average the hidden states of the selected visual tokens at the first transformer layer to obtain the representation of sample ğ‘ ğ‘– : xğ‘– = 1 Vğ‘– LLM(1) (ğ‘£ ğ‘— ğ‘– ), (5) ğ‘£ ğ‘— ğ‘– Vğ‘– where LLM(1) () denotes the output hidden state of the first transformer layer."
        },
        {
            "title": "3.2 Subspace-Aware Global Selection\nMost existing data selection methods adopt a local view by explicitly\nmodeling relationships between samples, typically through pair-\nwise similarity or influence-based measures, and make selection\ndecisions based on local, sample-to-sample comparisons. In con-\ntrast, we move beyond modeling pairwise relationships between\nsamples and instead focus on how each individual sample relates to\nthe global representation space as a whole. Building on this view,\nwe take a global, subspace-aware perspective: we aim to identify a\nsubset of representations that best span and approximate the subspace\nof the original representation space, where subspace refers to the\ndominant low-rank principal subspace of the sample representation\nmatrix, i.e., the linear space spanned by the top singular directions\nthat capture the majority of variance in the data. The selected subset\nretains most of the information relevant to the best rank-ğ‘˜ approxi-\nmation of the full dataset, and is thus nearly as informative as the\noriginal data under a low-rank representation view. By focusing on\nthis dominant subspace, the selected subset is expected to better\ncapture the global representational characteristics of the full dataset,\nwhich empirically helps models trained on it achieve performance\ncomparable to training on the full dataset.",
            "content": "To this end, we adopt leverage-score-based row selection strategy inspired by CUR matrix decompositions [7, 37]. CUR decomposition approximates matrix by selecting subset of its rows and columns such that the selected rows and columns capture the dominant low-rank structure of the original matrix. Given the attentionconditioned sample representations {x1, . . . , xğ‘ } obtained in Section 3.1, we stack all sample representations into matrix: = [x1; x2; . . . ; xğ‘ ] Rğ‘ ğ‘‘, (6) where each row of is sample representation. We first perform column-wise centering on to obtain Xğ‘ . We then estimate the dominant low-rank structure of Xğ‘ via truncated singular value decomposition (SVD) [43], focusing only on the leading singular components: Xğ‘ Uğ‘˜ Î£ğ‘˜ ğ‘˜ , (7) where Î£ğ‘˜ = diag(ğœ1, . . . , ğœğ‘˜ ) Rğ‘˜ ğ‘˜ , and Uğ‘˜ = [ğ’”1, . . . , ğ’”ğ‘˜ ] Rğ‘ ğ‘˜ . Here, {ğœ1, . . . , ğœğ‘˜ } (ğœ1 ğœ2 . . . ğœğ‘˜ 0) denote the top-ğ‘˜ singular values of Xğ‘ , and {ğ’”1, . . . , ğ’”ğ‘˜ } are the corresponding left singular vectors. For truncated SVD, we select the smallest ğ‘˜ such that the cumulative energy of the top-ğ‘˜ singular values reach 90% of the total spectral energy (following common practice [1, 8, 13, 37] in lowrank approximation, we set the energy threshold to 90% throughout this work): ğ‘˜ = arg minğ‘˜ {1,...,ğ‘Ÿ } ğ‘˜ s.t. ğ‘˜ ğ‘—=1 ğœ 2 ğ‘— 90% ğ‘Ÿ ğ‘—= ğ‘— , ğœ 2 (8) where ğ‘Ÿ = rank(Xğ‘ ) is the rank of Xğ‘ , and ğ‘˜ denotes the dimensionality of the selected low-rank subspace, corresponding to the minimum number of dominant singular components required to explain 90% of the total spectral energy. 4 After that, we calculate an importance score (statistical leverage score) for each row of Xğ‘ (i.e., each sample): ğœ‹ğ‘– = (cid:16) ğ‘– ğ‘— ğ’” (cid:17) 2 , ğ‘˜ ğ‘—=1 (9) where ğ‘– {1, . . . , ğ‘ }, and ğ’”ğ‘– ğ‘— is the ğ‘–-th element of the ğ‘—-th left singular vector of Xğ‘ . The importance score has natural meaning: the statistical leverage or influence of the row on the best lowrank fit of matrix Xğ‘ . Finally, after obtaining the importance score for each sample, we select the top-ranked samples according to their importance scores to form the final subset. Overall, by prioritizing samples with the highest statistical leverage, the subset preserves most of the information relevant to reconstructing the best rank-ğ‘˜ approximation of the original representation matrix. In this sense, our method achieves effective representational compression, retaining the dominant structural information of the original data."
        },
        {
            "title": "4 Scalability Analysis\nIn this section, we analyze the scalability of ScalSelect with respect\nto the number of samples ğ‘ , and compare it with other multimodal\ndata selection methods to demonstrate its scalability advantage.",
            "content": "Scalability of ScalSelect. In the Attention-Conditioned Early Representation stage, each sample requires only single forward pass, and the method relies solely on the attention scores and hidden states from the first transformer layer of the LLM. Therefore, the time complexity of this stage is ğ‘‚ (ğ‘ ). In the Subspace-Aware Global Selection stage, the overall time complexity mainly consists of the following components: 1) Performing truncated SVD on the representation matrix Xğ‘ , which computes only the top-ğ‘˜ singular components Uğ‘˜ Rğ‘ ğ‘˜ and Î£ğ‘˜ Rğ‘˜ ğ‘˜ . This step costs ğ‘‚ (ğ‘ğ‘‘ğ‘˜) time. 2) Computing the importance scores, which has complexity of ğ‘‚ (ğ‘ ğ‘˜). Since the representation dimension ğ‘‘ ğ‘ is fixed by the model architecture and the subspace dimensionality ğ‘˜ ğ‘‘ ğ‘ is consistently small in practice (e.g., ğ‘˜ = 9 for LLaVA-V-625K as presented in Section 5.4), the overall selection process scales linearly with the dataset size ğ‘ . Scalability Comparison with Other Methods. Existing multimodal data selection methods based on training or gradients depend on proxy training or gradient backpropagation, introducing additional training and considerable computational overhead. Among training-free methods, several approaches depend on computing pairwise similarities or influence scores between samples, resulting in quadratic time complexity ğ‘‚ (ğ‘ 2) and limited scalability. Among recent training-free methods, COINCIDE [17] has time complexity of ğ‘‚ (cid:0)ğ¾ğ‘ + (cid:205)ğ¶ğ‘– 2(cid:1), when the sample number in each cluster ğ¶ğ‘– is balanced, its complexity reduces to ğ‘‚ (ğ¾ğ‘ + ğ‘ 2/ğ¾); however, it still relies on proxy model. PRISM [3] computes pairwise similarities between samples, leading to quadratic time complexity of ğ‘‚ (ğ‘ 2). RDS+ [14] achieves linear-time complexity, but relies on strong assumption that proxy dataset sharing the same data distribution as the evaluation set is available, which is unrealistic in practice and may introduce distribution bias. Overall, compared with these methods, ScalSelect achieves linear-time complexity in training-free manner, without relying on any external models or proxy datasets, demonstrating clear advantages in scalability."
        },
        {
            "title": "5 Experiments\n5.1 Experiment Setup\nDataset. We conduct experiments on a large-scale multimodal vi-\nsual instruction tuning dataset, LLaVA-V-625K, which we construct\nfrom LLaVA-665K [25] by removing 40K text-only samples and\nretaining the remaining 625K multimodal samples. LLaVA-V-625K\ncomprises diverse multimodal instruction data, covering tasks such\nas visual question answering (VQA), optical character recognition\n(OCR), region-level VQA, and visual conversation. To verify the\ngeneralization of our method across different datasets, we addi-\ntionally evaluate on the public 180K GPT-4-generated dataset of\nthe LRV-Instruction [24], which we refer to as LRV-Sub-180K. This\ndataset contains diverse visual question answering data with varied\ninstruction styles and visual content. Unless otherwise specified,\nall experiments are conducted on LLaVA-V-625K with a selection\nbudget of 100K samples (16%).",
            "content": "Models. Following the training setup of LLaVA-1.5 [25], we use the pretrained model before visual instruction tuning on LLaVA1.5-7B, which we refer to as LLaVA-Vicuna-7B, as the target model for most of our experiments. To verify the generality of our method across different model architectures, we additionally evaluate on two widely adopted VLMs, Qwen3-VL-4B-Instruct and Qwen3-VL8B-Instruct [2]. All models are fine-tuned for one epoch on 8 NVIDIA H100 (80GB) using the same training configuration, ensuring fair and controlled comparisons. Training hyperparameters are provided in Appendix A. Evaluation Benchmarks. We evaluate the fine-tuned models on diverse set of widely used multimodal benchmarks to assess various capabilities of VLMs. To evaluate the overall capabilities of multimodal large language models, including multimodal understanding, coarse-grained and fine-grained perception, as well as cognition and reasoning, we adopt three widely used benchmarks: 1) MMBench [28], which includes an English version (MMBenchEn) and Chinese version (MMBench-Cn); 2) MME [11], which comprises perception subset (MME-P) and cognition subset (MME-C); and 3) AI2D [16], benchmark for diagram understanding. To assess hallucination tendencies, we adopt POPE [19], reporting its two core metrics: accuracy (POPE-A) and precision (POPE-P). For scientific question answering, we adopt ScienceQA-IMG (SQAIMG), the vision-based subset of ScienceQA [33], for evaluation. For OCR tasks, we adopt OCRBench [29] for evaluation. Baselines. We compare ScalSelect with range of representative and widely used multimodal data selection methods. Traditional heuristics include Random, Length, and Perplexity [38], where Random randomly selects samples, Length uses sample length as an importance score, and Perplexity selects samples with medium perplexity values. For strong recent representative methods, we compare COINCIDE [17], PRISM [3], and RDS+ [14], which are widely adopted and commonly reported to achieve competitive performance under various experimental settings in prior work. We focus on baselines applicable without additional optimization or warm-up training for fair comparison. We also report Full-Finetune results using the entire training dataset as an upper-bound reference. Rel. denotes the average relative performance across benchmarks, computed as the mean percentage normalized by the reference method set to 100% in each table. Method MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. Full-Finetune Random Length Perplexity COINCIDE PRISM RDS+ ScalSelect (Ours) 63.96 57.96 49.10 31.74 62.16 61.49 56.05 59.19 56.73 51.74 36.43 32.47 55.44 52.96 50.16 52.80 1463. 278.57 1418.85 1282.97 1403.08 1422.01 1396.24 1414.66 1400.34 295.36 292.14 278.93 274.29 276.43 302.85 308.57 53.79 50.78 37.44 38.02 51.70 50.87 50.84 51.75 85. 84.96 81.82 84.81 83.79 83.38 84.84 83.96 93.05 90.71 96.71 93.02 90.99 93.26 92.15 94.73 67.63 65.20 52.70 48.44 66.83 64.06 64.45 65.29 20. 17.90 13.60 18.60 18.30 18.80 18.60 19.40 100.00% 95.66% 83.10% 81.80% 96.85% 96.01% 95.71% 97.85% Table 1: Performance comparison with baselines on LLaVA-V-625K under 100K selection budget. Method #Samples MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. Full-Finetune 625K ScalSelect 50K 100K 200K 300K 400K 63. 57.79 59.19 61.10 63.05 65.30 56.73 51.68 52.80 54.15 55.09 58.97 1463.87 278.57 1282.22 1400.34 1435.79 1517.26 1425. 314.29 308.57 298.93 290.00 299.29 53.79 50.87 51.75 53.21 54.18 54.21 85.24 83.99 83.96 84.50 84.70 85.74 93. 93.64 94.73 93.87 93.05 93.60 67.63 63.11 65.29 65.39 66.53 67.72 20.30 18.10 19.40 19.50 19.30 19.80 100.00% 95.34% 97.85% 98.67% 99.66% 101.16% Table 2: Performance under different selection budgets. #Samples denotes the number of samples selected from LLaVA-V-625K for fine-tuning."
        },
        {
            "title": "5.2 Main Results\nPerformance Compared to Baselines. Table 1 compares ScalSe-\nlect with a range of representative baselines on LLaVA-V-625K un-\nder a fixed 100K selection budget. We observe that simple heuristic\nmethods, including Length and Perplexity, consistently underper-\nform Random sampling across most benchmarks, indicating that\ninstruction length or perplexity alone is insufficient for identifying\nimportant training samples, and tend to induce skewed and less\ninformative sample compositions. COINCIDE exhibits mixed perfor-\nmance across benchmarks, achieving competitive results on some\nevaluation tasks (e.g., MMBench) while showing clear weaknesses\non MME-C and POPE-P. This pattern indicates that emphasizing\ninstance-level diversity can benefit MMBench, but may come at the\ncost of degraded reasoning robustness. On another broad-coverage\nbenchmark, MME, ScalSelect significantly outperforms COINCIDE,\nachieving a substantially higher score on MME-C while maintaining\ncompetitive performance on MME-P. These results reveal a trade-\noff between performance on different broad-coverage benchmarks,\nsuch as MMBench and MME, reflecting different emphases on\nmodel capability. Furthermore, ScalSelect consistently outperforms\nall baselines on MME-C, AI2D, and OCRBench, and matches or\nslightly surpasses Full-Finetune on MME-C and POPE-P. ScalSelect\nalso achieves a clear advantage on OCRBench, demonstrating its\neffectiveness on tasks requiring fine-grained, instruction-relevant\nvisual attention (see Section 5.4 for further analysis). Overall, under\nthe 100K selection budget, ScalSelect achieves the highest average\nrelative performance among all evaluated methods, reaching 97.85%\nof Full-Finetune performance. These results suggest that ScalSelect\noffers a favorable balance between scalability and effectiveness\nunder limited data budgets.",
            "content": "Performance Under Different Selection Budgets. Table 2 reports the performance of ScalSelect on LLaVA-V-625K under different selection budgets. As the number of selected samples increases from 50K to 400K, the performance consistently improves across most benchmarks, demonstrating the effectiveness and stability of ScalSelect under varying selection budgets. With only 50K selected samples, ScalSelect already retains over 95% of the Full-Finetune performance on average. Notably, when selecting 300K samples (less than half of the full dataset), ScalSelect attains performance that is nearly on par with Full-Finetune, achieving 99.66% relative performance. With 400K selected samples, ScalSelect surpasses Full-Finetune with relative performance of 101.16%, and achieves comparable or better results on most benchmarks, demonstrating that effective data selection can mitigate redundancy and improve training efficiency. In addition, we observe an apparent trade-off between MME-P and MME-C. Specifically, as the number of selected samples grows from 50K to 300K, performance on MME-P consistently improves, while performance on MME-C shows mild decline. At 400K, the two metrics become more balanced. This observation indicates that different evaluation aspects may respond differently to the selection budget, highlighting the importance of balancing perceptual and cognitive performance."
        },
        {
            "title": "5.3 Extended Results\nPerformance Across Different Models. Table 3 presents the\nperformance of ScalSelect with 100K selected samples on LLaVA-V-\n625K across different model architectures, including LLaVA-Vicuna-\n7B, Qwen3-VL-4B-Instruct, and Qwen3-VL-8B-Instruct. Compared\nto LLaVA-Vicuna-7B, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-\nInstruct exhibit stronger performance under the 100K selection\nbudget, both slightly exceeding Full-Finetune. This observation",
            "content": "6 Model Method #Samples MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. LLaVA-Vicuna-7B Qwen3-VL-4B-Instruct Qwen3-VL-8B-Instruct Full-Finetune ScalSelect Full-Finetune ScalSelect Full-Finetune ScalSelect 625K 100K 625K 100K 625K 100K 63.96 59.19 82.06 84.30 84.30 82.46 56.73 52. 71.47 81.39 82.74 82.06 1463.87 1400.34 1589.11 1652.75 1700.21 1709.07 278.57 308. 651.43 658.93 676.43 701.43 53.79 51.75 78.95 79.92 80.47 81.83 85.24 83. 88.37 88.17 88.54 87.97 93.05 94.73 97.03 96.86 96.39 94.39 67.63 65. 87.26 91.03 88.40 92.12 20.30 19.40 69.10 75.40 66.00 77.30 100.00% 97.85% 100.00% 104.00% 100.00% 102.39% Table 3: Performance across different models. Results are reported on LLaVA-V-625K with LLaVA-Vicuna-7B, Qwen3-VL-4BInstruct, and Qwen3-VL-8B-Instruct under the same selection budget of 100K samples. Dataset Method #Samples MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. LLaVA-V-625K LRV-Sub-180K Full-Finetune Random ScalSelect Full-Finetune Random ScalSelect 625K 100K 100K 180K 29K 29K 63.96 57.96 59.19 34.19 35.98 36.30 56.73 51.74 52.80 22.20 20.75 21.69 1463.87 1418.85 1400. 1082.21 848.76 840.59 278.57 295.36 308.57 258.93 233.93 281.79 53.79 50.78 51.75 40.97 41.32 43.39 85.24 84.96 83. 75.36 60.18 65.04 93.05 90.71 94.73 69.13 62.37 62.69 67.63 65.20 65.29 58.60 58.06 60.78 20.30 17.90 19. 16.50 16.50 16.60 100.00% 95.66% 97.85% 100.00% 93.05% 97.51% Table 4: Performance across different datasets. Results are reported with LLaVA-Vicuna-7B on LLaVA-V-625K and LRV-Sub-180K under the same selection budget of 16%. Method 625K 100K 300K 400K Train projector Freeze projector 100.00% 100.00% 97.85% 98.30% 99.66% 99.06% 101.16% 100.20% Table 5: Performance under different projector freezing settings. Target model is LLaVA-Vicuna-7B. All settings freeze the visual encoder. Freeze Projector fine-tunes only the LLM, while Train Projector fine-tunes both the LLM and the projector. suggests that more capable models may be more sensitive to data redundancy and can benefit more from high-quality and informative subsets selected by ScalSelect. Performance Across Different Datasets. Table 4 presents the performance of ScalSelect with LLaVA-Vicuna-7B across different datasets, including LLaVA-V-625K and LRV-Sub-180K, under the same selection budget of 16%. On both datasets, ScalSelect consistently outperforms Random sampling and achieves performance close to Full-Finetune. Notably, on LRV-Sub-180K, ScalSelect with only 29K selected samples attains 97.51% of the Full-Finetune performance, surpassing Random sampling by clear margin. These results suggest that ScalSelect generalizes well across datasets with different instruction styles and data distributions. Performance Under Different Projector Freezing Settings. Table 5 compares ScalSelect under two commonly used visual instruction tuning paradigms: freezing the projector and fine-tuning only the LLM, or fine-tuning both the LLM and the projector. Overall, the performance differences between the two settings are relatively small across different selection budgets. This indicates that ScalSelect is robust to different projector freezing strategies and can be effectively applied under both mainstream visual instruction tuning paradigms."
        },
        {
            "title": "5.4 Ablation and Further Analysis\nInfluence of Instruction-Conditioning. We analyze the influ-\nence of instruction conditioning by comparing ScalSelect with\nScalSelect-NoInsCon, where all visual tokens are selected without\nconditioning on instruction-to-vision attention signals. As shown in\nTable 6, removing instruction conditioning leads to an overall degra-\ndation in performance, suggesting that incorporating instruction-\nrelevant attention cues is beneficial for constructing effective sam-\nple representations. In particular, methods that incorporate instruc-\ntion conditioning substantially outperform ScalSelect-NoInsCon on\nOCRBench, highlighting the importance of instruction-conditioned\ncues for tasks that require focusing on localized and instruction-\nrelevant visual regions. By selecting visual tokens that receive\nhigh attention from text instructions, ScalSelect helps emphasize\ninstruction-relevant visual regions while reducing the influence of\ntask-irrelevant areas. This effect is further reflected in the learned\nsubspace dimensionality ğ‘˜. ScalSelect-NoInsCon results in a larger\nğ‘˜ value, which is consistent with more redundant information of\nits representation space.",
            "content": "Influence of Column-Wise Centering. We further investigate the impact of column-wise centering by comparing ScalSelect with ScalSelect-NoCenter, where centering is not applied before subspace decomposition. As shown in Table 6, removing column-wise centering leads to significant performance degradation, with the selected subspace dimensionality collapsing to ğ‘˜ = 1. This behavior suggests that, without centering, the mean component of the representation matrix dominates the variance structure. As result, the leading singular vector primarily captures the global mean of the representations, while meaningful variations among samples are largely suppressed. Consequently, the principal components tend to be absorbed by the mean, effectively flattening the representation space and diminishing sample diversity. By performing columnwise centering, ScalSelect removes the global mean effect and allows the subspace decomposition to focus on variance-driven directions Method ScalSelect (Ours) ScalSelect-NoInsCon ScalSelect-NoCenter 9 18 1 MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. 59.19 61.72 55.21 52.80 55.44 48.04 1400.34 1394.02 1356.01 308.57 288.57 270.00 51.75 50.84 51.18 83.96 84.82 84. 94.73 92.24 91.10 65.29 64.40 66.29 19.40 17.90 19.30 100.00% 98.88% 96.09% Table 6: Influence of instruction-conditioning & Influence of column-wise centering. ScalSelect-NoInsCon removes instruction conditioning by selecting all visual tokens without conditioning on instruction-to-vision attention signals. ScalSelect-NoCenter disables column-wise centering before subspace decomposition. ğ‘˜ denotes the selected subspace dimensionality as described in Section 3. Method MMBench-En MMBench-Cn MME-P MME-C AI2D POPE-A POPE-P SQA-IMG OCRBench Rel. 9 59 260 First Layer (Ours) Middle Layer Deep Layer 308.57 260.00 284.29 Table 7: Influence of LLM Layer Selection. ğ‘˜ denotes the selected subspace dimensionality as described in Section 3. First Layer (Ours) uses the hidden states from the first transformer layer, Middle Layer uses the hidden states from middle transformer layer (the 16th layer), and Deep Layer uses the hidden states from the penultimate transformer layer. 100.00% 95.84% 98.37% 1400.34 1413.59 1357.62 19.40 18.70 19.60 94.73 94.43 90.87 83.96 82.11 85.02 52.80 50.45 52.86 65.29 63.06 64. 51.75 48.87 50.74 59.19 57.34 59.53 that reflect genuine differences among sample representations. This leads to richer and more expressive subspace with higher effective dimensionality, which in turn supports more reliable importance estimation and improved selection performance. Influence of LLM Layer Selection. We study the effect of LLM layer selection by constructing sample representations from different transformer layers, including the first layer, middle layer (the 16th layer), and the penultimate layer. For the deep setting, we use the penultimate layer instead of the final layer, as the last layer is typically more specialized toward output generation and may discard fine-grained representational information [9]. As shown in Table 7, using hidden states from the middle layer leads to the worst overall performance. This suggests that intermediate layers may place less emphasis on fine-grained visual grounding and instead focus more on abstract or mixed-modal representations, making them less suitable for sample representation in data selection. Using hidden states from the deep layer improves performance on OCRBench, even surpassing the first-layer setting. This observation suggests that later LLM layers may encode more detailed or layout-sensitive visual cues that are beneficial for OCR-related tasks, which is consistent with prior empirical findings [48, 49]. Overall, extracting hidden states from the first transformer layer yields the best and most balanced performance across benchmarks. We also observe that the selected subspace dimensionality ğ‘˜ increases substantially when using deeper LLM layers, indicating more complex and less compressible representation space compared to early layers. This result suggests that early LLM layers capture informative cross-modal interactions and provide stable and effective basis for sample representation and data selection. Probing the Distribution of Importance Scores. We further examine the distribution of the importance scores produced by ScalSelect. As shown in Figure 2, the scores exhibit highly skewed, long-tailed distribution: small fraction of samples contributes disproportionately to the dominant representation subspace, while the majority of samples have negligible leverage. In addition, the ranking curve exhibits sharp initial drop followed by long, Figure 2: Distribution of Importance Scores. Left: Histogram of importance scores, exhibiting highly skewed, long-tailed distribution. Right: Ranking curve of importance scores sorted in descending order, exhibiting sharp initial drop followed by long, gradually decaying tail. gradually decaying tail, indicating that few top-ranked samples dominate the contribution, while importance decreases steadily over the remaining samples. This suggests that ScalSelect supports stable and robust selection across wide range of selection budgets. This observation is consistent with the empirical trends reported in Section 5.2."
        },
        {
            "title": "6 Conclusion\nWe propose ScalSelect, a scalable and training-free multimodal\ndata selection method for efficient visual instruction tuning. ScalS-\nelect leverages instruction-conditioned early representations and\nsubspace-aware global selection to preserve the dominant structure\nof the full dataset without pairwise comparisons or auxiliary mod-\nels. Extensive experiments across multiple models, datasets, and\nselection budgets demonstrate that ScalSelect retains most of the\nperformance of full-data training using only a small fraction of the\ndata, and even surpasses it in some settings. We believe ScalSelect\noffers a practical and principled solution for large-scale multimodal\ndata selection and opens new directions for subspace-oriented data\nselection in vision-language learning.",
            "content": "8 References [1] HervÃ© Abdi and Lynne Williams. 2010. Principal component analysis. Wiley interdisciplinary reviews: computational statistics 2, 4 (2010), 433459. [2] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. 2025. Qwen3-VL Technical Report. arXiv:2511.21631 [cs.CV] https://arxiv.org/abs/2511.21631 [3] Jinhe Bi, Yifan Wang, Danqi Yan, Wenke Huang, Zengjie Jin, Xiaowen Ma, Artur Hecker, Mang Ye, Xun Xiao, Hinrich Schuetze, et al. 2025. Prism: Self-pruning intrinsic selection method for training-free multimodal data selection. arXiv preprint arXiv:2502.12119 (2025). [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision. Springer, 370 387. [5] Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, and Heng Huang. 2024. Your vision-language model itself is strong filter: Towards high-quality instruction tuning with data selection. arXiv preprint arXiv:2402.12501 (2024). [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271 (2024). [7] Petros Drineas, Michael Mahoney, and Shan Muthukrishnan. 2008. Relativeerror CUR matrix decompositions. SIAM J. Matrix Anal. Appl. 30, 2 (2008), 844881. [8] Carl Eckart and Gale Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika 1, 3 (1936), 211218. [9] Kawin Ethayarajh. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. arXiv preprint arXiv:1909.00512 (2019). [10] Qianhan Feng, Wenshuo Li, Tong Lin, and Xinghao Chen. 2025. Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement. In Proceedings of the Computer Vision and Pattern Recognition Conference. 41784188. [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2025. Mme: comprehensive evaluation benchmark for multimodal large language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [12] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. 2024. Exploring the frontier of vision-language models: survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214 (2024). [13] Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp. 2011. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review 53, 2 (2011), 217288. [14] Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, and Pradeep Dasigi. 2025. Large-Scale Data Selection for Instruction Tuning. arXiv preprint arXiv:2503.01807 (2025). [15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and visionlanguage representation learning with noisy text supervision. In International conference on machine learning. PMLR, 49044916. [16] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. diagram is worth dozen images. In European conference on computer vision. Springer, 235251. [17] Jaewoo Lee, Boyang Li, and Sung Ju Hwang. 2024. Concept-skill transferabilityarXiv preprint based data selection for large vision-language models. arXiv:2406.10995 (2024). [18] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems 36 (2023), 2854128564. [19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023). [20] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. 2025. Survey of State of the Art Large Vision Language Models: Benchmark Evaluations and Challenges. In Proceedings of the Computer Vision and Pattern Recognition Conference. 15871606. [21] Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, and Kai Chen. 2025. Euclids Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks. arXiv preprint arXiv:2509.24473 (2025). [22] Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, et al. 2025. PhysBrain: Human Egocentric Data as Bridge from Vision Language Models to Physical Intelligence. arXiv preprint arXiv:2512.16793 (2025). [23] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. 2025. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 53345342. [24] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565 (2023). [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2629626306. [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llavanext: Improved reasoning, ocr, and world knowledge. [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems 36 (2023), 3489234916. [28] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024. Mmbench: Is your multi-modal model an all-around player?. In European conference on computer vision. Springer, 216233. [29] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. 2024. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences 67, 12 (2024), 220102. [30] Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, and Min Zhang. 2025. Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents. arXiv preprint arXiv:2502.19917 (2025). [31] Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao, Yu Li, Mengzhang Cai, Yun Zhu, Zhanping Zhong, Qizhi Pei, et al. 2026. ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch. arXiv preprint arXiv:2601.13606 (2026). [32] Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, and Ji-Rong Wen. 2025. Less is more: High-value data selection for visual instruction tuning. In Proceedings of the 33rd ACM International Conference on Multimedia. 37123721. [33] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS). [34] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Jiayi Ji, Jie Lou, Debing Zhang, and Rongrong Ji. 2025. Mllm-selector: Necessity and diversity-driven high-value data selection for enhanced visual instruction tuning. arXiv preprint arXiv:2503.20502 (2025). [35] Adyasha Maharana, Jaehong Yoon, Tianlong Chen, and Mohit Bansal. 2024. Adapt-: Scalable continual multimodal instruction tuning via dynamic data selection. arXiv preprint arXiv:2410.10636 (2024). [36] Michael Mahoney et al. 2011. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning 3, 2 (2011), 123224. [37] Michael Mahoney and Petros Drineas. 2009. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences 106, 3 (2009), 697702. [38] Max Marion, Ahmet ÃœstÃ¼n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564 (2023). [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. [40] Gaurav Shinde, Anuradha Ravi, Emon Dey, Shadman Sakib, Milind Rampure, and Nirmalya Roy. 2025. Survey on Efficient Vision-Language Models. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 15, 3 (2025), e70036. [41] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. 2025. Transformer layers as painters. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 2521925227. [42] Hao Tang, Siyue Yu, Jian Pang, and Bingfeng Zhang. 2025. Training-free Synthetic Data Selection Method for Semantic Segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 72297237. [43] Michael Wall, Andreas Rechtsteiner, and Luis Rocha. 2003. Singular value decomposition and principal component analysis. In practical approach to microarray data analysis. Springer, 91109. [44] Haoran Wei, Yaofeng Sun, and Yukun Li. 2025. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234 (2025). [45] Changti Wu, Shijie Lian, Zihao Liu, Lei Zhang, Laurence Tianruo Yang, and Kai Chen. 2025. DynaSolidGeo: Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry. arXiv preprint arXiv:2510.22340 (2025). [46] Xindi Wu, Mengzhou Xia, Rulin Shao, Zhiwei Deng, Pang Wei Koh, and Olga Russakovsky. 2024. Icons: Influence consensus for vision-language data selection. arXiv preprint arXiv:2501.00654 (2024). [47] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. 2024. Deepseekvl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302 (2024). [48] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 11921200. [49] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2021. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 25792591. [50] Suorong Yang, Peng Ye, Wanli Ouyang, Dongzhan Zhou, and Furao Shen. 2024. clip-powered framework for robust and generalizable data selection. arXiv preprint arXiv:2410.11215 (2024). [51] Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Bosheng Qin, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, and Yueting Zhuang. 2025. Mastering collaborative multi-modal data selection: focus on informativeness, uniqueness, and representativeness. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 155165. [52] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024. Vision-language IEEE transactions on pattern analysis and models for vision tasks: survey. machine intelligence 46, 8 (2024), 56255644. [53] Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. 2025. Cross-modal information flow in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1978119791. [54] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. 2023. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087 (2023). [55] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023). Experiment Details A.1 Settings of Extracting Representation For Instruction-Conditioned Early Representation, we perform inference using the target VLM with maximum context length (max_len) of 4096 tokens. For LLaVA-Vicuna-7B, input images are processed following the default configuration of LLaVA-1.5, with fixed resolution of 336 336. For Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, input images are resized such that the maximum number of visual tokens does not exceed 576, with patch resolution of 32 32. A.2 Training Hyperparameters All experiments are conducted on 8 NVIDIA H100 GPUs (80GB) using LLaMA-Factory. The detailed training hyperparameters are summarized in Table 8. The same training configuration is used across all models, datasets, and selection budgets to ensure fair comparison. Hyperparameter Value Batch size per device Gradient accumulation steps Learning rate (LR) LR schedule LR warmup ratio Weight decay Epoch DeepSpeed stage Cut length 4 2 2e-5 Cosine decay 0.03 0 1 3 Table 8: Training Hyperparameters. Sensitivity of the Attention-Mass Threshold We further analyze the effect of the attention-mass threshold ğœ used in the Instruction-Conditioned Early Representation. Table 9 reports the average retained visual token ratio and the overall relative performance under different values of ğœ {0.85, 0.90, 0.95}, conducted on LLaVA-Vicuna-7B using the LLaVA-V-625K dataset. As ğœ increases, the proportion of retained visual tokens grows gradually, resulting in diminishing marginal gains in token reduction. Meanwhile, the overall relative performance remains stable across this range, with only minor variations. These results indicate that the attention-mass threshold primarily affects representation compactness rather than introducing strong performance sensitivity. Based on this observation, we fix ğœ = 0.9 as reasonable default choice that balances visual coverage and representation compactness throughout this work. ğœ 0.85 0.90 0.95 Retained visual token ratio Rel. 77.8% 84.6% 92.0% 99.26% 100.00% 99.33% Table 9: Retained visual token ratio and overall relative performance under different attention-mass thresholds ğœ. A.3 Settings of Baselines All baseline methods included in our experiments are applied by following the original experimental settings and implementation details described in their respective papers."
        },
        {
            "title": "ScalSelect",
            "content": "We analyze the subspace approximation property of the selected subset in ScalSelect. All notations follow Section 3 of the main paper. Proposition (Relative-Error Subspace Approximation, Sampling Reference) Let Xğ‘˜ denote the best rank-ğ‘˜ approximation of the full representation matrix Xğ‘ . Let Xğ‘† be the submatrix formed by the samples selected according to leverage scores. If samples are selected via randomized leverage-score sampling, then, with probability at least 99%: Xğ‘ ğ‘ƒXğ‘† Xğ‘ ğ¹ (1 + ğœ€) Xğ‘ Xğ‘˜ ğ¹ , (10) where ğ‘ƒXğ‘† denotes the orthogonal projection onto the row space of Xğ‘† , and ğœ€ (0, 1) is an accuracy parameter controlling the approximation slack. with the standard ğ‘‚ (ğ‘˜ log ğ‘˜/ğœ€2) oversampling rate) yields, with probability at least 99% [7, 37]: ğ´ ğ‘ƒ Cğ´ğ¹ (1 + ğœ€) ğ´ ğ´ğ‘˜ ğ¹ . Finally, mapping back to our notation, the sampled columns of ğ´ = ğ‘‹ ğ‘ correspond to the sampled rows forming ğ‘‹ğ‘† , and ğ‘ƒ corresponds to ğ‘ƒğ‘‹ğ‘† . Using the invariance of the Frobenius norm under transposition, we obtain (11) ğ‘‹ğ‘ ğ‘ƒğ‘‹ğ‘† ğ‘‹ğ‘ ğ¹ = ğ´ (ğ‘ƒ Cğ´) ğ¹ = ğ´ ğ‘ƒ Cğ´ğ¹ (1 + ğœ€) ğ´ ğ´ğ‘˜ ğ¹ = (1 + ğœ€) ğ‘‹ğ‘ ğ‘‹ğ‘˜ ğ¹ , (12) which completes the proof. Proof. We adapt the relative-error analysis for leverage-score (subspace) sampling in CUR/CX decompositions [7, 37]. Let ğ´ = ğ‘ Rğ‘‘ ğ‘ . Selecting subset of samples (rows) from ğ‘‹ğ‘ is equivğ‘‹ alent to selecting the corresponding subset of columns from ğ´. Let ğ´ğ‘˜ be the best rank-ğ‘˜ approximation of ğ´, and let ğ‘ƒ denote the orthogonal projector onto the span of the sampled columns of ğ´ (equivalently, the span of the sampled rows of ğ‘‹ğ‘ ). By leveragescore sampling (i.e., sampling proportional to the squared â„“2-norms of the rows of the top-ğ‘˜ right singular vector matrix of ğ´), the relative-error CX guarantee states that, for any accuracy parameter ğœ€ (0, 1), sampling sufficiently large number of columns (e.g., Remark on Deterministic Selection. The relative-error bound stated above follows the classical analysis of randomized leverage-score sampling in CUR/CX decompositions [7, 37], and serves as theoretical reference for the design of ScalSelect. In practice, leverage scores are commonly used as deterministic importance measure, and we accordingly adopt deterministic top-leverage selection strategy by selecting the highest-scoring samples, well-established heuristic motivated by leverage-score sampling theory that improves stability by removing randomness [7, 36]. The above result provides theoretical motivation for using leverage scores as principled criterion for sample selection."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Harbin Institute of Technology",
        "Huazhong University of Science and Technology",
        "The Hong Kong Polytechnic University",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Zhongguancun Academy",
        "Zhongguancun Institute of Artificial Intelligence"
    ]
}