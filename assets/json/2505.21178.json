{
    "paper_title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
    "authors": [
        "Mingyang Song",
        "Mao Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the \"walk before you run\" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks."
        },
        {
            "title": "Start",
            "content": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning Mingyang Song, Mao Zheng Tencent Hunyuan nickmysong@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "As test-time scaling becomes pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the models reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of sample are correct, following the \"walk before you run\" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks. The code, training dataset, and model checkpoints will be publicly released1. 5 2 0 2 7 2 ] . [ 1 8 7 1 1 2 . 5 0 5 2 : r Figure 1: detailed evaluation of accuracy and response length throughout the training steps. 1https://github.com/nick7nlp/ConciseR Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Test-time scaling [1, 2] has demonstrated robust correlation between extending the generation length of Chain-of-Thought (CoT) [3] and improving the reasoning capabilities of Large Language Models (LLMs). The advent of large reasoning models, such as GPT-o1 [4] and DeepSeek-R1 [5], represents significant breakthrough in natural language processing, especially in tackling complex and intricate reasoning tasks. An interesting phenomenon observed during reinforcement learning post-training via Group Relative Policy Optimization (GRPO) [6] is the emergence of an \"aha moment\" [5], which refers to pivotal inflection point at which the model spontaneously initiates self-correction behaviors. These emergent behaviors develop autonomously through the models exploration of the solution space rather than through explicit programming. Prior research [7, 8] has found distinctive pattern following this moment: the response length of the model tends to increase significantly, accompanied by improvements in overall performance. Despite the lack of clear understanding of why this occurs, this phenomenon has led many researchers to advocate for longer responses, leveraging additional computational resources in the hope of further enhancing accuracy. However, generating excessively long CoT reasoning responses substantially increases computational overhead in both the model training and deployment phases. Furthermore, recent studies [9, 10] have discovered an intrinsic overthinking phenomenon in reasoning models, where these models persistently produce verbose rationalizations. This tendency manifests as the inclusion of irrelevant contextual information and unnecessary reflective behaviors. Such information and behaviors not only inefficiently consume computational resources but also compromise reasoning accuracy by causing models to deviate from valid logical pathways to incorrect conclusions. To address these issues, recent studies [8, 11, 12, 13, 14] are researching efficient reasoning methodologies based on the GRPO algorithm for training the model to produce more concise CoT responses, and have discovered trade-off between the CoT response length and model reasoning capabilities in most cases, i.e., the shorter the length, the worse the performance. It is understandable that achieving efficient reasoning, improving ability via more concise CoT, is inherently more challenging. This is in contrast to boosting performance by merely increasing the response length, as the former requires significantly higher model capabilities. Therefore, we highlight the critical importance of the timing for optimizing response length when training using GRPO-based algorithms. Adhering to the \"walk before you run\" principle, we consider that during training, response length optimization is only enabled when all rollouts for training sample are correct. Motivated by this, we propose ConciseR, which is simple yet effective two-stage reinforcement learning framework for concise reasoning. Specifically, the first stage aims to enhance the models reasoning capabilities through group relative policy optimization with clip-higher and dynamic sampling techniques. Meanwhile, we introduce the entropy bonus in the first stage, which is used to encourage exploration in policy gradient to incentivize the reasoning capabilities of the model further. The second stage enforces conciseness and improves efficiency via length-aware group relative policy optimization (incorporating max response length to calculate the length reward). Experiments demonstrate that ConciseR is compatible with RL models that incentivize reasoning, achieving reductions in response length while improving accuracy across benchmarks such as AIME 2024, AMC 2023, MATH-500, Minerva, and Olympiad datasets. As shown in Figure 1, ConciseR gradually activates the reasoning ability of the model in the first stage, then rapidly compresses the CoT response length in the second stage to achieve concise reasoning. Notably, the reduction in response length has immediate implications for computational efficiency, resource utilization, and response time, making the approach practically appealing and cost-effective."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 Proximal Policy Optimization (PPO) Proximal Policy Optimization (PPO) [15] is one of the policy gradient methods that introduces clipped surrogate objective for policy optimization. By using clipping to constrain policy updates within proximal region of the previous policy, PPO stabilizes training and improves sample efficiency. 2 Specifically, PPO updates the policy by maximizing the following objective: JPPO(θ) = EqD,otπθold (q) min (cid:20) πθ(ot q, o<t) πθold (ot q, o<t) ˆAt, clip (cid:18) πθ(ot q, o<t) πθold (ot q, o<t) , 1 ε, 1 + ε (cid:19) (cid:21) , ˆAt (1) where πθold is the policy before the update, ε is the clipping hyper-parameter, indicates the question from the data distribution D, and ˆAt is an estimator of the advantage function of the t-th token. Here, standard and traditional way to estimate ˆAt is to compute the Generalized Advantage Estimation (GAE) [15] with learned value model. However, in the context of LLM RL scaling, learning the value model is computationally expensive, so methods that estimate ˆAt without learned value model are practically preferred. 2.2 Group Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO) [6] introduces policy gradient framework that eliminates the reliance on explicit value function by utilizing comparative advantage estimation within group of responses. This method samples multiple candidate outputs for each input question and computing advantages based on the relative rewards among these candidates within their respective groups. Specifically, GRPO first samples group of responses {o1, o2, . . . , oG} per question and computes their returns = {r1, r2, . . . , rG}, then calculates the advantage of the i-th response oi as, ˆAi = ri mean({r1, r2, , rG}) std({r1, r2, , rG}) . (2) Similar to PPO, GRPO uses clipped objective with KL penalty and optimizes the policy model πθ by maximizing the following objective: JGRPO(θ) =E qD,{oi}G i=1πθold (q) 1 (cid:88) i=1 (cid:110) min (cid:104) τi(θ) ˆAi, clip (τi(θ), 1 ε, 1 + ε) ˆAi (cid:105) βDKL[πθπref] (cid:111) , where τi = πθ(oiq) πθold(oiq) , DKL(πθπref) = πref(oiq) πθ(oiq) log πref(oiq) πθ(oiq) 1. (3) (4) Here, πref represents the reference model and the term DKL(πθπref) indicates KL penalty term to limit how much the trained model πθ can deviate from the reference model πref. 2.3 Decouple Clip and Dynamic Sampling Policy Optimization (DAPO) An in-depth analysis [12] reveals that the naive GRPO baseline suffers from several significant issues, such as entropy collapse, reward noise, and training instability. To address this issue, DAPO introduces four key techniques to make RL shine in the long-CoT RL scenario, including Clip-Higher, which enhances the diversity of the model and avoids entropy collapse. Dynamic Sampling, which improves training efficiency and stability. Token-Level Policy Gradient Loss, which plays crucial role in reinforcement learning scenarios involving Long-CoT reasoning responses. Overlong Reward Shaping, length-aware penalty mechanism designed to shape the reward for truncated samples to reduce reward noise and stabilize training. Similar to GRPO, DAPO estimates the advantage in group-relative manner and optimizes the policy model via the following objective, JDAPO(θ) = qD,{oi}G i=1πθold (q) 1 (cid:88) i=1 (cid:110) min (cid:104) τi(θ) ˆAi, clip (τi(θ), 1 εl, 1 + εh) ˆAi (cid:105)(cid:111) (5) s.t. 0 < {oiis_equivalent(oi, a)} < G, where εl and εh indicate the lower and higher clipping range."
        },
        {
            "title": "3 Methodology",
            "content": "Our primary goal is to let LLM generate more concise CoT response without sacrificing the models performance. To this end, we propose novel two-stage reinforcement learning training paradigm guided by the principle of \"Aim for 100% accuracy first; speed comes with mastery.\" Specifically, the first stage aims to incentivize the reasoning capabilities of the base model via group relative policy optimization with clip-higher and dynamic sampling, thus ensuring accuracy and robust model reasoning. Subsequently, the second stage explicitly enforces conciseness and improved efficiency via length-aware group relative policy optimization, aligning with our objective of achieving mastery through precision, where conciseness naturally follows from reliable and accurate reasoning. 3.1 Group Relative Policy Optimization with Clip-Higher and Dynamic Sampling (GRPO++) In the first stage, the model is trained to incentivize the reasoning capabilities, which aims to enhance the models problem-solving capacity, with an expected increase in response length as GRPO mostly encounters negative rewards and encourages the trained model toward longer responses. To this end, in this paper, we adopt GRPO with two key components of DAPO, clip higher and dynamic sampling, and further introduce an entropy bonus to encourage greater exploration capability in the model, named GRPO++. Similar to the original approach, GRPO++ estimates the advantage in group-relative manner and optimizes the policy model using the following objective: JGRPO++(θ) = qD,{oi}G i=1πθold (q) 1 (cid:88) i=1 (cid:110) min (cid:104) τi(θ) ˆAi, clip (τi(θ), 1 εl, 1 + εh) ˆAi (cid:105) + αH(πθ) (cid:111) , (6) s.t. where αH(πθ) denotes the entropy bonus. 0 < {oiis_equivalent(oi, a)} < G, Table 1: Analyze the character-level output length and the frequency of reflections of correct and incorrect responses for DEEPSEEK-R1-DISTILL-QWEN-1.5B and -7B on AIME 2024. Model # Average Output Length TOTAL CORRECT INCORRECT # Average Frequency of \"Wait\" and \"wait\" TOTAL CORRECT INCORRECT DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-7B 43176 34193 21859 52629 52376 109 75 49 35 138 124 Figure 2: Distribution of average length for correct and incorrect answers under the same question. 3.2 Length-Aware Group Relative Policy Optimization (L-GRPO) Recent studies [8, 11] have found that the reasoning response length is not strongly correlated with the correctness of the answer; that is, long CoT reasoning response does not necessarily represent 4 Table 2: Training Template. {question} will be replaced with the specific question during training. conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Please reason step by step, and put your final answer within boxed{}. User: {question} Assistant: correct result, and short CoT reasoning response does not necessarily represent an incorrect one. On the contrary, the correct CoT reasoning responses are usually shorter in length, while incorrect reasoning responses tend to be longer. Therefore, we first analyze the response lengths of DEEPSEEKR1-DISTILL-QWEN-1.5B and -7B, as shown in Table 1 and Figure 2. Table 1 shows that incorrect responses are obviously longer and include more detailed reasoning processes. Simultaneously, to differentiate whether the incorrect results all stemmed from complex problems, thereby causing the CoT for incorrect responses to be longer in the statistical results, we analyze the models responses at the sample level, as shown in Figure 2. Interestingly, in most cases, for the same question, correct responses are still shorter. Furthermore, we conduct deeper analysis of the correct responses to the same question and find that correct answers may exhibit excessive reflection, which leads to long CoT, resulting in longer CoT responses, as shown in Figure 4. Based on the above analysis, we reshape the reward function in GRPO. When the models rollout results for question are all correct, we further optimize the models reasoning length for that question by using the remaining maximum response length as reward (under the specified context length, the more remaining context length, the higher the reward), as calculated below. Ai = ˆri mean({ˆr1, ˆr2, , ˆrG}) std({ˆr1, ˆr2, , ˆrG}) , ˆri = ri + λ ˆLi, ˆLi = (cid:40) 1 Li LMax 0, , if (cid:80)G if (cid:80)G i=1 ri = i=1 ri = (7) where Li is the length of the i-th response and LMax indicates the max response length. Then, we optimize the policy model using the following objective: JL-GRPO(θ) = qD,{oi}G i=1πθold (q) 1 (cid:88) i=1 (cid:110) min (cid:104) τi(θ) ˆAi, clip (τi(θ), 1 εl, 1 + εh) ˆAi (cid:105) βDKL[πθπref] + αH(πθ) (cid:111) , (8) s.t. 0 < {oiis_equivalent(oi, a)}. 3.3 Rule-based Reward Model Using trained reward model typically introduces the issue of reward hacking [2429]. To mitigate this issue, we directly adopt the final accuracy from verifiable task as the outcome reward, calculated according to the following rule: ri(oi, a) = (cid:26)1, 0, if is_equivalent(oi, a) if not is_equivalent(oi, a) (9) where indicates the ground-truth answer and oi contains the predicted answer. Additionally, it is important to note that the trained model must adhere strictly to the training prompt by generating the chain-of-thought within the <think></think> tags and subsequently presenting the final answer within the <answer></answer> tags with the boxed tag. 3.4 Training Dataset Curation To select and curate high-quality data for scaling RL, we include challenging problems from DeepScaleR [8], DAPO-Math-17K [12], and MATH [16] to enhance problem difficulty and diversity in our data mixture: 5 Table 3: Overall performance on five competition-level reasoning benchmarks. Our models outperform prior state-of-the-art approaches with zero RL paradigm. indicate the results from [20]. Model Qwen2.5-1.5B-Base [21] Qwen2.5-1.5B-Instruct [21] Qwen2.5-Math-1.5B-Base [22] Qwen2.5-Math-1.5B-Instruct [22] DeepSeek-R1-Distill-Qwen-1.5B [5] DeepScaleR-1.5B-Preview [8] FastCuRL-1.5B-Preview [11] FastCuRL-1.5B-V3 [11] Qwen2.5-7B-Base [21] Qwen2.5-7B-Instruct [21] Qwen2.5-Math-7B-Base [22] Qwen2.5-Math-7B-Instruct [22] Eurus-2-7B-PRIME [23] Open-Reasoner-Zero-7B [24] SimpleRL-Zero-7B [25] SimpleRL-Zero-Math-7B [25] Oat-Zero-7B [13] ConciseR-Zero-7B-Preview (Stage-1) ConciseR-Zero-7B (Stage-2) AIME 2024 MATH-500 AMC 2023 Minerva Olympiad Avg. Score 0.0 1.3 11.3 12.0 28.8 43.1 43.1 49.6 3.3 12.3 20.7 15.7 17.8 19.7 14.0 22.7 28.0 42.8 43.3 3.3 57.5 51.7 74.7 82.8 87.8 88.0 90.5 64.6 77.1 64.3 82.9 80.1 83.9 77.9 76.9 79. 83.0 83.0 2.5 26.2 44.0 26.7 62.9 73.6 74.2 78.5 30.0 52.8 56.2 67.0 63.0 59.5 58.0 62.2 66.2 73.9 76.7 1.8 19.4 11.3 35.0 26.5 30.2 31.6 34.7 25.7 34.9 17.3 35.0 37.5 31.6 33.0 30.1 34. 31.8 31.5 1.5 20.3 26.0 37.9 43.3 50.0 50.4 54.5 29.0 38.7 29.0 41.3 43.9 47.6 39.0 39.3 43.8 45.1 46.0 1.82 24.9 28.9 37.3 48.9 56.9 57.5 61.6 30.5 43.2 37.5 48.4 48.5 48.5 44.4 46.2 50. 55.3 56.1 Figure 3: detailed evaluation of accuracy and response length throughout the training steps. DeepScaleR2, which contains approximately 40K unique mathematics-specific problem-answer pairs collected from AIME (1984-2023), AMC (prior to 2023), Omni-MATH, and Still datasets [17, 18, 19]. DAPO-Math-17K3, which contains approximately 17K problem-answer pairs, each paired with an integer as the answer. DAPO-Math-17K was compiled from the Art of Problem Solving (AoPS4) website and official competition websites using combination of web scraping and manual annotation. MATH5 (Level 3-5), which contains approximately 8K problem-answer pairs. Each problem has step-by-step solution which can be used to teach models to generate explanations. After obtaining the above datasets, we employ Math-Verify6 to re-extract answers from the provided textual solutions, selecting only those cases where the extracted answer matches the corresponding answer in the dataset. We discard any samples that are empty, incomplete, or duplicates. Finally, we obtain approximately 59K reasoning problems as the training dataset. It should be noted that in the first stage, we use the 59K data to incentivize the models reasoning ability. Still, in the second stage, we use the MATH (Level 3-5) data as the training set to optimize the models reasoning length. 2https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset 3https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k 4https://artofproblemsolving.com/ 5https://huggingface.co/datasets/EleutherAI/hendrycks_math 6https://github.com/huggingface/Math-Verify 6 Figure 4: Comparison of responses of ConciseR-Zero-7B-Preview to the same question."
        },
        {
            "title": "4 Experiments",
            "content": "Training Details. In this paper, we train our models using the verl7 framework [26] and leverage Qwen2.5-Math-7B [22] as the base model. During training, we utilize the Adam [39] optimizer with constant learning rate of 1 106. We leverage batch size of 128 with each question generating 32 rollouts, the maximum response length is set to 3,072 tokens, and training is conducted using mini-batches of size 128. As for the Clip-Higher, similar to the prior work [12], we set the clipping parameter εl to 0.2 and εh to 0.28, which effectively balance the trade-off between exploration and exploitation for RL. Specifically, for GRPO++, we set the entropy coefficient α to 0.001. For L-GRPO, we set the KL penalty coefficient β to 0.01 and set the λ to 0.000002. Evaluation Benchmarks. Similar to the prior work [13, 11], the performance of our models is evaluated on diverse suite of competition-level benchmarks including AIME 20248 (comprises 30 challenge problems), AMC 20239 (contains 40 mathematical problems, covering algebra, geometry, number theory, and combinatorics), Minerva Math [17], MATH-500 [16] (is challenging benchmark comprising competition-level problems), and OlympaidBench [27]. Evaluation Setup. In this paper, our two-stage RL training framework aims to enhance the reasoning performance while reducing the response length, thereby enabling more concise reasoning. To this end, we adopt the Pass@k evaluation metric, reporting Pass@1 accuracy computed with non-zero sampling temperature. Therefore, we set the maximum response length to 3,072 tokens. Specifically, we select temperature of 0.6 combined with top-p value of 0.95 to generate multiple responses (typically 32 samples) for each query. The used training template is shown in Figure 2. Baselines. We conduct comprehensive evaluations against several baselines with zero RL paradigm, including Qwen2.5 [21], Qwen2.5-Math [22], SimpleRL-Zero [25], Open-Reasoner-Zero-7B [24], Eurus-2-7B-PRIME [23], and Oat-Zero-7B [13]. Furthermore, we also present the results of the models after RL based on the DeepSeek-R1-Distill-Qwen-1.5B model [5], including DeepScaleR1.5B-Preview [8], FastCuRL-1.5B-Preview, and FastCuRL-1.5B-V3 [11]. 4.1 Main Results The experimental results reported in Table 3 clearly demonstrate that our proposed model, ConciseR, significantly outperforms existing baselines with zero RL paradigm on five widely recognized reasoning benchmarks. Specifically, ConciseR achieves an average accuracy improvement of 55.2% compared to the base model, Qwen2.5-Math-7B. Meanwhile, our method, GRPO++, also consistently surpasses all baselines, showing superior overall performance averaged across the five benchmarks. Figure 3 illustrates changes in accuracy and response length during the training process of L-GRPO across the five benchmarks. As indicated, the average accuracy on each benchmark remains stable throughout training, without exhibiting any noticeable degradation. Interestingly, the average response 7https://github.com/volcengine/verl 8https://huggingface.co/datasets/AI-MO/aimo-validation-aime 9https://huggingface.co/datasets/AI-MO/aimo-validation-amc 7 length on each benchmark consistently decreases, with reductions of 21%, 22%, 20%, 22%, and 23% observed on AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks, respectively. This demonstrates that our training approach successfully maintains model accuracy while generating more concise and efficient responses. Figure 5: Count of keyword occurrences out of 14,022 responses (1558 questions 11 test times)."
        },
        {
            "title": "5 Discussions",
            "content": "5.1 Analysis of Changes in Reasoning Patterns Inspired by prior work that observes the models reflective behavior by constructing keyword pool, we have built carefully selected keyword pool to observe changes in the thinking patterns of the responses during training. In our experiment, the keyword pool is limited to: check, rethink, reassess, evaluate, re-evaluate, evaluation, examine, however, reconsider, analyze, double-check, check again, recheck, verify, and wait. Then, we present the occurrences of various keywords in the responses generated by different training stages and steps in Figure 5 and Figure 6. Interestingly, when comparing the first and second stages, the frequency with which the model uses code to verify results has significantly increased (as reflected in the frequency of the keyword \"python\"). The model may have discovered that verifying results by writing code is more efficient. Meanwhile, keywords like \"re-check\" have decreased relatively, and other keywords have remained unchanged. 5.2 Case Study An interesting observation is that python code is used for verification during mathematical problem solving, e.g., Questions (a) and (b) in Figure 7. Specifically, for Question (a), the model utilizes program code to calculate the answer. For Question (b), the model first presents the solution process through mathematical reasoning and then spontaneously writes program code to verify the correctness of the approach. Such cases illustrate how models employ procedural reasoning to self-correct and engage in subsequent attempts. 5.3 Failure Experience In this section, we discuss our failure experiences in reward shaping. These experiments could also be regarded as an ablation study in L-GRPO. During the initial design of L-GRPO, we consider directly comparing the generation length of samples within group, assigning higher rewards to samples with relatively shorter CoT reasoning responses. We then combine the length score with the accuracy reward to encourage the trained model to obtain correct answers through shorter CoT reasoning responses, as illustrated by the following two equations, ˆri = ri + λ ˆLi, ˆLi = (cid:40) Max({L1,L2,...,LG})Li Max({L1,L2,...,LG})Min({L1,L2,...,LG}) , 0, if (cid:80)G if (cid:80)G i=1 ri = i=1 ri = , (10) 8 Figure 6: Count of keyword occurrences out of 15,580 responses (1558 questions 11 test times). Figure 7: Illustration of cases. ˆri = Gri=0 1 Gri=0 ri + ˆLi (cid:80)G i=1 , ˆLi (cid:40) ˆLi = 0, 1 Li (cid:80)G i=1 Li , if is_equivalent(oi, a) if not is_equivalent(oi, a) . (11) However, we find that this direct rewarding easily causes the model to skip the reasoning process and immediately start guessing answers, manifesting as an empty reasoning response within the <think></think> tags while directly outputting the final answer within the <answer></answer> tags. On the contrary, indirectly using the maximum context length to design the reward function can, to some extent, avoid the issues mentioned above."
        },
        {
            "title": "6 Related Work",
            "content": "Recent advances in reinforcement learning have significantly enhanced the reasoning capabilities of large language models. pivotal development in this domain is OpenAIs o1 [4], which leverages large-scale RL training to promote CoT reasoning. This approach has resulted in notable improvements in complex mathematics and coding benchmarks. DeepSeek-R1 [5] demonstrates that pure RL 9 post-training via GRPO, without the need for supervised warm-up, can directly induce robust reasoning abilities. Remarkably, this kind of method not only achieves performance competitive with o1 but also exhibits emergent behaviors such as self-verification and multi-step planning. This paradigm shift significantly reduces memory and computational overhead compared to earlier GRPO implementations [24, 25, 28], all while maintaining competitive performance levels. Recent algorithmic variants have focused on enhancing training efficiency [8, 9, 11, 12, 13, 14, 25, 29], yet they preserve GRPOs core methodology of parallel CoT sampling across groups. These advancements collectively contribute to more efficient and robust training methodologies for LLMs, thereby enhancing their reasoning capabilities and performance on complex tasks."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose ConciseR, which introduces simple yet effective two-stage reinforcement learning framework. First, it incentivizes the models reasoning capabilities via GRPO++, and then it reduces the models response length to improve the quality of the CoT response implicitly via L-GRPO. Importantly, we innovatively propose that during training, response length optimization is only triggered when all rollouts for given training sample are correct. This embodies the \"walk before you run\" principle. Experiments demonstrate that ConciseR consistently achieves the best efficiency-accuracy synergistic improvement, significantly outperforming existing efficient reasoning methods across five benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. [2] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [4] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [7] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. [8] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://github.com/ agentica-project/deepscaler, 2025. Notion Blog. [9] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [10] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks, 2025. [11] Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models, 2025. [12] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [13] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [14] Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning via reinforcement learning. arXiv preprint arXiv:2504.05185, 2025. [15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 11 [17] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [18] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. CoRR, abs/2410.07985, 2024. [19] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. [20] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility, 2025. [21] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [22] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [23] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [24] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [25] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. [26] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, page 12791297. ACM, March 2025. [27] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [28] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [29] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan"
    ]
}