{
    "paper_title": "CauSight: Learning to Supersense for Visual Causal Discovery",
    "authors": [
        "Yize Zhang",
        "Meiqi Chen",
        "Sirui Chen",
        "Bo Peng",
        "Yanxi Zhang",
        "Tianyu Li",
        "Chaochao Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 2 8 1 0 . 2 1 5 2 : r CauSight: Learning to Supersense for Visual Causal Discovery Yize Zhang1,2,3* Meiqi Chen4* Sirui Chen1,5* Bo Peng1,2,3* Yanxi Zhang1,4 Tianyu Li2 Chaochao Lu1 1Shanghai AI Laboratory 2Shanghai Innovation Institute 3Shanghai Jiao Tong University 4Peking University 5Tongji University {zhangyize, luchaochao}@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight. 1. Introduction Causal thinking is regarded as hallmark of human intelligence [41, 43, 46, 51]. Emulating this capacity on modern vision-language models (VLMs) [3, 6, 15, 29, 60, 63] is core to achieving not just recognition, but genuine reasoning about our visual world [5, 9, 22, 24, 26, 34, 58]. For instance, as Figure 1 shows, the underlying causal graph provides clear mechanistic narrative for an embodied agent tasked with retrieving laptop: the initial action must be to stabilize the glass, since directly removing the laptop would trigger causal chain reaction, collapsing the books and consequently dropping the glass. These *Equal contribution. Corresponding author. Figure 1. comparison between VLMs that understand (a) scene graph, which only specifies spatial relations between entities; (b) causal graph, which captures causal mechanisms between entities. Genuine reasoning and safe deployment require VLM to discover causal relations between entities. causal graphs [1, 20] could support logical, safe, and precise downstream decision-making with robust generalization, even under changing conditions [56]. To discover such causal graphs across diverse realworld visual scenarios, primary steps involve identifying causal entities and inferring their pairwise causal relationships. We follow the definition of causality in the visionlanguage context based on the concept of causal dispositions [14, 31, 33], to describe causal relationship as existing when one entity inherently possesses the ability to influence the state of another. This can be further illustrated through counterfactual reasoningif the cause were absent, the effect would not sustain its current state [17, 40]. We thus position visual causal discovery as the problem of grounding visual concepts in language through explicit reasoning about their causal relationships. However, this very task remains significant and ongoing challenge in the field. On one side, previous efforts have rarely focused on causal modeling of scenes in the vision-language context. The landscape of scene graph research [10, 21, 54] was oriented toward object detection and predicate recognition, remaining agnostic to the underlying causality. As illustrated in Figure 1, conventional scene graphs merely depict static adjacency or capture associative structures among entities, without causally modeling how changes in one affect another. This inherent limitation severely restricts their value for downstream reasoning and intervention tasks. In contrast, causal graphs explicitly reveal the chain of cause-andeffect relationships among entities, offering dynamic, interactive, and unambiguous representation of visual scenes. On the other side, even the most advanced VLMs currently struggle with the fundamental task of visual causal discovery [14]. It requires the model not only to identify entities and their attributes, but also to infer causal relationships via counterfactual reasoning, while eliminating confounders. Based on our extensive testing in section 5, the average recall rate of causal graphs for open-source models like Qwen2.5-VL series [4] is only 10.0%, even proprietary models such as OpenAI o3 [35] and GPT-4.1 [2] achieve merely 7.3% and 10.2%, respectively. Such performance hinders the practical use of causal graphs in applications such as robotic manipulation and autonomous driving that require safe and reliable decision-making [28, 52]. To address the aforementioned bottlenecks, we introduce the Visual Causal Graph dataset (VCG-32K), the first large-scale collection of over 32,000 images annotated with entity-level causal graphs. VCG-32K is built upon 11,428 samples from MS-COCO [30] and 20,828 from Objects365 [47], both widely serving as major sources for visual understanding. Guided by the definition of visual causality described above, we employ systematic annotation process to ensure consistency and reliability, enabling rigorous benchmarking on visual causal discovery. We further develop CauSight, novel VLM capable of generating underlying causal graphs across diverse visual scenarios through causally aware reasoning. Our training recipe consists of three components: (1) training data curation from the proposed VCG-32K dataset; (2) Tree-ofCausal-Thought (ToCT), an automated approach for synthesizing visually grounded reasoning trajectories; (3) reinforcement learning (RL) [11, 49] with our designed causal reward to optimize the models policy for visual causal discovery. Specifically, for training data curation, we use the MS-COCO portion of the VCG-32K dataset as the training corpus, enabling cross-dataset evaluation on the Objects365 subset. As these data offer only final labels but no process supervision, we propose ToCT to synthesize highquality trajectories: teacher model1 repeatedly executes actions of region selection, entity recognition, and causality orientation to perform multi-step reasoning. Monte Carlo Tree Search (MCTS) [7] is employed to ensure the quality of these trajectories by maintaining multiple branches and propagating value estimates [55, 59, 61]. The synthesized trajectories are then filtered and serve as demonstrations for the policy model2 to develop an initial reasoning policy through supervised fine-tuning (SFT) [37, 62]. Lastly, we refine the policy model using the Group Relative Policy Optimization (GRPO) algorithm [48] guided by carefully designed, graph-based causal reward, which encourages accurate visual causal discovery and faithful structural generation. Experimental results demonstrate that our model, CauSight, achieves superior performance, surpassing its base model and the leading proprietary GPT-4.1 by absolute gains of 27.4% ( 8.2 improvement) and 21.0% ( 3.1 improvement), respectively. Additionally, CauSight exhibits strong cross-dataset generalization. The main contributions of our work are three-fold: We formulate the task of visual causal discovery and introduce the VCG-32K dataset, large-scale collection of over 32,000 images annotated with entity-level causal graphs to facilitate research on this problem. We introduce the Tree-of-Causal-Thought (ToCT) approach for synthesizing high-quality reasoning trajectories, combined with reinforcement learning framework optimized by graph-based causal reward to enhance visual causal discovery, resulting in novel VLM CauSight. CauSight shows superior performance on visual causal discovery, with markedly improvements over baseline models and strong cross-dataset generalization. Further analysis provides valuable insights for future research. 2. Task Formulation: Visual Causal Discovery Visual causal discovery aims to construct visually grounded causal graph = (V, E) from single image, which consists of two key components: basic entities and their causal relationships E. Entities. Each graph contains entities = {vi}N i=1, with vi = (c, b) where is the category label and R4 is the bounding box. 1We use Qwen2.5-VL-72B for synthesizing trajectories for its strong reasoning performance and low cost compared with proprietary models. 2We use the term policy model to refer to the model being trained; in this work, we adopt Qwen2.5-VL-7B as our base model for its favorable balance between capability and training cost. Causal Relationships. Edges = {eij}i=j are directed and predicate-labeled: eij = (vi, κ, vj), κ (e.g., support). Following causal dispositions [14, 31, 33], exists if an intervention on vi changes vj: eij p(vj do(vi = 0)) = p(vj). (1) Here we let vi {0, 1} denote whether an entity exists, where vi = 1 means present and vi = 0 means absent. In this formulation, p(v) denotes an entity state (e.g., presence, sit). Interventions do() here represent removal of the entity (i.e., setting vi = 0). For example, as shown in Figure 1, if we remove the laptop, the states of books and glass will be changed. Evaluation. Predicted graphs are evaluated structurally, ignoring entity or predicate labels. Entities are matched to ground-truth via the Hungarian algorithm [25] with GIoU [44]. An edge (vi, vj) is correct if it exists in the ground-truth with the same causal direction j. 3. The VCG-32K Dataset Existing visual understanding datasets, such as those focused on scene graphs [10, 21, 54], remaining agnostic to the underlying causality. They provide spatial relations (e.g., on) but miss the inner causal mechanism (e.g., support, lift). Moreover, these datasets suffer from bounding box annotation errors. To address the gaps, we construct the Visual Causal Graph dataset (VCG-32K) with three key features: (1) refined bounding boxes for accurate localization of entities {vi}N i=1, (2) explicitly annotated causal relationships vi vj, and (3) causal mechanism types κ explaining why the causal relationship exists. These rich annotations guide models toward causally aware reasoning beyond perception-level visual understanding. VCG-32K is construced with images from the MSCOCO [30] and Objects365 [47] datasets. We employ 50 trained annotators for dataset construction. Figure 2 illustrates the two-stage annotation pipeline: bounding box refinement and causal relationship labeling. In the first stage, annotators refine the existing bounding boxes. They delete or modify boxes with incorrect boundaries, label errors (e.g., overly broad, abstract concepts, or incorrect entity names), and entities without direct contact with any other entities. Annotators are also allowed to add new entities that have causal relationships with others. In the second stage, annotators proceed to identify and categorize causal relationships eij = (vi, κ, vj). causal relationship exists when three conditions are met: (1) vi is in direct contact with vj, (2) the presence of vi maintains the current state of vj, (3) removing vi would cause vj to lose its current state. We conduct quality checks throughout both stages to ensure the average annotation accuracy exceeds 95%: 10 reviewers randomly sample items, evaluate annotation accuracy, Figure 2. The two-stage annotation pipeline of VCG-32K: bounding box refinement and causal relationship labeling. and correct any errors. In total, the resulting dataset comprises 32,256 images, 299,262 entities, 2,287 entity categories, and 185,321 causal relationships (5.75 relationships per image in average). For more details, see the Appendix. 4. Method In this section, we present the recipe for developing CauSight. We start by introducing tree-search-based trajectory synthesis approach for initializing the reasoning policy in subsection 4.1, followed by reinforcement learning stage for further optimization in subsection 4.2. Figure 3 illustrates part of single synthesized reasoning trajectory, and Figure 4 provides an overview of the training pipeline. 4.1. Tree-of-Causal-Thought Visual causal discovery seeks to ground visual concepts in language, thereby requiring VLMs to internalize causally aware reasoning process. However, as VCG-32K offers final answers but no process supervision, fine-grained reasoning trajectory cannot be reliably acquired; collecting expert rationales is prohibitively costly and inefficient. Consequently, we propose Tree-of-Causal-Thought (ToCT), an automated approach for synthesizing high-quality causally aware reasoning trajectories. We first present the key actions, then the MCTS procedure, and finally how the resulting small ToCT dataset supervises model training. Key Actions. Constructing ToCT involves three key acregion selection Ar, entity recognition Ae, and tions: Figure 3. Illustration of single synthesized reasoning trajectory. The teacher model can repeatedly execute three key actions to extend the reasoning trajectory. Figure 4. Overview of the training pipeline. Tree-of-Causal-Thought expands single reasoning path into tree structure to select high-quality trajectories, which serve as demonstrations to initialize CauSights reasoning policy. The subsequent RL further optimizes this policy for visual causal discovery with our designed causal reward. causality orientation Ac. (1) Region selection zooms into specific sub-regions, facilitating more comprehensive search for causal pairs and helping to exclude confound- (2) Entity recognition grounds entities in the ing factors. selected region. (3) Causality orientation infers the existence and direction of causal relationships between entities. The actions are repeatedly executed in fixed loop: Ar Ae Ac Ar . . . At each step t, the reasoning state st = {regions1:t1, causality1:t1, at1} updates all previously explored regions, discovered causal relationships and last executed action. The model selects the next action at {Ar, Ae, Ac} following the looping order, and then generates an intermediate reasoning result rt πθ( st, at)3. This process iterates until predefined step limit is reached. The full reasoning trajectory is thus represented as τ = r1 r2 rT , which is the accumulation of intermediate results. Note that we employ more capable teacher model to generate high-quality trajectories that better capture causally aware reasoning patterns. Monte Carlo Tree Search. The MCTS algorithm expands single trajectory into tree structure, maintaining multiple branches and propagating value estimates to enhance the quality of the optimal trajectory. Each search iteration consists of four phases: selection, expansion, simu3πθ refers to the policy of the VLM parameterized by θ. lation, and backpropagation. During selection phase, the algorithm traverses the tree from the root to identify the most promising child node using the well-known Upper Confidence Bounds for Trees (UCT) rule [23], which balances exploration and exploitation: UCT(s, a) = Q(s, a) + (cid:115) ln (P a(s)) (s, a) , (2) where (s, a) and (P a(s)) denote the visit counts of the current node and its parent, respectively, and Q(s, a) is updated during backpropagation. In the expansion phase, the model executes an action based on the current node state to generate new child nodes C(s, a), thereby expanding the search tree in both width and depth. Next, in the simulation phase, the model performs an imagined rollout from the selected node until reaching leaf node. This process estimates foresighted value by completing hypothetical reasoning trajectory that anticipates future outcomes. Finally, during backpropagation, the estimated value is propagated upward to update all nodes along the selected branch: Q(s, a) = (cid:80) cC(s,a) Q(c) (c) (cid:80) cC(s,a) (c) . (3) The reward signal originates from evaluating the leaf nodes state against the ground-truth causal graph, providing supervision for value propagation. After fixed number of search iterations, the node values converge, and the optimal trajectory is obtained by greedily selecting the highest-value nodes along the reasoning tree. Reasoning Policy Initialization. Although the ToCT approach produces controllable reasoning trajectories, it occasionally fails to outperform vanilla one-step reasoning on certain cases. To ensure that the policy model learns from trajectories that not only reflect structured causally aware reasoning but also lead to superior solutions, we retain only those ToCT-induced trajectories that outperform their vanilla one-step counterparts. This guarantees that the model is trained on higher-quality examples, minimizing the risk of introducing suboptimal reasoning patterns. We initialize the models reasoning policy by conducting supervised fine-tuning (SFT) on the filtered trajectories: LSFT(θ) = E(q,τ )DSFT [log πθ(τ q)] , (4) Where denotes the input, and τ is the trajectory to be learned by the policy model πθ. 4.2. Reinforce with Causal Reward Although imitation learning can instill causally aware reasoning patterns, the VLMs capability for visual causal discovery remains limited by the narrow coverage of synthetic trajectories. These trajectories provide useful yet constrained supervisionthe model can mimic existing reasoning paths but struggle to generalize. To address this, we introduce RL to transform the model from passive imitator into an adaptive causal learner that continually optimizes its causal discovery policy through iterative interaction and feedback. We start RL from the SFT-trained policy and use the GRPO algorithm. GRPO dispenses with the separate value function typically required in PPO-style objectives by leveraging relative advantage computed over group sampled outputs for one input. This design reduces computational overhead and aligns with the visual causal discovery task, where relative comparisons across rollouts capture nuanced improvements in graph reconstruction quality. Formally, the optimization objective is given by: JGRPO(θ) =E [qD,{oi}G i=1πθold (q)] 1 G (cid:88) ( i=1 min(RiAi, clip(Ri, 1 ϵ, 1 + ϵ)Ai)), std({r1,r2,...,rG}) Ri = πθ(oiq) πθold (oiq) , Ai = rimean({r1,r2,...,rG}) Causal Reward Design. As outlined in section 2, our goal is to improve the VLMs ability to understand and reconstruct causal graph structures. Accordingly, we design causal reward to promote (i) accurate visual causal discov- . (5) ery and (ii) faithful structural generation: R(τ ) = λr Recall(τ ) + λp Precision(τ ) (cid:125) (cid:123)(cid:122) visual causal discovery (cid:124) +λf Format(τ ). (6) The Recall and Precision are defined as: Epred Egt Egt , Precision = Recall = Epred Egt Epred , (7) where Epred and Egt denote the sets of predicted and ground-truth directed edges, respectively. By incorporating the Recall term, the policy model is encouraged to align its predicted causal structure with the ground-truth causal graph. The Precision term is introduced to suppress false positives and balance the trade-off between completeness and correctness. Finally, Format evaluates the compliance of the models structural generation with the predefined output schema, which is illustrated in the Appendix. 5. Experiments 5.1. Setup Baselines. We compare our CauSight against three categories of baselines: (1) state-of-the-art proprietary models, including Gemini 2.5 Pro [18], OpenAI o3 [35], and GPT series [2, 36]; (2) leading open-source models, represented by the Qwen2.5-VL family [4] with varying parameter scales; (3) an SFT variant4, directly fine-tuned on ground-truth formatted labels (using the same base model as CauSight). Training Data. We use the MS-COCO portion of the VCG-32K dataset as the training corpus, enabling crossdataset evaluation on the Objects365 subset. The entire training set contains 11,078 samples. ToCT is performed on 6,000 of these samples, yielding 3,631 trajectories after filtering. The remaining 7,447 samples are used for RL. Benchmarks and Metrics. We evaluate models on 350 samples from the MS-COCO subset of VCG-32K for the main benchmark, and another 350 from the Objects365 subset to test cross-dataset performance. For broader assessment of generalization, we include three out-of-domain (OOD) benchmarks: Math-V and MathVista for multimodal mathematical reasoning, and BLINK for visual perception. We report graph recall, precision, and F1 score computed at GIoU threshold of 0.5. Implementation Details. For ToCT, we set the step limit to 12, expand up to 10 child nodes per step, and run 20 search iterations. For GRPO, training runs for 15 epochs on 4 nodes with 8 H200 GPUs each, using batch size of 512 and maximum response length of 4,096 tokens. Each rollout samples 5 outputs per prompt. Further details and prompts templates are provided in the Appendix. 4the SFT variant in the baseline is not the same model as the policy model SFTed on ToCT-induced trajectories, so please do not confuse them. Model Gemini 2.5 Pro OpenAI o3 GPT-4.1 GPT-5 Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B VCG-32K-COCO VCG-32K-365 Average Recall Precision F1 Score Recall Precision F1 Score Recall Precision F1 Score 5.7 8.6 16.5 5.1 4.7 14.0 19.0 2.5 7.7 15.6 2.9 7.9 14.1 26. in-domain 3.1 7.0 15.0 3.4 5.4 12.3 20.1 12.0 37.6 3.0 6.0 3.9 3.0 2.9 7.7 11. 12.1 28.1 1.3 6.4 3.6 1.5 6.6 10.3 20.5 cross-domain 19.5 42.3 1.7 5.1 3.4 1. 3.8 7.7 13.4 13.8 31.1 4.4 7.3 10.2 4.1 3.8 10.9 15.4 11.7 31.2 1.9 7.1 9.6 2. 7.3 12.2 23.6 17.2 45.5 2.4 6.1 9.2 3.6 4.6 10.0 16.8 12.9 34.4 Qwen2.5-VL-7B + SFT CauSight (ours) 11.2 34.2 14.8 48.7 Table 1. Comparison of CauSight with proprietary, open-source, and the SFT variant.(%) We report graph-level Recall, Precision, and F1 Score on both the in-domain (MS-COCO) and cross-domain (Objects365) subsets of VCG-32K. CauSight comprehensively outperform diverse baselines. 5.2. Results on VCG-32K As shown in Table 1, CauSight achieves substantial improvement over all baselines on both in-domain (VCG-32KCOCO) and cross-domain (VCG-32K-365) settings. It surpasses the leading proprietary model (GPT-4.15) by an absolute gain of 21.0% in graph recall, and outperforms the larger open-source model (Qwen2.5-VL-72B) by 15.8%. CauSight also maintains strong precision, and consistent F1 score improvement across both subsets. Notably, the performance gap is especially evident on the cross-domain VCG-32K-365 benchmark. CauSight boosts graph recall by 16.0% and F1 score by 17.3% over the SFT variant. This striking contrast suggests that visually grounded reasoning is the key for VLMs to achieving robust generalization. Our multi-stage training recipe enables the model to internalize transferrable causal principles, thereby extrapolating causal dependencies to novel scenes, whereas supervised training on totally formatted outputs can severely harm the generalization ability. The comparison with proprietary models further highlights that, despite their massive scale and pretraining coverage, general-purpose VLMs still struggle to establish coherent causeeffect structures from visual scenes. In contrast, CauSight explicitly integrates causal priors and decision optimization through ToCT and RL, enabling more systematic understanding of visual interactions. These results validate the effectiveness of our recipe in bridging the gap between perception and reasoning, and underscore that causally aware reasoning is the cornerstone for both interpretability and generalization in visual understanding. 5We found that GPT-5 performs poorly in this visually grounded reasoning task, so in subsequent experiments we adopt GPT-4.1. Model Recall RecallR Loss Gemini 2.5 Pro OpenAI o3 GPT-4.1 Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B CauSight (ours) 5.7 8.6 16.5 4.7 14.0 19.0 34.2 10.33 9.6 23.0 5.7 18.2 21.8 37.2 44.8 10.6 27.9 18.1 23.1 12.9 8.0 Table 2. Causal Reasoning-Induced Performance Loss.(%) CauSight effectively mitigates reasoning-induced loss toward reachable upper bound. 5.3. Reasoning and Detection In this section, we further investigate the respective impact of reasoning and detection capabilities on the overall task performance, primarily reporting the graph recall results on VCG-32K-COCO benchmark. Reasoning-Induced Performance Loss. We isolate the impact of reasoning from detection by evaluating it in controlled setting. Specifically, for each causal graph, we compute reachable RecallR, defined as the maximum achievable recall given the set of correctly recognized entities (i.e., assuming perfect causally aware reasoning over detected nodes). Let Recall denote the actual graph recall obtained by the model. We then quantify the reasoning-induced performance loss as: = RecallR Recall RecallR . (8) This metric reflects the relative gap between the models achievable upper bound and its actual causally aware reasoning capability. As shown in Table 2, the reasoning gap markedly affects the recall performance across modFigure 5. Detection-dependent performance stability. Graph recall is evaluated under varying GIoU thresholds, summarized by the Recall Stability Index (RSI). CauSight achieves strong causal discovery performance while maintaining high RSI, indicating balanced integration of detection and reasoning capabilities. ure 5 shows, GPT-4.1 exhibits highest stability, indicating superior detection capability together with competitive recall. In contrast, directly applying SFT on the base model severely degrades detection capability, as reflected by the markedly low RSI. CauSight achieves balanced improvement: it substantially boost causal discovery performance with only minor drop in RSI. To summarize, by comparing the reasoning-induced loss with RSI, we observe an opposite trend between reasoning and detection capabilities. For example, OpenAI o3 shows low reasoning-induced performance loss (10.6%), yet exhibits the weakest RSI; conversely, Gemini 2.5 Pro demonstrates excellent detection performance but suffers an extremely high reasoning-induced loss (44.8%), suggesting an absence of causally aware reasoning ability. These findings highlight that connecting visual perception to reasoning remains challenging problem. Meanwhile, CauSight not only achieves leading visual causal discovery performance but also maintains harmonious balance between detection and reasoning. 5.4. Ablation Study on Training Recipe We conduct an ablation study on our training recipe, evaluating on both the in-domain (VCG-32K-COCO) and crossdomain (VCG-32K-365) benchmarks. As shown in Table 3, both the ToCT approach and the subsequent RL stage contribute substantially to the overall performance gains. The introduction of ToCT-induced trajectories provide supervised prior that enables the model to initialize structured reasoning policy, while RL further optimizes the policy through dynamic feedback. On average, CauSight improves graph recall by 27.4% over the base model, and by 21.1% and 6.7% over the model without RL and that without ToCT, respectively. Notably, despite the limited number of ToCT-induced trajectories, initializing the model with SFT on them before RL resulted in marked advantage over using RL alone, with 10.1% increase in graph recall on the cross-domain benchmark. This again underscores that Figure 6. Model generalizability across three OOD benchmarks. Qwen refers to Qwen2.5-VL-7B, +SFT to the SFT variant in the baseline. Each cell corresponds to the models accuracy. els6. While larger models such as Qwen2.5-VL-72B and GPT-4.1 achieve solid graph recall, they still suffer substantial performance loss toward the reachable upper bounds, suggesting insufficient causal comprehension. In contrast, CauSight achieves recall of 34.2%, outperforming even much larger models while reducing the loss to 8.0%, effectively mitigating reasoning-induced degradation. Detection-Dependent Performance Stability. It is inherently challenging to evaluate models detection capability in isolation on this task, since the detected entities are closely intertwined with their causal relations. To partially disentangle these factors, we evaluate graph recall under different GIoU thresholds (ranging from 0.3 to 0.7), which reflects its sensitivity to detection strictness, thus indicating models underlying detection capability. Meanwhile, we introduce Recall Stability Index (RSI), denoted as: RSI[0,1] = max 0, 1 (cid:18) std({Recallt}) mean({Recallt}) (cid:19) , (9) where Recallt denotes graph recall at GIou threshold t. RSI quantifies how robust graph recall is across thresholds, i.e., how strongly it depends on detection capability. As Fig6Since the model fine-tuned directly on ground-truth labels (the SFT variant) follows structured output and no longer falls under the category of reasoning models, its relevant results are not reported. Figure 7. Qualitative comparison on the base model, the SFT variant and our CauSight. Model VCG-32K-COCO VCG-32K-365 Average Recall Precision F1 Score Recall Precision F1 Score Recall Precision F1 Score CauSight (ours) w/o ToCT w/o RL Qwen2.5-VL-7B 34.2 31.0 10.7 4.7 48.7 50.9 6.8 7. 37.6 35.3 7.3 5.4 28.1 18.0 9.4 2.9 42.3 36.5 6.3 6. 31.1 21.9 6.6 3.8 31.2 (+27.4) 24.5 (+20.7) 10.1 (+6.3) 45.5 (+38.2) 43.7 (+36.4) 6.6 (-0.7) 34.4 (+29.8) 28.6 (+24.0) 7.0 (+2.4) 3. 7.3 4.6 Table 3. Ablation results on the training recipe.(%) Both the ToCT approach and the subsequent RL stage play critical roles in enhancing the performance of CauSight. Integrating SFT on ToCT-induced trajectories before RL yields evident advantage over applying RL alone, especially on the cross-domain benchmark VCG-32K-365. enhanced reasoning is key to achieving better generalization as discussed in subsection 5.2. Quality analysis of the synthesized trajectories are also provided in the Appendix. table for causal discovery. These results highlight its effectiveness in capturing complex causal patterns within visual scenes. 5.5. General Capability Assessment To evaluate the generalizability of CauSight, we compare it to the base model and the SFT variant on three benchmarks: Math-V and MathVista for multimodal mathematical reasoning, and BLINK for visual perception. Figure 6 presents the results. Our findings indicate that CauSight largely preserves generalization capabilities comparable to the original model. Conversely, models trained directly with SFT exhibit severe degradation. This is particularly evident on math reasoning tasks, where the performance of the SFT variant dropped by over 20%. 5.6. Qualitative Comparison Figure 7 presents qualitative comparison among the base the SFT variant, and our CauSight. CauSight model, achieves high-fidelity causal relation detection, producing detailed and structurally consistent outputs. Additionally, under our multi-stage training using graph reward without category-label supervision, the model is able to perform genuine attribute-based reasoning. For example, it can correctly revise the ground-truth label chopping board to 6. Related Work Causal Discovery and Reasoning. Causal reasoning has been widely explored with structural causal models (SCMs) [39] offering formal basis. Classical approaches [42, 50] work well in low-dimensional settings but scale poorly to complex data. Recent studies combine deep learning with causal modeling for induction from interactive data [8, 16, 34], though mostly in synthetic domains. Graph-based and differentiable DAG methods [27, 57] improve scalability but rarely model explicit causal structures from visual observations. Our work directly learns interpretable causal graphs from raw images in the visionlanguage context. Causal Evaluation in LLMs and VLMs. LLMs have been tested on causal reasoning in textcovering commonsense, extraction, and multi-step reasoning [12, 13, 32, 45]but visual grounding remains limited. Existing visual QA and causal benchmarks [19, 38, 53, 58, 64] touch on causality but lack explicit structures and rely on shallow event reasoning. We address this gap by enabling explicit causal graph learning from images for fine-grained, entitylevel reasoning in vision-language contexts. 7. Conclusions This work introduces CauSight, novel VLM capable of constructing causal graphs across diverse visual scenes. We formulate the task of visual causal discovery and propose the first large-scale Visual Causal Graph dataset (VCG32K), to support further research in this field. We believe that causal understanding is pivotal for AI systems to truly generalize to complex environments and unseen tasks."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is supported by Shanghai Artificial Intelligence Laboratory."
        },
        {
            "title": "References",
            "content": "[1] Ahmed Adamos Abdulaal, Hadjivasiliou, Nina Montana Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, and Daniel C. Alexander. Causal modelling agents: Causal graph discovery through synergising metadataand dataIn International Conference on driven reasoning. Learning Representations, 2024. 1 [2] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report, 2023. 2, 5 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani S. Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. ArXiv, abs/2308.01390, 2023. 1 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. ArXiv, abs/2502.13923, 2025. 2, 5 [5] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Joseph Pal. meta-transfer objective for learning to disentangle causal mechanisms. ArXiv, abs/1901.10912, 2019. 1 [6] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Huijuan Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction to vision-language modeling. ArXiv, abs/2405.17247, 2024. [7] Cameron Browne, Edward Jack Powley, Daniel Whitehouse, Simon M. M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez Liebana, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4:143, 2012. 2 [8] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racani`ere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Manfred Otto Heess. Woulda, coulda, Counterfactually-guided policy search. ArXiv, abs/1811.06272, 2018. 8 shoulda: [9] Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. ArXiv, abs/1412.2309, 2014. 1 [10] Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, and Alexander G. Hauptmann. comprehensive survey of scene graphs: Generation and application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:126, 2021. 2, 3 [11] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021. 2 [12] Meiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kunpeng Wang, Jing Shao, and Yan Zhang. Ergo: Event relational graph transformer for document-level event causality identification. ArXiv, abs/2204.07434, 2022. [13] Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, and Dongsheng Li. Learning to teach ArXiv, large language models logical reasoning. abs/2310.09158, 2023. 8 [14] Meiqi Chen, Bo Peng, Yan Zhang, and Chaochao Lu. Cello: Causal evaluation of large vision-language models. In Conference on Empirical Methods in Natural Language Processing, 2024. 1, 2, 3 [15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Instructblip: Towards general-purpose visionHoi. ArXiv, language models with instruction tuning. abs/2305.06500, 2023. 1 [16] Ishita Dasgupta, Jane X. Wang, Silvia Chiappa, Jovana Mitrovic, Pedro A. Ortega, David Raposo, Edward Hughes, Peter W. Battaglia, Matthew M. Causal reaBotvinick, and Zeb Kurth-Nelson. ArXiv, soning from meta-reinforcement learning. abs/1901.08162, 2019. 8 [17] Melanie Frappier. The book of why: The new science of cause and effect. Science, 361:855 855, 2018. 2 [18] Google. Gemini 2.5: Pushing the frontier with long context, ArXiv, advanced reasoning, multimodality, and next generation agentic capabilities. abs/2507.06261, 2025. 5 [19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in International Journal of visual question answering. Computer Vision, 127:398 414, 2016. 8 [20] Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, and Yoshua Bengio. Efficient causal graph discovery using large language models. ArXiv, abs/2402.01207, 2024. 1 [21] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Image retrieval using scene graphs. 2015 Fei-Fei. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 36683678, 2015. 2, 3 [22] Nan Rosemary Ke, Aniket Rajiv Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo Jimenez Rezende, Yoshua Bengio, Michael C. Mozer, and Christopher Joseph Pal. Systematic evaluation of causal discovery in visual model based reinforcement learning. ArXiv, abs/2107.00848, 2021. 1 [23] Levente Kocsis and Csaba Szepesvari. Bandit based In European Conference on monte-carlo planning. Machine Learning, 2006. [24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced International Journal of dense image annotations. Computer Vision, 123:32 73, 2016. 1 [25] Harold W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics (NRL), 52, 1955. 3 [26] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J. Russell, and P. Abbeel. Learning plannable representations with causal infogan. ArXiv, abs/1807.09341, 2018. 1 [27] Sebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural dag learning. ArXiv, abs/1906.02226, 2019. 8 [28] Tianyu Li, Li Chen, Huijie Wang, Yang Li, Jiazhi Yang, Xiangwei Geng, Shengyin Jiang, Yuting Wang, Hang Xu, Chunjing Xu, Junchi Yan, Ping Luo, and Hongyang Li. Graph-based topology reasoning for driving scenes, 2023. 2 [29] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1578 1597, 2025. [30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, 2014. 2, 3 [31] David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Leon Bottou. Discovering causal signals in images. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5866, 2016. 1, 3 [32] Qing Lyu, Li Zhang, and Chris Callison-Burch. Reasoning about goals, steps, and temporal ordering with wikihow. In Conference on Empirical Methods in Natural Language Processing, 2020. 8 [33] Stephen Mumford and Rani Lill Anjum. Getting causes from powers, 2011. 1, 3 [34] Suraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal induction from visual observations for goal directed tasks. ArXiv, abs/1910.01751, 2019. 1, 8 [35] openai. Openai o3 and o4-mini system card, 2025. 2, 5 [36] openai. Gpt-5 system card, 2025. 5 [37] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. 2 [38] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. Visualcomet: Reasoning about the dynamic context of still image. In European Conference on Computer Vision, 2020. 8 [39] Judea Pearl. Causality. Cambridge university press, 2009. 8 [40] Judea Pearl. Structural counterfactuals: brief introduction. Cognitive science, 37 6:97785, 2013. 2 [41] Derek C. Penn and Daniel J. Povinelli. Causal cognition in human and nonhuman animals: comparative, critical review. Annual review of psychology, 58:97 118, 2007. [42] Jonas Peters. Uva-dare ( digital academic repository ) causal discovery with continuous additive noise models. 2014. 8 [43] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: Foundations and learning algorithms. 2017. 1 [44] Seyed Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian D. Reid, and Silvio Savarese. Generalized intersection over union: met2019 ric and loss for bounding box regression. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 658666, 2019. 3 [45] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. ATOMIC: an atlas of machine commonsense for ifthen reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 30273035. AAAI Press, 2019. 8 [46] Laura E. Schulz and Elizabeth Baraff Bonawitz. Serious fun: preschoolers engage in more exploratory play when evidence is confounded. Developmental psychology, 43 4:104550, 2007. 1 [47] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 84298438, 2019. 2, 3 [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. [49] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. ArXiv, 2024. 2 [50] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti J. Kerminen. linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res., 7: 20032030, 2006. 8 [51] Udbhav and Rachna. Sapiens: brief history of humankind. Journal of the Practice of Cardiovascular Sciences, 3:61 62, 2017. 1 [52] Huijie Wang, Tianyu Li, Yang Li, Li Chen, Chonghao Sima, Zhenbo Liu, Bangjun Wang, Peijin Jia, Yuting Wang, Shengyin Jiang, Feng Wen, Hang Xu, Ping Luo, Junchi Yan, Wei Zhang, and Hongyang Li. Openlane-v2: topology reasoning benchmark for unified 3d hd mapping. In NeurIPS, 2023. 2 [53] Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. Fvqa: Fact-based visual IEEE Transactions on Pattern question answering. Analysis and Machine Intelligence, 40:24132427, 2016. [54] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. ArXiv, abs/2207.11247, 2022. 2, 3 [55] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, and Karthik Tree of thoughts: Deliberate probArXiv, Thomas L. Griffiths, Yuan Cao, Narasimhan. lem solving with large language models. abs/2305.10601, 2023. 2 [56] Shu Yu and Chaochao Lu. ADAM: An embodied causal agent in open-world environments. In The Thirteenth International Conference on Learning Representations, 2025. 1 [57] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. In International Conference on Machine Learning, 2019. 8 [58] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual com2019 IEEE/CVF Conference monsense reasoning. on Computer Vision and Pattern Recognition (CVPR), pages 67136724, 2018. 1, 8 [59] Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm selftraining via process reward guided tree search. ArXiv, abs/2406.03816, 2024. [60] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46:56255644, 2023. 1 [61] Yize Zhang, Tianshu Wang, Sirui Chen, Kun Wang, Xingyu Zeng, Hongyu Lin, Xianpei Han, Le Sun, and Chaochao Lu. Arise: Towards knowledgeaugmented reasoning via risk-adaptive search. ArXiv, abs/2504.10893, 2025. 2 [62] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Llamafactory: Unified efficient fine-tuning Ma. In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 2 [63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. ArXiv, abs/2304.10592, 2023. 1 [64] Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 49955004, 2015. CauSight: Learning to Supersense for Visual Causal Discovery"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Synthesized Trajectories In this section, we present detailed analysis on the quality of ToCT-induced reasoning trajectories. As discussed in section 4.1, the ToCT approach could produce suboptimal solutions compared to their vanilla one-step reasoning counterparts on certain cases. We provide the overall quantification results in Table 4. The ZERO represents samples where both the ToCT and the vanilla approaches produce solutions with 0 recall. These samples are regarded as relatively extreme cases, e.g., those with excessively high difficulty. We can see that after removing the extreme cases (w/o ZERO), mean recall of ToCT achieves 0.42, with 13% absolute gain over the vanilla approach. We further analyzed the data and found that about 50% of the samples had an > 0.1 improvement (ToCT over vanilla). Figure 15 shows the specific results of 6,000 samples (scatter). Our rule for filtering high-quality trajectories is that the ToCT approach is required to strictly outperform the vanilla one-step reasoning on the same image. This ensures that filtered trajectories not only follow the designed reasoning patterns but also lead to superior solutions, which minimize the risk of introducing suboptimal reasoning paths. Finally, after the filtering, we retain 3,631 from the total 6,000 trajectories. The mean recall of the filtered trajectories is 0.50, while that of their vanilla counterparts is 0.21. w/ ZERO w/o ZERO Metric Vanilla ToCT Vanilla ToCT Mean Median 0.22 0.00 0.32 0.22 0.29 0.13 0.42 0.33 Table 4. Recall comparison between ToCT and vanilla approaches under different conditions. The ToCT approach is overall superior to vanilla one-step reasoning. B. Dataset We sample candidate images from the MS-COCO and Object 365 datasets based on the number of entities present in each image and the diversity of their associated labels. The annotation process consists of two stages: bounding box refinement and causal-relationship labeling. Bounding box refinement requirements. To define the target objects for annotation, all entities present in the image are considered, including both objects and humans. Each entity must be annotated individually and cannot be merged with others. Entities smaller than 3030 pixels are excluded. The annotation process is based on existing preannotated bounding boxes. Annotators are required to review these boxes and modify, remove, or add annotations according to the current guidelines. Pre-annotated bounding boxes should be corrected or deleted under three conditions. (1) Boundary errors: the bounding box contains excessive empty space or truncates part of the entity. (2) Label errors: the assigned label is overly broad or abstract (e.g., sky, air) or is factually incorrect (e.g., labeling man as woman). (3) Irrelevant entities: entities that have no direct physical contact or interaction with any other entity in the scene. Besides, new entities should be added only if they meet the following criterion: the entity engages in direct physical contact or force interaction with at least one other entity in the scene, but is not annotated by the existing dataset. Causal relationship labeling requirements. Given images with annotated bounding boxes and entity labels, annotators identify causal relationship attributes and directions for every ordered pair of entities and assign each relationship to one of the predefined categories, such as carry on and support. causal relationship exists when three conditions are simultaneously met: (1) vi is in direct contact with vj, (2) the presence of vi maintains the current state of vj, (3) removing vi would cause vj to lose its current state. To ensure the quality, we start the annotation process after all the annotators have completed the training and achieved an accuracy rate of 95% in the pre-annotated validation set. In total, 50 junior annotators participated in the labeling process, and 10 senior annotators conducted quality reviews. The senior annotators examined whether the annotations produced by junior annotators in both stages adhered to the corresponding guidelines, thereby further guaranteeing the overall quality of our VCG-32K dataset. Figure 8 shows the distribution of causal relations per image, the top 10 entity categories, and the top 10 causal relation categories for the MS-COCO and Objects365 subset of the final VCG-32K dataset, respectively. An example of VCG32K is shown below, where causal relationship is denoted by entity IDs, with the first entity causing the second entity. { \"dataset_id\": \"COCO\", \"img_id\": 0, \"entities\": [ { \"entity_id\": 0, \"entity_name\": \"woman\", \"bbox\": [502.6, 105.47, 25.83, 132.38] Figure 8. Dataset statistics. essential causal structure of scene. In more complex cases (1, 2, and 5), although the recall does not reach 1, it correctly identifies the main causal relationships (some correct edges are counted as false due to imperfect bounding box matching, as indicated by the black dashed lines). Meanwhile, in the simpler scenarios (3 and 4), the model even achieves perfect recall, showing strong generalization even under complex reasoning pattern. Additionally, we provide an actual reasoning path from CauSight as shown in Figure 9. The reasoning path is still well structured to include steps like focus on region, recognize entities and infer causal relationships, following the causally aware reasoning patterns we designed in subsection 4.1. }, ... ], \"causal_relationships\":[ \"carry_on\": [[0, 1], ...], \"support\": [[2, 3], ...] ... ] } C. Qualitative Comparisons We provide more qualitative comparisons on Qwen2.5-VL-7B (base model), Qwen2.5-VL-7B + SFT and our CauSight. For each, we present the ultimate generated causal graph and corresponding recall against the ground-truth. Black solid lines denote correctly constructed causal edges, black dashed lines indicate correctly inferred causal relationships but without precise bounding box matches, and gray solid lines represent incorrectly constructed causal edges. We observe that the base model struggles to construct causal graph in most cases, due to its limited detection and causal discovery capabilities. In our provided cases, the base model consistently achieves final recall of 0. The model SFTed with ground-truth labels shows slight improvement; however, it exhibits severe hallucinations in complex scenarios as Figure 10 and Figure 14 show (cases 1 and 5). For example, it incorrectly assumes that one container causally affects the food inside another container. In simpler scenarios, it tends to over-reason: although it achieves recall of 0.5 in cases 3 and 4, its precision remains extremely low because it falsely detects many nonexistent entities and infers their nonexistent causal relationships. Our model, CauSight, after multi-stage training, demonstrates strong performance. It reliably captures the Figure 9. Illustration of actual reasoning path from CauSight. Figure 10. Qualitative comparison (1). Figure 11. Qualitative comparison (2). Figure 12. Qualitative comparison (3). Figure 13. Qualitative comparison (4). Figure 14. Qualitative comparison (5). Figure 15. Specific results of 6000 samples (ToCT vs. vanilla, scatter + moving average, w/ ZERO and w/o ZERO.) ZERO represents samples where both the ToCT and the vanilla approaches produce solutions with 0 recall."
        },
        {
            "title": "Region Selection",
            "content": "You are analyzing the causal relationships between entities in the image through multiple steps. Your current reasoning trajectory is as follows: Explored regions: Identified causal pairs: Now we hope to look for new regions to discover more potential correlated entity pairs. Please select the next most worthy region to focus on and explain your thinking process. Note: the next region should be DIFFERENT from the previous explored regions. {explored regions}. {causal pairs}. -- If you think the exploration regions and identified causal pairs are SUFFICIENTLY COMPREHENSIVE, you should DIRECTLY output END TRACE and nothing else. Otherwise, your output format should be as follows: <think> (State the reason as concisely as possible for selecting the new focused region.) </think> <region name> (Output the name of the focused region and nothing else.) </region name> <bounding box> (Output the bounding box of the focused region with format [x1, y1, x2, y2] and nothing else, where (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the bounding box.) </bounding box>"
        },
        {
            "title": "Entity Recognition",
            "content": "Your task is to identify all entity pairs that may have correlations in the image. Each pair should have obvious potential correlations such as spatial dependence, support, grasping, placement, inclusion, etc. Think and output all these correlated entity pairs and their bounding boxes. Your output format should be as follows: <think> (Provide the concise thinking process for identifying correlated entity pairs.) </think> <entity pairs> (Output all the correlated entity pairs in the format of [\"entity1\": y2], \"entity2\": y1, x2, y2], ...]. entity1, entity2, ... (x2, y2) is the bottom-right coordinate of the bounding box.) </entity pairs> [x1, y1, x2, y2], \"entity3\": in the format. [x1, y1, x2, y2], \"entity4\": [x1, [x1, y1, x2, You should use ACTUAL ENTITY NAME to replace the placeholders (x1, y1) is the top-left coordinate and"
        },
        {
            "title": "Causality Orientation",
            "content": "Based on the image, your task is to determine whether causal relationships exist between the following entity pairs. Entity pairs: {entity pairs} The causality criteria are as follows: For example, if the entity pairs are {{\"A\": or {{\"B\": [x1, y1, x2, y2], \"A\": - is in direct contact with B. - As presence maintains Bs current state. - Removing would cause to lose its current state. Then is the cause and is the effect. (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the bounding box. [x1, y1, x2, y2], \"B\": [x1, y1, x2, y2]}}: [x1, y1, x2, y2]}} Your output format should be as follows: <think> (Consider entity pairs and keep the reasoning as concise as possible.) </think> <causal pairs> (Output entity pairs with causal relationships only and if necessary, swap the ORDER of entities pairs to ensure the cause precedes the effect.) </causal pairs> E2E Visual Causal Discovery Identify all causal relationships between entities in the image based on the following criteria: - is in direct contact with B. - As presence maintains Bs current state. - Removing would cause to lose its current state. Then is the cause and is the effect. Please provide your reasoning process and output all the entity pairs with causal relationships and their bounding boxes in the following format: <think> (Provide your reasoning process for analyzing the image.) </think> <causal pairs> (Output all the entity pairs with causal relationships and their bounding boxes in the format of [\"cause\": y1, x2, y2], \"effect\": to replace the placeholders cause and effect in the format. top-left coordinate and (x2, y2) is the bottom-right coordinate of the bounding box.) </causal pairs> [x1, You should use ACTUAL ENTITY NAME (x1, y1) is the [x1, y1, x2, y2], \"effect\": [x1, y1, x2, y2], ...]. [x1, y1, x2, y2], \"cause\":"
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Tongji University"
    ]
}