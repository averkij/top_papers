{
    "paper_title": "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion",
    "authors": [
        "Xiang Zhang",
        "Yang Zhang",
        "Lukas Mehl",
        "Markus Gross",
        "Christopher Schroers"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in Novel View Synthesis (NVS), generating high-fidelity views from single or sparse observations remains a significant challenge. Existing splatting-based approaches often produce distorted geometry due to splatting errors. While diffusion-based methods leverage rich 3D priors to achieve improved geometry, they often suffer from texture hallucination. In this paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion model designed to synthesize high-fidelity novel views from a single image. Specifically, we propose an aligned synthesis strategy for precise control of target viewpoints and geometry-consistent view synthesis. To mitigate texture hallucination, we design a texture bridge module that enables high-fidelity texture generation through adaptive feature fusion. In this manner, SplatDiff leverages the strengths of splatting and diffusion to generate novel views with consistent geometry and high-fidelity details. Extensive experiments verify the state-of-the-art performance of SplatDiff in single-view NVS. Additionally, without extra training, SplatDiff shows remarkable zero-shot performance across diverse tasks, including sparse-view NVS and stereo video conversion."
        },
        {
            "title": "Start",
            "content": "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion XIANG ZHANG, ETH Zürich, Switzerland and DisneyResearchStudios, Switzerland YANG ZHANG, DisneyResearchStudios, Switzerland LUKAS MEHL, DisneyResearchStudios, Switzerland MARKUS GROSS, ETH Zürich, Switzerland and DisneyResearchStudios, Switzerland CHRISTOPHER SCHROERS, DisneyResearchStudios, Switzerland 5 2 0 2 8 1 ] . [ 1 2 5 7 2 1 . 2 0 5 2 : r Fig. 1. Performance comparison. Diffusion-based methods, e.g., ViewCrafter, usually hallucinate contents that are inconsistent with the input view. Splatting-based approaches, e.g., DepthSplat, often suffer from distorted geometry due to splatting errors. By contrast, our method produces novel views with consistent geometry and high-fidelity texture, achieving significantly better performance than previous arts on different tasks. Note that our model is trained only on the single-view novel view synthesis and is directly applied to the other tasks, showing promising cross-domain and cross-task performance. Despite recent advances in Novel View Synthesis (NVS), generating highfidelity views from single or sparse observations remains significant challenge. Existing splatting-based approaches often produce distorted geometry due to splatting errors. While diffusion-based methods leverage rich 3D priors to achieve improved geometry, they often suffer from texture hallucination. In this paper, we introduce SplatDiff, pixel-splatting-guided video diffusion model designed to synthesize high-fidelity novel views from single image. Specifically, we propose an aligned synthesis strategy for precise control of target viewpoints and geometry-consistent view synthesis. To mitigate texture hallucination, we design texture bridge module that enables high-fidelity texture generation through adaptive feature fusion. In this manner, SplatDiff leverages the strengths of splatting and diffusion to generate novel views with consistent geometry and high-fidelity details. Extensive experiments verify the state-of-the-art performance of SplatDiff in single-view NVS. Additionally, without extra training, SplatDiff shows remarkable zero-shot performance across diverse tasks, including sparse-view NVS and stereo video conversion. Authors addresses: Xiang Zhang, ETH Zürich, Zürich, Switzerland and DisneyResearchStudios, Zürich, Switzerland, xiangz.ethz@gmail.com; Yang Zhang, DisneyResearchStudios, Zürich, Switzerland, yang.zhang@disneyresearch.com; Lukas Mehl, DisneyResearchStudios, Zürich, Switzerland, lukas.mehl.-nd@disneyresearch.com; Markus Gross, ETH Zürich, Zürich, Switzerland and DisneyResearchStudios, Zürich, Switzerland, grossm@inf.ethz.ch; Christopher Schroers, DisneyResearchStudios, Zürich, Switzerland, christopher.schroers@disneyresearch.com. CCS Concepts: Computing methodologies Computational photography; 3D imaging. Additional Key Words and Phrases: Novel view synthesis, pixel splatting, video diffusion model"
        },
        {
            "title": "INTRODUCTION",
            "content": "Novel view synthesis (NVS) has attracted considerable interest in the fields of computer vision and computer graphics, showing various applications like augmented/virtual reality, 3D generation, and stereo video conversion [Gao et al. 2024; Mehl et al. 2024; Yu et al. 2024]. Compared with previous optimization-based NVS approaches, e.g., neural radiance field (NeRF) [Mildenhall et al. 2020] and 3D Gaussian splatting (3DGS) [Kerbl et al. 2023], that usually require dense input views and per-scene optimization, an emerging trend is to generate novel views from sparse views or even single image in feed-forward manner [Chen et al. 2025; Han et al. 2022; Szymanowicz et al. 2024a; Yu et al. 2024]. Due to the limited information from single/sparse views, such an NVS task is highly ill-posed and requires comprehensive scene understanding, including geometry, texture, and occlusion. Early works have proposed several techniques to tackle this challenging task, e.g., GAN-based inpainting for disocclusion regions [Wiles et al. 2020], feed-forward neural scene prediction [Yu et al. 2021], and implicit 3D transformers [Rombach , Vol. 1, No. 1, Article . Publication date: February 2025. 2 Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers et al. 2021]. In addition, various 3D representations are designed for efficient view synthesis, such as density fields [Wimbauer et al. 2023], multi-plane images [Han et al. 2022], and layered depth images [Shih et al. 2020]. Despite significant progress being achieved, previous methods are often confined to specific domains and struggle to generalize to complex in-the-wild scenes. This usually arises from the limited prior knowledge of the 3D world. Recent advancements in generative diffusion models have shown promising performance in variety of 3D vision tasks [Fu et al. 2025; Ke et al. 2024b; Zhang et al. 2024], including novel view synthesis [Sargent et al. 2024; Yu et al. 2024]. In order to generate highquality images/videos across wide array of domains, diffusion models are generally trained over internet-scale datasets, gaining rich prior knowledge of the visual world [Rombach et al. 2022; Xing et al. 2025]. Benefiting from this, generative diffusion models exhibit outstanding performance in generating geometry-consistent content, naturally fitting the requirements of novel view synthesis. Many works are devoted to repurposing diffusion models for high-quality NVS, like semantic-preserving generative warping [Seo et al. 2024] and point-conditioned video diffusion [Yu et al. 2024]. However, because of the generative nature, diffusion models often introduce hallucinated contents, such as different textures, when generating novel views. Consequently, existing diffusion-based NVS methods usually struggle with texture hallucination, failing to preserve the original appearance present in the input view (e.g., Fig. 1). Another popular trend is to render novel views with splattingbased approaches, e.g., Gaussian splatting [Chen et al. 2025; Xu et al. 2024]. For example, Flash3D employs zero-shot depth estimator to predict the 3D Gaussian position and directly estimates the parameters of the 3D Gaussian for novel view rendering [Szymanowicz et al. 2024a]. By enforcing the appearance consistency with pixel- /feature-level constraints, splatting-based NVS methods generally preserve better textures than diffusion-based approaches. However, since single or sparse observations provide only limited cues for the scene geometry, splatting-based methods often suffer from splatting errors, e.g., misalignment due to inaccurate depth, resulting in novel views with distorted geometry (e.g., Fig. 1). In this paper, we present SplatDiff, video diffusion model guided by pixel splatting, designed to leverage the strengths of splatting and diffusion for high-fidelity novel view synthesis. The motivations behind our designs are as follows: (i) Pixel Splatting: Compared with the popular 3D Gaussian splatting techniques, we found that the simple pixel splatting, such as forward warping [Niklaus and Liu 2020], better preserves appearance under single or sparse input views. While 3D Gaussians can theoretically model complex visual effects, e.g., view-dependent appearance, estimating accurate Gaussian parameters (such as opacity) from limited observations remains significant challenge. In addition, the estimation errors of Gaussian parameters often result in artifacts and cloudy effects (floaters) in novel views, yielding worse visual results than pixel splatting (e.g., see Fig. 8). (ii) Video Diffusion: Our video diffusion model is designed to synthesize consistent and high-fidelity novel views with the guidance of splatted results. When input observations are limited, the splatted views often exhibit disocclusion regions that vary across different viewpoints. By training on large-scale video datasets, video generative diffusion models gain deep understanding of visual , Vol. 1, No. 1, Article . Publication date: February 2025. elements such as geometry and texture. Leveraging this video diffusion prior, we synthesize realistic and consistent contents across varying viewpoints. To achieve consistent geometry and high-fidelity texture, we incorporate two key components in SplatDiff: novel strategy to create training pairs for aligned synthesis and texture bridge to inject texture details into the diffusion decoder. Specifically, we first fine-tune the pre-trained video diffusion model with Training Pair Alignment (TPA) and Splatting Error Simulation (SES) for aligned synthesis. TPA enforces the geometry and brightness consistency between the splatted and generated views, enabling precise control of target viewpoints. Meanwhile, our video diffusion model learns to eliminate splatting errors (e.g., flying pixels in Fig. 4b) with SES, generating aligned novel views with consistent geometry. To tackle texture hallucination, we propose texture bridge that aggregates the features from the splatted views for texture preservation. Additionally, texture degradation strategy is introduced to facilitate the adaptive fusion of splatted views and diffusion outputs for high-quality synthesis. In summary, our main contributions are: We introduce SplatDiff, pixel-splatting-guided video diffusion model for synthesizing novel views with consistent geometry and high-fidelity texture from single image. An aligned synthesis method to enable precise control of novel views while maintaining consistent geometry. In addition, we design texture bridge module to achieve highfidelity synthesis through adaptive feature fusion. SplatDiff excels in single-view NVS, sparse-view NVS, and stereo video conversion, demonstrating remarkable crossdomain and cross-task performance with training only on the single-view NVS task, as illustrated in Fig. 1."
        },
        {
            "title": "2 RELATED WORK\n2.1 Feed-Forward Novel View Synthesis",
            "content": "A significant number of attempts are devoted to synthesizing novel views from single/sparse observations in feed-forward manner. Due to the limited input information, early works usually adopt depth estimation methods to model scene geometry and then utilize inpainting approaches for content synthesis [Rockwell et al. 2021; Rombach et al. 2021; Wiles et al. 2020]. To generate realistic novel views, several techniques are developed, including GAN-based inpainting [Wiles et al. 2020], VQ-VAE outpainter [Rockwell et al. 2021], and implicit 3D transformer [Rombach et al. 2021]. Recently, novel scene representations are proposed to achieve high-quality view synthesis. For instance, pixelNeRF combines convolutional networks with NeRF representation to render novel views from two images [Yu et al. 2021]. Meanwhile, layer-based representations, e.g., multi-plane images (MPI) [Han et al. 2022; Khan et al. 2023; Li et al. 2021; Tucker and Snavely 2020] and layered depth images (LDI) [Jiang et al. 2023; Shih et al. 2020], are exploited for efficient rendering. However, since previous feed-forward NVS methods are mainly designed for specific domains, they often suffer from performance drops in complex scenes due to the limited model capability. High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 3 Fig. 2. Overview of SplatDiff. Given the input view, we first estimate the depth information from depth estimator and then perform pixel splatting to generate splatted views as diffusion conditioning. In our splatting-guided diffusion, we fine-tune the latent U-Net for aligned synthesis, producing consistent novel views while correcting splatting errors. Meanwhile, texture bridge module is designed to aggregate the encoder features for high-fidelity synthesis."
        },
        {
            "title": "2.2 Diffusion-Based Novel View Synthesis",
            "content": "Diffusion models have demonstrated exceptional performance in generating realistic images and videos [Rombach et al. 2022; Xing et al. 2025], reflecting profound understanding of the 3D world. To utilize the diffusion prior for novel view synthesis, previous attempts develop conditional diffusion frameworks, e.g., 3D featureconditioned diffusion [Chan et al. 2023] and viewpoint-conditioned diffusion [Liu et al. 2023], to generate novel views for simple inputs like 3D objects [Zheng and Vedaldi 2024]. Considering complex real-world scenes, multi-view diffusion models are often employed to synthesize high-quality novel views, which are then used to generate 3D scenes (e.g., 3D Gaussians) for novel view rendering [Liu et al. 2024; Wu et al. 2024]. Based on this, ZeroNVS combines diverse training datasets to acquire zero-shot NVS performance [Sargent et al. 2024], and Cat3D designs an efficient parallel sampling strategy for fast generation of 3D-consistent images [Gao et al. 2024]. In addition, GenWarp exploits the diffusion prior to achieve semanticpreserving warping [Seo et al. 2024]. Recent works also explore the potential of video diffusion models for novel view synthesis. For instance, ViewCrafter constructs point-conditioned video diffusion model to iteratively complete the point cloud for consistent view rendering [Yu et al. 2024], and StereoCrafter proposes tiled processing strategy to generate stereoscopic videos with video diffusion models [Zhao et al. 2024]. While diffusion-based NVS approaches excel at synthesizing realistic novel views, the generative nature of diffusion models often introduces hallucinated content (e.g., Fig. 1), leading to inconsistent texture across different viewpoints."
        },
        {
            "title": "2.3 Splatting-Based Novel View Synthesis",
            "content": "Splatting-based NVS approaches are typically trained in regression manner with pixel-level or feature-level constraints [Zhang et al. 2018]. As result, they often preserve better textures compared to diffusion-based methods. Previous study employs depth-based warping to achieve real-time novel view rendering [Cao et al. 2022]. With the rapid advancement of 3DGS techniques [Kerbl et al. 2023], considerable amount of attention has been drawn to feed-forward Gaussian splatting methods. The pioneer work pixelSplat estimates Gaussian parameters from neural networks and dense probability distributions, achieving efficient novel view synthesis with pair of images [Charatan et al. 2024]. Following this, several techniques are developed for improved performance and efficiency, including cost volume encoding [Chen et al. 2025] and depth-aware transformer [Zhang et al. 2025]. Recent method DepthSplat integrates monocular features from depth models and achieves better geometry in the estimated 3D Gaussians [Xu et al. 2024]. Instead of utilizing multi-view cues, another line of work focuses on predicting Gaussian parameters from single image. Splatter Image obtains 3D Gaussian parameters from pure image features [Szymanowicz et al. 2024b], and Flash3D employs zero-shot depth models for generalizable single-view NVS [Szymanowicz et al. 2024a]. However, due to the challenges of estimating accurate geometry from limited observations, existing splatting-based methods often suffer from splatting errors, resulting in novel views with distorted geometry (e.g., Fig. 1). By contrast, our SplatDiff leverages the geometric priors of diffusion models to correct splatting errors, achieving geometry-consistent and high-fidelity novel view synthesis."
        },
        {
            "title": "3 SPLATTING-GUIDED DIFFUSION",
            "content": "As depicted in Fig. 2, SplatDiff synthesizes novel views from single image in feed-forward manner. Given the input image, we predict the depth using an off-the-shelf depth estimator and then perform pixel splatting according to the camera poses. Since the splatted views often contain splatting errors and unknown regions, e.g., disocclusion, we leverage the video diffusion prior and propose splatting-guided diffusion to refine the splatted results. Specifically, we propose fine-tuning strategy to synthesize geometry-aligned contents while correcting the distortions caused by splatting errors. To tackle texture hallucination, we design texture bridge to aggregate encoder features for high-fidelity novel view synthesis. In the following sections, we first provide brief introduction to pixel splatting and video diffusion models in Sec. 3.1. The designs of the aligned synthesis strategy and the texture bridge module are then presented in Secs. 3.2 and 3.3, respectively. , Vol. 1, No. 1, Article . Publication date: February 2025. 4 Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers"
        },
        {
            "title": "3.2 Aligned Synthesis",
            "content": "Splatted View ViewCrafter Ours Unknown Region Diff. Map (ViewCrafter) Diff. Map (Ours) Fig. 3. Misalignment. The difference map shows the absolute difference between the splatted view and the generated view. Diffusion-based methods, e.g., ViewCrafter, often generate misaligned contents in novel views, resulting in significant differences across the image. In contrast, our SplatDiff is faithful to inputs and shows differences mainly around the unknown region."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Pixel Splatting. Given transformation map, e.g., optical flow, pixel splatting projects pixels from the input image to the target image, which has been widely used in computer vision tasks like frame interpolation [Niklaus and Liu 2020]. To generate novel views, we compute the view transformation map with camera poses and the estimated depth. Similar to prior works [Mehl et al. 2024; Niklaus and Liu 2020], we employ softmax splatting to resolve splatting collisions and assign pixel importance based on the depth information. This approach gives higher blending weights to foreground objects, preserving the geometric layout of the input view. Video Diffusion. Diffusion models typically consist of forward process and reverse process [Ho et al. 2020; Song et al. 2021]. The forward process 𝑞(x𝑡 x0, 𝑡) converts data x0 𝑝data (x) into Gaussian noise x𝑇 (0, I) by gradually adding noise at each step 𝑡 {1, . . . ,𝑇 }, and the learned reverse process 𝑝𝜃 (x𝑡 1x𝑡 , 𝑡) transforms random Gaussian noise to new sample with denoising network 𝜖𝜃 . At each denoising step, the network 𝜖𝜃 is supervised by min 𝜃 x𝑝data,𝜖N (0,I),𝑡 (𝑇 ) 𝜖 𝜖𝜃 (x𝑡 , 𝑡) 2 2 , (1) where 𝜖 is the Gaussian noise, and x𝑡 denotes the noisy sample at step 𝑡. By learning to estimate the added noise with 𝜖𝜃 , diffusion models are able to generate new data samples via iterative denoising. We build SplatDiff upon latent video diffusion models to balance performance and computational complexity. As shown in Fig. 2, we first generate the latent representations R𝐶 ℎ𝑤 for each image R3𝐻 𝑊 with latent encoder. Following that, we concatenate the latent codes = Cat({z}) as conditioning and perform the forward and reverse processes in the latent space, i.e., 𝑞(Z𝑡 Z0, 𝑡) and 𝑝𝜃 (Z𝑡 1Z𝑡 , 𝑡). Finally, the novel views are generated by decoding the denoised latent codes Z0 to the image space. Based on this pipeline, we propose the aligned synthesis strategy and the texture bridge module to achieve high-fidelity novel view synthesis. , Vol. 1, No. 1, Article . Publication date: February 2025. (2) Previous diffusion-based NVS approaches often generate textures and geometry that are misaligned with the conditioning  (Fig. 3)  , making precise control of target viewpoints challenging. This misalignment typically stems from the use of unaligned pairs during diffusion training. Given the depth and the camera poses p, naive training methods often construct training pairs {vtgt, xtgt} with vtgt = Render(xsrc, d, p), where Render() denotes novel view renderer, e.g., point cloud renderer [Yu et al. 2024]. xsrc, xtgt, and vtgt correspond to the source input view, the target view, and the rendered view, respectively. Then, the diffusion model is trained to predict xtgt conditioned on vtgt. However, the conditioning vtgt often shows different texture and geometry with the target view xtgt (e.g., Fig. 4a) due to several factors, e.g., different lighting and depth estimation error. This renders the diffusion model to produce misaligned novel views with the conditioned view, as shown in Fig. 4b. To this end, we propose the Training Pair Alignment (TPA) strategy for aligned synthesis. Training Pair Alignment. Instead of generating the diffusion conditioning from the input view xsrc, TPA utilizes the target view xtgt to construct aligned training pairs. As illustrated in Fig. 4a, we first estimate the view transformation map from xsrc and xtgt with an optical flow estimator [Xu et al. 2023]. With the estimated flow, we then generate splatting mask msplat via pixel splatting, where msplat indicates the valid splatting regions. Finally, we construct the aligned training pairs { vtgt, xtgt} by masking the target view, i.e., vtgt = xtgt msplat. Since vtgt aligns with xtgt in both geometry and texture, the diffusion model trained with TPA learns to adhere to the conditioned view, producing consistent novel views (e.g., Fig. 4b). However, due to the existence of splatting errors in real splatted views, the model trained solely with TPA tends to be misled by the splatting errors, resulting in artifacts as shown in the green box in Fig. 4b. To address this, we propose the Splatting Error Simulation (SES) method to enhance the models robustness against splatting errors. Splatting Error Simulation. Splatting errors in pixel splatting often appear as flying pixels, e.g., green box in Fig. 4b, which typically arise from inaccuracies in the transformation map, e.g., blurred depth discontinuities [Shih et al. 2020]. As result, pixels around object boundaries might be projected to incorrect positions in the target view, leading to distorted geometry with flying pixels. To resolve this, we propose to simulate splatting errors in the training pairs (Fig. 4a). Since the flying pixels often stem from the object boundaries, we first generate an edge mask medge from the transformation map using Sobel operator. Next, we extract the edge regions esrc from the input view xsrc by esrc = xsrc medge. The extracted edge regions esrc are then splatted to the target view using the optical flow to generate the splatted edge etgt. Finally, we simulate the splatting errors in vtgt by random perturbation: (3) ˆvtgt = etgt merror + vtgt (1 merror), (4) where ˆvtgt denotes the rendered view with simulated splatting errors, and merror is randomly generated mask indicating where to introduce splatting errors. By using { ˆvtgt, xtgt} as training pairs, our High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 5 (a) Comparison between naive and our training pair generation Input View Splatted View Naive Training Training w/ TPA Training w/ TPA and SES (b) Results of the proposed training pair alignment (TPA) and splatting error simulation (SES) Fig. 4. Aligned Synthesis. Naive training pair generation often produces pairs that are locally unaligned in geometry and texture (green box in (a)), which results in unaligned novel view synthesis (naive training in (b)). By masking the target view, the proposed training pair alignment (TPA) enables aligned synthesis, but the generated contents tend to be misled by the splatting errors in the splatted view (training with TPA in (b)). Combining TPA with splatting error simulation (SES) in training, our SplatDiff learns to handle splatting errors and generates geometryand texture-aligned novel views. Fig. 5. Texture bridge. The texture bridge is designed to complement multi-scale features from the encoder to the decoder, enabling high-fidelity synthesis. For training, we intentionally degrade the texture of the target view with the diffusion model, simulating degraded view with hallucinated textures. This facilitates the texture bridge to learn how to adaptively fuse information from the splatted view and the diffusion outputs. diffusion model learns to correct splatting errors by utilizing its rich geometric prior, while maintaining aligned synthesis (Fig. 4b)."
        },
        {
            "title": "3.3 Texture Bridge",
            "content": "Although the diffusion model can generate geometry-consistent novel views with our aligned synthesis strategy, its outputs often contain hallucinated textures that are inconsistent with the input view  (Fig. 1)  . One potential solution is to preserve the texture of the input image by utilizing the splatted view. However, directly blending the splatted view and the diffusion output faces several challenges: (i) the splatted view usually contains aliasing artifacts, such as jagged edges, and may exhibit blurred details due to the resampling process, which can degrade the quality of the novel view, and (ii) detecting and removing splatting errors in the splatted view is crucial for achieving high-quality NVS. Thus, we propose the texture bridge module to adaptively fuse the splatted view with the diffusion output for high-fidelity synthesis. As depicted in Fig. 5, the texture bridge consists of fusion block at each feature scale. Let f𝑖 enc denote the 𝑖-th scale encoder feature extracted from the splatted view, and let f𝑖 dec represent the 𝑖-th scale decoder feature. We first fuse the features at each scale with the texture bridge, i.e., 𝑖 fuse = Fuse𝑖 (f 𝑖 enc , 𝑖 dec ), (5) fuse and then restore the novel view by passing the fused features {f𝑖 } through the latent decoder. Each fusion block Fuse𝑖 () is implemented using two residual blocks [He et al. 2016], though more advanced architecture, e.g., self-attention, could be employed for better performance. With multi-scale fusion, the texture bridge effectively aggregates fine-grained features for high-quality synthesis. Training with Texture Degradation. Our texture bridge is designed to adaptively select optimal features from the splatted view and the diffusion output for view synthesis. For example, the texture bridge should mainly utilize the diffusion output when encountering splatting errors, while relying more on the splatted view in the regions with texture hallucination. To achieve this, one possible , Vol. 1, No. 1, Article . Publication date: February 2025. Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers Table 1. Quantitative evaluation of single-view novel view synthesis. * denotes methods with two input views. Best and second-best results are marked. RealEstate10K Dataset Method Easy Set PSNR SSIM LPIPS DISTS FID Hard Set PSNR SSIM LPIPS DISTS FID Syn-Sin SV-MPI BTS Splatter Image MINE AdaMPI GenWarp ViewCrafter Flash3D SplatDiff (Ours) - 27.10 - 28.15 28.45 28.03 16.94 20.61 28.46 28.53 - 0.870 - 0.894 0.897 0.892 0.519 0.705 0.899 0.895 - - - 0.110 0.111 0.104 0.318 0.242 0.100 0.096 - - - - - - 0.137 0.139 0.062 0.059 - - - - - - 12.27 13.71 4.55 3. 22.30 23.52 24.00 24.15 24.75 23.54 16.05 16.64 24.93 25.17 0.740 0.785 0.755 0.810 0.820 0.809 0.488 0.588 0.833 0.820 - - 0.194 0.177 0.179 0.184 0.356 0.347 0.160 0.154 - - - - - - 0.149 0.185 0.098 0.088 - - - - - - 12.66 18.30 8.42 6.04 DL3DV-10K Dataset Method Easy Set PSNR SSIM LPIPS DISTS FID Hard Set PSNR SSIM LPIPS DISTS FID GenWarp ViewCrafter DepthSplat* SplatDiff (Ours) 17.72 23.21 24.37 26.14 0.463 0.694 0.790 0. 0.309 0.182 0.168 0.113 0.357 0.349 0.109 0.068 77.31 46.60 52.01 24.23 16.74 19.77 21.79 22.48 0.409 0.565 0.709 0.732 0.350 0.249 0.221 0. 0.352 0.349 0.124 0.092 81.58 59.56 61.37 41.22 training approach is to feed the texture bridge with the splatted view and the corresponding diffusion output, then supervise the decoded results using the target view. However, the diffusion outputs often differ significantly from the target view in the unknown regions (e.g., disocclusion), resulting in sub-optimal training performance. To overcome this, we propose texture degradation strategy to facilitate model training. Specifically, we employ the diffusion model to degrade the texture of the target view and train the texture bridge using the degraded view  (Fig. 5)  . As result, the degraded view shares similar contents to the target view while containing hallucinated textures for training. For supervision, we employ the ℓ1 loss L1 and the perceptual loss LLPIPS [Zhang et al. 2018], i.e., = L1 + 𝛼 LLPIPS, where 𝛼 = 0.1. With the texture bridge, our SplatDiff addresses texture hallucination through the adaptive fusion of the splatted view and the diffusion output. Meanwhile, the texture bridge also learns to refine the input features, e.g., aliasing artifacts and blurry details in the splatted view, for high-fidelity novel view synthesis. (6)"
        },
        {
            "title": "4 EXPERIMENTS AND ANALYSIS\n4.1 Experimental Settings",
            "content": "days on single NVIDIA RTX A6000 GPU. At inference, we apply the DDIM scheduler [Song et al. 2021] with 50-step sampling. Datasets and Evaluation. We use two datasets RealEstate10K [Zhou et al. 2018] and DL3DV-10K [Ling et al. 2024] for training and evaluation. For each dataset, two evaluation sets (easy and hard sets) are created with different baseline ranges. In RealEstate10K, we follow the setting in Flash3D [Szymanowicz et al. 2024a] to skip 5 and random 30 frames for the easy and hard sets. Since DL3DV10K features faster camera motion and more complex scenes, we skip 3 and 6 frames for the easy and hard sets. For the methods requiring depth as inputs, we use the same depth models for fair comparisons (UniDepth [Piccinelli et al. 2024] in RealEstate10K and DepthSplat [Xu et al. 2024] in DL3DV-10K). Quantitative evaluation is conducted with pixel-level metrics (PSNR and SSIM), feature-level metrics (LPIPS [Zhang et al. 2018] and DISTS [Ding et al. 2022]), and distribution-level metric FID [Heusel et al. 2017]. Since the generated novel views are often not perfectly aligned with the ground-truth due to depth estimation errors, the importance of these metrics follows the hierarchy: distribution-level > feature-level > pixel-level. In-the-wild samples are also collected for qualitative evaluation."
        },
        {
            "title": "4.2 Benchmarking",
            "content": "Implementation Details. We implement SplatDiff based on the open-source video diffusion model DynamiCrafter [Xing et al. 2025] with ViewCrafter weight initialization [Yu et al. 2024]. We employ the AdamW optimizer [Loshchilov and Hutter 2019] to train SplatDiff under 320 512 patches and batch size 16. For aligned synthesis, we only fine-tune the latent U-Net for 1.5K steps with learning rate 1105. Afterward, we train the texture bridge for 10K steps with learning rate 1104. The total training takes around 2.5 Tab. 1 shows the single-view NVS results of SplatDiff compared with prior arts. Previous diffusion-based NVS approaches, e.g., ViewCrafter, struggle to achieve good metrics due to the hallucinated geometry and textures as depicted in Figs. 6 and 7. Although splatting-based methods, e.g., Flash3D, better preserve texture from the input view, the generated novel views often suffer from geometry distortion due to splatting errors (Fig. 6a). Compared with them, our SplatDiff produces the best novel views with consistent geometry and , Vol. 1, No. 1, Article . Publication date: February 2025. High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 7 Input View Target View ViewCrafter Flash3D SplatDiff (Ours) (a) Visual comparisons on the RealEstate10K dataset Input View Target View ViewCrafter DepthSplat* SplatDiff (Ours) (b) Visual comparisons on the DL3DV-10K dataset Fig. 6. Qualitative comparisons on the single-view novel view synthesis task. * indicates that the method uses two views as input. , Vol. 1, No. 1, Article . Publication date: February 2025. 8 Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers Input View Splatted View ViewCrafter SplatDiff (Ours) Fig. 7. Qualitative comparisons on in-the-wild high-resolution (5761024) samples. Table 2. Ablation on single-view novel view synthesis. TPA, SES, TB, and TD represent training pair alignment, splatting error simulation, texture bridge, and texture degradation. Best and second-best results are marked. Table 3. Quantitative evaluation of sparse-view novel view synthesis. indicates the averaged results of the two generated novel views. The best and second-best results are marked. ID TPA SES TB TD DL3DV-10K Dataset PSNR SSIM LPIPS DISTS FID Method DL3DV-10K Dataset (In-domain) PSNR SSIM LPIPS DISTS FID #1 #2 #3 #4 #5 23.21 24.90 25.00 26.05 26.14 0.694 0.749 0.749 0.825 0.826 0.182 0.178 0.178 0.118 0. 0.349 0.100 0.098 0.070 0.068 46.60 43.97 43.59 25.49 24.23 high-fidelity textures as shown in Fig. 6. In addition, when better depth maps are available (e.g., on DL3DV-10K, where the depth is estimated from two views), SplatDiff shows significantly better performance than both diffusion-based and splatting-based approaches (Tab. 1). With our aligned synthesis strategy and texture bridge module, SplatDiff even outperforms the two-view method DepthSplat and generates better visual results with fine-grained details (Fig. 6b)."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "In Tab. 2, we study the effectiveness of each design: training pair alignment (TPA), splatting error simulation (SES), texture bridge (TB), and texture degradation (TD). (i) Aligned Synthesis: Compared with the baseline model (#1), the model with TPA (#2) enforces the consistency between the conditioned view and the generated view, boosting the novel view synthesis performance (e.g., 1.69 dB PSNR gain). With SES, the diffusion model further learns to correct splatting errors by leveraging its rich geometric prior. Consequently, the model with TPA and SES achieves aligned synthesis while preserving geometric consistency (Fig. 4b). (ii) Texture Bridge: To , Vol. 1, No. 1, Article . Publication date: February 2025. MVSplat DepthSplat SplatDiff (Ours) 17.54 19.05 21.42 0.529 0.610 0.619 0.402 0.313 0. - 0.163 0.130 - 105.09 71.26 Method DTU Dataset (Cross-domain) PSNR SSIM LPIPS DISTS FID pixelSplat MVSplat TranSplat DepthSplat SplatDiff (Ours) SplatDiff (Ours) 12.89 13.94 14.93 16.01 15.96 16. 0.382 0.473 0.531 0.612 0.590 0.616 0.560 0.385 0.326 0.334 0.264 0.285 - - - 0.201 0.147 0.164 - - - 130.75 82.45 98.98 handle texture hallucination, TB aggregates the multi-scale features from the splatted view and the diffusion output, leading to significant improvements across all metrics (#4 in Tab. 2). Furthermore, training with TD improves feature fusion and refines the input features for better synthesis. As result, the model incorporating all designs achieves the best NVS performance (#5 in Tab. 2)."
        },
        {
            "title": "4.4 Applications",
            "content": "We directly apply the SplatDiff pre-trained on single-view NVS to different tasks, i.e., sparse-view NVS and stereo video conversion. By simply modifying the model inputs, SplatDiff demonstrates remarkable cross-domain performance as shown in Tabs. 3 and 4. High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 9 Input Views Target View DepthSplat Ours Inputs Target View DepthSplat Ours (a) Visual comparisons on the DL3DV-10K dataset (b) Visual comparisons on the DTU dataset Fig. 8. Sparse-view novel view synthesis results on the DL3DV-10K (in-domain) and DTU (cross-domain) datasets. Input View (Left) ViewCrafter StereoCrafter SplatDiff (Ours) Fig. 9. Stereo video conversion results on the Spring dataset. Right-eye views are synthesized based on the input left-eye views. Table 4. Quantitative evaluation of stereo video conversion. The best and second-best results are marked. Method Spring Dataset PSNR SSIM LPIPS DISTS FID ViewCrafter StereoCrafter SplatDiff (Ours) 18.41 26.46 30. 0.526 0.765 0.908 0.266 0.192 0.073 0.187 0.175 0.070 143.31 135.86 46.37 Sparse-View Novel View Synthesis. We evaluate the sparse-view NVS performance with two input views. Given the two splatted views, we simply select the one with fewer unknown regions as the primary input and use the other one to fill the unknown regions with blurred blending mask. We follow DepthSplat to conduct the in-domain evaluation on the DL3DV-10K dataset. Under limited observations, it is challenging to estimate accurate parameters for 3D Gaussians, which often leads to cloudy results in novel views (Fig. 8a). With pixel splatting, SplatDiff achieves state-of-the-art performance as shown in Tab. 3. We also follow MVSplat to conduct the cross-domain evaluation on the DTU dataset [Jensen et al. 2014]. Due to the domain gap, the estimated depth is less accurate, leading to misalignment between the splatted view and the target view. In such cases, Gaussian splatting approaches, e.g., DepthSplat, often blur details to handle the misalignment, resulting in slightly higher pixel-level metrics in Tab. 3. By contrast, our SplatDiff achieves significantly better visual quality with fine-grained details (Fig. 8b). To mitigate the misalignment in SplatDiff, we additionally average the novel views generated from the two splatted views, which outperform DepthSplat across all metrics (SplatDiff in Tab. 3). Stereo Video Conversion. Stereo video conversion is widely used in movie production, and thus we employ the Spring dataset [Mehl et al. 2023] for evaluation, which features high-resolution stereo videos from the Blender movie \"Spring\". We generate the splatted right-eye views for SplatDiff from the input left-eye videos, and the ground-truth disparity is used in all methods for fair comparisons. As shown in Fig. 9, it is challenging for ViewCrafter to handle dynamic scenes, resulting in significantly different contents in the novel views. Although StereoCrafter generates more consistent novel views with the inputs, the synthesized views often suffer from , Vol. 1, No. 1, Article . Publication date: February 2025. 10 Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers texture hallucination and blurry details. Benefiting from the aligned synthesis strategy, our SplatDiff can be directly applied to dynamic videos without additional training. Meanwhile, the proposed texture bridge preserves high-fidelity details from inputs, achieving the best stereo video conversion performance (Tab. 4 and Fig. 9)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose SplatDiff, pixel-splatting-guided video diffusion model designed for geometry-consistent and high-fidelity novel view synthesis. With the aligned synthesis strategy, SplatDiff achieves precise control of target viewpoints while effectively correcting geometry distortion caused by splatting errors. In addition, the proposed texture bridge recovers high-fidelity texture via adaptive feature fusion. Extensive experiments across single-view novel view synthesis, sparse-view novel view synthesis, and stereo video conversion verify the versatility and state-of-the-art performance of SplatDiff."
        },
        {
            "title": "REFERENCES",
            "content": "Ang Cao, Chris Rockwell, and Justin Johnson. 2022. Fwd: Real-time novel view synthesis with forward warping and depth. In CVPR. 1571315724. Eric Chan, Koki Nagano, Matthew Chan, Alexander Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. Generative novel view synthesis with 3d-aware diffusion models. In ICCV. 42174229. David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. 2024. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR. 1945719467. Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. 2025. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV. Springer, 370386. Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. 2022. Image quality assessment: Unifying structure and texture similarity. IEEE TPAMI 44, 5 (2022), 25672581. https://doi.org/10.1109/TPAMI.2020. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. 2025. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In ECCV. Springer, 241258. Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo MartinBrualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. 2024. Cat3d: Create anything in 3d with multi-view diffusion models. In NeurIPS. Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. 2022. Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images. In ACM SIGGRAPH 2022 Conference Proceedings (Vancouver, BC, Canada) (SIGGRAPH 22). Association for Computing Machinery, New York, NY, USA, Article 14, 8 pages. https://doi.org/10. 1145/3528233.3530755 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. 770778. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS 30 (2017). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS, Vol. 33. 68406851. Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. 2014. Large scale multi-view stereopsis evaluation. In CVPR. 406413. Yutao Jiang, Yang Zhou, Yuan Liang, Wenxi Liu, Jianbo Jiao, Yuhui Quan, and Shengfeng He. 2023. Diffuse3D: Wide-Angle 3D Photography via Bilateral Diffusion. In ICCV. 89989008. Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, and Konrad Schindler. 2024a. Video Depth without Video Models. arXiv preprint arXiv:2411.19189 (2024). Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. 2024b. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR. 94929502. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. 42, 4 (July 2023). https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ Numair Khan, Lei Xiao, and Douglas Lanman. 2023. Tiled multiplane images for practical 3d photography. In ICCV. 1045410464. Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. 2021. Mine: Towards continuous depth mpi with nerf for novel view synthesis. In ICCV. 1257812588. , Vol. 1, No. 1, Article . Publication date: February 2025. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. 2024. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR. 2216022169. Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. 2024. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767 (2024). Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV. 92989309. Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In ICLR. Lukas Mehl, Andrés Bruhn, Markus Gross, and Christopher Schroers. 2024. Stereo Conversion with Disparity-Aware Warping, Compositing and Inpainting. In WACV. 42604269. Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andrés Bruhn. 2023. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In CVPR. 49814991. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. Simon Niklaus and Feng Liu. 2020. Softmax splatting for video frame interpolation. In CVPR. 54375446. Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. 2024. UniDepth: Universal Monocular Metric Depth Estimation. In CVPR. 1010610116. Chris Rockwell, David Fouhey, and Justin Johnson. 2021. Pixelsynth: Generating 3d-consistent experience from single image. In ICCV. 1410414113. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. 10684 10695. Robin Rombach, Patrick Esser, and Björn Ommer. 2021. Geometry-free view synthesis: Transformers and no 3d priors. In ICCV. 1435614366. Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. 2024. ZeroNVS: Zero-Shot 360-Degree View Synthesis from Single Image. In CVPR. 94209429. Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. 2024. GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping. In NeurIPS. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3d photography using context-aware layered depth inpainting. In CVPR. 80288038. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising diffusion implicit models. In ICLR. Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, João Henriques, Christian Rupprecht, and Andrea Vedaldi. 2024a. Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from Single Image. arXiv preprint arXiv:2406.04343 (2024). Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. 2024b. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR. 1020810217. Richard Tucker and Noah Snavely. 2020. Single-view view synthesis with multiplane images. In CVPR. 551560. Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. Synsin: End-to-end view synthesis from single image. In CVPR. 74677477. Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel Cremers. 2023. Behind the scenes: Density fields for single view reconstruction. In CVPR. 90769086. Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. 2024. Reconfusion: 3d reconstruction with diffusion priors. In CVPR. 2155121561. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. 2025. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV. Springer, 399417. Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. 2024. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862 (2024). Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. 2023. Unifying flow, stereo and depth estimation. IEEE TPAMI (2023). Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR. 45784587. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. 2024. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048 (2024). Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, and Haoqian Wang. 2025. Transplat: Generalizable 3d gaussian splatting from sparse multi-view images with transformers. In AAAI. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In CVPR. 586595. Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, and Christopher Schroers. 2024. Betterdepth: Plugand-play diffusion refiner for zero-shot monocular depth estimation. In NeurIPS. Sijie Zhao, Wenbo Hu, Xiaodong Cun, Yong Zhang, Xiaoyu Li, Zhe Kong, Xiangjun Gao, Muyao Niu, and Ying Shan. 2024. Stereocrafter: Diffusion-based generation of long and high-fidelity stereoscopic 3d from monocular videos. arXiv preprint arXiv:2409.07447 (2024). Chuanxia Zheng and Andrea Vedaldi. 2024. Free3d: Consistent novel view synthesis without 3d representation. In CVPR. 97209731. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018. Stereo magnification: learning view synthesis using multiplane images. ACM Trans. Graph. 37, 4, Article 65 (July 2018), 12 pages. https://doi.org/10.1145/3197517. 3201323 MORE EXPERIMENTS A.1 Texture Degradation The goal of our texture bridge is to adaptively fuse the splatted view and the diffusion output for high-quality view synthesis. As depicted in Fig. 10, the splatted view shows better textures (green box) but contains unknown regions and splatting errors. Although the diffusion output generates reasonable contents for the unknown regions (red box), they often suffer from texture hallucination (green box). Ideally, the texture bridge should learn to detect the problematic regions, e.g., splatting errors and hallucinated textures, and adaptively utilize better features for synthesis. However, directly using the diffusion output often leads to sub-optimal training performance mainly due to the inconsistent contents in the unknown regions (red box in Fig. 10). To address this, we propose the texture degradation strategy and generate the degraded views for training. As shown in Fig. 10, the degraded view shares similar contents with the target view while imitating the texture hallucination effects in real diffusion outputs. By substituting the diffusion output with the degraded view for training, our texture bridge better learns to rely more on the generated contents for the unknown regions while maintaining the ability to detect hallucinated textures. A.2 Visual Comparisons in Ablation Study Fig. 11 provides visual results for the ablation study (Tab. 2 in the main paper). We also compute the absolute difference map between the splatted views and the corresponding novel views to visualize the difference regions. It is obvious that the baseline model synthesizes misaligned novel views and produces hallucinated textures, leading to significant difference regions across the entire image. With the aligned synthesis strategy, the model (#3 in Tab. 2 of the main paper) generates geometry-consistent results but still suffers from texture hallucination as shown in the green box of Fig. 11. By contrast, our SplatDiff utilizes the information from the splatted view with the proposed texture bridge, achieving high-fidelity novel view synthesis. MORE VISUAL RESULTS B.1 Zero-Shot Performance To verify the zero-shot performance of SplatDiff, we provide additional visual comparisons on diverse in-the-wild samples, including landscapes, buildings, animals, and paintings  (Fig. 12)  . Previous High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 11 approaches, e.g., ViewCrafter [Yu et al. 2024], often synthesize contents that differ from the inputs, like the hallucinated lighting and the rooftop texture in the first-row example of Fig. 12. In contrast, our method preserves the geometric layout while recovering finegrained details as shown in the green box of Fig. 12. In addition, benefiting from the aligned synthesis strategy, our method corrects the splatting errors and fills reasonable contents for the unknown regions, e.g., the green boxes in the bird and the axe examples depicted in Fig. 12. B.2 Video Results in Stereo Conversion In Fig. 13, we provide more stereo video conversion results on the Spring dataset [Mehl et al. 2023]. Since ViewCrafter is trained only on static scenes, it is challenging for it to handle dynamic inputs, resulting in severe content hallucination as shown in Fig. 13. Although our method is trained on the same static datasets as ViewCrafter, SplatDiff exhibits promising zero-shot performance in dynamic scenes. This is because our texture bridge module leverages the splatted views to maintain the same motion as the input video. Meanwhile, with our aligned synthesis strategy, the diffusion model performs similarly to inpainting models and fills reasonable contents for the unknown regions, e.g., disocclusions in the last two rows of Fig. 13a, achieving high-quality synthesis of the stereo video. Compared with StereoCrafter [Zhao et al. 2024] that is designed specifically for stereo video conversion, our method still shows superior performance in synthesis quality and consistency. As depicted in Fig. 13a, StereoCrafter tends to produce novel views with smoothed details, e.g., the rocks on the road, and fills blurred contents for the disocclusion regions. For consistency, while StereoCrafter generates better novel views than ViewCrafter, inconsistent details are often observed in the outputs of StereoCrafter, such as the girls eyes in the green box of Fig. 13b. Compared with previous approaches, our method produces the best stereo conversion results with consistent geometry and realistic details, verifying the effectiveness and versatility of SplatDiff."
        },
        {
            "title": "C LIMITATIONS AND FUTURE WORKS",
            "content": "While our SplatDiff achieves remarkable performance on many novel view synthesis tasks, limitations still exist: (i) View-Dependent Effects: Compared with the popular Gaussian splatting techniques, SplatDiff has demonstrated the effectiveness of the pixel splatting method under limited input views. However, since current pixel splatting methods generally assume the brightness constancy across different viewpoints, how to handle the view-dependent effects, e.g., reflective surfaces, remains an open problem. Although Gaussians are capable of modeling view-dependent effects, estimating accurate Gaussian parameters from limited observations is challenging. One potential solution is to leverage the rich prior from foundation models to facilitate the modeling of view-dependent effects, and we leave it for future works. (ii) Long-Range Consistency: Most existing video diffusion models are designed to output pre-defined number of images, and thus multiple inferences are required to handle long-range inputs, e.g., long videos in stereo conversion. However, due to the generative nature of diffusion models, the synthesized contents are usually different in multiple inferences. Although our , Vol. 1, No. 1, Article . Publication date: February 2025. 12 Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers Splatted View Diffusion Output Degraded View Target View Fig. 10. Example of texture degradation. Although the original diffusion output fills the unknown regions in the splatted view, the generated contents usually differ from the target view (red box), resulting in sub-optimal training performance. Thus, we employ the degraded view for training, which shares similar contents with the target view while imitating the texture hallucination effects (green box) in real diffusion outputs. Splatted View Baseline (#1) w/ Aligned Synthesis (#3) w/ All (#5) Unkown Region Diff. Map (Baseline) Diff. Map (w/ Aligned Synthesis) Diff. Map (w/ All) Fig. 11. Visual comparisons of ablation study. The difference map shows the absolute difference between the splatted view and the corresponding novel view. The model ID is consistent with Tab. 2 in the main paper. The baseline model generates misaligned novel view with the conditioned splatted view, showing significant differences across the image. With the proposed aligned synthesis strategy, model #3 better follows the conditioning but still suffers from texture hallucination (green box). Combining the aligned synthesis and the texture bridge, our model (#5) synthesizes geometry-aligned novel views while recovering high-fidelity texture. approach utilizes the texture bridge to maintain the consistency on the splatted regions, the contents on the unknown regions, e.g., disocclusions, might differ. To achieve long-range consistency, one could draw inspiration from the recent video-based methods, e.g., rolling inference [Ke et al. 2024a], for improved consistency of diffusion outputs, and combine the techniques in our SplatDiff for high-fidelity view synthesis. , Vol. 1, No. 1, Article . Publication date: February 2025. High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 13 Input View Splatted View ViewCrafter SplatDiff (Ours) Fig. 12. Qualitative comparisons on in-the-wild high-resolution (5761024) samples. , Vol. 1, No. 1, Article . Publication date: February 2025. Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, and Christopher Schroers Input Views (Left) ViewCrafter StereoCrafter SplatDiff (Ours) (a) Visual comparisons on details Input Views (Left) ViewCrafter StereoCrafter SplatDiff (Ours) (b) Visual comparisons on consistency Fig. 13. Stereo video conversion results on the Spring dataset. Right-eye views are synthesized based on the input left-eye views. , Vol. 1, No. 1, Article . Publication date: February 2025."
        }
    ],
    "affiliations": [
        "DisneyResearchStudios, Switzerland",
        "ETH Zürich, Switzerland"
    ]
}