{
    "paper_title": "VLM-Guided Adaptive Negative Prompting for Creative Generation",
    "authors": [
        "Shelly Golan",
        "Yotam Nitzan",
        "Zongze Wu",
        "Or Patashnik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creative generation is the synthesis of new, surprising, and valuable samples that reflect user intent yet cannot be envisioned in advance. This task aims to extend human imagination, enabling the discovery of visual concepts that exist in the unexplored spaces between familiar domains. While text-to-image diffusion models excel at rendering photorealistic scenes that faithfully match user prompts, they still struggle to generate genuinely novel content. Existing approaches to enhance generative creativity either rely on interpolation of image features, which restricts exploration to predefined categories, or require time-intensive procedures such as embedding optimization or model fine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a training-free, inference-time method that promotes creative image generation while preserving the validity of the generated object. Our approach utilizes a vision-language model (VLM) that analyzes intermediate outputs of the generation process and adaptively steers it away from conventional visual concepts, encouraging the emergence of novel and surprising outputs. We evaluate creativity through both novelty and validity, using statistical metrics in the CLIP embedding space. Through extensive experiments, we show consistent gains in creative novelty with negligible computational overhead. Moreover, unlike existing methods that primarily generate single objects, our approach extends to complex scenarios, such as generating coherent sets of creative objects and preserving creativity within elaborate compositional prompts. Our method integrates seamlessly into existing diffusion pipelines, offering a practical route to producing creative outputs that venture beyond the constraints of textual descriptions."
        },
        {
            "title": "Start",
            "content": "VLM-Guided Adaptive Negative Prompting for Creative Generation SHELLY GOLAN, Technion YOTAM NITZAN, Adobe Research ZONGZE WU, Adobe Research OR PATASHNIK, Tel Aviv University 5 2 0 2 2 1 ] . [ 1 5 1 7 0 1 . 0 1 5 2 : r Fig. 1. Our method generates creative concepts such as novel pets, uniquely designed jackets, and unconventional buildings by steering the generation away from conventional patterns using VLM-Guided Adaptive Negative Prompting process. Creative generation is the synthesis of new, surprising, and valuable samples that reflect user intent yet cannot be envisioned in advance. This task aims to extend human imagination, enabling the discovery of visual concepts that exist in the unexplored spaces between familiar domains. While text-toimage diffusion models excel at rendering photorealistic scenes that faithfully match user prompts, they still struggle to generate genuinely novel content. Existing approaches to enhance generative creativity either rely on interpolation of image features, which restricts exploration to predefined categories, or require time-intensive procedures such as embedding optimization or model fine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, training-free, inference-time method that promotes creative image generation while preserving the validity of the generated object. Our approach utilizes vision-language model (VLM) that analyzes intermediate outputs of the generation process and adaptively steers it away from conventional visual concepts, encouraging the emergence of novel and surprising outputs. We evaluate creativity through both novelty and validity, using statistical metrics in the CLIP embedding space. Through extensive experiments, we show consistent gains in creative novelty with negligible computational overhead. Moreover, unlike existing methods that primarily generate single objects, our approach extends to complex scenarios, such as generating Authors Contact Information: Shelly Golan, Technion, , shellygo2@gmail.com; Yotam Nitzan, Adobe Research, ; Zongze Wu, Adobe Research, ; Or Patashnik, Tel Aviv University, . coherent sets of creative objects and preserving creativity within elaborate compositional prompts. Our method integrates seamlessly into existing diffusion pipelines, offering practical route to producing creative outputs that venture beyond the constraints of textual descriptions."
        },
        {
            "title": "Introduction",
            "content": "A growing body of research [Hertzmann 2018; Ivcevic and Grandinetti 2024; Yongjun et al. 2025] revolves around somewhat philosophical question: what are creativity and originality, and can computers create art? One suggestion by Boden [2009] is to categorize computational creativity along spectrum of increasing novelty. At the lowest level, combinatorial creativity produces unexpected combinations of existing concepts, such as hybrid creature that merges features of bee and giraffe. Exploratory creativity goes further by discovering new possibilities within known domain while maintaining validity, for instance, inventing an animal species with entirely new but biologically plausible traits. At the highest level, transformational creativity challenges the boundaries of existing categories altogether, such as conceiving an organism so unlike current life forms that it forces us to reconsider the definition of animal itself. 2 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik GPT-4o GPT-o3 SDXL Fig. 2. Images generated with GPT-o3 [OpenAI 2025], GPT-4o [OpenAI 2024], SDXL [Podell et al. 2023], FLUX-dev [Black Forest Labs 2024], and SD3.5 [Esser et al. 2024] using the prompt Professional high-quality photo of new type of pet. SD3.5 FLUX Recent advances in text-to-image (T2I) diffusion models have demonstrated strong capabilities in generating photorealistic images from natural language prompts. These models excel at reproducing and recombining simple visual concepts from their training data, allowing for combinatorial creativity to some extent. However, they still struggle with novelty that falls under the category of exploratory and transformational creativity. This limitation reflects an inherent tension in generative modeling between mode coverage (i.e., capturing the full distribution), and mode seeking (i.e., generating high-quality typical samples). For example, known technique that attempts to navigate this tradeoff is Classifier-free guidance (CFG). Lower guidance scales increase diversity but compromise text alignment, while higher scales improve prompt adherence but generate more typical outputs. Our experiments show that simple prompt modifications fail to produce creative outputs from current models. As demonstrated in Figure 2, adding creativity-related terms such as creative or new type of produces outputs that remain similar to conventional pets like blue cat with wings, kittens, dogs, or ferret-like animal with long ears. On the other hand, our blue pet, presented in Figure 1, cannot be described as combination of known pets. Existing frameworks for creative generation fall into two paradigms: combinatorial approaches that blend predefined concept pairs through rule-based searches [Li et al. 2024] or learnable tokens [Feng et al. 2024], and exploratory methods like ConceptLab [Richardson et al. 2024] that optimize textual embeddings to discover novel concepts. Specifically, ConceptLab formulates creative generation as an iterative optimization problem over learned textual embedding, minimizing loss function that balances two objectives: maintaining similarity to broad target category while maximizing the distance from known subcategories in the CLIP embedding space. While these demonstrate progress, they require either per-concept optimization procedures, specialized training on curated datasets, or predefined concept specifications, limiting their practical deployment and scalability. To address these limitations, we propose VLM-Guided Adaptive Negative-Prompting, training-free method that integrates into any diffusion sampler without modifying pretrained weights or requiring curated datasets. Unlike previous approaches, our method operates entirely at inference time through closed-loop feedback mechanism (Figure 3). We leverage lightweight vision-language model (VLM) to adaptively steer the generation process away from its typical predictions and thus towards unexplored regions of possible outputs. Our approach utilizes the VLM to analyze intermediate denoising predictions at each timestep, identify dominant objects, and adaptively convert these observations into negative prompts that are integrated into the next denoising step. Through experiments across multiple VLM models, diffusion pipelines, and human evaluation studies, we demonstrate consistent improvements in exploratory creativity while maintaining categorical coherence. Our analysis reveals how adaptive negative prompting guides the denoising trajectories toward unexplored semantic regions and highlights the importance of VLM feedback during inference. Through extensive ablation studies, we validate our key design choices, including dynamic negative prompt accumulation and per-generation adaptation, showing superiority over alternative approaches. Furthermore, we demonstrate capabilities beyond existing methods, including the generation of coherent creative sets and the preservation of creativity within complex compositional prompts, showcasing the versatility of our VLM-guided approach."
        },
        {
            "title": "2 Related Work",
            "content": "Foundations of Creative Generation. The pursuit of extending human imagination with machine learning has motivated extensive research in computational creativity, from algorithmic design tools [Cohen-Or and Zhang 2016; Sims 1991, 1994; Sun et al. 2025] to theoretical frameworks examining whether computers can create art or merely serve as sophisticated tools for human artists [Hertzmann 2018]. Early work, such as Xu et al. [2012], introduced setevolution framework for creative 3D shape modeling by steering the generation towards user-preferred shapes while maintaining diversity. Other works [Elgammal et al. 2017; Sbai et al. 2019] proposed modifying losses and training objectives to generate creative art by maximizing deviation from established styles while minimizing deviation from the general art distribution. Concept Blending and Combinatorial Creativity. significant portion of computational creativity involves combinatorial approaches. Some works [Liew et al. 2022; Zhou et al. 2025] leveraged diffusion models to blend different visual and semantic concepts for the generation of novel outputs. Dorfman et al. [2025] extended this to multiple visual inputs by crafting composite embeddings, stitched from the projections of multiple input images onto concept-specific CLIP-subspaces identified through text. For text-based concept pairs, Li et al. [2024] suggested balance swap-sampling, which generates creative combinatorial objects by randomly exchanging intrinsic elements of text embeddings and selecting high-quality combinations based on CLIP distances. Feng et al. [2024] takes different approach and re-defines creativity as learnable token. They iteratively sample diverse text pairs from their proposed dataset to form adaptive prompts and restrictive prompts, and then optimize the similarity between their respective text embeddings. While these combinatorial approaches recombine user-specified concepts, we instead discover novel concepts within broad categories without predefined targets. VLM-Guided Adaptive Negative Prompting for Creative Generation 3 Fig. 3. Overview of our VLM-guided negative prompting method. To generate creative image (e.g., new type of pet), we sample Gaussian noise and perform an augmented denoising process that maintains an adaptive list of negative prompts. At each denoising step, we query pre-trained Vision-Language Model (VLM) to identify visual concepts present in the intermediate output and update the list accordingly, steering the denoising process away from them. For example, we add the token cat to the accumulating list to shift the denoising trajectory away from generating an image resembling cat as well as the previously detected pets. VLM-Guided Creativity Approaches. Recent research leverages Vision-Language Models (VLMs) to guide creative generation. Feng et al. [2025] uses VLMs to supervise distribution-conditional generation, enabling multi-class concept blending through learnable encoder-decoder framework. While the above approaches focus on combinatorial creativity through concept blending, Richardson et al. [2024] introduces ConceptLab, which tackles the more challenging task of exploratory creativity. They formulate the Creative Text-to-Image (CT2I) generation as an optimization process of learned textual embedding. To prevent convergence to existing concepts, ConceptLab incorporates question-answering VLM that adaptively adds new constraints to the optimization problem. These VLM-guided approaches rely on per-concept optimization procedures that require multiple iterations and substantial computational resources. Our approach leverages VLMs as real-time oracles during the denoising process to reduce computational overhead. Optimization-Free Creative Generation. Han et al. [2025] boosts creativity in Stable Diffusion by amplifying features during denoising, primarily affecting color and textures. While we share the goal of optimization-free creativity enhancement, our method operates through dynamic negative prompting to guide the generation away from conventional semantic patterns rather than amplifying existing features. The advantage of such optimization-free approaches lies in their immediate applicability to existing models without requiring additional training or complex optimization procedures."
        },
        {
            "title": "3 Method",
            "content": "Our VLM-Guided Adaptive Negative-Prompting method enhances creative generation in diffusion models through closed-loop feedback mechanism that dynamically navigates the denoising process away from familiar visual patterns. As illustrated in Figure 3, our method monitors the intermediate denoiser outputs using VisionLanguage Model (VLM), which identifies dominant elements (e.g., cat) and accumulates them as dynamic negative prompts during the generation process. This adaptive accumulation refines the guidance signal at each denoising step. We begin by establishing the necessary background on negative prompting in Section 3.1 and detailing our VLM-guided synthesis strategy in Section 3.2."
        },
        {
            "title": "Prompting",
            "content": "Diffusion models generate images by gradually denoising sample from pure noise 𝑥𝑇 over series of time steps. Latest diffusion models, including Stable Diffusion 3.5 [Esser et al. 2024] used in our experiments, employ flow matching [Lipman et al. 2023] to generate images through iterative denoising. Let 𝑥𝑡 denote the noisy image at timestep 𝑡 [𝑇 , ..., 0]. In flow matching, the model learns velocity field 𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐) conditioned on text embedding 𝑐 = 𝐸 (𝑝) derived from prompt 𝑝 via text encoder 𝐸. The denoising process follows 𝑑𝑥𝑡 𝑑𝑡 = 𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐). During sampling, we the probability flow ODE: can estimate the clean image at any timestep using the following equation: ˆ𝑥 (𝑡 ) 0 = 𝑥𝑡 𝑡 𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐) Classifier-free guidance (CFG) [Ho and Salimans 2021] improves conditional generation by combining conditional and unconditional predictions: 𝑣 𝑤 𝜃 = 𝑣𝜃 (𝑥𝑡 , 𝑡, ) + 𝑤 (𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐) 𝑣𝜃 (𝑥𝑡, 𝑡, )), where denotes the unconditional (null) embedding, and 𝑤 is the guidance scale. When 𝑤 = 0, the model generates unconditional samples; as 𝑤 increases, the model increasingly favors features aligned (1) 4 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik with the conditioning text. The guidance operates by amplifying the difference between conditional and unconditional predictions. When 𝑤 = 0, the model generates unconditional samples. As 𝑤 increases, the model increasingly favors features that align with the conditioning text. This mechanism was naturally extended [Saharia et al. 2022] to negative prompting, in which the model is explicitly discouraged from generating features associated with negative prompt 𝑝𝑛𝑒𝑔. Instead of subtracting the unconditional prediction, we subtract negatively conditioned prediction: 𝜃 = 𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐𝑛𝑒𝑔) + 𝑤 (cid:0)𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐𝑝𝑜𝑠 ) 𝑣𝜃 (𝑥𝑡, 𝑡, 𝑐𝑛𝑒𝑔)(cid:1) , ˆ𝑣 𝑤 where 𝑐𝑛𝑒𝑔 = 𝐸 (𝑝𝑛𝑒𝑔) represents the negative prompt embedding derived from the unwanted concepts 𝑝𝑛𝑒𝑔. This formulation steers generation away from 𝑐𝑛𝑒𝑔 and toward 𝑐𝑝𝑜𝑠 by amplifying their differences. We further explain the intuition and the effect of negative prompting in Appendix 8. (2)"
        },
        {
            "title": "3.2 VLM-Guided Adaptive Negative Prompting\nTo generate a creative image from a given prompt 𝑝𝑝𝑜𝑠 , we sample\ninitial Gaussian noise 𝑥𝑇 ∼ N (0, 𝐼 ) and initiate an augmented de-\nnoising process in which, at each denoising step, we dynamically\nsteer the generation away from common visual concepts identified\nthrough VLM analysis, as illustrated in Figure 3. Given the interme-\ndiate prediction ˆ𝑥 (𝑡 )\n, at each timestep 𝑡 ∈ [0,𝑇 ], we query the VLM\n0\nto identify the dominant features present in the image. We denote\nthe questioning process as follows:\n(cid:16)\nˆ𝑥 (𝑡 )\n0",
            "content": "𝑟 (𝑡 ) = , 𝑞 (𝑡 ) (cid:17) (3) , Where is the VLM model, 𝑞 (𝑡 ) is the question, and 𝑟 (𝑡 ) is the VLM response at timestep 𝑡. Each response 𝑟 (𝑡 ) is added to growing set of negative prompts: 𝑝 (𝑡 ) 𝑛𝑒𝑔 = . This creates feedback loop where each timesteps guidance reflects all previously identified dominant features, progressively steering toward more creative outputs. 𝑛𝑒𝑔 𝑟 (𝑡 ) with initialization 𝑝 (𝑇 ) 𝑛𝑒𝑔 = 𝑝 (𝑡 +1) Runtime Analysis. Our method adds minimal overhead of 13 seconds when used in the least efficient setting. Querying ViLT [Kim et al. 2021] for 28 steps while using the SD3.5-large decoder for 𝑥0 predictions takes total of 35 seconds, compared to 22 seconds for standard SD3.5-large single image generation. In contrast, [Richardson et al. 2024] requires approximately 8 minutes to train each concept on single seed, and C3 requires approximately 30 minutes for amplification factor search using 10 samples per concept."
        },
        {
            "title": "4 Experiments",
            "content": "We comprehensively evaluate our approach through qualitative comparisons with existing creative generation methods, user study, and quantitative metrics. We validate our design choices with extensive ablations examining the necessity of the VLM feedback, the accumulation strategy, and seed-specific adaptation. Finally, we present use cases and practical applications that our approach enables, extending the capabilities of previous creativity methods. Additional results and implementation details are in Sections 5 to 9. We display in Figure 5 the diverse creative outputs of our approach across categories ranging from pets to bags. Through seed variation Fig. 4. Trade-off between novelty and category coherence in our user study. Higher values are better for both axes. Our method (star) uniquely achieves high scores on both dimensions compared to other creative generation methods. alone, our method explores wide spectrum of novel concepts without requiring retraining or additional optimization."
        },
        {
            "title": "4.1 Qualitative Evaluation",
            "content": "We begin by comparing our method with the two competing approaches for exploratory creativity within category: ConceptLab [Richardson et al. 2024] and C3 [Han et al. 2025]. As can be seen in Figure 6, ConceptLab generates creative objects but often sacrifices category validity. For example, it may produce cup that cannot be drunk from or couch with no seat. In contrast, our method produces objects that are both valid and creative. For fair comparison, we use the same base models as ConceptLab and C3, while also demonstrating that our method leverages newer models to produce better results. ConceptLab and C3 have several assumptions preventing them from integrating seamlessly to any base diffusion model. In Figure 7, we compare our method with images generated by state-of-the-art models, including Stable Diffusion 3.5 [Esser et al. 2024], FLUX.1-dev [Black Forest Labs 2024], and GPT-4o [OpenAI 2024], all prompted with requests for creative or new type of variations. These comparisons demonstrate that even the most advanced generative models, when used with standard prompting, produce typical category exemplars such as regular cars and fruits rather than creative variations. In contrast, our results present novelty while maintaining validity. For example, the vehicle has wheels and space for driver, yet does not correspond to any existing vehicle type. 4.2 User Study Quantitative evaluation remains fundamental challenge in computational creativity research [Lamb et al. 2018]. We conduct user study to evaluate the human-perceived creativity and semantic validity of images generated by our VLM-guided approach compared to existing methods. We collected total of 3,200 responses (25 participants 32 image pairs 4 comparisons), across 8 different categories. The full setup is described in Appendix 10. For each image pair, participants evaluate Creativity/Novelty: How creative or VLM-Guided Adaptive Negative Prompting for Creative Generation 5 Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative sofa VLM questions 𝑞: What is the shape of the sofa?,What is the design of the sofa?, What is the color of the sofa? Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative building VLM questions 𝑞: What is the design of the building?,What is the shape of the building?, What is the building made of? Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of pet VLM questions 𝑞: What pet do you identify in the photo? Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative bag VLM questions 𝑞: What is the design of the bag?,What is the bag made of?, What is the color of the bag? Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of fruit VLM questions 𝑞: What fruit do you identify in the photo? Fig. 5. Qualitative results of our method across different object categories. In all categories, our method generates creative shapes and appearances while preserving object semantics. For instance, buildings with unique forms and textures that retain windows, doors, and balconies, or bags made of varied materials that remain recognizable as bags. novel is the interpretation of the broad category? and validity: How well does the image maintain its identification as the specified category? Figure 4 presents the results. Creative Prompting methods (SD3.5 and GPT-4o), explicitly requesting novelty via prompts such as new photo of [category], cluster in the upper-left region with high category validity but minimal novelty, confirming our qualitative findings that simple prompt modifications fail to produce creative exemplars. Creative-generation methods (ConceptLab and C3) achieve moderate creativity results but at significant cost in validity. In contrast, our method achieves both high novelty and validity, maintaining both high creativity and validity."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "A natural question is whether the in-the-loop VLM guidance is necessary or does one of two offline alternatives suffices: (i) using an LLM to derive negative list from the positive prompt alone, or (ii) using VLM to analyze random image once and then statically replaying the resulting list across all seeds. We study four design variants to validate our adaptive negative prompting approach, as presented in Figure 8. First, we tested whether GPT-4o could generate static negative prompt lists directly from the main object in the positive prompts. Second, applying our accumulated negative 6 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik t n k d s . 5 3 - O S - 3 D - O . 5 3 - O Cup Plant Pet Sofa Chair Car Building Teddy bear Fig. 6. Left: Comparison with ConceptLab [Richardson et al. 2024] (top row) and our VLM-Guided method using Kandinsky2 [Razzhigaev et al. 2023] (middle row) and SD3.5 (bottom row). Right: Comparison with C3 [Han et al. 2025] using SDXL [Podell et al. 2023] (top row) and our method using SDXL (middle row) and SD3.5 (bottom row). Our method consistently generates more diverse and imaginative variations while maintaining recognizability within each category. 4 - . - 1 F . 5 3 r Pet Hair style Cultural Outfit Vehicle Fruit Musical instrument Plant Chess set Fig. 7. Creative generation comparison across different categories. Despite prompts explicitly requesting novelty (A new type of [category] or creative [category]), GPT-4o, FLUX and SD3.5 produce typical category exemplars. Our method generates novel variations that navigate unexplored modes of the semantic space. Each column uses identical seeds across all methods for fair comparison. prompts statically (replaying) from the beginning yields less creative outputs. Third, reusing negative prompts across different seeds (Cross-Seed replay) produces suboptimal results. Finally, removing accumulation allows generations to cycle back to the conventional patterns previously identified. Our method achieves the best scores across all reported metrics in Table 1. The full ablation studies are presented in Appendix 6. They examine computational efficiency (i.e., timestep reduction), VLM robustness across different models, question design impact, and positive prompt variations, all confirming the robustness of our approach. VLM-Guided Adaptive Negative Prompting for Creative Generation 7 c f n i g GPT-4o 28 concepts GPT-4o 15 concepts GPT-4o 10 concepts Ours (GPT-4o) Replay (PerSeed) Replay (CrossSeed) No Accumulation Ours Fig. 8. Left: Non-Adaptive LLM Approach: GPT-4o (𝑛 [10, 15, 28]) - static LLM list of 𝑛 negative concepts applied at all steps. Ours (GPT-4o) dynamic, VLM-guided negatives using GPT-4o as our VLM. Right: Replay (Per-Seed) - reuse the accumulated VLM list from the same seed at all steps; Replay (Cross-Seed) - reuse list extracted from different seed at all steps; No Accumulation - use only the current steps VLM answers (no carry-over); Ours - adaptive accumulation of negative prompts."
        },
        {
            "title": "4.4 Quantitative Evaluation",
            "content": "Existing methods employ different strategies to quantify and evaluate creativity. ConceptLab measures the difference between CLIP similarity to the positive concept prompt and the maximum CLIP similarity to any negative concept prompt. We refer to this measure as relative typicality. C3 evaluates three dimensions of creativity: novelty, diversity, and validity. We evaluate creativity through complementary metrics that capture novelty, diversity, and validity as well. For novelty, we measure relative typicality (multiplied by 100 for readability) and the GPT Novelty Score. For the diversity we measure Vendi score and total variance. For validity, we employ CLIP alignment and GPT-4 verification. While these metrics have known limitations for creative outputs, as creativity inherently deviates from training distributions, they provide consistent comparative baselines. The formal definitions of the metrics and additional details are presented in Appendix 11. Fig. 9. Top 5 subcategory distribution of 100 generated pets per method classified with GPT-4o. Quantitative Results. Table 1 summarizes the quantitative results. We achieve significant gains in diversity and novelty metrics with minimal tradeoff in CLIP and GPT scores. All metrics are averaged across four categories: vehicle, plant, pet, and garment (100 images each), so improvements reflect cross-category behavior. Our method using Qwen2.5-3B and BLIP-2 achieves the best balance across all three creativity dimensions, leading in novelty and diversity, and maintaining competitive validity, while other methods either sacrifice creativity for validity or vise versa. The design variants we evaluate under-perform our dynamic, per-step, per-seed approach, highlighting the importance of both timing and seedspecific guidance. no-accumulation variant also trails our method, indicating that remembering previously discovered negatives is beneficial. Notably, while ConceptLab achieves the highest CLIP score, it shows the lowest GPT verification score. This happens because their optimization process maximizes the CLIP-space distance from negative concepts but can produce adversarial examples that satisfy mathematical constraints without maintaining semantic validity. This manifests as objects that technically align with CLIP embeddings but fail human and GPT-4 verification as functional category members (e.g., cups without cavities and sofas without seating surfaces). In contrast, our method maintains the highest performance across all three evaluation dimensions: validity, diversity, and novelty. GPT Novelty Score. In Figure 9, we present the distribution of subcategories classified with GPT-4o over 100 images of pets generated with ConceptLab, C3, Creative Prompting, and Our VLM-Guided method. While Creative Prompting and C3 generate recognizable dogs and cats, with ConceptLab exhibiting intermediate behavior, our approach primarily produces unknown or unclassifiable pets, approximately 87%, demonstrating our methods ability to avoid known subcategories."
        },
        {
            "title": "4.5 Use Cases",
            "content": "Diverse scenarios. Our method generates novel objects within semantic categories and can be used for practical applications by placing these objects in diverse contexts and scenes. Recent controllable generation models like Flux.1-dev Kontext [Black Forest Labs 2025] enable users to take our creatively generated objects 8 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik Table 1. Quantitative evaluation of creative generation methods across different prompting strategies. Reference: SD3.5 with photo of [category]. Creative Prompting: SD3.5 with photo of creative [category]. VLM-Guided: Our adaptive negative prompting approach. C3 and ConceptLab images are generated as explained in the corresponding papers. The metrics are averaged over 400 samples, equally generated 100 from 4 categories: pet, plant, garment, vehicle. In bold are best results underline for second best. For validity we exclude the baselines (Reference & Creative Prompting) from the marking."
        },
        {
            "title": "Reference\nCreative Prompting",
            "content": "GPT-4o 10 Concepts GPT-4o 15 Concepts GPT-4o 28 Concepts Cross-Seed Replay No Accumulation C3 ConceptLab Ours ViLT Ours BLIP-1 Ours BLIP-2 Ours Qwen2."
        },
        {
            "title": "Relative\nTypicality",
            "content": ""
        },
        {
            "title": "GPT Novelty\nScore",
            "content": ""
        },
        {
            "title": "Total\nVariance",
            "content": "Vendi"
        },
        {
            "title": "CLIP\nScore",
            "content": ""
        },
        {
            "title": "GPT\nScore",
            "content": "1.640 1.645 0.655 0.885 1.043 1.703 1.610 1.075 1.922 1.835 2.005 2.190 2.100 0.065 0. 0.093 0.108 0.100 0.065 0.060 0.233 0.238 0.1575 0.230 0.370 0.401 0.188 0.191 0.272 0.277 0.276 0.265 0.274 0.271 0. 0.298 0.299 0.318 0.308 3.174 3.139 4.973 5.040 5.067 4.584 4.355 4.726 5.119 5.347 5.414 5.794 5.476 0.282 0. 0.262 0.262 0.260 0.261 0.262 0.254 0.270 0.264 0.264 0.261 0.264 1.000 0.933 0.867 0.805 0.828 0.843 0.875 0.895 0. 0.893 0.856 0.898 0.917 and seamlessly integrate them into various environments while preserving their unique characteristics, as shown in Figure 10. Fig. 10. Creative object in different scenes generated using Flux.1-dev Kontext [Black Forest Labs 2025]. Left column: Novel objects generated by our VLM-guided method. Columns 2-4: The same creative objects placed in various contexts and applications while preserving their distinctive features. Beyond single objects. Our method extends naturally from generating individual creative objects to producing coherent sets of related items that share unified creative vision. By applying our approach to prompts that describe collections e.g., Creative tea set, as presented in Figure 11, we demonstrate that our method maintains validity and consistency across multiple objects while exploring creative variations. Tea set Chess set Cutlery set Luggage set Fig. 11. Creative sets generated by our method demonstrating coherent collections of related objects. Each set exhibits individual creativity in its components while maintaining stylistic and functional consistency across the collection. Complex prompts. Figure 12 displays how our VLM-guided approach seamlessly integrates with elaborate prompt descriptions, photo of an imaginary pet surfing on board near an island, photo of new type of plant blooming in an arctic field next to penguins, photo of new type of fruit sliced on ceramic plate on sunlit windowsill and photo of woman wearing creative jacket in french cafe enabling creative exploration even within complex compositional requirements. The adaptive negative prompting mechanism operates orthogonally to these additional constraints, it identifies and steers away from conventional modes of the requested object described as creative, while respecting the stylistic and compositional requirements specified in the prompt. Fig. 12. Creative objects presented in complex environment described by the prompt."
        },
        {
            "title": "5 Conclusions",
            "content": "We introduced VLM-Guided Adaptive Negative-Prompting, an inference time method that leverages the strength of vision-language models to dynamically steer diffusion models toward more creative outcomes. By querying VLM throughout the denoising process and accumulating seed-specific negative prompts, our approach pushes generation away from conventional patterns while preserving categorical coherence. The fact that VLM is capable of analyzing noisy VLM-Guided Adaptive Negative Prompting for Creative Generation 9 Zorana Ivcevic and Mike Grandinetti. 2024. Artificial intelligence as tool for creativity. In booktitle of Creativity. Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision. https://arxiv.org/abs/2102. Carolyn Lamb, Daniel G. Brown, and Charles L. A. Clarke. 2018. Evaluating Computational Creativity: An Interdisciplinary Tutorial. In Association for Computing Machinery. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. Jun Li, Zedong Zhang, and Jian Yang. 2024. TP2O: Creative Text Pair-to-Object Generation Using Balance Swap-Sampling. In Computer Vision ECCV 2024. Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. 2022. MagicMix: Semantic Mixing with Diffusion Models. In arXiv preprint arXiv:2210.16056. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2023. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations. OpenAI. 2024. GPT-4o System Card. https://arxiv.org/abs/2410.21276 OpenAI. 2025. Introducing OpenAI o3 and o4-mini. Technical Report. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv:2307.01952 [cs.CV] https: //arxiv.org/abs/2307.01952 Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. 2023. Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion. arXiv:2310.03502 [cs.CV] https://arxiv.org/abs/2310. 03502 Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. 2024. ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints. In ACM Transactions on Graphics. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv:2205.11487 [cs.CV] https://arxiv.org/abs/2205. Othman Sbai, Mohamed Elhoseiny, Antoine Bordes, Yann LeCun, and Camille Couprie. 2019. DesIGN: Design Inspiration from Generative Networks. In Computer Vision ECCV 2018 Workshops. Karl Sims. 1991. Artificial evolution for computer graphics. In Proceedings of the 18th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 91). Karl Sims. 1994. Evolving virtual creatures. In Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 94). Zhida Sun, Zhenyao Zhang, Yue Zhang, Min Lu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2025. Creative Blends of Visual Concepts. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 117. Kevin Turner. to RGB without Upscaling. https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204. Decoding latents 2022. Timothy Alexis Vass. 2024. Explaining the SDXL latent space. Technical Report. Kai Xu, Daniel Cohen-Or, and Baoquan Chen. 2012. Fit and diverse: set evolution for inspiring 3D shape galleries. In ACM Transactions on Graphics. Li Yongjun, Li Xinyue, and Wang Lizheng. 2025. Generating creativity through ChatGPT: an empirical investigation in open innovation platforms. In Information Technology and Management. Yufan Zhou, Haoyu Shen, and Huan Wang. 2025. FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion. arXiv:2502.05606 [cs.CV] intermediate states and providing guidance strong enough to redirect the trajectory highlights its potential as powerful mechanism for creative exploration. While our VLM-guided approach demonstrates effective creative exploration, several limitations can be addressed in future research. First, our method introduces computational overhead through VLM inference at each timestep, though our ablation studies show this can be reduced to the first 10-15 steps without significant quality loss. Second, the quality of creative outputs depends on the VLMs ability to identify emerging patterns in noisy intermediate predictions; while we demonstrate robustness across various VLMs, more sophisticated vision-language models generally yield better results. Third, our approach requires careful question design for optimal performance; different question formulations work better for different semantic categories, and automating this selection remains an open challenge. Looking ahead, we believe that the integration of feedback-driven guidance will open new directions for creativity in generative models, and future work may extend this paradigm to other domains, such as video, 3D, or multimodal content creation."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Roy Ganz, Guy Ohayon, Omer Belhasim and Tomer Borreda for their early feedback and helpful suggestions."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. Yuanhao Ban, Ruochen Wang, Tianyi Zhou, Minhao Cheng, Boqing Gong, and Cho-Jui Hsieh. 2024. Understanding the Impact of Negative Prompts: When and How Do They Take Effect?. In European Conference on Computer Vision. Black Forest Labs. 2024. FLUX.1 tools. Technical Report. Black Forest Labs. 2025. FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space. Margaret A. Boden. 2009. Computer Models of Creativity. AI Magazine 30, 3 (Jul. 2009), 23. doi:10.1609/aimag.v30i3. Daniel Cohen-Or and Hao Zhang. 2016. From inspired modeling to creative modeling. In Vis. Comput. Sara Dorfman, Dana Cohen-Bar, Rinon Gal, and Daniel Cohen-Or. 2025. IP-Composer: Semantic Composition of Visual Concepts. arXiv:2502.13951 [cs.CV] Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, and Marian Mazzone. 2017. CAN: Creative adversarial networks generating Art by learning about styles and deviating from style norms. In 8th International Conference on Computational Creativity, ICCC. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, and Xin Geng. 2024. Redefining <Creative> in Dictionary: Towards an Enhanced Semantic Understanding of Creative Generation. arXiv:2410.24160 [cs.CV] Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, and Xin Geng. 2025. DistributionConditional Generation: From Class Distribution to Creative Generation. arXiv:2505.03667 [cs.CV] Dan Friedman and Adji Bousso Dieng. 2022. The Vendi Score: Diversity Evaluation Metric for Machine Learning. In Transactions on Machine Learning Research. Jiyeon Han, Dahee Kwon, Gayoung Lee, Junho Kim, and Jaesik Choi. 2025. Enhancing creative generation on stable diffusion-based models. In Proceedings of the Computer Vision and Pattern Recognition Conference. Aaron Hertzmann. 2018. Can Computers Create Art?. In MDPI AG. Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications. 10 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik Appendix This appendix provides comprehensive details supporting our main paper. Section 6 presents extensive ablation studies. Section 7 provides technical implementation specifications. Section 8 extends about the foundations of negative prompting. Section 9 details the qualitative evaluation framework and the generation process of the evaluated methods. Section 11 details evaluation metrics. Section 10 describes our human evaluation protocol."
        },
        {
            "title": "6 Ablations",
            "content": "g l g e J 𝑛𝑒𝑔 = 𝑝 (1) 𝑛𝑒𝑔 = = 𝑝 (𝑇 ) Non-Adaptive LLM Approach. We used GPT-4o [OpenAI 2024] to generate lists of common sub-categories for each creative prompt at several sizes 𝑁 [10, 15, 28]. For instance, given the prompt photo of creative jacket, we asked GPT-4o: List the 𝑁 most common types of jackets. single list, separated by commas. Each description is single word. typical result is: bomber, biker, trucker .... We then formatted the list as static negative prompt 𝑝𝐿𝐿𝑀 𝑛𝑒𝑔 and applied it uniformly throughout the entire denoising process 𝑝 (0) 𝑛𝑒𝑔 = 𝑝𝐿𝐿𝑀 𝑛𝑒𝑔 . As shown in Figure 8, this approach produces less creative results compared to our dynamic method. For example, in the second row, our generated jacket features smooth, cloud-like spherical ornaments that are atypical for jackets, whereas LLM-based lists yield colorful yet conventional wool or fabric designs and do not portray creative ornaments. We attribute this to the lack of alignment between the static, seedindependent LLM-generated list and the actual generative trajectory. Such prompts cannot account for the specific visual patterns that emerge during the denoising process, nor for those encoded in the sampled initial noise. While the LLM provides semantically reasonable negative concepts, it lacks the visual awareness to recognize which particular modes are being generated from the specific sampled noise at each timestep, resulting in generic rather than targeted steering. 𝑡 =1 𝑛𝑒𝑔 = (cid:208)𝑇 Non-Dynamic Replay Approaches. To isolate the importance of the dynamic process, we tested whether the accumulated negative prompts from our full dynamic negatives list could be replayed statically from the beginning of the generation. In this experiment, we first ran our complete dynamic method to generate the final 𝑝 (𝑡 ) accumulated negative prompt 𝑝𝑇 𝑛𝑒𝑔 for given seed, then used this pre-accumulated prompt uniformly throughout fresh denoising process: 𝑝 (𝑡 ) 𝑛𝑒𝑔 = 𝑝 (𝑇 ) 𝑛𝑒𝑔 for all timesteps 𝑡 [0,𝑇 ]. Despite using the same negative concepts that our dynamic method accumulates, this static application produces less creative results. For example, the bag in Figure 8 in the last row generated with the adaptive method has flower ornaments and unique shape while the bag under the Replay (Per-Seed) column looks like regular plastic bag. This demonstrates that timing and responsiveness to emerging visual patterns are crucial; the same negative prompts, when applied at the wrong times, fail to provide effective steering. The dynamic nature of our approach, which introduces negative concepts precisely when the corresponding visual patterns begin to emerge, is essential for successful creative exploration. We further investigate whether negative prompts can be reused across different generation seeds to reduce computational overhead. 0-5 00-15 Fig. 13. Effect of limiting VLM guidance to different ranges of denoising timesteps. Columns correspond to applying our method during only the first 5, 10, 15, 20, or all 27 timesteps, while rows show results for Building, Bag, and Jacket categories. 0-20 0-27 We collected accumulated negative prompts 𝑝 (𝑇 ) 𝑛𝑒𝑔 from successful creative generations and applied them to random seeds while maintaining the same positive prompt. This cross-seed reuse consistently produces suboptimal results, emphasizing that each generation seed follows unique trajectory through the semantic space and requires its own adaptive negative prompting strategy. When the VLMs analysis of intermediate predictions ˆ𝑥 (𝑡 ) is tailored to the specific seeds 0 denoising path, we achieve superior creative results, as shown in Figure 8 under the column Replay (Cross-Seed). For example, the bag in the last row under the Replay (Cross-Seed) column looks like regular paper bag compared to our unique bag design. This finding reinforces the notion that the effectiveness of our method stems from its ability to provide adaptive, trajectory-specific guidance rather than applying generic steering patterns. Non-Accumulating Approach. Next, we explore the importance of our accumulation strategy. To test its contribution, we modify our approach to use only the current VLM response as the negative prompt at each timestep. Specifically, we replace the negative prompt with 𝑝 (𝑡 ) 𝑛𝑒𝑔 = 𝑟 (𝑡 ) for each 𝑡 [0,𝑇 ], discarding all previously accumulated information. This non-accumulating variant, shown in Figure 8 under the column No Accumulation, fails to maintain memory of previously identified conventional modes, allowing the generation to cycle back toward familiar patterns that were detected and should have been avoided in earlier denoising steps. For example, the building in the first row under the column No Accumulation remains similar to the SD3.5 baseline building, whereas our method produces unique, asymmetrically shaped building. For fair comparison, the VLM query is identical across methods: at every timestep, we ask What type of bag is this?. Timesteps Analysis. Our method introduces VLM evaluations at each denoising timestep, which unavoidably increases computational overhead compared to standard diffusion sampling. To improve practical efficiency, we investigate whether the number of VLM queries can be reduced without compromising creative quality. Specifically, we analyze the minimum number of timesteps requiring VLM intervention to achieve effective creative steering. As shown in Figure 13, applying VLM guidance during only the first 10 to Table 2. Exact GPT4o lists used as 𝑝LLM neg for the Jacket category in Figure 8. VLM-Guided Adaptive Negative Prompting for Creative Generation 11 𝑁 =10 bomber, puffer, anorak, field biker, blazer, trucker, varsity, parka, trench, 𝑁 =15 𝑁 =28 biker, blazer, parka, bomber, puffer, trench, anorak, field, harrington, peacoat, safari, quilted, windbreaker trucker, varsity, biker, blazer, trucker, varsity, parka, bomber, puffer, trench, anorak, field, harrington, peacoat, safari, quilted, windbreaker, denim, leather, fleece, rain, down, coach, double utility, chore, breasted, cagoule, car, duffle, mac Table 3. Exact GPT4o lists used as 𝑝LLM neg for the Sofa category in Figure 8. 𝑁 =10 𝑁 =15 𝑁 =28 sectional, chaise, recliner, futon, sleeper, modular, tuxedo, chesterfield, camelback loveseat, loveseat, sectional, chaise, recliner, futon, sleeper, modular, camelback, tuxedo, lawson, slipcovered, daybed, settee chesterfield, midcentury, loveseat, chesterfield, midcentury, sectional, chaise, recliner, futon, sleeper, modular, camelback, tuxedo, lawson, slipcovered, daybed, settee, track arm, roll arm, armless, curved, divan, sofa bed, pit, pallet, reclining, convertible, chaise end, bench, ottoman Category Pet Plant Garment Table 4. Exact GPT-4o lists used as 𝑝LLM neg for all categories in the LLM ablation study presented in Table 1. 𝑁 =10 𝑁 =15 𝑁 =28 cat, fish, dog, rabbit, hamster, guinea pig, turtle, lizard, snake bird, cat, fish, bird, dog, rabbit, hamster, guinea pig, turtle, lizard, snake, parrot, ferret, chinchilla, hedgehog, tarantula cat, bird, fish, dog, rabbit, hamster, guinea pig, turtle, lizard, snake, parrot, ferret, chinchilla, hedgehog, tarantula, gecko, bearded dragon, cockatiel, budgerigar, finch, tortoise, hermit crab, dwarf hamster, betta, goldfish, lovebird axolotl, newt, shrub, moss, grass, tree, cactus, succulent, vine, herb, flower fern, shrub, grass, moss, tree, cactus, herb, flower, palm, orchid, bamboo, lily, rose fern, vine, succulent, grass, shrub, succulent, fern, vine, moss, tree, cactus, herb, flower, palm, orchid, bamboo, lily, rose, sunflower, maple, oak, pine, conifer, broadleaf, sedge, evergreen, reed deciduous, daisy, tulip, ivy, shirt, dress, pants, skirt, jacket, coat, t-shirt, blouse sweater, hoodie, shirt, dress, pants, skirt, jacket, t-shirt, coat, suit, blouse, cardigan, jumpsuit sweater, jeans, shorts, hoodie, pants, sweater, jeans, jumpsuit, cardigan, dress, coat, blouse, shirt, jacket, t-shirt, suit, trenchcoat, overcoat, tracksuit, dungarees, kimono, sari parka, waistcoat, leggings, skirt, hoodie, shorts, blazer, raincoat, sweatshirt, chinos, Vehicle car, truck, bus, van, motorcycle, bicycle, scooter, train, tram, subway car, truck, bus, van, motorcycle, bicycle, tram, subway, boat, ship, ferry, airplane, helicopter scooter, train, train, scooter, car, truck, bus, van, motorcycle, bicycle, tram, subway, boat, ship, ferry, airplane, kayak, helicopter, submarine, seaplane, jet, hovercraft, atv, snowmobile, forklift, tractor, bulldozer glider, yacht, canoe, Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik Table 5. Accumulated lists reused for static application in Fig. 8. Category Building Bag Accumulated negative list brick, regular building, glass, modern, skyscraper, concrete, moderne, modernist, futuristic, curved tote, satchel, hobo, backpack, clutch, messenger, crossbody, duffel, bucket, wristlet t a i u New type Innovative Unique Fig. 14. Effect of positive prompt wording on creative generation using our method. Columns correspond to alternative prompt formulations (New type, Innovative, Unique, Creative and simply photo of [category]), while rows show results for different semantic categories Across categories, our approach produces diverse and imaginative outputs. Creative None 15 timesteps sufficiently steers generation toward creative outputs. This efficiency results from the momentum effect described in [Ban et al. 2024] and explained in our Section 2, where early negative prompt accumulation establishes persistent creative trajectories that continue throughout the remaining denoising process. This finding enables improved computational efficiency, making our approach more practical for real-world deployment. For all methods in this analysis, the VLM query is identical and fixed at every queried step: What is the style of the [category]?. Positive Prompt Selection. Our approach demonstrates flexibility in positive prompt formulation, accepting various creativityindicating phrases such as creative, innovative, new, novel, unique, and other similar terms to produce creative outputs. Our VLM-guided approach works effectively even with ambiguous positive prompts, such as new type of.... As demonstrated in Figure 14, different formulations of creative prompts yield diverse creative outputs while maintaining the fundamental steering behavior and the effectiveness of our method as well as validity. When the indicative adjective is removed entirely from the positive prompt (e.g., using simply photo of [obj]), the resulting images are diverse and aesthetically pleasing; however, they lack the creative qualities that distinguish our method. Robustness to VLM Model Selection. Our method demonstrates robustness across variety of Vision-Language Models that differ in architecture, training data, model size, and capabilities. As shown in Figure 15, we successfully achieve creative outputs using models ranging from lightweight options such as ViLT [Kim et al. 2021] d B t a ViLT BLIPGPT-4o Qwen2.5 BLIP-2 Fig. 15. Comparison of outputs when guiding our method with different Vision-Language Models (VLMs). Columns correspond to GPT-4o [OpenAI 2024], Qwen2.5 [Bai et al. 2025], BLIP-2 [Li et al. 2023], BLIP-1 [Li et al. 2022], and ViLT [Kim et al. 2021], while rows show three semantic categories: Unique Building, New Pet, and Creative Jacket. Across models, our approach consistently produces creative and coherent results, with stronger VLMs generally yielding more novelty, demonstrating robustness of the method to the choice of VLM. d B c g SD3.5 Material Color Fig. 16. Effect of the VLM question design on creative generation. Rows correspond to three semantic categories. The first column shows Stable Diffusion 3.5 baseline. The remaining columns apply our VLM-Guided Adaptive Negative-Prompting while asking the VLM about (i) the material, (ii) the dominant colors, (iii) the objects shape, and (iv) its design. Design Shape and BLIP-1 [Li et al. 2022] to more sophisticated models like BLIP-2 [Li et al. 2023], Qwen2.5 [Bai et al. 2025], and GPT-4o [OpenAI 2024]. While more capable VLMs generally produce higher quality creative results, the consistent creative steering behavior across different model choices validates the generalization capabilities of our approach. This robustness ensures that practitioners can select VLMs based on their specific computational constraints and quality requirements while maintaining the fundamental creative exploration functionality. For all methods in this analysis, the VLM query is identical and fixed at every queried step: What type of [category] is this?. Question Design for Creative Exploration. The choice of question formulation is critical design parameter that determines which visual features are identified and which are steered away from, directly VLM-Guided Adaptive Negative Prompting for Creative Generation 13 Table 6. Runtime with VLM-in-the-loop guidance. Total seconds for SD3.5large single-image generation when querying different VLM oracles at either every denoising step (28) or only the early steps (15). The baseline performs no VLM queries. All runs use the same prompt and seed. VLM Steps Runtime (Seconds) Baseline No VLM ViLT BLIP-1 BLIPFig. 17. Correlation between the VLM answers across different timesteps and the final generated image Qwen2.5-3B 28 28 15 28 15 28 28 15 22 35 29 36 30 43 33 71 influencing the creative output. Based on our empirical findings, we recommend object-focused questions (e.g., What is the main object in this image?) for generating new types of variations within familiar categories(animals, furniture, buildings, etc.). Style or attribute focused questions (e.g., What is the style/design/texture/material in this image?) are optimal for aesthetic novelty and creativity while preserving category coherence. Figure 16 presents the variations of the question 𝑞 (𝑡 ) choice and the direct influence on the output. For example, when the VLM is prompted about materials, the bag output transforms from regular leather to knitted, colorful material. VLM Prediction Analysis. To understand how our VLM-guided approach effectively steers generation despite operating on noisy intermediate predictions, we analyze the VLMs ability to identify emerging semantic patterns throughout the denoising process. We examine the correlation between VLM predictions on early, blurry ˆ𝑥0 estimates and the final generated content across timesteps 0 to 27. Figure 17 shows that VLM correlation rapidly increases during the initial denoising steps, reaching approximately 90% within the first 3 to 5 timesteps, despite the highly noisy nature of the early predictions. The high correlation between early VLM predictions and final outputs validates our approach of accumulating negative prompts from the beginning of the denoising process, as the predictions of the VLM are meaningful even under noisy conditions."
        },
        {
            "title": "Implementation Details",
            "content": "Unless noted, experiments use SD3.5 large, 28 steps and classifierfree guidance (CFG) 4.5. The default VLM is Qwen2.5-VL-3B-Instruct; we also support BLIP2 [Li et al. 2023], BLIP1 [Li et al. 2022], ViLT [Kim et al. 2021], and GPT-4o [OpenAI 2024]. We run on single NVIDIA A40, at 10241024 resolution. VLM Feedback Window. We allow the user to query the VLM over predefined window of steps to minimize overhead. Let 𝑡start and 𝑡stop be the step indices when both are provided; otherwise, they are set by default to 0 and 28. Within this window we query at fixed frequency 𝑓 . The default is set to 𝑓 = 1 (every step), but users may increase 𝑓 to reduce calls (e.g., every 2 or 4 steps). The feedback window and frequency integrate directly into our guidance loop; see 3 for how VLM answers are accumulated and applied. Adaptive Negative Prompting Construction. At each step 𝑡 [0,𝑇 ], we decode ˆ𝑥0 to RGB and ask set of questions {𝑞𝑖 } (𝑡 ) . We then apply light normalizer: remove unwanted prefixes, e.g., it looks like, drop leading articles, and collapse whitespace and punctuation. We maintain single negative prompt string, containing list of negatives with: (i) case-insensitive deduplication, (ii) re-encoding only when changes, and (iii) all the negatives are separated by commas. During the VLM feedback window, we update the negative half of the CFG embedding pair from the comma-joined string of negatives and keep the positive half unchanged. When leaving the VLM feedback window, we clear the negative prompt and replace it with an empty string. Decoding ˆ𝑥0: VAE vs. linear approximation. The diffusion model operates in latent space. Therefore, obtaining clean image predictions ˆ𝑥0 for input to the VLM requires passing them through the VAE decoder, which is costly at every denoising step. Prior works [Turner 2022; Vass 2024] have empirically shown that the decoders of common text-to-image diffusion models can be well-approximated by linear transformation, enabling significant acceleration of the decoding process. For example, Vass [2024] showed that, in the case of SDXL, this linear transformation can be expressed by the matrix: 𝑤 = 60 60 5 60 10 60 25 70 15 50 5 35 . similar linear transformation can be applied to SD3.5 with different weight matrix. In our method, using this linear approximation yields creative results comparable to those obtained with the full decoder, while substantially reducing computational overhead. Full Runtime Analysis. Our method adds only modest overhead in the lightweight-VLM regimes (ViLT/BLIP-1/BLIP-2), and reducing the amount of querying offers simple, effective way to trade compute for guidance strength. 14 Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik"
        },
        {
            "title": "10 User Study",
            "content": "Participants view pairwise comparisons of images generated from the same broad category (e.g., pet, building, vehicle). Each comparison shows outputs from our method versus one of the four baselines. Creative prompts: SD3.5 and GPT-4o using photo of creative/new type of [category] and creative generation methods: ConceptLab and C3."
        },
        {
            "title": "11 Metrics and Evaluation",
            "content": "Evaluation Setup. The core idea of our evaluation protocol is to represent images in the CLIP embedding space and compute metrics that characterize the resulting distribution. Standard metrics like the CLIP score measure one-to-one image-text similarity, which is problematic for creativity evaluation creative outputs should deviate from typical patterns while maintaining category membership. Table 7. User study results showing average ratings (1-5 scale) for novelty and category coherence. Our method achieves the highest novelty while maintaining strong categorical identity."
        },
        {
            "title": "Method",
            "content": "SD3.5 GPT-4o ConceptLab C3 VLM-Guided (Ours) Novelty validity 1.753 2. 3.502 2.934 4.550 4.886 4.785 3.950 3.945 4.503 Fig. 18. Distribution of fruit CLIP embeddings in 2D PCA space and the Kernel Density Estimation (KDE) of the distributions. Reference images (green): photo of fruit. Creative baseline (blue): photo of new type of fruit. Our VLM-guided method (red): explores diverse regions with minimal overlap with reference. creative pet that scores lower than typical cat on CLIP alignment might actually represent more successful creative generation. Specifically, we use the following metrics: (1) For validity assessment, we employ the CLIP score and GPT-4o verification to ensure outputs remain recognizable as valid category members despite their creative variations. Our goal is not to maximize CLIP score but to remain relatively close to reference values while exploring novel variations; (2) For novelty assessment, we compute relative typicality to measure the difference between broad category similarity (e.g., pet) and average subcategory similarity (e.g., cat, dog), ensuring outputs avoid conventional modes, alongside GPT4o Novelty Score which counts how often GPT-4o cannot classify the specific type and responds unknown; (3) For diversity assessment, we use distribution-based metrics (total variance and Vendi score [Friedman and Dieng 2022]) that quantify the spread of creative exploration in the CLIP embedding space. To evaluate and compare the methods quantitatively, we generate 100 images from four different categories: pet, garment, plant and vehicle using our method, C3, ConceptLab, and two baselines. Reference images are generated with SD3.5 from the prompt photo of [category] and Creative Prompting uses the prompt photo of creative / new type of [category]. VLM-Guided Adaptive Negative Prompting for Creative Generation 15 For the GPT score, we provide GPT-4o with generated image and ask it, Is this [category]?. Then we compute the number of times the answer was yes divided by the overall amount of images. Subcategory Selection. For relative typicality computation, we use the following subcategories: Pet: cat, dog, hamster, rabbit, bird, fish, turtle, mouse, gerbil, insect. Vehicle: car, truck, motorcycle, bicycle, bus, train, scooter, van, airplane, drone. Plant: tree, flower, cactus, fern, grass, bush, wildflower, moss, wild mushroom. Garment: shirt, jacket, dress, pants, coat, sweater, hoodie, socks, underwear."
        },
        {
            "title": "12 More Results",
            "content": "See Figure 19 for additional qualitative samples demonstrating diverse, controllable deviations from conventional object categories. Visualizing the Distribution. We begin by visualizing the resulting distribution in CLIPs space. To do so, we project embeddings to two dimensional space via PCA. In Figure 18, we visualize the CLIP embedding distributions for Reference, Creative Prompting, and our VLM-guided approach. The background distribution is computed on discrete grid of size 50 50. The density at any point 𝑝 is estimated using Gaussian KDE. The plot in Figure 18 shows that our approach pushes mass away from typical exemplars, while the Creative Prompting remains close and overlaps with the Reference distribution. Novelty and Diversity. To quantify deviation from conventional patterns, we employ two complementary metrics: Relative Typicality measures creative deviation from familiar subcategories while maintaining broad category coherence. For generated image we extract CLIP embedding 𝑧𝑖 , using CLIP-ViT-B32, and measure the alignment to the broad category text prompt embedding 𝑡𝑐 e.g., photo of pet, and subcategory text prompts embeddings e.g., photo of cat, photo of dog etc.). Overall, we compute: 𝑇rel (𝑧𝑖 ) = cosine_similarity(𝑧𝑖, 𝑡𝑐 ) max 𝑗 {1,...,𝑚} cosine_similarity(𝑧𝑖, 𝑡 ( 𝑗 ) 𝑠 ), 𝑠 (4) where 𝑡𝑐 is the CLIP text embedding of the broad category prompt and {𝑡 ( 𝑗 ) }𝑚 𝑗=1 are the embeddings of subcategory prompts. Positive values indicate the image aligns more with the broad category than with any specific known subcategory, suggesting successful creative generation within the category boundaries. GPT Novelty Score quantifies how often GPT-4o cannot identify the specific type of object. We query GPT-4o to classify each generated image into known subcategories. The score represents the fraction of images classified as unknown or unrecognizable variants, directly measuring deviation from familiar modes. The Vendi score [Friedman and Dieng 2022] quantifies diversity through the Shannon entropy of the eigenvalues of normalized similarity matrix. Formally, given collection of samples 𝑥1, . . . , 𝑥𝑛 and positive semi-definite similarity function 𝑘 : with 𝑘 (𝑥, 𝑥) = 1, let 𝐾 R𝑛𝑛 denote the kernel matrix with 𝐾𝑖 𝑗 = 𝑘 (𝑥𝑖, 𝑥 𝑗 ). The Vendi score is defined as: (cid:32) (cid:33) Vendi(X) = exp 𝜆𝑖 log 𝜆𝑖 = exp tr (cid:18) (cid:19) (cid:19) (cid:18) 𝐾 𝑛 log 𝐾 𝑛 , (5) 𝑛 𝑖=1 where 𝜆1, . . . , 𝜆𝑛 are the eigenvalues of 𝐾/𝑛, with the convention that 0 log 0 = 0. This metric can be interpreted as the effective number of dissimilar elements in the sample, ranging from 1 (all identical) to 𝑛 (all maximally distinct). Total Variance, computed as the trace of the covariance matrix Tr(Σ) = (cid:205)𝑑 𝜆𝑖 , measures overall variability across all dimensions 𝑖=1 in the CLIP embedding space. Higher values indicate greater dispersion and exploration spread. validity. While diversity and novelty distinguish creative concept from an existing one, validity ensures that it is practical, preventing it from being merely eccentric or nonsensical. We compute the practicality of the generated concepts with two metrics, CLIP text-image alignment score and GPT score to verify semantic validity. Shelly Golan, Yotam Nitzan, Zongze Wu, and Or Patashnik Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type garment Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of musical instrument Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of pet Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of vehicle Positive prompt 𝑝𝑝𝑜𝑠 : photo of new type of fruit Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative chair Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative cup Positive prompt 𝑝𝑝𝑜𝑠 : photo of creative jacket Fig. 19. More qualitative results of our method across different object categories."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Technion",
        "Tel Aviv University"
    ]
}