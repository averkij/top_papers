{
    "paper_title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation",
    "authors": [
        "Qinyu Zhao",
        "Jaskirat Singh",
        "Ming Xu",
        "Akshay Asthana",
        "Stephen Gould",
        "Liang Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and Harmon adopt diffusion sampling to improve the quality of image generation. However, this strategy leads to low inference efficiency, because it usually takes 50 to 100 steps for diffusion to sample a token. This paper explores how to effectively address this issue. Our key motivation is that as more tokens are generated during the autoregressive process, subsequent tokens follow more constrained distributions and are easier to sample. To intuitively explain, if a model has generated part of a dog, the remaining tokens must complete the dog and thus are more constrained. Empirical evidence supports our motivation: at later generation stages, the next tokens can be well predicted by a multilayer perceptron, exhibit low variance, and follow closer-to-straight-line denoising paths from noise to tokens. Based on our finding, we introduce diffusion step annealing (DiSA), a training-free method which gradually uses fewer diffusion steps as more tokens are generated, e.g., using 50 steps at the beginning and gradually decreasing to 5 steps at later stages. Because DiSA is derived from our finding specific to diffusion in autoregressive models, it is complementary to existing acceleration methods designed for diffusion alone. DiSA can be implemented in only a few lines of code on existing models, and albeit simple, achieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$ for FlowAR and xAR, while maintaining the generation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 9 2 0 2 . 5 0 5 2 : r DiSA: Diffusion Step Annealing in Autoregressive Image Generation Qinyu Zhao1, Jaskirat Singh1, Ming Xu1, Akshay Asthana2, Stephen Gould1, Liang Zheng1 1 Australian National University 2 Seeing Machines Ltd {qinyu.zhao,jaskirat.singh,mingda.xu,stephen.gould,liang.zheng}@anu.edu.au {akshay.asthana}@seeingmachines.com"
        },
        {
            "title": "Abstract",
            "content": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and Harmon adopt diffusion sampling to improve the quality of image generation. However, this strategy leads to low inference efficiency, because it usually takes 50 to 100 steps for diffusion to sample token. This paper explores how to effectively address this issue. Our key motivation is that as more tokens are generated during the autoregressive process, subsequent tokens follow more constrained distributions and are easier to sample. To intuitively explain, if model has generated part of dog, the remaining tokens must complete the dog and thus are more constrained. Empirical evidence supports our motivation: at later generation stages, the next tokens can be well predicted by multilayer perceptron, exhibit low variance, and follow closer-to-straight-line denoising paths from noise to tokens. Based on our finding, we introduce diffusion step annealing (DiSA), training-free method which gradually uses fewer diffusion steps as more tokens are generated, e.g., using 50 steps at the beginning and gradually decreasing to 5 steps at later stages. Because DiSA is derived from our finding specific to diffusion in autoregressive models, it is complementary to existing acceleration methods designed for diffusion alone. DiSA can be implemented in only few lines of code on existing models, and albeit simple, achieves 5 10 faster inference for MAR and Harmon and 1.4 2.5 for FlowAR and xAR, while maintaining the generation quality."
        },
        {
            "title": "Introduction",
            "content": "A growing number of autoregressive models introduce diffusion sampling to generate continuous tokens, such as MAR [12], FlowAR [24], xAR [25], and Harmon [38], which significantly improves generation quality. At inference time, these models take autoregressively generated tokens as input and adopt diffusion process to sample the next tokens. An illustration is shown in Figure 1(a-d). Although the diffusion process yields higher image quality for autoregressve models, it suffers from low inference efficiency because tens of denosing steps are needed to generate each token. For example, MAR [12] denoises 100 times while xAR [25] does 50 times. Our preliminary experiments show that the many-step diffusion process accounts for about 50% inference latency in MAR and 90% in xAR. Naively reducing the number of diffusion steps would accelerate these models but will significantly degrade generation quality. For example, with 10 diffusion steps, the Fr√©chet Inception Distance (FID) of xAR-L on ImageNet 256256 increases shapely from 1.28 to 8.6, and MAR-L even fails to generate meaningful images. This paper thus aims to address the efficiency issue. We are motivated by the finding that as more tokens are generated, token distributions become more constrained, and tokens become easier to sample. In other words, early generation stages are reliant on stronger distribution modeling and token sampling, while late stages are less so. Preprint. Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA Figure 1: Overview. Architecture of four autoregressive + diffusion models included in this study: (a) MAR [12]; (b) FlowAR [24]; (c) xAR [25]; (d) Harmon [38]. (e) This paper improves the efficiency of these models by reducing diffusion steps without compromising generation quality. We provide three pieces of empirical evidence to our finding. First, we train multilayer perceptron (MLP) or repurpose the original model head, based on the hidden representation of generated tokens, to predict the outcomes of the diffusion process. As shown in Figure 2, in early stages of generation, the MLP prediction is inaccurate and lacks details. In comparison, as more tokens are generated, MLP prediction becomes increasingly accurate, indicating that the autoregressive model now provides stronger conditions for the diffusion head. Second, variance in diffusion sampling gradually decreases during generation, indicating that the distribution of the next token becomes increasingly constrained. Third, based on the straightness metric [15], we show that denoising paths from noise to tokens become closer to straight lines, suggesting that we could take larger step sizes. The above finding dictates that fewer diffusion steps are needed in late generation stages than in early stages, forming the proposal of the diffusion step annealing (DiSA) method. Instead of using the same number of diffusion steps throughout the generation process, DiSA uses more diffusion steps (e.g., 50) for early tokens and gradually fewer steps (e.g., from 50 to 5) for later tokens. DiSA is training-free and can be easily implemented on top of existing autoregressive diffusion models that share similar token generation mechanisms, such as those in Figure 1(a-d). Moreover, because DiSA comes from our finding specific to diffusion in autoregressive models, it can be effectively used together with existing acceleration methods specifically designed for diffusion. Experiments show that DiSA is very useful: it consistently improves the inference efficiency of MAR by 5 10 and FlowAR and xAR by 1.4 2.5 without sacrificing image generation quality. In summary, this paper covers three main points. First, we reveal that the role of diffusion in autoregressive models is different along the generation process. Second, based on this insight, we design new sampling strategy, DiSA, for scheduling diffusion steps in autoregressive image generation. Third, experiments demonstrate that DiSA delivers very useful acceleration during inference while exhibiting competitive or better generation performance."
        },
        {
            "title": "2 Related-Work",
            "content": "Autoregressive models meet diffusion. common practice for autoregressive image generation is to quantize an image into discrete tokens [23, 2, 11] and train autoregressive models on the tokens. main bottleneck for these models is that discrete tokens introduce quantization errors, limiting the generation quality [35, 12, 4]. To address this, MAR [12] uses continuous tokens and adopts diffusion model head to sample the next tokens in autoregressive models. Other continuous-token design appears later [24, 25, 38]. These methods have good generation quality but low efficiency. Acceleration techniques for diffusion models. It is well-established area in diffusion. Fast sampling processes have been proposed, such as DDIM [29], DPM-Solver [17], and DPM-Solver++ [18], to name few. These methods are designed specifically for diffusion and can be used together with our approach. In comparison, less attention has been paid to accelerating diffusion in autoregressive models. LazyMAR [39] introduces two caching techniques, while CSpD [37] applies speculative 2 decoding for speeding up the inference of MAR. These works mainly focus on the autoregressive part of MAR, without modifying the diffusion process, so are orthogonal to our approach. Besides, FAR [5] replaces the diffusion head of MAR with short-cut model, achieving 2.3 acceleration. FAR is trained from scratch, while our method is training-free."
        },
        {
            "title": "3.1 Revisiting Existing Models",
            "content": "With an image tokenizer, an image can be represented as sequence of tokens x1, x2, . . . , xn. For example, we can use VAE [10, 26] to encode an image to 256 tokens. Image generation can be framed as sampling from the joint distribution of image tokens p(x1, x2, . . . , xn). Sampled tokens are decoded by the tokenizer back into images. An autoregressive model formulates the generation of an image as next-token prediction task: p(x1, x2, . . . , xn) = (cid:89) i=1 p(xi x1, . . . , xi1) where xi p(xi x1, . . . , xi1). (1) Note that recent works propose new autoregressive paradigms [33, 25]. In next-scale prediction [33], given the tokens of low-resolution image, the model generates the tokens of higher resolution. MAR and xAR generate group of tokens in each autoregressive step. For these models, xi represents group of tokens. We interchangeably use xi to denote these tokens for simplicity. Recent autoregressive models adopt diffusion process to sample xi p(xi x1, . . . , xi1). MAR [12] uses an encoder-decoder backbone , which takes tokens previously generated as input and predicts condition vector zi = (x1, x2, . . . , xi1) for the next token. The diffusion model head, conditional on zi, denoises sampled noise to token via reverse process. At training time, parameters in œµŒ∏ and are updated based on the diffusion loss [9, 21]: L(zi, xi) = Ei,œµ,t (cid:104)(cid:13) (cid:13)œµ œµŒ∏(xi t, zi)(cid:13) (cid:13) 2(cid:105) , xi = Œ±txi + 1 Œ±tœµ, (2) where œµ Rd is noise vector sampled from (0, I) and U(1, . . . , ). and are Gaussian and Uniform distributions, respectively. Œ±t defines noise schedule [9, 21]. FlowAR [15] uses VAR [33] as the backbone and flow matching [15, 19] as the the model head vŒ∏. Similar to MAR, the backbone takes previous generated tokens as input, and predicts condition vector zi for each next token. With sampled noise token, the flow matching head predicts velocity for denoising the token. At training time, the flow matching loss is calculated as: L(zi, xi) = Ei,œµN (0,I),t[0,1] (cid:104)(cid:13) (cid:13)(œµ xi) vŒ∏(xi t, zi)(cid:13) (cid:13) 2(cid:105) , where xi = (1 t)xi + tœµ. (3) xAR [25] takes both previously generated tokens and sampled noise as input. The model runs 50 times for denoising the noise into tokens and continues to sample the next tokens. Harmon [38] is unified model for both text-to-image (T2I) and image-to-text generation. This study focuses on its T2I ability. The backbone in Harmon takes the text prompt and generated tokens as input and produces condition vector for the next token. diffusion head, conditional on the vector, denoises sampled noise to the next token."
        },
        {
            "title": "3.2 More Tokens Generated, Stronger Constraints on Later Tokens",
            "content": "The diffusion process in the four models samples the next token from the condition distribution xi p(xi x1, . . . , xi1). Our key motivation is that, as more tokens are generated, the condition becomes stronger, making the distribution more constrained and the next tokens easier to sample. We will show empirical evidence to support the motivation. First, next tokens can be well predicted at later autoregressive generation stages. We probe the condition from the generated tokens, i.e., we use model to predict the sampled xi based on the hidden representation of the generated tokens {x1, . . . , xi1}. For MAR and Harmon, we train MLP 3 - L - o - x . 5 1 - r AR Step 18 AR Step 26 AR Step 42 AR Step"
        },
        {
            "title": "Generated",
            "content": "AR Step 1 AR Step 2 AR Step 3 AR Step"
        },
        {
            "title": "Generated",
            "content": "AR Step 0 AR Step 2 Paper artwork, layered paper, colorful Chinese dragon surrounded by clouds. AR Step 1 AR Step"
        },
        {
            "title": "Generated",
            "content": "A photo of pink stop sign. realistic landscape shot of the Northern Lights dancing over snowy mountain range in Iceland. Happy dreamy owl monster sitting on tree branch, colorful glittering particles, forest background, detailed feathers. AR Step 0 AR Step 26 AR Step AR Step"
        },
        {
            "title": "Generated",
            "content": "Figure 2: Image prediction results at different stages of generation. In each image pair, the left image shows the currently generated tokens, while the right shows the final image we predict based on the generated tokens. The prediction results are inaccurate and lack details in early stages but become increasingly accurate as more tokens are generated. This is consistent across the four models. 4 Figure 3: Diffusion processes in later generation stages show (a-b) lower variance and (c) closer-tostraight-line denoising paths. (a) Two examples. In each example, the autoregressive step increases from top to bottom rows. 0%, 10%, 20% of tokens have been generated, respectively, as shown in the first column. We observe that the variance of sampled images drops from top to bottom rows. (b) Variance of diffusion-sampled tokens decreases along the autoregressive steps. The y-axis uses logarithmic scale and each line represents different token dimension. (c) Straightness of denoising paths increases from early to late stages. All results are obtained from the MAR-B model. model to replace the original model head. The MLP predicts xi directly based on the condition from the generated tokens zi. For FlowAR and xAR, we repurpose the original model head for flow matching. Specifically, we feed sampled noise with = 1 into the model, obtain the estimated velocity vŒ∏(xi t=1 is purely noisy, the model has to directly predict the xi based on zi. = 1, zi), and predict the next token as xi t=1 v. Since xi 0 = xi As shown in Figure 2, in the early stage of generation, the predicted tokens and the generated images are blurry and in low quality. But as more tokens have been generated, the MLP predictions become increasingly more accurate, suggesting that stronger conditions are provided by the generated tokens. Second, next tokens have lower variance at later autoregressive steps. We explore the variance in the distribution of the next tokens. Specifically, we use MAR to generate 10K images. When generating each xi, we sample 100 possible xi and calculate the variance in sampling. The generated examples and the average variance are shown in Figure 3(a-b). As seen, as more tokens are generated, the distribution of the next token becomes increasingly constrained. Third, diffusion paths at later stages are closer to straight lines. Rectified Flow [15] proposes that straight paths from noise to data distribution are preferred, because they can be simulated with coarse time discretization and hence need fewer steps at inference time. Inspired by this, we measure the straightness of denoising path {xt}1 t=0 under condition z. S({xt} t=0, z) = Et[0,1] (x1 x0) vŒ∏(xt t, z)2(cid:105) (cid:104) . (4) MAR and Harmon use diffusion process and are not trained on the rectified flow loss function. Thus, we calculate the cosine similarity between the score (the gradient of the data distribution density) [30, 31, 1] and the straight direction from the noisy token to the clean token. S({xt} t=0, z) = Et [cos (x0 xt, xt log pŒ∏(xt t, z))] , (5) where xt log pŒ∏(xt t, z) = 1 œµŒ∏(xt t, z). As shown in Figure 3(c), in the later stage of generation, the diffusion paths become closer to stright line, indicating that we can use larger step size and fewer diffusion steps are needed [15]. The results on FlowAR, xAR, and Harmon and details of implementation are shown in the Appendix. 1 Œ±t"
        },
        {
            "title": "3.3 Diffusion Step Annealing",
            "content": "Based on the observation, we propose training-free sampling strategy, DiSA. In the early stage of generation, the distribution of the next tokens is diverse so we allow the diffusion process to run more 5 Figure 4: Impact of different numbers of diffusion steps in early generation stages Tearly and in late stages Tlate on (a) MAR-B; (b) MAR-L. In the first and third columns, we fix Tlate = 50 and reduce Tearly, which significantly degrades generation quality. But as shown in the second and fourth columns, if we fix Tearly = 50 and decrease Tlate, the degradation in generation quality is marginal. times, e.g., 50 steps. In the later stage, as the distribution of the next token is more constrained, we assign gradually fewer steps to diffusion, e.g., 5 steps. We introduce and compare three different time schedulers in DiSA: two-stage, linear, and cosine. Let (k) denote the number of diffusion steps when the autoregressive step is k. Tearly and Tlate are two parameters to control the number of steps. In short, the two-stage method is just cutting the generation into the early and late stages. In the early stage, the diffusion process runs Tearly while in the late stage, runs Tlate times. The linear and cosine methods transition smoothly from Tearly to Tlate in the generation process. Specifically, they are defined as follows, Two-stage: (k) = (cid:26)Tearly, < K/2 otherwise Tlate, , Linear: (k) = Tearly + (Tlate Tearly) k/K, Cosine: (k) = Tlate + (Tearly Tlate) (cid:18) cos( 1 2 (cid:19) œÄ) + 1 , (6) (7) (8) where is the total number of the autoregressive steps and (k) is rounded to the nearest integer. preliminary experiment based on MAR is conducted to validate our method. We implement three time schedulers on MAR-B and MAR-L, modify values of Tearly and Tlate, and evaluate the model on ImageNet 256256 generation. Fr√©chet Inception Distance (FID) [7] and Inception scores [28] on 50K sampled images are reported to measure the generation quality. The number of autoregressive step is set to 64, and the default values of Tearly and Tlate are both 50. In Figure 4, reducing number of diffusion steps in early stages degrades the generation quality, but using fewer diffusion steps in later stages does not, which supports our motivation again. We use the linear scheduler in subsequent experiments, which has relatively better performance among the three methods. We find that reducing Tlate to less than 20 leads to poor generation results in MAR. The main reason is that the diffusion head has inaccurate prediction around = 999. Thus, we let the diffusion start with = 950, i.e., adding an initial time offset, following the practice in diffusion models [29, 14, 13, 36]. This allows us to further reduce the diffusion steps in MAR. For FlowAR and xAR, we do not observe this phenomenon and the sampling process starts with = 1.0. We discuss this further in Section 4.2."
        },
        {
            "title": "3.4 More Insights and Discussions",
            "content": "MAR vs MAE. We show that an MLP can well predict the remaining tokens. This bridges the underlying mechanism between MAR and masked auto-encoder (MAE) [6]. The former uses generative method to unmask an image, while the latter uses deterministic way to do so. This is also consistent with recent findings where MAR encodes semantic information for an image [38]. Difficulty level of token distribution modeling. Condition vectors in later generation stages offer more information, making token distributions easier to model. This may also hold in other 6 Table 1: System-level method comparison on ImageNet 256256 Our method significantly improves the inference efficiency of MAR, FlowAR, and xAR, while maintaining their generation quality. Diffusion steps b means starting with steps and transition to steps via Eq. (7). The average inference time per image and speed-ups of different methods are reported. Model #params AR steps Diff steps FID LDM-4 [27] DiT-XL/2 [22] GIVT [35] MAR-B [12] LazyMAR-B [39] FAR-B [5] MAR-B + DiSA MAR-L [12] LazyMAR-L [39] FAR-L [5] MAR-L + CSpD [37] MAR-L + DiSA MAR-H [12] 400M 675M 304M 208M 208M 172M 208M 479M 479M 406M - 479M 943M LazyMAR-H [39] 943M MAR-H + CSpD [37] MAR-H + DiSA - 943M VAR-d30 [34] FlowAR-S [24] FlowAR-S + DiSA FlowAR-L [24] FlowAR-L + DiSA FlowAR-H [24] FlowAR-H + DiSA xAR-B [25] xAR-B + DiSA xAR-L [25] xAR-L + DiSA xAR-H [25] xAR-H + DiSA 2.0B 170M 170M 589M 589M 1.9B 1.9B 172M 172M 608M 608M 1.1B 1.1B A x - - 256 256 64 64 32 256 64 32 256 64 64 32 256 - 64 32 256 64 64 32 - 64 32 10 5 5 5 5 5 5 4 4 4 4 4 4 - 250 - 100 50 100 100 8 505 255 100 50 100 100 8 - 505 255 100 50 100 100 - 505 255 - 25 2515 25 2515 50 50 50 5015 50 5015 50 5015 3.60 2.27 3.35 2.31 2.39 (+0.08) 2.45 (+0.14) 2.64 (+0.33) 2.37 (+0.06) 2.31 (+0.00) 2.35 (+0.04) 1.78 1.86 (+0.08) 1.93 (+0.15) 2.11 (+0.33) 1.99 (+0.21) 1.81 (+0.03) 1.77 (-0.01) 1.88 (+0.10) 1.55 1.65 (+0.10) 1.69 (+0.14) 1.94 (+0.39) 1.60 (+0.05) 1.57 (+0.02) 1.72 (+0.17) 1.92 3.70 3.74 (+0.04) 1.87 1.90 (+0.03) 1.67 1.69 (+0.02) 1.67 1.68 (+0.01) 1.28 1.23 (-0.05) 1.24 1.23 (-0.01) IS 247.7 278.2 - 281.7 281.0 (-0.7) 281.3 (-0.4) 276.0 (-5.7) 265.5 (-16.2) 282.3 (+0.6) 282.9 (+1.2) 296.0 294.0 (-2.0) 297.4 (+1.4) 284.4 (-11.6) 293.0 (-3.0) 303.7 (+7.7) 298.3 (+2.3) 295.1 (-0.9) 303.7 299.8 (-3.9) 299.2 (-4.5) 284.1 (-19.6) 301.6 (-2.1) 303.1 (-0.6) 303.4 (-0.3) 323.1 235.1 235.2 (+0.01) 273.1 274.8 (+1.7) 276.3 273.8 (-2.5) 265.2 265.5 (+0.3) 292.5 287.3 (-5.2) 301.6 300.5 (-1.1) Pre. Rec. Time (s) Speed-Up 0.87 0. 0.84 0.82 0.82 - - - 0.83 0.83 0.81 0.80 - - - - 0.81 0.81 0.81 0.80 - - - 0.80 0.80 0.82 0.81 0.81 0.80 0.80 0.80 0.80 0.80 0.79 0.82 0.79 0.83 0.79 0.48 0. 0.53 0.57 0.57 - - - 0.56 0.56 0.60 0.61 - - - - 0.61 0.61 0.62 0.62 - - - 0.62 0.61 0.59 0.51 0.51 0.62 0.61 0.62 0.62 0.62 0.62 0.62 0.66 0.64 0.66 - 1.859 - 0.650 0.134 0.061 0.045 - 0.114 0.057 1.102 0.250 0.106 0.080 - - 0.216 0.108 1.957 0.462 0.191 0.145 - 0.404 0.209 0.039 0.024 0.018 0.124 0.082 0.423 0.167 0.130 0.084 0.394 0.255 0.896 0.577 - - - 1.0 4.8 10.6 14.3 2.3 5.7 11.3 1.0 4.4 10.4 13.8 1.4 1.5 5.1 10.2 1.0 4.2 10.2 13.5 2.3 4.8 9.3 - 1.0 1.4 1.0 1.5 1.0 2.5 1.0 1.6 1.0 1.5 1.0 1.6 We test the latency of generating batch of 128 images instead of 256 to reduce memory usage. Estimated based on their paper. autoregressive models. For example, recent works use Gaussian Mixture Model to model token distribution [35, 40]. It is possible that the early stage needs more Gaussian components while later stages require fewer. We leave this as future work. Strong diffusion conditions in computer vision. It is intuitive to understand that the condition vector which summarizes more previously generated tokens is more informative. Therefore, fewer diffusion steps would still sample good token. This is consistent with some existing works in image contour detection and depth estimation using diffusion models: because of the strong image condition, few and even one diffusion step would yield competitive results [16, 32, 41]. In T2I generation, text prompt seems weak condition. Our prediction results on Harmon in Figure 2 show that, text prompt helps to determine the basic the structure of the image, leaving details for generation."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details, Datasets, and Metrics Experiments mainly includes four pretrained models: MAR [12], FlowAR [24], xAR [25], and Harmon [38]. MAR, FlowAR, and xAR are evaluated on the ImageNet 256 256 generation task. We report FID [7], IS [28], Precision, and Recall, following common practice in image generation [1]. We also measure the inference time of generating batch of 256 images for these models. Harmon is evaluated on the T2I benchmark GenEval [3]. Averaged accuracy and inference time are reported. All experiments are run on 4 NVIDIA A100 PCIe GPUs. 7 Table 2: Text-to-image generation of Harmon on GenEval benchmark. The accuracy on each task and the average inference time per image are reported. AR steps Diff steps Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Time per image (s) 32 64 50 505 25 255 50 505 100 100 0.99 0.99 0.05 0.99 0.99 0.99 0.99 0.99 0.86 0.85 0.00 0.89 0.88 0.89 0.86 0.90 0.64 0.69 0.00 0.74 0.71 0.68 0.69 0. 0.87 0.86 0.00 0.86 0.88 0.87 0.89 0.86 0.43 0.48 0.00 0.46 0.48 0.41 0.48 0.49 0.49 0.52 0.00 0.54 0.53 0.55 0.50 0. 0.71 0.73 0.01 0.75 0.74 0.73 0.73 0.74 12 8 17 14 24 17"
        },
        {
            "title": "4.2 Evaluation",
            "content": "DiSA consistently improves the effiency of baseline AR+Diffusion models. We apply DiSA to MAR, xAR, and FlowAR and compare the performance on the ImageNet 256 256 generation task in Table 1. Overall, DiSA consistently enhances the efficiency of the baseline models while maintaining competitive generation quality. For MAR, the original best performance is achieved with 256 autoregressive steps and 100 diffusion steps. After integrating DiSA to MAR, e.g., 50 5, we report speed-up of 5.7 on MAR-B, 5.1 on MAR-L, and 4.8 on MAR-H, respectively. The generation quality change is minor: DiSA results in the same FID on MAR-B and increases FID by 0.02 on MAR-H. If we further reduce MAR to 32 autoregressive steps and 25 5 diffusion steps, DiSA results in 9.3-11.3 speed-ups on MAR with slightly degraded generation quality. For example, DiSA achieves 11.3 faster inference on MAR-B while increasing FID by 0.04. Similarly, FlowAR-H with DiSA achieves 2.5 speed-up while maintaining competitive FID of 1.69 and IS of 273.8. In the case of xAR models, DiSA provides up to 1.6 speed-up with negligible impact on performance metrics. Interestingly, xAR-L shows 1.6 speed-up and even improved FID from 1.28 to 1.23 with DiSA. These results clearly indicate the usefulness of DiSA. Comparison with other acceleration methods on MAR. DiSA is faster than CSpD [37] and FAR [5], and is competitive to LazyMAR [39]. Note that LazyMAR works on caching techniques for MAR, without modifying the diffusion process, and is orthogonal to DiSA. It is interesting to combine LazyMAR and DiSA in future work. DiSA is also useful on T2I generation models. As shown in Table 2, DiSA can also speed up Harmon on T2I generation tasks on GenEval. As seen, Harmon with DiSA uses 8 seconds per image, 5 faster than the original implementation, while achieving comparable performance. Table 3: Existing methods speed up MAR sampling and can be used together with DiSA for further speed-up. The number of autoregressive steps is 64. Method Time (s) #Steps FID IS Original Time offset + DiSA DDIM 25 50 100 25 50 505 6.78 4.30 4. 4.61 4.64 4.17 25 50 505 4.16 4.06 4.00 148.8 174.5 173.7 171.0 171.1 173.7 178.2 176.6 179. 179.4 176.1 177.1 179.5 176.1 177.2 17.0 21.9 30.6 16.8 20.7 17.0 17.7 22.1 17.9 17.4 20.6 17. 18.5 22.0 19.0 DiSA is complementary to existing diffusion acceleration methods. We implement several existing diffusion acceleration techniques on MAR-B. Time offset: We start the diffusion process from = 950 instead of = 999. Faster samplers: We include DDIM [29], DPM-Solver [17], and DPM-Solver++ [18]. Note that FlowAR uses the Euler sampler while xAR uses the EulerMaruyama sampler [20, 8], so we omit detailed discussions of the two samplers here. DPM-Solver++ 15 25 2510 15 25 2510 4.58 4.35 4.37 4.57 4.34 4. DPM-Solver + DiSA + DiSA + DiSA As shown in Table 3, existing techniques designed for diffusion can accelerate sampling in autoregressive models. Time offset reduces the number of diffusion steps but suffers from slight quality degradation. DDIM achieves remarkable FID of 4.06 at 50 steps and 4.16 at 25 steps. DPM-Solver and DPM-Solver++ show comparable performance and reduce the number of diffusion steps to 25. Our method is complementary to these diffusion acceleration approaches. If we combine time offset with DiSA, inference time can be reduced to 17.0 and FID is improved to 4.17. With similar inference speed, time offset uses 25 steps and FID is 4.61. For the other three solvers, combining with DiSA also improves the inference speed while maintaining comparable generation quality. 8 Figure 5: Speed-quality trade-off for (a) MAR-B with {16, 32, 64, 128} autoregressive steps; (b) MAR-B with {25, 50, 100} diffusion steps; (c) MAR-L with {16, 32, 64, 128} autoregressive steps; (d) MAR-L with {25, 50, 100} diffusion steps; (e) FlowAR-L with {8, 10, 15, 20, 25 } flow matching steps; (f) xAR-B and (g) xAR-L with {15, 20, 25, 40, 50} flow matching steps; and (h) Harmon-1.5B with different autoregressive and diffusion steps. - M 5 . 1 - r - A F - x Figure 6: Sample image generation results. For MAR-H and Harmon-1.5B, we present the samples generated using DiSA. For FlowAR and xAR, each image pair is generated with the same random seed, where the first is generated without DiSA while the other is with DiSA. We find that DiSA helps generate similar quality images while speeding up image generation by 2.5 and 1.6 respectively. Trade-off between efficiency and quality. We show the trade-off of speed and generation quality in Figure 5. For MAR-B and MAR-L, we evaluate different autoregressive and diffusion steps. FlowAR-L, xAR-B, and xAR-L are evaluated with different flow matching steps. Harmon-1.5B runs with different autoregressive and diffusion steps on the GenEval benchmark. As seen, under different settings, DiSA can significantly improve the inference speed of these models, while maintaining the generation quality. We also present sample generation results in Figure 6. Detailed results and more examples are provided in the Appendix."
        },
        {
            "title": "5 Conclusion",
            "content": "W study how to effectively reduce the number of diffusion steps in autoregressive models. We find that as more tokens are generated, the reliance on many diffusion steps is alleviated. Based on this, we propose DiSA, training-free strategy that gradually decreases the number of diffusion steps during the generation process. This approach is easy to implement and significantly improves inference speed while maintaining competitive image quality. Our study provides interesting insights into the diffusion process in autoregressive image generation, and our future work will focus on how perception models and generative models converge."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to express our sincere appreciation to Tianhong Li, Zhanhao Liang, and Zhengyang Yu for the insightful discussion that greatly inspired our thinking during the course of this project. We are also deeply thankful to Caixia Zhou, Xingjian Leng, Sam Bahrami, Francis Snelgar, Qingtao Yu, Yunzhong Hou, Weijian Deng, Yang Yang, Yuchi Liu, Zeyu Zhang, and all our lab colleagues for their invaluable support. Their collaborative efforts, insightful discussion, and constructive feedback have been crucial in shaping and improving our paper. This work was supported by an Australian Research Council (ARC) Linkage grant (project number LP210200931)."
        },
        {
            "title": "References",
            "content": "[1] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 5, 7 [2] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [3] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 7 [4] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. 2 [5] Tiankai Hang, Jianmin Bao, Fangyun Wei, and Dong Chen. Fast autoregressive models for continuous latent generation. arXiv preprint arXiv:2504.18391, 2025. 3, 7, 8 [6] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. 6 [7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, 7 [8] Desmond Higham. An algorithmic introduction to numerical simulation of stochastic differential equations. SIAM review, 43(3):525546, 2001. 8 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [10] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 3 [11] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 2 [12] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 1, 2, 3, 7 [13] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. 6 [14] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 6 [15] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3, 5 [16] Zihang Liu, Zhenyu Zhang, and Hao Tang. Semantic-guided diffusion model for single-step image super-resolution. arXiv preprint arXiv:2505.07071, 2025. 7 [17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 2, [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 2, 8 10 [19] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 3 [20] Gisiro Maruyama. Continuous markov processes and stochastic equations. Rendiconti del Circolo Matematico di Palermo, 4:4890, 1955. 8 [21] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. 3 [22] William Peebles and Saining Xie. Scalable diffusion models with Transformers. In ICCV, 2023. 7 [23] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 2 [24] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. 1, 2, [25] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 1, 2, 3, 7 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 7 [28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6, [29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2, 6, 8 [30] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 5 [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 5 [32] Ziyang Song, Zerong Wang, Bo Li, Hao Zhang, Ruijie Zhu, Li Liu, Peng-Tao Jiang, and Tianzhu Zhang. Depthmaster: Taming diffusion models for monocular depth estimation. arXiv preprint arXiv:2501.02576, 2025. [33] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 3 [34] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. 7 [35] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. GIVT: Generative infinite-vocabulary Transformers. arXiv:2312.02116, 2023. 2, 7 [36] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022. 6 [37] Zili Wang, Robert Zhang, Kun Ding, Qi Yang, Fei Li, and Shiming Xiang. Continuous speculative decoding for autoregressive image generation. arXiv preprint arXiv:2411.11925, 2024. 2, 7, 8 [38] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. 1, 2, 3, 6, 7 [39] Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, Yulin Wang, Xuming Hu, Huiqi Li, and Linfeng Zhang. Lazymar: Accelerating masked autoregressive models via feature caching. arXiv preprint arXiv:2503.12450, 2025. 2, 7, 8 [40] Qinyu Zhao, Stephen Gould, and Liang Zheng. Arinar: Bi-level autoregressive feature-by-feature generative models. arXiv preprint arXiv:2503.02883, 2025. 7 [41] Caixia Zhou, Yaping Huang, Mochu Xiang, Jiahui Ren, Haibin Ling, and Jing Zhang. Generative edge detection with stable diffusion. arXiv preprint arXiv:2410.03080, 2024."
        }
    ],
    "affiliations": [
        "Australian National University",
        "Seeing Machines Ltd"
    ]
}