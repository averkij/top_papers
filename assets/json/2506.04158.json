{
    "paper_title": "Image Editing As Programs with Diffusion Models",
    "authors": [
        "Yujia Hu",
        "Songhua Liu",
        "Zhenxiong Tan",
        "Xingyi Yang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 5 1 4 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Image Editing As Programs with Diffusion Models",
            "content": "Yujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, and Xinchao Wang National University of Singapore {yujia.hu,songhua.liu,zhenxiong,xyang}@u.nus.edu,xinchao@nus.edu.sg Figure 1: Visual results of our IEAP. Rows 1 and 3 showcase complex multi-step edits (Row 1 is further decomposed into individual instructions), while Row 2 shows single-instruction edits. Single instructions are underlined if needing to be reduced to atomic operations."
        },
        {
            "title": "Abstract",
            "content": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via lightweight adapter sharing the same DiT backbone and is specialized for specific type of edit. Programmed by vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this Corresponding Author Preprint. Under review. way, IEAP generalizes robustly across wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available here."
        },
        {
            "title": "Introduction",
            "content": "Image editing lies at the heart of wide range of applications from photo retouching and content creation to visual storytelling and scientific visualization [42, 5, 62]. With the advent of diffusion models [23, 53, 47], the field has shifted towards highly precise and controllable manipulations [45, 12, 61]. The inherently progressive denoising process enables multi-stage pipelines [24, 4, 2] and localized editing methods [10, 74, 57], and its native support for multi-modal inputs has inspired unified frameworks that integrate heterogeneous signals within single model [33, 15, 68, 18]. More recently, text-to-image pipelines based on Diffusion Transformers (DiTs) [46, 13, 31] have set new standards in generative fidelity. However, their capacity for instruction-driven editing [41, 27] remains under-explored. Notably, although there are few existing methods [77, 37] that have extended DiTs to instruction-driven editing, they are always restricted to narrow set of common editing operations and lack evaluation on comprehensive editing tasks. To address this limitation, we initiate taxonomy study of image editing instructions to systematically assess the editing capabilities of current DiT-based conditional generation methods. Our empirical analysis reveals an interesting performance dichotomy: While current methods demonstrate proficiency in structurally-consistent edits where the layouts of the input and output images remain aligned, they exhibit significant degradation when handling structurally-inconsistent operations that require layout modifications. To overcome this issue, we introduce Image Editing As Programs (IEAP), unified framework atop the DiT architecture which is capable of handling diverse types of editing operations efficiently and robustly in this paper. Notably, we show that structurally-inconsistent instructions can in fact be reduced to small set of simple operations, which are called as atomic operations in our paper. Thus, instead of treating each edit as monolithic, end-to-end task, IEAP levarages the Chain-of-Thought (CoT) reasoning [63] to break the original editing command into sequence of atomic operations, which are namely Region of Interest (RoI) localization, RoI inpainting, RoI editing, RoI compositing and global transformation, and then executes them in sequential manner via neural program interpreter [49]. The five atomic operations serve as the fundamental building blocks for complex editing tasks. As such, through the sequential combination of atomic operations, IEAP can robustly handle complex, multi-step instructions that are typically confound in conventional end-to-end approaches. Extensive experiments show that our framework demonstrates state-of-the-art performance across standard benchmarks, excelling in both structural preservation and alteration tasks through atomiclevel operation decomposition compared to other approaches. Simultaneously, the CoT reasoning and programming pipeline of IEAP enable significantly more accurate and semantically more coherent edits under complex, multi-step instructions even compared to the leading proprietary models. Our main contributions can be summarized as follows: We present comprehensive taxonomy and empirical analysis of instruction-driven editing in DiT-based conditional generation, revealing performance dichotomy between structurallyconsistent and -inconsistent edits. We introduce Image Editing As Programs (IEAP), unified framework on the DiT backbone that leverages CoT reasoning to parse free-form instructions into sequential atomic operations and then executes them sequentially by neural program interpreter, thereby enabling robust handling of layout-altering and complex edits. Extensive experiments demonstrate that IEAP achieves state-of-the-art performance in both structure-preserving and -altering scenarios, delivering notably higher accuracy and semantic fidelity especially on complex, multi-step instructions compared to existing methods."
        },
        {
            "title": "2 Related Work",
            "content": "Conditional image generation. Early conditional image generation approaches like ControlNet [74] typically adopt plug-in control adapters to incorporate single condition [3, 16, 35] like segmentation mask or diverse conditional inputs [79, 48, 26, 40, 67] to guide the generation of images. Recently, the field of conditional image generation has witnessed remarkable breakthroughs through the integration of DiTs [13, 46, 31], with continuous innovations improving output quality and edit precision [45]. Some methods [66, 32, 65, 9] aim to create unified DiT foundation for versatile conditional image generation and editing by integrating diverse inputs within single framework. while approaches like OminiControl [59] and so on [60, 76, 38, 77, 64] leverage LoRA-based fine-tuning [25] for lightweight and effective control. Instructional image editing. Instruction-based image editing [41, 27] enables intuitive, languagedriven modifications of existing images. Early works like InstructPix2Pix [6] establishes paired instructionimage datasets for supervised fine-tuning of diffusion models. For subsequent works, some of them focus on architectural refinement [38, 37, 78, 34, 20], which introduce specialized conditioning units and multi-stage training to improve control granularity and consistency, others concentrate on data-centric enhancements [73, 17, 55, 8], that expand instruction coverage and diversify edit examples. Moreover, some approaches [72, 28, 33, 15] has unified LLM-based [1] language reasoning with diffusion-based synthesis in single framework, and some [69, 75] leverage CoT [63] and in-context learning [21] to enhance the reasoning ability of models for more complex editing tasks. More recently, some works [14, 77, 37] have advanced image editing with DiTs. For instance, ICEdit [77] leverages the in-context generation capabilities of large-scale DiTs to achieve flexible few-shot instruction editing, while Step1X-Edit [37] focuses on large-scale data construction and multi-modal integration to enable general-purpose image editing with performance approaching proprietary models."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Diffusion Transformer Fundamentals. The image generation process of text-guided DiTs [46, 13, 31] is accomplished by successively denoising input tokens in multiple steps. At step t, the model processes: St = [Xt, CT ] (1) where Xt RN represents noisy image tokens and CT RM denotes text tokens, they share the embedding dimension d. Image tokens use Rotary Position Embedding (RoPE) [58] with spatial coordinates (i, j), while text tokens fix positions at (0, 0), enabling Multi-Modal Attention (MMA) [44] mechanisms to model cross-modal interactions. Unified Conditioning Framework. To integrate visual control signals, the prior work [59] extends the baseline formulation by incorporating encoded condition images: St = [Xt, CT , CI ] (2) where CI RN denotes latent tokens from condition images via the pretrained VAE encoder [30, 52]. This unified sequence enables tri-modal fusion within transformer architectures, eliminating spatial misalignment inherent in feature concatenation baselines. Moreover, an auxiliary adaptive positional encoding mechanism further preserves spatial consistency across these modalities by assigning coordinates to each token type with minimal overhead. Gap in Instruction-Driven DiT Editing. Despite the rapid advances in DiT-based conditional image generation [59, 76, 38, 64], research on instruction-driven editing [41, 27] remains scarce. The few existing methods [77, 37] that do support instructional edits are typically confined to small set of routine operations, and lack comprehensive evaluation across diverse editing scenarios, leaving DiTs true editing potential unclear. This gap motivates us to conduct taxonomy study of DiTs ability in instructional image editing, which is detailed in Sec. 3.2. 3 Figure 2: Results of our preliminary experiments. Figure (a) shows the GPT-4o scores for three editing types across instruction faithfulness and semantic consistency, ranging from 1 to 5. Figure (b) shows the representative failure cases from local semantic editing."
        },
        {
            "title": "3.2 Preliminary Experiments and Observations",
            "content": "To this end, we conduct comprehensive evaluation of diffusion models for instruction-driven editing, uncovering an interesting performance dichotomy: While these methods excel at structurallyconsistent edits, they falter dramatically on structurally-inconsistent operations that demand explicit layout modifications. Taxonomy and Experimental Setup. To enable systematic analysis [27, 70, 69], we first categorize instruction-based image editing into three main types: local semantic editing, which modifies the identity, position or size, e.g., add, remove, replace, action change, move and resize; local attribute editing, which adjusts certain properties of objects, e.g., color change, texture change, appearance change, expression change, and background change; and overall content editing, which alters the whole image consistently, e.g., tone transfer and style change. Then we use AnyEdit dataset [70] and OminiControl [59] to train models on the above editing types, accompanied by GPT-4o [29] to rate each edit on instruction faithfulness and semantic consistency. Results and Analysis. As shown in Fig. 2(a), both local attribute editing and overall content editing attain relatively high GPT-4o scores, whereas local semantic editing exhibits notable performance drop. As illustrated in Fig. 2(b), the cases of add and action change alter unrelated areas like the background, and the remaining four cases demonstrate complete failure. We attribute this discrepancy to the fact that, unlike local attribute and overall content edits, local semantic edits require explicit spatial-layout modifications. For instance, add and delete operations necessitate instance-level scene recomposition, while move and resize further demand precise coordinate system recalibration. Key Insight. Based on the above analysis, spatial-layout modification remains critical challenge for diffusion-based editing models; conversely, edits that preserve the original layout demonstrate substantially better performance. We speculate that, with limited training data, it is difficult for the model to learn the complex patterns underlying layout-changing tasks. Although DiT architectures [46, 13, 31] employ powerful full-attention mechanisms to capture long-range dependencies, they still struggle with editing operations that require nontrivial scene reconfiguration. Due to the combinatorial complexity of spatial-layout modifications and the empirical limitations of DiT architectures, we propose to simplify the layout-editing paradigm through decomposition, which is detailed in Sec. 4."
        },
        {
            "title": "4.1 Program with Atomic Operations",
            "content": "The insight in Sec. 3.2 motivates us to decouple semantic and spatial reasoning. Building on this foundation, we propose programmatic reduction framework that systematically decomposes complex editing instructions into modular atomic operations. Specifically, we first formulate instruction-driven image editing as an executable program via Chain-of-Thought (CoT) reasoning [63], and then use neural program interpreter [49] to transcode the reasoning graph into dynamic execution plan, sequentially invoking relevant atomic modules. 4 Figure 3: Our pipeline. The original instruction is first parsed by VLM into atomic operations, which are then sequentially executed via neural program interpreter."
        },
        {
            "title": "4.2 General Pipeline",
            "content": "We abstract all editing instructions into five atomic primitives: (1) RoI Localization: Identify and isolate the relevant region in the image that the instruction refers to, serving as the spatial grounding step for subsequent localized edits; (2) RoI Inpainting: Introduce new visual content or remove existing elements within the localized region, enabling semantic-level additions, substitutions, or deletions; (3) RoI Editing: Modify visual attributes within the region, such as color, texture, or appearance, to reflect fine-grained property changes specified by the instruction; (4) RoI Compositing: Reintegrate the edited region into the full image while preserving spatial coherence and visual continuity; (5) Global Transformation: Adjust the overall content for coherent full-image modifications, such as changing the illumination, weather, or style of the whole image. The overall pipeline is shown as Fig. 3. We reduce any editing instruction into an arbitrary combination of the five atomic operations described above, which can be formulated as: (cid:77) Ak, Ak {Aloc, Ainp, Aedit, Acomp, Aglobal} (3) k=1 where denotes the free-form editing instruction, (cid:76) represents the sequential program combination, is the number of atomic operations, Aloc, Ainp, Aedit, Acomp, and Aglobal represent the five atomic primitives respectively. RoI Localization. All problematic local semantic edits share common first step: localizing Region of Interest (RoI) in the image for editing. Given an image and an editing instruction , we first employ Large Language Model (LLM) [1] to locate the text RoI: ρ = MLLM(T ), (4) where ρ represents the text RoI extracted by the LLM MLLM. Subsequently, we achieve accurate localization of image RoI by: = Mseg(I, ρ), (5) where denotes the image RoI segmented by the segmentation model Mseg [71]. For add operation, the instruction may not specify text RoI, or the specification may be ambiguous. In such cases, we first derive the overall layout of all candidate objects using the capability of segmentation models [50, 71], and then prompt the LLM to determine the appropriate image RoI based on . Regarding move and resize, once the image RoI is obtained, we update the spatial layout of the image using an LLM [1]. Specifically, we provide the LLM with set of in-context examples that define our layout representation and demonstrate representative editing patterns [36]. Given the current layout and the instruction , the LLM is prompted to produce modified layout Ledit, as formulated below: Tags = MLLM(I), = Mseg(Tags), Ledit = MLLM(L, ). (6) 5 Figure 4: Example procedure. Figure (a) and Figure (b) illustrate the procedures of action change and movement respectively. We then derive the geometric differences between and Ledit and convert them into the corresponding affine transformations, consisting of translation, scaling, and reshaping, and apply it to to update the spatial configuration, yielding the transformed mask R. RoI Inpainting. Once the image RoI has been localized, we apply inpainting to seamlessly fill and complete the region. For additive and substitutive operations, which aim to introduce new objects, we employ prompt-conditioned inpainting process to guide the generation of new content. Specifically, we first extract the semantic entity from the instruction via an LLM [1]: = MLLM(T ), (7) and then construct composite prompt in the form: add on the black region. For removal operations, which aim to eliminate existing content without introducing new semantics, we adopt background-oriented infilling strategy, setting as fill in the hole of the image. The edited image Iedit is then generated by: Iedit = Minpaint (I (1 R), ) , (8) where Minpaint denotes the inpainting model trained by us. RoI Editing. When operations pertain to property change are performed, we use the trained attribute editing model Mattr to perform edits in this stage to obtain Iedit: Iedit = Mattr (I, ) . (9) RoI Compositing. To ensure seamless integration of the edited RoI with its surrounding context, we first construct an annular mask Mann by applying morphological dilation and erosion [51, 54] to the transformed RoI mask R: Mann = Dilate(R, k1) Erode(R, k2). (10) Then, we employ fusion network Mfusion, trained on ring-masked object boundaries, to refine the pre-composited image Iprep using the generated annular mask. The final edited image is obtained as: Iedit = Mfusion (Iprep (1 Mann), ) , (11) where is set as inpaint the black-bordered region so that the objects edges blend smoothly with the background to guide seamless boundary blending. Global Transformation. Like RoI editing, in the scenarios involving global transformation, we use the trained global transformation model Mglobal to perform edits in this final stage to obtain Iedit."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "Training Settings. We train four specialized models for RoI inpainting, RoI editing, RoI compositing, and global transformation respectively. All models are fine-tuned on FLUX.1-dev [31] using LoRA 6 Figure 5: Comparison results of ours with baseline methods on representative editing cases. Others exhibit poor performance even on some common editing operations, while our approach demonstrates superior effectiveness across all operations. [25], with default settings for rank 128 and alpha 128. Training is conducted with batch size of 1 and runs for 50,000 iterations each. We use the Prodigy optimizer [39], enabling safeguard warmup and bias correction, with weight decay of 0.01. The experiments are conducted on single NVIDIA H100 GPU (80GB). Dataset Setup. For both the RoI editing and global transformation models, we sample from the relevant subsets of the AnyEdit [70] dataset and apply GPT-4o [29] to filter the data of some types that have numerous noisy examples. To cover facial expression edits absent in AnyEdit, we integrate the CelebHQ-FM dataset [11], which offers consistent identities and annotated expressions suitable for our instruction schema. The RoI inpainting and RoI compositing models are trained on samples from the add, remove and replace splits of AnyEdit. For each sample, we first obtain the image RoI according to the editing instruction. In the RoI Inpainting training setup, we set the pixels within image RoI to black as input to train. For RoI Compositing, we set k1 and k2 as 3 in default to blackout the annular mask region of image RoI as input for training. Method MagicBrush test AnyEdit test CLIPim CLIPout L1 DINO CLIPim L1 DINO GPT InstructPix2Pix MagicBrush UltraEdit ICEdit Ours 0.838 0.886 0.911 0. 0.922 0.229 0.241 0.227 0.236 0.247 0.112 0.074 0.061 0.058 0.060 0.758 0.859 0.883 0. 0.897 0.801 0.824 0.833 0.847 0.882 0.110 0.128 0.114 0.110 0.096 0.765 0.742 0.772 0. 0.825 3.83 3.90 3.93 4.13 4.41 Table 1: Quantitative results on MagicBrush and AnyEdit test set. Method Local Semantic Editing Local Attribute Editing Overall Content Editing CLIPim L1 DINO GPT CLIPim L1 DINO GPT CLIPim L1 DINO GPT InstructP2P MagicBrush UltraEdit ICEdit 0.826 0.860 0.867 0.881 0.132 0.738 0.106 0.796 0.095 0.812 0.088 0. 3.74 3.90 3.86 4.08 0.790 0.809 0.801 0.825 0.135 0.737 0.117 0.762 0.092 0.793 0.095 0.795 3.92 4.21 3.94 4.06 0.766 0.763 0.754 0.759 0.156 0.642 0.187 0.616 0.201 0.611 0.188 0. 3.91 3.99 4.41 4.45 Ours 0.907 0.081 0.854 4.42 0. 0.083 0.821 4.54 0.895 0.107 0.879 4.51 Table 2: Quantitative results on different types of editing operations. Evaluation Settings. We evaluate our method on two benchmarks: MagicBrush test set [73], widely used dataset spanning diverse editing types, and AnyEdit test set [70], from which we select 16 instruction-based editing categories. For MagicBrush, we follow previous works [73, 78, 15, 55] and report CLIPimg, CLIPout [22], L1, and DINO [7, 43] scores to measure the similarity between the generated results and ground-truth images. While for AnyEdit, where some categories lack reference captions required for calculating CLIPout, we instead leverage GPT-4o [29] to assign ratings on scale from 1 to 5 across three aspects: instruction faithfulness, semantic consistency, and aesthetic quality. The final quality score is computed as the average of these three dimensions. We first compare our method with existing state-of-the-art open-source baselines, including InstructPix2Pix [6], MagicBrush [73], UltraEdit [78], and ICEdit [77]. In addition, to demonstrate the competitiveness of our approach against powerful proprietary multimodal foundation models in complex image editing scenarios, we further make comparisons with SeedEdit (Doubao) [56], Gemini 2.0 Flash [19], and GPT-4o [29]."
        },
        {
            "title": "5.2 Comparisons with State of the Art.",
            "content": "Qualitative Comparisons. Fig. 5 shows the results of ours against other four methods [6, 73, 78, 77] on some representative editing cases, where our method demonstrates comprehensive superiority over others in accurate instruction execution, structural consistency, and instance-level fidelity. Quantitative Comparisons. Table 1 exhibits the quantitative comparison results of our method and other approaches [6, 73, 78, 77] on MagicBrush test set [73] and AnyEdit test set [70]. The results show that our method demonstrates state-of-the-art performance on both datasets. On MagicBrush, our method achieves the best performance in terms of caption alignment, semantic consistency, and preservation of fine-grained structural details. Although it incurs marginal increase in pixel-level deviation compared to the best [77], this is far outweighed by the substantial gains in perceptual quality and semantic fidelity. Furthermore, on AnyEdit, our approach yields significant and comprehensive improvements across all evaluation metrics, further highlighting its superiority over existing techniques. To provide more fine-grained analysis of editing performance, we group subset of the instructionbased categories from the AnyEdit test set [70] into three macro-tasks: local semantic editing, local attribute editing and overall semantic editing. For local attribute editing, we augment with some CelebHQ-FM [11] test images to evaluate facial expression changes. The quantitave comparison results are shown in Tab. 2, where our method consistently outperforms other candidates across all three task categories and evaluation metrics. Comparisons with Cutting-Edge Multimodal Models. To demonstrate the superiority of our reduction strategy on complex editing tasks, we also conduct comparative experiments against prominent closed-source multimodal models [56, 19, 29]. As illustrated in Fig. 6, our method rivals, and in most cases surpasses the performance of these leading models on intricate scenarios requiring 8 Figure 6: Comparisons on Complex Instructions with Leading Multimodal Models. Our method achieves comparable or even better edit completeness and pre-post consistency. multiple sequential edits. Unlike competing approaches, which frequently omit specified instructions or introduce extraneous alterations unrelated to the editing directives, our framework faithfully executes each instruction while maintaining superior image consistency and instance preservation."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "Settings CLIPim CLIPout L1 DINO GPT w/o CoT & Reduction 0.873 0.241 0.117 0. 4.10 0.861 w/o RoI Inpainting 0.900 w/o RoI Editing 0.900 w/o Layout Reconfiguration w/o Annular Mask Integration 0.906 0.218 0.244 0.245 0.252 0.124 0.775 0.088 0.843 0.088 0.848 0.083 0.854 3.65 4.23 4.31 4.39 Full 0.907 0.252 0.081 0.854 4.42 Figure 7: Qualitative ablation of action change operation. Table 3: Ablation results on AnyEdit local semantic editing test set. Module-wise Ablation Studies. To quantify the impact of each key component in our framework, we perform series of ablation studies on the AnyEdit local semantic editing test set as we split in Sec. 5.2. As shown in Tab. 3, we first substitute our CoT reasoning and reduction pipeline with end-to-end editing pipeline, resulting in marked performance deterioration across all metrics. Next, we replace our specialized RoI inpainting and RoI editing models respectively with the generic inpainting model from [59], which induces performance declines of varying degrees. We then remove the LLM-guided layout reconfiguration and instead employing random layout modifications for relevant operations, which incurs noticeable performance decline. Finally, omitting the annular mask integration produces modest drop, underscoring its role in precise boundary delineation. Fig. 7 exhibits the ablation results on an example of action change, visually showcasing each modules necessity. Collectively, these ablation results confirm that each component in our pipeline contributes significantly in handling robust local semantic editing tasks requiring layout changes."
        },
        {
            "title": "6 Conclusions, Limitations and Future Work",
            "content": "In this paper, we propose Image Editing As Programs (IEAP), unified DiT-based framework for instruction-driven image editing. By defining five atomic operations and using CoT reasoning to convert instructions into sequential programs, IEAP processes the ability to handle both simple and complex edits. Experiments demonstrate that IEAP outperforms state-of-the-art methods in both structure-preserving and structure-altering tasks, especially for complex edits. Despite its strong overall performance, there are also some limitations. First, for complex shadow changes, our method sometimes leaves shadows inconsistent after compositing operations. Second, multiple editing iterations may induce progressive image quality decay. Future work could focus on addressing these issues via physics-aware shadow modeling and diffusion-based quality restoration."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, et al. Gpt-4 technical report, 2024. [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4):111, 2023. [3] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18370 18380, 2023. [4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. [5] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009. [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [8] Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, and Smaranda Muresan. Learning to follow objectcentric image editing instructions faithfully. arXiv preprint arXiv:2310.19145, 2023. [9] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. [11] Brian DeCann and Kirill Trapeznikov. Comprehensive dataset of face manipulations for development and evaluation of forensic tools, 2022. [12] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:1622216239, 2023. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [14] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 29692977, 2025. [15] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instructionbased image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. [16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages 89106. Springer, 2022. [17] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1270912720, 2024. [18] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi. Pair diffusion: comprehensive multimodal object-level image editor. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86098618, 2024. 10 [19] Google. Experiment with gemini 2.0 flash native image generation. Technical report, Google AI Studio, 2025. [20] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing by attention modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69866996, 2024. [21] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training, 2022. [22] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. [24] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [26] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. [27] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 127, 2025. [28] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83628371, 2024. [29] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, et al. Gpt-4o system card, 2024. [30] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [31] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [32] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318, 2024. [33] Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal instruction following. arXiv preprint arXiv:2312.06738, 2023. [34] Sijia Li, Chen Chen, and Haonan Lu. Moecontroller: Instruction-based arbitrary image manipulation with mixture-of-expert controllers. arXiv preprint arXiv:2309.04372, 2023. [35] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. [36] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models, 2024. [37] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [38] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling, 2025. [39] Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner. arXiv preprint arXiv:2306.06101, 2023. [40] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. 11 [41] Thanh Tam Nguyen, Zhao Ren, Trinh Pham, Thanh Trung Huynh, Phi Le Nguyen, Hongzhi Yin, and Quoc Viet Hung Nguyen. Instruction-guided editing controls for images and multimedia: survey in llm era. arXiv preprint arXiv:2411.09955, 2024. [42] Byong Mok Oh, Max Chen, Julie Dorsey, and Frédo Durand. Image-based modeling and photo editing. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 433442, 2001. [43] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [44] Zexu Pan, Zhaojie Luo, Jichen Yang, and Haizhou Li. Multi-modal attention for speech emotion recognition. arXiv preprint arXiv:2009.04107, 2020. [45] Rishubh Parihar, VS Sachidanand, Sabariswaran Mani, Tejan Karmali, and Venkatesh Babu. Precisecontrol: Enhancing text-to-image diffusion models with fine-grained attribute control. In European Conference on Computer Vision, pages 469487. Springer, 2024. [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [48] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. [49] Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015. [50] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [51] Jean-Francois Rivest, Pierre Soille, and Serge Beucher. Morphological gradients. Journal of Electronic Imaging, 2(4):326336, 1993. [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [54] Khairul Anuar Mat Said and Asral Bahari Jambek. Analysis of image processing using morphological erosion and dilation. In Journal of Physics: Conference Series, volume 2071, page 012033. IOP Publishing, 2021. [55] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [56] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. [57] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. [58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [59] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. 12 [60] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. [61] Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, and Chris Xiaoxuan Lu. Click to grasp: Zero-shot precise manipulation via visual diffusion descriptors. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1161011617. IEEE, 2024. [62] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [64] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [65] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. arXiv preprint arXiv:2412.17098, 2024. [66] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [67] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free diffusion: Taking\" text\" out of text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86828692, 2024. [68] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint: unified framework for multimodal image inpainting with pretrained diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 31903199, 2023. [69] Siwei Yang, Mude Hui, Bingchen Zhao, Yuyin Zhou, Nataniel Ruiz, and Cihang Xie. Complex-Edit: Cot-like instruction generation for complexity-controllable image editing benchmark, 2025. [70] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [71] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos, 2025. [72] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, and Yu Zhang. Nexus-gen: unified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356, 2025. [73] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [75] Xinyu Zhang, Mengxue Kang, Fei Wei, Shuang Xu, Yuhe Liu, and Lin Ma. Tie: Revolutionizing text-based image editing for complex-prompt following and high-fidelity editing, 2024. [76] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer, 2025. [77] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer, 2025. [78] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [79] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023."
        },
        {
            "title": "Technical Appendices and Supplementary Material",
            "content": "In this part, we provide additional algorithm illustration, implementation details, more comparison results, more visualization results, and more analysis and discussions of the proposed approach."
        },
        {
            "title": "A Algorithm Illustration",
            "content": "To better elaborate the details of the proposed IEAP, we provide an algorithmic illustration for the whole pipeline in Alg. 1. instr [i] Categories and instructions cat C[i], if cat {Add, Remove, Replace} then Algorithm 1 IEAP: Image Editing As Programs Input: I: input image path : original instruction {RoI_Localization, RoI_Inpainting, . . . , Global_Transformation}: editing primitives cot_with_gpt(): CoT prompt to GPT4o extract_instructions(): parse CoT output infer_with_DiT(op, ): invoke DiT for primitive op roi_localization(I, instr): returns mask for region of interest fusion(I1, I2): blends two intermediate outputs layout_change(I, instr): compute geometric transform Output: final edited image 1: uri encode_image_to_datauri(I) 2: (C, ) cot_with_gpt(uri, ) 3: (0) 4: for = 1 to do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for 29: return (C) roi_localization(I (i1), instr) Ibg infer_with_DiT(RoI Inpainting, M, instr) Iact infer_with_DiT(RoI Editing, (i1), instr) (i) infer_with_DiT(RoI Compositing, fusion(Ibg, Iact), instr) roi_localization(I (i1), instr) Ibg infer_with_DiT(RoI Inpainting, M, instr) Ilc layout_change(I (i1), instr) (i) infer_with_DiT(RoI Compositing, fusion(Ibg, Ilc), instr) else if cat {Appearance Change, Background Change, Color Change, Material Change, Expression Change} then (i) infer_with_DiT(RoI Editing, (i1), instr) roi_localization(I (i1), instr) infer_with_DiT(RoI Inpainting, M, instr) (i) (i) infer_with_DiT(Global Transformation, (i1), instr) else if cat {Tone Transfer, Style Change} then raise ValueError(Invalid category: cat) else if cat = Action Change then else if cat {Move, Resize} then end if else"
        },
        {
            "title": "B Implementation Details",
            "content": "In this section, we present the prompts employed to leverage VLM for CoT reasoning over complex instructions, providing further details on the layout-adjustment prompts. Below are the detailed prompts used to invoke the VLM for the CoT process on complex instructions: Now you are an expert in image editing. Based on the given single image, what atomic image editing instructions should be if the user wants to {instruction}? Lets think step by step. Atomic instructions include 13 categories as follows: - Add: Introduce new object, person, or element into the image, e.g.: add car on the road - Remove: Eliminate an existing object or element from the image, e.g.: remove the sofa in the image - Color Change: Modify the color of specific object, e.g.: change the color of the shoes to blue - Material Change: Alter the surface material or texture of an object, e.g.: change the material of the sign like stone - Action Change: Modify the pose or action of an instance, e.g.: change the action of the boy to raising hands - Expression Change: Adjust the facial expression, e.g.: change the expression to smiling - Replace: Substitute one object in the image with different object, e.g.: replace the coffee with an apple - Background Change: Change the background scene to another, e.g.: change the background into forest - Appearance Change: Modify visual attributes such as patterns or accessories, e.g.: make the cup have floral pattern - Move: Change the spatial position of an object within the image, e.g.: move the plane to the left - Resize: Adjust the scale or size of an object, e.g.: enlarge the clock - Tone Transfer: Change the global atmosphere or lighting conditions, e.g.: change the weather to foggy, change the time to spring - Style Change: Modify the entire image to adopt different visual style, e.g.: make the style of the image to cartoon Respond *only* with numbered list. Each line must begin with the category in square brackets, then the instruction. Please strictly follow the atomic categories. The operation (what) and the target (to what) are crystal clear. Do not split replace to add and remove. Always place [Tone Transfer] and [Style Change] instructions at the end of the list. For example: 1. [Add] add car on the road 2. [Color Change] change the color of the shoes to blue 3. [Move] move the lamp to the left Do not include any extra text, explanations, JSON or markdown, just the list. Below are the detailed prompts used to adjust the layout of move and resize operations: You are an intelligent bounding box editor. will provide you with the current bounding boxes and the editing instruction. Your task is to generate the new bounding boxes after editing. Lets think step by step. The images are of size 512x512. The top-left corner has coordinate [0, 0]. The bottom-right corner has coordinnate [512, 512]. The bounding boxes should not overlap or go beyond the image boundaries. Each bounding box should be in the format of (object name, [top-left coordinate, top-left coordinate, bottom-right coordinate, bottom-right coordinate]). Do not add new objects or delete any object provided in the bounding boxes. Do not change the size or the shape of any object unless the instruction requires so. Please consider the semantic information of the layout. When resizing, keep the bottom-left corner fixed by default. When swaping locations, change according to the center point. If needed, you can make reasonable guesses. Please refer to the examples below: Input bounding boxes: [(\"bed\", [50, 300, 450, 450]), (\"pillow\", [200, 200, 300, 230])] Editing instruction: Move the pillow to the left side of the bed. Output bounding boxes: [(\"bed\", [50, 300, 450, 450]), (\"pillow\", [70, 270, 170, 300])] Editing instruction: Input bounding boxes: [(a car, [21, 281, 232, 440])] Editing instruction: Move the car to the right. Output bounding boxes: [(a car, [121, 281, 332, 440])] Input bounding boxes: [(\"dog\", [150, 250, 250, 300])] Editing instruction: Enlarge the dog. Output bounding boxes: [(\"dog\", [150, 225, 300, 300])] Input bounding boxes: [(\"chair\", [100, 350, 200, 450]), (\"lamp\", [300, 200, 360, 300])] Editing instruction: Swap the location of the chair and the lamp. Output bounding boxes: [(\"chair\", [280, 200, 380, 300]), (\"lamp\", [120, 350, 180, 450])] Now, the current bounding boxes is {bbox}, the instruction is {instruction}. Below are the detailed prompts used to adjust the layout of add operations: You are an intelligent bounding box editor. will provide you with the current bounding boxes and an add editing instruction. Your task is to determine the new bounding box of the added object. Lets think step by step. The images are of size 512x512. The top-left corner has coordinate [0, 0]. The bottom-right corner has coordinnate [512, 512]. The bounding boxes should not go beyond the image boundaries. The new box must be at least as large as needed to encompass the object. Each bounding box should be in the format of (object name, [top-left coordinate, top-left coordinate, bottom-right coordinate, bottom-right coordinate]). Do not delete any object provided in the bounding boxes. Please consider the semantic information of the layout, preserve semantic relations. If needed, you can make reasonable guesses. Please refer to the examples below: Input bounding boxes: [(a green car, [21, 281, 232, 440])] Editing instruction: Add bird on the green car. Output bounding boxes: [(a bird, [80, 150, 180, 281])] Input bounding boxes: [(stool, [300, 350, 380, 450])] Editing instruction: Add cat to the left of the stool. Output bounding boxes: [(a cat, [180, 250, 300, 450])] Here are some examples to illustrate appropriate overlapping for better visual effects: Input bounding boxes: [(the white cat, [200, 300, 320, 420])] Editing instruction: Add hat on the white cat. Output bounding boxes: [(a hat, [200, 150, 320, 330])] Now, the current bounding boxes is {bbox}, the instruction is {instruction}."
        },
        {
            "title": "C More Quantitative Results",
            "content": "Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.847 0.889 0.897 0.925 0. 0.264 0.277 0.274 0.277 0.278 0.092 0.068 0.056 0.057 0.829 0.892 0.909 0.915 0.056 0. 4.50 4.66 3.36 4.60 4.68 4.40 4.76 4.24 4.80 4.84 4.26 4.62 4.22 4.76 4. 4.39 4.68 3.94 4.72 4.71 Table 4: Quantitative comparison results on AnyEdit Add test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.800 0.853 0.846 0.895 0.916 0.202 0.211 0.211 0.212 0.230 0.108 0.083 0.066 0. 0.721 0.800 0.802 0.875 0.057 0.886 2.74 3.08 2.50 4.06 4.18 3.42 3.60 3.54 4. 3.88 3.20 3.18 3.44 4.32 3.66 3.12 3.29 3.16 4.29 3.91 Table 5: Quantitative comparison results on AnyEdit Remove test set. 16 Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.766 0.806 0.779 0. 0.866 0.234 0.248 0.242 0.228 0.252 0.179 0.148 0.142 0.128 0.588 0.671 0.621 0.614 0. 0.701 3.72 4.52 3.80 3.68 4.68 3.68 4.48 4.40 4.02 4.68 3.80 4.38 4.40 4. 4.48 3.73 4.46 4.20 3.91 4.61 Table 6: Quantitative comparison results on AnyEdit Replace test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.829 0.831 0.847 0.827 0.848 0.254 0.266 0.259 0.255 0. 0.164 0.156 0.157 0.152 0.774 0.784 0.781 0.745 0.154 0.798 3.46 2.96 2.92 2.68 4. 3.84 4.28 4.22 4.04 4.86 3.58 4.28 4.24 4.04 4.68 3.63 3.84 3.79 3.59 4. Table 7: Quantitative comparison results on AnyEdit Action Change test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.881 0.902 0.923 0. 0.963 0.219 0.219 0.211 0.213 0.223 0.127 0.088 0.074 0.063 0.771 0.828 0.867 0.868 0. 0.903 3.82 2.94 3.48 3.28 3.88 4.44 3.94 4.40 4.64 4.44 4.36 3.90 4.40 4. 4.38 4.21 3.59 4.09 4.07 4.23 Table 8: Quantitative comparison results on AnyEdit Relation test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.831 0.875 0.908 0.895 0.923 0.241 0.258 0.262 0.253 0. 0.124 0.094 0.073 0.074 0.746 0.802 0.889 0.841 0.066 0.921 2.94 2.80 3.22 3.14 4. 3.56 3.88 4.38 4.28 4.32 3.62 4.00 4.38 4.26 4.28 3.37 3.56 4.00 3.89 4. Table 9: Quantitative comparison results on AnyEdit Resize test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.815 0.852 0.857 0. 0.886 0.280 0.294 0.277 0.273 0.285 0.139 0.094 0.068 0.085 0.744 0.815 0.845 0.808 0. 0.833 3.60 3.96 4.04 4.04 4.06 4.08 4.32 4.62 4.42 4.72 3.92 3.98 4.42 4. 4.80 3.87 4.09 4.36 4.21 4.53 Table 10: Quantitative comparison results on AnyEdit Appearance test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.725 0.746 0.796 0.799 0.801 0.224 0.230 0.257 0.241 0. 0.216 0.228 0.169 0.166 0.582 0.567 0.747 0.757 0.165 0.759 3.40 4.58 3.48 3.04 4. 3.60 4.38 4.36 4.16 4.68 3.44 4.46 3.14 3.88 4.70 3.48 4.47 3.66 3.69 4. Table 11: Quantitative comparison results on AnyEdit Background Change test set. 17 Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.886 0.898 0.890 0.896 0.911 0.279 0.282 0.280 0.278 0.276 0.120 0.087 0.065 0.073 0.876 0.869 0.87 0. 0.059 0.876 3.60 4.20 3.80 4.72 4.62 4.40 4.82 4.40 4.80 4. 4.00 4.62 4.20 4.64 4.78 4.00 4.55 4.13 4.72 4.71 Table 12: Quantitative comparison results on AnyEdit Color Change test set. Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.776 0.770 0.699 0.796 0.882 0.068 0.064 0.073 0. 0.052 0.936 0.940 0.907 0.943 0.945 3.74 3.86 3.14 3.16 4.34 4.60 4.48 4.10 4. 4.72 4.30 4.18 3.80 4.30 4.50 4.21 4.17 3.68 4.02 4.52 Table 13: Quantitative comparison results on Expression test set. Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.746 0.778 0.765 0.787 0. 0.130 0.110 0.086 0.086 0.055 0.549 0.621 0.598 0.616 0.696 4.00 3.36 3.34 3.48 4. 4.18 4.06 4.28 3.92 4.48 4.04 3.84 4.04 3.58 4.18 4.07 3.75 3.89 3.66 4. Table 14: Quantitative comparison results on Material Change test set. Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.710 0.692 0.703 0. 0.922 0.212 0.214 0.201 0.219 0.097 0.463 0.440 0.467 0.458 0.915 3.56 3.12 4.02 4. 4.44 4.32 4.64 4.8 4.82 4.64 3.94 4.00 4.62 4.36 4.44 3.94 3.92 4.48 4. 4.51 Table 15: Quantitative comparison results on AnyEdit Style Change test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.822 0.834 0.804 0.812 0.868 0.260 0.266 0.268 0.260 0.268 0.100 0.159 0.201 0.157 0.821 0.791 0.767 0. 0.116 0.843 3.72 3.56 4.12 4.06 4.44 4.48 4.64 4.62 4.88 4. 3.92 3.98 4.26 4.56 4.44 4.04 4.06 4.33 4.50 4.51 Table 16: Quantitative comparison results on AnyEdit Tone Transfer test set. Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.815 0.835 0.833 0.906 0.908 0.134 0.081 0.066 0. 0.056 0.647 0.697 0.756 0.842 0.794 3.40 1.82 2.58 2.98 3.42 4.04 3.56 4.02 4. 4.48 4.80 3.50 4.02 3.40 4.46 4.08 2.96 3.54 3.59 4.12 Table 17: Quantitative comparison results on AnyEdit Counting test set. 18 Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.773 0.806 0.825 0. 0.833 0.208 0.174 0.167 0.171 0.169 0.581 0.631 0.669 0.629 0.662 3.46 2.98 2.82 3. 3.88 4.18 3.88 4.38 4.16 4.44 4.08 4.04 4.38 4.06 4.52 3.91 3.63 3.86 3. 4.28 Table 18: Quantitative comparison results on AnyEdit Implicit Change test set. Method CLIPim L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.887 0.900 0.922 0.898 0.938 0.111 0.100 0.077 0.079 0.084 0.858 0.874 0.911 0.864 0. 4.30 4.12 3.24 4.16 4.18 4.50 4.36 4.4 4.46 4.56 4.30 4.54 4.36 4.20 4. 4.37 4.34 4.00 4.27 4.37 Table 19: Quantitative comparison results on AnyEdit Move test set. Method CLIPim CLIPout L1 DINO GPTIF GPTF GPTAQ GPTavg InstructPix2Pix MagicBrush UltraEdit ICEdit IEAP(Ours) 0.688 0.680 0.732 0.810 0.788 0.243 0.255 0.279 0.289 0.285 0.189 0.156 0.147 0. 0.742 0.786 0.843 0.811 0.162 0.786 1.04 1.02 1.96 4.18 3.96 4.38 4.48 4.46 4. 4.58 3.92 4.10 3.98 4.68 4.06 3.11 3.20 3.47 4.43 4.20 Table 20: Quantitative comparison results on AnyEdit Textual Change test set."
        },
        {
            "title": "D More Visualization Results",
            "content": "In this section, we provide more visualization results, as shown below: Figure 8: More Visualization Results. 19 Figure 9: More Visualization Results. 20 Figure 10: More Visualization Results. Figure 11: More Detailed Visualization Processes of the pipeline."
        },
        {
            "title": "E Analysis and Discussions",
            "content": "E.1 Runtime Performance Analysis We evaluate the time required for each atomic operation of IEAP on single NVIDIA H100 GPU. Empirical measurements indicate that the RoI Localization stage requires approximately 3 to 5 per operation. Other editing primitives, including RoI Inpainting, RoI Editing, RoI Compositing, and Global Transformation, each consumes roughly 7 to 9 per operation. Consequently, complete multi-step edit involving atomic operations exhibits total latency of Ttotal = (cid:88) i= Ti with Ti = (cid:26)3 to 5 s, if operationi = RoI Localization, 7 to 9 s, otherwise. While this per-operation cost precludes real-time interactivity, it remains acceptable for batch-oriented workflows in digital content creation, scientific visualization, and other offline editing scenarios. E.2 Limitations and Future Work Limitations. Despite its strengths, IEAP exhibits several limitations in handling dynamic scenes and complex physical interactions. First, the RoI compositing may introduce geometric distortions or texture discontinuities when editing highly dynamic or non-rigid content, such as motion-blurred instances, and fluid or smoke effects. For example, in the task of changing the cats action to jumping, in Fig. 6, the rapid motion of fur can produce blurred regions that fail to blend naturally with the background. Second, RoI compositing struggles to simulate physically consistent lighting effects in scenes with reflective or refractive surfaces, sometimes resulting in mismatched shadow directions and illumination conflicts between edited objects and their environments. For example, in the task of change the action of the woman to dancing, in Fig. 4, the shadows before and after editing remain the same, but the action of the woman has changed, so it is unnatural. Third, the DiT-based architecture and multi-stage atomic operations incur substantial inference latency for 5 to 9 per operation on single H100 GPU, precluding real-time interactivity in applications such as AR/VR. Finally, the requirement for high-memory GPUs like NVIDIA H100 (80 GB) limits reproducibility for resource-constrained researchers, and multi-iteration editing can exacerbate image quality degradation over successive operations. Future Work. As for future work, several avenues may be pursued to overcome the identified limitations. To begin with, physics-aware compositing techniques and motion-compensated inpainting could be explored to better accommodate dynamic blur and fluid effects, thereby ensuring seamless integration of non-rigid edits. Meanwhile, differentiable lighting models or neural rendering modules may be incorporated to enforce global illumination consistency, particularly in reflective and refractive contexts. On the performance front, model distillation, operation fusion, and sparse attention strategies could be investigated to reduce per-operation latency and facilitate interactive editing. To enhance accessibility, memory optimization and support for smaller-footprint architectures amenable to commodity GPUs may be implemented. Moreover, iterative refinement and error-correction mechanisms may be developed to mitigate quality degradation over successive editing steps. Furthermore, beyond still-image editing, an extension to video-based complex instruction editing could be considered, where temporal coherence and motion consistency present additional challenges and opportunities for dynamic, multi-step visual manipulation. E.3 Societal Impacts and Ethical Safeguards Positive Societal Impacts. The proposed IEAP framework introduces modular and interpretable approach to complex image editing, which holds significant potential to benefit range of creative and technical domains. By decomposing high-level visual instructions into atomic operations, IEAP enables users to perform multi-step edits with enhanced precision and control. This capability is particularly valuable in digital content creation, advertising, and education, where fine-grained manipulation of visual content is often required. For example, IEAPs ability to support structurally inconsistent modifications can streamline visual storytelling workflows or facilitate the generation of accurate scientific visualizations for publications and teaching materials. Furthermore, its potential extensions to fields such as medical imaging by enabling localized enhancement of diagnostic visuals, 22 and accessibility technology by generating descriptive visual representations for users with visual impairments, demonstrate the frameworks broader societal utility and interdisciplinary relevance. Negative Societal Impacts and Ethical Safeguards. Despite its benefits, IEAPs high-fidelity editing capabilities also introduce ethical risks, particularly in the domains of misinformation and privacy. The frameworks precision in altering visual content could be misused for the creation of deepfakes or manipulated images intended for disinformation, identity falsification, or reputational harm. Operations such as Remove or Replace could be exploited to tamper with sensitive or private imagery, potentially infringing on individual rights. To address these concerns, the development and deployment of IEAP adhere to strict ethical standards. Specifically, safeguards include the implementation of data filtering pipelines, such as the use of GPT-4o-filtered subsets of AnyEdit and the compliance-oriented CelebHQ-FM dataset, to reduce harmful biases and content. Additionally, the modular nature of IEAP facilitates transparency and traceability in the editing process, supporting future content provenance systems designed to detect and flag manipulated media. All these safeguards jointly contribute to ongoing efforts in AI safety and accountability."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}