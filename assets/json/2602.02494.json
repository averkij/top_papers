{
    "paper_title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
    "authors": [
        "Dulhan Jayalath",
        "Oiwi Parker Jones"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL ."
        },
        {
            "title": "Start",
            "content": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Dulhan Jayalath Oiwi Parker Jones PNPL , University of Oxford {dulhan,oiwi}@robots.ox.ac.uk 6 2 0 2 2 ] . [ 1 4 9 4 2 0 . 2 0 6 2 : r Figure 1. MEG-XL introduces long-context MEG pre-training. When fine-tuned, this approach generalises to decoding words in brain-to-text with less labelled subject data than required by the supervised state-of-the-art (SOTA) and brain foundation models (FMs). Abstract Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves dataefficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pretrain with only few seconds of context. Thus, we propose MEG-XL, model pre-trained with 2.5 minutes of MEG context per sample, 5-300 longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Finetuning on the task of word decoding from brain data, MEG-XL matches supervised performance with fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/ neural-processing-lab/MEG-XL. 1. Introduction Across modalities in deep learning, extending context has unlocked capabilities that short contexts could not provide. For example, task performance has improved by pre-training with longer, un-fragmented documents in language models (Dai et al., 2019; Beltagy et al., 2020), and by including more task examples in context at inference time (Brown et al., 2020). In audio modelling, dilated convolutions have extended the receptive field, capturing long-range structure that local windows miss (van den Oord et al., 2016). In video generation, models have struggled with temporal coherence when context is too short to maintain consistency across frames (Yan et al., 2023). The general pattern is that if signal carries structure at timescale , then models limited to context shorter than cannot exploit it. We postulate that neural recordings during speech perception If the brain encodes speech-relevant are no different. information across extended timescales, especially if this content mirrors linguistic structure, then models with access to extended neural windows should recover information unavailable to short-context approaches. Notwithstanding these predictions, prior brain-to-text decoders typically operate on brief samples of brain data in the range of milliseconds to seconds (Moses et al., 2021; Defossez et al., 2022; Tang et al., 2023; Willett et al., 2023; Card et al., 2024). This falls far short of the scale of linguistic context reflected by modern LLMs. While some neural 1 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training decoding systems independently incorporate longer linguistic context via external language models (Moses et al., 2021; Willett et al., 2023), they nonetheless ignore the long-range neural context present in the brain data itself. Short context windows are chosen to broadly match the timescale of the unit being decoded (a phoneme, syllable, or word). This framing implicitly assumes that the relevant neural information is local: that decoding the current moment requires only the current signal. But brain recordings carry structure beyond the immediate stimulus. Subject-specific neural patterns, noise characteristics, scanner properties, and other factors can all shape the signal. In speech, phonemes group into syllables, syllables into words, and words into phrases, sentences, and discourses. Just as words in sentence are interdependent, their corresponding neural representations are likely correlated. However, short contexts are too brief to capture and leverage these long-range dependencies. Extending neural context has already shown promise in noninvasive decoding, where sensors sit outside the skull. While non-invasive approaches enable safe and scalable data collection, they have lower fidelity than surgical implantsa gap that longer context windows may help close. dAscoli et al. (2025) decoded words by modelling MEG signals at the sentence level, providing neural responses to all words in perceived sentence to transformer at once. Their work demonstrated that additional neural context could improve word decoding accuracy by 50% over isolated word classification. Despite this promising advance, reaching reasonable accuracy still requires tens of hours of subject training data, with their analyses showing hours-per-subject matters more than total hours. Future clinical applications of speech brain-computer interfaces (BCIs) will need to work for patients with minimal labelled data because paralysed patients, who are the intended users, may not be able to easily provide training recordings (Willett et al., 2023; Card et al., 2024). Pre-trained models should improve data efficiency by learning transferable representations across subjects, reducing the need for subject-specific training data. Yet existing methods (a) seem to only perform well in data-rich settings (Anonymous, 2025) and (b) rarely exploit long neural context because they are designed for tasks that seemingly require only short context. While Jayalath et al. (2025) represents the only non-invasive pre-trained model tested on speech decoding, this model uses sample windows of just 0.5s as it was intended for simple speech sub-tasks where the stimulusor the relevant neural responsedoes not span much longer than half of second. Outside of speech, typical evaluation datasets include TUAB and TUEB (Obeid & Picone, 2016) where samples are often split to only 5 or 10 seconds long. As result, foundation models like LaBraM (Jiang et al., 2024), BioCodec (Avramidis et al., 2025), and EEGPT (Wang et al., 2024) are all pre-trained with short samples of at most 10 seconds. Leveraging long contexts is also limited by computational resources. BrainOmni (Xiao et al., 2025) and CBraMod (Wang et al., 2025) are the only non-specialised models to be trained with relatively long input contexts (up to 30s) avoiding the computational barrier of quadratic attention by factorising attention in time and sensors. Thus, Overcoming these limitations presents formidable challenge. to improve the data efficiency of brain-to-text while maintaining the advantages of extended neural context, we introduce MEG-XL, long-context pre-training framework and model. The name pays homage to Transformer-XL (Dai et al., 2019), one of the first long-context transformer language models and where XL stands for extra long. Accordingly, our method learns to model 2.5-minute long MEG samples, aiming to elicit both the advantages of improved decoding accuracy with long neural context, as shown by dAscoli et al. (2025), and the data efficiency afforded by long-context priors acquired through pre-training. These samples are 5-300 longer than prior pre-trained models, and correspond to 191k tokens1. This is similar to contexts in contemporary language models, e.g. GPT-4 Turbo (OpenAI, 2023), though the notion of token differs across domains. Using masked token prediction, MEG-XL learns from hundreds of subjects and hundreds of hours of recordings across rest, motor, speech, and other tasks. Like Xiao et al. (2025), it also overcomes the computational barrier of long-context modelling with memory-efficient criss-cross attention (Wang et al., 2025). Fine-tuned on contextual word decoding across three MEG speech datasets, the model generalises to new subjects with limited data better than the state-of-the-art supervised approach (dAscoli et al., 2025). Therefore, pre-training successfully compensates for limited subject-specific data; with extensive deep single-subject recordings, supervised models can eventually exploit rich in-domain patterns. Compared to brain foundation models, MEG-XL outperforms all alternatives in the shallow low-data regime, where models have access to limited subject data, while remaining competitive when given deep subject recordings. Performance may scale with the context length used for pre-training. With linear probing, we find that models pre-trained with longer neural contexts exhibit better representations for brain-to-text decoding. We also find evidence that long neural contexts may be beneficial beyond brain-to-text. Zero-shot prediction of masked brain signals from unseen datasets improves with models trained on longer neural context. Analysing attention patterns, we find this benefit stems from learning selective, hierarchical attention, with local processing in early layers expanding to global integration in later layers. 1Number of embeddings input to MEG-XL per sample, calculated via channels timesteps tokenizer compression ratio. 2 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Our main contributions are as follows: Long-Context: we scale neural pre-training to the minute-regime with contexts 5300 longer than prior work. MEG-XL models 2.5-minute MEG samples, allowing it to learn long-range dependencies in brain activity that short-window approaches cannot. Data-Efficient Transfer: we address clinical crosssubject transfer by generalising with less data. MEG-XL significantly outperforms state-of-the-art supervised methods and comprehensive set of modern foundation models when subject-specific data is scarce. Interpretation: the capacity to exploit long-range structure is learned skill that improves with scale. We provide empirical evidence that increasing pretraining context improves (a) brain-to-text and (b) zeroshot prediction of unseen data. This improvement stems from the emergence of selective, hierarchical attention patterns that short-context models never learn. 2. Method We introduce MEG-XL, self-supervised framework for learning representations from long-context MEG recordings (Figure 2). We adopt masked token prediction as our pre-training objective because it hides portions of the MEG signal and the model learns to reconstruct them from surrounding context. This directly trains the model to leverage extended temporal windows since predicting masked segments requires extracting information from whatever remains visible. Masked token prediction is also well-tested approach, with success across many domains (Devlin et al., 2019; Baevski et al., 2020; He et al., 2022). As MEG signals are highly autocorrelated in time, our objective masks long blocks of tokens to avoid trivially interpolating from neighbouring time points and encourage learning non-trivial, longer range relationships from neural data. To tokenize brain signals to serve as targets for this objective, our approach draws inspiration from generative audio models, particularly SoundStorm (Borsos et al., 2023), which use residual vector quantization (RVQ) in the tokenizer because residual tokens capture high-frequency time-series data with better fidelity. However, unlike brain models, audio models typically operate on single-channel waveforms while MEG recordings are multivariate, with hundreds of spatially distributed sensors. Thus, our framework adapts this method with sensor embeddings which have become standard in neural foundation models, e.g. Xiao et al. (2025). Importantly, long neural contexts are tokenized into long spans of tokens. To efficiently model so many tokens without the computational bottleneck of quadratic selfattention (Vaswani et al., 2017), we use criss-cross transformer (Wang et al., 2025), which provides an efficient factorisation of the transformer attention mechanism across time and channels. 2.1. Tokenizer Raw MEG signals are first converted to discrete tokens using frozen neural tokenizer. Given multi-channel MEG recording RCT with sensors and time samples, we apply BioCodec (Avramidis et al., 2025) independently to each channel. This tokenizer uses RVQ (Zeghidour et al., 2022; Defossez et al., 2023) with = 6 levels and vocabulary size = 256, to produce codes {0, . . . , 1}CT where = /r and = 12 is the temporal downsampling factor of the tokenizer. RVQ captures both slow dynamics and high frequency patterns through hierarchical decomposition, allowing the codebook to have more capacity without exponential vocabulary growth (Zeghidour et al., 2022; Defossez et al., 2023; Avramidis et al., 2025). Unlike most other work, which resample MEG recordings to 200-300Hz, we downsample to 50Hz because (a) this follows dAscoli et al. (2025) who introduced the word decoding task that we are interested in, and (b) it significantly reduces the number of tokens per sample, alleviating the memory bottleneck of modelling long contexts with attention. Although BioCodec was trained on EEG rather than MEG, and at 250Hz instead of 50Hz, we found it could reconstruct our data well (Appendix E). Alternative approaches, such as BrainTokenizer (Xiao et al., 2025), compress across channels in addition to time; this degraded reconstruction quality and therefore risked discarding more task-relevant information. We note that the tokenization serves two purposes: it compresses the input by factor of r, enabling the transformer to attend over longer temporal contexts, and it provides natural prediction target for self-supervised learning with masked token prediction. 2.2. Model Architecture Input embeddings. For each sensor and timestep , we construct an embedding by (1) looking up the frozen codebook vectors at each RVQ level {1, . . . , Q}, (2) concatenating these vectors, and (3) projecting to the model dimension dmodel: h(0) c,t = Wproj (cid:104) e(1) zc,1,t ; . . . ; e(Q) zc,Q,t (cid:105) (1) where e(q) codebook and [; ] denotes concatenation. Rdcodebook denotes the k-th entry of the q-th Sensor embeddings. MEG sensors vary in position, orientation, and type across recording systems. Following Xiao et al. (2025), we incorporate this structure through three 3 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 2. Overview of the MEG-XL pre-training framework. (Left) frozen BioCodec tokenizer independently encodes each MEG channel into discrete tokens across Q=6 residual quantization levels, providing prediction targets for self-supervised learning. (Middle) Token embeddings, which are concatenated across quantization levels and projected, are combined with sensor position, orientation, and type embeddings, then processed by criss-cross transformer. projection head maps transformer embeddings back to residual tokens. (Right) In pre-training, we randomly mask contiguous 3-second blocks uniformly across all sensors, forcing the model to predict masked tokens from temporal context rather than interpolating across channels, until 40% of tokens are masked. additive embeddings. Position and orientation are encoded via Gaussian Fourier features (Tancik et al., 2020): γ(v) = [cos(2πBv), sin(2πBv)] (2) where Rdfourier/23 contains frequencies sampled from (0, σ2). Separate Fourier embeddings encode sensor position pc R3 and orientation oc R3. Sensor type (gradiometer or magnetometer) is encoded via learned embedding. All three are projected to the transformer dimension dmodel and summed with the token embedding. Architecture. The transformer consists of = 8 layers and each transformer block follows pre-layer normalisation architecture: RMSNorm (Zhang & Sennrich, 2019) precedes the criss-cross attention and feedforward sub-layers, with residual connections applied after each. The feedforward network uses 4 hidden expansion with SELU activation (Klambauer et al., 2017). The criss-cross attention modules (Wang et al., 2025; Xiao et al., 2025) split the feature dimension in half and apply temporal and spatial attention in parallel to the respective halves: H(ℓ) = H(ℓ1) + (cid:104) SpatialAttn(H(ℓ1) :dmodel/2); TemporalAttn(H(ℓ1) dmodel/2:) (cid:105) (3) where [; ] denotes concatenation along the feature dimension. Temporal attention operates independently per sensor across the time axis, while spatial attention operates independently per timestep across sensors. Rotary position embeddings (Su et al., 2024) encode temporal position within temporal attention blocks. The factorization to temporal and spatial attention reduces the quadratic cost of full attention from O((CT )2) to O(C 2 +T 2). We find this essential for modelling long context neural data, especially as our tokenizer does not compress the sensor channel dimension. 2.3. Masked Prediction Objective We train the model to predict masked tokens from surrounding context in 2.5-minute long samples. We use temporal block masking, masking randomly selected 3-second blocks until we mask 40% of the sequence, where the masking percentage was determined through manual tuning. Interestingly, this is below the 75% used on images in masked autoencoders (He et al., 2022), far above the 15% used by BERT for language modelling (Devlin et al., 2019), but close to the 49% in wav2vec 2.0 (Baevski et al., 2020), self-supervised auditory speech model. We use large masking blocks of 3s, rather than shorter periods, because it forces the network to model neural patterns across long periods of time. It also covers the length of decodable neural responses to words (Kutas & Federmeier, 2011; Fyshe et al., 2019). The mask is applied uniformly across all sensors at the selected timesteps, preventing the model from only interpolating across simultaneous sensor readings. Masked tokens are replaced with learnable mask embedding. Let {1, . . . , } denote the set of masked timesteps. For each masked position, the model predicts the discrete code at each RVQ level via linear head: p(zc,q,t XM) = softmax(Wqh(L) c,t ) (4) The training objective is the average cross-entropy across 4 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Table 1. Comparison to Foundation Models. We end-to-end fine-tune all models for word decoding, with individual details on fine-tuning provided in Appendix A.3. The quoted uncertainty is the standard error of the mean over three seeds. Best results are indicated in bold and second-best in bold-italic. Results with indicate the best result beats the second-best with < .05 (with Welchs t-test). With 13% of training data With 100% of training data"
        },
        {
            "title": "Model",
            "content": "Params. MEG-MASC"
        },
        {
            "title": "Armeni",
            "content": "LibriBrain MEG-MASC"
        },
        {
            "title": "LibriBrain",
            "content": "BioCodec EEGPT BIOT BBL BrainOmni LaBraM MEG-XL (Ours) 1.0M 4.7M 3.2M 15M 8.4M 5.8M 20M 19.8 0.3 19.6 0.5 20.0 2.2 21.5 0.5 18.7 0.7 33.2 1.0 47.0 0.9 20.0 0.2 20.3 0.1 20.2 0.2 22.3 0.4 21.0 1.2 26.3 2.4 54.9 0.5 19.9 0.1 20.3 0.4 20.6 0.3 32.1 1.2 29.7 9.3 40.3 0.1 57.3 0.4 31.2 2.1 26.3 2.2 31.3 1.7 35.9 1.2 19.1 2.8 31.1 1.0 46.4 1.3 37.1 0.8 20.8 0.2 35.7 4.4 39.1 0.2 62.3 0.8 42.0 0.5 61.2 0.4 41.9 1.0 22.9 0.3 45.6 0.4 49.9 0.3 63.0 0.1 47.7 0.3 63.0 0.4 masked positions and RVQ levels: = 1 (cid:88) (cid:88) (cid:88) tM c=1 q=1 log p(zc,t,q XM) (5) We note that different MEG systems have different sensor counts. We pad recordings to maximum channel count and we use sensor mask to exclude padded channels from both attention computations and the loss. This enables training on heterogeneous datasets without architectural changes. 3. Experiments Our experiments evaluate our pre-trained approach on word decoding by fine-tuning it end-to-end on speech-task MEG datasets. We compare it to the state-of-the-art supervised method in contextual word decoding (dAscoli et al., 2025) as well as several recent brain foundation models across data regimes. We then investigate generalisation when models are pre-trained with increasing neural context. Pre-training data. We pre-train on approximately 300 hours of MEG data compiled from the CamCAN (Shafto et al., 2014), MOUS (Schoffelen et al., 2019), and SMN4Lang (Wang et al., 2022) datasets (see Appendix A.2) Contextual word decoding task. We follow dAscoli et al. (2025)s word-locked epoch decoding strategy. Given sequence of = 50 words, we construct the input by extracting 3-second neural windows Xn, aligned 0.5s before word onset, and concatenate them along the time axis. This results in single input tensor of 150s (50 words 3s), denoted as {Xn}N n=1. This approach (a) strictly replicates the supervised baseline for fair comparison, and (b) partially removes the confound of variable speech rates while preserving the temporal evolution of each words neural response. Although this input is not perfectly continuous in physiological time, it still aligns with our pre-training strategy as the 150-second context window in pre-training covers the duration of the 50-word sequence in fine-tuning. This allows our model to leverage its learned long-range priors without architectural modification. The model is trained to predict semantic target word embeddings {wn}N n=1 extracted from T5 large language model (Raffel et al., 2020) using contrastive SigLIP variant (Zhai et al., 2023) that masks repeated words in sequence. For each word n, we extract transformer outputs from the last layer h(L) c,t over the corresponding time interval, average across time, and flatten across channels to obtain rn RCd. An MLP head maps this to the predicted embedding: ˆwn = MLP(rn). We fine-tune the transformer and MLP head end-to-end. At inference, words are predicted by nearest-neighbor retrieval, maximising cosine similarity: ˆyn = arg maxy cos( ˆwn, wy). Evaluation data. We evaluate with the three largest English-language perceived speech MEG datasets  (Table 2)  . These are LibriBrain ( Ozdogan et al., 2025; Landau et al., 2025), deep dataset of 52 hours of MEG from single subject listening to audiobooks, Armeni et al. (2022), in which 3 subjects each listened to 10 hours of audiobooks, and MEG-MASC (Gwilliams et al., 2023), broad study with 27 subjects listening to 2 hours each of short stories. We split sequences into train, validation, and test splits in the ratio 80:10:10. The same stimuli presented to different subjects are always assigned to the same set, preventing the model from exploiting stimulus-specific features on held-out subjects (Jo et al., 2024). Table 2. Evaluation datasets. The three datasets span different regimes: shallow multi-subject (MEG-MASC), deep single-subject (LibriBrain), and somewhere in-between (Armeni). Dataset Subjects Hrs./Subject Total Hrs. MEG-MASC Armeni LibriBrain 27 3 2 10 52 54 30 52 Metric. Following dAscoli et al. (2025), we measure word decoding performance with top-10 balanced accuracy over fixed retrieval vocabulary. This is the macro average 5 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 3. Pre-training enables generalisation with less subject data. We compare MEG-XL to the state-of-the-art supervised method (dAscoli et al., 2025) and baseline trained from scratch (MEG-XL with random init.) across varying amounts of fine-tuning data. MEG-XL consistently outperforms its randomly initialised counterpart, confirming that the gains stem from learned priors rather than architecture alone. On Armeni and MEG-MASC, where per-subject data is shallower, MEG-XL outperforms dAscoli et al. (2025) throughout most of the data range. On LibriBrain, deep single-subject dataset, both methods perform similarly until approximately 2.5 hours of training data, after which their supervised method pulls ahead. This suggests pre-training may substitute for subject-specific data as when recordings are scarce, learned priors help. In rare cases where recordings are abundant, learning from scratch eventually wins. of top-10 accuracy over word classes, accounting for word frequency imbalance. dAscoli et al. (2025) use retrieval sets of the top-50 and top-250 most frequent words. For consistency, we use top-50; top-250 results in Appendix show similar trends. We report this metric on the test set using the best validation checkpoint (early stopping). 3.1. Comparison to Foundation Models Comparing MEG-XL to six state-of-the-art pre-trained models, our approach shows the best generalisation under data constraints and joint best with all data. Table 1 reports performance at 13% and 100% of training data. Here, we randomly subsample the aggregate training set. Since our multi-subject datasets are balanced, these ratios correspond to proportional reduction in data per subject. Below 13%, most baselines collapse to chance performance, making comparison uninformative; this threshold thus represents the low-data regime where foundation models can plausibly compete. With limited data, MEG-XL outperforms all baselines by substantial margins. With full data, MEG-XL matches or exceeds all methods on LibriBrain and MEGMASC, while remaining within 1.1 percent of BrainOmni on Armeni. Notably, BrainOmnis strong performance on LibriBrain and Armeni does not transfer to MEG-MASC, suggesting that this model does not perform well with shallow per-subject data. This is the regime that matters for clinical deployment as transfer to new users needs models that generalise from little subject data. MEG-XL is most effective precisely in the data-constrained regime where other foundation models collapse. While recent analysis suggests pre-trained models struggle with limited data (Anonymous, 2025)a trend confirmed by our baselines, which largely remain at chance levels with 13% training dataMEG-XL breaks this pattern. It achieves 4757% accuracy against 20% random baseline, significantly outperforming the only other viable alternative, LaBraM. Thus, MEG-XLs pre-training may capture structure that helps generalise with greater data efficiency. 3.2. Data Efficiency Compared to Supervised Learning Evaluations of brain foundation models often lack supervised baselines trained on equivalent data, obscuring the true value of pre-training. To address this, we benchmark MEG-XL directly against the state-of-the-art supervised method (dAscoli et al., 2025) across the full data spectrum. Our approach performs well in the low-data regime, generally outperforming dAscoli et al. (2025) (Figure 3), except on deep single-subject data. The significant gap between the pre-trained and randomly initialised MEG-XL models demonstrates that the architecture itself is insufficient without the learned priors acquired during pre-training. Against dAscoli et al. (2025), on MEG-MASC, our model improves over 25% at certain points and the benefits persist across all 54 hours of training data (2 hours per subject). On Armeni et al. (2022), fine-tuning our model leads to accuracy roughly 10% above the supervised method until around 15 hours of data (5 hours per subject). Conversely, on LibriBrain both methods perform similarly until 2.5 hours, after which the dAscoli et al. (2025) method pulls ahead. Pretraining helps most with more subjects and when per-subject 6 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 4. Linear probing shows that models pre-trained with more context generalise better to word decoding. We pre-train models with increasing context, fixed masking percentage, and constant optimisation steps, then evaluate the strength of their representations with linear probes (frozen backbone). We compare two conditions: full context, where all models see 150s of input to isolate representation quality, and matched context, where the input is restricted to the pre-training length. The lack of divergence between the lines suggests models cannot leverage inference context that exceeds their pre-training context. We train the linear probes with 7% of the training data. We could not expand further than 150s due to GPU memory limits. Token-matched pre-training shows similar trends (Appendix D). data is shallow, but deep single-subject recordings eventually favour learning from scratch. These results suggest one relevant axis of scale for clinical neural decoding may be breadth across subjects. When per-subject data is constrained, as with paralysed patients, pre-training on many subjects substitutes for extensive individual training. Recent work has questioned whether brain foundation models improve over supervised baselines, finding marginal gains of 1-2% despite orders of magnitude more parameters (Lee et al., 2025b), and poor performance in lowdata regimes (Anonymous, 2025). In contrast, our results show 1025% gains over the supervised state-of-the-art (dAscoli et al., 2025), with fewer parameters (20M vs 200M), specifically in the low-data regime that is most important for clinical deployment. The difference may lie in alignment between pre-training and downstream task. Foundation models assume generality, expecting representations to transfer broadly. However, speech decoding may demand temporal structure that short-context pre-training discards. Domain-aligned pre-training with extended context outperforms more generic models trained with more data. The optimal pre-training strategy depends not only on scale but on the temporal and structural demands of the target task. 3.3. Longer Context Improves Representations To what extent can we exploit neural context in pre-training? We pre-train models with increasing context length, freeze their weights, and train linear probes on the final-layer representations to predict word embeddings. Figure 4 shows that longer pre-training context yields better word decoding across datasets, with diminishing returns after one hundred seconds. With full context, differences reflect representation quality, not inference-time context; with matched context, we ensure that differences are not due to mismatch between pre-training and probing context. Moreover, the small difference between full and matched context probing suggests that additional inference-time context provides no benefit unless the model was pre-trained to use it. This mirrors the length generalisation challenge in large language models, where performance typically plateaus when inference context exceeds the pre-training context (Dai et al., 2019; Press et al., 2022), confirming that long-context utilisation is also learned capability in the neural data domain. Could the benefit of neural context extend beyond brain-totext? Figure 5 (top) shows that extending neural context during pre-training consistently improves zero-shot masked token prediction across all three unseen datasets. This scaling appears log-linear, indicating that longer context improves modelling of neural data. Notably, while linear probing for speech decoding plateaus after 100 seconds, masked prediction does not. This discrepancy suggests that the utility of context is task-dependent, as not all information captured in the neural signal is necessary for every downstream task. 3.4. What Does Long-Context Pretraining Teach? We analyse temporal attention patterns across models pretrained with varying context lengths. Short-context models attend diffusely from the first layer, spreading attention uniformly across time. Longer-context models attend locally in early layers, then progressively expand their temporal reach through depth (Figure 5 bottom left). This hierarchical processing coincides with lower attention entropy. Thus, longer-context models are more selective about which timesteps matter (Figure 5 bottom right). These results suggest long-context pretraining teaches models when to attend far versus near. Together with Figure 5 (top), this also provides further evidence that long-context data can only be properly leveraged with long-context pre-training. 7 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 5. (Top) Extending neural context improves zero-shot prediction of brain activity from unseen datasets and subjects. We mask the central 3s subsegment of samples from unseen datasets and measure improvement in token prediction accuracy (relative to chance) of models pre-trained on increasing neural context. Scaling improves masked prediction, with the trend remaining through 150s. Only GPU VRAM limits prevent increasing it further. Chance accuracy is 1/256. (Bottom) Long-context pretraining induces selective and hierarchical attention. (Left) Models pretrained on longer context attend locally in early layers before expanding to integrate distant context; short-context models attend diffusely throughout. (Right) Attention entropy decreases with context length, indicating more selective attention patterns. We provide 150s context at inference. See Appendix for attention distance and entropy calculations. 4. Discussion Supervised training on large within-subject data remains the gold standard and is hard to beatbut this currently limits applicability to new subjects such as paralysed patients who cannot provide long training recordings. We show that pre-training with long-context reduces the data requirement from dozens to 1-2 hours of total data and only tens of minutes per subject, vastly expanding the range of applicable subjects. Thus, although clinical deployment remains distant, requiring extension to imagined speech and patient populations, the subject-specific data bottleneck may be softer than assumed. Pre-training on long neural context across subjects can substitute for extensive per-subject data. These improvements in data efficiency are driven by scaling neural context in pre-training. As we show, representations continue to generalise better in word decoding, with diminishing returns after 100s of context. Models pre-trained on longer windows also improve at zero-shot prediction of masked brain activity, where performance scales to 150s without saturation. This may suggest that context could help on other tasks, though here we focus on word decoding. Whether the asymmetry between tasks reflects task-specific ceilings or linear readout limits remains unclear. Further scaling may benefit some tasks more than others. The reason long-context pre-training helps is that it teaches models to attend more selectively and to hierarchically process near-to-far context. This is an ability that short-context models never acquire. Thus, instead of unlocking the ability to attend to far context, the main advantage lies in learning when and how to make use of distant context. These findings come with some caveats. We focus on perceived speech as foundational step before addressing imagined speech. We also evaluate on relatively limited 50word vocabulary. Performance decreases with larger vocabularies (Appendix B), though this parallels the development of open-vocabulary decoding in surgical studies, where results were first obtained on 50-word vocabularies before scaling up to hundreds of thousands of words (Moses et al., 2021; Willett et al., 2023). While we characterise the attention patterns that drive generalisation, understanding the nature of the learned structure also remains unresolved. To conclude, neural decoders have historically operated on windows shorter than the structure they aim to recover. While supervised approaches have begun to widen this window, e.g. dAscoli et al. (2025), providing long contexts to model is insufficient for data-efficient generalisation without appropriate long-context statistical priors. Just as language models require pre-training on long documents to master coherence, brain-to-text systems benefit from pre-training with long-context neural activity to decode speech with limited subject data. By pre-training on extended contexts, we can now move beyond accessing long windows to acquiring the long-context priors to exploit them, recovering usable information that both short-context pre-trained models and purely supervised methods leave behind in data-scarce regimes. MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training"
        },
        {
            "title": "Acknowledgements",
            "content": "DJ would like to thank Yonatan Gideoni, Benjamin Ballyk, Gilad Landau, Jonas Jurß, Mariya Hendriksen, Botos Csaba, Alexandru Dobra, and Melanie Schneider for their feedback at various stages of this work. We would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work. http://dx.doi.org/10. 5281/zenodo.22558. We are especially grateful to the ARC support team for their timely support as conference deadlines approached. Data for this project was provided by the Cambridge Centre for Ageing and Neuroscience (CamCAN). CamCAN funding was provided by the UK Biotechnology and Biological Sciences Research Council (grant number BB/H008217/1), together with support from the UK Medical Research Council and University of Cambridge, UK. DJ is supported by an AWS Studentship from the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (AIMS) (EP/S024050/1). OPJ and the PNPL group are supported by the MRC (MR/X00757X/1), Royal Society (RGR1241267), NSF (2314493), NFRF (NFRFT-2022-00241), and SSHRC (895-2023-1022)."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work towards non-invasive speech decoding, with potential applications in brain-computer interfaces for individuals who have lost the ability to speak. Clinical deployment remains distant as current performance is below what communication aids require, and substantial work remains. Neural decoding technologies raise obvious privacy concerns, as they involve inferring mental content from brain activity. The present work uses only publicly available research datasets with their own ethics approvals and decodes perceived speech rather than covert thought. As decoding capabilities improve, the field will need norms around consent, data ownership, and the boundary between assistive and surveillant applications. These are questions we do not resolve here but consider essential."
        },
        {
            "title": "References",
            "content": "Anonymous. Are EEG foundation models worth it? Comparative evaluation with traditional decoders in diverse BCI tasks. 2025. Under review at The Fourteenth International Conference on Learning Representations. Avramidis, K., Feng, T., Jeong, W., Lee, J., Cui, W., Leahy, R. M., and Narayanan, S. Neural codecs as biosignal tokenizers. arXiv preprint arXiv:2510.09095, 2025. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Borsos, Z., Sharifi, M., Vincent, D., Kharitonov, E., Zeghidour, N., and Tagliasacchi, M. SoundStorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636, 2023. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Card, N. S., Wairagkar, M., Iacobacci, C., Hou, X., SingerClark, T., Willett, F. R., Kunz, E. M., Fan, C., Nia, M. V., Deo, D. R., Srinivasan, A., Choi, E. Y., Glasser, M. F., Hochberg, L. R., Henderson, J. M., Shahlaie, K., Stavisky, S. D., and Brandman, D. M. An accurate and rapidly calibrating speech neuroprosthesis. New England Journal of Medicine, 391(7):609618, 2024. Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V., and Salakhutdinov, R. Transformer-XL: Attentive language models beyond fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 29782988. Association for Computational Linguistics, 2019. Dash, D., Ferrari, P., Dutta, S., and Wang, J. NeuroVAD: Real-time voice activity detection from non-invasive neuromagnetic signals. Sensors (Basel, Switzerland), 20, 2020. Armeni, K., Guclu, U., van Gerven, M. A. J., and Schoffelen, J.-M. 10-hour within-participant magnetoencephalography narrative dataset to test models of language comprehension. Scientific Data, 9, 2022. Defossez, A., Caucheteux, C., Rapin, J., Kabeli, O., and King, J.-R. Decoding speech perception from noninvasive brain recordings. Nature Machine Intelligence, 5:1097 1107, 2022. 9 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. Trans. Mach. Learn. Res., 2023, 2023. Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics, 2019. dAscoli, S., Bel, C., Rapin, J., Banville, H. J., Benchetrit, Y., Pallier, C., and King, J.-R. Towards decoding individual words from non-invasive brain recordings. Nature Communications, 16, 2025. Elvers, G., Landau, G., and Parker Jones, O. Elementary, My Dear Watson: Non-invasive neural keyword spotting in the LibriBrain dataset. In Data on the Brain & Mind Workshop at NeurIPS 2025, 2025. Fyshe, A., Sudre, G. P., Wehbe, L., Rafidi, N. S., and Mitchell, T. M. The lexical semantics of adjectivenoun phrases in the human brain. Human Brain Mapping, 40: 4457 4469, 2019. Gwilliams, L., King, J.-R., Marantz, A., and Poeppel, D. Neural dynamics of phoneme sequences reveal positioninvariant code for content and order. Nature Communications, 13, 2022. Gwilliams, L., Flick, G., Marantz, A., Pylkkanen, L., Poeppel, D., and King, J.-R. Introducing meg-masc highquality magneto-encephalography dataset for evaluating natural speech processing. Scientific Data, 10, 2023. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. B. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1597915988. IEEE, 2022. Jayalath, D., Landau, G., Shillingford, B., Woolrich, M., and Parker Jones, O. The Brains Bitter Lesson: Scaling speech decoding with self-supervised learning. In Fortysecond International Conference on Machine Learning, 2025. Jiang, W., Zhao, L., and Lu, B. Large brain model for learning generic representations with tremendous EEG data in BCI. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Jo, H., Yang, Y., Han, J., Duan, Y., Xiong, H., and Lee, W. H. Are EEG-to-text models working? The Fourth International Workshop on Human Brain and Artificial Intelligence at IJCAI, 2024. Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 971980, 2017. Kutas, M. and Federmeier, K. D. Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP). Annual review of psychology, 62:62147, 2011. Landau, G., Ozdogan, M., Elvers, G., Mantegna, F., Somaiya, P., Jayalath, D., Kurth, L., Kwon, T., Shillingford, B., Farquhar, G., Jiang, M., Jerbi, K., Abdelhedi, H., Mantilla Ramos, Y., Gulcehre, C., Woolrich, M., Voets, N., and Parker Jones, O. The 2025 PNPL competition: Speech detection and phoneme classification in the LibIn NeurIPS 2025 Competition Track, riBrain dataset. 2025. Lee, J., Feng, T., Kommineni, A., Kadiri, S. R., and Narayanan, S. Enhancing listened speech decoding from EEG via parallel phoneme sequence prediction. In 2025 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2025, Hyderabad, India, April 6-11, 2025, pp. 15. IEEE, 2025a. Lee, N., Barmpas, K., Panagakis, Y., Adamos, D., Laskaris, N., and Zafeiriou, S. Are large brainwave foundation models capable yet? Insights from fine-tuning. In Fortysecond International Conference on Machine Learning, 2025b. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Moreira, J. P. C., Carvalho, V. R., Mendes, E. M. A. M., Fallah, A., Sejnowski, T. J., Lainscsek, C., and Comstock, L. An open-access eeg dataset for speech decoding: Exploring the role of articulation and coarticulation. Scientific Data, 12, 2025. Moses, D. A., Metzger, S. L., Liu, J. R., Anumanchipalli, G. K., Makin, J. G., Sun, P. F., Chartier, J., Dougherty, M. E., Liu, P. M., Abrams, G. M., Tu-Chan, A., Ganguly, K., and Chang, E. F. Neuroprosthesis for decoding speech in paralyzed person with anarthria. New England Journal of Medicine, 385(3):217227, 2021. 10 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Obeid, I. and Picone, J. W. The temple university hospital EEG data corpus. Frontiers in Neuroscience, 10, 2016. OpenAI. Introducing GPT-4 Turbo. https://openai. com/index/introducing-gpt-4-turbo/, November 2023. Ozdogan, M., Landau, G., Elvers, G., Jayalath, D., Somaiya, P., Mantegna, F., Woolrich, M., and Parker Jones, O. LibriBrain: Over 50 hours of within-subject MEG to improve speech decoding methods at scale. In The Thirty-Ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. Panachakel, J. T. and Ramakrishnan, A. G. Decoding covert speech from EEG-a comprehensive review. Frontiers in Neuroscience, 15, 2021. Press, O., Smith, N. A., and Lewis, M. Train Short, Test Long: Attention with linear biases enables input length In The Tenth International Conference extrapolation. on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. Schoffelen, J.-M., Oostenveld, R., Lam, N. H. L., Udden, J., Hulten, A., and Hagoort, P. 204-subject multimodal neuroimaging dataset to study language processing. Scientific Data, 6, 2019. Shafto, M. A., Tyler, L. K., Dixon, M., Taylor, J. R., Rowe, J. B., Cusack, R., Calder, A. J., Marslen-Wilson, W. D., Duncan, J. S., Dalgleish, T., Henson, R. N. A., Brayne, C., and Matthews, F. E. The Cambridge Centre for Ageing and Neuroscience (Cam-CAN) study protocol: cross-sectional, lifespan, multidisciplinary examination of healthy cognitive ageing. BMC Neurology, 14, 2014. Su, J., Ahmed, M. H. M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Tang, J., LeBel, A., Jain, S., and Huth, A. G. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, 26(5):858866, 2023. van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., and Kavukcuoglu, K. Wavenet: generative model for raw audio. In The 9th ISCA Speech Synthesis Workshop, SSW 2016, Sunnyvale, CA, USA, September 13-15, 2016, pp. 125. ISCA, 2016. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Neural Information Processing Systems, 2017. Wang, G., Liu, W., He, Y., Xu, C., Ma, L., and Li, H. EEGPT: pretrained transformer for universal and reliable representation of EEG signals. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. Wang, J., Zhao, S., Luo, Z., Zhou, Y., Jiang, H., Li, S., Li, T., and Pan, G. CBraMod: criss-cross brain foundation model for EEG decoding. In The Thirteenth International Conference on Learning Representations, 2025. Wang, S., Zhang, X., Zhang, J., and Zong, C. synchronized multimodal neuroimaging dataset for studying brain language processing. Scientific Data, 9, 2022. Willett, F. R., Kunz, E. M., Fan, C., Avansino, D. T., Wilson, G. H., Choi, E. Y., Kamdar, F. B., Glasser, M. F., Hochberg, L. R., Druckmann, S., Shenoy, K. V., and Henderson, J. M. high-performance speech neuroprosthesis. Nature, 620:1031 1036, 2023. Xiao, Q., Cui, Z., Zhang, C., Chen, S., Wu, W., Thwaites, A., Woolgar, A., Zhou, B., and Zhang, C. BrainOmni: brain foundation model for unified EEG and MEG signals. In The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025. Yan, W., Hafner, D., James, S., and Abbeel, P. Temporally consistent transformers for video generation. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 3906239098. PMLR, 2023. Yang, C., Westover, M. B., and Sun, J. BIOT: biosignal transformer for cross-data learning in the wild. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end neural 11 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training audio codec. Process., 30:495507, 2022. IEEE ACM Trans. Audio Speech Lang. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 11941 11952. IEEE, 2023. Zhang, B. and Sennrich, R. Root mean square layer normalization. In Wallach, H. M., Larochelle, H., Beygelzimer, A., dAlche-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1236012371, 2019. 12 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training A. Details on Experimental Setup A.1. Preprocessing We follow minimal preprocessing pipeline similar to Defossez et al. (2022) and dAscoli et al. (2025). We preprocess all recordings with 0.1Hz high-pass and 40Hz low-pass filter and then resample the recording to 50Hz. Although technically this risks creating aliasing artefacts, it follows the standard word decoding preprocessing pipeline in dAscoli et al. (2025) and reduces the number of timepoints, allowing for longer contexts. Appendix contains results with Nyquist-compliant resampling. We standardise each contiguous 3s long subsegment within 2.5-minute sample independently. We first baseline correct each subsegment by subtracting the channel-wise mean of the first 0.5s, then we scale such that the subsegment has median zero and upper quartile and lower quartile of 1 and -1 respectively. Standardising by subsegment makes the pre-training data distribution more representative of that seen in the downstream word decoding task. Finally, we clamp the signal to the range (5, 5). While 0.1Hz high-pass filter attenuates signals with periods longer than 10 seconds, i.e. slow drift, this does not prevent modelling structure at longer timescales. The frequency content of the signal and the timescale of dependencies in that signal are distinct. The high-pass filter removes slow-drift and scanner artifacts. It does not remove the statistical dependencies between neural responses separated by minutes. model with access to 2.5 minutes of (filtered) signal can still learn these dependencies while model with access to 2 seconds cannot. A.2. Pre-training Datasets We use continuous 2.5-minute windows of MEG data as samples from the following datasets: CamCAN (Shafto et al., 2014), healthy ageing study with rest, sensorimotor, and passive sensory task data from approximately 700 subjects, MOUS (Schoffelen et al., 2019), 104-subject language study with data from reading and listening in Dutch, and SMN4Lang (Wang et al., 2022), another language study with 12 subjects listening to extended natural speech in Chinese (Mandarin). For CamCAN, we use only the rest and sensorimotor task subset, for MOUS, we use only the listening task data, and for SMN4Lang we use all the available MEG data. This totals approximately 300 hours of pre-training data and over 800 subjects. A.3. Foundation Model Baselines For each baseline, unless otherwise specified, we provide full 2.5-minute long samples and fine-tune pre-trained checkpoints end-to-end alongside an MLP head (with hidden dimension 2048) that predicts the target embedding. The output embeddings from model backbones are sliced according to their alignment with word stimuli and pooled in the time dimension before being flattened and concatenated. These new embeddings, each corresponding to word, are given independently to the MLP head. We use higher learning rate for the MLP, which is trained from scratch, just like we do with MEG-XL (see the hyperparameters in Table 3). We provide each foundation model with data of the same sample rate as used for pre-training. BioCodec. Avramidis et al. (2025) trained BioCodec as single-channel tokenizer across thousands of hours of EEG data. When evaluating BioCodecs generalisation, we treat the tokenizer as pre-trained foundation model applied to each sensor channel independently. To resolve spatial features across channels, we follow the authors suggestion of processing BioCodec embeddings with two linear transformers over time and sensors. We then pool embeddings in the time dimension before feeding the result to the two-layer MLP. BIOT. Yang et al. (2023) trained BIOT as foundation model for generic biosignals, but used primarily EEG data in pre-training. As the model was pre-trained with maximum of 18 channels of data and MEG has many more channels, we insert randomly initialised trainable projection before BIOT that reduces the channel dimension to comply. As the learnable positional embeddings in the model have maximum trained length, we had to reduce the neural context to 24 seconds for this model. We used the EEG-six-datasets-18-channels checkpoint that was trained on the most data. EEGPT. EEGPT (Wang et al., 2024) is another pre-trained foundation model designed for generalising from EEG data. This model was pre-trained to support at most 58 channels. To support MEG data with many more channels, we add randomly initialised trainable projection before the network to reduce the channel dimension of our MEG data to comply. 13 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 6. Generalisation Across Training Data Regimes (Top 250 Words as Retrieval Set). Results in the main paper use top-50 word retrieval sets. As expected, trends with top-250 word retrieval remain the same with the larger vocabulary leading to degraded performance across all methods. See the caption in Figure 3 for details. BBL. The Brains Bitter Lesson (Jayalath et al., 2025) developed model pre-trained on MEG data and designed for simple speech decoding tasks (speech detection, phonetic feature classification). BrainOmni. Xiao et al. (2025) trained BrainOmni with mix of both MEG and EEG data. The models tokenizer leverages sensor position, orientation, and type. As our approach also uses this information, we provide it directly to BrainOmni with our datasets. LaBraM. LaBraM (Jiang et al., 2024) is large-scale EEG foundation model trained with masked patch prediction objective. As LaBraMs learned time embeddings limit its context length, we had to reduce the neural context to 15 seconds. A.4. Supervised Word Decoding Baseline To collect experimental results for dAscoli et al. (2025), we ran the code released as part of the supplementary materials of their publication, following the instructions provided in their README files. They did not support data loader for LibriBrain, since this dataset was not evaluated in their work. Therefore, we implemented our own for evaluation. A.5. Hyperparameters We provide all hyperparameters in Table 3. A.6. Computational Resources All experiments were performed on individual NVIDIA A100, L40S, or H100 GPUs. Pre-training MEG-XL took approximately 12 hours on one H100. Fine-tuning MEG-XL with 30-50 hours of data took similar amount of time. B. Larger Retrieval Set Results For posterity, we provide the same comparison results as in the main body of the paper, except with the top 250 most frequent words as the retrieval set instead of the top 50 words. These are provided in Figure 6 and Table 4. C. Nyquist-Compliant Resampling To match the preprocessing in dAscoli et al. (2025), we applied 40Hz low-pass filter before resampling the brain data to 50Hz. This made it possible to do like-for-like comparison with dAscoli et al. (2025). However, this technically violates tenet of signal processingspecifically, the Nyquist criterion requires the sample rate to be at least twice the low-pass filter cut-off. To flout this risks introducing aliasing artefacts. In this appendix, we therefore repeat experiments from the main text but with 100Hz resampling. As this doubles the number of timepoints per sample, we reduced the context length from 14 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Table 3. Hyperparameters. We use the implementation of criss-cross attention provided by Xiao et al. (2025) in their source code. Parameter Data Original sampling rate Re-sampled rate Low-pass filter High-pass filter Standardization Baseline correction Clamping range Sample length Model Gaussian Fourier Features (GFF) embedding dim. (dfourier) GFF sensor position σ GFF sensor orientation σ Transformer dim. (dmodel) Transformer layers Transformer heads Transformer attention Mask block duration Number of blocks masked Pre-Training Training steps Batch size Learning rate Weight decay Warmup steps Gradient clipping Optimizer Loss Fine-Tuning MLP head hidden dim. Training steps Early stopping patience Early stopping metric Batch size Learning rate (transformer) Learning rate (MLP head) Weight decay Gradient clipping Optimizer Loss Value 1000Hz 50Hz 40Hz 0.1Hz IQR = [-1, 1] / 3s subsegment [0.0s, 0.5s] / 3s subsegment [-5, 5] 150s 256 1.8 1.0 512 8 8 Criss-cross attention (Wang et al., 2025) 3s 20 (40% of blocks) 35000 1 1e-4 1e-4 250 1.0 AdamW (Loshchilov & Hutter, 2019) Cross-entropy on masked tokens 2048 50 epochs (max. with early stopping) 10 epochs Top-10 balanced accuracy on val. 50 words 1e-5 1e-3 1e-4 1.0 AdamW (Loshchilov & Hutter, 2019) D-SigLIP (Zhai et al., 2023; dAscoli et al., 2025) 15 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Table 4. Comparison to Foundation Models (Top 250 Words as Retrieval Set). See the caption in Table 1 for details. With 13% of training data With 100% of training data"
        },
        {
            "title": "Model",
            "content": "Params. MEG-MASC"
        },
        {
            "title": "Armeni",
            "content": "LibriBrain MEG-MASC"
        },
        {
            "title": "LibriBrain",
            "content": "BioCodec EEGPT BIOT BBL BrainOmni LaBraM MEG-XL (Ours) 1.0M 4.7M 3.2M 15M 8.4M 5.8M 20M 4.7 0.1 4.4 0.1 3.5 0.7 3.8 0.6 3.7 0.8 9.9 0.6 16.8 0.4 4.0 0.3 4.0 0.2 4.0 0.1 4.7 0.2 4.7 0.5 6.5 0.9 19.7 0.3 4.0 0.1 3.9 0.2 4.2 0.0 7.7 0.5 7.6 3.5 12.0 0.3 19.8 0.4 10.1 0.5 6.7 0.5 8.9 0.4 9.2 0.6 4.1 0.9 8.9 1.1 16.8 1. 10.6 0.3 4.0 0.1 9.9 1.8 10.2 0.2 26.8 0.5 14.1 0.5 25.4 0.1 12.4 0.4 5.1 0.1 13.3 0.2 16.1 0.2 25.6 0.1 16.9 0.2 25.4 0.3 Table 5. Nyquist-compliant resampling results. Resampling to 100Hz avoids aliasing by respecting the Nyquist criterion (resampling frequency 2 low-pass filter frequency). However, it requires reducing context length to 75s due to GPU VRAM constraints. With 13% of training data With 100% of training data Model MEG-MASC Armeni LibriBrain MEG-MASC Armeni LibriBrain MEG-XL (50Hz) MEG-XL (100Hz) 47.0 0.9 41.9 1.4 54.9 0.5 51.7 1.0 57.3 0.4 52.2 0.4 46.4 1.3 41.8 2.7 61.2 0.4 56.7 1.2 63.0 0.4 58.6 0. 150s to 75s to satisfy GPU memory constraints. Table 5 compares the two configurations. D. Token-Matched Neural Context Scaling Evaluating the effect of neural context length is complicated by confounds during pre-training. If training steps are held constant while context length increases, models with longer context see more total information despite equal gradient updates. Conversely, if training data is held constant, shorter-context models undergo more gradient steps. Since optimisation is non-linear, this comparison is also imperfect. In this section, we adopt principled compromise where we pre-train models for the same number of steps while ensuring they observe identical numbers of unique tokens (and therefore equivalent total information). Specifically, we randomly sub-sample and expose the network to 42% of the pre-training data for all token-matched experiments. The results in Figure 7 show that the same trends hold as in the main body of the paper. Increasing pre-training context continues to improve both downstream linear probing performance (with diminishing returns or regression at 150s), and zero-shot prediction of masked brain activity. Consistent results under this controlled setting generally rule out improvements due to additional information exposure. E. Tokenizer Comparison As discussed in Section 2.1, we opted for the single-channel EEG pre-trained BioCodec tokenizer (Avramidis et al., 2025) over the MEG pre-trained BrainTokenizer (Xiao et al., 2025) because of BioCodecs ability to reconstruct our MEG data, even at 50Hz, with lower reconstruction error. This is likely due to the fact that BioCodec does not compress the channel dimension, entailing trade off as it results in more tokens than BrainTokenizer. Nevertheless, as the field has not yet advanced to stage where we know precisely which parts of the neural signal are relevant for speech decoding, opting for tokenizer with lower reconstruction error helps avoid discarding task-relevant information from the signal. Although tokenizer that compresses in time can learn cross-channel representations, we leave this task to our transformer backbone. F. Analysing Temporal Attention Heads We analyse the temporal self-attention layers of our criss-cross transformer layers. For each temporal attention layer, we compute attention weights as = softmax(QK/ d), where is the head dimension. We compute two complementary metrics to characterise attention patterns. The mean attention distance measures how far 16 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 7. (Top) Linear probing for word decoding with token-matched pre-training. We pre-train models exactly the same way as described in the caption in Figure 4, except with fixed training tokens so that each pre-trained model sees exactly the same training data (token-matched). Here, we see similar trends with larger contexts systematically improving downstream performance. Diminishing returns, and potentially regression in performance, appear after 150s of pre-training context. The results are noisier as consequence of three pre-training seeds (and linear probes) per context length instead of five due to computational constraints. (Bottom) Zero-shot prediction of masked brain activity with token-matched pre-training. The results here are collected the same way as described in the caption of Figure 5 (top), except with fixed training data for each pre-training run (token-matched). The trends remain very similar. each query position attends on average: MAD(t) = (cid:88) At,k k, (6) where At,k is the attention weight from query position to key position k. This metric is averaged across query positions and attention heads, then calculated for each layer. We report values in seconds, converting from timesteps by multiplying by 0.24 = r/f = 12/50 where = 12 is the tokenizers downsampling ratio and = 50 is the preprocessed sample rate of our data. The attention entropy quantifies the uniformity of each attention distribution: H(t) = (cid:88) At,k log At,k. (7) Higher entropy indicates more uniform attention distribution (attending broadly across positions), while lower entropy indicates concentrated attention on fewer positions. Both metrics are computed over 100 randomly sampled MEG segments from held-out data, with results aggregated across 5 random seeds per pre-training context length. G. Why Non-Invasive Decoding? While intracranial approaches have been undeniably successful (Moses et al., 2021; Willett et al., 2023; Card et al., 2024), invasive brain-computer interfaces require craniotomy, carrying risks of infection, bleeding, and tissue damage. This is significant concern for patients who are often already medically fragile due to conditions like ALS or locked-in syndrome. Implanted electrodes also degrade over time as scar tissue encapsulates the array, with signal quality declining over months to years and devices like Utah arrays eventually requiring replacement surgery. Beyond individual patient burden, invasive approaches face scalability problem as one cannot easily, or ethically, implant thousands of subjects to build foundation models. Non-invasive methods like MEG and EEG allow large-scale data collection from healthy volunteers, enabling the pre-training/fine-tuning paradigm where pre-training data is, relatively speaking, abundant, and thus more easily scaled. They also lower the accessibility barrier allowing more volunteers to participate. The ethical case is also simpler as placing 17 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Figure 8. BioCodec vs BrainTokenizer (originating from BrainOmni). BioCodec reconstructs signals with lower reconstruction error (MSE of 0.41 vs 0.69). The plot shows preprocessed 5-second sample taken from the MOUS dataset at 50Hz across three channels. 18 MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training helmet on someones head is easier to justify than implanting device in their brain. The obvious trade-off is the signal-to-noise ratio as intracranial recordings are orders of magnitude higher in fidelity. This has confined much prior non-invasive work to simpler speech sub-tasks such as speech detection (Dash et al., 2020; Jayalath et al., 2025; Ozdogan et al., 2025), phoneme recognition (Panachakel & Ramakrishnan, 2021; Gwilliams et al., 2022; Lee et al., 2025a; Moreira et al., 2025; Ozdogan et al., 2025), keyword spotting (Elvers et al., 2025), semantic reconstruction (Tang et al., 2023), and isolated word classification (Defossez et al., 2022; Moreira et al., 2025). The challenge for non-invasive decoding is therefore to close the gap to invasive methods through better modelling, extracting more information from noisier signals, but with access to more data. Long-context pre-training, as explored in this work, represents solution to this problem."
        }
    ],
    "affiliations": [
        "PNPL, University of Oxford"
    ]
}