{
    "paper_title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
    "authors": [
        "Yichao Fu",
        "Junda Chen",
        "Siqi Zhu",
        "Zheyu Fu",
        "Zhongdongming Dai",
        "Aurick Qiao",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solution paths, at the cost of increasing compute demands and response latencies. Existing serving systems fail to adapt to the scaling behaviors of these algorithms or the varying difficulty of queries, leading to inefficient resource use and unmet latency targets. We present Dynasor, a system that optimizes inference-time compute for LLM reasoning queries. Unlike traditional engines, Dynasor tracks and schedules requests within reasoning queries and uses Certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustaining 3.3x higher query rates or 4.7x tighter latency SLOs in online serving."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 3 9 9 0 2 . 2 1 4 2 : r a"
        },
        {
            "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
            "content": "Yichao Fu1 Junda Chen1 Siqi Zhu2 Zheyu Fu1 Zhongdongming Dai1 Aurick Qiao3 Hao Zhang1,3 1UC San Diego 2Tsinghua University 3Snowflake"
        },
        {
            "title": "Abstract",
            "content": "The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solution paths, at the cost of increasing compute demands and response latencies. Existing serving systems fail to adapt to the scaling behaviors of these algorithms or the varying difficulty of queries, leading to inefficient resource use and unmet latency targets. We present Dynasor, system that optimizes inference-time compute for LLM reasoning queries. Unlike traditional engines, Dynasor tracks and schedules requests within reasoning queries and uses certaindex, proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustaining 3.3 higher query rates or 4.7 tighter latency SLOs in online serving."
        },
        {
            "title": "1 Introduction",
            "content": "Large language model (LLM) capabilities have advanced rapidly, evolving from basic language understanding to solving complex mathematical problems [5, 38, 47], generating and debugging code [20, 52, 53], and analyzing case law [25]. key driver for this progress is the emergence of sophisticated inference-time algorithms that enhance LLM reasoning [16, 48, 51]. These LLM reasoning algorithms enable LLMs to reflect on their own outputs, explore different reasoning paths, and generate more robust and accurate answers to challenging questions. However, they also yield more output tokens, sometimes by orders of magnitude [9, 38, 47], driving up inference compute demands and response latencies. Equal contribution. Although reasoning algorithms are diverse, they share in common that they each admit flexible trade-off between inference compute and output accuracy, known as inference-time scaling. For example, self-consistency (SC) [48] generates multiple solutions to query and aggregates the final answer via majority voting. Higher accuracy might be achieved by generating more solutions, but at the cost of more GPU cycles. Similarly, Monte Carlo Tree Search (MCTS) [12, 16] expands nodes of solution tree, each representing possible solution step. Expanding more nodes into deeper or wider tree increases the likelihood of finding correct solution but consumes more GPU resources to decode additional tokens. While reasoning algorithms and their inference-time scaling properties have been extensively studied in experimental settings, deploying them in real-world and multi-tenant environments raises additional challenges in resource allocation. First, different user queries may pose varying levels of difficulty to solve, resulting in different amounts of inference compute required. Second, assuming work-conservation, allocating more compute to one user query inevitably delays or reduces compute resources available for others. Third, the best end-to-end latency is achieved by allocating the least amount of time needed to reach correct answer, and no more than that. All of these objectives call for adaptively allocating inference compute between different user queries to effectively balance accuracy and cost. Fig. 1(a) illustrates the characteristics of inference-time scaling on GSM8K [7], popular math reasoning benchmark. While uniformly scaling inference compute (number of generated tokens) by 2.5 across all queries improves overall accuracy from 60% to 80%, several opportunities for better resource allocation remain. As shown in Fig. 1(b), harder queries (blue zone) such as P3 may not be allocated sufficient compute to reach the correct answer. Simpler queries (green zone) such as P1 could have completed faster with less compute. Other queries (e.g., P4) may remain unsolved regardless of compute, and could have been terminated earlier. An alternative allocation e.g., shifting compute from P1 to P3 while decreasing compute for P4 could have yielded both 1 (a) (b) (c) Figure 1: (a) Inference time scaling of self-consistency [48] on GSM8K [7]. As we uniformly increase the number of output tokens (x-axis) per question, the overall accuracy (y-axis) grows then plateaus (see 2.2). (b) Canonical resource allocation in existing systems vs. ideal allocation for four queries P1-P4 with vary difficulties. (c) Correlations between certaindex and the compute required to obtain correct answer. Each point is question. Statistically higher certaindex indicates lower compute needed. higher accuracy and lower total compute cost. Although many LLM serving systems [1, 23, 57] exist, they all fall short in effectively scheduling and allocating inference compute for LLM reasoning programs. Their schedulers [13, 50] operate at the granularity of individual input/output sequences, while LLM reasoning algorithms may invoke multiple input/output sequences corresponding to, e.g., different paths in MCTS. This design yields control of inference-time scaling out of the scheduler entirely and into the hands of the user, who has only partial information about all active queries, or even the relative difficulty of their own query, which are crucial for effective compute allocation. Furthermore, existing systems are agnostic to which individual input/output sequences belong to given parent LLM reasoning query, making it impossible for the scheduler to optimize for end-to-end latency SLOs of those parent queries. These shortcomings typically lead to naive or uniform inference compute scheduling that suffer from wasted compute, degraded accuracy, and missed latency targets. To address these limitations, we designed Dynasor, system that orchestrates inference-time compute for dynamically serving reasoning programs. Unlike prior inference serving systems, Dynasor is explicitly aware of, and can control, the schedule of input/output requests associated with each reasoning query. This enables Dynasor to measure the progress of different reasoning queries while they run, estimate their individual trade-offs between allocating more compute and progress towards final answer. Dynasor adaptively allocates more compute for difficult queries, less compute for easy queries, and early-terminates out-of-reach queries. It also minimizes the end-to-end latency of reasoning queries by gang-scheduling their group of input/output requests to mitigate stragglers, and promotes fairness by moderating compute allocations across competing reasoning queries. To measure statistically meaningful notion of reasoning progress, Dynasor uses novel proxy variable, which we refer to as certaindex, inspired by prior work on uncertainty quantification of LLMs [11, 15, 21, 22]. Intuitively, certaindex quantifies how certain the LLM is in approaching its final answer (which may be correct or incorrect) at reasoning. Fig. 1(c) plots the correlation between certaindex values and the amount of compute needed to yield correct answers. When certaindex is high, the LLM is certain about its current answer, suggesting that more compute would not change its answer and less compute can be allocated. When certaindex is low, the LLM is uncertain about its current answer, and allocating more compute to explore alternative solution paths may help it approach more certain answer. Crucially, while LLM reasoning algorithms are diverse, they typically all possess notion of certaindex that can be efficiently estimated at inference-time (3.1). For example, in self-consistency, certaindex can be computed as the entropy of answers generated across all solution trajectories. For algorithms that involve reward models, the outputs of the reward model itself can be used as measure of certaindex. Dynasor uses certaindex as key narrow interface between the system and its applications, allowing it to support wide range of current and future reasoning algorithms. We implement Dynasor as scheduling layer compatible with existing serving engines [23, 24, 57]. Our evaluations on various datasets, LLMs, and reasoning algorithms show that in batch processing scenarios, Dynasor saves up to 50% compute to reach the same overall accuracy; in online serving, it sustains up to 3.3 more queries or 4.7 tighter latency SLOs at the same attainment rates. Specifically, this paper makes the following contributions: We identify the trade-offs among inference-time compute, question difficulty, and task latency and accuracy in serving reasoning queries, and develop certaindex as an indicative signal for reasoning progress, and show it is simple to extract, effective, and general across diverse tasks and algorithms. We develop Dynasor, an adaptive inference-serving system that leverages certaindex as key narrow interface between its scheduler and diverse reasoning applications. Dynasor adaptively allocates compute and schedules reasoning requests across queries, optimizing accuracy, cost, latency, and fairness. Comprehensive evaluations across various datasets and reasoning algorithms show that Dynasor significantly outperforms prior state-of-the-art systems, including SGLang [57] and Parrot [29], using up to 50% fewer tokens to reach similar overall accuracy in batch processing, and sustaining 3.3 higher request arrival rates and 4.7 more stringent latency SLOs in online serving."
        },
        {
            "title": "2 Background and Motivation",
            "content": "Traditional LLM services operate on plain token generation paradigm: given an input prompt, the LLM generates output tokens sequentially until termination condition is met, such as stop token or maximum sequenece length, is met. More complex applications are implemented by composing workflows of individual input/output requests. Today, LLMs are increasingly applied to solve harder tasks requiring logical reasoning, multi-step planning, and iterative workflows. This paradigm, known as LLM reasoning, often requires generating multiple output sequences to user query, each representing different reasoning trajectory, and responds with final answer only after multiple reasoning trajectories have been generated and considered. This shift introduces fundamental distinction between serving traditional LLM queries and reasoning programs. It expands nodes by sampling continuations from the LLM step-by-step, until reaching leaf node containing candidate solution. Each solution is scored via reward model or simulation, and the score is back-propagated to its ancestral nodes to update their reward values. This completes one iteration (or rollout). MCTS iteratively performs iterations, starting each from node with the currently highest reward. is the key knob that controls the inference-time compute increasing allows deeper and broader exploration of potential solutions. Internalized Chain-of-thought (ICoT). Lastest LLMs, such as OpenAI o1 [38] and Qwen-QWQ [47], can internalize reasoning behaviors at training, without needing for explicit reasoning algorithms. These models generate extended chain-of-thoughts [49] sequences to derive candidate answers to reasoning queries. They iteratively refine these answers by reflecting on prior outputs and continuing sequential decoding until termination, yielding the final solution. The number of refinement rounds n, which corresponds to total decoded tokens, directly determines inference-time compute. We examine this approach further in 6.7."
        },
        {
            "title": "2.1 LLM Reasoning Algorithms",
            "content": "2."
        },
        {
            "title": "Inference Time Scaling",
            "content": "We describe four widely used reasoning algorithms. While their specifics differ, they share two core operations: (1) expansion: generating tokens to expand solution trajectories, and (2) aggregation: combining results from trajectories to derive final answer. Increasing compute for expansion generally improves the quality of answers during aggregation. Self-consistency (SC). Fig. 2(a) depicts the computational process of SC [48]. Starting with prompt P, SC expands trajectories T1,T2,...,Tn by sampling different outputs from the LLM (e.g., varying random seeds or temperature). Each trajectory represents reasoning path that terminates in candidate answer. During aggregation, SC applies majority voting across answers in T1:n and selects the one that appears most frequently. The key compute-control parameter is n, which dictates the number of generated trajectories. Rebase. Rebase [51], as shown in Fig. 2(b), also begins by generating independent outputs I1,I2,...,In from the input prompt P. Unlike SC, these outputs represent intermediate steps, which may or may not contain the final solution. They are ranked and assigned with scores s1,s2,...,sn, typically by learned reward model. Rebase selects nodes with higher normalized exponential scores esi/n j=1 es as parent nodes, from which the next reasoning steps are branched out. This process is repeated until candidate solutions are returned. The final result is aggregated by (weighted) majority voting across all candidate answers or selecting the top-scored answer (a.k.a. best-of-n). Rebase structures the reasoning process as solution tree, where trees depth (i.e., solution steps to reach an answer) depends on LLM outputs and its width is exactly n, which controls its inference-time compute. Monte Carlo Tree Search (MCTS). Starting from the prompt P, MCTS [12, 16] iteratively builds solution tree (Fig. 2(c)). In each algorithm above, control parameter n, which we call knob, governs the inference-time compute and the number of output tokens generated. Regardless of their differences, they all share flexible trade-off: increasing improves the likelihood of yielding the correct answer but requires more compute to generate additional tokens. Fig. 1 demonstrates this scaling behavior for SC on GSM8K: as the token budget per question increases (x-axis), overall accuracy rises (y-axis). Similar trends are observed for other algorithms and datasets (see 6) and are reported in recent LLM advancements, such as OpenAI o1 [38] and open models [42, 47]. The exact scaling curve depends on both dataset difficulty and model capabilities. Using Fig. 1 as an example, we identify three characteristics that influence resource allocation and scheduling in serving systems: (1) Easy Zone (green): Low token budgets suffice for simpler queries, and over-allocating tokens should be avoided to prevent unnecessary latency. (2) Scaling Zone (blue): This regime shows that increasing compute significantly boosts accuracy, highlighting the importance of efficient resource allocation to balance cost (3) Impossible Zone (gray): Additional and accuracy. computation provides no benefits on questions beyond the models capabilities. Early termination of queries in this zone can minimize unnecessary resource consumption."
        },
        {
            "title": "2.3 Existing LLM Inference Systems",
            "content": "Prior works on serving systems have extensively optimized LLM inference at the system-level. Given independently arriving requests, these systems leverage request batching to maximize GPU utilization [3, 18, 27, 55], memory paging to mitigate memory fragmentation and enable higher concurrency [23], disaggregation of prompt and output 3 Figure 2: Illustration of the workflow of different LLM reasoning algorithms discussed in 2.1 token processing to prevent interference between requests in different phases [39, 58], and more. However, all of these works assume independent input/output requests, and are agnostic to higher-level applications, such as LLM reasoning programs, that may submit interdependent inference requests. On the other hand, ParrotServe [29] and SGLang [57] co-design the frontend and backend of LLM inference servers, and are aware of multi-request applications. ParrotServe provides an abstraction for users to specify dependencies between requests, enabling more effective cross-request scheduling. SGLang introduces programming primitives for multi-request workflows, optimizing execution by reusing intermediate KV-cache memory across requests. Although both ParrotServe and SGLang address multi-request applications, they do not target LLM reasoning programs, which exhibit unique characteristics of inference-time scaling and statistical measures of progress, and demand more adaptive method of scheduling offered by Dynasor. Next, we develop the certaindex measure."
        },
        {
            "title": "3 Certaindex Based Resource Allocation",
            "content": "Large language models (LLMs) have an inherent ability to \"know when they know\", meaning they can self-assess their confidence in their answers. This ability, discovered in prior work [21], allows LLMs to indicate their certainty while generating tokens. In this section, we develop certaindex, measure of this confidence, as proxy for reasoning progress: high certainty suggests the LLM is nearing final answer or directly indicates lower absolute compute needed to reach final answer, whether correct or not. Our goal is to measure and track certaindex during inference, enabling dynamic resource allocation, prioritization of complex queries, scaling back simpler queries, and terminating unpromising queries. including semantic measures [11, 22], Various methods have been proposed to estimate LLM log uncertainty, probability entropy [21, 31], and hidden state-based indicators [4, 10, 44]. While certaindexs mathematical formulation varies across reasoning algorithms, its core interpretation remains the same: the LLMs certainty in its reasoning paths."
        },
        {
            "title": "3.1 Measuring Certaindex",
            "content": "This section presents two basic formulations of certaindex. Certaindex in typical reasoning algorithms. Reasoning algorithms like SC, MCTS, and Rebase expand multiple reasoning paths to derive aggregated final answers (2.1). To quantify LLMs certainty among these paths, we employ semantic entropy [22], which is derived from empirical entroy. Given question P, reasoning paths are generated and clustered into groups based on their answers: C1,C2,...,Cm, where Ci denotes the number of paths in answer group Ci. log Ci Ci The empirical entropy is calculated as: = . i=1 intuitively measures the diversity of answer distributions. higher indicates greater uncertainty, where reasoning paths diverge into many different answers. Conversely, lower suggests higher certainty, typically when most paths converge to the same answer (large Ci for some group i). The maximum entropy max(H ) = logn occurs when each path yields unique answer (m = n,Ci = 1 for all i). We normalize by max(H ) to obtain certaindex: = max(H )H max(H ) [0,1]. (1) For SC/Rebase/MCTS on exact-answer tasks such as arithmetic and multiple-choice, those exact answers can be extracted by applying string-matching on the generated outputs, and grouped based on their equality. In more open-ended generation tasks such as code (e.g., LiveCodeBench [19]) or flexible mathematical expressions [54], answer extraction becomes non-trivial. We employ small embedding models [41] (e.g., 100M parameters) to compute textual similarities between final outputs, and cluster reasoning paths based on semantic proximity, which enables efficient answer grouping across varying problem formats. Both clustering methods are computationally lightweight: string matching is fast and efficient; running the embedding model and calculating similarity remains computationally insignificant compared to LLM prefill and decode operations. Certaindex in reasoning algorithms with reward system. For reasoning algorithms that incorporate reward model (e.g., MCTS, Rebase), we simply use the reward models normalized output [0,1] as measure of certainty. This approach builds on prior research demonstrating that reward signals can effectively guide resource allocation in program ex4 Figure 3: Correlations between certaindex strength (y-axis) and ground truth steps to solution (x-axis) on 12 (algorithm, task dataset, LLM) settings where algorithm {SC, Rebase, MCTS, ICoT}, dataset {LiveCodeBench [19], GSM8K, ASDiv [34], GAME24 [54]}, and LLM {Llama [33], Gemma [46], Phi [2], QWQ [47]}. How certaindex is measured in each setting is shown in the label of each plot. Certaindex is measured at the reasoning step marked by the black line. The orange line indicates the thresholding-based allocation. The green line illustrates more fine-grained approach through curve fitting. For all plots (except MCTS), both certaindex values and oracle steps were averaged across multiple runs to combat randomness. ecution [45]. We collect the terminal reward scores from each reasoning path and aggregate them to compute certaindex. The aggregation method varies by algorithm: for MCTS, we use the mean reward across its different paths, while for Rebase, we take the maximum reward value. higher aggregated reward indicates stronger certainty in the reasoning paths validity, while lower score suggests uncertainty. These reward scores are collected during the normal execution of the reasoning algorithms and thus do not incur additional overhead. Combining multiple certaindex indicators. Certaindex can also be composed by multiple signals, provided they effectively capture certainty during the LLM reasoning process. When multiple signals are available, they can be combined by applying individual thresholds to each metric. For instance, in our MCTS/GSM8K experiments  (Fig. 3)  , we use two distinct metrics to measure certaindex: the reward score and the entropy-based measurement . Each metric is compared against its respective threshold, Rτ and Hτ respectively. program is considered to meet the certainty requirement only if all metrics exceed their thresholds."
        },
        {
            "title": "3.2 Effectiveness of Certaindex",
            "content": "This section empirically demonstrates that certaindex correlates strongly with the computational resources required to reach correct solutions across diverse models, reasoning algorithms, and task datasets. Higher certaindex values consistently indicate lower total compute needs. Correlation. We measure certaindex at intermediate reasoning steps and analyze its correlation with the ground-truth number of steps required to yield correct answers, as determined by an oracle. Queries are categorized as solvable or unsolvable, where unsolvable queries cannot produce correct answers even with near-infinite compute. Figure 3 illustrates this correlation across 12 (model, algorithm, task) settings. On solvable queries, Pearson Correlation values between certaindex and required compute range from 0.17 to 0.75 (mean 0.52), indicating strong correlation between high certaindex and fewer steps needed to solve query. We next demonstrate two potential resource allocation strategies using certaindex. Thresholding-based allocation. We measure certaindex at specific reasoning step, shown as red vertical line in each plot of Fig. 3. straightforward allocation strategy sets threshold value (orange horizontal lines) and halts inference for any query with certaindex exceeding t. In all plots, nearly no solvable queries fall in the upper-right area beyond the red and orange lines, indicating that an appropriately chosen can effectively early-terminate queries once their certaindex is greater than t. For these queries, this reduces resource usage for solvable queries and prevents over-allocation to unsolvable ones (red dots above the orange line), without sacrificing accuracy. Pareto-frontier allocation. Ideally, more refined allocation, depicted by the fitted green curve, dynamically determines the maximum inference compute allocation for each certaindex value. In addition to the thresholding-based allocation, which only impacts queries with certaindex above t, this allocation caps resources for all queries at any certaindex while preserving accuracy. We discuss the comparison of these two allocation strategies for specific dataset/model/reasoning algorithm combinations in 6.4. 5 Figure 4: Certaindex Values Across Different Detection Steps in Self-Consistency Reasoning"
        },
        {
            "title": "3.3 Comparing Certaindex with Other Signals",
            "content": "One may ask if there exist other signals better than certaindex. We compare certaindex with two alternative metrics for estimating resource needs. (1) Reasoning path length: Longer reasoning paths (more tokens) often indicate harder problems, suggesting potential correlation between path length and (2) Mean compute needs (e.g., more reasoning paths). normalized log probability: Established as measure of LLM confidence [31], higher log probabilities may correlate fewer samples needed for correct answers in SC. Using the (SC, GSM8K, Llama3.1-8B-instruct) setup, we evaluated certaindex alongside these metrics. Four metrics were tested: certaindexs entropy measure , mean output length [13], mean normalized log probability [31], and their linear combinations. As shown in Fig. 5, achieved the strongest correlation with ground truth compute requirements (Pearson Correlation of 0.68, Kendalls Tau of 0.61), outperforming other metrics and matching complex combinations. These results confirm that certaindex is simple yet effective proxy for estimating inference computational demands, offering robust performance across tasks and models. Our end-to-end token-to-accuracy evaluations in 6.5 compare different signals for resource allocation, confirming certaindexs superior performance. Figure 5: Correlation between certainty measurements and mean steps required to solve problems on solvable problems. We obtain the ground-truth mean steps by solving the queries using the LLM multiple times and counting the average steps."
        },
        {
            "title": "Certaindex and Detection Steps",
            "content": "Fig. 4 demonstrates the relationship between certaindex values and program steps across different detection points in single run, using SC on the GSM8K dataset with Llama3.1 6 8B Instruct model. The figure consists of six subplots, each representing different detection step ranging from 5 to 30, indicated by \"Detect @knob\" values. Each point in the plots represents an individual problem, categorized into three types: Early Stoppable Problems (pink), Solvable Problems (blue), and Unsolvable Problems (gray). The x-axis shows the number of steps taken, while the y-axis displays the certaindex value (C = ). horizontal orange dashed line indicates the certaindex threshold value, and vertical red dashed line marks the step at which certaindex is collected. The consistent pattern across all detection points demonstrates strong correlation between certaindex and required reasoning steps, with Pearson correlation coefficients exceeding 0.5 for solvable problems. As detection steps increase from 5 to 30, we observe 30% increase in early-stopped problems, indicating that higher certaindex values reliably predict when the LLM is converging toward final answer. Certaindex works as reliable proxy for reasoning progress. However, this improved accuracy trades off against potential compute savings, as later detection points leave less opportunity for early termination. Despite this tradeoff, certaindex proves to be reliable predictor of required reasoning steps across all detection timings, maintaining its effectiveness regardless of measurement timing."
        },
        {
            "title": "5 Dynasor: System Design",
            "content": "We present Dynasor, an end-to-end LLM serving system that optimizes reasoning programs using certaindex. Dynasor provides simple interface supporting diverse reasoning algorithms and integrates resource allocation and request scheduling for efficient LLM serving. Fig. 6a illustrates the architecture of Dynasor, which consists of three main components: (a) Reasoning Program Abstraction (Program) offering unified interface for reasoning algorithms, (b) an Application Runtime, which dynamically allocates resources based on certaindex, and (c) System Runtime, managing request-level scheduling on the backend. Dynasor is straightforward to use. Developers define reasoning programs through the provided abstraction, implementing certaindex and scaling knob control functions. These programs are submitted to the application runtime, which monitors certaindex values, dynamically adjusts resource Figure 6: Left(a): Dynasor Architecture. Middle(b): Reasoning Program Interface. Right(c): Example Program (SC). allocation, and forwards requests to the system runtime for execution. Programs either scale up for further computation or terminate early when resources are no longer allocated."
        },
        {
            "title": "5.1 Reasoning Program Abstraction",
            "content": "A reasoning program maintains three runtime properties: (1) certaindex, the certainty measure of the reasoning progress; (2) knob, the intrinsic scaling factor of the program; and (3) state, which stores the intermediate variables and results in previous steps. The Reasoning Program Abstraction (or Program) provides unified framework for developers to define variety of reasoning algorithms. It introduces narrow interface for the program to interact with the application runtime. Fig. 6a shows the structure of reasoning program. Developers only implement (1) update_certaindex (), which calculates and updates the certaindex at particular inference step, and (2) execute(), which runs the reasoning algorithm (Fig. 6b). Fig. 6c illustrate simple example implementation of SC: the update_certaindex () function computes the entropy across different branches, and the execute() function iteratively expands the generation. After each iteration, it aggregates the result and update certaindex. This process iterates until the program depletes its resources or exits."
        },
        {
            "title": "5.2 Application Runtime",
            "content": "5.2.1 Certaindex-Based Intra-Program Scheduler The certaindex-based intra-program scheduler controls the resource allocation of the program at runtime. The scheduler lifetime is described as follows: Initialization. When new program arrives, the scheduler initializes it with predefined maximum resource cap and allocates resources for its first run. It also establishes resource scheduling policy to guide future allocations. Resource Allocation. The scheduler continuously monitors each programs certaindex and uses it to determine resource allocation. As programs run, they repeatedly request resources from the scheduler while updating their certaindex to reflect the progress of reasoning. When multiple programs run concurrently, the scheduler can prioritize resource allocation between them via certaindex-based intra-program allocation policy. Termination. When programs certaindex exceeds limit based on its allocation policy or reaches the maximum resource cap, the scheduler denies further resources allocation for this program. The program then receives termination signal and aggregates results based on its generation history. We use the SC in Fig. 6c to illustrate the scheduler behavior. The program is first submitted to the application runtime, where the intra-program scheduler is initialized and assigned the initial resource (5 branches) to the program. SC sets simple certaindex threshold to determine whether the program should terminate. As the program expand, it updates certaindex, and request resource from the scheduler for next iteration. The scheduler checks the certaindex against the policy, and decide if it should allocate resource for the program to continue running. Resource allocation policy. resource allocation policy determines compute allocation (e.g., branches in SC and iterations in MCTS) for each program based on its certaindex. 3.2 discussed two certaindex-based allocation policies: simple thresholding and pareto-frontier, both implemented in the system. Additional alternatives are analyzed in 6.4. Developers can easily configure the intra-program scheduler to use other certaindex-based policies. 5.2.2 Profiler-Guided Policy Calibration We explain how to determine (or calibrate) resource allocation policy, focusing on certaindex threshold policy as our example. Determining an effective resource allocation policy is critical for reasoning program performance (6.4). Program submitters often lack insight into runtime patterns, and program characteristics may shift due to algorithmic changes or data distribution shifts. Dynasor provides an optional profile-based hyperparameter tuner to help users identify optimal scheduling policies for certaindex-based resource allocation. Users submit batch of programs with labeled data - such as verified answers to questions (e.g., math problem solutions), response rankings, or reward model scores. The profiler collects runtime metrics, including certaindex and token usage, to determine optimal resource allocation across different certaindex ranges while maintaining accuracy requirements. For threshold-based allocation, the profiler stops allocating resources when programs certaindex exceeds the threshold, and otherwise sets resources to maximum cap. The threshold is calibrated to meet the accuracy requirements (e.g., not hurt accuracy in all calibration data points). This calibration process can be 7 when decoding dominates the generation time, where is constant. While Gang scheduling alone significantly improves performance without causing starvation, adding SJF optimization can potentially lead to starvation of programs with longer estimated lengths. Gang scheduling alone already provides substantial performance benefits, making it viable standalone option. For deployments using SJF, we implement priority escalation mechanism where programs that have been waiting too long receive elevated priority, effectively preventing starvation. As demonstrated in 6.6, Dynasor promotes fairness compared to other baseline scheduling strategies. 5.3.2 Other System Runtime Components Prefix Cache Manager. Dynasor leverages prefix cache sharing. Independent branches of program share unified prompt KV-cache, while dependent reasoning chains reuse their longest common prefixes. For algorithms with reward models, Dynasor reuses prefixes during reward evaluation, reducing latency. The manager also handles automatic cache eviction. When memory is constrained, the KV cache of the programs with no active requests is assigned lower priority and evicted first. Program Context Manager is thin wrapper that tracks registered programs and their runtime characteristics. It provides cache eviction hints to the prefix cache manager based on program behaviors. Unlike the static program DAGs used in SGLang/ParrotServe, this component offers more flexibility by supporting dynamic generation patterns required by algorithms like MCTS and Rebase. Execution Backend manages the execution of LLM requests, optimizing model performance through techniques like CUDAGraph. This layer is designed to be adaptable to various execution engines including vLLM, TensorRT, and SGLang."
        },
        {
            "title": "5.4 System Implementation",
            "content": "We build Dynasor on top of SGLang (version 0.3.3 post1). The intra-program scheduler is implemented as Python library on the client side, while the inter-program scheduler is integrated into the server-side scheduler. Using Dynasor, we adapt various reasoning algorithms (2.1) to the Program interface, including their custom certaindex implementations. Our system comprises approximately 3,000 lines of Python code, covering the Program interface, application runtime, and system runtime. Adapting each reasoning program requires an additional 40 to 150 lines of code to define certaindex logic and integrate with the Program interface. This implementation overhead is minimal compared to the original implementations of these algorithms, which spans up to 4,000 lines of code."
        },
        {
            "title": "6 Evaluation",
            "content": "In this section, we evaluate Dynasor across four reasoning algorithms (SC [48] , Rebase [51], MCTS [12, 16], and ICoT) on diverse datasets [7, 17, 19, 34] in both offline and online Figure 7: Illustration of Gang Scheduling periodically executed in real-world serving scenarios to meet the dynamics of data distribution shifts."
        },
        {
            "title": "5.3 System Runtime",
            "content": "5.3.1 Program-Aware Inter-Program Scheduler The Program-Aware Inter-Program Scheduler optimizes scheduling and memory management at the program level, reducing per-program latency through two key strategies: (1) gang scheduling algorithm to prioritize requests originated from the same program, and (2) approximate Shortest-JobFirst (SJF) scheduling algorithm to reduce head-of-the-line (HoL) blocking and improve per-request latency. Gang Scheduling. Gang scheduling groups requests from the same program together to minimize stragglers and reduce overall completion time. Fig. 7 demonstrates the advantage of Gang scheduling over sequential scheduling using an example with two programs arriving at t=0, where the system has batch size of 2. Each program has two requests: program 1s requests take 4 ms each, and program 2s take 5 ms each. By prioritizing one program at time, Gang scheduling (left) reduces average latency from 9 ms to 6.5 ms compared to sequential scheduling (right). Approximating Shortest Job First (SJF). Our inter-program scheduler implements an approximating SJF scheduling algorithm to mitigate HoL blocking and improve average per-program latency. programs total execution time depends on two key factors: total compute requirements (knobs, e.g., number of branches/iterations) and token length per branch/iteration. While per branch/iterations exact LLM generation lengths cannot be known in advance [13], we can estimate token length per iteration by leveraging program locality and using historical averages from previous iterations. This estimation, combined with the compute requirements, helps predict total execution time. In Dynasor, certaindex controls the total compute requirements (deciding how many iterations to run), while SJF uses the estimated token length per iteration to optimize execution order. Starvation Prevention. Dynasor promotes finish-time fairness [30], which compares programs completion time in shared system (Tshared) to its estimated independent completion time (Tindependent). This metric naturally suits LLM serving as it accounts for generation length. We define finishtime fairness as φ = Tshared/#output tokens in our case, which is approximately proportional to the standard finish-time fairness since Tindependent can be estimated as k#output tokens 8 Figure 8: Token-to-accuracy metric on batch processing workloads. Mean performance and std (error bars) of 10 runs are reported. settings. Our experiments demonstrate that Dynasor consistently outperforms state-of-the-art LLM serving systems. Specifically, for offline workloads (6.2), Dynasor reduces token usage by up to 52.2% while maintaining accuracy requirements. In online serving (6.3), Dynasor handles up to 3.3 higher request rates and meets 4.7 stricter SLOs, meeting latency constraints for over 90% of requests (P90) compared to leading serving engines like SGLang [57] and ParrotServe [29]. Additionally, we perform ablation studies to analyze the contributions of different system components."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "General Settings. Tables 2 and 3 summarize our offline and online evaluation workloads, respectively. Table 1 lists the certaindex hyperparameters for each workload, including thresholds (tuned on test sets) and detection steps (detect @knob). In our main experiments, we use threshold-based policy (3.2 and 5.2.1): we scale knob and detect certaindex at the detection step (detect @knob), terminating the program if its certaindex values meet all threshold conditions. All experiments run on GPU cluster equipped with A100 (80GB) GPUs. GPU usage varies by method: Rebase and MCTS use two GPUs to separately serve their LLM and reward models, while SC requires only one GPU for request processing. For final answer selection, SC uses simple majority voting, Rebase applies weighted majority voting, and MCTS selects the best path using the reward model via best-of-n. Table 1: Hyperparameter configurations for certaindex. Rebase is evaluated on the MATH-OAI [28] subset of the MATH benchmark. Algorithm SC SC SC MCTS MCTS Rebase Rebase Dataset MATH GSM8K LiveCodeBench GSM8K ASDiv GSM8K MATH Thres. ( Hτ) Thres. (Rτ) Detect @knob / / / 0.4 0.4 0.99 / 0.7 0.7 0.4 0.99 / 0.85 0. 5 5 5 3 3 16 16 Batch processing and metrics. For offline workloads, we measure tokens-to-accuracy, the total number of generated tokens needed to achieve specific accuracy levels across 9 reasoning queries. This metric reflects the accuracy-cost trade-off, critical for end users as LLM platforms charge based on token usage. Table 2 details the offline experimental settings. Each problem is allocated maximum computation budget (parameterized by Resource Cap) defining the sampling branch limit for SC, with limit for Rebase, and iteration limit for MCTS. Resource caps correspond to different points on the same line in Fig. 8. The resource cap in our experiments may correspond to the reasoning_effort configuration parameter in o1 [37], as higher resource cap means higher accuracy. While scheduling methods may terminate programs early, they cannot exceed these caps. Online serving and metrics. For online workloads, we simulate query arrivals using dataset queries and assign deadlines to each query. System performance is evaluated by deadline attainment, the percentage of queries completed within their deadlines. We vary arrival rates and SLO scales to assess system performance under different conditions. Tab. 3 outlines the online experiment settings. Request arrivals follow Poisson process with varying rates. Deadlines are difficulty-aware, determined using simple policy: oracle difficulty for each query is estimated through sufficient number of trial runs for each algorithm-dataset combination, classifying queries as always correct (difficulty factor = 1), always incorrect (= 3), or variable (= 2). Deadlines are calculated as the product of the SLO scale, the difficulty factor, and base deadline, accounting for the additional time required for more challenging problems."
        },
        {
            "title": "6.2 Batch Processing",
            "content": "Baselines. For offline evaluation, we compare Dynasor against modified SGLang intra-program scheduler using the following policies: (1) baseline-even. It allocates resources uniformly across all reasoning programs using the resource cap settings in Tab.2. Different cap values result in different accuracy levels, as larger caps allow more computation for all problems. Specifically, for each algorithm, all queries receive identical resources: SC uses the same number of branches, Rebase uses the same width, and MCTS uses the same number of search iterations, as specified in Tab. 2. This baseline reflects the behavior of reasoning programs in current systems: user may allocate equal resources on all problems. (2) baselineTable 2: Offline workload Configurations. LiveCodeBench (LCB), Llama2 7B [12], Skywork7B [35], Llemma 7B and 34B [51] are fine-tuned models used in different settings as LLM/reward model. (Algo., Dataset, LLM) / (SC, LCB, Llama3.1 8B) / (SC, GSM8K, Llama3.1 8B) Skywork 7B (MCTS, ASDiv, Llama2 7B) (MCTS, GSM8K, Llama2 7B) Skywork 7B (Rebase, MATH, Llemma 7B) Llemma 34b (Rebase, GSM8K, Llemma 34B) Llemma 34b Reward Model # Samples Resource Cap 400 1000 300 300 500 500 5,10,15,20,25,30 5,10,15,20,25,30 3,7,10,15,20 3,7,10,15,20 16,32,64,128 16,32,64,128 Table 3: Online workload Configurations Algorithm Dataset LLM Reward Model Base Deadline (s) / SC MCTS Skywork 7B Rebase GSM8K Llemma 34B Llemma 34b MATH Llama3.1 8B ASDiv Llama2 7B 240 60 length. This policy uses the cumulative token count generated at specific step (specified as Detect@knob in Table 1) as the programs ending signal. The detection step matches the one used in the certaindex-based approach. The scheduler continues resource allocation if the historical token count over specified number of steps exceeds predefined threshold, indicating the problems difficulty requires longer reasoning chains with more tokens. Output token length is commonly used as scheduling indicator in existing LLM serving systems [13, 23, 50]. It also correlates strongly with the compute requirements (knob size) as we have discussed in 3.3. Fig. 8 compares the token-to-accuracy metric on different workloads. Our method consistently outperforms all baselines across different workloads by reducing token usage by 9-48% compared to baseline-even and 9-52% compared to baselinelength without sacrificing accuracy. On SC-LiveCodeBench, Dynasor can reduce token usage by 9.5% compared to baseline-even, and reduce up to 21% token usage compared to baseline-length, without accuracy dropping. On SC-GSM8K, Dynasor achieves up to 47.8% and 47% token savings vs. baseline-length and baseline-even, respectively. On MCTS, our method shows 9% and 11.8% fewer tokens usage compared to baseline-length, 13% and 13.1% fewer tokens usage compared to baseline-even on ASDiv and GSM8k respectively. For Rebase, we obtain 52.2% and 11.3% token reduction compared to baseline-length and baseline-even on Math, and 43.5% and 48% token reduction compared to baseline-length and baseline-even respectively on GSM8K. These gains primarily come from the intra-program scheduling algorithm, which accurately identifies high-certaindex programs and terminate them early without accuracy loss, as explained in 3 and calibrated in 5.2.2. This early termination strategy significantly reduces resource consumption by eliminating unnecessary sampling compared with baseline-even. In contrast, baseline-length leads to accuracy degradation even with less aggressive compute pruning, highlighting the effectiveness of certaindex-based resource allocation."
        },
        {
            "title": "6.3 Online Serving",
            "content": "Baselines. For online evaluation, we evaluate Dynasor against modified SGLang with different inter-program schedulers on P90 deadline attainment. SGLang [57]. SGLang represents strong baseline as it incorporates key optimizations in LLM serving: its longest-prefix matching (LPM) algorithm efficiently batches requests with common prefixes, while prefix caching reduces redundant computation. ParrotServe [29]. ParrotServe uses gang scheduling (App-FIFO) to prioritize requests in the same program. We implemented its scheduler on top of SGLang for fair comaprison. By grouping requests from the same program together, it minimizes context switches and maximizes KV cache reuse across batches. This baseline applies to gang scheduling at the program level. (a) Fig. 9 presents three key performance trade-offs: Program arrival rate, (b) SLO scale, and (c) Accuracy vs. SLO attainment. For experiments (a) and (b), we use fixed resource cap settings (SC: 20, MCTS: 15, Rebase: 128) to compare Dynasor with baselines, which lack adaptive compute scheduling for LLM reasoning programs. We measure SLO attainment while ensuring accuracy is maintained. Our method employs the same resource allocation policy validated in 6.2 and A, which preserves accuracy. In experiment (c), we explore the relationship between achievable accuracy and SLO attainment by varying resource cap settings. Rate vs. SLO attainment. Our system achieves much higher sustainable request rates under P90 deadline attainment: 1.6 3.3 and 1.6 3.2 compared to SGLang and Parrot respectively. These gains stem from two key factors. First, our intra-program scheduler identifies and early-terminates programs with high confidence, freeing resources for pending requests. Second, our inter-program scheduler prioritizes requests from the same program and enables program-level schedule (e.g., program-level SJF), increasing batch turnover rate. While our approach requires two rounds of execution with minimal knobs to detect certaindex, potentially impacting parallelism, it still delivers substantial performance gains: approximately 1.6 for SC and 3.3 for Rebase, even in these highly parallel workloads where requests could be issued all at once. As shown in Fig. 9, SGLang and Parrot achieve similar performance on MCTS due to its inherently lower parallelism, which limits the impact of scheduling optimizations. SLO scale vs. SLO attainment. Fig. 9(b) demonstrates how different systems maintain deadline attainment under fixed request rate (8, 16, 2 programs/s for SC, MCTS, Rebase). At these rates, Dynasor allows tighter SLO scales: 1.3 4.7 tighter than SGLang, and 1.73.3 tighter than Parrot. Accuracy vs SLO attainment. We measure system performance under fixed request rates (8, 16, 1 programs/s for SC, MCTS, Rebase) across different pre-configured resource cap settings (as in Tab. 2), where each resource cap determines the Figure 9: Evaluation on 3 online workloads on Dynasor against baselines. Rows from top to bottom: (a) Program arrival rate vs SLO attainment, (b) SLO scale vs SLO attainment, (c) Accuracy vs SLO Attainment. Table 4: Token consumption comparison of different scheduling strategies while maintaining accuracy Allocation Method SC/MATH MCTS/ASDiv Baseline Static Thres. (Ours) + Initial Step Curve Fit.(Ours) + 5-Step Thres. (Ours) + Single-Step Thres. (Ours) + Dynamic Curve Fitting (Ours) 1.18M 1.05M (-11.0%) 1.04M (-11.9%) 1.03M (-12.7%) 1.03M (-12.7%) 1.01M (-14.4%) 354K 308K (-13.0%) 306K (-13.6%) 307K (-13.3%) 306K (-13.6%) 298K (-15.8%) maximum computational budget and consequently affects serving accuracy. Fig. 9 (c) shows the trade-off between accuracy and SLO attainment across these configurations. On all three workloads, Dynasor shows 0.7% - 2% accuracy improvement compared to SGLang and Parrot under the same SLO attainment. This improvement stems from our ability to redistribute compute between simple and hard queries, enabling us to solve more queries while maintain similar SLO attainment that baselines could not match unless given much more compute. Throughput. Our system shows equal average throughput (token per second) in all online settings to baselines. This is because that under the given request rates, all workloads saturate the GPU memory and making the system memory bound. This also validates the introduced scheduler has no observable overhead."
        },
        {
            "title": "6.4 Fine-grained Resource Allocation",
            "content": "We compare simple thresholding against fine-grained resource allocation in this section. 6.2 and 6.3 adopt simple static threshold mechanism: extract certaindex at fixed step and decide if to terminate program based of the value of certaindex. While effective, there is room for optimization through more sophisticated scheduling strategies. To explore this, we conducted experiments in offline batch processing using the setting (SC, MATH) and (MCTS, ASDiv), both with maximum step limit of 20 as per-query resource cap, to evaluate more complex allocation strategies that enable early termination while maintaining accuracy. The results, measured in token savings, are presented in Tab. 4. For the static threshold, we collected certaindex and apply the threshold in Tab. 1. For all following methods, we calibrate the mapping between certaindex and compute requirements across 10 different runs, ensuring no accuracy drop for each method in all runs. We then evaluate different methods token savings by measuring compute requirements for each problem in the dataset using their empirical certaindex. We first examine an Initial Step Curve Fitting approach, which fits skyline curve (exemplified by the green lines in Fig. 3) using certaindex values collected at the same steps as the static threshold. Although this method enables finer-grained resource allocation for varying certaindex values, it relied on early steps certaindex values, which may be inaccruate as reasoning progresses, resulting in marginal improvements (less than 1% compute savings) compared to simple static thresholding. To enhance prediction accuracy, we test more frequent certaindex collection: every 5 steps (5-Step Thres.) and every step (Single-Step Thres.), based on which we similarly apply threshold filtering or skyline curve fitting at every step (Dynamic Curve Fitting). These approaches achieve higher token savings (up to 3.4% over static threshold). However, implementing them introduces scheduling trade-offs in real-world deployment due to their impact on scheduling and parallelism. Specifically, frequent certaindex collection may disrupt the concurrent execution of reasoning programs. For instance, in SC, instead of running 20 samples concurrently, we must process them in sequential batches (e.g., 4 batches of 5 samples) to track the certaindex. This shift from parallel to sequential execution sub11 stantially increases latency. Our benchmark shows an increase from 289s to 366s in mean latency when serving 500 programs. Given these practical constraints, we opt to implement the simple static threshold in our end-to-end experiments, prioritizing system performance over marginal token savings; but note in applications which prioritize cost over latency, such allocation strategies remain effective and are implemented in Dynasor."
        },
        {
            "title": "6.5 Ablation Studies",
            "content": "We ablate the effectiveness of two core proposed techniques: resource scheduler and gang scheduling algorithm. Components Contributions. The speedup of the system comes from two major sources: (1) intra-program scheduling with dynamic resource allocation and (2) inter-program scheduling leveraging program-level locality. Figures 10 and 11 show the contribution of different components using SC workloads on GSM8K and MATH datasets, measuring mean latency and maximum sustainable rate under the same P90 attainment, respectively. Using LPM (SGLang) as the baseline, we analyze three factors: gang scheduling, SJF, and certaindexbased resource management. Gang scheduling improves latency and throughput in both uniform and certaindex-aware resource allocation by ensuring the requests in the same program are scheduled together, reducing resource fragmentation and context switching overhead. For GSM8K workload, certaindex-based resource management dominates the improvements, reducing mean latency by up to 60% through token savings of up to 50%, while Gang scheduling and SJF contributions are more modest. For MATH workload, certaindexaware allocation achieves 1.2x peak rate through early termination of low-value computations (as only 10% tokens are saved on MATH), while SJF provides the most significant gain with 1.9x peak rate by prioritizing shorter jobs and reducing head-of-line blocking. These results validate the effectiveness of our two-level scheduling approach: intra-program optimization through certaindex-aware allocation, combined with inter-program optimization through gang scheduling. Figure 10: Performance improvement breakdown in online self-consistency (GSM8K) workload: Impact of gang scheduling, certaindex-based resource allocation, and SJF on mean latency given fixed request rate of 16 pps. Choosing Different Thresholds. We show that certaindex threshold selection is vital. Fig. 12 demonstrates the impact of different certaindex thresholds. For SC on GSM8k, we selected an entropy (H ) threshold of 0.5, as higher thresholds led to overly aggressive termination and accuracy degradation. 12 Figure 11: Performance improvement breakdown of online self-consistency (MATH) under fixed P90 SLO constraints. Figure 12: Performance comparison with different entropy threshold or reward score threshold. Similarly, for MCTS on ASDiv, we chose score (R ) threshold of 0.4, as lower thresholds (<0.4) decreased accuracy while higher thresholds (>0.4) increased token consumption without proportional accuracy gains. These results highlight the importance of careful threshold policy calibration in balancing compute efficiency and accuracy as we discussed in 5.2.2. Comparison to other signals. We further compare our certaindex-based approach with two alternative signals that may guide inference-time compute allocation: lengthbased [13] and LLM activation-based methods [8]. The first has been discussed in 3.3 and 6.2 and is simple and popular scheduling indicator [13, 23] in existing systems. We carefully tune it with varying lengths. The activation-based method [8], which uses neural network predictor trained on LLM activation with workload-specific data, to predict queries resource requirement. Conceptually, it shall capture similar or more signals than what certaindex provides. In Figure 13, our certaindex-based method consistently outperforms both across workloads, even achieving higher accuracy with lower number of tokens. The length-based method, which terminates based on generation sequence length, either sacrifices accuracy with higher cutoffs or shows minimal token savings with lower cutoffs. The activation-based method shows accuracy degradation in SC tasks and poor token efficiency in MCTS model-based resource predictor is non-trivial to train and suffer from query distribution shift. These comparisons validate the simplificy and effectiveness of our certaindex-based resource scheduling and align with our findings in 3.3 that certaindex is more powerful in guide reasoning program resource allocation. Figure 13: Performance comparison with LLM activationbased predictor and output length based scheduling."
        },
        {
            "title": "6.6 Fairness Analysis",
            "content": "We evaluate finish-time fairness (5.3.1) using the ratio of latency to baseline token count, where baseline tokens represent the amount needed when all programs run to their maximum resource cap - this is equivalent to the actual tokens used in systems without our optimization, and serves as the reference point for systems using our optimization. Fig. 14 shows the comparison on certaindex-based resource scheduling, gang scheduling, and SJF on finish-time fairness on MATH using SC with rate of 8 pps using Llama3.1 8B. Gang scheduling shows consistent fairness improvement compared to non-gang scheduling. This shows that prioritizing existing programs can improve finish-time fairness. Certaindex-based resource allocation also consistently improve finish-time fairness due to resource cutting by the intra-program scheduler. Adding SJF shows fairness improvement at the later 50% fraction of the job compared to without SJF, and at the later 65% fraction of job compared to LPM. In all case, SJF shows the fairness metric no worse than even resource allocation. Note that despite improvements in finish-time fairness, starvation may still occur. We provide options to mitigate this issue, as discussed in 5.3.1. Since GPU memory capacity is an important resource consideration in todays LLM serving systems, we also study the how varying GPU memory affects the SLO attainment, and defer the experiments to Appendix B."
        },
        {
            "title": "6.7 Case Study: Serving o1-like LLMs",
            "content": "We conduct case study to evaluate Dynasors effectiveness in serving models with reasoning behaviors similar to OpenAIs lastest o1. Since o1 is proprietary, at the time of submission, we use the closest open source LLM Qwen-QWQ [47], which is trained with internalized CoT and shows comparable performance to o1. We find QWQ generates lengthy single CoT paths with continuous reflection. We evaluate the models tokens-to-accuracy performance by varying token generation limits across 500 GSM8K problems. The original model, which only produces answers after completing entire CoT paths, showed poor tokens-to-accuracy performance under lower token limits (green curve in Fig. 15). This limitation arises because the model frequently fails to reach conclusions within constrained budgets. To improve it, we let QWQ generate 32 tokens for each Figure 14: Finish-Time Fairness. Figure 15: Serving Qwen-QWQ using Dynasor. problem per iteration and inject Final Answer prompt to instruct the model to elicit answers at the end of every iteration (QWQ Guided). This approach reduces token consumption by 30% while maintaining the same accuracy. ) of Finally, we monitors the certaindex (specifically, answers from different iterations and terminates generation when the certaindex exceeds confidence threshold (QWQ Cut). QWQ Cut achieves an additional 14% reduction in token consumption without accuracy loss, demonstrating the effectiveness of our approach for efficiently serving latest models where reasoning behaviors are internalized."
        },
        {
            "title": "7 Related Work",
            "content": "We discuss more related work besides those in 2.3. Goodput-Optimized Serving Systems. Optimizing goodput has become recent focus in machine learning systems. Symphony [6], Nexus [43], Clockwork [14], and Shepherd [56] adopt eager scheduling with preemption to improve the goodput of serving traditional deep neural networks. Conceptually, the closest to our work is Pollux [40], which optimizes goodput objective that balances the system throughput and statistical efficiency at training. Dynasor can be also seen as goodput-optimized system, but focused at inference. It dynamically manages inference-time compute for each reasoning query to minimize latency and maximize throughput while preserving overall accuracy. Adaptive resource allocation for LLM queries. Recent studies [8, 26, 32, 36, 45] explore adaptive compute allocation for LLM inference based on query difficulty. RouteLLM [36] routes queries between multiple LLMs to reduce cost, but it requires prior knowledge of traffic patterns. Capability-Aware Self-Evaluation [32] estimates the likelihood of failing to generate preferred responses. Damani et al. [8] predicts the reward distribution for inputs under compute budget, allocating more resources to promising queries. However, this approach depends on trained predictors that are hard to obtain and vulnerable to query distribution shifts. Sun et al. [45] 13 examine the relationship between reward model outputs and inference-time resource allocation, providing an alternative measurement of certaindex. Specific to SC, Li et al. [26] show that many SC programs can be early exited using sliding window detector without compromising accuracy, echoing our findings. Building on these ideas, we propose certaindex, simple, plug-and-play indicator that generalizes across diverse reasoning algorithms and datasets."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced Dynasor for adaptively orchestrating inferencetime compute for LLM reasoning programs. By leveraging certaindex as proxy for reasoning progress, Dynasor optimizes compute allocation across diverse reasoning queries. Our evaluations demonstrate significant performance gains over existing systems. We hope Dynasor sets new bar for serving the emerging LLM reasoning programs."
        },
        {
            "title": "References",
            "content": "[1] Deepspeed model implementations for inference (mii), 2023. [2] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [3] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369, 2023. [4] Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin Edelman. Distinguishing the knowable from the unknowable with language models. arXiv preprint arXiv:2402.03563, 2024. [5] DeepMind AlphaProof and AlphaGeometry Teams. Ai achieves silver-medal standard solving international mathematical olympiad problems.25 july 2024, 2024. [8] Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, and Jacob Andreas. Learning how hard to think: Input-adaptive allocation of lm computation. arXiv preprint arXiv:2410.04707, 2024. [9] Deepseek. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power, 2024. [10] Hanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of llms hidden states. arXiv preprint arXiv:2402.09733, 2024. [11] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. [12] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. [13] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. Efficient llm scheduling by learning to rank. arXiv preprint arXiv:2408.15792, 2024. [14] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. Serving DNNs like clockwork: Performance In 14th USENIX predictability from the bottom up. Symposium on Operating Systems Design and Implementation (OSDI 20), pages 443462. USENIX Association, November 2020. [15] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1813518143, 2024. [16] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. [6] Lequn Chen, Weixin Deng, Anirudh Canumalla, Yu Xin, Danyang Zhuo, Matthai Philipose, and Arvind Krishnamurthy. Symphony: Optimized dnn model serving using deferred batch scheduling, 2024. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [18] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference, 2024. 14 [19] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [20] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [21] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. [22] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Linguistic invariances for Semantic uncertainty: uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Efficient serving of llm-based applications with semantic In 18th USENIX Symposium on Operating variable. Systems Design and Implementation (OSDI 24), Santa Clara, CA, July 2024. USENIX Association. [30] Kshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram Venkataraman, Aditya Akella, Amar Phanishayee, and Shuchi Chawla. Themis: Fair and efficient {GPU} cluster scheduling. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pages 289304, 2020. [31] Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650, 2020. [32] Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation, 2024. [33] AI Meta. Introducing llama 3.1: Our most capable models to date. Meta AI Blog, 2024. [34] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021. [35] Skywork o1 Team. Skywork-o1 open series, November 2024. [36] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. [25] Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and Yu Philip. Large language models in law: survey. AI Open, 2024. [37] OpenAI. Chat completion api reference. https: //platform.openai.com/docs/api-reference/ chat/create, 2024. Accessed: 2024-12-19. [26] Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. Escape sky-high cost: Early-stopping self-consistency for multistep reasoning. arXiv preprint arXiv:2401.10480, 2024. [27] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph Gonzalez, et al. Alpaserve: Statistical multiplexing with model parallelism for deep learning serving. arXiv, 2023. [28] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [29] Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, and Lili Qiu. Parrot: [38] OpenAI. Learning to reason with llms, 2024. [39] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2023. [40] Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, and Eric P. Xing. Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), pages 118. USENIX Association, July 2021. [41] Reimers. Sentence-bert: Sentence embeddings arXiv preprint using siamese bert-networks. arXiv:1908.10084, 2019. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [43] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. Nexus: gpu cluster engine for accelerating dnn-based video analysis. In ACM SOSP, 2019. [44] Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. The curious case of hallucinatory (un) answerability: Finding truths in the hidden states of over-confident large language In Proceedings of the 2023 Conference on models. Empirical Methods in Natural Language Processing, pages 36073625, 2023. [45] Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290, 2024. [46] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [47] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [48] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [50] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference arXiv preprint serving for large language models. arXiv:2305.05920, 2023. [51] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Inference scaling laws: An emand Yiming Yang. pirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. [52] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [53] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. [54] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. [55] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for {Transformer-Based} generative models. In USENIX OSDI, 2022. [56] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. Shepherd: Serving dnns in the wild. 2023. [57] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. arXiv preprint arXiv:2312.07104, 2024. [58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving, 2024. 16 Token-to-accuracy Performance of Dynasor on MATH using SC Figure 16 presents the token-to-accuracy results using Llama3.1 8B Instruct model, where resources are allocated by Dynasor using the threshold and detect @knob configurations described in Tab. 1. Our proposed method achieves the same accuracy while reducing computational costs by 11%. Memorys Affects on Dynasor Fig. 17 shows how varying GPU memory availability (2550% of the total 80GB) affects the P90 SLO attainment when serving Llama-3 8B SC/GSM8K. Our system (with gang scheduler) consistently outperforms all baselines across different memory configurations. Removing the gang scheduler from our system reduce the deadline attainment by 1321% across differnet memory constraints. Similarly, comparing SGLang (w/o gang) and Parrot (with gang), the deadline attainment can vary among 28 46% across different memory utilization. These results demonstrate the effectiveness of gang scheduling in the inter-program scheduler for improving deadline attainment in reasoning tasks under different memory constraints. Figure 16: Token-to-accuracy Performance Using SC on MATH Figure 17: Dynasor outperforms baselines in different memory settings"
        }
    ],
    "affiliations": [
        "Snowflake",
        "Tsinghua University",
        "UC San Diego"
    ]
}