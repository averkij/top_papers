{
    "paper_title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly",
    "authors": [
        "Zhaowei Wang",
        "Wenhao Yu",
        "Xiyu Ren",
        "Jipeng Zhang",
        "Yu Zhao",
        "Rohit Saxena",
        "Liang Cheng",
        "Ginny Wong",
        "Simon See",
        "Pasquale Minervini",
        "Yangqiu Song",
        "Mark Steedman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 0 1 6 0 1 . 5 0 5 2 : r MMLONGBENCH: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly Zhaowei Wang1 Wenhao Yu2 Xiyu Ren1 Jipeng Zhang1 Yu Zhao3 Rohit Saxena Liang Cheng3 Ginny Wong5 Simon See5 Pasquale Minervini3,4 Yangqiu Song1 1CSE Department, HKUST 2Tencent AI Seattle Lab Mark Steedman3 3University of Edinburgh 4Miniml.AI 5NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA {zwanggy, yqsong}@cse.ust.hk m.steedman@ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "The rapid extension of context windows in large visionlanguage models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in single forward pass. In this work, we introduce MMLONGBENCH, the first benchmark covering diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLONGBENCH is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via cross-modal tokenization scheme that combines vision patches and text tokens. Through thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide comprehensive analysis of the current models vision-language longcontext ability. Our results show that: i) performance on single task is weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLONGBENCH1 provides the missing foundation for diagnosing and advancing the next generation of LCVLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in long-context modeling unlocked wide array of new capabilities for both large language models [LLMs; 1, 2] and large visionlanguage models [3, 4, LVLMs;]. In particular, long-context visionlanguage models (LCVLMs) represent an important step forward by enabling LVLMs to process hundreds of images and thousands of interleaved text tokens in single forward pass. This allows applications such as document-level visual question answering [5], multi-hop reasoning across web pages [6], and instruction following grounded in complex visual contexts [7, 8]. To support such capabilities, researchers have proposed various techniques to extend the context windows of LVLMs, as seen in models such as LongVILA [9] and GPT-4o [10]. However, the development of effective evaluation benchmarks is lagging behind. It remains unclear how well current LCVLMs perform in long-context settings, what types of tasks they struggle with, and how robust they are to input length variation. Here, we take closer look and find that existing benchmarks suffer from the following shortcomings and provide summary of key feature comparisons in Table 1: 1The code and data are available at https://github.com/EdinburghNLP/MMLongBench. Preprint. Under review. Table 1: Comparison of benchmarks for long-context vision-language models: MM-NIAH [18], Visual Haystack [16], MMNeedle [11], MMLongBench-Doc (MMLB-Doc) [5], M-Longdoc [21], LongDocURL [17], and our MMLONGBENCH. Note that Summ refers to summarization, and DocVQA stands for document visual question answering (VQA). denotes the number of input tokens, and Mixed indicates that the dataset includes both natural and synthetic images. Type of tasks Benchmark features VRAG NIAH ICL Summ DocVQA Image Type Control Multiple MM-NIAH [18] Visual Haystack [16] MMNeedle [11] MMLB-Doc [5] M-Longdoc [21] LongDocURL [17] MMLONGBENCH (Ours) Mixed Natural Natural Synthetic Synthetic Synthetic Mixed Limited coverage of downstream tasks: Existing benchmarks predominantly focus on single type of long-context vision-language task, such as needle-in-a-haystack (NIAH) [11] or longdocument VQA [5]. However, performance on single type of task cannot reflect the broader long-context visual reasoning capabilities required for various downstream applications [12]. Other applications, like long-context RAG [13], many-shot in-context learning [14], and longdocument summarization [15] are entirely absent from current evaluations. Insufficient coverage of image types: Most existing benchmarks focus narrowly on either natural images [11, 16] such as photographs of everyday scenes, objects, or people or synthetic images [5, 17], such as scanned documents, web pages, or app screenshots. This narrow focus leads to an incomplete understanding of model performance across diverse image types. Lack of context length control: Existing benchmarks miss consensus on cross-modality length control, especially image tokens. For example, MM-NIAH [18] follows InternVL1.5 [19] to compute text and image tokens together, but other works, such as Visual Haystack [16] and LongDocURL [17], only report the number of images as the context length. This inconsistency makes it difficult to compare model performance across different benchmarks. Single context length: Many long-context benchmarks with text-only inputs [12, 20] standardize context lengths to few values (e.g., 8K, 32K, 128K) and provide each example with contexts at each standard length. Hence, model developers can easily know the performance change with different lengths. However, such practice is not followed in LCVLM evaluations. For example, in MM-NIAH [18], models are evaluated on long essays (such as web pages) with randomly varying lengths, which complicates systematic analysis of context length effects. To enable comprehensive evaluation, in this paper, we introduce MMLONGBENCH, benchmark covering diverse set of long-context vision-language tasks across five different categories. Specifically, in addition to multimodal Needle-in-a-haystack (NIAH) and long-document VQA (DocVQA) tasks, we also include visual retrieval-augmented generation (VRAG), many-shot in-context learning (ICL), and Summarization in our benchmark. VRAG examples are drawn from knowledge-based VQA datasets [22, 23] using Wikipedia passages to populate long contexts. ICL examples are image classification problems in four domains [2427], which require models to perform on-the-fly classification based on hundreds of in-context examples. In the summarization task, models are required to summarize image-based PDF documents [15, 28]. Overall, our benchmark includes diverse downstream tasks and image types to enable comprehensive evaluation. In MMLONGBENCH, we use unified token-count method which counts image tokens based on the number of patches produced by current vision encoders, followed by 2 2 pixel unshuffle. This approach is consistent with practices adopted in most recent models, such as Qwen2.5-VL [29] and InternVL3 [30], making it well-suited for long-context evaluation. For standard lengths, we equip all examples with five different lengths of context, ranging from 8K to 128K tokens, enabling thorough analysis of performance changes as the context length increases. Further, we also ensure all datasets are easily extendable to longer contexts. Finally, to understand the progress of LCVLMs and how different multimodal long-context capabilities correlate with one another, we evaluated 46 models of various architectures, scales, and 2 training approaches. Our analysis reveals three key findings: i) performance on single task poorly reflects overall long-context ability; ii) while closed-source models show higher scores, long-context vision-language tasks present significant challenges for both closed-source and open-source models, highlighting the need for future improvements; and iii) models with stronger reasoning ability tend to exhibit better long-context capabilities, as exemplified by the thinking versions of Gemini models. Furthermore, our error analysis shows that Optical Character Recognition (OCR) and cross-modality retrieval abilities remain bottlenecks in current LCVLMs. Overall, our benchmark underscores the importance of evaluating LCVLMs across comprehensive long-context vision-language tasks. We hope these insights will help guide future model development and evaluation."
        },
        {
            "title": "2 Related Work",
            "content": "Long-context vision-language models (LCVLMs). The context window of LLMs has experienced fast growth from less than 8K [31, 32] to 128K tokens [1, 10] or more [33, 34]. To support this, techniques such as longer pre-training length [1, 35, 36], position extrapolation [3739], and efficient architectures [4042] have been developed. With this progress, recent literature also investigated how to extend the context length of LVLMs to build LCVLMs, such as Gemini-2.5 [43], Qwen2.5VL [29], and others [3, 30, 44, 45]. In addition, several recent works on LVLMs made efforts to compress vision tokens to accommodate longer input sequences [4651]. Meanwhile, growing body of works has adopted various techniques from LLMs to extend LVLMs context length, such as position extrapolation [52] and more efficient model architectures [53]. With extended context lengths, LCVLMs can support various applications, such as multi-hop reasoning across web pages [6] and instruction following grounded in complex visual contexts [7, 8, 54]. Long-context benchmarks. Needle-in-a-haystack (NIAH) [55] is one of the first commonly adopted tasks to evaluate the text-pure long context ability of LLMs, as it can be procedurally generated with arbitrarily long lengths and needle position [56]. This task inserts needle at specific depths of long essay and tests models ability to recall it. Recent works have also extended the NIAH task to more complex versions [5759]. However, several benchmarks [12, 20] discover that using single NIAH task only partially reflects LLMs overall long-context ability. As result, numerous benchmarks with broad coverage of diverse downstream applications have been constructed [12, 14, 20, 6063] to provide comprehensive evaluation. In contrast, the evaluation of LVLMs long-context capability remains limited. Existing benchmarks only involve either NIAH [11, 16, 18, 64] or long-document VQA [5, 17], lacking comprehensive coverage across diverse vision-language applications. As result, frontier LCVLMs [10, 33] only report long-context performance on other modalities, such as video [9, 65, 66] or audio [10, 33], and neglect the prevalent use cases of long-context vision-language inputs. While recent MileBench [67] claims to be comprehensive long-context benchmark with various text-image tasks, our closer inspection reveals that it actually contains lot of short-context tasks, and the average length is only about 9K tokens. Datasets like DocVQA[68], WebQA [69], and OCRVQA [70] contain only one image per sample and minimal context, making MileBench unqualified as true long-context benchmark. In this work, we introduce the first comprehensive benchmark that evaluates wide range of vision-language downstream applications across five standardized input lengths."
        },
        {
            "title": "3 Our Benchmark — MMLONGBENCH",
            "content": "In our work, we seek to address the limitations of current benchmarks by meeting the following criteria: i) broad coverage of both diverse vision-language downstream tasks and different image types, ii) unified token-count method across different modalities and datasets, and iii) multiple standardized context lengths for each example, ranging from 8K to 128K tokens. In this section, we describe the task categories and datasets included in MMLONGBENCH, highlighting how they address the limitations of existing evaluation benchmarks. An overview of MMLONGBENCH is provided in Table 2, and several concrete examples are given in Appendix E. 3.1 Diverse Real Long-Context Applications for LCVLMs Visual retrieval-augmented generation (VRAG) evaluates an LCVLMs ability to ground on relevant information retrieved from large corpus, while filtering out distractors and irrelevant 3 Table 2: Overview of datasets in MMLONGBENCH. We include datasets covering key long-context capabilities, with 13,331 examples in total. Image types are shown per dataset; Mixed indicates both natural and synthetic images. SubEM and Acc indicate substring exact match and accuracy. Category Visual RAG Dataset InfoSeek ViQuAE Metrics SubEM SubEM Image Natural Natural Size Description 1,128 Long-tail entity question answering 1,144 Question answering based on TriviaQA Needle-in-ahaystack Many-Shot In-Context Learning Summarization VH-Single VH-Multi MM-NIAH-Ret MM-NIAH-Count MM-NIAH-Reason Natural Acc Acc Natural SubEM/Acc Mixed Acc Mixed SubEM/Acc Mixed 1,000 Retrieve an image from an album 1,000 Retrieve multiple images from an album 1,200 Retrieve text/image needles in web pages 1,178 Count text/image needles in web pages 1,158 Reason about text/image needles in web pages Stanford Cars Food101 SUN397 iNat2021 GovReport Multi-LexSum Acc Acc Acc Acc Model-based Model-based Natural Natural Natural Natural Synthetic Synthetic Synthetic Synthetic Synthetic 458 500 500 500 241 50-category car classification 50-category food classification 50-category scene classification 50-category species classification Summarizing government reports in PDF Summarizing multiple legal documents in PDF 961 Long PDF document VQA 1,153 Long PDF document VQA 1,064 Slide deck understanding and reasoning Long-Document VQA MMLongBench-Doc LongDocURL SlideVQA SubEM/Acc SubEM/Acc SubEM/Acc content. To evaluate this capability, we use factual knowledge-based VQA as representative task. This task requires answering questions about the named entity identified in an image, such as Who designed the building in this picture? We include InfoSeek [22] and ViQuAE [23] in this category. To build long context, we insert the gold passage(s) (the passage with the answer) among large set of distracting passages retrieved from Wikipedia. For ViQuAE, we use gold passages from KILT [71] as it is constructed upon TriviaQA [72]. For InfoSeek, we choose the lead section of the named entitys Wikipedia page as the gold reference and remove all examples for which the answer cannot be found. Then, we split Wikipedia pages into 100-word passages and incrementally add retrieved passages that do not contain the answer or the named entity as distractors until we reach the given input length L. For retrieval, we use the named entity instead of the image itself as the query, because text-based retrieval achieves higher recall and provides harder distractors. In ViQuAE, each example contains single gold passage, and we insert it at six evenly distributed positions, while in Infoseek, the lead sections often contain hundreds of words, resulting in multiple passages. We randomly shuffle them into three permutations. We use the substring exact match (SubEM) as the metric, following previous work [73]. See more details in Appendix A.1. Needle-in-a-haystack (NIAH) measures how well an LCVLM can recall small but important piece of information embedded within long sequence of mostly unrelated visual and textual inputs. Those tasks have been widely adopted because they are easy to build (can be procedurally generated with long corpus) and simple to control (can combine any context length and needle position). For this category, we select multiple tasks from Visual Haystack [VH; 16] and MM-NIAH [18]. VH requires models to retrieve images of target objects (the needle) in an image haystack. It is available in two versions: VH-Single and VH-Multi, for finding single image and multiple images. MM-NIAH contains retrieval (Ret), counting (Count), and reasoning (Reason) tasks in interleaved text and images; each task features both text and image needles. In VH, we obtain the needle images and the target objects from the original datasets. Then, we accompany these images with multiple negative distractors until the image haystack reaches given input length L. We report accuracy as the metric following the original work [16]. In MM-NIAH, the haystacks are composed of web documents [74] with interleaved text and images. We include all three tasks of retrieval, counting, and reasoning in our benchmark. Similar to VRAG, we split text in each web document into 100-word passages and add passages and images so that the context achieves the input length. We use SubEM and accuracy to evaluate retrieval and reasoning tasks following the original paper [18]. In the counting task, we use the accuracy of summed needle counts for better robustness. Refer to Appendix A.2 for more details. Many-shot in-context learning (ICL) tests the models capability to adapt to new multimodal tasks on the fly by observing multiple in-context examples, without requiring any parameter updates. Following prior work on long-context LLMs [20, 75, 76], we focus on image classification datasets with large label spaces. Here, we collect four datasets with diverse domains: Stanford Cars [24] for cars, Food101 [25] for food, SUN397 [77] for scenes, and iNat2021 [27] for species. We adjust the 4 number of shots to control the input length L, and the number of exemplars in each class is balanced. The 128K context window can accommodate approximately 500 images. To ensure sufficient shots per class, we randomly sample 50 classes from each dataset. We report accuracy on each dataset. One difference from existing work on many-shot ICL with LCVLMs [78] is that we map the original natural language labels (e.g., food names) to class IDs (e.g., 0, 1, ...), requiring models to learn new tasks rather than relying on pre-training knowledge. Appendix A.3 covers more details. Summarization (Summ) evaluates an LCVLMs ability to generate concise outputs from long multimodal documents while preserving all salient information. We choose GovReport [15] (government report summarization) and Multi-LexSum [28] (multi-document legal summarization), as their PDF-formatted documents are long and easily accessible. Our evaluation provides models with PDF-formatted documents rather than OCR-extracted text used in previous works [79, 80]. We truncate the document from the end based on the input length L. Following previous work [20], we use LLM-based evaluation for both datasets instead of the commonly used ROUGE-L, as it better reflects human judgment. More details, such as the LLM-based metric, are provided in Appendix A.4. Long-document VQA assesses the models aptitude for answering questions that require reasoning over information dispersed across multiple images and text segments within an extended document. We include commonly adopted datasets for evaluating long-document VQA: SlideVQA [81], MMLongBench-Doc [5], and LongDocURL [17]. For documents longer than input length L, we truncate the documents evenly from both sides while keeping the answer pages. For shorter documents, we alternately pad the left and right sides with randomly sampled negative documents up to length L. However, the padding documents may occasionally contain information related to the question and potentially change the answer. To ensure the validity of questions, we preface each question with the prompt Based on the Document <Original Doc ID>, answer the following question. We follow the metrics used in LongDocURL but remove questions with long answers, thereby avoiding LLM-based answer extraction. We list specific details in Appendix A.5. 3.2 Cross-Modality Token Counting Various long-context applications of LCVLMs usually involve varying text-to-image ratios. For example, VRAG contains only one image related to named entity, whereas the context in LongDocument VQA primarily consists of images. When building comprehensive benchmark for LCVLMs, the initial challenge lies in standardizing the context length of diverse datasets with different text-image combinations. In this work, we count both text tokens and visual tokens together as the total input length of L, in contrast to prior works [11, 16, 64] that simply use the image number as context length. We use the Llama2 tokenizer [31] to calculate the number of text tokens following previous practice [20]. To count image tokens, we divide each image into 14 14 patches and apply 2 2 pixel unshuffle to compress the visual token number. Note that this patch size and the pixel unshuffle operation are both commonly adopted in current LVLMs [19, 29, 30, 44, 47, 51, 82, 83]. This method ensures compatibility with modern LVLMs, making it well-suited for evaluation. 3.3 Standardized Input Length The input length is an important factor to consider when we evaluate models long context ability, as longer inputs can provide more information but also challenge models to filter out distracting information. As aforementioned in Section 3.1, we can control the input length for each dataset either by adjusting the number of passages, images, or exemplars, or by truncating the PDF-formatted documents. This allows us to present each example in our benchmark under multiple standardized input lengths and better understand how performance changes as the context length increases. Specifically, our benchmark provides five input lengths L: 8K, 16K, 32K, 64K, and 128K tokens, using binary prefixes = 210, and the input length can be easily extended beyond 128K if needed."
        },
        {
            "title": "4 Evaluation and Analysis",
            "content": "With broad task coverage, unified token counting, and standardized input length, we are now able to thoroughly examine LCVLMs long-context ability across multiple dimensions. In total, we evaluate 46 LCVLMs on MMLONGBENCH. To the best of our knowledge, our evaluation provides the most thorough and controlled comparison of the vision-language long-context ability on broad real-world 5 Figure 1: Performance on MMLONGBENCH. We report results for selected frontier models, and the full results of all models are provided in Figure 21. Note that Claude-3.7-Sonnet supports at most 100 images, and we mark the results as N/A for cases with more images (More in Appendix D.4) applications. These models include closed-source models GPT-4o [10], Claude-3.7 [34], and Gemini 2 and 2.5 [43, 84], as well as open-source model families, such as Qwen2.5-VL [29], InternVL3 [30], and Gemma3 [3]. We also consider position extrapolation methods, such as YaRN [37] and V2PE [52] (See Appendix D.5). We list all models evaluated in Table 10. Following existing works [20], we use greedy decoding for all models for consistency and randomly sample 100 examples from each dataset. More details are in Appendix C. 4.1 Evaluation on MMLONGBENCH across Tasks and Context Lengths We present the performance of selected frontier LCVLMs in Figure 1, and the full results of all 46 models are reported in Figure 21. We analyze model performance from multiple perspectives and summarize our main findings as follows: All models struggle, but closed-source models perform better. Here, we consider the performance at the longest input length of 128K tokens. In general, we observe that all models struggle on our vision-language long-context tasks. For example, even GPT-4o only achieves 62.9 on average, while open-source models perform even worse. We find that Gemini-2.5-Pro stands out as the strongest 6 Figure 2: Distribution of long-document VQA (DocVQA) with respect to performance on MMNIAH variants. We find that the models are concentrated in the coral-shaded areas. LCVLMs. Other than ICL, Gemini-2.5-Pro outperforms open-source models by about 20 absolute points. On ICL, although the gap is relatively smaller, due to the strong performance of Qwen2-VL72B, there is still difference of about 14 points. While the other closed-source models continue to surpass open-source models, the margin is often under 10 points. Further, Ovis2-34B achieves score of 41.6 on summarization, similar to GPT-4o (42.4). Qwen2.5-VL-32B achieves SubEM score of 64.6 on VRAG, even better than Gemini-2.0-Flash. These findings show that while current closed-source models generally perform better, open-source ones are also competitive. Models can generalize to longer context lengths. Another interesting observation is that some models can generalize to longer context lengths than they are officially designed for. For example, although the context window for Qwen2-VL-72B during training is only 32K tokens, the model can generalize to 128K input length and still achieve an average score of 51.9. We also observed similar effects on other models, such as Ovis2-34B and InternVL2.5-26B. This phenomenon is likely because the underlying LLMs of those LCVLMs have been trained with longer context windows [29]. We leave further investigation to future work. Reasoning can improve multimodal long-context ability. We include Gemini-2.0-Flash-T in our evaluation, which is the thinking version of Gemini-2.0-Flash. From the results, we observe that the reasoning ability can consistently improve the Gemini-2.0-Flash on all tasks. While the changes for VRAG, Recall, and ICL are modest, summarization and DocQA exhibit marked improvements of 25.3% and 10.1%, respectively. Then, Gemini-2.5 models exhibit even stronger performance, which are natively designed as thinking models. Different models exhibit different strengths. Generally, we find that model performance varies considerably across different tasks. For instance, Qwen2.5-VL-32B outperforms InternVL3-38B on VRAG, but underperforms on NIAH. Similarly, Ovis2-34B excels at summarization but struggles on DocQA. These findings further support the necessity of comprehensive benchmark and low correlation between different task categories. In Appendices D.5 and D.6, we also provide additional analysis about the performance of position extrapolations and the lost-in-the-middle phenomenon. 4.2 Can Needle-in-a-Haystack Task Reflect LCVLMs Overall Long-context Ability? The needle-in-a-haystack (NIAH) task has been primarily used to evaluate LCVLMs long-context abilities. However, it remains unclear whether strong performance on NIAH reliably reflects overall long-context capability on diverse tasks. In this section, we first analyze the difficulty of existing NIAH benchmarks and find that current NIAH tasks are challenging, resulting in limited differentiation between models. Further, we compute Spearmans rank correlation (ρ) between NIAH performance and that on other tasks. Our results show that none of these NIAH tasks consistently correlates with performance across diverse, practical scenarios. Text-image interleaved NIAH tasks are challenging. In Figure 3, we find that even state-of-the-art models like GPT-4o and Gemini-2.5 struggle to surpass Figure 3: Model performance on VHMulti dataset. Random guess yields 50% accuracy, highlighting its difficulty. 80% accuracy on VH-Single when the context length is just 8K tokens (approximately 22 images). Most models are just slightly better than random guess (50%). This demonstrates that locating objects in large set of images is still hugely challenging for current LCVLMs. See more discussion in Appendix D.1. Then, we plot the performance of different models on the retrieval, counting, and reasoning tasks of MM-NIAH against their performance on long-document VQA in Figure 2. We find that most models achieve low performance on the counting and reasoning tasks, with scores below 30 and 40, respectively. The difficulty of the tasks and low performance result in poor separability between models. While the retrieval is an easier task, it still does not align well with DocVQA tasks. In short, we find that both VH and MM-NIAH present significant challenges to current LCVLMs, thus showing limited differentiation between models and weak alignment with other tasks. NIAH tasks fail to reflect overall long-context abilities. As shown in Figure 4, none of the NIAH tasks exhibit strong correlation with the broader set of long-context tasks. This suggests that performance on NIAH tasks may not be reliable indicator of general long-context capabilities. In particular, Visual Haystack (VH) tasks show especially low correlations due to their high difficulty, as discussed above, which results in limited ability to distinguish between models. In MM-NIAH, counting and reasoning tasks show weak correlations with several downstream tasks, with coefficients below 0.8. The retrieval task also shows weak alignment with ICL performance. Interestingly, simpler tasks like retrieval with single needle in unrelated essays tend to correlate better with diverse task categories, which is consistent with our findings in Figure 2. We further examine the differences between text-based and image-based needles in Appendix D.2. Figure 4: Spearmans ρ across all 46 models at 128K tokens. 4.3 Cross-Category Correlations Identify Long-Document VQA as Reliable Proxy We perform cross-category correlation analysis of model performance. We find that different categories do not consistently show strong correlation (< 0.85) with each other, as shown in Figure 5. Specifically, VRAG and NIAH closely correlate because retrieval is the central capability of both tasks. further investigation shows that VRAG achieves its highest correlation (of 0.93) with the retrieval task (MM-NIAHRet) in Figure 13, reinforcing the shared emphasis on retrieval. Meanwhile, In contrast, summarization and long-document VQA show high correlation of 0.88, likely due to their shared input format imageformatted PDF documents. This suggests that image types affect category correlations. In contrast, ICL tasks show relatively weak correlations with other categories. The ICL tasks evaluate models ability to induce new classification rules from numerous exemplars, skill orthogonal to recalling facts in long contexts. This further demonstrates that model developers should consider various long-context skills to draw more holistic picture of LCVLMs. See Appendix D.3 for detailed dataset-level correlations and additional category-wise insights. Figure 5: Spearmans ρ between all categories with =128K. For each category, the Avg excludes the correlation with itself. Long-document VQA as reliable proxy for long-context capabilities. As shown in Figure 5, long-document VQA achieves the highest average correlation with other categories, indicating that it is more aligned with the broader range of long-context tasks. For example, questions from LongDocURL [17] cover not only simple retrieval but also complex understanding and reasoning. Meanwhile, long-document VQA exhibits the smallest standard deviation, showing that it is also stable and balanced. Taken together, these findings suggest long-document VQA is more representative and reliable proxy than the commonly adopted NIAH for reflecting overall system performance, allowing model developers to iterate more rapidly without the overhead of full-scale evaluation. 8 Figure 6: Error Analysis on MMLongBench-Doc. Instead of PDF-formatted documents, we feed OCR-extracted plain text to LCVLMs ( w/ OCR) and also test corresponding LLMs: Qwen2.5-7B and Qwen2.5-32B ( w/ LLM). We also show scores on examples with different answer sources. 4.4 Error Analysis Our evaluation shows that current LCVLMs have significant room for improvement. To better understand their limitations, we analyze model predictions in detail. In Figure 6, we show the performance of using another pipeline for DocVQA on MMLongBench-Doc. Here, we convert PDF-formatted documents to plain text with OCR ( w/ OCR) and feed them to LCVLMs. There is no clear winner between the PDF-formatted and OCRextracted pipelines across all models. While Qwen2.5-VL models perform better with imaged-formatted PDF documents in most cases, Gemma3-27B prefers plain text for shorter input lengths ( 32K). Furthermore, we performed fine-grained analysis by categorizing examples according to their answer sources into two groups: text-pure and vision-needed. As expected, using PDF documents leads to higher scores in vision-needed cases, whereas plain text yields better performance in text-pure cases, especially with longer inputs (64K and 128K). This suggests that OCR capability remains bottleneck for current LCVLMs when handling longcontext inputs. Future work could explore combining both pipelines to further enhance performance. Meanwhile, when using OCR-extracted text, replacing LCVLMs with the corresponding LLMs, Qwen2.5-7B and Qwen2.5-32B ( w/ LLM), yields better results in text-pure cases of DocVQA. Figure 7: Error analysis on ViQuAE. We replace the image with its original entity name ( w/ name) and also test text-only counterparts: Qwen2.5-7B and Qwen2.5-32B ( w/ LLM). We also examine the sources of errors in the VRAG category in Figure 7. Since ViQuAE is built on TriviaQA [72], we replace all images in ViQuAE questions with their corresponding entity names and feed those text-only questions into LCVLMs. All models show varying degrees of improvement, with Gemma3-27B achieving the largest gain of 26.4 points (at 128K), suggesting that bottleneck of LCVLMs lies in cross-modality information retrieval. Besides, providing entity names as input to corresponding LLMs improves model performance. These results illustrate common trade-off between multimodal and text-only long-context abilities during the training of LCVLMs."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we have introduced MMLONGBENCH, the first comprehensive benchmark for evaluating long-context visionlanguage models (LCVLMs) across wide spectrum of downstream tasks. By covering five distinct task categorieswhile unifying cross-modal token counting and standardizing context lengths, MMLONGBENCH provides rigorous, extensible foundation for diagnosing the strengths and weaknesses of frontier LCVLMs. Our evaluation of 46 models reveals that i) evaluation on single task does not reliably predict overall long-context capability, ii) even frontier models face significant challenges, particularly in OCR accuracy and cross-modal retrieval, and iii) models endowed with enhanced reasoning mechanisms (e.g., thinking variants) consistently outperform their base counterparts in long-context settings. Looking forward, we hope MMLONGBENCH will serve as standard yardstick for the community to benchmark new LCVLMs and to drive research on more efficient vision-language token encodings, more robust position-extrapolation schemes, and improved multi-modal retrieval and reasoning capabilities."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors of this paper were supported by the ITSP Platform Research Project (ITS/189/23FP) from ITC of Hong Kong, SAR, China, and the AoE (AoE/E-601/24-N), the RIF (R6021-20) and the GRF (16205322) from RGC of Hong Kong, SAR, China. We also thank the support from NVIDIA AI Technology Center (NVAITC) and the valuable suggestions on the design of this benchmark provided by Yuxiang Wu, Shizhe Diao, and Hongming Zhang."
        },
        {
            "title": "References",
            "content": "[1] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bap tiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cantón Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin R. Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Niko lay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Ro main Sauvestre, Ron nie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sa hana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Vir ginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang 10 Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pe dro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollár, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [2] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [3] Gemma Team Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gael Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Róbert Istvan Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andras Gyorgy, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Boxi Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, J. Michael Wieting, Jonathan Lai, Jordi Orbay, Joe Fernandez, Joshua Newlan, Junsong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Dehghani Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Ardeshir Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vladimir Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab S. Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam M. Shazeer, Oriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray Kavukcuoglu, Clément Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Leonard Hussenot. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [4] Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. [5] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [6] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68646890, 2024. [7] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. [8] Yongqi Li, Wenjie Li, and Liqiang Nie. Mmcoqa: Conversational question answering over text, tables, and images. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 42204231, 2022. [9] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [10] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [11] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking longcontext capability of multimodal large language models. arXiv preprint arXiv:2406.11230, 2024. 12 [12] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? In First Conference on Language Modeling. [13] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. arXiv preprint arXiv:2406.15319, 2024. [14] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [15] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14191436, 2021. [16] Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph Gonzalez, Trevor Darrell, and David Chan. Visual haystacks: vision-centric needle-in-a-haystack benchmark. arXiv preprint arXiv:2407.13766, 2024. [17] Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, et al. Longdocurl: comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating. arXiv preprint arXiv:2412.18424, 2024. [18] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. Advances in Neural Information Processing Systems, 37:2054020565, 2024. [19] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiao wen Dong, Hang Yan, Hewei Guo, Conghui He, Zhenjiang Jin, Chaochao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, and Yu Qiao. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [20] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. [21] Yew Ken Chia, Liying Cheng, Hou Pong Chan, CHAOQUN LIU, Maojia Song, Mahani Aljunied, Soujanya Poria, and Lidong Bing. M-longdoc: benchmark for multimodal super-long document understanding and retrieval-aware tuning framework. [22] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and MingWei Chang. Can pre-trained vision and language models answer visual information-seeking questions? In The 2023 Conference on Empirical Methods in Natural Language Processing. [23] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José Moreno, and Jesús Lovón Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, pages 31083120, 2022. [24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. [25] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. 13 [26] Jianxiong Xiao, Krista Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring large collection of scene categories. International Journal of Computer Vision, 119:322, 2016. [27] Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Benchmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1288412893, 2021. [28] Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey. Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities. Advances in Neural Information Processing Systems, 35:1315813173, 2022. [29] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [30] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Cong He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Ying Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kai Zhang, Hui Deng, Jiaye Ge, Kaiming Chen, Limin Wang, Mingsong Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [31] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melissa Hall Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [32] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [33] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [34] Anthropic. Claude 3.7 sonnet. URL https://www.anthropic.com/news/claude-3-7-sonnet, 2024. [35] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In Proceedings of the 41st International Conference on Machine Learning, pages 1412514134, 2024. [36] 01.AI Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 14 [37] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations. [38] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: extending llm context window beyond 2 million tokens. In Proceedings of the 41st International Conference on Machine Learning, pages 1109111104, 2024. [39] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [40] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [41] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Longrange transformers with unlimited length input. Advances in Neural Information Processing Systems, 36:3552235543, 2023. [42] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling. [43] Google. Our most https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march2025/, 2025. ai model. intelligent Gemini 2.5: URL [44] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [45] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. CoRR, 2024. [46] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18741883, 2016. [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [48] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [49] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37: 8787487907, 2024. [50] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. [51] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 15 [52] Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. arXiv preprint arXiv:2412.09616, 2024. [53] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. [54] Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, and Dong Yu. Divscene: Benchmarking lvlms for object navigation with diverse scenes and objects. arXiv preprint arXiv:2410.02730, 2024. [55] Gregory Kamradt. Needle in haystack-pressure testing llms, 2023."
        },
        {
            "title": "URL",
            "content": "https://github.com/gkamradt/LLMTest_NeedleInAHaystack, 2024. [56] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 11:157173, 2024. [57] Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963, 2024. [58] Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1533915353, 2024. [59] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models. In The Twelfth International Conference on Learning Representations. [60] Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: zero-shot benchmark for long text understanding. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 79777989, 2023. [61] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. [62] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. Infbench: Extending long context evaluation In Proceedings of the 62nd Annual Meeting of the Association for beyond 100k tokens. Computational Linguistics (Volume 1: Long Papers), pages 1526215277, 2024. [63] Wai Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Yuxin Jiang, Lifeng Shang, Qun Liu, and Kam-Fai Wong. M4le: multi-ability multi-range multi-task multidomain long-context evaluation benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1556815592, 2024. [64] Yujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. From text to pixel: Advancing long-context understanding in mllms. arXiv preprint arXiv:2405.14213, 2024. [65] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [66] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [67] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. 16 [68] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [69] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504, 2022. [70] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. [71] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: benchmark In Proceedings of the 2021 Conference of the for knowledge intensive language tasks. North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25232544, 2021. [72] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. [73] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2023. [74] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023. [75] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024. [76] Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. In First Workshop on Long-Context Foundation Models@ ICML 2024. [77] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. [78] Yixing Jiang, Jeremy Andrew Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan Chen, and Andrew Ng. Many-shot in-context learning in multimodal foundation models. In ICML 2024 Workshop on In-Context Learning, 2024. [79] Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations. [80] Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024. [81] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1363613645, 2023. [82] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 17 [83] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. [84] Google. era, google-gemini-ai-update-december-2024/#ceo-message. agentic new ai model https://blog.google/technology/google-deepmind/"
        },
        {
            "title": "Introducing\nURL",
            "content": "gemini 2024. 2.0: our the for [85] Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1206512075, 2023. [86] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. [87] Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning learns in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 82988319, 2023. [88] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [89] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356, 2022. [90] Daniel Deutsch, Rotem Dror, and Dan Roth. Re-examining system-level correlations of automatic summarization evaluation metrics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60386052, 2022. [91] Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. Wice: Real-world entailment for claims in wikipedia. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75617583, 2023. [92] Shiyue Zhang and Mohit Bansal. Finding balanced degree of automation for summary evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 66176632, 2021. [93] Arie Cattan, Paul Roit, Shiyue Zhang, David Wan, Roee Aharoni, Idan Szpektor, Mohit Bansal, and Ido Dagan. Localizing factual inconsistencies in attributable text generation. arXiv preprint arXiv:2410.07473, 2024. [94] Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Booookscore: systematic exploration of book-length summarization in the era of llms. In The Twelfth International Conference on Learning Representations. [95] Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Fables: Evaluating faithfulness and content selection in book-length summarization. In First Conference on Language Modeling. [96] Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. [97] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [98] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 18 [99] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [100] kaiokendev."
        },
        {
            "title": "Things",
            "content": "im learning while training superhot, 2023."
        },
        {
            "title": "URL",
            "content": "https://kaiokendev.github.io/til, 2023. [101] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [102] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [103] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [104] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations."
        },
        {
            "title": "A Dataset Details",
            "content": "In this appendix, we provide more details on how to build long-context examples based on existing datasets. A.1 Visual Retrieval-Augmented Generation Gold Passage. InfoSeek [22] is large-scale dataset for factual knowledge-based VQA featuring long-tail entities from Wikipedia [85]. For InfoSeek, we use the lead section from Wikipedia as gold passages, which is the first section on each page and serves as summary of the whole page. The lead section may be long, so we chunk it into multiple 100-word passages. We remove all the queries whose corresponding lead section does not contain the correct answer. ViQuAE [23] repalces the named entities in questions from TriviaQA [72] with corresponding entity images from Wikimedia Commons2. We obtain gold passages for each question from the KILT benchmark [71], which provides human annotations of gold passages for queries in TriviaQA. Length Control. We populate the context with hard negative passages from Wikipedia, and the version we used is the Wikipedia 2019-08-01 dump [71]. We follow the KILT benchmark to preprocess Wikipedia articles into 100-word passages. For retrieval, we adopt retrieval-and-rerank pipeline, where BM25 is first used for coarse retrieval, followed by reranking with dense embedding from Alibaba-NLP/gte-large-en-v1.5 [86]. Here, we replace the image in each question with its original entity name for better retrieval accuracy, because text-based retrieval achieves higher recall and provides harder distractors. Previous work [20] shows that this pipeline presents significantly greater challenge than randomly sampled passages. Also, using real embedding model for retrieval is consistent with various downstream visual retrieval-agumented generation tasks and can better reflect downstream application performance. A.2 Needle-in-a-Haystack Length Control. For the Visual Haystack dataset [16], we directly use the needles and target objects from the original dataset. Then, we change the number of negative distractors to build long-context examples with given input length L. Note that the original dataset simply reports the image number as context length, ignoring different image sizes. Here, we count image tokens based on patches split by vision encoders as discussed in Section 3.2. There are two tasks in the dataset: VH-Single and VH-Multi, where the target objects are contained in single-needle image or multiple-needle images, respectively. For MM-NIAH [18], the contexts in the original dataset are composed of entire web pages from OBELICS [74]. However, using full web pages makes it difficult to control input length at fine granularity since web pages typically contain tens of thousands of tokens. To solve the issue, we chunk the text content of web pages into 100-word passages, as we did for the Wikipedia corpus in VRAG. Meanwhile, the images in MM-NIAH contain only few hundred tokens. Thus, we can achieve fine-grained control over the context length and incrementally add text passages and images to reach given length L. Metrics. We report the accuracy on Visual Haystack, exactly the same as in the original work. The MM-NIAH dataset contains three different tasks: needle retrieval, counting, and reasoning. We use MM-NIAH-Ret, MM-NIAH-Count, and MM-NIAH-Reason as their abbreviations, respectively. In each task, there are both text-needle and image-needle examples. In MM-NIAH-Ret and MMNIAH-Reason, we use substring exact matching (SubEM) for text-needle examples and accuracy for image-needle examples, exactly following the original paper [18]. In MM-NIAH-Count, we find that the soft accuracy metric proposed in the original paper [18] can be exploited: simply predicting list of zeros ([0, 0, ...]) results in score over 30 on image-needle examples. Thus, we report the accuracy of the total count of the needle in the haystack instead of comparing the list of needle counts, which we find is more reliable. Last but not least, we sample text-needle and image-needle examples evenly in all three tasks. 2https://commons.wikimedia.org/ Figure 8: Comparison between ROUGE-L and the GPT-4o evaluation on summarization datasets. GPT-4o evaluation reflects the performance gain on Gemma3-27B with increased input length, and it also clearly sets apart open-source models with different sizes. In comparison, ROUGE-L remains almost the same for all models and input lengths. Please refer to the original MM-NIAH paper [18] for comprehensive details of all three tasks and two needle modalities. A.3 Many-Shot In-Context Learning Class Sampling. We include Stanford Cars [24], Food101 [25], SUN397 [77], and iNat 2021 [27]. At the input length of 128K tokens, we randomly sample 50 different classes from each dataset because we only put about 500 images in the context window. With 50 classes, we can ensure that there are about 10 exemplars from each class, which is sufficient. For iNat 2021, since the dataset contains substantially more classes (over 10,000 species), we randomly sample 50 classes from the Birds supercategory and 50 classes from the Plants supercategory. For every single example, all the exemplars and the test image are either from the Birds classes or the Plants classes, ensuring the task remains 50-way classification problem. Meanwhile, for shorter input lengths, we need to reduce the class number to ensure sufficient shots per class. Specifically, we randomly sample 5, 10, 20, and 40 classes for the input length of 8K, 16K, 32K, and 64K tokens. With those class numbers, we find that the number of exemplars per class is similar to that when there are 128K classes. Label mapping and length control. We employ label mapping strategy to ensure that models perform classification based on in-context exemplars instead of relying on their pre-trained knowledge. Each label is randomly mapped to an integer {0, 1, . . . , 1}, where is the number of classes, following established practices [20, 87]. Throughout the evaluation, we provide models with images and their corresponding integer labels. Following Li et al. [75], we arrange exemplars into demonstration rounds, each of which includes exactly one exemplar per label in random order. We concatenate these demonstration rounds, with the last round truncated, to build examples of input length L. Also, the label distribution is balanced in all datasets and input lengths. A.4 Summarization Preprocessing. GovReport [15] consists of reports written by the U.S. Government Accountability Office (GAO)3 and the Congressional Research Service (CRS)4. GAO reports constitute the majority of the dataset (more than 12K) and provide enough coverage for evaluation. Since CRS reports have different format from GAO reports and there are only few CRS reports available, we only use GAO reports in our benchmark. Summaries of GAO reports are written by experts and are structured into three aspects: Why GAO did this study, What GAO found, and What GAO recommends. Those summaries are written at the beginning pages of the PDF-formatted GAO documents. We use PyMuPDF 5 to detect those answers and remove the corresponding pages to ensure no answer leakage in the inputs. Multi-LexSum [28] consists of multi-document summarization problems about civil rights lawsuits, and the summaries are written by domain experts (i.e., lawyers and law students). 3www.gao.gov 4crsreports.congress.gov 5https://pymupdf.readthedocs.io 21 Both datasets are constructed using the OCR-extracted plain text as the input. In our evaluation, we replace the OCR-extracted plain text with the original PDF-formatted documents. We screenshot each page of each PDF-formatted document with 144 DPI, following common practices [5]. Different from previous works [5, 17], we do not concatenate images to reduce the token numbers and instead directly feed them into LCVLMs since we are stress-testing the models long-context capability. Length Control. To control the input length L, we truncate document pages from the end. When there are multiple documents in Multi-LexSum, we truncate each document evenly from the end. Additionally, we discard examples that exceed the 128K context length by more than 24K tokens, as adding them would require truncating too many pages to fit within the context window. In this way, we can avoid confounding effects on model performance caused by the loss of key information during page truncation. Data Scale. In long-form generation tasks, each summary typically contains many atomic claims to be verified, in contrast to short outputs of other categories, such as VRAG. There are 15,951 claims in 387 examples in these two datasets, indicating large scale for evaluation. Model-Based Metric The N-gram overlap metrics, such as ROUGE [88], have long been condemned for their poor correlation with human judgment for long-form generation [89, 90]. To ensure reliable evaluation for summarization, we adopt the reference-based LLM evaluation method proposed in HELMET [20]. Specifically, we first break down the gold reference summary into set of atomic claims with GPT-4o, following prior work [9193]. Next, we ask the model to check for three properties of model predictions: precision, recall, and fluency. We utilize GPT-4o to assess if each sentence in the generated summary is supported by the gold reference (precision) and if each claim is present in the generated summary (recall). The F1 score is computed from the recall and precision. We also prompt GPT-4o to assess the fluency of the generated summary. The fluency is assigned value of 0 if the output is incoherent, incomplete, or repetitive, and value of 1 if it is fluent and coherent. The final score is the product of fluency and F1 score. Our empirical study in Figure 8 demonstrates that InternVL3-2B achieves ROUGE-L scores comparable to GPT-4o. Moreover, ROUGE-L exhibits minimal difference across different input lengths from 8K to 128K tokens. These observations reveal that ROUGE-L has low discriminative capacity and often fails to effectively distinguish between the quality of generated texts. In contrast, GPT4-o evaluation shows significant gap across different input lengths and shows lower scores for models with shorter context windows, such as InternVL2.5. Atomic Claims Verification. We manually checked 100 atomic claims from 25 Multi-LexSum summaries and another 100 claims from 25 GovReport summaries. We found that only one claim was not factually accurate. Then, we checked the coverage of the claims and found no key facts were missing. This manual verification shows that GPT-4o is virtually always reliable for the decomposition task. For Multi-LexSum, we follow HELMET [20] and use the short summary to obtain atomic claims, where the dataset also provides long and tiny summary for each case. GPT-4o Judgment Verification. We show the detailed prompts for evaluating the fluency, precision, and recall in Tables 4 to 9, following previous works [20, 91, 94, 95]. We further conduct human analysis to verify the evaluation metric. From quantitative analysis, we found that GPT-4o can consistently distinguish fluent and non-fluent outputs. The agreement between the model judgements and human judgements is 100% for randomly sampled outputs from GovReport and Multi-LexSum. Then, we sample 10 generated summaries for both GovReport and Multi-LexSum (20 in total) and check 5 atomic claims evaluations for each summary. The models we used are Gemini-2.5-Pro and Qwen2.5-VL-32B. We follow similar procedure and manually check the precision and recall of those sampled summaries. For precision, we observed Cohens κ = 0.90 for GovReport and κ = 0.89 for Multi-LexSum, suggesting almost perfect agreement. Meanwhile, for recall, we observed Cohens κ = 0.90 for GovReport and κ = 93 for Multi-LexSum, which are also near perfect. Inspecting the disagreements, we find that most disagreements come from partially supported cases. We identified two common underlying reasons for partially supported cases when measuring precision and recall, respectively. First, sentence in generated summary may contain two points: While 22 Figure 9: Data distribution of MMLongBench-Doc and LongDocURL after our pre-processing. Both datasets remain well-distributed, and their distributions are similar to the ones in the original paper. agencies generally documented their review, inconsistencies and documentation gaps existed. We found the reference summary only supports inconsistencies and documentation gaps existed, and the agencies generally documented part is an entailment inferred by the model. Such inferred (entailed) information causes lot of partially supported cases when measuring precision. Second, the claims in the gold reference may include specific details, such as some geographic locations or organization names. These details may not be explicitly mentioned in the generated summary, causing the partially supported cases for recall. A.5 Long-Document VQA Preprocessing. MMLongBench-Doc [5] and LongDocURL [17] contain questions on various kinds of documents, such as financial reports, guidebooks, and academic papers, and the answer formats include string, integer, float, and list. More importantly, the rule-based evaluation method commonly adopted on those datasets depends on answer formats. First, we find that there is proportion of noisy answer format annotations. For example, list answer like [Top 10 File Categories Sorted By Disk Space, Last 12 Months Modified Disk Space History] is annotated as being in string format, and answers in string format are also written in list format, such as [PRIVACY SCREEN OPTIONS]. Also, percentages like 86% and 52% are labeled as float. Therefore, our first step with these datasets is to correct the mislabeled answer formats and discard the instances for which the correct answer format cannot be recovered. Second, both datasets rely heavily on LLMs, such as GPT-4o, to extract the answer from model predictions. This leads to high evaluation costs and poses challenges for large-scale evaluation, like 46 models in our work. Then, we take closer examination of different formats of answers: (1) For integer and float answers, we find that numbers can be extracted with regular expressions; (2) For string answers, if the answer is short (less than 5 words), we find that model predictions are also short. Thus, we can directly use automatic metrics like ROUGE without the need for answer extractions; (3) As result, only long-form string answers require LLM-based extraction; Since long-form string answers (> 5 words) constitute only small proportion of these datasets, we simply discard those instances to enable scalable evaluation without relying on GPT-4o. We find that the retrained short string answers are mostly entity names; (4) Note that for list answers, we evaluate each element in the list (i.e., integer, float, or string), and the answer extraction method is determined by the type of each element. Evaluation Metrics. We follow previous works [5, 17] and employ the same rule-based scoring method that applies different strategies depending on the format of the reference answer: (1) For String format answers, we initially use regular expressions to determine whether the answers require exact matching (e.g., telephone numbers, email addresses, website addresses, filenames, times, dates, etc.). If the answer needs an exact match, we perform substring exact matching (SubEM) with score of 0 or 1. Otherwise, we follow previous works [20, 62, 81, 96] and calculate ROUGE F1 scores; (2) For integer answers, we perform an exact match comparison, and the score is either 0 or 1; (3) For float answers, we treat the model prediction and gold reference as the same if the relative error is less than 1%; (4) For list answers, we evaluate each element separately based on its answer type 23 and take the average. Here, we follow LongDocURL to use the Greedy List Match: for each element in the reference list, we compute its score against each element in the prediction list and greedily select the highest score as its matching score. This metric does not require the predicted list to follow the same element order as the reference list, thereby providing greater tolerance in evaluation. After all the filtering, we find both datasets remain well-distributed as shown in Figure 9. Different from them, SlideVQA [81] features questions based on 20-page slide decks, which contain rich layout information and less dense text. The answer formats in the dataset are string, integer, and float, and do not cover list answers. We use the same rule-based scoring method as described for MMLongBench-Doc and LongDocURL. Length Control. The input lengths of DocVQA tasks are also easy to control. If an example exceeds given length L, we truncate the document evenly from both sides while preserving the answer pages. If the document cannot fill the length L, we alternately pad the left and right sides with randomly sampled negative documents until the required length is reached. Notably, we also truncate the last padding document as needed to control the length at the granularity of pages, instead of documents. The randomly sampled padding documents are not guaranteed to be truly irrelevant or negative. They may occasionally contain information related to the question, which could potentially change the answer. To ensure models attend to the original document, we preface each question with the prompt Based on the Document <Original Doc ID>, answer the following question. A.6 Image Resizing and Statistics The number of tokens per image is determined by the image size in our benchmark, as we discussed in Section 3.3. In MM-NIAH, we find that many images from the OBELICS dataset are unnecessarily large (up to 80006000 pixels) and are not text-rich. Then, we resize those images long edge to 1024 pixels while preserving their aspect ratio. We calculate the average number of images per example in all the datasets and input lengths in Table 3. From the table, we can find that our benchmark covers various text-to-image ratios. For example, VRAG tasks are text-centric and only contain one image per example, while ICL represents image-centric tasks with hundreds of images. MM-NIAH tasks are intermediate and feature both substantial text and multiple images. A.7 License All the data collected are based on previously open-sourced datasets, and all licenses are publicly available."
        },
        {
            "title": "B Full Model List",
            "content": "We list all 46 models [3, 10, 29, 30, 34, 43, 44, 47, 4951, 8284, 97, 98] we evaluated in Table 10. All 46 models have pixel unshuffle operation to reduce the token counts of images. This is consistent with our token counting methods (Section 3.2). The only exception is Pixtral-12B, but we can resize its image (to 0.5 on each side) to reduce the image tokens. Thus, we can fit Pixtral-12B on our GPU server and avoid extremely long input sequences. 24 Table 3: Average number of images per example in all datasets and input lengths. The values in subscript denote the standard deviations. (T) and (I) represents the text and image needle in each task of MM-NIAH. VRAG NIAH ICL Summarization DocVQA Data Length InfoSeek ViQuAE VH-Single VH-Multi MM-NIAH-Ret (T) MM-NIAH-Count (T) MM-NIAH-Reason (T) MM-NIAH-Ret (I) MM-NIAH-Count (I) MM-NIAH-Reason (I) Stanford Cars Food101 SUN397 Inat GovReport Multi-LexSum MMLongBench-Doc LongDocURL SlideVQA 8K 1.00.0 1.00.0 21.70.9 21.70.9 3.81.4 3.71.4 3.81.4 8.11.0 5.71.2 6.51.0 36.11.3 25.00.8 37.02.0 31.20. 2.00.1 3.00.1 3.31.3 3.62.1 7.50.9 16K 1.00.0 1.00.0 44.71.3 44.81.3 7.62.0 7.62.0 7.62.0 11.81.5 9.31.6 10.21.4 72.62.3 52.00.8 80.13.7 66.10. 6.00.0 6.00.2 6.92.4 7.24.5 16.22.1 32K 1.00.0 1.00.0 90.91.9 91.01.9 15.42.7 15.42.7 15.52.7 19.32.2 16.92.2 17.62.1 156.30.9 106.11.4 161.34.2 134.61. 12.00.0 12.10.5 13.84.8 14.28.1 33.22.7 64K 128K 1.00.0 1.00.0 183.42.9 183.42.8 30.43.8 30.33.9 30.53.9 34.23.4 31.73.2 32.53. 324.05.3 215.52.3 326.82.2 271.01.6 25.00.0 25.20.9 28.08.2 28.614.4 67.23.5 1.00.0 1.00.0 368.24.3 368.24.3 59.35.5 59.35.7 59.35.7 63.85.5 61.55.6 62.35.7 628.89.0 432.50.9 656.68.2 543.81. 50.70.5 51.31.8 56.412.1 55.318.3 135.25.2 25 Table 4: GovReport Fluency Evaluation Prompt Task: GovReport Metric: Fluency Please act as an impartial judge and evaluate the fluency of the provided text. The text should be coherent, non-repetitive, fluent, and grammatically correct. Below is your grading rubric: - Score 0 (incoherent, repetitive, or incomplete): Incoherent sentences, repetitive sentences (even if not by exact words), incomplete answers, or gibberish. Note that even if the answer is coherent, if it is repetitive or incomplete, it should be given score of 0. - Examples: - - Incomplete: \"Summary:\" \"Summary: U.S. agencies engaged export and controls controls controls controls Incoherent: diversion prevent items U.S. activities compliance allies transshipment risk misuse exported misuse misuse illicit illicit against interests or.\" - Repetitive: selected programs. selected programs. selected programs. selected programs.\" \"Summary:The audit focused on determining the cost and schedule performance of The audit focused on determining the cost and schedule performance of The audit focused on determining the cost and schedule performance of The audit focused on determining the cost and schedule performance of - Score 1 (coherent, non-repetitive answer): Coherent, non-repetitive, fluent, grammatically correct answers. If the text is coherent, non-repetitive, and fluent, but the last sentence is truncated, it should still be given score of 1. Examples: - \"Why GAO Did This Study: Tobacco use is the leading cause of preventable death and disease in the United States. In 2009, the Family Smoking Prevention and Tobacco Control Act (Tobacco Control Act) granted FDA, an agency within the Department of Health and Human Services (HHS), authority to regulate tobacco products, including marketing and distribution to youth. The act established CTP, which implements the act by educating the public on the dangers of tobacco use; developing the science needed for tobacco regulation; and developing and enforcing regulations on the manufacture, marketing, and distribution of tobacco products. The act authorized FDA to assess and collect user fees from tobacco manufacturers and importers. GAO review the authority and resources provided to FDA for regulating the manufacture, marketing, and distribution of tobacco products. tobacco user fees for key activities using its authorities granted in the act, and (2) any challenges FDA encountered in using its authorities. GAO analyzed data on tobacco user fees collected and spent on key activities by FDA as of March 31, 2014; reviewed documents related to FDAs key activities, as well as relevant laws, regulations, and guidance; and interviewed CTP, public health, and tobacco industry officials... [about 150 more words]\" This report examines (1) how FDA spent The Tobacco Control Act mandated that Now, read the provided text, and evaluate the fluency using the rubric. Then output your score in the following json format: {\"fluency\": Text: \"{text}\" 1}. 26 Table 5: Multi-LexSum Fluency Evaluation Prompt Task: Multi-LexSum Metric: Fluency Please act as an impartial judge and evaluate the fluency of the provided text. The text should be coherent, non-repetitive, fluent, and grammatically correct. Below is your grading rubric: Score 0 (incoherent, repetitive, or incomplete): Incoherent sentences, repetitive sentences (even if not by exact words), incomplete answers, or gibberish. Note that even if the answer is coherent, if it is repetitive or incomplete, it should be given score of 0. - Examples: - - Incomplete: \"Summary:\" Incoherent: the the able the the the the the Ã?n\" \"Summary: The plaintiff the the the the able the the the the the the the the - Repetitive: \"Summary: The U.S. government brought criminal case against four defendants. Summary: The U.S. government brought criminal case against four defendants. Summary: The U.S. government brought criminal case against four defendants. Summary: The U.S. government brought criminal case against four defendants.\" Score 1 (coherent, non-repetitive answer): Coherent, non-repetitive, fluent, grammatically correct answers. If the text is coherent, non-repetitive, and fluent, but the last sentence is truncated, it should still be given score of 1. - Examples: - \"This case is about an apprenticeship test that had disparate impact on Black apprenticeship applicants. The Equal Employment Opportunity Commission (EEOC) filed this lawsuit on December 27, 2004, in U.S. District Court for the Southern District of Ohio.\" - \"The plaintiffs sought declaratory and injunctive relief, as well as attorneys fees and costs, under the Americans with Disabilities Act, the Rehabilitation Act of 1973, the Social Security Act, and the Nursing Home Reform Act. The case was certified as class action on behalf of all Medicaid-eligible adults with disabilities in Cook County, Illinois, who are being, or may in the future be, unnecessarily confined to nursing facilities and with appropriate supports and services may be able to live in community setting. The defendants denied the allegations and argued that the plaintiffs claims were not typical of the class and that the class definition was too broad. The case is ongoing, with discovery and expert testimony scheduled for the fall of\" Now, read the provided text, and evaluate the fluency using the rubric. Then output your score in the following json format: {\"fluency\": 1}. Text: \"{text}\" 27 Table 6: GovReport Precision Evaluation Prompt Task: GovReport Metric: Precision Please act as an impartial judge and evaluate the quality of the provided summary of government report from U.S. Government Accountability Office (GAO). The summary should discuss one or more of the following: why GAO did this study, what GAO found, and what GAO recommends. Below is your grading rubric: Precision: - Evaluate the provided summary by deciding if each sentence in the provided summary is supported by the information provided in the expert summary. sentence is still supported even if some minor details (e.g., dates, entity names, or locations) are not explicitly mentioned in the expert summary. sentence is not supported if its major facts are not mentioned, contradicted, or introduce new information not present in the expert summary (e.g., extra analysis or commentary). - Score: the number of sentences in the provided summary that are supported by the expert summary. - Examples: use the following examples to guide your evaluation. Example 1: Expert summary: <start of summary>Why GAO Did This Study: The Congressional Budget Office projects that federal deficits will reach $1 trillion in 2020 and average $1.2 trillion per year through 2029, further adding to the more than $16 trillion in current debt held by the public. As result, Treasury will need to issue substantial amount of debt to finance government operations and refinance maturing debt. To support its goal to borrow at the lowest cost over time, Treasury must maintain strong demand from diverse group of investors for Treasury securities. GAO prepared this report as part of continuing efforts to assist Congress in identifying and addressing debt management challenges. This report (1) identifies factors that affect demand for Treasury securities and (2) examines how Treasury monitors and analyzes information about the Treasury market to inform its debt issuance strategy. GAO analyzed data on investor holdings of Treasury securities; surveyed non-generalizable sample of 109 large domestic institutional investors across 10 sectors (67 responded); reviewed Treasury analysis and market research; and interviewed market participants across sectors, experts on foreign investors, and Treasury officials... [about 300 more words] <end of summary> Provided summary: <start of summary>The U.S. Government Accountability Office (GAO) conducted performance audit from June 2018 to December 2019 to assess the management of federal debt by the Department of the Treasury. The audit aimed to evaluate how Treasury manages its debt to finance the federal deficit and refinances maturing debt while minimizing costs. Treasury issues various types of securities, including Treasury bills, notes, bonds, and inflation-protected securities, with maturities ranging from few weeks to 30 years, to attract diverse investor base and maintain healthy secondary market. The audit found that Treasurys regular and predictable framework for issuing securities supports reliable demand, but changes in market conditions and policies pose risks to the liquidity, depth, and safety of Treasury securities. Treasury uses market outreach, auction and market metrics, and analytical models to inform its debt issuance decisions but lacks policies for bilateral market outreach and quality assurance for analytical models. The report recommends Treasury finalize its market outreach policy and establish quality assurance policy for analytical models to ensure transparency and appropriate documentation. Treasury agreed with the recommendations and plans to implement them.<end of summary> Reasoning: Sentence 1 is not supported (audit dates and \"performance audit\" not mentioned). Sentence 2 is supported (aligns with Treasurys goal of borrowing at lowest cost). Sentence 3 is not supported (specific security types and maturity ranges not listed). Sentence 4 is supported (risks to liquidity, depth, safety are mentioned). Sentence 5 is supported (mentions the three inputs and missing policies). Sentence 6 is supported (matches the recommendations). Sentence 7 is supported (Treasury agreed). Therefore, the precision score is 5. Output: {\"precision\": 5, \"sentence_count\": 7} Example 2: ... Now, read the provided summary and expert summary, and evaluate the summary using the rubric. First, think step-by-step and provide your reasoning and assessment on the answer. Please keep your response concise and limited to single paragraph. Then output your score in the following json format: {\"precision\": Expert summary: <start of summary>{expert_summary}<end of summary> Provided summary: <start of summary>{summary}<end of summary> 7, \"sentence_count\": 20}. 28 Table 7: Multi-LexSum Precision Evaluation Prompt Task: Multi-LexSum Metric: Precision Please act as an impartial judge and evaluate the quality of the provided summary of civil lawsuit. The summary is based on set of legal documents, and it should contain short description of the background, the parties involved, and the outcomes of the case. Below is your grading rubric: Precision: - Evaluate the provided summary by deciding if each sentence in the provided summary is supported by the information provided in the expert summary. sentence is considered supported if its major facts align with the information in the expert summary. sentence is still considered supported even if some of its minor details, such as dates, entity names, or the names of laws and previous court cases, are not explicitly mentioned in the expert summary. sentence is not supported if its major facts are not mentioned or contradicted in the expert summary. - Score: the number of sentences in the provided summary that are supported by the expert summary. - Examples: use the following examples to guide your evaluation. Example 1: Expert summary: \"This lawsuit, brought in the the U.S. District Court for the Central District of California, was filed on June 3, 2020. The plaintiffs were represented by attorneys from the ACLU of Southern California. This lawsuit followed nation-wide protests that occurred in response to the killing of George Floyd by police officer in Minneapolis. While most protests were peaceful, some ended in violence, property destruction, rioting, and looting. Many cities, including Los Angeles and San Bernardino, issued curfews in an attempt to quell these riots. This action challenged these curfews as violations of free speech and assembly, free movement, due process, and challenged the San Bernardino curfew as violation of the establishment clause (the San Bernardino curfew included provision that exempted attendants of religious meetings from the curfew.)... [about 100 more words].\" Provided summary: \"In June 2020, Black Lives Matter - Los Angeles and several individuals filed lawsuit in the U.S. District Court for the Central District of California against Los Angeles Mayor Eric Garcetti, other city officials, and the City of San Bernardino, challenging the constitutionality of curfew orders imposed during protests against police violence. The plaintiffs, represented by the ACLU of Southern California, argued that the curfews violated their First Amendment rights to free speech and assembly, as well as their freedom of movement, by suppressing political protests and other activities. The lawsuit also claimed that the curfews were not narrowly tailored to address any emergency and lacked sufficient notice. However, the plaintiffs voluntarily dismissed the case in July 2020 after the defendants lifted the curfew orders and did not reinstate them in the following weeks.\" Reasoning: The first sentence in the provided summary is well supported by the expert summary even though some entity names are not explicitly mentioned. The second sentence is also well supported by the expert summary, as it mentions the ACLU of Southern California and the First Amendment rights. The third sentence is not supported by the expert summary, as it does not mention the lack of narrow tailoring or sufficient notice. The fourth sentence is well supported by the expert summary, as it mentions the voluntary dismissal of the case in July 2020. Therefore, the precision score is 3. Output: {\"precision\": 3, \"sentence_count\": 4} Example 2: ... Now, read the provided summary and expert summary, and evaluate the summary using the rubric. First, think step-by-step and provide your reasoning and assessment on the answer. Please keep your response concise and limited to single paragraph. Then output your score in the following json format: {\"precision\": 2, \"sentence_count\": 6}. Expert summary: \"{expert_summary}\" Provided summary: \"{summary}\" 29 Table 8: GovReport Recall Evaluation Prompt Task: GovReport Metric: Recall Please act as an impartial judge and evaluate the quality of the provided summary of government report from U.S. Government Accountability Office (GAO). The summary should discuss one or more of the following: why GAO did this study, what GAO found, and what GAO recommends. The text should contain all the major points in the expert-written summary, which are given to you. Below is your grading rubric: Recall: - Evaluate the provided summary by deciding if each of the key points is present in the provided summary. key point is considered present if its factual information is mostly-supported by the provided summary. If key point contains multiple facts, it is considered supported if most of the facts are present. - Score: the number of key points mostly-supported by the provided summary. - Examples: Use the following example to guide your evaluation. Example 1: Key points: 1. The Future Combat System (FCS) program is the centerpiece of the Armys effort to transition to lighter combat force. 2. The FCS program is the centerpiece of the Armys effort to transition to more agile combat force. 3. The FCS program is the centerpiece of the Armys effort to transition to more capable combat force. 4. By law, GAO is to report annually on the FCS program. 5. Law requires the Department of Defense (DOD) to hold milestone review of the FCS program. 6. This milestone review is now planned for 2009. 7. This report addresses (1) what knowledge will likely be available in key areas for the review. 8. This report addresses (2) the challenges that lie ahead following the review. 9. To meet these objectives, GAO reviewed key documents and performed analysis. 10. GAO attended demonstrations and design reviews to meet these objectives. 11. GAO interviewed DOD officials to meet these objectives. 12. The Army will be challenged to demonstrate the knowledge needed to warrant an unqualified commitment to the FCS program. 13. This challenge will occur at the 2009 milestone review. 14. The Army has made progress. 15. Knowledge deficiencies remain in key areas. [31 more points] Summary: <start of summary>Why GAO Did This Study: The Future Combat System (FCS) program is the centerpiece of the Armys effort to transition to lighter combat force. By law, GAO is to report annually on the FCS program. This report addresses (1) what knowledge will likely be available in key areas for the review, and (2) the challenges that lie ahead following the review. To meet these objectives, GAO reviewed key documents and interviewed DOD officials. What GAO Found: The Army will be challenged to demonstrate the knowledge needed to warrant an unqualified commitment to the FCS program. While the Army has made progress, knowledge deficiencies remain in key areas. Specifically, all critical technologies are not currently at minimum acceptable level of maturity. Actual demonstrations of FCS hardware and software have been limited. Network performance is also largely unproven. DOD could have at least three programmatic directions to consider for shaping investments in future capabilities. [106 more words]<end of summary> Reasoning: The summary covers: FCS as Armys transition centerpiece (point 1), GAOs reporting requirement (point 4), report objectives (points 7, 8), GAOs methods (points 9, 11), Armys challenges (point 12), progress and deficiencies (points 14, 15), technology issues (points 16, 19, 21), three programmatic directions (points 27, 29, 31, 33, 34, 36, 38, 41-43). It omits: \"more agile/capable\" (points 2, 3), 2009 milestone review (points 5, 6, 13), demonstrations attendance (point 10), design requirements issues (points 17, 18), small-scale concepts (point 20), program immaturity explanation (points 22, 23), funding competition (points 24-26), challenges after review (point 28), production before design demonstration (points 30, 32), technology testing issues (point 35), $50 billion funding (point 37), surrogate systems (points 39, 40), and increment justification (points 44-46). The summary supports 22 key points. Output: {\"supported_key_points\": [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 19, 21, 27, 29, 31, 33, 34, 36, 38, 41, 42, 43], \"recall\": 22} Now, read the provided summary and key points, and evaluate the summary using the rubric. First, think step-by-step and provide your reasoning and assessment on the answer. Please keep your response concise and limited to single paragraph. Then output your score in the following json format: {\"supported_key_points\": [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 19, 21, 27, 29, 31, 33, 34, 36, 38, 41, 42, 43], \"recall\": 22}, where \"supported_key_points\" contains the key points that are present in the summary and \"recall\" is the total number of key points present in the summary. Key points: {keypoints} Summary: <start of summary>{summary}<end of summary> 30 Table 9: Multi-LexSum Recall Evaluation Prompt Task: Multi-LexSum Metric: Recall Please act as an impartial judge and evaluate the quality of the provided summary of civil lawsuit. The summary is based on set of legal documents, and it should contain short description of the background, the parties involved, and the outcomes of the case. The text should contain all the major points in the expert-written summary, which are given to you. Below is your grading rubric: Recall: - Evaluate the provided summary by deciding if each of the key points is present in the provided summary. key point is considered present if its factual information is well-supported by the provided summary. - Score: the number of key points present in the provided summary. - Examples: use the following examples to guide your evaluation. Example 1: Key points: 1. The case challenged curfews in Los Angeles and San Bernardino, California. 2. The curfews were issued in response to the nationwide protests following the police killing of George Floyd in Minneapolis. 3. The complaint argued that the curfews violated free speech, free assembly, free movement, and Due Process. 4. The complaint also argued that the San Bernardino curfew violated the Establishment Clause. 5. The complaint sought injunctive and declaratory relief. 6. The plaintiffs voluntarily dismissed the case on July 7, 2020. 7. The dismissal occurred because the city had rescinded the curfews and not attempted to reinstate them. Summary: In June 2020, Black Lives Matter - Los Angeles and several individuals filed lawsuit in the U.S. District Court for the Central District of California against Los Angeles Mayor Eric Garcetti, other city officials, and the City of San Bernardino, challenging the constitutionality of curfew orders imposed during protests against police violence. The plaintiffs, represented by the ACLU of Southern California, argued that the curfews violated their First Amendment rights to free speech and assembly, as well as their freedom of movement, by suppressing political protests and other activities. The lawsuit also claimed that the curfews were not narrowly tailored to address any emergency and lacked sufficient notice. However, the plaintiffs voluntarily dismissed the case in July 2020 after the defendants lifted the curfew orders and did not reinstate them in the following weeks. Reasoning: The summary states that the plaintiffs challenged the constitutionality of curfew orders against Los Angeles and San Bernadino, so key point 1 is present. The summary does not mention that the curfew orders were issued in response to the nationwide protest that resulted from the police killing of George Floyd in Minneapolis, so key point 2 is missing. The summary does mention that the complaint argued that the curfews violated the First Amendment rights to free speech and assembly, so key point 3 is present. The summary does not mention that the complaint argued that the San Bernardino curfew violated the Establishment Clause, so key point 4 is missing. The summary does not mention that the complaint sought injunctive and declaratory relief, so key point 5 is missing. The summary mentions that the plaintiffs voluntarily dismissed the case in July 2020 after the defendants lifted the curfew orders and did not reinstate them in the following weeks, so key point 6 and 7 are present. Finally, key points 1, 3, 6, and 7 are present in the summary, so the recall score is 4. Output: {\"recall\": 4} Example 2: ... Now, read the provided summary and key points, and evaluate the summary using the rubric. First, think step-by-step and provide your reasoning and assessment on the answer. Please keep your response concise and limited to single paragraph. Then output your score in the following json format: {\"recall\": 2}. Key points: {keypoints} Summary: \"{summary}\" 31 Table 10: Length means the training length (default) or claimed context window (denoted by ). All LCVLMs are instruction-tuned. Image Porc. stands for Image Processing, which is mainly Dynamic Resolution ViT [47] or Dynamic Tiling [48]. The positional embedding includes RoPE [99], M-RoPE [47], linear scaling [39, 100] LongRoPE [38], Dynamic-NTK, NTK-by-parts or YaRN [37]. Name Length Image Proc. Positional Emb. # Params Proprietary (No model details except the claimed context lengths. gpt-4o-2024-11-20 claude-3-7-sonnet-20250219 gemini-2.0-flash-001 gemini-2.0-flash-thinking-exp-01-21 gemini-2.5-flash-preview-04-17 gemini-2.5-pro-preview-03-25 128,000 200,000 1,048,576 1,048,576 1,048,576 1,048,576 ? ? ? ? ? ? ? ? ? ? ? ? Qwen2-VL & Qwen2.5-VL Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct-AWQ Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct-AWQ InternVL2, InternVL2.5, & InternVL3 InternVL2-1B InternVL2-2B InternVL2-4B InternVL2-8B InternVL2_5-1B InternVL2_5-2B InternVL2_5-4B InternVL2_5-8B InternVL2_5-26B InternVL3-1B InternVL3-2B InternVL3-8B InternVL3-14B InternVL3-38B Ovis Ovis2-1B Ovis2-2B Ovis2-4B Ovis2-8B Ovis2-16B Ovis2-34B Gemma-3 gemma-3-4b-it gemma-3-12b-it gemma-3-27b-it Idefics2 idefics2-8b idefics2-8b-C (chatty) Mantis-8B-Idefics2 Idefics Idefics3-8B-Llama3 Phi-based Phi-3-vision-128k-instruct Phi-3.5-vision-instruct Phi-4-multimodal-instruct NVILA 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT 32,768 Dynamic-Resolution ViT M-RoPE M-RoPE M-RoPE M-RoPE M-RoPE M-RoPE M-RoPE 8,192 8,192 8,192 8,192 16,348 16,348 16,348 16,348 16,348 32,768 32,768 32,768 32,768 32,768 32,768 32,768 32,768 32,768 32,768 32,768 Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling Dynamic Tiling RoPE Dynamic-NTK LongRoPE Dynamic-NTK RoPE Dynamic-NTK RoPE Dynamic-NTK Dynamic-NTK Dynamic-NTK Dynamic-NTK Dynamic-NTK Dynamic-NTK Dynamic-NTK RoPE RoPE RoPE RoPE RoPE RoPE 131,072 131,072 131,072 Dynamic Tiling Dynamic Tiling Dynamic Tiling Linear Scaling Linear Scaling Linear Scaling 8,192 Dynamic-Resolution ViT 8,192 Dynamic-Resolution ViT 8,192 Dynamic-Resolution ViT RoPE RoPE RoPE 10, Dynamic Tiling NTK-by-parts 131,072 131,072 131,072 Dynamic Tiling Dynamic Tiling Dynamic Tiling LongRoPE LongRoPE LongRoPE NVILA-Lite-2B-hf-preview NVILA-Lite-8B-hf-preview 32,768 32,768 Dynamic Tiling Dynamic Tiling RoPE RoPE Pixtral pixtral-12b 131,072 Dynamic-Resolution ViT RoPE ? ? ? ? ? ? 2B 7B 72B 3B 7B 32B 72B 0.9B 2.21B 4.15B 8.08B 0.9B 2.2B 4.2B 8.1B 25.5B 0.9B 1.9B 8.1B 15.1B 38.4B 1B 2B 4B 8B 16B 34B 4B 12B 27B 8B 8B 8B 8B 4.2B 4.2B 5.6B 2B 8B 12B Table 11: The number of tokens produced by models without pixel unshuffle for the inputs (64K and 128K tokens) of the ICL and Visual Haystack (VH) datasets. The numbers are in thousands (K). These models cannot process long sequences of images efficiently."
        },
        {
            "title": "ICL",
            "content": "VH 64K 128K 64K 128K Llama-3.2-11B Llava-onevision-7b mPLUG-Owl3-7B 1,821K 3,626K 1,174K 2,358K 496K 996K 930K 1,870K 558K 1,142K 1,398K 2,786K B.1 LVLMs Beyond our Evaluation Token Efficiency. Beyond the models in our evaluation, there are also lot of excellent models, such as Llava-onevision [101], Llama3.2 [1], mPLUG-Owl3 [45]. However, we find that those models dont have pixel unshuffle operations. While Pixtral-12Bs ViT can take images with dynamic resolution, the ViTs of these three models use dynamic tiling, such as 560 560 tiles for Llama3.211B. Thus, unlike Pixtral-12B, we cannot reduce the number of image tokens by simply resizing the input image, since these models do not accept images smaller than the predefined tile size. As shown in Table 11, these models cannot efficiently process high-resolution images as whole, generating number of tokens that is at least ten times the pre-defined input length (64K or 128K). This causes extremely long input sequences, and we cannot fit them on our GPU server. Challenge for Model Integration. DeepSeek-VL [102], DeepSeek-VL2 [48], and Long-Llava [53] are also excellent, but they present additional challenges. These models do not provide standard API in the Transformers framework [103], which we used for inference. Therefore, these models cannot be loaded directly via Transformes, and we need to develop them based on their GitHub repositories. As result, integrating these models requires prohibitive amount of engineering effort to adapt their codebases, making it impractical within our current scope. We leave their integration for future work."
        },
        {
            "title": "C Experimental Setup",
            "content": "lengths As previously described, we evaluate all 46 models across different {8192, 16384, 32768, 65536, 131072}. We evaluate the proprietary models using their API. The specific versions we used are as follows: input - GPT-4o: gpt-4o-2024-11-20 - Claude-3.7-Sonnet: claude-3-7-sonnet-20250219 - Gemini-2.0-Flash: gemini-2.0-flash-001 - Gemini-2.0-Flash-T: gemini-2.0-flash-thinking-exp-01-21 - Gemini-2.5-Flash: gemini-2.5-flash-preview-04-17 - Gemini-2.5-Pro: gemini-2.5-pro-preview-03-25 For all open-source models, we evaluate them on an 8A100 (80GB each) GPU server. We use the HuggingFace framework [103] to deploy models and generate outputs. Since all models are instruction-tuned, we apply the chat templates to all datasets. We load models in BF16 with FlashAttention2 [104] for faster inference. The largest open-source models tested in our work have 72B parameters. Our computational resources are limited to 8 A100 GPUs; thereby, we cannot evaluate models with over 100B parameters, such as Llama4 [4] at 128K tokens. We sampled 100 examples from each dataset to evaluate models. This amount actually results in 600 examples for single-needle tasks: ViQuAE, VH-Single, and MM-NIAH-Ret, and 300 examples for multi-needle tasks: InfoSeek, VH-Multi, and MM-NIAH-Count. This is because we test 6 different depths (i.e., [0, 0.2, 0.4, 0.6, 0.8, 1.0]) for single-needle examples and 3 different permutations for multi-needle examples to mitigate the positional bias. Note that in MM-NIAH, we sample the text-needle and image-needle examples evenly, with 50 of each type. The MM-NIAH-Reason is more complex since image-needle (I) examples have single needle, while text-needle (T) ones Figure 10: The performance of selected models on VH-Single and VH-Multi. The accuracy of random guess is 50%. We find that these two tasks are very challenging. have multiple needles. There are 300 examples for MM-NIAH-Reason (I) (50 6 depths) and 150 examples for MM-NIAH-Reason (T) (50 3 permutations). Due to the depth imbalance in MM-NIAH-Reason, we compute the average score for each subset separately and report their mean as the final result."
        },
        {
            "title": "D Addtional Results",
            "content": "We provide more evaluation results in addition to Section 4. D.1 The Difficulty of Visual Haystack We discussed the difficulty of Visual Haystack in Section 4.2. As shown in Figure 3, current LCVLMs achieve the performance only slightly higher than random guessing on VH-Multi. Here, in Figure 10, we present the performance of the selected models on both VH-Single and VH-Multi, providing complete view. We find that models also perform poorly on VH-Single. Task Correctness. We manually checked number of examples from the Visual Haystack dataset and didnt find any errors in the task labels. As shown in Figure 10, Gemini-2.5-Pro achieves an accuracy of 85.4 on VH-Single, and GPT-4o achieves an accuracy of 70.3 on VH-Multi. These results are significantly higher than random guess (50%), which demonstrates the correctness of the dataset labels and our implementation. D.2 Correlation between NIAH and Variou Downstream Tasks We discussed the correlation between NIAH tasks and various downstream applications in Section 4.2. Here, we provide detailed version of the task correlation in Figure 11. For the three tasks in MMNIAH, we also report the correlations on the subsets containing only text-needle or only image-needle examples. We can find that subsets of image-needle examples correlate less with various downstream tasks compared to text-needle examples, especially MM-NIAH-Count (I) and MM-NIAH-Reason (I) We further show the performance of the selected models on the text-needle and image-needle subsets in Figure 12. From it, we observe that models exhibit weak performance on MM-NIAH-Count (I) and MM-NIAH-Reason (I). This challenging nature leads to low degree of separability between different models. D.3 Correlation between Datasets We plot the correlation between all MMLONGBENCH datasets and category averages in Figure 13. Generally, the datasets in each category strongly correlate with each other. The VH-Single and VHMulti are exceptions, due to their high difficulty. Also, MM-NIAH-Count exhibits relatively weak correlations with MM-NIAH-Ret and MM-NIAH-Reason, suggesting that counting is different skill from retrieving needles (key information) and subsequently reasoning over them. 34 Figure 11: Spearmans correlation at 128K input length, calculated across 46 LCVLMs, between all NIAH and other downstream tasks. Figure 12: Results of selected models on the subsets of MM-NIAH containing only text-needle or image-needle examples. (T) and (I) represent text-needle and image-needle examples, respectively. D.4 Performance of Claude At the time of our evaluation, the Claude 3 family of models can take up to 100 images6 in single request. However, few datasets, such as Food101, VH-Single, or SlildeVQA, contain hundreds of images at input lengths of 64K and 128K tokens. As result, it is impossible to process all images in single pass through the model. For each input length, if one or more datasets within category contain samples with more than 100 images, we exclude that category from evaluation at that input length. We provide the statistics about the average image number per example in each dataset at all five input lengths in Table 3 and Appendix A.6. 6https://docs.anthropic.com/en/docs/build-with-claude/vision#basics-and-limits 35 Figure 13: Spearmans correlation at 128K input length, calculated across 46 LCVLMs, between all MMLONGBENCH datasets and category averages. When testing Claude-3.7-Sonnet, we mark those untestable cases as N/A in our results to distinguish them from genuine model failures (which receive score of 0). D.5 Positional Embedding Extrapolation Experiments In this section, we evaluate two positional embedding extrapolation methods, namely YaRN [37] and V2PE [52]. Experimental results indicate that the current positional extrapolation methods still pose significant challenges for effectively extending the context window of LCVLMs. Adding YaRN to Qwen2.5-VL. According to its technical reports [29], Qwen2.5-VL models are pre-trained with context length of 32K tokens during the Long-Context Pre-Training stage. Meanwhile, its HuggingFace model card shows that we can use YaRN [37], with scaling factor of 4, to extend its context length to 128K tokens7. In Figure 14, we test the performance of using YaRN. We have two observations: (1) Using YaRN may hurt the performance on shorter input lengths. For example, at 8K tokens, the DocVQA score of Qwen2.5-VL-32B decreases from 67.8% to 63.4%; (2) On average across the entire MMLONGBENCH, YaRN only substantially improves the performance 7https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct#processing-long-texts 36 Figure 14: Results of applying YaRN [37] to Qwen2.5-VL models. We find that YaRN substantially improves the performance of Qwen2.5-VL-3B. However, the SoTA performance from larger models (i.e., 32B and 72B) only fluctuates slightly. Figure 15: Results of applying V2PE [52] to InternVL2-2B. We find V2PE is very sensitive to the visual increment δ and overfitted to NIAH tasks. Note that we use ROUGE-L here. It is already sufficient to distinguish between models, so there is no need to use the costly GPT-4o evaluation. The numbers in parentheses (i.e., 16, 64, and 256) correspond to the visual increment δ { 1 256 }, respectively. 64 , 1 16 , 1 of Qwen2.5-VL-3B (from 21.2 to 30.2 at 128K). However, the SoTA performance from large models (i.e., 32B and 72B) only fluctuates slightly. To ensure fair comparison, we do not apply YaRN to Qwen2.5-VL in our main evaluations. Since YaRN is used in zero-shot way here, applying it would lead to an unfair comparison over other models that do not employ YaRN. Adding V2PE to InternVL2. Ge et al. [52] proposed positional embedding extrapolation method called V2PE, where it assigns smaller positional increments to visual tokens than textual tokens. They further applied V2PE to InternVL2 and trained the model to enhance its performance on MM-NIAH-Ret (I). In this experiment, we evaluate the V2PE-256K checkpoints8 with the visual increments δ { 1 256 }. As shown in Figure 15, we find that (1) V2PE is very sensitive to different visual increments. For example, when we use 1 64 , the performance is even worse than InternVL2-2B, which leads to extra hyperparameter tuning; (2) The V2PE (256) shows extremely high performance on NIAH tasks, which can be attributed to the fact that it was trained on MM-NIAH. The sharp performance difference on NIAH tasks versus other categories suggests that V2PE is strongly overfitted to the MM-NIAH dataset. 16 and 1 64 , 1 16 , 1 8https://huggingface.co/OpenGVLab/V2PE/tree/main/V2PE-256K 37 D.6 Lost in the Middle Existing works found that text-pure LLMs often struggle to recall needles in the middle of the input sequence, named lost in the middle [56]. On our benchmark, we extend the previous analysis to text-image long-context tasks with input length up to 128K tokens. We place the needle at six different evenly spaced depths in the context (i.e., [0, 0.2, 0.4, 0.6, 0.8, 1.0]) and evaluate the LCVLMs ability to retrieve it. In our study, the needle may be gold passage, an image, or key sentence. We show the results in Figure 16 for ViQuAE, Figure 17 for VH-Single, Figure 18 for MM-NIAH-Ret (T), Figure 19 for MM-NIAH-Ret (I), Figure 20 for MM-NIAH-Reason (I). We observe similar phenomenon in many LCVLMs on the text-image long-context tasks. For example, the InternVL3-14B in Figure 18 and Ovis2-34B in Figure 19 both exhibit much better performance when the needle is at depths 0 and 1.0. Furthermore, as we extend the context to longer lengths (e.g., 128K tokens), we observe cases where the model tends to favor either the very beginning or the very end of the context, but not both simultaneously. For example, as shown in Figure 16, InternVL3-8B prefers the very beginning of the context (depth 0) at 128K tokens, whereas Qwen2.5-VL-72B favors the very end (depth 1.0). D.7 Error Analaysis Details We conducted two error analyses in Section 4.4. We provide more details of those two analyses in this section. First, for MMLongBench-Doc, we used PyMuPDF 9 to extract the plain text from PDF-formatted documents. For ViQuAE, the entity names are already provided in the dataset, since it is constructed based on TriviaQA. The text-pure LLMs we used for Qwen2.5-VL models are Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct, which are instruction-tuned versions. D.8 Full Model Evaluation Results In Figure 21, we provide the results of all 46 models. We also plot the performance of all 46 models on each dataset in Figures 22 to 25. D.9 Idefics2 Performance The Idefics2-8B and Idefics-8B-C only have training context window of 2K tokens [49]. We find this leads to very poor long-context generalization. Also, the LLM used in Idefics2 is Mistral7B-v0.1 [32], whose training length is only 8K tokens. From Figure 18, we observe that Idefics2 models perform well only when the needle depth is 1.0 and the context is short (8K or 16K tokens). Additionally, we conduct sanity check by removing all negative images and retaining only the needle images in Visual Haystack (i.e., one image for single-needle examples and two or three images for multi-needle examples). As shown in Table 12, we observe that both models achieve performance much higher than random guess (50%), indicating the correctness of the implementation. Table 12: Sanity check of Idefics2-8B and Idefics2-8B-C. Here we use the Visual Haystack dataset. We remove all negative images and only retain needle images (i.e., one image for single-needle examples and two or three images for multi-needle examples). Model VH-Single VH-Multi Idefics2-8B Idefics2-8B-C 79.33 69.00 67.67 58."
        },
        {
            "title": "E Prompts and Data Examples",
            "content": "We list few data examples with prompts in Figures 26 to 31. For NIAH tasks, we provide examples of both VH and MM-NIAH, as their input formats are different. 9https://pymupdf.readthedocs.io"
        },
        {
            "title": "F Limitation",
            "content": "For limitation of evaluated models, while we already provide an extensive coverage of 46 frontier LCVLMs, there are still some models that we cannot cover due to token efficiency or integration challenges of codebases, as we discussed in Appendix B.1. We leave those works for future study. Meanwhile, the largest open-source models we evaluated are up to 72B in size (Qwen2-VL-72B and Qwen2.5-VL-72B). As we discussed in Appendix C, our computational resources are limited to 8 A100 (80G) GPUs; thereby it is hard to deploy and evaluate larger models with over 100B parameters at the input length of 128K tokens, such as Llama4 [4]. For evaluating summarization, we use model-based metric (See Appendix A.4) that can provide much better alignment with human judgment than N-gram overlap metrics, such as ROUGE-L. However, we find using GPT-4o to provide the evaluation is expensive, which prevents the long-context community from conducting evaluations with hundreds or even thousands of models. Therefore, it is necessary to find an alternative evaluation method with lower cost."
        },
        {
            "title": "G Broader Impact",
            "content": "The long-context ability of LVLMs unlocked large range of applications, including understanding documents with hundreds of pages and reasoning over dozens of web pages automatically. This ability can also help users to summarize long document or revise large-scale code repository. Meanwhile, there are also large number of instruction-following scenarios grounded in complex visual contexts, such as long-term dialogue with humans or dialogue-based navigation for robots. Looking ahead, our MMLONGBENCH will serve as standard evaluation for the whole community to benchmark new LCVLMs and to stimulate the development of models with more efficient vision-language token encodings, more robust position-extrapolation schemes, and improved OCR, multi-modal retrieval, and reasoning capabilities. 39 Figure 16: Performance of models on ViQuAE at different depths. Depth is the position of the gold passage, and its values are [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], where 0.0 is the beginning of the context (the top of each heatmap) and 1.0 is the end (the bottom of each heatmap). 40 Figure 17: Performance of models on VH-Single at different depths. Depth is the position of the image containing the target object, and its values are [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], where 0.0 is the beginning of the context (the top of each heatmap) and 1.0 is the end (the bottom of each heatmap). Figure 18: Performance of models on MM-NIAH-Ret (T) at different depths. Depth is the position of the text needle, and its values are [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], where 0.0 is the beginning of the context (the top of each heatmap) and 1.0 is the end (the bottom of each heatmap). 42 Figure 19: Performance of models on MM-NIAH-Ret (I) at different depths. Depth is the position of the image needle, and its values are [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], where 0.0 is the beginning of the context (the top of each heatmap) and 1.0 is the end (the bottom of each heatmap). 43 Figure 20: Performance of models on MM-NIAH-Reason (I) at different depths. Depth is the position of the needle image used for reasoning, and its values are [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], where 0.0 is the beginning of the context (the top of each heatmap) and 1.0 is the end (the bottom of each heatmap). Figure 21: Results of all 46 models on MMLONGBENCH at various lengths. 45 Figure 22: Results of 46 models on categories VRAG and Summ at various lengths. 46 Figure 23: Results of 46 name on the category NIAH at various lengths. Figure 24: Results of 46 models on the category ICL at various lengths. 48 Figure 25: Results of 46 models on the category DocVQA at various lengths. 49 Figure 26: Example of InfoSeek dataset in the VRAG category. Figure 27: Example of Visual Haystak-Single dataset in NIAH category. Note: The input image list is shown in two columns for display clarity; in the actual input, the images are arranged in single sequence. 51 Figure 28: Example of MM-NIAH-Ret dataset in NIAH category. 52 Figure 29: Example of the Stanford Cars dataset in the ICL category. Note: The input image list is shown in three columns for display clarity; in the actual input, the images are arranged in single sequence. Figure 30: Example of GovReport in the summarization category. We only show two pages due to limited space. 54 Figure 31: Example of LongDocURL dataset in the DocVQA category. We only show three pages due to limited space."
        }
    ],
    "affiliations": [
        "CSE Department, HKUST",
        "Miniml.AI",
        "NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA",
        "Tencent AI Seattle Lab",
        "University of Edinburgh"
    ]
}