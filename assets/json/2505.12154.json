{
    "paper_title": "Learning to Highlight Audio by Watching Movies",
    "authors": [
        "Chao Huang",
        "Ruohan Gao",
        "J. M. F. Tsang",
        "Jan Kurcius",
        "Cagdas Bilen",
        "Chenliang Xu",
        "Anurag Kumar",
        "Sanjeel Parekh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/."
        },
        {
            "title": "Start",
            "content": "Chao Huang1, Ruohan Gao2, J. M. F. Tsang3, Jan Kurcius3, Cagdas Bilen3, Chenliang Xu1, Anurag Kumar3, Sanjeel Parekh3 1University of Rochester, 2 University of Maryland, College Park, 3Meta Reality Labs Research 5 2 0 2 7 1 ] . [ 1 4 5 1 2 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent years have seen significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in disconnect between visual and acoustic saliency. To bridge this gap, we introduce novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating more harmonious audio-visual experience. We propose flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce new datasetTHE MUDDY MIX DATASET, leveraging the meticulous audio and video crafting found in movies, which provides form of free supervision. We develop pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through three-step processseparation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/. 1. Introduction Be it amateur recordings of memorable moments or professionally created contenttelling story and delivering the best audio-visual experience with video requires the right balance of audio and visual elements in the scene. Consider for example, the scene in Fig. 1c that depicts video of man talking in the sea. The scene is best represented when the focus is on the person and the sea at the appropriate moments. While there are several ways to visually highlight the intended objects during capture or in post-production [51, 64], they remain relatively under-explored and limited for the acoustics. In our case above, the mans speech can Figure 1. We propose new task that aims to transform poorly mixed audio into well-balanced mix using visual guidance. One of our key insights is to use well-curated audio-visual content from movie database as free supervision to learn the appropriate highlighting effect for audio (L2H). be obscured by the sound of crashing waves. Is it possible to automatically adjust the levels of the speech and the wave sounds according to the video content to ensure they are both acoustically salient and well-balanced? naive approach would involve first demixing the sounds into different source components and then remixing them at their respective intended levels. But doing so has two major drawbacks: (i) imperfect demixing could lead to the highlighting of undesired sources, and (ii) ensuring the right temporal variations and alignment with the video manually is laborious process. Some research efforts, such as in music remixing [27, 37, 69], focus on adjusting the levels and effects of individual instruments to recompose music tracks. However, this approach is limited to the music domain, neglecting the broader needs of natural audio composition across varied media contexts. In this paper, we aim to bridge this gap by introducing novel task, visually-guided acoustic highlighting. Our approach builds on the hypothesis that the visual stream in media is often curated with intent, implicitly conveying highlighted content. In contrast, due to the limitations of recording devices, such as microphones attached to video cameras that capture all sounds indiscriminately, audio often lacks intentional mixing, resulting in poorly balanced track. The goal of our task is to use the video as guidance to transform the poorly mixed audio with appropriate highlighting effects, ensuring better output audio mix. Dataset is key requirement for training learning-based model to perform this task. We observe that movies are inherently well-curated where audio is meticulously crafted alongside video to create intentional highlighting effects. This provides us with free supervision for the acoustic highlighting effect as illustrated in Fig. 1. Consequently, we build new dataset using movie clips spanning several generes from the Condensed Movie Dataset (CMD) [2]. To simulate real-world scenarios where audio may be poorly mixed or require enhancement, we introduce pseudo-data generation process that begins with high-quality movie audio and then applies imperfect separation, followed by adjustment and remixing of individual audio sources. We tackle acoustic highlighting as an audio-to-audio translation problem and propose transformer-based Visually-guided Acoustic Highlighting (VisAH) model. VisAH uses U-Net-like audio backbone with dual encoder [7] that takes both the spectrogram and the waveform as inputs to extract latent representations from the poorly mixed audio. In this latent space, transformer encoder processes context, such as the video stream or its corresponding caption, and transformer decoder with cross-attention integrates this video context to guide the transformation. The decoder then converts the poorly mixed audio representation into highlighted audio. This design is flexible, supporting easy adjustments in both the backbone and latent modules. Specifically, our model combines video context encoding and visually-guided audio decoding, enabling it to fully capture temporal and semantic trends in the video and leverage them for effective audio highlighting. To summarize, our main contributions are threefold: We propose to intelligently highlight the audio content in video guided by visual cues, and we design VisAH, mutimodal transformer-based model to achieve that goal. Leveraging the free supervision from movies, where both audio and video are already meticulously crafted, we introduce THE MUDDY MIX DATASET, new dataset curated for this task. Our method outperforms series of baselines, effectively highlighting audio across different types of video content. 2. Related Work Audio Remixing. Highlighting mixed audio track is identical to rebalancing its individual sources, i.e., transferring from one mixing style to another. In previous research, music mixing [27, 39, 50, 61] has been extensively studied, including creative manipulations that shape songs emotive and sonic identity. Reproducing the mixing style of target song typically involves balancing tracks using audio effects to achieve harmony and aesthetic appeal, often through knowledge-based [45] or learning-based [39, 56, 61] approaches. Related but different from these methods that focus on music, we handle broad range of sounds, including speech, music, and sound effects. Moreover, we propose to use visual cues in videos to guide the highlighting process. Video Highlight and Saliency Detection. Web videos, often created by professionals, are typically edited, such as trimming to capture key moments or adjusting camera focus to highlight engaging regions. This has led to the development of video understanding tasks, including video highlight detection [30, 34, 36, 41], which identifies key temporal segments, and video saliency prediction [16, 2325, 29], which identifies salient regions within scene. Developing methods to smartly emphasize the right visual content [22, 54] in video and the development of corresponding benchmark datasets [15, 30] have become essential areas of study. In our work, we focus on the mirror side of the problem: assuming the video is already well-curated to visually convey highlight information, as in movies, we leverage this visual narrative to highlight the audio stream accordingly. Audio-Visual Learning. Exploring connections between acoustic and visual signals has been widely studied across various tasks, including audio-visual localization [17, 19, 40, 47, 5860], representation learning from cross-modal supervision [1, 12, 43], audio-visual learning for 3D scenes [21, 31, 32], audio-spatialization leveraging visual cues [9, 13, 42], and sound generation from videos [3, 5, 33, 44]. Differently, we tackle new challenging audio-visual learning task, visually-guided acoustic highlighting. Most closely related to our task is visually-guided audio separation [10, 11, 18, 20, 43, 71], which addresses specific case of our problem by isolating target sound and suppressing others to zero, thereby achieving separation. However, these methods lack well-defined output for balanced mix and do not address the creation of poorly mixed inputs. Our approach offers new perspective by focusing on remixing audio to produce coherent, visually aligned output. 3. Approach We introduce the task of visually-guided acoustic highlighting, which aims to transform audio using visual guidance to achieve an appropriate highlighting effect. In Sec. 3.1, Figure 2. Overview of VisAH: (a) Our model takes poorly mixed audio waveform as input and produces the highlighted audio using dual U-Net architecture. For simplicity, skip connections are omitted in the illustration. (b) The latent highlighting transformer incorporates vision and text encoders to integrate temporal information, guiding the transformer decoder to transform audio features effectively. we elaborate on the task formulation and Sec. 3.2 discusses design of our proposed multimodal model VisAH. 3.1. Task Formulation Let RTvHW 3 represent the visual stream, sequence of Tv RGB images, and let RTa be the corresponding audio sequence. Here is considered poorly mixed audio sequence that lacks the intended highlighting effect, as is common in raw daily recordings where audio is directly captured without deliberate curation. Our goal is to predict an audio signal that preserves the content of but conveys the appropriate highlighting effects. In other words, we learn mapping from the poorly mixed audio and the video sequence to corresponding highlighted audio signal: a, (cid:55) s. (1) Creating highlighting effect in audio involves rebalancing the sources at different levels to reflect their relative prominence. When describing system design next we will assume access to data tuples of the form (a, v, s). How such dataset is created and utilized in practice will be the subject of our discussion in Sec. 4. 3.2. VisAH: Visually-guided Acoustic Highlighting To address the problem formulated in Eq. (1), we focus on two primary design choices: (i) how to structure the audio framework to accept poorly mixed audio as input and produce highlighted audio, and (ii) how to effectively incorporate contextual information, such as video streams or other modalities like text, to guide the acoustic highlighting. In this section, we elaborate on our framework design, presenting flexible approach that includes both an audio backbone and context-aware module. The overall architecture of VisAH is illustrated in Fig. 2."
        },
        {
            "title": "3.2.1 Audio Backbone",
            "content": "The audio backbone is designed to produce an output with the same shape as the input, making U-Net [52] architectures particularly suitable. We consider two common types of audio input: time-domain and frequency-domain. Frequency-domain input, often used in audio-visual separation tasks [4, 60, 71], captures distinct frequency patterns of sounds, while time-domain input, frequently employed in audio-only separation [38, 57], provides higher accuracy in reconstructing the final waveform. In this work, we unify the advantages of both input types within our audio backbone, offering flexibility for future studies to utilize either or both. Based on the HybridDemucs architecture [7], we implement dual U-Net model with two branches: one for spectrogram inputs and the other for waveform inputs. Spectrogram U-Net Encoder. Given an input audio signal RTa , we apply Short-Time Fourier Transform (STFT) to with window size of 4096 and hop length of 1024 to obtain its spectrogram. Specifically, we use the magnitude spectrogram as input, denoted by A. In the original HybridDemucs [7], the spectrogram is normalized using its mean and standard deviation. However, we omit this normalization, as we found that mean normalization can significantly suppress ambient sounds, reducing the models sensitivity to sound effects. The magnitude encoder consists of 5 layers, with each layer reducing the number of frequency bins by factor of 4, except for the final layer, which reduces it by factor of 8. After passing through the magnitude encoder, the frequency dimension is reduced to 1, aligning it with the output shape from the waveform branch. Details of the encoder design can be found in the supplementary materials. Waveform U-Net Encoder. In this framework, the waveform branch acts as residual path to capture fine-grained temporal details. To facilitate processing, we normalize the waveform input a. The encoder design mirrors that of the spectrogram U-Net encoder, with the main difference being the use of 1D convolutions instead of 2D convolutions. Latent Highlighting Module. With both the magnitude and waveform embeddings in the same shape, we add them element-wise to create unified audio embedding. An additional encoder layer then reduces the temporal dimension by half, producing fa RCaL, where represents the temporal dimension of the latent audio features. To transform fa into highlighted audio representations, we design latent highlighting module that incorporates both the audio features and contextual information (such as the encoded features of video streams or other multi-modal input) to output the features representing the highlighted audio, denoted as: (2) ˆfa = Hθ(fa, c), where θ is the model parameters. Since both the latent audio features and the contextual input are temporal signals, we utilize transformer-based framework to process them effectively, as depicted in Fig. 2(b). Waveform/Spectrogram U-Net Decoder. The refined features ˆfa will first pass through an additional decoder layer to double the temporal length. Next, the output features serve as the input to both the waveform and spectrogram U-Net decoders, each mirroring the structure of the corresponding encoder. The spectrogram decoder outputs predicted ratio mask, denoted as M, which represents the highlighting information. We multiply the mask with the original magnitude spectrogram element-wise to obtain refined magnitude A. Then we apply inverse STFT on this refined magnitude, using the phase information from the input to reconstruct the output waveform. In addition, the audio output from the waveform decoder is then combined with the spectrogram-based waveform output, producing the final prediction ˆs."
        },
        {
            "title": "3.2.2 Latent Highlighting Transformer",
            "content": "We now discuss the design of latent highlighting module Hθ introduced in Eq. (2). Latent audio features fa from the audio backbone capture temporal and semantic characteristics of the poorly mixed audio. To transform these features into representations that convey appropriate highlighting effects, we consider two key insights: (i) Audio captures information from the entire surrounding environment, while the visual field-of-view is narrower, focusing on salient regions and content. This necessitates leveraging the temporal dynamics of the visual context as guidance for acoustic highlighting. (ii) In movies, complex interactions often occur between different sources (such as speech, music, and sound effects), with music saliency, in particular, driven by emotional cues. Relying solely on visual signals may not fully convey these nuanced relationships. This prompts the question: can additional modality enhance this process? To study this, we design transformer-based latent highlighting module Hθ that can flexibly incorporate various types of temporal context, such as video streams or text captions. Context Encoding. Given the video sequence v, we use CLIP ViT-L/14 [48] to transform each frame into feature vector, denoted as fvid RCvidTv . To address the second insight mentioned above, we incorporate text captions as an additional modality. Vision Language Models (VLMs) have demonstrated impressive capabilities in summarizing images and reasoning about text. We leverage text captions as bridge to convey deeper sentiment and context beyond raw visual features. To generate captions automatically for each frame, we use InternVL2-8B [6]. Each caption is then embedded using T5-XXL encoder [49], resulting in textual embeddings denoted as ftext RCtextTv . Since raw frame and text features are extracted at per-frame level and lack temporal interaction, we apply transformer encoder for each modality to capture the temporal context, denoted as Evid and Etext. To preserve temporal order, we add sinusoidal positional encoding [62] to the input of each transformer encoder layer. We can concisely define contextual information encoding as sequence of the following operations: (3) ˆfi = Ei(Gi(fi)), where {video, text}, Gi() is linear projection layer to project Ci to C, the same channel dimension as fa. Context-aware Acoustic Highlighting. We use transformer decoder, D, to generate highlighted acoustic representations. Sinusoidal positional encoding is added to fa. The transformer decoder consists of multiple layers, each containing self-attention layer, cross-attention layer, and feed-forward layer. Rather than directly treating the decoders output as the final prediction, we interpret it as an offset to the original features, adding it back to fa to preserve the audios semantic content while adjusting inter-source differences. Additionally, we incorporate zero-initialized convolution layer [70], denoted Z(). This layer is 11 convolution with both weights and biases initialized to zero. The overall process is described as: ˆfa = fa + Z(D(fa, ˆfi)), (4) where the context ˆfi can include visual, textual, or both types of contextual information. In relation to Eq. (2), Hθ acts as the integrative component that connects D, Evid, and potentially Etext."
        },
        {
            "title": "3.2.3 Training and Inference",
            "content": "Our VisAH framework takes the input audio along with visual context or its textual captions to predict the highlighted audio ˆs. The loss is computed at the waveform level and backpropagated through the network. In this work, we use multiscale STFT (MR-STFT) loss [68] between the We select the CMD [2] as our data source, which includes 33,976 clips from 3,605 diverse movies spanning various genres, countries, and decades, covering salient parts of each film. Each clip is approximately two minutes long. We concentrate on films tagged in the Action category, ensuring good presence of multiple acoustic sources beyond speech and music. As movie in CMD may belong to multiple categories, our selection still remains diverse and covers range of genres. We leave further expansion of the dataset to future work. To prepare the data for training and evaluation, we segment each movie clip into 10-second segments and extract the video stream at 1 fps using ffmpeg, while filtering out segments that lack an audio stream1. Separate, Adjust, and Remix. Given high-quality movie audio s, we prepare the poorly mixed input through three-step process as shown in Fig. 3: 1. Separation. In practice, audio may consist of an infinite variety of sources, making it impractical to separate and remix every possible source individually. We follow the Cinematic Sound Demixing Challenge [8], which segments audio into three broad categories: speech, music, and sound effects. Accordingly, we apply three-stem separation model trained for cinematic audio source separation on the DnR v3 dataset [65], to decompose into three substreams: ˆsh (speech), ˆsm (music), and ˆse (sound effects). Additionally, we calculate any residual component, ˆsr, to ensure that = ˆsh + ˆsm + ˆse + ˆsr. This formulation guarantees that even if the separation is imperfect, the sum of all components matches the original audio track. 2. Adjustment. Using these imperfect separations, we alter their original relative levels, creating an input audio signal that intentionally mismatches the videos highlighting effect. Specifically, we adjust the relative loudness of each stream. We first measure the original loudness of each separated source using the pyloudnorm library [55]. For the source with the highest loudness, we apply Suppress action, reducing its loudness by randomly selected strength from the categories [high, moderate, low]. For the other two sources, we apply Highlight action, increasing their loudness by value chosen from [high, moderate, low]. To retain the original mixtures content, we use the combined track ˆse + ˆsr for the sound effects input signal. We implement the loudness adjustments as follows: [high, moderate, low] for highlighting corresponds to increases of {12, 9, 6} dB, while for suppressing, we apply decreases of {12, 9, 6} dB. 3. Remixing. After adjusting loudness, we remix the three sources linearly to create poorly mixed input that contrasts with the ground truth highlighting effect. 1All data collection and processing was done at the University of Rochester Figure 3. We generate poorly mixed audio from the well-mixed movie audio through the following steps: 1) Separation: We separate the ground truth movie audio into individual tracks for speech, music, and sound effects, allowing for some imperfections in the separation process; 2) Adjustment: For each separated track, we apply either suppression or emphasis, with the intensity selected from three levels: [high, moderate, low]; 3) Remixing: Finally, we combine the adjusted tracks through simple addition to create the poorly mixed input audio. predicted audio ˆs and the ground truth audio s, which is implemented by computing the ℓ1 distance between their amplitude spectrograms: = MR-STFT(ˆs, s). (5) The window sizes are set to 2048, 1024, and 512. It is worth noting that the training loss is intentionally simple, and any arbitrary waveform or spectrogram loss could be applied. We demonstrate that even standard loss can effectively drive training, leaving further exploration of loss design to future work. At test-time, given badly mixed input audio and the associated video frames as context, VisAH outputs wellhighlighted audio that is coherent both temporally and semantically with the provided visual guidance. 4. THE MUDDY MIX DATASET It is worth reiterating that training our model requires access to badly mixed input audio, well-highlighted output audio and the associated visual frames, as shown in Eq. (1). Our key observation is that movies serve as reliable source of well-mixed data, implicitly conveying what good highlighting and its synchronization with video sounds like. Our final requirement of having access to the corresponding badly mixed input audio is satisfied through the data modification process described in this section. It essentially involves demixing, adjusting and then remixing movie audio so as to disturb its original highlighting effect. Table 1. Main comparison: The best results are highlighted in bold, while the second best are highlighted with underline. We report metrics on waveform distance, semantic alignment, and time alignment. All results are multiplied by 100. Method Poorly Mixed Input DnRv3 [65]+CDX [8] Learn2Remix [69] LCE-SepReformer [26] VisAH (Ours) MAG ENV KLD IB W-dis 22.69 26.32 (16%) 19.07 (+16%) 17.18 (+24%) 10.08 (+56%) 6.30 7.62 (21%) 4.16 (+34%) 4.28 (+32%) 3.43 (+46%) 20.61 15.87 (+23%) 61.76 (199%) 30.99 (50%) 11.01 (+47%) 1.52 1.78 (17%) 8.27 (444%) 1.88 (24%) 0.80 (+47%) 1.94 2.84 (46%) 1.20 (+38%) 1.28 (+34%) 0.79 (+59%) Following this procedure, we generate input audio for each video clip, resulting in 15,078/1,927/1,789 clips for train/validation/test sets, respectively. Table 2. Ablation study on different context types. We compare no-context baseline with models using semantic (single frame or text caption) and temporal context (multiple frames or captions). 5. Experiments 5.1. Experimental Setting Implementation Details. In our experimental setup, the audio waveform is sampled at 44 kHz in stereo. We convert the input to mono by averaging the two stereo channels. Within the encoders, we set the dimensionality of the audio latent representation fa to Ca = 768, with the original channel dimensions for visual and text features set to Cvid = 768 and Ctext = 4096, respectively. During training, we use batch size of 12 per GPU and the Adam optimizer with learning rate of 0.0001. The model is trained for 200 epochs. All experiments are conducted on two RTX 4090 GPUs, with training taking approximately 18 hours to complete. Evaluation Metrics. We employ the following groups of objective metrics to evaluate output quality: Waveform distance: The simplest way to assess the closeness of the prediction to the target is through waveform distance. We use magnitude distance (MAG) [67] to evaluate audio quality in the time-frequency domain and envelope distance (ENV) [32] to assess quality in the time domain. Semantic alignment: Since our goal is to adjust the relative distribution of audio across three categories: human speech, music, and sound effects. We apply KL divergence (KLD) [35, 63] using the pre-trained PaSST [28] model to compare the label distributions of the target and generated audio. Additionally, given that the video provides guidance, we assess audio-to-video semantic relevance using the ImageBind [14] model, calculated as the cosine similarity between audio and video embeddings, denoted as IB score. Since we have the target movie audio, we use the difference between the target and predicted IB scores: IB = IB(v, s) IB(v, ˆs). (6) Time alignment: The relative variation between underlying sources (speech, music, and sound effects) can lead to significant timing differences, as each track follows its own temporal pattern. To test how well the model highlights all sources, we measure the minimum cost to align Context No Context +Semantic Vision +Semantic Text +Temporal Vision +Temporal Text MAG KLD IB 10.35 11.95 10.35 10.32 10.24 10. 11.67 11.83 11.18 11.01 0.99 0.91 0.84 0.88 0.80 the predicted audio distribution with the target distribution. This is quantified using Wasserstein Distance (W-dis)2. 5.2. Baselines This is novel task with no prior works. We adapt several methods for relevant and fair comparison with VisAH: Poorly Mixed Input: This is the manually created poorly mixed input according to our dataset creation strategy, serving as reference point for comparison. DnRv3 [65]+CDX [8]: To remix speech, music, and sound effects from the input and generate highlighted audio, we include an empirical baseline that adheres to the loudness distribution of these sources as specified by the CDX [8] challenge. We first apply the DnRv3 [65] separator to split the input audio into three tracks: speech, music, and sound effects. Next, we sample loudness values for each track according to their respective distributions. Finally, we adjust the loudness of each source and remix them to create the output audio. Learn2Remix [69]: Learn2Remix (L2R) utilizes ConvTasNet [38] as its backbone model to predict and remix different audio sources within feature spaces, making it well-suited for our task of adjusting and rebalancing the underlying speech, music, and sound effects. In our implementation, we adopt the more advanced SepReformer [53] model to replace the ConvTasNet backbone. We use the official code and train it on our dataset. Listen, Chat and Edit (LCE) [26]: LCE is text-guided sound mixture editor capable of performing various audio editing tasks, such as adjusting the volume of specific 2https : / / en . wikipedia . org / wiki / Wasserstein _ metric. Table 3. Ablation study on transformer encoders Evid and Etext. and represent vision frames and text captions, respectively. Note that the text captions are obtained automatically from the video. Table 4. Ablation study on dataset difficulty. We report the performance of Input (-I) and our Predictions (-P) at the three different levels of dataset difficulty. #layers #params context MAG KLD W-dis 0 3 6 0 3 6 55.3M 61.6M 67.9M 55.4M 61.7M 68.0M V 10.36 10.24 10.69 10.66 10.34 10.08 10.91 11.18 12.42 12.53 11.75 11. 0.83 0.81 0.83 0.85 0.81 0.79 Level High-I High-P Moderate-I Moderate-P Low-I Low-P MAG ENV KLD IB W-dis 27.70 12.03 22.70 8.73 16.40 9. 7.64 3.93 6.25 3.23 4.48 3.20 32.52 16.11 20.59 11.08 9.89 7. 2.35 1.25 1.50 0.81 0.75 0.35 2.38 0.97 1.92 0.65 1.37 0. sources based on text instructions. However, in our setup, we assume that explicit instructions on which sounds to highlight are unavailable and instead should be inferred from visual cues. To ensure fair comparison, we provide text captions as guidance for LCE. Originally, LCE uses ConvTasNet [38] and Sepformer [57] as the SoundEditor models. For fair comparison, we also replace the backbone with the SepReformer [53] model. 5.3. Quantitative Results"
        },
        {
            "title": "5.3.1 Comparison with Baselines",
            "content": "We compare our method with the baselines, and the results are presented in Tab. 1. The empirical baseline DnRv3+CDX performs worse than the input in waveform distance and time alignment metrics because it relies on non-specific statistical distribution rather than data-dependent remixing. However, it outperforms the other two baselines in KLD, which we hypothesize is due to the versatile loudness distributions of the movies in CDX; remixing the speech, music, and sound effects at those levels shifts the distribution toward real movies. On the other hand, Learn2Remix, an audioonly baseline, struggles to enhance audio without guidance. It improves the poorly mixed input in terms of waveform distance, which occurs because it learns the global loudness distribution of our dataset. However, it fails to achieve the necessary semantic alignment, reinforcing the need for contextual input. For LCE, we utilize text captions as guidance for acoustic highlighting, resulting in better outcomes compared to Learn2Remix. However, it still underperforms significantly when compared to our approach. This is because it is not method designed for acoustic highlighting and lacks the ability to capture the global trends required for the task. In contrast, VisAH demonstrates strong performance across all metrics, showcasing the effectiveness of our proposed framework. tual guidance. This means that the model relies solely on the input audio to learn how to highlight relevant information. However, when we incorporate single frame or its corresponding text caption, the semantic alignment metrics, KLD and IB, show improvement. This indicates that context plays crucial role in enhancing the models performance. Since audio is time sequence, the highlighting effects should ideally capture certain temporal patterns. We further conduct ablation experiments using the full length of video frames and captions, referring to this as temporal vision or text. The performance shows significant boost, underscoring the importance of temporal context. Number of Transformer Encoder Layers. We evaluate the impact of encoding temporal context by varying the number of transformer encoder layers. Without transformer encoders, video and text features are encoded frame by frame, which results in lack of interaction across time steps. As shown in Table Tab. 3, we find that increasing the number of layers generally improves performance, suggesting that temporal context reasoning is essential for effectively understanding video-level content. Specifically, we observe continuous improvement when using text context, while performance with vision context initially improves but then deteriorates when the number of layers further increases, which we hypothesize that the CLIP vision features are already compact. Analysis of Dataset Difficulty. In Sec. 4, we outline three levels of adjustments that can be made during the creation of the dataset, which are randomly selected. In Tab. 4, we provide an ablation of the impact of these difficulty levels. We create three test sets that consist solely of low, moderate, or high levels of adjustments. Our observations show continuous improvements in metrics as the difficulty of the dataset decreases. This supports both the design of our dataset and metrics, as well as the generalization capability of training on randomly selected levels."
        },
        {
            "title": "5.3.2 Ablations",
            "content": "5.4. Qualitative Analysis In this section, we review the design of context encoding and context choice, providing further insights into the task setup. Does Contextual Information Matter? In Tab. 2, we present naive baseline that does not utilize any contexQualitative Visualizations. We display the magnitude spectrograms and waveforms of the highlighted audio produced by various methods, along with the input and ground truth in Fig. 4. These visualizations illustrate that our method efFigure 4. We perform qualitative comparison by visualizing the waveform and magnitude spectrograms of the highlighted audio results from different methods, along with the input and ground truth. Our method produces results that are closest to the movie GT. The orange box denotes suppressed snippets, and green box indicates highlighted snippets. Figure 5. Subjective test: We ask users to rank the four methods based on audio-visual balance to evaluate acoustic highlighting. Figure 6. We demonstrate that our VisAH method enhances the quality of video-to-audio generation results. fectively captures temporal variations and performs acoustic highlighting across speech, music, and sound effect sources. of the videos, indicating strong highlighting performance, comparable to actual films at times. Subjective Test. We conduct subjective test to compare the highlighting results of our model with those of the LCE baseline, as well as the input and ground truth. Nine participants evaluated ten videos, each featuring four different audio tracks generated by various methods. They ranked the four methods based on perception of the balance between audio and visual quality. Our method achieves top-2 ranking rate of 77%, outperforming the LCE baseline and the input by 63% and 62%, respectively, as shown in Fig. 5. Interestingly, our method even surpasses the GT for 34% Application: Refinement of Video-to-Audio Generation. Our VisAH has several potential downstream applications, one of which is refining video-to-audio generation. In Fig. 6, we demonstrate that by using the audio from MovieGen [46] as input, along with the video as guidance, our VisAH produces audio that achieves better IB score. This indicates enhanced audio-visual alignment, and human preferences confirm these improvements. We encourage the readers to see and listen to examples on our demo webpage in the attached supplementary materials. 6. Conclusion We presented new taskvisually-guided acoustic highlightingto bridge the gap between visual and acoustic saliency in video content. To address this task, we have proposed VisAH, transformer-based multimodal framework that uses visual information to guide audio highlighting. By leveraging movies for free supervision, we develop pseudo-data generation process that simulates real-world video quality, allowing for labor-free training setup. Our evaluations show that our approach outperforms several baselines in both objective and human perceptual assessments. This framework enhances the alignment of audio-visual cues, offering more cohesive viewing experience."
        },
        {
            "title": "References",
            "content": "[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. European Conference on Computer Vision (ECCV), 2020. 2 [2] Max Bain, Arsha Nagrani, Andrew Brown, and Andrew Zisserman. Condensed movies: Story based retrieval with contextual embeddings, 2020. 2, 5 [3] Changan Chen, Puyuan Peng, Ami Baid, Sherry Xue, WeiNing Hsu, David Harwath, and Kristen Grauman. Action2sound: Ambient-aware generation of action sounds from egocentric videos. In ECCV, 2024. 2 [4] Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, Ziyao Instruments as queries Zeng, and Jianbo Shi. In Proceedings of the for audio-visual sound separation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1467514686, 2023. 3 iquery: [5] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and Chenliang Xu. Deep cross-modal audio-visual generation. In Proceedings of the on Thematic Workshops of ACM Multimedia 2017, 2017. [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 4 [7] Alexandre Défossez. Hybrid spectrogram and waveform source separation. In Proceedings of the ISMIR 2021 Workshop on Music Source Separation, 2021. 2, 3 [8] Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco A. Martinez-Ramirez, Wei-Hsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stoter, Alexandre Defossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Prasanna Mohanty, Roman A. Solovyev, Alexander L. Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, and Yuki Mitsufuji. The sound demixing challenge 2023 - music demixing track. Trans. Int. Soc. Music. Inf. Retr., 7:6384, 2023. 5, 6 [9] Ruohan Gao and Kristen Grauman. 2.5 visual sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 324333, 2019. 2 [10] Ruohan Gao and Kristen Grauman. Visualvoice: Audiovisual speech separation with cross-modal consistency. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1549015500. IEEE, 2021. 2 [11] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning to separate object sounds by watching unlabeled video. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3553, 2018. [12] Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman. Visualechoes: Spatial visual representation learning through echolocation. In European Conference on Computer Vision (ECCV), 2020. 2 [13] Rishabh Garg, Ruohan Gao, and Kristen Grauman. Geometryaware multi-task learning for binaural audio generation from video. In British Machine Vision Conference (BMVC), 2021. 2 [14] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 6 [15] Yury Gitman, Mikhail Erofeev, Dmitriy Vatolin, Bolshakov Andrey, and Fedorov Alexey. Semiautomatic visual-attention modeling and its application to video compression. In 2014 IEEE international conference on image processing (ICIP), pages 11051109. IEEE, 2014. 2 [16] Jonathan Harel, Christof Koch, and Pietro Perona. Graphbased visual saliency. Advances in neural information processing systems, 19, 2006. 2 [17] Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding objects localization via self-supervised audiovisual matching. Advances in Neural Information Processing Systems, 33, 2020. 2 [18] Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Davis: High-quality audio-visual separation with generative diffusion models. arXiv preprint arXiv:2308.00122, 2023. [19] Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Egocentric audio-visual object localization. arXiv preprint arXiv:2303.13471, 2023. 2 [20] Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Scaling concept with textguided diffusion models. arXiv preprint arXiv:2410.24151, 2024. 2 [21] Chao Huang, Dejan Markovic, Chenliang Xu, and Alexander Richard. Modeling and driving human body soundfields through acoustic primitives. arXiv preprint arXiv:2407.13083, 2024. 2 [22] Chao Huang, Susan Liang, Yunlong Tang, Li Ma, Yapeng Tian, and Chenliang Xu. Fresca: Unveiling the scaling space in diffusion models. arXiv preprint arXiv:2504.02154, 2025. 2 [23] Laurent Itti, Christof Koch, and Ernst Niebur. model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):12541259, 1998. [24] Samyak Jain, Pradeep Yarlagadda, Shreyank Jyoti, Shyamgopal Karthik, Ramanathan Subramanian, and Vineet Gandhi. Vinet: Pushing the limits of visual modality for audio-visual saliency prediction. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3520 3527. IEEE, 2021. [25] Lai Jiang, Mai Xu, Tie Liu, Minglang Qiao, and Zulin Wang. Deepvs: deep learning based video saliency prediction approach. In The European Conference on Computer Vision (ECCV), 2018. 2 [26] Xilin Jiang, Cong Han, Yinghao Aaron Li, and Nima Mesgarani. Listen, chat, and edit: Text-guided soundscape modification for enhanced auditory experience. arXiv preprint arXiv:2402.03710, 2024. 6, 1 [27] Junghyun Koo, Marco Martínez-Ramírez, Wei-Hsiang Liao, Stefan Uhlich, Kyogu Lee, and Yuki Mitsufuji. Music mixing style transfer: contrastive learning approach to disentangle audio effects. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 1, 2 [28] Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. In Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022, pages 27532757. ISCA, 2022. 6 [29] Alexander Kroner, Mario Senden, Kurt Driessens, and Rainer Goebel. Contextual encoderdecoder network for visual saliency prediction. Neural Networks, 129:261270, 2020. 2 [30] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34: 1184611858, 2021. [31] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Neural acoustic context field: Rendering realistic room impulse response with neural fields. arXiv preprint arXiv:2309.15977, 2023. 2 [32] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Av-nerf: Learning neural fields for realworld audio-visual scene synthesis. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 2, 6 [33] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Language-guided joint audio-visual editing via one-shot adaptation. In Proceedings of the Asian Conference on Computer Vision, pages 10111027, 2024. 2 [34] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified video-language temporal grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27942804, 2023. 2 [35] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 6 [36] Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30423051, 2022. 2 [37] Chien-Yu Lu, Min-Xin Xue, Chia-Che Chang, Che-Rung Lee, and Li Su. Play as you like: Timbre-enhanced multi-modal music style transfer. In Proceedings of the aaai conference on artificial intelligence, pages 10611068, 2019. [38] Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8):12561266, 2019. 3, 6, 7 [39] Marco Martínez Ramírez, Emmanouil Benetos, and Joshua Reiss. Deep learning for black-box modeling of audio effects. Applied Sciences, 10(2):638, 2020. 2 [40] Shentong Mo and Pedro Morgado. Localizing visual sounds the easy way. arXiv preprint arXiv:2203.09324, 2022. 2 [41] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2302323033, 2023. 2 [42] Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, and Oliver Wang. Self-supervised generation of spatial audio for 360 video. Advances in neural information processing systems, 2018. 2 [43] Andrew Owens and Alexei Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 631648, 2018. 2 [44] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward Adelson, and William Freeman. Visually indicated sounds. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. [45] Julian Parker, Sebastian Schlecht, Rudolf Rabenstein, and Maximilian Schäfer. Physical modeling using recurrent neural networks with fast convolutional layers. arXiv preprint arXiv:2204.10125, 2022. 2 [46] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 8, 1 [47] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. In European Conference on Computer Vision, pages 292308. Springer, 2020. 2 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4 [49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with Robert Adkins, William Ngan, et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023. [64] Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. Lave: Llm-powered agent assistance and language augmentation for video editing. In Proceedings of the 29th International Conference on Intelligent User Interfaces, pages 699714, 2024. 1 [65] Karn Warcharasupat, Chih-Wei Wu, and Iroro Orife. Remastering divide and remaster: cinematic audio source separation dataset with multilingual support. In 2024 IEEE 5th International Symposium on the Internet of Sounds (IS2), pages 110. IEEE, 2024. 5, 6 [66] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. In CVPR, 2024. 1 [67] Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, and Dahua Lin. Visually informed binaural audio generation without binaural audios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1548515494, 2021. 6 [68] Ryuichi Yamamoto, Eunwoo Song, Min-Jae Hwang, and JaeMin Kim. Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 60396043. IEEE, 2021. 4 [69] Haici Yang, Shivani Firodiya, Nicholas Bryan, and Minje Kim. Dont separate, learn to remix: End-to-end neural remixing with joint optimization. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 116120. IEEE, 2022. 1, 6 [70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 4 [71] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound In Proceedings of the European conference on of pixels. computer vision (ECCV), pages 570586, 2018. 2, unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [50] Marco Martínez Ramírez and Joshua Reiss. Modeling nonlinear audio effects with end-to-end deep neural networks. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 171175. IEEE, 2019. 2 [51] Anyi Rao, Xuekun Jiang, Yuwei Guo, Linning Xu, Lei Yang, Libiao Jin, Dahua Lin, and Bo Dai. Dynamic storyboard generation in an engine-based virtual environment for video production. In ACM SIGGRAPH 2023 Posters, pages 12. 2023. 1 [52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 3 [53] Ui-Hyeop Shin, Sangyoun Lee, Taehan Kim, and HyungMin Park. Separate and reconstruct: Asymmetric encoderdecoder for speech separation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 6, 7 [54] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In CVPR, 2024. 2 [55] Christian J. Steinmetz and Joshua D. Reiss. pyloudnorm: simple yet flexible loudness meter in python. In 150th AES Convention, 2021. [56] Christian Steinmetz, Nicholas Bryan, and Joshua Reiss. Style transfer of audio effects with differentiable signal processing. arXiv preprint arXiv:2207.08759, 2022. 2 [57] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. Attention is all you need in In ICASSP 2021-2021 IEEE Internaspeech separation. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2125. IEEE, 2021. 3, 7 [58] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 247263, 2018. 2 [59] Yapeng Tian, Dingzeyu Li, and Chenliang Xu. Unified multisensory perception: Weakly-supervised audio-visual video parsing. In European Conference on Computer Vision, pages 436454. Springer, 2020. [60] Yapeng Tian, Di Hu, and Chenliang Xu. Cyclic co-learning of sounding object visual grounding and sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27452754, 2021. 2, 3 [61] Soumya Sai Vanka, Christian Steinmetz, Jean-Baptiste Rolland, Joshua Reiss, and George Fazekas. Diff-mst: Differentiable mixing style transfer. arXiv preprint arXiv:2407.08889, 2024. 2 [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang,"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Project Page We have created project page (https://wikichao. github.io/VisAH/) to illustrate our method and showcase our results. We strongly encourage readers to visit this webpage and use headphones. Please note that the webpage may not be fully compatible with the Safari browser; therefore, we recommend using Google Chrome for an optimal viewing experience. On the demo page, we show the following applications: Comparisons to Other Methods. We present examples from THE MUDDY MIX DATASET, showcasing the following: the input poorly mixed video (which is created through the process described in Sec.4, the highlighting results produced by LCE [26], the outputs from our VisAH model, and the original movie clips for comparison. Video-to-Audio (V2A) Generation Refinement. Generating audio from video has recently gained popularity due to impressive video generation results and the growing demand for an immersive audio-visual experience. Existing V2A models, such as Seeing-and-Hearing [66] and the more recent MovieGen [46], have demonstrated promising outcomes. However, these methods primarily focus on generating temporally aligned audio for videos, which can sometimes neglect the subtle differences between audio sources. Our approach, inspired by cinematic techniques, serves as post-processing method to enhance audio quality in these cases. Real Web Video Refinement. Unlike movies, web videos are often recorded in less controlled environments, which can lead to undesirable effects. For example, viewers may experience an overpowering personal voice in egocentric videos or focus on distracting sound sources due to distance or background noise. In this context, we apply our model to web videos, aiming to deliver an improved cinematic-like audio-visual experience. Figure 7. Failure case analysis: the sound effect (waterfall) overwhelms the speech. Figure 8. Failure cases analysis: Our method highlights the breathing sound based on the video context but diverges from the movie audio ground truth. 8. Failure Case Analysis While our VisAH model is effective at highlighting audio guided by video content, there are scenarios where it might fail. Here, we provide case studies to illustrate the conditions under which such failures occur. In Fig. 7, the video captures natural waterfall scene with people hiking. The audio stream predominantly features the sound of the waterfall, with occasional moments of speech. Ideally, our VisAH model should balance these two audio sources to enhance the audio-visual experience. However, due to the overwhelming dominance of the waterfall sound, the speech becomes difficult to perceive. This results in the model failing to properly highlight the speech. As shown in Fig. 7, the input and output audio remain similar in this case, highlighting the challenge of separating and emphasizing speech under such conditions. In Fig. 8, we present an example where our method fails to align perfectly with the original movie ground truth. Specifically, the breathing sound between 7 and 10 seconds is not emphasized in the movies ground truth audio. However, the corresponding video frames during this period show close-up Figure 10. Design of magnitude and waveform encoders. Each encoder consists of five layers. The features from the waveform and magnitude encoders are combined through element-wise addition after the fifth layer, followed by an additional layer to encode the fused features. 10. Network Details We detail the design of the magnitude and waveform encoders, along with their input and output dimensions. As illustrated in Fig. 10, each encoder consists of five layers, and the output shapes for both branches after the fifth layer are identical. At each layer, the output features are used for skip connections (not shown in the figure). This design facilitates straightforward element-wise addition of the two branches. The fused feature is then processed through shared encoder layer before being passed to the latent highlighting module. Similarly, the magnitude and waveform decoders mirror the architectures of the encoders in reverse order. 11. Loss Function Details Here, we give more detailed illustration on the MR-STFT (Multi-Resolution Short-Time Fourier Transform) loss function used for training the model. The MR-STFT loss is implemented by computing the ℓ1 distance between the amplitude spectrograms of the predicted signal ˆs and the ground truth signal s. Mathematically, the loss function can be expressed as: LMR-STFT(ˆs, s) = (cid:88) k= ST Tk(ˆs) ST Tk(s)1 , where ST Tk() denotes the Short-Time Fourier Transform with the k-th window size, and represents the magnitude of the spectrogram. The window sizes are set to 2048, 1024, and 512, corresponding to different resolutions of the spectrogram. This multi-resolution approach allows the loss function to capture both fine-grained and coarse-grained spectral details of the signals. It is worth noting that the training loss is intentionally simple, and any arbitrary waveform or spectrogram loss could be applied. We demonstrate Figure 9. Screenshot of subjective test interface. shots of mans face, visually depicting the breathing action. Given these video conditions, our method predicts output audio that highlights the breathing sound, aligning with the visual context but diverging from the original movie audio. This failure highlights the need for deeper understanding of movie content to achieve better alignment with the intended audio design. 9. Subjective Test Design We illustrate the interface design of our subjective test in Fig. 9. The instructions emphasize that users should evaluate whether the speech, music, and sound effects in the videos are well-balanced and acoustically pleasing, and whether the audio aligns effectively with the video content. Participants are shown four videos: the poorly mixed input, the best-performing baseline (LCE), our method, and the movie ground truth. After watching all the videos, users are asked to rank them from 1 to 4, with 1 being the most effective in audio highlighting and 4 being the least effective. The analysis of the ranking results is presented in Fig. 5. that even standard loss, such as the MR-STFT loss, can effectively drive training and lead to high-quality results. Figure 11. An example of video frame and its generated caption. Figure 12. The improvement trend across the three difficulty levels is evaluated over five metrics. 12. Motivation for Text Condition. Textual captions supplement video frames by leveraging strong reasoning capabilities of MLLMs. In Fig. 11, the caption generated by InternVL2-8B captures not only visual content, such as the appearance of individuals and room decorations, but also the scenes atmosphere, demonstrating the added semantic richness that textual information can provide. Moreover, it provides information more explicitly (e.g. dark, elegant outfit\") than the visual encoder may extract. This supports the observation in Tab. 2 of why text conditioning outperforms visual signals. Regarding the performance metrics of the visual encoder in Tab. 3, we hypothesize that CLIP vision features are more compact, and the 1fps video sampling rate drops motion information. Consequently, vision features are easier to overfit, as observed with the peak performance when the number of vision encoder layers is 3, and more encoder layers cause smoothing. To address this, we can try adopting higher framerate (e.g., 8fps) or exploring motion-aware architectures such as temporal transformers or 3D convolutions, which better model temporal dynamics while minimizing computational overhead. Learned downsampling can be another potential solution. 13. Inference Time Comparison The inference times for VisAH, LCE, and L2R audio backbone are 0.028s, 0.017s, and 0.018s, respectively. While our method requires more time, it remains efficient for practical applications. 14. Analysis of Dataset Difficulty We visualize the improvement trends in Fig. 12 across different levels of dataset difficulty, as discussed in Sec 5.3.2 and shown in Tab. 4. The magnitude of improvement is similar for the high and moderate difficulty levels, demonstrating that our method is robust in highlighting audio sources, even when they are highly suppressed. In contrast, the lower improvement observed for the low-difficulty level is attributed to the fact that the input audio is already relatively close to the ground truth and thus inherently conveys the ground truth highlighting effects to some extent. Consequently, the potential for improvement is reduced in this group. 15. Limitations and Future Works Our method leverages versatile temporal conditions as guidance for audio highlighting, outperforming baseline methods and demonstrating applicability to real-world scenarios, including transferring knowledge from movies to daily and generated videos. However, there are areas where improvements can be made: (i) Multimodal Condition Fusion. In our approach, we use either the video or its corresponding frame captions as guidance, achieving effective highlighting results. However, integrating these two modalities remains an open challenge. Text captions can infer the sentiment of the movie, complementing the video stream. Designing more sophisticated strategy to fuse these modalities could enhance performance and remains an interesting direction for future research. (ii) Dataset Generation Strategy. This paper introduces three-step process for generating pseudo data through separation, adjustment, and remixing. While effective, each step can be further improved. For instance, employing multiple separators with varying granularity levels could offer greater flexibility and control. Additionally, replacing discrete loudness categories with continuous sampling could introduce more variability and challenge the model. Temporal loudness adjustments, such as varying the loudness at one-second intervals within 10-second audio clip, could further enrich the dataset and present more complex training scenarios. In summary, this work presents novel taskvisually guided acoustic highlightingalong with versatile dataset generation process and universal network. While our method demonstrates strong potential, many avenues for improvement remain, paving the way for future advancements in this area."
        }
    ],
    "affiliations": [
        "Meta Reality Labs Research",
        "University of Maryland, College Park",
        "University of Rochester"
    ]
}