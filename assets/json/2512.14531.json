{
    "paper_title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
    "authors": [
        "Ying Nie",
        "Kai Han",
        "Hongguang Li",
        "Hang Zhou",
        "Tianyu Guo",
        "Enhua Wu",
        "Xinghao Chen",
        "Yunhe Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN."
        },
        {
            "title": "Start",
            "content": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse Ying Nie 1 Kai Han 1 Hongguang Li 1 Hang Zhou 1 Tianyu Guo 1 Enhua Wu 2 3 Xinghao Chen 1 Yunhe Wang 1 5 2 0 2 6 1 ] . [ 1 1 3 5 4 1 . 2 1 5 2 : r Abstract The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: width-versatile path that generates mixture of sub-experts from single shared FFN, mimicking sparse expert routing without increasing parameters, and depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. difficulty-aware gating dynamically balances the two pathways, steering easy tokens through the efficient width-wise route and allocating deeper iterative refinement to hard tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github. com/huawei-noah/noah-research/ tree/master/VersatileFFN. 1. Introduction The remarkable success of Large Language Models (LLMs) is largely governed by scaling laws, which posit power1Huawei Noahs Ark Lab 2ISCAS 3University of Macau.Correspondence to: Kai Han <kai.han@huawei.com>, Yunhe Wang <yunhe.wang@huawei.com>. Technical Report. law relationship between model performance and parameter count (Kaplan et al., 2020; Hoffmann et al., 2022). Following this trajectory, the pursuit of state-of-the-art accuracy has led to the proliferation of models with hundreds of billions of parameters. This trend encompasses both dense architectures (Touvron et al., 2023; Chen et al., 2025; Zuo et al., 2025) and sparse Mixture-of-Experts (MoE) variants (Jiang et al., 2024; Dai et al., 2024; Comanici et al., 2025; Yang et al., 2025; Team et al., 2025), reflecting consistent drive toward increasing model scale. However, the massive parameter counts of these models incur prohibitive memory footprint that severely limits their practical deployment. High-end accelerators with large memory capacity are expensive and scarce, and distributing model weights across multiple devices introduces significant communication overhead and engineering complexity. As result, scaling models purely by increasing parameter counts faces growing infrastructural and economic constraints. In recent years, line of research has explored parameterefficient LLM architectures that maintain high performance under limited memory. Such methods include weight pruning (Frantar & Alistarh, 2023; Chen et al., 2025), quantization (Xiao et al., 2023; Fu et al., 2025), low-rank adaptations (Lu et al., 2025), and other compression techniques that reduce the storage footprint of pretrained models. However, these approaches primarily aim to approximate the capabilities of the original large model, and thus remain fundamentally constrained by its architectural capacity. They do not augment the models representational power, nevertheless, they trade off precision or connectivity for deployability, often hitting an upper bound determined by the base models design. There remains clear need for architectures that are parameter-efficient by design, capable of attaining stronger performance under the same parameter budget, rather than merely compressing existing large models. In this work, we introduce VersatileFFN, parameterefficient architecture that versatilely reuse parameters and computation in feed-forward networks (FFNs), serving as efficient replacement for the standard FFN in Transformer blocks while leaving the self-attention component unchanged. As illustrated in Figure 1, VersatileFFN inteVersatileFFN Figure 1. VersatileFFN integrates width-versatile pathway for rapid execution and depth-versatile pathway for deep reasoning, both derived from shared base FFN. This design enables flexible trade-off between the two computational dimensions. grates two complementary mechanisms: width-versatile pathway and depth-versatile pathway, which together enable flexible trade-offs between computational width and depth using tightly shared parameter set. In the width-versatile FFN pathway, we introduce the concept of versatile base expert, single and multicapable FFN, from which we derive mixture of subexperts through structured parameter reuse. Rather than instantiating separate experts, each sub-expert is formed by selectively activating and composing different, non-overlapping hidden subspaces of the same underlying versatile FFN. This approach enables the emulation of diverse expert behaviors with minimal memory overhead, preserving the adaptive token routing benefits of classical MoE architectures while avoiding the parameter proliferation typical of such designs. The depth-versatile FFN pathway achieves depth-wise versatility by recursively reusing the same base FFN multiple times, effectively emulating the function of multiple layers. Rather than instantiating separate parameters for each processing step, this design enables tokens to undergo iterative refinement through cyclical application of the shared MLP. GumbelSoftmax-based controller predicts token-specific iteration count, allowing the model to dynamically allocate more processing steps to harder tokens. This approach maintains full differentiability during training while preserving parameter efficiency, as the same weights are repurposed depth-wise to create versatile, adaptive computation structure. Inspired by the dual-process theory of human cognition (Kahneman, 2011), we design an architecture that dy2 namically allocates computation: easy tokens are processed rapidly via lightweight path, while hard tokens receive deeper, iterative reasoning, all under fixed parameter budget. difficulty-aware gating mechanism unifies the two pathways, using the expected iteration count from the depth-versatile controller as proxy for token difficulty. This signal modulates the fusion weights between the outputs of width-versatile FFN and depth-versatile FFN, favoring efficient processing on width-versatile for easy tokens and shifting toward depth-versatile for harder ones. Crucially, both pathways share the same MLP parameters, ensuring added capacity comes from computation, not memory. We conduct extensive experiments across multiple benchmarks and model scales. Models equipped with VersatileFFN consistently outperform other parameters-matched or FLOPs-matched methods, demonstrating the effectiveness of our dual-process design. 2. Related Work 2.1. Mixture-of-Experts The Mixture-of-Experts (MoE) architecture serves as foundational framework for scaling model capacity without proportional increase in computational cost. Originally introduced by (Jacobs et al., 1991; Jordan & Jacobs, 1994), the concept was later popularized in the large-scale networks by (Shazeer et al., 2017). Subsequent advancements, such as GShard (Lepikhin et al., 2020) and Switch Transformer (Fedus et al., 2022), have demonstrated that replacing standard feed-forward layers in Transformers with MoE layers facilitates efficient pre-training at the trillion-parameter scale, yielding substantial performance improvements. Recent advancements have focused on refining expert granularity and routing strategies (Jiang et al., 2024; Dai et al., 2024; Jin VersatileFFN et al., 2024; Huang et al., 2024a; Wang et al., 2024), enabling the successful deployment of large-scale MoE models in industrial applications (Yang et al., 2025; Comanici et al., 2025; Team et al., 2025; Zhao et al., 2025). Despite their inference efficiency, the massive storage requirements of standard MoE models present significant deployment hurdles. This has catalyzed research into parameter-efficient MoE, utilizing techniques such as expert merging (Li et al., 2022; Zhao et al., 2024), pruning (Sarkar et al., 2024; Lu et al., 2024), and quantization (Nie et al., 2022; Dong et al., 2024; Huang et al., 2024b; Zhou et al., 2025). Another emerging direction involves composing lightweight, task-specific modules (e.g. LoRA adapters) into mixture-based systems, as seen in LoRAHub (Huang et al., 2023), MoA (Feng et al., 2024), MoLE (Wu et al., 2024), and MoRAgent (Han et al.). While these approaches mitigate some memory constraints, they typically rely on instantiating distinct, physically separate expert parameters, which limits the extent of parameter efficiency compared to architectures designed for intrinsic reuse. 2.2. Recursive Computation and Adaptive Inference promising avenue for maximizing parameter efficiency is the decoupling of model depth from parameter count through recursive computation. Early works such as Universal Transformers (Dehghani et al., 2018) and ALBERT (Lan et al., 2019) demonstrate that cross-layer parameter sharing can induce beneficial inductive biases and improve parameter efficiency for language modeling. Recent theoretical analysis further establishes that such recursive architectures can emulate complex algorithms, acting as universal computers (Giannou et al., 2023; Gao et al., 2024), and generalize to sequence lengths far beyond those encountered during training (Fan et al., 2024; Gong et al., 2025). Beyond static parameter sharing, recursive structures facilitate dynamic computation, where the computational budget is adapted to the complexity of the input. Methods like Mixture-of-Depths (Raposo et al., 2024), Mixtureof-Recurrence (Bae et al., 2025) and Dynamic resolution network (Zhu et al., 2021) dynamically allocate inference FLOPs, allowing models to expend more thinking time on harder tokens or images. This iterative refinement is particularly potent for reasoning tasks, offering significant advantages over static counterparts. Several studies (Gatmiry et al., 2024; Saunshi et al., 2025; Merrill & Sabharwal, 2025; Zhu et al., 2025) indicate that increasing computational depth often yields greater performance gains than merely increasing width. While these approaches successfully exploit the depth dimension for efficiency, they typically treat the recurrent layer as monolithic block, leaving the potential for fine-grained, width-wise parameter reuse largely unexplored, gap our VersatileFFN aims to bridge. 3. Method 3.1. Architectural Overview Let RBT denote the input tensor to the Transformer block, where represents the batch size, the sequence length and the feature dimension. In standard Transformer layer, the input first undergoes Self-Attention mechanism followed by residual connection and normalization: = + Attention(LayerNorm(X)), (1) Subsequently, the hidden states are processed by FeedForward Network (FFN). We define the FFN transformation function F() as: = F(H) = + Wout ϕ(Wproj LayerNorm(H)). (2) where Wproj Rddhidden, Wout Rdhiddend denote the projection and output weights, respectively, and ϕ is distinct non-linear activation function. We introduce VersatileFFN, parameter-efficient alternative to the standard FFN. While retaining the canonical selfattention architecture, VersatileFFN reconfigures the feedforward computation into two complementary pathways that share the underlying FFN weights (i.e., Wproj and Wout): Width-Versatile (Ywidth): This pathway functions as virtual MoE module. It routes tokens to specialized sub-experts instantiated via structured subsets of the shared weights, facilitating rapid, domain-specialized response without increasing parameter count. Depth-Versatile (Ydepth): This pathway implements recursive computation mechanism. It iteratively refines token representations by reusing the full capacity of Wproj and Wout, thereby allocating increased computational depth to semantically complex tokens. The final output is synthesized through dynamic, difficulty-aware fusion of these two pathways: = λ Ywidth + (1 λ) Ydepth, (3) where λ [0, 1) acts as gating coefficient modulated by the token difficulty. Specifically, λ is derived from the predicted iteration count of the depth-versatile controller. This mechanism ensures flexible computational trade-off: ensuring broad semantic coverage for simple tokens via the width pathway, while reserving deep, iterative reasoning for harder tokens via the depth pathway, all within fixed parameter budget. 3 VersatileFFN 3.2. Width-Versatile Mechanism The Width-Versatile pathway, implemented as virtual MoE, augments the models representational capacity while circumventing the prohibitive memory overhead typically associated with physical expert instantiation. Construction of Virtual Experts. Rather than allocating discrete weight matrices for each expert, we leverage the shared weights Wproj and Wout as contiguous parameter substrate. We define set of virtual experts by extracting structured, non-overlapping subspaces from the hidden dimension dhidden as illustrated in Figure 2. Specifically, let dexpert denote the hidden dimension of single virtual expert, where dexpert < dhidden. We employ strided slicing strategy to map each expert {0, . . . , 1} to specific view of the shared parameters. This design adheres to two principles: (1) Parameter Efficiency, achieved through the dense reuse of the backbone weights. (2) Functional Orthogonality, ensured by the non-overlapping allocation, which prevents interference between experts. The stride is calculated to uniformly distribute expert views across the hidden dimension: = (cid:22) dhidden dexpert 1 (cid:23) . (4) The indices (k) proj corresponding to the k-th expert is: (5) out = (k) (k) proj = {j S < S + dexpert}. Leveraging the pre-trained weights of the dense model, we assign (k) proj to maintain the structural alignment between the projection and output. The effective weight matrices for the virtual expert are thus derived as: out = Wout[I (k) proj = Wproj[:, (k) proj], W(k) out, :]. (6) W(k) Sparse Token Routing. We employ learnable gating to orchestrate token assignment. Given the post-attention token representation RBT d, router Wg RdN computes the gating logits G(H) = WgH. We adopt standard Top-K strategy, activating only the experts with the highest routing scores. The output Ywidth is computed as the probability-weighted sum of the selected experts: Ywidth = (cid:88) gk Yk, (7) kTop-K(G(H)) where gj represents the Softmax-normalized gating probability, and the expert-specific transformation Yk(H) is: out ϕ(W(k) proj LayerNorm(H)). Yk = + W(k) (8) This sparse activation mechanism decouples computational cost from model capacity. Following previous experience (Fedus et al., 2022), we incorporate an auxiliary loadbalancing loss during training to prevent expert collapse. 4 Figure 2. Illustration of decoupling one large and real expert into multiple lightweight and virtual experts. 3.3. Depth-Versatile Mechanism The Depth-Versatile pathway introduces token-wise adaptive computation by applying the shared MLP block recursively, enabling dynamic depth allocation. Recursive Weight Application. In contrast to the WidthVersatile pathway, The Depth-Versatile pathway utilizes the entire shared backbone weights (Wproj, Wout) without slicing. This ensures that the recursive refinement process benefits from the full expressivity of the model parameters. Let F() denote the standard FFN transformation defined in Eq. 2. We generate sequence of intermediate representations by recursively applying F: H(ℓ) = F(cid:0)H(ℓ1)(cid:1), ℓ = 1, . . . , Lmax, (9) where H(0) = H, and Lmax denotes the maximum allowable iterations. This iterative process allows the model to progressively refine token representations, capturing complex dependencies through repeated non-linear transformations without increasing the parameter budget. Differentiable Loop Prediction. The optimal number of iterations per token is inherently discrete and nondifferentiable. To facilitate end-to-end training, we introduce prediction head Wloop RdLmax to estimate the required computational depth. The logits for the loop count are computed as Ploop(H) = WloopH. We then employ the Gumbel-Softmax relaxation (Jang et al., 2016) to sample differentiable probability vector Lmax1: = Softmax (cid:18) Ploop(H) + τ (cid:19) , (10) where Gumbel(0, 1) and τ is the temperature parameter. VersatileFFN We utilize the Straight-Through Estimator (STE) during training: the forward pass executes discrete selection (via argmax) to simulate the inference-time decision, while gradients are propagated through the continuous relaxation during the backward pass. To stabilize convergence, τ is annealed exponentially from an initial value (e.g. 5.0) to lower bound (e.g. 0.1). The output of the Depth-Versatile pathway is computed as the soft-weighted combination of intermediate states: Ydepth = Lmax(cid:88) ℓ= pℓ H(ℓ). (11) During inference, the probabilistic sampling is replaced by deterministic decision ˆℓ = arg max(p). The loop is executed exactly ˆℓ times, and the final state is taken as Ydepth = H(ˆℓ). 3.4. Difficulty-Aware Fusion The Width-Versatile and Depth-Versatile pathways represent distinct yet complementary computational paradigms: the former offers broad, parallelizable semantic capacity via virtual experts, while the latter facilitates intensive, sequential reasoning through iterative refinement. VersatileFFN synergizes these mechanisms via difficulty-aware fusion scheme, which leverages the predicted computational depth as proxy for token complexity. Expected Loop Count as Difficulty Proxy. We postulate that the requisite depth of recursive processing serves as an intrinsic measure of semantic difficulty. Linguistically trivial tokens (e.g. stopwords, high-frequency bigrams) typically necessitate minimal transformation (loop count 1), whereas semantically ambiguous or logically complex tokens benefit from deep, recurrent refinement (loop count Lmax). We quantify this difficulty by computing the expected loop count, E[L], derived from the soft probability distribution output by the loop predictor: E[L] = Lmax(cid:88) ℓ=1 ℓ pℓ. (12) During training, the differentiability of allows E[L] [1, Lmax] to serve as continuous signal for gradient propagation. During inference, although the loop execution becomes discrete, E[L] remains robust indicator of the models uncertainty and the tokens processing demand. Dynamic Fusion Modulation. To unify the two pathways, we define dynamic gating scalar, λ, which modulates the fusion balance based on the estimated difficulty. We formulate λ to be inversely proportional to the expected Algorithm 1 Training Computation of VersatileFFN Require: Input tensor X, Shared Weights Wproj, Wout, Max loops Lmax Ensure: Output tensor 1: Compute post-attention representation via Eq. 1 2: Width-Versatile Pathway: 3:"
        },
        {
            "title": "Construct virtual experts by slicing shared weights",
            "content": "via Eq. 5 and Eq. 6 Compute output Ywidth with Top-k routing via Eq. 7 4: 5: Depth-Versatile Pathway: 6: 7: 8: 9: Predict loop count probabilities via Eq. 10 Initialize H(0) for ℓ = 1 to Lmax do Update recursive hidden state H(ℓ) using full shared weights via Eq. end for Aggregate states to obtain Ydepth via Eq. 11 10: 11: 12: Difficulty-Aware Fusion: 13: 14: 15: Calculate expected loop count E[L] via Eq. 12 Compute difficulty-aware fusion scalar λ via Eq. 13 Compute final output by fusing pathways via Eq. 3 16: Return computational cost: λ = Lmax E[L] Lmax . (13) By construction, λ [0, 1). For easy tokens with low expected loops, λ 1, biasing the output toward the efficient width. Conversely, for hard tokens, λ 0 , shifting the focus toward the depth. The final output is synthesized according to Eq. 3. This mechanism automatically allocates computational resources: simple patterns are resolved rapidly via the lightweight virtual MoE, while complex reasoning tasks command the full depth of the recursive loop. Inference Optimization. The complete training procedure is summarized in Algorithm 1. To minimize latency during inference, we introduce two inference-time optimizations: Discrete Early-Exit: The Depth-Versatile pathway transitions from soft aggregation to hard cutoff. Recursion terminates immediately at the predicted step ˆℓ = arg max(p), avoiding unnecessary computation beyond the required depth. Conditional Parallelism: We employ threshold-based execution strategy. If the contribution of the WidthVersatile pathway is negligible (i.e., λ = 0) the branch is pruned entirely. Otherwise, both pathways are executed in parallel to maximize hardware utilization and computational throughput. 5 VersatileFFN weighting coefficient of 1 105. Evaluation Benchmark Setup. We evaluate our method on comprehensive set of benchmarks: PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), OpenBookQA (OBQA) (Mihaylov et al., 2018), SciQ (Welbl et al., 2017), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark et al., 2018), CommonsenseQA (COMM) (Clark et al., 2019), and Winogrande (WINO) (Sakaguchi et al., 2019). We utilize OLMES (OLMo et al., 2024) to assess the performance under zero-shot setting. Versatile Setup of Width and Depth. To determine the optimal configuration for the VersatileFFN architecture, specifically the number of active experts and total experts in the width (i.e., and in Eq. 7) and the maximum loops in the depth (i.e., Lmax in Eq. 11), we conduct ablation studies using the 300M model trained on 40B tokens. In these experiments, we maintained one shared expert for the widthversatile pathway, while disabling virtual experts within the depth-versatile pathway. The training loss and average accuracy across the above eight academic benchmarks during training is illustrated in Figure 3. For width-versatile, the losses (and average accuracies) for varying expert configurations, specifically 4-choose-1, 8-choose-2, 8-choose-4, and 16-choose-4 are 2.655 (50.16%), 2.633 (50.98%), 2.633 (50.96%), and 2.634 (50.22%), respectively. We select the 8choose-2 setting as our primary experimental configuration due to its higher performance. For depth-versatile, when the number of loops is 1, 2, 4, and 6, the final training losses are 2.654, 2.631, 2.625, and 2.623, respectively. The average accuracies are 50.09%, 51.47%, 51.98%, and 51.94%. It can be seen that 4 loops can achieve the best balance between accuracy and computation. Therefore, we set the maximum number of loops to 4 in our main experiment. 4.2. Evaluation Results To rigorously evaluate the efficacy of our proposed method, we conduct continued pre-training on pre-trained base model for one additional epoch, utilizing the identical corpus. We benchmark our approach against the following methods: 1) Mixture-of-Experts (MoE): sparse architecture that inherits the configuration of the dense base model but incorporates additional 8 small experts with activating top-2 experts during the forward pass. 2) k-Loop: variant of the dense architecture that retains the original architecture but introduces recurrence by iteratively applying the FFN times at each layer. Efficiency Comparison. We first provide an analysis of the efficiency in Table 2, including the average accuracy, model parameters, and FLOPs of the FFN part. We calculate the FLOPs as 1 token input and 1 token output. Since the same architecture is maintained, the number of parameters for methods k-Loop is consistent with that of the base model. (a) Various total experts and active experts. (b) Various maximum number of loops. Figure 3. The curves of training loss and average accuracy for various configurations of width-versatile and depth-versatile mechanisms. 4. Experiments 4.1. Implementation Details Experiments Setup. To validate our approach, we conducted pre-training experiments on the FineWeb-Edu dataset (Penedo et al., 2024), our implementation is based on OLMo2 (OLMo et al., 2024). Specifically, we train 354M parameter model on 40B tokens and 720M parameter model on 70B tokens, where both token sets are subsets derived from larger 100B token corpus (HuggingFaceFW, 2024). Following the architectural settings of the open-source OLMo2 models, we set the hidden dimensions to 1,024 for 354M model and 1,536 for the 720M model. Both models consist of 15 layers and utilize SwiGLU for activation and RMSNorm (Zhang & Sennrich, 2019) for normalization. Besides, The OLMo2 tokenizer with vocabulary size of 50,280 is employed in our models. We primaly conduct experiments under continued pretraining setting. Specifically, we initialize the model using the pre-trained checkpoint and continue training for one epoch on the original corpus. For all models, we set the training sequence length to 4,096 tokens. AdamW optimizer is employed with weight decay of 0.1 and gradient clipping threshold of 1.0. The learning rate follows cosine decay schedule, beginning with warm-up phase for the first 5% of total steps and subsequently decaying to 10% of the peak value. Regarding model-specific hyperparameters, we use global batch size of 1.5M tokens with peak learning rate of 5 104 for the 354M model, and global batch size of 1M tokens with peak learning rate of 4 104 for the 720M model. Furthermore, for ours and traditional MoE, we incorporate an auxiliary load balancing loss with 6 VersatileFFN Table 1. Comparison of zero-shot performance on standard NLP benchmarks. Results are reported for two model scales. Avg. indicates the average accuracy across all eight tasks. METHOD LOSS PIQA HELLASWAG OBQA SCIQ ARC-E ARC-C COMM WINO AVG. BASE MOE 2-LOOP 4-LOOP 6-LOOP 2.779 2.585 2.631 2.625 2.623 VERSATILEFFN 2.617 BASE MOE 2-LOOP 4-LOOP 6-LOOP 2.519 2.411 2.448 2.441 2.430 VERSATILEFFN 2. 65.13 68.93 67.90 68.44 67.16 69.10 69.48 72.31 71.33 71.87 71.98 72.03 Base Model Parameters: 354.71M 37.73 45.33 44.06 44.09 44.06 43.95 32.40 33.20 34.60 34.60 36.00 35.00 78.60 85.40 84.60 84.80 84.30 85. 56.32 63.16 64.56 64.56 64.04 64.56 Base Model Parameters: 720.81M 47.69 54.47 52.99 53.43 53.96 53.44 37.20 38.00 37.00 37.00 38.80 38.00 86.40 89.00 88.20 88.60 89.30 88.90 65.61 67.72 68.42 68.25 69.47 71. 29.77 29.77 28.76 30.77 30.77 30.10 34.11 34.45 35.45 36.45 34.45 36.12 32.76 34.64 35.46 36.53 35.95 36.69 35.14 37.10 37.84 39.39 38.98 40.79 51.14 51.38 51.78 52.01 53.20 53.51 55.01 53.91 55.41 55.64 55.49 55. 47.98 51.48 51.47 51.98 51.94 52.33 53.83 55.87 55.83 56.33 56.55 57.03 Table 2. Efficiency comparison for various methods. METHOD AVG. PARAMS (M) FFN FLOPS (M) Base Model Parameters: 354.71M BASE MOE 2-LOOP 4-LOOP 6-LOOP 47.98 51.48 51.47 51.98 51.94 VERSATILEFFN 52.33 354.71 543.59 354.71 354.71 354.71 354.90 377.49 471.86 754.98 1509.96 2264.96 1236. Base Model Parameters: 720.81M BASE MOE 2-LOOP 4-LOOP 6-LOOP 53.83 55.87 55.83 56.33 56.55 VERSATILEFFN 57.03 720.81 1145.69 720.81 720.81 720.81 721.09 849.35 1061.69 1698.70 3397.40 5096.10 2586.38 The proposed VersatileFFN method, however, introduces router and loop predictor, which results in slightly higher parameters. The MoE method, on the other hand, introduces real small experts on top of the base model, leading to significant increase in parameters. For FFN FLOPs, the computational cost of k-Loop method is clearly times that of base. We calculate the FLOPs of VersatileFFN based on the inference statistics of the ARC-c dataset. Specifically, we calculate the average loops of all layers Nmean and the proportion of loops not equal to the maximum loop, and then compute the FLOPs by Base Nmean + (M oE Base) . Benchmark Accuracy. We report the detailed zeroshot performance of VersatileFFN against other methods in Table 1. From the results, VersatileFFN consistently achieves the highest average accuracy across both model scales. Specifically, on the 720M scale, our method attains an average accuracy of 57.03%, outperforming the strong MoE baseline (55.87%) and the 6-Loop method (56.55%). It is worth noting that while MoE achieves the lowest loss 7 (2.411 vs. 2.430 for VersatileFFN), this advantage does not translate into downstream task accuracy. VersatileFFN demonstrates more robust generalization capabilities, particularly on reasoning-intensive tasks such as ARC-e and COMM, where it leads by significant margin (+3.33% over MoE on ARC-e). As shown in Table 2, key advantage of VersatileFFN is its parameter efficiency. Unlike MoE, which significantly increase the total parameters (from 720M to 1145M), VersatileFFN introduces negligible parameter overhead (< 0.1% increase). Regarding computational complexity, VersatileFFN is significantly more efficient than the 4-Loop and 6-Loop methods. For instance, at the 350M scale, VersatileFFN requires approximately 45% fewer FLOPs than the 6-Loop method while achieving superior accuracy. This indicates that VersatileFFN allocates computational resources more effectively than simply stacking recurrent loops, offering compelling balance between model size, training cost, and downstream performance. 4.3. Visualized Analysis Actual loops. We first visualize the actual loops executed at each layer on the ARC-c dataset, as shown in Figure 4. We observe distinct behavioral patterns depending on model size: the smaller 354M model tends to allocate higher computational budget to the final layers (with sharp increase in layers 1114), whereas the larger 720M model exhibits middle-heavy distribution, concentrating recurrence primarily within the intermediate layers. Features of two branches. We then analyze the features of the width-versatile and depth-versatile at Layer 0 using random sample from the ARC-c, as visualized in Figure 5. Although both branches inherit the same base parameters, the output features of the two branches are not exact replicas. This demonstrates that they learn complementary representations within globally aligned semantic space. VersatileFFN Table 3. Analysis on each components in VersatileFFN. METHOD WIDTH DEPTH GATING LOSS ACC. BASE VERSATILEFFN VERSATILEFFN VERSATILEFFN VERSATILEFFN 2.779 47.98 2.633 50.98 2.625 51.86 2.618 52.10 2.617 52.33 Various MoE experts and k-Loop. We further investigate the impact of loop depth and expert configuration in Table 4. As shown in (A), while increasing the loop count to 6 minimizes loss, the zero-shot average accuracy peaks at 4 loops, suggesting that excessive recurrence may lead to overfitting. From (B), we observe that the (8, 2) expert configuration achieves the optimal accuracy. Table 4. Analysis of the effect of: (A) Various Loops under MoE8-4; (B) Various experts under 4-Loop. (A) EFFECT OF LOOPS (B) EFFECT OF MOE EXPERTS LOOPS LOSS ACC. EXPERTS LOSS ACC. 2 4 6 2.625 2.617 2.615 51.54 52.33 51. 8-2 8-4 16-4 2.617 2.617 2.617 52.33 52.22 51.72 Performance without Continued Pre-training. Table 5 shows the performance without continued pre-training. VersatileFFN attains an accuracy of 51.14%, yielding 3.16% improvement over the Base and outperforming the 4-Loop. Table 5. Performance without continued pre-training. METHOD LOSS ACC. BASE 4-LOOP 2.779 2.641 VERSATILEFFN 2.654 47.98 50.81 51.14 5. Conclusion In this work, we present VersatileFFN, novel architectural paradigm that decouples model performance from parameter scaling by prioritizing computational flexibility over memory expansion. By integrating width-versatile path for efficient computing (System 1) and depth-versatile path for iterative reasoning (System 2), our method effectively synthesizes width and depth within strictly constrained parameter budget. Extensive experiments demonstrate that VersatileFFN consistently outperforms other methods, validating that intelligent weight reuse and adaptive computation are viable alternatives to simply scaling model size. We hope this work encourages further exploration into compute-heavy, memory-light architectures, paving the way for deploying sophisticated reasoning capabilities in resource-constrained environments where memory is the primary bottleneck. Figure 4. The average predicted loops per layer on ARC-c dataset. Left: 354M base model. Right: 720M base model. Figure 5. Heatmap of cosine similarity at Layer 0. Left: 354M base model. Right: 720M base model. Word cloud. In Figure 6, we analyze the word cloud of 354M model at Layer 0 on ARC-c. lower gating coefficients λ indicates more loops. The words in the left is dominated by specific action verbs and similar words such as clean, remove, cut, cup. Conversely, the right consists primarily of high-frequency, generic terms like make, use, water, will. These tokens typically rely on shallow surface statistics or syntactic cues, allowing the model to capture their representations with minimal recurrence. Figure 6. Word cloud for various gating coefficients on ARC-c. 4.4. Ablation Studies We conduct series of ablation studies using the 354M base mdoel on 40B tokens. Analysis on Each Components. From Table 3, we observe that both width-versatile and depth-versatile individually outperform the baseline. The combination of all modules yields the lowest loss and the highest average accuracy. This result confirms that the two branches are complementary, functioning synergistically to enhance model expressivity without redundancy. 8 VersatileFFN"
        },
        {
            "title": "References",
            "content": "Bae, S., Kim, Y., Bayat, R., Kim, S., Ha, J., Schuster, T., Fisch, A., Harutyunyan, H., Ji, Z., Courville, A., et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Chen, H., Qin, J., Guo, J., Yuan, T., Yin, Y., Zhen, H., Wang, Y., Li, J., Meng, X., Zhang, M., et al. Pangu light: Weight re-initialization for pruning and accelerating llms. arXiv preprint arXiv:2505.20155, 2025. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/ N19-1300. URL https://aclanthology.org/ N19-1300/. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Dai, D., Deng, C., Zhao, C., Xu, R., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and arXiv preprint Kaiser, Ł. Universal transformers. arXiv:1807.03819, 2018. Dong, P., Li, L., Zhong, Y., Du, D., Fan, R., Chen, Y., Tang, Z., Wang, Q., Xue, W., Guo, Y., et al. Stbllm: Breaking the 1-bit barrier with structured binary llms. arXiv preprint arXiv:2408.01803, 2024. Fan, Y., Du, Y., Ramchandran, K., and Lee, K. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Feng, W., Hao, C., Zhang, Y., Han, Y., and Wang, H. Mixture-of-loras: An efficient multitask tuning for large arXiv preprint arXiv:2403.03432, language models. 2024. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pp. 1032310337. PMLR, 2023. Fu, Z., Ding, N., Han, K., Yu, X., Li, X., Chen, X., Tang, Y., and Wang, Y. Eaquant: Enhancing post-training quantization for moe models via expert-aware optimization. arXiv preprint arXiv:2506.13329, 2025. Gao, Y., Zheng, C., Xie, E., Shi, H., Hu, T., Li, Y., Ng, M. K., Li, Z., and Liu, Z. On the expressive power of variant of the looped transformer. CoRR, 2024. Gatmiry, K., Saunshi, N., Reddi, S. J., Jegelka, S., and Kumar, S. Can looped transformers learn to implement multi-step gradient descent for in-context learning? arXiv preprint arXiv:2410.08292, 2024. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as programmable computers. In International Conference on Machine Learning, pp. 1139811442. PMLR, 2023. Gong, Z., Teng, J., and Liu, Y. What makes looped transformers perform better than non-recursive ones (provably). arXiv preprint arXiv:2510.10089, 2025. Han, J., Yan, B., Guo, T., Bai, Z., Zheng, M., Chen, H., and Nie, Y. Moragent: Parameter efficient agent tuning with mixture-of-roles. In Forty-second International Conference on Machine Learning. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Huang, C., Liu, Q., Lin, B. Y., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic arXiv preprint arXiv:2307.13269, lora composition. 2023. Huang, Q., An, Z., Zhuang, N., Tao, M., Zhang, C., Jin, Y., Xu, K., Chen, L., Huang, S., and Feng, Y. Harder tasks need more experts: Dynamic routing in moe models. arXiv preprint arXiv:2403.07652, 2024a. VersatileFFN Huang, W., Liao, Y., Liu, J., He, R., Tan, H., Zhang, S., Li, H., Liu, S., and Qi, X. Mixture compressor for mixture-of-experts llms gains more. arXiv preprint arXiv:2410.06270, 2024b. HuggingFaceFW. fineweb-edu (revision 22b0aca), 2024. https://huggingface.co/datasets/ URL HuggingFaceFW/fineweb-edu. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jin, P., Zhu, B., Yuan, L., and Yan, S. Moe++: Accelerating mixture-of-experts methods with zero-computation experts. arXiv preprint arXiv:2410.07348, 2024. Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2): 181214, 1994. Kahneman, D. Thinking, fast and slow. macmillan, 2011. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Lu, Y.-C., Chen, C.-Y., Chang, C.-C., Hu, Y.-F., and Wu, K.- C. Flrc: Fine-grained low-rank compressor for efficient llm inference. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1495614966, 2025. Merrill, W. and Sabharwal, A. little depth goes long way: The expressive power of log-depth transformers. arXiv preprint arXiv:2503.03961, 2025. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Nie, Y., Han, K., Diao, H., Liu, C., Wu, E., and Wang, Y. Redistribution of weights and activations for addernet quantization. Advances in Neural Information Processing Systems, 35:2273922751, 2022. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Penedo, G., Kydlıˇcek, H., Lozhkov, A., Mitchell, M., Raffel, C. A., Von Werra, L., Wolf, T., et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37: 3081130849, 2024. Raposo, D., Ritter, S., Richards, B., Lillicrap, T., Humphreys, P. C., and Santoro, A. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. Sarkar, S., Lausen, L., Cevher, V., Zha, S., Brox, T., and Karypis, G. Revisiting smoe language models by evaluating inefficiencies with task specific expert pruning. arXiv preprint arXiv:2409.01483, 2024. Saunshi, N., Dikkala, N., Li, Z., Kumar, S., and Reddi, S. J. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306, 2022. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Lu, X., Liu, Q., Xu, Y., Zhou, A., Huang, S., Zhang, B., Yan, J., and Li, H. Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. arXiv preprint arXiv:2402.14800, 2024. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. 10 VersatileFFN Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Zhu, R.-J., Wang, Z., Hua, K., Zhang, T., Li, Z., Que, H., Wei, B., Wen, Z., Yin, F., Xing, H., et al. Scaling latent reasoning via looped language models. arXiv preprint arXiv:2510.25741, 2025. Zuo, J., Nie, Y., Guo, T., Zhang, H., Hong, J., Sang, N., Gao, C., and Han, K. L-man: large multi-modal model In Proceedings of the unifying human-centric tasks. AAAI Conference on Artificial Intelligence, volume 39, pp. 1109511103, 2025. Wang, Z., Zhu, J., and Chen, J. Remoe: Fully differentiable mixture-of-experts with relu routing. arXiv preprint arXiv:2412.14711, 2024. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. In NUT@EMNLP, 2017. Wu, X., Huang, S., and Wei, F. Mixture of lora experts. arXiv preprint arXiv:2404.13628, 2024. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training In Internaquantization for large language models. tional conference on machine learning, pp. 3808738099. PMLR, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can machine really finish your sentence? In Korhonen, A., Traum, D., and M`arquez, L. s. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. Zhao, G., Fu, Y., Li, S., Sun, X., Xie, R., Wang, A., Han, W., Yang, Z., Sun, W., Zhang, Y., et al. Towards comprehensive scaling law of mixture-of-experts. arXiv preprint arXiv:2509.23678, 2025. Zhao, H., Qiu, Z., Wu, H., Wang, Z., He, Z., and Fu, J. Hypermoe: Towards better mixture of experts via transferring among experts. arXiv preprint arXiv:2402.12656, 2024. Zhou, Y., Li, Z., Zhang, J., Wang, J., Wang, Y., Xie, Z., Chen, K., and Shou, L. Floe: On-the-fly moe inference on memory-constrained gpu. arXiv preprint arXiv:2505.05950, 2025. Zhu, M., Han, K., Wu, E., Zhang, Q., Nie, Y., Lan, Z., and Wang, Y. Dynamic resolution network. Advances in Neural Information Processing Systems, 34:27319 27330, 2021."
        }
    ],
    "affiliations": [
        "Huawei Noah's Ark Lab",
        "ISCAS",
        "University of Macau"
    ]
}