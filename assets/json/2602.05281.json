{
    "paper_title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
    "authors": [
        "Pengyi Li",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths."
        },
        {
            "title": "Start",
            "content": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Pengyi Li 1 Elizaveta Goncharova 1 Andrey Kuznetsov 1 Ivan Oseledets"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 6 ] . [ 2 1 8 2 5 0 . 2 0 6 2 : r Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to lowentropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths. 1FusionBrain Lab, Russia Mathematics, Russia. <li.pengyi@fusionbrainlab.com>. 2Institute of Numerical Pengyi Li Correspondence to: Preprint. February 9, 2026. 1 In recent years, Large Language Models (LLMs) have made significant progress through Reinforcement Learning (RL)- driven post-training (Sutton et al., 1998; Schulman et al., 2017; Rafailov et al., 2023; Ouyang et al., 2022), particularly in complex reasoning tasks. As simple yet highly efficient training paradigm, Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024; Guo et al., 2025) leverages explicit verification signals to stably induce the generation of longer Chain-of-Thought (CoT) reasoning paths (Shao et al., 2024; Jaech et al., 2024; Team et al., 2025). This, in turn, leads to substantial performance gains on high-difficulty reasoning tasks, consistent with prior findings on the effectiveness of CoT reasoning and test-time scaling (Wei et al., 2022; Muennighoff et al., 2025). However, although RLVR can effectively improve task success rates, its training process is often accompanied by obvious entropy collapse and mode collapse phenomena, leading to reasoning paths generated by the model being highly concentrated on few dominant solutions. This issue inherently stems from the reward-weighted likelihood maximization objective: this objective continuously amplifies the probability mass of high-reward trajectories during optimization, thereby compressing the probability space occupied by lowfrequency but equally valid reasoning paths, weakening the models exploration capabilities (Yue et al., 2025). Addressing the aforementioned issues, existing works have attempted to mitigate mode collapse by introducing entropy regularization (Ziebart et al., 2008; Haarnoja et al., 2018; Cui et al., 2025; Wang et al., 2025b), clip-higher (Yu et al., 2025), dynamic clipping strategies (Yang et al., 2025b), or high-entropy token promotion mechanisms (Wang et al., 2025a). However, these methods remain largely limited to local modifications within the reward maximization framework, making it difficult to fundamentally improve the ability to model diverse reasoning paths. On the other hand, recent research indicates that applying penalties only to incorrect trajectories while maintaining relatively flat reward structure for correct ones can, to certain extent, increase the diversity of the solution space (Zhu et al., 2025b). This phenomenon further demonstrates that the relative probability structure among reasoning paths plays critical role in Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities shaping the models exploration behavior. From the perspective of generative modeling, the reasoning process of LLMs is essentially token-by-token probabilistic sampling process. When the policy becomes overly deterministic on few trajectories, the diversity of the sampling distribution inevitably drops (Li et al., 2025). Based on this observation, we propose new reinforcement learning paradigm, Probabilistic based GRPO (ProGRPO), which re-examines the construction of Advantage from the perspective of probability distributions. Specifically, ProGRPO utilizes the internal probability signals of the prompt and the generated answer from the LLM, combined with verifiable rewards, to reshape the Advantage distribution, thereby implicitly reshaping the effective reward-weighted trajectory distribution optimized by the policy gradient. This re-weighting mechanism based on probability structure can effectively alleviate the entropy collapse problem and significantly enhance the diversity of reasoning paths and training stability. Our main contributions include: Methodology: We propose ProGRPO, principled extension of GRPO that incorporates novel Advantage Re-weighting Mechanism (ARM). By introducing confidence-aware signals into the advantage function, our method achieves targeted exploration without compromising training stability. Broad Effectiveness: We validate ProGRPO across diverse reasoning and code generation benchmarks using Qwen2.5 (7B, 32B) and DeepSeek models. Our method consistently outperforms mainstream baselines like GRPO and FlowRL, demonstrating strong scalability and generalization across different model sizes. OOD Robustness: Beyond standard benchmarks, ProGRPO exhibits superior Out-of-Distribution (OOD) adaptability, maintaining robust performance on unseen data distributions. Significant Gains: ProGRPO substantially enhances both accuracy and output diversity. On Qwen2.57B, it improves Pass@1 and Pass@32 by 5.7% and 13.9% respectively over GRPO (and 8.0% / 7.5% over FlowRL), highlighting its superior exploration efficiency. 2. Preliminaries 2.1. REINFORCE cumulative reward. However, the gradient estimation suffers from high variance, leading to training instability. (cid:2)R(τ ) log πθ(τ )(cid:3) JREINFORCE(θ) = Eτ πθ (1) 2.2. Proximal Policy Optimization (PPO) PPO (Schulman et al., 2017) stabilizes training by enforcing trust region constraint via clipping mechanism, which restricts the size of the policy update. JPPO(θ) = EqD,oπθold (cid:34) (cid:18) 1 o (cid:88) t=1 min rt(θ)At, (2) clip (cid:0)rt(θ), 1 ϵ, 1 + ϵ(cid:1)At (cid:19)(cid:35) PPO requires an additional value function (Critic) to estimate the advantage At, which incurs significant computational overhead. 2.3. Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) eliminates the need for value function by using verifiable rewards and group-based relative advantages. It samples group of outputs {oi}G i=1 for each query and uses the group mean as the baseline. JGRPO(θ) = (cid:34) 1 (cid:88) i=1 1 oi oi (cid:88) t=1 qD,{oi}G i=1πθold (cid:18) min ri,t(θ)Ai, (3) clip (cid:0)ri,t(θ), 1 ϵ, 1 + ϵ(cid:1)Ai (cid:19)(cid:35) where the advantage Ai is computed by normalizing the rewards within the group: Ai = Rimean(R) . std(R) 3. Methodology 3.1. Advantage Re-weighting Mechanism (AMR) We redefine the advantage by incorporating sample-level signals derived from the model itself, using the low-probability token length normalized likelihood as confidence score. Ai = Ai, if (cid:80)G k=1 ri,k = Ai + α (cθ(qi) cθ(oi qi)) , otherwise (4) REINFORCE (Sutton et al., 1998) is the classic policy gradient algorithm. Its objective is to maximize the expected Here, c(qi) denotes the models confidence on the current prompt. Rather than relying on heuristic assumptions, we Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities ground this design in the principles of Curriculum Reinforcement Learning (Parashar et al., 2025), which posits that the difficulty of training samples (ranging from simple to hard) significantly impacts model optimization. Consequently, we incorporate c(qi) as dynamic control term to regulate the training process based on the models familiarity with the prompt. cθ(qi) = exp 1 low (cid:88) tT low log pθ(qi,t qi,<t) (5) cθ(oj qi) represents the models confidence in generating the answer oj for prompt qi. cθ(oj qi) = exp (cid:32) 1 low (cid:88) tT low log pθ (cid:0)oj,t qi, oj,<t (cid:1) (cid:33) (6) This Advantage reweighting indirectly reshapes the effective reward distribution, allowing us to score reasoning trajectories rather than simply maximize rewards. The reason for not modifying the reward directly is that within group, all answers might be correct or incorrect; directly penalizing or rewarding them could distort the update signal, compromising the stability and effectiveness of model training. By adjusting the Advantage instead, we preserve meaningful gradient signals while encouraging diverse and accurate reasoning paths. 3.2. Low-Probability Token Length Normalization We observe that applying length normalization to the full sequence likelihood can be suboptimal for reward modeling. Following the insights from (Wang et al., 2025a), predictive uncertainty is typically concentrated in small fraction of generation steps. Specifically, roughly 20% of token positions substantially influence the subsequent reasoning path, while at the remaining positions, the models next-token distribution is sharply peaked, with the top candidate often receiving probability above 0.9. In our framework, applying length normalization over the entire sequence would disproportionately dilute the reward signal by including these trivial high-confidence tokens, leading to weak and less informative training signal. To mitigate this, we define critical subset of tokens, denoted as low , which comprises the approximately 20% of posioi tions in the response oi that exhibit the highest predictive uncertainty. Consequently, we apply this selective length normalization to the confidence scores formulated in Equations 5 and 6. By focusing on this informative subset, we preserve meaningful confidence variations that more directly reflect the models reasoning quality, thereby providing more robust signal for policy optimization. 3.3. ProGRPO Finally, our overall objective function is given by Equation 7. JProGRPO(θ) = (cid:34) 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) i= t=1 (q,a)D,{oi}G i=1πθold (q) (cid:18) ri,t(θ) Ai, min (7) clip (cid:0)ri,t(θ), 1 εlow, 1 + εhigh (cid:1) Ai (cid:19)(cid:35) The final pseudocode is presented in Algorithm 1. detailed theoretical justification of ProGRPO is provided in Appendix A. 4. Experiments 4.1. Experimental Settings Table 1. Training Hyperparameters Hyperparameter Value GRPO Advantage Estimator Use KL Loss No Use Entropy Regularization No 512 Train Batch Size 8092 Max Response Length 32 PPO Mini-batch Size [0.8, 1.28] Clip Ratio Range 1 106 Learning Rate 1.0 Sampling Temperature Number of Rollouts (N ) 8 DAPO (Yu et al., 2025) Reward Function Training Setup. Our experiments are conducted under the GRPO framework; detailed hyperparameter settings are provided in Table 1. We do not employ any specially designed or task-specific prompts in this work. All models are trained and evaluated using the default prompting setup of the underlying language model. Training Dataset. We conduct experiments in two domains: mathematics and code generation. For mathematics, we train on the DAPO dataset (Yu et al., 2025). For code generation, we use the training split of the DeepCoder dataset (Luo et al., 2025). Base Models. Our experiments involve multiple model scales and families. We utilize Qwen2.5-7B, Qwen2.5-32B (Team, 2024) and DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) for general reasoning tasks, while leveraging DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025) specifically for the code domain. 3 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Algorithm 1 ProGRPO: Probabilistic Group Relative Policy Optimization 1: Input: Dataset D, Policy Model πθ, Reference Model πref 2: Hyperparams: Group size G, Learning rate η, Weight α, Clip εlow, εhigh Sample batch of prompts Initialize batch loss Lbatch = 0 for each prompt in do 1. Sampling Phase Generate outputs {o1, . . . , oG} from πθold( q) Compute rewards = {r1, . . . , rG} 2. Standard GRPO Advantage Compute µ = mean(R) and σ = std(R) Ai = riµ σ+δ for {1 . . . G} 3. Advantage Re-weighting (AMR) k=1 rk = 0 or (cid:80)G if (cid:80)G k=1 rk = then Ai Ai for all {1 . . . G} // Calculate Prompt Confidence (Eq. 5) Identify low-prob tokens low Compute cθ(q) for = 1 to do in prompt // Calculate Answer Confidence (Eq. 6) Identify low-prob tokens low Compute cθ(oi q) // Apply Re-weighting (Eq. 4) Ai Ai + α (cθ(q) cθ(oi q)) oi in answer oi else 3: while not converged do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: end for end if 4. Loss Computation (Eq. 7) Initialize prompt loss Lq = 0 for = 1 to do for = 1 to oi do Ratio ri,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) Lsurr = min ri,t(θ) Ai, clip (cid:0)ri,t(θ), 1 εlow, 1 + εhigh (cid:16) (cid:17) (cid:1) Ai Lq Lq + Lsurr end for end for Lq 1 (cid:80)G Lbatch Lbatch + Lq i=1 oi Lq 39: 40: 41: 42: end while end for Update parameters θ by minimizing Lbatch 32: 33: 34: 35: 36: 37: 38: 4.2. Evaluation Math Domain. We evaluate our method on set of widely used mathematical reasoning benchmarks, including 4 AIME2024 (Li et al., 2024), AIME2025 (Balunovic et al., 2025), AMC23 (Li et al., 2024), MATH500 (Hendrycks et al., 2021b), Minerva (Hendrycks et al., 2021a), and OlympiadBench (He et al., 2024). Code Domain. For code-related tasks, we conduct experiments on LiveCodeBench (Jain et al., 2024), CodeForces (Penedo et al., 2025), and HumanEval+ (Liu et al., 2023). For out-of-distribution (OOD) evaluation in the general domain, we use GPQA (Rein et al., 2024) and MMLU-Pro (Wang et al., 2024). During the evaluation, we set the sampling temperature to 0.6 and top-p to 0.95. We report performance using Pass@1 and Pass@k as the primary evaluation metrics. For the Qwen2.5 series (Bai et al., 2023), we perform evaluations on reasoning benchmarks with maximum output length of 8K tokens. For the DeepSeek-R1-Distill-Qwen series (Guo et al., 2025), we use maximum output length of 8K tokens for code-domain tasks and 32K tokens for reasoning tasks, reflecting their extended context capabilities. 4.3. Results Our primary experimental results are summarized in Tables 2 and 3. Across both mathematical reasoning and code generation domains, ProGRPO consistently outperforms the direct reward maximization baseline (GRPO) as well as the reward matching approach proposed by FlowRL. Mathematical reasoning. As shown in Table 2, ProGRPO achieves substantial improvements across all evaluated benchmarks and model scales. For Qwen2.5-7B, ProGRPO attains an average Pass@1 of 43.3%, improving over GRPO by +5.7% and over FlowRL by +8.0%. The gains are even more pronounced in the multi-sample regime, where ProGRPO reaches an average Pass@32 of 68.5%, surpassing GRPO and FlowRL by +13.8 and +7.5%, respectively. Notably, large margins are observed on challenging benchmarks such as AIME 2024 (+12.1 Pass@1 over FlowRL) and OlympiadBench (+7.7 Pass@1 over FlowRL). For Qwen2.5-32B, ProGRPO further scales favorably, achieving an average Pass@1 of 52.7%, which is +4.8% higher than GRPO. Similar trends are observed for DeepSeek-R1-Distill-Qwen-1.5B, where ProGRPO improves the average Pass@1 from 49.4% to 58.3%, demonstrating that our method remains effective even for smaller distilled models. Overall, these results indicate that ProGRPO delivers robust and consistent gains across model sizes and mathematical reasoning tasks. Code generation. Table 3 presents the results on code reasoning benchmarks. On LiveCodeBench, ProGRPO achieves an Avg@16 score of 36.47 and Pass@16 of 54.12, outperforming GRPO by +1.53 and +0.36, respectively. On Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Table 2. Main results across six mathematical reasoning benchmarks. Each cell reports Pass@1 / Pass@32 (%). GRPO w/ KL-Cov (Cui et al., 2025) exhibits high sensitivity to optimization hyperparameters. Although all experiments are conducted using the original implementation without modifications, training instability is occasionally observed, which negatively affects the final results. Method AIME 2024 AIME 2025 AMC 23 MATH 500 Minerva OlympiadBench Average Qwen2.5-7B (Max Response Len = 8K tokens) Baseline GRPO GRPO KL-Cov FlowRL ProGRPO (Ours) 5.4 / 30.0 9.2 / 26.7 6.1 / 26.7 14.6 / 33.3 21.3 / 53.3 2.5 / 20.0 6.1 / 30.0 8.4 / 36.7 10.4 / 40.0 15.9 / 50.0 32.4 / 82.5 65.5 / 80.0 46.3 / 80.0 54.0 / 85.0 67.2 / 92.5 54.7 / 92.6 75.3 / 87.0 56.9 / 87.6 66.9 / 87.0 80.5 / 94.2 22.0 / 54.4 33.8 / 51.1 23.2 / 54.8 30.9 / 54.4 32.0 / 53. 24.6 / 63.1 35.6 / 53.6 25.7 / 60.2 35.0 / 60.7 42.7 / 67.5 23.6 / 57.1 37.6 / 54.7 27.8 / 57.7 35.3 / 61.0 43.3 / 68.5 Qwen2.5-32B (Max Response Len = 8K tokens) Baseline GRPO ProGRPO (Ours) 5.7 / 43.3 16.8 / 43.3 34.6 / 60.0 2.2 / 26.7 13.9 / 26.7 24.8 / 46. 29.7 / 87.5 80.1 / 95.0 80.6 / 97.5 52.0 / 91.8 84.7 / 92.4 82.8 / 96.2 27.3 / 59.6 42.3 / 55.9 39.3 / 58.5 22.4 / 68.4 49.7 / 65.1 54.1 / 74.5 23.2 / 62.9 47.9 / 63.1 52.7 / 72.2 DeepSeek-R1-Distill-Qwen-1.5B (Max Response Len = 32K tokens) Baseline GRPO ProGRPO (Ours) 31.6 / 76.7 26.6 / 70.0 46.0 / 76.7 24.9 / 60.0 28.0 / 56.7 33.6 / 60.0 71.6 / 95.0 77.9 / 97.5 86.3 / 95.0 74.3 / 95.6 80.6 / 96.6 87.0 / 98.2 26.1 / 56.3 29.9 / 56.3 34.8 / 58. 49.1 / 80.1 53.1 / 79.2 62.0 / 81.8 46.3 / 77.3 49.4 / 76.1 58.3 / 78.3 Table 3. Performance comparison on code reasoning benchmarks. All models are evaluated with maximum response length of 8K tokens. * We state that FlowRLs results were reproduced using weights released by Hugging Face, however, this does not affect the overall conclusions of our study. Models LiveCodeBench Avg@ Pass@16 CodeForces Pct. Rating HumanEval+ Avg@16 DeepSeek-R1-Distill-Qwen-7B (Max Response Len = 8K tokens) Backbone GRPO FlowRL ProGRPO (Ours) 30.96 34.94 33.98 36.47 48.03 53.76 51.97 54.12 764.93 1243.24 1129.58 8.2 60.7 48. 1422.49 75.4 80.33 83.32 82.67 84.01 CodeForces, ProGRPO yields substantial improvement, reaching rating of 1422.49, which exceeds GRPO by nearly +180 rating and FlowRL by +293, corresponding to percentile increase to 75.4%. Additionally, ProGRPO attains the best performance on HumanEval+, achieving an Avg@16 score of 84.01%, further confirming its effectiveness in complex logic and syntax generation tasks. As shown in Figure 1, we further compare the Pass@K metric and observe that our method significantly surpasses the baseline base model across all evaluated settings. Table 4. Performance on out-of-distribution (OOD) generaldomain benchmarks. Models MMLU-PRO GPQA (Avg@4) Qwen2.5-7B +GRPO +ProGRPO (Ours) 48.7 52.1 54.3 32.4 38.9 42. In addition, we evaluate the generalization performance of our model under OOD settings. As shown in Table 4, our method still maintains clear advantage over GRPO. In summary, we have achieved consistent and significant performance improvements across varying model scales and architectures. These findings provide strong empirical support for our proposed confidence-based strategy, demonstrating its capability to substantially enhance both model diversity and generalization ability. 4.4. Analysis of Training We continuously monitored the evolution of entropy during training. As shown in the Figure 2, entropy first decreases, then increases, and eventually stabilizes. This behavior can be interpreted as follows: in the early stage of training, the model mainly focuses on learning small number of correct answers, causing the predictive distribution to contract and entropy to decrease. As training progresses and the number of correct answers within group increases, the model begins to allocate probabilities more evenly across samples, leading to smoother output distribution and corresponding rise in entropy, which eventually stabilizes. In contrast, GRPO consistently reinforces the most confident answers, driving probability mass toward few samples and resulting in entropy collapse. Table 5. Dataset-level Evaluation on AIME 2024: Accuracy and Diversity of Correct Solutions Setting Distinct-2 Self-BLEU Semantic Cosine GRPO ProGRPO (Ours) 0.1693 0.1443 0.9299 0. 0.9725 0.9233 5 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Figure 1. Pass@k comparison on AIME 2024, AIME 2025, and AMC 23 benchmarks using Qwen2.5-7B with FlowRL and GRPO and Ours. Figure 2. Training entropy across optimization steps for different methods. Higher entropy indicates increased exploration during policy optimization. We also investigate how diverse are the generations after the models training. Table 5 reports three diversity-related metrics Distinct-2, Self-BLEU, and Semantic Cosine computed at the dataset level for correct solutions on AIME 2024, where sentence representations are obtained using all-MiniLM-L6-v2 (Wang et al., 2020). Compared with the GRPO baseline, our method achieves substantially lower Self-BLEU and Semantic Cosine scores, indicating reduced lexical and semantic redundancy among generated solutions. Although Distinct-2 is slightly lower, this suggests that the improved diversity primarily stems from higherlevel structural and semantic variation in reasoning rather than surface-level n-gram diversification. Overall, these results demonstrate that our approach encourages more diverse yet valid reasoning trajectories, consistent with the observed gains under higher-entropy decoding. Overall, ProGRPO method demonstrates superior performance to GRPO in both reliability and diversity: it achieves higher average probabilities, exhibits greater stability for low-probability tokens, and generates richer outputs. This conclusion aligns with the comparative results presented in Figure 3a and Figure 3b. As shown Figure 4, we conduct systematic analysis of the output entropy on the AIME dataset. The results show that, compared with models trained using GRPO, our method significantly increases the entropy of the output distribu- (a) Boxplot comparison of model performance (OURS vs GRPO). (b) Histogram comparison of model performance (OURS vs GRPO). Figure 3. Comparison of model performance across three metrics (average probability, lower 20% probability, and entropy), with statistics computed over 32 rollouts per sample using the AIME2024 dataset. tion while maintaining comparable Pass@1 performance. Furthermore, when combined with the Pass@k metric, we observe that the model continues to achieve stable improvements in Pass@k under higher entropy levels. This indicates that the increased entropy does not arise from randomization, but rather from generating more diverse set of valid reasoning paths while preserving solution correctness. 6 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities which indicates more effective reinforcement of high-quality reasoning trajectories while maintaining training stability. However, further increasing α (e.g., to 0.7 or 1) degrades performance, suggesting that overly strong confidence signals can dominate the advantage function and weaken the supervision from the original reward. Overall, α = 0.3 achieves the best balance between performance and stability. In summary, cθ(qi) cθ(oj qi) with α = 0.3 is more robust choice, whereas 1 cθ(qi) cθ(oj qi)) tends to be more exploratory, as it encourages larger updates on harder problems compared to easier ones. 5. Related Work reinforcement Reasoning Models. learnRecently, ingdriven reasoning models have typically generated explicit and lengthy chains of thought before producing final answers, as exemplified by models such as OpenAI o1 (Jaech et al., 2024), DeepSeek (Shao et al., 2024), Kimi (Team et al., 2025), and Qwen (Bai et al., 2023) (Team, 2024) (Yang et al., 2025a). Within this paradigm, reinforcement learning with verifiable rewards has become dominant post-training approach, with widely adopted algorithms including GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025), DAPO (Yu et al., 2025), and CISPO (Chen et al., 2025). However, the generalization ability of reasoning models remains critical concern. Prior studies have shown that under the Pass@k metric, the performance advantage of RL-fine-tuned models over their base counterparts often diminishesor even vanishesas increases (Yue et al., 2025). This phenomenon highlights fundamental challenge arising from insufficient exploration mechanisms in reinforcement learning. Exploration in Reinforcement Learning. Exploration and entropy collapse have long been central challenges in reinforcement learning. Prior work has explored the use of entropy-based signals (Cheng et al., 2025) to guide exploration, yet these approaches often yield limited empirical improvements. Other studies introduce entropy regularization (Cui et al., 2025; Wang et al., 2025b) to enhance exploratory behavior, but they still face challenges in terms of stability and effectiveness. Recently, FlowRL (Zhu et al., 2025a) redefines the reward function to assign different scores to different reasoning paths (reward matching), thereby improving the models exploratory capabilitieswhile also effectively balancing the confidence across reasoning paths. Inspired by research on model confidence (Li et al., 2025) and FlowRL (Zhu et al., 2025a), we revisit the relationship between entropy collapse and confidence in policy learning. When the model is overly confident (Li et al., 2025), the policy tends to determinism, leading to entropy collapse. (a) Kernel density estimation (KDE). (b) Boxplot of rollout entropy. Figure 4. Comparison of rollout token-level entropy on AIME 2024 between OURS and the GRPO baseline. 4.5. Ablation Figure 5. Ablation study of average pass@k performance under different advantage formulations. As shown in Figure 5, compared with GRPO, our proposed algorithm consistently achieves stable and significant performance improvements across different models and advantage formulations. In particular, the effectiveness and robustness of our method are consistently validated when encouraging low-probability answers, as well as under advantage designs that combine perplexity both low and high with low-probability answer incentives. Table 6. Impact of the advantage reweighting coefficient α in Eq. 4 on ProGRPO performance, averaged over six benchmarks. Alpha Pass@1 Pass@ 0 (Pure GRPO) 0.3 0.7 1.0 37.6 43.3 40.9 39.0 54.7 68.5 59.8 56.2 As shown in Table 6, the averaged results across six benchmarks demonstrate that the hyperparameter α in Equation has significant impact on model performance. When α = 0, the model reduces to pure GRPO and yields relatively weak performance. Increasing α to 0.3 introduces moderate confidence-based advantage reweighting, leading to substantial improvements in both Pass@1 and Pass@32, 7 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Motivated by this observation, we pose complementary question: what happens when the model is insufficiently confident? To address this, we propose confidence-balancing method based on the correctness of answers, which explicitly regulates model confidence and introduces new research paradigm for achieving better trade-off between exploration and stability. 6. Conclusion In this paper, we propose ProGRPO, novel algorithm approached from the perspective of generative probabilities, designed to address the severe entropy collapse phenomenon observed during RLVR training. By introducing LowProbability Token Length Normalization and confidenceaware Advantage Reweighting mechanism (ARM), ProGRPO effectively mitigates mode collapse while preserving the models reasoning capabilities. Empirical results demonstrate that our method not only achieves competitive performance on standard metrics but also maintains significant advantage in multi-sample settings (e.g., Pass@k), indicating robust capability to generate diverse and correct solutions. Ultimately, ProGRPO presents novel and effective solution to the fundamental Exploration-Exploitation trade-off in LLM reasoning tasks, paving the way for more stable and exploratory reinforcement learning paradigms."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the field of Machine Learning by improving the stability and diversity of reinforcement learning with verifiable rewards for large language models. Our proposed method focuses on mitigating mode collapse during policy optimization, thereby encouraging more diverse and robust reasoning behaviors without introducing new model capabilities or application domains. The techniques presented in this work are primarily methodological and are intended to enhance existing training paradigms for reasoning and code generation tasks. We do not anticipate any significant negative societal or ethical consequences beyond those commonly associated with large language models, such as issues related to misuse or over-reliance, which are not exacerbated by our approach. Overall, we believe that this work contributes positively to the reliability and robustness of machine learning systems."
        },
        {
            "title": "References",
            "content": "Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Balunovic, M., Dekoninck, J., Petrov, I., Jovanovic, N., and Vechev, M. Matharena: Evaluating llms on arXiv preprint uncontaminated math competitions. arXiv:2505.23281, 2025. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Cheng, D., Huang, S., Zhu, X., Dai, B., Zhao, W. X., Zhang, Z., and Wei, F. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021a. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 8 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Li, P., Skripkin, M., Zubrey, A., Kuznetsov, A., and Oseledets, I. Confidence is all you need: Few-shot arXiv preprint rl fine-tuning of language models. arXiv:2506.06395, 2025. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:21558 21572, 2023. Luo, M., Tan, S., Huang, R., Patel, A., Ariyak, A., Wu, Q., Shi, X., Xin, R., Cai, C., Weber, M., et al. Deepcoder: fully open-source 14b coder at o3-mini level. Notion Blog, 2025. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Parashar, S., Gui, S., Li, X., Ling, H., Vemuri, S., Olson, B., Li, E., Zhang, Y., Caverlee, J., Kalathil, D., et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. Penedo, G., Lozhkov, A., Kydlıˇcek, H., Allal, L. B., Beeching, E., Lajarın, A. P., Gallouedec, Q., Habib, N., Tunstall, L., Codeforces. https://huggingface.co/datasets/ open-r1/codeforces, 2025. and von Werra, L. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33: 57765788, 2020. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. 9 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Yang, S., Dou, C., Guo, P., Lu, K., Ju, Q., Deng, F., and Xin, R. Dcpo: Dynamic clipping policy optimization. arXiv preprint arXiv:2509.02333, 2025b. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Zhu, X., Cheng, D., Zhang, D., Li, H., Zhang, K., Jiang, C., Sun, Y., Hua, E., Zuo, Y., Lv, X., et al. Flowrl: Matching reward distributions for llm reasoning. arXiv preprint arXiv:2509.15207, 2025a. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025b. Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 14331438. Chicago, IL, USA, 2008. 10 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities A. Theoretical Justification In this section, we provide rigorous mathematical justification for the proposed ProGRPO framework. Under the RLVR (Reinforcement Learning with Verifiable Rewards) setting, we demonstrate how ProGRPO overcomes the fundamental limitations of standard GRPO through its Adaptive Margin Reward (AMR) mechanism. A.1. Preliminaries and the Homogeneity Limitation Let be prompt sampled from dataset D, and πθ be the policy model. In each iteration, GRPO samples group of outputs = {o1, . . . , oG}. Binary Rewards: r(o) {0, 1}. Correctness Subsets: O+ = {o r(o) = 1} with cardinality = O+. Standard GRPO Statistics: The mean reward µ and standard deviation σ are: µ = , (cid:115) σ = (cid:18) 1 (cid:19) + ϵ (8) Lemma A.1 (Homogeneity of Advantage). In standard GRPO, for any two distinct correct responses oi, oj O+, the advantage values are identical: A(oi) = A(oj) = Apos > 0 (9) 1 µ σ Proof. Since r(oi) = r(oj) = 1, the linear transformation = (r µ)/σ maps all elements of O+ to the same scalar Apos. Consequently, the gradient update θJ (cid:80) A(o)θ log πθ(o) treats all correct trajectories indiscriminately. If πθ(oi) > πθ(oj) initially, the model enters positive feedback loop, exponentially increasing πθ(oi) while suppressing other valid paths oj. This leads to Entropy Collapse. A.2. Theorem 1: Convergence to Confidence Equilibrium and Difficulty Calibration The ProGRPO advantage for oi O+ is defined as: A(oi) = Apos + α (cG cθ(oi q)) (10) (cid:80)G j=1 cθ(oj q) is the prompt-specific group baseline. where cG = 1 Theorem A.2. The AMR mechanism stabilizes policy dynamics within O+ by: (i) inducing Maximum Entropy state through negative feedback, and (ii) eliminating the difficulty bias across different prompts. Proof. Part 1: Negative Feedback for Diversity. Consider o1, o2 O+. If cθ(o1 q) > cθ(o2 q), then A(o1) < A(o2). This differential advantage ensures that over-optimized paths receive smaller reinforcement signal than under-explored ones. Equilibrium is reached only when A(o1) = A(o2), implying cθ(o1 q) = cθ(o2 q), which corresponds to uniform distribution over the success manifold. Part 2: Why Prompt-Specific cG is Essential. Let D(q) represent the intrinsic difficulty of prompt q. For easy prompts, the models absolute confidence cθ is naturally high, while for hard prompts, cθ is low. Failure of Global Baseline: If we used fixed global threshold τ instead of cG, the term (τ cθ) would be consistently negative for all easy prompts (penalizing correct answers) and positive for all hard prompts (ignoring diversity). Calibration via cG: By defining the advantage relative to the group mean cG, we isolate the intra-prompt path discrepancy from the inter-prompt difficulty noise. Mathematically, let cθ(o q) = (q) + δ(o), where (q) is the prompt difficulty component and δ(o) is the path-specific variation. Then: cG cθ(oi q) = (q) + (cid:18) (f (q) + δ(oi)) = δ δ(oi) (11) (cid:88) 1 (cid:19) δ(oj) 11 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities The prompt-specific bias (q) is canceled out. This ensures that the diversity pressure is applied consistently across the entire dataset, regardless of whether prompt is easy or difficult. A.3. Theorem 2: Semantic Diversity vs. Syntactic Fluency Theorem A.3. AMR induces semantic-level diversity on reasoning paths while preserving the syntactic certainty of functional segments. Proof. Let an output be decomposed into functional tokens Sfunc (e.g., The answer is) and reasoning tokens Sreason. The confidence cθ is computed over the low-probability set low: low = {t [1, o] pθ(ot o<t) < bottom-20% threshold} For functional tokens, pθ(t) 1 due to grammatical determinism, hence Sfunc low = . Consequently: π(t) = 0, Sfunc (12) (13) Unlike standard Entropy Maximization which penalizes all tokens, AMR targets only the branching points in Sreason, protecting the models linguistic fluency. A.4. Theorem 3: Preservation of Correctness Theorem A.4. The AMR mechanism strictly bounds exploration within the valid reward landscape and does not encourage incorrect responses O. Proof. For an incorrect response oneg where r(oneg) = 0, the base advantage is Aneg = µ/σ < 0. Under AMR: To ensure oneg remains suppressed, we require Aneg < 0. This is satisfied when: Aneg = Aneg + α (cG cθ(oneg q)) α < Aneg sup cG cθ (14) (15) Since Aneg is significantly negative in the early-to-mid training stages and α is small hyperparameter (e.g., 0.1), the sign of the gradient remains negative. Thus, AMR acts as modulation of the penalty magnitude rather than reversal of the objective, ensuring incorrect paths are never reinforced. A.5. Theorem 4: Implicit Entropy Regularization on the Success Manifold Theorem A.5. Under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, the Advantage Modulation Rule (AMR) in ProGRPO induces an implicit entropy-maximizing bias over the set of correct solutions O+. Specifically, while preserving correctness, AMR promotes high-entropy policy over O+, thereby mitigating mode collapse. Proof. 1. Entropy collapse in standard GRPO. In standard GRPO, Lemma A.1 shows that all correct solutions O+ share the same positive advantage Apos. The policy gradient restricted to O+ is therefore θJGRPO (cid:12) (cid:12)O+ (cid:88) oiO+ Aposθ log πθ(oi q). (16) This corresponds to uniform likelihood amplification over competing correct trajectories. From an information-theoretic perspective, such undifferentiated reinforcement causes probability mass to concentrate on the initially slightly more likely paths (e.g., shorter or syntactically simpler solutions), leading to entropy collapse: H(πθ O+) 0. Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities 2. AMR as an entropy-promoting mechanism. Consider the Shannon entropy of the policy restricted to O+: HO+ = (cid:88) i=1 pi log pi, pi = πθ(oi q) jO+ πθ(oj q) (cid:80) . The gradient of HO+ encourages the distribution to become uniform over O+. In ProGRPO, the modulated advantage for correct solutions is A(oi) = Apos + α (EjG[cθ(oj q)] cθ(oi q)) , (17) (18) where the confidence score cθ(oi q) serves as monotonic proxy for log πθ(oi q) (e.g., defined as an average of token-level probabilities). Ignoring the constant term Apos, the AMR-induced gradient direction is θJAMR (cid:88) oiO+ α(c ci)θ log πθ(oi q), (19) where denotes the group-average confidence. Paths with higher-than-average confidence are down-weighted, while lower-confidence paths are amplified. This update direction is aligned with that of minimizing the KL divergence between the policy restricted to O+ and the uniform distribution, thereby implicitly encouraging higher entropy. 3. Stationary points. At stationary point, θJ = 0, which within O+ requires By the definition of AMR, this implies A(oi) = A(oj), i, j. cθ(oi q) = cθ(oj q), i, j, indicating equalized confidence (and thus probability mass) across all correct solutions. Consequently, the induced policy attains maximum-entropy configuration over O+. Therefore, ProGRPO preserves correctness while implicitly steering the policy toward high-entropy distribution on the success manifold, effectively preventing mode collapse. B. Additional Results Figure 6. Reproduction of the best-performing method proposed in Entropy Mechanism (Cui et al., 2025) 13 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities We reproduced the method proposed by Cui et al. (2025). During our experiments, we observed that although the method can achieve the reported performance in some cases, the training process is extremely unstable and prone to collapse, as show in Figure 6. This highlights the practical challenges of reproducing the approach and motivates the need for more stable alternatives. (a) Kernel density estimation (KDE). (b) Boxplot of rollout entropy. Figure 7. Rollout token-level entropy comparison on Math 500 between OURS and the GRPO baseline. Since the AIME2024 dataset contains only 30 samples, we also computed the entropy on the Math500 dataset (see Figure 7), and the results are consistent with the analysis on AIME2024. Analysis of single sample from the AIME2024 dataset (Figure 8) shows that our model achieves higher token-level entropy and more balanced probabilities across 32 rollouts. This is consistent with the inter-sample balancing mechanism in Equation 4, demonstrating that our model outperforms the GRPO baseline in both reliability and diversity of generation. Method AIME 2024 AIME 2025 AMC 23 MATH 500 Minerva Olympiad Avg Baseline GRPO with 1 cθ(oj qi) with 1 cθ(qi) cθ(oj qi) with cθ(qi) cθ(oj qi) 5.4 / 30.0 9.2 / 26.7 12.5 / 33.3 16.6 / 43.3 21.3 / 53. 2.5 / 20.0 6.1 / 30.0 9.7 / 30.0 15.2 / 46.7 15.9 / 50.0 32.4 / 82.5 65.5 / 80.0 66.3 / 95.0 70.0 / 90.0 67.2 / 92.5 54.7 / 92.6 75.3 / 87.0 73.7 / 91.2 81.8 / 95.0 80.5 / 94.2 22.0 / 54.4 33.8 / 51.1 32.7 / 52.9 36.4 / 57.4 32.0 / 53.3 24.6 / 63.1 35.6 / 53.6 36.0 / 61.1 45.8 / 70.0 42.7 / 67.5 23.6 / 57.1 37.6 / 54.7 38.5 / 60.6 44.3 / 67.1 43.3 / 68. Table 7. Ablation study on reward formulation. Each cell reports Acc / Pass@32 (%). As shown in Table 7, these results highlight that relative confidence reweighting, rather than absolute confidence penalties, is essential for selectively attenuating dominant paths while promoting under-explored correct reasoning trajectories. C. 14 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities Figure 8. Per-sample analysis of 32 rollouts, showing average token probability and the mean of the lowest 20% token probabilities."
        }
    ],
    "affiliations": [
        "FusionBrain Lab, Russia Mathematics, Russia"
    ]
}