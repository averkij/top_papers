{
    "paper_title": "Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems",
    "authors": [
        "Daniil Sherki",
        "Ivan Oseledets",
        "Ekaterina Muravleva"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 7 3 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "A PREPRINT",
            "content": "Daniil Sherki Skolkovo Institute of Science and Technology Sberbank, AI4S Center Moscow, Russian Federation daniil.sherki@skoltech.ru Ivan Oseledets Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute Moscow, Russian Federation i.oseledets@skoltech.ru Ekaterina Muravleva Skolkovo Institute of Science and Technology Sberbank, AI4S Center Moscow, Russian Federation e.muravleva@skoltech.ru"
        },
        {
            "title": "ABSTRACT",
            "content": "Solving Bayesian inverse problems efficiently remains significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations. Keywords Conditional Flow Matching Bayesian Inverse Problems Generative Modeling Inverse Problems"
        },
        {
            "title": "Introduction",
            "content": "Many natural processes can be mathematically modeled using appropriate formal representations. However, the challenge often lies in inferring latent parameters that are not directly observable. These parameters must typically be estimated from limited observations, giving rise to Bayesian inverse problems. The idea of Bayesian inversion is to parametrize the posterior distribution of model parameters, given observations and prior distribution on the model parameters. The main challenge is that typically the distribution is known up to normalization constant, making sampling from the posterior intractable. Classical methods like Markov Chain Monte Carlo (MCMC) Geyer [1992] rely on many forward problem solutions for each set of observations, which can be very time-consuming. The Bayesian inversion is widely used for addressing inverse problems across diverse domains such as physics and engineering [Cotter et al., 2009, Koval et al., 2024]. Its appeal lies in its ability not only to deliver solution estimate but also to quantify the associated uncertainty. Understanding the distribution of computed quantity is particularly valuable in applications like digital twins [Kapteyn et al., 2021]. For instance, one may need to recover parameters of an ODE system modeling disease spread from observed infection data, or reconstruct permeability fields from indirect measurements [Koval et al., 2024]. natural approach for tackling Bayesian inverse problems is to apply generative models. There are many available options, like variational autoencoders [Kingma and Welling, 2022], Generative Adversarial Networks (GAN) [Goodfellow et al., 2014] or diffusion models [Sohl-Dickstein et al., 2015], normalizing flows offer exact likelihood estimation [Gudovskiy et al., 2024] while avoiding these computational bottlenecks. In this work, we focus on recent generative modelling technique, conditional flow matching, and show that it can be efficiently and easily applied to different Bayesian inverse problems. PREPRINT Our contribution We formulate the Bayseian inverse problem as the problem of learning conditional probability distribution from samples, that can be easily constructed We propose transformer-based CFM architecture that can handle different number of observations We test our method on several inverse problems and compare it to the MCMC approach."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Classical approaches to solve Bayesian Inverse Problem The primary challenges associated with classical methods for solving Bayesian Inverse Problems include high computational costs, difficulties with variational methods, the need for numerous evaluations of the forward model, and limitations in real-time inversion capabilities. Specifically, sampling from complex Bayesian posterior distributions using statistical simulation techniques, such as Markov Chain Monte Carlo (MCMC), Hamiltonian Monte Carlo, and Sequential Monte Carlo, is computationally expensive. Variational inference algorithms, including mean-field variational inference and Stein variational gradient descent, face challenges in high-dimensional settings due to the difficulty of accurately approximating posterior distributions. Additionally, these methods require multiple evaluations of the forward model and complicated parametric derivatives, further increasing computational costs in high-dimensional scenarios. Consequently, classical approaches may be less efficient for real-time inversions, particularly when dealing with new measurement data, highlighting the need for more efficient alternatives such as deep learning-based methods Guan et al. [2023]. Deep Learning models for solving inverse problems Bayesian inverse problems have also been addressed using physics-informed neural networks [Raissi et al., 2019] with combining invertible flow-based neural networks (INN). In [Guan et al., 2023], this approach is shown to be effective but not universally applicable, as it requires designing custom loss function for each PDE to ensure efficient training. Additionally, the number of observations for the Darcy flow problem remains fixed. significant number of inverse problems in medicine can be effectively addressed using generative networks [Aali et al., 2023, Song et al., 2022]. Generative Adversarial Networks and Variational Autoencoders Generative Adversarial Networks (GANs), first introduced by Goodfellow et al. [2014], have become cornerstone of generative modeling. Recent advances demonstrate their applicability to Bayesian inverse problems. For instance, Mücke et al. [2022] proposed MCGAN, GAN-based framework to circumvent the computational burden of traditional MCMC methods. By replacing the physical forward model with trained generator during inference, their approach accelerates likelihood evaluations for complex PDE-based problems. GANs have also been integrated into hybrid Bayesian frameworks. In Patel et al. [2020], GAN was employed to approximate high-dimensional parameter priors within MCMC sampling. Similarly, Xia and Zabaras [2022] combined VAE-based prior with MCMC for posterior estimation. This method was called Multiscale deep generative model (MDGM). Although, these methods leverage generative models to enhance prior representation, their computational gains remain limited, as they still require iterative forward model evaluations during sampling. Other works, such as Goh et al. [2021] using VAEs embed generative models into variational inference frameworks. However, these approaches face trade-offs between approximation accuracy and scalability in high-dimensional settings. However these approaches have two fundamental disadvantages. First, they do not operate with an arbitrary number of observations, which can be critical in real-world problems. Second, these approaches often do not escape the iterative process itself due to the fact that such approaches do not explicitly generate posterior distribution. Flow Matching Flow Matching (FM) [Lipman et al., 2023] is an efficitient approach to generative modeling based on Continuous Normalizing Flows (CNFs) [Chen et al., 2018], enabling large-scale CNF training without simulation by regressing vector fields of fixed conditional probability paths. It generalizes diffusion models by supporting broader class of Gaussian probability paths, including Optimal Transport (OT) displacement interpolation, which enhances efficiency, stability, and sample quality. Compared to diffusion-based methods, FM allows faster training and sampling, improves likelihood estimation, and enables reliable sample generation using standard numerical ODE solvers, making it alternative for high-performance generative modeling. Whang et al. [2021] demonstrated their utility in Bayesian inverse problems by embedding Normalizing Flow into variational inference framework, enabling flexible posterior approximations. While these methods offer theoretical 2 PREPRINT guarantees on invertibility, their computational cost grows with model complexity, limiting their practicality for large-scale physical systems. Method Base model Exact likelihood estimation No middle-man Training Arbitrary number of observations MDGM MCGAN PI-INN CFM-Tr (ours) CFM + Transformer VAE based on CNN MCMC + GAN PI + flow-based model Table 1: Comparison of methods for solving Bayeisan Inverse problems. *MDGM use the PDE solution as holistic observation; the problem was not formulated as the recovery of the forward model from small number of observations The table 1 provides methods comparing for solving Bayesian inverse problems. Deep learning methods for solving Bayesian inverse problems exhibit distinct strengths and limitations. MDGM [Xia and Zabaras, 2022] leverages VAE-based convolution neural network with MCMC for multiscale inference, excelling in high-dimensional PDE-based problems but lacking exact likelihood estimation and flexibility for arbitrary observations. MCGAN [Mücke et al., 2022] combines MCMC with GANs for high-fidelity posterior sampling but suffers from computational complexity, lack of explainability, and fixed observation models. PI-INN [Guan et al., 2023] employs physics-informed flow-based models, enabling exact likelihood estimation and end-to-end training but struggles with variable observation sizes due to architectural constraints. In contrast, CFM-Tr integrates conditional flow matching with transformers, offering exact likelihood estimation, end-to-end training, and adaptability to arbitrary observations, making it suitable for dynamic inverse problems like real-time medical imaging. While MDGM and PI-INN are effective for structured problems, CFM-Tr addresses key limitations by combining flexibility, exact inference, and scalability."
        },
        {
            "title": "3 Methodology",
            "content": "Consider forward model defined as: = F(m, e) + η, where represents model parameters sampled from their prior distribution, denotes experimental conditions or design parameters, and η is random noise sampled from predefined noise distribution. The Bayesian inverse problem aims to infer unknown/unobservable parameters using known experiment parameters and observations from the forward model. The solution is characterized by posterior probability distribution, with density given by Bayes law: π(md, e) = π(dm, e) π(m) , where π(m) is the prior distribution encoding prior knowledge about parameters, π(dm, e) is the likelihood, and π(md, e) is the posterior distribution. The primary objective is to solve the inverse problem: given observations and experiment parameters e, infer the model parameters m. Since is not uniquely determined by and e, it is characterized by the conditional distribution π(md, e). The solution can be reformulated as learning the conditional distribution π(md, e). To achieve this, we employ the conditional flow matching (CFM) framework from [Lipman et al., 2023] (Algorithm 1). This involves first sampling from an unconditional prior distribution for (denoted as m0). Dataset The key idea is that we can easily sample from the joint distribution (mi, di, ei). In order to do that. we generate random model parameters (from the prior distribution) and random observation points. When pair mi, ei is given, we can compute di using the forward model. But mi is also sample from the conditional distribution π(mdi, ei). Thus, when forward model and prior distributions of model parameters and experimental parameters are known, we generate training data by sampling multiple variants of and and computing the forward model to obtain observations d. For each model parameter mi we sample di for several points ei, so our training data contains Algorithm 1: Conditional Flow Matching Training Algorithm Input: Dataset of paired samples (x1, e, d), neural network model vθ(t, x, e, d), conditioning data and d, time PREPRINT Uniform(0, 1), number of epochs Nepoch Output: Trained conditional flow model vθ(t, x, e, d) for 1 to Nepoch do for each minibatch of samples (x0, x1) do U(0, 1) x0 prior distribution xt x1 + (1 t) x0 Compute the target velocity: ut x1 x0 Predict the velocity: vt v(t, xt, e, d) Compute the loss: L(θ) (cid:2)(vt ut)2(cid:3) Compute gradients: θL(θ) Update θ using the optimizer and θL(θ) end end return vθ(t, x, e, d) // Sample set of the form (mi, di, ei), where di and ei are sequences of different lengths. The model should be able to sample mi given observations (di, ei). In order to do that, we utilize CFM. Training We define conditional interpolation path between (m0, d, e) and (m, d, e), where (d, m, e) is sampled from the dataset. The interpolation path is given by: mt = (1 t)m0 + m, [0, 1] In the CFM approach, we learn velocity field vθ(mt, t, d, e) that minimizes: Et,m0,(m,d,e)ρ (cid:2)vθ(mt, t, d, e) (m m0)2(cid:3) min θ Here, vθ is learnable function parameterized by θ that predicts the velocity field given inputs (mt, t, d, e). The input dimensions correspond to mt, t, d, and e, while the output dimension matches that of m. During training, elements (d, m, e) are sampled from the dataset, and m0 is drawn from the prior for each iteration of stochastic optimizer. neural network effectively represents vθ in our experiments. Once trained, samples from π(md, e) are generated by solving an ordinary differential equation (ODE) parameterized by the learned velocity field. key feature of our approach is the ability to handle arbitrary numbers of observations and design parameters as input. This capability stems from our transformer architecture, shown in Figure 2. Architecture We parametrize the velocity field by the transformer architecture with bi-directional attention, motivated by the Diffusion Transformer Peebles and Xie [2022]. Specifically, our transformer implementation uses linear projection of input parameters into the embedding space. Time is encoded using Timestep Embedder (from [Peebles and Xie, 2022]), which ensures proper time representation in the embedding space. Root Mean Square (RMS) normalization stabilizes learning dynamics. The activation function is = ReLU(x)2. Self-attention uses rotary position embeddings (RoPE), enabling the transformer to learn relative token positions and generalize to sequences longer than those seen during training. The architecture varies slightly across tasks to accommodate different input data representations. Specific implementations for tasks from Section 4 are detailed in Figure 6 in Appendix A. Model inference follows Algorithm 2, where the trained CFM model serves as the velocity field in the ODE. 4 PREPRINT Figure 1: Solving the inverse problem using flow-matching scheme Algorithm 2: Conditional Flow Matching Inference Algorithm Input: Trained CFM model vθ(t, x), conditioning data and d, initial sample x0, experiment parameters e, arbitrary observations Output: Generated parameters x(t = 0) prior distribution x(t = 1) dx return x(t = 1) dt = vθ(t, xt, e, d) Handling variable number of observations We need to be able to predict the model parameters from different number of observation points. As mentioned before, we generate datasets with varying numbers of observation points, where each batch corresponds to samples with specific number of points. During training, the model processes batches with different numbers of points sequentially. We propose two strategies: the first follows Algorithm 1, computing gradients for each batch and updating after fixed number of batches. Alternatively, as shown in Algorithm 3, we can accumulate gradients across batches with different numbers of points before performing single optimizer step."
        },
        {
            "title": "4 Numerical Experiments",
            "content": "We utilize numerical experiment formulations adopted from [Koval et al., 2024]. Specifically, we consider solutions of ordinary differential equation systems modeling disease propagation, as well as elliptic partial differential equations such as the Darcy Flow. These problem classes are widely employed in the literature on Bayesian inverse problems. 4.1 Simple nonlinear model In our experiments, we used the following forward model from Koval et al. [2024]: d(e, m) = e2m3 + exp(0.2 e) + η 5 PREPRINT Figure 2: Transformer architecture where η follows known noise distribution, specifically (0, σ2). In the simplest example from Koval et al. [2024], the model parameter is one-dimensional, uniformly distributed on [0, 1]. The experiment parameter is also one-dimensional from [0, 1] and uniformly distributed. We generate random triples (di, mi, ei) by: Sampling from U[0, 1] Sampling from U[0, 1] Sampling noise η from the noise distribution Computing = (m, e) + η After sampling, we obtain dataset in the form of an matrix, where = 3. These are samples from the joint distribution π(d, m, e). The prior distribution for training conditional flow matching was simple uniform distribution m0 U[0, 1]. After 10,000 runs of the trained model, the generation error is 1.5 103 0.9 103. Figure 3 shows example paths as we move from the prior distribution to the target distribution π(d, m, e). Notably, due to the efficient learning of Flow Matching, the paths are almost straight, indicating optimal transport. 6 PREPRINT Figure 3: Generation paths of variable conditional on different d, from prior uniform distribution 4.2 SEIR disease model The SEIR (Susceptible-Exposed-Infected-Recovered) model is mathematical framework used to simulate the spread of infectious diseases. In this case study, we simulate realistic scenario where we measure the number of infected and deceased individuals at random times and use this information to recover the control parameters of the ODE system. Following [Koval et al., 2024], we adopt the SEIR model, which assumes constant population size and is described by the following system of ordinary differential equations: dS dt dI dt = β(t)SI, dE dt = β(t)SI αE = αE γ(t)I, dR dt = γ(t)I where S(t), E(t), I(t), R(t) denote the fractions of susceptible, exposed, infected and removed individuals at time t, respectively. These are initialized with S(0) = 99, E(0) = 1, I(0) = R(0) = 0. The parameters to be estimated are β(t), α, γr, and γd(t), where the constants α and γr denote the rates of susceptibility to exposure and infection to recovery, respectively. To simulate the effect of policy changes or other time-dependent factors (e.g., quarantine and hospital capacity), the rates at which exposed individuals become infected and infected individuals perish are assumed to be time-dependent and parametrized as: β(t) = β1 + tanh(7(t τ )) (β2 β1) γ(t) = γr + γd(t) γd(t) = γd 1 + tanh(7(t τ )) 2 (γd 2 γd 1 ) where the rates transition smoothly from initial rates (β1 and γd We fix τ = 2.1 over time interval of [0, 4]. The experiment consists of choosing four time points = [a1, a2, a3, a4] U[1, 3] to measure the number of infected and deceased individuals di = [Iei, Rei] for [1, 4] (d R24). The 1 ) to final rates (β2 and γd 2 ) around time τ > 0. 7 goal is to optimally infer the 6 rates = [β1, α, γr, γd problem, we learn to smoothly transition from the distribution U[0, 1]6 to the distribution ˆm ρ(me, d). 2 ]. After training an MLP and solving the flow matching 1 , β2, γd To summarize the inputs and outputs: PREPRINT = [a1, a2, a3, a4] U[1, 3]: random measurement times di = [Iei, Rei] for [1, 4] (d R24): numbers of infected and deceased individuals = [β1, α, γr, γd 2 ]: ODE model parameters 1 , β2, γd Using ˆm, we can obtain the predicted dynamics of infected and deceased individuals ˆd. We measure accuracy using: ε = ˆd2 d2 For mtrue = [0.4, 0.3, 0.3, 0.1, 0.15, 0.6], after 1,000 calculations the average error is 2.05% 1.04% using 4-point multilayer perceptron (MLP) model."
        },
        {
            "title": "4.3 Permeability field inversion",
            "content": "We next consider the problem of solving two-dimensional elliptic PDE. This type of problem is common in the oil industry, where pressure observations from small number of wells are used to reconstruct the permeability field of an oil reservoir. The equation also has applications in groundwater modeling and many other domains. with boundary conditions: (κu) = 0 u(x = 0, y) = (y, e1) = exp (cid:18) (cid:18) u(x = 1, y) = g(y, e2) = exp (cid:19) (y e1)2 (cid:19) (y e2) 1 2σw 1 2σw The equation is solved using the finite element (FE) method with second-order Lagrange elements on mesh of size = 1 64 in each coordinate direction, where κ is custom 2D matrix. The discretization follows Dolgov et al. [2012]. In this example, the inverse problem consists of estimating the spatially-dependent diffusivity field κ given pressure measurements at pre-determined locations (xi, yi) Ω. To ensure κ is nonnegative, we impose Gaussian prior on the log diffusivity, = log(κ) (0, Cpr), with covariance operator Cpr defined using squared-exponential kernel: c(x, z) = σ2 exp (cid:20) z2 2ℓ (cid:21) for x, Ω, with σv = 1 and ℓ2 = 0.1. Using truncated Karhunen-Loève expansion of the unknown diffusivity field yields the approximation: m(x, m) nm(cid:88) i= (cid:112) mi λiϕi(x), where λi and ϕi(x) denote the i-th largest eigenvalue and eigenfunction of Cpr, respectively, and the unknown coefficients mi (0, 1). The Karhunen-Loève expansion is truncated after nm = 16 modes, capturing 99 percent of the weight of Cpr. The transformer architecture enables the use of different input types for the same problem. Here, in addition to the equation values themselves, we use the coordinates of measurement points. The specific architecture is detailed in Figure 6 in Appendix A. PREPRINT The input consists of vector of values of arbitrary length and two corresponding vectors of coordinates x, y. The final input is matrix = (d, x, y)T with shape (n, 3). Our results show that we can effectively recover the PDE coefficient using just few strategically placed measurement points. Figure 5 demonstrates that with 8 relatively uniformly distributed points over the solution field, we can obtain an almost identical solution (approximately 2."
        },
        {
            "title": "5 Results and Discussions",
            "content": "Table 2 presents the results of numerical experiments for our proposed method using the following error metric: ε = DE(m) DE( m) DE(m) where DE represents the solution of the differential equation (ODE or PDE) using either the true parameters or the generated parameters m, computed as an average over 10 generations from the flow matching model. The true solution of the ODE system and the reconstructed parameter distribution, obtained using only 4 data points, are illustrated in Figure 4. Figure 4: Probabilistic solutions to the inverse problem for mtrue = [0.4, 0.3, 0.3, 0.1, 0.15, 0.6] Table 2: The relative inference error of the trained model for two numerical experiments SEIR Problem 4.2 Permeability Field4.3 2 3 4 5 6 7 8 10.88% 2.39% 3.31% 1.47% 2.80% 1.37% 2.15% 0.99% 1.97% 0.91% 1.59% 0.75% 1.48% 0.71% 28.84% 3.43% 16.23% 1.53% 17.80% 1.99% 16.86% 1.76% 7.21% 1.26% 7.48% 1.23% 2.75% 0.60% PREPRINT Figure 5: PDE coefficient and solution: true (left) and reconstructed using Flow-matching (right) We compared our method with the Metropolis-Hastings MCMC (MH-MCMC) algorithm. For fair comparison, we ran MH-MCMC with enough iterations to achieve approximately the same relative error as our method (shown in Table 2). The results for the SEIR problem are presented in Table 3. Table 3: Relative errors for numerical experiments using MCMC 2 3 4 5 6 7 SEIR Problem Permeability field Nsample Relative Error Nsample Relative Error 15 000 10 000 5 000 5 000 5 000 5 000 10 000 31.39% 3.26% 3.24% 2.74% 1.64% 4.07% 1.44% 56.57% 39.40% 57.66% 55.71% 42.63% 36.13% 60.02% 10 000 10 000 10 000 10 000 10 000 10 000 5 Comparison with MCMC Comparing Tables 2 and 3, we observe that MCMC requires significantly more iterations to achieve similar accuracy as Conditional Flow Matching. This computational cost is particularly significant for elliptic PDEs. Even with 10,000 MCMC iterations, the error in solving the PDE remains above 30% for 6 or more observations, while Conditional Flow Matching achieves 2-8% error in these cases. The computational time difference is substantial: MCMC takes approximately 37 minutes for 10,000 samples, while Flow Matching requires only 1.08 seconds (on CPU) per inference using Algorithm 2 for the permeability inversion problem. For the SEIR model, the CFM transformer takes approximately 0.22 seconds. Our numerical experiments demonstrate the following key findings: 1. Deep learning generative models can be effectively trained to handle variable-length inputs for solving Bayesian Inverse Problems. This is achieved through Transformer architecture with Rotary Embeddings, enabling inference on sequences longer than those seen during training. 2. Flow Matching successfully learns non-trivial paths from prior to target distributions, as evidenced by the nearly straight paths shown in Figure 3. PREPRINT 3. The ability to handle arbitrary numbers of observations proves highly beneficial, with Table 2 showing consistent error reduction as the number of observations increases. Future research directions include combining Conditional Flow Matching with classical MCMC methods. Given MCMCs slow convergence, Flow Matching could serve as an improved prior distribution for MH-MCMC. Additionally, Conditional Flow Matching shows promise for determining optimal experiment design parameters e, which could further enhance its applicability to real-world problems."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "The CFM approach is not the only generative modelling technique available. Certainly, we can use normalizing flows, tensor methods Koval et al. [2024], generative adversarial networks. We also need to establish the efficiency of the approach for high-dimensional Bayesian inverse problems, which will require modifications of the architecture and additional scaling of the datasets. CFM learns to sample from the distribution, whereas the Bayesian optimal experiment design requires the evaluation of the log-likelihood. Although it is in principle possible, we did not study the complexity and accuracy of the evaluation of the logarithm of the posterior distribution. Another line of work is to introduce the guidance to the CFM objective that is enforced by the forward model. Once we got the estimate of the parameter m, we can check, if it really fits the observations; good question is how to modify the inference procedure to correct for possible errors. An important limitation is the study of the actual properties of the learned distribution. For the cases, when the model parameters are defined by the observations, the velocity field vt will likely not depend on the noise, but only on d, e, effectively solving the regression problem of predicting mi from di, ei. The usefulness and emergency of randomization with respect to the noise in CFM still needs to be studied. There are two options for using it. First, we can sample different noises, get estimates of the model parameters and plot the distributions (see the Section on the SEIR model, where some of the parameters show greater variability). The second option is to pick samples that provide better fit to the data. This still needs to be studied, since in some of the experiments we found differences in the distribution of provided by MCMC and CFM. The question has to be studied in more details."
        },
        {
            "title": "7 Conclusions",
            "content": "We believe that our method quite universal and can be adapted to large number of problems in short time if the problem is reduced to the standard Bayesian Inverse Problem formulation, since it can learn complex nonlinear distributions. Another advantage is the possibility of using an input that is not fixed in terms of the number of observations, where increasing the number of observed points increases the accuracy of the method in terms of recovering the solution from the generated parameters. Finally, we can use the learned distribution to do Bayesian optimal experiment design. References Asad Aali, Marius Arvinte, Sidharth Kumar, and Jonathan I. Tamir. Solving inverse problems with score-based generative priors learned from noisy data. In Proc. 57th Asilomar Conf. Signals Syst. Comput., pages 837843. IEEE, 2023. doi:10.1109/ieeeconf59524.2023.10477042. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Adv. Neural Inf. Process. Syst., volume 31. Curran Associates, 2018. S. Cotter, M. Dashti, J. C. Robinson, and A. Stuart. Bayesian inverse problems for functions and applications to fluid mechanics. Inverse Probl., 25:115008, 2009. doi:10.1088/0266-5611/25/11/115008. Sergey Dolgov, Boris N. Khoromskij, Ivan Oseledets, and Eugene Tyrtyshnikov. reciprocal preconditioner for structured matrices arising from elliptic problems with jumping coefficients. Linear Algebra Appl., 436(9):29803007, 2012. doi:10.1016/j.laa.2011.09.010. Charles Geyer. Practical markov chain monte carlo. Statistical science, pages 473483, 1992. Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, and Tan Bui-Thanh. Solving bayesian inverse problems via variational autoencoders, 2021. URL https://arxiv.org/abs/1912.04212. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https://arxiv.org/abs/1406.2661. PREPRINT Xiaofei Guan, Xintong Wang, Hao Wu, Zihao Yang, and Peng Yu. Efficient bayesian inference using physics-informed invertible neural networks for inverse problems, 2023. URL https://arxiv.org/abs/2304.12541. Denis Gudovskiy, Tomoyuki Okuno, and Yohei Nakata. Contextflow++: Generalist-specialist flow-based generative models with mixed-variable context encoding. In Negar Kiyavash and Joris M. Mooij, editors, Proc. 40th Conf. Uncertain. Artif. Intell., volume 244 of Proc. Mach. Learn. Res., pages 14791490. PMLR, Jul 2024. Michael Kapteyn, Jacob Pretorius, and Karen Willcox. probabilistic graphical model foundation for enabling predictive digital twins at scale. Nat. Comput. Sci., 1(5):337347, May 2021. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https://arxiv.org/abs/1312. 6114. Karina Koval, Roland Herzog, and Robert Scheichl. Tractable optimal experimental design using transport maps, 2024. URL https://arxiv.org/abs/2401.07971. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Nikolaj T. Mücke, Benjamin Sanderse, Sander Bohté, and Cornelis W. Oosterlee. Markov chain generative adversarial neural networks for solving bayesian inverse problems in physics applications, 2022. URL https://arxiv.org/ abs/2111.12408. Dhruv Patel, Deep Ray, Harisankar Ramaswamy, and Assad Oberai. Bayesian inference in physics-driven problems with adversarial priors, 12 2020. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys., 378: 686707, 2019. doi:10.1016/j.jcp.2018.10.045. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/1503.03585. Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models, 2022. URL https://arxiv.org/abs/2111.08005. Jay Whang, Erik M. Lindgren, and Alexandros G. Dimakis. Composing normalizing flows for inverse problems, 2021. URL https://arxiv.org/abs/2002.11743. Yingzhi Xia and Nicholas Zabaras. Bayesian multiscale deep generative model for the solution of high-dimensional inverse problems. J. Comput. Phys., 455:111008, 2022."
        },
        {
            "title": "The transformer architecture for two numerical experiments",
            "content": "A PREPRINT Figure 6: Transformer architecture for 4.2 (left) and 4.3 (right)"
        },
        {
            "title": "B Alternative algorithm",
            "content": "Algorithm 3: Conditional Flow Matching Training Algorithm v2 Input: Dataset of paired samples (x1, e, d), neural network model vθ(t, x, e, d), conditioning data and d, time Uniform(0, 1), number of epochs Nepoch Output: Trained conditional flow model vθ(t, x, e, d). for 1 to Nepoch do for each minibatch of samples (x0, x1) do U(0, 1) x0 prior distribution xt x1 + (1 t) x0 Compute the target velocity: ut x1 x0 Predict the velocity: vt v(t, xt, e, d) Compute the loss: L(θ) (cid:2)(vt ut)2(cid:3) Compute gradients: θL(θ) Accumulate gradients: ΣL(θ) = ΣL(θ) + θL(θ) end Update θ using the optimizer and ΣL(θ) end return vθ(t, x, e, d) 13 // Sample t"
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute",
        "Sberbank, AI4S Center",
        "Skolkovo Institute of Science and Technology"
    ]
}