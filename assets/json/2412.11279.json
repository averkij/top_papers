{
    "paper_title": "VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping",
    "authors": [
        "Hao Shao",
        "Shulun Wang",
        "Yang Zhou",
        "Guanglu Song",
        "Dailan He",
        "Shuo Qin",
        "Zhuofan Zong",
        "Bingqi Ma",
        "Yu Liu",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video face swapping is becoming increasingly popular across various applications, yet existing methods primarily focus on static images and struggle with video face swapping because of temporal consistency and complex scenarios. In this paper, we present the first diffusion-based framework specifically designed for video face swapping. Our approach introduces a novel image-video hybrid training framework that leverages both abundant static image data and temporal video sequences, addressing the inherent limitations of video-only training. The framework incorporates a specially designed diffusion model coupled with a VidFaceVAE that effectively processes both types of data to better maintain temporal coherence of the generated videos. To further disentangle identity and pose features, we construct the Attribute-Identity Disentanglement Triplet (AIDT) Dataset, where each triplet has three face images, with two images sharing the same pose and two sharing the same identity. Enhanced with a comprehensive occlusion augmentation, this dataset also improves robustness against occlusions. Additionally, we integrate 3D reconstruction techniques as input conditioning to our network for handling large pose variations. Extensive experiments demonstrate that our framework achieves superior performance in identity preservation, temporal consistency, and visual quality compared to existing methods, while requiring fewer inference steps. Our approach effectively mitigates key challenges in video face swapping, including temporal flickering, identity preservation, and robustness to occlusions and pose variations."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 9 7 2 1 1 . 2 1 4 2 : r VividFace: Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping Dailan He Hao Shao1,2 Shulun Wang2 Yang Zhou2 Guanglu Song2 Shuo Qin2 Zhuofan Zong1 Bingqi Ma2 Yu Liu2 (cid:66) Hongsheng Li1,3 (cid:66) 1CUHK MMLab 2SenseTime Research 3CPII under InnoHK Figure 1. Face swapping results of VividFace at 512 512 resolution. Our method produces high-fidelity and vivid outputs that accurately follow both pose and expression changes."
        },
        {
            "title": "Abstract",
            "content": "Video face swapping is becoming increasingly popular across various applications, yet existing methods primarily focus on static images and struggle with video face swapping because of temporal consistency and complex scenarios. In this paper, we present the first diffusion-based framework specifically designed for video face swapping. Our approach introduces novel image-video hybrid training framework that leverages both abundant static image data and temporal video sequences, addressing the inherent limitations of video-only training. The framework incorporates specially designed diffusion model coupled with VidFaceVAE that effectively processes both types of data to better maintain temporal coherence of the generated videos. To further disentangle identity and pose features, we construct the Attribute-Identity Disentanglement Triplet (AIDT) Dataset, where each triplet has three face images, with two images sharing the same pose and two sharing the same identity. Enhanced with comprehensive occlusion augmentation, this dataset also improves robustness against occlusions. Additionally, we integrate 3D reconstruction techniques as input conditioning to our network for handling large pose variations. Extensive experiments demonstrate that our framework achieves superior performance in identity preservation, temporal consistency, and visual quality compared to existing methods, while requiring fewer inference steps. Our approach effectively mitigates key challenges in video face swapping, including temporal flickering, identity preservation, and robustness to occlusions and pose variations. For more information, please refer to the project page. 1. Introduction In recent years, face swapping has emerged as crucial technology across various domains, from content cre1 ation [34] and privacy protection [47] to safe stunt scene production [36] and digital twin generation [33]. As video is predominant medium for communication, the demand for high-quality face swapping techniques has grown substantially. Video face swapping involves extracting identity features from source face and seamlessly integrating them with the attributes (such as expressions, poses, etc.) and background of target face while maintaining temporal consistency. However, despite the recent advancements, current face-swapping methods encounter difficulties in video contexts, as most are optimized for static images rather than dynamic video sequences. like DiffSwap [49]"
        },
        {
            "title": "Although recent works",
            "content": "Existing face swapping approaches can be broadly categorized into three main methodologies: 3D-based, GANbased, and diffusion-based methods. Traditional 3Dbased methods [3, 5, 31, 41], primarily utilizing 3D Morphable Models (3DMM) [4], often struggle with lowresolution outputs and face blending issues. GAN-based approaches [2, 9, 26, 28, 32, 50] encounter challenges with training instability, mode collapse, and producing lowresolution output, particularly in complex cases. Recently, diffusion models [20] have gained prominence in image synthesis tasks, offering advantages such as high-fidelity output, enhanced controllability, and high training stability. and REFace [1] have demonstrated the potential of diffusion models in image-level face swapping, significant challenges remain unaddressed, particularly for face swapping in videos. The video domain introduces additional complexities, including temporal consistency maintenance, handling large pose variations, and addressing occlusions. To tackle these challenges, we propose the first diffusion-based video face swapping framework VividFace that adopts novel image-video hybrid training strategy. This approach addresses the inherent limitations of video-only training, such as limited diversity of videos where hundreds of frames in single video tend to be highly similar. By incorporating abundant and readily accessible image data into the training process, our framework significantly enhances the diversity of training samples. Our framework incorporates specifically designed diffusion model that can process both static images and temporal video data, coupled with VidFaceVAE. Unlike the standard VAE [24], our VidFaceVAE is uniquely designed to process both face images and videos in unified embedding space and was trained with large-scale face dataset. It makes image data bypass motion-related layers while adaptively merging outputs from both image and video branches, effectively mitigating temporal flickering and text disorders commonly encountered in video face swapping. To enhance the identity similarity of swapped faces, we introduce the Attribute-Identity Disentanglement Triplet (AIDT) Dataset. Each data triplet consists of source face, target face, and GAN-generated decoupling face. The 2 source and target faces share the same identity but differ in pose and expression, while the GAN-generated face matches the target faces pose and expression yet features different identity. This dataset design improves the models ability to disentangle identity and pose features, enhancing both identity and attribute preservation. Additionally, the proposed occlusion data augmentation strategy adds various types of occluding objects to partially cover the faces with dynamic temporal patterns to the target images. To further address large pose variations, we incorporate 3D reconstruction of the target face as conditioning input, using 3D Morphable Model (3DMM). This 3D guidance helps the diffusion model accurately capture the pose and expression of the swapped face, promoting generalization across diverse videos. Experimental results demonstrate our frameworks superiority in terms of Frechet Video Distance (FVD), temporal consistency, and attribute/identity preservation, with fewer inference steps compared to existing methods. Besides, we also demonstrate the stability and generation of our method in multiple complex cases. To summarize, this paper makes the following contributions: We propose the first diffusion-based framework VividFace , specifically designed for video face swapping, featuring novel image-video hybrid training strategy that effectively leverages both static image and temporal video data. We introduce PIDT dataset construction method that enhances identity-expression disentanglement, along with comprehensive occlusion augmentation strategy that improves robustness to real-world scenarios. We effectively integrate 3D face reconstruction techniques and use the rendered output as additional conditioning information, enabling our framework to handle challenging conditions such as large pose variations while avoiding potential shortcuts in the generation process. Extensive experiments demonstrate our frameworks superior performance in terms of temporal consistency, identity preservation, and visual quality. Our ablation studies and component analysis provide valuable insights for future research in video face swapping. 2. Related Work 2.1. Face Swapping The frameworks of face swapping are generally categorized into three types: 3D-based [3, 5, 31, 41], GANbased [2, 9, 26, 28, 32, 50], and diffusion-based methods [1, 23, 49]. 3D-based frameworks typically employ the parameterized 3DMM [4] model to reconstruct the swapped face. Face2Face [41] transferred expressions from source to target face by fitting 3DMM face model to both faces. The authors in [31] show that face swapping with robust segmentation preserves identity in intra-subject swaps and reduces recognizability in inter-subject cases. HifiFace [45] additionally designed semantic facial fusion module for adaptive feature blending to make the results more photorealistic. However, these 3D-based methods exhibit low similarity with the source images and suffer from unrealistic texture and lighting due to the limited resolution of 3D face models. GAN [17] has been powerful tool for generating realistic synthetic images. The popular algorithm DeepFakes [11] utilizes an encoder-decoder architecture for identity-specific face swapping but lacks generalization. To improve adaptability, FSGAN [32] proposes subjectagnostic approach with recurrent reenactment module, inpainting and blending module. SimSwap [9] extends the flexibility with an ID Injection Module and Weak Feature Matching Loss, achieving high-fidelity results for arbitrary identities. FaceShifter [26] introduces two-stage framework with an Adaptive Embedding Integration Network and refinement module. However, GAN-based methods often struggle with balancing multiple loss functions and managing large shape differences or occlusions, leading to challenges in maintaining illumination consistency and identity fidelity in complex scenarios. Recently, diffusion models have become leading framework for image & video generation. Face swapping can also be formulated as an inpainting task, leveraging the identity and attribute features as condition vectors. DiffFace [23] first introduces diffusion-based framework with ID Conditional DDPM and target-preserving blending for stable, high-fidelity face swapping. DiffSwap [49] employs powerful diffusion model that follows the conditional inpainting paradigm to generate high-fidelity and controllable swapped faces. REFace [1] further advances this approach by reframing face-swapping as self-supervised inpainting task, improving identity transfer and fidelity with minimal inference time. However, these methods are limited to static images and do not address video face-swapping, which requires temporal consistency and more robust handling of large poses and occlusions. 2.2. Diffusion Models Diffusion models [20] have recently emerged as powerful generative framework, achieving state-of-the-art performance in various domains, including image synthesis [14, 20, 40], editing [22, 27, 39], super-resolution [16, 46], and video generation [6, 19, 30]. Unlike GANs, which often suffer training instability, diffusion models offer more stable training process by gradually denoising data from random noise, resulting in high-fidelity outputs. Notable advancements include Stable Diffusion [37], which enhances efficiency by operating in the latent space, and SVD [6], which incorporates temporal modules to scale diffusion models for video tasks. Conditioning mechanisms, such as cross-attention and concatenation, further improve controllability, enabling targeted generation across applications [42]. With these advantages, diffusion models have become increasingly popular for high-quality, versatile content creation. 3. Method In this section, we will introduce our method VividFace, the first diffusion model based video face swapping framework in detail. An overview of our framework is shown in Fig. 2. 3.1. Preliminaries Our method employs Stable Diffusion (SD) [37] as the backbone network. Stable Diffusion is text-to-image model built on the Latent Diffusion Model (LDM), which enables efficient image generation by operating within compressed latent space. SD uses variational autoencoder (VAE) [24] to map the original image x0 unto latent representation z0. reducing computational cost while preserving visual quality. The image is encoded as z0 = E(x0) and decoded back as x0 = D(z0). SD follows the Denoising Diffusion Probabilistic Model (DDPM) [20] framework, introducing Gaussian noise ϵ to the latent z0 across timesteps t, generating noisy latent zt over series of steps. During inference, the model denoises zt back to z0, guided by condition features. The denoising backbone ϵ0, based on U-Net [38], is trained to predict the noise and remove it progressively, using the objective: = Et,c,zt,ϵ (cid:2)ϵ ϵθ(zt, t, c)2(cid:3) , where represents text features derived from CLIP encoder [35]. SD uses U-Net with cross-attention mechanisms, to fuse text embeddings with latent features, enabling fine control over generated images based on text prompts. This allows SD to generate detailed, high-fidelity images while responding effectively to user input. 3.2. Hybrid Face Swapping Framework Video Face Swapping Task. The goal of video face swapping is to transfer the identity of source face onto target video while preserving the targets pose, expression, lighting, and background. Following the approach of DiffSwap [49], we model the video face swapping task as conditional inpainting. This involves masking the face region in the target frame and injecting conditioning vectors that represent the identity of the source face and the attributes of the target face to guide the generation process. Recent diffusion-based methods [1, 23, 49] have primarily focused on static image face swapping. However, directly applying these methods to video sequences by decomposing the video into multiple static images introduces new problems, such as temporal distortions, flickering, occlusions, and issues with large pose variations. 3 Figure 2. Overview of the proposed framework. During training, our framework randomly chooses static images or video sequences as the training data. In addition to the noise zt, three other types of inputs are integrated to guide the generation process: (1) face region mask, which controls the generation of facial imagery; (2) 3D reconstructed face, which helps guide the pose and expression, especially in cases of large pose variations; and (3) masked source images, which supply background information. These inputs are processed through the Backbone Network, which performs the denoising operation. Within the Backbone Network, we employ cross-attention and temporal attention mechanisms. The temporal attention module ensures temporal continuity and consistency across frames. Our face encoder extracts identity and texture features from the target face, as well as pose and expression details from the source face, and uses these features in cross-attention to produce realistic and high-fidelity results. To address these challenges, we propose an image-video hybrid framework, VividFace for video face swapping. Our framework also incorporates image-level training data to enhance face swapping performance. Specifically, we first train VidFaceVAE that can transform source images src RT 3HW into src R13HW or videos xv xi the latent space z0 RT CHW , where and indicate the spatial dimensions of the input data, is the count of generated frames, and is the feature dimension. In our framework, static images are treated as singleframe videos. We then train conditional diffusion model ϵθ(zt, t; C) to perform denoising on that latent space with specific emphasis on identity consistency, where denotes the conditioning vectors and denotes the denoising timestep. During training, as ground truth is unavailable when the source and target images come from different individuals, we utilize pairs of face images from the same individual as source-target training pairs. As illustrated in Figure 2, our framework randomly selects either static images or video sequences as training data. To synchronize gradients, each batch contains only one type of data. VidFaceVAE. As shown in Figure 3, our proposed VidFaceVAE is VAE framework designed to enhance the reconstruction quality of facial data, effectively handling both Figure 3. Overview of the proposed VidFaceVAE, capable of simultaneous encoding and decoding of both image and video data. Certain modules are specifically designed for video inputs, and image inputs bypass these modules as needed. video sequences and static images. The VidFaceVAE primarily consists of (2+1)D blocks, combining 2D spatial and 1D temporal convolutions to form pseudo-3D operators. For image inputs, the STFM (Spatial Temporal Fusion Module) outputs the result of the 2D ResBlock directly, bypassing the temporal ResBlock. For video inputs, the 4 Figure 4. Visualization of our occlusion data augmentation, which improves the stability and consistency of the generated videos. STFM combines the outputs from both the 2D and temporal blocks using learnable coefficient β, described as = β ospatial + (1 β) otemporal, where ospatial and otemporal denote the output from the spatial branch and the temporal branch. We do not involve the temporal downsampling modules in our VAE framework as it needs to process image data. The VidFaceVAE employs (2+1)D structure with two primary advantages: (1) It reduces the computational cost by decoupling spatial and temporal convolutions, making it more efficient than full 3D convolutions. (2) It enables the reuse of pretrained 2D VAE parameters and SD pretrained weights to accelerate convergence and improves the final performance. Unlike OD-VAE [8], we avoid the 3D-Causal-CNN, as temporal modules are skipped from images and our backbone network is not based on transformers. Causal convolution limits the models capacity and using casual conv processing static images can not bring performance improvement for both videos and images. src Temporal Modules. Inspired by the architectural concepts of AnimateDiff [19], EMO [42], and V-Express [44], we incorporate self-attention temporal layers into the features within frames. In our framework, temporal modules are exclusively applied to video sequences. For training with video inputs, we prepare motion frames xmotion RM 3HW , sampled from clips preceding the source videos, which are fed into the ReferenceNet to extract feature maps following each self-attention module. During the denoising process in the Backbone Network, we merge the temporal layer inputs with the pre-extracted motion features at matching resolutions along the frame dimension. The temporal dimension of these concatenated feature maps becomes + , enabling the application of temporal attention. This design enhances temporal coherence across frames. During training, we randomly initialize the motion frames as zero vectors to animate the generation of the first video clip. Figure 5. Visualization of our AIDT dataset. For video facial data, we present only the target and decoupling faces, as the source faces can be derived from any other frame within the same video clip. 3.3. Designs of Condition Vectors In our framework, several carefully designed condition vectors are used to guide the generation process, ensuring accurate and consistent visual outputs for both static images and video sequences. We formulate video face swapping as conditional inpainting task, where masked videos with cropped face regions provide the background and lighting conditions. The corresponding face regions guide the diffusion model on where are generated the faces. In many in-the-wild videos, faces often exhibit significant pose variations, which can lead diffusion models to produce suboptimal results, such as facial distortions and inaccurate pose estimations. To address this issue, we propose using 3D reconstruction technique to reconstruct the face and use its output as local guidance for pose and expression details. Specifically, we employ 3DMM [4] to extract BFM coefficients, setting the texture component to zero to reduce information leakage. Replacing the reconstructed face with the original face would introduce even more pronounced information leakage. Since the ground truth face is identical to the input target face, excessive information leakage would degrade the models generalization capabilities. To ensure that the generated face maintains the same identity as the source face while preserving attributes (such as pose, expression, etc.,), we inject crossattention features extracted by our face encoder as global context to the diffusion model. Face Encoder. The face encoder module in our framework plays critical role in extracting and integrating features from the target and source faces to guide the face-swapping process effectively. As illustrated in the right part of Figure 2, the face encoder is composed of three primary networks, each responsible for capturing distinct aspects of facial information: (1) identity net: this network focuses on extracting the core identity features from the target face; (2) texture net: this network is designed to capture detailed texture information from the target face, such as skin tone, fine facial features; (3) attribute net: the net extracts additional facial attributes from the source, such as pose, expression, and other dynamic features that contribute to realistic and expressive representation. The straightforward approach is to send the source image to both the identity and texture networks, while the target image is sent to the attribute network. However, challenge arises when the source and target faces do not belong to the same person, as the ground truth is unavailable in the real world. In most previous methods [1, 23, 49], the source and target images are assumed to be the same, meaning all three networks receive identical input. This results in difficulties for the face encoder in extracting distinct features and leading to information leakage. Specifically, this leakage causes the model to merely copy and paste the face region, effectively completing the task by superficially transferring facial features without meaningful feature disentangling or transformation. In contrast, our framework, built on the AIDT dataset (shown in Figure 5), employs source images (same identity, but different attributes) and decoupling images (same attribute, but different identity). These images help the face encoder disentangle and fuse different components of facial features, thus improving generalization when the source and target faces come from different individuals during inference. Details of the dataset construction can be found in Sec. 4. These extracted features are then passed through an attention mechanism within the Mixer module, where they are multiplied by corresponding weight coefficients before being fused. This process combines the identity, texture, and attribute features to create comprehensive cross-attention feature representation C. This fused representation provides rich contextual information to guide the diffusion model during face generation, ensuring both high fidelity and identity consistency in the swapped face across video frames. 3.4. Training Strategies Our training process involves three stages to progressively enhance model performance for video face swapping. The first stage focuses on training the VidFaceVAE , where we apply reconstruction, perceptual, and KL divergence losses to ensure high-quality reconstruction and well-structured latent space. The training data primarily consists of facial images and videos. Given the specifically designed architecture, the spatial modules are initialized using the original 2D VAE. In subsequent stages, the VAE is frozen and no longer updated. In the second stage, we pretrain the model using image data, while the ReferenceNet and temporal modules of the backbone network remain inactive. The backbone is initialized from the original SD weights. The final stage introduces image-video hybrid training, which incorporates temporal coherence by activating the temporal modules and utilizing video data. The temporal modules are initialized from AnimateDiff [19], enabling smooth frame transitions and reducing flicker artifacts. 4. AIDT Dataset In this section, we describe the construction of triplet pairs for our AIDT (Attribute-Identity Disentanglement Triplet) dataset, as illustrated in Figure 5. For image data, we first cluster the facial images based on identity similarity. From each cluster, we randomly select two images to form target-source pair that shares the same identity but has different attributes. To generate the decoupling image, which has different identity but the same attribute, we use the InsightFace Swapper to create synthetic images with distinct identity, while preserving the gender of the original face. We have observed that when the original and swapped faces belong to different genders, the results tend to degrade. Additionally, we exclude triplets with significant facial expression discrepancies by comparing the face landmarks. For video data, the process is similar, except that both the source and target images come from the same video clip, but not from the same frames as the target or motion images, which reduces the pose variation. Since video data is less abundant than image data, clustering does not yield enough pairs to form sufficient number of triplets. The AIDT dataset enables the face encoder to disentangle and fuse distinct facial componentsID features, texture features from the source face, and attribute features from the decoupling face. This enhances generalization, especially when the source and target faces belong to different individuals during inference. 5. Experiment 5.1. Implementation Details We collected approximately 300 hours of facial videos from the internet to train our models, and the facial images are partially sourced from VGGFace2-HQ [10]. In our experiments, we use latent space of size 13 64 64 and U-Net architecture for the ϵ0 denoising network. Images and video clips sampled from the dataset are resized and cropped to 512 512. The number of motion frames, , is set to 4, and the generated video length, , is set to 8 frames. For the face encoder, the identity network is based on ArcFace [12], while the texture and attribute networks are based on DINO [7]. We use SCRFD [18] for facial bounding box detection. The AdamW optimizer [29] is used for training. In the first stage of the VAE training, the learning rate is set to 5e-6 with batch size of 32. The weights of reconstruction, perceptual, and KL divergence loss are 1.0, 0.1, 1e-6 respectively. For the second and third stages, the learning rate is increased to 1e-5, with the batch size remaining at 32. During inference, we generate video clips using the DDIM sampling algorithm for 32 steps. 6 Figure 6. Qualitative comparison at 512 512 resolution. Our method generates high-fidelity results and handles challenging cases effectively, such as large poses (b) and occlusions (c). Corresponding videos are provided in the supplementary material.It is best viewed at larger scale for optimal evaluation. 5.2. Evaluation Protocol Considering that most previous baselines, such as CelebA [25] and FFHQ [21], are primarily focused on image face swapping, we propose new benchmark for video face swapping. Our benchmark includes 200 source images and 200 high-resolution target videos, with each video containing 128 frames and single trackable face. These videos and images feature unseen identities and backgrounds, ensuring diverse and challenging dataset. To evaluate performance, we generate 200 swapped videos using our framework. For comparison, since other methods are based on image-level face swapping, we perform face swapping frame by frame for those methods. For facial data reconstruction, we use SSIM, PSNR and LPIPS [48] to evaluate the quality of reconstructed images and videos. For video face swapping, we use FVD [43] to assess the overall quality of the generated videos. The attribute transfer error is measured by pose and expression errors. We use HopeNet [15] and Deep3DFaceRecon [13] to detect these attributes, and the L2 distance to the ground truth is used as the evaluation metric. For ID retrieval, we extract identity features from the source images using ArcFace, and for each swapped video, we perform face retrieval by searching for the most similar faces among all source images. The retrieval is measured by the average cosine similarity of all frames, and we report the Top-1 and Top-5 accuracy. Method FVD32 FVD128 ID retrieval Top-1 Top-5 Pose Expr. SimSwap [9] 1242.8 FSGAN [32] 1507.9 2404.7 DiffFace [23] DiffSwap [49] 1530.2 1336.9 REFace [1] 186.6 423.8 1404.9 809.3 311.9 VividFace 1201.1 122.6 76.5 24.5 1.5 14.5 71.9 78. 88.5 40.0 4.1 26.3 86.5 90.2 5.12 5.19 18.3 12.9 6.67 5.43 0.76 0.73 1.58 1.02 0.91 0. Table 1. Qualitative Comparison. Best is in bold and second best is underlined. our method achieves very competitive results compared with existing methods. Figure 7. Ablation on the different combinations of texture weights and attribute weights. 5.3. Comparisons with Existing Methods Qualitative Results. Since videos cannot be displayed in the PDF and due to submission policy restrictions on showing generated videos, we provide several comparison videos in the supplementary materials and strongly encourage the reader to view them. We perform quantitative comparison at 512 512 resolution. As shown in Figure 6 (a) and (d), our method generates high-fidelity swapped faces, with attributes that closely match the target faces. In Figure 6 (b), our method successfully transfers both face shape and expression under large pose variations, benefiting from the 3D reconstruction mask, while other methods exhibit generation artifacts. In Figure 6 (c), where toy and hand occlude the girls face, most other methods fail to handle the occlusion properly, with the toy and hand either displaced or fused together. Additionally, many methods result in noticeable facial deformations. In contrast, our method successfully recovers the occluded areas and maintains accurate face swapping, thanks to our augmentation strategy. Quantitative Results. In Table 1, we compare five open-source methods (two GAN-based and three diffusionbased). The results show that our method outperforms others in ID retrieval and FVD, generating high-fidelity swapped face videos while preserving the source identity. It also achieves comparable performance in pose and expression, maintaining target attributes effectively."
        },
        {
            "title": "Architecture\nEncoder Decoder",
            "content": "2D 2D (2+1)D 2D (2+1)D (2+1)D Facial videos SSIM PSNR LPIPS 37.61 0.967 38.77 0.976 41.11 0.983 0.048 0.039 0.027 Table 2. Comparison of different VAE architectures. Figure 8. Ablation on the occlusion data augmentation and 3D face reconstruction. 5.4. Analysis VAE architecture. Table 2 shows the reconstruction performance of different VAE architectures. The first model is pure 2D VAE (SD-VAE), the second uses (2+1)D decoder with 2D encoder, and the third is our proposed VidFaceVAE , which uses full (2+1)D encoder-decoder. The VidFaceVAE outperforms the others in all metrics, achieving the highest SSIM (0.983), PSNR (41.11), and the lowest LPIPS (0.027), indicating superior reconstruction quality for facial videos. This shows that incorporating both spatial and temporal processing leads to better results compared to 2D-only approaches. Face feature Mixing. In our experiment, the identity, texture, and attribute weights in the face encoder are set to 1.0, 0.6, and 0.6, respectively. In Figure 7, we demonstrate the effects of varying texture and attribute weights. As the texture weight increases, we observe an improvement in identity similarity. However, if the texture weight continues to 8 increase, there is loss in the preservation of the targets attributes (pose and expression). In the extreme case (attribute weight = 1.0, texture weight = 0.2), we find that the model almost simply pastes the source face onto the result.As the attribute weight increases, the targets attributes are better preserved, but identity similarity decreases. Ablation study. In the upper part of Figure 8, we present the results under occlusion conditions. Without the occlusion augmentation, we observe significant distortion of the face, and the occluder (e.g., hand) is either severely deformed or disappears entirely. After applying occlusion data augmentation, the output quality improves dramatically. In the lower part of Figure 8, we show results for large pose variations. Without the additional guidance from 3D face reconstruction in the denoising network, the generated face becomes highly unstable, with noticeable distortion and deformation. 6. Conclusion In this paper, we introduced novel diffusion-based framework for video face swapping, addressing key challenges such as temporal consistency, identity preservation, and handling large pose variations. Our image-video hybrid training strategy leverages both static images and video data, The improving model diversity and robustness. VidFaceVAE , coupled with custom Attribute-Identity Disentanglement Triplet (AIDT) dataset and 3D Morphable Model integration, enables accurate face swapping while mitigating issues like flickering and occlusions. Experimental results demonstrate that our framework outperforms existing methods in terms of FVD, temporal consistency, and identity preservation, while requiring fewer inference steps. Overall, our approach provides more efficient and effective solution for high-quality video face swapping and sets the stage for future advancements in the field."
        },
        {
            "title": "References",
            "content": "[1] Sanoojan Baliah, Qinliang Lin, Shengcai Liao, Xiaodan Liang, and Muhammad Haris Khan. Realistic and efficient face swapping: unified approach with diffusion models. arXiv preprint arXiv:2409.07269, 2024. 2, 3, 6, 8 [2] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Towards open-set identity preserving face synthesis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 67136722, 2018. 2 [3] Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Belhumeur, and Shree Nayar. Face swapping: automatically replacing faces in photographs. In ACM SIGGRAPH 2008 papers, pages 18. 2008. 2 [4] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 157164. 2023. 2, 5 [5] Volker Blanz, Kristina Scherbaum, Thomas Vetter, and In ComHans-Peter Seidel. Exchanging faces in images. puter Graphics Forum, pages 669676. Wiley Online Library, 2004. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [8] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for imarXiv preprint proving latent video diffusion model. arXiv:2409.01199, 2024. 5 [9] Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. Simswap: An efficient framework for high fidelity face In Proceedings of the 28th ACM international swapping. conference on multimedia, pages 20032011, 2020. 2, 3, 8 [10] Xuanhong Chen, Bingbing Ni, Yutian Liu, Naiyuan Liu, Zhilin Zeng, and Hang Wang. Simswap++: Towards faster and high-quality identity swapping. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 6 [11] DeepFakes. faceswap. https : / / github . com / deepfakes/faceswap, 2020. Accessed: 2024-11-01. 3 [12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 6 [13] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In IEEE Computer Vision and Pattern Recognition Workshops, 2019. 7 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [15] Bardia Doosti, Shujon Naha, Majid Mirbagheri, and David Crandall. Hope-net: graph-based model for hand-object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6608 6617, 2020. [16] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and Baochang Zhang. Implicit diffusion models for continuous super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1002110030, 2023. 3 [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 3 [18] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for effi9 cient face detection. arXiv preprint arXiv:2105.04714, 2021. 6 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3, 5, [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [21] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 7 [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models, 2023. 3 [23] Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, and KwangHee Lee. Diffface: Diffusion-based face swapping with facial guidance. arXiv preprint arXiv:2212.13344, 2022. 2, 3, 6, 8 [24] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [25] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55495558, 2020. 7 [26] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019. 2, 3 [27] Haonan Lin, Mengmeng Wang, Jiahao Wang, Wenbin An, Yan Chen, Yong Liu, Feng Tian, Guang Dai, Jingdong Wang, and Qianying Wang. Schedule your edit: simple yet effective diffusion noise schedule for image editing. arXiv preprint arXiv:2410.18756, 2024. [28] Zhian Liu, Maomao Li, Yong Zhang, Cairong Wang, Qi Zhang, Jue Wang, and Yongwei Nie. Fine-grained face In Proceedings of swapping via regional gan inversion. the IEEE/CVF conference on computer vision and pattern recognition, pages 85788587, 2023. 2 [29] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [30] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [31] Yuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, and Gerard Medioni. On face segmentation, face swapping, and face perception. In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), pages 98105. IEEE, 2018. 2 [32] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71847193, 2019. 2, 3, [33] Rogue One. Rogue one: star wars story. Genre, 14, 2016. 2 [34] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Ume, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, et al. Deepfacelab: Integrated, arXiv flexible and extensible face-swapping framework. preprint arXiv:2005.05535, 2020. 2 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [36] Yurui Ren, Ge Li, Yuanqi Chen, Thomas Li, and Shan Liu. Pirenderer: Controllable portrait image generation via semantic neural rendering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13759 13768, 2021. 2 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [39] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. 3 [40] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [41] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. Face2face: Real-time In Proceedface capture and reenactment of rgb videos. ings of the IEEE conference on computer vision and pattern recognition, pages 23872395, 2016. [42] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. 3, 5 [43] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 7 [44] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 5 [45] Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang, Jilin Li, Yongjian Wu, Feiyue Huang, 3d shape and semantic and Rongrong Ji. Hififace: prior guided high fidelity face swapping. arXiv preprint arXiv:2106.09965, 2021. 3 [46] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex Kot, and Bihan Wen. Sinsr: diffusion-based image superresolution in single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2579625805, 2024. 3 [47] Yifan Wu, Fan Yang, Yong Xu, and Haibin Ling. Privacyprotective-gan for privacy preserving face de-identification. Journal of Computer Science and Technology, 34:4760, 2019. 2 [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [49] Wenliang Zhao, Yongming Rao, Weikang Shi, Zuyan Liu, Jie Zhou, and Jiwen Lu. Diffswap: High-fidelity and controllable face swapping via 3d-aware masked diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85688577, 2023. 2, 3, 6, 8 [50] Yuhao Zhu, Qi Li, Jian Wang, Cheng-Zhong Xu, and Zhenan Sun. One shot face swapping on megapixels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 48344844, 2021."
        }
    ],
    "affiliations": [
        "CPII under InnoHK",
        "CUHK MMLab",
        "SenseTime Research"
    ]
}