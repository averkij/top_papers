{
    "paper_title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
    "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Ziyi Gao",
        "Xiaotong Lin",
        "Yun Liu",
        "Xing Fu",
        "Yu Cheng",
        "Yongchao Liu",
        "Weiqiang Wang",
        "Zhongle Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 2 9 4 4 1 . 2 0 6 2 : r Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Jiahao Yuan1,, Yike Xu1,, Jinyong Wen1, Baokun Wang1,, Ziyi Gao1, Xiaotong Lin1 Yun Liu1, Xing Fu1, Yu Cheng1, Yongchao Liu1, Weiqiang Wang1, Zhongle Xie2 1Ant Group, 2Zhejiang University China Abstract Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, framework shifting user modeling from static encoding to dynamic, queryaware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrialscale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipays production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor. CCS Concepts Information systems Collaborative filtering. Keywords User representation modeling, User Embedding, Large language model *Both authors contributed equally to this research. Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Jiahao Yuan1,, Yike Xu1,, Jinyong Wen1, Baokun Wang1,, Ziyi Gao1, Xiaotong Lin1 and Yun Liu1, Xing Fu1, Yu Cheng1, Yongchao Liu1, Weiqiang Wang1, Zhongle Xie2. 2018. Query as Anchor: Scenario-Adaptive User Representation via Large Language Model. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 15 pages. https: //doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nUser representation learning underpins modern industrial intel-\nligence systems by enabling personalized, data-driven decision\nmaking across applications such as recommendation [5], digital\nmarketing [31], and risk management [7]. In practice, users fre-\nquently engage in multiple business scenarios, each characterized\nby distinct behavioral patterns and decision objectives, which calls\nfor representations that are both transferable across tasks and adapt-\nable to scenario-specific contexts. Accordingly, existing systems\ntypically compress large-scale, heterogeneous user signals, includ-\ning textual profiles, interaction histories, and structured attributes,\ninto compact embeddings to support downstream modeling and\ninference.",
            "content": "Most existing user embedding methods learn static, task-specific representations, either trained from scratch [8] or based on pretrained language model (PLM) encoders optimized with contrastive objectives [22, 25]. While effective at capturing historical behavioral patterns, these approaches are inherently retrospective and lack the flexibility to adapt to diverse downstream decision scenarios. As result, industrial systems often rely on multiple task-specific user models for different applications, increasing system complexity, deployment overhead, and long-term maintenance cost, while still struggling with cross-domain generalization [15]. Recent research has explored large language models (LLMs) as user encoders, enhancing sequential and semantic understanding through fine-tuning on behavior sequences [20, 25] or by enriching user and item semantics [12]. However, real-world user behaviors are typically sparse, symbolic, and highly heterogeneous, which diverges significantly from the dense, language-centric data used in LLM pretraining [1, 13, 32]. This modality and semantic gap limits the ability of LLMs to generate adaptive and anticipatory user representations, especially in specialized domains where behavior distributions differ substantially from pretraining corpora. While instruction-aware embeddings [32] offer limited task conditioning, they remain insufficient for capturing the complex, dynamic, and noisy nature of industrial user data. Overall, existing user representation learning methods face three key challenges: (1) Limited scenario adaptability and cross-task generalization: Static embeddings cannot flexibly support diverse Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. downstream tasks such as credit assessment, marketing optimization, and risk management, each of which requires scenario-specific user understanding [26, 33]. (2) Modality and semantic gap: The symbolic sparsity and heterogeneous structure of behavioral logs are misaligned with language-centric pretraining, constraining LLM-based user representations from effectively generalizing across domains. (3) Heterogeneous data integration at scale: Industrial user data varies significantly in relevance across scenarios, requiring mechanisms to selectively attend to scenario-relevant signals, suppress noise, and compress large-scale behaviors without introducing prohibitive inference overhead. To address these challenges, we propose unified user representation learning framework that integrates industrial-scale pretraining with efficient scenario adaptation. We introduce Queryas-Anchor, query-conditioned mechanism that separates user behavior encoding from scenario-specific objectives. Our key contributions are summarized as follows: We construct UserU, an industrial-scale pretraining dataset that aligns heterogeneous user behaviors with user understanding semantics via future behavior prediction and QAbased supervision, providing strong behavioral and semantic priors for user embedding learning. We propose Query-as-Anchor, query-conditioned framework that generates scenario-adaptive user embeddings by re-anchoring the same behavioral profile under different downstream decision contexts, enabling reuse across multiple business scenarios. We introduce soft-prompt tuning and KV-cacheaware acceleration, enabling efficient scenario specialization and low-latency multi-scenario inference without retraining the backbone model. Extensive offline experiments and large-scale online A/B testing on Alipay demonstrate consistent performance gains across user engagement, risk control, and marketing scenarios, while significantly reducing system complexity and deployment overhead."
        },
        {
            "title": "2 Related Work",
            "content": "LLM for User Embedding. Large language models (LLMs) have demonstrated significant potential in user representation learning by integrating diverse data types, such as textual profiles, behavioral sequences, and structured attributes, into unified embeddings. Early works like BERT4Rec [25] and FOUND [7] fine-tuned LMs [23] on behavioral data to capture temporal dependencies via masked prediction and contrastive objectives [9], but they rely on static user embeddings [18], which limits adaptability to dynamic or evolving contexts. More recent instruction-aware models, such as Qwen3embedding [32] and KaLM-Embedding [13], have extended LLMs for task-specific embeddings [2, 10]. However, these models still face challenges when transitioning from general-purpose pretraining on large text corpora to sparse, symbolic, and highly contextual user behavior data, creating gap in both structure and semantics. In contrast, our Query as Anchor framework bridges this gap by introducing dynamic, query-aware embeddings that adapt to the evolving nature of user behaviors, improving adaptability and contextual sensitivity across diverse tasks and domains. Figure 1: Comparison between (A) General User Embedding [7] and (B) our Query-as-anchor. (A) learns transferable user representations across domains but generates fixed embeddings regardless of downstream context. (B) extends (A) with query-as-anchor modulation, enabling single model to produce adaptive, domain-specific embeddings via natural language instructions. Synthetic Data for User Embedding. Despite advancements in user representation learning, significant gap remains due to the lack of large-scale, comprehensive dataset specifically designed for user embedding pretraining. This limitation has spurred interest in generating synthetic data to supplement the training process. Early methods primarily focused on heuristic data augmentation and pseudo-labeling approaches [21, 29], which aimed to simulate user behaviors and interactions. More recently, large language models (LLMs) have been employed to generate realistic user behavior, intent patterns, and interaction sequences [7], offering scalable and diverse data generation capabilities. However, these techniques still face challenges in capturing the full complexity of user behaviors. The absence of dedicated pretraining dataset for user embeddings remains critical obstacle. To overcome this, we introduce UseU, large-scale dataset designed specifically for user embedding pretraining. By incorporating rule-based future behavior prediction and QA-understanding tasks, UseU enables more effective, contextaware learning of user representations."
        },
        {
            "title": "3.1 UserU Pretraining Dataset\nTo enhance user embedding performance across diverse tasks, we\nintroduce the User Understanding (UserU) Pretraining Dataset,\nwhich integrates dynamic, context-aware user behavior with task",
            "content": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 2: Overview of Query-as-Anchor Framework for our Q-Anchor Embedding. adaptability for real-world applications. Extending future prediction pretraining [7], we augment it with synthesized query-answer pairs to capture deeper user understanding. UserU combines two key components: behavior prediction dataset for future user actions ğ· ğ‘“ ğ‘¢ğ‘¡ğ‘¢ğ‘Ÿğ‘’ and LLM-generated UserQA dataset ğ·ğ‘¢ğ‘ğ‘ for comprehensive user representation via query-answer synthesis. Both data examples are detailed in Appendix A. Behavior-based Interaction Dataset Dğ‘“ ğ‘¢ğ‘¡ğ‘¢ğ‘Ÿğ‘’ . We construct behaviorgrounded supervision signal by pairing each users historical profile with future-action summary in unified queryanswer format (Appendix A). Concretely, for each user ğ‘–, we build three-month behavior profile uğ‘– from multi-modal logs. We then derive target ğ‘future by aggregating subsequent interactions into temporal bins and action categories, and selecting the frequencyand diversity-aware actions as the representative subset. Using fixed template query ğ‘future (e.g., What are the users most likely actions in the next period?), we form training pairs Dğ‘“ ğ‘¢ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = {(uğ‘– ğ‘future, ğ‘future ğ‘–=1, where denotes contextual concatenation. This dataset encourages the embedding to capture temporal regularities and encode predictive signals for near-future behaviors. )}ğ‘ ğ‘– ğ‘– Synthetic Query-Answer Dataset Dğ‘¢ğ‘ğ‘. To address the scarcity of user understanding training data and ensure better decoupling between pretraining data and downstream tasks for improved generalization, we propose self-reflect synthetic data generation approach to construct general-purpose user-query-answer alignment dataset. Inspired by the cold-start strategy in [6], we first initialize seed pool comprising 72 life-related user-understanding topics (e.g., financial planning, health management) via Qwen-Max. Given user profile uğ‘– , we prompt an LLM to retrieve the top-10 most relevant topics from P, and instantiate each topic into naturalistic query ğ‘ğ‘– grounded in uğ‘– . Conditioned on (uğ‘–, ğ‘ğ‘– ), the LLM then generates an answer ğ‘ğ‘– . To improve faithfulness and contextual validity, we apply post-generation reflection step in which rechecks the draft answer against uğ‘– and revises unsupported or inconsistent statements. The resulting dataset is Dğ‘¢ğ‘ğ‘ = {(uğ‘– ğ‘ğ‘–, ğ‘ğ‘– )}ğ‘€ ğ‘–=1, where denotes contextual concatenation."
        },
        {
            "title": "3.2 Query as Anchor\n3.2.1 Hierarchical Coarse-to-fine User Encoder. To effectively rec-\noncile the inherent sparsity of multi-source behavioral signals with\nthe dense semantic requirements of Large Language Models (LLMs),\nwe propose a hierarchical encoding architecture that distills raw\ninteractions into a multi-granularity representation space. Specifi-\ncally, for each modality ğ‘š âˆˆ M (where M = {ğµğ‘–ğ‘™ğ‘™, ğ‘€ğ‘–ğ‘›ğ‘–, ğ‘†ğ‘ƒğ‘€, ğ´ğ‘ğ‘,\nğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„,ğ‘‡ ğ‘ğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿ }), raw event sequences are first projected into ini-\ntial embeddings {hğ‘š,ğ‘¡ } via encoder and subsequently refined by\nmodality-specific event adapters:",
            "content": "ğ‘š,ğ‘¡ = MLP(ğ‘’ğ‘£ğ‘¡ ) z(ğ‘’ğ‘£ğ‘¡ ) ğ‘š (cid:0)LayerNorm(hğ‘š,ğ‘¡ )(cid:1) , (1) thereby preserving fine-grained atomic features of individual actions. To capture intra-modality trends while mitigating idiosyncratic noise, these event-level embeddings are aggregated through mean-pooling into summary vector z(ğ‘’ğ‘£ğ‘¡ ) ğ‘š , which is further transformed by shared modal adapter to yield unified modality embedding z(ğ‘šğ‘‘ğ‘™ ) = MLP(ğ‘šğ‘‘ğ‘™ ) (LayerNorm(z(ğ‘’ğ‘£ğ‘¡ ) ğ‘š )). At the apex of this hierarchy, global user-level representation z(ğ‘¢ğ‘ ğ‘Ÿ ) is derived by consolidating all modality-specific vectors through dedicated user adapter, capturing the holistic behavioral profile. The final comprehensive input tokens eğ‘– is constructed via the structured concatenation of representations across all three levels: ğ‘š (cid:104) eğ‘– = z(usr) ; {z(mdl) ğ‘š }ğ‘š ; {z(evt) ğ‘š }ğ¾ğ‘š ğ‘š (cid:105) , (2) where [; ] denotes the concatenation operation and ğ¾ğ‘š is the number of event tokens retained for modality ğ‘š. This hierarchical design lets the LLM attend to either fine-grained events or high-level behavior summaries conditioned on the query, while remaining compatible with its native embedding space. Conference acronym XX, June 0305, 2018, Woodstock, NY"
        },
        {
            "title": "Input Template of UserU",
            "content": "The following are heterogeneous user data from multiple sources, including PayBill transactions, Mini Program interaction logs, Super Position Model paths, App list, homepage search queries, and structured tabular features: Hierarchical User Tokens: eğ‘– Query: { User query or directive } <USER_EMB> Figure 3: Input Template of UserU. The Hierarchical User Tokens: eğ‘– injects the precomputed hierarchical embedding eğ‘– (Eq. 2). An optional instruction is followed by the special <USER_EMB> token, which signals the model to extract unified user embedding (Sec. 3.2.2)."
        },
        {
            "title": "3.2.2 Q-Anchor Pretraining Architecture.",
            "content": "Query-as-Anchor Dual-Tower Architecture. Building upon the hierarchical user representations, we propose dual-tower training architecture that operationalizes the Query-as-Anchor paradigm by aligning multi-modal behavioral signals with semantic task directives as illustrated in Fig. 2. The primary Anchor Tower ingests the hierarchical user tokens eğ‘– and appends the natural language query ğ‘ğ‘– as trailing semantic anchor. By positioning the query at the sequence terminus, the LLM backbone acts as query-aware aggregator that selectively distills intent-relevant features from the latent space of eğ‘– , ultimately projecting them into the anchored representation uğ‘–,ğ‘ = LLMğ‘ğ‘›ğ‘ (eğ‘–, ğ‘ğ‘– ). This structural decoupling is specifically engineered for industrial scalability; the computationally intensive hierarchical prefix eğ‘– is computed once and stored via KV-cache mechanisms, allowing the model to generate diverse, scenario-adaptive embeddings for multiple queries with negligible incremental latency. Concurrently, an asymmetric Semantic Tower projects the target answer ğ‘ğ‘– into dense vector vğ‘ğ‘– = LLMğ‘ ğ‘’ğ‘š (ğ‘ğ‘– ), providing high-fidelity linguistic target that serves as the groundtruth for user behavior synthesis and intent modeling. Specially, both towers share the same LLM parameters, which ensures that the behavioral features and semantic labels are mapped into unified latent space. Joint ContrastiveGenerative Optimization. Our Query-as-Anchor framework is trained with joint objective that combines discriminative contrastive alignment with generative grounding to produce user embeddings that are both distinctive and semantically rich. For each user ğ‘–, the anchor tower generates query-anchored embedding uğ‘–,ğ‘ from the hierarchical profile eğ‘– and the task-specific query ğ‘ğ‘– , while the semantic tower encodes the answer ğ‘ğ‘– into vğ‘ğ‘– . Contrastive alignment is implemented via query-conditioned InfoNCE loss: Lğ‘ğ‘™ = 1 ğµ ğµ ğ‘–=1 log exp(sim(uğ‘–,ğ‘, vğ‘ğ‘– )/ğœ ) ğ‘ğ‘– , Yuan et al. (3) ğ‘ğ‘– = exp(sim(uğ‘–,ğ‘, vğ‘ğ‘– )/ğœ ) + ğ‘šğ‘– ğ‘— exp(sim(uğ‘–,ğ‘, vğ‘ ğ‘— )/ğœ ) ğ‘— ğ‘– + ğ‘— ğ‘– ğ‘šğ‘– ğ‘— exp(sim(uğ‘–,ğ‘, uğ‘—,ğ‘ )/ğœ ) + ğ‘— ğ‘– ğ‘šğ‘– ğ‘— exp(sim(vğ‘ğ‘– , vğ‘ ğ‘— )/ğœ ), (4) where sim() is cosine similarity, ğœ is temperature, ğµ is the batch size, and ğ‘šğ‘– ğ‘— is margin-based mask that removes potential false negatives inspired by [32]. Specifically, we discard candidate negative ğ‘— for anchor ğ‘– if either its user embedding or its answer embedding is too similar to uğ‘–,ğ‘: ğ‘šğ‘– ğ‘— = (cid:40)0, 1, if ğ‘¥ {uğ‘—,ğ‘, vğ‘— } s.t. sim(uğ‘–,ğ‘, ğ‘¥ ) > sim(uğ‘–,ğ‘, vğ‘– ) + ğ‘margin, otherwise. (5) To bridge the granularity gap between sentence-level alignment and token-level grounding [16], we propose joint optimization objective that mitigates representation collapse while enhancing semantic density. While the contrastive loss Lğ‘ğ‘™ enforces global discriminativeness by aligning positive pairs in the latent space, it often overlooks the fine-grained linguistic nuances essential for complex intent modeling. To counteract this, we introduce an auxiliary Next-Token Prediction (NTP) task that requires the anchor tower to autoregressively reconstruct the target answer ğ‘ğ‘– : Lğ‘›ğ‘¡ğ‘ = ğ‘‡ ğ‘¡ =1 log ğ‘ƒ (ğ‘¦ğ‘¡ ğ‘¦<ğ‘¡ , ğ‘’ğ‘–, ğ‘ğ‘– ), (6) where ğ‘‡ is the sequence length. The total objective is weighted sum Lğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = Lğ‘ğ‘™ + Lğ‘›ğ‘¡ğ‘ , enabling the query to anchor the hierarchical user profile eğ‘– into compact, scenario-adaptive embedding that is both discriminative for downstream tasks and semantically rich for answer reconstruction. Soft Prompt Tuning. To further bridge the semantic gap be3.2.3 tween general-purpose user understanding and specialized downstream business logic, we introduce cluster-based soft prompt tuning mechanism as post-training adaption inspired by [19, 28]. As illustrated in Fig. 2 (C), while the LLM backbone and the coarseto-fine user encoder remain frozen to preserve the foundational multi-modal alignment, we introduce set of learnable prompt tokens that function as differentiable task controllers. These tokens modulate the latent space of the hierarchical embeddings uğ‘–,ğ‘ to better align with downstream class-specific logic. We optimize these learnable tokens and set of class prototypes {pğ‘˜ }ğ¾ using prototypical contrastive loss Lğ‘ğ‘¡ , which pulls user embeddings toward their respective category centers while pushing them away from irrelevant clusters: ğ‘˜=1 Lğ‘ğ‘¡ = 1 ğµ ğµ ğ‘–=1 log exp (cid:19) (cid:18) ğ‘–,ğ‘ pğ‘¦ğ‘– ğœ (cid:205)ğ¾ ğ‘˜=1 exp (cid:18) ğ‘–,ğ‘ pğ‘˜ ğœ , (cid:19) (7) where uğ‘–,ğ‘ denotes the prompt-conditioned user embedding for sample ğ‘–, ğ‘¦ğ‘– is its ground-truth label, and pğ‘˜ represents the learnable prototype (center) for class ğ‘˜. By maximizing the similarity between uğ‘–,ğ‘ and its corresponding class center pğ‘¦ğ‘– relative to all ğ¾ prototypes, the framework enforces discriminative clustering structure Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY Pretraining scalability (data/model scale) and prompt tuning scalability (training steps/prompt tokens) are detailed in Appendix C.2 and C.3, respectively. Baselines and Tasks. We compare against two categories of baselines. (1) General text embeddings encode natural language descriptions of user behaviors, including Qwen2.5-0.5B-Instruct without user-specific fine-tuning and several top-ranked embedding models on MTEB including KaLM-Embedding-Gemma3-12B-2511 [13], llama-embed-nemotron-8b [1], Qwen3-Embedding-8B [32]. (2) User representation models include contrastive approaches such as MSDP [8], One4all [24] and CPC [22], as well as LLM-based foundation models FOUND [7]. To study scalability, we also evaluate larger variants of our architecture via under the same training protocol. We evaluate all methods on 10 real-world binary classification tasks from Alipays production systems, grouped into three domains  (Table 1)  . Detailed positive and negative label definitions for each task are provided in Appendix  (Table 4)  . Table 1: Data information for user pretraining and test benchmarks, with number of tests per task. Dataset Domain Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› General (3.1) â¶ User Engagement Dğ‘¡ğ‘’ğ‘ ğ‘¡ â· Risk â¸ Marketing Sensitivity Scenario General Interest Community Active User Identification (Active), Concert Click Prediction (Concert), User Log-in Prediction (Login), Ant Forest Engagement (Forest) Fraud Detection (Fraud), Money Laundering Detection (Money) Takeout Interest (Takeout), Brand Sensitivity (Brand), Big Sale Sensitivity (Promo), Cost-Performance Sensitivity (Value) Number 1.024108 50w per task 50w per task 50w per task Evaluation Metrics. Following [7], we assess representation via linear probing on 10 binary classification tasks from Alipays user cognition system. We report AUC (Area Under the ROC Curve [4]) for discriminative performance and KS (Kolmogorov-Smirnov [3]) for critical decision-boundary separation."
        },
        {
            "title": "5 Main Results\n5.1 In-depth Performance Analysis",
            "content": "Q-Anchor delivers the strongest and most stable AUC and KS across all scenarios. As shown in Table 2 and Fig. 5, Q-Anchor (Prompt Tuned) achieves the best performance across all 10 benchmarks, with an average AUC of 0.8225 and KS of 0.5267. It surpasses the strongest general-purpose baseline, Llama-Embed-Nemotron-8B (AUC: 0.7488, KS: 0.3805), by +0.0737 (+9.84%) in AUC and +0.1462 (+38.4%) in KS, consistently across User Engagement, Risk, and Marketing domains (see Appendix C.1). These results indicate that the key limitation in industrial user modeling lies not in semantic capacity, but in representation alignment: generic text embeddings struggle to reconcile sparse, symbolic, multi-source behavioral logs. By contrast, Q-Anchor, pretrained on UserU with hierarchical behavior encoding, maps heterogeneous events into query-relevant signals without relying on massive parameterization. Figure 4: KV-Cache optimized Query-as-Anchor inference: pre-computed user prefixes enable sequence, low-latency reanchoring across diverse tasks. in the latent space. This strategy enables the model to bridge the semantic gap between general pretraining and specialized business labelssuch as high-risk vs. low-risk userswithout the catastrophic forgetting associated with full parameter fine-tuning, thereby maintaining the structural integrity and deployment efficiency of the Query-as-Anchor paradigm. 3.2.4 Query-as-Anchor: Accelerating Multi-Scenario Inference. To meet the demands of industrial deployment, Q-Anchor adopts KV-cache optimization that decouples user encoding from task querying. As shown in Fig. 4, for given user ğ‘–, the hierarchical profile eğ‘– is encoded once to produce shared-context KV cache. This cache is kept fixed as persistent semantic prefix during inference. Given set of downstream queries {ğ‘1, . . . , ğ‘ğ‘› }, we process them sequentially while reusing the same shared-context cache. For each query ğ‘ ğ‘— , the model only computes the incremental hidden states for the short query tokens, resulting in an amortized complexity of ğ‘‚ (ğ¿ğ‘ ğ‘— ) per task (and ğ‘‚ ((cid:205)ğ‘› ğ‘—=1 ğ¿ğ‘ ğ‘— ) for all queries). This design enables single comprehensive user representation to be efficiently re-anchored to many business scenarios with negligible per-scenario incremental latency, supporting high-throughput embedding generation in the Alipay ecosystem. Deployment details are provided in Appendix E."
        },
        {
            "title": "4 Experiments",
            "content": "Models and Implementation. We adopt Qwen2.5-0.5B-Instruct [27] as decoder-only backbone LLM for user representation learning. Heterogeneous user behaviors are encoded by modality-specific gte-base [17] encoders into dense embeddings, which are aggregated into unified user representations and fed into the backbone LLM for both training and inference. All baselines and ablations are trained under identical conditions: 50k fine-tuning steps with global batch size of 2,048, using AdamW with an initial learning rate of 2 104 and cosine decay. We employ LoRA [11] with rank 64 and ğ›¼ = 32 for pretraining inspired by [32], and user representations are fixed at 128 dimensions. Pretraining is conducted on 64 A100-80G GPUs with data parallelism, inference is performed on single A100-80G GPU for evaluation, and soft prompt tuning with 6 learnable tokens is carried out on single A100-80G GPU. Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 2: AUC performance on 10 key scenarios where our Q-Anchor (base version: pretrained) and (prompt-tuning version: post-trained) shows consistent improvement. KS results are detailed in Appendix C.1. Method Active Concert Login Forest Fraud Money Takeout Brand Promo Value User Engagement Risk Marketing Sensitivity Avg. AUC General Embedding Models Qwen2.5-0.5B-Instruct Qwen3-Embedding-0.6B Llama-Embed-Nemotron-8B KaLM-Embed.-Gemma3-12B User Embedding Models MSDP [8] One4all [24] CPC [22] FOUND [7] Q-Anchor Embedding (Ours) Q-Anchor (Base) Q-Anchor (Prompt Tuned) 0.5269 0.5378 0.5882 0.5597 0.6415 0.6515 0.6506 0. 0.5173 0.5226 0.5627 0.5359 0.5155 0.5568 0.5314 0.5527 0.7219 0.7294 0.7735 0.7609 0.9504 0.9509 0.9506 0.8131 0.7161 0.7287 0.8372 0.7729 0.9580 0.9614 0.9608 0. 0.6969 0.7106 0.8632 0.8229 0.9152 0.9203 0.9171 0.9083 0.6885 0.7123 0.8708 0.8174 0.8746 0.8782 0.8736 0.9235 0.6361 0.6914 0.8176 0.7812 0.7814 0.7609 0.7817 0. 0.5855 0.6076 0.7525 0.6564 0.6318 0.6289 0.6235 0.7294 0.5483 0.5508 0.5918 0.589 0.5850 0.5761 0.6101 0.6202 0.7134 0.7172 0.8303 0.7609 0.8389 0.8207 0.8259 0. 0.6351 0.6508 0.7488 0.7058 0.7692 0.7706 0.7725 0.7832 0.6568 0.6678 0.5739 0.5844 0.8420 0.8443 0.9700 0. 0.9218 0.9242 0.9382 0.9439 0.8799 0.8811 0.7979 0.8535 0.6189 0.6350 0.9049 0. 0.8104 0.8225 Figure 5: Average KS performance of Q-Anchor and baselines across 10 Alipay scenarios. Per-scenario results are provided in Appendix C.1. Robust cross-domain representation validates Query-asAnchor as one-model-for-many paradigm. The same encoder generalizes well across three heterogeneous domains Engagement, Risk, and Marketingwithout task-specific architectures. In Risk, Q-Anchor (Prompt Tuned) achieves 0.9439 on Money, outperforming strong user-embedding baselines such as FOUND (0.9235) and MSDP (0.8746), suggesting that the anchor query suppresses high-entropy transactional noise and amplifies decision-relevant patterns. In Marketing, the gains are even more pronounced: for Brand, AUC jumps from 0.7979 (Base) to 0.8535 (Prompt Tuned), indicating that the model not only transfers across domains but also adapts to domain-specific decision boundaries where subtle preference signals matter. This supports scalable alternative to maintaining many scenario-specific models: universal representation plus lightweight scenario conditioning. Q-Anchor (Base) scales more with data than with parameters. Figure 6 characterizes Q-Anchor (Base) under data scaling (20.48M102.4M pretraining pairs) and model scaling (Qwen2.50.5B3B) with fixed training/data budget. Increasing data yields consistent gains (Avg. AUC 0.80290.8105, Avg. KS 0.48950.5044). In contrast, model scaling is non-monotonic: Table 8 shows 0.5B (a) Pretraining Data Scale (b) Impact of Model Size Figure 6: Scalability analysis of Q-Anchor Embedding (Base). (a) Performance increases with more pretraining data. (b) Performance vs. model parameters (0.5B to 3.0B). performs best (Avg. AUC/KS=0.8105/0.5044), while 1.5B/3B bring no gains and sometimes regress (e.g., Brand, Promo), consistent with prior embedding-scaling observations [14]. To better understand this, we analyze larger models training gradient in Appendix C.2  (Fig. 11)  , showing that embedding quality depends more on pretraining data than model scale. Accordingly, we adopt 50k-step pretraining with the 0.5B backbone for the optimal accuracyefficiency trade-off; per-scenario results are provided in Appendix C.2. Q-Anchor (Prompt-tuned) scales efficiently with prompt tokens and tuning steps, saturating early. Figure 8 evaluates QAnchor Embedding (Prompt Tuned) under token scaling (116 tokens) and step scaling (100500 steps). Performance rises rapidly up to 6 tokens (Avg. AUC 0.81460.8225, Avg. KS 0.51400.5267), then saturates with minor fluctuations at 8/16 tokens. Increasing tuning steps yields steady gains (Avg. AUC 0.81590.8225, Avg. KS 0.51410.5267). Overall, prompt tuning is efficiency-friendly, achieving most gains with small prompt budget and modest optimization, motivating 6 tokens and 500 steps as our default. Perscenario results are in Appendix C.3. Scenario-adaptive tuning as performance multiplier with interpretable re-anchoring. Post-training with cluster-based soft prompts yields consistent lift in all scenarios (avg. 0.8104 to 0.8225), confirming that the base model captures general behavioral Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 7: Modal attention shift after prompt tuning. Left: Takeout InterestBill attention increases by +26.0%, capturing dining purchasing power. Right: Ant ForestSPM attention rises by +6.4%, consistent with navigation-heavy usage. Red arrows mark positive attention gains, highlighting QAnchors dynamic feature re-anchoring. (a) User Engagement (b) Risk (a) Prompt Tokens Scale (b) Training Steps Scale Figure 8: Scalability analysis of Q-Anchor Embedding (Prompt Tuned). (a) Performance vs. the number of learnable prompt tokens (1-16, Ours: 6). (b) Performance vs. the training-step budget (100500, Ours: 500). semantics while prompts specialize the embedding space to scenario boundaries. Importantly, the improvement is not black box: Figure 7 shows that prompt tuning changes where the model looks. For Takeout Interest, attention to the Bill modality increases by +26.0%, aligning with purchasing power as the dominant signal; for Ant Forest, the SPM modality rises by +6.4%, matching navigationintensive usage. The t-SNE visualizations (Figure 9) further corroborate this behavior: prompt-tuned representations form clearer scenario-consistent groupings than the universal space, reflecting better separation of positives vs. negatives. To avoid visualization bias, we corroborate t-SNE findings with PCA (Appendix C.4), which consistently shows sharper cluster separation under prompt tuning. Together, the quantitative gains and qualitative shifts indicate that Query-as-Anchor functions as semantic lensguiding the representation to focus more on semantically relevant multimodal evidence through minimal parameter updatesenabling fast, deployable specialization in changing industrial environments."
        },
        {
            "title": "5.2 Ablation Study\nWe ablate Q-Anchor (Base) by removing one component at a time\nwhile keeping the backbone, data, and training budget fixed. We\nevaluate (i) structural tokens (User token, modality tokens, or both),\n(ii) pretraining ingredients (margin-mask filtering, NTP, and con-\ntrastive alignment), and (iii) post-training (prompt tuning, and",
            "content": "(c) Marketing Sensitivity Figure 9: t-SNE Visualization of universal and prompt-tuned representation of 10 scenarios. PCA visualizations are provided in Appendix C.4. prompt tuning without pretraining). AUC results are in Table 3, with KS results in Appendix C.1. Ablation on Modality Token: Explicit structure helps DeepFind attribute evidence to the correct source. Removing the User or modality tokens slightly lowers Avg. AUC (0.8104 to 0.8086/ 0.8088), and removing both produces the largest drop in this block (0.8065). The effect concentrates on modality-sensitive marketing scenarios, especially Brand (0.7979 to 0.7819), where performance depends on which modality carries the signal rather than overall activity. These results support our design: user/modal markers inject minimal inductive bias about log structure and improve cross-modal aggregation; consistent degradations are also observed in KS (Appendix C.1), indicating reduced ranking separability. Ablation on Training Method: Contrastive alignment is the primary signal; auxiliary objectives act as regularizers. Removing contrastive learning causes the largest regression, reducing Avg. AUC from 0.8104 to 0.7667 and sharply degrading fine-grained scenarios (e.g., Brand 0.79790.6512; Money 0.93820.9091). This indicates that token-level modeling alone cannot impose queryconditioned separability, whereas contrastive supervision explicitly structures the embedding space. In comparison, margin-mask filtering and next-token prediction are supportive: disabling filtering Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 3: Ablation study (AUC) of Q-Anchor Embedding across modality, training method and prompt tuning. KS results are detailed in Appendix C.1. User Engagement Risk Marketing Sensitivity Avg. AUC Method Active Concert Q-Anchor (Base) 0.6568 0. Ablation on Modality Token / User Tok. / Modal Tok. / Modal & User Tok. Ablation on Training Method / Filter / NTP / Contrastive Q-Anchor (Prompt Tuned) Ablation on Prompt Tuning / pretrain 0.6525 0.6546 0.6544 0.6461 0.6577 0. 0.6678 0.5757 0.5701 0.5703 0.5693 0.5692 0.5456 0.5844 Login 0. 0.8424 0.8423 0.8403 0.8412 0.8392 0.8454 0.8443 Forest 0.9700 0.9702 0.9700 0. 0.9685 0.9701 0.9584 Fraud 0.9218 0.9219 0.9214 0.9211 0.9197 0.9208 0.9071 Money Takeout 0.9382 0.8799 Brand 0.7979 Promo 0.6189 Value 0.9049 0.9358 0.9372 0.9374 0.9357 0.9369 0.9091 0.8785 0.8778 0. 0.8728 0.8798 0.7967 0.7884 0.7903 0.7819 0.7867 0.7858 0.6512 0.6195 0.6210 0.6141 0.6218 0.6079 0.5980 0.9006 0.9036 0. 0.8854 0.8936 0.8143 0.9716 0.9242 0.9439 0.8811 0. 0.6350 0.9194 0.8104 0.8086 0.8088 0.8065 0.8047 0.8061 0.7667 0. 0.6557 0.5432 0.7546 0.9338 0.9189 0. 0.8793 0.7937 0.5833 0.9044 0.7782 (w/o Filter) lowers Avg. AUC to 0.8047, consistent with increased false negatives in noisy logs, and disabling NTP (w/o NTP) yields 0.8061, suggesting weaker local event modeling. KS shows an even steeper drop without contrastive alignment (Appendix C.1), and the same trend is reflected qualitatively by less coherent scenario clusters in Fig. 9. Overall, contrastive alignment defines the geometry, while filtering and NTP mainly stabilize training and improve robustness. Ablation on Pretraining: Pre-trained weights provide the essential behavioral prior for intent discovery. Excluding the pretraining phase (w/o pretrain) triggers systemic performance collapse: Average AUC falls from 0.8225 to 0.7781, while Average KS undergoes sharper relative regression of 11.2% (0.52670.4679). The degradation is most pronounced in complex scenarios (e.g., Money AUC: 0.94390.8153), where prompt tuning alone fails to reconstruct the high-dimensional latent structures necessary for fine-grained separation. This confirms that pretraining is not mere optimization aid but foundational requirement for distilling robust behavioral priors, without which the model cannot effectively generalize across noisy, long-tail user trajectories."
        },
        {
            "title": "5.3 Industrial Online A/B Testing\nWe evaluate Q-Anchor embeddings in two large-scale Alipay A/B\ntests over two weeks, with users randomly assigned to treatment\n(policy with embeddings) or control (fixed-time/rule-based). See\nAppendix D for pre-deployment offline results; online A/B outcomes\nare reported below:",
            "content": "Scenario I: Interactive Voice Response (IVR) Cash-Reserve Outreach. Q-Anchor embeddings were deployed in the live cashreserve outreach system to optimize send-time decisions according to user availability and responsiveness. In production, this representation-aware timing increased the drawdown rate by 12.5% and the average outstanding balance per user by 5.3%. Early-funnel engagement also improved: the cash-reserve product visit rate rose by 4.2%, and drawdown-page visits increased by 17.7%, reflecting stronger activation and smoother progression through the credit funnel. These results demonstrate that embedding-informed timing decisions enhance both immediate credit utilization and upstream engagement. Scenario II: Credit Delinquency Risk Identification. Q-Anchor embeddings were integrated into the credit risk scoring pipeline, improving the business-critical KS score by 1.96%. Results show that embedding-informed scoring enhances predictive performance and risk-aware credit allocation. Deployment at Alipay Scale. We run daily Q-Anchor embedding refresh for hundreds of millions of users on 100L20 cluster. For multi-scenario serving, Query-as-Anchor with shared prefix KV-cache encodes each user prefix once and reuses it across queries, so adding scenario only requires query-suffix computation (one extra L20 to maintain the same SLA), rather than replicating the entire 100-GPU pipeline for every scenario."
        },
        {
            "title": "6 Conclusion\nWe present Q-Anchor Embedding, a unified framework for indus-\ntrial user representation that closes the gap between sparse, noisy,\nheterogeneous behavior logs and LLM-level semantic representatin.\nQ-Anchor contributes: (1) UserU, an industrial-scale pretraining\ncorpus that couples rule-based future behavior supervision with\nreflection-verified LLM-synthesized user QA to inject temporal dy-\nnamics and semantic understanding; (2) Query-as-\nAnchor, a hierarchical coarse-to-fine user encoder and query- con-\nditioned alignment paradigm that distills multi-source signals into\ntask-adaptive embeddings; and (3) semantic re-anchoring via\nlightweight soft prompts that re-anchor modalities to match down-\nstream decision boundaries. Across 10 real-world benchmarks and\n2 online A/B tests, Q-Anchor achieves SOTA AUC and KS over\nLLM embeddings and strong baselines, while enabling interpretable,\nscenario-adaptive, low-cost, and transferable industrial user repre-\nsentations.",
            "content": "Ethical Considerations All experiments comply with Alipays data-governance, privacy, and security policies, with encrypted data/embeddings, strict access control, and audit logging to prevent unauthorized access or linkage. We present Q-Anchor Embedding to advance responsible user representation learning and industrial systems. Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY References [1] Yauhen Babakhin, Radek Osmulski, Ronay Ak, Gabriel Moreira, Mengyao Xu, Benedikt Schifferer, Bo Liu, and Even Oldridge. 2025. Llama-Embed-Nemotron8B: Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks. arXiv preprint arXiv:2511.07025 (2025). [2] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. [n. d.]. LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders. In First Conference on Language Modeling. [3] Vance Berger and YanYan Zhou. 2014. Kolmogorovsmirnov test: Overview. Wiley statsref: Statistics reference online (2014). [4] Andrew Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern recognition 30, 7 (1997), 11451159. [5] Zheng Chai, Zhihong Chen, Chenliang Li, Rong Xiao, Houyi Li, Jiawei Wu, Jingxu Chen, and Haihong Tang. 2022. User-aware multi-interest learning for candidate matching in recommenders. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 13261335. [6] Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. 2025. Little giants: Synthesizing high-quality embedding data at scale. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 13921411. [7] Bin Dou, Baokun Wang, Yun Zhu, Xiaotong Lin, Yike Xu, Xiaorui Huang, Yang Chen, Yun Liu, Shaoshuai Han, Yongchao Liu, et al. 2025. Transferable and Forecastable User Targeting Foundation Model. In Companion Proceedings of the ACM on Web Conference 2025. 181190. [8] Chilin Fu, Weichang Wu, Xiaolu Zhang, Jun Hu, Jing Wang, and Jun Zhou. 2023. Robust user behavioral sequence representation via multi-scale stochastic distribution prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 45674573. [9] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 68946910. [10] Yingzhi He, Xiaohao Liu, An Zhang, Yunshan Ma, and Tat-Seng Chua. 2025. Llm2rec: Large language models are powerful embedding models for sequential recommendation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 896907. [11] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. [n. d.]. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. [12] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation via llm-based semantic embedding learning. In Companion Proceedings of the ACM Web Conference 2024. 103111. [13] Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, et al. 2025. Kalm-embedding: Superior training data brings stronger embedding model. arXiv preprint arXiv:2501.01028 (2025). [14] Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. 2024. Scaling sentence embeddings with large language models. In Findings of the association for computational linguistics: EMNLP 2024. 31823196. [15] Chenglin Li, Yuanzhen Xie, Chenyun Yu, Bo Hu, Zang Li, Guoqiang Shu, Xiaohu Qie, and Di Niu. 2023. One for all, all for one: Learning and transferring user embeddings for cross-domain recommendation. In Proceedings of the sixteenth ACM international conference on web search and data mining. 366374. [16] Shiyu Li, Yang Tang, Ruijie Liu, Shi-Zhe Chen, and Xi Chen. 2025. Conanembedding-v2: Training an llm from scratch for text embeddings. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 1501115027. [17] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281 (2023). [18] Guanyu Lin, Chen Gao, Yinfeng Li, Yu Zheng, Zhiheng Li, Depeng Jin, and Yong Li. 2022. Dual contrastive network for sequential recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 26862691. [19] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 6168. [20] Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn OBanion, and Jun Xie. 2025. User-llm: Efficient llm contextualization with user embeddings. In Companion Proceedings of the ACM on Web Conference 2025. 12191223. [21] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [23] Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, and Jingbo Shang. 2024. Answer is All You Need: Instruction-following Text Embedding via Answering the Question. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 459477. [24] Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Minkyu Kim, Young-Jin Park, Jisu Jeong, and Seungjae Jung. 2021. One4all user representation for recommender systems in e-commerce. arXiv preprint arXiv:2106.00573 (2021). [25] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 14411450. [26] Yixuan Tang and Yi Yang. 2024. Do we need domain-specific embedding models? An empirical investigation. arXiv preprint arXiv:2409.18511 (2024). [27] Qwen Team et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407. 2, 3 (2024). [28] Sishi Xiong, Yu Zhao, Jie Zhang, Li Mengxiang, Zhongjiang He, Xuelong Li, and Shuangyong Song. 2024. Dual prompt tuning based contrastive learning for hierarchical text classification. In Findings of the association for computational linguistics ACL 2024. 1214612158. [29] Jiahao Yuan, Zhiqing Cui, Hanqing Wang, Yuansheng Gao, Yucheng Zhou, and Usman Naseem. 2025. Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning. arXiv preprint arXiv:2512.01282 (2025). [30] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. [n. d.]. When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method. In The Twelfth International Conference on Learning Representations. [31] Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, et al. 2024. Scaling user modeling: Largescale online user representations for ads personalization in meta. In Companion Proceedings of the ACM Web Conference 2024. 4755. [32] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176 (2025). [33] Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, and Tyler Derr. 2024. Can one embedding fit all? multiinterest learning paradigm towards improving user interest diversity fairness. In Proceedings of the ACM web conference 2024. 12371248. Data Example of UserU Dataset In this section, we present illustrative toy examples curated from the two core subsets of the UserU dataset: the Behavior-based Interaction Dataset (Dfuture) and the Synthetic Query-Answer Dataset (Duqa). These samples are designed to provide qualitative intuition of the data modality and the alignment tasks. Privacy and Demonstration Note: It is important to emphasize that the cases provided below are synthetic toy examples constructed solely for demonstration purposes. They do not represent the actual raw records of any specific individual. Details of Downstream Benchmarks As shown in Table 1 & Table 4, we evaluate on Dğ‘¡ğ‘’ğ‘ ğ‘¡ containing 10 real-world binary classification tasks, grouped into three domains. Common protocol. Each benchmark is user-level binary classification task. For task ğœ, we define prediction window and assign the label ğ‘¦ğœ ğ‘¢ {0, 1} by whether user ğ‘¢ triggers at least one target event associated with ğœ during that window. We report task definitions at the scenario level for privacy and compliance; all methods are evaluated under identical data construction and labeling rules. Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 4: Downstream test benchmarks and scenario-level binary label definitions. Labels are assigned by whether user triggers the target event within the prediction window. Each task contains 50w test data. Domain User Engagement Scenario (Task) Interest Community Active User Identification (Active) Target Whether user will be active in the Alipay interest community. Concert Click Prediction (Concert) User Log-in Prediction (Login) Whether user will log in. Ant Forest Engagement (Forest) Whether user will engage Whether user will click concert-related content. Risk Fraud Detection (Fraud) Money Laundering Detection (Money) Takeout Interest (Takeout) Marketing Sensitivity Brand Sensitivity (Brand) Big Sale Sensitivity (Promo) Cost-Performance Sensitivity (Value) with Ant Forest. Whether user will be associated with fraud-related risk event. Whether user will be associated with money-launderingrelated risk event. Whether user will show interest in takeout-related services. Whether user will respond to brand-related marketing stimuli. Whether user will respond to big-sale promotions. Whether user will respond to cost-performance offers. Positive (ğ‘¦=1) Any qualified interest-community activity occurs (e.g., community visit/app open, content browse, like, comment, post). Any click on items categorized as concertrelated occurs. At least one login event occurs. Any Ant Forest interaction occurs (e.g., enter/collect/participate). User is confirmed/flagged by the risk-control pipeline as fraud-related. Negative (ğ‘¦=0) No qualified interest-community activity occurs. No click on concert-related items occurs. No login event occurs. No Ant Forest interaction occurs. User is not flagged as fraud-related. is as by moneyconfirmed/flagged User compliance/risk-control laundering-related. Any click/visit/conversion on takeoutcategory content occurs. Any interaction with brand campaigns occurs (e.g., click/claim/participate). Any interaction with big-sale promotional content occurs. Any interaction with value-for-money themed content or items tagged as costperformance occurs. User is not flagged as money-launderingrelated. No click/visit/conversion on takeoutcategory content occurs. No interaction with brand campaigns occurs. No interaction with big-sale promotional content occurs. No such interaction occurs. Supplementary Experiments and Analysis C.1 Experimental Results on Alipay Benchmarks: KS Performance and Ablation Study KS Performance. As illustrated in Table 5, our Q-Anchor embedding achieves state-of-the-art KS performance across all 10 industrial scenarios, demonstrating superior discriminative power in ranking positive vs. negative user behaviors. As shown in Table 2, Q-Anchor (Prompt Tuned) attains the highest average KS (0.5267), outperforming not only general-purpose embedding models (e.g., Llama-Embed-Nemotron-8B: 0.3805) but also specialized user representation methods such as FOUND (0.4529) and CPC (0.4556). KS Ablation Study. Table 6 presents the KS ablations of QAnchor (Base) by removing one component at time while keeping the backbone, data, and training budget fixed. Overall, removing structural tokens (user/modality token) causes mild yet consistent degradations in Avg. KS, indicating that explicit log structure helps the encoder attribute evidence to the correct source and improves ranking stability. In contrast, the contrastive objective is the primary driver of discriminability: without contrastive learning, Avg. KS drops substantially from 0.5044 to 0.4215, with particularly large losses on fine-grained marketing scenarios (e.g., Brand: 0.4527 0.2169), showing that token-level modeling alone is insufficient to enforce query-conditioned separation. Disabling margin-mask filtering or NTP yields smaller but systematic decreases, suggesting they mainly stabilize training under noisy, sparse behavioral logs. Notably, the \"w/o pretrain\" ablation reveals that skipping the initial pretraining phase significantly weakens the models performance (Avg. KS drops by 11.16%), even with subsequent prompt tuning. (a) Alignment between raw multi-modal behavioral signals and future user trajectories. (b) High-level semantic reasoning derived from implicit behavioral patterns. Figure 10: Illustrative toy instances from the UserU dataset. Figure (a) illustrates the behavior-to-behavior prediction task, while Figure (b) showcases the behavior-to-semantic reasoning capability enabled by query-aware alignment. Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY Table 5: KS performance on 10 key scenarios where our Q-Anchor (base version: pretrained) and (prompt-tuning version: post-trained) shows consistent improvement. Method Active Concert Login Forest Fraud Money TakeOut Brand Promo Value User Engagement Risk Marketing Sensitivity Avg. KS General Embedding Models Qwen2.5-0.5B-Instruct Qwen3-Embedding-0.6B Llama-Embed-Nemotron-8B KaLM-Embed.-Gemma3-12B User Embedding Models MSDP [8] One4all [24] CPC [22] FOUND [7] 0.0421 0.0524 0.1302 0.0880 0.2165 0.2403 0.2402 0.1669 0.0280 0.0330 0.0857 0. 0.0608 0.1075 0.0921 0.0780 0.3326 0.3459 0.4003 0.3830 0.7731 0.7749 0.7729 0.4660 0.3174 0.3274 0.5201 0.4036 0.8104 0.8191 0.8197 0.7967 0.3034 0.3102 0.5758 0. 0.7026 0.7160 0.7077 0.6739 0.2958 0.3086 0.5868 0.4879 0.6069 0.6113 0.6028 0.7084 0.2066 0.275 0.4918 0.4207 0.4229 0.3886 0.4249 0.5762 0.1324 0.1581 0.3685 0. 0.2107 0.2135 0.2068 0.3348 0.0880 0.0814 0.1364 0.1312 0.1299 0.1261 0.1824 0.1753 0.3262 0.3270 0.5091 0.3937 0.5345 0.4980 0.5061 0.5521 0.2073 0.2219 0.3805 0. 0.4469 0.4495 0.4556 0.4529 Q-Anchor Embedding (Ours) Q-Anchor (Base) Q-Anchor (Prompt Tuned) 0.2228 0.2412 0.1023 0.1177 0.5216 0.5225 0.8437 0. 0.7054 0.7086 0.7450 0.7573 0.6210 0.6212 0.4527 0.5564 0.1727 0.1962 0.6567 0. 0.5044 0.5267 This underscores that pretraining effectively encodes robust behavioral prior that serves as necessary foundation for downstream adaptation. These patterns closely match the AUC ablations, confirming that our design choices jointly improve both classification performance and the ranking quality measured by KS. C.2 Scalability of Q-Anchor Embedding (Base) Scalability performance across pretraining data scales. Table 7 evaluates Q-Anchor (Base) under increasing pretraining data scales (10k50k steps, i.e., 20.5M102.4M samples with batch size 2048). Overall, performance improves steadily as we scale up pretraining, with the best overall results achieved at 50k steps: Avg. AUC increases from 0.8029 (10k) to 0.8105, and Avg. KS rises from 0.4895 to 0.5044. Gains are particularly clear in high-signal domains such as Risk and Marketing (e.g., Money AUC: 0.9340 0.9382; Value AUC: 0.8869 0.9049), suggesting that larger pretraining exposes the encoder to more diverse long-tail behavioral patterns and improves query-conditioned ranking separability. While few scenarios saturate early (e.g., Forest), the overall trend indicates that scaling pretraining data consistently strengthens both classification (AUC) and ranking quality (KS); therefore, we adopt 50k as our default setting in all subsequent experiments. Scalability performance across model scales. Table 8 compares Q-Anchor (Base) under different backbone scales (0.5B3B). We observe that scaling up the model does not monotonically improve performance: the 0.5B backbone achieves the best overall results (Avg. AUC/KS: 0.8105/0.5044), while larger models (1.5B and 3B) provide no consistent gains and even slightly regress on several scenarios (e.g., Brand and Promo). This non-monotonic scaling behavior is aligned with prior findings [14] that, under fixed data and training budgets, larger encoders may not translate into better embedding quality and can be harder to optimize for sentencelevel embedding objectives [14]. From an industrial perspective, the 0.5B configuration is also preferable for deployment due to its substantially lower latency and serving cost, making it better (a) Performance (Avg. AUC/KS) vs. model size (0.5B3B). (b) Gradient magnitude decreases with model scale. Figure 11: Scalability analysis of Q-Anchor (Base). (a) Smaller 0.5B backbone achieves best Avg. AUC/KS; larger models show no consistent gains. (b) Gradient magnitude diminishes as model size increases, explaining the nonmonotonic performance trend. accuracyefficiency trade-off for high-throughput online Alipay financial and risk systems. To investigate the non-monotonic performance observed across different model scales, we analyze the gradient dynamics during the late stages of training. As shown in Figure 11, both the average and maximum gradient magnitudes decrease sharply as the model size increases: the average gradient drops from 0.082 (0.5B) to 0.035 (1.5B) and further to 0.028 (3B), while the maximum gradient similarly decreases from 0.824 to 0.231 and 0.164, respectively. This empirical evidence indicates that under fixed data budgets, larger encoders produce increasingly flat optimization landscapes for sentence-level embedding objectives, leading to saturation of learning signals. Consistent with prior findings [14], this explains why larger models, despite their generative capacity, are harder to optimize for discriminative embedding alignment. From an industrial perspective, the 0.5B configuration achieves the best accuracyefficiency trade-off, offering stronger gradients, better embedding quality, and significantly lower serving latency and Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 6: Ablation study (KS) of Q-Anchor Embedding across modality, training method and prompt tuning. Method Active Concert Login Forest Fraud Money TakeOut Brand Promo Value Q-Anchor (Base) 0.2228 0.1023 0.5216 0.8437 0.7054 0. 0.6210 0.4527 0.1727 0.6567 0.5044 User Engagement Risk Marketing Sensitivity Avg. KS Ablation on Modality Token / User Tok. / Modal Tok. / Modal & User Tok. 0.2176 0.2186 0.2181 Ablation on Training Method / Filter / NTP / Contrastive 0.2082 0.2255 0.2028 0.1021 0.0982 0.0966 0.0982 0.0952 0.0662 0.5204 0.5192 0.5169 0.5176 0.5150 0.5213 0.8440 0.8415 0. 0.7056 0.7039 0.7030 0.8370 0.8432 0.7977 0.6986 0.7024 0.6690 0.7387 0.7407 0.7402 0.7386 0.7404 0.6752 0.6200 0.6187 0. 0.6071 0.6225 0.4534 0.4309 0.4321 0.4246 0.3824 0.4298 0.2169 0.1744 0.1775 0.1654 0.6478 0.6574 0.6414 0.1755 0.1524 0. 0.6133 0.6346 0.4702 0.5002 0.5008 0.4966 0.4877 0.4961 0.4215 Q-Anchor (Prompt Tuned) 0.2412 0. 0.5225 0.8480 0.7086 0.7573 0.6212 0. 0.1962 0.6978 0.5267 Ablation on Prompt Tuning / pretrain 0.2200 0. 0.4522 0.7822 0.6953 0.5645 0.6202 0. 0.1528 0.6550 0.4679 Table 7: Scalability performance of Q-Anchor Embedding (Base) across pretraining steps (10k50k) and corresponding data scales (20.5M102.4M samples, batch size=2048) (AUC & KS). Steps / Data scale Active Concert Login Forest Fraud Money TakeOut Brand Promo Value User Engagement Risk Marketing Sensitivity Avg. AUC Performance w/ 10k / 20.48M w/ 20k / 40.96M w/ 30k / 61.44M w/ 40k / 81.92M w/ 50k / 102.4M KS Performance w/ 10k / 20.48M w/ 20k / 40.96M w/ 30k / 61.44M w/ 40k / 81.92M w/ 50k / 102.4M 0.6443 0.6481 0.6472 0.6532 0.6571 0.2036 0.2110 0.2099 0.2183 0.2228 0.5630 0.5713 0.5735 0.5742 0. 0.0857 0.0988 0.1008 0.1015 0.1023 0.8419 0.8429 0.8429 0.8419 0.8420 0.5166 0.5205 0.5211 0.5204 0.5216 0.9707 0.9705 0.9706 0.9704 0.9700 0.8447 0.8445 0.8459 0.8447 0.8437 0.9170 0.9198 0.9203 0.9204 0. 0.6950 0.7012 0.7030 0.7025 0.7054 0.9340 0.9362 0.9365 0.9369 0.9382 0.7337 0.7369 0.7391 0.7405 0.7450 0.8759 0.8776 0.8782 0.8792 0.8799 0.6140 0.6167 0.6173 0.6201 0.6210 0.7750 0.7988 0.8027 0.7986 0. 0.4099 0.4534 0.4606 0.4536 0.4527 0.6206 0.6259 0.6229 0.6159 0.6190 0.1761 0.1848 0.1789 0.1685 0.1727 0.8869 0.8999 0.9041 0.9045 0.9049 0.6153 0.6469 0.6554 0.6572 0.6567 0.8029 0.8091 0.8099 0.8095 0. 0.4895 0.5015 0.5032 0.5027 0.5044 cost, making it ideal for high-throughput deployment in Alipays production systems. C.3 Scalability performance of Q-Anchor Embedding (Prompt Tuned) Scalability performance across learnable token scales. Table 9 studies the scalability of prompt tuning by varying the number of learnable prompt tokens from 1 to 16. Overall, performance improves rapidly when increasing tokens from 1 to 6, and then largely saturates: the best average performance is achieved at 6 prompt tokens (Avg. AUC/KS: 0.8225/0.5267). This suggests that small number of learnable tokens is sufficient to provide scenario-specific conditioning, while additional tokens bring diminishing returns and may introduce optimization noise on some tasks. Given this clear accuracyefficiency trade-off, we adopt 6 prompt tokens as the default configuration in all subsequent experiments. Scalability performance across learning step scales. Table 10 reports the scalability of prompt-tuned Q-Anchor embedding under different training-step budgets (100500, step size = 100). As the number of steps increases, performance improves consistently on both AUC and KS, with the average AUC rising from 0.8159 (100 steps) to 0.8225 (500 steps) and the average KS from 0.5141 to 0.5267. Notably, 500 steps achieves the best overall results, delivering the top average AUC/KS and leading most tasks, while 400 steps is typically the strongest runner-up. These results indicate that our method benefits from longer optimization yet remains robust under smaller step budgets, showing stable scalability across training steps. C.4 PCA Visualization of Universal and Prompt-tuned Representation To complement our t-SNE analysis and strengthen the reliability of qualitative insights, we further visualize the universal and prompt-tuned representations using PCA  (Fig. 12)  . Across all three domainsUser Engagement, Risk, and Marketing Sensitivitythe prompt-tuned embeddings consistently exhibit tighter intra-class Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY Table 8: Scalability performance of Q-Anchor Embedding (Base) across model scales (0.5B3B). Model Active Concert Login Forest Fraud Money TakeOut Brand Promo Value User Engagement Risk Marketing Sensitivity Avg. AUC Performance Q-Anchor-0.5B (Base) Q-Anchor-1.5B (Base) Q-Anchor-3B (Base) KS Performance Q-Anchor-0.5B (Base) Q-Anchor-1.5B (Base) Q-Anchor-3B (Base) 0.6571 0.6436 0. 0.2228 0.2010 0.1957 0.5739 0.5693 0.5626 0.1023 0.0969 0.0875 0.8420 0.8444 0.8449 0.5216 0.5223 0.5234 0.9700 0.9706 0. 0.8437 0.8442 0.8457 0.9218 0.9220 0.9220 0.7054 0.7052 0.7045 0.9382 0.9376 0.9370 0.7450 0.7428 0.7407 0.8799 0.8759 0. 0.6210 0.6151 0.6171 0.7979 0.7864 0.7824 0.4527 0.4295 0.4231 0.6190 0.6156 0.6066 0.1727 0.1657 0.1493 0.9049 0.9000 0. 0.6567 0.6486 0.6454 0.8105 0.8065 0.8044 0.5044 0.4971 0.4932 Table 9: Scalability performance of Q-Anchor Embedding (Prompt Tuned) across learnable token scales (116) for prompt tuning (Ours: 6 prompt tokens)."
        },
        {
            "title": "Marketing Sensitivity",
            "content": "Avg. AUC Performance / 1 prompt tok. / 2 prompt tok. / 4 prompt tok. / 6 prompt tok. / 8 prompt tok. / 16 prompt tok. KS Performance / 1 prompt tok. / 2 prompt tok. / 4 prompt tok. / 6 prompt tok. / 8 prompt tok. / 16 prompt tok. 0.6513 0.6576 0.6625 0.6678 0.6612 0.6649 0.2176 0.2240 0.2323 0.2412 0.2317 0.2351 0.5746 0.5755 0.5792 0.5844 0.5774 0. 0.1031 0.1032 0.1068 0.1177 0.1078 0.1058 0.8408 0.8408 0.8450 0.8443 0.8432 0.8435 0.5208 0.5202 0.5222 0.5225 0.5229 0.5229 0.9702 0.9696 0.9715 0.9716 0.9704 0.9740 0.8444 0.8433 0.8509 0.8480 0.8432 0.8577 0.9215 0.9221 0.9220 0.9242 0.9228 0. 0.7053 0.7054 0.7056 0.7086 0.7073 0.7082 0.9424 0.9407 0.9430 0.9439 0.9409 0.9389 0.7576 0.7529 0.7593 0.7573 0.7520 0.7461 0.8798 0.8810 0.8804 0.8811 0.8801 0.8793 0.6228 0.6233 0.6208 0.6212 0.6219 0.6209 0.8290 0.8380 0.8501 0.8535 0.8575 0. 0.5152 0.5292 0.5537 0.5564 0.5646 0.5694 0.6250 0.6269 0.6321 0.6350 0.6310 0.6351 0.1790 0.1865 0.1928 0.1962 0.1904 0.1975 0.9114 0.9170 0.9155 0.9194 0.9205 0.9205 0.6742 0.6893 0.6833 0.6978 0.6995 0.6988 0.8146 0.8169 0.8201 0.8225 0.8205 0. 0.5140 0.5177 0.5228 0.5267 0.5241 0.5262 Table 10: Scalability of Q-Anchor Embedding (Prompt Tuned) across training steps (100500) for prompt tuning (Ours: 500 steps)."
        },
        {
            "title": "Marketing Sensitivity",
            "content": "Avg."
        },
        {
            "title": "Value",
            "content": "AUC Performance 0.6541 / 100 0.6601 / 200 0.6622 / 300 0.6657 / 400 / 500 0.6678 KS Performance 0.2212 / 100 0.2287 / 200 0.2317 / 300 0.2364 / 400 / 500 0.2412 0.5775 0.5774 0.5806 0.5821 0.5844 0.1059 0.1041 0.1100 0.1116 0.1177 0.8441 0.8432 0.8434 0.8434 0.8443 0.5225 0.5204 0.5227 0.5227 0. 0.9703 0.9701 0.9698 0.9718 0.9716 0.8429 0.8433 0.8418 0.8466 0.8480 0.9230 0.9237 0.9234 0.9236 0.9242 0.7064 0.7076 0.7076 0.7079 0.7086 0.9382 0.9403 0.9428 0.9433 0.9439 0.7425 0.7488 0.7543 0.7553 0. 0.8787 0.8796 0.8810 0.8799 0.8811 0.6191 0.6210 0.6227 0.6214 0.6212 0.8329 0.8454 0.8484 0.8513 0.8535 0.5195 0.5452 0.5478 0.5538 0.5564 0.6308 0.6303 0.6281 0.6348 0.6350 0.1913 0.1912 0.1864 0.1958 0. 0.9091 0.9139 0.9167 0.9167 0.9194 0.6701 0.6797 0.6866 0.6917 0.6978 0.8159 0.8184 0.8196 0.8213 0.8225 0.5141 0.5190 0.5212 0.5243 0.5267 Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 11: Offline performance of Q-Anchor embeddings. Base uses universal embeddings; Prompt Tuned adds lightweight, scenario-specific soft prompts. Both scenarios report AUC and KS, with Delinquency primarily optimizing KS. Method AUC KS Scenario: IVR Response Business SOTA Q-Anchor (Base) Q-Anchor (Prompt Tuned) Scenario: Delinquency Business SOTA Q-Anchor (Base) Q-Anchor (Prompt Tuned) 0.6362 0.6534 0.6863 0.2411 0.2682 0.3016 - - - 0.1499 0.1534 0.1700 Pre-Deployment Offline Performance on"
        },
        {
            "title": "Downstream Business",
            "content": "Before online deployment, we evaluate Q-Anchor embeddings against current production SOTA (highly optimized, domain-specific handcrafted features and specialized models) across two disparate tasks: IVR Response Prediction and Credit Delinquency Identification. Table 11 summarizes the results. Robustness of Universal Representations. Even in its Base form, Q-Anchor consistently outperforms the Business SOTA across all metrics. Specifically, in the IVR scenario, it achieves 2.71% absolute lift in KS over handcrafted features. This confirms that our pre-training objective effectively distills foundational understanding of user behavior that generalizes across different financial contexts without any scenario-specific adaptation. Steering Performance with Minimal Overhead. By introducing scenario-specific soft prompts, Q-Anchor (Prompt Tuned) further unlocks the latent potential of the universal backbone. It reaches 0.3016 KS (+6.05% over SOTA) in IVR and 0.1700 KS (+2.01% over SOTA) in delinquency prediction. Crucially, this substantial performance gain is achieved with negligible incremental cost. As Q-Anchor reuses the expensive user-prefix KV-cache, the prompt tuning only involves optimizing few learnable vectors at the query suffix. This architecture allows us to \"steer\" the same multi-billion parameter representation toward diverse business goals, achieving superior balance between high-precision task adaptation and industrial-scale computational efficiency. Deployment Detail of Q-Anchor Embedding To serve embedding requests at Alipay scale, we deploy Q-Anchor Embedding with an incremental update pipeline that decouples heavy user-history encoding from lightweight scenario querying. Each day, the system ingests newly arrived multi-modal behaviors and updates the user profile by re-encoding only the affected modalities (i.e., modalities that receive new events) and refreshing their corresponding modalityand period-level summary tokens, rather than reprocessing all modalities and all raw events from the past 90 days end-to-end. For each modality ğ‘š M, frozen backbone with modality-specific event adapter encodes the days events into event-level tokens, which are further aggregated into modality (a) User Engagement (b) Risk (c) Marketing Sensitivity Figure 12: PCA Visualization of universal and prompt-tuned representation of 10 scenarios. clustering and clearer separation between positive and negative samples compared to the base universal representation. This effect is especially pronounced in high-stakes scenarios such as Brand and Money, where subtle behavioral signals determine model performance. The convergence of t-SNE and PCA visualizations with consistent quantitative improvements in AUC and KS underscores that lightweight scenario-specific conditioning effectively reshapes the embedding geometry to better align with downstream decision boundarieswithout modifying the underlying architecture. Query as Anchor: Scenario-Adaptive User Representation via Large Language Model Conference acronym XX, June 0305, 2018, Woodstock, NY ğ‘š summary token z(mdl) (Eq. 2). Newly generated tokens are updated into the historical token buffer to form the updated hierarchical prefix eğ‘– , while tokens outside the rolling window are expired. This design keeps representations fresh, bounded, and cost-stable. For online inference, we exploit Query-as-Anchor with prefix KV-cache sharing  (Fig. 4)  . For each user, the prefix eğ‘– is encoded once to build shared KV cache that is reused across downstream scenarios. Given scenario queries {ğ‘1, . . . , ğ‘ğ‘› } (where ğ‘› is the number of scenarios), we process the queries sequentially while reusing the same shared prefix cache. For each query ğ‘ ğ‘— , the model only computes the short query suffix on top of the cached prefix, reducing the marginal per-scenario cost to ğ‘‚ (ğ¿ğ‘ ğ‘— ) (and ğ‘‚ ((cid:205)ğ‘› ğ‘—=1 ğ¿ğ‘ ğ‘— ) for all scenarios) and enabling efficient multi-scenario re-anchoring (e.g., risk, marketing, engagement). Overall, the deployment provides (i) efficient daily refresh via delta updates and (ii) high-throughput multi-scenario serving by amortizing prefix encoding with KVcache reuse. Future Work While standard LLM scaling laws [30] suggest performance improves with model size, our experiments reveal Scaling Paradox in user embeddings: larger backbones (1.5B3B) exhibit gradient attenuation and performance stagnation compared to the 0.5B model, consistent with prior observations [14]. This indicates that embedding quality may follow discriminative scaling trajectory, governed by the signal-to-parameter ratio rather than raw parameter count. Future work will investigate gradient recovery and adaptive parameter tuning to overcome optimization plateaus and systematically extend scaling benefits to larger models. Within the standardized, fair comparison setup of this studywithout additional hyperparameter or learning-rate tricksQ-Anchor achieves optimal performance at minimal cost on the 0.5B backbone. Accordingly, this base model will remain the primary configuration for our deployment and large-scale adoption across diverse Alipay scenarios. Received 20 February 2007; revised 12 March 2009; accepted 5 June"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University"
    ]
}