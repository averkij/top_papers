{
    "paper_title": "Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games",
    "authors": [
        "Will Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. These games typically fall into three categories based on the flow of control: strictly sequential (players alternate single actions), deterministic response (some actions trigger a fixed outcome), and unbounded reciprocal response (alternating counterplays are permitted). A less-explored but strategically rich structure is the bounded one-sided response, where a player's action briefly transfers control to the opponent, who must satisfy a fixed condition through one or more moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs). We introduce a modified version of Monopoly Deal as a benchmark environment that isolates this dynamic, where a Rent action forces the opponent to choose payment assets. The gold-standard algorithm, Counterfactual Regret Minimization (CFR), converges on effective strategies without novel algorithmic extensions. A lightweight full-stack research platform unifies the environment, a parallelized CFR runtime, and a human-playable web interface. The trained CFR agent and source code are available at https://monopolydeal.ai."
        },
        {
            "title": "Start",
            "content": "Monopoly Deal: Benchmark Environment for Bounded One-Sided Response Games 5 2 0 2 0 3 ] . [ 2 0 8 0 5 2 . 0 1 5 2 : r Will Wolf williamabrwolf@gmail.com October 31, 2025 Abstract Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. Typically, these games fall into three categories based on the flow of control: strictly-sequential (where players alternate single actions), deterministic-response (where some actions trigger fixed outcome), and unbounded reciprocal-response (where alternating counterplays are permitted). less-explored but strategically rich structure exists: the bounded one-sided response. This dynamic occurs when players action briefly transfers control to the opponent, who must satisfy fixed condition through one or more sequential moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs). We introduce modified version of Monopoly Deal as benchmark environment that specifically isolates the BORG dynamic, where Rent action forces the opponent to sequentially choose payment assets. We demonstrate that the gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully converges on effective strategies for this domain without requiring novel algorithmic extensions. To support efficient, reproducible experimentation, we present lightweight, full-stack research platform that unifies the environment, parallelized CFR runtime, and humanplayable web interface, all runnable on single workstation. This system provides practical foundation for exploring state representation and policy learning in bounded one-sided response settings. The trained CFR agent and source code are available at https://monopolydeal.ai."
        },
        {
            "title": "Introduction",
            "content": "In many real-world scenarios, agents must make decisions under uncertainty (Puterman, 2014), forming beliefs about hidden information while anticipating the strategies of others (Osborne and Rubinstein, 1994). An auction is familiar example: bidders must decide how much to offer without knowing their opponents private valuations. To study such decision processes in controlled, interpretable domains, researchers often turn to imperfect-information card games with similar strategic elements, including Kuhn Poker (Kuhn, 1950), UNO!, Magic: The Gathering, 1 and Legends of Runeterra. Together, these environments have supported decades of progress in computational game theory and reinforcement learning, from early breakthroughs like solving headsup limit poker (Bowling et al., 2015) to recent superhuman achievements in complex multiplayer domains (Brown and Sandholm, 2019b). The structure of agentopponent interaction in these games typically falls into three categories based on the flow of control: 1. In strictly-sequential games, players alternate single actions. For instance, in Connect Four, Alice plays red, then Bob plays black. 2. In deterministic-response games, some actions trigger fixed, rule-based outcome. For example, if Alice plays Draw Two in UNO!, Bob draws two cards. 3. In unbounded reciprocal-response games, certain actions allow alternating counterplays before the turn resolves. For instance, in Magic: The Gathering, Alice casts spell, Bob plays counterspell, and Alice may respond again, continuing until both pass. While these interaction patterns capture most well-studied card games, they do not fully represent separate, common structure: where an action briefly transfers control to an opponent who must take short, non-reciprocal sequence of actions to satisfy fixed condition before play resumes. Such response phases are ubiquitous in structured real-world interactions, such as time-sensitive options trading or regulatory compliance workflows, where one party initiates request, and the opponent must respond with bounded, non-reciprocal sequence of actions (e.g., settling margin call or invoking cure period) before the main transaction resumes. We refer to games exhibiting this structure as Bounded One-Sided Response Games (BORGs). To study this interaction pattern in compact and reproducible form, we present modified version of Monopoly Deal that isolates the BORG dynamic while remaining compatible with standard extensive-form representations. Each player takes turns playing one or more cards, choosing among actions such as acquiring properties, collecting cash, or charging rent based on owned property sets. When Rent card is played, control temporarily transfers to the opponent, who must respond by selecting sequence of Cash or Property cards to satisfy the owed amount, or by canceling the demand with Just Say No card. Once the debt is resolved, the turn concludesforming bounded, one-sided response phase that is distinct from standard deterministic or reciprocal-response models. We apply Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2007; Lanctot et al., 2009; Brown and Sandholm, 2019a) to this environment using compact state representation that accommodates the response phases. This demonstrates that established regret-minimization techniques can be applied directly to BORGs and converge reliably, without requiring novel algorithmic extensions. To support reproducible experimentation, we develop lightweight, full-stack research platform that unifies the game environment, locally parallelized CFR runtime, and human-playable web interfaceall runnable on single workstation. The system emphasizes accessibility and introspection: users can launch and monitor training runs, inspect intermediate states and policies, and easily interact with learned models. This framework lowers the barrier to entry for studying BORGs. 2 Contributions. This work makes three primary contributions: 1. Formalizes the BORG dynamic within modified Monopoly Deal environment, providing reproducible benchmark for studying bounded one-sided response games. 2. Demonstrates the tractability of BORGs by showing that existing Counterfactual Regret Minimization (CFR) techniques apply directly and converge efficiently in this setting. 3. Presents lightweight research platform that unifies the Monopoly Deal environment, parallelized CFR runtime, and web interface for accessible, reproducible experimentation."
        },
        {
            "title": "2 Background and Notation",
            "content": "We model our environment as two-player, zero-sum, imperfect-information game represented in the Extensive-Form Game (EFG) setting (Osborne and Rubinstein, 1994; Kuhn, 1953) equivalent to partially observable stochastic game (Kaelbling et al., 1998). The game is defined by the tuple = (S, Z, A, P, u, I), where denotes the set of all reachable game states (or histories/nodes in the game tree), the set of terminal states, and A(s) the set of actions available at state s. (s) is the player function indicating whose turn it is, and ui : is the utility (reward) for player i. The game is strictly zero-sum: u1(z) = u2(z). Each player {1, 2} makes decisions based on their information set Ii, which is partition of the nodes in such that all states Ii are indistinguishable to player based on their private observations. The policy σi is probability distribution over actions conditioned on the current information set Ii. Bounded One-Sided Response Operator We restrict our analysis and subsequent environment implementation to the two-player instance of Monopoly Deal. To explicitly model the BORG dynamic within the EFG framework, we introduce boundedresponse operator ϱ that captures temporary, one-sided control transfers between players. For any stateaction pair (s, ai) that triggers response, ϱ(s, ai) denotes the finite subgraph of states reachable during the opponents bounded response phase. Within this subgraph, only the opponent = may act, producing sequence {a1 } until stopping condition τ (s) = True is met. The system then transitions deterministically to the post-response state (where the BORG phase results are applied and the turn is concluded) and returns control to the next player in turn order. , . . . , ak , a2 This structure defines piecewise-alternating control pattern that generalizes strictly sequential games. Unlike deterministic-response games, which trigger fixed, rule-based outcome, or unbounded reciprocal-response games, which allow alternating counterplays of arbitrary depth, BORGs introduce 3 non-reciprocal subphase in which one player acts repeatedly until fixed condition is satisfied. This formulation highlights common but underexplored pattern: finite, one-sided sequences of actions governed by fixed termination rule."
        },
        {
            "title": "Counterfactual Regret Minimization",
            "content": "Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2007) is regret-based method for computing approximate Nash equilibria in sequential, imperfect-information games. Let σ = (σ1, σ2) denote the joint strategy profile, and let πσ (s) denote the contribution of player is actions to the probability of reaching state under the joint policy σ. We define the counterfactual reach probability of state for player as πσ (s) = j=i σj(atIt), atpath(s) the product of the action probabilities of all other players (and chance) along the path from the root to s. Each player maintains an information set policy σi(I) over actions Ai(I), where denotes the set of states sharing the same observation sequence. The counterfactual value of taking action at information set under policy σ is vσ (I, a) = zZI πσ (z[I, a]) ui(z), where ZI denotes the set of terminal states reachable after taking action at information set (I, a) is the average utility to player over all terminal states reachable after I. Equivalently, vσ taking at weighted by the reach probability of all other players (and chance). The expected utility for player at information set under policy σi is then vσ (I) = Eaσi(I)[vσ (I, a)] = σi(aI) vσ (I, a). aAi(I) At each iteration t, the instantaneous regret for each action Ai(I) is accumulated as: rt (I, a) = vσt (I, a) vσt (I),"
        },
        {
            "title": "RT\ni",
            "content": "(I, a) = t=1 rt (I, a), and policies are updated via regret matching: σT +1 (I, a) = (I, a), 0) (I, a), 0) max(RT max(RT 1 Ai(I) if denominator > 0, otherwise. Convergence is typically measured by the maximum expected regret (MER). For an imperfectinformation game, CFR minimizes this metric to guarantee that the average strategy approaches Nash equilibrium as the number of iterations increases (Zinkevich et al., 2007; Lanctot et al., 2009). The MER is computed as the counterfactual-reach-weighted average of the maximum instantaneous regret across all visited information sets: MER = maxaAi(I) Ri(I, a) πi(I) πi(I) As the average regret RT approaches zero, the average strategy σT converges to Nash equilibrium. Monte Carlo CFR. In large-scale games, the full tree traversal required by vanilla CFR is computationally infeasible. To address this, Monte Carlo Counterfactual Regret Minimization (MCCFR) (Lanctot et al., 2009) was introduced as general family of domain-independent algorithms that uses sampling to estimate the expected utilities and accumulated regrets. The core theoretical result is that MCCFR performs the same regret updates as the full-traversal CFR in expectation, thus maintaining the convergence guarantees. It encompasses various sampling schemes, including outcome sampling and external sampling. In our implementation, we utilize the External Sampling (ES-CFR) External Sampling. variant (Lanctot et al., 2009) of MCCFR, which samples only external factors (opponent and chance actions) during its recursive traversal. However, different from the standard approach, our implementation uses action-based rollouts: at the specific information set where regret is being updated, all actions Ai(I) are enumerated, and all subsequent actions by all players are single-sampled for full-game trajectories. Furthermore, to reduce variance during training, our (s) from the instantaneous regret implementation omits the counterfactual reach probability πσ calculation, relying instead on the unweighted average utility derived from the rollouts."
        },
        {
            "title": "3 Modified Monopoly Deal Environment",
            "content": "We introduce two-player, zero-sum version of Monopoly Deal (Hasbro; Hasbro) that preserves the strategic structure of the original game while simplifying its rules to isolate bounded one-sided response dynamics. Each player seeks to complete colored property sets by acquiring properties, collecting rent, and managing cash resources under imperfect-information. Card Types. The deck contains four categories of cards. Property cards come in three colorsBrown, Green, and Pinkwith fixed rent progressions and associated cash values. Cash cards serve purely as currency for settling rent, and thus buffer against paying rent with acquired properties. Rent cards (colored Brown, Green, and Pink) allow player to charge rent proportional to the number of owned properties (#) of the Rent cards color, triggering response phase if # > 0. For instance, rent for Green properties is charged in amounts $2, $4, and $7 when the renting player owns 1, 2, and 3 Green properties, respectively. Just Say No cards cancel rent demand entirely. Turns and Streaks. turn is defined by the initiating players main action (e.g., playing card or passing). Crucially, single turn encompasses the entire resolution of that action, including any subsequent bounded response phase (ϱ). Thus, one game turn may contain multiple node transitions within the EFG tree. Consecutive turns taken by the same player form streak, which continues until the player has completed fixed number (e.g. 2) of turns (or passes voluntarily). When 5 streak ends, control transfers to the opponent, initiating her next turn. Players alternate streaks throughout the game. Hand Size. Players are initially dealt five cards, and select two new cards from the deck at the start of their streak. In the original game, players are required to discard cards above seven at the end of their turn. For simplicity, we remove this rule in our implementation. Bounded One-Sided Response. When Rent card is played, control transfers temporarily to the opponent under the response operator ϱ. The opponent must satisfy fixed rent amount by paying with cash or property, canceling with Just Say No card, or yielding if unable to pay. In the original game, the initiating player may cancel Just Say No card with another Just Say No card. To maintain the BORG structure, we disallow this in our implementation. Only the responding player acts during this phase, and play deterministically resumes once the rent is settled or blocked. This produces bounded, non-reentrant subphase distinct from reciprocalresponse games, where control may alternate repeatedly. Objective and Termination. player wins upon completing fixed number of full property sets. If the deck is exhausted and neither player meets this condition, the game ends in draw. Compared to the commercial game, this version reduces property types, removes most action cards (e.g. Sly Deal, Forced Deal), and limits cash denominations. These simplifications preserve strategic depth while yielding tractable, reproducible testbed for studying BORGs."
        },
        {
            "title": "4 Related Work",
            "content": "Our work sits at the intersection of three areas of sequential decision-making research: 1) Benchmark environments for studying specific game dynamics, 2) The demonstration of algorithmic tractability, and 3) The creation of lightweight, reproducible research platforms. Prior Research on Monopoly Deal. Previous research has explored implementing gameplaying algorithms for the original version of Monopoly Deal. This work used heuristic-driven player personalities (e.g., Aggressive, Defensive), hand-coded strategic priorities and graph-traversal techniques such as Breadth-First Search (BFS) to achieve varied and competitive play (Lazarusli et al., 2015). Our work differs by employing Counterfactual Regret Minimization (CFR) to derive robust policy through intent-based state abstraction and self-play alone. To measure the competitiveness of the CFR policy, we use similar heuristic-based agents as baseline opponents. Existing Benchmark Environments. The community currently benefits from numerous platforms for studying diverse games. OpenSpiel (Lanctot et al., 2019) provides unified interface for wide collection of board, card, and stochastic games together with standard algorithms for self-play learning. RLCard (Zha et al., 2020) offers lightweight toolkit focused on card games such as Texas Holdem and UNO!, with standardized API and strong reproducibility guarantees. PettingZoo (Terry et al., 2021) generalizes the OpenAI Gym interface (Brockman et al., 2016) to multi-agent settings, introducing the Agent Environment Cycle abstraction for turn-based play. Similarly, Hanabi Learning Environment (Bard et al., 2020) offers compact, domain-specific 6 benchmark for cooperative reasoning. Together, these frameworks emphasize modularity and standardized interfaces for broad experimentation in imperfect-information games. Our environment adds new, specific domainBORGs, represented by modified version of Monopoly Dealto this growing collection. Scaling Algorithms for Complex Games. To achieve superhuman performance in the most complex imperfect-information domains, such as No Limit Texas Hold Em, researchers have developed powerful learning algorithms and large-scale training frameworks. Early breakthroughs like DeepStack (Moravcik et al., 2017) and subsequent systems like Libratus and Pluribus (Brown and Sandholm, 2019b) required scaling Counterfactual Regret Minimization (CFR) to handle state spaces too large for tabular methods. To overcome this challenge, they introduced deep function approximation techniques for generalizing across states, leading to modern neural models like Deep CFR (Brown et al., 2018) and ReBeL (Brown et al., 2020), as well as the tooling and infrastructure (e.g. distributed GPU clusters) necessary to train these systems. This class of approaches represents the state-of-the-art for tackling domains where tabular CFR is intractable. In contrast, in order to begin studying the BORG dynamic (in an abstracted state space of modest size), our work uses classic tabular CFR alone to achieve competitive performance against baseline models. Lightweight and Reproducible Research Platforms. Different from research that relies on large-scale distributed systems, parallel tradition emphasizes high-throughput, reproducible training with efficient local execution. This philosophy underlies systems such as ELF (Tian et al., 2017), which introduced flexible C++/Python framework designed for high-efficiency simulation and fast local training, often achieving breakthrough performance on single-workstation hardware. Other efforts such as Sample Factory (Petrenko et al., 2020) and SLM Lab (Keng and Graesser, 2017) adopt modular, configuration-driven designs that facilitate local parallelism and consistent experiment logging on single workstation. Furthermore, reproducibility itself has become central theme in reinforcement-learning research, with works such as (Henderson et al., 2017) highlighting the importance of seeding, implementation variance, and robust evaluation protocols. Our work follows directly in this tradition: we provide lightweight, full-stack research platform designed for single-workstation execution, allowing for fast and transparent experimentation in the bounded-response domain."
        },
        {
            "title": "5.1 Design Goals",
            "content": "The system is lightweight, full-stack research platform for studying bounded one-sided response games through Monopoly Deal. Its design emphasizes reproducibility, clarity, and accessibility over raw throughput. The following goals guided the architecture. Fast convergence. The platform achieves stable Counterfactual Regret Minimization convergence within twenty minutes of wall-clock training time on single workstation. This efficiency results from the interaction between compact state abstraction and an efficient parallel CFR runtime. Introspection and Logging. All training metrics and checkpoints are logged continuously 7 to Weights & Biases. This enables researchers to monitor convergence, inspect policy updates, and audit experiment provenance. Human interaction. Trained agents can be easily loaded into web interface for interactive play, shortening the loop between quantitative training results and qualitative behavioral evaluation. Reproducibility. Deterministic random seeds, step-indexed checkpoints, and embedded commit hashes allow training runs to be resumed or replayed exactly. This emphasis aligns with broader community efforts toward reliable reinforcement-learning evaluation (Henderson et al., 2017). State Representation. The internal data model cleanly encodes hidden information, bounded response contexts, and action legality. This representation supports not only CFR but future algorithms requiring precise serialization of the games information sets. Together, these goals motivated modular, end-to-end design combining transparent environment, parallelized runtime, and an accessible interfacemaking high-fidelity experimentation feasible on single workstation."
        },
        {
            "title": "5.2 Game Environment",
            "content": "The environment uses simplified two-player variant of Monopoly Deal with reduced card set, which allows us to focus on the BORG dynamic without the complexity of the original game. Furthermore, the number of property types and required sets are restricted to keep the number of unique information sets manageable. This allows state abstraction techniques to begin from simpler starting point, and also allows the memory footprint of the regret and policy lookup tables to be kept within single-workstation RAM."
        },
        {
            "title": "5.3 Architecture Overview",
            "content": "The system comprises two principal stacksa training stack for self-play learning and serving stack for humanmodel interactionconnected through shared JSON checkpoint artifact. The training stack is launched through single command that configures the CFR learner and dispatches self-play games across local Ray workers. central process maintains the global policy, regret, and reach-probability managers, while each worker executes complete self-play game using the current snapshot. When game finishes, the learner aggregates regret and policy updates synchronously into unified global state. Metrics are logged to Weights & Biases, and final checkpoints are serialized as human-readable JSON snapshots containing configuration and learner states. The serving stack loads trained checkpoints into FastAPI backend, exposing endpoints that utilize the same game engine used in training. React/Next.js frontend provides an interactive web interface for humanmodel play and state visualization. All game metadata and move histories are persisted to PostgreSQL database for behavioral auditing and fault tolerance (allowing the game to be resumed exactly should the server fail). The stack supports three execution modeslocal development, local containerized development, and production deployment on Google Cloud Run to https://monopolydeal.ai. 8 Training Stack Experiment Launch CFR Runtime Learner + Ray Workers Game Engine W&B / GCS Metrics & Checkpoints Serving Stack Database Game & User State FastAPI Service Model Loading & Inference Next.js Application Human vs. Model Interface o c e i"
        },
        {
            "title": "J S O N P o l",
            "content": "Figure 1: System architecture: training and serving stacks. The training stack runs CFR self-play experiments and logs metrics and checkpoints. The trained policy is exported as JSON to the serving stack, where it is loaded into FastAPI service backed by database and accessed through Next.js frontend."
        },
        {
            "title": "5.4 Data Model and State Representation",
            "content": "Game state is modeled through hierarchy of immutable, serializable data-classes that cleanly separate public and private information under imperfect observability. The top-level GameState represents players information setthe view available to that playerrather than full world state. It comprises three types of information: Player and opponent state is captured by PlayerState and OpponentState objects. PlayerState stores the acting players private hand and public holdings, while OpponentState contains only public holdings. These holdings are partitioned into distinct piles: Hand, CashPile, and PropertyPile. This asymmetry ensures that each GameState instance encodes legal information set from the acting players perspective. Bounded One-Sided Response Game (BORG) Context is contained within the TurnState, which tracks the turn_idxthe global counter of turns playedand streak_idxthe counter of turns taken by the initiating player in given streak. The BORG dynamic is defined by an optional response_ctx, which, when present, represents the deterministic, finite subgraph of the response phase. This context records three elements: 1. The initiating action (e.g., Rent) that triggered the response. 2. The pre-response state of the initiating player. 3. The sequence of response actions taken by the opponent so far. The BORG structure is formally enforced by the actions response definition, e.g. 9 RentCardResponseDefinition. This interface is designed for extensibility; to implement new BORG action, the definition must provide methods to govern the phases logic: 1. Check if the phase is necessary (response_required). 2. Define the valid action set at any point (get_valid_responses). 3. Define the precise termination condition of the phase (response_complete). Configuration and metadata includes the GameConfig (which fixes ruleset parameters) and the random_seed (which supports deterministic play). All components implement shared Serializable interface. Frozen data-classes enforce immutability, and deterministic hash of the serialized encoding serves as the information-set key, enabling consistent indexing and checkpoint recovery. Each GameState also references the abstraction_cls and resolver_cls. The default IntentStateAbstraction maps concrete actions (e.g., playing Green Property card) to intent-based abstract actions (e.g., StartNewPropertySet) suitable for policy learning. The GreedyActionResolver deterministically deduplicates equivalent choices (e.g., selecting the highest-value cash card when multiple options exist for Cash abstract action selected from the players policy)."
        },
        {
            "title": "5.5 Runtime and Reproducibility Model",
            "content": "Training runs are orchestrated through central Counterfactual Regret Minimization (CFR) learner that dispatches self-play games to local Ray workers. The execution model supports three distinct parallelism strategies, managed by ParallelismStrategy class, which balance runtime efficiency against state consistency and determinism: 1. Sequential (none): The learner runs one self-play game at time. Updates are applied synchronously after each game, ensuring strict game-order consistency and maximum determinism, but with no parallel speedup. 2. Parallel Unordered Update (parallel-unordered-update): Workers launch games concurrently and return results asynchronously. The learner applies updates (merging regret and policy deltas) immediately upon worker completion without regard to game index. This mode is the fastest, allowing for approximately 20 minutes of wall-clock training time for convergence, but it is not deterministic. 3. Parallel Batch Ordered Update (parallel-batch-ordered-update): This is the default mode and is fully deterministic. Workers launch games in batches (sized by the number of CPUs). The learner waits for the entire batch to complete, then aggregates and applies all updates synchronously in ascending game-index order before launching the next batch. This ensures state consistency and full determinism but increases the wall-clock training time required for convergence. Random Seeding Reproducibility is supported via random seeds: single run-level seed initializes pseudo-random number generators, and each individual games trajectory is seeded deterministically using function of the primary seed and the game index. Global State Management single learner process maintains the global policy, regret, and reach-probability managers as shared objects. Each worker simulates complete self-play game using snapshot of the current global state. For the parallel modes, the learner ensures state consistency by putting the updated CFR object into Rays object store after each update sequence (game or batch), making the new state available for subsequent jobs. Checkpointing State persistence is managed by writing checkpoint containing the complete learner state (policy buffers, regret tables, etc.) and full configuration metadata (ruleset, abstraction class names, and seeds). Checkpoints are written according to configurable interval, with the default being only at the final game completion. The system supports mid-run resumption by synchronously loading the highest indexed checkpoint found in the specified checkpoint store. An example checkpoint can be found in Section of the appendix. Logging At launch, the system (optionally) logs all experiment configuration arguments to Weights & Biases (W&B) as experiment metadata. The learner then logs expected regret statistics, win rates against baseline opponents, information-set update quantiles, and median policy probabilities. The game index serves as the canonical step for visualization and logging, ensuring metric uniqueness and consistency even if worker completions occur out-of-order (as is possible in the unordered parallel mode)."
        },
        {
            "title": "5.6 Human Interaction and Policy Evaluation",
            "content": "Trained policies can be inspected and played against directly through an integrated web interface built on FastAPI backend and Next.js frontend. The same Game engine used for training is exposed as FastAPI service that loads serialized policy checkpoint and executes actions through identical environment logic. The frontend connects via API endpoints and renders each state transition in real time, allowing human players to compete against trained CFR agents. The interface supports detailed state inspection at each step, including the agents policy probabilities and information-set visit counts, and adjustable agent pacing to allow human player to follow the CFR agents decision flow. All game interactions are persisted to PostgreSQL database, including action histories, game outcomes, and player metadata, allowing for post-hoc analysis of completed games. Because the serving stack reuses the same deterministic engine and serialization pathway as training, every humanmodel game can be reconstructed exactly from its stored log. The coupling between training and interaction layers elevates the system from headless experiment runner to unified research platform, enabling policies to be trained, evaluated, visualized, and examined within single environment. screenshot of the web interface can be found in Section of the appendix."
        },
        {
            "title": "6.1 Monte Carlo CFR with Variance Reduction",
            "content": "Our implementation follows the tabular Monte Carlo Counterfactual Regret Minimization (MCCFR) variant. Instead of utilizing the standard recursive traversal of External Sampling (ES-CFR), we 11 employ custom, non-recursive, action-based rollout strategy. This approach, combined with compact state abstraction, enables rapid convergence for our small-scale environment. At the target information set for player i, all available actions Ai(I) are enumerated. For each action a, we execute full-game trajectories to estimate the actions counterfactual value. During these rollouts, all subsequent actionsby the target player i, opponent j, and chanceare singlesampled from their respective policies. The opponent uses the historical average policy σj(I), and the target player uses the latest, exploration-enabled policy σi(I). Intent-based abstraction. The state space is compressed through minimal intent-based abstraction that represents only the set of available abstract actions and the streak index at each decision point. Formally, each concrete information set is mapped to reduced representation ϕ(I) = (actions(I), streak_idx), where actions(I) is the sorted tuple of abstract actions available to the acting player. The action vocabulary includes StartNewPropertySet, AddToPropertySet, CompletePropertySet, Cash, AttemptCollectRent, JustSayNo, GiveOpponentCash, GiveOpponentProperty, Pass, Yield, and Other. No additional quantitative featuressuch as hand size, property counts, or opponent statisticsare encoded. This compact representation yields roughly one hundred unique information sets in practice, which produces competitive strategies with minimal memory overhead and fast convergence. Because the abstraction key is composed only of the current set of available abstract actions and the streak index, state within the BORG phase is differentiated from non-response state solely by the unique, restricted set of valid actions (e.g., GiveOpponentCash, JustSayNo) enforced by the response operator ϱ. This implicit differentiation allows us to use Counterfactual Regret Minimization on BORGs without modifying the algorithm itself. Learning procedure. At each iteration t, the learner samples full trajectories through self-play. For each visited information set with available actions Ai(I), we estimate the counterfactual value of action via unweighted Monte Carlo averaging: ˆvσt (I, a) = X n=1 ui(znI, a), where zn denotes terminal outcome sampled after taking at and following the current policy thereafter. This average serves as the estimator for the true counterfactual value vσt (I, a). Critically, we omit the counterfactual reach probability weighting πσ (z) that appears in the standard ES-CFR definition. While this introduces small bias relative to the exact MCCFR estimator, it reduces variance while still producing competitive, fast-converging policies."
        },
        {
            "title": "The instantaneous regret rt\ni",
            "content": "(I, a) is computed based on the estimated values: rt (I, a) = ˆvσt (I, a) vσt (I), where vσt (I, a) + rt RT (I) = aAi(I) σi(aI) ˆvσt (I, a), and the policy σT +1 (I, a). The regret is accumulated over time as RT (I, a) (I, a) is updated using regret matching as defined in Section 12 2. Stabilization heuristics. Two practical simplifications further improve numerical stability and model behavior: Action Clamping for Progress. Card-playing actions are essential for advancing the game state toward win, whereas non-card actions (e.g. Pass) represent passive delay. To ensure the agent prioritizes immediate progress, we modify the regret matching calculation: if any card-playing action holds positive accumulated regret, the regret of all non-card actions is clamped to 0, ensuring that the agent only chooses passive delay when it is the only action with positive regret. The regret for the Other action, which represents an action that does not correspond to meaningful high-level intent given the game state, is similarly suppressed. Exploration. During training, actions are sampled ε-greedily with ε = 0.1, fixed across all iterations, which encourages the policy to visit all available information sets. Policy averaging. Following standard CFR practice, the Nash Equilibrium solution is derived from the average policy, σi. For practical deployment, we maintain fixed-size buffer of recent strategies to compute the average policy σi used for evaluation. The buffer size is configurable (default = 10), and the average is taken uniformly over stored policies. This produces stable, low-variance policy estimate for deployment while keeping constant memory footprint."
        },
        {
            "title": "7 Experiments",
            "content": "We evaluate the systems ability to learn stable, high-performing policies in bounded one-sided response environments through self-play on our modified Monopoly Deal environment. Experiments focus on convergence speed, policy stability, and interpretability of learned strategies under the intent-based abstraction introduced in Section 6. All experiments were conducted on single Apple M1 workstation with 8 logical cores and 16 GB RAM, achieving convergence in approximately 19 minutes of wall-clock time using the parallel-unordered-update parallelism mode."
        },
        {
            "title": "7.1 Experimental Setup",
            "content": "Training uses Monte Carlo CFR with action-based rollouts and fixed ϵ-greedy exploration rate. As detailed in Section 6, this approach samples opponent/chance actions using the current policy. Each self-play game proceeds until termination or cap of 250 turns, with policy and regret updates applied synchronously after every game. Metrics are logged to Weights & Biases and include expected regret statistics, win rates against baseline opponents, information-set update count quantiles, and median policy probabilities. The full experiment configuration is shown in Table 1. 13 Parameter Game Configuration Required property sets Initial hand size New cards per turn Turns per streak Deck Composition Property cards (Brown/Green/Pink) Cash cards (Value $1 and $3) Rent cards (Brown/Green/Pink) Special cards (Just Say No) Total cards Training Parameters Abstraction class Resolver class Training games Test games per evaluation Evaluation interval RiskAwareSelector aggressiveness RiskAwareSelector temperature Simulations per action Buffer size Exploration rate (ϵ) Maximum turns per game CPUs used Parallelism mode Random seed Value 2 5 2 2 30 (10 each) 20 (10 each) 30 (10 each) 3 83 IntentStateAbstraction GreedyActionResolver 1, 000 20 50 games 0.5 2 20 10 0.1 250 8 parallel-unordered-update 1 Table 1: Training configuration for all experiments."
        },
        {
            "title": "7.2 Regret Convergence",
            "content": "We measure convergence using the maximum expected regret (MER) as defined in Section 2. This metric is standard in CFR literature, as it weights each information set by its relevance to the opponents strategy, ensuring that regret reduction is focused on frequently reached decision points. Figure 2 shows that regret declines during early training and achieves relatively stable, low-regret state within 1, 000 games (19 minutes of wall-clock training time). 14 g e p a 0. 0.2 0.1 0 0 Max Expected Regret During Training Max expected regret Max expected regret (smoothed) 100 200 300 400 500 700 800 900 1,000 Training game index Figure 2: Decline in maximum expected regret during training, demonstrating convergence of MCCFR under bounded one-sided response dynamics."
        },
        {
            "title": "7.3 Win Rate Evaluation",
            "content": "To assess empirical performance, the trained agent was evaluated against two fixed baselines: RandomSelector, which samples uniformly from all legal actions, and RiskAwareSelector, which biases toward propertyor cash-oriented actions according to fixed aggressiveness and temperature parameters. Each baseline was tested under two initialization conditions: agent-first, where the agent always plays first, and alternating-start, where the starting player is randomized. Figure 3 shows that the agent converges to nearly 100% win rate against the random baseline and roughly 75% win rate against the risk-aware baseline."
        },
        {
            "title": "Win Rate Against Baseline Opponents",
            "content": "1 r 0.5 0 100 200 300 400 500 700 800 900 1,000 Training game index Random (agent-first) Risk-aware (agent-first) Random (alternating) Risk-aware (alternating) Figure 3: Win rate over time against baseline opponents (20 games played at each evaluation) during CFR training. Solid lines correspond to games where the agent always plays first; dashed lines indicate randomized starts. The model achieves near-perfect play against random opponents and competitive win rates against more sophisticated opponents."
        },
        {
            "title": "7.4 Update Dynamics",
            "content": "To evaluate how the agent refines its policy, we track the cumulative number of updates per information set throughout training. Figure 4 shows the quantiles of these update counts on logarithmic scale. The median update count is roughly 50, while the maximum update count is approximately 2,000. Information Set Update Count Quantiles ) c l ( o a u s I 103 102 101 100 100 200 300 400 500 700 800 900 1,000 Training game index 0th 25th 50th 75th 100th Figure 4: Distribution of cumulative infoset update counts throughout training on logarithmic scale. Typical information sets (50th percentile) are visited roughly 50 times during training, while the most common information sets (100th percentile) are visited roughly 2,000 times."
        },
        {
            "title": "7.5 Policy Evolution",
            "content": "We next examine the evolution of the learned policy by tracking the median probability of each abstract action across information sets in which it has nonzero probability mass. Intuitively, agents favor JustSayNo and GiveOpponentCash actions over GiveOpponentProperty during the response phase, and AddToPropertySet and CompletePropertySet actions over Cash during normal play. Figure 5 shows the evolution of these probabilities. 16 Median Action Probabilities Throughout Training l b n c i M 1 0.5 0 0 100 300 400 500 600 700 900 1,000 1,100 Training game index Add to Property Set Complete Property Set Just Say No Attempt Collect Rent Give Opponent Cash Start New Property Set Cash Give Opponent Property Pass Figure 5: Median probability of abstract actions available to the target player throughout training. Actions that promote property building and retention are favored."
        },
        {
            "title": "8 Discussion",
            "content": "The experimental results validate the core hypothesis: that practical, high-performing policy can be derived from minimal intent-based abstraction using Monte Carlo Counterfactual Regret Minimization in bounded one-sided response environment. Within 1, 000 games (approximately 19 minutes of wall-clock training), the agent achieves competitive play against strong baselines, validating the soundness of the abstraction and the efficiency of the training runtime. Methodologically, the results underscore the value of the ϱ-operator for formalizing the BORG dynamic. This modeling enables turn-interrupting, yet finite, interactions to be integrated into the game state without requiring algorithmic modifications to the standard CFR framework. Its structure provides valuable intermediate testbed for sequential decision-making, bridging the gap between purely sequential environments and fully reciprocal, recursive games. Finally, we address key limitation of the current BORG formulation. The agents turn-interrupting responses are functionally multi-set decision, meaning the order of actions taken within the response phase does not affect the final outcome. The agents task is thus equivalent to selecting the optimal combination of cards to play. While this simplifies the initial convergence problem, addressing this multi-set constraint is necessary to fully realize sequential decision process within the response phase itself."
        },
        {
            "title": "9 Conclusion",
            "content": "We presented lightweight, full-stack system for studying Bounded One-Sided Response Games (BORGs) through modified version of Monopoly Deal. Our work formalizes the BORG dynamic and demonstrates its tractability, showing that classical Monte Carlo CFR techniques apply directly 17 and converge reliably on modest hardware. Beyond the empirical results, the platform itself constitutes key contribution. By integrating the environment, parallelized CFR runtime, and web interface into single, accessible, and highly transparent framework, we offer system that prioritizes reproducibility and introspection for future research. Future Work Future work will focus on studying sequential decision-making in the response phase itself, using higher-fidelity state space and modern reinforcement learning techniques. 1. Sequential Response Dependencies: An important next step is to introduce action dependencies within the response phase to move beyond the current multi-set decision structure. 2. Policy Generalization and Granularity: We plan to move beyond tabular methods to modern reinforcement learning techniques for imperfect-information settings, enabling us to learn policies over larger, more granular state spaces and potentially remove the intent-based state abstraction module entirely. 3. Distributed Training: As policy complexity increases, training deep neural networks using cloud resources may become necessary. This would require integrating distributed training functionality into our framework while upholding the modularity and introspection guarantees of our current design. Together, these extensions will broaden the platforms utility as foundation for practical research in bounded one-sided response games."
        },
        {
            "title": "References",
            "content": "Bard, Nolan, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Finbarr Timbers, Ryan Engel, Michael Lerch, Michael MacLeod, et al. (2020). The Hanabi Challenge: New Frontier for AI Research. In: Artificial Intelligence 280, p. 103216. doi: 10.1016/j.artint.2019.103216. Bowling, Michael, Neil Burch, Michael Johanson, and Oskari Tammelin (2015). Heads-up limit holdem poker is solved. In: Science 347.6218, pp. 145149. doi: 10.1126/science.1259433. Brockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba (2016). OpenAI Gym. eprint: arXiv:1606.01540. Brown, Noam, Anton Bakhtin, Adam Lerer, and Qucheng Gong (2020). Combining Deep Reinforcement Learning and Search for Imperfect-Information Games. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Ed. by Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. url: https://proceedings.neurips. cc/paper/2020/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html. Brown, Noam, Adam Lerer, Sam Gross, and Tuomas Sandholm (2018). Deep Counterfactual Regret Minimization. In: eprint: arXiv:1811.00164. 18 Brown, Noam and Tuomas Sandholm (2019a). Solving imperfect-information games via discounted regret minimization. arXiv preprint arXiv:1809.04040. doi: 10.48550/arXiv.1809.04040. url: https://arxiv.org/abs/1809.04040. (2019b). Superhuman AI for multiplayer poker. In: Science 365.6456, pp. 885890. doi: 10. 1126/science.aay2400. Hasbro, Inc. (n.d.[a]). Monopoly Deal Card Game. Item G0351, Quick-Playing Family Card Game for 2-5 Players, Ages 8+. url: https://shop.hasbro.com/en-us/product/monopoly-dealcard-game-quick-playing-family-card-game-for-2-5-players-ages-8-plus/G0351. (n.d.[b]). Monopoly Deal Official Rules. url: https://monopolydealrules.com/. Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger (2017). Deep Reinforcement Learning that Matters. eprint: arXiv:1709.06560. Kaelbling, Leslie Pack, Michael L. Littman, and Anthony R. Cassandra (1998). Planning and acting in partially observable stochastic domains. In: Artificial Intelligence 101.12, pp. 99134. doi: 10.1016/S0004-3702(98)00023-X. Keng, Wah Loon and Laura Graesser (2017). SLM Lab. https://github.com/kengz/SLM-Lab. Kuhn, Harold W. (1950). Simplified two-person poker. In: Contributions to the Theory of Games. Vol. 1. Princeton University Press, pp. 97103. (1953). Extensive games and the problem of information. In: Contributions to the Theory of Games II. Vol. 28. Princeton University Press, pp. 193216. Lanctot, Marc, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, János Kramár, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis (2019). OpenSpiel: Framework for Reinforcement Learning in Games. In: CoRR abs/1908.09453. arXiv: 1908.09453 [cs.LG]. url: http://arxiv.org/abs/1908.09453. Lanctot, Marc, Kevin Waugh, Martin Zinkevich, and Michael Bowling (2009). Monte Carlo Sampling for Regret Minimization in Extensive Games. In: Advances in Neural Information Processing Systems. Lazarusli, I. A., S. Lukas, and P. Widjaja (2015). Implementation of Artificial Intelligence with 3 Different Characters of AI Player on Monopoly Deal Computer Game. In: Intelligence in the Era of Big Data. ICSIIT 2015. Ed. by R. Intan, C. H. Chi, H. Palit, and L. Santoso. Vol. 516. Communications in Computer and Information Science. Berlin, Heidelberg: Springer. doi: 10.1007/978-3-662-46742-8_11. 19 Moravcik, Matej, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling (2017). DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker. In: Science 356.6337, pp. 508513. doi: 10.1126/science.aam6960. Osborne, Martin J. and Ariel Rubinstein (1994). Course in Game Theory. MIT Press. Petrenko, Aleksei, Zhehui Huang, Tushar Kumar, Gaurav S. Sukhatme, and Vladlen Koltun (2020). Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event. Vol. 119. Proceedings of Machine Learning Research. PMLR, pp. 76527662. url: http://proceedings.mlr.press/v119/petrenko20a. html. Puterman, Martin L. (2014). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley and Sons. Terry, J, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. (2021). Pettingzoo: Gym for multi-agent reinforcement learning. In: Advances in Neural Information Processing Systems 34, pp. 1503215043. Tian, Yuandong, Qucheng Gong, Wenling Shang, Yuxin Wu, and C. Lawrence Zitnick (2017). ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games. In: Advances in Neural Information Processing Systems (NIPS). Zha, Daochen, Kwei-Herng Lai, Songyi Huang, Yuanpu Cao, Keerthana Reddy, Juan Vargas, Alex Nguyen, Ruzhe Wei, Junyu Guo, and Xia Hu (2020). RLCard: Platform for Reinforcement Learning in Card Games. In: IJCAI. Zinkevich, Martin, Michael Johanson, Michael Bowling, and Carmelo Piccione (2007). Regret Minimization in Games with Incomplete Information. In: Advances in Neural Information Processing Systems."
        },
        {
            "title": "A Information Set Key Structure and Serialization",
            "content": "The system relies on consistent, deterministic identifier for every unique decision point encountered during training, known as the information set (InfoSet) key. This key facilitates state recovery and policy lookup. The InfoSet key is structured as unique composite string following the format: {player_idx}@{abstraction_cls_name}@{abstraction_key} 20 Serialization Process. The key is generated through three-step process: 1) The game state is mapped by the chosen abstraction_cls to compact, abstract state (e.g., tuple of available abstract actions and the streak index), 2) This abstract state is deterministically serialized into canonical JSON string, and 3) An MD5 hash of the resulting JSON string is computed to create the final abstraction_key. This process guarantees that any time player reaches specific decision point, the resulting InfoSet key is identical."
        },
        {
            "title": "B Baseline Opponent Policies",
            "content": "To provide empirical validation for the trained CFR agents performance, the model is evaluated against two fixed, non-learning baseline policies. These baselines establish lower bound and reasonable heuristic benchmark for strategic play (Section 7). Both models operate using the same abstract action space and choose concrete action via the deterministic GreedyActionResolver. 1. Random Selector (RandomSelector): This policy serves as the statistical baseline, sampling uniformly from the entire set of legal abstract actions available at any given information set. 2. Risk-Aware Selector (RiskAwareSelector): This policy implements simple heuristic strategy by balancing fixed aggressiveness parameter (ranging from 0 to 1). The policy biases its action choice toward prioritizing property acquisition (aggressiveness) over banking cash (caution), using temperature parameter to control the sensitivity of this preference. This model exhibits fixed, consistent strategy that is more competitive than pure random play. During evaluation, the CFR agent is measured against both baselines under two starting conditions: always going first (agent-first) and alternating who starts the game (alternating-start)."
        },
        {
            "title": "C Checkpoint Structure",
            "content": "To ensure full reproducibility and auditability, the system serializes the complete learner state after configurable number of games into single, human-readable JSON checkpoint. The checkpoint acts as single source of truth, enabling an experiment to be resumed or played back exactly from any saved index. The checkpoint is composed of three primary nested components that store the complete CFR state, alongside metadata section: 1. Game Configuration and Metadata: Contains all static parameters defining the environment and training run, including the deck configuration, abstraction class, resolver class, simulations per action, exploration ϵ, and more. 2. Policy Manager: Maintains rolling buffer of recent policy vectors for each player, tracking the buffer capacity and the number of times each information set has been updated. Once deserialized, this data structure maps information set keys to average and current probability vectors over abstract actions. 21 3. Regret Manager: Stores the total regret and visit counts for all abstract actions (represented by their numerical indices) in every encountered information set. Once deserialized, this data structure maps information set keys and available abstract actions to sum and mean regret values. 4. Counterfactual Reach Probability Counter: Tracks the counterfactual reach probability of each information set encountered during self-play. Presently, these values are used to compute the maximum expected regret during training. An abridged example of the checkpoint structure is shown in Figure 6. 22 { \"game_config\": { \"cash_card_values\": [1,3], \"required_property_sets\": 2, \"deck_size_multiplier\": 5, \"initial_hand_size\": 5, \"...\": \"...\" }, \"abstraction_cls\": \"IntentStateAbstraction\", \"resolver_cls\": \"GreedyActionResolver\", \"sims_per_action\": 50, \"epsilon\": 0.1, \"policy_manager\": { \"buffer_size\": 10, \"update_count\": { \"1@IntentStateAbstraction@d1daef9db...\": 1, \"...\": \"...\" }, \"player_buffers\": [ { } \"buffer\": { \"0@IntentStateAbstraction@c434147c...\": [ [1.0, 0.0, 0.0, ...] ], \"...\": \"...\" }, \"buffer_size\": 10 ] }, \"regret_manager\": { \"1@IntentStateAbstraction@d1daef9db...\": { \"0\": {\"sum\": 0.22, \"n\": 1.0}, \"8\": {\"sum\": -0.22, \"n\": 1.0}, \"...\": \"...\" }, \"...\": \"...\" }, \"cf_reach_prob_counter\": { \"1@IntentStateAbstraction@d1daef9db...\": 0.00012, \"...\": \"...\" } } Figure 6: Abridged structure of the JSON checkpoint, showing the three primary components (policy_manager, regret_manager, cf_reach_prob_counter) and core configuration metadata."
        },
        {
            "title": "D Interactive Web Interface",
            "content": "A web application is provided for researchers to play against trained CFR policies. Built on FastAPI backend and Next.js frontend, this interface bridges quantitative training results with qualitative behavioral analysis (Figure 7). Figure 7: Screenshot of the interactive web interface. The interface renders the game state in real time, displays the history of actions, and allows human player to compete against the trained CFR agent."
        }
    ],
    "affiliations": []
}