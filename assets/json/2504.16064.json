{
    "paper_title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "authors": [
        "Theodoros Kouzelis",
        "Efstathios Karypidis",
        "Ioannis Kakogeorgiou",
        "Spyros Gidaris",
        "Nikos Komodakis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 6 0 6 1 . 4 0 5 2 : r Boosting Generative Image Modeling via Joint Image-Feature Synthesis Theodoros Kouzelis Archimedes, Athena RC National Technical University of Athens Efstathios Karypidis Archimedes, Athena RC National Technical University of Athens Ioannis Kakogeorgiou Archimedes, Athena RC IIT, NCSR \"Demokritos\" Spyros Gidaris valeo.ai Nikos Komodakis Archimedes, Athena RC University of Crete IACM-Forth Figure 1: ReDi: Our generative image modeling framework bridges the gap between generative modeling and representation learning by leveraging diffusion model that jointly captures low-level image details (via VAE latents) and high-level semantic features (via DINOv2). Trained to generate coherent imagefeature pairs from pure noise, this unified latent-semantic dual-space diffusion approach significantly boosts both generative quality and training convergence speed."
        },
        {
            "title": "Abstract",
            "content": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains challenge. We introduce novel generative image modeling framework that seamlessly bridges this gap by leveraging diffusion model to jointly model low-level image latents (from variational autoencoder) and high-level semantic features (from pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent imagefeature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing new direction for representation-aware generative modeling. Code available at https://github.com/zelaki/ReDi. 40 30 20 5 DiT-XL/2 DiT-XL/2+ReDi (Ours) SiT-XL/2 SiT-XL/2+REPA SiT-XL/2+ReDi (Ours) 23 speed-up 6 speed-up 100K 400K 1M 2M 4M 7M Training Iteration Figure 2: Accelerated Training Training curves (without Classifier-Free Guidance) for DiT-XL/2, SiT-XL/2 and SiT-XL/2+REPA, showing that our ReDi accelerates convergence by 23 and 6 (compared to DiT-XL/2 and SiT-XL/2+REPA, respectively)."
        },
        {
            "title": "Introduction",
            "content": "Latent diffusion models (LDMs) (Rombach et al., 2022) have emerged as leading approach for high-quality image synthesis, achieving state-of-the-art results (Rombach et al., 2022; Yao et al., 2024; Ma et al., 2024). These models operate in two stages: first, variational autoencoder (VAE) compresses images into compact latent representation (Rombach et al., 2022; Kouzelis et al., 2025); second, diffusion model learns the distribution of these latents, capturing their underlying structure. Leveraging their intermediate features, pretrained LDMs have shown promise for various scene understanding tasks, including classification (Mukhopadhyay et al., 2023), pose estimation (Gong et al., 2023), and segmentation (Li et al., 2023b; Liu et al., 2023; Delatolas et al., 2025). However, their discriminative capabilities typically underperform specialized (self-supervised) representation learning approaches like masking-based (He et al., 2022), contrastive (Chen et al., 2020), selfdistillation (Caron et al., 2021), or vision-language contrastive (Radford et al., 2021a) methods. This limitation stems from the inherent tension in LDM training - the need to maintain precise low-level reconstruction while simultaneously developing semantically meaningful representations. This observation raises fundamental question: How can we leverage representation learning to enhance generative modeling? Recent work by Yu et al. (2025) (REPA) demonstrates that improving the semantic quality of diffusion features through distillation of pretrained self-supervised representations leads to better generation quality and faster convergence. Their results establish clear connection between representation learning and generative performance. Motivated by these insights, we investigate whether more effective approach to leveraging representation learning can further enhance image generation performance. In this work, we contend that the answer is yes: rather than aligning diffusion features with external representations via distillation, we propose to jointly model both images (specifically their VAE latents) and their high-level semantic features extracted from pretrained vision encoder (e.g., DINOv2 (Oquab et al., 2024)) within the same diffusion process. Formally, as shown in Figure 1, we define the forward diffusion process as q(xt, ztxt1, zt1) for = 1, ..., , where x0 = and z0 = are the clean VAE latents and semantic features, respectively. The reverse process pÎ¸(xt1, zt1xt, zt) learns to gradually denoise both modalities from Gaussian noise. This joint modeling approach forces the diffusion model to explicitly learn the joint distribution of both precise low-level (VAE) and high-level semantic (DINOv2) features. We implement this approach, called ReDi, within the DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024) frameworks with minimal modifications to their transformer architecture: we apply standard diffusion noise to both representations, combine them into single set of tokens, and train the standard diffusion transformer architecture to denoise both components simultaneously. Compared to REPA, our joint modeling approach offers three key advantages. First, the diffusion process explicitly models both low-level and semantic features, enabling direct integration of these complementary representations. Second, our method simplifies training by eliminating the need for additional distillation objectives. Finally, during inference, our unified approach enables Representation Guidance - where the model uses its learned semantic understanding to iteratively refine generated images, improving quality in both conditional and unconditional generation. Our contributions can be summarized as follows: 1. We propose ReDi, novel and effective method that jointly models image-compressed latents and semantically rich representations within the diffusion process, significantly improving image synthesis performance. 2. We provide concrete implementation of our approach for both diffusion (DiT) and flowmatching (SiT) frameworks, leveraging DINOv2 (Oquab et al., 2024) as the source of high-quality semantic representations. 3. We also introduce Representation Guidance, which leverages the models semantic predictions during inference to refine outputs, further enhancing image generation quality. 4. We demonstrate that our approach boosts performance in both conditional and unconditional generation, while significantly accelerating convergence (see Figure 2)."
        },
        {
            "title": "2 Related work",
            "content": "Representation Learning. Various approaches aim to learn meaningful representations for downstream tasks, with self-supervised learning emerging as one of the most promising directions. Early approaches employed pretext tasks such as predicting image patch permutations (Noroozi & Favaro, 2016) or rotation angles (Gidaris et al., 2018), while more recent methods utilize contrastive learning (Chen et al., 2020; Van den Oord et al., 2018; Misra & Maaten, 2020), clustering-based objectives (Caron et al., 2020, 2018, 2019), and self-distillation techniques (Grill et al., 2020; Chen & He, 2021; Caron et al., 2021; Gidaris et al., 2021). The introduction of transformers enabled Masked Image Modeling (MIM), introduced by BEiT (Bao et al., 2022) and evolved through SimMIM (Xie et al., 2022), MAE He et al. (2022), AttMask (Kakogeorgiou et al., 2022), iBOT (Zhou et al., 2022), and MOCA (Gidaris et al., 2024), with DINOv2 (Oquab et al., 2024) achieving state-of-the-art performance through scaled models and datasets. Separately, contrastive vision-language pretraining, initiated by CLIP (Radford et al., 2021a), established powerful joint image-text representations. Subsequent models like SigLIP Zhai et al. (2023) and SigLIPv2 (Tschannen et al., 2025) refined this framework through enhanced training techniques, excelling in zero-shot settings and image retrieval (Kordopatis-Zilos et al., 2025). Building on these advances, we leverage pretrained DINOv2 visual representations to enhance image generative modeling performance. Diffusion Models and Representation Learning Due to the success of diffusion models, many recent works leverage representations learned from pre-trained diffusion models for downstream tasks (Fuest et al., 2024). In particular, intermediate U-Net (Ronneberger et al., 2015) features have been shown to capture rich semantic information, enabling tasks such as semantic segmentation (Baranchuk et al., 2022; Zhao et al., 2023), semantic correspondence (Luo et al., 2023; Zhang et al., 2023; Hedlin et al., 2023), depth estimation (Zhao et al., 2023), and image editing (Tumanyan et al., 2023). Furthermore, diffusion models have been used for knowledge transfer by distilling learned representations through teacher-student frameworks (Li et al., 2023a) or refining them via reinforcement learning (Yang & Wang, 2023). Other works have shown that diffusion models learn strong discriminative features that can be leveraged for classification (Mukhopadhyay et al., 2023; Xiang et al., 2023). In complementary direction, REPA (Yu et al., 2025) recently demonstrated that aligning the internal representations of DiT (Peebles & Xie, 2023) with powerful pre-trained visual encoder during training significantly improves generative performance. Motivated by this observation, we propose to integrate images and semantic representations into joint learning process. Multi-modal Generative Modeling Unifying the generation across diverse modalities has recently attracted widespread interest. Notably, CoDi (Tang et al., 2023) leverages diffusion model that enables generation across text, image, video, and audio in an aligned latent space. joint representation for different modalities has been shown to have great scalability properties (Mizrahi et al., 3 Figure 3: Given an input image, the VAE latent and the principal components of DINOv2 are extracted. Then both modalities are noised and fused into joint token sequence which is given as input to DiT or SiT. 2023). For video generation, WVD (Zhang et al., 2024) incorporates explicit 3D supervision by learning the joint distribution of RGB and XYZ frames. Futurist (Karypidis et al., 2025) employs future semantic prediction using multimodal masked transformer. To boost temporal coherence, VideoJam (Chefer et al., 2025) models joint image-motion representation. To capture richer spatial semantics, GEM (Hassan et al., 2024) generates paired images and depth maps. Closely related to our work, MT-Diffusion (Chen et al., 2024) learns to incorporate various multi-modal data types with multitask loss including CLIP (Radford et al., 2021b) image representations. However, they do not quantitatively assess how this impacts the generative performance."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Denoising Diffusion Probabilistic Models (DDPM) Diffusion models (Ho et al., 2020) generate data by gradually denoising noisy input. The forward process corrupts an input x0 (e.g., an image or its VAE latent) over steps by adding Gaussian noise: xt = Î±tx0 + 1 Î±tÏµ, (1) where xt is the noisy input at step t, Î±t are constants that define the noise schedule, and Ïµ (0, I) is the Gaussian noise term. Following Ho et al. (2020), the reverse process learns to denoise xt by predicting the added noise Ïµ using network ÏµÎ¸() with parameters Î¸. The training objective is: Lsimple = Ex0,Ïµ,tÏµÎ¸(xt, t) Ïµ2. Although we also include the variational lower bound loss from Ho et al. (2020) to learn the variance of the reverse process, we omit it hereafter for brevity. (2) Unless otherwise specified, we focus on class-conditional image generation throughout this work. For notational simplicity, we omit explicit class conditioning variables from all mathematical formulations. Diffusion Transformers (DiT) The DiT Peebles & Xie (2023) implements ÏµÎ¸ using Vision Transformer Dosovitskiy et al. (2021). Given the patchified input xt RLCx (L tokens of dimension Cx), the model first computes embeddings: ht = xtWemb, Wemb RCxCd . (3) The transformer processes ht RLCd to produce ot RLCd . The final noise prediction is computed as: ÏµÎ¸(xt, t) = otWdec, Wdec RCdCx . (4) 3.2 Joint Image-Representation Generation Our goal is to train single model to jointly generate images and their semantic-aware visual representations by modeling their shared probability distribution. This approach captures the interdependent 4 structures and features of both modalities. While we frame our approach using DDPM, it is also applicable to models trained with flow-matching objectives Ma et al. (2024). high-level overview of our method is depicted in Figure 3. Let denote clean image, x0 = Ex(I) RLCx its VAE tokens (produced by the VAE encoder Ex()), and z0 = Ez(I) RLCz its patch-wise visual representation tokens (extracted by pretrained encoder Ez(), e.g., DINOv2 Oquab et al. (2024))1. To match the spatial resolution of x0, we assume Ez() includes bilinear resizing operation. During training, given x0 and z0, we define joint forward diffusion processes: xt = Î±tx0 + 1 Î±tÏµx, zt = Î±tz0 + 1 Î±tÏµz, (5) where Î±t controls the noise schedule and Ïµx (0, I), Ïµz (0, I) are Gaussian noise terms of dimensions RLCx and RLCz , respectively. The diffusion model ÏµÎ¸(xt, zt, t) takes as input xt and zt, along with timestep t, and jointly predicts the noise for both inputs. Specifically, it produces two separate predictions: Ïµx Î¸ (xt, zt, t) for the image latent noise Ïµx, and Ïµz Î¸(xt, zt, t) for the visual representation noise Ïµz. The training objective combines both predictions: Ljoint = x0,z0,t (cid:104) Ïµx Î¸ (xt, zt, t) Ïµx2 2 + Î»zÏµz Î¸(xt, zt, t) Ïµz 2 (cid:105) , (6) where Î»z balances the denoising loss for zt. 3.3 Fusion of Image and Representation Tokens We explore two approaches to combine and jointly process xt and zt in the diffusion transformer architecture: (1) merging tokens along the embedding dimension, and (2) maintaining separate tokens for each modality (see Fig. 4). Both methods require only minimal modifications to the DiT architecture, specifically defining modality-specific embedding matrices emb RCzCd , along emb RCxCd and Wz Wx dec RCdCx and with prediction heads Wx Wz dec RCdCz for xt and zt respectively. Merged Tokens The tokens are embedded separately and summed channel-wise: Figure 4: An illustration of our proposed token fusion approaches: (a) The tokens of the VAE latents and the DINOv2 are merged channel-wise, (b) The tokens are concatenated along the sequence dimension. ht = xtWx emb + ztWz emb RLCd . (7) The transformer processes ht to produce ot, with predictions: Î¸ = otWz Ïµz Î¸ = otWx Ïµx dec, dec. (8) This approach enables early fusion while maintaining computational efficiency, as the token count remains unchanged. Separate Tokens Tokens are embedded separately and concatenated along the sequence dimension: emb , ztWz where [ , ] denotes sequence-wise concatenation. The transformer outputs separate representations ot = [ox emb] R2LCd , ht = [xtWx ], with predictions: , oz (9) Î¸ = ox Ïµx Wx dec, Î¸ = oz Ïµz Wz dec. (10) This method provides greater expressive power by preserving modality-specific information throughout processing, at the cost of increased computation due to increased token count. Unless stated otherwise, we use the merged tokens approach for computational efficiency. 1For notational clarity, we incorporate the patchification step (typically with 22 patches in DiT architectures) into the encoder definitions Ex and Ez. 5 3.4 Dimensionality-Reduced Visual Representation In practice, the channel dimension of visual representations (Cz) significantly exceeds that of image latents (Cx), i.e., Cz Cx. We empirically observe that this imbalance degrades performance, as the model disproportionately allocates capacity to visual representations at the expense of image latents. To address this, we apply Principal Component Analysis (PCA) to reduce the dimensionality of z0 from Cz to Cz), preserving essential information while simplifying the prediction task. The PCA projection matrix is precomputed using visual representations sampled from the training set. All visual representations in Sections 3.2 and 3.3 refer to these PCA-reduced versions. (where 3.5 Representation Guidance To ensure the generated images remain strongly influenced by the visual representations during inference, we introduce Representation Guidance. This technique during inference modifies the posterior distribution to: ËpÎ¸(xt, zt) pÎ¸(xt)p(ztxt)wr , where wr controls how strongly samples are pushed toward higher likelihoods of the conditional distribution pÎ¸(ztxt). Taking the log derivative yields the guided score function: xtlog ËpÎ¸(xt, zt) =xtlog pÎ¸(xt) + wr =xtlog pÎ¸(xt) + wr (cid:0)xtlog pÎ¸(ztxt)(cid:1) (cid:0)xtlog pÎ¸(xt, zt) xtlog pÎ¸(xt)(cid:1). (11) (12) By recalling the equivalence of denoisers and scores (Vincent, 2011), we implement this representation-guided prediction ËeÎ¸(xt, zt, t) at each denoising step as follows: ËÏµÎ¸(xt, zt, t) = ÏµÎ¸(xt, t) + wr (ÏµÎ¸(xt, zt, t) ÏµÎ¸(xt, t)) . (13) Following Ho & Salimans (2022), we train both eÎ¸(xt, zt, t) and eÎ¸(xt, t) jointly. Specifically, during training, with probability pdrop, we zero out zt (setting ÏµÎ¸(xt, t) = ÏµÎ¸(xt, 0, t)) and disable the visual representation denoising loss by setting Î»z = 0 in Equation 6."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Implementation details. We follow the standard training setup of DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024), training on ImageNet at 256 256 resolution with batch size of 256. Following ADMs preprocessing pipeline (Dhariwal & Nichol, 2021), we center-crop and resize all images to 256 256. Our experiments utilize transformer architectures B/2, L/2, and XL/2 all using 22 patch size. For unconditional generation, we simply set the number of classes to 1, maintaining the original architecture. Images are encoded into VAE latent representations using SD-VAE-FT-EMA (Rombach et al., 2022) that produces outputs with 8 spatial downsampling factor and 4 output channels. For 256 256 images, this results in 32 32 4 latent features. Through patchification with 2 2 patches, the VAE encoder Ex() yields = 256 tokens, each with Cx = 16 channels (4 channels 22 patch size). For semantic representation extraction, we employ DINOv2-B with registers (Darcet et al., 2023; Oquab et al., 2024). The 768-dimensional embeddings are reduced to 8 dimensions via PCA (trained on 76,800 randomly sampled ImageNet images). After bilinear interpolation to match the VAEs 32 32 4 spatial resolution and 2 2 patchification, the encoder Ez() produces = 256 tokens with Cz = 32 channels each (8 channels 22 patch size). Sampling. For DiT models, we adopt DDPM sampling, while for SiT models, we employ the SDE EulerMaruyama sampler. The number of function evaluations is fixed at 250 across all experiments. When using Classifier-Free Guidance (CFG) (Ho & Salimans, 2022), we apply it only to the VAE channels, with guidance scale of = 2.4 (see Figure 6). For Representation Guidance, we set the guidance scale to wr = 1.5 for models and wr = 1.1 for XL models. Evaluation. To benchmark generative performance, we report Frechet Inception Distance (FID) (Heusel et al., 2017), sFID (Nash et al., 2021), Inception Score (IS) (Salimans et al., 2016), Precision (Pre.) and Recall (Rec.) (KynkÃ¤Ã¤nniemi et al., 2019) using 50k samples and the ADMs TensorFlow evaluation suite (Dhariwal & Nichol, 2021). 6 MODEL #PARAMS ITER. FID DiT-L/2 w/ REPA w/ ReDi (ours) SiT-L/2 w/ REPA w/ ReDi (ours) DiT-XL/2 w/ REPA DiT-XL/2 w/ REPA w/ ReDi (ours) SiT-XL/2 w/ REPA w/ ReDi (ours) SiT-XL/2 w/ REPA w/ REPA w/ ReDi (ours) w/ ReDi (ours) w/ ReDi (ours) 458M 400K 23.2 458M 400K 15.6 458M 400K 10.5 458M 400K 18.5 458M 400K 9.7 458M 400K 9.4 675M 400K 19.5 675M 400K 12.3 675M 7M 9.6 675M 850K 9.6 675M 400K 8.7 675M 400K 17.2 675M 400K 7.9 675M 400K 7. 7M 8.3 675M 1M 6.4 675M 4M 5.9 675M 675M 700K 5.6 1M 5.1 675M 3M 3.7 675M Table 1: FID Comparisons. FID scores on ImageNet 256256 without ClassifierFree Guidance for DiT and SiT models of various sizes with ReDi (ours) and REPA. MODEL EPOCHS FID SFID IS PRE. REC. Autoregressive Models VAR MagViTv2 MAR 350 1080 800 1.80 1.78 1.55 - - - 365.4 0.83 319.4 0.83 303.7 0.81 Latent Diffusion Models LDM U-ViT-H/2 DiT-XL/2 MaskDiT SD-DiT SiT-XL/2 FasterDiT MDT 200 240 1400 1600 480 1400 400 1300 3.60 2.29 2.27 2.28 3.23 2.06 2.03 1.79 - 5.68 4.60 5.67 - 4.50 4.63 4.57 247.7 0.87 263.9 0.82 278.2 0.83 276.6 0.80 - - 270.3 0.82 264.0 0.81 283.0 0.81 Leveraging Visual Representations REPA ReDi (ours) ReDi (ours) 800 350 600 1.80 1.72 1.64 4.50 4.68 4. 284.0 0.81 278.7 0.77 289.3 0.65 0.57 0.57 0.62 0.48 0.57 0.57 0.61 - 0.59 0.60 0.61 0.61 0.63 0.77 Table 2: Comparison with State-of-the-art. Quantitative evaluation on ImageNet 256 256 with ClassifierFree Guidance. Both REPA and ReDi (ours) employ SiT-XL/2 as the base model. Baselines. We employ various recent generative models as baselines. Specifically, we consider the following three approaches: (a) Autoregressive Models: VAR (Tian et al., 2024), MagViTv2 (Yu et al., 2024), MAR (Li et al., 2024), (b) Latent Diffusion Models: LDM Rombach et al. (2022), U-ViT-H/2 (Bao et al., 2023), DiT (Peebles & Xie, 2023), MaskDiT (Zheng et al., 2023), SD-DiT (Zhu et al., 2024), SiT (Ma et al., 2024), FasterDiT (Yao et al., 2024), MDT (Gao et al., 2023), (c) finally, we compare with REPA (Yu et al., 2025) that also leverages visual representations to enhance generative performance. 4.2 Enhancing the performance of generative models DiT & SiT. To demonstrate the effectiveness of our approach, we present performance gains for various-sized DiT and SiT models in Table 1. Our method, ReDi, consistently delivers substantial improvements across models of different scales. Notably, DiT-XL/2 with ReDi achieves an FID of 8.7 after just 400k iterations, outperforming the baseline DiT-XL/2 trained for 7M steps. Similarly, SiT-XL/2 with ReDi reaches an FID of 7.5 at 400k iterations, surpassing the converged SiT-XL at 7M steps. Additionally, Table 2 reports results for SiT-XL/2 with Classifier-Free Guidance (CFG) Ho & Salimans (2022). Once again, ReDi yields significant improvements, achieving an FID of 1.72 in just 350 epochs, outperforming the baseline trained to convergence over 1400 epochs. Comparison with REPA. We further compare our results with REPA, which also leverages DINOv2 features to enhance generative performance. Our approach, ReDi, consistently achieves superior generative performance with both DiT and SiT as the base models. As shown in Table 1, DiT-L/2 with ReDi achives an FID of 10.5 significantly outperforming DiT-L/2 with REPA. Notably, it even surpasses REPA trained for the same number of iterations with the larger DiT-XL/2, which achieves higher FID of 12.3. Further for SiT-XL models, ReDi attains an FID of 5.1 in just 1M iterations, while REPA requires 4M iterations to reach an FID of 5.9. These results highlight the effectiveness of our method in leveraging visual representations to significantly boost generative performance. Accelerating convergence. The aforementioned results indicate that ReDi significantly accelerates the convergence of latent diffusion models. As illustrated in Figure 2, ReDi speeds up the convergence"
        },
        {
            "title": "MODEL",
            "content": "#PARAMS FID DiT-B/2 (conditional) DiT-B/2 w/ ReDi (ours) w/ ReDi+RG (ours) DiT-XL/2 (conditional) DiT-XL/2 w/ ReDi (ours) w/ ReDi+RG (ours) 130M 130M 130M 130M 675M 675M 675M 675M 43.5 69.3 51.7 47. 19.5 44.6 25.1 22.6 Table 3: Unconditional Generation Performance. FID scores for unconditional generation on ImageNet 256 256 using DiT architectures. For comparison, we include conditional generation results (shown in gray). Models trained for 400K steps. RG denotes using Representation Guidance."
        },
        {
            "title": "MODEL",
            "content": "#PARAMS FID DiT-B/2 w/ ReDi DiT-B/2 w/ ReDi+ RG DiT-XL/2 w/ ReDi DiT-XL/2 w/ ReDi+ RG 130M 130M 675M 675M 25.7 20. 8.7 5.9 Table 4: FID with Representation Guidance. FID scores on ImageNet 256256 for DiT models with ReDi and ReDi+RG. RG denotes using Representation Guidance. Models trained for 400K steps. m 2 I Figure 5: Selected samples from our SiT-XL/2 w/ ReDi model trained on IMAGENET 256 256. Images and visual representations are jointly generated by our model. We use classifier-free guidance with = 4.0. of DiT-XL/2 and SiT-XL/2 by approximately 23, respectively. Even when compared with REPA, ReDi demonstrated 6 faster convergence. Comparison with state-of-the-art generative models. Ultimately, we provide quantitative comparison between ReDi and other recent generative models using Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) in Table 2. Our method already outperforms both the vanilla SiT-XL and SiT-XL with REPA with only 350 epochs. At 600 epochs ReDi reaches an FID of 1.64. We provide qualitative results of both generated images and visual representations in Figure 5. Improving Unconditional Generation. To establish the effectiveness of our method in improving generative models, we further present experiments for unconditional generation using DiT. As shown in Table 3, our ReDi significantly improves generative performance for various model sizes. Specifically, with our ReDi FID drops from 69.3 to 51.7 for and from 44.6 to 25.1 for XL models. 4.3 Impact of Representation Guidance on generative performance. Class Conditional Generation. In Table 4 we present the impact of Representation Guidance (RG) on generative performance. We observe that for both and XL models, Representation Guidance unlocks further performance enhancements by guiding the generated image to closely follow the semantic features of DINOv2. Particularly for DiT-XL w/ ReDi the FID drops from 8.7 to 5.9. Unconditional Generation. Representation Guidance is especially useful in unconditional generation scenarios, where the absence of class or text conditioning prevents the use of Classifier-Free 8 VAE-only CFG VAE & DINOv2 CFG 5 4 F 2 1 Classifier-Free Guidance weight 4 3 Figure 6: VAE-only vs. VAE & DINOv2 CFG. FID scores for SiT-XL with ReDi (trained for 400K steps) as function of classifierfree guidance weight w, comparing two configurations: (1) applying CFG only to VAE latents (VAE-only CFG) versus (2) applying CFG to both VAE and DINOv2 representations (VAE & DINOv2 CFG). 43 45 40 35 25 w/o DINOv2 36.9 31.9 29.2 29. 27 25.7 27.5 w/o DINOv2 1 4 8 12 16 32 # Principal Components Figure 7: Effect of number of principal components. FID-50K of DiT-B/2 ReDi with different number of DINOv2 Principal Components. The vanilla DiT-B/2 is illustrated with gray. No Classifier-Free Guidance is used. MODEL #TOKENS FID ReDi w/ MR ReDi w/ SP 256 512 25.7 24. Table 5: Performance of Modality Combination Strategies. FID scores on ImageNet 256 256 without CFG for DiT-B/2 with ReDi using Separate Tokens (SP) and Merged Tokens (MR). Guidance to enhance performance. As demonstrated in Table 3, Representation Guidance enhances the performance of ReDi with both and XL models, further closing the performance gap between unconditional and conditional generation. Notably, ReDi with Representation Guidance achieves an FID of 22.6, approaching the performance of the class-conditioned DiT-XL/2, which achieves an FID of 19.5. 4.4 Analysis Dimensionality reduction ablation. We begin the analysis of our method by ablating the impact of dimensionality reduction on the visual representations, as shown in Figure 7. Initially, we observe that jointly learning as little as one principal component yields significant improvements in generative performance. Increasing the component count continues to improve performance, up to = 8, beyond which further components begin to degrade the quality of generation. This suggests an optimal intermediate subspace where compressed visual features retain sufficient expressivity to guide generation without dominating model capacity. Merged Tokens vs. Separate Tokens. In Table 5, we evaluate the effectiveness of the two explored integration strategies, Merged Tokens (MR) and Separate Tokens (SP), for joint learning of image VAE latents and visual representations, using DiT-B/2 as our base model. While both approaches achieve comparable performance gains, SP demonstrates slightly better results. This advantage comes at significant computational cost: SP doubles the transformers input sequence length by introducing 256 additional DINOv2 tokens, resulting in approximately 2 greater compute demands during both training and inference (Kaplan et al., 2020). The MR strategy, by contrast, maintains the original sequence length while delivering similar performance improvements, thereby preserving computational efficiency. VAE-only Classifier-Free Guidance. As ReDi jointly models both VAE latents and visual representations, we investigate two Classifier-Free Guidance (CFG) strategies: applying CFG exclusively to VAE latents (VAE-only CFG) versus applying it to both modalities simultaneously (VAE & DINOv2 CFG). Our experiments in Figure 6 demonstrate that VAE-only CFG achieves superior results, yielding an FID of 2.39 compared to 2.86 for the VAE & DINOv2 CFG approach. Notably, VAE-only CFG also shows greater robustness to variations in the CFG weight parameter."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we explore the relationship between semantic representation learning and generative performance in latent diffusion models. Building on recent insights, we introduced ReDi, novel framework that integrates high-level semantic features with low-level latent representations within the diffusion process. Unlike prior approaches that rely on auxiliary objectives, ReDi jointly models the two distributions. We demonstrate that this simple approach is more effective at leveraging the semantic features and leads to drastic improvements in generative performance. We further proposed Representation Guidance, novel guidance method that leverages the jointly learned semantic features to enhance image quality. Across both conditional and unconditional settings, ReDi consistently improves generation quality and accelerates convergence, highlighting the benefits of our approach. Acknowledgements This work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. Hardware resources were granted with the support of GRNET. Also, this work was performed using HPC resources from GENCI-IDRIS (Grants 2024-AD011012884R3)."
        },
        {
            "title": "References",
            "content": "Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Bao, H., Dong, L., Piao, S., and Wei, F. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V., and Babenko, A. Label-efficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=SlxSY2UZQT. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision, pp. 132149, 2018. Caron, M., Bojanowski, P., Mairal, J., and Joulin, A. Unsupervised pre-training of image features on non-curated data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 29592968, 2019. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:99129924, 2020. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Chefer, H., Singer, U., Zohar, A., Kirstain, Y., Polyak, A., Taigman, Y., Wolf, L., and Sheynin, S. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. Chen, C., Ding, H., Sisman, B., Xu, Y., Xie, O., Yao, B. Z., Tran, S. D., and Zeng, B. Diffusion models for multi-modal generative modeling. arXiv preprint arXiv:2407.17571, 2024. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. simple framework for contrastive learning of visual representations. In ICML, 2020. 10 Chen, X. and He, K. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1575015758, 2021. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. Delatolas, T., Kalogeiton, V., and Papadopoulos, D. P. Studying image diffusion features for zero-shot video object segmentation. arXiv preprint arXiv:2504.05468, 2025. Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Fuest, M., Ma, P., Gui, M., Schusterbauer, J., Hu, V. T., and Ommer, B. Diffusion models and representation learning: survey. arXiv preprint arXiv:2407.00783, 2024. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. Gidaris, S., Bursuc, A., Puy, G., Komodakis, N., Cord, M., and PÃ©rez, P. Obow: Online bag-of-visualwords generation for self-supervised learning. In CVPR, 2021. Gidaris, S., Bursuc, A., SimÃ©oni, O., VobeckÃ½, A., Komodakis, N., Cord, M., and Perez, P. MOCA: Self-supervised representation learning by predicting masked online codebook assignments. Transactions on Machine Learning Research, 2024. Gong, J., Foo, L. G., Fan, Z., Ke, Q., Rahmani, H., and Liu, J. Diffpose: Toward more reliable 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. Grill, J.-B., Strub, F., AltchÃ©, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to selfsupervised learning. Advances in Neural Information Processing Systems, 33:2127121284, 2020. Hassan, M., Stapf, S., Rahimi, A., Rezende, P., Haghighi, Y., BrÃ¼ggemann, D., Katircioglu, I., Zhang, L., Chen, X., Saha, S., et al. Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. arXiv preprint arXiv:2412.11198, 2024. He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., and Girshick, R. Masked autoencoders are scalable vision learners. In CVPR, 2022. Hedlin, E., Sharma, G., Mahajan, S., Isack, H., Kar, A., Tagliasacchi, A., and Yi, K. M. Unsupervised semantic correspondence using stable diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=sovxUzPzLN. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Kakogeorgiou, I., Gidaris, S., Psomas, B., Avrithis, Y., Bursuc, A., Karantzalos, K., and Komodakis, N. What to hide from your students: Attention-guided masked image modeling. In ECCV, 2022. 11 Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Karypidis, E., Kakogeorgiou, I., Gidaris, S., and Komodakis, N. Advancing semantic future prediction through multimodal visual sequence transformers. arXiv preprint arXiv:2501.08303, 2025. Kordopatis-Zilos, G., Stojnic, V., Manko, A., Å uma, P., Ypsilantis, N.-A., Efthymiadis, N., Laskar, Z., Matas, J., Chum, O., and Tolias, G. ILIAS: Instance-level image retrieval at scale, 2025. Kouzelis, T., Kakogeorgiou, I., Gidaris, S., and Komodakis, N. Eq-vae: Equivariance regularized latent space for improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025. KynkÃ¤Ã¤nniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Li, D., Ling, H., Kar, A., Acuna, D., Kim, S. W., Kreis, K., Torralba, A., and Fidler, S. Dreamteacher: Pretraining image backbones with deep generative models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1669816708, 2023a. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Li, Z., Zhou, Q., Zhang, X., Zhang, Y., Wang, Y., and Xie, W. Open-vocabulary object segmentation with diffusion models. 2023b. Liu, J., Hu, T., Sonke, J.-j., and Gavves, E. Beyond generation: Exploring generalization of diffusion models in few-shot segmentation. In Proceedings of the NeurIPS 2023 Workshop on Diffusion Models, 2023. URL https://neurips.cc/virtual/2023/74849. Poster. Luo, G., Dunlap, L., Park, D. H., Holynski, A., and Darrell, T. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36:4750047510, 2023. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., and Xie, S. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, pp. 2340, 2024. Misra, I. and Maaten, L. v. d. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 67076717, 2020. Mizrahi, D., Bachmann, R., Kar, O., Yeo, T., Gao, M., Dehghan, A., and Zamir, A. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36:58363 58408, 2023. Mukhopadhyay, S., Gwilliam, M., Agarwal, V., Padmanabhan, N., Swaminathan, A., Hegde, S., Zhou, T., and Shrivastava, A. Diffusion models beat gans on image classification, 2023. Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In Leibe, B., Matas, J., Sebe, N., and Welling, M. (eds.), ECCV, pp. 6984, 2016. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. URL https://openreview. net/forum?id=a68SUt6zFt. 12 Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021a. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021b. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X."
        },
        {
            "title": "Improved",
            "content": "techniques for training gans. Advances in neural information processing systems, 29, 2016. Tang, Z., Yang, Z., Zhu, C., Zeng, M., and Bansal, M. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., HÃ©naff, O., Harmsen, J., Steiner, A., and Zhai, X. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Tumanyan, N., Geyer, M., Bagon, S., and Dekel, T. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19211930, 2023. Van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv e-prints, pp. arXiv1807, 2018. Vincent, P. connection between score matching and denoising autoencoders. Neural Computation, 23(7):16611674, 2011. doi: 10.1162/NECO_a_00142. Xiang, W., Yang, H., Huang, D., and Wang, Y. Denoising diffusion autoencoders are unified selfsupervised learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1580215812, 2023. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 96539663, 2022. Yang, X. and Wang, X. Diffusion model as representation learner. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1893818949, 2023. Yao, J., Wang, C., Liu, W., and Wang, X. Fasterdit: Towards faster diffusion transformers training without architecture modification. In NeurIPS, 2024. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., Gong, B., Yang, M.-H., Essa, I., Ross, D. A., and Jiang, L. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 13 Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid Loss for Language Image Pre-Training . In ICCV, pp. 1194111952, 2023. Zhang, J., Herrmann, C., Hur, J., Polania Cabrera, L., Jampani, V., Sun, D., and Yang, M.-H. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. Zhang, Q., Zhai, S., Bautista, M. A., Miao, K., Toshev, A., Susskind, J., and Gu, J. World-consistent video diffusion with explicit 3d modeling. arXiv preprint arXiv:2412.01821, 2024. Zhao, W., Rao, Y., Liu, Z., Liu, B., Zhou, J., and Lu, J. Unleashing text-to-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 57295739, 2023. Zheng, H., Nie, W., Vahdat, A., and Anandkumar, A. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T. ibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations (ICLR), 2022. Zhu, R., Pan, Y., Li, Y., Yao, T., Sun, Z., Mei, T., and Chen, C. W. Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer. In CVPR, pp. 84358445, 2024."
        }
    ],
    "affiliations": [
        "Archimedes, Athena RC IIT, NCSR 'Demokritos'",
        "Archimedes, Athena RC National Technical University of Athens",
        "Archimedes, Athena RC University of Crete IACM-Forth",
        "valeÐ¾.ai"
    ]
}