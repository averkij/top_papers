{
    "paper_title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
    "authors": [
        "Junyoung Lim",
        "Jaewoo Ahn",
        "Gunhee Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions."
        },
        {
            "title": "Start",
            "content": "CHARTCAP: Mitigating Hallucination of Dense Chart Captioning Junyoung Lim Jaewoo Ahn Gunhee Kim Seoul National University {junyoung.lim, jaewoo.ahn}@vision.snu.ac.kr, gunhee@snu.ac.kr https://junyoung-00.github.io/ChartCap/ 5 2 0 2 5 ] . [ 1 4 6 1 3 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, highquality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, largescale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design four-stage pipeline that generates captions using only the discernible data from the chart and employ cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions. 1. Introduction Charts are powerful tools for visualizing data distributions, trends, and patterns across various domains such as science, economics, and sociology. By presenting complex information in concise and intuitive manner [23, 43], charts help readers gain meaningful insights for decision-making process. However, charts involve complex spatial relationships among various elements such as axes, labels, and legends, which can be interpreted differently depending on the chart type. Consequently, it is challenging not only for humans but also for vision language models (VLMs) to interpret complex charts [7, 21, 52]. Chart captioning is one of the core tasks to assess VLMs ability to understand charts. Its goal is to generate natural descriptions of chart image [20]. An ideal caption should (1) avoid inaccuracies about the chart [2, 17, 32], and (2) include structural description of the chart components (e.g., title or legends) as well as key insights such as major statistics (e.g., maximum or minimum values) and perceptual patterns (e.g., data trends) [33, 54]. However, existing real-world chart datasets [14, 20, 25, 30, 50] suffer from two major issues: (1) they contain extraneous information in captions that cannot be inferred from the image, and (2) they fail to sufficiently capture the essential information specific to each chart type. First, the datasets contain extraneous information within their captions, mainly because charts are usually embedded in source documents and their original captions are simply paired with chart images without verification. Captions are often written based not only on the chart itself but also on the surrounding context. As result, these captions include the information that cannot be inferred from the chart image alone (but from text in the document together), as shown in Figure 1. This poses an ill-posed problem for expecting the model to predict information absent from the chart, ultimately leading to hallucination. Second, real-world chart datasets lack sufficient structural description and key insights in text; they often omit critical information that the chart image conveys (see Figure 1). It is partly because authors do not specify some details in the captions, assuming that human readers can easily infer them from the figure [7, 35]. Such information varies depending on the chart type; for example, scatter plots highlight clusters and distributions, while line charts emphasize temporal trends and changes [9]. Hence, typespecific caption schema, which specifies how to interpret critical information for each chart type, is required to enable models to generate informative captions. To address these issues, we propose CHARTCAP to improve VLMs captioning performance while mitigating hallucinations. CHARTCAP is large-scale dataset of realworld chart images, containing 565K chart-caption pairs 1 Figure 1. Comparison of the original caption and our CHARTCAP caption. The original caption includes extraneous information (in red), such as additional contextual details (e.g., missing error bars) and references to parameters (Ep, Es, hmin), which cannot be inferred from the chart image. In contrast, CHARTCAP caption follows the line chart schema, relying on the information visible in the image. It includes structural description (in green) and key insights (in blue). The chart is sourced from [6], collected by [25], and included in CHARTCAP. that (1) exclude extraneous information not verifiable from the chart image and (2) provide structural description and key insights in dense manner by following type-specific caption schema. Drawing on research in the data visualization domain [24, 40], we define caption schema that structures the core information to be included for each chart type. We then devise an automatic pipeline that generates captions using only the data inherent in each chart image, thereby minimizing the inclusion of extraneous information. Finally, we employ cycle consistency-based [45, 64] human verification to ensure high-quality data pairs. Figure 1 illustrates comparative example between CHARTCAP and the original captions. Moreover, we propose reference-free metric, the Visual Consistency Score (VCS), for evaluating chart captions. VCS exploits recently powerful large language model (LLM) that translates caption into Python code to generate chart. Then, it compares the reconstructed chart to the ground-truth chart, overcoming the limitations of existing automated metrics, which struggle to capture the deep semantic quality of captions and are highly dependent on the quality of reference captions. In head-to-head study, VCS demonstrated high agreement rate with human judgments, outperforming existing automatic metrics such as BERTScore [62]. Extensive experiments show that VLMs fine-tuned on CHARTCAP consistently generate more informative captions with fewer hallucinations, in terms of referencebased metrics, human evaluation, and the Visual Consistency Score, surpassing both open-source and proprietary models, including InternVL2.5 [8], Phi3.5-vision [1], ChartGemma [37], ChartInstruct-Llama2 [36] and Claude 3.5 Sonnet [4]. Moreover, the captions generated by CHARTCAP fine-tuned VLMs are more preferred to human-annotated captions from VisText [54] and Chart-toText [20] by human evaluators. In summary, our main contributions are as follows: 1. We propose CHARTCAP, large-scale 565K real-world chart caption dataset that is free from extraneous information and correctly conveys structural description and key insights via type-specific caption schema. 2. We propose the Visual Consistency Score (VCS), which evaluates the quality of chart captions by assessing deep semantic meaning without relying on reference captions. 3. Through extensive experiments, we show that VLMs trained on CHARTCAP generate high-quality, informative captions with fewer hallucinations. 2. Related Work 2.1. Datasets For VLMs to generate accurate and informative chart captions, the training data should consist of real chart images, contain correct information, and capture the key insights conveyed by the chart. While synthetic datasets are scalable, models trained on synthetic data tend to exhibit limited robustness when applied to real-world charts [59, 61]. AutoChart [65], VisText [54], ChartLlama [12], and ChartSFT [39] are generated programmatically from However, Chartraw data using visualization tools. Bench [59] shows that LLaVA [31] trained on synthetic ChartLlama [12] underperforms its pre-training baseline. 2 Dataset ChartLlama [12] VisText [54] AutoChart [65] ChartGemma [37] ChartSFT [39] MMC [30] ArxivCap [25] ChartSumm [50] Chart-to-Text [20] SciCap [14] CHARTCAP Real-world charts Free from extraneous info Type-specific schema Human annotation 12K 8K 56K Data pairs 11K 12K 24K 62K 1.0M 400K 3.9M* 84K 44K 134K 565K Table 1. Comparison of CHARTCAP with public chart captioning datasets. Datasets marked with include both real-world and synthetic charts. The asterisk (*) for ArxivCap indicates that it comprises both data-driven charts and non-data-driven ones such as conceptual diagrams or scientific illustrations. The Human annotation column means the number of chart-caption pairs annotated or verified by human. CHARTCAP encompasses 565K real-world chart-captions with human verification applied on the test set. In contrast, ChartInstruct [36] collects real charts from 157 websites. However, it remains inaccessible to the public due to legal constraints. ChartSumm [50] and Chartto-Text [20] collect chart-caption pairs from Statista, Pew, and Knoema, but are relatively small and lack informativeness [35]. ChartGemma [37] leverages Gemini 1.5 Flash to regenerate captions from chart images via zero-shot prompting. MMC-Instruct [30], SciCap [14], and ArxivCap [25] use scientific papers on arXiv, resulting in larger datasets, but model-generated captions are reported to be highly hallucinated [25]. On the other hand, our CHARTCAP leverages the visual diversity of real-world charts while excluding extraneous information and utilizing caption schema, thereby enabling VLMs to acquire more robust chart comprehension skills. More systematic comparison is presented in Table 1. 2.2. Automatic Evaluation Metrics Various automatic evaluation approaches have been popularly used to measure the quality of generated captions, including BLEU [44], ROUGE [27], METEOR [5], and BERTScore [62], to name few. Despite their widespread use, these automatic evaluation metrics share common limitations. First, they fail to capture deeper linguistic or semantic nuances, captions are measured by aligning words or short phrases even if the generated text contains factual errors or incoherent logic. Second, they are highly dependent on the quality of reference captions. Even if generated caption accurately describes an image, it may be unfairly penalized if the reference caption is inaccurate or overly concise. Fundamentally, the true ground-truth (GT) in image captioning is the image itself, yet existing automatic metrics do not directly compare captions to the visual content of the image. CLIPScore [13] utilizes CLIP [49] to directly compute the semantic similarity between an image and its caption. However, it primarily measures high-level semantic alignment [26] and cannot handle long captions, limiting its reliability as comprehensive evaluation metric for tasks that require precise and detailed descriptions. To address these challenges, we introduce metric, Visual Consistency Score, which evaluates generated caption by reconstructing the chart and computing the similarity between the reconstructed chart and the GT chart. 2.3. Hallucinations in VLMs Hallucination in VLMs refers to the instances where the model generates text that does not align with the visual content [51]. One of the primary causes of hallucination is the misalignment between vision and language modalities [57]. To address this, Ciem [16] and Jiang et al. [18] employ contrastive learning with carefully crafted question-answer pairs that push misaligned representations away from correct ones. Liu et al. [29] propose containing both positive and negative instructions to strengthen model robustness. Sun et al. [53] and RLHF-V [60] refine the training process using human feedback to reward factual outputs, while HA-DPO [63], FDPO [11], and CLIP-DPO [41] leverage preference optimization by ranking and filtering generated responses. However, they address object-centric tasks, leaving chart domain relatively unexplored. CHARTCAP tightly couples textual and visual cues in chart interpretation, enabling models trained on it to exhibit fewer hallucinations in more data-driven, abstract scenarios. 3. The CHARTCAP Dataset Building large-scale chart dataset with informative captions presents several challenges. First, it requires clear definition of what information should be included in each caption. Second, an automated procedure with an appropriate schema is needed to generate high-quality captions at minimum cost. Therefore, we define type-specific caption schema and caption generation pipeline with four phases. Additionally, we facilitate efficient human verification using cycle consistency [45, 64], comparing the original chart image against reconstructed image, enabling effective quality control of the test set. The Chart Corpora. To assemble real-world charts, we collect 3.1 million chart images from ArxivCap [25], ChartSumm-Knoema [50], ChartCheck [2], and ChartQAtrain [34] as pool of data. 3.1. Defining the Caption Schema We define type-specific caption schema that outlines the structural description and key insights for each of nine chart Figure 2. An example of the four-stage pipeline for our CHARTCAP: (a) filtering non-chart images, (b) classifying the chart type and extracting titles, (c) retrieving structural components and key insights, and (d) transforming the accumulated information into coherent, sentence-level caption. types, including line, bar, pie, histogram, scatter, area, bubble, choropleth map, and treemap, guided by the prior work in the field of data visualization. As reference, Visualization Analysis and Design [40] provides rigorous framework for designing visual representations. To define the key insights for each chart type, we leverage the test blueprint from the Visualization Literacy Assessment Test (VLAT) [24], which identifies cognitive tasks for non-expert readers. Based on this framework, we minimize the ambiguity inherent in the criteria for crafting informative, high-quality captions [33, 46]. The complete schema is detailed in Appendix A. 3.2. Automated Dataset Generation Pipeline We develop four-stage pipeline, as depicted in Figure 2, to automate caption generation while balancing accuracy and computational cost via combination of open-source and proprietary models. We report the accuracy of each stage by manual inspection on 100 randomly sampled instances. Filtering Non-Chart Images. We first employ InternVL2.5-8B [8] to filter out non data-driven chart images (e.g., diagrams, schematics, illustrations). During this phase, multi-chart images are also removed, leaving us with 1.2 images out of the initial set of 3.1 M. Manual inspection confirms 100% precision, implying that no false positives are retained. Type Classification and Title Extraction. We use GPT4o to obtain each charts type and title. We filter out the charts that do not belong to the nine predefined types, leaving 577k chart images. If an explicit title is not detected, we assign the placeholder not specified to serve as negative instruction, aiming to reduce hallucinations [29]. Manual evaluation shows an accuracy of 99%, with minor error due to ambiguous title placement within the chart. Extracting Type-Specific Information. In accordance with our caption schema, we obtain structural components and key insights. We use GPT-4o for coarse-grained tasks such as identifying overall trends, while using Claude 3.5 Sonnet for more fine-grained tasks (e.g., locating exact max or min values). Preliminary experiments find that GPT-4o struggles to extract precise numerical values. Experiment details for this model selection are provided in Appendix C. If no information is extracted, it is labeled not specified. Extracted information from the previous and current stages is accumulated in semi-structured format as shown in Figure 2. Manual evaluation yields 94% accuracy, with some misinterpretation occurring in logarithmic-scale charts, scatter plots with no distinct correlations, and charts containing inset plots. Finalizing the Caption. The semi-structured data is transformed into sentence-level captions. Given the relative simplicity of this stage, we use GPT-4o-mini to perform the transformation. Manual evaluation confirms that all transformations are accurate and preserve information. 3.3. Human Verification via Cycle Consistency Despite the high performance of proprietary models, human verification remains indispensable for guaranteeing the quality of CHARTCAP. However, manually inspecting vast numbers of image-caption pairs is prohibitively timeconsuming and expensive. To address this challenge, we introduce cycle consistency-based human verification process, taking advantage of the millisecond-scale speed of human visual perception [3], as illustrated in Figure 3. We generate Python code using Claude 3.5 Sonnet to recreate chart images from captions and then compare the reconstructed chart images with the originals. Applying human verification to 68K samples, we finalize 56K test set. To validate the logical soundness of this verification process, we conduct qualitative and quantitative evaluation, detailed in Appendix D. Our findings are as follows. 1. Compared to direct image-caption comparison, our verification process is approximately 24 times faster while maintaining an F1 score of 95%. 2. Our verification process ensures both caption correctness and informativeness, making it well-suited for CHARTCAPs dense-captioning objectives. 4 the average similarity across all samples: Visual Consistency Score ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Sim(Ii, ˆIi). OCRScore. To evaluate how well textual elements are preserved, Optical Character Recognition (OCR) can be applied to both Ii and ˆIi. Let Ti and ˆTi be the sets of text strings extracted from Ii and ˆIi, respectively. The OCRScore is as an F1 score based on precision (P ) and recall (R): = (cid:80)N (cid:12) (cid:12)Ti ˆTi (cid:12) (cid:12) ˆTi (cid:12) (cid:12) i=1 i=1 (cid:80)N (cid:12) (cid:12) , = (cid:80)N (cid:12) (cid:12) (cid:12) (cid:12)Ti ˆTi (cid:12) (cid:12)Ti (cid:12) (cid:12) i= i=1 (cid:80)N Figure 3. An illustration of the cycle consistency-based human verification for CHARTCAP. The original chart image (left) is compared with reconstructed one (right) using Python code from the caption (bottom). This process enables efficient human verification by assessing the accuracy and informativeness of the generated captions through visual consistency. 3.4. The Visual Consistency Score How can we evaluate that generated caption is faithful to its corresponding chart image? We believe that the best caption would correctly reproduce the chart, analogous to that best generative model (x) is the one that can generate data themselves. This intuition leads us to propose new captioning evaluation metric named the Visual Consistency Score (VCS), thanks to recent prominence of LLMs. Unlike natural images, charts have unique characteristic: they can be deterministically generated from an intermediate modalitynamely, code. Leveraging this property, we convert given caption Ci into code Gi, subsequently producing corresponding chart image ˆIi. By measuring the similarity between this generated chart image ˆIi and the original chart image Ii, the VCS quantitatively evaluates the accuracy and informativeness of the caption Ci. The VCS is computed by two-stage procedure, code generation and image comparison. Given caption Ci, an LLM is used to generate Matplotlib code Gi for recreating the chart. If Gi fails to execute, the code and runtime error message are supplied back to the LLM for debugging. This process is repeated until code execution succeeds, yielding valid Gi, which is then executed to generate the chart image ˆIi. The similarity between Ii and ˆIi is computed using cosine similarity with vision encoder. Finally, the VCS is 5 OCRScore = 2 + . Both VCS and OCRScore exhibit the highest agreement rates with human judgments among automated evaluation metrics such as BERTScore, demonstrating their practicality as reliable, scalable, and effective metrics for evaluating chart-caption quality. Detail of validation experiment is provided in Appendix E. We use Claude 3.5 Sonnet for code generation, due to its superior performance in generating code [19]. For the vision encoder, we employ three variants of SigLIP2 [56], each at resolution of 512, which achieves state-of-theart performance across variety of computer vision benchmarks [10, 28, 47, 55]. For OCR, we use PaddleOCR [42]. 3.5. Dataset Analysis Visual Consistency Score. We evaluate the Visual Consistency Score and OCRScore on 1K samples from each dataset. The results are presented in Table 2. CHARTCAP achieves the highest scores among all datasets, indicating that its captions are the most accurate to reconstruct the original chart information. The results indirectly reflect two key aspects: informativeness and the exclusion of extraneous information. Caption informativeness can be partially assessed by the average word count as CHARTCAP contains the longest captions with 231.1 words on average. Human Evaluation. We conduct head-to-head human evaluation by recruiting three annotators via Amazon Mechanical Turk (AMT), comparing 100 samples from CHARTCAP and ChartSumm [50] (the best dataset except ours). Each sample is evaluated based on informativeness, accuracy, fewer hallucinations, and overall preference. Details of human evaluation can be found in Appendix J. As illustrated in Figure 4, CHARTCAP consistently outperforms ChartSumm across all evaluated aspects, demonstrating higher overall quality recognized by human. Dataset Visual Consistency Score Large So400M Base OCRScore Word Count ArxivCap ChartSumm Chart-to-Text SciCap 0.7561 0.8940 0.6925 0.7861 0.7421 0.9008 0.7089 0.8015 0.7999 0.8898 0.7127 0. CHARTCAP 0.8983 0.9089 0.9133 0.1781 0.2635 0.0951 0.1843 0. 43.7 45.4 62.2 34.5 231.1 Table 2. Comparison of real-world chart datasets. The terms Large, So400M, and Base indicate three versions of the SigLIP2 encoder [56]: SigLIP2-{large, so400m, base}-512. Figure 4. Results of the head-to-head human evaluation comparing CHARTCAP with ChartSumm [50]. 4. Experiments We demonstrate the effectiveness of our CHARTCAP dataset: first, we show that VLMs fined-tuned on CHARTCAP attain strong dense captioning performance in terms of reference-based metrics, human evaluation, and the Visual Consistency Score. Second, we present that CHARTCAPtrained captioning models show compelling zero-shot captioning on two human-annotated benchmarks, VisText [54] and Chart-to-Text [20]. Base Models. We experiment with open-source, chart expert, and proprietary captioning models. For open-source models, we use InternVL2.5-78B [8], InternVL2.5-38B, InternVL2.5-8B, and Phi3.5-visionInternVL2.5-26B, 4B [1]. For chart expert models, we employ ChartGemma2B [37] and ChartInstruct-Llama2-7B [36]. For proprietary models, we use the Claude 3.5 Sonnet [4], which not only achieves the best performance in our dataset but also reports the state-of-the-art performance on ChartQA [34] and CharXiv [58]. Additional baselines are provided in Appendix H. Experiment Setup and Metrics. All models are prompted with the same instruction: Please provide detailed caption for the chart. along with the chart image as input. For metrics, we use SacreBLEU [48], ROUGE [27], METEOR [5], and BERTScore [62], with our Visual Consistency Score and OCRScore. Training Settings. We perform supervised fine-tuning using LoRA fine-tuning [15] on InternVL2.5-8B and 6 Figure 5. Results of human evaluation results comparing Phi3.5Vision-4BCHARTCAP against Claude 3.5 Sonnet (top) and Phi3.5Vision-4B (bottom) on the CHARTCAP test set. Phi3.5-vision-4B on the CHARTCAP training set (509K). We also fine-tune Phi3.5-vision-4B using 250K original captions from ArxivCap, ChartSumm-Knoema, and ChartCheck. Additionally, We fine-tune Phi-3.5-Vision-4B on the entire training set of ChartSumm. Fine-tuned models are denoted with the name of the training dataset (e.g., Phi3.5-Vision-4BCHARTCAP). 4.1. Results on the CHARTCAP Reference-based Metrics. Table 3 presents the results of the reference-based metrics on the CHARTCAP test Both InternVL2.5-8BCHARTCAP and Phi3.5-Visionset. 4BCHARTCAP achieve higher scores than all baseline models across all evaluation metrics. In contrast, Phi3.5Vision-4BOriginal and Phi3.5-Vision-4BChartSumm records significantly lower scores, even shows degradation of its base model. These results indicate that our fine-tuned models generate captions that align closely with the human-verified reference captions of CHARTCAP, which accurately capture the structural components and key insights of the charts sufficiently. Human Evaluation. We conduct human evaluation to assess caption accuracy, informativeness, and the extent of hallucination, as reference-based metrics do not measure absolute caption quality and struggle to effectively assess the degree of hallucination [22, 38]. For human evaluation, we select Phi3.5-Vision-4BCHARTCAP that shows the highest score on the reference-based metrics on CHARTCAP and compare head-to-head with one proprietary model (Claude 3.5 Sonnet) and one open-source model (Phi3.5Vision-4B). We randomly sample 100 captions generated Model Reference-based Metrics Visual Consistency Score OCRScore sacreBLEU ROUGE-L METEOR BERTScore Large So400M Base Claude 3.5 Sonnet 5.35 0. 0.2131 0.6606 0.8834 0.8771 0.8976 0. Proprietary Model ChartGemma-2B ChartIns-Llama2-7B InternVL2.5-78B InternVL2.5-38B InternVL2.5-26B InternVL2.5-8B InternVL2.5-8BCHARTCAP Phi3.5-Vision-4B Phi3.5-Vision-4BOriginal Phi3.5-Vision-4BChartSumm Phi3.5-Vision-4BCHARTCAP 0.73 0.62 8.15 5.88 5.32 3.60 19.47 8.41 0.09 1.31 23.82 Chart Expert Models 0.1607 0.1144 0.1082 0.0814 0.5946 0.5157 0.8314 0.6947 0.8184 0.6759 0.8565 0. 0.2351 0.1830 Open-source Models 0.2510 0.2331 0.2350 0.1770 0.3393 0.2466 0.0782 0.1509 0.3900 0.2336 0.2020 0.1972 0.1577 0.3729 0.2501 0.0384 0.1322 0.4084 0.6642 0.6551 0.6546 0.6139 0.7238 0.6626 0.5066 0.6008 0.7427 0.8841 0.8790 0.8751 0.8485 0.8913 0.8433 0.7782 0.8002 0. 0.8766 0.8700 0.8674 0.8372 0.8828 0.8323 0.7655 0.7873 0.8829 0.8985 0.8965 0.8873 0.8720 0.9068 0.8696 0.8137 0.8207 0.9092 0.4677 0.4300 0.4144 0.3456 0.5089 0.4875 0.1438 0.2042 0.5179 Table 3. Results of reference-based metrics, Visual Consistency Scores, and OCRScore on the CHARTCAP test set. by each model and recruit three crowd workers via AMT to select the better caption based on three aspects: (1) informativeness, (2) accuracy, and (3) fewer hallucinations. Further details on the human evaluation are provided in the Appendix J. As shown in Figure 5, Phi3.5-Vision-4BCHARTCAP ranks consistently higher in all three human evaluation criteria. This consistency suggests that human judges feel Phi3.5Vision-4BCHARTCAP generates more informative and accurate captions with fewer hallucinations compared to baseline models. Notably, despite having smaller model size, Phi3.5-Vision-4BCHARTCAP surpasses the strong proprietary model, Claude 3.5 Sonnet, according to human judgments. This highlights fine-tuning on high-quality data could overshadow the model scale. The Visual Consistency Score. Table 3 also presents the Visual Consistency Score and OCRScore for the CHARTCAP test set. Both InternVL2.5-8BCHARTCAP and Phi3.5Vision-4BCHARTCAP exhibit higher Visual Consistency Score and OCRScore relative to all baselines, signifying that the captions they produce align more closely with the groundtruth chart structure and text elements. This stronger grounding in chart content further explains why they offer more accurate, informative, and low-hallucination captions than non-fine-tuned variants or other baselines. 4.2. Results on Other Human-Verified Benchmarks We evaluate the zero-shot performance of previous captioning models on other human-verified benchmarks. We first test on the entire VisText test set, consisting of synthetic charts with human-authored captions following the caption schema from [33]. We also evaluate the models on the 1K PEW subset of Chart-to-Text, real-world dataset whose subset has undergone human verification. The experiment setup is the same as the previous experiment. Human Evaluation. We conduct human evaluation on 100 samples from the VisText test set, comparing captions generated by Phi3.5-Vision-4BCHARTCAP with Claude 3.5 Sonnet and the human-authored ground-truth captions, under the same evaluation protocol. As shown in Figure 6, Phi3.5-Vision-4BCHARTCAP outperforms both the ground-truth captions and Claude 3.5 Sonnet across all three evaluation aspects. Interestingly, human annotators judge that the model fine-tuned on CHARTCAP can generate better chart descriptions than human-authored ground-truth captions across all axes by large margin. The Visual Consistency Score. Table 4 5 present the Visual Consistency Score for the VisText test set, and the PEW subset of the Chart-to-Text dataset, respectively. As shown in both tables, InternVL2.5-8BCHARTCAP and Phi3.5Vision-4BCHARTCAP achieve the highest Visual Consistency Scores and competitive OCRScores among all baseline models. Again, these two models surpass even the humanannotated ground-truth captions in accurately reconstructIn particular, for the Vising the original chart images. Text dataset, only the CHARTCAP-trained models outperform the human-authored ground-truth captions. The results also highlight the generalizability and effectiveness of captioning models trained with CHARTCAP. Qualitative Examples. Figure 7 compares the captions and their reconstructed charts generated by Phi3.5Vision-4BCHARTCAP, human-authored ground-truth caption, and Claude 3.5 Sonnet for chart from VisText. The caption generated by Phi3.5-Vision-4BCHARTCAP provide precise and detailed descriptions of both the charts structural components and data. As result, its reconstructed 7 Model Visual Consistency Score OCRScore Ground-truth Caption Claude 3.5 Sonnet InternVL2.5-8B InternVL2.5-8BCHARTCAP Phi3.5-Vision-4B Phi3.5-Vision-4BCHARTCAP Large 0.9172 0.8970 0.9093 0.9401 0.8809 0.9443 So400M 0.9151 0.9008 0.9082 0.9355 0.8814 0.9382 0.3407 0.3286 0.3172 0.3360 0.3826 0.3414 Table 4. Visual Consistency Scores and OCRScore on the VisText test set. Figure 7. Qualitative examples from VisText, comparing (a) the ground-truth chart image with captions and their reconstructed charts from the captions of (b) human-authored ground-truth, (c) Phi3.5-Vision-4BCHARTCAP, and (d) Claude 3.5 Sonnet. Model Visual Consistency Score OCRScore Ground-truth Caption Claude 3.5 Sonnet InternVL2.5-8B InternVL2.5-8BCHARTCAP Phi3.5-Vision-4B Phi3.5-Vision-4BCHARTCAP Large 0.6925 0.7495 0.7362 0.7946 0.7370 0.7999 So400M 0.7089 0.7616 0.7478 0.8013 0.7490 0.8075 0. 0.1603 0.1272 0.1833 0.1786 0.1789 Table 5. Visual Consistency Scores and OCRScore on the PEW subset of the Chart-to-Text. quality by measuring the consistency between the original charts and the ones generated from captions. Models fine-tuned on CHARTCAP substantially enhance the quality of chart captions, even generates better caption than strong proprietary baseline and human annotated captions. As limitation, CHARTCAP utilizes caption-schema built upon the blueprint of nine chart types defined by VLAT [24], which restricts the diversity of chart types covered. It is an interesting future work to expand the captioning schema and integrate it into the proposed pipeline, which could enable the creation of more diverse largescale dataset. Figure 6. Human evaluation results comparing Phi3.5-Vision4BCHARTCAP against ground-truth captions (top) and Claude 3.5 Sonnet (bottom) on the VisText test set. chart closely resembles the original ground-truth, and consequently achieve the highest VCS and OCRScore. In contrast, the human-authored caption describes data trends in simplified manner (e.g., merely stating that values increase), resulting in reconstructed charts that exhibit overly simplified data trends. Similarly, the caption generated by Claude 3.5 Sonnet describes the data trend without sufficient detail and incorrectly classifies the chart type, leading to reconstructed chart that not only simplifies the trend but also displays an incorrect chart type. 5. Conclusion We introduced CHARTCAP, large-scale dataset of 565K real-world chart images paired with type-specific captions that include both structural components and key insights in dense manner while minimizing extraneous information. We constructed CHARTCAP via four-phase caption generation pipeline with systematically devised caption-schema and cycle consistency-based human verification. We also proposed the Visual Consistency Score to assess caption"
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the anonymous reviewers and Chaeyoung Lim for their valuable comments. This work is supported by the Samsung Electronics University R&D program [Efficient fine-tuning of large multimodal models for domainspecific figure description], Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS2019-II191082, SW StarLab, No. RS-2022-II220156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation, and No. RS-2021-II211343, Artificial Intelligence Graduate School Program of Seoul National University), National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C2005573), and Seoul R&BD Program (VC230004) through the Seoul Business Agency (SBA) funded by The Seoul Metropolitan Government. Gunhee Kim is the corresponding author."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. ArXiv preprint, abs/2404.14219, 2024. 2, 6 [2] Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sahar Tahmasebi, Oana Cocarascu, and Elena Simperl. Chartcheck: Explainable fact-checking over real-world chart images. In Findings of the Association for Computational Linguistics ACL 2024, pages 1392113937, 2024. 1, 3 [3] Kaoru Amano, Naokazu Goda, Shinya Nishida, Yoshimichi Ejima, Tsunehiro Takeda, and Yoshio Ohtani. Estimation of the timing of human visual perception from magnetoencephalography. Journal of Neuroscience, 26(15):39813991, 2006. 4 [4] Anthropic. Introducing Claude 3.5 Sonnet, 2024. 2, 6 [5] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, 2005. Association for Computational Linguistics. 3, 6 [6] Bernd Burghardt and Alexander K. Hartmann. Rna secondary structure design. Physical Review E, 75(2), 2007. 2 [7] Sandra Carberry, Stephanie Elzer, and Seniz Demir. Information graphics: an untapped resource for digital libraries. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 581588, 2006. 1 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. ArXiv preprint, abs/2412.05271, 2024. 2, 4, 6 [9] William Cleveland. The elements of graphing data. Wadsworth Publ. Co., 1985. 1 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: large-scale hierarchical image In 2009 IEEE Computer Society Conference on database. Computer Vision and Pattern Recognition (CVPR 2009), 2025 June 2009, Miami, Florida, USA, pages 248255. IEEE Computer Society, 2009. 5 [11] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1813518143. AAAI Press, 2024. [12] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. ArXiv preprint, abs/2311.16483, 2023. 2, 3 [13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation In Proceedings of the 2021 metric for image captioning. Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 3 [14] Ting-Yao Hsu, Lee Giles, and Ting-Hao Huang. SciCap: Generating captions for scientific figures. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 32583264, Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 1, 3 [15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 6 [16] Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun. Ciem: Contrastive instruction evaluation method for better instruction tuning. ArXiv preprint, abs/2309.02301, 2023. 3 [17] Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi Fung, Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, and Heng Ji. Do lvlms understand charts? analyzing and correcting factual errors in chart captioning. ArXiv preprint, abs/2312.10160, 2023. 1 [18] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703627046, 2024. [19] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe9 bench: Can language models resolve real-world github issues? ArXiv preprint, abs/2310.06770, 2023. 5 [20] Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 40054023, Dublin, Ireland, 2022. Association for Computational Linguistics. 1, 2, 3, 6 [21] Dae Hyun Kim, Enamul Hoque, and Maneesh Agrawala. Answering questions about charts and generating visual exIn CHI 20: CHI Conference on Human Facplanations. tors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pages 113. ACM, 2020. 1 [22] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency of abIn Proceedings of the 2020 stractive text summarization. Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 93329346, Online, 2020. Association for Computational Linguistics. [23] Jill Larkin and Herbert Simon. Why diagram is (sometimes) worth ten thousand words. Cognitive science, 11(1): 65100, 1987. 1 [24] Sukwon Lee, Sung-Hee Kim, and Bum Chul Kwon. Vlat: Development of visualization literacy assessment test. IEEE transactions on visualization and computer graphics, 23(1):551560, 2016. 2, 4, 8 [25] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. ArXiv preprint, abs/2403.00231, 2024. 1, 2, 3 [26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large visionlanguage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, Singapore, 2023. Association for Computational Linguistics. 3 [27] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, 2004. Association for Computational Linguistics. 3, 6 [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 5 [29] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large In The multi-modal models via robust instruction tuning. Twelfth International Conference on Learning Representations, 2023. 3, [30] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: Advancing multimodal chart understanding with In Proceedings of the 2024 large-scale instruction tuning. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12871310, Mexico City, Mexico, 2024. Association for Computational Linguistics. 1, 3 [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [32] Leo Yu-Ho Lo, Ayush Gupta, Kento Shigyo, Aoyu Wu, Enrico Bertini, and Huamin Qu. Misinformed by visualization: What do we learn from misinformative visualizations? In Computer Graphics Forum, pages 515525. Wiley Online Library, 2022. 1 [33] Alan Lundgard and Arvind Satyanarayan. Accessible visualization via natural language descriptions: four-level model of semantic content. IEEE transactions on visualization and computer graphics, 28(1):10731083, 2021. 1, 4, 7 [34] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 3, 6 [35] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. UniChart: universal visionlanguage pretrained model for chart comprehension and reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14662 14684, Singapore, 2023. Association for Computational Linguistics. 1, [36] Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartinstruct: Instruction tuning for chart comprehension and reasoning. ArXiv preprint, abs/2403.09028, 2024. 2, 3, 6 [37] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. ArXiv preprint, abs/2407.04172, 2024. 2, 3, 6 [38] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive sumIn Proceedings of the 58th Annual Meeting of marization. the Association for Computational Linguistics, pages 1906 1919, Online, 2020. Association for Computational Linguistics. 6 [39] Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. ArXiv preprint, abs/2401.02384, 2024. 2, 3 [40] Tamara Munzner. Visualization analysis and design. CRC press, 2014. 2, [41] Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Clip-dpo: Vision-language models as source of preference for fixing hallucinations in lvlms. In European Conference on Computer Vision, pages 395413. Springer, 2025. 3 [42] PaddleOCR. PaddleOCR Documentation, 2024. 5 10 [43] Anshul Vikram Pandey, Anjali Manivannan, Oded Nov, Margaret Satterthwaite, and Enrico Bertini. The persuasive power of data visualization. IEEE Transactions on Visualization and Computer Graphics, 20(12):22112220, 2014. 1 [44] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine In Proceedings of the 40th Annual Meeting of translation. the Association for Computational Linguistics, pages 311 318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. 3 [45] Fatemeh Pesaran Zadeh, Juyeon Kim, Jin-Hwa Kim, and Gunhee Kim. Text2Chart31: Instruction tuning for chart generation with automatic feedback. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1145911480, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2, 3 [46] Steven Piantadosi, Harry Tily, and Edward Gibson. The communicative function of ambiguity in language. Cognition, 122(3):280291, 2012. [47] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correIn 2015 spondences for richer image-to-sentence models. IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2641 2649. IEEE Computer Society, 2015. 5 [48] Matt Post. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium, 2018. Association for Computational Linguistics. 6 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 8748 8763. PMLR, 2021. 3 [50] Raian Rahman, Rizvi Hasan, Abdullah Al Farhad, Md Tahmid Rahman Laskar, Md Hamjajul Ashmafee, and Abu Raihan Mostofa Kamal. Chartsumm: comprehensive benchmark for automatic chart summarization of long and short summaries. ArXiv preprint, abs/2304.13620, 2023. 1, 3, 5, 6 [51] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035 4045, Brussels, Belgium, 2018. Association for Computational Linguistics. 3 [52] Chase Stokes, Vidya Setlur, Bridget Cogley, Arvind Satyanarayan, and Marti Hearst. Striking balance: Reader takeaways and preferences when integrating text and charts. IEEE Transactions on Visualization and Computer Graphics, 29(1):12331243, 2022. 1 [53] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. ArXiv preprint, abs/2309.14525, 2023. 3 [54] Benny Tang, Angie Boggust, and Arvind Satyanarayan. VisText: benchmark for semantically rich chart captioning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 72687298, Toronto, Canada, 2023. Association for Computational Linguistics. 1, 2, 3, 6 [55] Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. 5 [56] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 5, 6 [57] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language ArXiv models with instruction contrastive decoding. preprint, abs/2403.18715, 2024. 3 [58] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. ArXiv preprint, abs/2406.18521, 2024. [59] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts. ArXiv preprint, abs/2312.15915, 2023. 2 [60] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816, 2024. 3 [61] Xingchen Zeng, Haichuan Lin, Yilin Ye, and Wei Zeng. Advancing multimodal large language models in chart question answering with visualization-referenced instruction tuning. IEEE Transactions on Visualization and Computer Graphics, 2024. 2 [62] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation In 8th International Conference on Learning with BERT. Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 2, 3, 6 [63] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. ArXiv preprint, abs/2311.16839, 2023. 3 [64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 22422251. IEEE Computer Society, 2017. 2, 3 [65] Jiawen Zhu, Jinye Ran, Roy Ka-Wei Lee, Zhi Li, and Kenny Choo. AutoChart: dataset for chart-to-text generation task. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 16361644, Held Online, 2021. INCOMA Ltd. 2, 3 12 CHARTCAP: Mitigating Hallucination of Dense Chart Captioning"
        },
        {
            "title": "Supplementary Material",
            "content": "Chart Type Title Axes Categories Bubble Legends Labels Line Bar Pie Histogram Scatter Area Bubble Table 6. Caption schema specifying the structural elements required for each chart type. For readability, choropleth maps and treemaps are excluded due to their distinct characteristics. Choropleth maps include title, base map, color scale, geographic labels, data classes, and north arrow. Treemaps include title, tiles, hierarchy levels, and color coding. A. Type-specific Caption Schema A.1. Caption Schema for Structural Description The caption schema that defines the structural elements included in each chart type is shown in Table 6. A.2. Caption Schema for Key Insights The caption schema that specifies the key insights to be included for each chart type can be found in Table 7. In the case of Retrieve Value, the task involves reading data points to answer question, making it inapplicable to the captioning task. Therefore, if the data points had labels, those were extracted; otherwise, the initial, middle, and final data values were extracted in the caption. B. Prompt Demonstrations We present the prompts used in the dataset-generation pipeline and in chart regeneration for the cycle-consistencybased human-verification process and the Visual Consistency Score. Filtering Non-Chart Images: See Table 8 Type Classification and Title Extraction: See Table 9 Retrieving Type-Specific Information: See Tables and 11 Finalizing the Caption: See Table 12 Chart Regeneration: See Table 13 Code Debugging: See Table 14 C. Task Allocation Experiment C.1. Experiment Setup This experiment was designed to optimize the effective utilization of GPT-4o and Claude 3.5 Sonnet in extracting Figure 8. Accuracy of GPT-4o and Claude 3.5 Sonnet on coarsegrained tasks and fine-grained tasks. accurate information based on the caption schema. The tasks defined in Tables 6 and 7 were categorized into finegrained and coarse-grained tasks, and the performance of each model was evaluated accordingly. Coarse-grained tasks require broad attention across an image, such as understanding of overall data trends or comparisons between data series: Type Classification, Title, Category, Bubble, Legend, Label, Make Comparisons, Find Correlations / Trends, and Characterize Distribution. In contrast, fine-grained tasks require more localized attention, such as extracting precise numerical values or reading specific data points: Axes, Retrieve Value, Find Extremum, Determine Range, Find Clusters, and Find Anomalies. The experiment was conducted on 100 randomly sampled data, and the performance of both models was manually evaluated for each task category. The prompts used in this experiment can be found in Tables 10 and 11. C.2. Results The experimental results are shown in Figure 8. For coarsegrained tasks, GPT-4o achieved an accuracy of 96%, while Claude 3.5 Sonnet achieved 93%, with GPT-4o demonstrating slight advantage. The primary sources of error for Claude 3.5 Sonnet were in type classification and title generation, where the model tended to introduce hallucinated information by attempting to predict the overall theme of chart. Based on these findings, GPT-4o was selected for coarse-grained tasks in our pipeline. 1 Chart Type Retrieve Value Find Extremum Make Comparison Determine Range Find Correlations / Trend Characterize Distribution Find Clusters Find Anomalies Line Bar Pie Histogram Scatter Area Bubble Choropleth Map Treemap Table 7. Caption schema specifying the key insights required for each chart type. Filtering non-chart images. Please determine whether the image contains single, data-driven chart only. data-driven chart is visual representation directly based on numerical data. Note that an inset chart (a smaller chart embedded within larger chart) is not considered multi-chart. - If the image consists exclusively of single data-driven chart (with no additional visuals, such as natural images, illustrations, conceptual diagrams, or schematics) and does not contain multiple subplots, respond with: Single-Chart: yes - If the image contains any non-data-driven elements (e.g., natural images, illustrations, conceptual diagrams, schematics) or features multiple charts/subplots, respond with: Single-Chart: no Follow the exact response format: Single-Chart: Table 8. Prompt used for filtering non-chart images. For fine-grained tasks, GPT-4o achieved an accuracy of 68%, whereas Claude 3.5 Sonnet significantly outperformed it with an accuracy of 94%. The primary weakness of GPT-4o was its difficulty in accurately reading data coordinates, critical skill for tasks such as Retrieve Value and Find Extremum, resulting in incorrect values for maxima, minima, and other key numerical indicators. Because of this fundamental limitation, Claude 3.5 Sonnet was selected for fine-grained tasks in our pipeline. D. Validation of Cycle Consistency-based Human Verification Process D.1. Quantitative Evaluation To quantify the effectiveness of our verification process, we benchmark it against direct chartcaption comparison in terms of both accuracy and efficiency. Accuracy: On 100 randomly sampled pairs from CHARTCAP, our process achieved an F1 score of 94.7%, with recall of 90.0% and precision of 100.0%, ensuring that no incorrect captions were falsely validated. Efficiency: Direct chart-caption comparison required approximately 145 seconds per sample, whereas our process took only 6 seconds per sample, achieving 24 speedup. D.2. Qualitative Analysis To qualitatively assess the logical validity of our method, we define one axiom and one premise: Axiom: correct verification process should not classify incorrect data as correct. Premise: Human inspectors make no mistakes during verification. Figure 9 illustrates the four main scenarios that arise when regenerating charts from captions: 1. Scenario A. If the caption is incorrect, it produces faulty code leading to mismatched image, which is identified and removed. 2. Scenario B. If the caption lacks sufficient detail, an oversimplified chart is generated and subsequently filtered out. 3. Scenario C. Even if the caption is accurate, errors in code generation or execution can result in failed reconstruction, leading the sample to be excluded. 4. Scenario D. Only when the caption is both accurate and informative, and the chart regenerates without errors, does the sample pass verification. This process ensures that only captions containing both correct and adequately detailed information are retained. In summary, our process is designed to guarantee both the correctness and depth of information in CHARTCAP, 2 Type Classification and Title Extraction. [System] You are an expert in data visualization and chart interpretation. Your task is to provide accurate analysis of charts such as identifying and classifying the chart. [User] Please analyze the image to classify the chart type(s) and extract the main title according to the instructions below. - Identify the chart type(s) from the following list: [line, bar, pie, histogram, scatter, area, bubble, choropleth map, treemap]. - If it belongs to multiple chart types, list them separated by commas (e.g., bar, line). If it does not match any listed chart types or is 3D visualization, respond with: Type: none Title: not specified - If the image contains one or more valid chart types, extract the main title of the chart. If the title is not visible or unclear, respond with not specified. Follow the exact response format: Type: <list of chart type(s) or none> Title: <chart title or not specified> Table 9. Prompt used for type classification and title extraction. Extracting Type-specific Information (Coarse-grained) [System] You are an expert in data visualization and chart interpretation. Your task is to provide accurate analysis of charts such as identifying the components, key trends, and insights, without making any guesses. [User] Identify and describe the components of the given line chart. Only explain the components if they exist; otherwise, respond with not specified. Do not guess or include information not visible in the image, except for approximations in axes ranges, retrieving value of data points, and determining data point ranges. If the chart is multi-series, grouped, or includes an inset chart, compute and report information for each data series or category separately. * Type: Provide the type or types of the chart from line chart, bar chart, pie chart, histogram, scatter plot, area chart, bubble chart, choropleth map, and treemap. * Legends: Identify any legends or keys that globally explain symbols, colors, or data series. * Labels: Identify specific labels that annotate or describe individual elements, such as data points, bars, or segments of chart. Exclude axis labels and legends. * Data Comparison: Highlight specific similarities or differences between data points or categories in the chart. Focus on relative comparisons rather than extracting or explaining precise values. Avoid analyzing overall trends. * Data Correlations/Trends: Analyze patterns or relationships between variables, noting any trends. Only respond with the analyzed results, avoiding any additional statements or extraneous text. Follow the exact response format: <attribute 1>: <analysis result> <attribute 2>: <analysis result> ... Table 10. Prompt used for extracting coarse-grained, type-specific information from line charts. while substantially boosting the efficiency of large-scale verification. By combining logical verification with cycle consistency-based human verification process, we enable efficient quality control of CHARTCAP and mitigate the burden of manual inspection. ment rates across all three criteria, followed by OCRScore. These results indicate that both metrics reliably capture key aspects of caption quality as perceived by humans, validating their effectiveness as automatic metrics. F. LLM Fidelity in Caption-to-Code TranslaE. Validation of VCS with Human Evaluation tion To validate the effectiveness of the Visual Consistency Score (VCS), we performed head-to-head human evaluations. For every comparison between two baselines, we randomly sampled 100 chartcaption pairs from the three test setsCHARTCAP, VisText, and Chart-to-Text. Following the protocol in Appendix J, human annotators compared caption pairs and selected the better one with respect to informativeness, accuracy, and fewer hallucinations. We then computed the agreement rate as the proportion of comparisons in which the caption preferred by human annotators also received higher metric score. As shown in Table 15, VCS achieved the highest agreeWe observe that caption distortions during the first phase of VCS evaluationLLM caption-to-code translationare rare in practice. Interestingly, the distortion rate increases slightly when caption is less informative. To investigate this, we analyzed 100 caption-code pairs each from Phi3.5Vision-4BCHARTCAP (with the highest VCS) and Phi3.5Vision-4BOriginal (with the lowest VCS). We examined (1) whether the elements defined in the caption were correctly preserved, and (2) whether any content not described in the caption appeared in the code. As result, Phi3.5-Vision-4BCHARTCAP achieved caption-to-code accuracy of 99%, with the remaining 1% 3 Extracting Type-specific Information (Fine-grained). [System] You are an expert in data visualization and chart interpretation. Your task is to provide accurate analysis of charts such as identifying the components, retrieving data points, statistics, key trends, insights, without any guesses. [User] Identify and describe the components of the given line chart. Only explain the components if they exist; otherwise, respond with not specified. Do not guess or include information not visible in the image, except for approximations in axes ranges, retrieving value of data points, and determining data point ranges. If the chart is multi-series, grouped, or includes an inset chart, compute and report information for each data series or category separately. * Axes: Describe the axes, including titles, units, scales, and ranges. If categories are involved in the axes, list their names as well. * Retrieve Value: Retrieve the coordinates of the initial, middle, and end data points. Additionally, if specific numbers or values are labeled for any data points, also provide the coordinates of those points as well. * Find Extremum: Find the coordinate of the minimum and maximum data points for each data series. * Determine Range: Specify the range (span) of the dependent (response) variables values from the data points, not the range of the axis. Only respond with the analyzed results, avoiding any additional statements or extraneous text. Follow the exact response format: <attribute 1>: <analysis result> <attribute 2>: <analysis result> ... Table 11. Prompt used for extracting fine-grained, type-specific information from line charts. Finalizing the Caption. [System] You are an expert in converting provided bullet points into continuous sentences without omitting or adding any information. [User] Generate natural language caption for the chart based strictly on the provided information. Ensure the caption includes all the details given in the input without omitting anything or adding new information beyond what is explicitly stated. Explicitly mention that information is not provided if it is stated as not provided in the chart information. [Chart Information] {chart info} Respond only with the generated caption, including all the information provided. Caption: Table 12. Prompt used for finalizing the caption. due to the omission of the axis title. For Phi3.5-Vision4BOriginal, the accuracy was 96%, and in the remaining 4% of cases, the LLM hallucinated placeholder or arbitrary data values to fill in the missing details of oversimplified captions. These results show that (1) translation errors are infrequent, and (2) lower information density in captions tends to increase the likelihood of LLMs caption-to-code distortions, ultimately resulting in lower VCS. G. Sensitivity of VCS to Structural Errors Although SigLIPs attention mechanism on charts is not fully interpretable, we find that the model is reasonably sensitive to structural elements. We analyze 100 captions collected from baseline models and found three major error types: (1) misidentification of maxima/minima (20%), (2) axis hallucinations (13%), and (3) omission of data series (7%). After manually correcting these errors, VCS increased by 1.3%, 6.1%, and 4.7%, respectively, indicating that VCS is capable of detecting such structural issues. H. Additional Baselines We additionally fine-tuned Qwen2.5-VL-7B on CHARTCAP and evaluated it on the VisText and Chart-to-Text benchmarks. We also evaluated Phi3.5-Vision-4BChartSumm on the same benchmarks. As shown in Table 16, Qwen2.5-VL-7BCHARTCAP consistently outperforms its base model, whereas Phi3.5-Vision4BChartSumm underperforms Phi3.5-Vision-4BCHARTCAP and even degrades performance relative to its own base model. We conduct additional human evaluation under the protocol in Appendix directly comparing Phi3.5-Vision4BCHARTCAP and Phi3.5-Vision-4BChartSumm on VisText test set. As shown in Figure 10, Phi3.5-Vision-4BChartSumm received fewer preferences than Phi3.5-Vision-4BCHARTCAP across all three evaluation aspects. In summary, both automatic and human evaluations indicate that (1) CHARTCAP consistently improves the captioning performance of stateof-the-art models, and (2) CHARTCAP is more effective training dataset than ChartSumm. Regenerating Chart from Caption. [System] You are an expert in Python and the Matplotlib library. Your task is to generate complete Python script that precisely reflects every detail in the given chart description, without making any guesses. [User] Generate accurate Python code using Matplotlib library strictly based on the given description about chart. If the description lacks details about required chart components or data points, omit them from the code instead of making assumptions, but ensure that every detail in the description is included. Instead of using numpys sin, cos, or exp function, manually define data points to represent the chart if needed. Labels are elements that display and specify data points in the chart. They are different from axis labels (titles). [Description] {caption} Respond only the generated code. Code: Table 13. Prompt used for regenerating chart from caption. Debugging Erroneous Code. [System] You are an expert in Python and the Matplotlib library. Your task is to fix the code based on the provided error message. [User] [Erroneous Code] {code} [Error Message] {error message} Analyze the provided error message and fix the code accordingly. Make only the necessary changes to resolve the error while keeping all correctly functioning attributes unchanged. Return only the corrected code without any explanations or additional output. Corrected Code: Table 14. Prompt used for debugging erroneous code. Metric Informativeness Accuracy Fewer hallucinations SacreBLEU ROUGE METEOR BERTScore VCS (so400m) OCRScore 60.50 55.34 70.34 68.67 79.33 76. 59.50 55.67 68.34 67.00 77.00 75.00 59.83 57.00 69.34 68.34 77.33 74.00 Table 15. Agreement between automated metrics and human judgments (%). Higher is better. Model VisText Chart-to-Text VCS OCRScore VCS OCRScore Qwen2.5-VL-7B Qwen2.5-VL-7BCHARTCAP Phi3.5-Vision-4B Phi3.5-Vision-4BCHARTCAP Phi3.5-Vision-4BChartSumm 0.9044 0.9328 0.8814 0.9382 0.8677 0.3197 0.3436 0.3414 0.3826 0. 0.7739 0.8084 0.7490 0.8075 0.7281 0.1622 0.1817 0.1786 0.1789 0.0789 Table 16. Results of VCS and OCRScore on VisText and Chartto-Text.VCS is computed using SigLIP2-So400M-512. I. Training Hyperparameters Training was conducted using 6 RTX A6000 GPUs. The total training time was approximately 60 hours for InternVL2.5-8BCHARTCAP, 30 hours for Phi3.5-Vision4BCHARTCAP, 12 hours for Phi3.5-Vision-4BOriginal, 2 hours for Phi3.5-Vision-4BChartSumm, and 50 hours for Qwen2.5VL-7BCHARTCAP. The training hyperparameters used for these models are summarized in Table 17. J. Details of Human Evaluation Sampling and Setup. For each comparison, we randomly sampled 100 chartcaption pairs from two competing baselines. Crowd workers were shown the two pairs in random left-right order and asked to choose the better pair under three criteria: 1. Informativeness Does the caption adequately describe the charts structure and key insights (highlighted in green and blue in Fig 1)? 2. Accuracy How faithfully does the caption reflect the charts structure and key insights? 3. Fewer Hallucinations Does the caption avoid information that cannot be inferred from the chart (highlighted in red in Fig 1)? For the dataset-level study in 3.5 (CHARTCAP vs ChartSumm), we added fourth questionoverall preferenceto capture holistic quality while accounting for chart complexity. User Interfaces. Figure 11 illustrates the user interface 5 Figure 9. Examples of the four main scenarios that arise during the cycle consistency-based human verification process. In Scenario A, the caption incorrectly describes line chart with shaded area as an area chart. Consequently, the generated code reflects this incorrect information, leading to the reconstruction of chart that does not match the original. In Scenario B, the caption oversimplifies the data trend by merely describing it as decreasing. As result, the reconstructed chart follows simple downward trend, failing to capture the original complexity. In Scenario C, an error occurs during the code generation process, leading to the creation of an incorrect chart. Such coding errors were primarily observed when using NumPys nonlinear functions. Scenario shows that both the caption and the generated code must be accurate for the reconstructed chart to correctly match the original chart, demonstrating the necessity of precise and informative captions. Hyperparameter InternVL2.5-8BCHARTCAP Phi3.5-vision-4BCHARTCAP Phi3.5-vision-4BOriginal Phi3.5-vision-4BChartSumm Qwen2.5-VL-7BCHARTCAP Epochs Batch size Learning rate Optimizer Warmup ratio Scheduler LoRA rank LoRA alpha LoRA dropout 2 12 2e-5 AdamW 0.05 cosine 32 64 0.0 1 192 2e-5 AdamW 0.05 constant 32 32 0.05 1 192 2e-5 AdamW 0.1 cosine 32 32 0.05 1 192 2e-5 AdamW 0.05 constant 32 32 0.05 2 12 2e-5 AdamW 0.1 cosine 32 64 0.05 Table 17. Training hyperparameters for InternVL2.5-8B, Phi3.5-vision-4B, and Qwen2.5-VL-7B. Figure 10. Human evaluation results comparing Phi3.5-Vision4BCHARTCAP and Phi3.5-Vision-4BChartSumm on the VisText test set. used for model comparisons, while Fig 12 shows the interface used for dataset comparisons. Worker Qualification and Quality Control. To ensure reliable judgments, we administered qualification test to assess workers understanding of the task. Only those who passed were allowed to participate in the main evaluation. During the evaluation, workers were also required to provide brief justification for their choices, discouraging random or inattentive responses. Platform and Demographics. Evaluations were conducted on Amazon Mechanical Turk, with participation restricted to English-speaking countries (Australia, Canada, New Zealand, the United States, and the United Kingdom). We measured the Inter-annotator agreement. inter-annotator agreement using Gwets AC1. 3.5: 0.84 (informativeness), 0.80 (accuracy), 0. (fewer hallucinations), 0.80 (overall preference). 4.1: 0.47, 0.27, 0.27. 4.2: 0.70, 0.52, 0.22. E: 0.84, 0.71, 0.71. H: 0.91, 0.37, 0.24 Compensation. Workers were compensated $0.50 per HIT, corresponding to approximately $15 per hour, which exceeds the U.S. federal minimum hourly wage. 7 Figure 11. User interface for human evaluation comparing captions from different models on informativeness, accuracy, and fewer hallucinations. Figure 12. User interface for human evaluation comparing datasets (CHARTCAP vs. ChartSumm) on informativeness, accuracy, fewer hallucinations, and overall preference."
        }
    ],
    "affiliations": [
        "Seoul National University"
    ]
}