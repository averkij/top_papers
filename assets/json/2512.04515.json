{
    "paper_title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "authors": [
        "Liuzhou Zhang",
        "Jiarui Ye",
        "Yuanlei Wang",
        "Ming Zhong",
        "Mingju Cao",
        "Wanke Xia",
        "Bowen Zeng",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 5 1 5 4 0 . 2 1 5 2 : r EgoLCD: Egocentric Video Generation with Long Context Diffusion Liuzhou Zhang1 Jiarui Ye1 Yuanlei Wang2 Ming Zhong3 Mingju Gao4 Wanke Xia5 Bowen Zeng3 Zeyu Zhang1 Hao Tang1 1Peking University 2Sun Yat-sen University 3Zhejiang University 4Chinese Academy of Sciences 5Tsinghua University Equal contribution. Project lead. Corresponding authors: bjdxtanghao@gmail.com. Figure 1. The porposed EgoLCD generates long-form egocentric videos that maintain coherent scene transitions and consistent object layouts."
        },
        {
            "title": "Abstract",
            "content": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as problem of efficient and stable memory management. EgoLCD combines Long-Term Sparse KV Cache for stable global context with an attention-based shortterm memory, extended by LoRA for local adaptation. Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid5M benchmark demonstrate that EgoLCD achieves stateof-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing significant step toward building scalable world models for embodied AI. Code: https: //github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD. 1. Introduction The paradigm of video generation [24, 29, 36, 41] is rapidly evolving beyond content creation towards the ambitious goal of building learned world simulators [5, 14, 32]. Such models, capable of simulating future outcomes in response to actions, hold immense potential for training and evaluating embodied AI agents in rich, diverse environments that transcend the limitations of manually engineered simulators [12, 13]. critical yet underexplored frontier in this domain is the simulation of the human experience from first-person perspective. Egocentric video, which captures wearers direct interactions with the world, offers an unparalleled data source for teaching agents complex, multi-step tasks involving fine-grained object manipulation [31, 35, 50]. Large-scale egocentric datasets like Ego4D and EPIC-KITCHENS provide window into this experience, but leveraging them to build useful simulators requires generating long, logically coherent, and computationally tractable video sequences [9, 11]. However, generating long videos remains formidable challenge, as it fundamentally strains models computational resources and its capacity for long-term memory [39, 51]. The quadratic complexity of self-attention in standard Transformers makes processing long sequences computationally prohibitive [20]. Autoregressive (AR) models, which maintain generative memory via Key-Value (KV) caching to condition future frames on the past, offer an efficient alternative [28, 54]. Yet, this generative memory is often fragile. AR models are notoriously prone to content drifta form of generative forgetting where the model gradually loses track of object identity, appearance, and scene semantics over time. This issue is particularly acute for egocentric video, where high-variance camera motion, complex hand-object dynamics, and the necessity for strict procedural logic (e.g., following recipe) mean that even minor memory lapses can render simulation useless for agent training. To address these challenges, we introduce EgoLCD (Egocentric Long Context Diffusion), an end-to-end framework that treats long video generation as problem of robust and efficient memory management. Our approach integrates innovations from low-level memory operators to high-level reasoning paradigms. We also recognize that existing metrics inadequately capture the temporal nature of content drift. Standard evaluations often average quality scores, failing to penalize models whose memory degrades quickly. This motivates our development of new evaluation protocol to better assess long-term temporal stability. Through this multi-faceted approach, EgoLCD generates long-form egocentric videos, as showcased in Fig.1, that achieve new standard in consistency and logical coherence, marking significant step towards building robust, scalable world models for embodied AI [15 18, 25, 27, 37, 38, 45, 47]. In summary, our main contributions are: The proposed EgoLCD is built on long-context diffusion model with longshort memory design for egocentric long-video generation. Specifically, the Long-Term Sparse KV Cache serves as long-term memory, caching historical keyvalue pairs and retrieving them based on their importance to the target video clip. Meanwhile, the attention mechanism with limited context window acts as the core short-term memory, dynamically enhanced by LoRA parameters to enable fast adaptation to recent visual contexts during egocentric video generation. From the training perspective, we address the issue of forgetting in long video generation by introducing memory regulation loss that maintains alignment between historical memory and newly learned representations. novel Structured Narrative Prompting (SNP) methodBy providing ology for training and inference. temporally-ordered sequence of detailed captions, SNP serves as an external memory script, guiding the model through complex action sequences and enhancing its generation of precise temporal details, aligning with recent work on structured video captioning [10, 44]. To comprehensively validate the superiority of EgoLCD in long-form egocentric video generation, we conducted extensive experiments on the authoritative EgoVid-5M benchmark[42], building upon our two-stage training framework utilizing both general video corpora and egocentric datasets. We recognize that existing metrics, by averaging quality scores, fail to effectively capture content drift in long sequences. This motivated us to develop novel evaluation protocol specifically designed to assess long-term temporal stability. Experimental results demonstrate that EgoLCD, refined through twostage training, not only leads in all key metrics on the EgoVid benchmark but also exhibits exceptional temporal consistency under our new protocol, systematically proving its potential as high-performance egocentric video world model. 2. Related Work 2.1. Video Generation as World Simulators The World Model concept has evolved from model-based reinforcement learning to large-scale generative models. Initially, Ha and Schmidhuber used it to improve RL sample efficiency via compressed latent space [12], and models like DreamerV3 advanced it by learning behaviors in imagined trajectories [13]. Recent models like Sora, GAIA1, and Genie extend this idea to high-fidelity simulators trained on large video datasets [4, 32]. This has led to distinction between models for internal world understanding and external world simulation. Benchmarks like WorldFigure 2. Long-Short Memory & Structured Narrative Prompting. SimBench and EVA-Bench now assess visual quality and physical plausibility [22, 34]. While driving simulators model macro-level interactions, egocentric video datasets like Ego4D and EPICKitchens [8, 11] capture fine-grained human-object interactions. Works like EgoMimic and EgoVLA show that policies trained on first-person human and robot data improve manipulation [35, 50]. EgoLCD extends this, proposing that long, coherent egocentric video generation is key for simulating complex human tasks. 2.2. Long Video Generation Long video generation is constrained by the quadratic complexity of self-attention in Transformers, leading to the adoption of autoregressive (AR) models with Key-Value (KV) caching [28, 54]. Models like Ca2-VDM and CausVid optimize this pipeline by sharing caches and compressing the denoising process [28, 54]. Techniques like sub-2-bit quantization and dynamic layer-wise allocation further optimize KV caching [26, 40]. major issue in AR models is content drift, where visual and semantic identity degrade over time. Solutions include motion-guided losses, optical flow propagation, and temporal correlation structuring [7, 48, 53]. Some models blend low-frequency features for continuity with high-frequency features for detail [43]. To address content drift, recent models focus on longrange dependency modeling, such as Mixture of Contexts (MoC) [20] and Long Context Tuning (LCT) [23]. Our work, EgoLCD, contributes by developing diffusion model for long-context egocentric video generation. 3. The Proposed Method 3.1. Overview As shown in Fig. 3, EgoLCD is long-context diffusion framework for egocentric world modeling that generates temporally coherent long video sequences through blockwise architecture. It produces video segments sequentially, with each new segment conditioned on the preceding context to maintain long-range consistency. The framework employs semi-autoregressive (SemiAR) diffusion strategy for scalable long video synthesis. Current methods face several limitations: (1) inefficient context propagation causes temporal drift; (2) memory requirements grow quadratically with sequence length; (3) training-inference inconsistency in memory management; (4) difficulty balancing long-term coherence with shortterm adaptation to dynamic egocentric views. EgoLCD addresses these through novel long-shortterm memory design, featuring long-term sparse KV Figure 3. The overall framework of EgoLCD. cache for global context stability and short-term memory module for local pattern adaptation. The short-term memory rapidly captures evolving scene dynamics while selectively consolidating key information into long-term storage. EgoLCD enhances generation quality through structured narrative prompting and memory regulation loss. The narrative system maintains continuity by storing and retrieving prompts from semantic cache, while the memory loss constrains generation using historical segments as semantic anchors. These complementary approaches jointly ensure semantic and temporal consistency in long video generation. 3.2. Long-Short Memory Inspired by Titans [2], where attention acts as short-term memory and neural memory module captures persistent history, we adopt dual-memory design for EgoLCD. In our framework, the Long-Term Sparse KV Cache functions as long-term memory for distant dependencies, while the attention mechanism with limited context window serves as short-term memory, enhanced by LoRA parameters as implicit memory units, as shown in Fig. 2. This separation allows us to both retain long-horizon consistency and support rapid local adaptationmitigating forgetting in long egocentric video generation, aided by the integration of Memory Regulation Loss to ensure consistency in long-term memory usage. Sparse KV Caching as Long-Term Memory. Maintaining semantically consistent memory is essential for generating high-quality long content for egocentric video generation. However, storing all memory as KV Cache is very expensive. Hence, we propose Long-Term Sparse KV Cache that bridges the training-inference gap through unified memory operations. Our approach employs consistent sparse caching strategies across both phases, enabling efficient long-term dependency modeling while eliminating performance degradation for sustainable first-person video generation. Let denote the memory repository storing historical prompt embeddings and their corresponding sparse KV caches. For each new prompt embedding Et, we compute semantic relevance to historical embeddings Eu via cosine similarity: su = Et, Eu EtEu , Eu H, (1) where su represents the semantic similarity score, Et denotes the current prompt embedding, and Eu indicates the historical prompt embeddings in the memory repository. The retrieval set Rt containing the top-m most relevant historical prompts enables contextual fusion through concatenation operations: Kt = Concat(Kt, KRt), Vt = Concat(Vt, VRt), (2) where Kt and Vt represent the fused key and value tensors, Kt and Vt denote the current key-value pairs, and KRt and VRt correspond to the retrieved historical key-value pairs. The sparse caching mechanism employs probe-based importance scoring for memory compression. Given the probe token set P, we compute attention weights: This unified memory framework establishes persistent knowledge trajectories through H, enables context-aware retrieval via Rt, and achieves efficient compression through (l), providing an effective solution for egocentric longvideo generation while maintaining sub-linear memory complexity and ensuring training-inference consistency. A(l) probe = Softmax (cid:18) QP dk (cid:19) + , (3) where A(l) probe denotes the attention weight matrix for layer l, QP represents probe queries, indicates all keys, dk is the key dimension, and signifies the causal mask matrix. The importance score σ(l) for each token aggregates attention weights across probes and heads: σ(l) = (cid:88) (l) (cid:88) i=1 h=1 A(l) i,p,h, (4) where denotes the probe set size and (l) represents the number of attention heads in layer l. Position-aware normalized importance accounts for temporal locality in egocentric sequences: σ(l) = σ(l) + 1 , (5) where σ(l) indicates the normalized importance score, signifies the sequence length, and denotes the token position. The sparse token set (l) preserves tokens with maximal cumulative importance: (cid:40) r(l) = min 1 (cid:88) u=1 σ(l) π(l)(u) τ (cid:41) σ(l) , (cid:88) v=1 (6) (l) = {π(l)(1), π(l)(2), . . . , π(l)(r(l))}, where r(l) represents the number of retained tokens, τ denotes the importance threshold, and π(l) indicates the importance sorting permutation. During generation, each query qt attends exclusively to the sparse cache: h(l) = (cid:88) t,jV (l) α(l) , jI(l) (cid:16) exp q(l) (cid:16) , (l) / , (l) dk / (7) (cid:17) (cid:17) , dk (cid:80) kI(l) exp q(l) α(l) t,j = denotes the output hidden state, α(l) where h(l) the attention weight, and (l) cache. t,j represents indicates the sparse value Attention as Short-Term Memory with LoRA Enhancement. Drawing inspiration from recent advances in visual memory [30]which reinterpret lightweight adaptation layers such as LoRA as implicit memory unitswe conceptualize the limited-context-window attention mechanism as the core short-term memory module within the EgoLCD framework while treating LoRA parameters as auxiliary implicit memory units. In egocentric video generation, the visual scene evolves continuously as the camera wearer moves, necessitating mechanism capable of rapid adaptation to local changes without disrupting the long-term context. The attention mechanism, by leveraging its constrained context window and precise dependency modeling, functions as short-term memory, enabling the model to dynamically maintain transient representations within each generation block while preserving stable longterm information in the sparse KV cache. Concurrently, LoRA parameters serve as implicit memory units, providing lightweight and adaptive enhancements to the attention mechanism. The transient representations held in the attention context are progressively integrated into long-term memory via the sparse KV cache over time. This dual design empowers EgoLCD to effectively balance persistent world modeling with moment-to-moment egocentric variations, achieving agile short-term adaptation while upholding long-term coherence. Memory Regulation Loss. To tightly couple the dualmemory design with the generative dynamics, we introduce Memory Regulation Loss that regularizes the predicted velocity field using semantically retrieved long-term context. Let xt RCT0HW denote the ground-truth video clip at timestep t, where C, T0, H, and represent the channel dimension, temporal length, height, and width, respectively, and let ϵt be the sampled noise. The diffusion model predicts velocity field vθ(xt, t) following the rectified flow formulation, where θ denotes the model parameters. To inject long-range semantics into the supervision signal, we retrieve the top-m most relevant historical video segments from the memory repository H, denoted as {X (u)}uRt, where indexes the retrieved segments and Rt denotes the set of indices for the relevant historical segments. Since these segments may differ in temporal resolution, each segment is temporally resampled to match the current clip length T0 as follows: 3.3. Structured Narrative Prompting ˆX (u) = Resample(cid:0)X (u), T0, H, (cid:1), Rt, (8) where ˆX (u) represents the resampled historical segment for the u-th segment, where (u) is the original segment. The resampled segments are then aggregated to form semantic anchor representing the retrieved long-term context: Xcond = 1 Rt (cid:88) ˆX (u), uRt (9) where Rt is the number of relevant historical segments retrieved from memory. Xcond serves as the semantic anchor, synthesized representation of the long-term context that aggregates information from multiple historical video segments. Instead of supervising the velocity solely toward xt, we define memory-regularized target velocity that incorporates this semantic anchor: (10) mem = ϵt Xcond, where ϵt is the sampled noise, and mem is the target velocity conditioned on the memory, representing refined velocity that aligns with the semantic anchor. We define the memory regulation loss as the expected squared error between the predicted velocity and the memory-regularized target velocity: (cid:104) Lmem = vθ(xt, t) mem 2 (cid:105) , (11) where vθ(xt, t) is the velocity predicted by the model for the ground-truth video at timestep t. key advantage of this formulation is traininginference consistency. The model interacts with sparse memory retrieval during training in exactly the same way as in inference, eliminating the common traintest mismatch in longhorizon video generation. Additionally, by jointly optimizing retrieval, aggregation, and sparse memory fusion with the generative process, the memory module exhibits significantly improved stability and reliability during inference. The final training objective integrates the proposed memory regulation loss with the rectified flow objective and the auxiliary reconstruction loss: = LRF + λMAELMAE + γLmem, (12) where LRF is the rectified flow loss, LMAE is the mean absolute error (MAE) loss, λMAE is hyperparameter controlling the weight of the MAE loss, and γ is hyperparameter controlling the weight of the memory regulation loss. This unified objective effectively stabilizes long-term memory usage, reduces drift across generation blocks, and significantly improves temporal coherence in egocentric longvideo synthesis. To optimize the use of semantic caching and historical guidance, we propose structured narrative prompting system for each 5-second video segment. The process begins by dividing the input video into smaller, 5-second clips, ensuring that each clip represents coherent unit of action, scene, or transition. For each of these segments, GPT-4o [33] generates detailed captions that describe the visuals, characters, actions, and environment, capturing the essence of the scene. During the inference process, the video is generated sequentially, with each 5-second segment being rolled out in succession. To maintain semantic consistency, we reference the prompts of previous segments. Specifically, we retrieve the most semantically similar prompts from earlier clips, leveraging semantic KV cache to store these prompts along with their associated embeddings. These stored embeddings enrich the generation of the current segment by incorporating context from prior clips, ensuring smooth and coherent transitions both visually and narratively. The structured narrative prompting serves two main purposes. First, it provides detailed guidance for generating each individual segment, ensuring that every 5-second clip contributes meaningfully to the overall narrative. Second, it forms the foundation for effectively utilizing the semantic KV cache. By storing and accessing relevant semantic information from previous clips, the system can dynamically adjust the generation of future segments, optimizing both visual consistency and narrative continuity throughout the entire video. 4. Experiments 4.1. Datasets and Evaliation Metrics Datasets. (1) General Videos. The first is curated, general-domain dataset of 1,000 natural videos, which we collected to provide foundation for learning general visual features and motion dynamics. (2) Egocentric Videos. The second and primary source for domain specialization is the Ego4D dataset. Ego4D is massive-scale, diverse egocentric video dataset, offering 3,670 hours of daily-life activity videos that span hundreds of scenarios. This dataset is essential for our work as it provides extensive, long-form, first-person video that captures the unique viewpoints, camera motion, and complex hand-object interactions central to our task. All video data from both sources were processed and converted into the Structured Narrative Prompting (SNP) format to serve as model inputs. Evaliation Metrics. Standard evaluation metrics for video generation, such as Frechet Video Distance (FVD), often rely on average scores across entire sequences. This Perceptual quality metrics () NRDP metrics () Method Img. Quality Motion Smooth. Aesth. Quality Bg. Consist. Subj. Consist. Clarity Motion Aesthetic Background Subject MAGI [1] Self-Forcing [19] Framepack [49] SkyReels-v2 [6] EgoLCD (Ours w/o loss) EgoLCD (Ours) 0.6662 0.6805 0.6972 0.6567 0.6643 0.6852 0.9947 0.9947 0.9949 0.9926 0.9874 0. 0.6508 0.6283 0.6043 0.5267 0.5353 0.6047 0.9078 0.8203 0.8791 0.8924 0.8895 0.9588 0.8992 0.8481 0.9001 0.8640 0.8576 0.9597 2.7225 3.0798 4.2513 1.9503 1.8390 0.7551 0.0243 0.1549 0.0387 0.0461 0.0509 0.0119 3.8286 3.4683 1.4751 2.8957 2.0941 0. 0.3090 1.6108 5.9421 0.9323 1.1124 0.2945 0.6618 0.3716 4.3984 1.8292 1.7933 0.0844 Table 1. Comparison of different methods on perceptual quality and NRDP-based no-reference metrics. The best scores in perceptual quality metrics are bolded and the second-best scores are italicized. In the table, EgoLCD (w/o Loss) refers to EgoLCD trained without the memory regulation loss. approach is insufficient for long-context generation, as it obscures critical failure mode: content drift. model that generates high-quality initial segment but degrades rapidly may achieve misleadingly positive score. To establish comprehensive baseline, we first employ standard quality dimensions from VBench [21]. VBench offers fine-grained, hierarchical evaluation of video quality. We specifically utilize its objective metrics for Aesthetic Quality, Imaging Quality, Motion Smoothness, Background Consistency , and Subject Consistency to assess the general performance of generated videos. However, to specifically quantify temporal stability and penalize generative forgetting, we introduce new evaluation metric: the Normalized Referenced Drifting Penalty (NRDP). The design of NRDP is guided by three key principles. First, it uses the initial video segment (the first chunk) as high-quality reference, as error accumulation typically degrades subsequent chunks. Second, it applies decaying penalty, imposing stricter punishment on models that exhibit quality drift early in the sequence, which signals poorer core stability. Third, all drift calculations are normalized by the first chunks quality, ensuring fair comparison by removing biases from different models baseline capabilities. The NRDP calculation proceeds in four steps: 1. The video is segmented into uniform chunks (we use = 10). 2. base quality metric (from VBench) is computed for each chunk, yielding score sequence M1, . . . , MN . 3. The normalized drift Di for each subsequent chunk is calculated as its relative deviation from the reference chunk M1: Di = Mi M1 M1 . (13) 4. The final NRDP score is computed as weighted sum of these drifts, using monotonically decreasing weight wi (such as wi = + 1) to penalize early drift more severely: RDPM (V ) = (cid:88) i= wi Di. (14) We apply NRDP to the VBench base metrics, resulting in our primary stability metrics: NRDP-Aesthetic, NRDPMotion, and NRDP-Clarity (derived from Imaging Quality). 4.2. Implementation Details EgoLCD is built upon the SkyReels-v2-1.3B latent diffusion transformer. We employ two-stage training strategy: first on general videos for appearance and motion priors, and then on egocentric data for first-person specialization, with full memory components enabled. For optimization, we use AdamW with learning rate of 1e-5, weight decay of 1e-4, and 200 warm-up steps. Training employs bf16 precision with FSDP (sequence-parallel degree 4), global batch size of 8, and gradient checkpointing. The latent sequence length is capped at 75,600 tokens, with random dropout of video/KV conditions (0.2/0.1/0.1 probabilities) for robustness. fixed negative prompt suppresses common artifacts. We use rectified-flow scheduler (1,000 steps) and optimize hybrid loss that combines the rectified-flow objective, MAE reconstruction, and memory regulation loss. EMA decay is set to 0.99. Complete two-stage training requires approximately 50 hours on 8H100 GPUs. 4.3. Evaluation and Analysis on General Long"
        },
        {
            "title": "Video Generation",
            "content": "including MAGI We evaluate EgoLCD against several state-of-the-art base- [1], Self-Forcing [19], line models, Framepack [49], and our base model, SkyReels-v2 [6]. The evaluation covers both standard perceptual quality metrics and our proposed NRDP metrics for measuring temporal drift, with results shown in Table 1. In terms of perceptual quality, EgoLCD achieves strong performance across multiple dimensions. It obtains the best Method SVD [3] SVD [3] DynamiCrafter [46] DynamiCrafter [46] OpenSora [52] OpenSora [52] EgoLCD (Ours) EgoLCD (Ours) w. EgoVid CD-FVD Semantic Consistency Action Consistency Clarity Score Motion Smoothness Motion Strength (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) 591.61 548.32 243.63 236. 809.46 718.32 187.94 177.23 0.258 0.266 0.257 0.265 0.260 0.266 0.291 0. 0.465 0.471 0.481 0.494 0.489 0.494 0.510 0.517 0.479 0.485 0.473 0. 0.520 0.528 0.530 0.540 0.971 0.974 0.986 0.987 0.983 0.986 0.992 0. 18.897 21.032 9.357 18.329 7.608 15.871 20.732 22.351 Table 2. Evaluations across six metrics confirm that training with the EgoVid dataset leads to superior performance compared to the three baseline models. scores in both Background Consistency and Subject Consistency, demonstrating the effectiveness of our long-shortterm memory module in maintaining scene and object stability. The model also ranks first in Imaging Quality and Motion Smoothness, indicating good frame clarity and temporal coherence. While slightly lower than MAGI in Aesthetic Quality, EgoLCD shows meaningful improvement over its base model SkyReels-v2. EgoLCDs most significant advantages appear in the NRDP metrics designed to measure content drift. Our method achieves the best performance across all five NRDP dimensions, substantially outperforming all baseline models. The notable reductions in NRDP-Subject and NRDPAesthetic confirm EgoLCDs ability to maintain initial segment quality throughout long generation sequences, effectively addressing generative forgetting. The ablation study (EgoLCD without loss) further validates the importance of Memory Regulation Loss, as its removal leads to clear degradation in consistency performance. 4.4. Performance Analysis in Egocentric Video Generation Table 2 compares the propsoed EgoLCD with three baseline models: SVD [3], DynamiCrafter [46], and OpenSora [52], both with and without the EgoVid dataset, specifically in the context of egocentric video generation. The results demonstrate that our two-stage training strategy is highly effective: by first pre-training on general videos to establish robust appearance and motion priors, and then specializing on egocentric data with full memory components enabled, EgoLCD achieves significant performance improvements across all six evaluation metrics when trained with EgoVid. Notably, it achieves lower CD-FVD, indicating better temporal coherence, and shows improved semantic and action consistency, clarity, motion smoothness, and motion strength. Compared to the baseline models, EgoLCD outperforms them in all metrics, highlighting the effectiveness of its dual-memory design and the strategic use of the EgoVid dataset in enhancing long-context consistency and dynamic motion for first-person perspective video generation. 5. Limitations and Future Work Computationally, the sparse memory design still requires substantial GPU resources, and the fixed 5-second segment processing limits the temporal horizon. Methodologically, the reliance on precisely aligned text descriptions makes the model susceptible to noise-induced semantic drift, and the evaluation framework depends on automated metrics that may not reflect genuine user preferences. We plan to develop efficient memory management solutions to overcome generation length constraints and create noise-resistant narrative comprehension modules. Additionally, we will construct evaluation systems better aligned with human perception to reduce dependence on automated metrics. 6. Conclusion This work presents EgoLCD, framework addressing key challenge in long video generation: maintaining visual and semantic consistency over extended sequences. Instead of treating egocentric synthesis as solely generative task, EgoLCD reframes it as memory management problem through unified longshort memory design that preserves context while adapting to dynamic first-person scenes. The Long-Term Sparse KV Cache maintains global structure, while LoRA-enhanced short-term attention captures finegrained motion and appearance changes. Memory Regulation Loss enforces alignment with historical context, and Structured Narrative Prompting provides temporal guidance across multi-stage actions. Evaluations on the EgoVid-5M benchmark show consistent gains in perceptual quality, semantic stability, and motion coherence, while substantially mitigating generative forgetting and temporal drift."
        },
        {
            "title": "References",
            "content": "[1] Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale, 2025. 7 [2] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 4 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 8 [4] Tim Brooks et al. Genie: Generative Interactive Environments, 2024. [5] Jake Bruce et al. Genie: Generative Interactive Environments, 2024. 2 [6] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 7 [7] Yitong Chen et al. ARTDiff: AutoRegressive Temporal Diffusion for Sequential Data. In International Conference on Learning Representations (ICLR), 2024. 3 [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling Egocentric Vision: The EPICKITCHENS Dataset. In European Conference on Computer Vision (ECCV), 2018. 3 [9] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The EPIC-KITCHENS dataset: Collection, IEEE Transactions on Pattern challenges and baselines. Analysis and Machine Intelligence, 2020. [10] Yutong Fan et al. InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [11] Kristen Grauman, Andrew Westbury, et al. Ego4D: Around the World in 3,000 Hours of Egocentric Video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [12] David Ha and Jurgen Schmidhuber. World Models, 2018. 2 [13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. DreamerV3: Mastering Diverse Domains through World Models, 2023. 2 [14] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. GAIA-1: Generative World Model for Autonomous Driving, 2023. [15] Ting Huang, Dongjian Li, Rui Yang, Zeyu Zhang, Zida Yang, and Hao Tang. Mobilevla-r1: Reinforcing arXiv preprint vision-language-action for mobile robots. arXiv:2511.17889, 2025. 2 [16] Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. [17] Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. [18] Ting Huang, Zeyu Zhang, Ruicheng Zhang, and Yang Zhao. Dc-scene: Data-centric learning for 3d scene understanding. arXiv preprint arXiv:2505.15232, 2025. 2 [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 7 [20] Xun Huang et al. Long Video Generation as Internal Information Retrieval, 2025. 2, 3 [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models, 2023. 7 [22] Yunchao Li et al. EVA-Bench: Comprehensive BenchIn International mark for Embodied Video Anticipation. Conference on Machine Learning (ICML), 2024. 3 [23] Yilun Li et al. Long Context Tuning for Video Generation, 2025. [24] Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, et al. Fpsattention: Training-aware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025. 2 [25] Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025. 2 [26] Yijun Liu et al. VidKV: Towards Lower-Bit KV Cache Quantization for Video Large Language Models, 2025. 3 [27] Zeting Liu, Zida Yang, Zeyu Zhang, and Hao Tang. Evovla: Self-evolving vision-language-action model. arXiv preprint arXiv:2511.16166, 2025. 2 [28] Cheng-Jen Lu et al. Ca2-VDM: Causal and Cacheable Video Diffusion Model for Efficient Long-Term Video Generation. In International Conference on Machine Learning (ICML), 2025. 2, 3 [29] Jiabin Luo, Junhui Lin, Zeyu Zhang, Biao Wu, Meng Fang, Ling Chen, and Hao Tang. Univid: The open-source unified video model. arXiv preprint arXiv:2509.24200, 2025. 2 [30] Feipeng Ma, Hongwei Xue, Yizhou Zhou, Guangting Wang, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, and Xiaoyan Sun. Visual perception by large language models weights. Advances in Neural Information Processing Systems, 37:2861528635, 2024. 5 [45] Zhengri Wu, Yiran Wang, Yu Wen, Zeyu Zhang, Biao Wu, and Hao Tang. Stereoadapter: Adapting stereo depth estimation to underwater scenes. arXiv preprint arXiv:2509.16415, 2025. [46] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2024. 8 [47] Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reaarXiv preprint soning in vision-language-action models. arXiv:2510.01623, 2025. 2 [48] Kelvin C.K. Zhang et al. Motion-guided latent diffusion for real-world video super-resolution. In European Conference on Computer Vision (ECCV), 2024. 3 [49] Lvmin Zhang, Shengqu Cai, Muyang Li, Gordon Wetzstein, and Maneesh Agrawala. Frame context packing and drift prevention in next-frame-prediction video diffusion models, 2025. 7 [50] Yilun Zhang et al. EgoVLA: Egocentric Video Language Agents, 2025. 2, [51] Zeyu Zhang, Shuning Chang, Yuanyu He, Yizeng Han, Jiasheng Tang, Fan Wang, and Bohan Zhuang. Blockvid: Block diffusion for high-quality and consistent minute-long video generation. arXiv preprint arXiv:2511.22973, 2025. 2 [52] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 8 [53] Ying-Jun Zhou et al. Upscale-A-Video: TemporalConsistent Diffusion Model for Real-World Video SuperResolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [54] Ziyi Zhou et al. CausVid: Causal Autoregressive DiffuIn Proceedings of sion for Streaming Video Generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3 [31] Tushar Nagarajan and Kristen Grauman. Shaping embodied agent behavior with activity-context priors from egocentric video. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2 [32] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. [33] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, et al. Gpt-4o system card, 2024. 6 [34] Iran Qin et al. WorldSimBench: Comprehensive Benchmark for Evaluating World Simulators, 2024. 3 [35] Priyam Shah et al. EgoMimic: full-stack framework for scaling manipulation through egocentric-view human demonstrations, 2024. 2, 3 [36] Jingwei Shi, Zeyu Zhang, Biao Wu, Yanjie Liang, Meng Fang, Ling Chen, and Yang Zhao. Presentagent: Multimodal agent for presentation video generation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 760773, 2025. 2 [37] Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Fu Yujie, Zeyu Zhang, Shiyu Jiang, Miao Fang, et al. Hazards in daily life? enabling robots to proactively detect and resolve anomalies. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 73997415, 2025. [38] Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulaarXiv preprint tion with large vision-language models. arXiv:2505.16517, 2025. 2 [39] Inferix Team, Tianyu Feng, Yizeng Han, Jiahao He, Yuanyu He, Xi Lin, Teng Liu, Hanfeng Lu, Jiasheng Tang, Wei Wang, et al. Inferix: block-diffusion based next-generation arXiv preprint inference engine for world simulation. arXiv:2511.20714, 2025. 2 [40] Yuxuan Wan et al. MEDA: Dynamic Layer-wise KV Cache Allocation for Efficient Multimodal Long-Context Inference. In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2025. 3 [41] Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, et al. Drivegen3d: Boosting feedforward driving scene generation with efficient video diffusion. arXiv preprint arXiv:2510.15264, 2025. 2 [42] Xiaofeng Wang, Kang Zhao, Feng Liu, Jiayu Wang, Guosheng Zhao, Xiaoyi Bao, Zheng Zhu, Yingya Zhang, and Xingang Wang. Egovid-5m: large-scale video-action arXiv preprint dataset for egocentric video generation. arXiv:2411.08380, 2024. 2 [43] Zhentao Wang et al. FreeLong: training-free framework for consistent long video generation, 2025. [44] Shuqi Wu et al. Any2Caption: Towards Controllable Video Generation from Any Condition, 2025. 2 EgoLCD: Egocentric Video Generation with Long Context Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Algorithm Details In this section, we provide the detailed algorithmic procedures for the proposed EgoLCD framework, corresponding to the methodology described in Section 3. A.1. Inference Pipeline Algorithm 1 outlines the overall inference pipeline of EgoLCD. The process begins with Structured Narrative Prompting (Section 3.3), where the input script is segmented into coherent 5-second clips with detailed captions generated by GPT4o. During the generation loop, the model utilizes Semantic Retrieval via cosine similarity (Section 3.2) to fetch relevant historical context from the database H. This ensures that the current generation is conditioned on the most pertinent past events, effectively mitigating content drift in long-form video synthesis as discussed in Section 3.1. Algorithm 1 EgoLCD Inference Pipeline with Structured Narrative Prompting Require: Video Script S, Pretrained DiT Model θ, History Database 1: Preprocessing: Split into clips {C1, . . . , CN } via GPT-4o 2: for = 1 to do 3: 4: 5: Encode prompt Pt to get embedding Et Memory Retrieval: if is not empty then Compute semantic relevance: su Et,Eu EtEu Retrieve indices Rt Top-m({su}) based on relevance Fetch Historical KV: Kretr, Vretr Concat({K (u), (u) Rt}) else Kretr, Vretr None end if Generation (Diffusion): Initialize noise zt (0, I) Condition on previous clip Vt1 (if > 1) to maintain continuity for diffusion step = to 0 do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: vpred Model(zt, s, Pt, Kretr, Vretr) zt RectifiedFlowStep(zt, vpred) end for Vt Decode(zt) Memory Update: Ksparse, Vsparse SparseCompress(Kt, Vt) Append (Et, Ksparse, Vsparse) to 18: 19: 20: 21: 22: 23: 24: end for 25: return Concatenated Video = [V1, . . . , VN ] See Alg. 2 See Alg. 3 A.2. Dual-Memory DiT Block Algorithm 2 details the internal mechanism of our modified DiT block, which realizes the Long-Short Memory design (Section 3.2). Within each block, the short-term memory is handled by the standard attention window, enhanced by LoRA parameters (ϕ) to enable rapid adaptation to evolving egocentric viewpoints. Simultaneously, the long-term memory is injected by concatenating the retrieved sparse KV pairs (Kretr, Vretr) with the local features before the attention operation. This dual-pathway ensures the model maintains global consistency while responding to local dynamics. Algorithm 2 Dual-Memory DiT Block (Long-Short Context) Require: Latent x, Current KV (Kcur, Vcur), Retrieved KV (Kretr, Vretr), LoRA params ϕ 1: Input Projection: 2: Wqx + LoRAq(x) 3: Klocal Wkx + LoRAk(x) 4: Vlocal Wvx + LoRAv(x) 5: Long-Short Fusion: 6: if Kretr is not None then 7: Kf used Concat([Kretr, Klocal], dim = 1) Vf used Concat([Vretr, Vlocal], dim = 1) Kf used, Vf used Klocal, Vlocal 8: 9: else 10: 11: end if 12: Attention Mechanism: 13: Compute attention weights with fused memory: QKT ) used dk 14: Softmax( 15: AVf used 16: WoO + LoRAo(O) 17: return O, Klocal, Vlocal A.3. Sparse KV Compression To maintain computational efficiency and fixed memory overhead, we employ the Sparse KV Compression strategy described in Algorithm 3. Following the methodology in Section 3.2, we calculate the importance score σ(l) for each token by aggregating attention weights from probe tokens. These scores are normalized by position to account for temporal locality. Finally, threshold-based pruning is applied to retain only the most critical key-value pairs for the Long-Term Sparse KV Cache. Algorithm 3 Sparse KV Compression Strategy Require: Full KV Cache K, , Sequence Length L, Probe Set 1: Compute Importance: 2: for layer in Transformer do + ) 3: 4: Compute probe attention: A(l) Aggregate importance: σ(l) Normalize position: σ(l) (cid:80) σ(l) Lp+1 probe Softmax( QP KT (cid:80) dk A(l) i,p,h iP 5: 6: end for 7: Token Pruning: 8: Sort tokens by importance π(l) 9: Determine cut-off r(l) where (cid:80) σ τ (cid:80) σtotal 10: Select indices (l) {π(l)(1), . . . , π(l)(r(l))} 11: Compress: 12: Ksparse Gather(K, (l)) 13: Vsparse Gather(V, (l)) 14: return Ksparse, Vsparse B. Ablation Study To validate the effectiveness of the core components in EgoLCD, we conduct an ablation study on the EgoVid-5M benchmark. It is important to note that all variants in this study, including the baseline, adopt the proposed block-wise autoregressive generation strategy and are conditioned on Structured Narrative Prompting (SNP). This ensures that the performance gains Perceptual quality metrics () NRDP metrics () Method Img. Quality Motion Smooth. Aesth. Quality Bg. Consist. Subj. Consist. Clarity Motion Aesthetic Background Subject EgoLCD (w/o KV Cache) EgoLCD (w/o loss) EgoLCD 0.6567 0.6643 0.6852 0.9926 0.9874 0.9956 0.5267 0.5353 0.6047 0.8924 0.8895 0. 0.8640 0.8576 0.9597 1.9503 1.8390 0.7551 0.0461 0.0509 0.0119 2.8957 2.0941 0.9618 0.9323 1.1124 0.2945 1.8292 1.7933 0. Table 3. Ablation study on the effectiveness of key components in EgoLCD. All methods utilize block-wise generation and Structured Narrative Prompting (SNP). EgoLCD (w/o KV Cache) denotes the baseline without the sparse memory module. EgoLCD (w/o loss) denotes the model with memory but trained without the Memory Regulation Loss. The results demonstrate that even with SNP and blockwise inference, the internal Sparse KV Cache and Regulation Loss are indispensable for minimizing temporal drift (NRDP metrics). reported in Table 3 stem strictly from our architectural contributionsthe Long-Term Sparse KV Cache and the Memory Regulation Lossrather than the inference framework or prompt engineering. Impact of Long-Term Sparse KV Cache. The variant EgoLCD (w/o KV Cache) serves as our baseline (equivalent to the base model SkyReels-v2). Despite utilizing block-wise generation and detailed SNP guidance, this model suffers significantly from content drift, as evidenced by high error rates in NRDP-Subject (1.8292) and NRDP-Background (0.9323). This indicates that while SNP provides semantic continuity, it is insufficient for maintaining fine-grained visual consistency over long horizons without an explicit internal memory mechanism. By integrating the Long-Term Sparse KV Cache, the full EgoLCD model effectively bridges the gap between independent blocks, drastically reducing subject drift to 0.0844. This confirms that retrieving cached visual features is essential for preserving object identity across the block-wise generation process. Impact of Memory Regulation Loss. The EgoLCD (w/o loss) variant incorporates the Sparse KV Cache but is trained without the Memory Regulation Loss (Lmem). Although it outperforms the baseline by leveraging historical context, it still exhibits noticeable degradation compared to the full model (e.g., NRDP-Background 1.1124 vs. 0.2945). This suggests that simply attending to retrieved memory during the block-wise inference is not optimal; the model requires explicit supervision during training to learn how to align its generated velocity fields with the semantic anchors from the cache. The Memory Regulation Loss enforces this alignment, ensuring that the long-term context effectively regularizes the trajectory of each generation block. C. Implementation Details Training Configuration. We train the model on cluster of 8 NVIDIA H100 GPUs using Full Sharded Data Parallel (FSDP) and bf16 precision. The optimization is performed using AdamW with learning rate of 1 105, weight decay of 1104, and β parameters implied by an epsilon of 11015. We apply warmup of 200 steps and maintain an Exponential Moving Average (EMA) of model weights with decay rate of 0.99. To enhance generation diversity, we employ classifierfree guidance training by randomly dropping video conditions (p = 0.2) and text prompts (p = 0.1). The training utilizes dynamic bucket strategy for resolution management, with base resolution of 480p, batch size of 1 per GPU, and sequence parallel degree of 1. Inference Settings. For long-form video generation, we adapt the configuration to handle increased sequence lengths. We scale the sequence parallel degree to 4 to accommodate 81-frame sequences at resolution of 480 848 (aspect ratio 9:16). The generation process uses the Rectified Flow scheduler with 20 sampling steps, guidance scale of 5.0, and sample shift of 5.0. To ensure temporal continuity across segments, we employ an autoregressive generation strategy where each new video segment is conditioned on the last 9 frames of the preceding segment. Furthermore, we explicitly cache and reuse the attention key-value states from previous steps to maintain long-term consistency and computational efficiency without re-computation. The maximum sequence length is capped at 75,600 tokens to fit within memory constraints. D. Structured Narrative Prompting (SNP) Details In this section, we provide concrete examples of our Structured Narrative Prompting (SNP) strategy. We employ distinct prompting structures for the training and inference phases to address their specific requirements: dense frame alignment for training and temporal narrative guidance for inference. D.1. Training Phase: Dense Frame-Level Anchoring During training, we utilize the EgoVid-5M dataset. The prompts are characterized by dense, exhaustive descriptions aligned with specific frame ranges. This verbosity forces the model to learn fine-grained correspondences between visual features (e.g., texture, object placement) and textual concepts. Table 4 presents actual samples from the dataset (Kitchen Scene). Note the high density of object descriptions (blue tiles, stainless steel sink, utensils) which anchors the scenes identity across different clips. Table 4. Training SNP Examples (Real Data from EgoVid). These prompts are strictly aligned with frame indices and contain dense scene details to mitigate forgetting. Frame Range Dense Training Prompt (SNP) Frames 0 120 Overview Frames 416 536 Cabinet Interaction Frames 938 1058 Detailed Wall View The video begins with view of kitchen sink area, featuring stainless steel sink with two compartments. The left compartment is filled with water and various utensils, including green cutting board, knife, and some spoons. The right compartment is empty. On the countertop next to the sink, there are several items, including bottle of dish soap, sponge, knife, and plate with some food remnants. hand appears in the frame, pointing towards the sink area. The hand then moves to the right side of the sink, where an orange is placed on plate. The hand continues to move around the sink area, pointing at different objects. The video showcases small, well-organized kitchen with blue tiled walls and white cabinets. The countertop is neatly arranged with various items, including bottle of dish soap, plate with an orange and some greens, pot on the stove, and some utensils. person wearing red shirt is seen reaching for the dish soap bottle, opening the cabinet above the sink, and then closing it. The camera angle shifts slightly to provide closer view of the countertop and the items on it, emphasizing the tidy and organized nature of the kitchen. The video begins with close-up view of blue tiled wall in kitchen, featuring white cabinet above it. The camera then pans to the right, revealing more of the kitchen counter and additional blue tiles on the wall. white cabinet door is visible, and as the camera continues to move, it shows the interior of the cabinet, which contains various items such as red container and some white dishes. The camera then moves back to the left, providing broader view of the kitchen area, including sink and some utensils. D.2. Inference Phase: Temporal Narrative Guidance During inference, users provide prompts defined by time segments (seconds). The SNP strategy shifts focus to temporal progression (e.g., lighting changes) and attribute locking (e.g., clothing consistency). Table 5 demonstrates full generated sequence of professional presenter (0s 60s). The prompts explicitly guide the environment from dusk to deepest night while ensuring the subjects attributes (blonde hair, grey blazer) remain strictly consistent across all 12 segments. Table 5. Inference SNP Sequence (Presenter). continuous 60-second generation task guided by 12 sequential prompts. Time Segment Inference Prompt (SNP) 0s 5s 5s 10s 10s 15s 15s 20s 20s 25s 25s 30s 30s 35s 35s 40s 40s 45s 45s 50s 50s 55s 55s 60s woman with short blonde hair, wearing light grey blazer over white top, stands in front of cityscape at dusk. She is adorned with large, round earrings and has microphone clipped to her blazer. The background features tall skyscrapers with illuminated windows, reflecting the city lights. The scene emphasizes professional, realistic style. woman with short blonde hair, wearing light grey blazer over white t-shirt adorned with small black heart, stands in front of cityscape at dusk. She is equipped with black microphone and small, round, silver-colored earrings. The background features skyscrapers and twilight sky transitioning from blue to purple hues. She gestures while looking directly at the camera. The woman maintains neutral expression, looking directly at the camera. Her attirea light grey blazer over white t-shirt with microphone graphicremains consistent. The background persists as cityscape at dusk with tall skyscrapers and twilight sky, reinforcing the professional urban environment. Standing in professional studio setting, she holds pen in her right hand. She is adorned with black, curved hair clip. Behind her, the cityscape with tall buildings and twilight sky forms the backdrop. The composition remains clean and professional, with the woman as the focal point. The woman stands in front of the cityscape at dusk or night. She speaks and gestures. The background features tall buildings with illuminated windows against twilight gradient (blue to purple). The modern buildings with sleek lines contribute to contemporary skyline as she delivers her speech. She maintains serious expression with occasional lip movements. Her large, circular earrings are visible as she holds white object. The background features skyscrapers reflecting city lights against twilight sky. The blazer has notched lapel and single-breasted design. The woman adjusts her blazer collar. Behind her, the cityscape now shows deeper twilight hues with the first visible stars appearing. She holds tablet in her left hand while gesturing with her right. The microphones LED indicator glows red, confirming active recording. With relaxed posture, she leans against high table. Her t-shirts graphic aligns with her actual lapel mic. The city backdrop now features animated data visualizations projected onto the building facades, synchronizing with her speech about urban analytics. close-up shot reveals precise hand gestures. The lighting has progressed to full night, transforming the windows of distant skyscrapers into grids of golden light. Her blazers single button is now fastened, marking transition segment. The scene widens to reveal second camera operator reflected in the studio glass. The presenters blonde hair shows subtle highlights under the key lights. The cityscape now includes moving light trails from elevated trains weaving through the urban canyon. sudden laugh breaks her professional demeanor. In the background, fireworks display erupts over the city skyline, its colored bursts reflecting in the studios glass surfaces, adding dynamic energy to the night scene. She stands beside transparent touchscreen displaying metropolitan statistics. The deepest night sky makes the illuminated buildings appear to float in darkness, their glass facades mirroring the studio setup."
        }
    ],
    "affiliations": [
        "Chinese Academy of Sciences",
        "Peking University",
        "Sun Yat-sen University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}