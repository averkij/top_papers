{
    "paper_title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "authors": [
        "Zhaoyang Yu",
        "Jiayi Zhang",
        "Huixue Su",
        "Yufan Zhao",
        "Yifan Wu",
        "Mingyi Deng",
        "Jinyu Xiang",
        "Yizhang Lin",
        "Lingxiao Tang",
        "Yingchao Li",
        "Yuyu Luo",
        "Bang Liu",
        "Chenglin Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 6 5 3 2 . 0 1 5 2 : r RECODE: UNIFY PLAN AND ACTION FOR UNIVERSAL GRANULARITY CONTROL Zhaoyang Yu1, Jiayi Zhang1,2, Huixue Su3, Yufan Zhao3, Yifan Wu1,2, Mingyi Deng1, Jinyu Xiang, Yizhang Lin1, Lingxiao Tang4, Yingchao Li1, Yuyu Luo2, Bang Liu5, Chenglin Wu1 1DeepWisdom, 2The Hong Kong University of Science and Technology (Guangzhou), 3Renmin University of China, 4Zhejiang University, 5Université de Montréal & Mila zhaoyangyu713@gmail.com, alexanderwu@deepwisdom.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging unified cognitive representation where planning is fundamentally understood as high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose RECODE (Recursive Code Generation), novel paradigm that addresses this limitation by unifying planning and action within single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."
        },
        {
            "title": "INTRODUCTION",
            "content": "Human decision-making in the real world naturally operates across varying granularities, seamlessly integrating high-level planning with fine-grained actions. Consider the simple act of preparing breakfast: one effortlessly shifts from deciding high-level plan like making bacon and eggs to executing detailed, motor-level actions such as cracking an egg. Cognitive science attributes this fluid adaptability to the human brains integrated approach to control, which allows for decisions to be managed across different granularities. This integration is reflected in the brains shared representations for related cognitive processes (Prinz, 1997) and its hierarchical structure (Koechlin et al., 2003; Badre & Desposito, 2009). Thus, humans naturally master this dynamic control of decision granularity, an essential capability for adaptive, intelligent behavior. Achieving this adaptive intelligence is primary goal for LLM-based agents (Liu et al., 2025), yet current frameworks fall short because they operate at fixed granularities. The core issue is that existing approaches explicitly separate planning from action into distinct decision-making processes. For example, ReAct (Yao et al., 2023) agent as shown in Figure 1(a), alternates strictly between reasoning and primitive actions step-by-step, limiting it to fine-grained decision-making without strategic foresight. On the other hand, agent with planner module (Figure 1(b)) separates high-level planning from low-level action using predefined structures. Such rigid boundaries impede agents Corresponding author 1 Figure 1: comparison of LLM-based agent decision-making paradigms. (a) ReAct Agent operates in simple observation-action loop at fixed, fine-grained level. (b) Agent with Planner enforces rigid separation between high-level Planner and low-level Executor, limiting adaptability. (c) ReCode Agent unifies plan and action in code representation. The policy recursively refines high-level plans until primitive actions within single dynamic loop, enabling fluid control over decision granularities. from dynamically adjusting their decision granularity in response to evolving task complexities, ultimately resulting in brittle performance in complex, real-world environments. To address this failure, our key insight is that planning and action are not fundamentally distinct cognitive processes, but rather represent decisions at different levels of granularity. At its essence, plan is simply higher-level action, analogous to how pseudo code represents higher-level conceptual thinking compared to concrete, executable code. Motivated by this insight, we propose RECODE (Recursive Code Generation), novel paradigm that applies this idea by unifying plan and action within single code representation. As illustrated in Figure 1(c), ReCode implements this pseudo-code-like concept by treating high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching executable primitive actions. This recursive process inherently dissolves the rigid boundary between planning and action, enabling the agent to dynamically control its decision granularity. Furthermore, ReCodes recursive structure provides significant training advantage by inherently generating rich, hierarchical data. Unlike traditional approaches that yield flat action sequences, ReCode naturally produces structured decision trees capturing the entire cognitive process, from high-level planning down to executable actions. This comprehensive, multi-granularity training data significantly enhances the agents ability to learn complex task decompositions and adaptive decision-making strategies, improving both generalization and data efficiency. Extensive experiments conducted across diverse, complex decision-making benchmarks validate the effectiveness of the ReCode paradigm. ReCode consistently surpasses strong paradigm baselines such as ReAct (Yao et al., 2023) and CodeAct (Wang et al., 2024b), delivering substantial improvements in overall task performance and problem-solving efficiency. Furthermore, our experiments reveal that the hierarchical, multi-granular data generated by ReCode plays critical role in training flexible decision-making capabilities. These empirical results demonstrate ReCodes potential as transformative approach to building more adaptive, capable, and generalizable LLM-based agents. In summary, our contributions are as follows: We identify the lack of flexible decision granularity control as fundamental limitation in existing LLM-based agents, which stems from the rigid separation of plan and action. We propose ReCode, novel agent paradigm that achieves universal granularity control by unifying plan and action within single code representation. This unification is realized through recursive mechanism where the agent decomposes high-level placeholder functions until primitive actions. We demonstrate that this recursive structure inherently generates hierarchical, multi-granularity data, which enables models to learn hierarchical reasoning processes and leads to significant improvements in training data efficiency. We validate our approach through comprehensive experiments, showing that ReCode significantly surpasses strong baselines in both inference performance and training efficiency across diverse benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The powerful reasoning and language capabilities of Large Language Models (LLMs) have inspired significant research effort to create agents that can interact with complex environments and accomplish specific tasks (Ahn et al., 2022; Huang et al., 2023; Yao et al., 2023). These efforts have largely fallen into two dominant paradigms: agents following the ReAct paradigm (Yao et al., 2023) and agents with explicit planners. However, both approaches, in their current forms, struggle with the fundamental limitation of rigid separation between high-level planning and low-level action, which prevents flexible decision granularity control. LLM-based ReAct Agent Among these, paradigms like ReAct (Yao et al., 2023) are foundational, interleaving reasoning and primitive actions in step-by-step loop. Code has also emerged as an expressive medium for actions, with paradigms like CodeAct (Wang et al., 2024b) leveraging the extensive code pre-training of LLMs. These foundational paradigms have spawned numerous agent systems (Yang et al., 2023a; Wu et al., 2024; Zhang et al., 2025; Liang et al., 2025; Hu et al., 2025). Much of the subsequent research has focused on optimizing this step-by-step loop, such as through synthetic data generation methods (Zelikman et al., 2022; Yang et al., 2024b; Wang et al., 2024a) and novel training algorithms (Song et al., 2024; Qiao et al., 2024; Du et al., 2024; Lee et al., 2024; Xia et al., 2025; Xiong et al., 2025; Cao et al., 2025). While these optimization efforts improve performance, they do not address the paradigms fundamental lack of integrated, high-level planning, which limits its foresight in complex, long-horizon tasks. LLM-based Agent Planning Planning emerged as distinct research thrust, specifically to address this lack of foresight in ReAct Agent paradigm. Early approaches often adopted plan-andexecute strategy, where complete natural language plan is generated upfront before any actions are taken (Wang et al., 2023; Yang et al., 2023b; Kagaya et al., 2024; Sun et al., 2024a). However, these static plans are often brittle and difficult to adapt to dynamic environments. To address this, more sophisticated methods emerged, such as hierarchical planning frameworks (Paranjape et al., 2023; Sun et al., 2024b) and dynamic re-planning methods (Sun et al., 2023; Prasad et al., 2024), which continuously modify their plans based on environmental feedback. Nonetheless, these traditional planning methods still share the fundamental flaw of maintaining rigid separation between the high-level plan and the low-level action execution. While some recent approaches have begun to bridge this gap by explicitly integrating recursion or code-based processes (Liu et al., 2024; Schroeder et al., 2025; Zhang & Khattab, 2025), they still do not achieve full unification and adaptive granularity control. It is highlighted that the core limitation across all existing paradigms remains this rigid separation of plan and action. In contrast, our work is built on the key insight that plan and action are not distinct processes, but rather represent decisions at different levels of granularity. This forms the foundation for our paradigm, which is unifying plan and action in single representation."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PRELIMINARY LLM-based Agent Decision Process We model the interaction between an LLM-based agent and its environment as simplified decision-making process = S, A, O, T, R, where is the state space, is the primitive action space, is the observation space, : is the transition function, and : is the reward function. At each step, the agent receives an observation and produces decisions that ultimately translate into executable primitive actions A. Beyond the primitive action space, we introduce the plan space P, which encompasses intentions and goals at coarser granularities that cannot be directly executed but must be refined into sequences of primitive actions or intermediate sub-plans. We define the decision space as = AP, representing all possible outputs an agent can generate across different granularities. Current agent paradigms typically operate with predefined, fixed decision spaces. ReAct agents directly select from pre-determined action set A, while planner-based agents generate sequences over the same fixed or rely on manually specified plan templates. This constraint fundamentally limits adaptability, as agents cannot dynamically generate novel decisions suited to unforeseen contexts. 3 Figure 2: An overview of the ReCode. (a) The task instruction is transformed into an initial placeholder function via rule-based text-to-code method. (b) The system traverses the tree depth-first, automatically executing the code of current node and expanding placeholder functions into child nodes when encountered. (c) LLM-based expansion operates with clean context. Only the current function signature and available variables are provided, without any tree structure or execution history. Decision Granularity Real-world tasks demand decisions at varying granularities. Fine-grained decisions in correspond to immediately executable primitive actions such as run('crack egg'), while coarse-grained decisions in represent higher-level intentions requiring decomposition, such as prepare_breakfast(). The granularity forms natural hierarchy. Consider the breakfast preparation example. The decision prepare breakfast is coarser than cook eggs, which is in turn coarser than crack egg. Each level encompasses broader objectives and longer temporal horizons. The finest level consists of executable primitive actions, while the coarsest level represents the complete task specification. Motivated by the perspective that plans are essentially high-level actions at different abstraction levels, ReCode unifies decisions across all granularities within single code representation. Highlevel plans in take the form of placeholder functions, which are recursively refined into finergrained components until reaching executable primitive actions in A. This formulation enables the agent to dynamically generate decisions, whether plans or actions, at appropriate granularities for the given context, effectively creating an unbounded decision space. 3.2 METHOD OVERVIEW We introduce ReCode, an LLM-based agent paradigm with recursive code generation that achieves universal control of decision granularity. This paradigm begins by unifying plan and action into the same representation, enabling the entire decision-making process to be implemented through single, consistent operation. This unification naturally gives rise to tree structure where the highest-level task can be recursively decomposed until executable leaves. This dynamic process of building and executing decision tree is illustrated in Figure 2. Unify Plan and Action The key insight of ReCode is that plans and actions, despite their apparent differences, can be unified under single executable code representation. This unification addresses fundamental limitation in current LLM-based agent frameworks: the rigid separation between abstract planning and concrete primitive action, which prevents flexible decision granularity control. We represent both plans and actions as Python function calls. This establishes common computational substrate for the agents policy. Actions are executable and environment-specified that can directly interface with the environment, such as run('click the submit button'). Plans are expressed as unimplemented placeholder functions. These functions encapsulate sub-goals that require further expansion, like prepare_breakfast() and get_ingredients(). For example, high-level plan prepare_breakfast() can be expanded by the policy into code block containing other plans, like 4 get_ingredients() and cook_meal(). These, in turn, are recursively broken down until they resolve into sequence of primitive actions like run('open refrigerator') and run('turn on stove'). Recursive Code Generation Building on this unified representation, ReCode operates via recursive generation and execution loop, as detailed in Algorithm 1. The process begins with rule-based text-to-code transformation that converts the task instruction and initial observation into placeholder function, which is the root node. The agents policy model π then expands the current node into child code block. An executor processes the generated code block sequentially. When encountering primitive action, the executor directly executes it in the environment. When encountering placeholder function, the executor triggers an expansion by invoking ReCode again with this placeholder as the new current node. The decision tree grows in this process, gradually breaking down the task into more detailed plans until all leaves are executable primitive actions. The recursion terminates when generated code block contains only primitive actions. Execution then returns to the previous, coarser level. This process continues until the initial top-level placeholder is fully resolved, allowing the agent to seamlessly navigate across granularity levels. The policy adaptively determines when to maintain abstract planning and when to commit to concrete actions based solely on the current decision context, without explicit granularity supervision. The resulting execution trace forms hierarchical decision tree that captures the complete reasoning process from strategic planning to primitive execution. Algorithm 1 The ReCode Algorithm if is NONE then o0 RESET(E) TEXT2CODE(T, o0) end if code block π(c) for each child code unit in code block do Require: Task , Policy π, Environment E, current node 1: procedure RECODE(T, π, E, c) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end procedure if ISPRIMITIVE(u) then EXECUTE(u, E) RECODE(T, π, E, u) end for end if else Initialization Reset environment and get initial observation Convert task to root placeholder Generate children for current placeholder Primitive action Execute in environment Placeholder function Recursive expansion 3.3 IMPLEMENTATION DETAILS Bridging the ReCode paradigm from theory to practice requires several key engineering efforts to ensure robust and stable agent performance. Task Initialization The starting point for ReCode is converting the natural language task instruction into the root placeholder function. We employ simple rule-based method that directly encapsulates the task instruction and initial observation as string arguments within predefined template, such as solve(instruction, observation). This task-agnostic approach delegates the full responsibility of interpreting and decomposing the task to the learned policy model, compelling it to develop core planning capabilities from raw inputs and enhancing generalization across diverse tasks and environments. Context Management ReCode implements context management through unified variable namespace that persists throughout task execution. When expanding placeholder function, the system serializes all currently provided variables and their values into structured text description, which is injected into the prompt as context. After executing the generated code block, newly created or modified variables are updated in this shared namespace. This design creates hierarchical information flow where sub-tasks can access context established by parent tasks at any level of the 5 call stack. Crucially, the policy model only sees the current variable state, not the full execution history. This enforces explicit state management. The model must learn to consciously save important information to variables for future use. For example, capturing an actions output as obs = run('go to cabinet 1') allows subsequent inspection of the obs variable. This approach maintains concise and relevant context while guiding the model toward structured and deliberate planning. Error Handling and Recursion Control To ensure robust execution, ReCode addresses two practical challenges. First, code generated by LLMs is susceptible to syntax or runtime errors. We employ self-correction loop during inference. Upon execution failure, the system re-invokes the policy with the original placeholder and the error traceback as additional context, allowing it to recover from transient generation failures within single trajectory. Second, unbounded recursion can lead to infinite loops or excessively deep decomposition trees. We impose maximum recursion depth of 10 for our benchmark tasks, chosen as conservative upper bound above the empirically optimal depth (in Section 4.2), balancing planning complexity with guaranteed termination."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Environments We conduct experiments across three text-simulated environments that represent diverse decision-making challenges for LLM-based agents. ALFWorld (Shridhar et al., 2021) provides an embodied household environment where agents navigate and manipulate objects to complete daily tasks. WebShop (Yao et al., 2022) simulates an online shopping scenario requiring agents to search, compare, and purchase products based on specific requirements. ScienceWorld (Wang et al., 2022) presents scientific laboratory setting where agents conduct experiments and manipulate scientific instruments. All three environments are partially observable Markov decision processes where agents cannot fully infer the environment state from observations alone. For more detailed descriptions of the environments, please refer to Appendix A.1. Baselines To evaluate the effectiveness of ReCode, we divide our experiments into the inference part and the training part. For inference experiments, we compare against several mainstream paradigms: ReAct (Yao et al., 2023), which alternates between reasoning and action for iterative environment interaction; CodeAct (Wang et al., 2024b), extending ReActs action space from natural language to Python code. And some of the work focused on improving LLM-based agent planning: AdaPlanner (Sun et al., 2023), which pre-writes action sequences in Python and iteratively modifies them based on environmental feedback; and ADaPT (Prasad et al., 2024), which decomposes tasks on-demand using ReAct as the sub-task executor. ReAct CodeAct WebShop ALFWorld ScienceWorld Method Metric 27,607 18,958,782 100.0 12,833 15,356,801 100.0 0. 7,181 9,409,144 82.3 21.15 Data Pairs Tokens Avg. Reward% (Std) Data Pairs Tokens Avg. Reward% (Std) For training experiments, which evaluate how well paradigms data structure lends itself to learning, we conduct supervised finetuning (SFT) (Wei et al., 2022) experiments on ReCode, ReAct, and Codeto directly compare Act their learning efficiency. Also, we compare some advanced training methods on the ReAct-style agent, including ETO (Song et al., 2024), which learns from expert trajectories via Direct Preference Optimization (Rafailov et al., 2023); and WKM (Qiao et al., 2024), introducing parameterized world models for training on expert and synthetic trajectories. Due to differences in models, training setups, or environment configurations, we could not directly implement these latter works, but list them for reference. Table 1: Statistics of the SFT datasets. For ALFWorld, all filtered trajectories are successful tasks with reward of 1. Data Pairs Tokens Avg. Reward% (Std) 3,853 9,214,185 94.6 10.36 3,500 3,986,769 88.5 14.58 5,984 15,189,063 60.8 24.51 2,473 2,662,276 97.5 7.05 6,385 4,962,898 100.0 5,049 9,630,822 100.0 ReCode Evaluation We report average reward (%) as our primary metric across all experiments. The environments employ different reward settings. ALFWorld provides binary rewards (0 or 1) indicating 6 task completion, while WebShop and ScienceWorld offer dense rewards ranging from 0 to 1 based on task progress and accuracy. ALFWorld and ScienceWorld both include out-of-distribution evaluation sets to assess generalization capabilities. Implementation Details For inference experiments, all methods use GPT-4o mini (Hurst et al., 2024) to ensure fair comparison. We adapt few-shot prompts from prior work, making minimal modifications to fit each methods format while keeping the number of examples consistent. The few-shot examples used for ReCode are provided in Appendix A.3. For training experiments, we use Qwen2.5-7B-Instruct (Yang et al., 2024a) as the base model. We construct the training datasets through the following process: First, for each training instance, we use powerful teacher model, DeepSeek-V3.1 (DeepSeek-AI, 2024), to generate trajectories using ReAct, CodeAct, and ReCode respectively. Second, we filter these trajectories by keeping only the top 40% based on final reward. Third, from each retained trajectory, we extract input-output pairs as SFT training data, where the input excludes few-shot examples to focus on the models learning of the paradigm itself. Table 1 presents detailed statistics of the resulting training datasets. As shown, different paradigms produce vastly different amounts of training data from the same pool of source instances. ReCode generates significantly fewer but more compact data pairs (e.g., 6,385 pairs with 4.96M tokens on ALFWorld) compared to ReAct (27,607 pairs with 18.96M tokens). All models are trained using full-parameter fine-tuning on 8 NVIDIA A800 80GB GPUs with the open-source verl framework (Sheng et al., 2024). 4.2 INFERENCE RESULTS Method ALFWorld ScienceWorld Seen Unseen Seen Unseen WebShop Average ReAct GPT-4o mini CodeAct GPT-4o mini AdaPlanner GPT-4o mini ADaPT GPT-4o mini ReCode GPT-4o mini ReAct Gemini 2.5 Flash CodeAct Gemini 2.5 Flash ReCode Gemini 2.5 Flash ReAct DeepSeek-V3.1 CodeAct DeepSeek-V3.1 ReCode DeepSeek-V3. 59.29 72.14 75.00 34.29 83.57 85.71 52.86 90.00 90.71 51.43 95.71 64.18 85.07 92.54 41.04 96.27 85.07 63.43 96.27 88.06 59.70 94. 47.02 22.68 28.49 37.53 42.78 24.73 23.89 54.35 68.52 35.96 50.20 38.91 18.41 26.11 32.94 41.18 25.60 20.64 43.77 61.75 27.91 50. 27.62 21.37 29.17 32.79 39.97 39.74 43.67 46.57 22.94 36.50 55.07 47.4 43.9 50.3 35.7 60.8 52.2 40.9 66.2 66.4 42.3 69. Table 2: Average rewards on three environments in the few-shot inference setting. The table is divided into three sections based on the underlying model: GPT-4o mini, Gemini 2.5 Flash, and DeepSeek-V3.1. The optimal and suboptimal results in each section are marked in bold and underlined, respectively. Main Results As shown in Table 2, ReCode achieved significant performance improvements across all three environments, with an average score of 60.8, surpassing the best baseline method by 10.5 (relative 20.9%). Notably, compared to the ReAct and CodeAct paradigms, ReCode improved average performance by 13.4 and 16.9, respectively. The results demonstrate ReCodes exceptional capabilities across diverse task domains. In the ALFWorld environment, our method exhibits particularly strong performance, achieving high score of 83.57 on seen tasks and an outstanding 96.27 on unseen tasks, substantially outperforming all baseline methods. On WebShop, ReCode maintains competitive performance with score of 39.97, representing 21.9% improvement over ADaPT. In the ScienceWorld environment, while ReCode shows comparable performance to ReAct on seen data and notable improvement on unseen tasks. These consistent improvements across varied environments underscore the effectiveness of our approach in enhancing both task-specific performance and cross-domain adaptability. We provide detailed case studies in Section 4.4, showcasing representative trajectories from ALFWorld to illustrate ReCodes decision-making process. 7 Multiple Model Results To illustrate that the ReCode paradigm is applicable to most models, we also experiment with Gemini 2.5 Flash (Comanici et al., 2025) and DeepSeek-V3.1, with results shown in Table 2. The cross-model evaluation demonstrates ReCodes robustness and generalizability beyond single LLM. On Gemini 2.5 Flash, ReCode achieved an average performance of 66.2, maintaining consistent improvements over baseline methods. Similarly, when applied to DeepSeekV3.1, it obtained 69.2 average score, further validating the paradigms effectiveness. These results indicate that ReCodes performance gains are not dependent on specific model, but rather stem from the fundamental improvements in the reasoning and code generation process. Recursion Depth Analysis To validate our depth control mechanism, we conduct ablation experiments with varying maximum depth limits on the ScienceWorld seen set using GPT4o mini. As shown in Figure 3, performance exhibits clear inverted-U pattern, peaking at depth 8 (43.22%). Shallow depths restrict hierarchical decomposition, while excessive depths decline performance, likely due to over-decomposition fragmenting the decisionmaking process. The optimal range lies between depths 6-12. Based on these findings, we set the maximum depth to 10 across all environments, providing conservative upper bound that prevents unbounded recursion while rarely constraining practical reasoning. Figure 3: Effect of maximum recursion depth on agent performance in ScienceWorld seen set using GPT-4o mini. The star indicates the optimal depth achieving peak performance. Cost Analysis As shown in Table 3, ReCode is remarkably cost-efficient. On average, ReCode trajectory costs 78.9% less than ReAct and 84.4% less than CodeAct. This dramatic improvement stems from ReCodes structured exploration. Instead of the extensive, token-intensive, step-by-step reasoning cycles characteristic of reactive methods, ReCodes hierarchical decomposition allows the agent to make more abstract and potent decisions. This leads to shorter, more direct reasoning paths, significantly reducing the number of API calls and total tokens required to solve task while achieving superior performance. Method ALFWorld ScienceWorld Seen Unseen Seen Unseen WebShop Average ReAct CodeAct ReCode 10.55 5.24 2.11 9.76 5.92 1.99 10.29 13.42 1.91 11.07 12.13 2. 3.15 24.04 1.19 8.96 12.15 1.89 Table 3: The average cost (103 US dollars) on all benchmark environments using GPT-4o mini. All cost calculations refer to the official API pricing for GPT-4o mini. 4.3 TRAINING RESULTS This section demonstrates the effectiveness of ReCodes hierarchical data structure for agent training. We present comprehensive results across three environments and analyze the relationship between data and performance. Main Results Table 4 presents the training results using Qwen2.5-7B-Instruct. ReCode+SFT achieves strong average performance of 70.4% across all environments, surpassing both ReAct+SFT (67.6%) and CodeAct+SFT (55.8%). Notably, this performance is achieved with remarkable data efficiency. As shown in Table 1, ReCodes training set contains only 3,500 pairs, 3.7 times less than the 12,833 pairs used by ReAct, and its average reward (88.5%) is also lower than ReActs (100.0%). This demonstrates that ReCode+SFT can achieve superior results even with significantly less and potentially lower-quality training data. On individual benchmarks, ReCode+SFT shows dominant performance on ALFWorld and WebShop, and remains competitive on ScienceWorld. 8 Method ALFWorld ScienceWorld Seen Unseen Seen Unseen WebShop Average ReAct Qwen2.5-7B-Instruct CodeAct Qwen2.5-7B-Instruct ReAct+SFT Qwen2.5-7B-Instruct CodeAct+SFT Qwen2.5-7B-Instruct ReAct+ETO Llama-3-8B-Instruct ReAct+WKM Llama-3-8B-Instruct ReCode Qwen2.5-7B-Instruct ReCode+SFT Qwen2.5-7B-Instruct 57.86 67.86 90.00 86.43 64.29 68. 66.43 92.14 62.69 69.40 90.30 88.06 64.18 65.93 73.88 97.01 19.49 13.47 60.28 37.25 57.90 60. 15.84 59.85 14.85 9.29 54.64 33.59 52.33 54.75 15.38 52.39 32.33 37.43 42.61 33.48 64.57 66. 42.24 50.85 37.4 39.5 67.6 55.8 60.7 63.2 42.8 70.4 Table 4: Average rewards on three environments in the training setting. The optimal and suboptimal results in each section are marked in bold and underlined, respectively. Methods without our implementation, the performance were reported in the original paper of WKM. Data Efficiency Analysis To systematically investigate the source of ReCodes superior data efficiency, we conducted training on data subsets filtered by reward percentile {10, 20, 30, 40}. Figure 4 and Table 5 present the results on ScienceWorld. The results reveal distinct scaling behaviors. ReCode achieves 55.34% on seen tasks at = 20 using only 1,713 pairs, improving to 59.85% at = 40 with 3,500 pairs. In contrast, ReAct requires 12,833 pairs at = 40 to reach 60.28% on seen tasks. At matched percentiles, ReCode consistently outperforms ReAct with substantially fewer data pairs. At = 10, ReCode achieves 44.87% with 688 pairs versus ReActs 34.05% with 3,094 pairs (4.5 more). At = 20, ReCode reaches 55.34% with 1,713 pairs versus ReActs 36.34% with 6,320 pairs (3.7 more). Figure 4 shows that ReCodes curves rise steeply then plateau, while ReActs curves show gradual improvement requiring substantially more data, confirming that ReCodes hierarchical structure provides richer learning signals per training example. Method Percentile Data Pairs ReAct ReCode 10 20 30 40 10 20 30 3,094 6,320 9,488 12,833 688 1,713 2,499 3,500 ScienceWorld Seen Unseen 34.05 36.34 47.52 60.28 44.87 55.34 57.63 59. 31.66 30.96 44.82 54.64 38.33 49.93 50.97 52.39 Figure 4: Performance of ReCode (blue) and ReAct (red) on ScienceWorld under different filtering percentiles. Performance on seen tasks is represented by triangles, and performance on unseen tasks is represented by squares. Table 5: Training data statistics and corresponding ScienceWorld performance for ReAct and ReCode. The table lists the number of data pairs and the resulting average rewards for each method at different filtering percentiles (p). 4.4 CASE STUDY To provide concrete illustration of ReCodes decision-making process, we analyze representative trajectory from the ALFWorld environment for the task put two alarmclock in dresser. The complete execution trace and diagram of the code generation flow are available in Figure 5 of the Appendix A.4. Initially, the agent formulates hybrid decisions that integrate abstract sub-task placeholder functions with concrete, primitive actions. These initial decisions, shown in the following code block, include 9 functions like find_and_take for complex procedures that require further decomposition, and known, simple steps like run('go to dresser 1') as primitive actions. The agent then executes this hybrid code block sequentially. Primitive actions like run(...) are executed directly by the environment executor. While, placeholder functions such as find_and_take trigger an on-demand expansion, where the agent generates the necessary code to complete that specific sub-task just in time. This hybrid strategy allows the agent to maintain high-level, strategic view of the task while committing to concrete actions only when the steps are simple and certain. It combines the robustness of hierarchical planning with the efficiency of direct execution, providing flexible and powerful problem-solving framework that can adapt its level of abstraction as needed. # Initial setup and variable declaration all_location_IDs = declare_init_vars ( instruction , observation ) # Decompose task : find and place the first alarm clock obj_ID , all_location_IDs = find_and_take ( ' alarmclock ', all_location_IDs ) run ( 'go to dresser 1 ') put_in ( obj_ID , ' dresser 1 ') # Decompose task : find and place the second alarm clock update_all_location_IDs (...) obj_ID , all_location_IDs = find_and_take_again ( ' alarmclock ', all_location_IDs ) run ( 'go to dresser 1 ') put_in_again ( obj_ID , ' dresser 1 ')"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduced ReCode, novel paradigm for LLM-based agents that achieves universal control of decision granularity through recursive code generation. By unifying plan and action within single code representation, ReCode addresses fundamental limitations in current agent paradigms that rigidly separate decisions of different granularity. Our key insight is that planning is fundamentally high-level action at different granularities. This enables agents to dynamically decompose goals into hierarchical code structures without reliance on predefined action spaces. Extensive experiments across three diverse environments demonstrate ReCodes dual advantages. In inference, ReCode achieves superior performance with over 20.9% improvement. In training, ReCode exhibits remarkable efficiency using dramatically fewer data while achieving better results. The recursive structure naturally generates rich, multi-granularity training data that captures complete cognitive processes from strategic planning to concrete action. ReCode establishes foundation for more adaptive AI agents that can fluidly transition across decision granularities based on task complexity and situational demands."
        },
        {
            "title": "6 LIMITATIONS AND FUTURE WORK",
            "content": "While ReCode demonstrates significant potential, its effectiveness depends critically on the base models reasoning capabilities and few-shot example quality. The paradigm requires models to generate structured code within specific framework, and any deficiency in planning ability compromises subsequent decision quality. As observed in our experiments, ReCodes performance can be inconsistent, largely due to models difficulty adhering to the required code generation format or fully grasping the frameworks underlying logic. These limitations point to clear research directions centered on agent learning. primary focus should be enhancing models intrinsic ability to understand and operate within the ReCode framework through specialized pre-training objectives or comprehensive fine-tuning strategies. Reinforcement learning approaches could reward agents for creating efficient hierarchical plans, enabling them to optimize their own expansion processes. Additionally, exploring methods to make the framework more robust to generation errors and investigating automatic curriculum learning for progressive skill acquisition represent promising avenues. These directions would represent significant steps toward more autonomous and capable agent learning systems."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. David Badre and Mark Desposito. Is the rostro-caudal axis of the frontal lobe hierarchical? Nature reviews neuroscience, 2009. Zouying Cao, Runze Wang, Yifei Yang, Xinbei Ma, Xiaoyong Zhu, Bo Zheng, and Hai Zhao. Pgpo: Enhancing agent reasoning via pseudocode-style planning guided preference optimization. arXiv preprint arXiv:2506.01475, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2024. Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885, 2025. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. In NeurIPS 2024 Workshop on Open-World Agents, 2024. Etienne Koechlin, Chrystele Ody, and Frédérique Kouneiher. The architecture of cognitive control in the human prefrontal cortex. Science, 2003. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In International Conference on Machine Learning, 2024. Xinbin Liang, Jinyu Xiang, Zhaoyang Yu, Jiayi Zhang, Sirui Hong, Sheng Fan, and Xiao Tang. Openmanus: An open-source framework for building general ai agents, 2025. URL https: //doi.org/10.5281/zenodo.15186407. Anthony Liu, Xinhe Wang, Jacob Sansom, Yao Fu, Jongwook Choi, Sungryull Sohn, Jaekyeom Kim, and Honglak Lee. Interactive and expressive code-augmented planning with large language models. arXiv preprint arXiv:2411.13826, 2024. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From arXiv preprint brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv:2504.01990, 2025. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. 11 Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 42264252, 2024. Wolfgang Prinz. Perception and action planning. European journal of cognitive psychology, 1997. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Philip Schroeder, Nathaniel Morgan, Hongyin Luo, and James Glass. Thread: Thinking deeper with recursive spawning. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 84188442, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1074010749, 2020. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: In Proceedings of the 62nd Annual Exploration-based trajectory optimization of llm agents. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Advances in neural information processing systems, 36:5820258245, 2023. Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. In First Conference on Language Modeling, 2024a. Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. Pearl: Prompting large language models to plan and execute actions over long documents. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 469486, 2024b. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, 2023. Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. arXiv preprint arXiv:2402.11651, 2024a. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1127911298, 2022. 12 Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024b. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, 2024. Yu Xia, Yiran Shen, Junda Wu, Tong Yu, Sungchul Kim, Ryan Rossi, Lina Yao, and Julian McAuley. Sand: Boosting llm agents with self-taught action deliberation. arXiv preprint arXiv:2507.07441, 2025. Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, and Sujian Li. Mpo: Boosting llm agents with meta plan optimization. arXiv preprint arXiv:2503.02682, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024a. Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023a. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023b. Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. React meets actre: When language agents enjoy training data autonomy. arXiv preprint arXiv:2403.14589, 2024b. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 2022. Alex Zhang and Omar Khattab. Recursive language models, 2025. URL https://alexzhang13. github.io/blog/2025/rlm/. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATASETS ALFWorld ALFWorld (Shridhar et al., 2021) provides text-only game environment where the agent is required to complete series of long-horizon household tasks based on natural language instructions. Built upon the embodied vision benchmark ALFRED (Shridhar et al., 2020), ALFWorld inherits its six core task types: pick and place, examine in light, clean and place, heat and place, cool and place, and pick two and place. After executing command (e.g., go to countertop 1), the agent receives immediate textual feedback from the environment (e.g., You arrive at countertop 1. On the countertop 1, you see an egg 3, peppershaker 1, and tomato 1). ALFWorld only provides binary reward (1/0) to indicate whether the task was successfully completed. WebShop WebShop Yao et al. (2022) is an online shopping website environment constructed with 1.18 million real-world product data points. The agent must navigate the site by searching and clicking buttons to browse various products, with the goal of finding and purchasing specific item that meets the given requirements. We follow the data partitioning from MPO (Xiong et al., 2025), using test set of 200 instances. WebShop provides dense reward between 0 and 1 based on the agents actions and interactions with products. task is considered successful only if the reward is strictly equal to 1. ScienceWorld ScienceWorld is text-based simulation environment centered on completing basic scientific experiments. The agent needs to interact with the environment to measure certain values or to complete an experiment by applying scientific knowledge. The tasks in ScienceWorld are meticulously categorized, with each task having several variations and offering numerous different sub-goals. We adopt the data split from ETO (Song et al., 2024) and exclude Task-9 and Task-10 from the trajectories. ScienceWorld provides dense reward between 0 and 1 for each task, based on the completion of its sub-goals. The data statistics for each environment are summarized in Table 6. Table 6: Data statistics for the three environments Train Seen Unseen ALFWorld WebShop ScienceWorld 3553 1824 1483 140 200 194 134 - A.2 PROMPTS This appendix provides detailed prompt templates for ReAct, CodeAct and ReCode agents. ReAct Agent The ReAct agent employs reasoning and acting cycle pattern, solving problems by alternating between thinking and acting. The following are the prompt templates used in ALFWorld, ScienceWorld, and WebShop. ALFWorld Prompt of ReAct Agent ALFWorldPrompt = \"\"\" Interact with household to solve task. Your response should use the following format: Think: think ... or Action: --- Here are two examples. {examples} (End of examples) --- 14 Here is your task: \"\"\" ScienceWorld Prompt of ReAct Agent bedroom, greenhouse, art studio, hallway SciWorldPrompt = \"\"\" You are helpful assistant to do some scientific experiment in an environment. In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, (cid:44) You should explore the environment and find the items you need to complete the experiment. You can teleport to any room in one step. All containers in the environment have already been opened, you can directly get items from the (cid:44) containers. The available actions are: open OBJ: open container close OBJ: close container activate OBJ: activate device deactivate OBJ: deactivate device connect OBJ to OBJ: connect electrical components disconnect OBJ: disconnect electrical components use OBJ [on OBJ]: use device/item look around: describe the current room examine OBJ: describe an object in detail look at OBJ: describe container's contents read OBJ: read note or book move OBJ to OBJ: move an object to container pick up OBJ: move an object to the inventory pour OBJ into OBJ: pour liquid into container mix OBJ: chemically mix container teleport to LOC: teleport to specific room focus on OBJ: signal intent on task object wait: task no action for 10 steps wait1: task no action for step --- Here is the example: {examples} --- Now, it's your turn and here is the task. \"\"\" WebShop Prompt of ReAct Agent WebShopPrompt = \"\"\" You are doing web shopping task. will give you instructions about what to do. You have to follow the instructions. Every round will give you an observation, you have to respond to an action (cid:44) (cid:44) (cid:44) based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be one of the following structure: search[keywords] or click[value]. If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Think: think ... or Action: click[something]/search[keywords] --- Task: \"\"\" CodeAct Agent CodeAct agent solves problems by generating and executing Python code, interacting with the environment using specific functions. Prompt of CodeAct Agent CodeActPrompt = \"\"\" You are helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with variety of tool functions to assist you throughout (cid:44) the process. (cid:44) At each turn, you have two step to finish: 1. you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using \"<thought>\" tag, for example: <thought> need to print \"Hello World!\" </thought>. (cid:44) 2. After that, you should interact with Python programming environment and receive the corresponding output. Your code should be enclosed using \"<execute>\" tag, for example: <execute> print(\"Hello (cid:44) World!\") </execute>. (cid:44) The environment provides the following functions that you can only use: {tool_desc} Only use the functions above with `print()` to get environment feedback at the same time. --- Here is an example of how to use the functions: {in_context_example} (End of Example) --- \"\"\" ReCode Agent ReCode Agent shares set of prompt templates across all environments, providing different primitive action descriptions and examples based on the environment. Prompt of ReCode Agent ReCodePrompt = \"\"\" You are the EXPAND step in the LLM Agent loop. You need to replace the current placeholder function node (cid:44) with its code implementation. Decide how to implement the placeholder: - If the subtask of current function can be done in 1-2 primitive actions from the list below, write them (cid:44) - If it will take more than 2 primitive actions, instead break it into smaller placeholder functions. Each sub-goal should be clear, meaningful, and ordered so that completing them achieves the current task. (cid:44) directly using `run(action: str)`. All legal primitive actions are: {available_actions} And all of them should be used in the function `run(action: str) -> str`, which returns an observation in (cid:44) string format. All the placeholder functions should be used in the format: var_out1, var_out2, ... = (cid:44) (cid:44) snake_style_function_name(var_in1, var_in2=\"explicitly declared variables will also be registered\", ...), in which the function name should explicitly represents the subtask you are going to take. information is missing or uncertain (such as which target to use, what value to set, or which step to take next), write descriptive placeholder function that explicitly represents the missing decision), to be expanded later. Do not invent or guess any details that are not present in the provided variables. If essential (cid:44) (cid:44) (cid:44) Do not assume that any condition or prerequisite is already met unless explicitly confirmed. If something (cid:44) must be prepared, accessed, or changed, include explicit steps or sub-goals to do so. encluded with <think> and </think>. In your response: 1. Start with brief natural language explanation of how you will complete or break down the task, (cid:44) 2. Then output Python code with <execute> and </execute> tags, containing only valid actions or (cid:44) (cid:44) commands for this environment. Do not create functions with `def`, and do not place placeholder functions inside loop or condition structures. --- Here are some examples to guide the style and format, each example is ONLY ONE turn of the interaction: {examples} (End of Examples) --- The current function to expand is: {task} The variables you can use is: {variables} 16 \"\"\" A.3 FEWSHOT EXAMPLES We provide detailed examples used by the ReCode agent across different environments. Each example demonstrates the hierarchical decomposition approach where high-level tasks are progressively expanded into executable primitive actions. ALFWorld Examples The ReCode agent uses task-specific examples for different categories in ALFWorld. Each category represents distinct household task pattern with slight variations in logic while maintaining consistent structure. Here we present the cool task category as representative example. Cool Task Examples of ALFWorld ALFWorldExamples = \"\"\" [Input] The current function to expand is: solve(instruction, observation) The variables you can use is: - instruction (str): Your task is to: put cool mug in shelf. - observation (str): You are in the middle of room. Looking quickly around you, you see cabinet 9, cabinet 8, cabinet 7, cabinet 6, cabinet 5, cabinet 4, cabinet 3, cabinet 2, cabinet 1, (cid:44) coffeemachine 1, countertop 3, countertop 2, countertop 1, drawer 9, drawer 8, drawer (cid:44) 7, drawer 6, drawer 5, drawer 4, drawer 3, drawer 2, drawer 1, fridge 1, garbagecan (cid:44) 1, microwave 1, shelf 3, shelf 2, shelf 1, sinkbasin 1, stoveburner 4, stoveburner 3, (cid:44) stoveburner 2, stoveburner 1, and toaster 1. (cid:44) [Output] <think> The task requires cooling, which must be done with the only fridge present (`fridge 1`). The final target is shelf, and there are three shelves; will choose `shelf 1` as an explicit target. persist (cid:44) both IDs. also create `all_location_IDs` from the initial observation. The plan decomposes into (cid:44) (1) find and take mug by iterating known locations, handling closed receptacles via `open`; (2) (cid:44) cool the mug at the fridge after ensuring access; (3) place the mug on the chosen shelf. (cid:44) </think> <execute> import random cooling_location_ID = 'fridge 1' target_location_ID = 'shelf 1' obj = 'mug' all_location_IDs = re.findall(r'[a-z]+(?:[a-z]+)? d+', observation) random.shuffle(all_location_IDs) obj_ID = find_and_take(obj, all_location_IDs) cool_with(obj_ID, cooling_location_ID) put_in(obj_ID, target_location_ID) </execute> [Input] The current function to expand is: obj_ID = find_and_take(obj, all_location_IDs) The variables you can use is: - obj (str): mug - all_location_IDs (list[str]): ['cabinet 9', 'cabinet 8', 'cabinet 7', 'cabinet 6', 'cabinet 5', (cid:44) (cid:44) (cid:44) (cid:44) 'cabinet 4', 'cabinet 3', 'cabinet 2', 'cabinet 1', 'coffeemachine 1', 'countertop 3', 'countertop 2', 'countertop 1', 'drawer 9', 'drawer 8', 'drawer 7', 'drawer 6', 'drawer 5', 'drawer 4', 'drawer 3', 'drawer 2', 'drawer 1', 'fridge 1', 'garbagecan 1', 'microwave 1', 'shelf 3', 'shelf 2', 'shelf 1', 'sinkbasin 1', 'stoveburner 4', 'stoveburner 3', 'stoveburner 2', 'stoveburner 1', 'toaster 1'] [Output] <think> 17 The goal is to navigate the room methodically to find any instance of the target object and pick it up. will iterate through the known locations, always navigating first. If location produces no new (cid:44) information, will examine it. When location is closed, will open it before searching. Once (cid:44) observe concrete `{obj} k` mention, will parse that exact ID and take it from the current (cid:44) location. Common pitfalls include not examining after navigation (missing items), failing to open (cid:44) closed container, or taking without the full `obj_ID`. (cid:44) </think> <execute> for location_ID in all_location_IDs: obs = run(f'go to {location_ID}') if 'Nothing happens.' in obs: obs = run(f'examine {location_ID}') location_closed = 'closed' in obs if location_closed: obs = run(f'open {location_ID}') if f\" {obj} \" in obs: # Prevent names from containing relationships obj_ID = re.search(rf'b{obj} d+b', obs).group() run(f'take {obj_ID} from {location_ID}') break </execute> [Input] The current function to expand is: cool_with(obj_ID, cooling_location_ID) The variables you can use is: - obj_ID (str): mug 1 - cooling_location_ID (str): fridge 1 [Output] <think> Cooling is only valid with fridge. will ensure access to the fridge: navigate and issue the cooling command with the exact object ID. Pitfalls: using the wrong station (sink/microwave) or skipping (cid:44) navigation. (cid:44) </think> <execute> run(f'go to {cooling_location_ID}') run(f'cool {obj_ID} with {cooling_location_ID}') </execute> [Input] The current function to expand is: put_in(obj_ID, target_location_ID) The variables you can use is: - obj_ID (str): mug 1 - target_location_ID (str): shelf 1 [Output] <think> will navigate to the target location, examine if nothing happens, determine if it is closed, and open (cid:44) (cid:44) (cid:44) </think> if needed. Then will move the object in. This logic supports both containers (openable) and surfaces (non-openable, but harmless to check). Pitfalls: using the wrong ID in `examine` or skipping the `location_closed` check. <execute> obs = run(f'go to {target_location_ID}') if 'Nothing happens.' in obs: obs = run(f'examine {target_location_ID}') location_closed = 'closed' in obs if location_closed: obs = run(f'open {target_location_ID}') run(f'move {obj_ID} to {target_location_ID}') </execute> \"\"\" ScienceWorld Examples The ScienceWorld examples demonstrate scientific experiment tasks that require domain knowledge application and spatial reasoning. The agent must navigate between rooms, manipulate objects, and perform chemistry or physics operations. Here we present representative chemistry task: creating green paint through color mixing. 18 ScienceWorld Examples green paint. SciWorldExamples = \"\"\" [Input] The current function to expand is: solve(instruction, observation) The variables you can use is: - instruction (str): Your task is to use chemistry to create green paint. When you are done, focus on the (cid:44) - observation (str): This room is called the bedroom. In it, you see: the agent substance called air bed. On the bed is: mattress. On the mattress is: white pillow. closet. The closet door is open. In the closet is: nothing. picture table. On the table is: nothing. You also see: door to the hallway (that is open) [Output] <think> need to find materials to create green paint, which are likely to be found in an art studio. The (cid:44) (cid:44) </think> bedroom doesn't have the materials need, so should go to place where paint and painting supplies are likely to be found. <execute> run(\"teleport to art studio\") obs = run(\"look around\") solve(instruction, obs) </execute> green paint. [Input] The current function to expand is: solve(instruction, obs) The variables you can use is: - instruction (str): Your task is to use chemistry to create green paint. When you are done, focus on the (cid:44) - obs (str): This room is called the art studio. In it, you see: the agent substance called air large cupboard. The large cupboard door is open. In the large cupboard is: drawer, wood cup (cid:44) table. On the table is: glass cup (containing nothing). wood cup (containing yellow paint) wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is open) (containing yellow paint), wood cup (containing blue paint), wood cup (containing red paint). [Output] <think> Now that I'm in the art studio, can see blue paint and yellow paint which are the primary colors needed (cid:44) (cid:44) </think> to mix green paint. should call the mix_blue_and_yellow_paints function to handle the mixing process, then look around to see the result, and finally focus on the green paint. <execute> mix_blue_and_yellow_paints(obs) obs = run(\"look around\") focus_on_green_paint(obs) </execute> [Input] The current function to expand is: mix_blue_and_yellow_paints(obs) The variables you can use is: - obs (str): This room is called the art studio. In it, you see: the agent substance called air large cupboard. The large cupboard door is open. In the large cupboard is: drawer, wood cup (cid:44) table. On the table is: glass cup (containing nothing). wood cup (containing yellow paint) (containing yellow paint), wood cup (containing blue paint), wood cup (containing red paint). 19 wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is open) [Output] <think> can see blue paint and yellow paint available. need to follow the chemistry process: first pour the blue paint into the empty glass cup, then add yellow paint to the same container, and finally mix (cid:44) them together to create green paint. This follows the principle that blue and yellow are primary (cid:44) colors that combine to make green. (cid:44) </think> <execute> run(\"pour cup containing blue paint in art studio into cup containing nothing\") obs = run(\"pour cup containing yellow paint in art studio in cup containing blue paint in table\") obs = run(\"mix cup containing blue paint and yellow paint\") </execute> [Input] The current function to expand is: focus_on_green_paint(obs) The variables you can use is: - obs (str): This room is called the art studio. In it, you see: the agent substance called air large cupboard. The large cupboard door is open. In the large cupboard is: drawer, wood cup (cid:44) table. On the table is: glass cup (containing green paint). wood cup (containing yellow paint) wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is open) (containing yellow paint), wood cup (containing blue paint), wood cup (containing red paint). [Output] <think> Perfect! can see that the glass cup now contains green paint. The task is to create green paint and then focus on it. Now that the green paint is made, should concentrate on the final product. (cid:44) </think> <execute> run(\"focus on green paint\") </execute> \"\"\" A.3.1 WEBSHOP EXAMPLES The WebShop examples demonstrate e-commerce navigation tasks where the agent must search for products, evaluate options against specified requirements, and complete purchases. Note that since clickable buttons in the environment are represented with square brackets (e.g., [Search], [Buy Now]), we use INPUT: and OUTPUT: instead of [Input] and [Output] in these examples to avoid confusion. WebShop Examples WebShopExamples = \"\"\" INPUT: The current function to expand is: solve(instruction, observation) The variables you can use is: - instruction (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) - observation (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) [Search] 100.00 dollars 100.00 dollars OUTPUT: <think> need 6.76 fl oz bottle of L'eau d'Issey perfume with price lower than $100. need to: 1. First search for relevant products 20 2. Click to view product details 3. Decide whether to buy or exclude based on price and specification requirements Let me execute this task step by step. </think> <execute> search_obs = search_with_key_words(instruction, observation) item_id, detail_obs = click_item(search_obs, instruction) exclude_or_buy(search_obs, detail_obs, instruction) </execute> INPUT: The current function to expand is: search_obs = search_with_key_words(instruction, observation) The variables you can use is: - instruction (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) - observation (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) [Search] 100.00 dollars 100.00 dollars OUTPUT: <think> need to use the search function to find products that meet the criteria: 6.76 fl oz bottle of L'eau d'Issey with price lower than $100. The search keywords should include the product name, size (cid:44) specification, and price constraint. (cid:44) </think> <execute> search_obs = run(\"search[l'eau d'issey 6.76 fl oz bottle price < 100.00]\") </execute> INPUT: The current function to expand is: item_id, detail_obs = click_item(search_obs, instruction) The variables you can use is: - search_obs (str): [Back to Search] Page 1 (Total results: 50) [Next >] [B000VOHH8I] L'eau D'issey By Issey Miyake for MenEau De Toilette Spray, 6.7 Fl Oz Bottle $64.98 [B000MJZOPK] L'eau d'Issey by Issey Miyake for Women 3.3 oz Eau de Toilette Spray $49.98 [B0012S249E] L'eau D'issey By Issey Miyake For Women. Shower Cream 6.7-Ounces $31.36 [B01H8PGKZS] L'eau D'Issey FOR MEN by Issey Miyake - 6.7 oz EDT Spray $67.97 [B00G3C8FHE] L'Eau d'Issey pour Homme - Eau de Toilette 4.2 fl oz $51.25 [B000R94HRG] Issey Miyake L'Eau D'Issey Pour Homme Eau De Toilette Natural Spray $44.99 [B000C214CO] Issey Miyake L'eau D'issey Eau de Toilette Spray for Men, 4.2 Fl Oz $53.99 [B0018SBRDC] Issey Miyake L'eau d'Issey for Women EDT, White, 0.84 Fl Oz $27.04 [B000XEAZ9Y] L'eau De Issey By Issey Miyake For Men. Eau De Toilette Spray 6.7 Fl Oz $67.08 [B079HZR2RX] L'eau d'Issey Pure by Issey Miyake for Women 3.0 oz Nectar de Parfum Spray $71.49 - instruction (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) 100.00 dollars OUTPUT: <think> 21 matches my requirements: From the search results, can see multiple L'eau d'Issey products. need to choose the one that best (cid:44) - need 6.76 fl oz size - Price should be lower than $ Looking at the search results, product B000VOHH8I is \"L'eau D'issey By Issey Miyake for Men Eau De (cid:44) (cid:44) </think> Toilette Spray, 6.7 Fl Oz Bottle\" priced at $64.98. This is the closest to my required 6.76 fl oz size and is under $100. should click on this product to view its details. <execute> item_id = \"B000VOHH8I\" detail_obs = run(f\"click[{item_id}]\") </execute> INPUT: The current function to expand is: exclude_or_buy(search_obs, detail_obs, instruction) The variables you can use is: - search_obs (str): [Back to Search] Page 1 (Total results: 50) [Next >] [B000VOHH8I] L'eau D'issey By Issey Miyake for MenEau De Toilette Spray, 6.7 Fl Oz Bottle $64.98 ... - detail_obs (str): [Back to Search] [< Prev] size [2.5 fl oz] [6.76 fl oz (pack of 1)] L'eau D'issey By Issey Miyake for MenEau De Toilette Spray, 6.7 Fl Oz Bottle Price: $64.98 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] - instruction (str): need long lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than (cid:44) 100.00 dollars OUTPUT: <think> I'm looking at the product details, which shows L'eau D'issey By Issey Miyake for Men Eau De Toilette (cid:44) 1. 6.76 fl oz bottle size 2. L'eau d'Issey brand 3. Price lower than $ Spray. My requirements are: This product meets all requirements: - The exact 6.76 fl oz size option is available - It's the correct brand - The price $64.98 is well under $100 should select the 6.76 fl oz size option and proceed with the purchase. </think> <execute> run(\"click[6.76 fl oz (pack of 1)]\") run(\"click[Buy Now]\") </execute> \"\"\" A.4 CASE STUDY To provide concrete illustration of ReCodes decision-making process, we analyze representative trajectory from the ALFWorld environment for the task put two alarmclock in dresser. The agents complete execution flow, derived from the log file, is detailed below. Figure 5 provides high-level visualization of this recursive process. The agents primary goal is to place two alarm clocks in dresser. Given that it can only carry one object at time, sequential find-and-place strategy is necessary. 22 Figure 5: visualization of the ReCode execution flow for the task put two alarmclock in dresser in ALFWorld. The diagram shows how high-level plan, composed of placeholder functions (e.g., find_and_take_again), is recursively expanded on demand. Each arrow points from function call to the code block it generates. This process illustrates the dynamic transition from abstract planning to the generation of fine-grained, executable code. The following steps detail the agents turn-by-turn execution. It is important to note that this is not pre-generated script. Instead, the agent starts with the high-level plan from Step 1 and executes it line by line. When the executor encounters an unimplemented placeholder function (e.g., declare_init_vars), execution pauses, and recursive call is made to the LLM to generate the necessary code for that specific function. The newly generated code is then executed, and this cycle repeats until the entire plan is resolved. Step 1: Initial High-Level Plan First, the agent formulates an initial high-level plan by expanding the root solve function. This plan breaks down the complex task into sequence of abstract placeholder functions without specifying low-level primitive actions. This strategic scaffold includes steps for finding and placing each of the two alarm clocks, as well as updating its internal knowledge about object locations. # Initial plan generated by expanding solve (...) obj , target_location_ID , all_location_IDs = declare_init_vars ( instruction , observation ) 23 # Decompose task : find and place the first alarm clock obj_ID , location_ID = find_and_take ( obj , all_location_IDs ) put_in ( obj_ID , target_location_ID ) # Decompose task : find and place the second alarm clock all_location_IDs = update_all_location_IDs ( location_ID , target_location_ID , all_location_IDs ) obj_ID = find_and_take_again ( obj , all_location_IDs ) put_in_again ( obj_ID , target_location_ID ) Step 2: Declaring Initial Variables The agent begins by executing the first placeholder, declare_init_vars. It expands this function to parse the initial instruction and observation, identifying key variables. This step grounds the agent by establishing the target object (alarmclock), the destination (dresser 1), and comprehensive list of all searchable locations. # Code generated by expanding declare_init_vars (...) target_location_ID = ' dresser 1 ' obj = ' alarmclock ' all_location_IDs = [ ' bed 1 ', ' desk 1 ', ' drawer 5 ', ' drawer 4 ', ' drawer 3 ' , ' drawer 2 ', ' drawer 1 ', ' dresser 1 ', ' garbagecan 1 ' , ' laundryhamper 1 ', ' shelf 1 '] Step 3: Finding and Taking the First Alarm Clock Upon successfully executing the variable declarations, the executor proceeds to the next line in the plan: the find_and_take placeholder. This triggers another on-demand expansion cycle, prompting the agent to generate the following code for methodical search: # Code generated by expanding find_and_take (...) for location_ID in all_location_IDs : obs = run (f 'go to { location_ID } ') if ' Nothing happens . ' in obs : obs = run (f ' examine { location_ID } ') location_closed = ' closed ' in obs if location_closed : obs = run (f ' open { location_ID } ') if f\" { obj } \" in obs : obj_ID = re . search ( rf 'b{ obj } + ' , obs ) . group () run (f ' take { obj_ID } from { location_ID } ') break # Key environment feedback during execution : # You arrive at desk 1. On the desk 1, you see alarmclock 3, alarmclock 2... # You pick up the alarmclock 3 from the desk 1. Step 4: Placing the First Alarm Clock After the code from the previous step successfully executes and finds \"alarmclock 3\", the agent continues to the next task in its high-level plan, put_in. This function is then expanded to generate the code for navigating to the dresser and placing the object inside. # Code generated by expanding put_in (...) obs = run (f 'go to { target_location_ID } ') if ' Nothing happens . ' in obs : obs = run (f ' examine { target_location_ID } ') location_closed = ' closed ' in obs if location_closed : obs = run (f ' open { target_location_ID } ') run (f ' move { obj_ID } to { target_location_ID } ') # Environment feedback : # You move the alarmclock 3 to the dresser 1. 24 Step 5: Updating Location Knowledge After placing the first clock, the agent must update its search space for the second clock. It expands update_all_location_IDs, which intelligently slices the location list to resume searching from the last successful spot (desk 1) and removes the destination (dresser 1) to prevent redundant checks. # Code generated by expanding update_all_location_IDs (...) # location_ID is ' desk 1' from the previous step . all_location_IDs = all_location_IDs [ all_location_IDs . index ( location_ID ) :] if target_location_ID in all_location_IDs : all_location_IDs . remove ( target_location_ID ) # Resulting all_location_IDs for next search : # [' desk 1', ' drawer 5', ... , ' shelf 1 '] Step 6 & 7: Finding and Placing the Second Alarm Clock Finally, the agent repeats the findand-place cycle by expanding find_and_take_again and put_in_again. It reuses the same logic as before but operates on the updated location list. It successfully finds \"alarmclock 2\" on \"desk 1\", picks it up, navigates back to the dresser, and places it inside, completing the task. # Agent expands find_and_take_again (...) and finds the second clock . # Environment feedback : # You arrive at desk 1. On the desk 1, you see alarmclock 2, alarmclock 1... # You pick up the alarmclock 2 from the desk 1. # Agent expands put_in_again (...) and places the second clock . # Environment feedback : # You move the alarmclock 2 to the dresser 1."
        }
    ],
    "affiliations": [
        "DeepWisdom",
        "Renmin University of China",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Université de Montréal & Mila",
        "Zhejiang University"
    ]
}