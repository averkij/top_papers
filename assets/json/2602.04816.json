{
    "paper_title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
    "authors": [
        "Zhengqing Yuan",
        "Lichao Sun",
        "Yanfang",
        "Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training."
        },
        {
            "title": "Start",
            "content": "Horizon-LM: RAM-Centric Architecture for LLM Training Single-GPU Training of Hundreds-of-Billions Parameter Language Models with Mixed BF16/FP32 Precision Code: https://github.com/DLYuanGod/Horizon-LM Zhengqing Yuan1 Lichao Sun2 Yanfang (Fanny) Ye1 1University of Notre Dame 2Lehigh University 6 2 0 2 4 ] . [ 1 6 1 8 4 0 . 2 0 6 2 : r Abstract The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As result, scaling large models remains tightly coupled to multiGPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, memory-centric training system that redefines the roles of CPU and GPU for largemodel optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On single H200 GPU with 1.5 TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On standard single A100 machine, Horizon-LM achieves up to 12.2 higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) have achieved tremendous advances\nin recent years [8, 11, 17, 22, 24], powering a new generation of\nAI systems ranging from coding copilots [3, 4] and scientific assis-\ntants to multimodal [16] and multi-agent frameworks [10, 29]. A\nconsistent empirical finding is that larger models tend to achieve\nbetter performance across a wide spectrum of capabilities, including\nreasoning, generalization, and long-horizon planning [13]. Conse-\nquently, training larger foundation models has become a dominant\ntrend, triggering massive investments in computational infrastruc-\nture and exposing critical limitations of existing training systems.\nOver the past few years, large language models have rapidly\nevolved from hundred-billion-parameter systems to foundation\nmodels approaching or exceeding the trillion-parameter scale [19,\n25]. This explosive growth in model capacity has far outpaced the\nimprovement of single-GPU hardware. For example, mainstream\ndata-center accelerators have only increased from 80 GB HBM",
            "content": "Figure 1: Sustained TFLOPS across model scales on singal GH200 (Qwen2.5 for 7B-32B) and H200 (Qwen2.5 72B and GPT-oss 120B). HorizonLM remains efficient while offloading baselines become GPU memory-bound. on A100 GPUs [18] to under 180 GB on B200 [14], representing modest growth in per-device memory capacity. At the same time, the economic cost of scaling via GPUs has risen sharply. Individual high-end accelerators now cost on the order of $60,000 [23], and fully populated multi-GPU servers can exceed $510,000 [1], making large-scale GPU clusters increasingly expensive to acquire and operate. Moreover, recent surveys of U.S. universities indicate that among 167 institutions, only two universities achieve an average availability of more than one H100 GPU per student [7], highlighting the extreme scarcity and centralization of large-scale GPU resources. As result, the prevailing approach to large-model training, scaling out through massive GPU parallelism, faces growing memory, cost, and accessibility barriers. At the same time, the center of innovation is shifting beyond pretraining toward post-training regimes, including instruction tuning, alignment, domain adaptation, and agent specialization [15, 26]. Modern AI systems increasingly rely on repeatedly adapting large pretrained models to new tasks, tools, and interaction protocols [9]. Unlike trillion-parameter pretraining, these workloads are comparatively lightweight in computation and, in principle, could be performed on single node [31]. But fine-tuning or aligning existing models requires loading full model parameters and optimizer states into memory, rendering hundred-billion-parameter models inaccessible on commodity systems [30]. This creates fundamental mismatch: while LLM development is transitioning toward memorybound, node-scale post-training, existing training systems remain optimized for compute-bound, cluster-scale pretraining [20, 28]. As result, data scientists and application developers are effectively prevented from experimenting with and customizing the most capable foundation models. Recent large-model training systems have converged on offloadingbased designs, exemplified by DeepSpeed ZeRO [19] and ZeROInfinity [20], which extend GPU memory by dynamically migrating model states across GPU, CPU, and storage tiers. These approaches substantially increase effective capacity and have enabled trillionparameter training at cluster scale. However, they fundamentally preserve GPU-centric training paradigm: GPUs remain the owners of long-lived model replicas and full autograd graphs, while CPU and NVMe serve only as auxiliary spill buffers. This design choice imposes fundamental structural limitations. 1) First, as long as model execution is anchored to persistent GPUresident modules and full autograd graphs, large-model training inherently requires multi-GPU clusters and complex distributed orchestration [12]. Offloading can alleviate memory pressure, but it does not change the training unit: parameters must still be repeatedly gathered onto GPUs to instantiate long-lived model replicas. Consequently, large-model training remains intrinsically tied to multi-GPU execution and cannot operate in truly node-scale regime. 2) Second, GPU-centric offloading frameworks do not transform host memory into true parameter store [33]. Instead, CPU memory is repurposed as distributed runtime heap that simultaneously holds partitioned parameters, optimizer states, gradients, communication buffers, and autograd metadata. As result, host memory consumption is governed not only by model size, but by the complexity of the training runtime itself, making memory usage difficult to control, predict, or scale linearly with model capacity. 3) Third, because model execution, gradient propagation, and memory management are all entangled within the distributed training runtime, existing systems fundamentally lack the ability to bound and structure memory usage [5]. Even with abundant host memory, offloading frameworks cannot reliably exploit it as clean capacity resource, and frequently encounter host-side memory exhaustion when scaling to hundred-billion-parameter models on single GPU. We therefore present Horizon-LM, memory-centric largemodel training system that redefines the roles of CPU and GPU in the training pipeline. Horizon-LM treats host memory as the primary parameter store and uses GPUs only as transient compute engines, enabling hundred-billion-parameter models to be trained and adapted on single GPU with large host memory. 1) First, to eliminate the dependence on persistent GPU-resident models and full autograd graphs, Horizon-LM adopts CPU-master, GPUtemplate execution model. All model parameters and optimizer states reside exclusively in host memory, while GPUs host only lightweight, reusable layer templates. During training, parameters are streamed into GPU buffers on demand, used for computation, and immediately released, removing the need for long-lived GPU model replicas and fundamentally breaking the coupling between model scale and GPU count. 2) Second, to transform host memory into true parameter store rather than distributed runtime heap, Horizon-LM employs explicit execution and gradient propagation. Instead of constructing full training graphs, Horizon-LM performs block-wise recomputation and manual gradient propagation, with tightly controlled buffer lifecycles and bounded memory pools. This design makes host memory usage predictable, modeldominated, and largely independent of training-system complexity. 3) Third, to fully exploit large host memory while maintaining high device utilization, Horizon-LM introduces pipelined, doublebuffered GPU execution engine. Horizon-LM overlaps parameter prefetching, computation, and gradient offloading across multiple CUDA streams, enabling continuous GPU execution while keeping memory footprints bounded. This allows Horizon-LM to efficiently operate in node-scale regime and reliably support hundred-billionparameter post-training workloads on single GPU. In our evaluation, we demonstrate that Horizon-LM can train models up to 120B parameter scale on single H200 GPU equipped with 1.5TB RAM, regime that existing offloading-based systems fail to reliably support. Furthermore, at regular singal A100 machine, Horizon-LM achieves up to 12.2 higher training throughput than DeepSpeed ZeRO-3 with CPU offloading, while better final solutions. Moreover, Horizon-LM sustains high and stable training throughput across wide range of model scales."
        },
        {
            "title": "Frameworks",
            "content": "To address the GPU memory bottleneck in large-model training, class of systems has emerged that extend effective device memory by offloading model states across the GPUCPUstorage hierarchy. Classic examples include ZeRO-Offload and ZeRO-Infinity in DeepSpeed [19, 20], Offloading in FSDP [33] and Gemini in ColossalAI [6], which migrate parameters and optimizer states between GPU memory, host RAM, and NVMe to increase trainable scale. Recent work continues in this space: for example, TERAIO generalizes lifetime-aware tensor offloading across SSDs to better overlap migration with computation and reduce offloading overhead [32], and other heterogeneous resource-aware caching and migration schemes further optimize memory tier usage for LLM training [2]. These frameworks enable training of models far beyond native GPU memory, and increasingly exploit CPU and storage tiers to alleviate device pressure. Despite their effectiveness, these approaches fundamentally preserve the GPU-centric execution model: GPUs still host the executable module and full autograd graphs, and parameters must be repeatedly gathered on device for forward and backward passes."
        },
        {
            "title": "3 Design Challenges\n3.1 Memory Requirement\nHorizon-LM targets a node-scale regime in which modern post-\ntraining workloads increasingly operate on 7Bâ€“70B models, and\nstructurally extend to hundred-billion-parameter scales, by treating\nhost memory as the sole persistent parameter store. Under this\nregime, feasibility is fundamentally determined by two structural\nconditions: (i) host memory must scale linearly with model size\nunder mixed-precision training, and (ii) GPU memory must be\nbounded by the maximum per-layer footprint rather than total\nmodel size.\nMixed-precision host memory model. Horizon-LM adopts a mixed-\nprecision layout in which model parameters and gradients are\nstored in BF16, while optimizer states are maintained in FP32 on\nthe host. This layout matches the execution model of our system,\nwhich uses BF16 CPU-resident parameters and gradients together\nwith FP32 Adam moments. Let ğ‘ƒ denote the total number of model\nparameters. Let ğµğœƒ =2 bytes for BF16 weights, ğµğ‘”=2 bytes for BF16\ngradients, and ğµğ‘š=8 bytes for FP32 Adam optimizer states (first and\nsecond moments). The minimum host memory required to sustain\ntraining is therefore:",
            "content": "(1) ğ‘€ min CPU ğ‘ƒ (ğµğœƒ + ğµğ‘” + ğµğ‘š) = 12ğ‘ƒ bytes. This bound already implies terabyte-scale host memory requirements. For example, 100B-parameter model requires at least 1.2 TB of host memory for persistent states alone, while 300Bparameter model exceeds 3.6 TB, excluding dataset buffers and runtime overheads. Consequently, in the target regime, host memory provisioningnot GPU memorybecomes the primary capacity constraint. Host memory must be clean, model-proportional resource. In existing GPU-centric offloading systems, host memory is repurposed as distributed runtime heap that simultaneously holds partitioned parameters, optimizer states, gradients, communication buffers, and autograd metadata. As result, host memory consumption depends not only on model size, but also on runtime complexity, communication patterns, and graph structure, making it difficult to predict, bound, or reliably provision memory at the terabyte scale. In contrast, Horizon-LM requires host memory to behave as structured parameter store. Beyond persistent model and optimizer states, host memory contains only small, fixed-capacity set of pinned slabs for staging and gradient return. Thus, host memory consumption follows: ğ‘€CPU ğ‘ƒ (ğµğœƒ + ğµğ‘” + ğµğ‘š) + ğ‘†slab + ğ‘‚ (ğ‘ƒmax), where ğ‘†slab denotes bounded pinned-memory pools and ğ‘ƒmax is the maximum parameter size of any single layer. This makes host memory usage predictable, model-dominated, and linearly scalable with ğ‘ƒ. (2) GPU memory must be bounded by per-layer footprint. Under Horizon-LMs execution model, the GPU never hosts persistent model replica. Let ğ‘ƒğ‘– denote the parameter size of layer ğ‘– in bytes, and define ğ‘ƒmax = maxğ‘– ğ‘ƒğ‘– . Let ğ´max denote the maximum activation and operator workspace required by any single layer, and let ğ¾ be the activation checkpoint interval. streaming execution engine must satisfy depth-independent bound of the form: ğ‘€GPU ğ‘ğ‘ ğ‘ƒmax + ğ‘ğ‘ ğ¾ ğ´max + ğ‘ŠGPU, (3) where ğ‘ğ‘ captures constant-factor buffering (e.g., double buffering for prefetch and execution), ğ‘ğ‘ captures checkpoint storage, and ğ‘ŠGPU denotes bounded operator workspaces. In Horizon-LM, this bound is realized by constant number of reusable GPU layer templates, double-buffered streaming parameter buffers, and bounded set of activation checkpoints. Crucially, Eq. 3 is independent of total model depth and total parameter count. Width-sensitive feasibility. For Transformer models, total parameters scale as ğ‘ƒ = Î˜(ğ‘›ğ‘™â„2), while the largest per-layer footprint scales as ğ‘ƒmax = Î˜(â„2), where ğ‘›ğ‘™ is depth and â„ is hidden size. Therefore, single-GPU feasibility is governed not by total model size ğ‘ƒ, but by whether the widest layer fits within the bounded GPU budget. This decoupling enables arbitrarily deep models to be trained so long as their maximum-width layer can be streamed and executed. Implication. Node-scale hundred-billion-parameter training requires simultaneously enforcing: (i) mixed-precision, model proportional host memory layout (Eq. 12), and (ii) strictly bounded, layer-centric GPU footprint (Eq. 3). Meeting these invariants is nontrivial and directly motivates Horizon-LMs CPU-master parameter store, explicit execution model, and streaming-oriented design."
        },
        {
            "title": "3.2 Bandwidth and Streaming Requirement\nWhile memory capacity determines whether node-scale training is\nfeasible, sustained performance is governed by whether the CPUâ€“\nGPU interconnect can support the required streaming bandwidth.\nUnder Horizon-LMâ€™s CPU-master execution model, every layer\nexecution induces explicit hostâ€“device transfers of parameters and\ngradients. As a result, training throughput is fundamentally con-\nstrained by whether data movement can be overlapped with com-\nputation to avoid a bandwidth wall.\nStreaming volume per iteration. For a model with total parameter\nsize ğ‘ƒ bytes, each training iteration necessarily streams parameters\nfrom host to device and gradients back from device to host. Ignoring\nminor constants, the irreducible data movement per iteration is",
            "content": "ğ‘‰H2D ğ‘ƒ, ğ‘‰D2H ğ‘ƒ, (4) corresponding to one forward parameter stream and one backward gradient return. Thus, even with perfect scheduling, the system must sustain an aggregate streaming volume of approximately 2ğ‘ƒ bytes per iteration across the CPUGPU boundary. Bandwidth feasibility condition. Let ğµpcie denote the effective bidirectional bandwidth between host memory and the GPU (e.g., PCIe 5.0 or NVLink-C2C), and let ğ‘‡comp denote the total device compute time per iteration. To avoid becoming transfer-bound, Horizon-LM must satisfy max (cid:19) (cid:18)ğ‘‰H2D ğµpcie , ğ‘‰D2H ğµpcie ğ‘‡comp, (5) so that parameter prefetching and gradient offloading can be fully hidden behind GPU execution. Violating this condition directly degrades throughput, regardless of available GPU compute. Layer-level streaming, not monolithic transfers. Crucially, HorizonLM does not move ğ‘ƒ bytes as bulk transfer. Instead, streaming is decomposed at layer granularity. For layer ğ‘– with parameter size ğ‘ƒğ‘– , the transfer time must be overlapped with the execution of neighboring layers. Therefore, feasibility is governed by stronger, local condition: ğ‘ƒğ‘– ğµpcie ğ‘ƒğ‘– ğµpcie ğ‘‡ comp ğ‘– 1 or ğ‘‡ comp ğ‘– , (6) meaning that the prefetch of layer ğ‘– can be hidden under the computation of layer ğ‘–1 (or symmetrically, gradient offloading under later layers). This transforms the problem from total model size to the relationship between per-layer width and per-layer compute intensity. Width-sensitive bandwidth pressure. For Transformer models, both per-layer parameters and per-layer compute scale as Î˜(â„2), where â„ is the hidden size. As result, wider layers increase not only ğ‘ƒmax (affecting GPU feasibility, Eq. 3), but also the streaming pressure on the CPUGPU interconnect. Node-scale training is viable only when the widest layers exhibit sufficient arithmetic intensity to amortize their transfer cost. This directly motivates design in which: (i) transfers are strictly layer-scoped and contiguous, (ii) execution is aggressively pipelined, and (iii) copy and compute are fully decoupled. Implication. Meeting the memory invariants alone is insufficient. practical node-scale training system must transform large, synchronous parameter movement into fine-grained, overlapped streaming pipeline. This requires explicit orchestration of prefetch, execution, and offload at the layer level, as well as architectural support for multi-stream concurrency. These requirements directly motivate Horizon-LMs double-buffered streaming engine and multistream execution model, described in Section 3.7."
        },
        {
            "title": "3.3 Execution and Scheduling Requirement\nMeeting the memory and bandwidth conditions alone is insufficient.\nEven if host memory can accommodate all model states and the\nCPUâ€“GPU interconnect can sustain the required streaming rate,\nexisting training runtimes fundamentally fail to operate in a node-\nscale regime because their execution models remain GPU-centric\nand graph-bound. To support hundred-billion-parameter training\non a single GPU, the training system must satisfy a new set of\nexecution and scheduling requirements.\nGlobal autograd graphs are structurally incompatible with\nnode-scale streaming. Modern deep learning frameworks con-\nstruct a monolithic autograd graph that implicitly binds together\nforward execution, activation retention, gradient propagation, and\nmemory allocation. This design assumes that parameters, activa-\ntions, and optimizer states are long-lived GPU-resident objects.\nWhen parameters are streamed and evicted layer by layer, however,\nthe global graph abstraction collapses: intermediate activations can-\nnot be retained arbitrarily, parameter tensors do not persist across\nbackward execution, and gradient flows must be reconstructed",
            "content": "dynamically. As result, graph-based runtimes inevitably introduce hidden GPU state, uncontrolled host allocations, and depthproportional memory growth, violating both the bounded-GPU and structured-host-memory invariants. Execution must be explicit, block-scoped, and recomputationaware. In streaming regime, the system must precisely control what executes, when it executes, and what state is materialized. This requires replacing implicit graph execution with an explicit execution model that: (i) decomposes training into deterministic forward, recomputation, and local backward phases, (ii) scopes all device-resident state to blocks of bounded size, and (iii) releases parameters and activations immediately after use. Backward propagation can no longer rely on stored activation graphs and must instead be orchestrated as sequence of block-level recomputations followed by local gradient evaluation. Scheduling must be dataflow-driven rather than graph-driven. Without global training graph, the runtime must elevate scheduling to first-class system responsibility. The system must explicitly orchestrate: (i) parameter prefetch from host memory, (ii) binding of streamed parameters to executable operators, (iii) forward execution and selective checkpointing, (iv) recomputation boundaries during backward, and (v) immediate gradient offloading. These operations form fine-grained streaming dataflow that cannot be expressed or optimized within conventional graph runtimes. Correctness and performance therefore hinge on an explicit scheduler that enforces precise ordering, dependency tracking, and buffer lifetime management. Overlap-aware multi-stream execution is mandatory. Because every layer induces unavoidable data transfers, node-scale training system must guarantee that copy and compute are continuously overlapped. This requires scheduling substrate that can concurrently drive: (i) host-to-device prefetch for future layers, (ii) compute on the current layer, and (iii) device-to-host gradient offload for completed layers. The scheduler must maintain multiple inflight layers, resolve hazards across streams, and prevent structural bubbles that would serialize execution. Deterministic memory and execution invariants. Finally, the execution model must enforce by construction that: (i) GPU memory remains bounded by per-layer footprints and checkpoint intervals, (ii) host memory remains dominated by persistent model state rather than runtime artifacts, and (iii) execution order and gradient flows remain deterministic and reproducible. These properties cannot be retrofitted onto existing training runtimes; they require an execution architecture designed around streaming from the outset. Implication. Supporting node-scale hundred-billion-parameter training therefore requires rethinking training execution itself. The system must abandon global training graphs in favor of an explicit, block-wise, scheduler-driven execution model that treats parameter movement, recomputation, and gradient propagation as first-class operations. This requirement directly motivates HorizonLMs CPU-controlled execution engine, streaming schedulers, and multi-stream pipeline, described in Sections 3.6 and 3.7. Horizon-LM targets modern post-training workloadssuch as instruction tuning and domain adaptationwhere the primary bottleneck has shifted from cluster-scale throughput to node-scale memory capacity. In these regimes, the dominant constraint is the ability to materialize hundred-billion-parameter models and their associated optimizer states. Traditional training systems fundamentally couple model scale to GPU count by requiring persistent, device-resident replicas and monolithic autograd graphs. Horizon-LM breaks this coupling by elevating host memory to first-class parameter store and redefining GPUs as transient compute engines. By restructuring training as memory-centric streaming process, Horizon-LM enables large-scale model training on single node. The design is guided by two core objectives: achieving depth-independent memory scalability and sustaining high training throughput under continuous streaming regime."
        },
        {
            "title": "3.5 Training Abstraction: CPU-Master,",
            "content": "GPU-Cache Horizon-LM introduces CPU-master, GPU-cache execution model, shifting the training paradigm from GPU-centric persistence to hostcentric streaming. Unlike conventional systems that treat GPUs as long-lived model owners, Horizon-LM assigns asymmetric roles to the two resources. CPU as the Authoritative Store. All persistent training states, model weights (ğœƒ ), optimizer states (ğ‘š, ğ‘£), and accumulated gradients (ğº), reside exclusively in host memory. The CPU acts as the authoritative master, performing all optimizer updates directly on host-resident tensors. Parameters are transiently projected to the GPU only during active computation and are evicted immediately thereafter. GPU as Transient Execution Cache. The GPU is redefined as compute engine with stateless layer template pool. Rather than materializing full model, the GPU caches only the minimal parameters and activations required for the current layers execution. By dynamically binding streamed weights to reusable templates, the system ensures that the device footprint remains proportional to the layer width, effectively decoupling it from the total model depth ğ¿. Graph-less Explicit Execution. To avoid the memory explosion of monolithic autograd graphs, Horizon-LM employs schedulerdriven execution model. It replaces the framework-managed global graph with an explicit sequence of streaming, checkpointing, and localized recomputation. This provides deterministic memory usage and precise control over tensor lifetimes, ensuring that GPU memory consumption follows: ğ‘€GPU = ğ‘‚ (ğ‘ƒmax + ğ¾ ğ´max) where ğ‘ƒmax and ğ´max are the maximum parameter and activation sizes per layer, and ğ¾ is the checkpoint interval."
        },
        {
            "title": "3.6 End-to-End Execution Workflow\nHorizon-LM decomposes each training iteration into three struc-\ntured phases that operate within strictly bounded device memory.\nBy orchestrating parameter residency and activation lifetimes, the\nruntime avoids the memory overhead of a global autograd graph.\n1. Forward Streaming and Anchoring. The forward pass proceeds\nlayer-by-layer. For each layer ğ‘–, parameters ğœƒğ‘– are streamed into\nthe GPU and bound to a template to compute activation â„ğ‘– . To\nenforce a constant activation footprint, Horizon-LM retains only\nsparse checkpoints (every ğ¾-th layer) while immediately evicting\nintermediate activations and parameters. The phase concludes with\nloss anchoring, where the initial gradient ğ‘”ğ¿ is computed and the\noutput head gradients are offloaded to host memory.\n2. Block-wise Local Recomputation. Backward propagation is\nexecuted in reverse block-wise order. For each block of ğ¾ layers,\nthe system loads the nearest checkpoint from host memory and\nperforms a localized forward pass to reconstruct intermediate ac-\ntivations. This \"just-in-time\" reconstruction allows the system to\ngenerate necessary gradients without maintaining the full modelâ€™s\nactivation state on the GPU.\n3. Streaming Local Backward and Evacuation. Within each\nrecomputed block, Horizon-LM executes local backward steps. As\nparameters for each layer are streamed in, the system computes both\nthe activation gradient ğ‘”ğ‘– âˆ’1 and parameter gradient âˆ‡ğœƒğ‘– . Crucially,\nâˆ‡ğœƒğ‘– is immediately evacuated to host memory. After the final layer\nof the block is processed, all transient tensors are released, ensuring\nthat the GPU residency is never proportional to model depth.",
            "content": "Throughout this workflow, all authoritative states remain in host memory. Optimizer updates are applied directly to the CPU-resident tensors, keeping the GPU exclusively as transient compute engine."
        },
        {
            "title": "3.7 System Architecture\nFigure 2 illustrates the architectural decomposition of Horizon-LM\ninto two distinct domains: the CPU Master Domain for persistent\nstate ownership and the GPU Execution Domain for transient com-\nputation.",
            "content": "Algorithm 1: Horizon-LM End-to-End Execution Workflow (Single Training Step) Input: Input batch ğ‘¥, parameters {ğœƒğ‘– }ğ¿ checkpoint interval ğ¾ Output: Updated parameters {ğœƒğ‘– } Streaming forward: â„0ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘’ğ‘Ÿ = Embed(ğ‘¥ ) ; for ğ‘– 1 to ğ¿ do ğ‘–=1 in host memory, // host device ğœƒğ‘– StreamIn(ğ‘– ) ; â„ğ‘– ğ‘“ğ‘– (â„ğ‘– 1; ğœƒğ‘– ) ; if ğ‘– mod ğ¾ = 0 then Checkpoint(â„ğ‘– ) Release(ğœƒğ‘– ) ; Loss anchoring: â„“ (â„ğ¿ ) ; ğ‘”ğ¿ â„“/â„ğ¿ ; ğœƒhead BackwardHead(â„“ ) ; Offload(ğœƒhead ) ; Block-wise backward: ğ‘ ğ¿/ğ¾ ; while ğ‘ 0 do ğ‘— =ğ‘ğ¾ RecomputeBlock(â„ğ‘ğ¾ ) ; â„ğ‘ğ¾ LoadCheckpoint(ğ‘ğ¾ ) ; {â„ ğ‘— } (ğ‘+1)ğ¾ ğ‘– (ğ‘ + 1)ğ¾ ; while ğ‘– ğ‘ğ¾ + 1 do ğœƒğ‘– StreamIn(ğ‘– ) ; (ğ‘”ğ‘– 1, ğœƒğ‘– ) LocalBackward(ğ‘“ğ‘–, â„ğ‘– 1, ğ‘”ğ‘– ; ğœƒğ‘– ) ; Offload(ğœƒğ‘– ),; Release(ğœƒğ‘– ) ; ğ‘”ğ‘– ğ‘”ğ‘– 1 ; ğ‘– ğ‘– 1 ; ğ‘ ğ‘ 1 ; 2. Model-Proportional Host Memory. Host memory scales linearly with total model size and remains independent of transient runtime structures: ğ‘€CPU ğ‘ƒğ‘– + ğ‘€opt + ğ‘‚ (ğ‘ƒmax) ğ‘– By decoupling capacity from residency, Horizon-LM shifts the scaling bottleneck from scarce GPU memory to abundant host memory."
        },
        {
            "title": "4.1 CPU Parameter Store Design\nHorizon-LM treats host memory not merely as a secondary storage\ntier, but as the authoritative execution master. To support billion-\nparameter models on a single node, we design the CPU parameter\nstore to maximize PCIe throughput and minimize host-side orches-\ntration overhead.",
            "content": "Figure 2: HorizonLM architecture: CPU acts as the parameter store while GPUs execute transient layer templates via asynchronous parameter streaming and gradient offloading. 3.7.1 CPU Domain: Authoritative Parameter Store. The CPU domain acts as the systems control plane and authoritative repository. It consists of: Structured Parameter Store: Maintains all long-lived states (parameters, gradients, and optimizer moments). Tensors are organized in layer-contiguous layout to facilitate high-throughput sequential access. Execution Control Plane: scheduler that issues structured sequence of execution primitives (StreamIn, Bind, Compute, Offload). It orchestrates the lifecycle of every tensor without relying on persistent framework-managed graph. Staging Slabs: Fixed-capacity, page-locked staging areas that mediate all cross-domain data movement, ensuring stable host memory usage and preventing fragmentation. 3.7.2 GPU Domain: Transient Execution Cache. The GPU domain is stateless execution engine. It holds no persistent model state; all device-resident data are transient and drawn from bounded memory regions: Streaming Buffers: Device-side staging areas for asynchronous parameter ingestion and gradient evacuation. These buffers are managed as circular caches to overlap transfer with computation. Layer Template Pool: set of reusable operator structures (e.g., Attention, MLP). Parameters from the streaming buffers are dynamically bound to these templates for execution and released immediately thereafter. Execution Workspace: strictly partitioned memory region for rolling activations, sparse checkpoints, and recomputation buffers. The lifetime of each region is explicitly controlled by the CPU master. 3.7.3 Architectural Invariants. The architecture enforces two structural invariants that define its scalability: 1. Bounded GPU Footprint. GPU memory consumption is determined solely by the width of the widest layer (ğ‘ƒmax, ğ´max) and the checkpoint interval ğ¾: ğ‘€GPU = ğ‘‚ (ğ‘ƒmax + ğ¾ ğ´max) This bound is invariant to total model depth ğ¿, enabling the training of arbitrarily deep models on single device. Figure 3: End-to-end pipelined execution of Horizon-LM across compute, data movement, and CPU optimization. Layer-Contiguous Memory Tiling. Conventional frameworks often manage tensors as fragmented objects scattered across the heap. This fragmentation forces the system to issue numerous smallgranularity DMA requests, each incurring kernel launch overhead and PCIe transaction tail-latency. To address this, Horizon-LM implements Layer-Contiguous Tiling. For each Transformer layer ğ‘–, all associated states, including BF16 weights (ğœƒğ‘– ), BF16 gradients (ğœƒğ‘– ), and FP32 Adam moments (ğ‘šğ‘–, ğ‘£ğ‘– ), are packed into single, monolithic memory block. As shown in Algorithm 1 (Lines 5 and 23), this layout ensures that the StreamIn primitive can satisfy layers residency transition with single, large-burst DMA transfer, saturating the PCIe bus bandwidth. Furthermore, these tiles are aligned to 4KB page boundaries to facilitate zero-copy pinned staging. Pinned Slab Recycling. significant challenge in node-scale training is that pinning the entire models parameters would exhaust host physical memory and OS page table resources. Horizon-LM employs fixed-capacity Pinned Slab Pool that acts as staging area for the streaming engine. Instead of pinning the total model ğ¿, we only pin small number of \"active\" slabs. During the StreamIn phase, dedicated CPU worker thread performs JIT (Just-In-Time) copy from the pageable layer-contiguous store to pinned slab. By double-buffering these slabs, we ensure that while the GPU executes layer ğ‘–, the CPU is already packing and pinning layer ğ‘– + 1 (the loop-ahead logic in Alg. 1, Line 4). This approach keeps the host-side pinning overhead constant (ğ‘‚ (ğ‘ƒmax)) regardless of model depth. Authoritative CPU-Side Optimizer. In our \"CPU-master\" model, the GPU is strictly forward/backward engine, while the CPU is the sole optimizer. Once the Offload primitive materializes ğœƒğ‘– in host memory (Alg. 1, Line 26), the CPU-side optimizer immediately applies the Adam update to the FP32 moments and BF16 weights in the parameter store. This design yields two primary benefits: (1) Memory Decoupling: It eliminates the need for FP32 optimizer states on the GPU, saving approximately 12 bytes of device memory per parameter. (2) Latency Hiding: Optimizer updates are executed asynchronously by host threads, effectively interleaving the weight update of layer ğ‘– with the gradient computation of layer ğ‘–1 (the backward loop in Alg. 1, Lines 2130). Structural Aliasing for Tied Weights. To support models with tied embeddings, Horizon-LM maintains Virtual-to-Physical mapping. Both the embedding and LM head (Alg. 1, Lines 2 and 12) point to the same physical memory tile in the CPU store. The execution controller tracks the \"readiness\" of this shared tile; once the head gradients are processed and the optimizer updates the shared parameters, the embedding is automatically marked as ready for the subsequent iterations StreamIn, ensuring numerical consistency without redundant storage or synchronization barriers."
        },
        {
            "title": "4.2 Explicit Execution Mechanisms\nHorizon-LM replaces the implicit memory management of conven-\ntional runtimes with an explicit execution model. As visualized in the\npipelined schedule of Figure 3, the system orchestrates training as a\ndeterministic sequence of compute and data movement primitives\nto decouple device capacity from model scale.\nPipelined Forward Streaming. During the forward pass (Alg. 1,\nLines 4â€“9), the system utilizes a double-buffered mechanism for\nweight prefetching. As shown in Figure 3, while the GPU com-\npute stream executes layer ğ¹1 using Buffer 0, the data move-\nment stream concurrently fetches weights ğ‘Š2 into Buffer 1. This\nStream-Bind-Compute cycle ensures that parameter ingestion is\ncompletely overlapped with execution. Upon the completion of each\nlayer, the Release primitive (Line 9) immediately frees the corre-\nsponding buffer, keeping the parameter footprint strictly bounded.\nSparse Checkpointing and Block-wise Recomputation. To bound\nactivation memory, the scheduler invokes the Checkpoint primi-\ntive (Line 7) at a fixed interval ğ¾. In the backward phase of Figure 3,\nthe system operates on blocks in reverse order. For a given block,\nthe system first triggers a recomputation sequence (ğ‘…0, ğ‘…1, ğ‘…2) to\nreconstruct intermediate activations from the nearest checkpoint\nâ„ğ‘ğ¾ (Line 18). These recomputed states are transiently stored in\na bounded workspace and are consumed by the subsequent local\nbackward kernels (ğµ2, ğµ1, ğµ0).\nLocalized Backward and Gradient Evacuation. Within each\nrecomputed block, Horizon-LM executes LocalBackward (Line 24)\nto produce activation gradients and parameter gradients. A criti-\ncal design feature shown in Figure 3 is the immediate evacuation\nof gradients ğºğ‘– to the Slab pool. As soon as ğµ3 completes, the\ngradient-transfer stream initiates the offload of ğº3, while the com-\npute stream simultaneously proceeds to the recomputation ğ‘…0. This\nasynchronous offloading prevents gradient state from occupying\nprecious GPU workspace.\nAsynchronous CPU-Side Accumulation and Optimization. The\nCPU domain acts as an active participant in the pipeline rather than\na passive store. As gradients are materialized in the pinned slabs,",
            "content": "Memory-Mapped Workspace Management. To further reduce runtime jitter, all transient workspaces for recomputation (ğ‘…ğ‘– in Figure 3) and local activations are pre-allocated and memory-mapped at initialization. The engine manages these as stack-like structure: the RecomputeBlock primitive (Alg. 1, Line 18) pushes recomputed states onto this workspace, which are then popped and released by the LocalBackward primitive. This explicit lifecycle management guarantees that GPU memory fragmentation is zero, providing the deterministic ğ‘€GPU bound established in Section 3.7."
        },
        {
            "title": "4.4 Double Buffering and Multi-stream",
            "content": "Figure 4: Double-buffer streaming and slab-based gradient"
        },
        {
            "title": "Scheduling",
            "content": "CPU worker threads initiate the Acc (Accumulation) and optimizer logic (Alg. 1, Line 33). As depicted at the end of the Figure 3 timeline, the Step phase (optimizer update) occurs directly on host memory. By the time the next iterations forward pass ğ¹0 begins, the updated parameters have been synchronized, completing the \"CPU-master, GPU-cache\" loop without ever requiring persistent global training graph."
        },
        {
            "title": "4.3 GPU Streaming Engine\nThe GPU domain functions as a transient execution cache, engi-\nneered to maximize throughput via a \"just-in-time\" parameter sup-\nply chain. As depicted in the Data Movement lane of Figure 3, the\nengine provides high-bandwidth ingestion and evacuation while\nmaintaining a stateless device profile.\nFlat-Buffer Streaming and Zero-Copy View. To minimize the\noverhead of hundreds of individual CUDA API calls, Horizon-LM\nemploys a flat-buffer ingestion strategy. For each layer ğ‘–, the CPU\npacks all constituent tensors into a single contiguous pinned buffer\n(the ğ‘Šğ‘– blocks in Figure 3).The StreamIn primitive (Alg. 1, Line 5)\nissues a single asynchronous H2D copy. Upon arrival in the GPUâ€™s\nBuffer 0/1, the engine performs a zero-copy unflattening: it creates\ntensor \"views\" that point directly into the flat bufferâ€™s offsets. This\navoids repeated GPU-side memory allocations and ensures that\nparameter materialization occurs at near-line-rate PCIe speeds.\nStateless Template Binding. Horizon-LM decouples the layerâ€™s\nmathematical structure from its physical data through a stateless\ntemplate pool. Each template (e.g., Template A/B in Figure 2) en-\ncapsulates the CUDA kernels for Attention and MLP blocks but\npossesses no persistent weight pointers.Before execution, the Bind\nprimitive dynamically maps the views from the streaming buffer\nto the templateâ€™s input slots. As visualized in the alternating col-\nors of Figure 3, this \"ping-pong\" binding allows ğ¹1 to execute on\nTemplate A while ğ‘Š2 is being bound to Template B, eliminating the\nlatency of weight preparation from the critical path.Asynchronous\nGradient Evacuation.Gradients are treated as \"perishable\" tran-\nsient states that must be evacuated to prevent device memory bloat.\nOnce the LocalBackward primitive (Alg. 1, Line 24) generates âˆ‡ğœƒğ‘– ,\nthe engine immediately flattens these gradients and issues a D2H\ntransfer to the CPUâ€™s gradient slabs (the ğºğ‘– blocks in Figure 3).By uti-\nlizing a dedicated gradient-transfer stream (detailed in Section 4.4),\nthis evacuation overlaps with the compute streamâ€™s next recomputa-\ntion or backward kernel (ğ‘… ğ‘— or ğµ ğ‘— ). This ensures that the long-lived\naccumulation of gradients (ğ´ğ‘ğ‘ğ‘– ) occurs entirely within the CPU\ndomain, keeping the GPU footprint invariant to model scale.",
            "content": "To saturate the PCIe bandwidth and maximize GPU utilization, Horizon-LM implements multi-stream pipeline that aggressively overlaps data movement with computation. As visualized in Figure 3, the system orchestrates three concurrent hardware streams mediated by hierarchy of CUDA events. Weight Double-Buffering. To eliminate the latency of parameter ingestion (ğ‘Šğ‘– ), Horizon-LM maintains two sets of staging buffers in both the CPU and GPU domains. This enables \"ping-pong\" prefetching strategy: while the compute stream executes layer ğ¹ğ‘– using Buffer 0, the weight-transfer stream concurrently packs and streams layer ğ‘Šğ‘–+1 into Buffer 1. As shown in Figure 3, this overlapping ensures that the GPU compute units never stall for parameters, effectively converting sequential execution into steady-state streaming pipeline. Multi-Stream Orchestration. Horizon-LM separates execution into three dedicated CUDA streams to avoid false dependencies and global device synchronizations: Compute Stream (ğ‘†comp): Responsible for executing the Compute, RecomputeBlock, and LocalBackward primitives (Alg. 1, Lines 6, 18, and 24). Weight-Transfer Stream (ğ‘†H2D): Orchestrates asynchronous H2D copies of parameters ğœƒğ‘– (the ğ‘Šğ‘– blocks in Figure 3). Gradient-Transfer Stream (ğ‘†D2H): Manages the immediate evacuation of gradients ğœƒğ‘– to host-side gradient slabs (the ğºğ‘– blocks in Figure 3). Event-Driven Synchronization. The coordination across these streams is governed by lightweight event-driven protocol rather than heavy-weight host-side barriers: (1) Weights-Ready Event: Recorded by ğ‘†H2D after ğ‘Šğ‘– completes; ğ‘†comp waits on this event before invoking the Bind primitive for layer ğ‘–. (2) Backward-Done Event: Recorded by ğ‘†comp after the local gradient ğœƒğ‘– is materialized; this triggers ğ‘†D2H to initiate the evacuation ğºğ‘– . (3) Buffer-Free Event: Recorded by ğ‘†D2H after the gradient offload is finished. The ğ‘†H2D stream must wait for this event before reusing the corresponding buffer for the next iterations weight prefetch (Alg. 1, Line 26). Asynchronous Gradient Evacuation. The separation of ğ‘†D2H from the compute stream is critical for maintaining throughput during the backward pass. As shown in Figure 3, the evacuation of ğº3 occurs in parallel with the recomputation ğ‘…0 and ğ‘…1. By treating the gradient return as background task, Horizon-LM prevents the PCIe D2H latency from leaking into the critical path of the backward recomputation, ensuring that the GPUs floating-point throughput is limited only by the slower of the compute kernel or the H2D parameter bandwidth."
        },
        {
            "title": "4.5 Memory Pool and Resource Partitioning\nThe efficiency of a streaming execution model is contingent upon\npredictable memory behavior. Horizon-LM treats memory manage-\nment as an explicit system interface, where all temporary storage\nis provisioned at initialization to eliminate runtime allocation over-\nhead and fragmentation.\nStatic Host-Side Staging. To sustain the ğ‘†H2D and ğ‘†D2H streams\nwithout exhaustive pinning, Horizon-LM partitions host memory\ninto fixed-size regions. First, two pinned staging buffers (Buffer 0/1\nin Figure 3) are allocated to facilitate weight prefetching. This en-\nsures the host-side pinning footprint remains invariant to model\ndepth ğ¿. Second, a slab pool (the green blocks in Figure 3) manages\ngradient returns. Slabs are recycled only after the CPU-side accu-\nmulation (Acc) completes, providing a back-pressure mechanism\nthat prevents gradient offloading from overrunning host memory.\nDeterministic GPU Execution Cache. The GPU domain is parti-\ntioned into a set of functional workspaces with strictly controlled\nlifetimes:",
            "content": "Streaming Buffers: Dedicated buffers for the StreamIn primitive, sized to the maximum layer parameter volume ğ‘ƒmax. Activation Stack: pre-allocated workspace for rolling activations and recomputation blocks. By managing this as stack rather than heap, Horizon-LM avoids the fragmentation common in long-running training sessions. Checkpoint Anchors: dedicated region for every ğ¾-th activation â„ğ‘ğ¾ , which remains resident only until the corresponding block-wise backward pass is completed. Eliminating Runtime Jitter. Beyond reducing allocator latency, this pooling strategy is critical for the robustness of the pipelined schedule shown in Figure 3. By using pre-allocated, reusable buffers, Horizon-LM eliminates \"bubbles\" in the pipeline caused by dynamic memory allocation or garbage collection. This architectural choice ensures that the system maintains constant, high-throughput steady state, even when training hundred-billion-parameter models at the limit of the devices capacity."
        },
        {
            "title": "5.1 Authoritative Parameter Storage and Layout\nTo eliminate the overhead of thousands of small PCIe transfers,\nHorizon-LM enforces a Flat-Tensor Layout. During initialization,\nwe extract the metadata (shape, numel) of all transformer layers\nand allocate two types of host-side memory:",
            "content": "Master Store: Model parameters and FP32 Adam moments are stored in non-pinned host memory to maximize capacity. Pinned Staging Buffers: We allocate two fixed-size page-locked (pinned) buffers, each exactly matching the size of the largest transformer layer (ğ‘ƒmax). These buffers act as the H2D/D2H gateway, ensuring that all DMA transfers achieve near-peak PCIe bandwidth (e.g., 26GB/s on PCIe Gen4 x16)."
        },
        {
            "title": "Robustness",
            "content": "Weight Tying. For models tied Embedding and LM-Head, we implement aliased synchronization. If the LM-Head and Embedding weights are tied, the system records the underlying data_ptr. During the H2D sync phase, only one transfer is issued, and the pointers on the GPU are re-mapped to the same device memory address to prevent divergence. Fragmentation Control. We use the expandable_segments flag in the PyTorch allocator to prevent virtual memory fragmentation during recomputation. By explicitly calling record_stream on all transient buffers, we ensure that the allocator does not reclaim memory still in flight within the GradStream, avoiding silent data corruption. Table 1: Model configurations used in experiments. Model Total Params Layers Hidden Size FFN Size Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B GPT-OSS-120B (MoE) 7B 14B 32B 72B 120B 28 48 64 80 36 3584 5120 5120 8192 2880*12(expert) 18944 13824 27648 29568 2880*12(expert)"
        },
        {
            "title": "6 Evaluation\n6.1 Experimental Setup\nGH200 System. GH200 System are conducted on the GH200 Graceâ€“\nHopper nodes. Each node contains four GH200 superchips, where\neach superchip integrates a 72-core Grace ARM CPU one NVIDIA\nH100 GPU with 96 GB HBM3 memory, connected through NVLink-\nC2C with a peak bidirectional bandwidth of approximately 900 GB/s.\nFor evaluation, we intentionally restrict Horizon-LM to a single\nGH200 superchip, using only one H100 GPU and approximately\n500 GB of host memory from the local Grace CPU. This setup re-\nflects a realistic HPC usage scenario: on shared supercomputing\nsystems, GPUs are commonly allocated at single-device granu-\nlarity, and users frequently share nodes rather than reserving all\nGPUs. Moreover, HPC allocations and queue policies are typically\ngoverned by GPU-hours, making jobs that require 2-4 GPUs signif-\nicantly harder to schedule and more expensive in allocation cost\nthan single-GPU jobs. By design, Horizon-LM operates entirely\nwithin this single-GPU regime, demonstrating that hundred-billion-\nparameter training can be achieved without requiring multi-GPU\nreservations, thereby substantially improving practical schedulabil-\nity and accessibility on production HPC systems.\nH200 System. We additionally evaluate Horizon-LM on an 1 NVIDIA\nH200 SXM node equipped with 1 Intel Xeon Platinum 8558 CPUs\n(96 cores total) and 1.5 TB of system RAM. H200 GPU providing\n141 GB HBM3e memory and connected to the host viaPCIe Gen4.\nA100 System. We further evaluate Horizon-LM on a commodity\nPCIe-based server equipped with an Intel Xeon Platinum 8273CL\nprocessor, 600 GB of host memory, and a single NVIDIA A100 GPU\n(80 GB HBM2e) connected via PCIe Gen4. This setup represents a\nwidely available datacenter configuration that Horizon-LM does\nnot depend on NVLink-class interconnects and remains effective in\nconventional PCIe environments where host memory is abundant\nbut GPU memory is limited.\nDataset. We evaluate model accuracy on the MetaMathQA bench-\nmark, a large-scale mathematical reasoning dataset comprising\napproximately 395,000 English math problemâ€“answer pairs. Meta-\nMathQA is constructed via data augmentation techniques over\nbase reasoning benchmarks such as GSM8K and MATH, producing",
            "content": "Figure 5: Host (CPU) memory footprint versus model scale across training systems. diverse multi-step math word problems with deterministic groundtruth answers. In our experiments, we randomly divide the dataset into 70% training (approximately 276,500 samples) and 30% testing (approximately 118,500 samples). We report exact-match accuracy, defined as whether the models final predicted answer exactly matches the reference answer for each problem."
        },
        {
            "title": "6.2 Feasibility Boundary\nAll experiments in this subsection are conducted on two representa-\ntive single-GPU platforms to illustrate how the feasibility boundary\nshifts with available host memory capacity. Models from 7B to 32B\nparameters are evaluated on a GH200 system, while larger models\nfrom 72B to 120B are evaluated on an H200 system equipped with\n1.5 TB host RAM.\nHost Memory as the True Scaling Boundary. The line plot in\nFigure 5 reports the host memory footprint required to train models\nof increasing scale under different training systems. A clear trend\nemerges: while ZeRO-3 Offload, ZeRO-Infinity, and PyTorch Native\nall exhibit rapidly increasing host memory consumption as model\nsize grows, HorizonLM maintains a significantly flatter growth\ncurve. From 7B to 120B parameters, competing systems show near-\nexponential growth in CPU memory demand due to redundant\nparameter staging, fragmented tensor storage, and optimizer state\nreplication across offload buffers. In contrast, HorizonLMâ€™s flat-\ntensor layout and authoritative CPU master storage ensure that\nmemory growth is strictly proportional to the theoretical parameter\nfootprint, without auxiliary duplication. This result highlights a\ncritical feasibility boundary: for large models, host memory capacity,\nrather than GPU memory, becomes the primary limiting factor for\nsingle-device training. Existing offloading systems cross this bound-\nary rapidly beyond 30B parameters, while HorizonLM remains well\nwithin practical limits even at 120B scale.\nCompute Efficiency and Sustained TFLOPS. The Figure 1 reports\nsustained training throughput in TFLOPS across two architectures\n(GH200 and H200). At small scales (7B), PyTorch Native achieves\nhigh peak throughput due to full GPU residency, but this advantage\ncollapses once models exceed GPU memory capacity. ZeRO-3 and\nZeRO-Infinity suffer from substantial PCIe synchronization over-\nhead and fragmented transfers, leading to severe degradation in\nsustained compute. HorizonLM, however, maintains consistently\nhigh TFLOPS across all scales. On GH200, HorizonLM sustains 284\nTFLOPS at 7B, 264 TFLOPS at 14B, and remains above 250 TFLOPS\neven at 32B. On H200, the system continues scaling to 72B and\n120B while preserving high utilization. This stability arises from",
            "content": "Figure 6: Depth scalability with fixed model width (hidden and FFN) size. Table 2: Final accuracy comparison across systems at 7B and 14B scales. Table 3: Depth Scalability Configs Layers Parameters (B) Hidden Size GPU Alloc (GB) System 7B Accuracy (%) 14B Accuracy (%) Baseline ZeRO-3 Offload ZeRO-Infinity PyTorch Native Ours 33.47 88.93 88.97 88.91 88.99 37.58 92.41 92.36 - 92.52 two design properties: (1) large contiguous DMA transfers enabled by pinned staging buffers, and (2) overlap between compute and weight prefetch through double buffering and stream execution. Correctness Preservation at Scale. Table 2 shows that HorizonLM matches the numerical accuracy of standard full-GPU training and ZeRO-based baselines at both 7B and 14B scales. The negligible difference in accuracy confirms that HorizonLMs explicit recompute and CPU-master design do not introduce numerical drift or optimization instability. This validates that the systems memory and compute advantages do not trade off training correctness."
        },
        {
            "title": "6.3 Depth Scalability Results.\nAll experiments in this subsection are conducted on the GH200\nsystem. Table 3 and Figure (a)â€“(b) evaluate how training systems\nbehave when model depth increases while hidden size and GPU\nmemory allocation remain strictly constant (3.83 GB). This set-\nting isolates the systems capability to handle increasing parameter\ncounts purely through depth scaling, without granting additional\nGPU memory. Such a setup directly stresses the memory orchestra-\ntion, parameter movement, and recomputation efficiency of each\nsystem.\nThroughput under increasing depth. As shown in Figure 6(a),\nHorizon-LM maintains remarkably stable throughput as depth in-\ncreases from 28 to 180 layers. The throughput only decreases from\n284 TFLOPS to 227 TFLOPS, a modest 20.1% drop despite the model\ngrowing from 10.9B to 43.0B parameters (a 3.95Ã— increase in size).\nIn contrast, both ZeRO-3 Offloading and FSDP Offloading exhibit",
            "content": "28 42 56 84 132 180 7.62 10.88 14.14 20.67 31.85 43.04 3584 3584 3584 3584 3584 3584 3.83 3.83 3.83 3.83 3.83 3.83 severe throughput collapse as depth increases: At 42 layers, HorizonLM is already 1.37 faster than FSDP (272 vs. 199 TFLOPS) and 1.17 faster than ZeRO-3 (272 vs. 232 TFLOPS). At 56 layers, FSDP throughput drops catastrophically to 43 TFLOPS, making HorizonLM 6.14 faster (264 vs. 43). At 84 layers, ZeRO-3 degrades to 43 TFLOPS, where Horizon-LM becomes 5.93 faster (255 vs. 43), while FSDP already runs out of memory. Beyond 84 layers, both baselines encounter OOM, whereas Horizon-LM continues to scale to 132 and 180 layers with stable throughput. This demonstrates that existing offloading systems suffer from depth-induced communication and memory scheduling bottlenecks, where parameter movement and recomputation overhead grow superlinearly with depth. Host memory behavior. Figure 6(b) further reveals the host memory cost of enabling deeper models. At 42 layers, FSDP consumes 270 GB host memory and ZeRO-3 uses 92 GB, compared to only 115 GB for Horizon-LM. At 56 layers, Horizon-LM uses 145 GB, while FSDP increases to 330 GB (2.28 higher). At 84 layers, FSDP reaches 518 GB host memory before OOM, which is 2.50 higher than Horizon-LM (207 GB). At 132 and 180 layers, both baselines OOM due to host memory exhaustion, while Horizon-LM continues operating at 312 GB and 418 GB respectively."
        },
        {
            "title": "6.4 Width Scalability Results.\nTable 4 and Figure 7 evaluate scalability when model width (hid-\nden and FFN dimensions) increases while keeping the number of\nlayers fixed. Unlike the depth experiment where GPU allocation\nremains constant, width scaling directly increases per-layer tensor",
            "content": "Figure 8: Performance comparison on single A100 PCIe system. Host memory growth under width scaling. Figure 7(b) shows that width scaling shifts pressure heavily toward host memory due to the increased size of parameter slabs and activation staging buffers. At 3.0 width, Horizon-LM uses 174 GB host memory, compared to 295 GB for ZeRO-3 (1.69 higher). At 3.5, ZeRO-3 reaches 353 GB while Horizon-LM uses 263 GB (1.34 higher). At 4.0, ZeRO-3 explodes to 526 GB and fails shortly after, while HorizonLM remains at 308 GB. FSDP shows lower host memory at small widths but fails early (after 3.0) due to GPU memory fragmentation and activation pressure."
        },
        {
            "title": "6.5 Verification on Different Devices\nTo demonstrate that the advantages of Horizon-LM are not tied to\na specific hardware platform, we conduct an additional verification\nstudy on a single NVIDIA A100 (80GB, PCIe Gen4) system under\nan x86 CPU architecture. On this platform, we re-implement and\ncarefully tune two representative offloading baselines: ColossalAI-\nGemini and ZeRO-3 CPU Offloading. Both baselines are configured\nfollowing their official recommendations and adapted to the same\nsoftware environment to ensure fairness.",
            "content": "Figure 8 reports the achieved TFLOPS for 7B, 14B, and 32B models. Even on this different hardware stack, Horizon-LM consistently outperforms both baselines: At 7B, Horizon-LM reaches 128 TFLOPS, compared to 53 TFLOPS for Gemini and 36 TFLOPS for ZeRO-3, achieving 2.42 and 3.56 speedup respectively. At 14B, the gap widens: Horizon-LM sustains 122 TFLOPS, while Gemini drops to 15 TFLOPS and ZeRO-3 to 10 TFLOPS, corresponding to 8.13 and 12.20 improvements. At 32B, both Gemini and ZeRO-3 encounter out-of-memory errors, while Horizon-LM continues to operate at 114 TFLOPS."
        },
        {
            "title": "7 Conclusion\nHorizon-LM challenges the long-standing GPU-centric assumption\nin large-model training by showing that hundred-billion-parameter\nmodels can be trained on a single device when host memory is\ntreated as the authoritative parameter store and GPUs operate\npurely as transient compute engines. By replacing persistent GPU-\nresident replicas and global autograd graphs with an explicit, stream-\ning, CPU-master execution model, Horizon-LM enforces two key\ninvariants: GPU memory is bounded by per-layer footprint, and\nhost memory scales linearly and predictably with model size. Across\ndiverse platforms, Horizon-LM sustains high throughput, preserves",
            "content": "Figure 7: Width scalability with fixed model layers Table 4: Width Scalability Config (Layers = 4) Width Scale Hidden Size FFN Size GPU Alloc (GB) 1.0x 1.5x 2.0x 2.5x 3.0x 3.5x 4.0x 4.5x 5.0x 3584 5376 7168 8960 10752 12544 14336 16128 17920 18944 28416 37888 47360 56832 66304 75776 85248 3.83 7.01 11.07 15.99 21.78 28.44 35.97 44.36 53.62 sizes and therefore stresses GPU memory bandwidth, activation footprint, and parameter transfer volume. This experiment exposes fundamentally different bottleneck compared to depth scaling. Throughput degradation with width. All experiments in this subsection are conducted on the GH200 system. As shown in Figure 7(a), all systems experience throughput reduction as width increases due to the quadratic growth of matrix multiplications. However, the rate of degradation differs substantially. From 1.0 to 3.0 width, Horizon-LM drops from 406 to 264 TFLOPS (35.0% decrease). Over the same range, ZeRO-3 drops from 455 to 262 TFLOPS (42.4% decrease). FSDP drops from 501 to 281 TFLOPS (43.9% decrease). Although Horizon-LM starts slightly lower at small width (406 vs. 501 TFLOPS), its degradation curve is noticeably flatter. At 3.5 width, ZeRO-3 already falls to 160 TFLOPS while HorizonLM sustains 193 TFLOPS, making it 1.21 faster. At 4.0, ZeRO-3 further drops to 136 TFLOPS, where Horizon-LM is 1.07 faster. Beyond 4.0, both ZeRO-3 and FSDP encounter OOM due to GPU and host memory pressure, while Horizon-LM continues to operate up to 5.0 width. learning. In Proceedings of the international conference for high performance computing, networking, storage and analysis. 114. [21] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019). [22] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. 2025. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267 (2025). [23] Venkat Somala. 2025. NVIDIAs B200 Costs Around $6,400 to Produce, with Memory Accounting for Half. https://epoch.ai/data-insights/b200-cost-breakdown [24] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [25] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 (2025). [26] Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, et al. 2025. survey on post-training of large language models. arXiv e-prints (2025), arXiv2503. [27] Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. 2023. Zero++: Extremely efficient collective communication for giant model training. arXiv preprint arXiv:2306.10209 (2023). [28] Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, et al. 2025. Wlb-llm: Workload-balanced 4d parallelism for large language model training. arXiv preprint arXiv:2503.17924 (2025). [29] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. 2024. Autogen: Enabling next-gen LLM applications via multi-agent conversations. In First Conference on Language Modeling. [30] Manjiang Yu and Priyanka Singh. 2025. Differentially Private Fine-Tuning of Large Language Models: Survey. In International Conference on Advanced Data Mining and Applications. Springer, 100113. [31] Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, et al. 2025. EfficientLLM: Efficiency in Large Language Models. arXiv preprint arXiv:2505.13840 (2025). [32] Ziqi Yuan, Haoyang Zhang, Yirui Eric Zhou, Apoorve Mohan, Chung, Seetharami Seelam, Jian Huang, et al. 2025. Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage. arXiv preprint arXiv:2506.06472 (2025). [33] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. 2023. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277 (2023). numerical correctness, and remains stable under both depth and width scaling where existing offloading systems fail. These results suggest that, for modern post-training workloads, the critical systems challenge is no longer GPU capacity but how memory, bandwidth, and execution are organized around streaming rather than persistence, pointing toward new design space for node-scale large-model systems. References [1] Daniel Adeboye. 2025. How Much Does an NVIDIA B200 GPU Cost? //northflank.com/blog/how-much-does-an-nvidia-b200-gpu-cost https: [2] Sabiha Afroz, Redwan Ibne Seraj Khan, Hadeel Albahar, Jingoo Han, and Ali Butt. 2025. 10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training. In Proceedings of the 2025 ACM Symposium on Cloud Computing. 320333. [3] Anthropic. 2025. Claude Code: Agentic Coding Tool for the Terminal. https: //github.com/anthropics/claude-code Version 1.0.0, General Availability. [4] Cursor. 2025. Cursor: AI Code Editor and Coding Agent. https://github.com/ cursor/cursor Official repository. [5] Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, et al. 2024. Efficient training of large language models on distributed infrastructures: survey. arXiv preprint arXiv:2407.20018 (2024). [6] Jiarui Fang and Yang You. 2022. Meet Gemini: The Heterogeneous Memory Manager of Colossal-AI. https://colossalai.org/docs/advanced_tutorials/meet_ gemini. Colossal-AI Documentation. [7] GPUsPerStudent.org. 2025. H100-Equivalent GPUs Per CS Student: Trackhttps://www. ing Academic GPU Compute Availability in the United States. gpusperstudent.org/ [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [9] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems 36 (2023), 4587045894. [10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. 2023. MetaGPT: Meta programming for multi-agent collaborative framework. In The twelfth international conference on learning representations. [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [12] Ivan Ilin. 2025. Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source). arXiv preprint arXiv:2504.04520 (2025). [13] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720 (2024). [14] Zoe Kessler and Aditi Suresh. 2025. NVIDIA HGX B200 Reduces Embodied Carbon Emissions Intensity. NVIDIA Technical Blog. https://developer.nvidia.com/blog/ nvidia-hgx-b200-reduces-embodied-carbon-emissions-intensity Comparison of PCF summaries showing 24% reduction in embodied carbon emissions versus HGX H100. [15] Hanyu Lai, Xiao Liu, Junjie Gao, Jiale Cheng, Zehan Qi, Yifan Xu, Shuntian Yao, Dan Zhang, Jinhua Du, Zhenyu Hou, et al. 2025. survey of post-training scaling in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 27712791. [16] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. 2024. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision 16, 1-2 (2024), 1214. [17] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [18] NVIDIA Corporation. 2025. NVIDIA A100 Tensor Core GPU. https://www.nvidia. com/en-us/data-center/a100/. Accessed: 2026-02-02. Includes official datasheet and PCIe product brief describing Ampere architecture, 80GB HBM2e memory, and >2 TB/s bandwidth. [19] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 116. [20] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep"
        }
    ],
    "affiliations": [
        "Lehigh University",
        "University of Notre Dame"
    ]
}