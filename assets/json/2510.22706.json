{
    "paper_title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction",
    "authors": [
        "Hao Li",
        "Zhengyu Zou",
        "Fangfu Liu",
        "Xuanyang Zhang",
        "Fangzhou Hong",
        "Yukang Cao",
        "Yushi Lan",
        "Manyuan Zhang",
        "Gang Yu",
        "Dingwen Zhang",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 0 7 2 2 . 0 1 5 2 : r Under review as conference paper IGGT: FORMER FOR SEMANTIC 3D RECONSTRUCTION INSTANCE-GROUNDED GEOMETRY TRANSHao Li1,2,3, Zhengyu Zou1, Fangfu Liu4, Xuanyang Zhang3, Fangzhou Hong2, Yukang Cao2, Yushi Lan2, Manyuan Zhang4, Gang Yu3, Dingwen Zhang2(cid:66), Ziwei Liu2 1NWPU 2S-Lab, NTU 3StepFun, Inc. 4THU 5MMLab, CUHK Figure 1: IGGT: building upon our curated large-scale dataset InsScene-15K, we propose novel end-to-end framework that enables geometric reconstruction and contextual understanding in unified representation. This paradigm facilitates wide range of applications, including spatial tracking, 2D / 3D open-vocabulary segmentation, and scene grounding."
        },
        {
            "title": "ABSTRACT",
            "content": "Humans naturally perceive the geometric structure and semantic content of 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned models capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design 3D-Consistent Contrastive Learning strategy that guides IGGT to encode unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with novel data curation pipeline. Unlike previous methods that bound with specific language model, we introduce an Instance-Grounded Scene Understanding paradigm, where instance masks serve as the bridge connecting our Project Leader. (cid:66)Corresponding Authors. 1 Under review as conference paper unified representation with diverse Visual Language Models (VLMs) in plugand-play manner, substantially expanding downstream understanding capabilities. Extensive experiments on instance spatial tracking, open-vocabulary segmentation, and QA scene grounding demonstrate that IGGT outperforms state-ofthe-art methods in both quality and consistency for semantic 3D reconstruction. https://github.com/lifuguan/IGGT_official."
        },
        {
            "title": "INTRODUCTION",
            "content": "A foundational goal in the pursuit of spatial intelligence (Yang et al., 2025) is to build representations that mirror human understandingcapturing both the precise geometric structure and rich semantic content of scene from visual sensory inputs such as RGB images. Such representations are vital for enabling downstream tasks like robotic manipulation (Qu et al., 2025), AR / VR (Jiang et al., 2025), and planning (Zhang et al., 2024). Previous methods (Zust et al., 2025; Fan et al., 2024; Sun et al., 2025) tackle this challenge through fragmented paradigm, decoupling 3D geometric reconstruction and high-level semantic understanding into isolated tasks. Typically, they first leverage geometry-focused techniques (e.g., Multi-View Stereo (MVS) methods (Schonberger et al., 2016; Schonberger & Frahm, 2016) or off-the-shelf large Image-to-3D models (Wang et al., 2024; 2025)) to predict low-level 3D structures, followed by vision-language models (VLMs) (Bai et al., 2023; 2025) or 2D segmentation models (Cheng et al., 2022) to perform high-level semantic segmentation tasks. However, these disjointed approaches are inherently flawed, as they propagate errors between stages and fail to leverage the mutual context between shape and identity, preventing them from enhancing each others capabilities and hindering their ability to support model reconstruction. Recently emerged methods (Fan et al., 2024; Sun et al., 2025) attempt to bridge this gap by aligning spatial models with specific VLM (Li et al., 2022). However, these approaches suffer from three critical limitations. First, since 3D geometry contains low-level, fine-grained structural signals, forcing strict alignment with high-level textual concepts can over-smooth the representation, degrading high-frequency geometric details and undermining multi-view consistency. Second, this tight coupling to specific VLM architecture inherently restricts the performance to the base model (e.g., LSeg (Li et al., 2022)) and prevents the integration of newer, more powerful foundation models (e.g., CLIP (Radford et al., 2021), SigLIP (Tschannen et al., 2025)). Third, since these VLMs (Li et al., 2022; Ghiasi et al., 2022) are mainly trained on 2D imagetext pairs, their aligned features often fail to distinguish objects within the same semantic category, which significantly limits more downstream applications (e.g., , 3D instance-consistent tracking under large viewpoint changes and spatial QA when interfaced with VLMs). To address this, we propose Instance-Grounded Geometry Transformer (IGGT), novel end-to-end framework that unifies the representation for spatial reconstruction and contextual understanding. Instead of simply aligning geometry with language features, our key idea is to couple both factors by joint training and encourage the model to autonomously learn the relationship between 3D instancelevel semantics and their geometric structures, yielding mutual improvements in contextual understanding and geometry reconstruction. Specifically, 1) we employ large Unified Transformer to encode multi-view images into unified token representations of the 3D scene, which are decoded by Geometry Head and an Instance Head into geometric point maps and an instance clustering fields, respectively. 2) we employ cross-modal fusion block with window-shifted attention mechanism, enabling the Instance Head to leverage fine-grained geometric features at pixel level to enhance its spatial awareness. 3) To further improve multi-view consistency of the instance fields, we design 3D-consistent contrastive learning strategy that guides IGGT to learn both geometric structures and instance-grounded clustering features. As instance-level geometry-semantics aligned annotations remain scarce in the community, we facilitate this task by presenting large-scale dataset coined InsScene-15K, meticulously constructed dataset comprising high-quality RGB images, poses, depth maps, and 3D-consistent instance masks. One more thing, after training the full model (i.e., IGGT), we design an Instance-Grounded Scene Understanding strategy, where instance masks serve as the bridge connecting IGGT with diverse VLMs. Such paradigm not only enables the seamless, plug-and-play integration of various visionlanguage models (VLMs) such as CLIP and SigLIP to lift downstream task performance, but also Under review as conference paper Figure 2: Data Curation Pipeline. Our data is collected from various sources and then annotated by novel data engine driven by SAM2 (Ravi et al., 2024). (a) For video captured scenes (i.e., RE10k (Zhou et al., 2018)), we annotate them through customized SAM2 video dense prediction pipeline. (b) For RGBD-scan scenes (e.g., ScanNet++ (Yeshwanth et al., 2023)), we regenerate dense mask annotations for each image and align them with the projected coarse GT masks. extends to Large Multimodal Models (LMMs) (Bai et al., 2023; 2025), unlocking more sophisticated scene understanding and broader spectrum of applications like scene grounding. We validate our framework through extensive experiments on diverse downstream tasks (e.g., spatial tracking segmentation, open-vocabulary segmentation, and scene grounding), demonstrating its superiority over state-of-the-art methods in both task performance and 3D scene coherence. 2 INSSCENE-15K DATASET We construct the InsScene-15K dataset (in Sec. 2), where each scene includes corresponding RGB images, depth maps, poses, and 3D-consistent instance segmentation masks. To maintain consistency, we ensure that each instance retains unique ID across all views. Our data curation pipeline systematically integrates three distinct categories of data to ensure comprehensiveness and diversity, as illustrated in Fig. 2: 1) synthesis (Aria (Pan et al., 2023), Infinigen (Raistrick et al., 2024)); 2) Video captured (RE10K (Zhou et al., 2018)); 3) RGBD captured (Scannet++ (Yeshwanth et al., 2023)). For synthetic datasets (e.g., Aria and Infinigen), we simultaneously generate the RGB image, depth map, camera pose, and object-level segmentation masks for each rendered view. Since the simulation environment provides perfectly accurate 2D groundtruth masks (in Fig. 3 (a)), we use them directly without any post-processing. Moreover, regarding real-world scenarios, we propose novel data curation pipeline that includes multi-view mask annotation and refinement stages, driven by SAM2 (Ravi et al., 2024). Specifically, for real-world video-captured scenes such as RE10K (Zhou et al., 2018) (Fig. 2a), our method first employs SAM to generate dense mask proposals on the initial frame. These proposals are then used as prompts Figure 3: Visualization of mask annotations from three different sources. For the RGBD-scan scene, we additionally compare the vanilla ground-truth masks from ScanNet++ (Yeshwanth et al., 2023) with our refined annotations, along with their corresponding matched IDs and mIoU scores. 3 Under review as conference paper for the SAM2 video object segmenter to propagate masks temporally throughout the sequence. To handle new objects and mitigate drift, we adopt an iterative strategy that designates new keyframe whenever the unsegmented area increases, where SAM is reapplied to discover objects in the uncovered regions. After processing the entire video, final bi-directional propagation pass ensures high temporal consistency across object tracks. This curation strategy provides scalable and diverse annotations that enhance the generalization ability of our model. For challenging datasets with large-scale camera motion but coarse 3D annotations such as ScanNet++ (Yeshwanth et al., 2023), we first project the 3D annotations into 2D to obtain initial imagelevel object masks. While this guarantees multi-view consistency of object IDs, the masks are often coarse and imprecise. To improve their quality, we use SAM2 to generate fine-grained initial mask proposals that are accurate in shape but lack identity information. These proposals are then aligned with the projected ground-truth masks to assign consistent object IDs (Fig. 3 (c)), and proposals belonging to the same ID are merged into complete masks. The process is iteratively refined until all image regions are covered. This pipeline (Fig. 2 (b)) achieves both multi-view ID consistency and shape-accurate annotations, substantially improving 2D mask quality for real-world scenarios."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 OVERVIEW Our method consists of two main phases. Firstly, we propose IGGT (in Sec. 3.2), unified foundation model that simultaneously predicts instance-discriminative features at the spatial level and performs 3D reconstruction through 3D-consistent contrastive learning on large-scale datasets. Secondly, we propose an instance-grounded scene understanding strategy (Sec. 3.3). This strategy employs unsupervised clustering to partition the scene into instances by grouping the predicted features into masks with consistent instance IDs. These masks are then used to guide state-of-the-art vision-language models (VLMs, e.g., CLIP, OpenSeg) and large multimodal models (LMMs, e.g., GPT-4o, Qwen2.5-VL) to perform open-vocabulary scene querying and grounding tasks. 3.2 ARCHITECTURE OF IGGT As illustrated in Fig. 4, given input images {I RHW 3}N i=1, we aim to forge unified representation, enabling comprehensive 3D reconstruction and understanding in mutually reinforcing manner. Specifically, we propose IGGT F, which predicts camera parameters ti, depth map Di, point map Pi, and 3D-consistent, instance-level feature maps Si in feed-forward manner: : {Ii}N i=1 (cid:55) (ti, Di, Pi, Si)N i=1. (1) Our IGGT consists of three parts: 1) Large Unified Transformer to capture Unified Token Representation from multiple images; 2) two Downstream Heads with Cross-Modal Fusion Block to simultaneously predict geometric structures and corresponding instance features via mutual enhancement pattern; 3) 3D consistent supervision to Large Unified Transformer. We follow VGGT to construct 1B parameter large unified Transformer, designed to encode the multi-view images {Ii}N i=1 into set of powerful unified token representations {Ti RM D}N i=1, where denotes the numbers of the tokens for each image and is the dimension of the token. Our large Unified Transformer first adopts pretrained DINOv2 (Oquab et al., 2023) to extract patch-level image tokens. To support arbitrary multi-view inputs while maintaining permutation equivariance, learnable camera token is concatenated to each views token sequences. Subsequently, 24 blocks of intra-view self-attention and global-view cross-attention are applied to transform the image tokens into unified tokens {Ti}N i=1, capturing both local and global context, which enables holistic and globally consistent understanding of the 3D scene. Downstream Heads and Cross-Modal Fusion Block. We employ two downstream branchesGeometry Head and Instance Headto decode the unified tokens Ti} into geometric and instance features, respectively. The Geometry Head, inheriting its design from VGGT, is composed of three distinct modules: camera predictor, depth predictor, and point predictor. The camera predictor is tasked with regressing camera parameters, including extrinsics and intrinsics, from camera-specific tokens. For dense prediction, the depth and point predictors employ DPTlike architecture (Ranftl et al., 2021). This architecture reconstructs hierarchical geometric feature Under review as conference paper Figure 4: Overview of IGGT. Given input images, our method encodes them into series of Unified Token Representations, which are then processed by the Geometry Head and the Instance Head to produce high-quality geometric reconstructions and instance-grounded clusterings simultaneously. In the end, we introduce Instance-Grounded Scene Understanding to perform multiple applications. pt = {F pt l=1 from the unified tokens through progressive upsampling and multi-scale fusion network Φpt(). Similar to this dense prediction paradigm, our Instance Head Φins() also adopts DPT-like architecture to perform dense instance features ins i,(l)}4 = {F ins i,(l)}4 l=1: {F pt } = Φpt({Ti}), {F ins } = Φins({Ti}). (2) Moreover, to enhance the fine-grained spatial awareness of the instance head, we propose crossmodal fusion block Fwin(), which utilizes sliding window cross attention to embed spatial structure into the instance representation, making them more sensitive to object boundaries and spatial layouts while avoiding the quadratic complexity of global attention: ˆF ins i,(l) = ins i,(l) + Fwin(Q = ins i,(l), = pt i,(l), = pt i,(l)). (3) After that, we concatenate all refined instance features { ˆF ins 3 3 convolutional layer to 8 dimensional instance features Oins RN 8HW . 3D-Consistent Contrastive Supervision. We enforce 3D consistency on the instance features Oins RN 8HW by applying multi-view contrastive loss Lmvc, which is designed to pull features from the same 3D instance together across views while pushing features from different instances apart. Given set of sampled pixels P, the loss is formulated as: i,(l)} and map them through conventional Lmvc = λpull (cid:88) d(fpi , fpj ) + λpush pi,pj m(pi)=m(pj ) (cid:88) pi,pj m(pi)=m(pj ) max(0, d(fpi , fpj )) (4) Here, d(, ) is the L2 distance between normalized features, m(pi) is the instance ID of pixel pi. The coefficients λpull and λpush balance the pulling and pushing terms, while is margin hyperparameter that controls the discriminative between different instances. This objective structures the instance representations according to the 3D scene geometry, improving generalization. Overall, we train the whole model in multi-task loss: Loverall = Lpose + Ldepth + Lpmap + Lmvc, (5) where geometry supervision terms pose Lpose, depth Ldepth, and point map Lpmap are followed by the training paradigm of VGGT, which is used to supervise the outputs of the geometry head. 3.3 INSTANCE-GROUNDED SCENE UNDERSTANDING Unlike prior approaches that are tightly coupled with specific language model (e.g., for OpenVocabulary Segmentation) and thus limited to single type of task, we decouple our framework from specific language models and propose novel Instance-Grounded Scene Understanding strategy to support broad range of downstream tasks. As shown in Tab. 1, our method is the only one that simultaneously enables spatial tracking, image-to-3D reconstruction, and scene understanding, while achieving state-of-the-art performance across all tasks. Under review as conference paper Table 1: Quantitative Results on Scannet (Dai et al., 2017). Here we showcase the capability overview and report the spatial track quality, reconstruction accuracy, and 2D / 3D open-vocabulary semantic segmentation accuracy. The bold denotes the best results. Model Capability Spatial Track Recon. Metric Open-Vocab. Semantic Segment Recon. Understand Track T-mIoU T-SR Abs. Rel τ 2D mIoU 2D mAcc 3D mIoU LSeg OpenSeg NeRF-DFF Feature-3DGS LSM (2 Views) LSM (Multi-Views) SpaTracker+SAM SAM2* VGGT Ours - - - - - - 26.43 53.74 - 69. - - - - - - 38.57 71.25 - 98.66 - - 7.99 6.48 4.22 3.17 - - 1.84 1.90 - - 36.53 41.63 58.65 64.81 - - 83.60 83.71 58.11 42.33 45.40 57.69 53.07 53.40 - - - 60.46 65.76 68.06 65.29 63.26 53.86 59.50 - - - 81.84 - - 12.29 23.42 - 35.37 - - - 39. Table 2: Quantitative Results on Scannet++ (Yeshwanth et al., 2023). Here we report the spatial track quality, reconstruction accuracy, and 2D / 3D open-vocabulary semantic segmentation accuracy. Model Spatial Track Recon. Metric Open-Vocab. Semantic Segment T-mIoU T-SR Abs. Rel τ 2D mIoU 2D mAcc 3D mIoU LSeg OpenSeg Feature-3DGS LSM (2 Views) LSM (Multi-Views) SpaTracker+SAM SAM2* VGGT Ours - - - - - 16.15 44.16 - 73. - - - - - 23.68 57.89 - 98.90 - - 5.92 4.22 2.96 - - 2.75 2.61 - - 41.64 74.02 83.28 - - 85.41 85.66 22.61 13.92 22.47 17.76 17.88 - - - 31.31 34.42 48.13 33.14 26.95 27.84 - - - 70.78 - - 10.59 - 15.17 - - - 20. Instance Spatial Tracking. Specifically, inspired by SAMPart3D (Liu et al., 2025), we apply the density-based clustering algorithm HDBSCAN (McInnes et al., 2017) that gathers multi-view 2D instance features {Oins } into distinct clusters, where each cluster represents unique object instance present in the scene. Then we re-project the assigned cluster labels to their corresponding pixel locations produces set of 3D-consistent 2D instance masks {M ins k=1. Such paradigm enables dense tracking and segmentation of specific instances across multi-view images by leveraging explicit 3D priors, in stark contrast to existing methods that are either limited to discriminating category-level features or lose targets during significant camera motion. i,k }K Open-Vocabulary Semantic Segmentation. These 3D-consistent instance masks serve as effective prompts for any off-the-shelf VLMs (Radford et al., 2021; Ghiasi et al., 2022), enabling them to perform robust open-vocabulary semantic segmentation by assigning semantic category to each mask-defined region. Here we take OpenSeg (Ghiasi et al., 2022) as an example. It first produces image-wise features {F lang i=1, which considers contextual information to enable accurate visual-language alignment of the features. We then aggregate the features within each 2D instance mask {flang i=1 via average mask pooling, yielding compact representation for each instance. This step not only integrates the mask priors into the visual-language space, but also sharpens object boundaries and captures fine-grained local category cues, making the subsequent semantic assignment more accurate and robust. RDHW }N RD}K i QA Scene Grounding. Unlike prior methods that directly align 3D features with language embeddings, our approach offers greater flexibility by decoupling instance clusterings, which can then interact with LMMs (Bai et al., 2025; Team et al., 2023) to support object-centric QA in 3D scenes. Concretely, as shown in Fig. 4, given views, we highlight the image regions corresponding to the same instance with masks {M ins i=1 (rendered in red), and query the LMM with yes/no questions to verify object consistency across views. Finally, we aggregate all positive (yes) responses and concatenate the corresponding masks to form the final segmentation output. i,k}N 6 Under review as conference paper Figure 5: Qualitative results on Instance Spatial Tracking. We present two example scenes from ScanNet (Dai et al., 2017) and ScanNet++ (Yeshwanth et al., 2023), and compare our method with SAM2* and SpaTracker+SAM. All instances are visualized with distinct IDs and colors for clarity. Figure 6: We visualize our 3D-consistent PCA results with corresponding clustered masks derived from instance-grounded features. Similar colors in PCA indicate higher feature similarity between instances. For clustered masks, the same object instance shares the same color across multi-views."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Evaluation Details. We conduct comprehensive experiments on the ScanNet (Dai et al., 2017) and ScanNet++ (Yeshwanth et al., 2023) datasets. From each dataset, we randomly select 10 scenes and sample 8-10 images per scene, with the selection strategy designed to maximize spatial coverage of the scene while preserving sufficient overlap to ensure cross-view consistency. (a) For Instance Spatial Tracking evaluation, we evaluate tracking performance using Temporal mIoU (TmIoU) and Temporal Success Rate (T-SR). T-mIoU measures the segmentation accuracy of the same object across different views, while T-SR assesses whether the object is successfully tracked in every view. (b) For Open-Vocabulary Segmentation evaluation, we follow LangSplat (Qin et al., 2024) and LangSurf (Li et al., 2024b), which adopt mIoU and mAcc to measure 2D segmentation accuracy. In addition, we evaluate the 3D mIoU metric by aligning the reconstructed scene with the ground-truth point cloud. (c) For Reconstruction evaluation, we follow LSM (Fan et al., 2024) and VGGT (Wang et al., 2025) that utilize Absolute Relative Error (Abs. Rel) and Inlier Ratio (τ ) with threshold of 1.03 to assess each scene. The details of these metrics are shown in the appendix. Evaluation of Instance Spatial Tracking. To comprehensively evaluate the tracking quality of our proposed method and competing approaches, particularly under large viewpoint changes with multiple objects, we manually annotate subset of objects across several scenes with precise ground-truth labels (more visualization in the Appendix). For baseline methods, we modify SAM2 (Ravi et al., 2024) to support dense segmentation and tracking under multi-view inputs, denoted as SAM2*. In addition, we integrate SAM into SpaTrackerV2 (Xiao et al., 2025), where tracking points are used as prompts to perform dense segmentation. Tab. 1 and Tab. 2 present the quantitative results, demonstrating the significant superiority of our method. By leveraging implicit 3D reasoning, our approach successfully distinguishes object identities to achieve nearly 100% T-SR accuracy. In contrast, baseline methods fail at this crucial task, yielding T-mIoU below 30%, whereas our approach surpasses 60%. This performance gap is visually demonstrated in Fig. 5, where our method successfully tracks and segments the chair under large camera motions, while competing methods lose the track. 7 Under review as conference paper Figure 7: Qualitative Results of 2D Open-Vocabulary Segmentation on Scannet and Scannet++. Furthermore, we provide additional visualizations of our 3D-consistent instance features using Principal Component Analysis (PCA), along with their corresponding clustered masks, as shown in Fig. 6. As illustrated, IGGT produces 3D-consistent instance-grounded features that remain discriminative across multiple views: multiple instances with the same category exhibit similar yet distinguishable colors in the PCA space. This property serves as crucial foundation for the Instance Spatial Tracking task, as it enables consistent tracking and segmentation of individual objects even under large motions and in the presence of many similar instances. Evaluation of Open-Vocabulary Segmentation. We compare our method with other Image-to-3D feedforward method (Fan et al., 2024), per-scene optimized methods (Zhou et al., 2024; Kobayashi et al., 2022), and 2D methods (Ghiasi et al., 2022; Li et al., 2022) on both Scannet and Scanent++ datasets. The results are reported in Tab. 1 and Tab. 2. On ScanNet++, our method achieves leading performance, surpassing other approaches by 8.34% in mIoU for segmentation and 7.88% in mAcc for object localization. This performance improvement is attributed to our methods superior multi-view consistency, which helps correct object recognition errors caused by incomplete views, as illustrated in Fig. 7, where the sink is difficult to identify due to limited viewpoint coverage. On the other hand, we also evaluate the accuracy of depth estimation on multi-view inputs. The results show that our method is on par with VGGT on ScanNet, and outperforms VGGT on ScanNet++ by 0.14 in Abs. Rel and 0.25 in τ , benefiting from the mutual enhancement of semantics and geometry achieved through joint training. This performance improvement is further demonstrated in 3D segmentation (see Tab. 1 and Tab. 2), where our method outperforms previous approaches by 4.31% and 4.97% in terms of 3D mIoU. As shown in Fig. 8, our method achieves superior 3D semantic representations while also maintaining better segmentation consistency in the same regions. Figure 8: Visualization of 3D Open-Vocab. Segmentation. Applications of QA Scene Grounding. We present the QA application results in Fig. 9 on the Teatime scene from the LERF-OVS (Kerr et al., 2023) dataset, and compare our approach against the state-of-the-art Gemini 2.5 Pro (Comanici et al., 2025). As shown, our instancegrounded querying fully leverages the reasoning capacity of LMMs, achieving accurate segmentation for complex prompts and superior multi-view consistency compared to existing unified generationunderstanding models, thereby enabling more complex QA tasks in 3D scenes. Ablation Study. Here, we showcase the training curve of our IGGT in Fig. 11. Without the crossmodal fusion model, the instance head struggles to capture high-resolution geometric information, resulting in more difficult convergence, as reflected in the sharpness of the chairs edges in the PCA visualization. We also conduct ablations on integrating different VLMs into our method (e.g., LSeg (Li et al., 2022), CLIP (Radford et al., 2021), OpenSeg (Ghiasi et al., 2022)). As shown in the 8 Under review as conference paper Figure 9: Applications of QA Scene Understanding compared with vanilla Gemini 2.5 Pro Comanici et al. (2025) model. Figure 10: Visualization of our method using different VLMs. Table 3: Integration with Different VLMs. Method Scannet Scannet++ mIoU mAcc mIoU mAcc Ours w/ Lseg 60.46 Ours w/ CLIP 49.36 Ours w/ OpenSeg 58.12 81.84 62.68 78.75 22.72 21.52 31.31 63.56 61.36 70.78 Figure 11: Ablation on Cross-Modal Fusion. table, LSeg and OpenSeg, with better global context representation, achieve higher accuracy in handling background classes (e.g., cabinet). In contrast, CLIP, with superior text alignment capabilities, performs better on complex categories, such as DALL-E and Ottolegnghi shown in Fig. 10. This further demonstrates the flexibility of our method in utilizing different VLMs to achieve improved text query performance. 4.1 RELATED WORK Spatial Foundation Models. Image-to-3D reconstruction has evolved from early SfM pipelines like COLMAP (Schonberger & Frahm, 2016), which estimate camera poses and sparse point clouds, to more advanced methods like 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) for efficient novel view synthesis. Scene Representation Transformers (Sajjadi et al., 2022) represent images as latent tokens, enabling view synthesis without accurate poses, but still struggle with explicit geometry and generalization. DUSt3R (Wang et al., 2024) improves upon this by directly regressing dense point maps from unposed image pairs, while VGGT (Wang et al., 2025) scales this approach to multiple images with competitive accuracy. However, these methods remain focused on geometric reconstruction, often neglecting higher-level scene understanding. 3D Scene Understanding. Integrating semantics into 3D reconstruction is vital for scene understanding. Methods like LangSplat (Qin et al., 2024) inject vision-language features into 3D Gaussian Splatting, enabling semantic reasoning, but typically require dense multi-view inputs and per-scene optimization. Approaches such as Panst3R (Zust et al., 2025) and DUSt3R (Wang et al., 2024) attempt feed-forward scene understanding, but decouple geometry and semantics, limiting mutual benefits. Methods like LSM (Fan et al., 2024) and Uni3R (Sun et al., 2025) align spatial models with vision-language models (e.g., LSeg (Li et al., 2022)), but face limitations in integrating stronger VLMs and struggle with fine-grained, instance-level queries in complex scenes."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce IGGT, novel end-to-end framework that unifies the representation for both spatial reconstruction and contextual understanding in 3D scene. The key to our success is that we couple geometric and instance-level semantic features by joint training and unleash the potential of unified large transformer to achieve mutual improvements in contextual understanding and geometry reconstruction. To facilitate this task, we further present large scale dataset 9 Under review as conference paper called InsScene-15K, including high-quality RGB images, poses, depth maps, and 3D-consistent instance masks. Moreover, our proposed instance-grounded scene understanding strategy enables IGGT with plug-and-play integration of various VLMs and LMMs, unlocking broader range of applications. Extensive experiments demonstrate the superiority of our IGGT over the latest stateof-the-art methods in terms of high task performance and 3D coherence. We believe that IGGT provides promising research direction for crafting and understanding intricate 3D worlds jointly and will inspire more works in the future. 10 Under review as conference paper"
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work focuses on improving spatial reconstruction and understanding. While our model is trained on self-annotated datasets based on standard open-source images and tested in controlled settings, we acknowledge that any AI system may potentially exhibit biases or produce unexpected behaviors. Our research is intended for academic exploration only, and we emphasize that any such outcomes do not reflect the views of the authors. We support the development of AI technologies that are ethical, safe, and aligned with societal values."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "All code and model checkpoints will be publicly released to ensure reproducibility."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, and Guofeng Zhang. Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction. IEEE Transactions on Visualization and Computer Graphics, 2024. Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12901299, 2022. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, et al. Large spatial model: End-to-end unposed images to semantic 3d. Advances in neural information processing systems, 37:4021240229, 2024. Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation In European conference on computer vision, pp. 540557. Springer, with image-level labels. 2022. Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting In ACM SIGGRAPH 2024 conference papers, pp. for geometrically accurate radiance fields. 111, 2024. Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. arXiv preprint arXiv:2505.23716, 2025. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1972919739, 2023. 11 Under review as conference paper Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. Advances in neural information processing systems, 35:2331123330, 2022. Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. Hao Li, Yuanyuan Gao, Chenming Wu, Dingwen Zhang, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, and Junwei Han. Ggrt: Towards pose-free generalizable 3d gaussian splatting in real-time. In European Conference on Computer Vision, pp. 325341. Springer, 2024a. Hao Li, Roy Qin, Zhengyu Zou, Diqi He, Bohan Li, Bingquan Dai, Dingewn Zhang, and Junwei Han. Langsurf: Language-embedded surface gaussians for 3d scene understanding. arXiv preprint arXiv:2412.17635, 2024b. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. Leland McInnes, John Healy, Steve Astels, et al. hdbscan: Hierarchical density based clustering. J. Open Source Softw., 2(11):205, 2017. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2013320143, 2023. Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2005120060, 2024. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visuallanguage-action model. arXiv preprint arXiv:2501.15830, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu InWen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, and Jia Deng. finigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2178321794, June 2024. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. 12 Under review as conference paper Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luˇcic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62296238, 2022. Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 41044113, 2016. Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Xiangyu Sun, Liu Liu, Seungtae Nam, Gyeongjin Kang, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang, Eunbyung Park, et al. Uni3r: Unified 3d reconstruction and semantic understanding via generalizable gaussian splatting from unposed multi-view images. arXiv preprint arXiv:2508.03643, 2025. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Yuri Makarov, Bingyi Kang, Xing Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. arXiv preprint arXiv:2507.12462, 2025. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pp. 767783, 2018. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: highfidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1222, 2023. Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, and Parisa Kordjamshidi. Vision-and-language navigation today and tomorrow: survey in the era of foundation models. arXiv preprint arXiv:2407.07035, 2024. Under review as conference paper Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2167621685, 2024. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. Lojze Zust, Yohann Cabon, Juliette Marrie, Leonid Antsfeld, Boris Chidlovskii, Jerome Revaud, and Gabriela Csurka. Panst3r: Multi-view consistent panoptic segmentation. arXiv preprint arXiv:2506.21348, 2025. Under review as conference paper"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 USE OF LARGE LANGUAGE MODELS Large Language Models (LLMs) are used exclusively for minor grammar corrections and stylistic polishing of the manuscript. They are not involved in the design of the methodology, execution of experiments, analysis of results, or any other aspect of the scientific contribution. A.2 RELATED WORK Spatial Foundation Model Image-to-3D reconstruction is long-standing problem in computer vision. Early pipelines such as COLMAP (Schonberger & Frahm, 2016) and related SfM methods estimate camera poses and sparse point clouds, often followed by multi-view stereo (MVS) to obtain dense geometry (Yao et al., 2018). Building on such SfM-based initialization, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) introduced highly efficient representation for photorealistic novel view synthesis, inspiring reconstruction-oriented extensions (Chen et al., 2024; Huang et al., 2024). To reduce the reliance on accurate calibration, Scene Representation Transformers (Sajjadi et al., 2022; Li et al., 2024a) represent multiple images as latent scene tokens, enabling novel view synthesis under uncertain or missing poses, though they still struggle to produce explicit geometry and generalize reliably. DUSt3R (Wang et al., 2024) takes further step by directly regressing dense point maps from unposed image pairs, achieving pixel-aligned geometry without SfM initialization. In contrast, VGGT (Wang et al., 2025) scales this paradigm to dozens to hundreds of images in single feed-forward pass, jointly predicting cameras, depth, point maps, and tracks with competitive accuracy to optimization-based pipelines. Despite these advances, these methods remain focused on low-level geometric reconstruction while overlooking higher-level scene understanding. 3D Scene Understanding Integrating semantics into 3D reconstructions is crucial for higher-level scene understanding tasks. Recent efforts (Zhou et al., 2024; Li et al., 2024b; Qin et al., 2024) like LangSplat (Qin et al., 2024) inject vision-language features (e.g., CLIP (Radford et al., 2021)) into 3D Gaussian Splatting, enabling semantic reasoning over reconstructed scenes. However, these methods typically require dense multi-view inputs and per-scene optimization, which hinders scalability. More generalizable approaches like Panst3R (Zust et al., 2025) build on DUSt3R (Wang et al., 2024) to achieve feed-forward 3D scene understanding directly from posed or unposed images. Yet, they often decouple reconstruction from understanding and freeze the geometry module, which restricts mutual benefits between the two and leads to suboptimal semantic grounding. Parallel attempts such as LSM (Fan et al., 2024) and Uni3R (Sun et al., 2025) seek to bridge geometry and semantics by aligning spatial models with specific vision-language models (e.g., LSeg (Li et al., 2022)), but this tight coupling has two key drawbacks: (1) it prevents seamless integration of stronger VLMs (Tschannen et al., 2025; Simeoni et al., 2025) as they emerge, thereby constraining text query performance; (2) the alignment is typically at the category level rather than instancelevel, so these methods struggle with fine-grained, object-centric QA in scenes that contain multiple similar instances. To address this problem, our proposed framework, IGGT, addresses these limitations by learning unified representation for both reconstruction and understanding. Instead of tightly coupling with single VLM, we introduce an instance-grounded paradigm where instance masks serve as bridge to connect with diverse VLMs and Large Multimodal Models (LMMs) in plug-and-play manner, substantially expanding downstream capabilities. A.3 TRAINING DETAILS Our model is initialized with weights from VGGT (Wang et al., 2025) and fine-tuned on the InsScene-15K dataset, which contains 15,000 scenes. Training is performed on 8 NVIDIA A800 GPUs for 2 days using the AdamW optimizer. The learning rate is set to 1 106 for the large unified Transformer backbone and 1 105 for both the geometry and instance heads. For each training batch, we randomly sample 112 frames from randomly selected scene, yielding total of 24 images per batch. For hyper-parameter settings, we set λpull = 2.0, λpull = 1.0 and = 1.0. 15 Under review as conference paper Figure 12: Visualization of our manually annotated tracking GT and our tracking results. Under review as conference paper A.4 METRICS FOR DIFFERENT TASKS Instance Spatial Tracking. For the Instance Spatial Tracking task, we evaluate tracking performance using Temporal mIoU (T-mIoU) and Temporal Success Rate (T-SR). Given an object and its predicted masks { ˆM t=1, T-mIoU is defined as t=1 across views with corresponding ground-truth masks {M }T }T T-mIoU(o) = 1 (cid:88) t=1 ˆM ˆM o . T-SR evaluates whether the object is successfully tracked across all views, and is defined as T-SR(o) = (cid:104) {1, . . . , }, ˆM > (cid:105) , where [] denotes the indicator function. The final scores are averaged over all objects in the dataset. Figure 13: Visualization of the pipeline from RGB 3D points to semantic labeling, voxelization, and voxel comparison for 3D mIoU. Figure 14: We visualize the RGB and semantic 3D points of the ground truth, IGGT(Ours), LSM(Multi-Views), and Feature-3DGS. 3D Semantic Segmentation mIoU. To evaluate 3D semantic segmentation, we first obtain the RGB 3D points from per-image point maps and align them with the ground truth. Next, we assign semantic labels to the corresponding 3D points based on the results of 2D open-vocabulary segmentation. These labeled 3D points are subsequently voxelized, and the 3D mIoU is computed based on the voxel representation. Fig. 13 illustrates the overall pipeline. Additionally, Fig. 14 presents qualitative results of 3D open-vocabulary segmentation. For LSM, based on its two-view input, we apply the global alignment strategy of Dust3R to optimize the point maps across all views. For Feature3DGS, the ground-truth point maps are used as the initial input. However, due to the sparsity of input views, its reconstruction quality remains limited. 17 Under review as conference paper Figure 15: Comparison between vanilla Scannet++ GT masks and our refined results. A.5 ADDITION VISUALIZATION OF OUR INSSCENE-15K DATASET Fig. 15 presents the vanilla masks and the refined counterparts, together with the IDs that establish the correspondence between them. The refined masks contain fewer unannotated regions and align more closely with the actual objects. Training with these high-quality instance-level masks facilitates more accurate instance-level segmentation and tracking. A.6 DECLARATION OF LLM USAGE In the preparation of this work, the authors used LLM (e.g., GPT-4) in order to improve the readability and language of the manuscript. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article. A.7 LIMITATION Our method adopts an unsupervised clustering strategy on the proposed Instance-Grounded Clustering for post-processing. As result, the accuracy of object boundaries in the clustered masks cannot yet rival that of state-of-the-art segmentation models (e.g., SAM2 (Ravi et al., 2024)). Future work may integrate stronger DETR-based (Cheng et al., 2022) instance heads and larger annotated datasets to improve segmentation accuracy."
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "NWPU",
        "S-Lab, NTU",
        "StepFun, Inc.",
        "THU"
    ]
}