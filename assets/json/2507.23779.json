{
    "paper_title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
    "authors": [
        "Miaosen Zhang",
        "Ziqiang Xu",
        "Jialiang Zhu",
        "Qi Dai",
        "Kai Qiu",
        "Yifan Yang",
        "Chong Luo",
        "Tianyi Chen",
        "Justin Wagle",
        "Tim Franklin",
        "Baining Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the \\textbf{Phi-Ground} model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under $10B$ parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on ScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: \\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}"
        },
        {
            "title": "Start",
            "content": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding"
        },
        {
            "title": "ABSTRACT",
            "content": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \"Iron Man\", are becoming reality. GUI grounding is core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/ Keywords GUI grounding AI agent Large multi-modal model 5 2 0 2 1 3 ] . [ 1 9 7 7 3 2 . 7 0 5 2 : r Figure 1: Left: The comparison chart of our grounding model results across five GUI grounding benchmarks. Our model, trained specifically for the agent setting, achieved SOTA results on all benchmarks under this focus. Even in the general end-to-end model setting, our model attained SOTA results on three of the benchmarks. Right: The relationship between model performance and computational cost on ScreenSpot-pro demonstrates that our model supports the Pareto frontier, indicating its efficiency. Most GUI research traditionally considers only the parameter count for comparison, but our experiments highlight that computational cost during testing, such as the number of image tokens, also significantly impacts performance. The X-axis in the right figure represents D, where is the number of image tokens. Training and inference latency are more linearly correlated with than with . graph using latency as the X-axis closely resembles the right figure, but latency is often influenced by hardware and acceleration libraries such as vllm, so we did not use latency as X-axis. Phi-Ground Tech Report Figure 2: Agent evolution across physical and virtual worlds. Traditional systems rely on fixed controllers and predefined workflows to execute domain-specific tasks, either in physical environments (e.g., task-specific robots) or virtual environments (e.g., API-based Web/APP agents). In the modern era, intelligent automation has emerged. In the physical world, general-purpose robots perform versatile limb-based operations. In the virtual world, Computer Use Agents (CUAs) achieve human-level behaviors through general purpose planner, GUI grounding, enabling them to complete any virtual task achievable via mouse and keyboard interactions."
        },
        {
            "title": "Introduction",
            "content": "Large model based autonomous agents [1, 2], by leveraging their robust reasoning capabilities and interactions with real-world environments [3], enable humans to tackle more complex tasks and significantly enhance productivity. Given that substantial portion of modern human work is conducted via computers, Computer Use Agents (CUAs) hold immense potential and commercial value [4, 5]. While CUAs streamline virtual work, robots [6, 7] simplify physical tasks, as shown in Figure 2, and together, they are poised to drive revolution in productivity. With the development of reasoning models like O3 [8, 9, 10, 11], we are beginning to create prototypes of CUAs, such as OpenAI Operator [12] and Claude Computer Use [13]. However, there remains considerable gap before CUAs can be fully commercialized. This is due to the irreversible effects of many computer operations [14], such as closing unsaved files, as well as significant concerns regarding user privacy protection [15, 16, 17]. Specifically, CUAs can execute actions through the following methods: APIs (e.g., HTML/DOM, Office scripts), scripting tools (such as AutoHotkey, Power Automate, CLI), and simulated input device interactions (such as mouse clicks and keyboard inputs). Traditional approaches based on APIs [18, 19] and scripts are often constrained by the platform in use, environmental dependencies, and the specific APIs provided by applications [20]. In contrast, solutions based on GUI and simulated input device interactions [21] are aligned with human operations. This alignment not only facilitates user supervision, enhancing privacy and security, but also theoretically allows for any interaction that human can perform, without being limited by the application. As result, GUI-based CUAs are becoming focal point of research in this field, as illustrated in Figure 2. When accomplishing task, GUI-based CUA can be divided into two steps: temporal planning and grounding [22, 23]. Planning involves analyzing the task description and the current state to determine the actions that should be taken in the future, while grounding refers to the above mentioned simulation of input devices. Among all interactions, mouse click operations are the most important and common actions for GUI grounding. Since keyboard commands, such as pressing the \"A\" key, are discrete, MLLMs can effectively handle this type of grounding. Hence, our focus is primarily on mouse commands, where the main challenge lies in the fact that mouse command parameters are screen coordinates, and most MLLMs struggle to accurately identify these coordinates [24, 25, 26]. Therefore, specialized training is required for determining the precise click coordinates. 2 Phi-Ground Tech Report Figure 3: Three levels of task of CUAs. Each coral block represent an action step. We focus on the click action, as it is the most important and common operation. Others such as keyboard input can be effectively handled by MLLM. Motivated by the above considerations, this paper conducts detailed empirical study on the training of GUI grounding models. We divide GUI grounding into two components: the first component is spatial planning [27, 23], which involves identifying which specific element in the image needs to be manipulated to execute given instruction. The second component is localization, where, after determining the target location, the model needs to output the correct coordinates, as illustrated in Figure 3. Many existing CUA models [28, 29, 30] attempt to accomplish the grounding task in an end-to-end manner. However, due to the spatial planning component, which also demands strong level of expertise, common sense, and spatial reasoning abilities, there is need for larger or even deep reasoning models. To achieve better results at this stage, we have adopted two-phase approach [31]. Initially, an advanced MLLM provides detailed description of the location, followed by our trained grounding model outputting the specific coordinates. In this technical report, we delve into numerous often-overlooked details spanning data, algorithms, and training methodologies. Counterintuitively, we found that many techniques that appear sound and frequently appear in previous work, such as tokenized coordinates [32, 33, 34] , coordinate label smoothing [35, 36], and loss reweighting, become trivial when dealing with large-scale training. We have retained only the following techniques that remain meaningful under extensive training in the main text. Firstly, at the model input level, we discovered that the order of modality inputs can significantly impact the underlying mechanism of feature modeling, leading to notable differences in results. Secondly, data augmentation [37] is very common in traditional object detection training, yet is rarely mentioned in the era of large models. Our re-experiments indicate that certain data augmentations can greatly enhance results in high-resolution scenarios (such as ScreenSpotpro). Thirdly, the academic community predominantly focuses on the out-of-distribution generalization ability of models, with insufficient research on how models can appropriately perform in-domain continual learning in specific scenarios (such as considering only Photoshop software). This area, however, holds significant practical value. We discuss data strategies and training algorithms for in-domain post-training. Lastly, existing GUI grounding work typically considers only the size of the parameters when making comparisons. We incorporate the computational load during model inference (primarily influenced by the number of image tokens) into the study of scaling laws[38, 39, 40] and evaluations. Ultimately, we devised an efficient and rational training recipe, collecting over 40M data samples from multiple sources [21, 31, 41]. By scaling up the training volume, we successfully developed the Phi-Ground model family. In our evaluation process, to avoid the limitations of relying on single benchmark and to prevent systemic overfitting [42, 43] due to optimization for specific benchmark, we consider multiple benchmarks in both ablation studies and final evaluations. We conducted survey and collected four publicly available test datasets [25, 24, 44, 26], complemented by our internally constructed dataset focusing on commonly used software on Windows, which we refer to as the Gold dataset. This brings our total to five evaluation datasets, whereas most CUA research works [30, 31, 28, 29] on grounding typically utilize only one or two of these. The evaluation results demonstrate that, within the Agent setting we are focused on, our Phi-Ground model achieves state-of-the-art results across all benchmarks, with particularly high scores of 55.0 and 36.2 on ScreenSpot-pro [25] and UI-Vision [24], respectively. Furthermore, in the end-to-end model setting, we also achieved the best results in three of the benchmarks, scoring 43.2 and 27.2 on ScreenSpot-pro and UI-Vision, respectively. These findings indicate that our model possesses strong generalization capabilities. We also present detailed erroneous case study in Section 6.2 and the appendix. We consider GUI grounding to be classic scenario for multimodal model perception [45, 46, 47], and believe that our experiences can be effectively generalized to other fields involving multimodal perception. We hope our research will benefit related domains. Phi-Ground Tech Report Figure 4: In terms of coordinate representation and loss strategies, we experimented with the following approaches: (a) tokenized coordinate representation, (b) label smoothing for coordinates, (c) loss reweighting, and (d) direct textual representation of coordinates."
        },
        {
            "title": "2 Methodology",
            "content": "At the agent level, we adopt two-stage implementation approach: large multimodal model (such as GPT-4O) is utilized to generate detailed and specific Reference Expressions (RE), while smaller multimodal model that we have trained is responsible for generating coordinates based on the RE. At the model level, we fine-tune MLLM to directly output coordinates, which are generated in text form. The coordinates are represented as relative values scaled by 1000 and then rounded (e.g., value of 500 corresponds to coordinate value of 0.5). We present the following discussion and experiments on the aforementioned implementation. Coordinates Representations and Loss During the development, we explored several potential implementation approaches as follows: Tokenized coordinates. Some existing works represent regions as tokens [32, 33, 34], using learnable special tokens to denote specific position. However, in GUI grounding, the large image resolution makes using 2D regions as tokens impractical. We attempted to divide both the horizontal and vertical coordinates into 1000 intervals (which is necessary for large screens like 4K displays), with each interval represented by special token, similar to [48, 49]. Despite experimenting with various initialization and training strategies, our results indicated that introducing large number of unlearned tokens during finetuning pretrained MLLM can lead to model collapse and poor performance. For further details, see the Appendix C.1. Label smoothing. To approximate the classic regression loss, we applied label smoothing to the loss of digit tokens, making the magnitude of the loss proportional to the distance between the prediction and the target. For instance, if the target digit is \"5\", we assign smaller labels to \"4\" and \"6\" to indicate their proximity to the target. The detailed implementation can be found in the Appendix C.2. However, the experimental results indicate that this technique may offer improvements only when the training dataset is small and the batch size is minimal. When we eventually increased the batch size to 2048 and the total training samples reached million level, the application of this technique showed no significant impact or improvement. Loss re-weighting. Following similar idea as Label smoothing, we assigned different loss weights to the digits representing the hundreds, tens, and units places of the coordinates. The results were consistent with the previous findings: not only were the corresponding parameters extremely sensitive, but the technique also lost its advantages as the batch size and training volume increased. See Appendix. C.3. From the above experiments, we found that the most straightforward text output combined with GPT loss (NTP) can effectively facilitate training. We chose to use relative coordinates for expressing positions because their value range is fixed and relatively dense (as the resolution width and height of GUIs are generally greater than 1000 pixels), which facilitates better model learning. Additionally, since relative coordinates are floating-point numbers that are 1, the leading \"0.\" is insignificant. Therefore, following [31], we scaled these values by multiplying them by 1000. 4 Phi-Ground Tech Report Figure 5: CommonCrawl data processing pipeline."
        },
        {
            "title": "3.1 Processing Open Source Data",
            "content": "We utilized portion of the open-source data from OS-Atlas [31], which includes several subsets such as Windows, macOS, Linux, and Fineweb. Additionally, we incorporated data from SeeClick [26], E2ISynth [41] and GUIAct [21]. The total data volume amounts to approximately 10M entries. By employing the prompt outlined in Appendix E.1, we re-annotated all the data using GPT-4O to generate Long-gold type reference expressions (as described in Sec 4). Furthermore, we constructed training data by randomly combining three types of reference expressions."
        },
        {
            "title": "3.2 CommonCrawl Data",
            "content": "To acquire larger-scale data for better scaling up of training, we also obtained web pages from CommonCrawl [50] and rendered screenshots to generate training data. However, the web data contained significant amount of noisy data that caused training failures. To address this, we constructed highly specific data cleaning pipeline, as illustrated in Figure 5. Below are the detailed steps of each stage: Index and domain deduplication We utilized the CC-MAIN-2024-46 crawl from CommonCrawl. After basic deduplication of URLs (exact match), filtering by language (retaining only English), and webpage status (retaining only 2xx, 301, and 302), we were left with 2.6 billion URLs. These 2.6B URLs originate from 45.6 million unique domains, with the number of pages from the same domain displaying long-tail distribution. For instance, the largest domain contains 204K different pages. We observed that pages from the same domain exhibit strong consistency in layout. Therefore, to ensure the generalizability of our model, we performed random sampling so that no more than 50 pages were selected from each domain. After this round of sampling, we were left with 475.45M URLs. Rendering We utilized the Selenium library and Google Chrome Driver to render webpage screenshots. During the rendering process, we randomly selected from three different pixel areas corresponding to 1080p, 2K, and 4K screen resolutions. The aspect ratio of the images was randomly chosen between 2:1 and 1:2. For the elements within the webpage HTML, we designed several rules for filtering and retaining them, as detailed in Appendix D.1.1. This process allowed us to preserve elements that are likely to be interactive components. At this stage, we save webpage screenshots, element information, and layout graphs (with different types such as interactive text buttons, interactive icon buttons, and images corresponding to specific colors). After this stage, there retained 285M webpages. 5 Phi-Ground Tech Report Rule-based filtering Subsequently, we designed more fine-grained filters and deduplication techniques at the webpage and element levels based on the preserved webpages. The specific details are provided in Appendix D.1.2. These filters eliminated many erroneous and overly simplistic webpages. After this phase, 73.5M webpages remained. Element selection and labeling Finally, when selecting elements, we consider the distribution of element centroids and their types, as we found it necessary to do so in Sec. 5.3. Specifically, we discretize and uniformly sample across various regions of the canvas (see Appendix D.2). During sampling in discrete area, we prioritize sampling icon elements, as they are less frequent. We sample only one element per webpage screenshot. Consequently, after this stage, 10.5M elements and screenshots remain. Finally, we use GPT-4O to annotate all the data with prompt in Appendix E.1. Table 1: The applications distribution of the search queries. These applications are derived from an internal statistical list of the most frequently used applications on Windows."
        },
        {
            "title": "Domain",
            "content": "# Apps Example apps # Queries Office & Productivity Media Playback & Creation Gaming Platforms Social & Communication Security & System Tools Core Windows UI"
        },
        {
            "title": "Total",
            "content": "10 11 10 10 10 15 66 Word, Excel VLC, Photoshop, Clipchamp Steam, League of Legends Discord, Zoom CCleaner, Malwarebytes Settings, File Explorer - 50 55 50 50 50 75 Figure 6: Filtered and kept image of the Web Search Data with the classifier."
        },
        {
            "title": "3.3 Web Search Data",
            "content": "We construct complementary, high-resolution screenshot corpus with the Bing Image Search v7 API. We first generate queries from 6 UI domains which covering total of 66 desktop applications, as shown in Table 1. For each application we manually construct file search phrases such as Word ribbon interface or screen shot of VLC playback controls, producing 330 queries altogether. Every query retrieves 2048 candidate images and enforces minimum resolution of 200 200 px. The request also specifies Bings license flag to ensure that all the images we use comply with copyright requirements. Each downloaded image is then scored by CLIP-based classifier to remove non-screenshot images from the search result. For the final filtered screenshots, we employed OmniParserV2 [51] to annotate all bounding boxes. We then used the same method described in 3.2 to label reference expressions and perform rule-based filtering at the bounding box level. This process resulted in approximately 158K data samples."
        },
        {
            "title": "3.4 Human Labeled Data",
            "content": "To address our focus on specific scenarios (Windows and common applications) and to explore in-domain training techniques, we have developed pipeline for constructing human-labeled data. The data construction process involves three steps: In the first step, labelers use custom screen recording software to interact with the target scenarios, which include specific software or Windows system settings pages. They are required to navigate and operate various pages and functions of the software. The screen recording software automatically captures different pages and retrieves the UIATree of the page. If the software does not implement UIA, we utilize OmniParser-V2 [51] to obtain the bounding boxes of elements. In the second step, recognizing that the acquired bounding boxes contain numerous errors, redundancies, and non-interactive content (such as text portions in Word documents), we developed another software tool to enable labelers to remove erroneous bounding boxes. Finally, we employ the Long-gold method, as described in Sec. 4.2, to have GPT-4O annotate references for all bounding boxes. Phi-Ground Tech Report Figure 7: Examples of benchmarks used in evaluation. Ultimately, we generated 80K training samples derived from comprehensive suite including Microsoft Office, Windows settings, and over dozen common software applications such as 7zip, PhotoShop, ClipChamp and audio tools."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "To ensure the models generalization capability and to avoid systematic overfitting to well-known benchmarks such as ScreenSpot, we have gathered several recent open-source and internally developed evaluation datasets. This approach aims to ensure the comprehensiveness of our testing. ScreenSpot V1 & V2 ScreenSpot [26] is the first realistic GUI grounding benchmark that includes over 600 interface screenshots from mobile, desktop, and web environments. It encompasses two different types of elements, namely, texts and icons. However, due to several incorrect or ambiguous annotations in the original benchmark, OS-Atlas [31] introduces an updated version called ScreenSpot-V2, which corrects various mislabeled instances in the original ScreenSpot dataset. ScreenSpot Pro ScreenSpot-Pro [25] focuses on high-resolution GUI tasks in professional applications. It contains 1,581 samples encompassing 23 applications across 3 operating systems, and the micro-elements only cover average 0.07% of the screen, which leads to highly challenging assessment. UI-Vision UI-Vision [24] provides three fine-to-coarse grained tasksElement Grounding, Layout Grounding, and Action Prediction. Similar to ScreenSpot-pro, this benchmark encompasses another set of practical applications such as education and entertainment. It also categorizes three different types of references for testing purposes. Due to the selection of smaller buttons or those requiring more expert knowledge, this benchmark is equally challenging as ScreenSpot-pro. Moreover, it features larger sample size, enhancing the stability and reliability of the evaluations. Showdown-click-dev Showdown [44] is collection of 5,679 left clicks of humans performing various tasks in macOS desktop environment. subset containing 557 clicks was released on GitHub and huggingface, namely Phi-Ground Tech Report Showdown-click-dev. It reported grounding results and latency of several advanced CUAs like OpenAI Operator [12] and Claude computer use [13]. Gold Dataset (proprietary) We have also internally constructed an evaluation dataset tailored for application scenarios within Windows. This dataset includes six scenarios: Photoshop (213 samples), ClipChamp (179 samples), PowerPoint (82 samples), Excel (107 samples), Word (87 samples), and Windows settings (1266 samples). The bounding boxes and instructions (Short RE) for each scenario were annotated by professional engineers familiar with Windows. While these share the same domain as the human-labeled data in Sec 3.4, the labelers come from different teams and use different computer settings, such as theme colors and screen sizes. In contrast, the selection of elements and the generation of REs in our training sets human-labeled data are automated or AI-generated, lacking expert knowledge. We employ this Gold dataset to evaluate in-domain training research. Additionally, it serves as an effective validation of practicality within Windows scenarios. In subsequent experiments, to facilitate faster testing, we manually extracted smaller test set of 211 samples from this dataset, referred to as Gold-S. We present examples of these benchmarks in Figure 7. In addition, there are benchmarks like WinClick [52] that are still in the process of being open-sourced. As result, we are unable to present their results in this paper. However, we plan to showcase these results on the project homepage once they are released."
        },
        {
            "title": "4.2 Evaluation Protocols and metrics",
            "content": "Reference expression type Reference Expression (RE) refers to segment of referring text provided to grounding model or GUI agent model concerning specific interactive area, such as button. This expression can be an indirect instruction like \"close the webpage\", or direct and specific description such as \"the blue settings icon in the upper left corner\". The use of different REs during training and testing can significantly impact the models performance. Generally, we desire the model to generalize well across various REs. However, when using indirect REs, the model also needs some planning capability. To decouple model capabilities of planning and perception for research and enable efficient application in smaller models, previous works [31] introduced simplified agent setting: employing more powerful MLLMs like GPT-4O [53] and O4-mini [8] for planning and generating more detailed REs, while smaller grounding model is used to produce coordinates. In this paper, to minimize misunderstanding, we define the following three types of REs. Short / instruction. \"Short RE\" refers to more concise form, potentially containing some instructions that require planning. During testing, we use \"Short RE\" to denote the reference provided by the benchmarks. This type of RE will be represented as the \"End-to-end model setting\" in the subsequent results tables. Long / agent. Following [30, 31], for short RE, we can utilize advanced MLLM to expand it into three more explicit and detailed types of RE: functional, positional, and appearance. We allow the large model to generate these three types of references and concatenate the references together, forming what we call Long RE. During testing, the input to the MLLM consists of only the screenshot and the short RE, and the output is the Long RE, which is then passed to smaller model to generate coordinates. In subsequent tables, this approach is referred to as the \"Agent setting\". The specific system prompt for generating Long RE can be found in Appendix E.2. Long-gold. In the process of constructing the training dataset, on one hand, there is no short RE provided, while on the other, we can utilize ground truth (GT) bounding boxes. We also instruct the MLLM to generate the aforementioned three types of long REs. The difference lies in the input we provide to the MLLM, which includes screenshot annotated with the GT bbox and cropped image of the target region. This allows the model to better observe and generate high-quality REs. However, since the generation process relies on the GT, this Long-gold RE is used solely for generating training data and not for evaluation. The specific system prompt can be found in Appendix E.1. Since this paper primarily focuses on model perception rather than planning, and all training utilizes Long-gold RE, unless otherwise specified, all experimental results in Section 5 are based on Long RE generated by GPT-4O. Metrics In all tables within this paper, unless otherwise specified, the reported metric is click accuracy. The benchmark provides GT bounding box, and the model being tested generates click coordinate. If the models output format is bounding box, the center of the box is taken as the click coordinate. click is considered correct if it falls within the area of the GT bounding box; otherwise, it is deemed incorrect. This method is used to calculate the accuracy. 8 Phi-Ground Tech Report Figure 8: Illustration of the impact of modal input order on model training."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Input / Output Format The order of input modality. We investigated and experimented with the order in which text (or RE) and images are input into the model. The results displayed in Table 2 indicate that inputting text before images yields significantly better outcomes than the reverse order, aligning well with our expectations. Figure 8 illustrates the underlying mechanism: due to the use of causal masks in transformer decoders attention, earlier tokens cannot be updated using later tokens. When images are input into the model first, the image tokens in the later layers, adapted through training, can be understood as being tailored for grounding tasks but unrelated to the RE. Conversely, when the order is reversed, the modeling of image tokens becomes instruction-aware. For perceptual tasks, the effectiveness of image modeling directly influences the final results. This observation bears significant resemblance to existing research related to instruction-aware models [54, 55, 56]. simple modification of the modality order can effectively achieve this outcome. Table 2: Comparison of input order of modalities. Input format max_crops SSv2-Mobile SSv2-Desktop SSv2-Web SS-pro Gold-S"
        },
        {
            "title": "Text first",
            "content": "6 15 6 15 85.1 85.4 82.7 83.2 81.1 82.6 14.6 27. 84.3 86.1 87.5 (+2.4) 87.1 (+1.7) 83.2 (+0.5) 85.3 (+2.1) 83.1 (+2.0) 84.2 (+1.6) 18.7 (+4.1) 30.6 (+2.9) 86.7 (+2.4) 89.2 (+3.1) Output format. We investigated the impact of output formats on results, where output formats refer to the representation of coordinates, such as bounding boxes and points. Depending on the scenario, developers might need to employ different output formats. For instance, in scenarios involving the implementation of an OS agent, grounding only requires providing the coordinates for click, which means delivering point. However, some developers may wish the model to generate bounding box. For the output format of boxes, using the center point of box can serve to determine the click position, thereby accommodating the previous scenario. We discussed the following formats and implementation strategies: Point+OmniParser. The model will output text in the following format: < point > midx, midy < /point >, where midx and midy represent the coordinates of the central point. In scenarios involving box output, we first utilize OmniParser-V2 [51] to annotate all elements within the image. Subsequently, we select the target box using the points provided by the grounding model. XYXY. The model will output bounding box with format: < box > x1, y1, x2, y2 < /box >, where the (x1, y1) and the (x2, y2) are the top-left and bottom-right point. XYWH. The model will output bounding box with format: < box > x1, y1, w, < /box >, where the (x1, y1) is the top-left corner and w, are the width and height of the box. MidWH. The model will output bounding box with format: < box > mixx, midy, w, < /box >, where the (midx, midy) is the center point of the bounding box and w, are the width and height of the box. Results are shown in the table 3. We found that directly outputting the point format achieved the best Click ACC, while the MidWH format better balanced Click ACC and the precision of the detection box. These conclusions align well with intuition. In terms of output precision for detection boxes, the XYXY format demonstrated the best performance. We observed that integrating OmniParser-V2 did not yield better results. On the one hand, the performance on the 9 Phi-Ground Tech Report Table 3: Results of training using different output formats. Note that for the point+OP format, the Click ACC reported in the table is derived directly from using the models output point as the click target. In contrast, IoU-related metrics are calculated by selecting box annotated by OmniParserV2 based on the models output. The results in parentheses represent the average of the SSv2-desktop and SSv2-web subsets."
        },
        {
            "title": "Click ACC",
            "content": "IoU@0.3 IoU@0.5 IoU@0."
        },
        {
            "title": "IoU",
            "content": "Click ACC IoU@0."
        },
        {
            "title": "IoU",
            "content": "ScreenSpot-V2 ScreenSpot-pro point+OP"
        },
        {
            "title": "XYXY\nXYWH\nMidWH",
            "content": "85.5 84.7 84.2 85.0 47.3 (61.8) 39.3 (53.8) 22.0 (30.9) 36.7 (48.1) 72.4 (71.2) 73.6 (70.9) 73.6 (71.5) 59.6 (59.0) 58.9 (57.7) 57.1 (58.1) 26.6 (28.3) 22.5 (25.2) 21.7 (25.0) 53.7 (53.7) 52.6 (52.1) 51.6 (52.2) 30.6 30.0 29.4 30. 22.3 23.8 21.8 22.4 18.8 20.1 18.6 19.2 ScreenSpot-mobile subset was exceptionally poor. On other subsets, the results were still slightly lower compared to models that output box coordinates end-to-end. However, for high box precision required situations (IoU > 0.8) and high-resolution scenarios like ScreenSpot-Pro, the gap was reduced, indicating that OmniParsers implementation may have certain advantages in high-resolution contexts and generate more accurate boxes."
        },
        {
            "title": "5.2 Data Augmentation",
            "content": "Algorithm 1 random crop Require: img, box,Croping probability: random_crop, Minimal cropping ratio: min_crop Ensure: Cropped image img,Updated box box 1: if rand() < random_crop then (w, h) img.size 2: xlef box[0] 3: xright 1.0 box[2] 4: xcrop_f actor min_crop + rand() (1 5: min_crop) cropx1 xlef (1 xcrop_f actor) cropx2 (box[2] + xright xcrop_f actor) 6: 7: 8: 9: 10: 11: 12: 13: 14: end if ... Same for axis ... img CropImage(img, (cropx1, cropy1, cropx2, cropy2)) box UpdateBox(box) Algorithm 2 random resize and padding Require: img, Target dimensions (W, H), Random resize probability: random_resize, maximin screen size: max_screen_size w , ) max_screen_size Smax Ensure: Output image canvas, Updated box box 1: (w, h) img.size 2: canvas WhiteImage(W, H) 3: if rand() < random_resize then Smax min(1.0, 4: Smin 5: scale Smin + rand() (Smax Smin) 6: ( ˆw, ˆh) (w scale, scale) 7: img Resize(img, ( ˆw, ˆh)) 8: pos (randint(0, ˆw), randint(0, ˆh)) 9: 10: else 11: 12: 13: end if 14: canvas.paste(img, pos) 15: box UpdateBox(box) Scale the image to fit in canvas... pos (0, 0) We investigated the effects of two data augmentation techniques on the training of UI grounding models: Random Crop When users software interface is larger than their screen, or under certain interface scaling conditions, it may result in the display of an incomplete page. Additionally, the ScreenSpot contains small number of incomplete UI. Motivated by these observations, we introduce random crop as data augmentation technique. We define the right edge of the bounding box and the right edge of the image as coordinates 0 and 1, respectively. We then select random number within the range [min_crop, 1], which determines the new right boundary of the image. The same process is applied to other directions. Notably, we use the same random number for both the left and right sides (as well as the top and bottom), ensuring that the cropping is proportional. This approach maintains the positional integrity of objects, such that if box is located on the left side of the original image, it will remain on the left side of the cropped image. This helps avoid potential errors or changes in positional references. The detailed implementation is shown in Algorithm 1. 10 Phi-Ground Tech Report Random Resize When user reduces the software interface size or when the screen resolution is very high, items may become very small. To address this issue, we introduce random resize data augmentation technique. The core of this data augmentation method is to shrink the image and place it onto fixed-size white canvas. The canvas size is generally related to the design of the LMM model, particularly how it partitions the image into patches. We set maximum screen size (e.g., 4K), and the canvas size is randomly selected between the size of the images in the training set and the maximum screen size. The purpose of this approach is to leverage the inherent size of the training set images (for instance, if the training set images are already large, excessive resizing should be avoided). For more details, please refer to Algorithm 2. Table 4: Data augmentation ablation experiments."
        },
        {
            "title": "Data Augmentation",
            "content": "SSv2-desktop SSv2-web SSv2-mobile SS-pro Gold-S base base + R-crop base + R-resize base + R-crop + R-resize 85.9 86.2 85.9 86.5 84.7 84.9 85.1 85.1 88.1 87.9 88.3 88.3 24.8 23.6 32.8 (+8.0) 32.7 (+7.9) 90.0 89.8 89.1 91.0 We conducted several sets of hyperparameter experiments, and the optimal combination is as follows: The probability for Random Resize is set to 100%, with maximum screen size of 4096, while the probability for random cropping is 0.3 with min_crop = 0.7. We utilized these hyperparameters in all subsequent experiments. Table 5 presents the results of the ablation study on data augmentation. The results indicate that in high-resolution testing environments such as ScreenSpot-pro, employing random resize significantly improves performance. On the other hand, random cropping does not have substantial impact in various scenarios."
        },
        {
            "title": "5.3 Data Distribution",
            "content": "Table 5: Data ablation experiments. \"Bing\" refers to our BingSearch dataset, \"CC\" denotes the CommonCrawl dataset, and \"CC\" and \"CC-re\" represent the datasets before and after applying our proposed re-sampling technique, respectively."
        },
        {
            "title": "ID Data",
            "content": "SSv2 SS-pro Gold-S 1 2 3 4 5 base 97%base + 3%Bing 70%base + 30%CC 70%base + 30%CC-Re 67%base + 3%Bing + 30%CC-Re 85.3 85.5 85.8 85.5 86.1 32.3 32.7 31.5 33.1 33. 89.1 89.6 90.0 89.6 89.7 Figure 9: Data distribution. We analyzed the distribution of the relative positions of the center points of all detection boxes within each dataset, as illustrated in Figure 10. Our findings reveal that data from different sources exhibit distinct distribution patterns. For instance, we observed that web-rendering data such as FineWeb, SeeClick, and our CommonCrawl data (when not resampled) exhibit an identical distribution pattern, as shown in the first graph of Figure 10(a). These datasets almost completely lack buttons on the right side, which can be attributed to the common web design practice of placing sidebars on the left and reserving the right side for scroll bars. However, such distribution is not universal. For instance, in other datasets, the distribution tends to be more uniform. Our BingSearch demonstrates the best uniformity and diversity. Different software exhibits its unique distribution, as evidenced by our human-labeled data. Therefore, we proposed an algorithm to resample our own CommonCrawl data. The basic idea of the algorithm is to divide the image into 50 50 grid and sample fixed number of points from each grid cell. This approach ensures that the central points are uniformly distributed by area, as detailed in Appendix D.2. During the resampling process, we introduced sampling factor to balance the trade-off between sampling rate and uniformity, as illustrated in Figure 10(b). To validate the generated data and the effects of the aforementioned data resampling, we first constructed base setting using open-source data, as shown in Figure 9. The data proportions are roughly proportional to the size of the subset. Phi-Ground Tech Report Figure 10: Center point distribution of training and evaluation data. We sampled 5M data points from this distribution to train the model. Based on this, we gradually replaced with our data, such as replacing 3% of the BingSearch data (3% of 5M is approximately equivalent to 1 epoch of BingSearch) or 30% of the CommonCrawl data. The results are shown in Table 5. Comparing experiments ID-1, ID-2, and ID-3, the models performance on specific benchmarks improved after incorporating our data. Since we primarily replaced web data (Fineweb and SeeClick), this indicates that our data surpasses the web data from open-source datasets, mainly due to the greater variety of our data. Comparing experiments ID-3 and ID-4, there is significant improvement on the ScreenSpot-pro dataset. This suggests that uniform distribution of boxes is more beneficial for generalization in high-resolution scenarios. Finally, combining all our data achieved the best results (ID-5). 5.4 In-domain Post-training In this section, we will utilize our human-labeled data and explore fine-tuning strategies for target scenarios. In practical applications, developers might have small set of target software that they wish to cover with their agent. We will use Adobe Photoshop (PS) as an example. Our human-labeled data includes screen recordings processed using the method described in Sec. 3.4. The bounding boxes are derived from UIA-tree, while the references are sourced from GPT-4o. For our evaluation set, we focus on the PS subset of our Gold dataset, as well as the PS subset from ScreenSpot-pro. It is important to note that our Gold dataset was provided by another team, and both the bounding box and reference annotations were done by humans. Therefore, there may be discrepancies with our training set. Domain Finetuning When performing domain fine-tuning, we consider the following strategies: 12 Phi-Ground Tech Report Strategy involves directly incorporating domain-specific data into the pre-trained model. Strategy entails performing SFT of pre-trained model (without using domain data) using the domain data. Strategy first introduces small proportion of domain data during the pre-training phase, followed by using larger proportion of domain data during the SFT phase. We set the pre-training data size to 5M and the SFT data size to 200K. Training subset for too many epochs (e.g., more than 10) can lead to overfitting and other issues. Thus, we limit the number of epochs of domain data for both the pre-training and SFT stages to approximately 3 epochs. Due to the difference in total data volume, the proportion of domain-specific data will vary accordingly. Table 6: Comparison of data selection strategies during in-domain post-training and pretraining."
        },
        {
            "title": "Strategy",
            "content": "- A-1 A-2 B-1 B-2 C"
        },
        {
            "title": "Domain",
            "content": "base PS (r / ep) base PS (r / ep) SSv2 SSpro Gold-PS SSpro-PS 0% / 0 100% 99.5% 0.5% / 2.2 99.0% 1.0% / 4.5 - - - - - - 100% 100% 99.5% 0.5% / 2. 0% / 0 0% / 0 88.0% 12.0% / 2.1 76.0% 24.0% / 4.3 88.0% 12.0% / 2.1 86.1 86.2 86.0 84.2 84.4 85.8 33.3 32.7 34.4 31.4 30.7 34. 68.1 71.8 72.7 72.0 74.2 74.8 39.2 41.2 41.2 41.2 41.2 43.1 The results, as shown in Table 6, indicate that directly incorporating domain-specific data into the training set has minimal impact on the models general capabilities. However, the improvement in performance for the specific domain is quite limited. This may be due to the relatively small proportion of domain-specific data, which constitutes only minor part of the optimization objective. In the case of Strategy B, where domain-specific data is introduced only during the fine-tuning phase, we observed significant improvement in in-domain results. However, we also noted marked decline in the models general capabilities after fine-tuning. This suggests that incorporating previously unseen domain data during fine-tuning may lead to overfitting and catastrophic forgetting of pre-trained knowledge. Strategy effectively balances general capabilities and in-domain performance. It maintains strong general abilities from the pre-training phase while also achieving the best results within the domain. During our fine-tuning process, we found that the gradient norm for Strategy was smaller than that for Strategy B. This suggests that the decline in general capabilities observed in Strategy may also stem from training instability caused by the introduction of new data. Since Strategy is exposed to domain data during the pre-training phase, it experiences more tempered impact during fine-tuning. Post-training Algorithms Beyond the impact at the data level, we also explored the effects of training algorithms during the post-training phase. We primarily considered the following categories of algorithms: Supervised FineTuning, Curriculum Learning, and Reinforcement Learning. For reinforcement learning, we further examined several algorithms, including reject sampling finetuning (which is not strictly traditional reinforcement learning but has been shown by many studies [57, 58, 59, 60] to have similar characteristics and effects), DPO [61], REINFORCE [62], PPO [63], REINFORCE++ [64] and GRPO [65]. However, we failed to make positive increase on results with PPO, REINFORCE++, and GRPO, primarily due to three reasons: (1) The pure perception tasks lacked textual exploration and reasoning and hence, in this situation, the essence of using RL loss will be more closely aligned with the original SFT loss. (2) Although some previous work [66, 67, 68] has shown that RL can provide benefits in purely perceptual tasks, these studies typically begin with relatively low baselines or apply RL directly from scratch without prior SFT. In contrast, our work involves pre-training on in-domain tasks to an optimal level, leaving little room for further optimization. (3) The absence of exploration and reasoning led to low diversity in the answers among rollouts for the same sample, frequently resulting in all rollouts being either entirely correct or entirely incorrect (about 70% probability). This made the training of algorithms like GRPO trivial and further hindered the effective training of the critic model in PPO. This prompted us to consider using simpler RL algorithms such as reject sampling finetuning and DPO, as well as manual intervention in the selection of rollout samples. We employed unified framework, as illustrated in Figure 11, to integrate SFT, DPO, curriculum learning, and reject sampling finetuning. We ensured that the total number of samples used was consistent with the 200K samples used in SFT. Taking DPO as an example, the effective training samples only include \"Case 1\" in Figure 11, resulting in less than 100K samples being used for training. We implemented DPO using the trl library [69], which also includes several variants of DPO, such as sigmoid [61](original DPO implementation), hinge [70], IPO [71], exo [72], nca [73], 13 Phi-Ground Tech Report Table 7: Results of different post-training algorithms which further optimized pre-trained model that had reached saturation. To our surprise, we found that for purely perceptual tasks, DPO could still enhance the results of the pre-trained model. However, this conclusion primarily stems from data-level adaptability and robustness, which is fundamentally different from the mechanisms of LLM reasoning."
        },
        {
            "title": "Base Model",
            "content": "SFT Curriculum-L Reject Sampling DPO-sigmoid DPO-IPO DPO-NCA DPO-DiscoPoP"
        },
        {
            "title": "Domain",
            "content": "SSv2 SSpro Gold-S Gold-PS SSpro-PS 86.2 85.8 86.1 86.2 86.2 86.4 85.2 85. 32.7 34.2 33.7 34.5 35.2 34.8 33.9 34.8 88.5 89.1 89.4 88.6 90.5 90.3 88.7 91. 71.8 74.8 75.1 75.5 76.8 75.5 75.7 76.3 41.2 43.1 45.1 45.1 49.0 43.1 47.1 49. Figure 11: :DPO. :Curriculum learning. :Reject sampling finetuning. + = robust-dpo [74], SPPO [75], AOT [76], DiscoPoP [77], and APO [78]. For each variant of DPO, as well as for SFT, curriculum learning, and reject sampling, we conducted hyperparameter tuning. This included grid search for learning rates ranging from 3e-6 to 3e-4, as well as an exploration of the hyperparameters most influential in each algorithm, such as β in DPO. In Table 7, we present the results of various algorithms, including the top-performing variants of DPO. The results indicate that RL can consistently outperform SFT in purely perceptual tasks, even when starting from highly optimized pre-trained checkpoint, which is non-trivial conclusion. As previously mentioned, purely perceptual tasks lack text-level reasoning, exploration, and reflection. While existing studies [66, 67, 68] have shown that reinforcement learning can enhance models that are either not pre-trained or insufficiently pre-trained, this improvement likely stems from mechanisms similar to those used by SFT. It remains uncertain whether reinforcement learning is beneficial for model that has been pre-trained to its maximum potential. In conjunction with the results of curriculum learning, we summarize the principles underlying the advantages of RL in post-training as follows, which are fundamentally different from those in RL work related to LLM reasoning: Due to the targeted nature of rollouts selection by these algorithms, they exhibit greater robustness in the data distribution during post-training. The advantages of RL in post-training may stem from the adaptive data distribution selection, akin to curriculum learning. Similarly, because DPO selects samples that include both correct and incorrect outputs, it may mitigate the impact of simplistic data and, more importantly, erroneous ground truth. Compared to SFT and curriculum learning, which use ground truth coordinates for training, RL relies solely on the model it-selfs outputs. This results in more gradual and stable training process."
        },
        {
            "title": "5.5 Scaling",
            "content": "Scaling settings. We have set up the training settings for the scaling experiments by integrating all the conclusions drawn from previous sections. Specifically, we combined all available datasets at certain ratio. Note that the Seeclick dataset was excluded after experimentation due to the excessive number of elements on average in each screenshot. For instance, in the case of 40M training, Table 8 shows the proportion of each dataset used. For other training volumes, such as 20M, the proportions are calculated accordingly based on the weights. During the training process, we further increased the batch size to 8K. As illustrated in Figure 12, the training loss curve is exceptionally smooth and stable. We also observed some fluctuations in the performance across different benchmarks during training, which led us to conduct tests at regular intervals and ultimately select the checkpoint that performed best across all benchmarks. Table 9 presents the model cards for all the models trained. 14 Phi-Ground Tech Report Table 8: Detailed training data proportion for Phi-Ground-4B-16C with 40M training volume. : The number of samples here does not refer to the quantity of images or elements. In fact, because each element has three types of referencespositional, appearance, and functionalwe randomly combine them during training as model inputs. This combination could involve one, two, or all three types. Consequently, single element can be paired with various references, resulting in multiple samples. This explains why the numbers here differ from those described earlier."
        },
        {
            "title": "Dataset",
            "content": "Samples Epoch Weight BingSearch Linux MacOS Windows GUIAct E2ISynth Fineweb CommonCrawl Human"
        },
        {
            "title": "158 K\n149 K\n69 K\n3.5 M\n155 K\n180 K\n9.0 M\n25.3 M\n160 K",
            "content": "7.6 3.6 3.9 1.1 3.4 2.9 0.9 1.0 3.5 3.0% 1.3% 0.7% 10.0% 1.3% 1.3% 21.0 % 60.0% 1.4% Figure 12: Training and evaluation curves for Phi-Ground-4B-16C. Scaling effect of parameters-computation trade-off. Scaling laws [38, 39, 40] typically examine the relationship between model parameters and training data given fixed training budget. In our scenario, we further consider the computational cost during testing, which is not only related to the number of parameters but is also directly influenced by the number of image tokens. In perception tasks, the number of image tokens is highly correlated with the models capability (see Table 2). In this section, we aim to investigate the relationship among model parameters, the number of image tokens, and training data volume. This is particularly relevant in real-world applications where developers are concerned with both training performance under limited resources and application latency (rather than just parameter count). Conducting such experiments can offer more economical strategies. Table 9: Models card of Phi-Ground family."
        },
        {
            "title": "Training details",
            "content": "Phi-Ground-4B-7C Pretrained with Phi-3.5-Vision-Instruct as base model, which is 4.1B VLM with CLIP as image encoder. During training we set the num_crops of the model to 6 and make sure the images resolution is 2 3. The model will devide the image into 2 3 + 1 = 7 crops, in which there is global crop. Trained with 40M data; BS=8192; LR=8e-5. The training cost 200 A100 GPU days in total. Phi-Ground-4B-16C Same with the above but change the image resolution to 3 5 and num_crops to 15. Trained with 40M data; BS=8192; LR=8e-5. The training cost 440 A100 GPU days in total. Phi-Ground-4B-16C-DPO Same with Phi-Ground-4B-16C Three rounds of DPO finetuning with more desktop data from Phi-Ground-4B-16C. See Sec. 5.4 and Table 11 for more details. Phi-Ground-4B-29C Phi-Ground-7B-7C Same with Phi-Ground-4B-16C except for changing the image resolution to 4 7 and num_crops to 28. Trained with 20M data; BS=8192; LR=8e-5. The training cost 415 A100 GPU days in total. variation of Phi-4-MM [79] with 7B parameters. The model use SigLip as the image encoder. The input images resolution is 2 3 and num_crops of the model is 6 . Trained with 30M data; BS=8192; LR=1e-5. The training cost 350 A100 GPU days in total. Phi-Ground-7B-16C Same with Phi-Ground-7B-7C except for changing the image resolution to 3 5 and num_crops to 15. Trained with 15M data; BS=8192; LR=1e-5. The training cost 450 A100 GPU days in total. Phi-Ground-7B-16C-DPO Same with Phi-Ground-7B-16C. Three rounds of DPO finetuning with more desktop data from Phi-Ground-7B-16C. See Sec. 5.4 and Table 11 for more details. Phi-Ground-7B-29C Same with Phi-Ground-7B-7C except for changing the image resolution to 4 7 and num_crops to 28. Trained with 7.5M data; BS=8192; LR=1e-5. The training cost 390 A100 GPU days in total."
        },
        {
            "title": "Image tokens",
            "content": "1045 2353 2353 4237 1841 4161 7505 Specifically, we used the 4.1B Phi-3.5-Vision-Instruct model and the 7B Phi-4-MM model as the base models for training. For each model, we configured three different image settings corresponding to varying numbers of image 15 Phi-Ground Tech Report tokens. Specifically, the phi model family scales an image and crops it into grid-shaped square, such as 336 336 for Phi-3.5-V. By configuring the models num_crops parameter, we can adjust the maximum number of patches into which an image can be cropped. In our three image settings, we pad the images to shapes of 3 2, 5 3, and 7 4 using white padding, and set num_crops to 6, 15, and 28, respectively. Under these three configurations, the final images are divided into 7, 16, and 29 patches. This is because the model also includes default global image patch. During training, we fixed the total training budget for all models to 450 (50) NVIDIA A100-80G GPU days. Table 9 provides more detailed information on the image tokens and actual training durations for each model. In this manner, we trained six models under fixed training budget. Our aim is to understand which model design can most effectively achieve optimal performance given the available computational resources. Figure 13: Illustration of the evaluation results in relation to the training computation load. The Y-axis represents the benchmark scores in click accuracy, while the X-axis denotes the training computation per sample in TFLOPs. This training computation is estimated using the formula LOP = 6N D, where is the number of image tokens and is the number of model parameters. Figure 13 presents the training results of these six models. Based on our evaluations, the inference time generally aligns with the relationship depicted on the x-axis of the graph. Many current studies typically report only the number of parameters when discussing model performance, without emphasizing computational aspects, such as the number of image tokens. In our experiments, where the model architecture is fixed, we observe that for more advanced and challenging benchmarks like ScreenSpot-pro and UI-Vision, the impact of image tokens is significant. Specifically, when the number of image tokens is low, it may become bottleneck, resulting in the inability to perceive small objects and thus reducing the score. When the number of image tokens exceeds 2000, their impact gradually diminishes, meaning further increases in image tokens do not yield substantial marginal benefits akin to scaling laws. In some test datasets that do not require high resolution, such as ScreenSpot-V2, neither the model size nor the number of image tokens significantly affects performance. Furthermore, when the number of image tokens exceeds the benchmark requirements (as previously mentioned bottleneck), the impact on perception is very limited, as illustrated by the results in the Figure 13 with o4-mini as the planner. However, the difference between using planner and not using one is quite significant. Scaling post-training We attempted to extend the duration of DPO post-training to observe its effects. Our findings indicate that conducting long-epoch Offline DPO training directly can lead to an initial increase in model performance followed by decline, as illustrated by the gray line in Figure 14. We believe this phenomenon may be related to distribution shifting [80, 81], concept frequently discussed in previous research. Consequently, we plan to update the rollouts more frequently. Given our stringent rollout selection criteria, we opted to conduct multiple rounds of DPO to approximate the effects of Online DPO [80] efficiently. Specifically, we performed new rollout every 100 steps and initiated new round of training. Ultimately, after three rounds of DPO, we achieved the best results, as shown in Figure 14. Phi-Ground Tech Report Figure 14: Multi-turns DPO vs. Offline DPO for in-domain post-training. In the post-training phase, as previously mentioned, we increased the proportion of in-domain data, which primarily consists of Web Search and human-labeled data. After training, the results for in-domain data (our Gold dataset) improved significantly. Surprisingly, several general benchmarks, such as ScreenSpot-Pro and UI-Vision, also showed noticeable improvements. This could be partly because these benchmarks have some overlap with our target software (e.g., PPT), and partly because the distribution of desktop applications is similar to that of these benchmarks. Other benchmarks not shown here maintained their original scores or experienced slight improvements after DPO, with detailed results available in the experimental tables in Appendix B."
        },
        {
            "title": "6.1 UI Grounding Benchmark Results",
            "content": "Table 10 presents the test results of several open-source models with fewer than 10B parameters on our selected five GUI grounding test sets. The results in the upper block were obtained using the benchmarks built-in reference expressions, typically an instruction or short REs. In contrast, the lower block shows the results when we used o4-mini to generate Long REs, which were then tested by the grounding models. Table 10: The comparison of results across five GUI grounding test sets, which were tested by us, is presented. More detailed tables of results for additional open-source and closed-source models can be found in Appendix B. model SeeClick-9.6B [26] UGround-7B [30] UGround-v1-7B [30] OS-Atlas-4B [31] OS-Atlas-7B [31] UI-TARS-2B [28] UI-TARS-7B [28] UI-TARS-1.5-7B [29] Phi-Ground-4B-16C-DPO Phi-Ground-7B-16C-DPO SeeClick-9.6B UGround-7B UGround-v1-7B OS-Atlas-4B OS-Atlas-7B UI-TARS-2B UI-TARS-7B UI-TARS-1.5-7B Phi-Ground-4B-16C-DPO Phi-Ground-7B-16C-DPO ScreenSpot-V ScreenSpot-pro UI-Vision"
        },
        {
            "title": "AVG",
            "content": "basic functional spatial AVG"
        },
        {
            "title": "AVG",
            "content": "Gold-S ALL 64.6 73.2 87.1 73.5 85.5 87.2 93.0 86.9 84.4 83.3 43.5 84.6 89.8 41.9 80.6 85.5 91.5 89.9 92.8 92.6 49.7 78.3 86.1 59.6 77.2 79.7 90.2 87.6 86.4 84.8 64.4 90.0 92.2 64.6 86.2 92.2 93.1 92.7 92.7 92.6 End-to-end model setting (Use short REs) 43.9 72.7 89.4 74.5 84.0 82.8 89.4 90.0 78.1 79.3 55.1 76.1 87.7 71.9 84.1 84.7 91.6 89.0 84.1 83.8 1.1 16.5 31.1 3.7 18.9 27.7 35.7 42.6 38.0 43.2 9.4 11.5 15.4 - 12.2 - 20.1 28.8 33.4 36.8 4.7 12.2 17.1 - 11.2 - 24.3 27.5 34.3 37.1 Agent setting (Use long REs) with O4-mini as planner 43.3 88.5 91.9 57.7 88.9 90.0 92.9 92.0 92.4 93.0 53.5 89.0 92.1 57.4 84.7 90.4 93.0 92.2 92.3 93.4 5.2 25.1 30.5 - 17.2 - 33.2 35.1 43.8 44.2 5.0 22.8 29.3 - 16.6 - 33.4 35.1 42.1 43.8 1.2 23.7 32.5 2.0 21.1 35.9 40.6 48.8 51.5 55.0 2.1 2.8 6.25 - 3.67 - 8.4 10.7 5.8 7.6 3.2 11.3 13.9 - 7.5 - 16.7 17.9 21.0 20.5 5.4 8.8 12.9 - 9.0 - 17.6 22.3 24.5 27.2 4.5 19.7 24.6 - 13.8 - 27.8 29.4 35.6 36.2 24.6 46.5 57.8 15.8 41.1 59.8 66.1 67.2 58.2 62.5 19.6 62.4 66.7 18.0 45.5 66.4 69.8 71.6 73.5 73. 51.7 74.9 84.4 47.9 66.4 79.2 87.2 86.7 87.2 84.4 39.4 83.2 88.0 37.0 68.3 85.6 90.9 90.4 95.2 93.8 20.4 54.9 66.4 22.0 48.8 60.1 76.8 77.2 78.2 79.6 15.6 59.5 73.8 18.2 52.2 77.1 81.5 80.3 88.4 88.2 Phi-Ground Tech Report Our grounding model is trained specifically for the agent setting, meaning that the training dataset primarily consists of various combinations of REs. As result, our model achieves significant advantages and SOTA results across all benchmarks in the agent setting. Specifically, ScreenSpot-pro achieved an accuracy of 55.0. UI-Vision also attained result of 36.2, which is the highest for this benchmark. Furthermore, our results on the Showdown benchmark surpass those of commercial models like OpenAI Operator and Claude Computer Use (see Table 15 in the Appendix). In the end-to-end model setting, our model consistently outperforms others on the ScreenSpot-Pro, UI-Vision, and our Gold dataset, although its performance on ScreenSpot-V2 is relatively average. ScreenSpot V1 and V2 (with V2 sharing most of its data with V1) have long been crucial benchmarks for GUI grounding testing. We observed that some models perform well on ScreenSpot-V2 but do not demonstrate the same significant advantages on newly emerging benchmarks. This could be result of developers optimizing their models based on single benchmark over time. In our approach, we did not include any mobile data in the training set (as this is not our focus scenario), nor did we balance the training set with icon and text-based buttons (since the scenarios faced by product users are not balanced either). However, these techniques might significantly impact the accuracy on ScreenSpot-V1 and V2. Throughout our development process, the selection and ablation of each technique were carefully considered across multiple benchmarks. As result, our model exhibits more balanced performance and better generalization."
        },
        {
            "title": "6.2 Error Analysis",
            "content": "Figure 15: Types and Proportions of Errors on the ScreenSpot-pro Benchmark. In each image, the red rectangles represent the regions corresponding to the ground truth. Red circles indicate erroneous outputs from the previous stage, while green circles denote correct outputs from the current stage. The centers of the green circles fall within the ground truth boundaries. To avoid obstructing the image content, we have enlarged the green circles in some of the images. To analyze and illustrate the errors made by current grounding models as case study, we selected challenging benchmark, ScreenSpot-Pro, as an example. We employed the Phi-Ground-4B-DPO as the grounding model and designed cascading approach to sequentially process the benchmark data. Specifically, as shown in Figure 15, Stage involves using the benchmark-provided instructions as reference expressions (also known as short REs) for the total of 1,534 test samples from ScreenSpot-Pro. As previously mentioned, our model successfully resolved 38% of the test cases, leaving 951 incorrect samples. In Stage B, we used O4-mini as planner for the remaining 951 samples to generate long REs as input for the model. This approach further resolved 292 samples, leaving 659 samples unresolved. Across both stages, we successfully addressed 57% of the benchmark samples. In Stage C, for the remaining 659 samples, we intended to use human experts as planners to generate REs to observe the potential error rates in the planner section. However, this approach was deemed too costly. Therefore, we first had O4-mini generate REs using the Long-Gold method (see Sec. 4.2), which involves disclosing the GT bbox during RE generation. Human experts then reviewed the samples, GT, and REs generated by O4-mini to correct any errors and 18 Phi-Ground Tech Report produce the final REs for model input. This stage resolved an additional 236 samples. In Stage D, we recognized that for REs of similar quality, different styles and emphases might lead to varying results. Thus, we repeated the RE and grounding process of Stage seven times for the remaining 423 samples, resolving an additional 88 samples. Ultimately, 335 samples remained with errors largely unrelated to RE quality. We will analyze the errors and their implications in each stage in detail below. We first observe that end-to-end grounding models lack spatial reasoning capabilities, as illustrated in the example from Stage of Figure 15. When certain keywords appear in the instruction, such as \"keyword\" in Example 1 or \"screen\" in Example 2, the grounding model tends to directly highlight the locations of these words in the image. However, in Example 1, the interactive region is actually the white rectangular input box, and clicking on the text of the label might result in failed interaction. Such spatial reasoning requires degree of common sense, rather than being purely grounding task. The introduction of planner addresses this type of task effectively by directly describing \"white rectangle\" in the RE. We refer to such errors as \"planning omissions,\" which account for 19% of the total sample and 30.7% of the total errors. However, the planning of O4-mini may also encounter errors, particularly in scenarios where the target area contains multiple similar regions or when specialized application knowledge is required. In such cases, the planners hallucinations can lead to mistakes. For instance, as illustrated in Figure 15 Stage C, the markdown display button is typically located in the upper right corner. However, in the example shown, two work pages are open, causing the button to be centered. This resulted in an incorrect RE by O4-mini, leading to erroneous grounding. After manually correcting the erroneous RE, our model was able to produce the correct result. We refer to this type of error as \"planning error,\" which accounts for 15.4% of the total samples and 24.8% of the total errors. For the remaining samples, we observed that the grounding model might exhibit preference for certain style of RE, even when the quality is consistent, as evidenced by the pass@8 metric indicating new correct samples. However, this influence is minor, affecting only 5.7% of the samples and accounting for 9.3% of the errors. Additionally, we found that 117 samples (7.6% of the samples and 12.3% of the errors) were impacted because the target area or its vicinity contained languages not covered by our model, such as Chinese. Due to the strict selection of training data, primarily from the CommonCrawl dataset, which exclusively includes only English data, the model failed to correctly recognize many straightforward and easy situations due to language issues. Ultimately, 218 samples remained as particularly challenging data, making up another 14.2% of the samples and 22.9% of the errors. For the remaining errors, we provide more detailed case study in Appendix F. We categorize these errors into several identifiable types: Accuracy issues arise due to excessively extreme screen sizes and shapes. For instance, screens with an ultra-wide aspect ratio may result in output coordinates that deviate from the intended target area. Language descriptions fail to adequately constrain areas of spatial planning, such as when generating tables and instructing to click on blank cell in the 13th row and 8th column. Such specific spatial tasks present significant challenges for both the planner and the grounding model. Regions that are difficult to describe using natural language."
        },
        {
            "title": "7 Social Impacts and Open Questions",
            "content": "With the development of CUA, we have both expectations and concerns regarding this direction. Primarily, there is the issue of user privacy. During our training process, we have verified the legality of the licenses for the open-source datasets used, and ensured that licenses are valid in Bing search filtering and web filtering. However, when CUA is successfully deployed in user environments in the future, the need for grounding and planning may require screenshots of users screens to be uploaded to the cloud, potentially leading to privacy breaches. Throughout the entire research and product deployment process, we may need to establish relevant protocols, legal frameworks, or algorithms to ensure the protection of user privacy. Secondly, there is the issue of accountability for erroneous actions performed by CUAs. There are instances where CUAs might execute irreversible and harmful operations, such as closing software without saving files or even deleting important documents. At the system level, we need to explore human-computer collaboration methods that allow CUAs to efficiently replace human labor while ensuring human oversight. From the perspective of GUI grounding, we have observed that errors due to incorrect grounding can have more severe consequences. This is because trained grounding model, when making mistakes, still outputs interactively meaningful regions rather than blank areas, thus increasing the likelihood of irreversible impacts. For instance, the multiplication symbol on calculator might be mistakenly interpreted as command to close software due to similar symbols, leading to unintended software closure. Some recent studies [82, 83] have attempted to use MLLM to verify actions post-grounding, but these have shown limited 19 Phi-Ground Tech Report effectiveness and increased time costs. Developing benchmark to evaluate the potential harmfulness of GUI grounding models could also be highly beneficial."
        },
        {
            "title": "8 Conclusion",
            "content": "In conclusion, we have developed the Phi-Ground model family, which significantly enhances GUI grounding capabilities by improving the perception of interactive elements in digital interfaces. Our comprehensive empirical study identified critical factors such as data distribution, input/output formats, and computational efficiency that influence model performance. Using two-stage approach, we combined advanced MLLMs for generating detailed REs with specialized grounding model for precise coordinate output, achieving state-of-the-art results across various benchmarks, including challenging ones like ScreenSpot-pro and UI-Vision. While our models demonstrate promising results, we acknowledge the societal implications of deploying CUAs, especially regarding user privacy and error accountability. Our research not only advances GUI grounding but also offers insights applicable to other multimodal perception tasks, contributing to the development of more reliable and efficient CUAs."
        },
        {
            "title": "References",
            "content": "[1] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 2024. [2] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 2025. [3] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428, 2024. [4] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [5] Pascal Sager, Benjamin Meyer, Peng Yan, Rebekka von Wartburg-Kottler, Layan Etaiwi, Aref Enayati, Gabriel Nobel, Ahmed Abdulkadir, Benjamin Grewe, and Thilo Stadelmann. comprehensive survey of agents for computer use: Foundations, challenges, and future directions. arXiv preprint arXiv:2501.16150, 2025. [6] Mohsen Soori, Behrooz Arezoo, and Roza Dastres. Artificial intelligence, machine learning and deep learning in advanced robotics, review. Cognitive Robotics, 2023. [7] Demetris Vrontis, Michael Christofi, Vijay Pereira, Shlomo Tarba, Anna Makrides, and Eleni Trichina. Artificial intelligence, robotics, advanced technologies and human resource management: systematic review. Artificial intelligence and international HRM, 2023. [8] OpenAI. Introducing openai o3 and o4-mini, 2025. [9] Anthropic. Claude sonnet 4, 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [12] OpenAI. Operator system card, 2025. [13] Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024. [14] Michael Greenberg, Jennifer Byington, and David Harper. Mobile agents and security. IEEE Communications magazine, 1998. [15] Jose Such, Agustín Espinosa, and Ana García-Fornes. survey of privacy in multi-agent systems. The Knowledge Engineering Review, 2014. [16] Sohye Lim and Hongjin Shim. No secrets between the two of us: Privacy concerns over using ai agents. Cyberpsychology: Journal of Psychosocial Research on Cyberspace, 2022. [17] Cartrysse and JCA Van Der Lubbe. Privacy in mobile agents. In IEEE First Symposium onMulti-Agent Security and Survivability, 2004. IEEE, 2004. 20 Phi-Ground Tech Report [18] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. ICLR, 2018. [19] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. NIPS, 2023. [20] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. ICML, 2025. [21] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [22] Segev Shlomov, Aviad Sela, Ido Levy, Liane Galanti, Roy Abitbol, et al. From grounding to planning: Benchmarking bottlenecks in web agents. arXiv preprint arXiv:2409.01927, 2024. [23] Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, and Tianmin Shu. Realwebassist: benchmark for long-horizon web assistance with real-world users. arXiv preprint arXiv:2504.10445, 2025. [24] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, Tamer Özsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. [25] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [26] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [27] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. [28] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [29] ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. [30] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. ICLR, 2025. [31] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. ICLR, 2025. [32] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In ECCV. Springer, 2022. [33] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. NIPS, 2023. [34] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. ICLR, 2024. [35] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. ICLR, 2023. [36] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. [37] Parvinder Kaur, Baljit Singh Khehra, and Er Bhupinder Singh Mavi. Data augmentation for object detection: review. In 2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS). IEEE, 2021. [38] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. 21 Phi-Ground Tech Report [39] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [40] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In NIPS, 2022. [41] Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, and Yan Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis. arXiv preprint arXiv:2504.11257, 2025. [42] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, et al. careful examination of large language model performance on grade school arithmetic. NIPS, 2024. [43] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et al. Skywork: more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023. [44] General Agents Team. The showdown computer control evaluation suite, 2025. [45] Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, and Baining Guo. Visualcritic: Making lmms perceive visual quality like humans. arXiv preprint arXiv:2403.12806, 2024. [46] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, Chenyu Yi, and Alex Kot. Benchlmm: Benchmarking cross-style visual capability of large multimodal models. In ECCV. Springer, 2024. [47] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV. Springer, 2024. [48] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [49] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML. PMLR, 2022. [50] Common crawl - open repository of web crawl data, 2025. [51] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [52] Zheng Hui, Yinheng Li, Tianyi Chen, Colby Banbury, Kazuhito Koishida, et al. Winclick: Gui grounding with multimodal large language models. arXiv preprint arXiv:2503.04730, 2025. [53] OpenAI. Hello gpt-4o, 2024. [54] Samuel Lavoie, Polina Kirichenko, Mark Ibrahim, Mahmoud Assran, Andrew Gordon Wilson, Aaron Courville, and Nicolas Ballas. Modeling caption diversity in contrastive vision-language pretraining. arXiv preprint arXiv:2405.00740, 2024. [55] Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, and Chong Luo. Viarl: Adaptive temporal grounding via visual iterated amplification reinforcement learning. arXiv preprint arXiv:2505.15447, 2025. [56] Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Siddharth Garg, and Farshad Khorrami. Emma: Efficient visual alignment in multi-modal llms. arXiv preprint arXiv:2410.02080, 2024. [57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [58] Walter Gilks and Pascal Wild. Adaptive rejection sampling for gibbs sampling. Journal of the Royal Statistical Society: Series (Applied Statistics), 41(2), 1992. [59] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [60] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 22 Phi-Ground Tech Report [61] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NIPS, 2023. [62] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3), 1992. [63] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [64] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [65] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [66] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [67] Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. [68] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [69] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https: //github.com/huggingface/trl, 2020. [70] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. [71] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics. PMLR, 2024. [72] Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient exact optimization of language model alignment. In ICML, 2024. [73] Huayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems, 37:117784117812, 2024. [74] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. In International Conference on Machine Learning, pages 4225842274. PMLR, 2024. [75] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. [76] Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jarret Ross. Distributional preference alignment of llms via optimal transport. NIPS, 2024. [77] Chris Lu, Samuel Holt, Claudio Fanconi, Alex Chan, Jakob Foerster, Mihaela van der Schaar, and Robert Lange. Discovering preference optimization algorithms with and for large language models. NIPS, 2024. [78] Karel DOosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, and Shikib Mehri. Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment. ACL, 2025. [79] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [80] Biqing Qi, Pengfei Li, Fangyuan Li, Junqi Gao, Kaiyan Zhang, and Bowen Zhou. Online dpo: Online direct preference optimization with fast-slow chasing. arXiv preprint arXiv:2406.05534, 2024. [81] Yi Ren and Danica Sutherland. Learning dynamics of llm finetuning. ICLR, 2024. [82] Tiange Luo, Lajanugen Logeswaran, Justin Johnson, and Honglak Lee. Visual test-time scaling for gui agent grounding. arXiv preprint arXiv:2505.00684, 2025. [83] Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, and Insik Shin. Safeguarding mobile gui agent via logic-based action verification. arXiv preprint arXiv:2503.18492, 2025. 23 Phi-Ground Tech Report"
        },
        {
            "title": "A Experiment Settings",
            "content": "Due to resource constraints and the evolution of the development process, different ablation experiments may have utilized varying hyper-parameters and data configurations. We have documented the detailed setup for each ablation experiment in the table below. Table 11: Detailed training data configuration and hyper-parameters."
        },
        {
            "title": "Data configure",
            "content": "Tab. 2, Tab. 3 Tab. 4, Tab. 17 Tab. 5 Tab. 6 Training samples: 2M Dataset: [(SeeClick, 0.075 ep), (Fineweb, 0.062 ep), (Windows, 0.088 ep), (MaxOS, 0.77 ep), (Linux, 0.45 ep), (GUIAct, 0.44 ep)] Training samples: 5M Dataset: [(SeeClick, 0.19 ep), (Fineweb, 0.16 ep), (Windows, 0.22 ep), (MaxOS, 0.77 ep), (Linux, 0.65 ep), (GUIAct, 0.63 ep)] (BingSearch, 0.95 ep) Training samples: 5M Base: [(SeeClick, 0.19 ep), (Fineweb, 0.16 ep), (Windows, 0.23 ep), (MaxOS, 0.77 ep), (Linux, 0.65 ep), (GUIAct, 0.63 ep)] Training samples: 5M A-pretrain: [(SeeClick, 0.13 ep), (Fineweb, 0.11 ep), (Windows, 0.15 ep), (MaxOS, 0.51 ep), (Linux, 0.43 ep), (GUIAct, 0.52 ep), (BingSearch, 0.95 ep), (CC, 0.06 ep)] Phi-Ground pre-train In Table 8 and 9 Phi-Ground DPO Training samples: 400K for each round DPO: [(Human, 0.5ep), (Windows, 0.03 ep), (MaxOS, 0.14 ep), (Linux, 0.2 ep), (GUIAct, 0.2 ep), (BingSearch, 0.33 ep), (CC, 0.004 ep)]"
        },
        {
            "title": "Hyper parameters",
            "content": "lr=8e-5; batch size=2048; weight decay=0.01; random-resize=0.5 max-grad-norm=0.1; warmup-steps=50 lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 weight decay=0.01; max-grad-norm=0.1; warmup-ratio=6%; Others in Table 9 lr=1e-5; batch size=256; weight decay=0.01; max-grad-norm=0.1; warmup-steps= 24 Phi-Ground Tech Report"
        },
        {
            "title": "B Detailed Evaluation Results",
            "content": "Table 12: Detailed ScreenSpot-V2 results."
        },
        {
            "title": "Text",
            "content": "Icon/Widget Text Icon/Widget Text Icon/Widget AVG. End-to-end model setting (Use short REs)"
        },
        {
            "title": "78.4\nSeeClick\nUGround-7B*\n85.1\nUGround-v1-7B*\n83.6\n87.2\nOS-Atlas-4B\n95.2\nOS-Atlas-7B\n95.2\nUI-TARS-2B\n96.9\nUI-TARS-7B\nUI-TARS-1.5-7B*\n92.2\nPhi-Ground-4B-7C\n87.6\nPhi-Ground-4B-16C\n90.2\nPhi-Ground-4B-16C-DPO 91.7\nPhi-Ground-4B-29C\n90.7\nPhi-Ground-7B-7C\n88.1\nPhi-Ground-7B-16C\n83.9\nPhi-Ground-7B-16C-DPO 90.2\n97.2\nPhi-Ground-7B-29C",
            "content": "50.7 61.2 90.5 59.7 75.8 79.1 89.1 81.5 75.7 77.1 77.1 80.0 70.0 72.1 76.4 77.9 70.1 84.6 85.8 72.7 90.7 90.7 95.4 91.0 93.6 92.3 94.4 92.3 93.2 94.0 93.6 91.9 29.3 71.9 86.3 46.4 63.6 68.6 85.0 84.2 71.4 76.8 78.3 76.8 71.4 73.4 75.9 71.9 55.2 84.3 95.5 85.9 90.6 87.2 93.6 95.5 94.1 93.7 94.1 92.0 94.4 94.4 96.5 95.1 SeeClick* 55.7 UGround-7B* 90.2 UGround-v1-7B* 83.1 OS-Atlas-4B* 49.5 OS-Atlas-7B* 85.0 UI-TARS-2B* 91.2 UI-TARS-7B* 93.3 UI-TARS-1.5-7B* 92.8 Phi-Ground-4B-7C 92.7 Phi-Ground-4B-16C 90.2 Phi-Ground-4B-16C-DPO 91.2 Phi-Ground-4B-29C 90.2 Phi-Ground-7B-7C 90.2 Phi-Ground-7B-16C 89.6 Phi-Ground-7B-16C-DPO 92.2 Phi-Ground-7B-29C 91.2 Agent setting (Use long REs) with GPT-4O as planner 67.9 95.8 97.6 78.6 94.4 95.8 96.2 96.5 95.8 96.5 97.2 96.2 97.6 96.5 92.2 97. 24.1 72.9 84.3 48.7 68.6 77.3 78.3 75.4 75.4 72.9 73.4 74.5 74.4 73.4 73.9 75.4 43.6 91.9 85.8 67.0 84.3 90.6 91.5 91.5 94.6 92.7 92.7 92.7 92.3 93.6 93.2 93.2 28.8 68.3 92.9 30.1 66.9 77.7 83.5 80.6 76.2 80.0 80.0 79.3 80.7 81.4 80.0 80.7 SeeClick* 60.6 UGround-7B* 92.6 UGround-v1-7B* 92.6 OS-Atlas-4B* 50.5 OS-Atlas-7B* 86.0 UI-TARS-2B* 93.6 UI-TARS-7B* 93.1 UI-TARS-1.5-7B* 93.6 Phi-Ground-4B-7C 93.6 Phi-Ground-4B-16C 93.6 Phi-Ground-4B-16C-DPO 94.6 Phi-Ground-4B-29C 93.1 Phi-Ground-7B-7C 93.6 Phi-Ground-7B-16C 93.6 Phi-Ground-7B-16C-DPO 95.1 Phi-Ground-7B-29C 94.7 Agent setting (Use long REs) with O4-mini as planner 57.1 92.9 93.8 63.0 86.0 93.4 94.7 92.9 96.8 97.9 98.7 97.5 98.3 96.8 99.6 97.9 76.8 97.5 97.9 73.9 94.7 97.9 98.2 97.5 94.7 93.4 97.5 95.6 95.1 94.7 96.6 94. 26.3 76.6 86.9 33.3 75.2 77.4 89.8 86.1 84.7 88.3 90.1 91.2 89.1 90.5 90.1 86.1 51.9 82.5 86.4 55.3 77.7 86.4 87.9 87.9 87.5 88.5 87.9 87.5 87.0 88.0 88.5 88.5 25 32.5 61.1 83.2 63.1 77.3 78.3 85.2 84.5 54.3 63.5 62.0 63.5 60.6 63.5 62.0 63.5 55.3 76.4 85.6 58.7 75.5 81.7 85.0 84.1 76.0 76.9 76.9 76.4 78.8 79.3 80.0 77.9 29.5 84.0 91.0 52.4 81.8 86.5 91.0 91.0 84.0 85.9 86.0 85.0 85.9 85.0 86.4 84. 55.1 76.1 87.7 71.9 84.1 84.7 91.6 89.0 80.8 83.4 84.1 83.4 81.3 81.8 83.8 82.5 48.1 84.3 88.5 58.7 80.9 86.9 88.8 87.8 85.5 86.0 86.4 86.1 86.8 86.7 87.2 87.0 53.5 89.0 92.1 57.4 84.7 90.4 93.0 92.2 91.0 91.9 92.3 92.1 92.1 91.9 93.4 91.7 Phi-Ground Tech Report"
        },
        {
            "title": "Office",
            "content": "OS AVG."
        },
        {
            "title": "Text",
            "content": "Icon AVG. Text Icon AVG. Text Icon AVG. Text Icon AVG. Text Icon AVG. Text Icon AVG. Text Icon AVG. Table 13: ScreenSpot-Pro results."
        },
        {
            "title": "0.6\nSeeClick [26]\n7.1\nOS-Atlas-4B [31]\n16.9\nShow-UI-2B\n14.9\nCogAgent-18B\n16.2\nAria-UI\n26.6\nUGround-7B\n22.0\nClaude Computer Use [13]\n33.1\nOS-Atlas-7B\n47.4\nUI-TARS-2B\n-\nUGround-v1-7B\n58.4\nUI-TARS-7B\nUI-TARS-1.5-7B*\n58.4\nPhi-Ground-4B-7C\n32.5\nPhi-Ground-4B-16C\n59.1\nPhi-Ground-4B-16C-DPO 64.3\nPhi-Ground-4B-29C\n64.9\nPhi-Ground-7B-7C\n57.8\nPhi-Ground-7B-16C\n68.2\nPhi-Ground-7B-16C-DPO 70.8\nPhi-Ground-7B-29C\n66.9",
            "content": "SeeClick* 1.3 UGround-7B* 33.1 UGround-v1-7B* 46.8 OS-Atlas-4B* 2.6 OS-Atlas-7B* 34.5 UI-TARS-2B* 48.1 UI-TARS-7B* 56.5 UI-TARS-1.5-7B* 57.1 Phi-Ground-4B-7C 42.9 Phi-Ground-4B-16C 49.4 Phi-Ground-4B-16C-DPO 48.1 Phi-Ground-4B-29C 54.5 74.2 Phi-Ground-7B-7C Phi-Ground-7B-16C 53.2 Phi-Ground-7B-16C-DPO 55.8 Phi-Ground-7B-29C 55.8 SeeClick* 3.3 UGround-7B* 41.1 UGround-v1-7B* 62.9 OS-Atlas-4B* 3.3 OS-Atlas-7B* 41.4 UI-TARS-2B* 64.2 UI-TARS-7B* 66.2 UI-TARS-1.5-7B* 67.5 Phi-Ground-4B-7C 69.5 Phi-Ground-4B-16C 74.2 Phi-Ground-4B-16C-DPO 76.2 Phi-Ground-4B-29C 76.2 Phi-Ground-7B-7C 52.0 Phi-Ground-7B-16C 76.8 Phi-Ground-7B-16C-DPO 80.8 Phi-Ground-7B-29C 78.1 0.0 0.0 1.4 0.7 0.0 2.1 3.9 1.4 4.1 - 12.4 14.1 5.6 18.8 15.3 15.3 12.5 16.7 16.7 20.1 1.4 2.1 2.8 0.0 0.0 6.2 10.4 13.2 6.3 16.0 16.0 17.4 20.3 16.0 16.7 18.8 0.0 4.9 4.9 0.7 1.4 6.3 11.9 23.8 14.7 29.4 33.6 22.4 14.6 32.2 32.9 29.4 0.3 3.7 9.4 8.0 8.4 14.7 12.6 17.7 26.4 35.5 36.1 37.2 19.5 39.6 40.6 40.9 35.9 43.3 44.6 44. 1.3 18.1 25.5 1.3 17.5 27.9 34.2 35.9 25.2 33.2 32.6 36.6 48.0 35.2 36.9 37.9 1.7 23.5 34.7 2.0 21.6 36.1 39.8 46.3 42.9 52.4 55.4 50.0 33.9 55.1 57.5 54.4 1.0 3.0 9.1 9.6 23.7 27.3 25.9 28.8 42.9 - 50.0 57.6 26.8 48.0 46.5 48.5 39.9 53.5 56.6 57.1 1.0 33.3 37.4 2.5 29.8 42.4 49.0 49.5 33.8 49.0 52.0 50.0 53.3 52.0 54.0 56.6 0.5 34.4 42.1 2.1 30.4 51.8 55.9 59.5 44.6 59.5 62.1 63.1 41.4 66.2 66.7 67.2 0.0 1.4 0.0 0.0 2.1 2.8 3.4 2.8 6.3 - 9.1 14.7 7.0 9.1 12.6 7.7 9.8 11.2 13.3 9. 0.0 4.9 4.9 0.0 3.5 9.1 14.0 14.7 13.3 16.8 18.9 14.0 19.0 18.2 20.3 15.4 0.0 5.6 7.0 0.0 5.6 10.6 14.1 23.9 20.4 31.0 31.9 27.5 11.9 35.9 37.3 31.0 End-to-end model setting (Use short REs) 1.9 1.5 1.9 6.1 6.1 11.1 11.9 10.3 14.6 13.5 18.0 33.3 8.8 14.2 18.4 18.8 14.9 24.1 24.5 23.0 3.5 9.0 13.2 22.2 27.1 31.9 33.9 37.5 56.9 - 63.9 68.3 39.9 53.8 54.5 58.7 53.2 58.7 58.0 60.5 2.5 2.0 2.5 7.1 7.6 14.2 14.5 12.2 17.8 - 20.8 39.1 10.2 15.7 20.3 21.8 17.8 26.4 26.9 26.9 0.0 5.5 7.3 1.8 6.4 2.7 15.8 7.3 17.3 - 31.8 23.4 17.3 25.5 26.4 23.6 24.6 25.5 29.1 25. 0.0 0.0 0.0 3.1 1.6 1.6 3.7 4.7 4.7 - 9.4 15.6 4.7 9.4 12.5 9.4 6.3 17.2 17.2 10.9 2.0 7.5 10.6 13.4 18.1 19.3 25.8 24.4 39.8 38.8 50.0 49.0 30.0 41.5 42.3 43.5 40.7 44.3 45.5 47.8 0.6 2.3 5.3 5.6 14.7 17.0 16.8 17.9 27.6 27.8 32.8 39.6 18.5 31.7 32.3 31.4 27.3 35.8 38.4 37.2 Agent setting (Use long REs) with GPT-4O as planner 0.6 0.8 11.5 21.4 13.4 23.8 1.1 1.5 6.3 18.8 10.7 28.4 14.9 34.3 24.5 34.9 11.9 25.2 17.2 35.5 17.6 38.1 17.6 34.9 22.3 38.9 19.5 37.8 39.9 23.4 21.8 39.3 1.0 14.7 16.8 1.5 7.2 13.2 18.3 28.4 15.2 17.8 18.8 20.8 25.0 21.8 25.4 24.4 1.6 25.3 37.2 2.0 24.0 43.1 49.8 48.6 38.7 42.7 45.8 47.4 50.6 48.6 48.2 49. 0.0 1.6 3.1 0.0 3.3 3.1 4.7 12.5 1.6 15.6 14.1 7.8 14.1 12.5 17.2 14.1 0.0 5.5 15.5 0.0 7.4 14.5 24.5 23.6 16.4 20.9 25.5 20.9 26.7 22.7 25.5 25.5 2.8 40.6 53.8 3.5 36.6 65.0 69.2 67.8 55.9 59.4 61.5 67.8 68.3 68.5 65.7 68.5 Agent setting (Use long REs) with O4-mini as planner 0.3 1.2 11.9 22.3 17.7 27.3 1.2 1.5 10.1 19.9 16.5 34.4 21.5 38.3 37.7 44.5 18.5 34.4 26.9 47.5 28.1 49.4 25.8 48.1 16.1 29.0 30.4 53.4 54.3 31.5 35.0 51.9 0.0 3.1 6.2 0.0 8.1 9.4 10.9 20.3 4.7 18.8 15.6 14.1 7.8 17.2 18.8 18.8 1.5 14.8 21.4 2.1 10.8 18.9 25.0 43.4 23.0 29.6 32.1 29.6 18.8 34.7 35.7 40. 1.6 27.5 37.7 2.4 27.6 46.6 53.8 53.0 50.2 51.4 53.0 56.3 43.9 57.5 59.5 59.5 1.0 9.5 19.0 0.0 13.5 23.8 28.6 29.5 23.8 27.6 31.4 28.6 20.0 35.2 35.2 35.2 2.1 40.8 51.4 4.2 38.0 63.4 72.5 70.4 69.7 69.0 69.0 76.8 62.2 73.9 77.5 77.5 1.1 5.1 15.3 13.0 20.3 31.6 30.1 33.9 50.3 - 63.3 73.6 46.6 69.0 69.0 71.8 63.2 76.4 76.4 79.3 0.6 37.9 53.4 2.3 35.3 53.4 61.5 67.8 48.3 62.1 63.2 67.8 75.2 63.2 65.5 61.5 3.7 47.2 60.9 4.4 37.3 65.8 71.4 83.2 72.0 82.6 82.6 83.2 58.1 83.2 83.9 83. 0.0 3.8 7.5 0.0 1.9 11.3 16.3 5.7 17.0 - 20.8 34.7 34.0 42.0 42.0 36.0 22.0 42.0 44.0 40.0 0.0 8.0 18.0 2.0 10.4 18.0 20.0 42.0 36.0 50.0 44.0 34.0 33.3 42.0 46.0 32.0 0.0 11.1 20.0 2.2 13.3 20.0 22.2 42.2 51.1 48.9 53.3 40.0 28.0 51.1 57.8 51.1 0.9 4.8 13.5 10.0 16.1 27.0 26.9 27.4 42.6 48.8 53.5 65.0 43.8 62.9 62.9 63.8 54.0 68.8 69.2 70.5 0.4 31.2 45.5 2.2 29.8 45.5 52.2 62.1 15.5 59.4 58.9 60.3 66.0 58.4 61.2 54.9 2.9 39.3 51.9 3.9 32.0 55.8 60.7 74.3 67.5 75.2 76.2 73.8 51.3 76.2 78.2 76. 2.8 5.6 10.3 5.6 4.7 17.8 11.0 27.1 21.5 - 30.8 49.5 27.1 45.8 44.9 47.7 42.1 53.3 55.1 57.9 0.0 24.3 34.6 2.8 19.6 29.0 33.6 40.2 33.6 48.6 49.5 44.9 58.8 42.1 43.0 48.6 0.0 35.3 47.1 2.9 25.7 43.1 51.0 58.8 52.0 67.6 66.7 63.7 35.5 69.6 71.6 72.5 0.0 0.0 2.2 0.0 0.0 0.0 4.5 4.5 5.6 - 16.9 17.2 16.9 21.3 24.7 18.0 19.1 22.5 25.8 38.0 0.0 2.2 5.6 0.0 6.9 11.2 13.5 18.0 12.4 21.3 24.7 15.7 20.5 14.6 18.0 16.9 0.0 3.4 11.4 0.0 6.8 12.5 12.5 22.7 22.7 33.0 36.4 20.5 14.6 31.8 31.8 30. 1.5 3.1 6.6 3.1 2.6 9.7 8.1 16.8 14.3 26.1 24.5 35.1 22.4 34.7 35.7 34.2 31.6 39.3 41.8 19.8 0.0 14.3 21.4 1.5 13.9 20.9 24.5 30.1 24.0 36.2 38.3 31.6 41.1 29.6 31.6 34.2 0.0 20.5 30.5 1.6 16.9 28.9 33.2 42.1 38.4 51.6 52.6 43.7 26.0 52.1 53.2 53.2 1.8 5.0 10.8 12.0 17.1 25.0 23.4 28.1 39.6 - 47.8 57.5 29.8 47.5 49.0 51.3 44.6 55.2 56.4 57.8 1.1 30.4 39.7 2.5 26.7 41.2 47.5 51.4 37.3 46.6 47.8 50.1 57.3 49.4 51.1 51.7 1.9 34.6 46.3 3.1 29.8 50.2 55.8 63.0 53.3 61.9 63.1 63.8 43.9 65.8 67.6 68. 0.0 1.7 2.6 0.8 2.0 2.8 7.1 4.0 8.4 - 16.2 18.2 12.0 19.0 20.0 16.5 15.2 20.0 21.8 19.0 0.3 3.8 7.3 0.2 4.4 9.8 14.5 18.5 12.7 20.7 21.8 17.3 21.5 17.8 21.8 19.5 0.2 6.0 10.2 0.3 7.0 12.8 16.2 25.7 20.6 30.3 32.8 24.9 15.3 33.4 34.6 31.5 1.1 3.7 7.7 7.7 11.3 16.5 17.1 18.9 27.7 31.1 35.7 42.6 23.0 36.7 38.0 38.0 33.4 41.8 43.2 43.0 0.8 20.3 27.3 1.6 18.2 29.2 34.9 38.8 27.9 36.7 37.9 37.8 43.6 38.0 39.9 39.4 1.2 23.7 32.5 2.0 21.1 35.9 40.6 48.8 40.8 49.8 51.5 48.9 33.0 53.4 55.0 54. 26 Phi-Ground Tech Report Model Ed (215) Br (56) De (376) Pr (605) Cr (438) En (82) Overall (1772) Ed (215) Br (56) De (376) Pr (605) Cr (438) En (82) Overall (1772) Ed (212) Br (31) De (338) Pr (740) Cr (586) En (28) Overall Final Avg (1935) Basic Functional Spatial GPT-4o Gemini-1.5-pro Gemini-Flash-2. Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5VL-7B InternVL2-8B InternVL2.5-8B Qwen-2VL-7B MiniCPM-V-8B ShowUI-2B AriaUI-25.3B UGround-v1-7B OSAtlas-7B UGround-7B Aguvis-7B UI-TARS-7B UI-TARS-1.5-7B * CogAgent-9B SeeClick-9.6B Phi-Ground-4B-7C Phi-Ground-4B-16C 2. 0.47 0.00 3.26 6.51 0.47 0. 0.93 2.79 4.19 5.12 10.7 11. 10.7 10.2 16.7 15.4 22.1 11. 7.44 26.5 27.0 Phi-Ground-4B-16C-DPO 29.8 Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C 29.3 28.4 35.3 Phi-Ground-7B-16C-DPO 35.3 Phi-Ground-7B-29C UGround-v1-7B* OSAtlas-7B* UGround-7B* UI-TARS-7B* UI-TARS-1.5-7B* SeeClick-9.6B* Phi-Ground-4B-7C Phi-Ground-4B-16C 33.5 21.9 11.7 17. 23.3 24.9 3.72 26.0 27.0 Phi-Ground-4B-16C-DPO 26. Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C 28.4 30.7 29. Phi-Ground-7B-16C-DPO 30.7 Phi-Ground-7B-29C 31.6 UGround-v1-7B* OSAtlas-7B* UGround-7B* UI-TARS-7B* UI-TARS-1.5-7B * SeeClick-9.6B* Phi-Ground-4B-7C Phi-Ground-4B-16C 31. 16.4 23.4 30.4 30.8 5.61 35. 37.9 Phi-Ground-4B-16C-DPO 37.4 Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C 36. 38.3 36.9 Phi-Ground-7B-16C-DPO 38.8 Phi-Ground-7B-29C 38.8 0. 0.00 0.00 16.1 12.5 0.00 0. 8.93 7.14 21.4 16.1 23.2 35. 23.2 23.2 37.5 41.1 59.6 14. 23.2 42.9 50.0 48.2 53.6 53. 53.6 57.1 46.4 44.6 30.4 25. 44.6 47.3 8.93 50.0 46.4 51. 48.2 51.8 51.8 50.0 48.2 56. 32.7 41.8 54.5 61.8 10.9 60. 63.6 63.6 61.8 65.5 65.5 67. 63.6 1.86 1.60 0.27 5.32 7. 1.33 0.00 3.46 3.72 7.71 9. 13.0 19.7 13.3 14.9 22.3 21. 29.5 12.5 13.0 27.2 30.9 30. 32.3 32.8 38.9 37.1 36.3 27. 19.1 22.4 28.8 32.3 4.01 32. 33.3 33.6 33.3 33.1 37.1 38. 36.7 32.8 21.9 29.0 36.3 36. 5.46 36.3 40.2 41.8 40.7 41. 44.0 44.0 42.9 1.16 0.83 0. 6.94 11.24 1.65 0.02 2.31 3. 7.44 9.09 12.9 15.0 12.6 10. 16.2 21.2 31.8 13.7 8.43 31. 37.2 39.7 34.5 34.7 38.8 40. 34.0 21.8 14.1 17.9 27.6 29. 3.80 32.9 35.7 38.1 36.5 32. 35.2 38.0 33.4 29.4 15.8 23. 32.5 35.4 5.15 40.2 46.7 48. 45.7 40.4 46.0 47.0 43.7 1. 0.46 0.68 1.83 9.13 0.68 0. 1.37 0.68 3.65 3.42 8.22 11. 8.22 7.53 12.6 13.2 19.8 8. 5.48 18.7 24.4 24.9 22.4 23. 24.0 26.0 23.3 17.6 8.03 12. 19.4 22.1 4.34 23.7 26.5 26. 26.5 27.4 28.1 27.9 28.8 24. 11.0 19.5 26.3 27.4 3.72 27. 32.8 35.5 32.3 32.6 35.1 35. 35.3 4.88 0.00 0.00 4.88 11. 1.22 0.14 4.88 12.2 18.3 19. 20.7 18.3 22.0 19.5 26.8 39. 50.0 15.9 17.1 38.3 40.7 45. 48.1 45.7 50.5 56.8 51.9 25. 22.5 24.7 46.9 50.6 3.70 48. 53.1 56.8 48.1 51.9 54.3 61. 60.5 43.8 30.0 45.0 53.8 60. 7.50 60.0 63.8 67.5 67.5 63. 66.3 70.9 70.0 1.58 0.79 0. 5.08 9.48 1.24 0.11 2.48 3. 7.11 8.07 12.2 15.4 12.2 11. 17.8 20.1 28.8 12.0 9.42 27. 32.0 33.4 31.6 31.9 35.8 36. 33.0 22.8 14.3 18.0 26.7 29. 4.12 30.8 33.0 34.0 33.3 32. 34.6 36.2 34.0 30.5 17.2 25. 33.2 35.1 5.21 37.3 42.1 43. 41.7 40.2 43.3 44.2 42.7 Closed-Source VLMs 0.83 3.19 0. 1.79 1.79 19.6 7.14 0.27 0. 4.79 8.24 0.17 0.66 5.95 9. Open-Source VLMs 1.16 0.80 0.00 0.00 7.14 12.5 19.6 0. 3.72 3.19 6.38 0.00 2.81 3. 4.63 1.40 0.00 0.47 5.12 5. 0.47 0.00 1.40 2.79 4.19 0. 0.46 0.23 2.51 6.16 0.46 0. 1.60 0.68 2.97 Open-Source GUI Agents ( 10B) 4.11 9.31 5.12 12.5 8. 12.6 15.4 11.6 12.1 17.2 20. 24.3 11.6 4.65 27.4 28.4 32. 27.0 25.6 32.6 34.4 28.8 26. 13.1 19.5 24.7 26.5 3.72 28. 29.3 29.3 29.8 29.3 29.8 32. 29.3 32.5 17.9 25.5 35.8 36. 4.25 39.2 44.8 67.6 39.2 46. 42.3 45.1 40.4 19.6 33.9 16. 25.0 35.7 41.1 56.9 14.3 7. 15.4 22.3 11.4 15.2 21.5 25. 26.2 11.4 5.32 14.6 16.5 12. 11.2 18.0 26.5 31.2 14.7 3. Our Phi-Ground Models 26.9 46.4 34.9 48.2 51.8 46. 53.6 62.5 62.5 51.8 32.2 32. 31.6 33.0 36.7 39.1 36.2 39. 41.2 38.5 37.2 38.7 40.8 34. GPT-4O as planner 22.0 28.5 33.9 30.4 26.8 42.9 47.3 5. 42.9 41.1 41.1 44.6 50.0 51. 50.0 46.4 18.1 22.1 29.0 28. 4.52 30.9 32.7 34.0 33.5 32. 37.5 39.1 36.4 13.5 18.5 28. 31.0 3.47 32.4 37.9 39.1 36. 33.2 36.2 39.0 35.7 O4-mini as planner 27.2 32.5 44.6 30. 33.9 50.0 55.4 7.14 58.9 60. 62.5 60.7 43.6 45.5 49.1 49. 20.5 26.8 35.2 35.2 6.56 36. 38.0 39.1 40.2 40.2 42.6 44. 43.7 13.9 19.6 34.6 36.5 4. 39.6 43.5 44.6 41.6 37.6 42. 43.7 39.3 10.5 11.6 7.53 7. 13.0 16.0 19.1 8.22 4.34 21. 21.2 23.1 20.1 21.0 23.7 25. 25.8 16.9 9.40 12.6 16.9 20. 3.21 21.5 23.7 23.8 24.9 24. 24.2 25.1 26.7 23.2 11.6 17. 23.2 24.9 3.52 26.3 30.8 31. 29.1 37.6 43.4 43.0 42.3 3. 0.00 0.00 4.88 4.88 1.22 1. 6.10 6.10 11.0 15.9 22.0 19. 13.4 20.7 24.4 45.1 40.7 18. 7.32 35.8 43.2 45.7 37.0 39. 43.2 50.6 43.2 27.2 21.0 24. 37.0 39.5 3.70 42.0 48.2 45. 48.1 42.0 49.4 50.6 49.4 42. 33.3 42.5 51.2 60.0 10.0 60. 60.0 63.8 63.8 45.0 45.0 42. 40.0 1.52 0.28 0.40 5.19 7. 0.79 0.11 2.82 3.22 5.30 7. 14.0 17.1 11.2 12.2 18.3 24. 27.5 12.2 4.68 29.2 32.5 34. 31.3 31.5 34.8 37.1 33.0 23. 14.3 18.5 26.0 28.2 3.73 29. 32.8 33.4 32.8 31.2 33.8 35. 33.8 29.3 16.6 22.8 33.4 35. 5.03 37.1 40.7 42.1 39.6 39. 42.8 43.8 41.4 0.94 0.94 0. 2.83 6.60 0.47 0.00 0.94 0. 0.47 0.94 3.77 4.25 3.77 2. 5.19 6.60 8.96 3.30 0.47 6. 5.66 6.13 7.08 6.11 6.13 7. 8.02 7.08 4.74 6.13 12.3 9. 0.47 9.91 11.3 10.4 11.8 11. 12.3 12.7 11.3 11.3 6.67 10. 16.0 17.5 1.42 16.5 17.9 19. 17.5 17.9 17.0 17.0 13.7 0. 0.00 0.00 9.68 9.68 0.00 0. 3.23 3.23 3.23 9.68 9.68 6. 6.45 0.00 9.68 12.9 20.0 0. 6.45 6.45 3.23 0.00 3.23 9. 3.23 6.50 6.45 9.68 9.68 9. 9.68 9.68 3.23 12.9 16.1 12. 12.9 16.1 16.1 19.4 19.4 16. 12.9 19.4 25.8 25.8 9.68 22. 29.0 29.0 32.3 29.0 29.0 29. 29.0 1.48 0.89 0.00 5.03 7. 1.48 0.00 1.78 2.37 1.78 2. 4.44 9.76 5.62 4.14 6.51 11. 10.9 1.18 3.25 5.92 7.10 9. 4.14 5.31 7.40 8.33 7.69 13. 8.04 10.4 14.2 16.0 3.55 16. 15.7 18.3 15.7 11.2 14.8 15. 13.6 19.8 13.4 15.1 22.5 23. 3.85 25.4 24.3 26.9 26.3 27. 27.5 26.6 26.6 1.22 0.54 0. 2.43 7.43 0.00 0.14 0.68 1. 0.27 2.70 4.86 6.35 3.65 2. 4.05 9.19 11.6 4.05 1.22 4. 5.82 5.68 3.65 5.80 6.08 6. 4.86 7.31 3.52 5.28 10.8 9. 1.49 10.1 9.91 12.6 11.0 9. 10.5 11.6 10.9 12.6 5.27 10. 16.9 16.2 2.84 19.2 20.8 22. 20.0 19.5 21.0 21.5 20.5 0. 0.34 0.00 2.56 7.85 0.51 0. 0.68 0.51 0.17 0.68 2.22 4. 2.22 2.22 4.78 5.80 8.70 1. 2.73 2.90 3.58 4.27 2.56 5. 6.32 8.41 4.78 6.48 3.95 5. 5.98 9.04 1.71 8.70 8.01 7. 9.04 9.21 9.74 9.40 9.91 11. 5.87 9.76 11.6 15.2 3.08 13. 15.1 15.1 14.9 13.9 15.6 15. 14.6 3.57 0.00 0.00 7.14 10. 0.00 0.00 3.57 3.57 3.57 3. 7.14 14.3 7.14 7.14 14.3 17. 29.6 7.14 3.57 18.5 3.70 3. 7.41 14.8 3.70 3.70 0.00 14. 3.70 3.70 22.2 18.5 3.70 14. 11.1 18.5 3.70 14.8 11.1 14. 14.8 37.0 29.6 22.2 44.4 44. 11.1 44.4 29.6 40.7 37.0 44. 48.2 48.1 40.7 1.03 0.57 0. 3.15 7.60 0.51 0.05 0.98 1. 1.45 2.07 3.98 6.25 3.67 2. 5.06 8.37 10.7 2.63 2.07 4. 5.28 5.79 3.83 5.90 6.31 7. 5.63 8.17 4.68 6.41 10.2 10. 1.86 10.9 10.6 11.9 11.2 10. 11.3 11.9 11.3 13.9 7.48 11. 16.7 17.9 3.16 18.6 19.6 21. 19.7 19.5 20.6 20.5 19.5 1. 0.55 0.30 4.47 8.27 0.85 0. 2.09 2.70 4.34 5.94 10.1 12. 9.02 8.83 13.7 17.6 22.3 8. 5.39 20.5 23.3 24.5 22.2 23. 25.6 27.2 23.9 18.1 11.1 14. 21.0 22.6 3.24 23.8 25.5 26. 25.8 24.6 26.6 27.9 26.4 24. 13.8 19.7 27.8 29.4 4.47 31. 34.1 35.6 33.7 33.1 35.6 36. 34.5 Table 14: Results of UI-Vision [24]. The final column shows the overall average. Abbreviated category labels: Ed (Education), Br (Browsers), De (Development), Pr (Productivity), Cr (Creativity), En (Entertainment). The best model within each size category is highlighted in bold, and the runner-up is underlined. We tested Agent setting results of UI-Vision of several open-source GUI models ( 10B parameters). 27 Phi-Ground Tech Report Table 15: Showdown-click-dev results. : For the latency of the models we tested, we report the inference speed of the models accelerated using the vllm Python library if supported, otherwise we report the latency using huggingface transformers, marked with hf. This may be faster than the results provided by the benchmark itself, but the comparison between the models we tested remains fair. For the settings with GPT-4O and O4-mini as planners, we directly added 2.5 seconds (aligned with the original benchmark) and 8 seconds (our tested average level, which may be highly dependent on the endpoint) to the original model latency, respectively. *: Results from the original GitHub repository."
        },
        {
            "title": "Model",
            "content": "Accuracy(%) Latency(ms) Model Accuracy(%) Latency(ms) 67.15 46.50 57.81 54.40 59.96 58.17 55.11 57.45 61.04 62.48 61.22 60.68 62.84 62.59 61.04 59.25 63.02 64.39 61.93 69.60 72.12 73.51 69.96 71.94 72.12 73.87 71.40 445 1871 (hf) 209 122 212 212 401 168 313 313 2622 2712 2712 2901 2668 2813 2813 3103 8122 8212 8212 8401 8168 8313 8313 8603 End-to-end model setting (Use short REs) GPT-4O* Qwen2.5-VL-73B-Instruct* Gemini 2.0 Flash* UI-TARS-72B-SFT* Claude 3.7 Sonnet (Computer Use)* Molmo-72B-0924* Operator (OpenAI CUA)* seeclick OS-ATLAS-4B OS-ATLAS-7B UI-TARS-2B UI-TARS-7B 5.21 24.78 33.39 54.40 53.68 54.76 64.27 24.60 15.80 41.11 59.78 66.07 2500 3790 3069 1977 9656 6599 6385 847 (hf) 1288 (hf) 1788 (hf) 186 UI-TARS-1.5-7B UGround-7B UGround-v1-7B Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C seeclick OS-ATLAS-4B OS-ATLAS-7B UI-TARS-2B UI-TARS-7B UI-TARS-1.5-7B UGround-7B UGround-v1-7B seeclick OS-ATLAS-4B OS-ATLAS-7B UI-TARS-2B UI-TARS-7B UI-TARS-1.5-7B UGround-7B UGround-v1-7B Agent setting (Use long REs) with GPT-4O as planner 15.62 13.46 40.22 58.89 61.58 61.40 52.96 57.99 Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C 3347 (hf) 3788 (hf) 4288 (hf) 2687 2745 2950 4371 (hf) Agent setting (Use long REs) with O4-mini as planner 19.60 17.99 45.50 66.37 69.78 71.58 62.41 66.73 Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C 8847 (hf) 9288 (hf) 9788 (hf) 8188 8243 8454 9871 (hf) 8214 28 Phi-Ground Tech Report model"
        },
        {
            "title": "PhotoShop ClipChamp Excel",
            "content": "PowerPoint Word Windows Setting AVG. Gold-S Table 16: Gold dataset evaluation results."
        },
        {
            "title": "Gold dataset",
            "content": "SeeClick-10B [44] UGround-7B [30] UGround-v1-7B [30] OS-ATLAS-4B [31] OS-ATLAS-7B [31] UI-TARS-2B [28] UI-TARS-7B [28] UI-TARS-1.5-7B [29] Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C SeeClick-10B UGround-7B UGround-v1-7B OS-ATLAS-4B OS-ATLAS-7B UI-TARS-2B UI-TARS-7B UI-TARS-1.5-7B Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C SeeClick-10B UGround-7B UGround-v1-7B OS-ATLAS-4B OS-ATLAS-7B UI-TARS-2B UI-TARS-7B UI-TARS-1.5-7B Phi-Ground-4B-7C Phi-Ground-4B-16C Phi-Ground-4B-16C-DPO Phi-Ground-4B-29C Phi-Ground-7B-7C Phi-Ground-7B-16C Phi-Ground-7B-16C-DPO Phi-Ground-7B-29C 59.76 86.01 95.73 60.55 90.91 84.90 93.75 88.62 91.86 93.28 93.99 94.15 91.94 93.28 95.18 96.28 48.46 86.88 95.57 61.34 87.67 93.91 94.39 86.88 96.76 96.92 97.71 96.36 97.15 96.36 97.94 97.00 49.72 87.75 96.76 53.04 88.62 94.86 95.26 89.09 99.13 98.97 99.21 98.97 99.29 98.81 99.21 98. 1.41 29.11 49.30 5.63 32.86 46.95 56.34 63.85 46.01 58.22 64.79 56.81 57.28 69.95 72.30 67.61 End-to-end model setting (Use short REs) 14.53 65.36 79.33 21.23 70.39 67.60 76.54 65.92 62.57 73.18 68.72 68.72 69.83 69.83 69.27 67.60 21.50 48.60 57.01 14.95 27.10 55.14 72.90 83.18 52.34 76.64 78.50 80.37 66.36 77.57 72.90 79.44 13.41 58.54 69.51 13.41 31.71 57.32 85.37 80.49 65.85 81.71 87.80 87.80 81.71 87.8 90.24 90.24 11.63 41.86 47.67 16.28 39.53 48.84 75.58 81.40 58.14 76.74 75.58 73.26 68.60 77.91 77.91 72. Agent setting (Use long REs) with GPT-4O as planner 1.41 38.97 52.11 7.98 31.92 55.40 62.91 66.67 65.26 70.89 76.06 70.89 69.48 73.24 76.06 71.83 9.50 64.25 73.18 18.44 62.57 76.54 74.86 62.57 71.51 78.77 77.65 70.95 75.42 75.42 75.98 73.18 11.21 57.94 67.29 15.89 26.17 71.96 78.50 85.05 68.22 84.11 87.85 78.50 75.70 89.72 86.92 84.11 6.10 56.10 74.39 15.85 41.46 80.49 82.93 84.15 78.05 86.59 89.02 91.46 87.80 91.46 89.02 89.02 2.33 53.49 62.79 10.47 50.00 69.77 81.40 87.21 75.58 83.72 84.88 80.23 74.42 81.40 86.05 77. Agent setting (Use long REs) with O4-mini as planner 2.35 39.44 52.11 6.57 35.68 57.28 67.14 63.38 70.42 75.59 79.81 73.71 71.83 76.53 81.22 76.53 20.11 70.39 77.09 17.88 65.92 82.12 82.12 74.86 82.12 88.27 88.83 87.71 85.47 89.39 89.94 86.03 4.94 54.32 80.25 16.05 39.51 81.48 88.89 81.48 81.48 87.65 88.89 90.12 88.89 91.36 91.36 90.12 6.90 48.28 68.60 9.20 49.43 73.26 77.91 87.21 73.26 82.56 84.88 82.56 80.23 79.07 81.40 80.23 9.43 56.60 67.92 6.60 33.96 73.58 77.36 85.85 73.58 83.96 88.68 83.96 78.30 86.79 85.85 84. 29 20.37 54.91 66.42 22.01 48.75 60.12 76.75 77.24 62.8 76.63 78.23 76.85 72.62 79.39 79.63 78.88 13.17 59.60 70.89 21.66 49.97 74.68 79.17 78.76 75.90 83.50 85.52 81.40 80.00 84.60 85.33 82.17 15.57 59.46 73.79 18.22 52.19 77.10 81.45 80.31 80.00 86.17 88.38 86.17 84.00 86.99 88.16 85.99 51.66 74.88 84.36 47.87 66.35 79.15 87.20 86.73 76.78 85.78 87.20 83.41 85.31 84.83 84.36 84.83 32.23 81.52 84.83 38.39 67.30 87.20 85.31 88.63 88.63 91.47 91.00 89.57 88.63 90.05 91.00 88. 39.42 83.17 87.98 37.02 68.27 85.58 90.87 90.38 92.79 95.19 95.19 90.87 92.79 92.31 93.75 92.31 Phi-Ground Tech Report"
        },
        {
            "title": "C Coordinates Representations and Loss",
            "content": "As discussed in the main text, we experimented with various coordinate representations and loss function designs. We found that these techniques can accelerate training when dealing with small datasets. However, when the training dataset exceeds 1 million samples, these methods do not exhibit significant improvements. Consequently, the content presented in this section highlights approaches that failed to scale. We disclose these findings to help future researchers avoid similar pitfalls. Overall, the development of all the techniques discussed in this section stems from the following considerations: Unlike regression loss, modeling with natural language treats the difference between \"19\" and \"20\" as gap of two tokens, which should theoretically be equivalent in distance to that between \"18\" and \"19\". Furthermore, differences in the units, tens, and hundreds places should have varying impactswe might tolerate errors in the units place, but errors in the hundreds place are entirely unacceptable. These aspects highlight disparity to regression loss and our expectations. However, experimental results indicate that using the most straightforward next token prediction and expressing coordinates in natural language is well enough, and there is no significant difference in outcomes among these techniques when the batch size is extremely large and the training volume is very high. C.1 Tokenized Coordinates It has been observed that in most LLM, numbers are tokenized by digits, meaning the number 123 would be tokenized into three separate tokens: \"1\", \"2\", and \"3\". This form of representation offers limited interpretability in the context of images. In previous work, many researchers have modeled regions within an image using newly introduced special tokens. We also attempted this approach; however, when processing screenshots, the images often have extremely high resolutions, and buttons are relatively small. If we divide both the height and width into 1,000 discrete intervals and assign new token to each square region, this would add 1 million tokens to the model, which is entirely impractical. Therefore, we opted to model the coordinate values from 1 to 1,000 as new tokens, for example < 123 > for value 123, using two tokens to represent single position: < 123 >< 456 >. In this way, we added only 1000 special tokens to the model. We discussed various strategies for initializing these 1000 tokens. To better illustrate our point, we first introduce the following definition: let the function emb : Rn represent the retrieval of the embedding for specific token from pre-trained model. Then we define the following variables: Crand = random(mean = EAN ({emb(v), }), std = ST D({emb(v), })) Cdigit = random(mean = EAN ({emb(v), digit}), std = ST D({emb(v), digit})) Mdigit = 1 digit (cid:88) emb(v) vdight Ep = emb(\"point\") We then consider the following five initialization methods: The implementation in the Hugging Face Transformers library (hf) calculates the mean and variance of all pre-trained embeddings to generate random vector, which is then used to initialize all newly added tokens. We adapted the hf method by restricting the embeddings used for calculating the mean and variance to only digit tokens, naming this approach R-digit. The digit-mean method directly uses the mean of the embeddings of digit tokens from the pre-trained model to initialize larger number tokens. The term main-digit refers to using the embedding of the digit in the hundredths place as the initialization. For example, for the number 234, the initialization would use the embedding of the digit \"2.\" Prefix-learning method first freeze all parameters except the embedding of newly added tokens, and then used the learned embedding to initialize in the later training. Additionally, we handle <point>, </point> and digits 0 to 9 differently. The specific assignments can be found in Table 17, where the training results are also presented. We set the training volume for these experiments to 5 million samples and detailed setting can be found in Table 11. Regrettably, the results in Table 17 indicate that all initialization methods significantly underperform compared to directly using natural language to express coordinates. Additionally, during training, we observed slow convergence and significant fluctuations in the gradient norm. These phenomena suggest that the introduction of too many special tokens, which have not undergone large-scale pre-training, can interact adversely with the pre-trained parameters, leading to training collapse. However, when high resolution is required, 1000 special tokens become necessary. Thus, in the field of UI grounding, this technique appears to be less practical. 30 Phi-Ground Tech Report Table 17: The details and result of different initialization methods for newly added special tokens. method name hf R-digit digit-mean main-digit prefix-learning <point> </point> <p 0> <p 1> . . . <p 9> <p 10> <p 11> . . . <p 999> Gold-S"
        },
        {
            "title": "Initial value for special tokens",
            "content": "Crand Ep Ep Ep - Crand Ep Ep Ep - Crand Cdigit emb(\"0\") emb(\"0\") - Crand Cdigit emb(\"0\") emb(\"0\") -"
        },
        {
            "title": "Crand\nCdigit",
            "content": "Crand Cdigit emb(\"9\") Mdigit emb(\"0\") emb(\"0\") - - Crand Cdigit Mdigit emb(\"0\") -"
        },
        {
            "title": "Natural language",
            "content": "- - - - - - - Crand Cdigit Mdigit emb(\"9\") - - 81.7 83.2 74.6 12.6 82.3 88.9 C.2 Label Smoothing It is fairly intuitive to consider that applying label smoothing to digit tokens might produce an effect similar to that of regression loss. Specifically, for instance, when the ground truth token is the digit 5, the standard Cross-Entropy loss assigns label of 1 to digit 5 and 0 to all other tokens. However, if we apply label smoothing and assign certain degree of smoothing to digits 4 and 6, we effectively inform the model that digits 5 and 4, 6 are numerically close. This approach results in an outcome akin to regression loss. In fact, it is possible to derive how to set label smoothing parameters to achieve equivalence with using regression loss. We will next derive the formula for our label smoothing technique. First, we provide the following notations. Let = (x0, x1, ...xn) be tokenized sentence and xn is digit token whose digit value(integer number) is , is the token id of xn. We usually model the probability given by language model as p(x) = p(x0)p(x1x0) . . . p(xnx0, x1, . . . , xn1). Define as the vocabulary set, Vd as the set of digit token and = Vd Vt. Now lets recall the formulation of classical language modeling loss: The last term: Llm = (log p(x0) + log p(x1x0) + . . . + log p(xnx0, . . . , xn1)) Ln = log p(xnx0, . . . , xn1) := log pn = (cid:88) i=1 y(i) log p(i), where p(i) is the softmax result of last hidden activations and yi is the label. In general cross-entropy loss, the label is just one-hot encoding: y(i) = (cid:26)1 0 if = else In the following, we will derive how to approximate the regression loss with an appropriate designed label y(i). For Vd let the digit number of digit token id be K. We design regularized MSE loss as: LM SE = EkVd [(K )2] + ψEkVt[1], where the second term is punishment for other none-digit tokens and ψ is the punishment factor. We can calculate that: LM SE = EkVd [(K )2] + ψEkVt[1] p(k) (K )2p(k) + ψ (cid:88) (cid:88) = kVd (cid:88) kVt (K )2p(k) ψ (cid:88) p(k) + ψ (cid:88) p(k) + ψ (cid:88) p(k) kVd/{t} (cid:88) kVd/{t} kVd/{t} kVt [(K )2 ψ]p(k) + ψ(1 p(t)) = = kVd/{t} = ψ p(t) + (cid:88) [1 kVd/{t} (K )2 ψ ]p(k) 1 Phi-Ground Tech Report This will be equivalent to optimize loss: LM SE = (cid:80)V i=1 y(i)p(i), where: y(i) = 1 1 d(K,T ) ψ if = if Vd {t} Vt (1) d(K, ) is the distance function, for MSE, it should be d(K, ) = (K )2. We directly use Equation 1 for label smoothing, denoted as ˆLM SE = (cid:80)V i=1 y(i) log p(i). It is important to note that this differs from LM SE. However, by leveraging the convexity of the logarithmic function, we can easily demonstrate that ˆLM SE serves as an upper confidence bound for LM SE. From practical standpoint, the information embedded in Equation 1 suggests that the label values are larger for positions closer to the target, which is highly intuitive. Table 18: The scaling effect of label smoothing technique is very limited. d(K, ) d(K, ) = (K )2 d(K, ) = (K )2 d(K, ) = d(K, ) = no label smoothing ψ 10 30 10 30 = 50K, BS = 64 ScreenSpot-V2 Gold-S = 1M BS = 2048 ScreenSpot-V2 Gold-S 52.5 55.8 54.6 60.1 52.3 66.9 66.2 67.6 69.6 63.2 81.7 83.3 84.3 84.1 84. 85.9 88.1 87.8 87.6 88.8 The results, as shown in Table 18, indicate that this technique demonstrates clear advantage when dealing with smaller datasets and smaller batch sizes. However, when we increased the training data size to 1 million and the batch size to 2048, we observed little to no improvement, with results falling within the margin of fluctuation. This suggests that while the approach may offer some acceleration benefits in resource-constrained scenarios, it holds limited significance for large-scale training. In such cases, with larger batch sizes, the models optimization direction aligns with the regression loss, reducing the scaling advantage of this technique. C.3 Loss Re-weighting We attempt to assign different weights to the loss of different tokens. For instance, we assign higher weights to tokens in the tens place compared to those in the units place. This approach ensures that the model prioritizes the correctness of the tens place over the units place. If expressed formally using equations, it can be represented as follows: Lre = (cid:88) t=0 wt log p(xtxt1..., x0). Initially, we set wt = 1.0 when xt is not digit token. Then we consider different settings of weights for digit tokens, as shown in Table 19. Table 19: Experiment on selecting parameters for loss reweighting. weights for digit position tens units hundrads = 50K, BS = 64 ScreenSpot-V2 Gold-S = 1M BS = 2048 ScreenSpot-V2 Gold-S 1.0 2.0 4. 1.0 1.0 1.0 1.0 1.5 2.0 1.0 1.0 1.0 1/10 1/ 10 1/ ln 1/100 1/10 1/(ln 10)2 52.3 6.3 0.0 55.4 54.7 57.6 63.2 16.2 0. 66.8 63.6 64.8 84.2 15.5 0.0 84.5 83.3 84.1 88.8 22.7 0. 85.8 86.7 86.7 The first block in the table represents the control group without using reweighting techniques. We first confirm that the weight of digit tokens cannot exceed 1.0, even though these format-related tokens appear in almost all data. We found that if the weight of digit tokens is even slightly greater than 1.0, it causes the model to output in an unexpected format 32 Phi-Ground Tech Report during testing. This results in parsing errors, causing the test results to be nearly zero. In contrast, in models trained under normal conditions, the proportion of parsing errors is nearly zero. When the weight of digit tokens is proportionally adjusted to smaller than 1.0 and the model is trained accordingly, we obtain results similar to those described in Section C.2. Specifically, when training resources are extremely limited, we observe that the reweighting technique accelerates model convergence and achieves consistently better results in low-sample scenarios. However, when the data size and batch size increase significantly, the benefits of this technique are minimal or unstable. Such results are insufficient to support us to widespread this technique. The reason for this phenomenon may be that when the training volume and batch size increase, learning based on higher numeric values exhibits greater certainty and stability (or lower perplexity), which facilitates more effective learning. In contrast, learning based on lower numeric values might fluctuate due to errors present in the dataset, leading to slower learning. This process is similar to our reweighting technique, and therefore, as the training volume and batch size increase, this technique is effectively replaced. Data Pre-processing Details D.1 CommonCrawl Data Pre-processing D.1.1 Rendering Resolutions and Filtering Rendering resolutions. When rendering each web page, we initially select screen size with equal probability (1/3 chance) from the options of 1080p, 2.5k, and 4k. The corresponding screen area (Space) are 1920 1080, 2560 1440, and 3840 2160, respectively. For given screen area, we then randomly choose an aspect ratio (Rw, Rh) from the following set of aspect ratios: (cid:26) (Rw, Rh) (1 + , 2 )i = 0, 1, ..., . (cid:27) The final screen size (W, H) used for rendering can be calculated using the following formula: (W, H) = (Rw S, Rw S), = (cid:39) (cid:38)(cid:114) Space RwRh Filtering. During webpage rendering, we implemented certain filtering processes. Unlike the filtering described in Section D.1.2, this stage of filtering requires the relevant code (such as JavaScript) used in browser rendering. In contrast, the subsequent filtering refers to offline filtering conducted after the necessary information has been stored. Therefore, even though both processes involve rule-based filtering, they are described in two separate sections. During webpage rendering, we can use JavaScript to obtain interactive information, and based on this information, we have established the following filtering rules for all HTML elements. We first retain only those elements that meet any of the following conditions: Interactive Tags: The HTML tag name of the element is one of button, input, textarea, select, a, form Event Attribute: The elements have specific JavaScript methods (functions) attached, such as onclick, onmousedown, onmouseup, onmouseover, onmouseout, onkeydown Role attribute: Elements with the following role attributes are generally interactive: button, link, textbox, menuitem, option, checkbox, radio, tab, switch. Interactive class: The class name of the element is string type and the class name is one of btn, button, input, link, nav, menu, item. Is icon: Tag name is one of i, span, svg and the class name is one of fa, fas, far, fal, fab, material-icons Is image: The tag name is img Subsequently, we remove all elements that are not visible on the screen, such as those that require scrolling to be viewed. We then store all relevant information of the elements that meet the criteria, in JSON format, along with the rendered images. This includes details such as bounding box coordinates, HTML code, and various attributes. We also store layout diagram, which matches the size of the rendered screenshot. However, different types of elements are represented by different colors occupying their respective areas; for example, interactive text is marked in red, and images in cyan. This type of layout diagram allows for the expression of button positions without focusing on the content itself and is used for data deduplication at the webpage level. 33 Phi-Ground Tech Report D.1.2 Offline Rule-based Filtering Once the data has been stored, we consider the following filtering rules: Boxes deduplication. If box completely encompasses multiple other boxes, we first remove the outer box. Such boxes are typically div containers of module area, containing multiple related buttons. When one box contains another (determined by an IoU greater than certain threshold), we remove the larger box. This situation is common in web design with nested containers and errors generated by OmniParser when using it to create boxes. Remove empty boxes. In both webpage rendering and OmniParser, there are instances where certain boxes appear in blank, solid-colored area devoid of any content. For each candidate box, we crop the corresponding region from the screenshot and use the pixel standard deviation to directly determine if the area is solid-colored. If it is deemed to be solid-colored, we delete the box. Text content recognition. Sometimes, the text content within screenshot, such as long sentence, is recognized as an element. We wish to retain buttons with text but not these non-interactive content texts. To achieve this, we use the aspect ratio of the box as filtering criterion. If the aspect ratio of box exceeds certain threshold, we discard that box. D.2 Re-sampling algorithm Algorithm 3 Re-sampling algorithm Require: The center point set of training dataset = {(xi, yi)} using relative coordinates. Segmentation granularity locate (int(x//(1.0/N )), int(y//(1.0/M ))) all_box[locate].append((x, y)) N, , Sampling factor ψ. Ensure: Sampled center point set ˆC 1: all_box {(i, j) : list() {0, 1, ...N 1}, {0, 1, ...M 1}} 2: for (x, y) in do 3: 4: 5: end for 6: dist [len(v) all_box.values()] 7: dist.sort() 8: keep_number dist[int(N ψ)] 9: ˆC list() 10: for in all_box do 11: 12: 13: end for N_Sample min(len(v), keep_number) ˆC.extend(random_sample(v, N_Sample)) 34 Phi-Ground Tech Report"
        },
        {
            "title": "E Prompts",
            "content": "E.1 Reference expression generation prompt. Prompt for generating Long-Gold RE / Training data generation System prompt: User will provide you with screenshot, in which specific area will be highlighted with red rectangular box. We will also provide cropped image of the corresponding area, and (optionally) additional information related to the area to help you understand it. Your task is to generate several references regarding the target area on the original image. Specific task requirements are as follows: You will analyze and output dictionary in JSON file format. The key-value pairs included are as follows: area_type: Choose one from icon, text. These represent whether the target area is an indicative icon, text. interactive: bool, indicating whether this element in the screenshot scenario is an interactive element (e.g., clickable, inputable, etc.). If it is static text or an image, then it is not interactive. context: Generate background context describing why the current screen and area would be used. For example, if the area is the close button of image file, the context could be that the user is editing file and has completed their task at this moment. functional_reference: reference about the target area, involving the function of the target area. positional_reference: reference about the target area by describing the position of the target area, such as layout and nearby elements. appearance_reference: reference about the target area by describing the appearance of the target area. Ensure that anyone can uniquely identify this area in the screenshot through any one of the references. Dont mention red rectangular box. Your output references should only include the element description itself and follow the requirements. Do not start with \"the target element\" or \"the element\" Your output should follow this format strictly: # l A e form l o # p s { s e o and k . . . . , . . . , \" a _ e \" : \" e t \" : \" t \" : \" c n _ e c \" : . . . , \" e n _ e c \" : . . . , } . . . , . . . Phi-Ground Tech Report Model input: # Screenshot with highlight # Cropped target image Model output: # Analyze The screenshot shows PowerPoint presentation interface. The highlighted area is located on the toolbar of the PowerPoint application and is labeled as \"Shapes.\" This area is used to insert various shapes into the presentation slides, an essential feature for users looking to enhance their slides with graphical elements. The toolbar section where the \"Shapes\" button is located is common area for formatting and editing tools, making it easily accessible for users during the slide creation process. # Output { } \" n \" , e , n c e u \" a _ e \" : \" e t \" : \" t \" : e o n add p a i e c e . \" , \" c n _ e c \" : p a \" i n _ e c \" : h t e t . \" , \" e n _ e c \" : e t e s e p h \" While k on w o p e t , u i e t i s t s h t n i r i more p o \" p u n s o add i i o P t s a n . \" , \" a on t b n t Home , Layout t and h f t r g \" t s i i e and u u , Shapes e a . \" 36 Phi-Ground Tech Report E.2 Planners generation prompt for agent setting. Prompt for generating Long RE / Agent settings for evaluation System prompt: User will provide you with screenshot and short instruction related to the area to help you understand it.Your task is to classify the target area and generate several references regarding the target area on the original image. Specific task requirements are as follows: You will analyze and output dictionary in JSON file format. The key-value pairs included are as follows: context: Generate background context describing why the current screen and area would be used. For example, if the area is the close button of image file, the context could be that the user is editing file and has completed their task at this moment. functional_reference: reference about the target area, involving the function of the target area. positional_reference: reference about the target area by describing the position of the target area, such as layout and nearby elements. appearance_reference: reference about the target area by describing the appearance of the target area. Ensure that anyone can uniquely identify this area in the screenshot through anyone of the references. Ensure that the reference is complete and independent. Your output should follow this format strictly: # l A e form l o # p s { s e o and k . . . . , \" t \" : \" c n _ e c \" : . . . , \" e n _ e c \" : . . . , . . . } Model input: # Screenshot # Instruction adjust the options of \"show previous\" 37 Phi-Ground Tech Report Model output: # Analyze The screenshot displays macOS System Settings window focused on the \"Notifications\" section. The user intends to adjust the \"Show previews\" option, which is part of the Notification Center settings. This area allows users to control how notifications are displayed on their device, particularly when the screen is unlocked. The screen is divided into two main sections: the left pane with different settings categories and the right pane showing detailed options for the selected category, which is \"Notifications\" in this case. # Output { } \" The Show v s i l s e r o g n t c o t g d r e d l d on i v when i \" The r \" t \" : how v s o i i s e u c . \" , \" c n _ e c \" : t w h o i i r e r shown on i l e . \" , \" i n _ e c \" : h n s i , \" e n _ e c \" : a n When Unlocked . \" System t s window e \" The Show v s i s t below d r i Show v s h e t e e c n when o e t o i i e r o o i i . \" , \" The Show v s i s dropdown menu r s i g"
        },
        {
            "title": "F More Cases Study",
            "content": "Error grounding case study with human level reference. Case-1: Similar Icons Reference: Toggles visibility of all annotations (dimensions, notes, symbols) in the graphics area. Located in the heads-up view toolbar at the top center of the viewport, immediately to the right of the Temporary Axes button and left of the Measure button. Blue uppercase letter on light gray square background. Output: 38 Phi-Ground Tech Report Error grounding case study with human level reference. Case-2: Precision Error Reference: Convert Entities command projects selected edges or curves from the model into the active sketch as sketch entities.Located on the Quick Access Toolbar at the very top of the SOLIDWORKS window, immediately to the right of the Redo icon and before the application menu bar.Grey square button showing diagonal blue dashed line connecting two white square endpoints. Output: Error grounding case study with human level reference. Case-3: Lack spatial reasoning Reference: Grid cell for selecting 9x7 table dimension when inserting new table. Cell in the ninth column of the seventh row within the table size preview grid under the Table button in the Insert tab. Light-blue interior square outlined by white inner border and thicker blue outer border. Output: 39 Phi-Ground Tech Report Error grounding case study with human level reference. Case-4: Two same area Reference: scatter plot template used to generate an X-Y scatter diagram from worksheet data. icon located in the top section of the 2D plot palette, in the row of scatter type graphs beneath the line and bar icons; it is the second icon from the left in that row. white rectangular button with several solid black dots arranged randomly inside and the Chinese label displayed underneath Output: Error grounding case study with human level reference. Case-5: Interactive area Reference: Toggles the applied Digital Glitch effect on or off for live comparison in the Effect Controls panel. Found immediately to the left of the \"Digital Glitch\" effect name under the adjustment layers effects list in the top-left panel. Small grey lowercase \"fx\" icon on dark background matching the style of other effect-toggle buttons. Output:"
        }
    ],
    "affiliations": []
}