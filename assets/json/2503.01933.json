{
    "paper_title": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective",
    "authors": [
        "Rakshit Aralimatti",
        "Syed Abdul Gaffar Shakhadri",
        "Kruthika KR",
        "Kartik Basavaraj Angadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios."
        },
        {
            "title": "Start",
            "content": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective 5 2 0 2 3 ] . [ 1 3 3 9 1 0 . 3 0 5 2 : r Rakshit Aralimatti AI Developer SandLogic Technologies Pvt Ltd rakshit.aralimatti@sandlogic.com Syed Abdul Gaffar Shakhadri Lead AI Developer SandLogic Technologies Pvt Ltd. syed.abdul@sandlogic.com Kruthika KR AI Researcher SandLogic Technologies Pvt Ltd kruthika.kr@sandlogic.com Kartik Basavaraj Angadi AI Developer SandLogic Technologies Pvt Ltd kartik.angadi@sandlogic.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Deploying large-scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs)Shakti-100M, Shakti-250M, and Shakti-500Mwhich target these constraints head-on. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios. Keywords Shakti Small Language Model Edge Device Domain Specific Task Performance Optimization"
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed surge in large-scale language models (LLMs) such as GPT-3[1] and LLaMA[2], which exhibit remarkable language comprehension and generation abilities across tasks like summarization, question answering, and creative writing. Despite these capabilities, their deployment typically hinges on substantial computational resourcessignificant GPU clusters, vast memory requirements, and reliable network connectivitymaking them impractical for devices with limited power and storage capacity. Consequently, when applied to settings such as autonomous drones, mobile health services, or on-premise enterprise solutions, the high computational footprint and associated privacy concerns pose serious obstacles. promising direction to alleviate these challenges is Edge AI, wherein models run directly on local hardware, eliminating reliance on remote servers and improving data privacy, latency, and resilience against network failures. However, simply scaling down massive model often leads to severe compromises in language understanding and output quality. This predicament has prompted the emergence of Small Language Models (SLMs), designed from inception to operate under tight resource constraints while maintaining strong performance. Such models rely on novel architectural techniques (e.g., more memory-efficient attention mechanisms [3], [4]), quantization to reduce model precision without unduly affecting accuracy [15], and streamlined training or fine-tuning methods[5], [6], [7]. Within this landscape, the Shakti series stands out for its balanced approach to efficiency and versatility. Building on foundational insights from Shakti-2.5B [8], the new modelsShakti-100M, Shakti-250M, and Shakti-500Mdemonstrate that smaller parameter counts, when combined with robust architectural choices like Rotary Positional Embeddings (RoPE) [9] and specialized attention variants [3], [4], can rival or surpass larger models in real-world scenarios. Further, they offer quantized versions (int8, int5, int4) that minimize memory usage and increase tokens-per-second (TPS) throughput, even on devices as constrained as Raspberry Pi boards or entry-level GPUs. In parallel with these technical gains, Responsible AI has become paramount. Shakti models incorporate mechanisms to mitigate bias, handle sensitive data privately, and reduce carbon footprints through on-device inference. By pre-training on carefully curated corpora (e.g., Common Crawl [10]) and employing fine-tuning strategies such as Supervised Fine-Tuning (SFT) [5], Direct Preference Optimization (DPO) [7], and Reinforcement Learning from Human Feedback (RLHF) [6], these models align not only with user preferences but also with ethical standards in emerging AI regulations. Early evaluations on specialized taskssuch as healthcare QA, finance analytics, and legal contract analysis suggest that Shaktis carefully honed parameter sizes and domain-targeted training open up cost-effective, scalable, and privacy-preserving solutions."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Large Language Models Transformer-based architectures such as GPT-3 [1] and LLaMA [2] brought about paradigm shift in natural language generation and understanding by massively scaling parameter counts. These models excel in tasks like summarization, translation, and conversational AI. Nonetheless, their reliance on large GPU clusters and extensive memory limits their viability for edge contexts. More recent efforts like Mistral 7B [11] illustrate that parameter-efficient networks can rival larger architectures on certain benchmarks, but they still tend to be too big for tightly constrained devices like smart sensors or mobile ASICs. 2.2 Edge and On-Device AI Concurrently, Edge AI has emerged as response to the drawbacks of cloud-dependent ML deployments. Techniques for model compressionsuch as pruning, knowledge distillation, and quantizationhave been widely studied to reduce inference cost. For instance, Big Bird proposed sparse attention mechanism to handle long sequences more efficiently, while GQA [4] further refined multi-head attention for memory and computational savings. Although these methods significantly lower resource requirements, most assume the availability of at least moderate GPU capabilities or well-optimized CPU clusters. Truly resource-constrained or battery-powered scenarios necessitate even more streamlined model designs. 2.3 Small Language Models In response to these constraints, Small Language Models (SLMs) adopt an approach that prioritizes efficiency from the outset. This can involve rethinking attention patterns, replacing large embedding layers with more compact structures, and tailoring the training process to smaller parameter regimes. Some SLMs, such as SmolLM [12] or Boomer [13], [14], attempt to compress or distill knowledge from massive teachers. Others, like Shakti-2.5B [8], reimagine the internal architectureincorporating, for instance, Rotary Positional Embeddings (RoPE) [9] or advanced attention variants [3], [4] to retain crucial language capabilities despite fewer trainable parameters. Additionally, techniques like Quantization-Aware Training [15] enable these models to operate reliably at int8, int5, or int4 precision, significantly reducing memory usage and power draw. 2.4 2.4 Responsible AI and Bias Mitigation As smaller models expand into consumer devices and sensitive domains, issues of fairness, bias, and toxicity cannot be overlooked. Benchmarks such as BBQ [16] and ToxiGen [17] help detect unwanted outputs across various demographic and cultural dimensions, and CrowS-Pairs [18] measures stereotypical biases in model predictions. Addressing these concerns, SLM developers have begun incorporating Reinforcement Learning from Human Feedback (RLHF) [6] and Direct Preference Optimization (DPO) [7], alongside data-curation and alignment techniques that target harmful behaviors. By curating training sets and iteratively refining models outputs, Shakti and similar SLMs aim to maintain high usability while upholding ethical standards."
        },
        {
            "title": "3 Shakti-SLMs Architecture",
            "content": "The Shakti seriesShakti-100M, Shakti-250M, and Shakti-500Mis designed for efficient, scalable, and adaptable language modeling under edge constraints. Each variant is optimized for different computational budgets: Shakti-100M 2 (10 layers, 640 hidden dim) is suited for ultralightweight applications like IoT and mobile, Shakti-250M (16 layers, 1024 hidden dim) is ideal for domain-specific tasks in finance and healthcare, while Shakti-500M (24 layers, 2048 hidden dim) is tailored for complex multilingual and legal tasks. To optimize memory efficiency, variable grouped query attention (GQA) [4][8] is used in Shakti-100M and Shakti-250M to reduce key-value projections, whereas Block Sparse Attention [3] in Shakti-500M enables efficient long-context processing. The models integrate Rotary Positional Embeddings (RoPE) [9] for longer sequence modeling without increasing parameter counts and leverage SiLU [19] activation with Pre-Normalization to enhance training stability, particularly for smaller-scale models. For real-time inference, Sliding Window mechanism [20] inspired by Longformer reuses attention caches to efficiently process long inputs while reducing memory overhead. These design choices ensure that Shakti models maintain strong performance while minimizing computational cost, making them well-suited for deployment in resource-constrained environments."
        },
        {
            "title": "4 Training and Fine-Tuning Methodologies",
            "content": "The Shakti model seriescomprising Shakti-500M, Shakti-250M, and Shakti-100Mundergoes structured training regimen to optimize performance across various applications. This process includes foundational pre-training, supervised fine-tuning (SFT), and preference alignment through either Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO). The training methodologies of Shakti-500M, Shakti-250M, and Shakti-100M are built upon the principles outlined in the Shakti-2.5B model[8] but have been adapted to meet the resource constraints and deployment requirements of the smaller variants. 4.1 Pre-training The pre-training process is foundational phase in training Shakti models, designed to establish comprehensive understanding of language patterns, grammar, and general knowledge. This phase leverages large-scale, diverse text corpora to expose the models to varied linguistic structures, ensuring adaptability across multiple domains and contexts. Using an unsupervised token prediction approach, the models learn to predict subsequent tokens in sequences, capturing linguistic nuances, contextual relationships, and semantic depth. Shakti models, built on Transformer-based architectures like their predecessors (e.g., GPT-2 [21] and LLaMA [22]), utilize the self-attention mechanism to effectively learn dependencies between tokens, even in long sequences. While the general-purpose pre-training corpus includes sources such as Common Crawl and curated datasets, the Shakti-250M model incorporates domain-specific texts to enhance applicability in specialized fields such as healthcare, finance, and legal services. This tailored approach ensures the Shakti-250M model is better equipped to address industry-specific requirements. key innovation in the Shakti-500M model is the inclusion of quantization-aware training (QAT) [15]. This technique optimizes performance for low-resource devices by reducing memory consumption with low-bit representations while preserving model accuracy, making the model highly efficient for deployment in resource-constrained environments. By combining large-scale, diverse datasets with targeted domain-specific corpora in Shakti-250M, Shakti models achieve robust understanding of language, forming strong foundation for specialized task adaptation. This approach ensures the models generalize effectively across multiple domains while maintaining flexibility for further fine-tuning to address specific use cases. 4.2 Supervised Fine-Tuning (SFT) In the supervised fine-tuning[5] phase, all Shakti models are trained on instruction-specific and task-specific labeled datasets. This process enables the models to adapt their foundational language understanding to specialized applications, ensuring alignment with the unique requirements of domains such as conversational AI, finance, and healthcare. By leveraging labeled datasets, the models refine their outputs, improving their ability to generate accurate, contextually relevant, and domain-specific responses. 4.3 Reinforcement Learning from Human Feedback (RLHF) The Shakti-500M model employs RLHF to fine-tune outputs based on human evaluative feedback, adjusting responses to better align with human preferences regarding relevance, coherence, and accuracy.[6]. This method incorporates feedback from human evaluators to adjust the models outputs based on criteria such as relevance, coherence, and accuracy. RLHF fine-tunes the model to better align with human preferences, enabling Shakti-500M to deliver responses suitable for complex, multi-turn interactions. This makes it an ideal choice for enterprise-level applications requiring nuanced and human-like conversational abilities. 3 4.4 Direct Preference Optimization (DPO) In contrast, Shakti-250M and Shakti-100M models employ Direct Preference Optimization[7] (DPO) as computationally efficient alternative to RLHF[6]. DPO aligns these models with user preferences while minimizing computational resource demands.This approach enables these models to achieve high domain-specific accuracy and deliver quality real-time responses, making them suitable for deployment in resource-constrained environments such as mobile devices and IoT applications. Recent studies have demonstrated that DPO can fine-tune language models to align with human preferences effectively, offering simpler and more stable alternative to traditional RLHF method. This approach allows Shakti-250M and Shakti-100M to achieve high domain-specific accuracy and deliver high-quality, real-time responses. These attributes make them particularly suited for deployment in resource-constrained environments, such as mobile devices and IoT applications. Shakti models have several unique advantages that distinguish them from other models in their category. Deployment flexibility is key advantage, as Shakti models are uniquely optimized for deployment on low-resource devices, such as smartphones, IoT systems, and wearables. This differentiates them from other larger models, which require significant computational power and are less suitable for edge deployment. Additionally, the inclusion of quantization-aware training during pre-training and fine-tuning makes Shakti models more efficient for int4, int5, and int8 precision deployments, which is not feature of many other comparative models in the same parameter range. The use of Direct Preference Optimization (DPO) allows Shakti-250M and Shakti-100M to achieve alignment with human preferences at significantly lower computational cost compared to models relying solely on RLHF. This enables real-time application capabilities without sacrificing quality."
        },
        {
            "title": "5 Dataset Details",
            "content": "The Shakti-SLM models are trained on extensive and diverse datasets sourced from wide range of text-rich materials. These datasets provide broad foundation for language understanding, domain expertise, and adaptability across multiple tasks. The selection and curation of these datasets play crucial role in ensuring high-quality learning, with detailed dataset information available in the corresponding table. Shakti-100M: The foundational training for Shakti-100M utilizes large-scale dataset from diverse text-rich sources, providing broad language understanding and domain knowledge. The supervised fine-tuning (SFT) phase refines the model for instructional and conversational tasks, enhancing its ability to follow instructions, generate accurate responses, and assist in specific applications. The DPO stage aligns the models responses with human preferences, improving output quality and relevance. detailed list of datasets used for training is available in the corresponding table. Shakti-250M: Shakti-250M is trained on an extensive dataset to establish foundational language knowledge and domain-specific expertise. Pre-training includes general and domain-specific corpora, enabling proficiency in fields like finance and legal applications. The supervised fine-tuning (SFT) phase refines the model for instruction-following and specialized tasks in healthcare, finance, and legal contexts. The DPO stage further fine-tunes outputs to align with preferred behaviors in these domains. comprehensive list of datasets used in each stage is provided in the corresponding table. Shakti-500M: The Shakti-500M model undergoes pre-training on diverse corpora to develop general language understanding and knowledge across various domains. The supervised fine-tuning (SFT) phase adapts the model for instruction-based applications, enhancing problem-solving, conversational AI, and coding capabilities. RLHF further refines responses through human feedback, ensuring contextual relevance and accuracy. complete list of datasets used in different training stages is available in the corresponding table. All the datasets used across different training stages for all Shakti models are mentioned in Table 1."
        },
        {
            "title": "6 Evaluation and Competitive Study",
            "content": "6.1 Comparative Performance Analysis In this section, we compare the Shakti series models (Shakti-100M, Shakti-250M, and Shakti-500M) against other leading models in the same or larger parameter range, based on academic benchmark results across variety of tasks. The comparison provides insights into the relative performance, efficiency, and suitability of the Shakti models for various real-world applications. The performance of the Shakti series models was evaluated on standard benchmarks to ensure consistency and fairness. For comparison models, results from available benchmarks were used, and for those not available, evaluations were conducted by us. 4 Shakti-100M Shakti-250M Shakti-500M Common Crawl [10] Fineweb-EDU-Dedup [23] TxT360 [26] Fineweb-EDU-Dedup [23] AIR-Bench/qa finance [24] Common Crawl [10] Vidhaan/LegalCitationWorthiness [25] Cosmopedia v2 [27] lavita/medical-qa-datasets [33] The Thome [40] Magma-Pro-300K-Filtered-H ruslannmv/ai-medical-chatbot Infinity-instruct [41] [28] [34] OpenHermes-2.5-H4 [29] axion/pmc llama instructions Self-oss-instruct-sc2-H4 [30] Everyday-conversationsllama3.1-2k [31] Instruct-data-basics-smolim-H4 [32] [35] windupdate/reddit finance 43 250k [36] Marina-C/question-answerSubject-Finance-instruct [37] isacus/open-australian-legal-qa [38] mb7419/legal-advice-reddit [39] UltraFeedback Binarized [42] NickyNicky/nano finance 200k[43] Dhananjay22/legal-dpo [44] PreTraining SFT DPO RLHF Table 1: Training datasets used for different Shakti model sizes. UltraFeedback Binarized [42] 6.1.1 Popular Benchmark and Result Analysis for Shakti-100M The Shakti-100M model demonstrates strong benchmark performance, as illustrated in Figure 1,despite being significantly smaller than many competing models. It consistently matches or outperforms larger models in key evaluations, highlighting the effectiveness of its optimized training process on carefully curated dataset. This approach enables the model to extract intricate patterns and generate accurate predictions, proving that size alone is not the sole determinant of performance. critical factor in Shakti-100Ms success is the balanced size of its pre-training dataset. Models trained on datasets that are either too large or too small often struggle to achieve optimal results. With 1T token pre-training dataset, Shakti-100M maintains this balance, delivering strong performance across diverse tasks. These results emphasize the importance of strategic data selection and curation in achieving high accuracy and efficiency in language models. 6.1.2 Popular Benchmark and Result Analysis for Shakti-250M The Shakti-250M model demonstrates outstanding efficiency and performance, competing effectively against larger models, as shown in Figure 2, such as Boomer-1B [13] and Llama 3.2 1B [46]. Despite its smaller size and more limited training dataset, it achieves impressive results across various NLP tasks. This strong performance highlights its ability to handle diverse language challenges while maintaining computational efficiency. key factor behind Shakti-250Ms success is its optimized training process, which leverages clean and well-curated datasets. This approach ensures that the model captures essential linguistic patterns and nuances, enabling high accuracy even with fewer pre-training tokens. While larger models may excel in specific scenarios, Shakti-250M strikes an optimal balance between model size, efficiency, and accuracy. 5 Figure 1: Comparison results on academic benchmarks for Shakti-100M, Boomer-634M[14], SmolLM-135M[12], SmolLM-360M[12], and AMD-Llama-135M[45], which are in the same parameter range. 6.1.3 Popular Benchmark and Result Analysis for Shakti-500M The Shakti-500M model delivers exceptional performance, as illustrated in Figure 3, across various NLP tasks, competing effectively with both similar-sized and larger models. Its strong results stem from well-balanced and carefully curated training dataset, allowing it to maximize the efficiency of its optimized architecture. Despite its relatively smaller size, the model consistently achieves competitive benchmark scores, demonstrating its ability to handle diverse language challenges effectively. key contributor to Shakti-500Ms success is its emphasis on data quality and architecture optimization. By leveraging thoughtfully curated dataset, it achieves high accuracy without relying on excessive model parameters. While larger models may have advantages in specific areas, Shakti-500M maintains an optimal balance between size and efficiency. 6 Figure 2: Comparison results on academic benchmarks for Shakti-250M, Boomer-1B[13], Boomer-634M[14], Qwen2.50.5B[47], SmolLM-360M[12], and Llama 3.2 1B[46]. 6.2 Domain Specific Performance Analysis Shakti-250M, tailored with domain-specific training on Finance, Legal, and Healthcare datasets, showcases its specialized capabilities. This section highlights its performance on domain-specific benchmarks and prompt-based evaluation for each domain, emphasizing its efficiency in handling specialized tasks compared to other models. 6.2.1 Domain Specific Benchmark Result Shakti-250M demonstrates exceptional performance in the healthcare and finance domains, as summarized in Figure 4, making it versatile model for domain-specific applications. In healthcare, it excels in tasks requiring complex medical reasoning and shows strong capabilities in understanding and applying clinical knowledge, outperforming expectations of its size. The models compact size and efficiency make it an excellent choice for edge devices and IoT deployment in both healthcare and finance applications. Its ability to deliver reliable and accurate insights under resource-constrained environments opens possibilities for real-time medical assistance, remote diagnostics, on-device health monitoring, financial forecasting, and decision-making tools. 6.2.2 Prompt-Based Evaluation Table 2 presents the performance of the Shakti-250M model across healthcare, finance, and legal domains. Answer Relevancy: The Shakti-250M model demonstrates strong domain adaptability, achieving Answer Relevancy Scores of 0.85 (healthcare), 0.86 (finance), and 0.81 (legal), showcasing its ability to generate contextually relevant responses. Summarization Score: In the legal domain, the model attains summarization score of 0.86, reflecting its capability to generate concise summaries with moderate fidelity and coverage. 7 Figure 3: Comparison results on academic benchmarks for Shakti-500M, Boomer-1B[13], Boomer-634M[14], Qwen2.50.5B[47], and Llama 3.2 1B[46]. Factual Accuracy: In the finance domain, Shakti-250M achieves an average factual accuracy score of 0.83, indicating its ability to extract essential information while leaving some room for improvement in precision and detail. Domain Average Answer Relevancy Score Summarization Score Factual Score HealthCare Legal Finance 0.85 0.81 0.86 - 0. - - - 0.83 Table 2: Average Answer Relevancy score of Shakti-250M model across domains as mentioned, Summarization score for Shakti-250M model for Legal domain, Average Factual Score for Finance domain.Answer Relevancy: Answer relevancy measures the degree to which the model-generated response aligns with the expected or correct answer, reflecting its accuracy and contextual relevance.Summarization Score: Summarization Score calculates the alignment and coverage of the summary generated for the input.Factual Score: Factual Score evaluates the correctness of factual information in the models output, measuring how well it captures and reproduces essential details from the input. score near to 1 indicates better performance of the model in the respective task. Shakti-SLMs Multilingual Capabilities The Shakti models are designed with robust multilingual capabilities, enabling them to cater to wide range of linguistic contexts and applications. This is achieved through specialized tokenizer that supports multiple languages, ensuring efficient representation and processing of diverse linguistic structures. The models can be fine-tuned or aligned with data from various languages, including Indian languages such as Kannada, Hindi, Telugu, and Tamil, as well as widely spoken global languages like Spanish, French, and German. This flexibility makes the Shakti series particularly valuable 8 Figure 4: Comparison results on medical and finance domain benchmarks for Shakti-250M, Phi-1.5-1.3B[48], Gemma2B[49], and Opt-2.7B[50] models, specifically for the Medical domain. in multilingual environments, where seamless language adaptation is crucial for effective communication and user engagement. By supporting such broad linguistic spectrum, the Shakti models democratize access to AI-powered solutions across different regions, breaking language barriers and fostering inclusivity."
        },
        {
            "title": "8 Quantization",
            "content": "Quantization reduces model weight precision from FP32 to lower-bit formats (int4, int5, int8), significantly improving memory efficiency and inference speed while maintaining accuracy. This optimization enables Shakti-100M, Shakti250M, and Shakti-500M models to run efficiently on resource-constrained hardware, including mobile devices, IoT systems, and drones. 8.1 Quantization Techniques We apply advanced quantization techniques that balance performance and accuracy: Block-wise quantization with scaling factors: Converting weights into 4-bit (Q4 0, Q4 1), 5-bit (Q5 0, Q5 1), and 8-bit (Q8 0) formats. Precision enhancement: Assigning individual scaling factors to weight blocks. Memory mapping (mmap): Minimizing RAM usage by directly accessing weights from disk. CPU-specific optimizations: Accelerating inference using AVX2, ARM NEON, and other architecturespecific instructions. 9 8.2 Model Size Optimization Quantization significantly reduces model size, enabling deployment on resource-constrained devices. Figure 5 illustrates the memory footprint reduction across our model variants. Figure 5: Model size comparison before and after quantization. FP32 represents the original model size, while Q8, Q5, and Q4 represent increasingly aggressive quantization levels. Note the substantial reduction in memory footprint, with Q4 models requiring approximately 8x less memory than their FP32 counterparts. 8.3 Performance Across Hardware Platforms Our quantized Shakti models demonstrate strong performance across various hardware platforms, from high-end GPUs to edge devices. Comprehensive testing across multiple device categories reveals superior throughput compared to similar-sized competitor models. Figure 6 illustrates the performace of different quantized models across different hardware platform. 8.3.1 High-Performance Hardware NVIDIA L40s GPU (Linux-based VM with AMD EPYC 7R13, 40 GB RAM): Shakti-500-Q4 delivers 583.88 tokens per second (TPS), outperforming SmolLM2-360M-Q4 (281.98 TPS) Intel Xeon Platinum 8488C CPU (8 cores, 15 GB RAM): Shakti-500-Q4 achieves 72.02 TPS, surpassing Qwen2.5-0.5B-Q4 (45.89 TPS) Apple MacBook Pro (M3 Max, 36 GB RAM, macOS): Shakti-250-Q4 reaches 385.00 TPS, demonstrating efficiency for general-purpose computing 8.3.2 Resource-Constrained Devices Raspberry Pi 5 (ARM Cortex-A76, 8 GB RAM, Raspberry Pi OS): Shakti-500-Q4 achieves 29.54 TPS, outperforming SmolLM2-360M-Q4 (28.99 TPS) iPhone 14 (A15 Bionic, 6 GB RAM, iOS 18): Shakti-500-Q4 delivers 62.4 TPS, while Shakti-100-Q4 reaches 153.7 TPS The performance data illustrates that Shakti models are particularly well-suited for deployment in resource-constrained environments while maintaining competitive performance on high-end hardware. This versatility enables real-time AI applications across wide spectrum of devices, from data centers to edge computing."
        },
        {
            "title": "9 Responsible AI",
            "content": "The Shakti models embody strong commitment to Responsible AI principles, addressing key aspects such as fairness, transparency, and environmental sustainability. By leveraging on-device processing, these models prioritize user data 10 Figure 6: Performance comparison (tokens per second) of Shakti models across different hardware platforms. The graph demonstrates how our models maintain high throughput even on resource-constrained devices compared to similar-sized competitors. privacy, minimizing the risk of exposure to security vulnerabilities inherent in cloud-based systems. The adoption of quantization techniques further reduces the carbon footprint associated with model deployment, aligning with global sustainability goals. Additionally, deliberate efforts to mitigate biases during training enhance the trustworthiness of the models across diverse applications. The Shakti series fosters equitable access to advanced technology through decentralized AI solutions, promoting inclusivity and upholding ethical AI practices. 9.1 Benchmarking on Responsible AI Datasets Bias Benchmark for QA (BBQ) [16]: Shakti-500M achieved 54.08% accuracy in measuring biases across gender, race, and religion, outperforming Shakti-250M which is 50.2%. ToxiGen [17]: On this dataset detecting toxicity across 13 minority groups, Shakti-500M scored 51.5% accuracy compared to Shakti-250Ms 47.5%, showing improved ability to differentiate toxic content. Implicit Hate Speech Dataset [51]: Shakti-500M reached 69.04% accuracy in identifying subtle hate speech, significantly higher than Shakti-250M which is 63%. CrowS-Pairs [18]: For bias evaluation across age, disability, gender, and race, Shakti-500M achieved lower likelihood difference of 3.02 and 51.9% stereotype percentage versus 3.11 and 52.07% for Shakti-250M, indicating reduced bias. Results summarized in Tables 3 and 4 demonstrate Shakti models capabilities in bias mitigation, toxicity detection, and fairness enhancement."
        },
        {
            "title": "10 Conclusion",
            "content": "The Shakti series of small language models represents paradigm shift in delivering efficient, secure, and highperformance AI solutions tailored for resource-constrained environments. Designed to address the limitations of large language models, Shakti modelsspanning 100M, 250M, and 500M parametersdemonstrate the potential of small language models in enabling real-time, privacy-preserving computation for edge deployment. Leveraging advanced 11 Dataset Accuracy of Shakti-250M Accuracy of Shakti-500M BBQ Average Toxigen ImplicitHate 50.2 47.5 63 54.08 51. 69.04 Table 3: Accuracy scores of the Shakti models on Bias Benchmark for QA (BBQ), ToxiGen, and Implicit Hate Speech datasets. Higher accuracy indicates the models improved ability to mitigate biases, detect nuanced toxicity, and accurately classify implicit hate speech, showcasing alignment with Responsible AI principles. Model likelyhood diff pct stereotypes Shakti-250M Shakti-500M 3.11 3.02 52.07 51.9 Table 4: Evaluation of the Shakti models on the Crows-Pairs dataset using Likelihood Difference and Percentage of Stereotypes metrics. Lower values in Likelihood Difference indicate reduced preference for stereotypical over non-stereotypical sentences, while lower Percentage of Stereotypes signifies the models fairness and ability to minimize bias.. quantization techniques such as int8, int5, and int4, these models minimize memory usage and maximize throughput, achieving exceptional token-per-second performance across diverse hardware platforms, including mobile phones, IoT devices, and GPUs. This makes them ideal for applications requiring low latency and high efficiency, ensuring scalability without compromising accuracy. Pre-training on diverse, large-scale text corpora equips Shakti models with deep understanding of linguistic patterns, semantic relationships, and contextual nuances, enabling them to generalize effectively across wide range of tasks. Fine-tuning techniques, including Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), further refine these models, aligning their outputs with specific domains and tasks. This combination of foundational pretraining and targeted fine-tuning enables the Shakti models to deliver performance comparable to larger language models with significantly fewer parameters, making them ideal for resource-constrained environments. These advantages are consistently reflected in benchmark results, where the models excel in domains such as healthcare, legal, and finance, delivering superior reasoning, factual reliability, and efficiency. The robust architecture of Shakti models incorporates Rotary Positional Embeddings, Variable Grouped Query Attention, and Block Sparse Attention, ensuring low latency and computational efficiency. These architectural innovations optimize memory usage and enable scalability across wide range of hardware platforms. The models are also designed to support quantization (int8, int5, and int4), allowing them to achieve high tokens-per-second throughput on devices ranging from mobile phones to GPUs. This quantization-ready design enhances their compatibility with edge devices while maintaining accuracy and performance. Together, the advanced architecture and efficient training methodologies make Shakti models benchmark for deploying high-performing AI in resource-constrained environments. Each model in the Shakti series is optimized for specific use cases, showcasing their versatility and adaptability. Shakti-100M, lightweight, general-purpose model, is tailored for ultra-low-resource devices like smartwatches, consumer electronics, and IoT systems. It excels in tasks such as text summarization, chatbot functionalities, and context-aware assistants, making it indispensable for devices with limited computational resources. Shakti-250M is specifically designed for domain-specific applications in healthcare, legal, and finance sectors, with its ability to operate securely on-premise ensuring data privacy and preventing information leakage. This model is particularly adept at specialized tasks such as patient diagnostics, contract analysis, and financial advising, thanks to its domain-specific fine-tuning. Shakti-500M, the most advanced model in the series, balances general-purpose functionality with enhanced capabilities for complex tasks. With support for multilingual processing and long-context understanding, it is ideal for applications such as customer support chatbots, virtual assistants, and content creation, with deployment potential spanning industries like e-commerce, enterprise communication, and media. By adhering to Responsible AI principles, Shakti models emphasize fairness, trustworthiness, and accountability. Rigorous data filtering ensures unbiased outputs, while on-device processing enhances privacy and aligns with global sustainability goals by reducing reliance on energy-intensive cloud infrastructures. Use cases across industries highlight the practical impact of Shakti models, from delivering real-time insights in healthcare to powering smart assistants in IoT devices. Their ability to operate securely and efficiently underpins their growing significance in sensitive workflows and privacy-critical environments. In conclusion, the Shakti series exemplifies the future of edge AI, bridging the gap between state-of-the-art performance and practical deployment. With innovative architecture, efficient quantization, and specialized fine-tuning, these models redefine the capabilities of small language models, making them scalable, privacy-centric, and inclusive. By democratizing access to AI and addressing real-world challenges, the Shakti series sets new benchmark for sustainable, impactful, and responsible AI deployment across industries."
        },
        {
            "title": "Future Scope",
            "content": "Future developments in the Shakti series aim to enhance multilingual capabilities, particularly in underrepresented languages, to further democratize AI accessibility. Additionally, exploring more efficient training methodologies, such as adaptive pre-training and task-specific finetuning, can further optimize resource consumption. Expanding support for edge computing scenarios, such as integrating Shakti models with federated learning frameworks, could provide robust solutions for collaborative and secure AI deployments. Finally, incorporating advanced feedback mechanisms, such as continuous learning from real-world usage, will improve model alignment and responsiveness in dynamic application settings."
        },
        {
            "title": "References",
            "content": "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [3] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2020. [4] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. [5] Ph.D. Cameron R. Wolfe. Understanding and using supervised fine-tuning (sft) for language models. https: //cameronrwolfe.substack.com/p/understanding-and-using-supervised, 2023. [6] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [7] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [8] Syed Abdul Gaffar Shakhadri, Kruthika KR, and Rakshit Aralimatti. Shakti: 2.5 billion parameter small language model optimized for edge ai and low-resource environments, 2024. [9] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. [10] Common Crawl. https://commoncrawl.org/. Accessed: 2024-12-31. [11] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. [12] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Smollm - blazingly fast and remarkably powerful, 2024. Accessed: 2024-12-31. [13] Bud Ecosystem. Boomer-1b. https://huggingface.co/budecosystem/boomer-1b. Accessed: 2024-12-31. 13 [14] Bud Ecosystem. Boomer-634m. https://huggingface.co/budecosystem/boomer-634m. Accessed: 202412-31. [15] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models, 2024. [16] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. Bbq: hand-built bias benchmark for question answering, 2022. [17] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection, 2022. [18] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: challenge dataset for measuring social biases in masked language models, 2020. [19] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, 2017. [20] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. [21] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI, 2018. [22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [23] Skymizer AI Team. Fineweb-edu-dedup 45b. https://huggingface.co/datasets/skymizer/ fineweb-edu-dedup-45B, 2024. Accessed: 2024-12-31. [24] AIR-Bench Team. Qa finance en. https://huggingface.co/datasets/AIR-Bench/qa_finance_en, 2024. Accessed: 2024-12-31. [25] Vidhaan Team. Legalcitationworthiness. https://huggingface.co/datasets/Vidhaan/ LegalCitationWorthiness, 2024. Accessed: 2024-12-31. [26] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: top-quality llm pre-training dataset requires the perfect blend, 2024. [27] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia. https://huggingface.co/datasets/HuggingFaceTB/cosmopedia, February 2024. [28] HuggingFaceTB Team. Magpie-pro-300k-filtered-h4. https://huggingface.co/datasets/ HuggingFaceTB/Magpie-Pro-300K-Filtered-H4, 2024. Accessed: 2024-12-31. [29] HuggingFaceTB Team. Openhermes-2.5-h4. https://huggingface.co/datasets/HuggingFaceTB/ OpenHermes-2.5-H4, 2024. Accessed: 2024-12-31. [30] HuggingFaceTB Team. Starcoder2-self-instruct-oss-50k. https://huggingface.co/datasets/ HuggingFaceTB/self-oss-instruct-sc2-H4, 2024. Accessed: 2024-12-31. [31] HuggingFaceTB Team. Everyday conversations llama 3.1-2k. https://huggingface.co/datasets/ HuggingFaceTB/everyday-conversations-llama3.1-2k, 2024. Accessed: 2024-12-31. Instruct-data-basics-smollm-h4. [32] HuggingFaceTB Team. https://huggingface.co/datasets/ HuggingFaceTB/instruct-data-basics-smollm-H4, 2024. Accessed: 2024-12-31. [33] Lavita Team. Medical qa datasets. https://huggingface.co/datasets/lavita/medical-qa-datasets, 2024. Accessed: 2024-12-31. [34] Ruslanmv Team. Ai medical chatbot. https://huggingface.co/datasets/ruslanmv/ ai-medical-chatbot, 2024. Accessed: 2024-12-31. [35] Axiong Team. Pmc llama instructions. https://huggingface.co/datasets/axiong/pmc_llama_ instructions, 2024. Accessed: 2024-12-31. [36] Winddude Team. Reddit finance dataset. https://huggingface.co/datasets/winddude/reddit_ finance_43_250k, 2024. Accessed: 2024-12-31. [37] Marina-C Team. Question-answer subject finance. https://huggingface.co/datasets/Marina-C/ question-answer-Subject-Finance-Instruct, 2024. Accessed: 2024-12-31. [38] Umar Butler. Open australian legal qa. https://huggingface.co/datasets/umarbutler/ open-australian-legal-qa, 2023. Accessed: 2024-12-31. 14 [39] MB7419 Team."
        },
        {
            "title": "Legal",
            "content": "advice reddit. https://huggingface.co/datasets/mb7419/ legal-advice-reddit, 2024. Accessed: 2024-12-31. [40] Arcee-AI Team. The tome. https://huggingface.co/datasets/arcee-ai/The-Tome, 2024. Accessed: 2024-12-31. [41] BAAI Team. Infinity-instruct. https://huggingface.co/datasets/BAAI/Infinity-Instruct, 2024. Accessed: 2024-12-31. [42] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023. [43] NickyNicky Team. Nano finance 200k en/es chatml gemma/orpo/dpo. https://huggingface.co/datasets/ NickyNicky/nano_finance_200k_en_es_chatML_gemma_orpo_dpo, 2024. Accessed: 2024-12-31. [44] Dhananjayg22 Team. Legal dpo. https://huggingface.co/datasets/Dhananjayg22/legal-dpo, 2024. Accessed: 2024-12-31. [45] AMD. Amd-llama-135m. https://huggingface.co/amd/AMD-Llama-135m. Accessed: 2024-12-31. [46] Meta. Llama-3.2-1b. https://huggingface.co/meta-llama/Llama-3.2-1B. Accessed: 2024-12-31. [47] Qwen Team. Qwen2.5: party of foundation models. https://qwenlm.github.io/blog/qwen2.5/, September 2024. Accessed: 2024-12-31. [48] Microsoft. Phi-1 5. https://huggingface.co/microsoft/phi-1_5. Accessed: 2024-12-31. [49] Google. Gemma-2b. https://huggingface.co/google/gemma-2b. Accessed: 2024-12-31. [50] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. [51] Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: benchmark for understanding implicit hate speech. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 345363, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics."
        }
    ],
    "affiliations": [
        "SandLogic Technologies Pvt Ltd"
    ]
}