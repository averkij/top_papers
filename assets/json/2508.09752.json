{
    "paper_title": "$μ$-Parametrization for Mixture of Experts",
    "authors": [
        "Jan Małaśnicki",
        "Kamil Ciebiera",
        "Mateusz Boruń",
        "Maciej Pióro",
        "Jan Ludziejewski",
        "Maciej Stefaniak",
        "Michał Krutul",
        "Sebastian Jaszczur",
        "Marek Cygan",
        "Kamil Adamczewski",
        "Jakub Krajewski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have seen a growing interest and adoption of LLMs, with $\\mu$Transfer becoming a key technique for tuning hyperparameters in large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a leading architecture in extremely large models. However, the intersection of these two advancements has remained unexplored. In this work, we derive a $\\mu$-Parameterization ($\\mu$P) for MoE, providing theoretical guarantees for feature learning across model widths in both the router and experts. We empirically validate our parameterization and further investigate how scaling the number of experts and granularity affects the optimal learning rate."
        },
        {
            "title": "Start",
            "content": "µ-Parametrization for Mixture of Experts Jan Małasnicki * 1 Kamil Ciebiera * 1 Mateusz Boru 1 2 Maciej Pioro 3 4 Jan Ludziejewski 1 Maciej Stefaniak 1 Michał Krutul 1 3 Sebastian Jaszczur 1 3 Marek Cygan 1 5 Kamil Adamczewski 3 6 Jakub Krajewski * 1 3 5 2 0 2 3 1 ] . [ 1 2 5 7 9 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent years have seen growing interest and adoption of LLMs, with µTransfer becoming key technique for tuning hyperparameters in large-scale training. Meanwhile, Mixture-ofExperts (MoE) has emerged as leading architecture in extremely large models. However, the intersection of these two advancements has remained unexplored. In this work, we derive µParameterization (µP) for MoE, providing theoretical guarantees for feature learning across model widths in both the router and experts. We empirically validate our parameterization and further investigate how scaling the number of experts and granularity affects the optimal learning rate. 1. Introduction Scaling deep learning models has become fundamental driver of progress in modern AI (Fedus et al., 2022; Touvron et al., 2023; OpenAI, 2025). Nevertheless, efficiently tuning hyperparameters across different model sizes remains major challenge, often requiring extensive trial-and-error or computationally expensive grid searches (Feurer et al., 2015). The µParameterization (µP) (Yang, 2021) offers principled solution by enabling stable and predictable training dynamics across model widthswithout the need to retune learning rates or initialization schemes. By reparameterizing models to preserve feature learning in the infinitewidth limit, µP makes it possible to identify optimal hyperparameters on small models and seamlessly transfer them to larger ones, significantly reducing tuning costs. On the other hand, Mixture-of-Experts (MoE) models have *Core contributors 1University of Warsaw 2Syntro 3IDEAS NCBR 4Institute of Fundamental Technological Research, Polish Academy of Sciences 5Nomagic 6Wroclaw University of Science and Technology. Correspondence to: Jan Małasnicki <jan.malasnicki@gmail.com>, Kamil Ciebiera <kp.ciebiera@student.uw.edu.pl>, Mateusz Borun <mateusz@syntro.ai>, Jakub Krajewski <gim.jakubk@gmail.com>. emerged as compelling approach for scaling large language models efficiently, offering substantial computational savings through sparse activation (Clark et al., 2022; Ludziejewski et al., 2024). However, the sparsity patterns and routing mechanisms intrinsic to MoE architectures fall outside the scope of current µP theory. Consequently, it remains unclear whether the parameterization developed for dense Transformers can be directly applied to MoE models, or whether adaptations are required to retain the transferability and stability benefits that µP provides. In this work, we extend µP to Mixture-of-Experts (MoE) architectures, offering both theoretical grounding and empirical validation. Our main contributions are: Theoretical framework for µP in MoE. Building on (Yang et al., 2022), we derive parameterization scheme that ensures that feature learning is preserved across all weights within MoE layers. Empirical validation of learning rate transfer. We show that our parameterization enables consistent learning dynamics across model widths, confirming that hyperparameters can be transferred effectively in MoE setups. Investigation of hyperparameter transferability across other MoE parameters. We observe that learning rate transferability breaks down across different values of top-k and granular expert sizes, highlighting boundary of current hyperparameter transferability in MoE settings. 2. Background and Related Work Mixture of Experts. Mixture of Experts was originally introduced by (Jacobs et al., 1991), and later proposed in the context of language modeling by (Shazeer et al., 2017). This approach has since been succesfully integrated into the Transformer (Vaswani et al., 2023) architecture in multiple works, including (Fedus et al., 2022; Lepikhin et al., 2020; Du et al., 2022; Zhou et al., 2022). In Transformer model, the MoE layer is typically constructed by replacing the Feed-Forward component with set of experts. Each expert retains the design of the original Feed-Forward layer, 1 µ-Parametrization for Mixture of Experts consisting of two linear layers with non-linearity between them. Crucially, for any given input token, only fraction of these experts are activated. The selection of experts for each token is determined by routing mechanism - simple linear layer followed by softmax normalization and Top-k choice. In the standard, Switch layer (Fedus et al., 2022), each of the experts is of the same size as in the corresponding dense (non-MoE) Transformer. This assumption is relaxed in finegrained MoE (Dai et al., 2024; Ludziejewski et al., 2024), where for granularity G, the hidden size of each expert is reduced by factor of G, while the number of experts and the routers top-k value are both multiplied by G. This way, the model has greater flexibility in mapping tokens to experts, while the number total and activated parameters remains approximately constant. transfer. Zero-shot hyperparameter Standard parametrization (SP) often fails to preserve stability and hyperparameter transfer in scaling neural networks. To overcome this limitation, (Yang, 2021) introduced Maximal Update Parametrization (µParametrization, or µP). µP ensures that each layer in network receives updates of the same order of magnitude during training, regardless of width. This allows for what is known as the feature learning regime, where internal representations evolve in meaningful way as training progresses. Crucially, µP enables hyperparameter transfer across model sizes: one can tune learning rates and initialization on small model and zero-shot transfer them to large one, as shown empirically and theoretically in (Yang et al., 2022). This paradigm, called µTransfer, has been shown to dramatically reduce the cost of training large models while maintaining performance. Later works (Yang et al., 2024; Everett et al., 2024) reformulate and generalize µP theory, while (Dey et al., 2025) and (Yang et al., 2023) include transfer across model depths. Despite its success on many architectures such as Transformers and ResNets, extending µP to Mixture-of-Experts models remains an open challenge. In this work, we address this open problem. 3. Principled approach to µP for MoE In this section, we analyze the behavior of MoE (Fedus et al., 2022) during training. We derive parameterization that ensures feature learning in all of its weights across different model widths. 3.1. Intuition In Tensor Programs 5 (TP5) (Yang et al., 2022), weight matrices are categorized based on the dimensions they connect. weight is referred to as hidden weight if it maps from an infinitely wide layer to another infinitely wide layer, as is typical within the internal blocks of deep networks. In contrast, an output weight maps from an infinitely wide layer to layer of fixed finite size, such as classification head. This structural distinction determines how the weight should be scaled at initialization and during optimization. Hidden weights receive gradient updates of magnitude Θ(1/n), where denotes the layer width. Output weights, by contrast, receive gradients of constant order Θ(1). Differentiation between those weights ensures stable training and enables zero-shot hyperparameter transfer across model scales, core goal of the µTransfer paradigm (Yang et al., 2022). Using convention from TP5 expert weights map infinite dimension to infinite dimension, so they should act as hidden weights, and router weight maps infinite to finite dimension, so it should act as output weight. We will now verify those intuitions. 3.2. Definitions and notation We model the MoE layer in the form of Switch Transformer (Fedus et al., 2022). It consists of: router matrix Rnexpertsn, Two layers of an expert E: E1 Rnexperts4nn, E2 Rnexpertsn4n. where by we denote the width of the model that gets scaled to infinity. The forward pass of the MoE layer is computed as: E(x) = E2ReLU(E1x), R(x) = softmax(top-k(Rx)), MoE(x) = E(x)T R(x). We now derive the correct µParameterization (µP) for Mixture-of-Experts (MoE) architectures by adapting the principles established in Tensor Programs (TP5) (Yang et al., 2022). Our goal is to ensure that all relevant quantities (activations, gradients, and updates) scale in way that allows for stable and predictable training dynamics across width, enabling hyperparameter transfer. 3.3. Desiderata Following TP5, we define the key properties that correctly µ-parametrized model must satisfy: 1. At initialization, all hidden representations h(x) in the network should scale as Θ(1). 2. The model output logits (x) should be O(1) at initialization. 3. After one optimization step, the changes to hidden representations h(x) and output logits (x) should be Θ(1). 2 µ-Parametrization for Mixture of Experts Embedding Unembedding Attention (Q, K, V, O) Feed-forward (dense) Experts (MoE) Router (MoE) Init. Var. Multiplier LR (Adam) 1.0 1.0 1.0 1. 1/fan in 1/fan in 1/fan in 1/fan in 1.0 1/fan in 1. 1.0 1.0 1.0 1/fan in 1.0 1/fan in 1/fan in 1/fan in 1/fan in 1.0 Table 1: The table presents parameterizations of dense and MoE Transformers, showing parameter scaling in big-O notation. Dense transformer µP is indicated in blue. MoE parameterizations build on dense µP. simpleP MoE is marked in red, while the theoretically grounded µP MoE is shown in green. These desiderata ensure that no matter how wide the model is, the output of each layer and the logits stay constant in size and so the feature learning (Yang et al., 2022) occurs. By vector Rn is Θ(na), we mean v2 = Θ(n2a) where is the standard Euclidean norm (same as in (Yang et al., 2022)). Intuitively, this means that the typical entry of is of size Θ(na). We use the same definition for vector being O(na) and Ω(na), and we use similar definitions for matrices to mean typical entry size. We will assume that all layers other than MoE layers follow parametrization from TP5. 3.4. Derivation 3.4.1. INITIALIZATION If we initialize E1, E2, according to TP5, we get: E(x) = Θ(1) Rx = O(1) R(x) = Θ(1) MoE (x) = Θ(1) Only non-standard part is Rx, but it has no direct effect on the size of the output of the layer, which stays Θ(1). 3.4.2. OPTIMIZER STEP In TP5, gradients with respect to hidden activations are typically Θ(1/n), and gradients with respect to output layer (pre-)activations are Θ(1). We verify that the MoE components obey the same behavior. We assume the same gradient norms as in TP5, = Θ(1/n) for hidden layers, and = Θ(1) for output layer. In the case of MoE, this means MoE(x) = Θ(1/n). Thus we obtain: E(x) = R(x)MoE(x)T = Θ(1) Θ(1/n) = Θ(1/n) (1) Since R(x), MoE (x) are 1-dimensional vectors the entry size is simply product of entry sizes. Then: 3 R(x) = E(x)MoE(x) = Θ(1) Θ(1/n) = Θ(1) (2) This equality is less obvious and requires proof that E(x) and MoE (x) are correlated. We prove that correlation in Appendix B. For correlated vectors v, Rn quantity vT has expected size Θ(v)Θ(u) corr(v, u) n, which follows from the Law of Large Numbers. For R(x), since Rx is O(1) and we take top-k over constant k, those operations do not change size of the gradient and gradient over Rx is still Θ(1), and E1x, E2ReLU(E1x) are Θ(1/n) since they mimic standard MLP layers. That means E1, E2, receive the same gradient sizes as hidden weights and output weight, respectively. This shows that they behave in the same way in training as their respective weight types from TP5, which shows that our intuition is correct and E1, E2 should be scaled as hidden weights, while should be scaled as output weight. 4. Experimental results and alternative views on hyperparameter transfer in MoE This section presents experimental results on learning rate transfer in MoE Transformers, providing empirical validation of our µP-based parameterization for MoE models. 4.1. Parameterizations for MoE In Section 3 we develop theory for µP for MoE where we reparametrize both router and experts. In the experimental validation, we also include simplified parameterization where each expert is treated as dense MLP without modifying the router, an approach we term simple-Parameterization MoE, or simpleP-MoE. The simpleP-MoE follows the intuitions from µP in dense Transformers, leveraging the structural similarity between each expert and Transformers MLP block. The details of both parameterizations are summarized in Table 1. µ-Parametrization for Mixture of Experts Figure 1: The plots present MoE performance for varying learning rates in the following set-ups: standard parametrization (SP) with no scaling on the left. simpleP - treating each Expert like FeedForward layer in the middle. µP - our theory applied to MoE layer on the right. While in the case of SP, the optimal learning rate is different for different model sizes, both reparameterizations achieve learning rate transfer across model widths. 4.2. Model width: We conduct experiments verifying transferability of learning rate with respect to the model width (see Figure 1). Both µP-MoE and simpleP-MoE achieve learning rate transfer. In both parameterizations the optimal learning rate seems to shift little, so that wider models have slightly higher optimal value. This result is similar to the original TP5 and to our experiments on dense models (Figure 3 in Appendix). This may be due to instabilities of architectures with large depth-to-width ratio, although proper investigation would be an interesting future work direction. Two training instances of µP MoE with an embedding dimension of 128 diverged, suggesting that µP MoE may be less stable than simpleP. However, this observation is not conclusive and would require further experiments to validate. 4.3. Scaling other MoE dimensions: In the previous section we have shown µ-Parametrization for MoE for varying model width. In this section, we investigate whether two other parameters of MoE architecture necessitate respective reparameterization. The first one is the number of experts that describes MoE models size. This parameter increases the model performance without increasing the computational cost, but sacrifices memory footprint (Clark et al., 2022; Ludziejewski et al., 2025). The other parameter is granularity as defined in Sec. 2. This parameter enables control over the expert size, keeping computational cost fixed. It can be used to adjust experts size to the available hardware like in (Dai et al., 2024). Figure 2(a) shows learning rate grid searches for varying numbers of experts. The experiments confirm that having more experts leads to lower final loss. We find that the optimal learning rate is stable. In Figure 2(b), we show MoE performance for different learning rates and granularities. Figure 2: (a) Varying the number of experts. Given our muP parametrization the optimal learning rate is preserved across varied number of experts. (b) Varying granularity. Learning rate is not preserved across different granularities. Contrary to the number of experts, optimal learning rate does not directly transfer between granularities. This is most likely the result of top-k adjusted by the granularity factor, or decreasing hidden dimension of each expert, as scaling the number of experts alone does not change the optimal learning rate. It is important to keep in mind that both ablations break the assumption of constant router output dimensions, which would call for adjustment of our theory. We leave the investigation of these results for future work. 5. Conclusions In this work, we derive µ-Parameterization for MoE Transformer models, enabling hyperparameter transfer across model width. We empirically validate our theoretical findings. Additionally, we explore two other MoE scaling strategies and observe that scaling the top-k or the expert hidden dimension disrupts learning rate transferability, while scaling the number of experts preserves it. Investigating granularity scaling remains direction for future work. 4 µ-Parametrization for Mixture of Experts"
        },
        {
            "title": "References",
            "content": "Clark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., Van Den Driessche, G. B., Rutherford, E., Hennigan, T., Johnson, M. J., Cassirer, A., Jones, C., Buchatskaya, E., Budden, D., Sifre, L., Osindero, S., Vinyals, O., Ranzato, M., Rae, J., Elsen, E., Kavukcuoglu, K., and Simonyan, K. Unified scaling laws for routed language models. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 40574086. PMLR, 17 23 Jul 2022. URL https://proceedings.mlr. press/v162/clark22a.html. Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y. K., Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. Dey, N., Zhang, B. C., Noci, L., Li, M., Bordelon, B., Bergsma, S., Pehlevan, C., Hanin, B., and Hestness, J. Dont be lazy: Completep enables compute-efficient deep transformers, 2025. URL https://arxiv.org/ abs/2505.01618. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts, 2022. Everett, K., Xiao, L., Wortsman, M., Alemi, A. A., Novak, R., Liu, P. J., Gur, I., Sohl-Dickstein, J., Kaelbling, L. P., Lee, J., and Pennington, J. Scaling exponents across parameterizations and optimizers, 2024. URL https: //arxiv.org/abs/2407.05872. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and Hutter, F. Efficient and robust automated machine learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 29622970, 2015. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. doi: 10.1162/neco.1991.3.1.79. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., Sepassi, R., Tucker, P., and Zhou, C. Gshard: Scaling giant models with conditional computation and automatic sharding. In Proceedings of the 37th International Conference on Machine Learning (ICML), 2020. URL https://arxiv.org/abs/ 2006.16668. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. Ludziejewski, J., Krajewski, J., Adamczewski, K., Pioro, M., Krutul, M., Antoniak, S., Ciebiera, K., Krol, K., Odrzygozdz, T., Sankowski, P., Cygan, M., and Jaszczur, S. Scaling laws for fine-grained mixture of experts. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 3327033288. PMLR, 2127 Jul 2024. URL https://proceedings. mlr.press/v235/ludziejewski24a.html. Ludziejewski, J., Pioro, M., Krajewski, J., Stefaniak, M., Krutul, M., Małasnicki, J., Cygan, M., Sankowski, P., Adamczewski, K., Miłos, P., and Jaszczur, S. Joint moe scaling laws: Mixture of experts can be memory efficient, 2025. URL https://arxiv.org/abs/ 2502.05172. OpenAI. Gpt-4o technical report. arXiv preprint arXiv:2410.21276, 2025. Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. 2018. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. In Proceedings of the 37th International Conference on Machine Learning, pp. 55415551. PMLR, 2020. URL https://arxiv. org/abs/1910.10683. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., µ-Parametrization for Mixture of Experts Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Yang, G. Tp4: Feature learning in infinite-width neural networks. In International Conference on Learning URL https:// Representations (ICLR), 2021. openreview.net/forum?id=K19h3z-4Z. Yang, G., Hu, E. J., et al. Tp5: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. URL https:// arxiv.org/abs/2203.03466. Yang, G., Yu, D., Zhu, C., and Hayou, S. Tensor programs vi: Feature learning in infinite-depth neural networks, 2023. URL https://arxiv.org/abs/ 2310.02244. Yang, G., Simon, J. B., and Bernstein, J. spectral condition for feature learning, 2024. URL https: //arxiv.org/abs/2310.17813. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-experts with expert choice routing, 2022. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 6 µ-Parametrization for Mixture of Experts Figure 3: This figure shows our experiments on learning rate transfer in dense models. Standard Parameterization (SP) on the lft has different optimal learning rate for each model width, while µP has stable optimum. A. MuP for Dense Transformer In this section we verify the findings from (Yang et al., 2022) by implementing the µP for dense models. As opposed to standard parametrization, in the case of muP reparametrization, the optimal learning rates transfer between different model widths. B. Expertgradient covariance lemma We now formalize the intuition that in µP-parametrized Switch-MoE block, each active experts forward activation correlates with its backward gradient at order Θ(1/n), and that the routers gradient norm remains Θ(1) both immediately after initialization and again after one µ-SGD/Adam update. Lemma B.1 (Expertgradient covariance). Let an L-block Switch-MoE be µP-parametrized with width , fixed number of experts nexperts = O(1), and fixed top-k = O(1). For each block ℓ, define nexperts (cid:88) y(ℓ) = R(ℓ) (cid:0)x(ℓ)(cid:1) E(ℓ) (cid:0)x(ℓ)(cid:1), e=1 δ(ℓ) = y(ℓ)L Rn. (cid:1)2 = Θ(n2) = δ(ℓ) = Θ(n1) for typical j. (3) (Hℓ) Assume the inductive hypothesis 1 (cid:88) (cid:0)δ(ℓ) j= Then for every block ℓ and any active expert e, at both = 0 : immediately after initialization, = 1 : after one µ-SGD/Adam step, 7 we have Proof. Notation. For block ℓ, set µ-Parametrization for Mixture of Experts Cov(cid:0)E(ℓ) e,j , δ(ℓ) (cid:13)r(ℓ) L(cid:13) (cid:13) (cid:1) = Θ(n1), (cid:13)2 = Θ(1). ℓ,e x(ℓ)(cid:1) Rn, ℓ,e ReLU(cid:0)E(1) (cid:0)x(ℓ)(cid:1) R, ee = E(2) Re = R(ℓ) δ = δ(ℓ) Rn, y(ℓ) x(ℓ) (ℓ) = Rnn, so that δ(ℓ1) = (J (ℓ)) δ and (ℓ) ij (0, 1/n) under µP. Step 1: Steins lemma (holds at = 0 and = 1). Fix expert e, coordinate j, and define = ee,j (0, σ2), (cid:88) = Re ee,j, e=e g(z) = [L(y(ℓ))]j, y(ℓ) = Re + c. Then g(z) = Re [L(y(ℓ))]j and by (Hℓ), [L(y(ℓ))]j = Θ(n1). Thus Cov(Z, g(Z)) = σ2 E(cid:2)g(Z)(cid:3) = σ2 Re E(cid:2)L(y(ℓ))(cid:3) = Θ(1) Θ(1) Θ(n1) = Θ(n1). (4) (5) (6) (7) Remark. Because by induction the block-(ℓ + 1) Hessian entries already scale like Θ(n1), and passing any such matrix back through µP-linear layer (whose weights are (0, 1/n)) multiplies each term by another 1/n but sums over of them, the net effect is still Θ(n1). In other words, 1/n factor per weight-matrix multiplication exactly preserves the Θ(n1) scale of (cid:2)L(y(ℓ))(cid:3) j. Step 2: Router-gradient norm (holds at = 0 and = 1). Summing the covariances over j, E(cid:2)e δ(cid:3) = (cid:88) j= Cov(ee,j, δj) = Θ(n1) = Θ(1). (8) Since Var(e δ) = O(n1), Chebyshevs inequality gives δ = Θ(1) with high probability, i.e. the second line of (4). Step 3: One-step update. Under µ-SGD/Adam with LR η/n on experts and η on router, the factors in (7) and (8) change by at most (1 + O(n1)) factor, so both lines of (4) hold at = 1. Step 4: Depth induction. Using δ(ℓ1) = (J (ℓ))δ(ℓ) and (ℓ) establishing (Hℓ1). The base case ℓ = is given by Tensor-Programs V; induction completes the proof. ij (0, 1/n), one shows 1 j(δ(ℓ1) (cid:80) )2 = Θ(n2), C. Experimental setup All models in this study are decoder-only Transformers trained on the C4 dataset (Raffel et al., 2020). We use the GPT-2 tokenizer (Radford et al., 2018) and optimize with AdamW (Loshchilov & Hutter, 2019). Training follows cosine decay schedule with linear warmup for the first 1% of steps. Weights are initialized with normal distribution, as the theory of 8 µ-Parametrization for Mixture of Experts Tensor Programs assumes (Yang et al., 2022). Mixed precision training is applied, with Attention component computed at high precision. The models employ MLP with ReLU activations. MoE models are Switch Transformers (Fedus et al., 2022). As standard MoE setup we used 8 Experts, 1 of which is activated per token. All models have Attention head dimension of 64. Two auxiliary losses are used for the Router: z-loss weighted at 0.001 (Zoph et al., 2022) and load balancing weighted at 0.01 (Fedus et al., 2022). All models for 3 have 24 layers and are trained for 16B tokens. Experiments for MoE are smaller for technical reasons. All Models in 1 are trained for 1B tokens. They have 8 transformer layers. Models in 2 have 12 blocks, trained for 2.5B tokens."
        }
    ],
    "affiliations": [
        "IDEAS NCBR",
        "Institute of Fundamental Technological Research, Polish Academy of Sciences",
        "Nomagic",
        "Syntro",
        "University of Warsaw",
        "Wroclaw University of Science and Technology"
    ]
}