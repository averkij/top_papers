{
    "paper_title": "WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation",
    "authors": [
        "Jing Wang",
        "Ao Ma",
        "Ke Cao",
        "Jun Zheng",
        "Zhanjie Zhang",
        "Jiasong Feng",
        "Shanyuan Liu",
        "Yuhang Ma",
        "Bo Cheng",
        "Dawei Leng",
        "Yuhui Yin",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from a lack of clear guidance on physical information due to a significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier, enhancing the model's physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose a novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving a considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 3 5 1 8 0 . 3 0 5 2 : r WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation Jing Wang1,2, Ao Ma2, Ke Cao2, Jun Zheng1, Zhanjie Zhang2, Jiasong Feng2, Shanyuan Liu2, Yuhang Ma2, Bo Cheng2, Dawei Leng2, Yuhui Yin2, Xiaodan Liang1,3 Equal Contribution, Corresponding Authors 1Shenzhen Campus of Sun Yat-Sen University, 2360 AI Research, 3Peng Cheng Laboratory, wangj977@mail2.sysu.edu.cn, lengdawei@360.cn, xdliang328@gmail.com Figure 1. Overview of our physical dataset WISA-32K. (Left) Examples of 17 physical phenomena across 3 physics categories in WISA32K. (Top right) WISA-32K contains of approximately 32,000 video clips, with 47% related to Dynamics, 24% to Thermodynamics, and 29% to Optics. (Bottom right) Distribution of frame counts across all videos in WISA-32K."
        },
        {
            "title": "Abstract",
            "content": "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from lack of clear guidance on physical information due to significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and Physical Classifier, enhancing the models physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the Project Page. 1. Introduction Many recent studies (e.g., Cosmos [1], Kling [13], StepVideo-T2V [20], Sora [25], and CogVideoX [40]) have endeavored to develop robust text-to-video (T2V) models for building world simulators [6, 38, 42]. While these models are capable of generating highly realistic and text-consistent videos, leveraging the scale of their data and architectures, 1 they still face challenges in understanding abstract physical principles and producing videos that fully align with realworld physical laws [3, 22]. The substantial gap between abstract physical laws and their visual manifestations presents significant challenge for injecting physical guidance into T2V models. Physical principles or laws are often conveyed through abstract natural language, reflecting the underlying operational logic of the real world. In contrast, generative models map textual descriptions directly to the visual appearance of objects, including their color and shape. There is complex logical reasoning process between physical principles and the visual physical phenomena they give rise to. However, generative models, which are trained to map learned data distributions, struggle to extract appropriate physical information from single textual instruction and translate it into physically consistent visual representation for specific scenario. This challenge becomes even more pronounced in video generation, where the strict temporal order of physical events must be preserved. To this end, we propose the World Simulator Assistant (WISA), which decomposes abstract physical principles into multiple categories of physical information and introduces them to T2V models for physics-aware generation. Specifically, it decomposes physical principles into textual physics descriptions, qualitative physics categories, and quantitative physical properties, and designs appropriate conditional injection methods for each type of information. The textual physical description outlines the physical principles to be considered in the scene, the resulting physical phenomena, and their specific visual manifestations. WISA concatenates it with caption before text encoder. Qualitative physics categories indicate the types of physical phenomena that may be involved in the scene. WISA considers 17 common physical phenomena across three major branches of physics (i.e., dynamics, thermodynamics, and optics), such as collision in dynamics, refraction in optics, and melting in thermodynamics. Considering that different physical phenomena require distinct physical feature, inspired by MoE [29] and MoH [11], WISA propose Mixture-of-Physical-Experts Attention (MoPA), which assigns expert heads to each physics category, with only the relevant expert heads activated during sampling to handle the corresponding physical phenomena. Quantitative physics properties represent physical quantities closely related to the physical processes (e.g., density, time, and temperature). WISA encodes these properties as physical embeddings and injects them into the model via AdaLN [26]. In addition, WISA employs Physical Classifier, which is designed to recognize qualitative physics categories, to assist in perceiving physical properties. However, extracting above physical information from general scene video in existing datasets [24, 35] is suboptiFigure 2. Comparison between general scene videos in Koala-36M and videos with distinct physical phenomena in WISA-32K. mal approach. Firstly, general scene videos often feature the interweaving of multiple physical phenomena. Individual physical phenomena are not prominently visualized, which makes it difficult to accurately extract physical information and establish precise connection between the physical data and its corresponding visual manifestation. Secondly, in these datasets, only few videos distinctly highlight specific physical phenomena as representative examples, while most videos treat physical phenomena as secondary elements. For instance, in the Figure. 2, the flow of water is secondary element. Despite having physical information guidance, the T2V models is unable to perceive the physical principles of fluid motion from this type of data. To address these challenges, we collect and construct WISA-32K, dataset containing 32,000 videos that represent 17 physical phenomena across three major branches of physics as shown in Figure. 1, designed as data assistant for world simulators. Specifically, based on the previously defined physics categories, we collect videos that clearly exhibit obvious physical phenomena corresponding to each category (e.g., as shown in the lower part of Figure. 2). We then apply shot boundary detection, aesthetic quality filtering, and video captioning to the raw videos. Subsequently, we leverage GPT-4o mini to extract and decompose the physical information from the video captions into textual physics descriptions, qualitative physics categories, and quantitative physics properties for WISA. Our contributions can be summarized as follows: We propose physical principle decoupling method, bridging the gap between physical laws and generative modeling. In this method, physical principles are represented as structured physical information, encompassing textual physical descriptions, qualitative physics categories, and quantitative physical properties. We present the World Simulator Assistant (WISA), which guides T2V models to efficiently learn specific physical phenomena based on structured physical information, through specialized designs such as Mixture-of-PhysicalExperts Attention (MoPA) and Physical Classifier. We manually collect 32,000 video clips that clearly show2 case physical phenomena, creating the first large-scale physics video dataset, WISA-32K. It broadly covers common physical phenomena observed in the real world, encompassing 17 types of physical events (e.g., Collision, Melting, and Reflection) across three major branches of physics: Dynamics, Thermodynamics, and Optics. Quantitative and qualitative experimental results demonstrate WISA and WISA-32K can effectively assist basic T2V models in producing videos that better align with real-world physical laws, while introducing only 3.5% increase in parameter count and 5% inference time. 2. Related Work 2.1. Text-to-Video Generation Early text-to-video (T2V) generation research [4, 710, 31, 32] primarily extend image generation models [5, 16 18, 21, 27] with temporal capabilities to enable video generation. These methods often suffered from limited realism and restricted motion dynamics. The powerful 3D spatiotemporal modeling and scalability of Diffusion Transformers [14, 26] have greatly advanced the development of visual generation models. Enabled by Diffusion Transformers, series of recent T2V works (including OpenSora [41], Cosmos [1], Sora [25], CogVideoX [40], HunyuanVideo[x], Kling [13], Wan2.1 [30], and Step-Video-T2V [20]) significantly improve the realism and motion quality of video generation by scaling up model parameters and training data. These works are widely considered as promising pathway towards building World Simulator. However, they still struggle to generate videos that fully comply with realworld physical laws as they essentially fit the data distribution [12] from general-scene datasets such as Koala-36M [35] and OpenVid [24], where physical laws are not explicitly reflected and physical phenomena are not prominently presented (e.g., in the upper part of Figure. 2). In contrast, our carefully curated WISA-32K dataset prioritizes the explicit presentation of typical physical phenomena as the primary criterion for video collection as presented in Figure. 1. And it provides detailed and structured physical information annotations, making it valuable data assistant for enhancing the physical consistency of video generation. 2.2. Physical-aware Video Generation Recently, researchers [2, 3, 15, 19, 22, 23, 36, 39] have increasingly focused to improving and evaluating the physical consistency of generated videos. On the one hand, Videophy [3] and PhyGenBench [22] build test samples that reflect various physical laws, and they evaluate how well generated videos follow real-world physical laws by either training physics classification models with manual annotations or using question-answering methods based on VisionLanguage models [37]. On the other hand, DANO [15], MotionCraft [2], and PhysGen [19] parse objects from images and estimate their rigid motion in differentiable manner by considering physical properties such as mass, inertia, friction, and rotation. Based on these estimations, they animate the images into videos. However, these methods are restricted to fixed physical categories (e.g., rigid motion) and static scenarios that involve only object motion, which hinders their generalizability. PhyT2V [39] leverages large language models and vision-language models to extract physical inconsistency information from generated videos. Based on the extracted physical feedback, it iteratively refines the textual description over multiple rounds, improving video generation quality. Although this approach offers generality, it introduces significant inference overhead and fails to enhance the generative models ability to encode physical knowledge. In this paper, WISA incorporates structured physical information into the generative model, enhancing its physical perception and enabling it to handle various physical phenomena more effectively. 3. WISA-32K 3.1. Data Collection and Annotation Physical Laws Definition: We select three fundamental categories of physics that are universally relevant in life: Dynamics, Thermodynamics, and Optics. Seventeen physical phenomena associated with these categories are then considered in WISA-32K. Dynamics: We consider six common dynamic phenomena encountered in daily situations: Collision, Rigid Body Motion, Elastic Motion, Liquid Motion, Gas Motion, and Deformation. For instance, the swinging of pendulum serves as an example of Rigid Body Motion. Thermodynamics: We select six common thermodynamic phenomena observed in typical life scenarios: Melting, Solidification, Vaporization, Liquefaction, Explosion, and Combustion. For example, time-lapse of melting ice cream illustrates the Melting phenomenon. Optics: We define five common optical phenomena: Reflection, Refraction, Scattering, Interference and Diffraction, and Unnatural Light Sources. For example, video showing the reflection on lake illustrates the Reflection. Based on the 17 physical phenomena outlined above, we manually collected 32,000 video samples, intentionally excluding videos with text. Additionally, we did not consider certain physical phenomena (e.g., sublimation, condensation) due to their infrequent occurrence in life and the challenges associated with collecting data for these phenomena. Pre-processing and Caption: The video data is manually collected, ensuring the exclusion of videos containing text or low-quality content. Consequently, only simple pre-processing techniques are applied. We use PySceneDetect [28] to split the raw videos into individual scene clips, 3 Figure 3. Pipeline of WISA-32K. We first define 17 common physical phenomena and, based on this, manually collect 32,000 video samples that clearly illustrate these phenomena. Then, we perform shot detection and aesthetic filtering on the raw videos. Text description are extracted using Qwen2-VL, and detailed physical annotations are generated with GPT-4o mini. followed by filtering based on aesthetic scores. Then, we utilize Qwen2-VL [34] to generate video captions using the following prompt: {Please describe the content of this video in as much detail as possible, including the objects, scenery, animals, and camera movements within the video.} The caption length is limited to 256 tokens. 3.2. Physical Information Decompose We believe that simple video captions are not sufficient to clearly represent the physical information and related physical phenomena in video. As shown in the Figure. 3, we further constructed structured physical annotations to analyze the physical information from multiple dimensions. Specifically, we decompose the physical information into: textual physical descriptions, qualitative physics categories, and quantitative physical properties. Textual physical descriptions: Provide detailed explanation of the physical principles to be considered and the resulting intuitive physical phenomena, while supplementing the missing physical information in the prompt. Qualitative physics categories: Based on the physical laws defined in Sec. 3.1, we annotate the physical phenomena present in each video and identify which of the 17 physical phenomena are involved. Three categories of anomalies (i.e., No obvious dynamic phenomenon, No obvious thermodynamic phenomenon, and No obvious optical phenomenon) are introduced to account for scenarios that do not involve dynamics, thermodynamics, or optical phenomena. Furthermore, nine categories of visual phenomena are introduced, two of which pertain to whether the shot exhibits motion, while the remaining seven correspond to changes in the state of moving entities (i.e., Object decomposition and splitting, Mixing of multiple objects ... The detailed explanation please refer to Supplementary Material B). There are total of 29 qualitative physics categories. Quantitative physical properties: Three physical attributes related to multiple physical phenomena are annotated, namely the density of primary motion physics, the time range during which the physical phenomenon occurs, and the temperature range during which the physical phenomenon occurs. Due to the significant computational overhead and cost associated with video multi-modal models, the annotation of the above physical information is carried out using GPT-4o mini based on caption. Specifically, we conduct five rounds of annotation to label qualitative physical phenomenon categories (i.e., dynamics, thermodynamics, optics, motion, the state of objects), and three rounds to annotate quantitative physical attributes (i.e., Density, Time and Temperature). Detailed annotation prompts and example are provided in the Supplementary Material and E. We sample 100 examples and manually evaluate the multi-modal annotation scheme and the caption-based annotation scheme using GPT-4o mini. Thanks to the accurate captions provided by Qwen2-VL, the caption-based annotation scheme achieves performance only slightly lower than the multi-modal scheme (76% vs. 78%). However, the caption-based annotation method offers substantial cost advantages (approximately 2k vs. 10k tokens per sample) and provides greater convenience for users when only textual descriptions are available. More analysis of WISA-32K please refer to the Supplementary Material and F. 4 Figure 4. Overview of the proposed WISA. WISA introduces the Physical Module and Physical Classifier, which leverage structured physical annotations to guide and assist T2V models in generating physics-aware videos. Specifically, for qualitative physical categories, WISA constructs Mixture-of-Physical-Experts Attention within the Physical Module, where each attention head corresponds to specific physical phenomenon. The relevant physical expert is activated by the input qualitative physical category. The Physical Classifier predicts the physical categories relevant to the video and is supervised by inputted categories to understand abstract physical principles. 4. Method 4.1. Overview Given textual physical descriptions, qualitative physical categories, and quantitative physical properties, we design the WISA framework to efficiently incorporate these conditions into existing T2V models (i.e., CogVideoX [40]). To facilitate the learning of physical knowledge while preserving the models original capabilities with limited video data, we design three distinct condition injection methods tailored to each of the three categories of physical information, as illustrated in Figure. 4. Specifically, for the textual physical descriptions, we concatenate them with the video caption and leverage the generative models inherent semantic understanding to generate visual phenomena described in text (Such as amplitude of the swing gradually decreases over time in Figure. 4). For qualitative and quantitative physical conditions, WISA introduces the Physical Module. In this module, we propose Mixture-of-Physical-Experts Attention (MoPA), which assigns expert heads to each physics category to model category-specific features. Quantitative physical quantities are encoded as physical embeddings and then integrated into the denoising feature within the module using AdaLN. Additionally, we introduce qualitative Physical Classifier to help the model understand the physical conditions. Due to the significant computational and parameter cost introduced by MoPA, only one physical module is inserted after all the Diffusion transformer blocks to accelerate training and reduce the overall burden. Detailed explanations and elaborations of the Physical Module and Physical Classifier are provided in Sec. 4.2 and Sec. 4.3. 4.2. Physical Module Most videos from real-world scenes involve the coupling of multiple physical phenomena. Even when decomposed into distinct physical categories in WISA-32K, it remains challenging for T2V models to comprehend the abstract qualitative physical categories and accurately model specific types of physical phenomena. To address this challenge, we propose Mixture-of-Physical-Experts Attention within the Physical Module. Inspired by MoH [11], this mechanism assigns each head in the multi-head self-attention to specific class of physical phenomena and activates the output of the relevant head only when the corresponding phenomenon is present. This approach treats each head as an expert in its domain, enabling it to independently model the properties of particular physical phenomenon. Specifically, qualitative physical categories are encoded as Pc RC, where denotes the number of defined physical phenomena (i.e., 29). Here, = 0 indicates that the corresponding category is not activated, and = 1 indicates that the corresponding category is activated, with being the category index. Physical categories cannot be absolutely correct and may contain noise, such as incorrect activations or suppressions. To mitigate the impact of these noises on training, we employ random perturbation operation, where the positions with = 1 are set to 0.1 and the positions, and the positions with = 0 are set to 1.0 with certain probability (i.e., 0.2), resulting ˆPc. After the multi-head self-attention operation, the denoising fea5 ture Fh RN dh (where presents the number of head and = C, and denotes head dimension) will interact with ˆPc to activate and suppress the experts corresponding to different physical phenomena. The feature dimension is then restored through concatenation and linear layer. The mathematical representation of this process is as follows: ˆPc = Random(Pc), Fh = MHSA(F ), Fo = Linear(Reshape(Fh ˆPc)) (1) where Random denotes random perturbations operation, MHSA represents multi-head self-attention, and denotes element-wise multiplication. Due to the large variations in the time and temperature spans of different physical phenomena, we first represent the temperature and time in the quantitative information using scientific notation, with coefficients and exponents. These values are mapped through linear layer, concatenated with the timestep embedding, and injected by AdaLN. Generative models often consist of multiple transformer blocks with large feature dimensions, inserting the Physical Module after every block would lead to an explosion in both parameters and computational complexity. Additionally, it could result in loss of the models inherent capabilities and cause slower convergence of the shallow Physical Module. Therefore, we insert the Physical Module only after the final transformer block, achieving efficient physical information guidance while mitigating the aforementioned issues. 4.3. Physical Classifier To guide the generative model in understanding abstract physical categories and modeling physical properties, we introduce Physical Classifier after the Physical Module to predict qualitative physical categories. Multiple physical phenomena may be coupled in video, we use multi-label binary cross-entropy (BCE) loss for supervision. Lpc = (cid:88) i=1 (P log(f c) + (1 )log(1 c)), (2) where is the number of physical categories, and fc RC represents the predicted probabilities, which are obtained by passing the denoising feature through the Physical Classifier and the sigmoid function. To balance the introduced classification loss Lpc and the diffusion loss Ldif usion, we adopt the following loss function to optimize the physics-aware generative model. = Ldif usion + λLpc/(1 + Lpc.detach), (3) where λ is balance coefficient. 6 Table 1. Quantitative evaluation using VideoCon-Physics conducted on the Videophy and PhyGenBench prompt lists. The best and second performing metrics are highlighted in bold and underline respectively. denotes the scores reproduced by us. Method Inference VideoPhy [3] PhyGenBench [22] Time (s) SA () PC () SA () PC () VideoCrafter2 [7] HunyuanVideo [16] CogvideoX-5B [40] Cosmos [1] PhyT2V (Round 4) [39] PhyT2V (Round 4) [39] WISA - - 210 600 1800 1800 220 0.47 0.46 0.60 0.57 0.59 0. 0.67 0.36 0.28 0.33 0.18 0.42 0.37 0.38 - - 0.39 0.43 0.38 - 0.40 - - 0.41 0.14 0.42 - 0.43 5. Experiments 5.1. Setup Training Setting: We select the current representative open-source T2V model, CogVideoX-5B, as the base T2V model to validate the effectiveness of WISA. More training detail please refer to Supplementary Material A. Evaluation: We select VideoCon-Physics from Videophy [3] to evaluate the physical law consistency (PC) and semantic coherence (SA) of the generated videos. We use 160 carefully crafted prompts from PhyGenBench [22] and 344 prompts from Videophy, designed to reflect various physical principles, for testing. We consider PC and SA return values greater than or equal to 0.5 as PC = 1 and SA = 1, and values less than 0.5 as PC = 0 and SA = 0. 5.2. Quantitative comparison We select four general text-to-video generation models (i.e., VideoCrafter2, HunyuanVideo, CogVideoX-5B and Cosmos-Diffusion-7B) and PhyT2V, method specifically designed to enhance physical properties, for quantitative comparison, as shown in Table. 1. VideoPhy: WISA achieves state-of-the-art performance on both SA and PC metrics, while maintaining high efficiency. Compared to the baseline (CogVideoX-5B), WISA improves SA and PC scores by 0.07 and 0.05, respectively, demonstrating that our proposed method significantly enhances the realism of generated videos. PhyT2V improves its performance by iteratively analyzing physical errors in generated video captions and adjusting the input prompts based on feedback from VideoCon-Physics its cumbersome pipeline, which inscores. However, volves multiple rounds of Tarsier-34B [33] inference for video generation, introduces extremely long inference timeapproximately 9 times longer than the original generation model (CogVideoX-5B). Cosmos exhibits poor performance due to the disordered physical processes and inconsistent temporal sequences. Figure 5. Qualitative comparison between WISA and existing T2V methods. WISA exhibit better alignment with real-world physical laws. Figure 6. More samples generated by WISA, covering additional physical phenomena. PhyGenBench: We also evaluate our method on the prompts from PhyGenBench, achieving SOTA results, which demonstrates the generalizability of WISA. 5.3. Qualitative comparison We further provide qualitative comparison with existing methods to demonstrate the advantages of WISA. As shown in the Figure. 5, for the example of erasing pencil marks with an eraser, WISA generates video where the pencil marks are cleanly removed as the eraser passes over them. In contrast, CogVideoX-5B fails to generate any pencil marks, PhyT2V makes the pencil marks even darker after erasing, and Cosmos does not show the erasing process at all. In the example on the right, WISA successfully simulates the process of an apple falling into water: the water surface remains calm before the apple enters, splashes form as the apple impacts the water, and the apple experiences buoyant force after submersion. However, CogVideoX5B generates chaotic water and apple movements, PhyT2V omits the falling process, and Cosmos mistakenly generates two apples at the end. Additional videos generated by WISA, demonstrating various physical phenomena, are also presented in the Figure. 6. All the videos mentioned above are available in Project Page. 5.4. Ablation Study We conduct ablation studies on VideoPhy using VideoConPhysics to verify the effectiveness of key components in our 7 Table 2. Ablation study on the key components of WISA. Setting SA () PC () Structure Baseline only LoRA w/o Physical Module w/o Physical Classifier Data 32K sample from Koala-36M WISA-32K 0.60 0.64 0.64 0.66 0.62 0.67 0.33 0.34 0.33 0.36 0.33 0.38 Figure 7. Human evaluation on VideoPhy prompts. method, as shown in the Table. 2. As expected, removing the Physical Module leads to performance drop, due to the lack of qualitative and quantitative physical information guidance. Similarly, the Physical Classifier helps the generative model perceive and model physical properties, which benefits both semantic consistency and physical law consistency. Moreover, the training data of the evaluation model VideoCon-Physics [3] comes from samples generated by nine T2V models, which leads to distribution shift compared to the real-world videos in WISA-32K. As result, solely relying on LoRA brings only limited improvements. Furthermore, we explore the role of clearly-defined physical phenomena data and general scene data in enhancing physical perception. We sample 32,000 videos from Koala-36M, label the physical information, and train WISA, which results in limited improvement. This showcases that videos with clearly physical phenomena in WISA-32K are highly beneficial for modeling physical properties. 5.5. Human Evalution The physical consistency of generated videos is abstract and difficult to quantify directly. Therefore, we conduct human evaluation to assess the effectiveness of WISA. Specifically, we selected three representative models for comparison. The evaluation considered two aspects: semantic consistency and physical alignment. Each candidate model is ranked in both aspects, receiving score based on its ranking: 3 points for first place, 2 points for second, and 0 points for last. The results, shown in Figure. 7, demonstrate that WISA achieves significant advantage in physical alignment, while also maintaining strong semantic consistency. 5.6. Attention Map Analysis We further conduct visual analysis of the Mixtureof-Physical-Experts attention maps, aiming to investigate whether different physical experts focus on the regions corFigure 8. Attention maps of different physical experts. responding to distinct physical phenomena. As shown in the Figure. 9, the rigid body motion expert perfectly focuses on the swing region, while the non-dynamics expert attends to the static background with no apparent motion. This demonstrates that the MoPA effectively models and captures the corresponding physical attributes. 6. Limitation Although our approach significantly improves the ability of existing T2V models to generate videos that align with realworld physical laws, it still has the following limitations: 1) Limited physical categories: We collect 32,000 videos in WISA-32K, covering 17 types of physical phenomena. However, due to constraints in time and manpower, the dataset does not include all physical phenomena encountered in real world, such as corrosion or vacuum environments. 2) Limited physical information guidance: WISA primarily provides high-level semantic guidance and lacks detailed constraints at the physical mechanism level (e.g., energy conservation, Newtons laws). However, introducing more detailed physical principle constraints currently requires modeling object motion based on image or 3D information, which suffers from poor generalization and can only handle limited categories and scenarios. How to incorporate physical principle constraints into text-to-video generation while maintaining generalization remains an area worth further research. 3) Failure case: Due to the limited data and parameter, WISA cannot generate videos that perfectly align with physical principles in all scenarios. 7. Conclusion In this paper, we present WISA framework, which decomposes physical principles into structured physical information, including textual physical descriptions, qualitative physical categories, and quantitative physical properties. To 8 help T2V models learn these physical aspects effectively, WISA incorporates two key components: the Mixtureof-Physical-Experts Attention and the Physical Classifier. Building on this, we construct WISA-32K, dataset containing 32,000 video clips that cover 17 physical phenomena across three fundamental categories of physics, providing high-quality data foundation. Experimental results show that WISA and WISA-32K can effectively help producing videos that better align with real-world physical laws, while the additional computational overhead is under 5%. We hope that WISA can provide valuable insights to the research on building powerful world simulators."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 1, 3, 6 [2] Luca Savant Aira, Antonio Montanaro, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: PhysicsarXiv preprint based zero-shot video generation. arXiv:2405.13557, 2024. 3 [3] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 2, 3, 6, 8 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 3, 6 [8] Jiasong Feng, Ao Ma, Jing Wang, Bo Cheng, Xiaodan Liang, Dawei Leng, and Yuhui Yin. Fancyvideo: Towards dynamic and consistent video generation via cross-frame textual guidance. arXiv preprint arXiv:2408.08189, 2024. Yin, Devi Parikh, and Ishan Misra. Factorizing text-to-video generation by explicit image conditioning. In European Conference on Computer Vision, pages 205224. Springer, 2024. [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [11] Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. Moh: Multihead attention as mixture-of-head attention. arXiv preprint arXiv:2410.11842, 2024. 2, 5 [12] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 3 [13] Kuaishou. Kling. https://klingai.kuaishou. com/, 2024. Accessed: 2024-09-03. 1, 3 [14] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [15] Simon Le Cleach, Hong-Xing Yu, Michelle Guo, Taylor Howell, Ruohan Gao, Jiajun Wu, Zachary Manchester, and Mac Schwager. Differentiable physics simulation of dynamics-augmented neural objects. IEEE Robotics and Automation Letters, 8(5):27802787, 2023. 3 [16] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 3, 6 [17] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [18] Shanyuan Liu, Dawei Leng, and Yuhui Yin. Bridge diffusion model: bridge non-english language-native text-toarXiv image diffusion model with english communities. preprint arXiv:2309.00952, 2023. 3 [19] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2024. 3 [20] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 1, 3 [21] Yuhang Ma, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Dawei Leng, and Yuhui Yin. Hico: Hierarchical controllable diffusion model for layout-to-image generation. Advances in Neural Information Processing Systems, 37:128886128910, 2024. 3 [22] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 2, 3, 6 [9] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi [23] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn 9 [38] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model arXiv with natural preprint arXiv:2406.09455, 2024. 1 language actions and video states. [39] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physics-grounded text-to-video generation. arXiv preprint arXiv:2412.00596, 2024. 3, 6 [40] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 3, 5, [41] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 3 [42] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. 1 physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. 3 [24] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 2, 3 [25] OpenAI. Sora. https://openai.com/, 2024. Accessed: 2024-09-03. 1, [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, 3 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [28] PySceneDetect Developers. Pyscenedetect. https:// www.scenedetect.com/. 3 [29] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. 2 [30] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. 3 [31] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [32] Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, and Xiaodan Liang. Qihoo-t2x: An efficiency-focused diffusion transformer via proxy tokens for text-to-any-task. arXiv e-prints, pages arXiv2409, 2024. 3 [33] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 6 [34] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4 [35] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. 2, 3 [36] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024. 3 [37] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. 10 A. Training Detail We select the current representative open-source T2V model, CogVideoX-5B, as the base T2V model to validate the effectiveness of the proposed WISA. We train WISA on our constructed WISA-32K for 8,000 steps, using learning rate of 2e-5 and batch size of 8. The video resolution is set to 480x720, with 49 frames. The LoRA rank is set to 128, and the LoRA alpha is set to 16. During training, only the physical module, physical classifier, and parameters in LoRA are updated, with total of 187M learnable parameters. The experiments are conducted on 8 A100 GPUs, each with 80GB of memory. B. The Definition of Physical Categories We define total of 29 qualitative physical categories, organized into 5 major classes. The physical categories within each class, along with their corresponding category IDs, are listed as follows: Dynamics: 1. Collision, 2. Rigid Body Motion, 3. Elastic Motion, 4. Liquid Motion, 5. Gas Motion, 6. Deformation, and 7. No obvious dynamic phenomenon Thermodynamics: 8. Melting, 9. Solidification, 10. Vaporization, 11. Liquefaction, 12. Explosion, 13. Combustion and 14. No obvious thermodynamic phenomenon Optics: 15. Reflection, 16. Refraction, 17. Scattering, 18. Interference and Diffraction, 19. Unnatural Light Sources, and 20. No obvious optical phenomenon Camera motion: 21. Yes, 22. No The state of object: 23. Liquids Objects Appearance, 24. Solid Objects Appearance, 25. Gas Objects Appearance, 26. Object decomposition and splitting, 27. Mixing of Multiple Objects, 28. Object Disappearance and 29. No Change Specifically, Liquids objects appearance: new liquids appear from the camera over time and due to external forces, such as water squeezed out of towel. Solid objects appearance: new solids appear from the camera over time and due to external forces, such as Chemical reaction produces precipitate or car drives in from outside the camera. Gas objects appearance: new gas appear from the camera over time and due to external forces. Object decomposition and splitting: Over time and under the action of external forces, an object is broken into multiple sub-parts: such as fruits and vegetables being cut in half. Mixing of multiple objects: Over time and with the action of external forces, two objects of the same state mix together, such as two solutions mixing. Object disappearance: As time passes and external forces act, objects disappear from the camera. No change: No change in the state of the object Figure 9. Accuracy of qualitative physical category annotations. C. Dataset Property Analysis We visualize the distribution of different physics categories and video frame counts in WISA-32K, as shown in the paper Figure. 1. Dynamics frequently occurs in daily life, it accounts for the largest proportion at 47%. Optics and thermodynamics, which typically require specific temperature or environmental conditions, account for 29% and 24%, respectively. The proportions of each subcategory are shown in the outer ring of the figure. Based on the labels of the manually collected videos, we evaluate the accuracy of the qualitative physical category annotations. The results are shown in the Figure. 9, where the accuracy for dynamics, optics, and thermodynamics reaches 84%, 71%, and 64%, respectively, with an overall accuracy of 75%. D. More Examples and Annotation Following the proposed physical information annotation pipeline, we construct the WISA-32K dataset. Several example videos and their corresponding annotations are shown in the Figure. 10. This pipeline enables accurate and detailed annotation of physical information, ensuring that each video is comprehensively labeled with its relevant physical properties and phenomena. E. Annotation Prompts The detailed prompt used for physical information annotation is illustrated in the Figure. 11, Figure. 12, and Figure. 13. F. Word Cloud We conducted word frequency analysis on the textual physical description in the dataset and generated the word cloud shown in Figure. 14. To filter out irrelevant words, we retained only nouns and selected them based on their frequency, from highest to lowest. Notably, physical terms such as motion, phenomenon, and light appear more frequently, highlighting the strong physical relevance of the dataset. 11 Figure 10. The video data and its detailed annotations in WISA-32K. G. Discussion of Quantitative Evaluation During the quantitative evaluation, we observe several misjudgments in VideoCon-Physics, as shown in the Figure. 15. Specifically, WISA generates physically plausible process where the object enters the water first, followed by the splash aligning well with real-world physical laws. However, this sample only receives low score of 0.08 from VideoCon-Physics. We further conduct simple test using Qwen2.5-VL for evaluation, and the model also struggles to distinguish the correct or incorrect sequence of physical events. These findings show the limitations of existing video-based physics evaluation metrics, indicating that future research into more reliable physical property assessments for videos is necessary. 12 Figure 11. Prompts for annotating textual physical descriptions and quantitative physical properties 13 Figure 12. Prompts for annotating qualitative physics categories 14 Figure 13. Prompts for annotating qualitative physics categories 15 Figure 14. Word cloud generated from textual physical description, where larger words indicate higher frequencies in the dataset text Figure 15. Human and machine evaluation results do not fully align."
        }
    ],
    "affiliations": [
        "AI Research",
        "Peng Cheng Laboratory",
        "Shenzhen Campus of Sun Yat-Sen University"
    ]
}