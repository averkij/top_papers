{
    "paper_title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "authors": [
        "Yuchao Gu",
        "Weijia Mao",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling."
        },
        {
            "title": "Start",
            "content": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction Yuchao Gu, Weijia Mao, Mike Zheng Shou* Show Lab, National University of Singapore https://farlongctx.github.io 5 2 0 2 5 ] . [ 1 5 2 3 9 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16 longer vision contexts. Furthermore, we propose long short-term context modeling, where high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both shortand long-video generation, providing simple yet effective baseline for video autoregressive modeling. 1. Introduction Advanced long-context autoregressive language models have demonstrated remarkable capabilities, enabling various applications that require test-time scaling, such as extended conversations [1], chain-of-thought reasoning [22, *Corresponding Author. Figure 1. Evaluation on Long Video Prediction. FAR effective exploits long video contexts and achieves accurate prediction. 42, 44], in-context learning [14], and retrieval-augmented generation [19]. However, video modeling has not achieved comparable progress. Recent video autoregressive modeling [32] directly adapts the paradigm of language models [8], where frames are factorized into discrete [16, 62] codes for next-token prediction (denoted as Token-AR). However, Token-AR still fails to achieve comparable quality to video diffusion transformers [7] due to the unidirectional modeling of visual tokens [18] and the irreparable information loss caused by vector quantization. On the other hand, video diffusion models [26, 27, 37, 55], which generate long videos using progressive sliding window, struggle to effectively use earlier context. In this paper, we introduce Frame AutoRegressive (FAR) model, specifically designed for video autoregressive modeling. FAR is trained using frame-wise flow matching objective with autoregressive contexts. Unlike TokenAR, which learn causal dependencies between discrete tokens, FAR captures causal dependencies between continuous frames while still allowing full attention modeling within each frame. However, as hybrid AR-Diffusion model [10, 31, 58], FAR also encounters common issue observed in such models, namely, the discrepancy in observed contexts between training and inference. During training, later frames are exposed only to noised context frames due to diffusion objective, whereas inference relies on clean context frames. Recent methods [30, 65] mitigate this issue by appending clean copy of the noised sequence during training, but this approach doubles the training cost. To address the discrepancy of observed context, we propose training FAR with stochastic clean context. During training, we randomly replace portion of noisy frames with clean frames and assign them unique timestep embedding beyond the diffusion schedule to indicate representation extraction from clean context. During inference, this special embedding guides the model to effectively utilize clean context frames. We demonstrate FAR with stochastic clean context achieve same training efficiency to video diffusion transformers while achieving better convergence, served as strong autoregressive video generation baseline. Building on FAR, we investigate long-context video modeling and explore two common settings, similar to those in language modeling: (1) test-time lengthy extrapolation and (2) long-sequence training. Unlike language modeling, long-context video modeling suffers from visual redundancy. On one hand, RoPE [51] exhibits weak temporal decay, leading to the accumulation of redundant visual context and degrading test-time extrapolation performance. On the other hand, training on long videos is computationally expensive, as vision tokens grow significantly faster than language tokens. To address these challenges, we propose to balance the locality and long-range dependency. For test-time temporal extrapolation, we introduce FlexRoPE, which incorporates controllable temporal decay into RoPE using linear bias. FlexRoPE is applied only at test time and remains compatible with models trained using RoPE. It enhances temporal locality and effectively reduces redundancy from distant contexts while still allowing the learned RoPE to model long-range dependencies. For long-video training, we introduce long short-term context modeling. Specifically, we maintain high-resolution short-term context window to ensure fine-grained temporal consistency, while using unlimited long-term context window with aggressive patchification to reduce redundant context tokens. This strategy enables efficient training on long video sequences with manageable token context length. Our contributions are summarized as follows: 1. We introduce FAR, an strong autoregressive video generation baseline, combined with stochastic clean context to bridge the training-inference gap in observed context. 2. Building on FAR, we introduce FlexRoPE to enable 16 test-time temporal extrapolation, and long short-term context modeling for efficient long-video training. 3. FAR achieves state-of-the-art performance in both shortand long-video generation. 2. Related Work 2.1. Video Generation Video Diffusion Models. Recent advances in video generation have led to the scaling of video diffusion transformers [7, 33, 61] for text-to-video generation, resulting in superior visual quality. Pretrained text-to-video models are subsequently fine-tuned to incorporate images as conditions for image-to-video generation [23, 59, 61]. The trained image-to-video models can be utilized for autoregressive long-video generation using sliding window [37, 55], but their ability to leverage visual context is limited by the sliding windows size. In this work, we show that FAR achieves better convergence than video diffusion transformers for short-video generation while naturally supporting variablelength visual context. Token Autoregressive Models. Video generation based on token autoregressive models (i.e., Token AR) aims to follow the successful paradigm of large language models. These models typically quantize continuous frames into discrete tokens [21, 62] and learn the causal dependencies between tokens using language models [28, 32]. While they achieve plausible performance, their generation quality remains inferior to that of video diffusion transformers due to information loss from vector quantization. Additionally, unidirectional visual token modeling may be suboptimal [18]. Subsequent studies have explored continuous tokens [34] without vector quantization but have not demonstrated their effectiveness in video generation. In this work, we show that FAR can learn causal dependencies from continuous frames and achieve better performance than Token AR in both shortand long-video modeling. Hybrid AR-Diffusion Models. To leverage the strengths of both continuous latent spaces and autoregressive modeling, recent studies [40, 58, 64] have explored hybrid ARDiffusion models. These models typically employ diffusion objective for image-level modeling with autoregressive contexts. Hybrid AR-Diffusion models are widely applicable to both visual [10, 31, 58] and language generation [5, 57]. Recent research has also applied it in framelevel autoregressive modeling [10, 31] for video generation. However, they suffer from training-inference discrepancy in the observed context. Some studies [30, 65] have attempted to mitigate this issue by maintaining clean copy of the noised sequence during training, but this approach doubles the training cost. Among these methods, FAR efficiently addresses the training-inference gap through the proposed stochastic clean context, demonstrating its superior performance in long-context video modeling. 2.2. Long-Context Language Modeling Test-time Lengthy Extrapolation. Lengthy extrapolation is an appealing characteristic of autoregressive models, alFigure 2. Illustration of FARs Training and Inference Pipeline. In short-video training, portion of frames is randomly replaced with clean context frames, marked with unique timestep embedding (e.g., -1) beyond the flow-matching scheduler. In long-video training, we adopt long short-term context modeling. long-term context window with aggressive patchification is adopted to reduce redundant vision tokens, while short-term context window is used to model fine-grained temporal consistency. Table 1. Model Variants of FAR. We follow the model size configurations of DiT [46] and SiT [38]. Models FAR-B FAR-M FAR-L FAR-XL FAR-B-Long FAR-M-Long #Layers Hidden Size MLP #Heads Params 12 12 24 28 12 12 768 1024 1024 1152 768 1024 3072 4096 4096 4608 3072 12 16 16 18 12 16 130M 230M 457M 674M 150M 280M lowing them to be trained on short sequences while performing inference on longer ones. However, extrapolation performance primarily depends on the characteristics of the position embedding. Two common relative position embeddings that support extrapolation are RoPE [51] and ALiBi [48]. RoPE encodes relative distance through dotproduct operations, while ALiBi achieves this using attention bias. Subsequent studies [6, 47] have further advanced RoPE to enhance extrapolation performance. In this work, we introduce FlexRoPE and demonstrate its superior performance compared to RoPE and ALiBi in temporal extrapolation for video modeling. Long Sequence Training. straightforward approach to improving long-context modeling performance is to directly train the model on longer sequences. Recent work in language modeling has explored efficient long-sequence finetuning with position interpolation [12, 13]. However, vision tokens scale much faster than language tokens as context increases. To address this, we introduce long short-term context modeling to eliminate visual redundancy in long-video training. 2.3. Long-Context Video Modeling Figure 3. Visualization of Attention Mask. FAR enables full attention within frame while maintaining causality at the frame level. In long-context training, we adopt aggressive patchification for long-term context frames to reduce tokens. which require the ability to exploit long-range context and memorize the observed environment. However, existing video diffusion transformers lack effective mechanism to utilize long-range context. Although early work [60] has explored long-context video prediction, it has been limited in visual quality and long-range consistency. In this work, we introduce FAR, scalable framework for both shortand long-context autoregressive video modeling. 3. Preliminary 3.1. Flow Matching Flow Matching [2, 35, 36] is simple alternative objective for training diffusion models. Rather than modeling the reverse process with stochastic differential equations, Flow Matching learns continuous vector field that deterministically conntect two distribution. Recent advancements in video generation models have enabled their use as interactive world simulators [9, 45, 53], Specifically, given data sample x0 pdata(x) and noise sample x1 (0, I), we construct continuous traFigure 4. Effect of Stochastic Clean Context. This technique eliminate training-inference gap in observed context. jectory connecting them via linear interpolation: [0, 1]. x(t) = (1 t)x0 + tx1, This formulation implies constant velocity: dx(t) dt = = x1 x0. (1) (2) To enable the model to learn the optimal transport between the data and noise distributions, we introduce learnable time-dependent velocity field vθ(x, t). During training, random time (0, 1) is sampled, and the model is optimized by minimizing the following objective: L(θ) = Ex0,x1,t (cid:104) vθ(x(t), t) v2(cid:105) . (3) 3.2. Autoregressive Models Autoregressive models are class of probabilistic models where each element in sequence is conditioned on its preceding elements, denote as context. Formally, given sequence of tokens (x1, x2, . . . , xn), an autoregressive model assumes that each token xi is generated based on its previous tokens (x1, x2, . . . , xi1). The generative process can be expressed as factorization of the joint probability: (cid:89) p(xix1, x2, . . . , xi1). (4) p(x1, x2, . . . , xn) = i=1 By modeling each token conditioned on its preceding tokens, autoregressive models naturally capture the sequential dependencies inherent in data. 4. FAR In this section, we first present the framework of FAR in Sec. 4.1. Then, we discuss the difficulties and solutions in training FAR in Sec. 4.2. In Sec. 4.3, we analyze the key design that enables FAR for long-context video modeling. 4.1. Framework Overview Architecture. As shown in Fig. 2 (a), FAR is built upon the diffusion transformer [38, 46]. We adopt the model configuration of DiT [46] and Latte [39], as listed in Tab. 1. The key architectural difference between FAR and video Figure 5. Comparison of FAR and video diffusion transformer. FAR achieves better convergence than video diffusion transformer in unconditional video generation on UCF-101. diffusion transformers (e.g., Latte [39]) lies in the attention mechanism. As shown in Fig. 3(a), for each frame, we apply causal attention at the frame level while maintaining full attention within each frame. We adopt this causal spatiotemporal attention for all layers, instead of the interleaved spatial and temporal attention used in Latte. In FAR, image generation and image-conditioned video generation are jointly learned thanks to the causal mask, whereas video diffusion transformer [39] requires additional image-video co-training. Basic Training Pipeline. The training pipeline of FAR is illustrated in Fig. 2 (a). Given video sequence X, we first employ pretrained VAE to compress it into the latent space RT HW , where , H, and denote the number of frames, height, and width of the latent features, respectively. Note that although we primarily adopt an image VAE in this work, FAR can also be trained with video VAE since our autoregressive unit is the latent frame. Following diffusion forcing [10], we independently sample timestep for each frame. We then interpolate between the clean latent and the sampled noise using Eq. (1) and apply the frame-wise flow matching objective in Eq. (3) for learning. The key difference between FAR and image flow matching lies in that we adopt causal spatiotemporal attention, allowing each frame to access previous context frames during denoising. 4.2. Short-Video Modeling Training-Inference Gap in Observed Context. As hybrid AR-diffusion model, FAR also encounters traininginference gap in the observed context. As illustrated in Fig. 2(a), each clean latent is fused with sampled noise for the flow matching objective, as defined in Eq. (1). Consequently, later frames can only access the noised version of previous frames during training. However, during inference, this leads to distribution shift when clean context frames is used. As shown in the example in Fig. 4(a), the traininginference gap in the observed context leads to distribution shift when inferring with clean context. Although adding Figure 6. Visualization and Comparison of Various Temporal Position Embeddings. The proposed FlexRoPE incorporates linear bias to induce controllable temporal decay at test time, enhancing extrapolation performance as context increases. In contrast, other methods degrade when the inference context exceeds the training window. mild noise to the context during inference can help mitigate this effect, it still causes low-level flickering, degrading the quality of the generated video. Recent works [30, 65] attempt to address this issue by maintaining clean copy of the noised sequence during training. However, this approach doubles the training costs. Our Solution: Stochastic Clean Context. To bridge the gap in observed context, we introduce stochastic clean context for training FAR. As illustrated in Fig. 2(a), we randomly replace portion of the noised frames with their corresponding clean context and assign them unique timestep embedding (e.g., -1) beyond the flow-matching timestep scheduler. These clean context frames are excluded from loss computation and are implicitly learned through later frames that use them as context. During inference, this unique timestep embedding guides the model to use clean context effectively. Training FAR with stochastic clean context does not add extra computation and does not conflict with different timestep sampling strategies during training (e.g., logitnormal sampling [17]). It effectively resolves the traininginference discrepancy, as exemplified in Fig. 4(b). FAR vs. Video Diffusion Transformer. FAR and video diffusion transformer differ only in their training schemes. FAR is trained with independent noise and causal attention, while the video diffusion transformer is trained with uniform noise and full attention. This raises an interesting question: Can FAR surpass video diffusion transformers? To explore this, we convert FAR to video diffusion transformer as baseline, denoted as Video DiT. We align the training settings to compare the two paradigms. As shown in Fig. 5, FAR achieves better convergence than the Video DiT, demonstrating its potential to become strong baseline for autoregressive video modeling. 4.3. Long-Context Video Modeling In this section, we discuss the challenge of long-context video modeling. We focus on two practical long-context settings, similar to those in language models: test-time temporal extrapolation (Sec. 4.3.1) and long-video training (Sec. 4.3.2). 4.3.1. Test-Time Temporal Extrapolation Weak Temporal Decay. An appealing characteristic of autoregressive models is their potential to be trained on short sequences while being tested on long sequences, enabling lengthy extrapolation at test time. This capability relies on effective positional embedding. Following 3D-RoPE [61] for video data, the spatial and temporal dimensions (i.e., height, width, and frame) are treated as independent 1DRoPE embeddings. Consequently, we keep RoPE positional embedding for height and width unchanged while focusing only on the temporal position embedding. In this study, we examine RoPE [51] and ALiBi [48], two common positional embedding methods for capturing temporal relative distances. From the visualization in Fig. 6(a), RoPE does Table 2. Quantitative Comparison of Conditional and Unconditional Video Generation on UCF-101. We follow the evaluation setup of Latte [39]. denotes FVD reported on 10,000 videos. Methods Type Params Double Train Cost Cond. Gen Uncond. Gen FVD2048 FVD2048 Resolution-128128 MAGVITv2-MLM [62] MAGVITv2-AR [62] TATS [20] Non-AR Token-AR Token-AR 307 840 331 FAR-L (Ours) Frame-AR 457 58 109 332 99 (57) Resolution-256256 LVDM [26] Latte [39] CogVideo [28] OmniTokenizer [56] ACDIT [30] MAGI [65] FAR-L (Ours) FAR-XL (Ours) Video-DiT Video-DiT Token-AR Token-AR Frame-AR Frame-AR Frame-AR Frame-AR 437 674 9.4 650 677 850 457 674 - - 626 191 111 - 113 108 - - 420 280 372 478 - - - 421 303 279 4.3.2. Long-Video Training Token Redundancy in Long Video. Visual data contains spatial redundancy, causing vision tokens to expand much faster than language tokens as context increases. For example, video sequence of 128 frames requires more than 8K tokens, with 64 tokens per frame, as illustrated in Fig. 7. As result, training and inference on long videos become computationally inefficient. Our Solution: Long Short-Term Context Modeling. To address the token redundancy in video, we introduce long short-term context modeling, which exploits the spatial and temporal locality in video data. As illustrated in Fig. 2(b), we maintain high-resolution short-term context window to learn fine-grained temporal consistency and low-resolution long-term context window, where we adopt aggressive patchification to reduce the number of context tokens. During training, given that the data has maximum sequence length of frames, we fix the short-term context window to frames and randomly sample the longterm context frames from the range [0, n]. The attention mask with long short-term context modeling is shown in Fig. 3(b), where the long-term context uses fewer tokens. As demonstrated in Fig. 7, this strategy ensures that increasing the vision context length maintains manageable token context length. To prevent interference between long-term and short-term contexts, we adopt separate projection layers for each context, inspired by MM-DiT [17]. This approach results in slightly larger parameter size during long-video training, as shown in Tab. 1. 5. Experiment 5.1. Implementation Details We follow the DiTs structure [46] to implement FAR. To compress video latents, we train series of image DCAE [11] on the corresponding dataset, resulting in 64 tokens per frame. All models are trained from scratch without image pretraining. We provide detailed settings in Tab. 7. Figure 7. Relation between Token Context Length and Vision Context Length. With the proposed long short-term context modeling, the token context length scales more slowly as the vision context length increases compared to uniform context modeling. not impose sufficient temporal decay, leading to accumulated redundant visual context. Similarly, ALiBi exhibits this issue in parts of attention heads with small slopes. To evaluate extrapolation performance, we gradually increase the number of context frames at test time and measure the resulting improvement in predictions. From Fig. 6(b), position interpolation of RoPE performs slightly better than position extrapolation. Additionally, we retrain the model using ALiBi as temporal position embedding. ALiBi applies linear decay based on temporal relative distance, with different decay rates assigned to each attention head. Our results suggest that ALiBi achieves slightly better extrapolation than RoPE due to its explicit temporal decay mechanism. However, all solutions exhibit performance degradation as vision context increases. Therefore, we aim to develop more effective temporal position embedding method to improve temporal extrapolation. Our Solution: FlexRoPE. To address this problem, we propose FlexRoPE, which explicitly controls temporal decay to suppress redundant visual context while still allowing the model to capture long-range dependencies using RoPE. The FlexRoPE is defined as: Attention(qi, kj) = Softmax used in training (cid:125)(cid:124) (cid:123) (cid:122) , RoP E(qi, kj) λ j (cid:125) (cid:123)(cid:122) (cid:124) FlexRoPE, used in inference (5) where the temporal decay is flexibly controlled by the slope λ, and and represent the temporal indices of the frame. We visualize FlexRoPE with λ = 0.2 in Fig. 6(a). FlexRoPE is inference-compatible with models trained using RoPE since it does not modify the dot-product computation but instead compensates for RoPEs temporal decay. We compare FlexRoPE with RoPE in Fig. 6(b). FlexRoPE effectively balances locality and long-range correspondence, leading to improved performance as the context frame increases, whereas RoPE interpolation and extrapolation exhibit poorer extrapolation performance. Table 3. Quantitative Comparison on Short Video Prediction. We follow the evaluation setup of MCVD [54] and ExtDM [63], where denotes the number of context frames and denotes the number of predicted frames. Methods Params = 4, = Methods Params SSIM PSNR LPIPS FVD = 2, = 14 = 2, = 28 SSIM PSNR LPIPS FVD SSIM PSNR LPIPS FVD RaMViD [29] LFDM [43] MCVD-cp [54] ExtDM-K2 [63] 235 108 565 119 0.639 0.627 0.658 0.754 21.37 20.92 21.82 23.89 0.090 0.098 0.088 0. FAR-B (Ours) 130 0.818 (a) Evaluation on UCF-101 (6464) 25.64 0.037 396.7 698.2 468.1 394. 194.1 RaMViD [29] LFDM [43] VIDM [41] MCVD-cp [54] ExtDM-K4 [63] FAR-B (Ours) 235 108 194 565 121 130 0.758 0.770 0.763 0.838 0. 17.55 17.45 16.97 19.10 20.04 0.085 0.084 0.080 0.075 0.053 166.5 167.6 131.7 87.8 81.6 0.691 0.730 0.728 0.797 0.814 0.849 20. 0.038 0.819 (b) Evaluation on BAIR (6464) 99.3 16.51 16.68 16.20 17.70 18.74 19.40 0.109 0.106 0.096 0.078 0.069 0. 238.7 276.8 194.6 119.0 102.8 144.3 Table 4. Quantitative Comparison on Long-Context Video Prediction. We follow the evaluation setup of TECO [60], where denotes the number of context frames and denotes the number of predicted frames. Methods Params = 144, = = 36, = 264 SSIM PSNR LPIPS FVD Methods Params = 144, = = 36, = 264 SSIM PSNR LPIPS FVD FitVid [3] CW-VAE [49] Perceiver AR [25] Latent FDM [24] TECO [60] FAR-B-Long (Ours) 165 111 30 31 169 0.356 0.372 0.304 0.588 0.703 12.0 12.6 11.2 17.8 21.9 0.491 0.465 0.487 0.222 0.157 150 0.104 (a) Evaluation on DMLab (6464) 0. 22.3 176 125 96 181 48 64 FitVid [3] CW-VAE [49] Perceiver AR [25] Latent FDM [24] TECO [60] 176 140 166 33 274 FAR-M-Long (Ours) 280 0.343 0.338 0.323 0.349 0.381 0.448 13.0 13.4 13.2 13.4 15.4 16.9 0.519 0.441 0.441 0.429 0. 0.251 (b) Evaluation on Minecraft (128128) 956 397 76 167 116 39 5.2. Quantitative Comparison 5.2.1. Video Generation Dataset and Evaluation Setting. We benchmark both unconditional and conditional video generation on the UCF101 dataset [50], which consists of approximately 13,000 videos. Following Latte [39], we use the entire dataset for training. For evaluation, we randomly sample 2,048 videos to compute FVD [52] against the ground-truth videos. For conditional video generation, we set the guidance scale to 2.0 during inference. Main Results. From the results listed in Tab. 2, we achieve state-of-the-art performance in both unconditional and conditional video generation. Specifically, Latte [39] is based on video diffusion transformer, while OmniTokenizer [56] is based on Token AR. Our method significantly outperforms both. Furthermore, compared to recent frameautoregressive models [30, 65], which require twice the training cost, FAR achieves superior performance without any additional training cost. 5.2.2. Short-Video Prediction Dataset and Evaluation Settings. We evaluate FAR on the UCF-101 [50] and BAIR [15] datasets, following the evaluation settings in MCVD [54] and ExtDM [63]. We randomly sample 256 videos based on provided context frames, each with 100 different trajectories, and select the best trajectory to compute pixel-wise metrics. For FVD, we report the average over all trajectories. Main Results. We summarize the results in Tab. 3. Unlike previous works such as MCVD [54] and ExtDM [63], which introduce complex multi-scale fusion strategies and optical flow, FAR achieves superior results on both datasets without requiring additional design. 5.2.3. Long-Video Prediction Dataset and Evaluation Settings. We benchmark longcontext video modeling results on action-conditioned video prediction using the Minecraft and DMLab datasets [60]. The Minecraft dataset contains approximately 200K videos, while the DMLab dataset contains about 40K videos. Each video consists of 300 frames with action annotations. We follow the evaluation setup in TECO [60], which uses 144 observed context frames to predict 156 future frames and compute pixel metrics. Additionally, we compute FVD on 264 generated frames based on 36 context frames. Main Results. We summarize the results in Tab. 4. The previous work, TECO [60], adopts aggressive downscaling for all frames to reduce tokens for temporal modeling, creating trade-off between training efficiency and prediction accuracy. In contrast, FAR employs long short-term context modeling, effectively achieving the lowest prediction error (i.e., LPIPS) without prohibitive computation cost. 5.3. Qualitative Comparison We present qualitative comparison of long-video prediction in Fig. 8. Compared to previous methods, FAR effectively utilizes the observed context and generates predictions that most closely resemble the ground truth, demonstrating its ability to leverage long-range context. 5.4. Ablation Study Stochastic Clean Context. We have visualized the effectiveness of stochastic clean context in Fig. 4. Based on the quantitative evaluation of video prediction in Tab. 5, FAR with stochastic clean context achieves significantly improved performance. Figure 8. Qualitative Comparison of Long-Context Video Prediction on DMLab. FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods. Table 5. Ablation Study of Stochastic Clean Context on UCF101. Stochastic clean context mitigates the training-inference discrepancy in observed context, leading to improved performance. Methods = 1, = 15 SSIM PSNR LPIPS FVD FAR w/o. Stochastic Clean Context FAR w/. Stochastic Clean Context 0.540 0.596 16.42 18.46 0.211 0. 399 347 Table 6. Ablation Study on the Resolution of Long-Term Context. The speed is averaged over the generation of 300 frames. Context Resolution SSIM PSNR LPIPS FVD Speed (fps) 1 1 2 2 4 0.411 0.423 0.433 15.25 15.84 16.26 0.312 0.291 0.276 40 40 37 2.94 2.88 1.42 Figure 9. Ablation Study of the Short-Term Context Window Size. Performance saturates as the window size increases. Long-Term Context Resolution. We investigate the impact of long-term context resolution on prediction accuracy and inference speed. As the context resolution increases, pixel-level metrics improve; however, the overall video quality remains similar. Nonetheless, inference speed significantly degrades at higher context resolutions due to the increased number of tokens involved in computation. Therefore, we select 22 resolution for the long-term context as balance between computational efficiency and long-term context performance. Short-Term Context Window Size. We evaluate the impact of the short-term context window size on performance. As shown in Fig. 9, video quality (FVD) quickly saturates as the short-term context window size increases, while pixel-level metrics continue to improve but also approach saturation at window size of 16. Therefore, we set the short-term context window size to 16 by default. 6. Conclusion In this paper, we introduce FAR for autoregressive video modeling. FAR enables learning causal dependency of continuous frames and demonstrates better convergence than video diffusion transformers. Building upon FAR, we explore its application in long-context video modeling. We identify visual redundancy as key challenge, leading to poor temporal extrapolation performance at test time and inefficient long-video training. To address these issues, we propose balancing locality and long-range dependency. Specifically, we introduce FlexRoPE to enable 16 temporal extrapolation at test time and incorporate long short-term context modeling for efficient long-video training. FAR achieves state-of-the-art performance on both shortand long-video modeling, highlighting its potential as new foundation model for video generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 3 [3] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2021. 7 [4] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2286122872, 2024. 12 [5] Loıc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta Costa-juss`a, David Dale, et al. Large concept models: Language modeling in sentence representation space. arXiv e-prints, pages arXiv2412, 2024. 2 [6] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 2 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 1 [9] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 3 [10] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2025. 1, 2, 4 [11] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 6, 13 [12] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. 3 [13] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient finetuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. 3 [14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [15] Frederik Ebert, Chelsea Finn, Alex Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. CoRL, 12(16):23, 2017. 7 [16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 1 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 5, 6 [18] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 1, 2 [19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2, 2023. 1 [20] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In European Conference on Computer Vision, pages 102118. Springer, 2022. 6 [21] Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives of vectorquantized tokenizers for image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76317640, 2024. [22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [24] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. Advances in Neural Information Processing Systems, 35:2795327965, 2022. 7 [25] Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. General-purpose, long-context autoregressive modeling with perceiver ar. In International Conference on Machine Learning, pages 85358558. PMLR, 2022. 7 [26] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 1, [27] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 1 [28] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, 6 [29] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022. 7 [30] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. 2, 5, 6, 7 [31] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 1, 2 [32] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 1, 2 [33] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [34] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2025. [35] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [37] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. 1, 2 [38] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 3, 4 [39] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 4, 6, 7, 13 [40] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 2 [41] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 91179125, 2023. [42] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 1 [43] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generIn Proceedings of ation with latent flow diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. 7 [44] OpenAI. Learning to reason with llms, 2024. 1 [45] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model. 2024. 3 [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3, 4, 6 [47] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 3 [48] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 3, 5, 12 [49] Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational autoencoders. Advances in Neural Information Processing Systems, 34:2924629257, 2021. 7 [50] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 7 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2, 3, 5, [52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 7 onel Ni, et al. Taming teacher forcing for masked autoregressive video generation. arXiv preprint arXiv:2501.12389, 2025. 2, 5, 6, 7 [53] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 3 [54] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems, 35:2337123385, 2022. 7 [55] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 1, 2 [56] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. 6, [57] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. 2 [58] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2 [59] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2024. 2 [60] Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel. Temporally consistent transformers for video generation. In International Conference on Machine Learning, pages 3906239098. PMLR, 2023. 3, 7, 13 [61] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 5 [62] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1, 2, [63] Zhicheng Zhang, Junyao Hu, Wentao Cheng, Danda Paudel, and Jufeng Yang. Extdm: Distribution extrapolation difIn Proceedings of the fusion model for video prediction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931019320, 2024. 7, 13 [64] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2 [65] Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Ligeneration datasets. Additionally, restricted by the available datasets, we only experiment with FAR on up to 300 frames (about 20 seconds), not fully investigating its ability on minute-level videos. 7.4.2. Future Work One future direction is to scale up FAR to benchmark it against video diffusion transformers on large-scale text-tovideo generation tasks. Additionally, we plan to simulate longer video dataset (minute-level) for better evaluating the models long-context ability. Finally, it would be interesting to explore whether FAR can adapt to sequential vision modeling beyond videos, for example, the image sequences as demonstrated in LVM [4]. 7. Appendix 7.1. Experimental Settings As shown in Tab. 7, we list the detailed training and evaluation configurations of FAR. For the ablation study in this paper, we halve the training iterations while keeping other settings the same. 7.2. Qualitative Comparison We provide additional visualization of long-video prediction results on DMLab and Minecraft in Fig. 10 and Fig. 11. From the results, FAR better exploits the provided context and provides more consistent results in later predictions compared to previous works. 7.3. Ablation Study of FlexRoPE We compare different position embeddings on 16 temporal extrapolation. We focus on two settings: The first is unique in video generation, where we directly unroll 16 longer sequence from 1 context frame. Second, we follow the long-context language model [48], gradually adding more context frames and comparing the last 16-frame predictions. 16 Extrapolation: = 1, = 255. In this evaluation, we directly generate videos that are 16 longer than the training sequence length (i.e., 16 frames), using only the first frame as condition. As shown in Fig. 12, while RoPE extrapolation can adapt to periodic motion extrapolation, our proposed FlexRoPE achieves superior results in both periodic and non-periodic extrapolation. 16 Extrapolation: = 240, = 16. Following the common practice to evaluate long-context ability in language models, we collect 256 frames, use different context frames [0, 240], and allow the model to infer the last 16 frames to test performance. The quantitative results are demonstrated in Fig. 6 in the main paper. In Fig. 13, we visualize the 16 temporal extrapolation inference results. We can see that RoPE [51] (PE, position extrapolation) at test time results in the worst performance, accumulating redundant context and failing to extrapolate. Meanwhile, RoPE (PI, positional interpolation) breaks the learned video speed, resulting in poor motion. Although ALiBi [48] performs better than RoPE (PI and PE), it still influences the learned motion distribution and falls far from the GT. Compared to these methods, FlexRoPE achieves the best temporal extrapolation results. 7.4. Limitations and Future Work 7.4.1. Limitations The primary limitation lies in the lack of scaled-up experiments. Although FAR demonstrates great potential, we still lack large-scale training on million-level text-to-video Table 7. Experimental Configurations of FAR. We follow the evaluation settings from Latte [39], MCVD [63], and TECO [60]. Hyperparameters Short-Video Generation Short-Video Prediction Long-Video Prediction Cond. UCF-101 Uncond. UCF-101 BAIR UCF-101 Minecraft DMLab Dataset Configuration Resolution Total Training Samples 256/128 13, 256/128 13,320 64 43,264 64 9,624 128 194,051 64 39,375 Training Configuration Batch Size Latent Size Training Sequence Length LR LR Schedule Warmup Steps Total Training Steps Stochastic Clean Context Short-Term Context Window Long-Term Context Resolution 32 88 (DC-AE [11]) 16 1 104 constant - 400K 0.1 16 - 32 88 (DC-AE [11]) 16 1 104 constant - 400K 0.1 16 - 32 88 (DC-AE [11]) 32 1 104 constant - 200K 0.1 32 - 32 88 (DC-AE [11]) 16 1 104 constant - 200K 0.1 16 - 32 88 (DC-AE [11]) 300 1 104 constant 10K 1M 0.1 16 2 32 88 (DC-AE [11]) 300 1 104 constant 10K 1M 0.1 16 22 Samples Guidance Scale Reference Work 42048 2.0 Latte [39] 42048 - Latte [39] 100256 - MCVD [63] 100256 - MCVD [63] 4256 1.5 TECO [60] 4256 1.5 TECO [60] Evaluation Configuration Figure 10. Qualitative Comparison of Long-Context Video Prediction on DMLab. FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods. Figure 11. Qualitative Comparison of Long-Context Video Prediction on Minecraft. FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods. Figure 12. Comparison of Position Embeddings for 16 Temporal Extrapolation. We leverage the model (trained on 16 frames) to infer 255 future frames based on the provided 1 context frames. PE denotes position extrapolation. We encourage readers to click and play the video clips in this figure using Adobe Acrobat. Figure 13. Comparison of Position Embeddings for 16 Temporal Extrapolation. We leverage the model (trained on 16 frames) to infer 16 future frames based on the provided 240 context frames. PI denotes position interpolation, and PE denotes position extrapolation. We encourage readers to click and play the video clips in this figure using Adobe Acrobat."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}