{
    "paper_title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": [
        "Ruining Li",
        "Yuxin Yao",
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Joan Lasenby",
        "Shangzhe Wu",
        "Andrea Vedaldi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 9 7 1 1 . 2 1 5 2 : r PARTICULATE: Feed-Forward 3D Object Articulation Ruining Li1* Yuxin Yao2* Chuanxia Zheng1,3 Christian Rupprecht1 Joan Lasenby2 Shangzhe Wu2 Andrea Vedaldi1 1University of Oxford 2University of Cambridge https://ruiningli.com/particulate 3Nanyang Technological University Figure 1. Articulated 3D objects predicted by PARTICULATE. Our model infers articulated structures directly from static 3D meshes in single feed-forward pass, enabling fast inference across diverse objects, including those synthesized by 3D generative models."
        },
        {
            "title": "Abstract",
            "content": "tive results show that PARTICULATE significantly outperforms state-of-the-art approaches. We present PARTICULATE, feed-forward approach that, given single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is transformer network, Part Articulation Transformer, which processes point cloud of the input mesh using flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on diverse collection of articulated 3D assets from public datasets. During inference, PARTICULATE lifts the networks feed-forward prediction to the input mesh, yielding fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. PARTICULATE can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualita- *Equal contribution. Equal advising. 1. Introduction Our everyday environment is populated with objects whose functionality arises not only from their shape, but also from the motion of multiple interconnected parts. Cabinets, for example, have doors and drawers that open through rotational and translational motion, constrained by hinges and sliding tracks. Humans can effortlessly perceive the articulated structure of such objects. Developing similar capabilities for machines is crucial for robots that must manipulate these articulated objects in daily tasks [21, 44, 59], and can simplify the creation of interactive digital twins for gaming and simulation [16, 56]. In this paper, we consider the problem of estimating the articulated structure from single static 3D mesh. Prior work has attempted to model the rich variety of everyday articulated objects via procedural generation [26, 41]. However, scaling such rule-based approaches for the long tail of real-world objects remains extremely challenging. We posit that learning-based approaches that capture articulation priors from large collections of 3D assets offer greater potential for generality. 1 Several authors have already proposed learning-based approaches for closely related tasks. [3133, 54] focus on learning part segmentation on 3D point clouds. These methods, however, primarily predict semantic part segmentation on static objects, without modeling the underlying articulations between parts. Other learning-based methods [11, 20, 29, 30] aim to generate 3D articulated objects directly. Despite great promise, these models are typically trained on only few object categories, assume key attributes such as the kinematic structure are known priori, and often rely on part retrieval to assemble fully articulated 3D assets, which limits their ability to produce accurate and diverse articulated objects. Seeking to address these limitations, we introduce PARTICULATE, unified learning-based framework that takes an input 3D mesh of an everyday object and outputs the underlying articulated structure. PARTICULATE predicts full set of articulation attributes, including articulated 3D part segmentation, kinematic structure, and parameters of articulated motion. It does so in single forward pass, taking only seconds. In particular, our design of tasking PARTICULATE to analyze existing 3D assets rather than synthesizing them from scratch is motivated by the rapid advances in 3D generative models [7, 4850, 61]: by focusing on the orthogonal task of articulation estimation, PARTICULATE can leverage increasingly high-fidelity 3D generators to enable one-stop creation of articulated 3D objects with realistic geometry and appearance  (Fig. 1)  . The resulting assets can be seamlessly imported into physics engines for simulation purposes. At the core of PARTICULATE is an end-to-end trained Part Articulation Transformer, designed to be flexible and scalable. To this end, the network operates on point clouds, which is an explicit and highly structured representation of the input mesh and is applicable to virtually all 3D shapes. It consists of standard attention blocks paired with multiple decoder heads to predict different articulation attributes. This flexible design allows us to train this network on diverse collection of articulated 3D assets from public datasets, including PartNet-Mobility [59] and GRScenes [52]. To further enhance generalization to novel objects, we augment the raw point cloud with 3D semantic part features obtained from PartField [32] as input to the network. The resulting model predicts the articulated structure of input meshes of new objects in arbitrary articulated poses, including those generated by off-the-shelf 3D generators (e.g., [50]), in feed-forward manner. To evaluate our model, we introduce new challenging benchmark dataset of 220 high-quality 3D assets with accurate articulation annotations, crafted by Lightwheel under the CC-BY-NC license [27]. We also establish comprehensive evaluation protocol for the 3D articulation estimation task, with new metrics that more faithfully reflect preMethod (cid:0) (cid:130) (cid:18) 3D 3D (cid:244) (cid:18) $ 1+ Input Output Inference time ()o ()o ()o ()o ()o ()o PARIS [28] NAP [20] MeshArt [11] ArtFormer [46] ArtiLatent [5] CAGE [29] SINGAPO [30] ArticulateAnything [19] GEOPARD [13] DreamArt [35] FreeArt3D [4] Kinematify [53] Articulate AnyMesh [40] PARTICULATE (ours) ()* ()o N/A N/A N/A N/A 30 sec ()* ()* 10 sec N/A ()* 10 min N/A N/A 10 min 20 min 15 min 10 sec Table 1. Related work overview on partly-rigid articulated object modeling. We summarize existing methods required input and output, and their inference time for single object. Inputs include (cid:0): text, (cid:130): image or multi-view images, (cid:18): kinematic chain, 3D: 3D geometry (e.g., mesh or point cloud). Outputs include : 3D bounding box, 3D: 3D geometry (e.g., mesh or point cloud), (cid:244): part segmentation, (cid:18): kinematic chain, $: motion axis, : motion range, 1+: support multi-joint articulation. SINGAPO [30], NAP [20], and CAGE [29] perform part-based retrieval from 3D part database to obtain articulated 3D objects (*). MeshArt [11], ArtiLatent [5], Articulate-Anything [19], and Kinematify [53] all perform unconditional generation and optionally take additional inputs for conditional generation such as texts, images, and 3D point clouds (o). GEOPARD [13] further requires segmented 3D point clouds as input (). N/A indicates the inference time is not reported in the original paper. diction quality than those used in prior work [30, 40]. In summary, our key contributions are: (1) PARTICULATE, feed-forward model that, from single static 3D mesh of everyday objects, infers the full articulated strucincluding articulated part segmentation, kinematic ture, structure, and motion parameters, that can be directly exported to physics simulators. (2) We show that PARTICULATE generalizes well to unseen objects, including AIgenerated 3D models. (3) We release challenging benchmark for articulation estimation. Experiments show PARTICULATEs significant gains via metrics that are more consistent with human preferences. 2. Related Work 3D part segmentation. Recent 3D part segmentation methods increasingly target zero-shot settings [1, 31, 47, 62, 66]. Most approaches leverage 2D foundation models such as SAM [18] and GLIP [22] by rendering 3D assets into multiple views and lifting the resulting 2D masks back to 3D. However, these lift-from-2D approaches, which infer 3D structure from visible surface masks in rendered views, are inherently limited by view coverage and struggle to recover occluded or internal parts. 2 To alleviate these issues, recent work learns native 3D part segmentation models [32, 36, 37, 63]. These models are trained on large datasets of 3D objects with (pseudo) ground-truth part segmentation, and hence acquire openworld capabilities. However, they mostly predict semantic parts, which often are not meaningful for articulation. By contrast, our PARTICULATE jointly predicts articulated parts with their kinematic structure and motion constraints, producing fully articulated 3D objects from static meshes. Articulated object modeling. Prior work has explored reconstructing articulated objects from multi-view images capturing different articulation states. popular paradigm is per-instance optimization using neural radiance fields (NeRFs [38]) [28, 39, 45, 55, 56, 58] or Gaussian splatting (GS [17]) [34, 57]. While effective in capturing objectspecific articulation, these methods are slow and rely on densely sampled, posed images across multiple articulation states which are hard to acquire in practice. To improve scalability, authors have developed generative priors for articulated objects. [11, 20, 29, 30, 46] train diffusion [14] or autoregressive models on 3D datasets with articulation annotations [12, 59] to generate articulated objects. Despite their promise, these models are usually trained on limited set of object categories, assume prior knowledge of key attributes such as kinematic structure, and often depend on part retrieval to assemble articulated assets, limiting the accuracy and diversity of the resulting objects. More recently, motivated by the success of Internetscale foundation models, authors have proposed to leverage them for articulation modeling. These methods usually are training-free or require only light fine-tuning. For instance, DragAPart [23] and DreamArt [35] fine-tune pretrained 2D generators to synthesize articulation videos [25], which are then used to optimize the articulation attributes. FreeArt3D [4], conditioned on images across different articulation states, generates articulated objects by probing pre-trained 3D generator [60]. Articulate-Anything [19] and Articulate AnyMesh [40] prompt vision language models (VLMs) to reason about part articulation. Despite their impressive generalization across many categories, these methods suffer from several limitations, including slow perobject inference, difficulty handling internal or subtle parts, and limited multi-joint support. In contrast, we propose data-driven approach that utilizes flexible and scalable network to infer all articulation attributes in single feed-forward pass, enabling recovery of articulated objects in seconds rather than hours and achieving superior performance across diverse categories. 3. Method static 3D mesh of everyday objects. We formulate the problem in Sec. 3.1, followed by details of our network architecture in Sec. 3.2 and the decoder heads in Sec. 3.3. We then describe training and inference details in Secs. 3.4 and 3.5. 3.1. Problem Definition Given 3D mesh = (V, ) with vertices and faces , our goal is to predict its (partly rigid) articulated structure A, which specifies (1) the segmentation of the mesh faces into multiple articulated parts; (2) the kinematic tree of part hierarchy; and (3) the constrained rigid motion of each part. Formally, is 4-tuple (P, S, K, ), where is the number of parts. : [F ] [P ] assigns each face to part, segmenting the mesh into articulated parts1. The kinematic tree is collection of edges [P ] [P ]. While kinematic trees are undirected by default, for most everyday objects, it is possible and convenient to define base part (e.g., the casing for microwave). We can then orient the edges from the base part outward. With this, (p, c) indicates joint connecting part with its parent part p. We assume single base part for each object, and hence forms an arborescence rooted at b. each relative to its parent. where specifies one aspect of The motion constraint is parameterized by (Mtp, Mpd, Mra, Mpr, Mrr), component the rigid motion Specifically, of each part Mtp [P ] {fixed, pri, rev, both} tells the type : of motion: fixed, prismatic (allowing linear sliding), revolute (allowing rotation around single axis), or [P ] S2 tells the direction of each part both. Mpd : ps prismatic motion2 the prismatic directions). Mra : [P ] S2 R3 specifies the rotation axis of each parts revolute motion (i.e., the revolute axes). Both Mpd and Mra are defined in the mesh Ms coordinate system. Mpr : [P ] R2 and Mrr : [P ] R2 define the range [lmin, lmax] and [θmin, θmax] of each parts prismatic motion and revolute motion respectively (i.e., the prismatic ranges and revolute ranges), with respect to the mesh Ms articulation state. (i.e., Our parameterization (P, S, K, ) of captures the full set of its articulation attributes, and can be easily converted to the Universal Robot Description Format (URDF), allowing PARTICULATEs predictions to be used in physics simulators. 3.2. Part Articulation Transformer Unlike per-shape optimization methods [35, 40, 53], which rely on handcrafted heuristics or distil 2D priors from vision-language foundation models (VLMs), we train We introduce PARTICULATE, feed-forward approach that predicts the articulated structure, including the articulated parts, kinematic tree, and motion constraints, from single 1Note [n] := {1, 2, . . . , n} is the set of positive integers up to n. 2Strictly speaking, Mpd is only defined on the parts with prismatic motion, i.e., {p [P ] : Mtp(p) {pri, both}}. We annotate its domain as [P ] for simplicity (similar with Mra, Mpr and Mrr below). 3 Figure 2. Overview of PARTICULATE. Our model consumes point cloud sampled from the input mesh and predicts its articulated structure. The backbone consists of standard attention blocks that operate on the point vectors and set of learnable part query vectors. The resulting latent vectors are then processed by specialized decoder heads dedicated for different components of the articulated structure. The entire model is trained end-to-end in fully supervised manner. At inference time, PARTICULATE projects the point-level segmentation predictions back onto the original mesh to reconstruct complete articulated 3D model. feed-forward network fθ, dubbed Part Articulation Transformer, on repository of diverse articulated 3D assets to predict articulated structure = fθ(M) from an input mesh in an end-to-end fashion. This approach offers several advantages: (a) fast inference, and (b) ability to deal with subtle or internal parts for which VLM-based cues are often unreliable. We design fθ to be flexible and scalable, so that we can train on all articulated objects from public datasets spanning multiple categories to improve generalizability, in contrast to prior work [29, 30] which only handles few similar categories. To this end, following recent works on 3D deep learning on meshes [32, 36, 64], our model operates on point cloud = {pi R3}N i=1. Comparing to other mesh representations, point cloud is explicit (so the ground-truth part affiliation of each point is available), highly structured, and applicable to all 3D meshes. fθ processes P, together with set of learnable part queries, into latent point and part vectors with multiple attention blocks. The latent vectors are later consumed by the decoder heads (Sec. 3.3) to predict the articulation attributes S, and . The overall architecture is illustrated in Fig. 2. Inputs. For each point pi P, we also provide fθ with its associated surface normal ni R3 and feature vector fi Rd obtained using PartField [32], which augments the raw point coordinates with richer geometric and semantic information. In particular, PartField is 3D feature field trained to match 2D semantic parts, which captures global shape context and offers strong semantic part priors beneficial to our task. We use separate MLPs to map these perpoint inputs pi, ni and fi into the model working dimension and take their sum as each points latent vector pi RD. Part queries. Since the number of articulated parts in M, , is unknown priori, inspired by [3], we initialize set of Pmax learnable part queries = {qj RD}Pmax j=1 , where Pmax is set to be much larger than in typical mesh M. Attention blocks. The backbone of fθ follows standard transformer [51] architecture. Inspired by [2], we apply sequence of self-attention and cross-attention modules between the latent point vectors {pi}N i=1 and the part queries {qj}Pmax j=1 . Specifically, fθ consists of attention blocks, each with alternating modules of (a) query self-attention; (b) query to point cloud cross-attention; and (c) point cloud to query cross-attention. We do not apply self-attention across point latent vectors {pi}N i=1 to keep modest memory footprint (as Pmax), which enables us to infer with very dense point clouds at test time. The attention blocks output the processed latent point vectors {pi}N and latent part vectors {qj}Pmax j=1 . i=1 3.3. Decoder Heads i=1 and {qj}Pmax We use several prediction heads to decode the articulation attributes, including the part segmentation, kinematic structure and parameters of part motion constraints, from the obtained latent vectors {pi}N j=1 . Note that the predictions of these heads would have part dimension Pmax instead of . We will later show how to match Pmax query parts with ground-truth parts for supervision during training in Sec. 3.4 and predict articulated structures with < Pmax parts during inference in Sec. 3.5. Next, we describe these decoder heads in detail. Part segmentation. matrix RN Pmax using an MLP hS: Si,j = hS(pi, qj). fθ predicts the point mask as logit (1) Kinematic tree. Another MLP hK takes pair of latent part vectors (qi, qj) and outputs the log-likelihood of query part being the parent of query part j, forming soft version of the kinematic trees adjacency matrix RPmaxPmax: Ki,j := log P[i is the parent of j] = hK(qi, qj). (2) 4 Motion types, motion ranges & prismatic directions. Separate MLPs htp, hpr, hrr and hpd take individual latent part vectors qi to predict per-part-query motion type, prismatic range, revolute range and prismatic direction respectively: Mi Mi Mi Mi tp = htp(qi) R4, pr = hpr(qi) R2, rr = hrr(qi) R2, pd = hpd(qi)/hpd(qi)2 S2, (3) (4) (5) (6) ra := (di where Mi tp R4 is the logits of query part being each of the four motion types and hpd(qi) R3 is normalized to unit direction vector. Over-parameterization of revolute axes. While we could similarly assign another MLP to predict query parts revolute axes Mi ra) S2 R3 (e.g., as Plücker coordinates following [20]), where di ra denotes the direction and xi ra fixed point on the axis, we empirically find that this approach often predicts slightly shifted axes3. To resolve this, we propose to over-parameterize the location of each revolute axis. We use an MLP hcp to predict, for every point pj that belongs to the query part i, the location xi on the axis closest to point pj, taking the latent point vector pj and the latent part vector qi as input4. Formally, ra, xi xi := arg min on the revolute axis Mi ra pj2 = hcp(pj, qi) (7) for every pair (i [Pmax], [N ]) where point pj is affiliated to query part (refer to Secs. 3.4 and 3.5 for details on how to obtain this affiliation during training and inference). We assign an additional MLP hrd to predict (only) the direction of each revolute axis as usual: di ra = hrd(qi)/hrd(qi)2. At test time, we can recover more accurate estimate of Mi ra S2 R3 by aggregating xi ra (see Sec. 3.5). and di (8) 3.4. Training The network fθ is trained end-to-end on public datasets of articulated 3D objects. For each training instance, we sample point cloud and obtain the ground-truth point-wise part segmentation {0, 1}N , kinematic tree {0, 1}P and motion constraints = (Mtp {0, 1}P 4, Mpd RP 3, Mpr RP 2, Mrr RP 2, dra RP 3, RN 3) as direct supervision for fθ. 3We hypothesize that this is due to overfitting when we train fθ on modest-sized datasets to directly regress this low-dimensional axis representation. Only the axis location (i.e., xi ra), suffers from overfitting. This is because for our task, most motion axes are axisaligned (both during training and at test time), so mode-collapsing looks fine at test time. ra), not direction (i.e., di 4This is analogous, in spirit, to [65]s camera parameterization. 5 Matching predicted parts with ground-truth. To properly supervise fθs per-part-query predictions (of dimension Pmax), we assign unique part query to each ground-truth part [P ]. Following [3], we use the Hungarian algorithm to find the optimal assignment ˆπ, where each ground-truth part [P ] is matched with part query ˆπ(i) [Pmax], by maximizing the matching utility: ˆπ = arg max π (cid:88) (cid:88) i=1 j=1 Sj,i log exp(Sj,π(i)) k=1 exp(Sj,k) (cid:80)Pmax . (9) We compute ˆπ on the fly during training. Note that ˆπ also determines each point pjs corresponding latent part query vector qˆπ(i) (where Sj,i = 1) for input to the MLP hcp. Training losses. We train the network fθ end-to-end using multi-task loss: = LS + LK + LM , (10) where LM = LMtp + LMpr + 0.1LMrr + LMpd + Ldra + Lxra is the combined loss for motion constraints. LS = CE(S, S) is the cross-entropy loss for the part segmentation, where RN is obtained by column-wise permuting according to ˆπ (i.e., j,i = Sj,ˆπ(i), and analogously for the starred notations below). LK = BCE( K, K) is the binary cross-entropy loss for the kinematic tree. The terms in LM supervise the motion parameters, comparing each part querys prediction with its matched ground truth: LMtp = CE( pr Mpr1, LMrr = radra1, and Lxra = x1. rrMrr1, LMpd = pdMpd1, Ldra = tp, Mtp), LMpr = 3.5. Inference We now describe how PARTICULATE infers the 3D articulated structure from static mesh M. Feed-forward prediction. Given the input mesh = (V, ), we first sample point cloud P, ensuring that each face has at least one point sampled on it. We then feed into fθ to obtain the per-point part affiliation si = Si,j for each point i. Note that while each arg maxj[Pmax] si [Pmax], the number of present articulated parts = {si : [N ]} in is often much smaller than Pmax. We obtain the face-level part segmentation by assigning to each face the majority part affiliation among its sampled points. We denote the part queries which correspond to an actual part of as QA = {S(f ) : [F ]} [Pmax]. Next, we compute the kinematic tree from fθs prediction RPmaxPmax, where Ki,j represents the log likelihood of part query being the parent of part query j. To do so, we run Edmonds algorithm to extract the maximum spanning arborescence (MSA) of K. The induced arborescence of this MSA on the present parts QA forms K. rr tp pr and MQA We can directly read off the network predictions MQA , MQA pd , MQA to get the motion types Mtp, prismatic directions Mpd, prismatic ranges Mpr and revolute ranges Mrr of the present parts QA. For the revolute axes, we first infer their directions as dQA . Then, for each point pj P, we use the MLP hcp to predict the closest point on the axis of its part sj, so that each point effectively votes for the axis location. We take the median of these votes as the axis location. Formally, Mra(i) S2 R3 is computed as: ra Mra(i) = (cid:16)di (cid:17) ra, median({hcp(pj, qi) : sj = i}) (11) for QA. Together, the inferred tuple (S, K, ) where = (Mtp, Mpd, Mra, Mpr, Mrr) defines the 3D articulated structure A. Refinement with connected components. Many handcrafted 3D assets have well-defined connected components, which we can leverage to refine the segmentation. For such input meshes, after obtaining the preliminary per-face part segmentation S, we ensure that all faces within the same connected component are assigned to the same part, selected as the one with the largest surface coverage in C. 4. Experiments We evaluate PARTICULATE on recovering articulated structures from static 3D meshes and compare it with prior work that addresses this task in full or in part (Sec. 4.2). We structure the evaluation around two sub-tasks: articulated part segmentation (Sec. 4.2.2) and articulated motion prediction (Sec. 4.2.3), so we can compare our all-in-one approach with methods that handle only one of them. In Sec. 4.3, we discuss the contribution of data and network design choices. 4.1. Experiment Details and GRScenes Training datasets. PARTICULATE is trained on the PartNet-Mobility [59] [52] datasets. For [59], we follow the train-test split by [30]. For [52], we use all its articulated assets. We set Pmax=16 and discard objects with more than 16 articulated parts. This yields 3, 800 objects across 50 categories. Evaluation datasets. We evaluate PARTICULATE on the test split of PartNet-Mobility [59], which contains 7 comIn addition, we introduce new chalmon categories. lenging benchmark for the 3D articulation estimation task, which contains 220 high-quality articulated objects spanning 13 categories crafted by Lightwheel and released under CC-BY-NC license [27]. Implementation details. During training, our model is given point clouds of fixed size =2048, among which 50% are sampled uniformly on the mesh surface and the rest 50% are sampled from the sharp edges where dihedral angles are larger than 30 [6]. We sample random articulated state for each training iteration and compute the 6 PartField [32] features on the fly. All point clouds are first normalized to [0.5, 0.5]3, followed by data augmentation. Each point cloud is scaled by factor randomly drawn from U(0.95, 1.05) and translated by vector from (0, 0.02)3. Our model consists of B=8 attention blocks with total of 150M parameters. The final model is optimized for 100K iterations using AdamW with global batch size of 128 on 8 H100 GPUs. At inference time, the model receives point clouds of size 102, 400 sampled from the input mesh using the same procedure to ensure coverage of all mesh faces. 4.2. Results and Comparisons 4.2.1. Qualitative Results on Synthesized 3D Meshes In Fig. 3, we visualize the articulated 3D objects of variety of categories predicted by PARTICULATE from static 3D meshes generated by Hunyuan3D [15], an off-the-shelf 3D generator. Our model recovers articulated parts faithful to the kinematic structure of the input objects, with plausible motion constraints for the movable parts. Notably, despite being trained only on artist-created assets, our model generalizes well to AI-generated meshes, yielding fully automatic pipeline for generating articulated 3D assets directly from text prompts or images. 4.2.2. Comparisons on Articulated Part Segmentation Baselines. We compare against four state-of-the-art 3D part segmentation methods: PartField [32], P3SAM [36], SINGAPO [30], and Articulate AnyMesh [40]. For reference, we also include Naive Baseline, which simply treats the entire object as single (fixed) part. As PartField, P3SAM, and Articulate AnyMesh leverage face connectivity for part segmentation, we likewise refine SINGAPO and our outputs using mesh connected components as described in Sec. 3.5, and report metrics both with and without this refinement. PartField allows specifying the number of output parts, so we provide it with the number of ground-truth (GT) parts. P3SAM additionally requires point prompts, so we supply one point randomly sampled from each GT part. SINGAPO does not take 3D mesh as input, and instead generates bounding boxes for the parts conditioned on single object-centric image and retrieves their geometries from repertoire of part meshes. To reduce any disadvantage from not having access to the original mesh, we make the following adjustments when evaluating SINGAPO: first, instead of one-shot generation, we generate 10 samples per instance using different rendered views for conditioning, and report 1@10 metric computed using the best sample among the 10; second, on the Lightwheel evaluation set, we extract part geometries directly as submeshes of the original mesh within the generated bounding boxes, rather than retrieving them from the part repository curated from the PartNet-Mobility training set. We use the official implementations of all baselines and the model weights publicly Figure 3. Qualitative results of PARTICULATE, illustrating the input meshes and the predicted articulated parts with their motion constraints. We also show each 3D object in two different articulated states. All input meshes are generated using an off-the-shelf 3D generator. released by the authors without further finetuning. Metrics. We report three metrics for part segmentation: generalized Intersection over Union (gIoU) [42], mean Intersection over Union (mIoU), and bidirectional part-wise Chamfer distance (PC), computed between predicted and GT parts. Since the predicted and GT parts have unknown correspondence and may differ in count, we first perform Hungarian matching between the two sets based on pairwise part-centroid distances. Prior work [20, 29, 30] computes metrics only on matched part pairs and ignores unmatched parts. However, as we show in Sec. 6.1, metrics computed in this manner do not sufficiently penalize missing small parts: under this protocol, the Naive Baseline outperforms all baseline methods on all metrics, making these scores uninformative for assessing part-segmentation quality. To better assess the full set of predictions, we introduce penalty for any predicted or GT part that remains unmatched. Specifically, we compute each metric Dd over all predicted parts {ppred i=1 and GT parts {pgt }N Dd ="
        },
        {
            "title": "1\nN",
            "content": "1 2 (cid:88) i=1 d(ppred ) +"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j=1 }M j=1: d(pgt ) , (12) where d(pi) = d(pi, H(pi)) if (predicted or GT) part pi is matched to (GT or predicted) part H(pi), and is otherwise set to penalty value ϵ= 1 for gIoU, 0 for mIoU, and half the diagonal length of the input meshs bounding box for PC. Here, selects the metric (gIoU, mIoU, or PC), and Dd reports that metric with penalties applied to unmatched parts. We report Dd averaged over all instances in the test set as the final score, where specifies the metric (gIoU, mIoU, or PC). All metrics are evaluated based on 106 points uniformly sampled from the input mesh. Results. We report the quantitative results on the PartNetMobility test set and our introduced Lightwheel benchmark in Tab. 2, where PARTICULATE consistently outper-"
        },
        {
            "title": "Method",
            "content": "gIoU PC mIoU gIoU PC mIoU"
        },
        {
            "title": "Lightwheel",
            "content": "PartNet-Mobility Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) -0.004 0.295 0.406 -0.149 0.202 0.252 -0.128 0.185 0.255 0.174 0.166 0.431 0.296 0.210 0.612 0.889 0.001 0.891 PartField [32] 0.053 0.111 0.248 0.183 0.123 0.361 P3SAM [36] 0.117 0.171 0.403 -0.116 0.261 0.267 SINGAPO [30] 0.262 0.124 0.468 -0.077 0.229 0.300 SINGAPO [30] (1@10) 0.271 0.117 0.471 -0.027 0.212 0.322 Articulate AnyMesh [40] 0.144 0.206 0.447 0.383 0.104 0.542 PARTICULATE (ours) 0.901 0.000 0.902 0.286 0.184 0.551 Table 2. Part segmentation results. We report generalized IoU (gIoU), part-wise Chamfer distance (PC), and mean IoU (mIoU), all computed with penalties applied to unmatched parts. : leveraging mesh connectivity. : SINGAPO retrieves part geometries from part library on the PartNet-Mobility test set, which assumes mesh connectivity. Therefore, its metrics without leveraging connectivity are left empty. 1@10: computed using the best sample out of 10 predictions per instance. Colors: best and second best. forms all baselines on both datasets. Note that PartField and P3SAM are given privileged information at test time including the exact number of GT parts, therefore avoiding any penalty for unmatched parts. All methods except P3SAM experience performance drop on the more challenging Lightwheel dataset, which contains more diverse assets with substantially finer part annotations. In Fig. 4, we visualize the part segmentation results of PARTICULATE and the baselines on the Lightwheel benchmark. PartField and P3SAM are trained for semantic part segmentation, where part definitions differ from those of articulated parts. Consequently, their predicted segments often fail to coincide with the GT parts (Fig. 4b). SINGAPO is trained only on limited number of categories, and hence fails to generalize to more diverse objects (Fig. 4c). Articulate Anymeshs VLM-based pipeline cannot handle internal"
        },
        {
            "title": "Data",
            "content": "Axis overparam."
        },
        {
            "title": "Rest state",
            "content": "gIoU PC OC gIoU PC mIoU PM+GRS PM+GRS PM 0.259 0.215 0.008 0.286 0.184 0.231 0.227 0.013 0.252 0.175 0.236 0.224 0.008 0.262 0.186 0.551 0.504 0. Table 4. Ablations on training data (PM: PartNet-Mobility, GRS: GRSCenes) and revolute axis over-parameterization, evaluated on the Lightwheel [27] benchmark. Colors: best and second best. Hungarian matching based on part-centroid distances. Metrics. Since the predicted parts may differ substantially from the GT parts, directly comparing the predicted motion parameters with those of the matched GT parts is not always sensible. Following prior work [29, 30], we instead compare the resulting articulated geometries. Specifically, given the predicted articulated parts and their motion constraints, we fully articulate the predicted articulated asset, where every part is moved to its maximum extent. We then compare the resulting fully articulated geometry with the ground-truth counterpart also in its fully articulated state, and report the part-wise gIoU and Chamfer distance (PC) from Sec. 4.2.2 as proxies for motion accuracy. As in the segmentation metrics, unmatched parts incur penalty. Furthermore, we introduce new metric, whole-object Chamfer distance (OC), which measures the overall bidirectional Chamfer distance between the entire predicted geometry and its GT counterpart under fully articulated state. Results. The results are reported in Tab. 3, where PARTICULATE significantly outperforms all baselines across all metrics on both datasets. This includes Articulate AnyMesh, which is given the ground-truth motion range for each predicted part. These results demonstrate the superiority of our end-to-end approach in estimating articulated motion constraints. 4.3. Ablations Over-parameterization of revolute axes. We evaluate the effectiveness of our proposed over-parameterization of revolute axes by comparing it with an alternative design in which single MLP directly regresses each axiss direction and location in R6 following [20]. As shown in Tab. 4 (A vs. C), the model with over-parameterization outperforms the one without, largely because it mitigates overfitting and helps average out errors in individual location estimates. Data. To examine the impact of training data scale, we train separate model, using identical hyperparameters, on only the PartNet-Mobility [59] dataset. As shown in Tab. 4 (A vs. B), reducing the training set leads to slightly worse model. Note that the model still outperforms the baselines by wide margin, which indicates that the gains of our approach mainly come from its simple, data-efficient design. Figure 4. Qualitative comparison on the Lightwheel benchmark. Compared with semantic part segmentation methods (Partthe segments of PARTICULATE Field [32] and P3SAM [36]), better correspond to articulated parts. In contrast to Articulate AnyMesh [40] and SINGAPO [30], which fail to predict internal structures invisible from external views (e.g., a, and d), PARTICULATE recovers most of these parts and faithfully infers their motion constraints."
        },
        {
            "title": "Method",
            "content": "gIoU PC OC gIoU PC OC"
        },
        {
            "title": "Lightwheel",
            "content": "PartNet-Mobility Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) -0.008 0.317 0.018 0.296 0.216 0.027 -0.154 0.310 0.013 -0.134 0.238 0.013 0.155 0.197 0.008 0.851 0.005 0.001 SINGAPO [30] -0.083 0.335 0.013 0.255 0.184 0.046 SINGAPO [30] (1@10) -0.035 0.259 0.014 0.264 0.168 0.041 Articulate AnyMesh [40] 0.129 0.257 0.012 0.378 0.251 0.022 PARTICULATE (ours) 0.259 0.215 0.008 0.863 0.004 0.001 Table 3. Results evaluated using fully articulated geometries, where every part is moved to its maximum extent. We report partwise gIoU and PC as in Tab. 2, and whole-object Chamfer distance (OC), which measures the overall bidirectional Chamfer distance between the whole predicted geometry and its GT counterpart. Notations (, , and 1@10): the same as Tab. 2. Colors: best and second best. parts which are invisible under the rest state (Fig. 4a and Fig. 4b). By contrast, our data-driven approach accurately recovers articulated parts, including internal ones, across wide variety of object categories. 4.2.3. Comparisons on Articulated Motion Prediction We now compare our methods predicted motion constraints (i.e., types, axes and ranges) with those of the baselines. Baselines. Out of the four baselines we compare for part segmentation, SINGAPO [30] and Articulate AnyMesh [40] also predict articulated motion, which we compare here. Note that Articulate AnyMesh predicts the motion type and axis for each part but does not estimate its range. For quantitative evaluation, we assign each predicted part the motion range of its matched GT part, using the same 5. Conclusion We have presented PARTICULATE, feed-forward approach that directly predicts all attributes of the underlying articulated structure from single static 3D mesh. Our approach departs from prior methods by training an end-to-end network with simple and scalable architecture on diverse collection of articulated 3D assets. The resulting model is significantly faster and more accurate at recovering articulated structures than prior methods. PARTICULATE also excels on AI-generated objects, enabling the creation of articulated assets compatible with physics simulators directly from images or text prompts. Limitations. Despite the promising results, PARTICULATE has some limitations. First, while PARTICULATE generalizes well to unseen instances, it fails to recover articulated objects with very different kinematic structures from those seen during training. Fundamentally, this is because the available training data remains several orders of magnitude smaller than datasets in other domains (e.g., ImageNet [10] and LAION [43] for images or Objaverse [8, 9] for static 3D meshes). Second, although combining PARTICULATE with an off-the-shelf 3D generator enables the creation of diverse articulated assets, these assets often exhibit inter-part penetrations, due to both artifacts in generated meshes and imperfect motion predictions. Enhancing the physical plausibility of the generated articulated assets (e.g., via post-training [24]) to enable large-scale sim-toreal training for robotics is promising direction for future work. Acknowledgments. Ruining Li is supported by Toshiba Research Studentship. Chuanxia Zheng is supported by NTU SUG-NAP and National Research Foundation, Singapore, under its NRF Fellowship Award NRF-NRFF172025-0009. Christian Rupprecht is supported by an Amazon Research Award. This work is partially supported by the UKRI AIRR programme."
        },
        {
            "title": "References",
            "content": "[1] Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zero-shot semantic segmentation of 3d shapes. In ICCV, 2023. 2 [2] Sergio Arnaud, Paul McVay, Ada Martin, Arjun Majumdar, Krishna Murthy Jatavallabhula, Phillip Thomas, Ruslan Partsey, Daniel Dugas, Abha Gejji, Alexander Sax, et al. Locate 3D: Real-world object localization via self-supervised learning in 3d. In ICML, 2025. 4 [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. 4, 5 [4] Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, and Minghua Liu. FreeArt3D: Training-free articulated object generation using 3d diffusion. In SIGGRAPH Asia, 2025. 2, 3 [5] Honghua Chen, Yushi Lan, Yongwei Chen, and Xingang Pan. ArtiLatent: Realistic articulated 3d object generation via structured latents. In SIGGRAPH Asia, 2025. [6] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In CVPR, 2025. 6 [7] Deemos. Rodin text-to-3D gen-1 (0525) v0.5, 2024. 2 [8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: universe of 10m+ 3d objects. In NeurIPS, 2023. 9, 13 [9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 9, 13 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: large-scale hierarchical image database. In CVPR, 2009. 9 [11] Daoyi Gao, Yawar Siddiqui, Lei Li, and Angela Dai. MeshArt: Generating articulated meshes with structure-guided transformers. In CVPR, 2025. 2, 3 [12] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. GAPartNet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts. In CVPR, 2023. 3 [13] Pradyumn Goyal, Dmitry Petrov, Sheldon Andrews, Yizhak Ben-Shabat, Hsueh-Ti Derek Liu, and Evangelos Kalogerakis. Geopard: Geometric pretraining for articulation prediction in 3d shapes. In ICCV, 2025. 2 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [15] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3D 2.1: From images to high-fidelity 3D assets with production-ready PBR material. arXiv, 2506.15442, 2025. 6 [16] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: Building digital twins of articulated objects from interaction. In CVPR, 2022. 1 [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D Gaussian Splatting for real-time radiance field rendering. SIGGRAPH, 42(4), 2023. 3 9 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [19] Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, and Eric Eaton. Articulate-Anything: Automatic modeling of articulated objects via vision-language foundation model. In ICLR, 2025. 2, 3 [20] Jiahui Lei, Congyue Deng, William Shen, Leonidas Guibas, and Kostas Daniilidis. Nap: Neural 3d articulation prior. In NeurIPS, 2023. 2, 3, 5, 7, 8, 12 [21] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. BEHAVIOR-1K: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. In CoRL, 2023. 1 [22] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022. 2 [23] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. DragAPart: Learning part-level motion prior for articulated objects. In ECCV, 2024. 3 [24] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. DSO: Aligning 3D generators with simulation feedback for physical soundness. In ICCV, 2025. [25] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Puppet-master: Scaling interactive video generation as motion prior for part-level dynamics. In ICCV, 2025. 3 [26] Xinyu Lian, Zichao Yu, Ruiming Liang, Yitong Wang, Li Ray Luo, Kaixu Chen, Yuanzhen Zhou, Qihong Tang, Xudong Xu, Zhaoyang Lyu, et al. Infinite mobility: Scalable high-fidelity synthesis of articulated objects via procedural generation. arXiv preprint arXiv:2503.13424, 2025. 1 [27] Lightwheel. Simready: Simulation-ready 3d assets. https://simready.com/, 2025. Accessed: 2025. 2, 6, 8, 12 [28] Jiayi Liu, Ali Mahdavi-Amiri, and Manolis Savva. PARIS: Part-level reconstruction and motion analysis for articulated objects. In ICCV, 2023. 2, [29] Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi-Amiri, and ManoIn lis Savva. CAGE: Controllable articulation generation. CVPR, 2024. 2, 3, 4, 7, 8, 12 [30] Jiayi Liu, Denys Iliash, Angel Chang, Manolis Savva, and Ali Mahdavi Amiri. SINGAPO: Single image controlled In ICLR, 2025. generation of articulated parts in objects. 2, 3, 4, 6, 7, 8, 12, 13 [31] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. PartSLIP: Low-shot part segmentation for 3d point clouds via pretrained imagelanguage models. In CVPR, 2023. 2 ing 3d feature fields for part segmentation and beyond. In ICCV, 2025. 2, 3, 4, 6, 7, 8, 12 [33] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In CVPR, 2019. 2 [34] Yu Liu, Baoxiong Jia, Ruijie Lu, Junfeng Ni, Song-Chun Zhu, and Siyuan Huang. Building interactable replicas of complex articulated objects via gaussian splatting. In ICLR, 2025. [35] Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, and Siyuan Huang. DreamArt: Generating interactable articulated objects from single image. arXiv preprint arXiv:2507.05763, 2025. 2, 3 [36] Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-SAM: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. 3, 4, 6, 7, 8, 12 [37] Ziqi Ma, Yisong Yue, and Georgia Gkioxari. Find any part in 3d, 2025. 3 [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 3 [39] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning disentangled signed distance functions for articulated shape representation. In ICCV, 2021. 3 [40] Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, and Chuang Gan. Articulate AnyMesh: Open-vocabulary 3d articulated objects modeling. arXiv preprint arXiv:2502.02590, 2025. 2, 3, 6, 7, 8, 12, 13, [41] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen Indoors: Photorealistic indoor scenes using procedural generation. In CVPR, 2024. 1 [42] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression. In CVPR, 2019. 7 [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. 9 [44] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-DArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. iGibson 1.0: simulation environment for interactive tasks in large realistic scenes. In IROS, 2021. 1 [45] Chaoyue Song, Jiacheng Wei, Chuan Sheng Foo, Guosheng Lin, and Fayao Liu. Reacto: Reconstructing articulated objects from single video. In CVPR, 2024. 3 [32] Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. PartField: Learn- [46] Jiayi Su, Youhe Feng, Zheng Li, Jinhua Song, Yangfan He, Botao Ren, and Botian Xu. ArtFormer: Controllable gener10 [62] Yuheng Xue, Nenglun Chen, Jun Liu, and Wenyun Sun. ZeroPS: High-quality cross-modal knowledge transfer for zeroshot 3d part segmentation. In 3DV, 2025. 2 [63] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. SAMPart3D: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. 3 [64] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3DShape2VecSet: 3d shape representation for neural fields and generative diffusion models. ACM TOG, 42 (4):116, 2023. 4 [65] Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In ICLR, 2024. 5 [66] Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. PartSLIP++: Enhancing lowshot 3d part segmentation via multi-view instance segmentation and maximum likelihood estimation. arXiv preprint arXiv:2312.03015, 2023. 2 ation of diverse 3d articulated objects. In CVPR, 2025. 2, [47] George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv eprints, pages arXiv2408, 2024. 2 [48] Tencent Hunyuan3D Team. Hunyuan3D 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 2 [49] Tencent Hunyuan3D Team. Hunyuan3D 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. [50] Tencent Hunyuan3D Team. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details, 2025. 2 [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4 [52] Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, et al. GRUtopia: Dream general robots in city at scale. arXiv preprint arXiv:2407.10943, 2024. 2, 6 [53] Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu, and Lan Xu. Kinematify: Open-vocabulary arXiv preprint synthesis of high-dof articulated objects. arXiv:2511.01294, 2025. 2, [54] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM TOG, 2019. 2 [55] Fangyin Wei, Rohan Chabra, Lingni Ma, Christoph Lassner, Michael Zollhoefer, Szymon Rusinkiewicz, Chris Sweeney, Richard Newcombe, and Mira Slavcheva. Self-supervised neural articulated shape and appearance models. In CVPR, 2022. 3 [56] Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, and Stan Birchfield. Neural implicit representation for building digital twins of unknown articulated objects. In CVPR, 2024. 1, 3 [57] Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, and Cewu Lu. Reartgs: Reconstructing and generating articulated objects via 3d gaussian splatting with geometric and motion constraints. In NeurIPS, 2025. 3 [58] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. Dˆ2nerf: Self-supervised decoupling of dynamic and static objects from monocular video. NeurIPS, 2022. 3 [59] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In CVPR, 2020. 1, 2, 3, 6, 8 [60] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D latents for scalable and versatile 3d generation. arXiv, 2412.01506, 2024. 3 [61] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In CVPR, 2025. 11 PARTICULATE: Feed-Forward 3D Object Articulation Supplementary Material 6. Additional Experimental Results 6.1. Additional Details of Evaluation Metrics Motivation for imposing penalties on unmatched parts. Recall that we apply Hungarian matching to obtain the correspondence between the articulated parts predicted by the model and the ground-truth parts. Certain parts are unmatched, as the number of predicted parts is not necessarily equal to the number of GT parts. Unlike previous work [20, 29, 30] that simply ignores the unmatched parts, we impose penalty (corresponding to the worst possible score for each evaluation metric, e.g., 1 for gIoU and 0 for mIoU) on each unmatched part. To motivate this metric design, in Tab. 5 and Tab. 6, we report the evaluation results of the non-penalizing version used in prior work. Note that the naive baseline, which treats the entire object as single static part, outperforms all baseline methods in nonpenalizing gIoU, PC and mIoU for both the rest state and the fully articulated state, even though human judges generally find the baseline results  (Fig. 4)  more reasonable. This is because the single part predicted by the naive baseline is matched to the objects base part, which typically occupies most of the volume and is surrounded by smaller adjacent parts, resulting in favorable metric scores. By penalizing missed small parts, our proposed metric is more consistent with human-judged quality. Motivation for the whole-object Chamfer distance. In Tab. 3, we reported the whole-object Chamfer distance (OC), which measures the bi-directional Chamfer distance between the entire predicted and ground-truth point clouds at the fully articulated state. In contrast to the part-averaged metrics (i.e., gIoU and PC), OC does not rely on matching predicted parts to ground-truth parts and instead provides more global evaluation of both part segmentation and moIn addition, many parts move tion parameter estimation. farther from the base part at the fully articulated state, thus yielding poorer (and more appropriate) OC value for the naive baseline. 6.2. Per-Category Evaluation Results Tab. 7 presents the evaluation results on all 13 categories in the Lightwheel dataset [27]. While PARTICULATE outperforms baseline methods on almost all categories, the performance drops significantly on out-of-distribution categories such as stand mixer and stovetop, indicating our approach is still bottlenecked by the limited diversity in the training data. Articulate Anymesh [40] performs better specifically on the stand mixer category. This is because stand mixer never"
        },
        {
            "title": "Method",
            "content": "gIoU PC mIoU gIoU PC mIoU"
        },
        {
            "title": "Lightwheel",
            "content": "PartNet-Mobility Without Penalty Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) PartField P3SAM SINGAPO [30] SINGAPO(best) [30] (1@10) Articulate AnyMesh [40] PARTICULATE (ours) With Penalty Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) PartField [32] P3SAM [36] SINGAPO [30] SINGAPO [30] (1@10) Articulate AnyMesh [40] PARTICULATE (ours) 0.687 0.135 0.201 0. 0.065 0.390 0.310 0.449 0.574 0.717 -0.004 -0.149 -0.128 0.174 0.053 0.117 -0.077 -0.027 0.144 0.286 0.019 0.049 0.036 0.024 0.105 0.046 0.043 0.026 0.027 0.010 0.295 0.202 0.185 0. 0.111 0.171 0.229 0.212 0.206 0.184 0.687 0.342 0.375 0.548 0.251 0.503 0.432 0.518 0.617 0.734 0.406 0.252 0.255 0.431 0.248 0.403 0.300 0.322 0.447 0.551 0.897 0. 0.370 0.390 0.538 0.563 0.581 0.901 0.296 0.889 0.183 -0.116 0.262 0.271 0.383 0.901 0.003 0.000 0.040 0.023 0.008 0.006 0.025 0.000 0.210 0. 0.123 0.261 0.124 0.117 0.104 0.000 0.897 0.891 0.421 0.403 0.570 0.585 0.621 0.902 0.612 0.891 0.361 0.267 0.468 0.471 0.542 0.902 Table 5. Part segmentation results at the rest state with and without penalty applied to unmatched parts. The metrics without penalty do not sufficiently penalize missing small parts: under this protocol, the Naive Baseline outperforms all baseline methods on all metrics. : leveraging mesh connectivity. 1@10: reporting best out of 10 predictions. Colors: best and second best. Figure 5. Failure cases. Our model can struggle with objects with atypical articulation configurations (left) or with AI-generated shapes which lack internal structures (right). appears in its training set and its articulation pattern is distinct from all other training objects. With its strong VLMbased generalization, Articulate Anymesh handles this unseen category particularly well. For the range hood category, objects often contain very large base component that occupies the majority of the objects volume. SINGAPO [30] tends to predict oversized parts, especially when the input object falls outside its training distribution. Because the ground-truth base part is already extremely large, SINGAPOs tendency to over-predict part extents accidentally aligns with the dominant fixed region, leading to inflated IoU values for this specific category."
        },
        {
            "title": "Method",
            "content": "gIoU PC OC gIoU PC OC"
        },
        {
            "title": "Lightwheel",
            "content": "PartNet-Mobility Without Penalty Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) SINGAPO [30] SINGAPO [30] (1@10) Articulate AnyMesh [40] PARTICULATE (ours) With Penalty Naive Baseline SINGAPO [30] SINGAPO [30] (1@10) PARTICULATE (ours) SINGAPO [30] SINGAPO [30] (1@10) Articulate AnyMesh [40] PARTICULATE (ours) 0.680 0.128 0.194 0. 0.302 0.438 0.553 0.684 -0.008 -0.154 -0.134 0.155 -0.083 -0.035 0.129 0.259 0.056 0.018 0.190 0.013 0.104 0.013 0.064 0.008 0.186 0.013 0.086 0.014 0.092 0.012 0.052 0.008 0.317 0.018 0.310 0.013 0.238 0.013 0.197 0. 0.335 0.013 0.259 0.014 0.257 0.012 0.215 0.008 0.897 0.853 0.523 0.555 0.577 0.863 0.296 0.851 0.255 0.264 0.378 0.863 0.011 0. 0.004 0.001 0.079 0.046 0.066 0.043 0.187 0.022 0.004 0.001 0.216 0.027 0.005 0.001 0.184 0.046 0.168 0.041 0.251 0.022 0.004 0.001 Table 6. Results evaluated using fully articulated geometries with and without penalty applied to unmatched parts. The metrics without penalty do not sufficiently penalize missing small parts: under this protocol, the Naive Baseline outperforms all baseline methods on all part-wise metrics (i.e., gIoU and PC). : leveraging mesh connectivity. 1@10: reporting best out of 10 predictions. Colors: best and second best. 6.3. Failure Cases We show typical failure cases of PARTICULATE in Fig. 5. Notably, while our model is trained on large variety of articulated objects, the training dataset remains several orders of magnitude smaller than those used by open-domain 3D generators (i.e., Objaverse [8, 9]). As result, PARTICULATE can struggle with objects whose articulation configurations deviate significantly from those represented in our training data (left). Besides, many AI-generated 3D assets lack realistic internal structures (right), introducing distribution shift (since all objects in our training set contain well-defined internal parts) that can cause our model to fail. Enhancing the models robustness to such noisy, unclean inputs is valuable direction for future work. 13 Methods Metrics Microwave Dishwasher Electric Kettle Refrigerator Blender Oven P3SAM PartField"
        },
        {
            "title": "SINGAPO",
            "content": "SINGAPO Articulate AnyMesh [40] PARTICULATE (ours) PARTICULATE (ours) mIoU (rest) gIoU (rest) PC (rest) mIoU (rest) gIoU (rest) PC (rest) mIoU (rest) gIoU (rest) gIoU (high) PC (rest) PC (high) OC (high) mIoU (rest) gIoU (rest) gIoU (high) PC (rest) PC (high) OC (high) mIoU (rest) gIoU (rest) gIoU (high) PC (rest) PC (high) OC (high) mIoU (rest) gIoU (rest) gIoU (high) PC (rest) PC (high) OC (high) mIoU (rest) gIoU (rest) gIoU (high) PC (rest) PC (high) OC (high) 0.492 0.252 0. 0.408 0.254 0.103 0.337 0.003 0.001 0.138 0.362 0.016 0.392 0.080 0.079 0.171 0.435 0.019 0.671 0.434 0.405 0.147 0.216 0.011 0.659 0.494 0.462 0.113 0.110 0.000 0.788 0.624 0.576 0.115 0.116 0. 0.411 0.156 0.144 0.273 0.087 0.104 0.346 -0.080 -0.084 0.195 0.401 0.013 0.331 -0.075 -0.077 0.225 0.425 0.013 0.583 0.283 0.265 0.218 0.235 0.003 0.697 0.582 0.544 0.083 0.164 0. 0.796 0.667 0.618 0.093 0.177 0.025 0.637 0.515 0.110 0.320 0.218 0.090 0.304 -0.286 -0.328 0.313 0.593 0.059 0.490 0.072 0.008 0.288 0.338 0.054 0.408 0.092 0.053 0.218 0.472 0. 0.470 0.175 0.127 0.202 0.276 0.017 0.641 0.349 0.290 0.213 0.274 0.018 0.498 0.229 0.162 0.235 0.068 0.070 0.168 -0.126 -0.129 0.126 0.176 0.007 0.209 -0.040 -0.047 0.124 0.182 0. 0.362 0.081 0.077 0.157 0.215 0.011 0.476 0.289 0.266 0.105 0.109 0.001 0.604 0.455 0.413 0.080 0.083 0.001 0.514 0.282 0.116 0.330 0.140 0.072 0.050 -0.328 -0.331 0.236 0.318 0. 0.102 -0.271 -0.271 0.261 0.341 0.025 0.503 0.275 0.247 0.133 0.182 0.021 0.438 0.202 0.177 0.121 0.186 0.019 0. 670 0.457 0.416 0.129 0.181 0.019 0.364 0.092 0.169 0.202 -0.029 0. 0.220 -0.162 -0.166 0.187 0.299 0.008 0.251 -0.139 -0.145 0.240 0.366 0.009 0.432 0.122 0.107 0.235 0.279 0.011 0.476 0.176 0.159 0.225 0.237 0.005 0.507 0.178 0.155 0.246 0.254 0.005 Categories Stand Mixer 0.569 0.365 0.146 0.445 0.367 0.063 0.071 -0.335 -0.337 0.326 0.573 0.040 0.067 -0.349 -0.349 0.355 0.625 0.040 0.425 0.210 0.178 0.155 0.451 0. 0.322 0.010 -0.004 0.235 0.419 0.043 0.273 -0.074 -0.083 0.267 0.461 0.045 Toaster Toaster Oven Coffee Machine Stove Stovetop 0.359 0.046 0.187 0.192 -0.026 0.146 0.248 -0.172 -0.176 0.208 0.281 0.007 0.283 -0.114 -0.117 0.241 0.326 0.007 0.441 0.130 0.120 0.222 0.258 0. 0.390 0.102 0.093 0.187 0.198 0.001 0.507 0.198 0.182 0.214 0.227 0.001 0.395 0.135 0.152 0.187 -0.056 0.125 0.193 -0.180 -0.182 0.192 0.293 0.009 0.229 -0.160 -0.164 0.247 0.364 0. 0.387 0.063 0.057 0.236 0.297 0.016 0.451 0.107 0.093 0.249 0.254 0.002 0.487 0.138 0.117 0.252 0.254 0.002 0.393 0.119 0.156 0.205 0.048 0.083 0.301 -0.144 -0.146 0.273 0.342 0. 0.376 -0.057 -0.057 0.289 0.329 0.013 0.340 0.000 -0.005 0.210 0.227 0.014 0.267 -0.143 -0.148 0.224 0.261 0.012 0.460 0.060 0.055 0.287 0.328 0.012 0.253 -0.201 0.212 0.170 -0.166 0. 0.225 -0.190 -0.196 0.163 0.205 0.004 0.279 -0.132 -0.136 0.225 0.279 0.005 0.369 -0.033 -0.040 0.281 0.292 0.002 0.347 0.134 0.126 0.120 0.125 0.001 0.447 0.204 0.188 0.152 0.161 0.001 0.377 -0.034 0. 0.222 -0.023 0.134 0.176 -0.234 -0.241 0.109 0.134 0.001 0.254 -0.114 -0.117 0.185 0.202 0.001 0.289 -0.129 -0.129 0.273 0.289 0.000 0.192 -0.085 -0.087 0.169 0.177 0.001 0.263 -0.064 -0.068 0.189 0.207 0. Range Hood 0.097 -0.223 0.233 0.056 -0.204 0.144 0.443 -0.000 -0.001 0.259 0.284 0.003 0.515 0.089 0.089 0.265 0.283 0.002 0.236 -0.104 -0.104 0.280 0.279 0. 0.211 -0.155 -0.160 0.323 0.328 0.002 0.285 -0.093 -0.094 0.282 0.288 0.002 Table 7. Per-category evaluation results against baselines on the Lightwheel dataset. We report the results both at the rest state (rest) and at the fully articulated state (high). The categories are ranked based on gIoU (rest) averaged over all methods (left is higher). : leveraging mesh connectivity. Colors: best and second best."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "University of Cambridge",
        "University of Oxford"
    ]
}