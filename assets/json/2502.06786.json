{
    "paper_title": "Matryoshka Quantization",
    "authors": [
        "Pranav Nair",
        "Puranjay Datta",
        "Jeff Dean",
        "Prateek Jain",
        "Aditya Kusupati"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to $10\\%$ more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model."
        },
        {
            "title": "Start",
            "content": "2025-02-"
        },
        {
            "title": "Matryoshka Quantization",
            "content": "Pranav Nair*,1, Puranjay Datta*,1, Jeff Dean1, Prateek Jain1 and Aditya Kusupati1 1Google DeepMind, *Equal contribution Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models especially to low precisions like int4 or int2 requires trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model. 1. Introduction Due to their impressive performance, there is strong push to deploy deep learning models, particularly large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024) in large number of scenarios. Due to autoregressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transferring model weights from high-bandwidth memory (HBM) to the SRAM or due to transferring weights/activations in distributed cluster. Quantizing weights and/or activations can significantly reduce the overall communication load and is, therefore, one of the most popular techniques for reducing inference costs (Dettmers et al., 2022). While floating-point representations are standard for training, integer data types such as int8, int4, and int2 are appealing alternatives for inference. However, current methods for quantizing to these varying integer precisions typically treat each target precision as an independent optimization problem, leading to collection of distinct models rather than single, versatile one. Furthermore, quantizing to extremely low precisions like int2 is known to be highly inaccurate. In this work, we pose the question of whether both of the above challenges can be addressed; that is, can we train single model from which we can extract multiple accurate lower-precision models? We answer this question in the affirmative by introducing Matryoshka Quantization (MatQuant), novel multi-scale training method that leverages the inherent nested (Matryoshka) structure (Kusupati et al., 2022) within integer data types (Figure 1a). Specifically, slicing the top bits of an int8-quantized weight can directly yield an int4 or int2 model. Existing quantization techniques often neglect this structure, which limits the potential for multi-scale adaptable models operating at various bit-widths with optimal performance. Instead, MatQuant simultaneously optimizes model weights across multiple precision levels (e.g., int8, int4, int2). At high level, we represent each model parameter at different precision levels using shared most significant bits (MSBs), and then jointly optimize the loss for each precision level. This allows us to develop single quantized model that can effectively operate at any of the chosen bit-widths, offering spectrum of accuracy-versus-cost options. MatQuant is general-purpose technique, applicable to most 1 5 2 0 2 0 1 ] . [ 1 6 8 7 6 0 . 2 0 5 2 : r Matryoshka Quantization (c) (a) (b) Figure 1 (a) MatQuant is multi-scale quantization training technique using the inherent Matryoshka structure of int8 int4 int2. (b) Empirical gains of MatQuant on downstream tasks, especially > 8% for int2, on Gemma-2 9B with OmniQuant. (c) The right-shifted quantized weight distribution as consequence of MatQuants training mechanism that maximises accuracies across all precisions. learning-based quantization methods, such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). We demonstrate the efficacy of MatQuant when applied to quantizing the Feed-Forward Network (FFN) parameters of standard LLMs (Gemma-2 2B, 9B, and Mistral 7B) (Vaswani et al., 2017) typically, FFN is the main latency block hence the focus on improving the most significant components latency. Our results show that MatQuant produces int8 and int4 models with comparable accuracy to independently trained baselines, despite the benefit of shared model parameters. Critically, the int2 models generated by MatQuant significantly outperform their individually trained counterparts, with 8% higher accuracy on downstream tasks (Figure 1b). We also extend MatQuant to quantize all weights of Transformer layer. Finally, we find that quantizing with MatQuant shifts the quantized weight distribution toward higher values, contributing to improved int2 performance (Figure1c). Beyond improving chosen precision performance, MatQuant allows for seamless extraction of interpolative bit-widths, such as int6 and int3. MatQuant also admits dense accuracy-vs-cost pareto-optimal trade-off by enabling layer-wise MixnMatch of different precisions. This ensures deployment of say an effective int3 sized model even if the underlying hardware only supports int4 and int2. Overall, MatQuant and its variants present significant step toward developing multi-scale models with high flexibility and performance, pushing the boundaries of low-bit quantization for efficient LLM inference. 2. Related Work Model weight quantization is an extremely powerful and prevalent technique for making resourceintensive neural networks suitable for deployment constraints especially modern-day LLMs. Quantization algorithms can be categorized as either learning-free or learning-based. Learning-free methods use limited data to calibrate model parameters without relying on gradient descent. Learning-based methods, however, utilize gradient descent to update either model parameters or auxiliary parameters to aid in quantization. 2 Matryoshka Quantization Learning-free Quantization Methods. Naive quantization methods, such as MinMax, absmax, and zero-point quantization, aim to directly map the range of model weights to the target bitwidth see (Dettmers et al., 2022) for detailed background. Dettmers et al. (2022) further improved this by identifying the need to handle outliers with higher precision than the rest of the model weights. The core principle of more recent learning-free quantization methods remains similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ (Frantar et al., 2022) improves upon min-max quantization by iterating over all the coordinates, quantizing them one at time, and updating the remaining fullprecision coordinates to minimize the layer-wise activation reconstruction error. AWQ (Lin et al., 2023), SmoothQuant (Xiao et al., 2023), and AffineQuant (Ma et al., 2024) scale the weights and activations to reduce outliers, thus making them easier to quantize. QuIP (Chee et al., 2024), FrameQuant (Adepu et al., 2024), and QuaRoT (Ashkboos et al., 2024) multiply the weights and activations by orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM (Kim et al., 2024) uses clustering to obtain the optimal buckets for quantization, and CDQuant (Nair and Suggala, 2024) improves upon GPTQ by greedily choosing the coordinates to descend along. While learningfree methods are inexpensive and work well at higher bit-widths, they are often suboptimal in the low-precision regime, which benefits greatly from learning-based techniques. Learning-based Quantization Methods. Quantization Aware Training (QAT) (Abdolrashidi et al., 2021; Jacob et al., 2018) is logical approach to ensure that models are easy to quantize during inference while retaining high accuracy. However, because QAT involves updating all the model parameters, its adoption Several recent for LLMs has been limited. works improve the performance and efficiency of QAT. LLM-QAT (Liu et al., 2024a) and BitDistiller (Du et al., 2024) enhance QAT with knowledge distillation from the full-precision model. EfficientQAT (Chen et al., 2024) minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for QAT to converge. On the other hand, some techniques significantly reduce the overhead by learning only the auxiliary parameters, such as scaling factors and zero-points, that aid in quantization instead of updating the actual weight matrices. For example, OmniQuant (Shao et al., 2023) does not update the model parameters; instead, it learns additional scales and shifting parameters (that aid with quantization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant (Liu et al., 2024b) uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant, etc.) is widely adopted due to their appeal of achieving QAT-level accuracy at fraction of the cost. Multi-scale Training. Training across multiple data scales (resolutions) was heavily popularized in computer vision for both recognition and generation (Adelson et al., 1984; Denton et al., 2015; Lin et al., 2017). More recently, the paradigm of multi-scale training has shifted to models (Devvrit et al., 2023; Kusupati et al., 2022; Rippel et al., 2014; Yu et al., 2018), where the data remains the same, and models of varying capacity, all nested within one large model, are trained jointly. This joint, nested (Matryoshka-style) learning with varying model sizes results in smooth accuracyvs-compute trade-off and is beneficial in many downstream applications and real-world deployments. However, the most obvious structure with nested nature is the bit structure of the integer data type. Given the success of multi-scale training for inputs, outputs, and model weights, it is imperative to explore it further for integer data types, especially in the context of quantization, which aids in the deployment of resourceintensive LLMs. 3. Matryoshka Quantization We introduce MatQuant, general-purpose, multi-scale training technique that works seam3 Matryoshka Quantization lessly with popular learning-based quantization methods such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). As long as the model or auxiliary parameters are optimized with gradient descent, MatQuants multi-scale training technique can be used across chosen bit-widths, leveraging the inherent nested structure of integer data types. In this section, we will elaborate on the preliminaries behind QAT and OmniQuant, alongside our novel proposed approach, MatQuant. 3.1. Preliminaries 3.1.1. Quantized Aware Training Quantized Aware Training (QAT) learns ğ‘-bit quantized model by optimizing for the end-toend cross entropy loss using gradient descent. It uses the quantized weights for the forward pass and straight through estimator (STE) (Bengio et al., 2013) to propagate gradients through the quantization operator during the backward pass. To mathematically formulate QAT, we define MinMax quantization of real-valued vector ğ‘¤ in ğ‘ bits as follows: ğ‘„MM(ğ‘¤, ğ‘) = clamp (cid:16) (cid:106) ğ‘¤ ğ›¼ (cid:109) + ğ‘§ ğ›¼ = max(ğ‘¤) min(ğ‘¤) 2ğ‘ 1 , (cid:17) , 0, 2ğ‘ 1 min(ğ‘¤) ğ›¼ ğ‘§ = (1) These auxiliary parameters aid with quantization. Similar to QAT, OmniQuant also uses straight through estimator during optimization. However, unlike QAT, OmniQuant operates with limited data, making it much more attractive for resourcescarce settings. OmniQuant adds two learnable scales, ğ›¾ and ğ›½, to MinMax quantization as follows: (cid:109) ğ‘„Omni(ğ‘¤, ğ‘) = clamp ğ›¾ max(ğ‘¤) ğ›½ min(ğ‘¤) 2ğ‘ ğ›¼ = (cid:16) (cid:106) ğ‘¤ ğ›¼ + ğ‘§ , ğ‘§ = (cid:17) , 0, 2ğ‘ 1 ğ›½ min(ğ‘¤) ğ›¼ (3) OmniQuant also adds another set of learnable shifting and scaling parameters to the FFNs affine projections as follows: ğ‘‹ğ‘Š+ğ‘ ((ğ‘‹ ğ›¿) ğ‘ )ğ‘„Omni(ğ‘Š ğ‘ )+ğ‘+ğ›¿ğ‘Š (4) where ğ‘‹ â„ğ‘›ğ‘‘ is the input to the affine transformation, ğ‘Š â„ğ‘‘ğ‘‘o is the linear projection associated with the affine transformation, ğ‘ â„ğ‘‘o is the bias vector, ğ›¿ â„ğ‘‘ and ğ‘  â„ğ‘‘ are learnable shift and scale parameters respectively. With the goal of optimizing the layer-wise L2 error (where layer consists of an Attention block followed by an FFN block), OmniQuants overall objective can be portrayed as follows: min ğ›¾,ğ›½,ğ›¿,ğ‘  ğ¹ğ‘™ (ğ‘Š ğ‘™ ğ¹), ğ‘‹ğ‘™) ğ¹ğ‘™ (ğ‘„Omni(ğ‘Š ğ‘™ ğ¹), ğ‘‹ğ‘™)2 2 (5) where ğ‘„MM(ğ‘¤, ğ‘) is the ğ‘-bit quantized version of ğ‘¤, ğ›¼ is the scaling factor and ğ‘§ is the zero point. Let ğ‘Šğ¹ represent weights of Transformer LLM and let = {(ğ‘¥1, ğ‘¦1), . . . , (ğ‘¥ğ‘ , ğ‘¦ğ‘)} be labelled dataset where ğ‘¥ğ‘– and ğ‘¦ğ‘– represent the input and output respectively. With ğ¿CE as the cross entropy loss, the optimization of QAT is: 1 ğ‘ min ğ‘Šğ¹ ğ‘– [ ğ‘ ] LCE (ğ¹(ğ‘¥ğ‘–; ğ‘„MM (ğ‘Šğ¹, ğ‘)), ğ‘¦ğ‘–) (2) where ğ¹() represents the LLMs forward pass. 3.1.2. OmniQuant OmniQuant, unlike QAT, does not update the model parameters. Instead, it learns additional scaling and shifting parameters through gradient descent over layer-wise L2 error reconstruction. where ğ¹ğ‘™ () represents the forward pass for single layer ğ‘™, ğ‘Š ğ‘™ ğ¹ represents the layer parameters and ğ‘‹ğ‘™ represents the layers input. Note that the above objective is optimized independently for each of the ğ¿ Transformer layers. 3.2. MatQuant MatQuant is general purpose framework to develop single model that can do well at any precision. It is multi-scale training technique that works with most learning-based quantization schemes like QAT and OmniQuant discussed earlier. At its core, taking inspiration from Kusupati et al. (2022), MatQuant optimizes the quantization loss for several target bit-widths jointly. To have single model for various integer precisions, we nest smaller bit-widths into large ones 4 Matryoshka Quantization leveraging the inherent Matryoshka nature of the integer data type. So, if we want to extract ğ‘Ÿ-bit model from ğ‘-bit model (0 < ğ‘Ÿ < ğ‘), we can just slice out the ğ‘Ÿ most significant bits (MSBs) using right shift, followed by left shift of the same order. Formally, the ğ‘†(ğ‘ğ‘, ğ‘Ÿ) operator slices the most significant ğ‘Ÿ bits from ğ‘-bit quantized vector ğ‘ğ‘: ğ‘†(ğ‘ğ‘, ğ‘Ÿ) = (cid:25)(cid:19) (cid:18) (cid:22) ğ‘ğ‘ 2ğ‘ğ‘Ÿ 2ğ‘ğ‘Ÿ (6) Once we have this structure, we can optimize for several precisions by slicing the MSBs from the largest bit-width we are optimizing for. Let ğ‘… = {ğ‘Ÿ1, ğ‘Ÿ2, ..., ğ‘Ÿğ¾ } be the bit-widths we want to optimize for, ğ‘„(, ) represent the quantization function of the base algorithm (i.e., any learning-based quantization scheme), () represent the loss function pertaining to the base algorithm, ğ¹() represent the forward pass required to compute the loss, ğœƒ represent the set of model/auxiliary parameters we are optimizing for and let ğ‘Šğ¹ represent the model parameters. MatQuants overall objective can be formulated as follows: min ğ‘ƒ 1 ğ‘ ğ‘– [ ğ‘ ] ğ‘Ÿğ‘… ğœ†ğ‘Ÿ (cid:0)ğ¹(ğ‘†(ğ‘„(ğœƒ, ğ‘), ğ‘Ÿ), ğ‘¥ ğ‘– ), ğ‘¦ ğ‘– (cid:1) (7) ğ‘– = ğ‘¦ğ‘– for QAT and ğ‘¦ where ğ‘¦ ğ‘™ ) for OmniQuant, and ğ‘¥ ğ‘™ for OmniQuant. ğœ†ğ‘Ÿ is the loss reweighing factor for bit-width ğ‘Ÿ. ğ‘– = ğ¹ğ‘™ (ğ‘Š ğ‘™ ğ‘– = ğ‘¥ğ‘– for QAT and ğ‘¥ ğ¹, ğ‘‹ ğ‘– ğ‘– = ğ‘‹ ğ‘– In this work, we default to training MatQuant with three bit-widths, ğ‘… = {8, 4, 2}, and subsequently perform grid search over ğœ†ğ‘Ÿ. This process aims to optimize performance such that the model performs well across all targeted precision levels. Further, while the focus of this paper is primarily on integer data types, we discuss the possibility of extending MatQuant to floating-point representations in Section 5.5. key point to note is that MatQuant primarily alters the quantized weight distributions across precision levels compared to the base quantization algorithm (OmniQuant or QAT). Figure 1c illustrates the differences in the quantized weight histograms obtained with and without MatQuant on Gemma-2 9B using OmniQuant. Upon close observation, we find that all the distributions of MatQuant are shifted to the right; that is, weights quantized with MatQuant tend to use more higher-valued weights. While this might not significantly impact int8 or even int4 models, int2 models benefit from utilizing more of the possible quantized weights compared to the baseline. Because int2 favors higher-valued weights, this effect propagates to higher-valued weights for int4, and then to int8. This observation highlights the potential overparameterization and freedom in the int8 data type to accommodate the more stringent needs of int2 during joint training. We further explore the effects of this phenomenon in Section 5.3 to develop better standalone quantization technique for single target precision. 3.2.1. Interpolative Behavior Slicing. Although we explicitly train MatQuant for three precisions (int8, int4, int2), we find that the resulting model, when quantized to interpolated bit-widths like int6 & int3 by slicing (Eq. 6) the int8 model, performs on par with baseline trained explicitly for that precision. It is also significantly better than slicing an int8 quantized model. We attribute this strong interpolation in bit-width space to MatQuant, and present more results in Sections 4.1 & 4.2. MixnMatch. MatQuant also enables the use of different precisions at different layers through layer-wise MixnMatch (Devvrit et al., 2023), even though we never trained for these combinatorial possibilities. These large number of models, obtained at no cost, densely span the accuracy-vs-memory trade-off. We explore several MixnMatch strategies and find that having higher precision (int8) in the middle layers and lower precision (int2) at the start and end is Pareto-optimal among hundreds of possible models. See Section 4.3 for detailed experiments. 4. Experiments In this section, we present an empirical evaluation of MatQuant working with two popular learning5 Matryoshka Quantization Table 1 MatQuant with OmniQuant across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the int3, int6 models obtained for free through interpolation from MatQuant perform comparably to the explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks () while log pplx (perplexity) is computed on C4 validation set (). Data type Method Gemma-2 2B Gemma-2 9B Mistral 7B OmniQuant Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. bfloat16 int8 int4 int2 int int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 68.21 68.25 67.85 62.98 67.03 66.54 37.68 51.33 55.70 67.66 68.06 68.01 42.00 64.37 63. 2.551 2.552 2.580 2.794 2.598 2.617 17.993 3.835 3.355 2.565 2.554 2.582 5.781 2.727 2. 74.38 74.59 74.33 72.19 74.33 74.26 35.75 60.24 68.25 74.61 74.23 74.50 55.76 73.23 73. 2.418 2.418 2.446 2.546 2.451 2.470 14.892 3.292 2.823 2.424 2.420 2.446 3.830 2.549 2. 73.99 73.77 73.46 46.59 73.62 73.13 36.25 59.74 65.99 73.50 74.10 73.59 34.60 71.68 71. 2.110 2.110 2.132 4.139 2.136 2.155 10.831 3.931 2.569 2.122 2.112 2.139 8.539 2.211 2. based quantization methods: OmniQuant (Section 4.1) and QAT (Section 4.2). We demonstrate MatQuants efficiency on Transformerbased LLMs. Unless otherwise mentioned, our primary focus is on weight quantization within the parameter-intensive FFN blocks of the Transformer layer. train for total of 10M tokens for all models except the int2 baseline, where we train the model for 20M tokens (Shao et al., 2023). For QAT experiments, we sample fixed set of 100M tokens from the C4 dataset and train all our models using batch size of 16 and sequence length of 8192 for single epoch. For our experiments, we chose the default target quantization precisions to be int8, int4, and int2. Furthermore, we showcase the interpolative nature of MatQuant through evaluations on int6 and int3, as well as its elastic ability to densely span the accuracy-vs-cost trade-off using layerwise MixnMatch (Section 4.3). Finally, we ablate on improving the performance of MatQuant (Sections 5.1 and 5.2) and extend MatQuant to the quantization of FFN and Attention parameters. (Section 5.3). Further training and fine-grained evaluation details are in the Appendix. Baselines. For OmniQuant and QAT, our primary baselines (referred to as Baseline in the tables and figures) are models trained explicitly for given precision. When interpolating the models trained with MatQuant for int6 and int3, we do not perform any additional training. However, the baselines are trained explicitly for 6 and 3 bits respectively. We also compare against sliced int8 OmniQuant/QAT baseline model to the corresponding precision (referred to as Sliced int8 in the tables). Models and Data. We experiment with Gemma2 (Gemma-Team, 2024) 2B, 9B, and Mistral 7B (Jiang et al., 2023) models. For OmniQuant experiments, we sample 128 examples with sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using batch size of 4. We Datasets. Following Evaluation recent work (Frantar et al., 2022; Ma et al., 2024), we evaluate all the methods based on log perplexity and average zero-shot accuracy across collection of downstream tasks. We use C4s test 6 Matryoshka Quantization Table 2 MatQuant with QAT across Gemma-2 2B, 9B and Mistral 7B models. MatQuant performs on par with the baseline for int4 and int8 while significantly outperforming it for int2. Even the int3, int6 models obtained for free through interpolation from MatQuant perform comparably to the explicitly trained baselines. Task Avg. is average accuracy on the evaluation tasks () while log pplx (perplexity) is computed on C4 validation set (). Data type Method Gemma-2 2B Gemma-2 9B Mistral 7B QAT Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. bfloat16 int8 int4 int2 int int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 68.21 67.82 67.68 67.20 67.03 67.05 39.67 47.74 52.43 67.55 67.75 67.60 60.23 61.75 62. 2.551 2.458 2.471 2.458 2.512 2.521 9.317 3.433 3.153 2.462 2.460 2.476 2.913 2.678 2. 74.38 74.17 74.77 73.25 73.26 73.71 40.35 56.02 62.32 74.12 74.31 74.55 68.57 69.9 70. 2.418 2.29 2.301 2.338 2.324 2.332 7.144 2.923 2.756 2.294 2.293 2.303 2.565 2.43 2. 73.99 73.48 72.41 71.83 72.13 71.63 38.40 54.95 61.29 73.30 72.71 72.70 65.29 68.82 66. 2.110 2.084 2.085 2.164 2.105 2.111 10.594 2.699 2.474 2.088 2.077 2.089 2.441 2.197 2. set to calculate perplexity, and for downstream evaluations, we test on ARC-c, ARC-e (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and Winogrande (Sakaguchi et al., 2020). 4.1. MatQuant with OmniQuant Table 1 shows the efficacy of MatQuant when used with FFN-only OmniQuant and compared to explicitly trained OmniQuant baselines for the target precisions, i.e., int8, int4, and int2, across all the models. While the average downstream accuracy of MatQuant for int8 and int4 quantization is within 0.5% of the corresponding independently trained baselines, the int2 quantized models of MatQuant are 4.37%, 8.01%, and 6.35% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Similar trends and improvements follow when measuring performance through validation log perplexity. Further, the quantized int4 and int2 models sliced from the int8 OmniQuant baseline suffer significant drop in accuracy around int4, demonstrating that the nested structure of int8 is not well utilized. Sliced Interpolation. Beyond the target quantization granularities (int8, int4, and int2), MatQuant allows for bit-width interpolation to bit-widths not optimized during training. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is comparable to explicitly trained baselines for both precisions. 4.2. MatQuant with QAT To further demonstrate the generality of MatQuant, we experiment on the same models using the popular QAT technique. Following the trend of experimental results with OmniQuant, we show in Table 2 that the models trained using MatQuant with QAT are comparable to the explicitly trained baselines for all the targeted bit-widths of int8 and int4. However, int2 quantized models using MatQuant are 4.69%, 6.30%, and 6.34% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Sliced Interpolation. Models trained using MatQuant with QAT exhibit strong interpolative performance similar to that of MatQuant with 7 Matryoshka Quantization OmniQuant. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is comparable to explicitly trained baselines for both interpolated bit-widths. While OmniQuant only trains the auxiliary parameters needed for quantization, QAT also updates the weight parameters. This potentially results in severe overfitting to the C4 subset used in the experiments. We observe this overfitting in all the experiments presented in Table 2, where the log perplexities improve for QAT compared to OmniQuant, while the downstream accuracies suffer. This also highlights the need for high-quality data for QAT to realize its benefits; otherwise, users are better off using resource-friendly methods like OmniQuant. 4.3. Layerwise MixnMatch Alongside the strong slicing-based interpolative properties, quantization with MatQuant also enables another form of elastic and interpolative behavior through MixnMatch. MixnMatch provides mechanism to obtain combinatorial number of strong models by using different quantization granularities, from the target bit-widths i.e., int8, int4, and int2 across layers. Figure 2 shows the ability of MixnMatch to densely span the Pareto-optimal accuracy-vs-bitsper-FFN-parameter (memory/cost) trade-off for Figure 2 MixnMatch on Gemma-2 9B model trained using MatQuant with OmniQuant allows elastic pareto-optimal accuracy-vs-cost model extraction for free during deployment. Table 3 Design choice ablation for re-weighting of (int8, optimizes. Note that MatQuant Single Precison MatQuant. loss the 3 target bit-widths int2) that MatQuant explicitly (0, 0, 1) int4, Data type Weightings Gemma-2 2B Gemma-2 9B Mistral 7B int8 int4 int2 (1, 1, 1) (1, 1, 2) (2, 2, 1) 2, 2, 1) ( (1, 1, 1) (1, 1, 2) (2, 2, 1) 2, 2, 1) ( (1, 1, 1) (1, 1, 2) (2, 2, 1) 2, 2, 1) ( 67.42 67.31 67.85 67.3 66.11 66.70 66.54 66.46 55.71 57.08 55.70 55.29 Task Avg. 73.97 73.45 74.02 74.33 73.88 73.75 74.33 74. 68.52 67.93 66.72 68.25 73.46 73.41 73.82 73.82 73.13 73.29 73.5 72.97 65.99 66.28 63.49 57.85 the Gemma-2 9B model trained using MatQuant with OmniQuant sometimes even improving on the bfloat16 model accuracy. While there are many more feasible models, we only showcase the best models obtained through the strategy described in Section 3.2.1 and further expanded in Appendix A. Interestingly, the MixnMatch models with effective bit-width of 3 and 6 are as accurate as models obtained through slicing. This opens up possibilities for effective serving depending on hardware support (Section 5.4). 5. Ablations and Discussion In this section, we present design ablations to improve MatQuant. Section 5.1 discusses the effect of non-uniform weighting across target precisions (int8, int4, int2), and Section 5.2 explores enabling co-distillation of lower precision levels (int4, int2) from the highest precision quantized model (int8). During the process of extending MatQuant to all Transformer parameters, not just the FFN block, we uncovered an interesting hybrid quantization algorithm (between Baseline and MatQuant). Section 5.3 further details this method, called Single Precison MatQuant, which stabilizes the otherwise QAT baseline for all the Transformer weights. Finally, we also discuss extending MatQuant beyond integer data types and the considerations for effective deployment on current hardware. 8 Matryoshka Quantization 5.1. Weightings (ğœ†ğ‘Ÿ) for MatQuant Depending on the constraints, we may wish to maximize the accuracy of one of the target bitwidths in MatQuant. Equation 7 provides general formulation of MatQuant that supports grid search on the weights ğœ†ğ‘Ÿ for bit-width ğ‘Ÿ. The results in Section 4 are with the weights that have balanced performance across target precisions. Table 3 shows the weight multiplier ablation results for Gemma-2 2B, 9B, and Mistral 7B. While equal weighting for all precisions works well, we see that higher weights for specific precision results in increased accuracy for that bit-width. This re-weighting to improve int8 and int4 models often results in minor accuracy drop for the int2 models. We can consider re-weighting as scaling the importance of the bits during training, and finding an optimal grid-search-free recipe is an interesting research question. 5.2. Co-distillation for MatQuant Given the nested nature of the models trained using MatQuant, we explored co-distillation, where the outputs from higher-precision model are used as the target for the lower-precision nested model, either in standalone fashion or alongside the ground truth target (weighted equally). Table 4 shows the effects of co-distillation applied to MatQuant with both OmniQuant and QAT on Gemma-2 9B. While int8 and int4 show no significant improvement, the nested int2 model benefits substantially from the int8 supervision, reaching 1.65% higher accuracy than the non-codistilled MatQuant with OmniQuant. This helps us push the int2 quantized Gemma-2 9B beyond 70% average downstream accuracy for the first time across all our experiments. Co-distillation in MatQuant opens up avenues for interesting design choices that can further leverage the inherent nested structure of integer data types. 5.3. Single Precison MatQuant In Tables 1 and 2, MatQuant performs on par with the explicitly trained baselines for int4, int8, and the interpolated int3 and int6 precisions. However, the int2 models show significant accuracy improvement. To investigate this, we conducted Table 4 Design choice ablations for co-distillation within MatQuant. represents distilling the y-bit model from the x-bit model. We note that the accuracy for int2 has significantly improved while minimally impacting the other bit-widths. Data type Config. Task Avg. log pplx. Task Avg. log pplx. OmniQuant QAT int int4 int2 [8, 4, 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] 73.97 73.40 73.46 73. 73.88 73.84 73.01 73.12 68.52 69.2 70.17 69.72 2.451 2.467 2.466 2.466 2.481 2.488 2.495 2.518 2.809 2.796 2.778 2.804 74.77 74.72 74.62 74. 73.71 73.76 73.78 73.48 62.32 61.81 62.51 62.12 2.301 2.298 2.299 2.302 2.332 2.328 2.329 2.330 2.756 2.740 2.746 2.746 simple ablation in MatQuant by removing the loss terms for int4 and int8 (i.e., ğ‘… = {2} in Equation 7 or setting ğœ†4 = ğœ†8 = 0) and present the results in Table 5. We call this version of MatQuant as Single Precison MatQuant. With Single Precison MatQuant, we observe further boost of up to 1.67%, in the accuracy of int2 models at 2% accuracy drop in the corresponding int4 and int8 models int2 is still nested within int8. This improvement likely stems from the six additional bits available during MatQuant-style training to optimize the int2 representation. In the case of Single Precison MatQuant, gradient descent is free to tune these six additional bits to improve the overall quality of the int2 model. In MatQuant, since we have additional losses to preserve the performance of the int4 Table 5 Single Precison MatQuant significantly improves upon the baseline for int2 and, at times, outperforms MatQuant. Crucially, int8 and int4 performances of Single Precison MatQuant experience significant accuracy decrease (Tables 21 & 22). int2 Gemma-2 2B Gemma-2 9B Mistral 7B Method Task Avg. log pplx. Task Avg. log pplx. Task Avg. log pplx. OmniQuant S.P. MatQuant MatQuant QAT S.P. MatQuant MatQuant 51.33 57.38 55.71 47.74 53.18 52.43 3.835 3.185 3.292 3.433 3.090 3.153 60.24 68.58 68. 56.02 62.53 62.32 3.292 2.857 2.809 2.923 2.706 2.756 59.74 67.36 65.99 54.95 61.55 61.29 3.931 2.464 2. 2.699 2.435 2.474 9 Matryoshka Quantization Table 6 Extending MatQuant with QAT to FFN + Attention parameters. Baseline QAT destabilizes for int2 and int3 but improves significantly through MatQuant & Single Precison MatQuant. Data type Method Gemma-2 9B Mistral 7B QAT Task Avg. log pplx. Task Avg. log pplx. bfloat16 int8 int4 int2 int6 int Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant 74. 74.61 75.07 73.56 72.98 74.11 39.05 - 47.78 47.17 74.56 74.65 75.04 64.23 - 68.69 66.94 2. 2.353 2.374 2.43 2.40 2.436 13.116 - 3.705 3.837 2.358 2.357 2.379 2.908 - 2.569 2.91 73. 73.73 73.58 71.42 71.87 71.5 38.39 - 34.69 43.33 73.71 73.72 73.36 39.36 - 68.41 59.45 2. 2.091 2.101 2.246 2.132 2.166 12.066 - 7.564 3.806 2.094 2.093 2.106 4.918 - 2.245 2.703 and int8, the int2 performance is slightly worse than Single Precison MatQuant. However, since the int4 and int8 models are typically very close in accuracy to the bfloat16 model, MatQuant can shift some of the weights to improve the int2 model. As int4 and int8 models have substantially more quantized buckets than int2, we hypothesize that shifting some weights into adjacent buckets may not significantly affect their performance; however, it can significantly impact int2s performance. In fact, in the weight distributions presented in Fig 1c, we observe that MatQuant results in model where larger number of weights are assigned to the higher-valued buckets. Conclusively, MatQuant and Single Precison MatQuant inherently seem to be better way of doing lowbit quantization. FFN + Attention Weight Quantization. We present results for FFN + Attention quantization for QAT in Table 6. For int8, int4 and the interpolated int6 model, MatQuant performs on par with the Baseline. However, we found int2 and int3 to be very unstable while quantizing both, the FFN and the Attention parameters. Most recent works that do QAT for both the blocks Chen et al. (2024); Du et al. (2024); Liu et al. (2024a) either do some form of warm starting for the quantized parameters, or have additional distillation and auxiliary loss functions. In the naive setup of minimizing the loss with respect to the ground truth, we find QAT to be very unstable at lower precisions. However, both MatQuant and Single Precison MatQuant are very stable further highlighting the benefits brought by MatQuant style training. 5.4. Deployment Considerations Current hardware accelerators have native support for serving int8 and int4 quantized models. Additionally, custom-implemented CUDA kernels can can support various low-precision bit-widths, like int2 and int3 (Chee et al., 2024; Frantar et al., 2022). MatQuant can generate large number of models at inference time. Depending on the serving environment, we can choose between MixnMatch models and homogeneous sliced models. For example, suppose the serving environment has memory constraint equivalent to an int3 model but lacks optimized support for int3, while supporting int2. In this case, MixnMatch model performing comparably to the int3 model could be deployed. More generally, as depicted in Figure 2, MatQuant densely spans the memory-versus-accuracy curve and can be leveraged to obtain the most performant model for specific serving constraint. MatQuant can enable further research on hardware software co-design to effectively support elastic bit-widths on-the-fly during inference time. 5.5. Extension to Floating Point Extending MatQuant to floating-point representations, such as FP8 and FP4, presents significant challenges. Given that the exponent is encoded within the bit representation and contributes to the value as power of 2 (i.e., effectively log2), slicing it results in buckets whose sizes increase exponentially, unlike the integer case, where bucket sizes are constant. For example, slicing the first two bits from int8 yields buckets of 0, 64, 128, 192. Here, the bucket size (64) is constant; however, this would not be the case when slicing two exponent bits from FP8. This is promising avenue for future research that could 10 Matryoshka Quantization further unlock the benefits of MatQuant, even during large-scale pretraining. 6. Conclusions In this work, we presented MatQuant, novel multi-scale training technique that leverages the nested structure of integer data types to simultaneously optimize model weight quantization across multiple precisions (int8, int4, and int2) within single model. This general-purpose method, applicable to learning-based quantization techniques like OmniQuant and QAT, produces models with comparable accuracy to baselines for int8 and int4, while achieving significant improvements, up to 10% (using codistillation), for int2 models. MatQuant further enables bit-width interpolation and layerwise mix-and-match for flexible accuracy-cost trade-offs, promising more efficient deployment of large models across various hardware settings. Finally, MatQuant also helped discover Single Precison MatQuant, which significantly improves standalone low-bit quantization."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful to Varun Yerram, Shreya Pathak and Devvrit for assistance in setting up inference pipelines, Praneeth Netrapalli, Rakesh Shivanna, Tom Duerig, Abhijit Ogale, Jon Shlens, Ali Farhadi and Rahul Sukthankar for helpful discussions, support and feedback."
        },
        {
            "title": "References",
            "content": "E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 29(6):3341, 1984. H. Adepu, Z. Zeng, L. Zhang, and V. Singh. Framequant: Flexible low-bit quantization for transformers. arXiv preprint arXiv:2403.06082, 2024. S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. CoRR, abs/2404.00456, 2024. doi: 10.48550/ARXIV.2404.00456. URL https:// doi.org/10.48550/arXiv.2404.00456. Y. Bengio, N. LÃ©onard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense In The Thirty-Fourth in natural language. AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL https:// doi.org/10.1609/aaai.v34i05.6239. J. Chee, Y. Cai, V. Kuleshov, and C. M. De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. A. Abdolrashidi, L. Wang, S. Agrawal, J. Malmaud, O. Rybakov, C. Leichner, and L. Lew. Pareto-optimal quantized resnet is mostly 4-bit. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30913099, 2021. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. arXiv preprint Gpt-4 technical report. arXiv:2303.08774, 2023. M. Chen, W. Shao, P. Xu, J. Wang, P. Gao, K. Zhang, Y. Qiao, and P. Luo. Efficientqat: Efficient quantization-aware training for large language models. CoRR, abs/2407.11062, 2024. doi: 10.48550/ARXIV.2407.11062. URL https://doi.org/10.48550/arXiv. 2407.11062. C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, 11 Matryoshka Quantization editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 29242936. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1300. URL https: //doi.org/10.18653/v1/n19-1300. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think try you have solved question answering? CoRR, arc, the AI2 reasoning challenge. abs/1803.05457, 2018. URL http://arxiv. org/abs/1803.05457. E. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using laplacian pyramid of adversarial networks. Advances in neural information processing systems, 28, 2015. T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318 30332, 2022. F. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hajishirzi, S. Kakade, A. Farhadi, P. Jain, et al. Matformer: Nested transformer for elastic inference. arXiv preprint arXiv:2310.07707, 2023. D. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu, and N. Xu. Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 102 116. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG. 7. URL https://doi.org/10.18653/v1/ 2024.acl-long.7. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Gemma-Team. Gemma 2: Improving open language models at practical size. ArXiv, URL https: abs/2408.00118, 2024. //api.semanticscholar.org/CorpusID: 270843326. B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27042713, 2018. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https:// doi.org/10.48550/arXiv.2310.06825. S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization. In Forty-first International Conference on MaICML 2024, Vienna, Auschine Learning, July 21-27, 2024. OpenReview.net, tria, URL https://openreview.net/ 2024. forum?id=0jpbpFia8m. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quanMatryoshka Quantization tization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. International Conference on Machine Learning, pages 17461754. PMLR, 2014. T.-Y. Lin, P. DollÃ¡r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and LLM-QAT: data-free quantizaV. Chandra. tion aware training for large language models. In L. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 467484. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024. FINDINGS-ACL.26. URL https://doi.org/ 10.18653/v1/2024.findings-acl.26. Z. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary, R. Krishnamoorthi, V. Chandra, Y. Tian, and T. Blankevoort. Spinquant: LLM quantization with learned rotations. CoRR, abs/2405.16406, 2024b. doi: 10.48550/ARXIV.2405.16406. URL https://doi.org/10.48550/arXiv. 2405.16406. Y. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang, S. Wen, F. Chao, and R. Ji. Affinequant: Affine transformation quantization for large language arXiv preprint arXiv:2403.12544, models. 2024. P. A. Nair and A. S. Suggala. Cdquant: Accurate post-training weight quantization of large pre-trained models using greedy coordinate descent. CoRR, abs/2406.17542, 2024. doi: 10.48550/ARXIV.2406.17542. URL https:// doi.org/10.48550/arXiv.2406.17542. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout. In K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In The ThirtyFourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6399. URL https:// doi.org/10.1609/aaai.v34i05.6399. W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. https://api.semanticscholar. URL org/CorpusID:13756489. G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. MÃ rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 47914800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https: //doi.org/10.18653/v1/p19-1472. 13 Matryoshka Quantization A. Addition Training Details We run all our experiments on TPUv5e chips. For OmniQuant experiments, we use constant learning rate of 1ğ‘’ 3 and for QAT experiments, we linearly warmup the learning rate to 1ğ‘’ 5 for 150 and use consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with sequence length of 2048 from the C4 dataset (Raffel et al., 2020) and train using batch size of 4. We train for total of 10M tokens for all models except the int2 baseline, where we train the model for 20M tokens (Shao et al., 2023). For QAT experiments, we sample fixed set of 100M tokens from the C4 dataset and train all our models using batch size of 16 and sequence length of 8192 for single epoch. For Attn + FFN experiments with QAT, we sample fixed set of 300M tokens from C4 and train with batch size of 16 for single epoch. MixnMatch For fixed effective bits-per-FFN layer, where each layer was quantized to either int2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid, Increasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2, the central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy followed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers, and int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending and descending order, respectively, across the layers. Our experimental results demonstrated that, for given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others. Allocating higher precision (int8) to the middle layers helped preserve critical information, while the initial and final layers performed adequately with lower bit precision (int2 and int4), leading to more efficient and effective quantization scheme. B. Detailed Downstream Evaluations for OmniQuant ad QAT Tables 7, 8, 9, 10, 11, and 12 present downstream evaluation results on Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. C. Detailed Downstream Evaluations for MatQuant Re-weighting Tables 13, 14, and 15 present downstream evaluation results for OmniQuant reweighting experiments on Gemma-2 2B, Gemma-2 9B and Mistral 7B. D. Detailed Downstream Evaluations for Co-Distillation Tables 16 and 17 present the downstream evaluation and perplexity results and for MatQuant codistillation on Gemma-2 9B with OmniQuant and QAT. E. Detailed Evaluations for FFN + Attention Quantization Tables 18 and 19 present the downstream evaluation and perplexity results for FFN + Attention quantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Matryoshka Quantization F. Detailed Evaluation for Single Precison MatQuant and 23 present the downstream evaluation results Tables 20, 21, 22, comparing Single Precison MatQuant to MatQuant and the Baseline for int2 quantization of Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since Single Precison MatQuant slices 2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the Single Precison MatQuant model trained for 2-bits on int4 and int8. Downstream evaluation and perplexity results for this are presented in Tables 21 and 22. We also plot the weight distribution for Single Precison MatQuant in Figure 3. Figure 3 The Figure presents the weight distribution for Gemma-2 9B when trained with Single Precison MatQuant for int2 quantization. The right-shifted quantized weight distribution is consequence of Single Precison MatQuants training mechanism that heavily optimizes for the first 2 MSBs of the int8 representation. 15 Matryoshka Quantization Table 7 Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Gemma-2 2B. Data type Method Gemma-2 2B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 50.09 71.59 76. int8 int4 int2 int6 int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 50 48.04 41.81 48.46 45. 23.81 31.31 34.39 48.55 49.32 47.1 23.21 46.25 44.45 71.46 71.8 66.2 70.96 70.29 23.53 53.58 59. 71.25 71.76 71.46 34.43 68.64 68.56 76.36 75.78 71.35 74.22 74.8 53.06 62.2 62.69 75.87 76.48 76. 58.2 72.97 69.11 69.69 69.76 67.64 62.64 67.66 66.07 24.78 40.78 52.11 69.18 69.52 67. 30.48 62.24 62.28 78.29 78.24 78.07 75.95 77.26 77.58 51.8 66.05 69.86 78.35 78.56 77. 56.69 76.06 75.95 63.14 63.69 63.22 59.91 63.61 62.27 49.09 54.06 55.56 62.75 62.75 63. 49.01 60.06 62.59 68.21 68.25 67.42 62.98 67.03 66.11 37.68 51.33 55.71 67.66 68.06 67. 42 64.37 63.82 Table 8 Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Gemma-2 9B. Data type Method Gemma-2 9B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 58.96 77.57 83.33 int int4 int2 int6 int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 59.47 58.11 55.97 58.79 57.25 23.21 39.16 48. 59.04 59.22 58.87 35.84 57.17 55.46 77.31 78.03 75.04 78.37 77.36 24.92 63.43 72.18 77.53 77.27 78. 57.32 77.06 76.14 83.94 83.27 81.19 83.55 84.86 38.13 72.11 79.2 84.68 83.21 83.61 67.61 83.79 84. 77.31 77.35 76.17 73.81 76.71 75.52 25.37 52.24 68.11 77.1 77.1 76.18 48.58 74.45 74. 81.12 81.39 81.18 80.52 81.45 81.5 51.36 72.63 76.17 81.23 81.12 81.45 68.61 80.36 80. 67.96 68.11 67.09 66.61 67.09 66.77 51.54 61.88 66.77 68.11 67.48 67.09 56.59 66.54 67. 74.38 74.59 73.97 72.19 74.33 73.88 35.75 60.24 68.52 74.61 74.23 74.21 55.76 73.23 72. 16 Matryoshka Quantization Table 9 Table presents the downstream evaluation results for MatQuant when applied to OmniQuant on Mistral 7B. Data type Method Mistral 7B OmniQuant ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 49.57 73.74 84. int8 int4 int2 int6 int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 49.23 48.04 27.65 49.23 48. 23.72 36.69 41.38 48.98 50.26 48.46 22.78 46.33 45.65 73.19 73.44 46.72 73.23 72.69 25.29 61.36 67. 72.01 73.65 72.98 24.66 70.71 71.21 83.88 84.13 49.17 83.94 83.49 43.21 70.06 71.62 83.46 84.04 84. 37.86 82.72 80.43 80.61 80.41 79.37 36.88 79.9 78.82 25.45 57.47 71.98 79.95 80.55 79. 24.12 77.74 78.31 81.18 81.39 81.12 64.09 81.34 81.12 50.49 70.67 77.86 81.72 81.66 81. 49.24 80.74 81.07 74.43 74.51 74.66 55.01 74.11 74.43 49.33 62.19 65.67 74.9 74.43 75. 48.93 71.82 72.61 73.99 73.77 73.46 46.59 73.62 73.13 36.25 59.74 65.99 73.5 74.1 73. 34.6 71.68 71.55 Table 10 Table presents the downstream evaluation results for MatQuant when applied to QAT on Gemma-2 2B."
        },
        {
            "title": "Data type Method",
            "content": "Gemma-2 2B"
        },
        {
            "title": "QAT",
            "content": "ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "bfloat16 50.09 71.59 76.45 int8 int int2 int6 int"
        },
        {
            "title": "Baseline\nMatQuant",
            "content": "Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 47.78 46.25 46.08 46.16 44. 25.6 24.66 28.24 47.78 47.7 46.5 38.74 39.68 38.65 70.66 71.21 69.36 71.59 70.45 26.3 43.22 51. 70.79 70.88 70.71 63.13 65.28 67.34 75.08 75.6 75.78 73.73 75.81 57.98 62.17 64.19 74.25 74.92 75. 65.57 67.03 70.49 69.69 69.92 69.97 68.05 68.72 68.43 25.82 38.39 46.76 69.73 69.72 69. 58.86 62.68 61.47 78.29 78.35 78.4 78.18 78.62 78.35 52.12 64.42 68.66 77.64 78.07 78. 74.81 77.04 75.41 63.14 65.11 64.64 65.75 63.38 64.88 50.2 53.59 55.01 65.11 65.19 64. 60.3 58.8 61.72 68.21 67.82 67.68 67.2 67.03 67.05 39.67 47.74 52.43 67.55 67.75 67. 60.23 61.75 62.51 17 Matryoshka Quantization Table 11 Table presents the downstream evaluation results for MatQuant when applied to QAT on Gemma-2 9B."
        },
        {
            "title": "Data type Method",
            "content": "Gemma-2 9B"
        },
        {
            "title": "QAT",
            "content": "ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "bfloat16 58.96 77.57 83.33 int8 int int2 int6 int"
        },
        {
            "title": "Baseline\nMatQuant",
            "content": "Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 58.11 58.19 57.42 56.91 57. 23.89 33.45 39.85 57.85 57.94 58.02 50 53.07 51.62 75.38 76.18 75.08 75.42 76.64 27.61 55.43 65. 75.13 76.14 75.63 68.1 75.04 71.93 80.12 81.5 78.1 75.38 75.2 57.95 62.26 65.93 80.67 79.63 81. 75.2 66.61 78.78 77.31 78.7 79.57 76.97 78.06 78.71 30.16 54.8 64.08 78.63 78.93 79. 71.31 74.94 73.99 81.12 81.5 82.15 81.23 81.39 81.66 54.68 70.51 75.68 81.56 82.1 81. 79.43 80.03 80.14 67.96 71.19 71.03 70.72 72.38 72.14 47.83 59.67 62.75 70.88 71.11 71. 67.4 69.69 67.64 74.38 74.17 74.77 73.25 73.26 73.71 40.35 56.02 62.32 74.12 74.31 74. 68.57 69.9 70.68 Table 12 Table presents the downstream evaluation results for MatQuant when applied to QAT on Mistral 7B."
        },
        {
            "title": "Data type Method",
            "content": "Mistral 7B"
        },
        {
            "title": "QAT",
            "content": "ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "bfloat16 49.57 73.74 84.4 int8 int int2 int6 int"
        },
        {
            "title": "Baseline\nMatQuant",
            "content": "Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline MatQuant 48.89 46.76 47.18 47.27 45. 25.34 29.78 34.3 48.21 47.7 47.53 40.1 44.54 38.82 71.63 70.37 70.41 70.62 68.64 26.47 48.23 55. 71.51 71.3 71 61.49 67.97 62.42 82.42 82.51 80.37 81.28 82.02 54.95 64.5 71.83 82.42 82.23 81. 72.91 73.98 77.74 80.61 81.69 79.73 79.84 78.95 79 25.18 55.11 65.89 81.67 79.84 79. 68.72 76.31 71.1 81.18 81.18 80.9 80.25 81.12 81.07 48.48 70.84 75.52 81.72 80.79 81. 77.97 79.65 78.07 74.43 75.06 74.19 72.93 73.56 73.4 49.96 61.25 65.11 74.27 74.43 74. 70.56 70.48 70.48 73.99 73.48 72.41 71.83 72.13 71.63 38.4 54.95 61.29 73.3 72.71 72. 65.29 68.82 66.44 18 Matryoshka Quantization Table 13 Tables presents the downstream evaluation results on Gemma-2 2B for MatQuant loss reweighting when applied to OmniQuant. Weightings: (ğ‘¥, ğ‘¦, ğ‘§) (ğœ†8, ğœ†4, ğœ†2) (from Equation 7). Data type Weightings ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "Gemma-2 2B int8 int4 int2 int6 int (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, 2) (1 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( 48.04 47.35 47.44 47.7 48.38 48.46 47.95 47.35 45.65 46.33 46.42 45.56 46.84 47.35 45.9 46.33 34.39 32.76 35.07 34.22 34.47 33.45 34.04 32. 47.1 47.44 47.61 47.78 48.55 48.29 48.38 47.44 44.45 43.17 41.98 41.64 41.98 42.66 43.17 43.17 71.8 71.34 72.43 71.89 72.31 71.84 71.72 71.34 70.29 70.92 70.96 71.55 70.88 71.68 70.83 70.92 59.64 56.99 62.04 60.4 57.95 57.49 58.84 56.99 71.46 71.42 71.89 71.63 72.69 71.76 71.51 71. 68.56 68.73 68.6 66.71 68.35 66.54 66.71 68.73 75.78 75.66 76.02 75.63 76.3 75.93 75.26 75.66 74.8 73.7 74.71 75.75 74.92 72.69 75.11 73.7 62.69 63.46 65.78 64.98 63.94 65.02 65.11 63.46 76.02 74.95 75.9 75.47 76.3 75.72 75.84 74.95 69.11 64.74 70.34 71.62 68.41 70.46 60.03 64. 67.64 67.99 67.45 67.21 68.32 68.35 68.13 67.99 66.07 67.67 65.78 66.18 66.48 66.79 66.97 67.67 52.11 51.99 54.26 54.3 51.84 52.22 51.77 51.99 67.47 67.85 67.37 67.2 68.02 68.42 68.24 67.85 62.28 61.31 61.95 61.94 63.74 63.61 62.71 61.31 78.07 78.07 78.02 78.07 78.35 77.91 78.07 78. 77.58 77.26 77.58 77.48 77.91 77.26 77.37 77.26 69.86 70.29 71.65 71.38 69.75 70.4 70.89 70.29 77.91 77.86 78.24 77.86 78.67 78.02 78.18 77.86 75.95 76.39 75.9 76.01 76.17 75.63 76.77 76.39 63.22 63.38 63.85 63.38 63.46 63.14 62.75 63.38 62.27 62.9 63.14 63.69 62.19 63.38 62.27 62. 55.56 56.27 56.27 57.22 56.27 55.64 57.14 56.27 63.61 63.3 63.77 63.61 63.85 63.38 63.85 63.3 62.59 61.48 63.3 61.09 60.77 62.98 61.64 61.48 67.42 67.3 67.54 67.31 67.85 67.6 67.31 67.3 66.11 66.46 66.43 66.7 66.54 66.52 66.41 66.46 55.71 55.29 57.51 57.08 55.7 55.7 56.3 55. 67.26 67.14 67.46 67.26 68.01 67.6 67.67 67.14 63.82 62.64 63.68 63.17 63.24 63.65 61.84 62.64 19 Matryoshka Quantization Table 14 Tables presents the downstream evaluation results on Gemma-2 9B for MatQuant loss reweighting when applied to OmniQuant. Weightings: (ğ‘¥, ğ‘¦, ğ‘§) (ğœ†8, ğœ†4, ğœ†2) (from Equation 7). Data type Weightings ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "Gemma-2 9B int8 int4 int2 int6 int (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, 2) (1 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( 58.11 57.68 58.11 56.91 58.79 58.53 58.62 59.13 57.25 56.74 57.42 57.59 58.62 58.19 58.28 57.94 48.72 49.83 48.55 48.29 46.76 46.76 46.76 46. 58.87 57.51 58.79 57.34 59.04 59.22 58.36 59.3 55.46 56.23 56.4 55.63 55.2 54.44 56.14 56.31 78.03 77.4 77.86 77.1 77.48 77.31 77.27 78.07 77.36 77.74 78.28 77.82 78.28 77.82 78.16 78.11 72.18 73.91 74.24 72.94 73.27 73.7 72.35 72.6 78.03 77.53 77.82 77.23 77.4 77.65 77.82 78. 76.14 76.05 77.86 76.05 76.56 75.63 75.67 77.4 83.27 83.73 81.04 82.39 82.66 82.63 84.31 84.16 84.86 85.08 82.51 84.28 83.67 83.91 84.53 84.98 79.2 78.75 81.5 74.74 71.96 77.65 75.35 79.3 83.61 83.55 81.38 82.57 82.66 82.17 83.79 84.5 84.04 82.6 80.64 82.39 84.19 80.55 83.33 83. 76.17 76.1 76 75.93 76.55 76.54 76.54 76.46 75.52 75.5 75.97 75.32 76.01 76.62 76.41 76.5 68.11 67.27 68.44 68.34 67.98 67.01 67.51 67.58 76.18 75.98 76.21 75.89 76.55 76.62 76.47 76.57 74.49 74.85 75.11 74.21 74.87 74.97 74.96 75.62 81.18 81.18 81.18 81.18 81.23 80.96 81.34 80. 81.5 80.85 81.34 81.12 81.5 81.99 81.72 81.01 76.17 77.2 76.5 77.58 76.77 77.58 76.39 77.69 81.45 80.9 81.07 81.12 81.56 81.23 81.23 80.85 80.14 80.9 79.87 80.3 80.2 80.96 80.52 80.41 67.09 67.64 67.09 67.17 67.4 67.56 66.85 67.25 66.77 66.85 67.56 66.38 67.88 67.72 67.09 67. 66.77 66.46 65.9 65.67 63.61 65.98 67.56 65.75 67.09 67.17 67.72 67.17 68.03 67.8 67.25 67.4 67.32 67.01 68.51 67.17 67.72 67.72 67.72 66.54 73.97 73.95 73.55 73.45 74.02 73.92 74.16 74.33 73.88 73.79 73.85 73.75 74.33 74.37 74.36 74.26 68.52 68.9 69.19 67.93 66.72 68.11 67.65 68. 74.21 73.77 73.83 73.55 74.21 74.11 74.15 74.5 72.93 72.94 73.06 72.62 73.12 72.38 73.06 73.25 20 Matryoshka Quantization Table 15 Tables presents the downstream evaluation results on Mistral 7B for MatQuant loss reweighting when applied to OmniQuant. Weightings: (ğ‘¥, ğ‘¦, ğ‘§) (ğœ†8, ğœ†4, ğœ†2) (from Equation 7). Data type Weightings ARC-c ARC-e BoolQ HellaSwag"
        },
        {
            "title": "PIQA Winogrande Average",
            "content": "Mistral 7B int8 int4 int2 int6 int (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, 2) (1 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( (1, 1, 1) 2, (1 2) 2, 1, 2) ( (1, 1 2) (2, 2, 1) 2, 2, 1) 2, 1) 2, 1) ( (2, 2, ( 48.04 48.46 47.95 48.21 49.06 49.06 48.98 48.98 48.21 49.15 47.95 48.46 49.15 48.89 47.87 48.29 41.38 40.78 40.36 40.36 37.2 37.29 39.68 34. 48.46 49.06 47.95 48.38 48.46 48.81 49.4 49.23 45.65 47.7 46.33 45.99 47.95 44.45 46.84 47.01 73.44 73.19 73.4 73.02 73.48 73.57 73.95 73.86 72.69 72.81 72.43 73.44 72.81 72.69 72.05 72.47 67.42 66.2 67.09 67.17 62.46 64.35 65.24 61.24 72.98 73.44 73.48 72.94 72.94 73.48 73.65 73. 71.21 72.05 72.43 71.09 73.36 69.7 72.73 71.59 84.13 84.28 84.46 84.34 84.74 84.56 84.50 84.56 83.49 83.39 83.43 84.07 83.88 82.72 83 82.84 71.62 73.61 75.35 74.83 67.74 61.1 68.93 60.61 84.07 84.59 84.43 84.34 84.13 84.34 84.4 84.43 80.43 82.81 81.8 80.73 82.57 82.11 80.95 81. 79.37 79.19 79.11 79.03 79.73 79.64 79.60 79.55 78.82 78.71 79.24 78.9 79.8 79.53 79.56 79.52 71.98 72.68 72.46 71.64 70.29 68.88 66.64 58.07 79.64 79.51 79.28 79.15 79.89 79.67 79.68 79.55 78.31 78.74 79.03 78.77 79.31 77.68 78.79 78.89 81.12 81.12 81.34 81.28 81.56 81.39 81.61 81. 81.12 80.79 81.01 81.01 81.88 81.66 81.23 81.07 77.86 77.75 77.48 77.53 76.55 74.86 75.19 72.63 81.18 81.28 81.45 81.18 81.5 81.34 81.28 81.12 81.07 81.12 82.1 80.85 81.39 80.2 81.56 81.39 74.66 74.74 74.51 74.59 74.35 74.27 74.90 74.74 74.43 74.66 74.03 73.88 73.48 73.88 74.27 73. 65.67 67.4 65.9 66.14 66.69 65.19 64.09 59.98 75.22 74.74 75.14 74.59 74.9 74.9 74.74 74.66 72.61 72.77 73.4 72.53 74.9 71.74 73.01 72.45 73.46 73.5 73.46 73.41 73.82 73.75 73.92 73.82 73.13 73.25 73.01 73.29 73.5 73.23 73 72.97 65.99 66.4 66.44 66.28 63.49 61.94 63.29 57. 73.59 73.77 73.62 73.43 73.64 73.76 73.86 73.76 71.55 72.53 72.51 71.66 73.25 70.98 72.31 72.22 21 Matryoshka Quantization Table 16 Table presents the downstream evaluation and perplexity results for our MatQuant codistillation experiments on Gemma-2 9B with OmniQuant. OmniQuant Gemma-2 9B Data type Config. ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 int4 int2 int6 int3 [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] 57.59 57.17 56.4 57.68 57.51 56. 48.81 49.15 49.83 57.42 57.51 56.4 55.63 54.35 55.2 77.27 77.36 77.82 78.45 77.61 77.99 74.03 75.34 75. 77.19 77.48 78.03 75.88 76.85 76.98 81.83 82.2 82.32 82.97 80.46 82.54 81.65 83.12 79.79 81.87 82.32 82. 80.12 79.33 82.45 75.48 75.82 75.02 75.5 74.74 74.77 68.1 68.79 68.38 75.42 75.88 75.14 74.01 74.6 73. 81.01 80.96 80.63 80.85 81.12 80.58 77.48 77.64 77.86 81.01 81.07 80.79 80.36 80.47 80.41 67.25 67.25 67. 67.56 66.61 66.3 65.11 67.01 67.4 67.8 66.61 67.4 67.96 67.4 68.43 73.4 73.46 73.32 73.84 73.01 73. 69.2 70.17 69.72 73.45 73.48 73.4 72.33 72.17 72.84 2.467 2.466 2.466 2.488 2.495 2.518 2.796 2.778 2. 2.468 2.467 2.498 2.549 2.543 2.58 Table 17 Table presents the downstream evaluation and perplexity results for our MatQuant codistillation experiments on Gemma-2 9B with QAT. QAT Gemma-2 9B Data type Config. ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 int int2 int6 int3 [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] [8, 4, 8 2] [8, 4, 2, 8 2] [8, 4, 2, 8 4; 2] 58.11 57.51 58.11 57.42 56.91 57.51 39.51 40.78 40.19 57.85 57.17 57. 51.96 50.94 51.45 76.43 76.43 76.14 76.35 75.8 75.76 65.03 66.5 65.7 76.09 75.97 76.09 71.55 71.76 72. 81.25 81.53 81.68 77.55 78.44 75.96 66.88 67.55 65.57 81.47 82.2 82.29 78.07 78.78 78.84 79.12 78.95 79. 78.06 77.76 77.96 63.37 63.67 63.83 78.98 79 78.95 73.17 73.09 73.46 82.05 82.1 82.26 81.61 81.39 81. 75.08 75.95 75.3 81.88 81.83 82.10 79.43 79.05 79.6 71.35 71.19 71.51 71.59 72.38 71.98 61.01 60.62 62. 71.27 71.9 71.27 66.93 66.77 67.96 74.72 74.62 74.8 73.76 73.78 73.48 61.81 62.51 62.12 74.59 74.68 74. 70.18 70.06 70.62 2.298 2.299 2.302 2.328 2.329 2.33 2.74 2.746 2.746 2.301 2.302 2.305 2.485 2.486 2. 22 Matryoshka Quantization Table 18 Table presents the downstream evaluation results for MatQuant FFN + Attention quantization on Gemma-2 9B with QAT. Data type Method Gemma-2 9B ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 58.96 77.57 83. int8 int4 int2 int6 int3 Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant 58.62 59.04 57.42 56.06 57. 24.74 - 24.91 28.24 58.53 58.87 59.81 43.6 - 50.85 45.22 77.02 77.9 76.73 74.96 76.77 25.63 - 41.62 39. 77.15 77.06 77.9 64.98 - 73.11 69.32 83.43 84.4 81.62 79.27 84.19 58.53 - 62.26 62.17 82.48 83.12 84. 72.66 - 71.13 78.5 77.31 79.01 78.76 76.02 77.83 77.51 25.5 - 40.87 39.13 79.04 78.81 78. 66 - 72.01 68.72 81.12 81.34 81.12 80.58 80.25 80.74 50.71 - 63.38 63.49 81.5 81.23 81. 75.95 - 79.38 76.01 67.96 68.27 69.22 68.98 69.53 68.11 49.17 - 53.67 50.75 68.67 68.82 67. 62.19 - 65.67 63.85 74.38 74.61 75.07 73.56 72.98 74.11 39.05 - 47.78 47.17 74.56 74.65 75. 64.23 - 68.69 66.94 23 Matryoshka Quantization Table 19 Table presents the downstream evaluation results for MatQuant FFN + Attention quantization on Mistral 7B with QAT. Data type Method Mistral 7B ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average bfloat16 49.57 73. 84.4 int8 int4 int2 int6 int Baseline MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant Sliced int8 Baseline MatQuant Sliced int8 Baseline S.P. MatQuant MatQuant 49.23 49. 45.99 48.04 47.01 22.78 - 23.21 22.27 49.32 49.32 49.15 20.65 - 41.98 34.64 72.9 72.31 71.76 71.72 69. 24.03 - 23.82 32.49 73.53 73.4 71.76 31.57 - 65.53 55.13 83.49 83.76 81.41 78.87 82.02 58.75 - 37.83 62. 82.66 82.48 83.73 44.34 - 79.39 70.43 80.61 80.26 80.2 76.95 78.93 76.81 24.63 - 24.67 32. 80.16 80.24 80.13 28.79 - 74.42 58.61 81.18 81.28 81.18 80.41 80.36 80.25 50.54 - 49.02 59. 81.12 81.28 81.18 59.41 - 79.22 73.39 74.43 75.22 74.74 71.98 73.32 72.93 49.64 - 49.57 51. 75.45 75.61 74.19 51.38 - 69.93 64.48 73.99 73.73 73.58 71.42 71.87 71.5 38.39 - 34.69 43. 73.71 73.72 73.36 39.36 - 68.41 59.45 Table 20 Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2 quatization of Gemma-2 2B with OmniQuant and QAT. int2 Gemma2-2B Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Task Avg. log pplx. OmniQuant QAT S.P. MatQuant Baseline MatQuant S.P. MatQuant Baseline MatQuant 34.64 31.31 34.39 28.92 24.66 28.24 64.06 53.58 59.64 53.79 43.22 51.73 65.69 62.2 62. 62.84 62.17 64.19 53.07 40.78 52.11 48.41 38.39 46.76 69.7 66.05 69.86 69.86 64.42 68.66 57.14 54.06 55. 55.25 53.59 55.01 57.38 51.33 55.71 53.18 47.74 52.43 3.185 3.835 3.292 3.090 3.433 3.153 Matryoshka Quantization Table 21 Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with OmniQuant. Note that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model were sliced post training. Gemma-2 9B Data type Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 int4 int2 S.P. MatQuant OmniQuant MatQuant S.P. MatQuant OmniQuant MatQuant S.P. MatQuant OmniQuant MatQuant 56.48 59.47 58.11 57.17 58.79 57.25 49.74 39.16 48.72 76.85 77.31 78. 77.02 78.37 77.36 74.66 63.43 72.18 73.36 83.94 83.27 74.28 83.55 84.86 80.92 72.11 79.2 74.87 77.35 76. 74.41 76.71 75.52 66.57 52.24 68.11 80.74 81.39 81.18 80.69 81.45 81.5 76.06 72.63 76.17 66.77 68.11 67. 67.56 67.09 66.77 63.54 61.88 66.77 71.51 74.59 73.97 71.85 74.33 73.88 68.58 60.24 68.52 2.525 2.418 2. 2.543 2.451 2.481 2.857 3.292 2.809 Table 22 Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2, int4, int8 quatization of Gemma-2 9B with QAT. Note that the model was trained with Single Precison MatQuant for int2, the int4 and int8 model were sliced post training. Gemma-2 9B Data type Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Average log pplx. int8 int4 int S.P. MatQuant QAT MatQuant S.P. MatQuant QAT MatQuant S.P. MatQuant QAT MatQuant 55.97 47.78 46.25 55.2 46.16 44.37 41.21 33.45 39. 76.18 70.66 71.21 76.01 71.59 70.45 66.2 55.43 65.66 80.09 75.08 75.6 74.74 73.73 75.81 65.02 62.26 65. 75.43 69.92 69.97 74.19 68.72 68.43 64.31 54.8 64.08 80.69 78.35 78.4 80.41 78.62 78.35 76.06 70.51 75. 68.9 65.11 64.64 68.9 63.38 64.88 62.35 59.67 62.75 72.88 67.82 67.68 71.57 67.03 67.05 62.53 56.02 62. 2.429 2.29 2.301 2.429 2.324 2.332 2.706 2.923 2.756 Table 23 Table presents downstream evaluation and perplexity results for Single Precison MatQuant, comparing it with MatQuant and the Baseline for int2 quatization of Mistral 7B with OmniQuant and QAT. int2 Mistral 7B Method ARC-c ARC-e BoolQ HellaSwag PIQA Winogrande Task Avg. log pplx. OmniQuant QAT S.P. MatQuant Baseline MatQuant S.P. MatQuant Baseline MatQuant 39.93 36.69 41.38 34.64 29.78 34.3 66.25 61.36 67.42 56.19 48.23 55. 76.97 70.06 71.62 70.73 64.5 71.83 72.99 57.47 71.98 66.77 55.11 65.89 78.07 70.67 77.86 75.52 70.84 75. 69.93 62.19 65.67 65.43 61.25 65.11 67.36 59.74 65.99 61.55 54.95 61.29 2.464 3.931 2.569 2.435 2.694 2."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}