{
    "paper_title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding",
    "authors": [
        "Jiahao Meng",
        "Shuyang Sun",
        "Yue Tan",
        "Lu Qi",
        "Yunhai Tong",
        "Xiangtai Li",
        "Longyin Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose a novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces a cybernetic loop consisting of an MLLM Inference System, a Sensor, and a Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at https://github.com/marinero4972/CyberV."
        },
        {
            "title": "Start",
            "content": "CyberV: Cybernetics for Test-time Scaling in Video Understanding Jiahao Meng1 Shuyang Sun2 Yue Tan1 Lu Qi2 Yunhai Tong Xiangtai Li2 Longyin Wen2 1 Peking University 2 ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces cybernetic loop consisting of an MLLM Inference System, Sensor, and Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at https://github.com/marinero4972/CyberV. Date: June 10, 2025 Correspondence: Yunhai Tong at yhtong@pku.edu.cn, Xiangtai Li at xiangtai.li@bytedance.com 5 2 0 2 9 ] . [ 1 1 7 9 7 0 . 6 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Understanding dynamic visual scenes in videos is fundamental challenge, with applications ranging from autonomous driving to content analysis and human-robot interaction. Multimodal Large Language Models (MLLMs) have recently emerged as powerful paradigm, demonstrating impressive capabilities by integrating pre-trained large language models with visual encoders to process and reason about video content [4, 49, 53, 65]. However, deploying these models effectively, particularly for understanding long, complex, or nuanced videos at test time, presents significant hurdles. Current MLLMs often struggle with the computational demands of processing extended video streams (test-time scaling), exhibit brittleness to variations or unexpected 1 Figure 1 Performance comparison on the VideoMMMU benchmark. Left: CyberV boosts small open-source models with only 7B parameters to outperform GPT-4o; with larger model, CyberV surpasses the prior state-of-the-art and approaches the human level. Right: CoT reasoning improves results when using Qwen2.5-VL-7B, but multi-round reflection via Wait degrades performance. events in the input (lack of robustness), and are prone to generating inaccurate, inconsistent, or hallucinatory interpretations (limited accuracy) [13, 21, 24, 29, 65]. In particular, as shown in Figure 1, directly adding the reflection prompts to the output during inference does not perform well in video understanding. Even the current state-of-the-art MLLM cannot effectively scale up its capabilities. We argue that these limitations stem from the feed-forward processing pipeline inherent in current MLLM architectures. These models typically process videos in single, often computationally intensive, pass, lacking mechanisms for dynamic adaptation, self-correction, or targeted analysis based on evolving understanding or specific task demands. This contrasts sharply with biological systems, which continuously use feedback to regulate behavior and adapt to complex environments. To address this gap, we propose incorporating principles from cybernetics the study of control, communication, and self-regulation in systems [3, 57] into the design of MLLMs for video understanding. Cybernetics provides rich theoretical framework centered on feedback loops, adaptive control, and goal-oriented behavior, enabling stable and effective operation in complex, dynamic settings. We hypothesize that by redesigning video MLLMs as cybernetic systems, capable of self-monitoring, self-correction, and adaptive resource allocation during inference, we can significantly enhance their performance. Implementing these principles, we introduce CyberV, framework structured as dynamic feedback loop to create more robust and accurate MLLMs for video understanding. The core of this cybernetic system consists of three components: the MLLM Inference System, the Sensor and the Controller. Specifically, we generate responses by applying various scaling strategies within the MLLM inference system. These responses may come directly from the base MLLM model or be enhanced through techniques such as chain-of-thought prompting or the incorporation of key frames. The sensor monitors the inference processes and collect intermediate interpretations, such as the attention drift among different outputs and the prediction results, as evidence for the re-analysis of the controller. Given the evidence, the controller calculates the confidence score and determines whether to terminate the loop or to forcibly trigger self-correction process to avoid unreliable responses during inference. If the termination condition is not met, the controller takes action by injecting the generated feedback into the MLLMs input for the next round of inference, closing the entire cybernetic loop. We demonstrate the efficacy of the proposed cybernetically-inspired mechanisms on challenging video understanding benchmarks. Our experiments demonstrate that our method can remarkably improve the performance of relatively small model by large margin. As shown in Figure 1, it improves the accuracy of Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5%, allowing both models to surpass GPT-4o on the VideoMMMU benchmark. Furthermore, when applied to larger model, Qwen2.5-VL-72B, our approach 2 yields 10.0% improvement over the baseline, achieving performance comparable to human experts. In addition to excelling on knowledge-centric benchmarks that benefit from logical reasoning, our method can also generalize to broader video domains: it delivers consistent gains on general-purpose benchmarks such as VideoMME and WorldSense, with 1.1% improvements on both. Extensive ablation studies show the effectiveness of each component in our CyberV framework. Our contributions can be summarized as follows: 1. We propose CyberV, test-time adaptive scaling framework based on cybernetic feedback control that enhances frozen MLLMs without training or extra components (vision expert models). 2. We introduce an attention-based monitoring mechanism and an adaptive scoring controller that jointly govern strategy selection during inference. 3. CyberV empowers small models to outperform proprietary systems like GPT-4o, and enables large opensource models to achieve state-of-the-art results on VideoMMMU. Extensive experiments demonstrate the effectiveness and the generalization capability of our approach."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-modal Large Language Models in Video. Multi-modal large language models (MLLMs) have recently achieved significant success [5, 27, 37, 38, 43, 46, 58, 66], with the video domain receiving growing attention. In recent years, numerous video-specific multimodal large language models (MLLMs) [12, 21, 32, 47, 55, 6062, 65] have been developed to tackle challenges inherent in video understanding, such as temporal reasoning and crossframe alignment. Meanwhile, several open-sourced foundation models such as the Qwen-VL series [4, 5, 52], InternVL series [911, 66] aim to unify image and video processing within single framework. While these architectures demonstrate impressive perception capabilities, most existing MLLMs struggle with complex reasoning over video content. To address this gap, recent efforts [19, 40, 42, 44, 45] introduce reinforcement learning and test-time scaling strategies to enhance language model reasoning, and have been extended to video understanding through models such as Video-R1 [14], VideoChat-R1 [33], and TinyLLaVA-Video-R1 [64]. In parallel, video chain-of-thought (CoT) prompting methods such as Video-of-Thought [13], Chain-of-Shot [24], Logic-in-Frames [20], R3CoT [23] decompose complex video reasoning tasks into manageable sub-problems, addressing them step-by-step from low-level perception to high-level cognition. However, most existing approaches require supervised post-training or auxiliary models. In contrast, our work explores training-free, single-model strategy that performs strongly on knowledge-intensive video tasks, suggesting that simple, modular inference techniques can yield robust multi-modal reasoning. Test Time Scaling. This direction [6, 7, 28, 36, 40, 48, 56] is promising strategy for improving LLM performance by allocating more compute during inference rather than increasing model size. TTS methods generally fall into two categories: Sequential Scaling, which prolongs the reasoning process (e.g. chain of thought [56], reflection [40]); and Parallel Scaling, which explores multiple reasoning paths and selects the best (e.g. best-of-n [7, 28]). Parallel methods are often combined with sequential strategies to form complex search procedures, such as tree search [6, 18, 36], with majority voting [54], output reward models (ORMs) [1, 59] and process reward models (PRMs) [34, 50] often used to verify reasoning steps. Recent works have instantiated these strategies in various ways: s1 [40] adopts sequential scaling via lightweight budget-forcing mechanism to control inference depth adaptively, while [36] formulates compute-optimal policies for test-time search by balancing performance and computational cost. While effective in textual tasks, TTS remains underexplored in video understanding. Our findings suggest that directly applying existing techniques often yields limited gains, highlighting the need for modality-aware scaling strategies. Cybernetics in Machine Learning and AI Systems. Cybernetics, first formalized by Wiener [57], provides theoretical framework for self-regulating systems composed of three core components: sensor for observing system states, controller for decision-making, and plant or system being controlled [17, 39, 57]. While these principles influenced early AI research [3, 39, 51], their integration into modern deep learning remains limited [8]. Some recent works have introduced feedback mechanisms into neural architectures, such as CNN-F [26], which adds recurrent generative feedback to improve adversarial robustness, and AdaRefiner [63], 3 which establishes closed-loop interaction between language models and RL agents. However, these approaches often require architectural modifications or specialized training. In contrast, our approach utilizes the MLLM Inference System, Sensor, and Controller to apply cybernetic principles. By modeling video understanding with these components, we significantly improve performance without the need for additional training."
        },
        {
            "title": "3.1 Cybernetic View for Video Test-Time Scaling",
            "content": "Test-time scaling for multimodal large language models (MLLMs), particularly in video understanding tasks, presents significant challenges. Unlike text-only reasoning, where techniques such as chain-of-thought prompting can be applied with moderate success, MLLMs face greater complexity due to the temporal, visual, and semantic richness of video data. Existing approaches [5, 13, 24, 29, 65, 66] often apply static inference strategies that do not adapt to input difficulty, uncertainty, or reasoning failure, leading to inefficiencies and suboptimal performance. To overcome these limitations, we propose cybernetic framework that transforms test-time inference into feedback-driven, adaptive process inspired by the principles of control and regulation in cybernetics. We conceptualize video understanding during inference as closed-loop control system consisting of three interdependent components: MLLM Inference System: This is the plant in the cybernetic loop, responsible for executing inference over multimodal input. These responses serve as raw material for further evaluation. Sensor: The Sensor monitors the forward execution of the MLLM and extracts key signals such as predicted options and attention drift among different responses. These signals reflect the inference reliability, forming the basis for later decision-making. Controller: The Controller is the central decision-making unit of the cybernetic system. It receives multiple signals from the Sensor, and evaluates the confidence of each candidate response using rule-based scoring ensemble. Based on thresholding policy, it decides whether to accept the output or trigger further inference with generated feedback. Specifically, given video , query that includes the question and the video subtitles (if available), and strategy set Π = {π1, . . . , πN }, the frozen model generates candidate responses {ri}N i=1, where each ri can be expressed as ri = Mπi(q, ). These responses are then passed back to the Sensor and Controller for evaluation, forming closed feedback loop. This iterative process allows the model to monitor its own interpretations, adjust inference paths in real time, and allocate computational resources more efficiently based on task relevance and video complexity. Unlike prior test time scaling approaches [40, 56] that apply fixed reasoning templates regardless of context, our method dynamically modulates processing depth and focus, enhancing both accuracy and robustness. As illustrated in Figure 2, the proposed framework CyberV implements this cybernetic loop to adaptively scale inference at test time without any parameter updates or supervision. It empowers frozen model to handle diverse video understanding tasks by actively managing its reasoning process in response to control signals. The pseudo-code of CyberV is presented in the Appendix D."
        },
        {
            "title": "3.2 MLLM Inference System: Executing Test-time Scaling Strategies",
            "content": "The MLLM inference system, serving as the plant of the cybernetic loop, can execute diverse test-time scaling strategies over the multimodal input. We adopt the Best-of-N (BoN) scheme that executes forward passes in parallel to generate set of candidate responses. Each inference path may vary in its configuration, including direct answer using the base model, chain-of-thought prompting to encourage reasoning, such as Thinking Process:, or the incorporation of visually enhanced inputs such as injected key frames. This design enables the system to adaptively combine structured reasoning and perceptual reinforcement based on task uncertainty. Compared to more complex search strategies, such as performing tree search, Best-of-N offers simpler yet effective alternative. 4 Figure 2 Overall framework of CyberV. CyberV models test-time video understanding as closed-loop cybernetic process comprising three modules: MLLM Inference System, Sensor, and Controller. The MLLM Inference System executes inference scaling strategies with frozen MLLM on multi-modal input, generating outputs. The Sensor monitors the forward process of the MLLM, extracting intermediate signals such as parsed prediction and attention drift. The Controller aggregates multiple signals to evaluate response reliability with Score Forest and triggers an self-correction actions when confidence falls below threshold through Inference Feedback module. The updated input is then used to re-invoke the MLLM. This feedback loop enables adaptive and robust test-time reasoning."
        },
        {
            "title": "3.3 Sensor: Signal Extraction from MLLM Forward Processes",
            "content": "The Sensor monitors the forward execution of the MLLM and extracts informative signals that serve as the basis for confidence evaluation and feedback decisions. n=1, obtained by parsing each of the textual responses n=1, where ˆyn C, and is the candidate set of choices in the predictions, e.g. A, B, C, etc. The One key signal is the predicted answer label {ˆyn}N {rn}N parsing relies on explicit pattern matching or approximate content alignment to handle free-form text. Additionally, the Sensor evaluates the models perceptual behavior by quantifying attention drift. As the video and subtitles can be segmented according to the number of frames and timestamps, we compare the attention distribution over these segments across two settings: base response and chain-of-thought prompting variant. Specifically, the video is divided into K1 segments and the subtitles into K2 segments. For each attention head {1, . . . , H}, where is the total number of attention heads, we define the attention scores from the answer token to the video and subtitle segments in the final layer as Avideo [0, 1]1K2 . The attention drift signal video [1, 1]1K1 for video part and sub [1, 1]1K2 for subtitles part is defined as: [0, 1]1K1 and Asub video ="
        },
        {
            "title": "1\nH",
            "content": "H (cid:88) h=1 (cid:0)Avideo h,cot Avideo h,base (cid:1) , sub ="
        },
        {
            "title": "1\nH",
            "content": "H (cid:88) h=1 (cid:0)Asub h,cot Asub h,base (cid:1) . (1) where the subscript base and cot refer to the base and chain-of-thought model responses, respectively. The subtitle part is not considered when the video has no audio. These attention differences reveal whether the models focus has shifted away from the most relevant visual or subtitle segments during reasoning. larger negative value of suggests degraded perceptual grounding, prompting the controller to inject key frames that re-anchor the models attention to the critical visual evidence. Beyond attention and answer prediction, the Sensor can also collect other forward-pass signals to characterize response quality. For example, the softmax confidence of the predicted token, logit stability across strategies, 5 and repetition patterns in the response text. Together, these features offer rich diagnostic view of the models current inference behavior and serve as input to the control mechanism that governs adaptive reasoning."
        },
        {
            "title": "3.4 Controller: Decision Making and Feedback Construction",
            "content": "The Controller governs the adaptive reasoning process by making two key decisions: whether the current inference results are sufficiently reliable for output, and if not, how to generate actionable feedback to revise the models input for the next iteration. It comprises two coordinated modules: Score Forest for response evaluation and an Inference Feedback module for corrective input construction. Score Forest: Confidence-Aware Evaluation and Thresholding. Given candidate responses and multiple signals from the sensor, the Score Forest assigns each response multi-dimensional score vector sn = (sn,1, . . . , sn,m) [0, 1]m, capturing semantic, probabilistic, and attention-related qualities, where is the number of trees in the forest and each tree maps the extracted signals to score within the range of 0 to 1 via distinct mechanism. In our implementation, the scoring mechanisms include, for instance, the softmax confidence of the predicted answer token, an indicator of logit stability across reasoning strategies, binary repetition penalty reflecting output redundancy, visual attention retention score quantifying how well visual grounding is preserved, and normalized rank score that captures the relative quality of each candidate among all responses. For each response {1, . . . , }, we compute Sn [0, 1] as an aggregation of individual scores sn,i, weighted by hyper-parameters {βi}m i=1: Sn = (cid:88) i= βisn,i, (cid:88) i=1 βi = 1. (2) Furthermore, we can calculate the top-scoring option based on these scores. The score of the best option, If the top score satisfies the opScore [0, ], can be calculated by opScore = maxcC confidence threshold, i.e., opScore τ , where the threshold τ [0, 1], the controller selects the top-score answer as the final output. Otherwise, the low confidence score indicates unreliable initial reasoning, and the controller triggers the Inference Feedback module to initiate corrective update. Note that the widely used majority voting policy can be viewed as special case of the Score Forest, where the score Sn = 1 for each response and the threshold τ = 0. n:ˆyn=c Sn. (cid:80) Inference Feedback: Visual Correction for Self-Revision. When confidence is insufficient, the Controller invokes the Inference Feedback module to construct enhanced input that guides the next round of reasoning. This module identifies the top-k visual and subtitle segments that exhibit the greatest decrease in attention. Specifically, we define: Ivideo {1, . . . , K1} and Isub {1, . . . , K2}, where each set contains the indices of the top-k segments with the largest attention decrease: Ivideo = TopK-Indices(video ), Isub = TopK-Indices(sub ). (3) For Ivideo, we can directly extract the corresponding frames through the indices. For Isub, we trace their timestamps to locate the aligned frames. The union of these yields the final set of key frames. To restore the models degraded attention due to reasoning steps, the identified key frames can be seamlessly re-integrated into the original input sequence. To further refine the models focus, we also support more visual content enhancement methods. Temporally, we perform dense sampling around selected key frames while sparsely sampling elsewhere. Spatially, we compute region-question relevance and apply zoom-in to emphasize evidencerich regions. These enhanced inputs are then sent back to the MLLM inference system, enabling the model to refocus on critical evidence."
        },
        {
            "title": "4 Experiments",
            "content": "Benchmarks. We conduct experiments on various video understanding benchmarks, grouped into two categories: knowledge-centric and general-purpose videos. VideoMMMU [25] represents the knowledge-centric category, consisting of 300 expert-level educational videos and 900 questions across six academic domains. It is well-suited for assessing models reasoning and knowledge application. For general-purpose videos, 6 Table 1 Performance on the VideoMMMU benchmark (accuracy %). The results are grouped by evaluation track (Perception, Comprehension, Adaptation) and academic discipline (Art, Business, Science, Medicine, Humanities, and Engineering). Yellow rows indicate open-source MLLMs, while blue rows indicate proprietary models. Models marked with w/sub indicate our baselines with subtitle input. Model Overall Results by Track Results by Discipline Percep. Compr. Adapt. Art. Biz. Sci. Med. Hum. Eng. Human Expert LLaVA-OneVision-7B [29] VILA1.5-40B [35] LLaVA-Video-7B [65] InternVL2-8B [10] LLaVA-OneVision-72B [29] LLaVA-Video-72B [65] Gemini 1.5 Flash [49] Aria [30] Gemini 1.5 Pro [49] Qwen2.5-VL-7B [5] (w/ sub) InternVL3-8B [66] (w/ sub) GPT-4o [41] Qwen2.5-VL-72B [5] (w/ sub) Claude 3.5 Sonnet [2] 74.4 33.9 34.0 36.1 37.4 48.3 49.7 49.8 50.8 53.9 55.0 57.4 61.2 64.3 65.8 InternVL3-8B [66] (+Ours) Qwen2.5-VL-7B [5] (+Ours) Qwen2.5-VL-72B [5] (+Ours) 62.9 (+5.5) 63.3 (+8.3) 74.3 (+10.0) 84.3 40.0 38.7 41.7 47.3 59.7 59.7 57.3 65.7 59.0 72.7 77.0 66.0 84.7 72. 77.3 78.0 85.7 78.7 31.0 30.7 33.3 33.3 42.3 46.0 49.0 46.7 53.3 53.7 49.7 62.0 63.0 69.7 60.3 62.0 76.3 60.3 30.7 32.7 33.3 31.7 43.0 43.3 43.0 40.0 49.3 38.7 45.7 55.7 45.3 55. 51.0 50.0 61.0 81.0 78.8 74.2 70.5 49.2 57.1 65.1 55.6 61.9 69.8 63.5 71.4 57.1 73.0 61.9 69.5 79.4 66. 65.1 76.2 82.5 29.6 27.3 34.1 34.1 46.2 44.7 53.0 47.7 59.1 56.1 59.1 66.9 66.7 75.0 67.4 65.9 78.0 34.9 23.5 32.6 30.3 40.2 41.7 43.2 44.7 49.1 46.2 53.0 51.6 62.9 56.1 62.1 54.5 68.2 31.8 38.0 42.6 34.1 54.3 58.9 49.6 58.9 57.4 58.1 60.5 64.8 68.2 58. 62.0 64.3 78.3 84.8 46.7 41.9 45.7 41.9 60.0 57.1 59.1 62.9 58.1 73.3 74.3 69.5 81.9 75.2 80.0 75.2 83.8 69.9 29.2 32.5 27.4 38.1 44.0 45.1 45.7 43.7 50.3 47.8 51.3 57.1 54.3 66. 56.0 59.3 69.3 we use VideoMME [16] and WorldSense [22]. VideoMME includes 900 videos from six domains and 2,700 multiple-choice questions, while WorldSense contains 1,662 audio-visual videos and 3,172 questions spanning 26 task types, emphasizing multimodal integration. Together, these benchmarks enable comprehensive evaluation of model performance across diverse content types, temporal scales, and reasoning demands. Evaluation Metrics. We use accuracy as the evaluation metric for all benchmarks. For VideoMMMU, which includes both multiple-choice and open-ended questions, we follow its official protocol. For VideoMME, WorldSense, which contains only multiple-choice questions, accuracy is computed as the proportion of correct predictions. Implementation Details. We adopt Qwen2.5-VL [5] and InternVL3 [66] as the base models for all evaluations, using 64 uniformly sampled frames per video for Qwen2.5-VL and 32 frames for InternVL3. Subtitles, when needed, are extracted using Faster-Whisper Large-v3.1. We adopt two-round Best-of-N scheme across all benchmarks. In the Controller, response confidence is estimated via the Score Forest, and directly key frame injection is employed as the feedback mechanism. On the VideoMMMU dataset, we set = 8 and τ = 0.3 in the first round, using one base strategy and seven chain-of-thought (CoT) variants. In the second round, we set = 1 and τ = 0. All βi = 1 with = 5 in the experiments. Additional implementation details are provided in the Appendix B."
        },
        {
            "title": "4.1 Main Results",
            "content": "Knowledge-Centric Video Understanding. Table 1 shows results on the VideoMMMU benchmark. We compare the proposed method against two major categories of models: (1) open-source MLLMs, (2) proprietary models such as GPT-4o and Claude 3.5 Sonnet. Human expert performance is also provided for reference. Our results show that CyberV consistently enhances model performance across range of model scales. When applied to Qwen2.5-VL-7B, it achieves notable +8.3% improvement over the base model, surpassing GPT-4o by 2.1% and approaching Claude 3.5 Sonnet. On InternVL3, it brings +5.5% gain, outperforming GPT-4o by 1.7%. For Qwen2.5-VL-72B, CyberV further boosts performance by +10.0%, exceeding Claude 3.5 Sonnet by 8.5% and reaching accuracy on par with human experts. These results show that even smaller open-source models 1https://github.com/SYSTRAN/faster-whisper 7 Table 2 Performance on general-purpose video understanding benchmarks (accuracy %). w/ sub means adding subtitles. The two benchmarks are grouped based on video length and audio categories, respectively. Model VideoMME (w/sub) WorldSense (w/sub) Overall Short Medium Long Overall Speech Event Music Gemini 1.5 Pro [49] GPT-4o [41] 81.3 77.2 LLaVA-OneVision-7B [29] Qwen2.5-VL-7B [5] Qwen2.5-VL-7B [5] (+CoT) Qwen2.5-VL-7B [5] (+Ours) 69.6 70.5 68.2 (-2.3) 71.6 (+1.1) 84.5 82.8 79.3 76.3 73.1 76.8 81.0 76. 66.9 69.1 67.2 70.8 77.4 72.1 62.4 66.0 64.2 67.1 39.3 50.1 43.9 46.0 43.9 (-2.1) 47.1 (+1.1) 39.6 51. 44.0 46.2 43.9 47.4 38.9 50.2 42.7 45.2 43.5 46.6 39.2 49.9 45.7 47.3 44.6 48.0 can outperform proprietary LLMs through effective cybernetic inference-time scaling. In addition to the accuracy, we observe that CyberV is especially effective on comprehension and application tracks, where reasoning and knowledge transfer are essential. By discipline, the most significant gains occur in business, science, medicine and engineering. These often require symbolic manipulation and mathematical deduction, indicating that our method is crucial for reasoning and knowledge-intensive video tasks. General-Purpose Video Understanding. Table 2 presents results on VideoMME and WorldSense, two benchmarks encompassing wide range of everyday video types. As observed in this table, prompting the model to think before answering leads to degraded performance across both datasets. This highlights key limitation of current multimodal models: due to insufficient cross-modal alignment, these models struggle to integrate visual observations, audio transcripts, and question semantics into coherent reasoning trajectories. In contrast to simple chain-of-thought prompting, CyberV consistently improves performance over the base model on general-purpose video benchmarks: +1.1% on VideoMME and +1.1% on WorldSense. While the absolute gains are modest, they highlight the value of content-aware test-time control that enhances model focus when initial reasoning is insufficient. As result, CyberV demonstrates strong generalization across diverse video domains and offers stable enhancement for test time scaling."
        },
        {
            "title": "4.2 Ablation Study on MLLM Inference System and Sensor.",
            "content": "We conduct series of ablation experiments on the VideoMMMU benchmark to investigate the contribution of each component in the CyberV framework. All experiments are conducted under the zero-shot setting using Qwen2.5-VL-7B. Performance boost via multiple inference strategies. As shown in Table 3a, directly answering with subtitles improves accuracy by 6.4%, demonstrating the critical role of audio information in knowledge-centric video tasks. Adding the chain-of-thought prompt further boosts performance to 58.2%, validating the effectiveness of textual reasoning. However, excessive reasoning may introduce distraction and attention drift. By directly incorporating attention-guided key frames augmentation based on our cybernetic system, performance further improves to 60.0%. Note that here we adopt the simplest form of our framework: one base and one CoT response in the first round, followed by one response with key frames in the second round. BoN outperforms complex search schemes. We further justify our choice of Best-of-N as the core inference framework. Beyond Best-of-N, PRM-guided tree search is widely used scaling strategy that decomposes the reasoning process into multiple steps, where candidates are selected from at each step. As shown in Table 3b, tree-based search methods yield some improvement, but still fall short of BoN (N=8). We further investigate the impact of different values of in the Appendix C. Sensor benefits from different attention sources. The Sensor module extracts intermediate signals such as attention drift. We compare two configurations under fixed frame budget: one using only video part attention, and one also incorporating subtitle-guided attention based on timestamp alignment. As shown in Table 3c, subtitle-based drift offers slight gain, indicating its complementary grounding value. However, for questions lacking clear temporal anchors, subtitle-aligned frames may introduce noise, suggesting the need for more refined filtering. 8 Table 3 Ablation Study on MLLM Inference System and Sensor. Accuracy (%) on VideoMMMU under different inference configurations and search schemes for MLLM inference system, and different sources of attention drift for Sensor. (a) Impact of different scaling strategies. + denotes cumulative addition. (b) Comparison of more search KF means key schemes. frames. (c) Comparison of different sources of attention drift. + denotes cumulative addition. Strategy Acc (%) Base +Subtitles +CoT +Key Frames 48.6 55.0 (+6.4) 58.2 (+9.6) 60.0 (+11.4) Scheme Acc (%) Attn Source Acc (%) Base (+CoT&KF) +Best of (Ours) +Tree Search 60.0 63.3 62.8 Base (+CoT) + Video-Part + Subtitles-Part 58.2 59.9 60.0 Table 4 Ablation Study on Sensor and Controller. Accuracy (%) on VideoMMMU under different scoring policies (Score Forest) and visual selfcorrection methods (Inference Feedback) in Controller. (a) Scoring policies. (b) Visual self-correction methods. Scoring Method Acc (%) Method Acc (%) Base (+CoT) Majority Voting Score Forest (One round) Score Forest (Ours) 58.2 61.9 62.8 63. Base (+CoT) + Key Frames + Dense Sampling + Spatial Zoom-in 58.2 60.0 60.3 60.7 Table 5 Stability analysis under dif- +Ours ferent disturbance levels. refers to the simplest form mentioned in Section 4.2. Setting Base +Ours Uniform sampling Disturb rate = 0.2 Disturb rate = 0.4 Disturb rate = 0.6 55.0 55.0 55.4 52.0 60.0 60.4 60.2 60."
        },
        {
            "title": "4.3 Ablation Study on Controller.",
            "content": "Score Forest outperforms majority voting. We evaluate the Controllers ability to make decisions based on uncertainty signals. Under the Best-of-N (N=8) setting, our score forest which aggregates multi-dimensional uncertainty outperforms simple majority voting (62.8% vs. 61.9% shown in Table 4a). With additional visual clues incorporated in next loop, our method further improves to 63.3%. These results demonstrate that principled confidence modeling is more effective than uniform voting in test-time scaling, and confirm the critical role of the Controller in our cybernetic loop. Different visual self-correction methods are effective. In addition to adding key frames directly, we further analyze the impact of more visual self-correction methods in the Controller. As shown in Table 4b, enhancing visual clues based on key frames with temporal dense sampling yields 60.3%, while spatial zoom-in achieves the best performance at 60.7%. These results validate the effectiveness of multi-dimensional visual scaling in improving model focus and answer accuracy. Due to the additional complexity of these methods, we use direct key frames injection in the main experiments."
        },
        {
            "title": "4.4 Stability Analysis",
            "content": "To assess the robustness of our framework, we conduct stability analysis inspired by control theory. We introduce temporal perturbations by replacing uniform frame sampling with non-uniform variant, where each frame index is randomly shifted within range determined by the disturb rate. As shown in Table 5, our method consistently outperforms the baseline across all disturbance levels, even as the baseline degrades. These results demonstrate that our method is stable and robust to non-uniform temporal distortions, confirming the strength of our cybernetic test-time scaling strategy in dynamically adapting to sampling perturbations while maintaining reliable performance."
        },
        {
            "title": "4.5 Visualization",
            "content": "Figure 3 illustrates the effectiveness of our control system in identifying forgotten yet critical visual information via attention difference after applying CoT. By reintegrating these cues, the system is able to correct CoTinduced errors, demonstrating the effectiveness of the self-correction mechanism. More visualizations and limitations of our work are provided in the Appendix E,F. 9 Figure 3 Attention map visualizations on VideoMMMU. Red boxes highlight segments where attention significantly drops after applying CoT. They may correspond to content that contains critical information. Under our control system, adding key frames after CoT helps rectify previously incorrect responses."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose CyberV, training-free, extra-model-free, test-time adaptive scaling framework designed to enhance video understanding performance in multimodal large language models (MLLMs). Inspired by cybernetic principles, CyberV integrates closed-loop architecture with MLLM Inference System-Controller-Actuator design to monitor attention shifts, evaluate prediction uncertainty, and dynamically execute self-correction strategies. Extensive experiments across diverse video benchmarks demonstrate the effectiveness of CyberV, achieving substantial gains on knowledge-centric and general-purpose video understanding tasks. Future work will explore more effective and efficient strategies to further improve complex multimodal reasoning. Acknowledgement. This work is supported by the National Key Research and Development Program of China (No. 2023YFC3807600)."
        },
        {
            "title": "References",
            "content": "[1] Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. [2] Anthropic. Claude Team. Introducing Claude 3.5 Sonnet. https://www.anthropic.com/claude/sonnet, 2024. [3] William Ross Ashby. An introduction to cybernetics. 1956. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078, 2024. [7] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [8] Peter Cariani. On the importance of being emergent. Constructivist Foundations, 5(2):89, March 2010. URL https://constructivist.info/5/2/089. Retrieved 13 August 2012. [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [12] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. URL https://arxiv.org/abs/2406.07476. [13] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong Li Lee, and Wynne Hsu. Video-of-thought: step-by-step video reasoning from perception to cognition. In ICML, pages 1310913125, 2024. [14] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [15] Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, and Neel Nanda. Do know this entity? knowledge awareness and hallucinations in language models. arXiv preprint arXiv:2411.14257, 2024. [16] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025. [17] Stephen Gage. The boat/helmsman. Technoetic Arts, 5(1):1524, January 2007. ISSN 1477-965X. doi: 10.1386/tear.5.1.15_1. [18] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah Goodman. Stream of search (sos): Learning to search in language. In COLM, 2024. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 11 [20] Weiyu Guo, Ziyang Chen, Shaoguang Wang, Jianxiang He, Yijie Xu, Jinhui Ye, Ying Sun, and Hui Xiong. Logic-in-frames: Dynamic keyframe search via visual semantic-logical verification for long video understanding. arXiv preprint arXiv:2503.13139, 2025. [21] Kai Han, Jianyuan Guo, Yehui Tang, Wei He, Enhua Wu, and Yunhe Wang. Free video-llm: Prompt-guided visual perception for efficient training-free video llms. arXiv preprint arXiv:2410.10441, 2024. [22] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. [23] Rongpei Hong, Jian Lang, Jin Xu, Zhangtao Cheng, Ting Zhong, and Fan Zhou. Following clues, approaching the truth: Explainable micro-video rumor detection via chain-of-thought reasoning. In THE WEB CONFERENCE, 2025. [24] Jian Hu, Zixu Cheng, Chenyang Si, Wei Li, and Shaogang Gong. Cos: Chain-of-shot prompting for long video understanding. arXiv preprint arXiv:2502.06428, 2025. [25] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [26] Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, and Anima Anandkumar. Neural networks with recurrent generative feedback. https://doi.org/10.48550/arXiv.2007.09200, Jul 2020. First submitted on 17 Jul 2020, latest version 10 Nov 2020. [27] Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, and Yunhe Wang. Memory-space visual prompting for efficient vision-language fine-tuning. arXiv preprint arXiv:2405.05615, 2024. [28] Noam Levi. simple model of inference scaling laws. arXiv preprint arXiv:2410.16377, 2024. [29] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [30] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [31] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. [33] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [34] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2024. [35] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. [36] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [37] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-ofresolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. [38] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Jiayi Ji, Jie Lou, Debing Zhang, and Rongrong Ji. Mllm-selector: Necessity and diversity-driven high-value data selection for enhanced visual instruction tuning. arXiv preprint arXiv:2503.20502, 2025. [39] Warren S. McCulloch and Walter Pitts. logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4):115133, December 1943. ISSN 0007-4985. doi: 10.1007/BF02478259. 12 [40] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling. arXiv preprint Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393, 2025. [41] OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. [42] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [43] Miao Rang, Zhenni Bi, Chuanjian Liu, Yehui Tang, Kai Han, and Yunhe Wang. Eve: Efficient multimodal vision language models with elastic visual experts. arXiv preprint arXiv:2501.04322, 2025. [44] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [45] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [46] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, et al. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuray. arXiv preprint arXiv:2502.05177, 2025. [47] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. [48] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [49] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [50] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [51] Heinz von Foerster. Cybernetics: Circular causal and feedback mechanisms in biological and social systems. In Transactions of the Seventh Conference. Josiah Macy Jr. Foundation, 1952. [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [53] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, 2023. [54] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023. [55] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, volume 35, pages 2482424837, 2022. [57] Norbert Wiener. Cybernetics or Control and Communication in the Animal and the Machine. MIT press, 1948. [58] Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Guannan Jiang, Xiaoshuai Sun, and Rongrong Ji. Controlmllm: Training-free visual prompt learning for multimodal large language models. Advances in Neural Information Processing Systems, 37:4520645234, 2024. 13 [59] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. [60] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [61] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP (Demos), 2023. [62] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [63] Wanpeng Zhang and Zongqing Lu. Adarefiner: Refining decisions of language models with adaptive feedback. https://doi.org/10.48550/arXiv.2309.17176, Sep 2023. First submitted on 29 Sep 2023, last revised 3 May 2024. [64] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641, 2025. [65] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [66] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Overview",
            "content": "In this appendix, we first provide more implementation details of our method in B. Then, we present additional experimental results and ablation studies on several component designs in C. Next, we include the PyTorchstyle pseudo-code of our algorithm in to offer clearer understanding of the overall inference process. After that, we present more visualization results in E. Finally, we discuss the limitations and future work in F."
        },
        {
            "title": "B More Implementation Details",
            "content": "Inference details on each benchmark. For all benchmarks, we adopt two-round control loop with fixed score aggregation weight βi = 1 and number of scoring trees = 5. For VideoMMMU [25], we set = 8 and τ = 0.3 in the first round, including one base model response and seven chain-of-thought (CoT) variants. The base response uses deterministic decoding with temperature = 0.0, while CoT responses use sampling with temperature = 1.0, top-p = 0.5, and top-k = 5. In the second round, we set = 1 and τ = 0.0, applying strategy that first generates CoT reasoning trace, then augments it with attention-guided key frames before producing the final answer. During key frames selection, we use TopK-Indices with = 5 for both video and subtitle segments. The total number of key frames is capped at 20 to control computational cost. For VideoMME [16], we adopt the simplest form of the cybernetic loop: in the first round, we perform one base and one CoT inference, using temperature = 0.0 and τ = 0.5. The second round also performs single pass, incorporating key frames augmentation to guide the final prediction. For WorldSense [22], we set = 16 and τ = 0.7 in the first round, consisting of one base and fifteen CoT responses, all generated using temperature = 1.0. In the second round, we set = 8, apply key frames augmentation, and rescore the candidate responses to obtain the final output. For MVBench [31], with results shown in Appendix C, we set = 8 in the first round with τ = 0.7 and temperature = 1.0. The second round follows the same setting as that used for VideoMME. All experiments are conducted on 8 GPUs, each equipped with 80 GB of memory."
        },
        {
            "title": "C More Results and Ablation studies",
            "content": "Results on MVBench. We also evaluate our method on MVBench [31], challenging general-purpose video benchmark specifically designed to evaluate temporal comprehension in video understanding. As shown in Table 6, naive chain-of-thought prompting degrades Qwen2.5-VL-7Bs performance by 4.6%, dropping to 62.1%. This highlights the challenges for multimodal large models to make effective reasoning on video understanding tasks, which is consistent with the results observed in other mentioned genral-purpose benchmarks. In contrast, the proposed approach CyberV yields improvements, boosting Qwen2.5-VL-7Bs performance to 67.5%. The results further demonstrate that our method effectively generalize across diverse video domains. Table 6 Performance on MVBench (accuracy %). Model Gemini 1.5 Pro [49] GPT-4o [41] Overall 60.5 64. LLaVA-OneVision-7B [29] Qwen2.5-VL-7B [5] Qwen2.5-VL-7B [5] (+CoT) Qwen2.5-VL-7B [5] (Ours) 56.7 66.7 62.1 67.5 (+0.8) Comparison of number of paths in BoN. We also study the effect of varying the number of inference paths in the BoN setting. As shown in Figure 7a, performance steadily improves with larger and saturates around N=8, which offers the best trade-off between diversity and computational cost. 15 Pseudo-code of the CyberV architecture. # Core module : Sensor # signal extraction from MLLM forward processes class Sensor : # Core module : MLLM Inference System # Executing Inference Strategies class MLLMSystem : def monitor ( outputs ) : signals = [] signals . append ( get_predicitons ( outputs ) ) signals . append ( get_attn_drift ( outputs ) ) ... return signals # Core module : Controller # decision making and feedback construction class Controller : def score_forest ( out , attn_drift ) : # cal score for each response # aggregate scores for each option return score_list def e re ce _ fe db ck ( signals ) : # key frames extraction video_based_kfs , sub_based_kfs = ... kf = # visual cues enhancement # Here is an example : directly augment video_based_kfs sub_based_kfs key frames action = ( \" Add key frames . \" , kf ) return action def decide ( signals ) : scores = score_forest ( signals ) response = best_answer ( scores ) if is_confident ( scores ) : return True , response , None else : action = nf re ce _ fe db ck ( signals ) return False , response , action # an MLLM . For example , Qwen2 .5 - VL -7 self . model = ... def execute ( inputs ) : outputs = [] # Use BoN to get responses for in inputs [ strategies ]: out = self . model . forward ( inputs , ) outputs . append ( out ) return outputs # Cybernetic loop for one inference round def run_one_loop ( inputs ) : # Step 1: Run MLLM with multiple strategies out = MLLMSystem . execute ( inputs ) # Step 2: Monitor outputs to extract signals signals = Sensor . monitor ( out ) # Step 3: Decide when & how to trigger self - correction flag , response , action = Controller . decide ( signals ) return flag , response , action # Closed - loop inference over multiple rounds def cyber_v ( inputs ) : max_rounds = ... round_now = 1 while True : flag , response , action = run_one_loop ( inputs ) if flag or round_now == max_rounds : return response inputs = update ( inputs , action ) round_now += 1 Figure 4 Pseudo-code of the CyberV architecture. The MLLM Inference System, Sensor and Controller cooperate to form closed-loop control cycle for test-time video understanding. Comparison of attention extraction from different number of layers. Recent studies in model interpretability suggest that the final layers of large language models tend to capture more high-level semantic information that directly contributes to the models output decisions [15]. Motivated by this, we evaluate the effectiveness of extracting attention signals from different depths of the LLM component in Qwen2.5-VL-7B, which contains 28 transformer layers in total. Specifically, we experiment with extracting attention maps from the last 1, 4, and 7 layers, and report the results in Table 7b. Extracting attention solely from the final layer yields an accuracy of 60.0%. Including the last 4 layers slightly reduces performance to 59.4%, while using the last 7 layers gives marginal improvement to 60.2%. Overall, incorporating more layers introduces minor fluctuations, but yields consistent improvement over the baseline. For simplicity and computational efficiency, we finally extract attention only from the last layer in all experiments. Pseudo-code of CyberV Figure 4 illustrates the PyTorch-style pseudo-code of the CyberV architecture, which models video understanding as closed-loop control process. It consists of three core modules: the MLLM Inference System executes various scaling strategies; the Sensor monitors intermediate outputs and extracts key signals such as predictions and attention drift; and the Controller evaluates response reliability and, if needed, constructs feedback (e.g., key frames augmentation) to trigger self-correction. These components interact iteratively to improve reasoning quality without additional training. Table 7 More ablation studies. Performance on VideoMMMU under different model settings. (a) Different number of paths (N) in BoN. (b) Attention extraction from different layers. Here, we use Qwen2.5-VL-7B and compare the last 1st, 4th, and 7th layers. #Paths Accuracy (%) 2 4 8 16 32 60.0 60.9 63.3 62.7 62. Attention from Accuracy (%) Base (No key frame) Last 1 layer Last 4 layers Last 7 layers 58.2 60.0 59.4 60."
        },
        {
            "title": "E More Visualization Results",
            "content": "More attention map visualizations. In the main paper, we show that incorporating attention-guided key frames into the second-round inference can effectively correct CoT-induced errors, particularly in cases where the base model initially produces the correct answer but CoT leads to an incorrect one. This demonstrates the utility of the cybernetic feedback loop in mitigating reasoning drift. However, as illustrated in Figure 5, applying visual self-correction indiscriminately (without confidence-based filtering) can introduce new errors. In this example, both the base model and CoT initially provide the correct answer with high confidence, yet the second round reversed the decision due to the influence of noisy and irrelevant key frames. The contrast between this failure case and successful examples underscores the importance of incorporating confidence-aware control to selectively trigger feedback only when necessary, thereby enhancing overall robustness and preserving performance on easy cases. Case studies on WorldSense. We conduct visualization studies on the WorldSense benchmark to qualitatively evaluate the effectiveness of our model. As depicted in Figures 6, 7, and 8, these case studies illustrate both the strengths and limitations of the proposed cybernetic loop. Figure 6 illustrates the models successful identification of abnormal events in complex video scene. Through iterative reasoning and re-examination, the model accurately detects an artillery attack, demonstrating the ability of the cybernetic loop to refine attention and correct initial misinterpretations. In Figure 7, the models capability in object counting under dynamic conditions is showcased. Within fast-paced VR game setting, CyberV correctly counts four cartoon pillows to the right of blue lightsaber. This highlights the models strength in spatial reasoning and its capacity to integrate visual and textual cues through feedback mechanisms, effectively avoiding common counting errors. Figure 8 presents failure case in which the model misidentifies missing item in refrigerator. Due to incomplete or imprecise key frames selection by the Sensor module, the second round of inference draws an incorrect conclusion, falsely reporting missing potato instead of the actual hamburger. This case shows limitation of our method: inaccurate key frames extraction may, in certain instances, hinder the effectiveness of second-round visual enhancement, thereby failing to support meaningful self-correction."
        },
        {
            "title": "F Limitations and Future Work Discussion",
            "content": "While CyberV demonstrates notable improvements in test-time video understanding, several limitations remain. First, the current key frames extraction relies on attention drift over video and subtitle segments. Although some critical frames are often covered, this approach may introduce noisy or irrelevant frames. Our cybernetic loop can mitigate their impact through selective correction, but more principled methods for noise filtering, temporal search and the utilization of signals remain important directions for future work. Second, current state-of-the-art MLLMs exhibit limited capacity for temporally grounded reasoningthat is, the ability to precisely align and integrate the information from visual frames, subtitles, and questions along the temporal axis during the reasoning process. As result, although CyberV yields significant gains on knowledge-centric benchmarkswhich rely more on symbolic reasoning, logical inference, and mathematical 17 Figure 5 Without confidence-based filtering in the Controller, high-confidence correct answers in the first round also need to trigger unnecessary key frames extraction, leading to errors in the second round due to noisy frames. In this case, although the question refers to 3:28 of 4:09 video, the selected key frames focus on the beginning and end, resulting in an incorrect revision. derivation, capabilities that existing MLLMs are relatively better equipped to scaleits improvements on perceptual-heavy benchmarks are less pronounced. We believe that combining CyberV with future MLLMs possessing stronger multi-modal temporal grounded reasoning capabilities may yield greater benefits. Another limitation lies in inference efficiency. As the number of inference paths (N ) and iterations increases, test-time latency grows manyfold. Developing more efficient implementations of the cybernetic loop, potentially via strategy pruning, presents another valuable avenue for future research. Overall, while CyberV opens up novel perspective on test-time adaptive reasoning, its full potential can be further unlocked through improvements in both base model capability and control system efficiency. 18 Figure 6 CoT fails to answer but our method performs well. 19 Figure 7 Both the base model and CoT fail to answer but our method performs well. Figure 8 Noisy frames may degrade model performance."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Peking University"
    ]
}