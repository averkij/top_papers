{
    "paper_title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "authors": [
        "Jingtong Yue",
        "Ziqi Huang",
        "Zhaoxi Chen",
        "Xintao Wang",
        "Pengfei Wan",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 8 5 8 0 . 1 1 5 2 : r Simulating the Visual World with Artificial Intelligence: Roadmap Jingtong Yue1 Ziqi Huang2 Zhaoxi Chen2 Xintao Wang3 Pengfei Wan3 Ziwei Liu2 1Robotics Institute, Carnegie Mellon University 2S-Lab, Nanyang Technological University 3Kling Team, Kuaishou Technology https://world-model-roadmap.github.io/"
        },
        {
            "title": "Abstract",
            "content": "The landscape of video generation is shifting, from focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as window into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in world model, built upon video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link. Keywords: World Model, Video Generation, Conditioned Video Generation"
        },
        {
            "title": "1.1 Motivation",
            "content": "World models, which aim to simulate the real world, have long posed significant challenges in artificial intelligence, influencing various applications, such as robotics, autonomous driving, and gaming. While the specific capabilities required for world models are numerous and not yet precisely defined, approaches such as 3D generation [13], 3D/4D scene generation, and video generation demonstrate one or more relevant abilities, such as motion dynamics, interaction and controllability, visual quality, 3D consistency, and generation efficiency. Consequently, these approaches have been adopted in recent works as potential pathways toward realizing world models. While these approaches each excel in partial capabilities toward world modeling, video generation offers one direct and comprehensive pathway, which may be promising tool for building world model. From cognitive science perspective, vision is the dominant sensory modality through which both humans and embodied agents perceive, learn, and reason about the world. Visual streams not only convey spatial layout and object properties but also encode temporal dynamics and causal relationships crucial for prediction and planning. Even complex 3D or 4D simulations can be rendered into videos or images for interpretation, project lead. corresponding authors. 1 Fig. 1 Overview of 4 Generations and 3 Core Capabilities from Video Generation to World Model. The figure illustrates the key capabilities emphasized in the first through third generations of world models, as well as our insight for future world models. We outline long-term vision of world models that can simulate broad range of environments across multiple spatial and temporal scales. The figure highlights four foundational characteristics: real-time responsiveness, stochasticity, multiscale planning, and intrinsic physical faithfulness. These collectively support the long-term goal of zero-shot generalization. meaning that any human or embodied agent grounds its understanding in visual sequences. This intrinsic reliance on visual representation makes video generation uniquely natural and information-rich foundation for constructing world models. Recent advances in end-to-end video generation indicate that such models can now serve as high-quality visual renderers, enabling the exploration of video-based approaches to world modeling. Recent advancements in techniques, such as diffusion models [47] and autoregressive transformers [810], have made it possible to generate high-quality videos grounded in fundamental world knowledge. Current methods [1127] based on these backbones are now capable of producing long-duration, high-quality videos with superficial faithfulness, endowing them with the ability to simulate real-world environments with high fidelity and incorporate multimodal conditioning. As result, there is an increasingly evident trend of utilizing video generation models as world models. In this survey, we define physical world model as sophisticated digital engine that encodes comprehensive world knowledge to simulate real-world dynamics in accordance with intrinsic physical and mathematical laws. Such models serve both as high-fidelity simulators for advancing domains like robotics, autonomous driving, and gaming, and as controlled testbeds for training and evaluating intelligent agents under realistic yet safe conditions. By modeling the physical world, they can also support physics-based engineering, high-stakes decision-making, and other real-world tasks. Recent breakthroughs [1113, 15, 17, 18, 2137] in video generation models, driven by improvements in diffusion models [3847], autoregressive backbones [48], variational autoencoders [49, 50], image generation techniques [35, 5159], controllable image generation [60, 61], and enhancements in training or inference efficiency [6272], as well as more flexible condition injection modules [73, 74], and advances in video rendering [75], mark pivotal moment for the field. These models can now generate high-quality videos grounded in fundamental world knowledge. At this stage, they begin to play central role in constructing world models. Moreover, with the rapid advancement of technologies such as virtual reality (VR) and embodied AI, the integration of world models into interactive, real-time environments has become increasingly feasible. These technological trends, together with the maturation of video generation methods, indicate that we are on the cusp of new era where world models will play central role in shaping autonomous systems, intelligent agents, and immersive virtual environments. This trend is further evidenced in Figure 2, which illustrates the dynamics of research attention over recent years. Specifically, while discussions and mentions of world models have been present since 2018, they have remained relatively steady in terms of annual publication volume. While starting in 2024, video generation witnessed an explosive surge in both technical advances and the number of related works, which in turn catalyzed renewed wave of progress in world models. This pattern highlights clear interdependence: the rapid advances in video generation are not merely parallel developments but are becoming key enablers that support and accelerate the evolution of world models. Consequently, the present moment is particularly critical for the community to systematically discuss the evolution from video generation to world models, and to chart their future directions as an integrated research frontier. Despite these advances, challenges remain at both the conceptual and structural levels. Conceptually, the definition of world model remains ambiguous, making it difficult to unify perspectives and evaluate progress. Structurally, the field lacks well-established taxonomy to organize modeling capabilities, developmental stages, and potential trajectories. These gaps highlight an urgent need for systematic elucidation and comprehensive survey to consolidate existing knowledge and guide the next stage of research. Current surveys [7688] have laid important groundwork by summarizing related methodologies, datasets, and applications. However, there remains need to explicitly clarify what aspects have been thoroughly addressed, and which areas, such as real-time integration, controllable video-to-world pipelines, or comprehensive evaluation 2 Fig. 2 Overview of Annual Papers and Articles Paper Counts in Video Generation & World Model Fields. The article count was derived from searches conducted using the fixed keyword combination video generation and world model from Google Scholar and arXiv. metrics, are still underexplored. Accordingly, this survey aims to chart clear path from video generation toward comprehensive world modeling, providing guidance for future research and development in this emerging field. In this paper, we begin with formal definitions and provide detailed discussion centered around the taxonomy of world models. We define physical world model as digital simulation engine embedded with comprehensive world knowledge, capable of predicting the next scene conditioned on environmental states and contextual priors. Each prediction step can be represented as triplet (Current Scene,Navigation Mode,Prior Information), which collectively determine the evolution of the simulated environment. The model focuses on capturing the causal and spatiotemporal dynamics of the physical environment, while external inputs such as navigation modes or actions act as stimuli that perturb or impact the environments evolution. Hence, the world model is formulated as an interactive environment system that responds to external interventions without explicitly modeling the decision-making process that generates them. Based on this definition, we systematically analyze how video generation models have evolved toward world modeling and propose four-generation taxonomy according to model capability, as illustrated in Figure 1. Generation 1 - Faithfulness: Accurate Simulation of the Real World; Generation 2 - Interactiveness: Controllability and Interactive Dynamics; Generation 3 - Planning: Modeling the Future Evolution of Complex Systems; Generation 4 - Stochasticity: Modeling Outlier and Low-Probability Events. We categorize and analyze existing approaches based on their foundation model ability, navigation mode types, application domains, and conditioning strategies. This perspective helps clarify how navigation signals influence video generation and what architectural designs are most effective for different world modeling tasks. Such clear generational breakdown allows for systematic evaluation of the progress in world modeling and facilitates discussion of the remaining challenges and gaps between current video generation systems and ideal world models. We summarize our contributions as follows: global taxonomy of world models: We propose four-generation taxonomy for categorizing the evolution of video generation towards world models, grounded in the world models core capabilities, faithfulness, interactiveness, and planning. Clarification of the definition of world model: We define the core task as next-scene video prediction, and provide formal equation that characterizes the world modeling process in terms of input, internal state, and output. Formal definition of navigation modes: We define the scope and characteristics of navigation modes, distinguishing them from spatial conditions to prevent conceptual overlap and to clarify their differences in interaction flexibility. Future perspectives: We discuss the key capabilities that must be achieved for video generation models to evolve into fully-fledged world models, offering insights for future research direction."
        },
        {
            "title": "1.2 Position",
            "content": "World models [89, 90] have traditionally been regarded as tools enabling AI agents to perceive and interact with their environment, often inspired by human cognition and grounded in so-called common sense. In this survey, we distinguish two complementary perspectives, physical axis emphasizing external dynamics and 3 Fig. 3 The Characteristics of Physical World Model and Mental World Model. This figure highlights the distinct inputs, internal processes, and outputs, as well as the interaction through perception, planning, and guidance between the physical world model and the mental world model. mental axis emphasizing internal simulation and intention modeling, following the conceptual distinction introduced by Huang in his blog post [91]. From this standpoint, the physical world model represents more fundamental and global conceptualization, aiming to capture both the evolution and the intrinsic laws of the physical world. In contrast, the mental world model [9295] can be regarded as specialized internal cognitive framework that may emerge in higher-generation physical world models, serving to represent an agents internal states, intentions, and preferences. clear distinction between these two perspectives is essential, both philosophically and in terms of their representational forms and capability requirements. Philosophically, the distinction mirrors the classical debate between subjective idealism and mechanistic materialism. The physical world model aligns with the latter, seeking to explain the world through objective, immutable physical laws and focusing on external dynamics independent of the agents subjective experience. In contrast, the mental world model reflects subjective, intentional stance, emphasizing the role of perception and intention in shaping the agents understanding of the world, consistent with subjective idealism, which holds that objective events may be influenced by an agents will or actions. As for capability demand, the physical world model emphasizes adherence to real-world physical laws, interaction grounded in intrinsic world knowledge, and essential planning based on objective dynamics. The mental world model, on the other hand, should possess abilities such as semantic understanding, active interaction, and counterfactual reasoning, enabling it to think and act more like human beings. Because of these fundamental differences, the mental world model cannot be conflated with the physical world model in our definition; instead, we explicitly delineate the scope and capabilities of the physical world model."
        },
        {
            "title": "1.3 Scope",
            "content": "This survey focuses on methods that advance video generation models towards world models. The objective of world model powered by video generation methods is to acquire structured world knowledge that can, in turn, enhance AI agents perception, reasoning, and planning capabilities. We consider world models as multimodal perceptual, interactive, and predictive systems that are capable of capturing the underlying dynamics, spatial structure, and semantics of the environment. These representations can support broad spectrum of downstream tasks, including but not limited to visual planning, counterfactual reasoning, generalization to novel scenarios, and cross-modal understanding, enhancing the performance of agents and promoting research in physics-related fields. In addition, the broader usages of powerful world knowledge representation are still worth exploring."
        },
        {
            "title": "2 Problem Definition and Taxonomy",
            "content": "Inspired by the concept of digital cousins from policy learning [96] and sim-to-real research, where virtual environments are designed to share semantic and geometric affordances with the real world without being exact replicas. In our context, we define physical world model as digital cousin of the real worldone that embeds objective world knowledge and generates observable futures under external conditioning, as illustrated in Figure 4. Unlike digital twin that replicates specific world instance, digital cousin emphasizes distributional realism, the capacity to simulate diverse, physically plausible yet semantically varied worlds, enabling world models to generalize beyond faithful reproduction. world model not only simulates the causal dynamics and spatiotemporal evolution of the environment but also encodes the behaviors, interactions, and goals of agents within it. In practice, the world model is initialized by external inputs, including textual prompts , current observations (images or video clips), and other external interventions that stimulate or guide how the environment evolves, such as audio signals Au, navigation modes (actions, text 4 Fig. 4 Overview of the World Model Defined in this Paper. The world model must take inputs such as text, images, videos, audios or their combinations. It may also incorporate external conditions for interaction, including spatial conditions and navigation conditions. video generation model is leveraged to process the intermediate state representations to produce video outputs, while other task-specific outputs may also be generated depending on the downstream application. commands, or trajectories), and spatial conditions X. These inputs define the starting state of the simulation and guide its evolution. Formally, the world mode is defined as follow: (1) where denotes the video generation model and represents the multimodal input space. The world model based on video generation model pipeline maps external inputs to sequence of observable video frames V1:T , which is stochastic generative process. = {T, O, Au, N, X} V1:T = (I), More specifically, under the decomposition, the world model corresponds to the latent representation St together with the transition function (i.e., representation) which captures the models internalized world knowledge (dynamics, object affordances, agent intents, etc.) and compute St+1 = (St, It). The video renderer is the function that translates those internal world states into pixel-level or perceptual outputs through Vt+1 = R(St+1). Returning to the high-level view, although we conceptualize video generation model as the combination of world model and video renderer, in practice the internal state of the world model is implicit. In other words, the video generation process still manifests as single, monolithic mapping process from input to output videos V1:T . Conceptually, the world model serves the same functional role as the environment dynamics in Markov Decision Process (MDP). During training, exposed to large-scale multi-view and multi-temporal data, the model can approximate fully observable environment where the latent world state St is effectively known. The learned transition (St, It) thus behaves analogously to an environment transition function (St+1St, At), where denotes the action space, in fully observable MDP, forming an objective world prior. During inference, the model receives only partial conditioning signals (text, image, audio, action etc.), corresponding to partial observations of the true latent state. The resulting generative process therefore aligns with Partially Observable MDP (POMDP), formulated as (St+1, Ot+1St, At). This dual interpretation reconciles the apparent contradiction between objectivity and subjectivity: training instills objective physical knowledge, while inference performs subjective reasoning grounded on that learned prior. It also clarifies that while the world model functions as an objective simulator of latent physics, its operation at inference time is conditioned by subjective, agent-like observations, bridging the two perspectives within single unified framework."
        },
        {
            "title": "2.1 Taxonomy of Four Generations from Video Generation to World Model",
            "content": "In this section, we provide brief version of the video generation to world models taxonomy, highlighting the development of core capabilities in each generation. For detailed comparisons, please refer to Section 3 and Figure 5. The four generations form an evolutionary ladder, that each subsequent level extends the previous one along the same capability axes rather than representing independent types. Generation 1 - Faithfulness: Superficial Simulation of the Real World In this generation, world models acquire the general ability of video generation models, enabling them to generate short videos lasting 2 to 5 seconds, barely satisfying human visual quality. The model also supports basic interactions conditioned on spatial signals with low flexibility and basic condition-video consistency. However, only basic commands and movements can be generated, and the ability to plan has yet to develop. These models typically use text, image, or video inputs as the initial state of the virtual world and spatial conditions as control signals."
        },
        {
            "title": "Navigation Mode",
            "content": "Temporality Content Independence Spatial-Reasoning Table 1 Comparison Between Spatial Conditions and Navigation Modes. In this table, we compare the key terms spatial condition and navigation mode, focusing on their main characteristics and differences. The comparison is conducted primarily across three dimensions: temporality, content independence, and spatial reasoning. Generation 2 - Interactiveness: Controllable and Interactive Simulation of the Real World In this generation, world models build upon their superficial physical plausibility to acquire basic 2D and 3D reasoning capabilities, as well as more flexible and navigational controllability. They are now capable of generating long videos with richer content complexity, better visual realism, and perfect temporal and video-text consistency. Moreover, simple task planning is developed to support basic task-oriented planning without physical evolution capability. These models are increasingly able to accept diverse conditioning and navigation mode inputs. Generation 3 - Planning: Real-Time and Complex Prediction of the Real World In this generation, planning has made significant advancements and emerged as key capability of world models, that is, given an initial state, they can autonomously generate infinite or arbitrarily long videos for complex tasks based on intrinsic physical knowledge with real-time controllability. Models of this generation are expected to achieve real-time and subject-centric video generation with external control signals. Generation 4 - Stochasticity: Low-Probability and Multi-Scale Modeling of the Real World In this generation, world models achieve balance between modeling regular, rule-based environments and capturing low-probability events. They are now capable of representing rare occurrences, such as genetic mutations, unexpected accidents, financial crises, and volcanic eruptions. Models are also expected to separately or simultaneously support three types of spatiotemporal scales: macroscopic scale, which may span decades, mesoscopic scale, which is aligned with real-world temporal and spatial dynamics, and microscopic scale, which has millisecond-level precision."
        },
        {
            "title": "2.2 Definition of Navigation Mode",
            "content": "To better characterize how video generation model functions as higher generation world model, particularly in terms of its interaction with and response to external signals, we introduce the concept of navigation mode. This formulation provides principled framework for understanding how external guidance signals condition the generative process within world model. It also enables clearer theoretical distinction between the spatial conditioning methods and navigation conditioning methods, which differ in their degrees of environmental interactivity and control flexibility. We formally define navigation mode as structured interface through which an external condition signal guides the generative process. As shown in the Table 1, condition signal is considered valid instance of navigation mode rather than spatial condition only if it satisfies triad of essential properties. We represent this triplet as {T, R, S}, capturing the fundamental requirements for temporality, content independence, and spatial reasoning. - Temporality: The navigation mode must be defined as temporally ordered sequence or influence the whole duration. This ensures that the guidance signal evolves over time, reflecting realistic changes in intent, observation, or control. Content Independence: The navigation mode must not explicitly reference the content and spatial characteristics within the video, including semantic maps, layouts, textual descriptions, motion poses, or depth maps. These types of conditions anchor the generation to specific, interpretable objectives and therefore require pairing with the original video content. They cannot be freely transferred to video generation in other background contexts, which inherently limits their interactive capability. Thus, we define the navigation mode as the content independence type of condition. Spatial Reasoning: The navigation mode must support spatial reasoning across the generated sequence. This implies that the world model must understand not just static spatial layouts but also dynamic transformations (e.g., agent motion, object displacement) to effectively fulfill the navigation mode. Only when all three criteria are satisfied can condition be said to activate the navigation mode of world model. This triadic formulation provides systematic way to assess whether video generation model exhibits genuine planning and interactiveness, as opposed to merely replicating appearance or motion patterns. As such, the navigation mode serves as litmus test for the maturity of world model based generation systems in both controlled and interactive settings. In addition, future applications such as VR/AR or embodied AI may unify navigation and interaction via shared state feedback loops, extending beyond the current taxonomy."
        },
        {
            "title": "3 Roadmap",
            "content": "As discussed in Section 1, video generation models fundamentally comprise two components: an implicit world model and video renderer. The world model is responsible for simulating and predicting the evolution of world states, encapsulating physical laws, causal dynamics, flexible controllability, and real-world planning. The video renderer, in turn, translates the internal states of the world model into visual outputs, specifically in video format, interpretable and accessible to both humans and intelligent agents. This formulation underscores that video generation is not solely about producing realistic visuals; rather, it is about simulating and visualizing coherent world dynamics. As video generation increasingly assumes the role of world model, its development can be characterized by four major generations. Figure 5 outlines the expected levels of faithfulness, interactivity, and planning across these generations, along with the corresponding advancements in capability at each stage. This figure further decomposes these three core dimensions into more fine-grained secondand third-level sub-capabilities. The characteristics and distinctions of the four generations are elaborated below: (1) Generation 1 - Faithfulness: Superficial Simulation of the Real World Superficial Faithfulness. Faithfulness is the primary capability emphasized in Generation 1, as it represents the foundational step from video generation towards world models, which shifts from static realism to realistic motion dynamics. This generation is categorized into two stages: the Basic Level and the Advanced Level. The Basic Level consists of traditional and classical approaches, primarily based on GANs and early diffusion models. In contrast, the Advanced Level encompasses video generation models that exhibit superficial faithfulness and low-level interactiveness, while still lacking explicit planning capabilities. In this axis, short and long faithfulness denote the temporal span over which realistic and physically consistent generations can be maintained. We adopt practical threshold where long corresponds to sequences of five seconds or longer at 24 FPS, following the convention that such durations begin to reveal models stability as world simulator rather than mere simple video generator or single motion synthesizer. This temporal coherence implicitly depends on the models internal memory mechanisms, even though memory itself is not presented as separate capability axis. Thus, although memory is important capability to the operation of any world model, we treat it as an underlying mechanism that enhances along with the mentioned observable capabilities, such as temporal faithfulness and consistent planning rather than as standalone dimension in this taxonomy. As illustrated in Figure 5, models in Generation 1 are expected to achieve superficial faithfulness, typically demonstrated by the ability to generate short video clips, such as animated video, with tolerable motion distortions. In addition, these models are required to maintain basic video-text consistency, accurately reflecting the primary subject and general background content. However, they often miss specific entities, distort style ordering, and produce unrealistic or incoherent motion patterns at relatively high frequency. Low-Level Interactiveness. In terms of interactiveness, Generation 1 models exhibit low-level interactiveness, enabling pixel-level interaction and limited controllability. These systems typically support only single short action composed of few steps, guided by simple commands such as jump, grab the cup, or turn left. In contrast, they fail to interpret or execute more complex instructions such as clean the table, cook dinner, or find the key in several drawers. Additionally, these models achieve basic condition-to-video consistency, where the generated motion aligns with input conditions to some extent but is often accompanied by subject distortions and incoherent background transitions. While some approaches may incorporate auxiliary conditioning inputs, they primarily rely on spatial modalities, such as sketches, layouts, depth maps, and segmentation masks, which results in limited flexible controllability. Without Obvious Planning Ability. Due to the restricted faithfulness and interactiveness in Generation 1, planning capability has not yet emerged at this stage. (2) Generation 2 - Interactiveness: Controllable and Interactive Simulation of the Real World Semantic and Navigational Interactiveness. Interactiveness is the core capability emphasized in Generation 2, termed as semantic and navigational interactiveness, marking significant advancement in the dimensions of control flexibility, condition-video consistency, subject-centric controllability, and intrinsic model competence. Models in this generation enable flexible control, particularly conditioned on navigation modes, including actions, text commands, and predefined trajectories. They also exhibit emerging basic reasoning abilities, such as inferring motion sequences from high-level commands. In terms of condition-video consistency, Generation 2 models demonstrate notable improvements: they can generate complete and coherent motions with minimal dynamic distortion, and maintain visually appropriate and semantically aligned backgrounds. Another hallmark of this generation is basic subject-centric external control, where the model is capable of interpreting and executing control signals directed at specific subject, such as instructing one agent to perform sequence of actions or dynamically adjusting the viewpoint around that subject. However, these models typically operate within relatively static or simplified backgrounds. In addition, the intrinsic model capabilities have advanced significantly, as evidenced by the integration of 3D spatial understanding Fig. 5 Overview of the Capabilities of World Model Across 4 Generations. This figure presents the three main capabilities of world models, along with their corresponding secondary capabilities under each category. and stronger semantic comprehension rather than basic pixel-level understanding. Together, these advancements position Generation 2 as transitional phase toward truly interactive and semantically grounded world modeling. Additionally, the long-term vision of interactiveness is building general-purpose simulator that models the real world and allows human or agent interaction. For example, simulation-based games such as Euro Truck Simulator or SimCity can be viewed as early domain-specific prototypes of interactive world models. Consistent Faithfulness. Beyond improved interactiveness, Generation 2 world models show significant progress in maintaining temporal coherence and videotext consistency across long and complex sequences. The ability to generate longer videos contributes directly to temporal consistency, ensuring stable object dynamics and scene layouts over time. Moreover, these models achieve perfect video-text consistency, faithfully rendering all mentioned entities, motions, and events aligned with the given inputs. This generation also begins to capture aspects of the basic physical world, including projective geometry and spatial appropriateness, which contribute to more physically plausible and coherent video generations. Simple Task Planning. Additionally, this generation marks the emergence of simple task planning capabilities. While still limited in scope, these models show early signs of task-oriented planning, such as generating video content that follows coherent intention or directive within approximate ten steps, albeit without physical evolution or complex interaction. They can handle basic short-horizon goal-directed behaviors observable in certain video and robot application models, such as those navigating via goal image or executing twostep actions (e.g., pick-and-place). This reflects an early form of planning competence, laying the groundwork for advanced decision-making and long-horizon reasoning in subsequent generations and downstream tasks. (3) Generation 3 - Planning: Real-Time and Complex Prediction of the Real World Complex Task Planning. Planning refers to the models ability to simulate the future evolution of given world state. Generation 3 world models take this capability to new level, achieving complex task planning. This includes generating long-term video sequences that exhibit self-evolving progress at mesoscopic spatiotemporal scale, enabling the simulation of complex tasks with dozens or even hundreds of motion steps involving multiple interacting entities, dynamic viewpoint transitions, and scene transformations. These planning outcomes are not static; they can adapt in real time to interactions from both the internal state and the external environment. The broader vision for this level of planning is to faithfully simulate the evolution of the physical world under complex systems, such as weather patterns, narrative plots, cooking processes in robotics, population dynamics, or animal migrations. vivid imagination of this capability is portrayed in Liu Cixins sci-fi novel The Mirror, where super simulator is capable of projecting the future of the world with arbitrary precision, not merely replaying the past, but modeling the living, ever-changing future. Physically Intrinsic Faithfulness. Beyond planning, Generation 3 world models reach physically intrinsic faithfulness, that is, new pinnacle in physical plausibility, enabling simulations that evolve according to 8 the intrinsic physical principles of the real world. The models are capable of generating arbitrarily long video sequences, which brings higher complexity and enables them to create new motions, entities, viewpoints, and scenes over time, all while maintaining temporal coherence. More impressively, these models internalize the laws of physics themselves. Generation 3 models show evidence of learning causal dynamics across multiple physical domains. Examples include rigid-body mechanics (e.g., free-fall, collisions), fluid dynamics, and potentially electromagnetic effects. This represents fundamental leap: instead of approximating appearances, these models simulate underlying causal processes, leading to greater scientific fidelity and application potential. Real-Time and Local Interactiveness. In terms of interaction, Generation 3 achieves real-time and local interactiveness with high-fidelity controllability, enabling frame-level interaction without perceptible delay. Users can engage with the world model seamlessly, issuing commands and stimuli that lead to instant, coherent changes, whether its modifying an objects trajectory, switching perspectives, or inserting new entity into the scene. In addition, local control becomes precise and expressive. These models support subject-centric manipulation with fine-grained attention to contextual and background consistency. For example, user can focus on single characters behavior while the surrounding environment continues to evolve naturally with rich, photorealistic details, all without compromising visual or physical consistency. (4) Generation 4 - Stochasticity: Low-Probability and Multi-Scale Modeling of the Real World Stochasticity Planning. Generation 4 world models advance planning capabilities by incorporating stochasticity-aware reasoning, enabling the simulation of both high-probability and low-probability events aligned to the real-world distribution. This supports not only deterministic future prediction but also probabilistic modeling of diverse potential outcomes, especially proactive modeling of black swan events such as earthquakes, tsunamis, financial crises, and asteroid impacts. Furthermore, Generation 4 achieves arbitrary spatial and temporal scale planning. In the spatial domain, the model can plan across macroscopic scales such as universe-level evolution and microscopic scales like microbial dynamics or atomic-level transitions. Similarly, in the temporal domain, the model is capable of operating across vast time scales, from long-term evolution spanning years or centuries (requiring time compression and critical event selection ability), to mid-scale physical world dynamics, down to fine-grained, high-frequency phenomena such as insect wing beats or human pupil micro-movements. This ability to plan across stochastic events and arbitrary scales represents critical step toward building general-purpose simulation engines that align more closely with the complexity and uncertainty of the real world. Some recent works [28, 97, 98] have begun to explore the mesoscopic-scale planning capabilities of world models, but both the microscopic and macroscopic scales remain largely underexplored. However, these scales are essential for faithfully simulating the physical world, as they capture fine-grained interactions and high-level structural dynamics, respectively. Physically Intrinsic Faithfulness. In Generation 4, the capacity for physical faithfulness remains consistent with that of Generation 3. This is because Generation 3 has already achieved high level of physical realism by accurately adhering to intrinsic physical laws and simulating plausible, causally coherent environments. As such, Generation 4 inherits and maintains this state-of-the-art fidelity, providing solid and reliable physical foundation upon which more advanced capabilities, such as stochasticity-aware planning and global interactiveness, are built. Global and Multi-Modal Interactiveness. In addition to its advanced planning capabilities, Generation 4 world models also exhibit leap in global and multi-modal interactiveness. These models are capable of predicting long-term, multimodal influences resulting from external interventions, allowing for sustained, temporally extended interactions across vision, language, and control modalities. At the core of this interactiveness is form of global control, where an internal agent equipped with mental world model acts as the primary decision-making entity within the simulated environment. Moreover, these models support multientity control by responding to external signals, enabling coordination among multiple agents or systems within the scene. The incorporation of dynamic and evolving backgrounds further enriches the simulation, allowing for more realistic and adaptive world modeling. To concretely distinguish the capabilities of Generations 14, consider consistent scenario: person making cup of coffee in kitchen. Generation 1 models can generate few frames of water pouring or static view of coffee cup appearing, with no awareness of human intent or task continuity. Generation 2 models can depict simple, goal-directed action sequence within short horizon, such as person picking up kettle and pouring water into cup following short term, simple commands such as grab the cup and pour the water into the cup. The motion is locally consistent, and the action follows visible goal, but the model lacks persistent understanding of the broader task or multi-step dependencies. Generation 3 models begin to exhibit real-time generation and interactive consistency, enabling video generation adaptively to abstract and long-horizon command such as make cup of coffee. They 9 Fig. 6 Chronological Overview of Methods from Video Generation to World Models. The figure presents chronological overview along the horizontal axis, categorizing existing methods by four application domains along the vertical axis: general scenes, robotics, autonomous driving, and gaming. Additionally, different colors are used to indicate different generations of world models. maintain spatial coherence of the kitchen layout and object positions over several seconds, showing flexible navigation and environmental awareness. Generation 4 models are envisioned to autonomously complete the entire coffee-making procedure, planning and executing multi-step actions such as heating water, grinding beans, pouring, and serving, while maintaining physical faithfulness, temporal continuity, and coherent interaction with dynamic objects in the scene. Moreover, through multiple inference cycles, realistic incidents, such as accidentally spilling hot water onto the table, may emerge naturally, reflecting plausible real-world possibilities. This progressive depiction within the same environment clarifies how each generation expands its prediction and planning ability."
        },
        {
            "title": "4 Methods: A Hierarchical Taxonomy",
            "content": "Before delving into each generation in detail, it is important to emphasize that the three core capabilities, faithfulness, interactiveness, and planning, develop in parallel rather than appearing sequentially. Each generation highlights one capability as its dominant focus, reflecting the primary research emphasis and maturity level of that stage, yet all three capabilities continue to advance simultaneously across generations. This coevolution reflects the natural progression of video generation models toward comprehensive world modeling, where visual realism, controllability, and long-horizon reasoning are continuously refined in tandem rather than introduced one after another. Similarly, the application domains, general scenes, robotics, autonomous driving, and gaming, analyzed in this section are not exclusive to any single generation. Instead, they overlap across all four, as the underlying modeling principles remain consistent while the level of capability and control gradually increases. We organize these applications under Generation 2 primarily because most of the recent works and open-source systems lie in this stage. Subsequent generations naturally extend these same applications toward higher-level planning, real-time interaction, and stochastic world simulation."
        },
        {
            "title": "4.1 Generation 1 - Faithfulness: Accurate Simulation of the Real World",
            "content": "As shown in Figure 5, in the first generation, the requirements for world models focus on the general video generation model capabilities and basic interaction characteristics. Similar to VBench series [194, 195], the focus is placed on the superficial faithfulness, including pixel-level frame quality, the duration of the generated video, and video-text consistency. Building upon this foundation, we systematically categorize the existing methods in Generation 1 for the video foundation models 4.1.1, spatial world models 4.1.2, and navigated world models 4.1.3. 10 Table 2 Summary and Comparison of Works for Video Foundation Models. * indicates models that include audio input or output. We compare models in terms of their backbone architectures, supported frame counts, and output resolutions. For models released in multiple duration or resolution versions, we report the highest available configuration by default."
        },
        {
            "title": "Backbone Frames Resolution",
            "content": "LTX-Video [99] JT-CV [101]"
        },
        {
            "title": "DiT",
            "content": "- OpenSora 1.3 [103]"
        },
        {
            "title": "DiT",
            "content": "EasyAnimate V5 [16] DiT Lumiere [105] Latte [107] Mira [109] SVD [111] SEINE [113] DynamiCrafter [115] Show-1 [117] ModelScope [119] Pika 1.5 [29] MAGVIT [122] Imagen Video [124] CogVideo [126] InstructVideo [128] Allegro [130] StreamingT2V [132] VideoTetris [134]"
        },
        {
            "title": "UNet",
            "content": "-"
        },
        {
            "title": "DiT",
            "content": "AR"
        },
        {
            "title": "UNet",
            "content": "Hailuo 02 [36] - MAGI-1 [12] AR-DiT Nova Reel [136] Wan 2.1 [13] Open-Sora 2.0 [14] MiracleVision [138] Vchitect-2.0 [37] HunyuanVideo [15] Mochi-1 [140] Gen-3* [141] -"
        },
        {
            "title": "DiT",
            "content": "- -"
        },
        {
            "title": "DiT",
            "content": "- EasyAnimateV5.1 [16] DiT Vidu Q1 [17] Luma Ray2 [35]"
        },
        {
            "title": "UNet",
            "content": "- CausVid [18] AR-DiT Movie Gen [19] Wan 2.2* [143] Luma Ray3* [145] V-JEPA 2 [28] Genie 3 [147]"
        },
        {
            "title": "DiT",
            "content": "- AR AR Generation 1 World Model 768 512 STIV [100] 1214 2158 Hailuo 01 [102] 1280 720 Open-Sora Plan [104] 1024 1024 King 1.0 [21] 1024 1024 VideoCrafter2 [106] 512 512 MagicVideo-V2 [108] 121 113 49 80 16 2880 384 HiGen [110] 1024 576 I2VGen-XL [112] 560 240 VideoCrafter1 [114] 1024 MAGVIT-v2 [116] 576 320 LaVie [118] 256 256 AnimateDiff v2 [120] 1280 Gen-2 [121] 128 128 MagicVideo [123] 1280 768 Make-A-Video [125] 480 RepVideo [127] 256 256 Emu Video [129] 1280 720 DimensionX [131] 256 VideoPoet [133] 512 320 Owl-1 [135] Generation 2 World Model"
        },
        {
            "title": "DiT",
            "content": "-"
        },
        {
            "title": "UNet",
            "content": "-"
        },
        {
            "title": "UNet",
            "content": "-"
        },
        {
            "title": "UNet",
            "content": "AR AR"
        },
        {
            "title": "UNet",
            "content": "1920 1080 Seedance 1.0 [26]"
        },
        {
            "title": "DiT",
            "content": "720 720 Veo 3* [33] 1280 720 Gen-4 [32] 1280 720 Step-Video-T2V [137] 1024 576 SkyReels-V2 [25] 720 480 PixverseV4.5 [139] - - -"
        },
        {
            "title": "DiT",
            "content": "- 768 432 Cosmos-Predict2 [24] DiT/AR 60 121 300 16 94 32 - 16 - 16 16 96 - - 16 16 16 80 - 240 192 240 200 121 80 512 512 1280 720 1024 576 1920 1080 512 1048 1048 448 256 1280 720 1024 576 640 360 512 512 512 1408 768 1024 1024 768 768 512 512 512 256 256 512 896 - 1920 1080 1280 720 1280 960 540 1280 720 1920 1080 1280 720 1280 720 APT2 [23] AR-DiT 1440 1280 720 1969 960 CogVideoX1.5 [22] 1280 Kling 2.1 Master [21] 1024 1024 Jimeng [30] 1920 1080 Sora [11] 1920 Pika 2.2* [29]"
        },
        {
            "title": "DiT",
            "content": "-"
        },
        {
            "title": "DiT",
            "content": "- 640 352 Self Forcing [20] AR-DiT 1920 1080 Imagine v0.9* [142] 1280 720 Sora 2* [144] 3840 2160 Vibes* [146] -"
        },
        {
            "title": "DiT",
            "content": "- 160 240 96 480 81 80 1800 240 1360 768 1920 1280 720 1920 1080 1920 1080 854 480 464 688 1920 - Generation 3 World Model 25 16 16 16 120 192 128 32 88 1200 16 240 96 81 120 120 40 129 240 49 120 240 121 120 240 - - MirageLSD [27] AR-DiT"
        },
        {
            "title": "Infinite",
            "content": "512 512 > 1440"
        },
        {
            "title": "4.1.1 Foundation Models with Visual-Centric World Knowledge",
            "content": "While video generation models in other domains [196] have also made significant progress, this survey focuses specifically on foundation models, pre-trained video generation methods that produce videos based solely on = {T, O} without relying on additional conditioning signals or modalities, due to their potential to support or even serve as comprehensive world models. More importantly, the capabilities of video foundation models largely determine the effectiveness of subsequent adaptations or fine-tuning for spatial conditions and navigation modes. With their foundational faithfulness and basic interactiveness, these models form the backbone upon which conditional mechanisms (e.g., ControlNet [74], Multi-Modal Transformer, CrossAttention, Concatenation, and Addition) can be applied to achieve high-quality, physically plausible, and task-aware video generation. 11 Table 3 Summary and Comparison of Works for Video Generation as World Model in General Scene. Method Condition Navigation Output Task Foundation Model Generation 1 World Model Moonshot [148] SparseCtrl [149] ConditionVideo [150] ControlVideo [151] VideoComposer [152] Text2Video-Zero [153] Diffusion4D [154] SV3D [155] V3D [156] VideoMV [157] Geometry Geometry Geometry Geometry Geometry Geometry 3D 3D 3D 3D 3D 3D PhysGen [158] Physics Go-with-the-Flow [159] Motion Prompting [160] SG-I2V [161] Image Conductor [162] TrailBlazer [163] MotionCtrl [164] 3D None None None None None DragAnything [165] Geometry PEEKABOO [166] FreeTraj [167] Direct-A-Video [168] None None None None None None None Trajectory None None None None None None None None Trajectory Trajectory Trajectory Trajectory Trajectory Trajectory Trajectory Trajectory Trajectory Trajectory FullDiT [169] Geometry Camera Motion ReCamMaster [170] CineMaster [171] AC3D [172] VD3D [173] CameraCtrl [174] GCD [175] CVD [176] CamCo [177] None 3D None None None None None None None Camera Motion Camera Motion Camera Motion Camera Motion Camera Motion Camera Motion Camera Motion Camera Motion Camera Motion Video Video Video Video Video Video Video, 3D Video, 3D Video, 3D Video, 3D Video, 3D Video, 3D Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Video Generation, Editing - Gen. Ani. Generation Generation Generation, Inpainting Generation, Inpainting Generation, Construction Generation, Construction Generation Generation, Construction Generation, Construction Generation, Construction Generation AnimateDiff [120] - - - - ModelScope [119] I2VGen-XL [112] SVD [111] SVD [111] ModelScope[119] I2VGen-XL[112] SEINE [113] Motion Control, Camera Control CogVideoX [22] Motion Control, Camera Control Lumiere [105] Motion Control, Camera Control SVD [111] Motion Control, Camera Control Animatediff [120] Motion Control ModelScope [119] Motion Control, Camera Control VideoCrafter1 [114] Motion Control, Camera Control SVD [111] Motion Control Motion Control ModelScope [119] VideoCrafter1 [114] Motion Control, Camera Control. ModelScope [119] Generation Camera Control Cameral Control Camera Control Camera Control Camera Control Camera Control Generation Camera Control Camera Control SketchVideo [178] Geometry DaS [179] GS-DiT [180] PISA [181] PhyT2V [182] Zhao et al. [183] WISA [185] CamCloneMaster [186] Context as Memory [187] 3DTrajMaster [188] CameraCtrl II [189] GEN3C [190] CamTrol [191] AETHER [98] SlowFast-VGen [192] Pandora [193] NWM [97] V-JEPA 2-AC [28] 3D 3D Physics Physics Physics Physics None None None None 3D 3D None None None None None Generation 2 World Model None None None None None None None Camera Motion Camera Motion Trajectory, Camera Motion Camera Motion Camera Motion Camera Motion Action Instruction Instruction Video Video Video Video Video Video Video Video Video Video Video Video Video Generation, Editing Motion Control, Camera Control, Motion Transfer, Animating Generation Generation Gen. Generation Generation Generation Generation Motion Control, Camera Control Camera Control Camera Control Camera Control Video, 3D, Action Generation Video Video Generation Generation Generation 3 World Model Action Action Video Video Generation, Planning Generation, Planning - V-JEPA 2 [28] Private - - - - SVD [111] AnimateDiff [120] SVD [111] AnimateDiff [120] SVD [111] CogVideoX [22] CogVideoX [22] CogVideoX [22] Open-Sora [103] CogVideoX [22] MMDiT [184] CogVideoX [22] Private Private Private - SVD [111] SVD [111] CogVideo [126] ModelScope [119] DynamiCrafter [115] In Generation 1, foundation models demonstrate superficial faithfulness and low-level interactiveness by generating short videos with basic textvideo consistency and limited control over simple object motions. They capture key visual details such as object boundaries, textures, and coherent foreground and background layouts. These capabilities support world models in downstream applications like autonomous navigation [160, 161, 163168, 174, 176] and interactive decision-making [197202]. We summarize representative pre-trained text-to-video (T2V) models [21, 105108, 110, 111, 114, 117 121, 126, 128130, 132, 133], along with commonly used pre-trained image-to-video (I2V) models [16, 21, 102, 104, 111, 112, 114, 116, 131, 133, 203], examining their backbone architectures, video lengths, resolutions, and post-training capabilities related to interactivity and embodiment which are core abilities for serving as world models. This survey also covers both open-sourced models [16, 99, 103, 104, 106, 107, 110, 114, 117 120, 127, 128, 130133] and commercial APIs [21, 29, 102, 121, 129]. For instance, CogVideo [126] is widely regarded as an early open-source, large-scale pretrained T2V model that played pivotal role in initiating the development of video foundation models. The release of Kling 1.0 [21], built on DiT backbone, further advanced generative AI toward longer duration and greater visual quality. Following the initial success of Generation 1 models in producing basic textvideo consistency and plausible motion dynamics, some research shifted focus toward long and efficient video generation, sometimes at 12 the expense of faithfulness. This led to the development of streaming video generation based on an autoregressive architecture. Specifically, LTX-Video [99] achieves semi-real-time generation, continuously producing video clips faster than they can be viewed, by leveraging high-compression Video-VAE and denoising transformer with full spatiotemporal attention. While notable for its generation speed, its limited fidelity of generated videos still places it within the first generation. However, Generation 1 models lack strong 3D dynamics, making it difficult to maintain physically consistent motion, realistic object interactions, and task-oriented planning ability. As result, generated videos may contain motion distortions, spatial misalignments, or other unrealistic artifacts that restrict their utility in more advanced interactive or long-horizon planning tasks. These limitations motivate the developments in Generation 2 foundation models, which aim to enhance visual faithfulness, controllable interaction, and planning ability for more complex world-modeling scenarios."
        },
        {
            "title": "4.1.2 Spatial World Model",
            "content": "Although Any2Caption [204] introduces the novel paradigm of any-condition-to-caption for video generation, the direct encoding of control signals and their integration with the video generation model remain essential. In particular, adopting conditioned world model approach, where control signals are explicitly represented and fused within the generation process, provides more intuitive and effective means of conditional guidance. In this section, we introduce spatial world models with input = {T, O, X}, which, although exhibiting some interaction capabilities, still suffer from limited video quality and controllability. Representative methods are summarized in Table 3 and Table 4. In general scenarios, geometry conditioned world models[148153, 205] leverage conditional inputs that are semantically or structurally aligned with background content. Typical conditions include sketches, canny edges, depth maps, motion vectors, and human poses, extending the controllability paradigm established by ControlNet[74] for image generation to the video domain. Representative works [149, 152156] support text-to-video generation with geometry priors and further extend to other tasks such as animation [149], interpolation [152, 153] and 3D construction [155, 156]. Notably, SparseCtrl [149] introduces an image reference as geometry guidance for I2V generation, and due to its flexibility, it has been widely adopted as baseline for controllable I2V/T2V generation under sparse conditioning. 3D prior world models [154157, 206, 207] utilize 3D point cloud, explicit 3D modeling, or multi-view imagery to improve spatial consistency in generated videos. While such priors are inherently tied to scene content and thus less adaptable across domains, their strong geometric grounding makes them more effective for producing 3D consistent videos and supporting 3D dynamics. Representative works include Diffusion4D [154], which incorporates explicit 3D point-cloud priors within video diffusion framework to achieve spatio-temporal consistency and high-fidelity 4D reconstruction, and Zero4D [206], which employs explicit reconstructed 3D geometry from single video in training-free manner to guide off-the-shelf video diffusion models for rapid 4D generation. Physics prior world models incorporate physical signals or principles into video generation, enabling models to capture and reproduce realistic physical laws. For example, PhysGen [158] models forces, torques, and object interactions to simulate classical mechanics phenomena such as collisions governed by Newtonian dynamics. This approach allows generated videos to exhibit motion that is more consistent with real-world physics, thereby enhancing their plausibility and potential for downstream simulation tasks. Due to the characteristics of the tasks and application scenarios, early autonomous driving research featured several spatial conditioned world models compared with robotics and gaming. For example, Delphi [208] and Panacea [209] employed only scene layouts as spatial conditions to synthesize realistic driving videos, whereas MagicDrive [210] combined layouts, bounding boxes, and fixed camera-pose parameters as hybrid spatial condition. Additionally, models [150, 151, 153] condition on human pose are also categorize in spatial condition world model. The human pose we mention refers to the skeletal structure of person in image space, represented as sequence of keypoints that condition motion or structure within fixed environment. Although human pose sequences have temporal form, they are spatially bound and depend on scene context, thus classified as spatial conditions rather than navigation conditions. In contrast, navigation conditions, such as trajectories or text commands are scene-independent and allow free navigation across different spatial contexts"
        },
        {
            "title": "4.1.3 Navigation World Model",
            "content": "Navigation world models inviting into the input set as = {T, O, Au, }. In general scenes, navigation world models are primarily developed for motion control tasks, including local subject motion [159161, 163 168] and camera motion [170176, 211]. These methods often build upon pre-trained video foundation models [22, 105, 111, 114, 119, 120], incorporating training-free control modules to enable interactive motion and camera control. However, due to current limitations in video length, generation quality, and the diversity of controllable entities, these approaches remain in the Generation 1 world models. For local motion control 13 Fig. 7 Overview of Condition Injection Strategies. This figure illustrates five representative strategies for condition injection. Subfigures (a) through (e) correspond to the ControlNet-based method, multi-modal Transformer, cross-attention, concatenation, and addition, respectively. tasks, DragAnything [165] offers intuitive point-based dragging to finely adjust subject motion, enabling precise spatial manipulation in generated videos. Peekaboo [166] elevates this to semantic-level control, where object-centric motion can be directed in line with textual prompts. Pushing further, Trailblazer [163] incorporates trajectory-driven navigation, supporting complex and continuous motion generation for both subjects and cameras, advancing toward more coherent and context-aware video navigation. In terms of camera control, existing methods [172, 173, 177] inject camera motion trajectories, represented in Plucker coordinates, into pre-trained foundation models via ControlNet-based modules. Alternatively, methods such as GCD [175] directly manipulate camera extrinsics, i.e., rotation and translation, as input to control camera viewpoints. Building on the general-scene setting, navigational world models [197200, 208210, 212217] have also been explored in more domain-specific contexts such as robotics, autonomous driving, and gaming. For instance, in robotics, RoboDreamer [213] incorporates additional modalities, such as sketches, to facilitate editable video generation, enabling interaction-level content manipulation. In autonomous driving, recent works, including ADriver-I [215], DriveDreamer [198], Drive-WM [199], MILE [200], and GenAD [216], further integrate high-level scene semantics and temporal planning priors to enhance realism, consistency, and task-oriented controllability. In addition, in virtual gaming, playable video generation [218] pioneers interactive, user-controlled gameplay videos, enabling flexible manipulation of in-game entities and camera perspectives. Moreover, MarioVGG [217] focuses on faithfully recreating game scenes by learning domain-specific visual dynamics, offering higher visual fidelity and consistency within structured game environments."
        },
        {
            "title": "4.2 Generation 2 - Interactiveness: Controllability and Interactive Dynamics",
            "content": "Compared with Generation 1, Generation 2 world models achieve marked leap in interactiveness, representing decisive step toward dynamic and flexible interactive world modeling. Within this generation, interaction can be realized either through spatial conditions (e.g., sketches, depth, motion pose) or through navigation modes (e.g., trajectory, action, instruction). Spatial conditioned approaches [178182, 185] excel in producing high-quality videos with strong 3D dynamics, accurate conditionvideo consistency, and stable scene interactions, though their flexibility in real-time control remains limited; nonetheless, these strengths firmly place them within the Generation 2 category. In addition, navigational world models [98, 188193] offer greater flexibility and transferability, as they do not depend on pre-existing contextual knowledge and can generalize across diverse entities and scenarios, making them inherently suitable for real-time and general-purpose applications. Starting from Generation 2, interactivity and controllability have become essential capabilities for video generation methods serving as world models. These requirements persist and evolve throughout Generation 3 and Generation 4, with increasingly higher demands placed on control granularity, control modes, modality diversity, real-time responsiveness, and control sensitivity. Given the advancement of controllable video generation models and the critical importance of integrating control signals into foundational video generation models in principled manner, we systematically mentioned five major condition injection strategies: Condition Injection via ControlNet, Multi-modal Transformer, Cross-Attention, Concatenation, and Addition, with the fundamental architectures shown in Figure 7. In this section, we review existing works in Generation 2, structured around world foundation models 4.2.1 and four representative generative scenarios: general scenes (4.2.2), robotics (4.2.3), autonomous driving (4.2.4), and gaming (4.2.5). Within each scenario, we organize our discussion by tracing the progression from spatial condition-based methods to the more generalizable navigation-mode-based methods, further breaking down each category according to its defining characteristics and modeling approaches."
        },
        {
            "title": "4.2.1 Foundation Models with Dynamic-Aware World Knowledge",
            "content": "Building upon the first generation, Generation 2 foundation models make decisive leap in interactiveness, representing major step toward more dynamic and flexible world modeling. These advances stem from large-scale pretraining, higher-capacity architectures, and richer conditioning interfaces, enabling the models to adapt more effectively to downstream control mechanisms. As result, spatial conditions and navigation-mode controls can now be integrated with higher video fidelity, stronger spatiotemporal consistency, text-video consistency, greater control flexibility, and better 3D and semantic-level interaction, qualities that directly improve performance in downstream tasks across diverse scenarios. Recent research builds on this momentum, consolidating video generation as the tool for controllable world models. These models [11, 1316, 21, 22, 25, 26, 140, 142145] commonly adopt DiT-based [4] or some of them leverage [18, 20, 23] hybrid architectures that combine diffusion models with causal, frame-by-frame autoregressive mechanisms, achieving fast, few-step distilled diffusion process to generate each frame. For instance, models such as MAGI-1 [12] and CausVid [18] extend Diffusion Forcing [219] for causal video generation. Self Forcing [20] follows this paradigm but removes exposure bias by training with full autoregressive rollout that matches the true inference distribution, enabling efficient video-level supervision and semi-real-time generation. And VMoBA [67] introduces mixture-of-block attention mechanism to enhance spatiotemporal representation learning in video diffusion models, achieving improved generation quality and efficiency. In addition, the Cosmos series [24, 31] illustrates how powerful foundation model, trained on diverse multimodal inputs, can serve as digital twin of the physical world while maintaining tight integration with external control, memory, and planning modules. When applied to complex, interactive environments, Cosmos demonstrates consistent, goal-aligned video generation across navigation and simulation tasks, underscoring how improved interactiveness and condition adaptability in Generation 2 lay the groundwork for more general-purpose, controllable world models. In addition, Luma AI introduced the Luma - Dream Machine, in which Luma Ray2 [35] is next-generation video generative model that integrates ultra-realistic detail, coherent motion, and logical event sequencing, making it particularly powerful in building world models."
        },
        {
            "title": "4.2.2 Video Generation as World Model in General Scene",
            "content": "In this section, we introduce methods that transition from video generation to world modeling in generalpurpose scenarios, with representative methods summarized in Table 3. The term general follows its definition in the broader video generation, referring to settings that are not tailored to any specific downstream application. Our discussion focuses on two main groups of methods: (1) Approaches with spatial condition [178182, 185], which incorporate various priors such as Geometry Conditions, 3D Priors, and Physical Priors. (2) Approaches with navigation mode [98, 188193], which condition generation on forms of controllable input, including action, trajectory, camera motion, and text instruction. Geometry Conditioned World Model. Geometry conditioned world models utilize conditions that are semantically or structurally aligned with the background content. These conditions include, but are not limited to, sketches, canny edges, depth maps, motion vectors, and human poses, inheriting the controllability paradigm introduced by ControlNet [74] for image generation. Compared to Generation 1 approaches, Generation 2 models achieve much finer-grained interaction and better temporal fidelity through sparse, yet semantically rich, conditioning signals. For example, SketchVideo [178] enables sketch-based video generation and editing by injecting sparse geometric cues, i.e., keyframe sketches, into pretrained DiT model, maintaining temporal consistency through inter-frame attention. Compared to Generation 1, it demonstrates significantly improved spatial controllability and interactive editing with minimal user input. 3D Prior World Model. 3D prior world models typically aim at 3D or 4D scene reconstruction, leveraging 3D priors to enhance the performance of video generation tasks in construction-oriented settings. In addition to this construction perspective, some approaches [179, 180] introduce 3D information specifically to benefit video generation itself. For example, DaS [179] leverages 3D point clouds to generate 3D tracking videos of the subject, which are then used as prompts to guide high-quality video synthesis. Physical Prior World Model. Physical prior world models [181183, 185] aim to enhance the physical plausibility of video generation by embedding explicit physical laws and principles into the modeling process. For instance, WISA [185] incorporates physical formulas and principles directly as embeddings. PhyT2V [182] leverages the physical reasoning capabilities of large language models (LLMs) to guide and improve the physical consistency of video generation models. Camera Motion Navigation World Model. Camera Motion Navigation World Models [186, 188191] also rely on trajectories for navigation. Compared to general trajectory navigation, the input trajectories here are derived from camera motion, in the form of explicit camera moving trajectories or implicit reference videos. Moreover, camera movement naturally simulates first-person perspectives of agents or humans, this has evolved into distinct and rapidly developing subtask. Therefore, we discuss it separately in this section. For example, CameraCtrl II [189] introduces an efficient diffusion framework that supports dynamic video 15 generation guided by explicit camera trajectories, enabling smooth scene exploration through autoregressive clip extension and lightweight camera injection. In contrast, GEN3C[190] further enhances 3D spatial consistency by constructing global 3D cache from depth predictions, allowing precise camera control and structurally faithful rendering across complex motion paths, advancing the realism and controllability of camera-based navigation models. Instruction Navigation World Model. Instruction Navigation World Models [98, 192, 193] utilize textbased instructions, but unlike conventional text prompts that describe background elements, such as scene type, weather, or task themes, these instructions instead guide the dynamic aspects of video generation. Specifically, they control motion-related factors of either single or multiple subjects in the video, or the perspective dynamics of first-person agent, which can be human, robot, or camera. For instance, models like Pandora [193] and SlowFast-Gen [192] can execute directional commands such as turn left or turn right. In addition, SlowFast-Gen [192] supports perspective-based actions such as zooming in and out from navigational viewpoint. In addition, AETHER [98] unifies action-conditioned video prediction and visual planning within single model from the agents first-person view. Remarkably, it also demonstrates strong zero-shot generalization capabilities."
        },
        {
            "title": "4.2.3 Video Generation as World Model in Robotics",
            "content": "Due to the inherently interactive nature of robotics, it has naturally aligned with Generation 2 world models that utilize navigation modes from the very beginning of incorporating video generation models. Purely spatial conditioned approaches [31] are rarely adopted in this domain, as they offer little practical value. In robotics, three primary types of navigation modes are commonly used: action, text instruction, and goal. The action mode typically refers to low-level physical signals such as force and torque applied by robotic arms. The text instruction mode involves simple, short-horizon commands, e.g.pick up the pen, while goal navigation encompasses more complex tasks that require long-term planning, such as clean up the desk. Goal navigation may also be defined by target image representing the desired end state. For clarity, we refer to short-horizon prompts as text instructions and use the term goal navigation for tasks that require planning over extended temporal horizons, including both textual goals and image-based goal points. Action Navigation World Model. Action navigation world models [223227] typically involve the prediction of actions as output, or alternatively, leverage video generation models to assist in forecasting the next action. For example, UVA [225] and PAD [223] employ Transformer architectures to jointly encode video latent features and encoded actions into unified representation, which is then decoded into both future video frames and an action policy. Instruction & Goal Navigation World Model. In the domain of text-guided robotics methods, world models [229233, 235] at the action planning level have already begun to exhibit characteristics of basic planning capability in Generation 2. Since the introduction of AVDC [221], there has been rapid progress in robotics world models capable of goal-directed planning based on image goals or textual instructions. For instance, COMBO [230] builds compositional world model for multi-agent cooperation by factorizing joint actions and generating video predictions to simulate diverse outcomes. In contrast, UniPi [233] treats decision-making as text-conditioned video generation task, where goal described in natural language is translated into future visual trajectories, from which control actions are extracted. In contrast, methods such as Dreamitate [271] adopt two-stage approach, which first uses video generation model to produce high-quality future video sequences, which are subsequently utilized as input for action prediction. This separation of video synthesis and policy learning allows the system to benefit from strong visual foresight, enhancing decision-making performance in complex environments. Hybrid Navigation World Model. Recently, hybrid navigation methods [237, 239, 240] have emerged, enabling models to handle both simple text instructions and image-goal navigation simultaneously. Beyond the text modality, models like UniSim [239] and GR-1 [237] support multi-modal navigation using both text and action inputs to guide video synthesis. These multi-control setups raise important questions about balancing robustness and controllability across modalities, as well as potential conflicts between control signals. Notably, UniSim [239] supports semi-real-time interaction at the video level, aligning with our vision for advanced forms of Generation 2 world models."
        },
        {
            "title": "4.2.4 Video Generation as World Model in Autonomous Driving",
            "content": "Autonomous driving represents another critical application domain for world models. Existing world models in this context are primarily employed to synthesize large-scale, multi-scene datasets or to perform visual and policy prediction tasks, serving as potential solutions to advance autonomous driving toward Level 4 autonomy. 16 Table 4 Summary and comparison of works for video generation as world model in robotics, autonomous driving, and gaming. In the table, BBox refers to Bounding Box. Method Condition Navigation Mode Output Modality Foundation Model t R v s o u i s o R i D m t n G iVideoGPT [197] IRASim [220] Seer [212] RoboDreamer [213] MagicDrive [210] DriveScape [214] Delphi [208] Panacea [209] MILE [200] Drive-WM [199] None None None Sketch Generation 1 World Model Action Trajectory Instruction Instruction, Goal Layout, BBox, Camera Position Layout, BBox Layout Layout Layout BBox None None None None Action Action Action Video, Action - Video Video Video Video Video Video Video Video, Action, Layout Video, Action Video, Action Open-Sora [103] - - - SVD [111] - - - - - - - DriveDreamer [198] BBox, Geometry ADriver-I [215] GenAD [216] MarioVGG [217] PVG [218] Playable Environments [222] None None None None None Instruction Instruction, Trajactory Video Video Controller Controller Controller Game Level AVDC [221] Video Video - - Instruction, Goal Video, Text Cosmos-Transfer1 [31] Blur, Edge, Depth, Segmentation None Generation 2 World Model PAD [223] WPE [224] UVA [225] DWS [226] HMA [227] Genie Envisioner [228] EVA [229] COMBO [230] VideoAgent [231] VidMan [232] UniPi [233] VPDD [235] AVDC [221] Luo et.al [236] GR-1 [237] RoboMaster [238] UniSim [239] NavigeteDiff [240] Cosmos-Drive-Dreams [214] UniScene [241] DreamForge [242] DiVE [243] None None None None None None None None None None None None None Depth None None None None None Layout Layout Layout, BBox, Camera Position Layout, Geometry DrivingDiffusion[244] Layout, Optical DrivingGPT [245] InfinityDrive [246] GAIA-1 [247] Epona [248] GEM [249] DrivingWorld [250] None None None None Geometry None Action Action Action Action Action Action Action Instruction Instruction Instruction Instruction Instruction Instruction Goal Goal Action, Instruction Action, Instruction Action, Instruction, Camera None None None None None Action Action Action Trajectory Trajectory Trajectory MagicDrive-V2 [251] Layout, BBox, Camera Position Trajectory DriveDreamer-2 [252] None Instruction GAIA-2 [253] MaskGWM [254] Vista [255] Hunyuan-GameCraft [256] MaaG [258] GameNGen [259] W.H.A.M. [260] Genie [261] PlayGen [257] GameGen-X [262] The Matrix [263] MineWorld [264] GameFactory [265] WORLDMEM [266] DIAMOND [267] Genie-2 [269] Oasis [270] Layout, Geometry Action, Instruction None None None None None None Sketch None None None None None None None None None Action, Trajectory, Instruction, Goal Action, Trajectory, Instruction, Goal Controller Controller Controller Controller Controller Controller Keyboard Keyboard Keyboard, Mouse Keyboard, Mouse Keyboard, Mouse, Camera Keyboard, Mouse Keyboard, Mouse Keyboard, Mouse Video Video, Action Video Video, Action Video Video Cosmos [24] - - - Open-Sora [103] iVdeoGPT [197] Video, Action SVD [111] Video Video Video Video, Action Video, Action Video Video Video, Action Video - DynamiCrafter [115] AVDC [221] AVDC [221] Open-Sora [103] Cliport [234] - - AVDC [221] Video, Action - Video Video Video Video, LiDAR Video Video Video Video Video Video Video, Trajectory Video Video Video Video Video Video Video CogVideoX [22] - - Cosmos [24] SVD [111] - Open-Sora [103] - - - - - SVD [111] - CogVideoX [22] SVD [111] - - SVD [111] Video Game Level PlayGen [257] PlayGen [257] Video Video Video Video Video Video Video Video Video - - - - - - - - - Video, Action EDM [268] Video Video - - In this section, we focus on two major research directions: layout-conditioned world models and those that incorporate multiple navigation modes, including text instructions, trajectory control, action-driven interaction, and hybrid navigation strategies. detailed comparison of representative methods is provided in Table 4. Layout Prior World Model. Layout Prior World Models [241244, 272] in Autonomous Driving primarily refer to models where the input condition encodes the spatial distribution of vehicles and obstacles on the 17 road. This typically includes HD maps, depth information, 3D bounding boxes, and the most common form, BEV (Birds-Eye View) layouts. Representative works like UniScene [241], which directly synthesizes both videos and LiDAR point clouds from layout-based inputs, facilitating large-scale autonomous driving dataset generation. Additionally, models like DreamForge [242] introduce camera poses to better align the layout with the generated video, while Driving Diffusion [244] leverages optical flow, effectively serving as an alternative form of layout conditioning. However, because such layout information often relies on annotations available only in curated datasets, these approaches may lack practicality for real-time deployment. Accurate and timely collection of layout annotations in real-world applications remains challenging. Therefore, we consider layout-based methods to be an early-stage bridge toward building full-fledged world models in the autonomous driving domain. Instruction Navigation World Model. In autonomous driving scenarios, text instruction world models [252] focus on ego-centric viewpoint changes, such as executing right-turn command. However, existing models [252] mainly target video generation conditioned on single-step instructions. Their performance in handling multi-step commands or long-horizon navigation plans remains largely underexplored, particularly in terms of stability and robustness. Trajectory Navigation World Model. Trajectory Navigation World Models [248251] navigate the video generation process by conditioning on either the ego-vehicle trajectory [249251] or the trajectories of surrounding vehicles [201]. Most of these approaches [250, 251] inject trajectory information via trajectory-encoded sequences embedded through Transformers. Others, like GEM [249], utilize trainable LoRA modules [273] to incorporate trajectory-based navigation in more flexible and adaptive manner. Action Navigation World Model. In Action Navigation World Models [245247], the term action refers to vehicle control signals such as steering and velocity. DrivingGPT [245] adopts GPT-style autoregressive framework to achieve action-navigated video prediction. Other models [246] start from an initial state derived from combination of textual descriptions and reference images, and then generate future video frames conditioned on action signals. GAIA-1 [247] represents an interactive autonomous driving world model that casts world modeling as an unsupervised sequence modeling problem, mapping multimodal inputs into discrete tokens and predicting the next token in the sequence to simulate future scenarios. Hybrid Navigation World Model. With the rapid advancement of autonomous driving, there is growing trend toward the development of unified navigation world models in this domain. Recent works [253255] have begun to support multiple conditioning signals and navigation modes simultaneously, e.g., [253] combining text instructions with action signals, text with trajectories, or [254, 255] even integrating all four modalities: text instructions, trajectories, action commands, and goal point images. This convergence indicates that autonomous driving world models are gradually evolving toward planning capabilities, aligning with the characteristics of third-generation world models as defined in this paper. However, despite these advances, real-time interaction remains largely absent from current autonomous driving world models. This limitation significantly hinders their practical application as decision-making or assistive modules in real-world autonomous vehicles."
        },
        {
            "title": "4.2.5 Video Generation as World Model in Gaming",
            "content": "Among the three primary application domains, even within general scene world modeling, the gaming domain has witnessed the fastest and most mature development of world models. This is largely driven by the inherent demand for real-time interaction in games, which has led most existing models in this area to support realtime feedback and responsiveness. In particular, open-world video generation in games closely aligns with our vision of thirdand even fourth-generation world models: constructing powerful representation model embedded with world knowledge, and then leveraging video renderer to observe the world from various perspectives and initial conditions. In this setting, the open-world game environment effectively functions as an observable virtual world, one that is both dynamic and interactive in real time. Table 4 shows the details of representative works in gaming with their navigation modes and control level. However, despite this progress, current game-oriented world models, such as those discussed in [88], still face significant limitations in planning capabilities, viewpoint generalization, physical consistency, and semantic world representation. Therefore, we regard them as advanced second-generation world models, with ample room for further advancement. Controller Navigation World Model. In the gaming domain, key control represents crucial navigation mode for world models. This includes both inputs from game controllers [256261] and keyboard-based [262 267, 269, 270] interactions. For instance, models [258260] rely on controller inputs for gameplay interaction, while GameGen-X [262] utilizes keyboard keystrokes to achieve video-level control. Other models [264267, 270] adopt combination of keyboard and mouse inputs to enable frame-level control over game environments. Naturally, the complexity and granularity of supported actions vary across these models. Notably, Googles Genie series [261, 269] marks significant advancement in this area. Genie [261] is the first generative interactive environment trained in an unsupervised manner using unlabelled Internet videos. 18 It can synthesize videos frame by frame from diverse inputs, including text-to-image prompts, hand-drawn sketches, text descriptions, or even real-world photos, all of which can be modulated via real-time action inputs. Its successor, Genie 2 [269], serves as large-scale foundation world model. It demonstrates emergent capabilities at scale, such as object manipulation, complex character animation, physical reasoning, and even the prediction of other agents behavior. The Genie series is thus progressively evolving toward the vision of an interactive open-world simulation model that underpins next-generation world modeling."
        },
        {
            "title": "Systems",
            "content": "In the third generation of world models, planning emerges as the core capability that begins to bridge the gap between environment-centered physical simulation and agent-centered reasoning. This stage encompasses both objective physical planning, such as navigation and scene evolution based on global environmental dynamics, and the potential of subjective or agentic planning, where the model internally simulates trajectories, intentions, or counterfactual futures from first-person perspective. In this sense, planning represents the first step toward integrating mental world model component within primarily physical framework, enabling the model to reason not only about how the world evolves, but also about why such evolutions may occur under different beliefs or goals. From theoretical standpoint, this generation naturally aligns with the MDP or POMDP duality: during large-scale training, models approximate fully observable physical world and learn an objective latent prior; during inference, they operate under partial observations, performing subjective reasoning over possible futures conditioned on actions, prompts, or multimodal inputs. Thus, Generation 3 represents the transition point where objective world knowledge and subjective inference begin to co-exist within unified planning process. Only limited number of methods currently reach this level of capability. The boundary between Generation 2 and Generation 3 is marked by the emergence of real-time inference ( 24 FPS) and navigation-aware generation, where the system can adaptively simulate world trajectories based on external or internal control signals. Models that demonstrate temporally consistent control and near-real-time rendering are regarded as exhibiting emergent potential toward this generation, even if fully consistent physics and complex multi-step planning have not yet been achieved. Representative examples include NWM [97], which enables robots to imagine trajectories in unknown environments from single input image, effectively performing self-navigated, belief-driven video prediction where the input image defines the initial world state. Similarly, Metas V-JEPA 2 [28] predicts the evolution of physical scenes and supports zero-shot robot planning in unseen environments, demonstrating the integration of predictive physical knowledge with adaptive task reasoning. Genie 3 [147] further exemplifies this generations capabilities, achieving real-time (24 FPS, 720 p) interaction with minute-scale visual memory, allowing for consistent scene dynamics even as objects move out of view. It also supports promptable world events, enabling users or agents to modify environmental conditions such as weather or layout mid-simulation, which shows an early form of interactive agentenvironment co-evolution. Despite these advances, current models remain limited in scalability, action richness, and long-duration temporal consistency. Nevertheless, planning in Generation 3 signifies the critical turning point where world models evolve from purely physical simulators toward agentic and mental world modeling, unifying the objective and subjective dimensions of intelligent simulation."
        },
        {
            "title": "4.4 Generation 4 - Stochasticity: Modeling Outlier and Low-Probability Events",
            "content": "While the third generation of world models primarily focuses on faithfully simulating regular and rulegoverned aspects of the physical world, they tend to favor high-probability trajectories or events during planning. However, the real world is inherently stochastic, characterized not only by deterministic rules but also by the occurrence of rare, unpredictable, and sometimes disruptive events. The fourth generation of world models aims to incorporate this essential dimension of realism by enabling the modeling and simulation of low-probability, outlier events. These rare events, though statistically infrequent, often play disproportionately significant role in the evolution of complex systems. Examples include genetic mutations leading to pathological outcomes such as cancer, or sudden accidents and coincidences in everyday life, such as traffic collisions. mature world model should thus strike principled balance between simulating the most likely futures and accounting for the possibility of unexpected deviations. This necessitates more probabilistic, uncertainty-aware formulation of planning, one that acknowledges the diversity of real-world dynamics beyond the average case. By embedding stochasticity into world modeling, Generation 4 moves closer to emulating the richness and unpredictability of the physical world, laying the foundation for agents capable of robust decision-making under uncertainty. 19 Beyond visual realism, future physical world models should incorporate multimodal sensory channels (e.g.. audio) to more faithfully emulate the perceptual diversity of real-world environments. Audio signals convey fine-grained prompts such as collisions, weather dynamics, or human and animal behaviors, which are often crucial for robust situational understanding. Integrating synchronized auditory modeling into world simulations can thus significantly enhance both the realism and the cognitive richness of the environment, providing agents with additional modalities to reason, predict, and act intelligently under uncertainty. In this sense, the emergence of audiovisual world models [13, 29, 33, 141, 142, 144] will likely coincide with the development of higher-level agent intelligence, as richer sensory feedback enables more adaptive and context-aware interaction within stochastic environments. The fourth generation of world models consists of two progressive stages. In the basic stage, the model is capable of learning the probability distributions of real-world events, including the spontaneous generation of everyday low-probability scenarios such as traffic accidents, rainy or snowy weather, and balanced stochastic outcomes like coin flips, newborn gender ratios, or vehicle turning directions at intersections. Built upon this probabilistic foundation, the model is further able to simulate extremely rare but high-impact events and generate coherent long-term evolutions conditioned on these events. Examples include financial crises, volcanic eruptions, genetic mutations, or asteroid collisions, events that, while statistically negligible, can fundamentally reshape future trajectories. While these efforts largely operate within the realm of human-scale physics, i.e., mesoscopic spatiotemporal modeling, we envision the advanced stage of Generation 4 models to extend planning capabilities toward both macroscopic and microscopic scales. At the macroscopic scale, world model should be able to simulate and plan over long-term horizons, e.g., forecasting the development of system over ten years given an initial state. Importantly, such planning is not expected to occur in real time; rather, the model must compress these ten years into manageable durations (e.g., one hour or even ten minutes), requiring multi-scale temporal modeling. Instead of predicting every single frame or timestep, the model should focus on summarizing salient events. While some existing works begin to address long-horizon forecasting and event-keyframe distillation, progress remains nascent. On the microscopic scale, tasks such as modeling biological phenomena (e.g., involuntary eye microsaccades) remain beyond the reach of current video generation models due to limited temporal resolution and lack of reasoning capabilities for such fine-grained fluctuations. Yet, this level of precision is crucial for improving both video generation quality and physical realism, and for faithfully simulating real-world subtleties. Most critically, this domain is still vastly underexplored in current literature, presenting significant opportunity for future research."
        },
        {
            "title": "4.5 Insight: Beyond Generation",
            "content": "Beyond Generation 4, we envision more advanced form of world models that are capable of simulating everything occurring at every time and everywhere. Starting from given initial state, repeated stochastic rollouts of the model would yield diverse plausible outcomes, resembling the conceptual structure of parallel universes. In this ultimate form, world models could provide downstream agents with virtually unlimited training trajectories and interaction scenarios, compensating for the lack of training data in downstream task domains. In addition, beyond purely visual simulation, the recent emergence of multimodal generation models [11, 29, 33, 141143, 145, 146] with both vision and audio modalities indicates clear trajectory toward unified audiovisual world models, where sound, narration, and perception are jointly simulated with physics-based consistency. Moreover, the current paradigm of world modeling is inherently Earth-centric: both environments and agents are restricted to observations on the Earth, where world is implicitly defined as Earth. However, the physical laws governing environments beyond our planet may differ significantly, most notably in their underlying physics. If future world models can generalize to arbitrary physical laws through fine-tuning or in zero-shot manner, this would unlock an entirely new set of downstream tasks, such as cosmic simulation or autonomous satellite testing. Such capability could profoundly advance human understanding of the broader universe."
        },
        {
            "title": "5.1 Two Complementary Development Directions",
            "content": "Precision Simulators. On the one hand, there is the pursuit of world models as accurate simulators. In this paradigm, the ultimate goal is to maximize fidelity to the real physical world, capturing its dynamics and stochasticity with unparalleled precision. Along this pathway, one might envision the eventual creation of 20 Fig. 8 Overview of Further Applications of World Models. World models promise broad and long-term impact across diverse domains: simulating molecular structures and physical laws in physics and chemistry, generating synthetic training data and serving as virtual testbeds in robotics and autonomous driving, and enabling drug testing and protein structure prediction in biology and medicine. world model that can pass Turing Test for reality: system so accurate that its generated simulations are indistinguishable from actual observations of the physical world. Such models would serve as powerful scientific instruments, enabling researchers to validate hypotheses and test interventions in silico before deploying them in the real world. World Models for Dicision and Control. In parallel with the vision and simulation centric trajectory of world modeling, another line of research has evolved within the reinforcement learning and robotics communities, where the focus lies on using learned world dynamics to support decision-making and control. Rather than striving solely for pixel-level realism, these models emphasize predictive internal representations that enable agents to plan, imagine, and act within latent space. Through learning compact transition dynamics, they allow an embodied system to anticipate outcomes of hypothetical actions, perform longhorizon reasoning, and optimize policies before real-world execution. Notable examples such as PlaNet [274], Dreamer Series [275277], and related latent-dynamics models have demonstrated that predictive simulation and planning can effectively bridge perception and control, providing complementary perspective to videogeneration-based world modeling. Together, these directions illustrate how world modeling can advance along both simulation fidelity and decision-oriented reasoning, converging toward more unified understanding of environment and agency. Generative Engines of World Knowledge. On the other hand, we envision world models as engines of world knowledge and generative creativity. In this paradigm, the focus shifts from merely replicating single reality to mastering world knowledge and enabling zero-shot generation of diverse possible world patterns. Such models, starting from single initial state, could instantiate arbitrary virtual worlds, each governed by its own consistent set of physical or abstract laws. Importantly, multiple inferences from the same initial condition would yield divergent yet plausible outcomes, thereby generating parallel universes. In this sense, world models would empower individuals to create and shape virtual worlds, not merely observing reality but actively creating new ones. These two development directions, precision simulators versus creative generative engines, represent contrasting yet complementary visions. Together, they highlight the profound transformative potential of world models: both as tools for faithfully understanding our universe, and as platforms for exploring the infinite possibilities of imagined ones."
        },
        {
            "title": "5.2 Applications and Societal Impact",
            "content": "Building upon the two directions of future world models realized based on the three core capabilities, direct models are poised to profound and potentially disruptive impact human modes of production and daily life, our ways of perceiving and understanding the world, the intellectual level of machine intelligence, and the methodologies employed by researchers across disciplines such as biology [278280], physics [281283], astronomy [284, 285], medicine [286, 287] and chemistry [288]. Such models have the potential to address many of humanitys challenges. On one hand, they tackle methodological and technological challenges. For example, in robotics [289298], an ideal world model could generate infinite real-world interaction data, resolving debates over whether developing better algorithms or collecting larger-scale data is more critical. In autonomous driving [299, 300], it would allow us to directly simulate endless failure cases, greatly enhancing vehicle safety. On the other hand, world models could address challenges in application domains. For instance, they could predict wildlife [301306] habitats under varying conditions, monitor microbial growth states, and simulate atmospheric changes, thereby forecasting scenarios in which endangered species may face extinction, identifying protective measures that maximize survival and reproduction, predicting human behaviors that exacerbate global warming and extreme weather, and developing strategies to mitigate the intensifying effects of climate change [307]. The applications extend even to physics, where such models could simulate multiple possible scenarios for cosmic formation or asteroid impacts on Earth."
        },
        {
            "title": "5.3 Conclusion and Outlook",
            "content": "In conclusion, the evolution of world models promises to reshape the boundaries of human knowledge, creativity, and problem-solving. By integrating accurate simulation with generative and zero-shot capabilities, these models could serve as both scientific laboratory and virtual sandbox, enabling humanity to explore, understand, and intervene in complex systems at unprecedented scales. The pursuit of these dual capabilities represents one of the most ambitious frontiers in artificial intelligence and offers vision of future in which humans and machines co-create and navigate multiple possible worlds."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Jiaming Song for the discussions and valuable feedback."
        },
        {
            "title": "References",
            "content": "[1] AI, L.: Genie. https://lumalabs.ai/genie?view=create (2025) [2] Liu, Z., Ye, W., Luximon, Y., Wan, P., Zhang, D.: Unleashing the potential of multi-modal foundation models and video diffusion for 4d dynamic physical scene simulation. In: CVPR (2025) [3] Huang, S., Sun, S., Wang, Z., Qin, X., Xiong, Y., Zhang, Y., Wan, P., Zhang, D., Jia, J.: Placiddreamer: Advancing harmony in text-to-3d generation. In: ACMMM (2024) [4] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV (2023) [5] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023) [6] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.: Scaling rectified flow transformers for high-resolution image synthesis. In: ICML (2024) [7] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020) [8] Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.: Image transformer. In: ICML (2018) [9] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: ICML (2021) [10] Razavi, A., Oord, A., Vinyals, O.: Generating diverse high-fidelity images with vq-vae-2. In: NeurIPS (2019) [11] Brooks, T., Peebles, B., Homes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C.W.Y., Wang, R., Ramesh, A.: Video generation models as world simulators. https: //openai.com/index/video-generation-models-as-world-simulators/. Accessed: 2024-06-13 (2024) [12] Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al.: Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211 (2025) [13] Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al.: Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025) [14] Peng, X., Zheng, Z., Shen, C., Young, T., Guo, X., Wang, B., Xu, H., Liu, H., Jiang, M., Li, W., et al.: Open-sora 2.0: Training commercial-level video generation model in 200 k. arXiv preprint arXiv:2503.09642 (2025) [15] Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al.: Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024) [16] Xu, J., Zou, X., Huang, K., Chen, Y., Liu, B., Cheng, M., Shi, X., Huang, J.: Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991 (2024) [17] Bao, F., Xiang, C., Yue, G., He, G., Zhu, H., Zheng, K., Zhao, M., Liu, S., Wang, Y., Zhu, J.: Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233 (2024) [18] Yin, T., Zhang, Q., Zhang, R., Freeman, W.T., Durand, F., Shechtman, E., Huang, X.: From slow bidirectional to fast autoregressive video diffusion models. In: CVPR (2025) [19] Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al.: Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720 (2024) 23 [20] Huang, X., Li, Z., He, G., Zhou, M., Shechtman, E.: Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009 (2025) [21] KlingAI: Kling. https://app.klingai.com/cn/. Accessed: 2025-06-13 (2024) [22] Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al.: Cogvideox: Text-to-video diffusion models with an expert transformer. In: ICLR (2025) [23] Lin, S., Yang, C., He, H., Jiang, J., Ren, Y., Xia, X., Zhao, Y., Xiao, X., Jiang, L.: Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350 (2025) [24] Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al.: Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575 (2025) [25] Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., et al.: Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074 (2025) [26] Gao, Y., Guo, H., Hoang, T., Huang, W., Jiang, L., Kong, F., Li, H., Li, J., Li, L., Li, X., et al.: Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113 (2025) [27] AI, D.: MirageLSD: Zero-Latency, Real-Time, Infinite Video Generation. https://mirage.decart.ai/. Technical Report (2025) [28] Assran, M., Bardes, A., Fan, D., Garrido, Q., Howes, R., Komeili, M., Muckley, M., Rizvi, A., Roberts, C., Sinha, K., Zholus, A., Arnaud, S., Gejji, A., Martin, A., Robert Hogan, F., Dugas, D., Bojanowski, P., Khalidov, V., Labatut, P., Massa, F., Szafraniec, M., Krishnakumar, K., Li, Y., Ma, X., Chandar, S., Meier, F., LeCun, Y., Rabbat, M., Ballas, N.: V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985 (2025) [29] labs, P.: Pika. https://pika.art/login. Accessed: 2025-07-02 (2023) [30] ByteDance: Jimeng. https://jimeng-ai.org/. Accessed: 2025-07-02 (2024) [31] Alhaija, H.A., Alvarez, J., Bala, M., Cai, T., Cao, T., Cha, L., Chen, J., Chen, M., Ferroni, F., Fidler, S., et al.: Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492 (2025) [32] Runway: Introducing Runway Gen-4 Our next-generation series of AI models for media generation and world consistency. https://runwayml.com/research/introducing-runway-gen-4. Accessed: 2025-0613 (2025) [33] Deepmind, G.: Veo3. https://deepmind.google/models/veo/. Accessed: 2025-07-02 (2025) [34] StepFun: Step-Video-T2V. https://yuewen.cn/videos. Accessed: 2025-07-02 (2025) [35] AI, L.: Luma. https://lumalabs.ai/dream-machine. Accessed: 2025-07-02 (2024) [36] AI, H.: Hailuo 02. https://hailuoai.video/. Accessed: 2025-07-02 (2025) [37] Fan, W., Si, C., Song, J., Yang, Z., He, Y., Zhuo, L., Huang, Z., Dong, Z., He, J., Pan, D., et al.: Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453 (2025) [38] Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.: Diffit: Diffusion vision transformers for image generation. In: ECCV (2024) [39] Zhang, Q., Song, J., Chen, Y.: Improved order analysis and design of exponential integrator for diffusion models sampling. arXiv preprint arXiv:2308.02157 (2023) [40] Song, J., Zhang, Q., Yin, H., Mardani, M., Liu, M.-Y., Kautz, J., Chen, Y., Vahdat, A.: Loss-guided diffusion models for plug-and-play controllable generation. In: ICML (2023) 24 [41] Mardani, M., Song, J., Kautz, J., Vahdat, A.: variational perspective on solving inverse problems with diffusion models. arXiv preprint arXiv:2305.04391 (2023) [42] Song, J., Vahdat, A., Mardani, M., Kautz, J.: Pseudoinverse-guided diffusion models for inverse problems. In: ICLR (2023) [43] Lim, J.H., Kovachki, N.B., Baptista, R., Beckham, C., Azizzadenesheli, K., Kossaifi, J., Voleti, V., Song, J., Kreis, K., Kautz, J., et al.: Score-based diffusion models in function space. arXiv preprint arXiv:2302.07400 (2023) [44] Tashiro, Y., Song, J., Song, Y., Ermon, S.: Csdi: Conditional score-based diffusion models for probabilistic time series imputation. In: NeurIPS (2021) [45] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021) [46] Zhu, J., Wang, Y., Pan, S., Wan, P., Zhang, D., Huang, G.: A-sdm: Accelerating stable diffusion through model assembly and feature inheritance strategies. IEEE Transactions on Neural Networks and Learning Systems (2025) [47] Zhong, T., Tian, X., Jiang, B., Wang, X., Tao, X., Wan, P., Zhang, Z.: Vfrtok: Variable frame rates video tokenizer with duration-proportional information assumption. arXiv preprint arXiv:2505.12053 (2025) [48] Meng, C., Yu, L., Song, Y., Song, J., Ermon, S.: Autoregressive score matching. In: NeurIPS (2020) [49] Zhao, S., Song, J., Ermon, S.: Infovae: Balancing learning and inference in variational autoencoders. In: AAAI (2019) [50] Zhao, S., Song, J., Ermon, S.: Learning hierarchical features from deep generative models. In: ICML (2017) [51] Dang, M., Singh, A., Zhou, L., Ermon, S., Song, J.: Personalized preference fine-tuning of diffusion models. In: CVPR (2025) [52] Zhang, Q., Song, J., Huang, X., Chen, Y., Liu, M.-Y.: Diffcollage: Parallel generation of large content with diffusion models. In: CVPR (2023) [53] Ye, Y., Li, X., Gupta, A., De Mello, S., Birchfield, S., Song, J., Tulsiani, S., Liu, S.: Affordance diffusion: Synthesizing hand-object interactions. In: CVPR (2023) [54] Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila, T., Laine, S., et al.: ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022) [55] Meng, C., Song, J., Song, Y., Zhao, S., Ermon, S.: Improved autoregressive modeling with distribution smoothing. arXiv preprint arXiv:2103.15089 (2021) [56] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., Ermon, S.: Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 (2021) [57] Zhao, S., Song, J., Ermon, S.: Towards deeper understanding of variational autoencoding models. arXiv preprint arXiv:1702.08658 (2017) [58] Liu, C., Hou, L., Zheng, M., Tao, X., Wan, P., Zhang, D., Gai, K.: Boosting resolution generalization of diffusion transformers with randomized positional encodings. arXiv preprint arXiv:2503.18719 (2025) [59] Shen, G., Wang, L., Lin, J., Ge, W., Zhang, C., Tao, X., Zhang, Y., Wan, P., Wang, Z., Chen, G., et al.: Sg-adapter: Enhancing text-to-image generation with scene graph guidance. arXiv preprint arXiv:2405.15321 (2024) [60] Atzmon, Y., Bala, M., Balaji, Y., Cai, T., Cui, Y., Fan, J., Ge, Y., Gururani, S., Huffman, J., Isaac, R., et al.: Edify image: High-quality image generation with pixel space laplacian diffusion models. arXiv preprint arXiv:2411.07126 (2024) [61] Sinha, A., Song, J., Meng, C., Ermon, S.: D2c: Diffusion-decoding models for few-shot conditional generation. In: NeurIPS (2021) [62] Zhou, L., Ermon, S., Song, J.: Inductive moment matching. arXiv preprint arXiv:2503.07565 (2025) [63] Song, J., Zhou, L.: Ideas in inference-time scaling can benefit generative pre-training algorithms. arXiv preprint arXiv:2503.07154 (2025) [64] Xu, D., Yuan, Y., Mardani, M., Liu, S., Song, J., Wang, Z., Vahdat, A.: Agg: Amortized generative 3d gaussians for single image to 3d. arXiv preprint arXiv:2401.04099 (2024) [65] Ozturkler, B., Liu, C., Eckart, B., Mardani, M., Song, J., Kautz, J.: Smrd: Sure-based robust mri reconstruction with diffusion models. In: MICCAI (2023) [66] He, H., Liang, J., Wang, X., Wan, P., Zhang, D., Gai, K., Pan, L.: Scaling image and video generation via test-time evolutionary search. arXiv preprint arXiv:2505.17618 (2025) [67] Wu, J., Hou, L., Yang, H., Tao, X., Tian, Y., Wan, P., Zhang, D., Tong, Y.: Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858 (2025) [68] Zhang, Y., Xing, J., Xia, B., Liu, S., Peng, B., Tao, X., Wan, P., Lo, E., Jia, J.: Training-free efficient video generation via dynamic token carving. arXiv preprint arXiv:2505.16864 (2025) [69] Shi, M., Yuan, Z., Yang, H., Wang, X., Zheng, M., Tao, X., Zhao, W., Zheng, W., Zhou, J., Lu, J., et al.: Diffmoe: Dynamic token selection for scalable diffusion transformers. arXiv preprint arXiv:2503.14487 (2025) [70] Yang, Z., Shen, G., Li, M., Hou, L., Liu, M., Wang, L., Tao, X., Wan, P., Zhang, D., Chen, Y.- C.: Efficient training-free high-resolution synthesis with energy rectification in diffusion models. arXiv preprint arXiv:2503.02537 (2025) [71] Yin, Y., Zhao, Y., Zheng, M., Lin, K., Ou, J., Chen, R., Huang, V.S.-J., Wang, J., Tao, X., Wan, P., et al.: Towards precise scaling laws for video diffusion transformers. In: CVPR (2025) [72] Han, D., Ye, T., Han, Y., Xia, Z., Pan, S., Wan, P., Song, S., Huang, G.: Agent attention: On the integration of softmax and linear attention. In: ECCV (2024) [73] Zhang, Y., Dong, W., Tang, F., Huang, N., Huang, H., Ma, C., Wan, P., Lee, T.-Y., Xu, C.: Motioncrafter: Plug-and-play motion guidance for diffusion models. IEEE Transactions on Visualization and Computer Graphics (2025) [74] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: ICCV (2023) [75] Liu, J., Liu, G., Liang, J., Yuan, Z., Liu, X., Zheng, M., Wu, X., Wang, Q., Qin, W., Xia, M., et al.: Improving video generation with human feedback. arXiv preprint arXiv:2501.13918 (2025) [76] Wang, Y., Liu, X., Pang, W., Ma, L., Yuan, S., Debevec, P., Yu, N.: Survey of video diffusion models: Foundations, implementations, and applications. arXiv preprint arXiv:2504.16081 (2025) [77] Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z., Jiang, Y.-G.: survey on video diffusion models. ACM Computing Surveys (2024) [78] Yu, J., Qin, Y., Che, H., Liu, Q., Wang, X., Wan, P., Zhang, D., Gai, K., Chen, H., Liu, X.: survey of interactive generative video. arXiv preprint arXiv:2504.21853 (2025) [79] DalCol, L., Oliveira, M., Santos, V.: Joint perception and prediction for autonomous driving: survey. arXiv preprint arXiv:2412.14088 (2024) [80] Zhu, Z., Wang, X., Zhao, W., Min, C., Deng, N., Dou, M., Wang, Y., Shi, B., Wang, K., Zhang, C., et al.: Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520 (2024) [81] Fu, A., Zhou, Y., Zhou, T., Yang, Y., Gao, B., Li, Q., Wu, G., Shao, L.: Exploring the interplay between 26 video generation and world models in autonomous driving: survey. arXiv preprint arXiv:2411.02914 (2024) [82] Melnik, A., Ljubljanac, M., Lu, C., Yan, Q., Ren, W., Ritter, H.: Video diffusion models: survey. arXiv preprint arXiv:2405.03150 (2024) [83] Lin, M., Wang, X., Wang, Y., Wang, S., Dai, F., Ding, P., Wang, C., Zuo, Z., Sang, N., Huang, S., et al.: Exploring the evolution of physics cognition in video generation: survey. arXiv preprint arXiv:2503.21765 (2025) [84] Guan, Y., Liao, H., Li, Z., Hu, J., Yuan, R., Zhang, G., Xu, C.: World models for autonomous driving: An initial survey. IEEE Transactions on Intelligent Vehicles (2024) [85] Liu, D., Zhang, J., Dinh, A.-D., Park, E., Zhang, S., Mian, A., Shah, M., Xu, C.: Generative physical ai in vision: survey. arXiv preprint arXiv:2501.10928 (2025) [86] Cho, J., Puspitasari, F.D., Zheng, S., Zheng, J., Lee, L.-H., Kim, T.-H., Hong, C.S., Zhang, C.: Sora as an agi world model? complete survey on text-to-video generation. arXiv preprint arXiv:2403.05131 (2024) [87] Sun, R., Zhang, Y., Shah, T., Sun, J., Zhang, S., Li, W., Duan, H., Wei, B., Ranjan, R.: From sora what we can see: survey of text-to-video generation. arXiv preprint arXiv:2405.10674 (2024) [88] Yu, J., Qin, Y., Che, H., Liu, Q., Wang, X., Wan, P., Zhang, D., Liu, X.: Position: Interactive generative video as next-generation game engine. arXiv preprint arXiv:2503.17359 (2025) [89] Yann, L.: Yann LeCun on vision to make AI systems learn and reason like animals and humans (2022). https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/ [90] Ha, D., Schmidhuber, J.: World models. arXiv preprint arXiv:1803.10122 (2018) [91] Huang, X.: Towards Video World Models (2025). https://www.xunhuang.me/blogs/world model.html [92] Kessler, S., Ostaszewski, M., Bortkiewicz, M., Zarski, M., Wolczyk, M., Parker-Holder, J., Roberts, S.J., Mi, P., et al.: The effectiveness of world models for continual reinforcement learning. In: CoLLA (2023) [93] Xu, Y., Parker-Holder, J., Pacchiano, A., Ball, P., Rybkin, O., Roberts, S., Rocktaschel, T., Grefenstette, E.: Learning general world models in handful of reward-free deployments. In: NeurIPS (2022) [94] Lu, C., Ball, P.J., Rudner, T.G., Parker-Holder, J., Osborne, M.A., Teh, Y.W.: Challenges and opportunities in offline reinforcement learning from visual observations. arXiv preprint arXiv:2206.04779 (2022) [95] Ball, P.J., Lu, C., Parker-Holder, J., Roberts, S.: Augmented world models facilitate zero-shot dynamics generalization from single offline environment. In: ICML (2021) [96] Dai, T., Wong, J., Jiang, Y., Wang, C., Gokmen, C., Zhang, R., Wu, J., Fei-Fei, L.: Automated creation of digital cousins for robust policy learning. arXiv preprint arXiv:2410.07408 (2024) [97] Bar, A., Zhou, G., Tran, D., Darrell, T., LeCun, Y.: Navigation world models. In: CVPR (2025) [98] Team, A., Zhu, H., Wang, Y., Zhou, J., Chang, W., Zhou, Y., Li, Z., Chen, J., Shen, C., Pang, J., et al.: Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945 (2025) [99] HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al.: Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103 (2024) [100] Lin, Z., Liu, W., Chen, C., Lu, J., Hu, W., Fu, T.-J., Allardice, J., Lai, Z., Song, L., Zhang, B., et al.: Stiv: Scalable text and image conditioned video generation. arXiv preprint arXiv:2412.07730 (2024) [101] JV-CV. https://jiutiancv.github.io/JV-CV-T2V/. Accessed: 2025-07-02 (2024) 27 [102] AI, H.: Hailuo AI. https://hailuoai.video/. Accessed: 2025-07-02 (2024) [103] Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., You, Y.: Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404 (2024) [104] Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L., et al.: Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131 (2024) [105] Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Liu, G., Raj, A., et al.: Lumiere: space-time diffusion model for video generation. In: SIGGRAPH (2024) [106] Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., Shan, Y.: Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In: CVPR (2024) [107] Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C., Qiao, Y.: Latte: Latent diffusion transformer for video generation. In: TMLR (2025) [108] Wang, W., Liu, J., Lin, Z., Yan, J., Chen, S., Low, C., Hoang, T., Wu, J., Liew, J.H., Yan, H., et al.: Magicvideo-v2: Multi-stage high-aesthetic video generation. arXiv preprint arXiv:2401.04468 (2024) [109] Ju, X., Gao, Y., Zhang, Z., Yuan, Z., Wang, X., Zeng, A., Xiong, Y., Xu, Q., Shan, Y.: Miradata: large-scale video dataset with long durations and structured captions. In: NeurIPS (2024) [110] Qing, Z., Zhang, S., Wang, J., Wang, X., Wei, Y., Zhang, Y., Gao, C., Sang, N.: Hierarchical spatiotemporal decoupling for text-to-video generation. In: CVPR (2024) [111] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023) [112] Zhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z., Wang, X., Zhao, D., Zhou, J.: I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145 (2023) [113] Chen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J., Wang, Y., Lin, D., Qiao, Y., Liu, Z.: Seine: Short-to-long video diffusion model for generative transition and prediction. In: ICLR (2023) [114] Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., et al.: Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023) [115] Xing, J., Xia, M., Zhang, Y., Chen, H., Yu, W., Liu, H., Liu, G., Wang, X., Shan, Y., Wong, T.-T.: Dynamicrafter: Animating open-domain images with video diffusion priors. In: ECCV (2024) [116] Yu, L., Lezama, J., Gundavarapu, N.B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Birodkar, V., Gupta, A., Gu, X., et al.: Language model beats diffusiontokenizer is key to visual generation. In: ICLR (2024) [117] Zhang, D.J., Wu, J.Z., Liu, J.-W., Zhao, R., Ran, L., Gu, Y., Gao, D., Shou, M.Z.: Show-1: Marrying pixel and latent diffusion models for text-to-video generation. In IJCV (2024) [118] Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al.: Lavie: High-quality video generation with cascaded latent diffusion models. IJCV (2025) [119] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang, S.: Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 (2023) [120] Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023) [121] Runaway: Gen-2. https://runwayml.com/product. Accessed: 2025-07-02 (2023) [122] Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A.G., Yang, M.-H., Hao, 28 Y., Essa, I., et al.: Magvit: Masked generative video transformer. In: CVPR (2023) [123] Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., Feng, J.: Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022) [124] Jonathan, H., William, C., Chitwan, S., Jay, W., Ruiqi, G., Alexey, G., Diederik, P.K., Ben, P., Mohammad, N., David, J.F., Tim, S.: Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022) [125] Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al.: Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022) [126] Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.: Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 (2022) [127] Si, C., Fan, W., Lv, Z., Huang, Z., Qiao, Y., Liu, Z.: Repvideo: Rethinking cross-layer representation for video generation. arXiv preprint arXiv:2501.08994 (2025) [128] Yuan, H., Zhang, S., Wang, X., Wei, Y., Feng, T., Pan, Y., Zhang, Y., Liu, Z., Albanie, S., Ni, D.: Instructvideo: Instructing video diffusion models with human feedback. In: CVPR (2024) [129] Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S.S., Shah, A., Yin, X., Parikh, D., Misra, I.: Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709 (2023) [130] Zhou, Y., Wang, Q., Cai, Y., Yang, H.: Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458 (2024) [131] Sun, W., Chen, S., Liu, F., Chen, Z., Duan, Y., Zhang, J., Wang, Y.: Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928 (2024) [132] Henschel, R., Khachatryan, L., Poghosyan, H., Hayrapetyan, D., Tadevosyan, V., Wang, Z., Navasardyan, S., Shi, H.: Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In: CVPR (2025) [133] Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Schindler, G., Hornung, R., Birodkar, V., Yan, J., Chiu, M.-C., et al.: Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125 (2023) [134] Tian, Y., Yang, L., Yang, H., Gao, Y., Deng, Y., Wang, X., Yu, Z., Tao, X., Wan, P., ZHANG, D., et al.: Videotetris: Towards compositional text-to-video generation. In: NeurIPS (2024) [135] Huang, Y., Zheng, W., Gao, Y., Tao, X., Wan, P., Zhang, D., Zhou, J., Lu, J.: Owl-1: Omni world model for consistent long video generation. arXiv preprint arXiv:2412.09600 (2024) [136] AWS, A.: Nova Reel. https://aws.amazon.com/ai/generative-ai/nova/. Accessed: 2025-07-02 (2025) [137] Ma, G., Huang, H., Yan, K., Chen, L., Duan, N., Yin, S., Wan, C., Ming, R., Song, X., Chen, X., et al.: Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248 (2025) [138] AI, M.: MiracleVision. https://www.miraclevision.com/. Accessed: 2025-07-02 (2025) [139] AIsphere: PixVerse. https://app.pixverse.ai/home (2025) [140] genmo: Mochi-1. https://www.genmo.ai/blog. Accessed: 2025-06-13 (2024) [141] Runway: Introducing gen-3 alpha: new frontier for video generation. https://runwayml.com/research/ introducing-gen-3-alpha. Accessed: 2025-06-13 (2024) [142] AI, X.: Imagine v0.9. https://grok.com/imagine. Accessed: 2025-10-13 (2025) [143] AI, W.: Wan2.2. https://github.com/Wan-Video/Wan2.2. Accessed: 2025-10-13 (2025) 29 [144] OpenAI: Sora 2. https://openai.com/zh-Hans-CN/index/sora-2/. Accessed: 2025-10-13 (2025) [145] AI, L.: Ray3. https://lumalabs.ai/dream-machine. Accessed: 2025-10-13 (2025) [146] Meta: Vibes. https://about.fb.com/news/2025/09/introducing-vibes-ai-videos/. Accessed: 2025-10- (2025) [147] Deepmind, G.: Genie 3. https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/ (2025) [148] Zhang, D.J., Li, D., Le, H., Shou, M.Z., Xiong, C., Sahoo, D.: Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827 (2024) [149] Guo, Y., Yang, C., Rao, A., Agrawala, M., Lin, D., Dai, B.: Sparsectrl: Adding sparse controls to text-to-video diffusion models. In: ECCV (2024) [150] Peng, B., Chen, X., Wang, Y., Lu, C., Qiao, Y.: Conditionvideo: training-free condition-guided video generation. In: AAAI (2024) [151] Zhang, Y., Wei, Y., Jiang, D., Zhang, X., Zuo, W., Tian, Q.: Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077 (2023) [152] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., Zhou, J.: Videocomposer: Compositional video synthesis with motion controllability. In: NeurIPS (2023) [153] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In: ICCV (2023) [154] Liang, H., Yin, Y., Xu, D., Liang, H., Wang, Z., Plataniotis, K.N., Zhao, Y., Wei, Y.: Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. In: NeurIPS (2024) [155] Voleti, V., Yao, C.-H., Boss, M., Letts, A., Pankratz, D., Tochilkin, D., Laforte, C., Rombach, R., Jampani, V.: Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In: ECCV (2024) [156] Chen, Z., Wang, Y., Wang, F., Wang, Z., Liu, H.: V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738 (2024) [157] Zuo, Q., Gu, X., Qiu, L., Dong, Y., Yuan, W., Peng, R., Zhu, S., Bo, L., Dong, Z., Huang, Q., et al.: Videomv: Consistent multi-view generation based on large video generative model (2024) [158] Liu, S., Ren, Z., Gupta, S., Wang, S.: Physgen: Rigid-body physics-grounded image-to-video generation. In: ECCV (2024) [159] Burgert, R., Xu, Y., Xian, W., Pilarski, O., Clausen, P., He, M., Ma, L., Deng, Y., Li, L., Mousavi, M., et al.: Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In: CVPR (2025) [160] Geng, D., Herrmann, C., Hur, J., Cole, F., Zhang, S., Pfaff, T., Lopez-Guevara, T., Aytar, Y., Rubinstein, M., Sun, C., et al.: Motion prompting: Controlling video generation with motion trajectories. In: CVPR (2025) [161] Namekata, K., Bahmani, S., Wu, Z., Kant, Y., Gilitschenski, I., Lindell, D.B.: Sg-i2v: Self-guided trajectory control in image-to-video generation. arXiv preprint arXiv:2411.04989 (2024) [162] Li, Y., Wang, X., Zhang, Z., Wang, Z., Yuan, Z., Xie, L., Shan, Y., Zou, Y.: Image conductor: Precision control for interactive video synthesis. In: AAAI (2025) [163] Ma, W.-D.K., Lewis, J.P., Kleijn, W.B.: Trailblazer: Trajectory control for diffusion-based video generation. In: SIGGRAPH (2024) [164] Wang, Z., Yuan, Z., Wang, X., Li, Y., Chen, T., Xia, M., Luo, P., Shan, Y.: Motionctrl: unified and flexible motion controller for video generation. In: SIGGRAPH (2024) 30 [165] Wu, W., Li, Z., Gu, Y., Zhao, R., He, Y., Zhang, D.J., Shou, M.Z., Li, Y., Gao, T., Zhang, D.: Draganything: Motion control for anything using entity representation. In: ECCV (2024) [166] Jain, Y., Nasery, A., Vineet, V., Behl, H.: Peekaboo: Interactive video generation via masked-diffusion. In: CVPR (2024) [167] Qiu, H., Chen, Z., Wang, Z., He, Y., Xia, M., Liu, Z.: Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863 (2024) [168] Yang, S., Hou, L., Huang, H., Ma, C., Wan, P., Zhang, D., Chen, X., Liao, J.: Direct-a-video: Customized video generation with user-directed camera movement and object motion. In: SIGGRAPH (2024) [169] Ju, X., Ye, W., Liu, Q., Wang, Q., Wang, X., Wan, P., Zhang, D., Gai, K., Xu, Q.: Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907 (2025) [170] Bai, J., Xia, M., Fu, X., Wang, X., Mu, L., Cao, J., Liu, Z., Hu, H., Bai, X., Wan, P., et al.: Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647 (2025) [171] Wang, Q., Luo, Y., Shi, X., Jia, X., Lu, H., Xue, T., Wang, X., Wan, P., Zhang, D., Gai, K.: Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. arXiv preprint arXiv:2502.08639 (2025) [172] Bahmani, S., Skorokhodov, I., Qian, G., Siarohin, A., Menapace, W., Tagliasacchi, A., Lindell, D.B., Tulyakov, S.: Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In: CVPR (2025) [173] Bahmani, S., Skorokhodov, I., Siarohin, A., Menapace, W., Qian, G., Vasilkovsky, M., Lee, H.-Y., Wang, C., Zou, J., Tagliasacchi, A., et al.: Vd3d: Taming large video diffusion transformers for 3d camera control. In: ICLR (2025) [174] He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., Yang, C.: Cameractrl: Enabling camera control for text-to-video generation. In: ICLR (2025) [175] Van Hoorick, B., Wu, R., Ozguroglu, E., Sargent, K., Liu, R., Tokmakov, P., Dave, A., Zheng, C., Vondrick, C.: Generative camera dolly: Extreme monocular dynamic novel view synthesis. In: ECCV (2024) [176] Kuang, Z., Cai, S., He, H., Xu, Y., Li, H., Guibas, L.J., Wetzstein, G.: Collaborative video diffusion: Consistent multi-video generation with camera control. In: NeurIPS (2024) [177] Xu, D., Nie, W., Liu, C., Liu, S., Kautz, J., Wang, Z., Vahdat, A.: Camco: Camera-controllable 3dconsistent image-to-video generation. arXiv preprint arXiv:2406.02509 (2024) [178] Liu, F.-L., Fu, H., Wang, X., Ye, W., Wan, P., Zhang, D., Gao, L.: Sketchvideo: Sketch-based video generation and editing. In: CVPR (2025) [179] Gu, Z., Yan, R., Lu, J., Li, P., Dou, Z., Si, C., Dong, Z., Liu, Q., Lin, C., Liu, Z., et al.: Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847 (2025) [180] Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., Li, H.: Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690 (2025) [181] Li, C., Michel, O., Pan, X., Liu, S., Roberts, M., Xie, S.: Pisa experiments: Exploring physics posttraining for video diffusion models by watching stuff drop. In: ICML (2025) [182] Xue, Q., Yin, X., Yang, B., Gao, W.: Phyt2v: Llm-guided iterative self-refinement for physics-grounded text-to-video generation. In: CVPR (2025) [183] Zhao, Q., Ni, X., Wang, Z., Cheng, F., Yang, Z., Jiang, L., Wang, B.: Synthetic video enhances physical fidelity in video synthesis. arXiv preprint arXiv:2503.20822 (2025) [184] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, 31 A., Boesel, F., et al.: Scaling rectified flow transformers for high-resolution image synthesis. In: ICML (2024) [185] Wang, J., Ma, A., Cao, K., Zheng, J., Zhang, Z., Feng, J., Liu, S., Ma, Y., Cheng, B., Leng, D., et al.: Wisa: World simulator assistant for physics-aware text-to-video generation. arXiv preprint arXiv:2503.08153 (2025) [186] Luo, Y., Bai, J., Shi, X., Xia, M., Wang, X., Wan, P., Zhang, D., Gai, K., Xue, T.: Camclonemaster: Enabling reference-based camera control for video generation. arXiv preprint arXiv:2506.03140 (2025) [187] Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., Liu, X.: Context as memory: Sceneconsistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141 (2025) [188] Fu, X., Liu, X., Wang, X., Peng, S., Xia, M., Shi, X., Yuan, Z., Wan, P., Zhang, D., Lin, D.: 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In: ICLR (2025) [189] He, H., Yang, C., Lin, S., Xu, Y., Wei, M., Gui, L., Zhao, Q., Wetzstein, G., Jiang, L., Li, H.: Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592 (2025) [190] Ren, X., Shen, T., Huang, J., Ling, H., Lu, Y., Nimier-David, M., Muller, T., Keller, A., Fidler, S., Gao, J.: Gen3c: 3d-informed world-consistent video generation with precise camera control. In: CVPR (2025) [191] Hou, C., Wei, G., Zeng, Y., Chen, Z.: Training-free camera control for video generation. In: ICLR (2025) [192] Hong, Y., Liu, B., Wu, M., Zhai, Y., Chang, K.-W., Li, L., Lin, K., Lin, C.-C., Wang, J., Yang, Z., et al.: Slowfast-vgen: Slow-fast learning for action-driven long video generation. In: ICLR (2025) [193] Xiang, J., Liu, G., Gu, Y., Gao, Q., Ning, Y., Zha, Y., Feng, Z., Tao, T., Hao, S., Shi, Y., et al.: Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455 (2024) [194] Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al.: Vbench: Comprehensive benchmark suite for video generative models. In: CVPR (2024) [195] Huang, Z., Zhang, F., Xu, X., He, Y., Yu, J., Dong, Z., Ma, Q., Chanpaisit, N., Si, C., Jiang, Y., et al.: Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503 (2024) [196] Yuan, Y., Song, J., Iqbal, U., Vahdat, A., Kautz, J.: Physdiff: Physics-guided human motion diffusion model. In: ICCV (2023) [197] Wu, J., Yin, S., Feng, N., He, X., Li, D., Hao, J., Long, M.: ivideogpt: Interactive videogpts are scalable world models. In: NeurIPS (2024) [198] Wang, X., Zhu, Z., Huang, G., Chen, X., Zhu, J., Lu, J.: Drivedreamer: Towards real-world-drive world models for autonomous driving. In: ECCV (2024) [199] Wang, Y., He, J., Fan, L., Li, H., Chen, Y., Zhang, Z.: Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In: CVPR (2024) [200] Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall, A., Cipolla, R., Shotton, J.: Model-based imitation learning for urban driving. In: NeurIPS (2022) [201] Zhu, J., Jia, Z., Gao, T., Deng, J., Li, S., Liu, F., Jia, P., Lang, X., Sun, X.: Other vehicle trajectories are also needed: driving world model unifies ego-other vehicle trajectories in video latent space. arXiv preprint arXiv:2503.09215 (2025) [202] Micheli, V., Alonso, E., Fleuret, F.: Transformers are sample-efficient world models. In: ICLR (2023) [203] Guo, X., Zheng, M., Hou, L., Gao, Y., Deng, Y., Wan, P., Zhang, D., Liu, Y., Hu, W., Zha, Z., et al.: I2v-adapter: general image-to-video adapter for diffusion models. In: SIGGRAPH (2024) 32 [204] Wu, S., Ye, W., Wang, J., Liu, Q., Wang, X., Wan, P., Zhang, D., Gai, K., Yan, S., Fei, H., et al.: Any2caption: Interpreting any condition to caption for controllable video generation. arXiv preprint arXiv:2503.24379 (2025) [205] Chen, W., Ji, Y., Wu, J., Wu, H., Xie, P., Li, J., Xia, X., Xiao, X., Lin, L.: Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning. arXiv preprint arXiv:2305.13840 (2023) [206] Park, J., Kwon, T., Ye, J.C.: Zero4d: Training-free 4d video generation from single video using off-theshelf video diffusion. arXiv preprint arXiv:2503.22622 (2025) [207] Yuan, Y.-J., Kobbelt, L., Liu, J., Zhang, Y., Wan, P., Lai, Y.-K., Gao, L.: 4dynamic: Text-to-4d generation with hybrid priors. arXiv preprint arXiv:2407.12684 (2024) [208] Ma, E., Zhou, L., Tang, T., Zhang, Z., Han, D., Jiang, J., Zhan, K., Jia, P., Lang, X., Sun, H., et al.: Unleashing generalization of end-to-end autonomous driving with controllable long video generation. arXiv preprint arXiv:2406.01349 (2024) [209] Wen, Y., Zhao, Y., Liu, Y., Jia, F., Wang, Y., Luo, C., Zhang, C., Wang, T., Sun, X., Zhang, X.: Panacea: Panoramic and controllable video generation for autonomous driving. In: CVPR (2024) [210] Gao, R., Chen, K., Xie, E., Hong, L., Li, Z., Yeung, D.-Y., Xu, Q.: Magicdrive: Street view generation with diverse 3d geometry control. In: ICLR (2024) [211] Bai, J., Xia, M., Wang, X., Yuan, Z., Fu, X., Liu, Z., Hu, H., Wan, P., Zhang, D.: Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760 (2024) [212] Gu, X., Wen, C., Ye, W., Song, J., Gao, Y.: Seer: Language instructed video prediction with latent diffusion models. arXiv preprint arXiv:2303.14897 (2023) [213] Zhou, S., Du, Y., Chen, J., Li, Y., Yeung, D.-Y., Gan, C.: Robodreamer: Learning compositional world models for robot imagination. In: ICML (2024) [214] Wu, W., Guo, X., Tang, W., Huang, T., Wang, C., Chen, D., Ding, C.: Drivescape: Towards highresolution controllable multi-view driving video generation. arXiv preprint arXiv:2409.05463 (2024) [215] Jia, F., Mao, W., Liu, Y., Zhao, Y., Wen, Y., Zhang, C., Zhang, X., Wang, T.: Adriver-i: general world model for autonomous driving. arXiv preprint arXiv:2311.13549 (2023) [216] Yang, J., Gao, S., Qiu, Y., Chen, L., Li, T., Dai, B., Chitta, K., Wu, P., Zeng, J., Luo, P., et al.: Generalized predictive model for autonomous driving. In: CVPR (2024) [217] Green, M.C., Mugrai, L., Khalifa, A., Togelius, J.: Mario level generation from mechanics using scene stitching. In: IEEE CoG (2020) [218] Menapace, W., Lathuiliere, S., Tulyakov, S., Siarohin, A., Ricci, E.: Playable video generation. In: CVPR (2021) [219] Chen, B., Mart Monso, D., Du, Y., Simchowitz, M., Tedrake, R., Sitzmann, V.: Diffusion forcing: Next-token prediction meets full-sequence diffusion. In: NeurIPS (2024) [220] Zhu, F., Wu, H., Guo, S., Liu, Y., Cheang, C., Kong, T.: Irasim: Learning interactive real-robot action simulators. arXiv preprint arXiv:2406.14540 (2024) [221] Ko, P.-C., Mao, J., Du, Y., Sun, S.-H., Tenenbaum, J.B.: Learning to act from actionless videos through dense correspondences. arXiv preprint arXiv:2310.08576 (2023) [222] Menapace, W., Lathuili`ere, S., Siarohin, A., Theobalt, C., Tulyakov, S., Golyanik, V., Ricci, E.: Playable environments: Video manipulation in space and time. In: CVPR (2022) [223] Guo, Y., Hu, Y., Zhang, J., Wang, Y.-J., Chen, X., Lu, C., Chen, J.: Prediction with action: Visual policy learning via joint denoising process. In: NeurIPS (2024) 33 [224] Quevedo, J., Liang, P., Yang, S.: Evaluating robot policies in world model. arXiv preprint arXiv:2506.00613 (2025) [225] Li, S., Gao, Y., Sadigh, D., Song, S.: Unified video action model. arXiv preprint arXiv:2503.00200 (2025) [226] He, H., Zhang, Y., Lin, L., Xu, Z., Pan, L.: Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825 (2025) [227] Wang, L., Zhao, K., Liu, C., Chen, X.: Learning real-world action-video dynamics with heterogeneous masked autoregression. arXiv preprint arXiv:2502.04296 (2025) [228] Liao, Y., Zhou, P., Huang, S., Yang, D., Chen, S., Jiang, Y., Hu, Y., Cai, J., Liu, S., Luo, J., et al.: Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635 (2025) [229] Chi, X., Fan, C.-K., Zhang, H., Qi, X., Zhang, R., Chen, A., Chan, C.-m., Xue, W., Liu, Q., Zhang, S., et al.: Eva: An embodied world model for future video anticipation. arXiv preprint arXiv:2410.15461 (2024) [230] Zhang, H., Wang, Z., Lyu, Q., Zhang, Z., Chen, S., Shu, T., Dariush, B., Lee, K., Du, Y., Gan, C.: Combo: compositional world models for embodied multi-agent cooperation. In: ICLR (2025) [231] Fan, Y., Ma, X., Wu, R., Du, Y., Li, J., Gao, Z., Li, Q.: Videoagent: memory-augmented multimodal agent for video understanding. In: ECCV (2024) [232] Wen, Y., Lin, J., Zhu, Y., Han, J., Xu, H., Zhao, S., Liang, X.: Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. In: NeurIPS (2024) [233] Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D., Abbeel, P.: Learning universal policies via text-guided video generation. In: NeurIPS (2024) [234] Shridhar, M., Manuelli, L., Fox, D.: Cliport: What and where pathways for robotic manipulation. In: PMLR (2022) [235] He, H., Bai, C., Pan, L., Zhang, W., Zhao, B., Li, X.: Learning an actionable discrete diffusion policy via large-scale actionless video pre-training. In: NeurIPS (2024) [236] Luo, Y., Du, Y.: Grounding video models to actions through goal conditioned exploration. In: ICLR (2025) [237] Wu, H., Jing, Y., Cheang, C., Chen, G., Xu, J., Li, X., Liu, M., Li, H., Kong, T.: Unleashing large-scale video generative pre-training for visual robot manipulation. In: ICLR (2024) [238] Fu, X., Wang, X., Liu, X., Bai, J., Xu, R., Wan, P., Zhang, D., Lin, D.: Learning video generation for robotic manipulation with collaborative trajectory control. arXiv preprint arXiv:2506.01943 (2025) [239] Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuurmans, D., Abbeel, P.: Learning interactive real-world simulators. In: ICLR (2024) [240] Qin, Y., Sun, A., Hong, Y., Wang, B., Zhang, R.: Navigatediff: Visual predictors are zero-shot navigation assistants. In: ICRA (2025) [241] Li, B., Guo, J., Liu, H., Zou, Y., Ding, Y., Chen, X., Zhu, H., Tan, F., Zhang, C., Wang, T., et al.: Uniscene: Unified occupancy-centric driving scene generation. In: CVPR (2025) [242] Mei, J., Hu, T., Yang, X., Wen, L., Yang, Y., Wei, T., Ma, Y., Dou, M., Shi, B., Liu, Y.: Dreamforge: Motion-aware autoregressive video generation for multi-view driving scenes. arXiv preprint arXiv:2409.04003 (2024) [243] Jiang, J., Hong, G., Zhou, L., Ma, E., Hu, H., Zhou, X., Xiang, J., Liu, F., Yu, K., Sun, H., et al.: Dive: Dit-based video generation with enhanced control. arXiv preprint arXiv:2409.01595 (2024) [244] Li, X., Zhang, Y., Ye, X.: Drivingdiffusion: Layout-guided multi-view driving scenarios video generation 34 with latent diffusion model. In: ECCV (2024) [245] Chen, Y., Wang, Y., Zhang, Z.: Drivinggpt: Unifying driving world modeling and planning with multimodal autoregressive transformers. arXiv preprint arXiv:2412.18607 (2024) [246] Guo, X., Ding, C., Dou, H., Zhang, X., Tang, W., Wu, W.: Infinitydrive: Breaking time limits in driving world models. arXiv preprint arXiv:2412.01522 (2024) [247] Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., Corrado, G.: Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080 (2023) [248] Zhang, K., Tang, Z., Hu, X., Pan, X., Guo, X., Liu, Y., Huang, J., Yuan, L., Zhang, Q., Long, X.-X., et al.: Epona: Autoregressive diffusion world model for autonomous driving. In: ICCV (2025) [249] Hassan, M., Stapf, S., Rahimi, A., Rezende, P., Haghighi, Y., Bruggemann, D., Katircioglu, I., Zhang, L., Chen, X., Saha, S., et al.: Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. In: CVPR (2025) [250] Hu, X., Yin, W., Jia, M., Deng, J., Guo, X., Zhang, Q., Long, X., Tan, P.: Drivingworld: Constructingworld model for autonomous driving via video gpt. arXiv preprint arXiv:2412.19505 (2024) [251] Gao, R., Chen, K., Xiao, B., Hong, L., Li, Z., Xu, Q.: Magicdrivedit: High-resolution long video generation for autonomous driving with adaptive control. arXiv preprint arXiv:2411.13807 (2024) [252] Zhao, G., Wang, X., Zhu, Z., Chen, X., Huang, G., Bao, X., Wang, X.: Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In: AAAI (2025) [253] Russell, L., Hu, A., Bertoni, L., Fedoseev, G., Shotton, J., Arani, E., Corrado, G.: Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523 (2025) [254] Ni, J., Guo, Y., Liu, Y., Chen, R., Lu, L., Wu, Z.: Maskgwm: generalizable driving world model with video mask reconstruction. In: CVPR (2025) [255] Gao, S., Yang, J., Chen, L., Chitta, K., Qiu, Y., Geiger, A., Zhang, J., Li, H.: Vista: generalizable driving world model with high fidelity and versatile controllability. In: NeurIPS (2024) [256] Li, J., Tang, J., Xu, Z., Wu, L., Zhou, Y., Shao, S., Yu, T., Cao, Z., Lu, Q.: Hunyuan-gamecraft: High-dynamic interactive game video generation with hybrid history condition. arXiv preprint arXiv:2506.17201 (2025) [257] Yang, M., Li, J., Fang, Z., Chen, S., Yu, Y., Fu, Q., Yang, W., Ye, D.: Playable game generation. arXiv preprint arXiv:2412.00887 (2024) [258] Chen, J., Zhao, Y., Huang, Y., Cui, L., Dong, L., Lv, T., Chen, Q., Wei, F.: Model as game: On numerical and spatial consistency for generative games. arXiv preprint arXiv:2503.21172 (2025) [259] Valevski, D., Leviathan, Y., Arar, M., Fruchter, S.: Diffusion models are real-time game engines. In: ICLR (2025) [260] Kanervisto, A., Bignell, D., Wen, L.Y., Grayson, M., Georgescu, R., Valcarcel Macua, S., Tan, S.Z., Rashid, T., Pearce, T., Cao, Y., et al.: World and human action models towards gameplay ideation. Nature (2025) [261] Bruce, J., Dennis, M.D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al.: Genie: Generative interactive environments. In: ICML (2024) [262] Che, H., He, X., Liu, Q., Jin, C., Chen, H.: Gamegen-x: Interactive open-world game video generation. In: ICLR (2025) [263] Feng, R., Zhang, H., Yang, Z., Xiao, J., Shu, Z., Liu, Z., Zheng, A., Huang, Y., Liu, Y., Zhang, H.: The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568 (2024) [264] Guo, J., Ye, Y., He, T., Wu, H., Jiang, Y., Pearce, T., Bian, J.: Mineworld: real-time and open-source 35 interactive world model on minecraft. arXiv preprint arXiv:2504.08388 (2025) [265] Yu, J., Qin, Y., Wang, X., Wan, P., Zhang, D., Liu, X.: Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325 (2025) [266] Xiao, Z., Lan, Y., Zhou, Y., Ouyang, W., Yang, S., Zeng, Y., Pan, X.: Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369 (2025) [267] Alonso, E., Jelley, A., Micheli, V., Kanervisto, A., Storkey, A.J., Pearce, T., Fleuret, F.: Diffusion for world modeling: Visual details matter in atari. In: NeurIPS (2024) [268] Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusion-based generative models. In: NeurIPS (2022) [269] Deepmind, G.: Parker-Holder, and Ball, and Bruce, and Dasagi, and Holsheimer, and Kaplanis, and Moufarek, and Scully, and Shar, and Shi, and others. https://deepmind. google/discover/blog/genie-2-a-large-scale-foundation-world-model. Accessed: 2025-06-13 (2024) [270] Decart, E., McIntyre, Q., Campbell, S., Chen, X., Wachen, R.: Oasis: universe in transformer. https://oasis-model.github.io. Accessed: 2025-06-13 (2024) [271] Liang, J., Liu, R., Ozguroglu, E., Sudhakar, S., Dave, A., Tokmakov, P., Song, S., Vondrick, C.: Dreamitate: Real-world visuomotor policy learning via video generation. In: CoRL (2024) [272] Ren, X., Lu, Y., Cao, T., Gao, R., Huang, S., Sabour, A., Shen, T., Pfaff, T., Wu, J.Z., Chen, R., et al.: Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042 (2025) [273] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: ICLR (2022) [274] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., Davidson, J.: Learning latent dynamics for planning from pixels. In: ICML (2019) [275] Hafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors by latent imagination. In: ICLR (2020) [276] Hafner, D., Lillicrap, T., Norouzi, M., Ba, J.: Mastering atari with discrete world models. In: ICLR (2021) [277] Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.: Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104 (2023) [278] Han, C., Lin, S., Wang, Z., Cui, Y., Zou, Q., Yuan, Z.: Reusability report: Exploring the transferability of self-supervised learning models from single-cell to spatial transcriptomics. Nature Machine Intelligence (2025) [279] Duan, H., Skreta, M., Cotta, L., Rajaonson, E.M., Dhawan, N., Aspuru-Guzik, A., Maddison, C.J.: Boosting the predictive power of protein representations with corpus of text annotations. Nature Machine Intelligence (2025) [280] Zhao, H., Zhang, O., Jiang, D., Wu, Z., Du, H., Wang, X., Zhao, Y., Huang, Y., Ge, J., Hou, T., et al.: Proteinpeptide docking with rational and accurate diffusion generative model. Nature Machine Intelligence (2025) [281] Li, M., Song, K., Zhao, M., You, G., Zhong, J., Zhao, M., Li, A., Chen, Y., Li, G., Kong, Y., et al.: Electron-density informed effective and reliable de novo molecular design and lead optimization with ed2mol. Nature Machine Intelligence (2025) [282] Maurizi, M., Xu, D., Wang, Y.-T., Yao, D., Hahn, D., Oudich, M., Satpati, A., Bauchy, M., Wang, W., Sun, Y., et al.: Designing metamaterials with programmable nonlinear responses and geometric constraints in graph space. Nature Machine Intelligence (2025) [283] Nazari, K., Mandil, W., Santello, M., Park, S., Ghalamzan-E, A.: Bioinspired trajectory modulation 36 for effective slip control in robot manipulation. Nature Machine Intelligence (2025) [284] Sanders, L.M., Scott, R.T., Yang, J.H., Qutub, A.A., Garcia Martin, H., Berrios, D.C., Hastings, J.J., Rask, J., Mackintosh, G., Hoarfrost, A.L., et al.: Biological research and self-driving labs in deep space supported by artificial intelligence. Nature Machine Intelligence (2023) [285] Scott, R.T., Sanders, L.M., Antonsen, E.L., Hastings, J.J., Park, S.-m., Mackintosh, G., Reynolds, R.J., Hoarfrost, A.L., Sawyer, A., Greene, C.S., et al.: Biomonitoring and precision health in deep space supported by artificial intelligence. Nature Machine Intelligence (2023) [286] Andani, S., Chen, B., Ficek-Pascual, J., Heinke, S., Casanova, R., Hild, B.F., Sobottka, B., Bodenmiller, B., Koelzer, V.H., et al.: Histopathology-based protein multiplex generation using deep learning. Nature Machine Intelligence (2025) [287] Ing, A., Andrades, A., Cosenza, M.R., Korbel, J.O.: Integrating multimodal cancer data using deep latent variable path modelling. Nature Machine Intelligence (2025) [288] Li, L., Zhang, Y., Wang, G., Xia, K.: Kolmogorovarnold graph neural networks for molecular property prediction. Nature Machine Intelligence (2025) [289] Jung, Y., Kwon, K., Lee, J., Ko, S.H.: Untethered soft actuators for soft standalone robotics. Nature Communications (2024) [290] Marcus, H.J., Ramirez, P.T., Khan, D.Z., Layard Horsfall, H., Hanrahan, J.G., Williams, S.C., Beard, D.J., Bhat, R., Catchpole, K., Cook, A., et al.: The ideal framework for surgical robotics: development, comparative evaluation and long-term monitoring. Nature medicine (2024) [291] Seong, M., Sun, K., Kim, S., Kwon, H., Lee, S.-W., Veerla, S.C., Kang, D.K., Kim, J., Kondaveeti, S., Tawfik, S.M., et al.: Multifunctional magnetic muscles for soft robotics. Nature Communications (2024) [292] Dai, T., Vijayakrishnan, S., Szczypinski, F.T., Ayme, J.-F., Simaei, E., Fellowes, T., Clowes, R., Kotopanov, L., Shields, C.E., Zhou, Z., et al.: Autonomous mobile robots for exploratory synthetic chemistry. Nature (2024) [293] Feng, W., Sun, L., Jin, Z., Chen, L., Liu, Y., Xu, H., Wang, C.: large-strain and ultrahigh energy density dielectric elastomer for fast moving soft robot. Nature Communications (2024) [294] Mao, L., Yang, P., Tian, C., Shen, X., Wang, F., Zhang, H., Meng, X., Xie, H.: Magnetic steering continuum robot for transluminal procedures with programmable shape and functionalities. Nature communications (2024) [295] Huang, S.-C., Zhu, Y.-J., Huang, X.-Y., Xia, X.-X., Qian, Z.-G.: Programmable adhesion and morphing of protein hydrogels for underwater robots. Nature Communications (2024) [296] Mao, Q., Liao, Z., Yuan, J., Zhu, R.: Multimodal tactile sensing fused with vision for dexterous robotic housekeeping. Nature Communications (2024) [297] Liu, C., Liu, Y., Xie, R., Li, Z., Bai, S., Zhao, Y.: The evolution of robotics: research and application progress of dental implant robotic systems. International Journal of Oral Science (2024) [298] Xia, H., Zhang, Y., Rajabi, N., Taleb, F., Yang, Q., Kragic, D., Li, Z.: Shaping high-performance wearable robots for human motor and sensory reconstruction and enhancement. Nature Communications (2024) [299] Abdel-Aty, M., Ding, S.: matched case-control analysis of autonomous vs human-driven vehicle accidents. Nature communications (2024) [300] Liu, H.X., Feng, S.: Curse of rarity for autonomous vehicles. Nature communications (2024) [301] Lee, S.X.T., Amir, Z., Moore, J.H., Gaynor, K.M., Luskin, M.S.: Effects of human disturbances on wildlife behaviour and consequences for predator-prey overlap in southeast asia. Nature Communications (2024) [302] Goldberg, A.R., Langwig, K.E., Brown, K.L., Marano, J.M., Rai, P., King, K.M., Sharp, A.K., Ceci, A., Kailing, C.D., Kailing, M.J., et al.: Widespread exposure to sars-cov-2 in wildlife communities. Nature Communications (2024) [303] Buysse, M., Koual, R., Binetruy, F., Thoisy, B., Baudrimont, X., Garnier, S., Douine, M., Chevillon, C., Delsuc, F., Catzeflis, F., et al.: Detection of anaplasma and ehrlichia bacteria in humans, wildlife, and ticks in the amazon rainforest. Nature Communications (2024) [304] Watt, A.E., Cummins, M.L., Donato, C.M., Wirth, W., Porter, A.F., Andersson, P., Donner, E., Jennison, A.V., Seemann, T., et al.: Parameters for one health genomic surveillance of escherichia coli from australia. Nature communications (2025) [305] Baker, A.L., Arruda, B., Palmer, M.V., Boggiatto, P., Sarlo Davila, K., Buckley, A., Ciacci Zanella, G., Snyder, C.A., Anderson, T.K., Hutter, C.R., et al.: Dairy cows inoculated with highly pathogenic avian influenza virus h5n1. Nature (2025) [306] Burton, A.C., Beirne, C., Gaynor, K.M., Sun, C., Granados, A., Allen, M.L., Alston, J.M., Alvarenga, G.C., Calderon, F.S. A., Amir, Z., et al.: Mammal responses to global changes in human activity vary by trophic group and landscape. Nature ecology & evolution (2024) [307] Nvidia: Earth-2. https://www.nvidia.com/en-us/high-performance-computing/earth-2/ (2024)"
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Robotics Institute, Carnegie Mellon University",
        "S-Lab, Nanyang Technological University"
    ]
}