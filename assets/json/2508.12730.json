{
    "paper_title": "Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods",
    "authors": [
        "Jaeung Lee",
        "Suhyeon Yu",
        "Yurim Jang",
        "Simon S. Woo",
        "Jaemin Jo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine Unlearning (MU) aims to remove target training data from a trained model so that the removed data no longer influences the model's behavior, fulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we observe that researchers in this rapidly emerging field face challenges in analyzing and understanding the behavior of different MU methods, especially in terms of three fundamental principles in MU: accuracy, efficiency, and privacy. Consequently, they often rely on aggregate metrics and ad-hoc evaluations, making it difficult to accurately assess the trade-offs between methods. To fill this gap, we introduce a visual analytics system, Unlearning Comparator, designed to facilitate the systematic evaluation of MU methods. Our system supports two important tasks in the evaluation process: model comparison and attack simulation. First, it allows the user to compare the behaviors of two models, such as a model generated by a certain method and a retrained baseline, at class-, instance-, and layer-levels to better understand the changes made after unlearning. Second, our system simulates membership inference attacks (MIAs) to evaluate the privacy of a method, where an attacker attempts to determine whether specific data samples were part of the original training set. We evaluate our system through a case study visually analyzing prominent MU methods and demonstrate that it helps the user not only understand model behaviors but also gain insights that can inform the improvement of MU methods."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 1 Unlearning Comparator: Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods Jaeung Lee, Suhyeon Yu, Yurim Jang, Simon S. Woo, and Jaemin Jo 5 2 0 2 8 ] . [ 1 0 3 7 2 1 . 8 0 5 2 : r AbstractMachine Unlearning (MU) aims to remove target training data from trained model so that the removed data no longer influences the models behavior, fulfilling right to be forgotten obligations under data privacy laws. Yet, we observe that researchers in this rapidly emerging field face challenges in analyzing and understanding the behavior of different MU methods, especially in terms of three fundamental principles in MU: accuracy, efficiency, and privacy. Consequently, they often rely on aggregate metrics and ad-hoc evaluations, making it difficult to accurately assess the trade-offs between methods. To fill this gap, we introduce visual analytics system, Unlearning Comparator, designed to facilitate the systematic evaluation of MU methods. Our system supports two important tasks in the evaluation process: model comparison and attack simulation. First, it allows the user to compare the behaviors of two models, such as model generated by certain method and retrained baseline, at class-, instance-, and layer-levels to better understand the changes made after unlearning. Second, our system simulates membership inference attacks (MIAs) to evaluate the privacy of method, where an attacker attempts to determine whether specific data samples were part of the original training set. We evaluate our system through case study visually analyzing prominent MU methods and demonstrate that it helps the user not only understand model behaviors but also gain insights that can inform the improvement of MU methods. Index TermsMachine unlearning, visual analytics, model comparison, model evaluation, data privacy. I. INTRODUCTION ACHINE Unlearning (MU) refers to the process of making trained model forget specific data it was trained on, such that those data no longer influence the models behavior [1]. This capability has become increasingly important as individuals invoke the right to be forgotten [2] under data privacy laws like the General Data Protection Regulation (GDPR) [3]. Regulators have also begun to enforce this: in 2021 case, the U.S. Federal Trade Commission (FTC) ordered company to delete not only the unlawfully collected data but also any models derived from it [4]. Beyond legal compliance, the ability to reliably remove data is now recognized as critical component for ensuring the safety and trustworthiness of advanced AI systems [5]. However, naıvely retraining model from scratch for each unlearning request is Jaeung Lee, Yurim Jang, Simon S. Woo, and Jaemin Jo are with Sungkyunkwan University, Suwon, Korea. E-mail: dlwodnd00@skku.edu, {jyl8755, swoo}@g.skku.edu, jmjo@skku.edu. Suhyeon Yu is with Rice University, Houston, TX, USA. E-mail: sy118@rice.edu. Jaemin Jo is the corresponding author. Manuscript received xx; xx. highly inefficient since modern machine learning models often require significant amount of time and computational cost to train [6]. To address this, MU methods aim to approximate the result of retraining with minimal adjustments to the models parameters. We observe that researchers in this emerging field struggle to analyze and understand the behavior of different MU methods. We identify two main hurdles to this challenge. First, despite the recent introduction of MU methods [7] [11], there is still no standardized evaluation protocol for systematically comparing them. Due to the absence of such protocol, the MU methods have been separately assessed using disparate metrics [12][16]. This not only hinders direct comparisons among methods but also makes it difficult to understand nuanced trade-offs between the methods. Second, existing evaluations primarily rely on quantitative metrics, which offer limited insight into the behaviors of different methods and fail to uncover the underlying decision-making processes happening in the model, which is insufficient to address the analytic tasks we identified. For example, one important task in MU research is to evaluate to what extent an MU model satisfies the privacy principle. This principle states that after unlearning, it should be guaranteed that no residual signals of the forgotten data remain, thereby thwarting attack [17][21]. For instance, an attacker might query an online medical diagnostic model to check whether particular patients records still shape its responses, which is an example of membership inference attacks (MIAs). Relying on metrics alone is insufficient for such an evaluation, as such summary metrics would fail to reveal which specific samples remain vulnerable or how model behavior exhibits subtle shifts after unlearning. These limitations call for an interactive approach that allows researchers to simulate attacks and observe how the model responds. To address the two hurdles, we present Unlearning Comparator, visual analytics system that enables systematic and multi-foci comparison between MU methods. Our system supports the side-by-side comparison between two models at three aspects: metrics, embeddings, and attacks. By spanning classand instance-level data as well as output and internal representations, our system aims to show how thoroughly each method removes targeted data without affecting the remainder, highlighting subtle representation changes that aggregate metrics overlook. In our case study and interviews, we demonstrate how this systematic comparison not only clarifies each methods behavior but also leads to refinements and new JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2 designs for novel MU method. We contribute: design study with MU researchers, incorporating domain analysis to identify user tasks and derive fourstage workflow; Unlearning Comparator, visual analytics system that supports the comparative evaluation of MU methods through visualizations of metrics, embeddings, and simulated attacks; The first (to our knowledge) visual analysis of prominent MU methods, which uncovers overlooked behavioral patterns and informs the development of novel, more effective MU method, Guided Unlearning (GU). II. BACKGROUND AND RELATED WORK A. Machine Unlearning Fundamentals"
        },
        {
            "title": "We call",
            "content": "the model trained on the entire dataset before unlearning the original model. As the input to an MU method, the user chooses the data to be unlearned by partitioning the entire dataset into retain set (data to keep) and forget set (data to remove). The goal of MU is to produce an unlearned model by efficiently modifying the original models parameters to approximate the behavior it would exhibit if the forget set had never been included. The performance and behavior of the unlearned model are often compared with retrained model, which is obtained by training the model from scratch on the retain set. While building retrained model is inefficient in practice, it serves as gold standard to assess the effectiveness of MU methods."
        },
        {
            "title": "The MU methods are commonly evaluated based on three",
            "content": "principles: accuracy, efficiency, and privacy [22][25]. Accuracy. The accuracy principle requires that the forget set should be unlearned without significantly degrading the models performance on the retain set. Since deep neural networks have intertwined internal representations, simply dropping certain parts of the representations may harm the overall accuracy. We use four standard accuracy measures: Unlearning Accuracy (UA) measures correctness on the forget set (lower is better) while Retain Accuracy (RA) measures correctness on the retain set (higher is better). If separate test set is available, the accuracy can also be measured for the unseen data, resulting in Test Unlearning Accuracy (TUA) (lower is better) and Test Retain Accuracy (TRA) (higher is better). Efficiency. The efficiency principle relates to the computational overhead that an MU method exhibits; for example, building retrained model can be considered as an MU method but is inefficient as it requires full retraining on the retain set. The efficiency of method is quantified by its Run Time (RT) represented in seconds. Privacy. The privacy principle ensures the true removal of the forget set without lingering signals to prevent attackers from detecting data influence. In practice, this is primarily evaluated through the success rate of membership inference attacks (MIAs) [17], [19], [20], which predicts whether specific data samples were part of the training set based on the models output. However, this single metric is often insufficient, as it fails to reveal which specific samples remain vulnerable and why, thus requiring more nuanced, interactive analysis. B. Machine Unlearning Methods To avoid the computational overhead of the retraining, most the MU methods take approximate approaches that adjust models weights without completely rebuilding it. We surveyed the literature and identified three widely adopted MU methods in the context of approximate unlearning. These baselines appear across many recent MU studies [9], [26] and represent complementary strategies. Fine-Tuning (FT): This method continues training (i.e., running more training epochs) only on the retain set, relying on catastrophic forgetting [27] of the forget set. Random Labeling (RL): This method assigns random labels to the forget set and fine-tunes the model on this modified dataset, causing the model to forget the original information associated with the forget set. Gradient Ascent (GA): This method adjusts the models parameters to maximize loss on the forget set to intentionally unlearn it. Beyond these baselines, more advanced methods have also been proposed recently. For example, SCRUB [28], which uses teacherstudent distillation framework to maximize loss on the forget set while minimizing loss on the retain set, and Saliency Unlearning (SalUn) [8], which identifies and masks weights most influenced by the forget set before applying random labeling plus targeted fine-tuning. Although these methods represent diverse strategies, few studies compared them under consistent protocol. Our system addresses this gap by allowing researchers to examine and contrast their trade-offs in accuracy, efficiency, and privacy. C. Distribution-based Approach for Privacy Evaluation While accuracy and efficiency are relatively straightforward to assess, privacy evaluation is more challenging. In classification tasks, attackers often employ prediction entropy (i.e., the spread of class probability outputs) as signal for membership inference. In this entropy-based MIA (E-MIA), sample is predicted as non-training if its entropy exceeds chosen threshold (i.e., the model is uncertain about the sample), and training otherwise (i.e., the model is certain). However, evaluating models privacy by simulating such attacks may be less reliable in unlearning scenarios, as these attacks can be easily circumvented; for example, one can deliberately make model flatten its output scores to increase the entropy and thereby appear resilient to E-MIA. To provide more rigorous privacy evaluation, we adopt and extend stronger MIA from prior work [15], which assumes that the attacker compares the unlearned and retrained models output distributions. If the unlearned model truly behaves as if it never saw the forget set, its distribution should align with that of the retrained model, providing stronger assurance that no residual data signals remain; in our extension, we incorporate both confidenceand entropy-based attacks under multiple thresholds, yielding unified Worst-Case Privacy Score (Section IV-A). JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 3 D. Visual Analytics for Machine Learning Visual analytics has been widely employed to help people develop, interpret, and improve machine learning models [29] [31]. It aims to bridge the gap between complex model behaviors and human understanding, which is essential for building trustworthiness and reliability [32]. For example, Uni-Evaluator [33] supports model evaluation through unified performance analysis across multiple tasks to guide debugging. To help understand internal mechanisms of models, interactive systems that visualize layer-wise activations [34] or architecture-specific explanations for CNNs [35] and Transformers [36], [37] have also been studied. Furthermore, general-purpose attribution techniques such as Grad-CAM [38] are now widely used to highlight important regions in input images. While these systems and techniques help analyze single model, they often fall short when there is need to assess differences between models in comparative manner, which is also common in machine learning. Model comparison aims to reveal relative strengths, weaknesses, and behavioral differences between models. Several visual analytics systems have been developed to support comparisons in certain domains of machine learning. Examples include analyzing efficiency-behavior trade-offs in model compression [39], comparing the internal structures of CNN architectures [40], and evaluating behavioral shifts when adapting language models to new tasks [41]. However, these comparison tools are not designed for MU, nascent field with unique, domain-specific challenges: new concepts such as the distinction of forget and retain sets, the notion of retrained model as gold standard, and the need for privacy evaluation. Overall, although prior VIS4ML systems have effectively supported model understanding across domains, MU introduces distinctive challenges for visual analytics. To address this gap, we present visual analytics system that enables unified workflow for the comparative evaluation of MU methods, thereby advancing research in this emerging field. III. DOMAIN ANALYSIS Our target users are MU researchers who want to compare the performance of different MU methods in terms of the three principles and understand their trade-offs. To identify their needs, we conducted design study by holding regular discussions over two months with two MU experts to gather insights into their analytical challenges and integrating their feedback throughout the development process. We also surveyed prior literature, including recent benchmarking studies [7], [9], [11], to extract common analysis and evaluation patterns. Synthesizing the insights from both sources allowed us to summarize their goals as follows: (1) evaluate how different MU methods unlearn data by enabling pairwise comparisons as unified workflow, and (2) leverage these insights to improve MU methods aligned with the three principles. While MU is an active research area in various domains such as image classification, text generation, and image generation, we focus specifically on class-wise unlearning for image classification as it provides the most widely used and robust environment for systematically evaluating MU methods. From this point, for simplicity, we refer to the forget set and retain set as the forget class and retain classes, respectively. A. User Tasks Based on the insights gained from the design study, we distilled the following five user tasks that our system must support: T1. Build and Screen Models. Many MU methods lack standardized training recipes as their optimization objectives and resulting loss scales can be unpredictable compared to standard training. This forces experts to experiment extensively with different MU methods and their hyperparameters. Therefore, an initial step is to build multiple candidate models and collect their summarized performance metrics. This allows experts to efficiently screen candidates and make an informed selection of pair of models for deeper comparison. T2. Compare Two Models Pairwise. We found that the experts compare two models to understand how MU has altered models behavior. For example, they compare an unlearned model with retrained model for benchmarking purposes (see T2.2). Thus, we focus on supporting these targeted pairwise comparisons. We identify three types of such tasks, each serving distinct purpose: T2.1 Compare the Original Model with the Retrained or an Unlearned Model. Comparing the original model with the retrained illustrates the ideal scenario in which the forget class was never included, providing reference for evaluating the MU methods. Comparing the original model with an unlearned then reveals how the method has altered the models behavior. T2.2 Compare the Retrained Model with an Unlearned Model. This task reveals how closely the unlearned model approximates the gold standard retrained model, which is the goal of MU. T2.3 Compare Unlearned Models with Each Other. Comparing two unlearned models allows experts to assess relative strengths and weaknesses across methods; for example, by comparing baseline MU method with its optimized variant, they can identify which performs better. T3. Investigate Class-wise Accuracy and Confidence. Our experts wanted to investigate if the accuracy for the forget class decreases as desired, without harming the retain classes, which is the accuracy principle. They also wanted to investigate the confidence (i.e., the probabilities assigned to classes) to reveal subtle behavioral changes missed by the final prediction alone. However, since distribution shifts due to unlearning can make absolute softmax-based confidence scores misleading, this task focuses on examining patterns of overand under-confidence, as well as overall calibration quality, rather than absolute confidence scores. T4. Analyze Changes in Layer Representations. Even if accuracy metrics indicate successful forgetting, prior studintermediate layers might ies [42], [43] have shown that still encode knowledge of the forget class, compromising the privacy principle. Hence, analyzing layer activations in the feature space before and after unlearning clarifies how the models internal representation changes and whether the forget class has been removed at the layer level. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 4 Fig. 1. Comparative MU evaluation workflow of Unlearning Comparator. The workflow guides users through four iterative stages. In the Build stage, users generate various unlearned models, followed by the Screen stage where they use summary metrics to select two for in-depth analysis. The Contrast stage involves comparing the selected pair from class-, instance-, and layer-level perspectives to understand model behaviors. Finally, the Attack stage verifies privacy by simulating membership inference attacks. Insights from all stages then guide iterative refinement of the unlearning methods. T5. Verify Privacy through Attack Simulation. One important task in MU research is to ensure the privacy of the models after unlearning. To this end, the experts wanted to simulate membership inference attacks (MIAs) on an unlearned model and its retrained counterpart to see if an attacker could still predict whether specific data point was part of the original training set. B. Comparative Machine Unlearning Evaluation Workflow We distill the user tasks into unified process, proposing the Comparative Machine Unlearning Evaluation Workflow, which consists of four stages: Build, Screen, Contrast, and Attack (see Fig. 1). In the Build stage, users specify the base model (e.g., the original model) as starting point, define the forget class, and choose an unlearning method along with hyperparameters, producing multiple candidate models (T1). Next, in the Screen stage, they review summarized performance metrics (T1) to decide which models warrant closer inspection. Users then proceed to the Contrast stage, where they compare two chosen models (T2). For instance, they can compare an unlearned model with the original model to inspect model changes (T2.1) or with the retrained model to assess proximity to the ideal outcome (T2.2), or compare two unlearned models to contrast their behaviors (T2.3). They examine accuracy shifts and prediction patterns to check that the forget class is removed without adversely affecting the retain classes (T3). They also investigate representation changes across the network and observe the embedding space to see how features have changed after unlearning (T4). Finally, in the Attack stage, users conduct MIAs (T5) to confirm that the forget class is no longer identifiable, exploring different attack metrics and strategies to assess privacy risks and detect vulnerable samples. After reviewing the insights from these stages, users can refine their method by adjusting hyperparameters or modifying the unlearning algorithm itself, and return to the build stage to iteratively improve accuracy, efficiency, and privacy. IV. THE UNLEARNING COMPARATOR SYSTEM In this section, we elaborate on Unlearning Comparator, visual analytics system designed for comparative evaluation of MU methods, supporting T1-T5. We chose two datasets, CIFAR-10 [44] and Fashion-MNIST [45], to demonstrate our system, as they are commonly employed in prior MU research. For model architectures, we included two representative architectures: well-known convolutional architecture ResNet18 [46] and more advanced transformer-based architecture, ViT-B/16 [47], pre-trained on ImageNet [48]. While these choices ensure consistency with existing literature, our system is designed to support using other datasets and architectures. A. Designing the Worst-Case Privacy Score To facilitate comparative privacy evaluation (T5), we define Worst-Case Privacy Score (WCPS) by synthesizing established MIA concepts. There are two widely adopted standard MIAs: (1) Confidence-based MIA (C-MIA) [7], [49] and (2) Entropy-based MIA (E-MIA) [50], [51]. Inspired by the NeurIPS 2023 Machine Unlearning Competition [15], we design WCPS under the assumption of stronger, more knowledgeable attacker, as those standard MIAs can be easily circumvented (Section II-C). Under this assumption, the adversary can access both the predicted labels confidence and the entropy of all logit outputs from unlearned or retrained models. Although this setting is infeasible in practice, it is adopted to consider an optimal adversary and thus facilitate conservative privacy evaluation. The attacker measures how distinguishable the output distributions (confidence or entropy) of the models are on the forget class to distinguish between training and non-training inputs. threshold-based decision rule is applied to perform the inference. We aggregate the results over multiple thresholds and both output statistics (confidence and entropy) to derive single worst-case score, denoted as WCPS. Score Definition. Let = (z1, z2, . . . , zn) denote the logit outputs for classes. The softmax function is given by: pi = ezi j=1 ezj (cid:80)n (1) The confidence function C(p) using the log-odds transformation is defined as: pmax = max pi, C(p) = log(pmax) log(1 pmax) (2) JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX Fig. 2. Unlearning Comparator assists MU researchers in analyzing and comparing MU methods. (A) The Model Builder (shown in Fig. 3A) creates unlearned models. (B) The Model Screening view lets users obtain an overview and select two models for deeper inspection. (C) The Metrics view highlights class-level performance and internal representation changes. (D) The Embedding Space view displays each models feature embeddings side-by-side. (E) The Attack Simulation view (shown in Fig. 8) performs membership inference attacks to verify that no residual signal remains about the unlearned data. The entropy H(p) is defined with temperature = 2.0, selected to better separate model output distributions, as follows: To obtain privacy score for each output statistic, we select the minimal St across all thresholds: p(T ) = ezi/T j=1 ezj /T (cid:80)n , H(p) = (cid:88) i=1 p(T ) log(p(T ) ) (3) For each attack, to distinguish whether sample comes from the retrained or unlearned model, we define thresholdbased decision rules over these metrics. These rules produce false positive rates (FPR) and false negative rates (FNR). We consider 100 evenly spaced thresholds across each metrics range, testing both directions of inequality for optimal attack performance. At each threshold t, let FPRt and FNRt respectively be the false positive and false negative rates observed at that threshold. Then we calculate ϵt as follows: (cid:16) ϵt = max (cid:16) 0, min log(cid:0) 1FPRt FNRt (cid:1), log(cid:0) 1FNRt FPRt (cid:1)(cid:17)(cid:17) . (4) Intuitively, ϵt measures the indistinguishability of the output distributions from the retrained and unlearned models at threshold t, capturing the degree of privacy achieved. higher ϵt indicates more successful attack and thus lower privacy. Accordingly, at each threshold t, we define the thresholdlevel Attack Score (ASt) and Privacy Score (P St) as follows: ASt = 1 2ϵt, St = 1 ASt. (5) SC = min tT (P SC ), SE = min tT (P SH ), (6) where SC at threshold t, and SH is computed using the confidence C(p) from Eq. 2 uses the entropy H(p) from Eq. 3. Finally, for the overall model, we apply worst-case perspective across the two output statistics by taking the minimum of their PS values: CP = min (cid:16) SC, SE (cid:17) . (7) Comparison with Standard MIAs. By design, WCPS assumes stronger attacker, providing more conservative privacy evaluation than standard C-MIA and E-MIA. To examine the correlation between these metrics in practice, we conducted an experiment using the FT method that simulates the process where the forget class representations are progressively overwritten by the retain classes representations. This continues even after the unlearning accuracy reaches zero, as merely achieving zero accuracy on the forget class does not guarantee complete removal of its influence. In this comparison, C-MIA and E-MIA are calculated as the proportion of the forget class identified as non-members by the attack. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 6 Fig. 3. Users configure unlearning settings in the Model Builder (A) to generate candidate models. By selecting multiple values for each hyperparameter, they can build multiple models at once; all combinations are generated automatically. They then review each models performance in the Model Screening view (B), which presents summary metrics and reveals epoch-wise metrics upon clicking row, allowing users to select two for deeper comparison. providing dedicated workspace for specific forget class. plus icon allows users to add new tabs for additional forget classes. Each tab consists of the Model Builder (Fig. 2A) for the Build stage, the Model Screening view (Fig. 2B) for the Screen stage, the Metrics view (Fig. 2C) and the Embedding Space view (Fig. 2D) for the Contrast stage, and the Attack Simulation view (Fig. 2E) for the Attack stage. 1) Model Building (the Build stage): The Model Builder creates unlearned models (Fig. 3A). Users specify the base model to unlearn, the chosen method, and multiple hyperparameter combinations. Models corresponding to all these combinations are automatically generated. To avoid complexity while allowing meaningful adjustments, we limit tunable hyperparameters to epochs, learning rate, and batch size, as these have been shown to play critical role in prior MU studies [9], [28] and offer sufficient control over the unlearning process. We offer three baseline MU methods from Section II-B (i.e., fine-tuning, random labeling, and gradient ascent). To minimize the computational overhead of training new models, we provide both the original model and the classwise retrained models in advance. Although such retrained models are rarely available in real-world applications, we include them here as essential benchmarks to enable controlled evaluation of MU methods in research context. These models follow standardized benchmarking recipes [53], [54] to ensure reproducibility and robustness. In addition to these baselines, users can build their custom models by (1) extending our provided Python code template to implement their own method or (2) uploading weight files for their unlearned models. The building process is shown in the Model Builder in real time (Fig. 2A). Once completed, newly created model automatically appears in the Model Screening view. 2) Model Screening (the Screen stage): The Model Screening view provides an overview of all models, including both system-provided models (original and retrained) and usercreated unlearned models (Fig. 3B). Each row in the table shows the models basic configuration and representative metrics for the three main MU principles described in Section II-A. Distinct color schemes are used for these principles: Fig. 4. Comparison of privacy metrics using FT beyond zero unlearning accuracy. C-MIA prematurely reaches and remains at 1.0 due to its reliance on raw confidence, while E-MIA incorrectly declines as the model confidently misclassifies samples. In contrast, WCPS progressively increases, reflecting the convergence toward the retrained models distribution. As shown in Fig. 4, WCPS gradually increases as epochs progress, reflecting that the forget class representations are progressively replaced, and the output distribution converges toward the retrained models distribution. However, C-MIA and E-MIA fail to capture this progression. Even at epoch 1, where the distribution has not yet converged (WCPS 0.4), CMIA prematurely reaches 1.0 (i.e., classifying all forget class samples as non-members). It remains at 1.0 due to its reliance on raw confidence values without log-odds transformation (Eq. 2), which would better separate confidence distributions; this reliance causes incorrect membership inference when confidence drops. E-MIA exhibits even more misleading behavior; it declines as epochs progress because the low entropy from confident misclassification to other classes is misinterpreted as evidence of membership. Notably, the WCPS plateaus around 0.8 rather than reaching 1.0 because FT removes information spanned by the retain classes, struggling to fully erase residual information unique to the forget class [52]. B. User Interface Unlearning Comparators user interface  (Fig. 2)  is designed to support each stage of our workflow  (Fig. 1)  . As shown at the top of Fig. 2, the interface consists of multiple tabs with each JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 7 Fig. 5. The Metrics view provides metrics that reveal how an unlearning method targets the forget class while preserving the retain classes. (A) Class-wise Accuracy chart displays the per-class accuracy differences to examine high-level trade-offs. (B) Prediction Matrix visualizes predicted proportion and average confidence to inspect misclassification patterns, and (C) Layer-wise Similarity chart shows the similarity of layer representations against the original or the retrained models to reveal changes in internal representations. Initial and final designs of the Prediction Matrix, both encoding Fig. 6. predicted proportion and average confidence. The initial design (A) uses circle size and color, while the final design (B) arranges them diagonally. UA, RA, TUA, and TRA for accuracy, RT for efficiency, and the WCPS (defined in Section IV-A) for privacy (abbreviated as PS in the UI for simplicity). Users can filter by method or sort the metrics to identify promising models. Additionally, clicking on any row reveals the per-epoch performance metrics as line chart (pop-up in Fig. 3B) to show the models unlearning progress and convergence. After this initial screening, they can select pair using the two radio buttons in the rightmost columns for closer examination in the subsequent view. In the pairwise visualization analysis, the selected Model and Model are shown using two distinct green and purple colors, respectively, while the retain class and forget class are for the retain classes and for the shown as symbols, forget class. For cases that require color-coding all 10 classes, we apply the Tableau10 color scheme. (cid:32) 3) Model Comparison (the Contrast stage): The Metrics view provides metrics that reveal how an MU method targets the forget class while preserving the retain classes. It comprises three components: (1) Class-wise Accuracy chart to analyze accuracy difference for each class, (2) Prediction Matrix to capture misclassification patterns, and (3) Layerwise Similarity chart to depict representation changes across the network. The Class-wise Accuracy chart visualizes each models perclass accuracy difference using diverging bar chart (Fig. 5A). dotted line denotes the average accuracy difference across all retain classes, enabling quick assessment of whether the forget class is adequately forgotten without overly compromising the retain classes. As shown in Fig. 5A, Model achieves higher accuracy on the airplane (forget class) and dog (one of the retain classes), while Model outperforms it on most other retain classes. Hovering over bar reveals the exact per-class accuracies and differences for both models. On the right, results on the test dataset are juxtaposed, helping users simultaneously gauge overall generalization. This design highlights the high-level trade-off between under-forgetting (insufficient removal) and over-forgetting (excessive removal). The Prediction Matrix provides detailed view of how model assigns predicted classes for each true class (Fig. 5B). Traditionally, confusion matrices encode the true class on the row axis and the predicted class on the column axis, using color to represent the proportion of samples falling into each cell. However, we aim to incorporate not just these proportions but also the models confidencemerely observing classification errors does not reveal how confidently model is mistaken. This finer observation relates to commonly recognized issue of confidence calibration [55], which can be exacerbated by the distribution shifts inherent to the unlearning process. As shown in Fig. 5B, users can compare two models prediction patterns side-by-side; for instance, in the tooltip of Model B, one cell has predicted proportion of 0.560 yet an average confidence of only 0.160, unlike Model A, illustrating how large mismatch can remain hidden if only proportions are considered. Our initial design took inspiration from prior work [56], [57] that uses size and color for double-encoding heatmap, representing predicted proportions with circle size and average confidence with color (Fig. 6A). However, during our study, we identified perception issue: when both visual channels encoded information within single glyph, users struggled to discern subtle variations in confidence (color), particularly when the proportions (size) were small, common scenario when unlearning scatters predictions across multiple classes. Therefore, we chose to use the diagonal-split encoding [58], [59], which addresses this by spatially separating the two channels (Fig. 6B): predicted proportion in the lower-left and average confidence in the upper-right. This design preserves color detail more clearly (Fig. 6-Orange Box) and makes their relationship more apparent (Fig. 6-Blue Box). The Layer-wise Similarity chart illuminates changes in internal representations across the network, offering perspective distinct from previous output-level views (Fig. 5C). For set of representative layers (e.g., the initial, final, and the boundary layers of each residual block in ResNet-18), we measure the Centered Kernel Alignment (CKA) [60] of each selected model against two references: the original model, to gauge JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 8 Comparative visual analysis of feature space for two unlearned models in the Embedding Space view. (A) Highlighting the forget class instances Fig. 7. reveals how their feature distribution differs between them. For instance, Model (right) contains dense, unforgotten cluster that can be explored in detail. (B) Linking interactions enable direct comparison. Hovering connects the same instance across models, while clicking on an instance reveals its image and compares predicted confidences. the magnitude of representational change, and the retrained model, to assess alignment with ideal representations. We chose to use CKA instead of measuring the distances between parameters [61], as CKAs permutation-invariant comparison of activation spaces can facilitate the comparison of functionally similar models. This chart allows users to visually track at the layer level when and how the representations for the forget and retain classes diverge. For example, in Fig. 5C, both models exhibit similar representational changes in the final fully connected layer. However, an internal layer (layer4.0) shows substantial drop in CKA for Model (0.246) but not for Model (0.822). This indicates that Model continues to encode much of the forget classs features in this layer, while Model removes them more substantially. The Embedding Space view examines the feature space to visually analyze and compare each models decision boundary  (Fig. 7)  . We pass each data sample through Model and Model up to the penultimate layer to obtain highdimensional feature vectors, then reduce them to two dimensions and display the resulting scatterplots side-by-side for direct comparison. Because final predictions rely on linear transformation of these penultimate layer outputs, visualizing them clarifies how the decision boundary is formed. Placing the two scatterplots this way lets users see how an unlearned models feature vectors shift from the original model and determine whether they resemble the retrained models decision boundary. We use UMAP [62] for dimensionality reduction because it is known to preserve global structure more effectively than other techniques, such as t-SNE [63], aiding in locating forget class samples that might scatter unpredictably. In this view, all training instances are displayed by default (Fig. 2D), but users can activate highlight mode to focus on specific subsets (Fig. 7A): Target to Forget (all forget class samples), Successfully Forgotten (forget class samples predicted differently from the true label), Not Forgotten (forget class samples still predicted as their true label), or Overly Forgotten (retain class samples misclassified). Users can explore shifts in the feature space to identify areas of interest, such as dense unforgotten clusters (Fig. 7A), and perform instance-level comparisons by linking corresponding instances to track single samples changing predictions and confidences between the two models (Fig. 7B). 4) Attack Simulation (the Attack stage): While previous views focused on comparing the two selected models from multiple perspectives, the Attack Simulation view compares their output distributions against the retrained model and applies MIAs to discriminate between them  (Fig. 8)  . We draw inspiration from Wattenberg et al. [64] for the interface design. The procedure for defining per-threshold attack scores and privacy score is detailed in Section IV-A. Accordingly, we obtain each forget class samples logit vector and transform it into confidence or entropy value (Eq. 2, Eq. 3). Each transformed sample appears as dot in two plots: (1) retrained model vs. Model and (2) retrained model vs. Model B. threshold slider overlays each plot, and the corresponding FPR, FNR, and attack score (Eq. 4, Eq. 5) are shown on the right as line charts, providing metric view of the attack (Fig. 8B). Users can perform various attacks in this view (Fig. 8A). They may select whether to use entropy or confidence and choose different threshold-setting strategies. For example, they can set threshold to maximize the attack score based on our defined FPR and FNR, select threshold with higher overall success, or apply common threshold for both Model and Model to achieve the highest attack score under identical conditions. Ultimately, across two attack metrics, threshold directions, and strategies, the worst-case scenariowhere the model is most vulnerabledefines the WCPS, and the user can check exactly which configuration leads to that worst case via dedicated button. To move beyond distribution-level inspection, we provide interaction and grid view for instance-level details (Fig. 8C). image Attack success or failure is shown through small thumbnails; hovering over an image or dot highlights the corresponding instance in the other model, letting users view precise numeric values and compare each samples position within the distribution. Clicking sample displays its original image and compares the retrained and unlearned models perclass confidence scores (Fig. 8D). As an example, Fig. 8B shows scenario on CIFAR-10 where airplane is the forget class. The retrained model exhibits diverse range of entropy values for those samples. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 9 Fig. 8. The Attack Simulation view to assess two unlearned models against the retrained model via MIAs. Gray, green, and purple dots indicate samples output by the retrained model, Model A, and Model B, respectively. (A) Users configure the attack metric and threshold-setting strategy, and can use button to inspect the worst-case scenario. (B) Dot plots display each models output distribution compared with the retrained model, with FPR/FNR and attack score tracked by threshold slider. (C) Success and Failure samples appear in grid for instance-level inspection. (D) Clicking on any sample reveals its original image and compares the retrained and unlearned models predicted confidence. By contrast, Model often produces higher entropy for the forget class, making the two distributions more easily distinguishable. Meanwhile, Model appears broadly similar to the retrained model yet features cluster of low-entropy outliers. By adjusting the threshold, users can isolate these outliers in the grid, apply filters, and inspect them individually. In Fig. 8D, for instance, Model assigns strong confidence to automobile, whereas the retrained model spreads its confidence among multiple classes. Under most thresholds, this distinct difference makes the sample easily singled out, indicating higher privacy risk. upon backend initialization. When users configure unlearning parameters and initiate execution through the UI, the frontend sends API calls to trigger real-time unlearning on the backend. Upon completion, the backend saves both model checkpoints and metrics for visualization as JSON files. The JSON is then transmitted to the frontend for rendering. Additionally, users can add custom MU methods by implementing Python hooks with minimal boilerplate code; once registered, these methods appear in the UI alongside existing MU methods. The source code is publicly available at https://github.com/ gnueaj/Machine-Unlearning-Comparator. C. Implementation Details Our implementation consists of React.js1 frontend that provides visualizations powered by D3.js2 and follows the Shadcn/UI3 design system, and FastAPI4 backend that manages model evaluation, unlearning operations, and data handling. The system provides pre-computed results for systemprovided models stored as JSON files, with their checkpoints automatically downloaded from Hugging Face repositories 1https://react.dev 2https://d3js.org 3https://ui.shadcn.com 4https://fastapi.tiangolo.com 5https://huggingface.co/jaeunglee/resnet18-cifar10-unlearning, https://huggingface.co/Yurim0507/resnet18-fashionmnist-unlearning, https://huggingface.co/Yurim0507/vit-base-16-cifar10-unlearning V. CASE STUDY To evaluate if and how our system assists users in understanding the behavior of MU methods, we conducted case study in collaboration with two experts introduced in Section III. To the best of our knowledge, this study presents the first comparative visual analysis of prominent MU methods to enhance their transparency and interpretability [65]. The case study consists of two stages. In the first analysis stage, the experts freely used our system to compare the behavior of five MU methods: three baseline methods (FT, RL, and GA) and two state-of-the-art MU methods (SCRUB and SalUn) as described in Section II-B. To examine whether the insights from the analysis stage could lead to actual improvements in MU performance, we conducted second, improvement stage, in which the experts developed novel MU method based on JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 10 Fig. 10. Finding 2 Examining the Class-wise Accuracy trade-offs. (A) Two GA variants with different batch sizes: compared to Model (small batch), Model (large batch) exhibits lower accuracy on the forget class with higher accuracy on the retain classes, indicating better unlearning. (B) An unlearned model, Model (FT), is compared to Model (Retrain) in both training and test datasets. It reveals performance degradation on semantically similar classes to the forget class (B-left) and overfitting in the retain classes (Bright). Fig. 9. Finding 1 Analyzing embedding shifts between the original model (Model A) and the retrained model (Model B). Top: When frog is the forget class, embeddings from this class are redistributed into nearby clusters (e.g., bird, cat, deer). Bottom: When dog is the forget class, most embeddings in the retrained model shift toward the cat cluster. their findings from the analysis stage. For the case study, we employed ResNet-18 architecture on the CIFAR-10 dataset, designating various classes (e.g., airplane, automobile, or deer) as the forget class one at time. A. Analysis Stage: Comparing the Existing MU Methods In the analysis stage, the two experts collaboratively used our system for week to (1) investigate the behavior of each MU method and (2) evaluate them based on MU principles. We summarize the visualizations the experts employed (Fig. 9, 10, 11, 12, and 13) and their corresponding findings (Finding 16) below. These findings later informed the development of novel MU method. 1) Retrained Model Behavior (Finding 1): Before analyzing each MU method in depth, they first compared the retrained model with the original model (T2.1) to understand how the model behaves when it has never seen the forget class. In one such analysis, as illustrated in Fig. 9, the Embedding Space view shows that many of the original models embeddings shift toward nearby clusters in the retrained model. This comparison provides reference point for evaluating how MU methods remove the forget class. 2) Hyperparameter and Class-wise Trade-offs (Finding 2): They tested different hyperparameter combinations for each method using the Model Builder and the Model Screening view, examining class-specific impacts through the Classwise Accuracy chart (T1, T2.3). During this process, they observed that different MU methods performed best under distinct hyperparameter settings. For example, GA performed well with larger batch size, higher learning rate, and fewer Fig. 11. Finding 3 Investigating misclassification patterns and detecting confidence mismatches. The first row is the forget class airplane, and each column represents the predicted class. By comparing this entire row to the retrained model (A), users can see how similarly each unlearned method allocates the forget class. Notably, brightness contrast between the two triangles in single cell reveals inconsistencies between confidence and predicted proportion (C and F). training epochs (Fig. 10A). This may be due to larger batches providing more stable gradients, enabling targeted forgetting of forget class representations. In contrast, FT showed better results with more training epochs, though this also made it more prone to overfitting (Fig. 10B-right). They also found that retain classes sharing similar features with the forget class (e.g., deer vs. animals such as frog or dog) exhibited greater accuracy degradation (Fig. 10B-left). These results highlight how the system helps users refine hyperparameters and better understand the inherent class-wise trade-offs of each method. 3) Misclassification Patterns (Finding 3): After tuning the hyperparameters for each MU method, they became particularly interested in how confident each method was in its predictions for the forget class versus the retain classes (T3). To investigate this, they used the Prediction Matrix  (Fig. 11)  . They found that most methods produced stable outputs (i.e., proportions closely matched their corresponding confidence) for the retain classes. However, the methods showed distinct differences in the forget class. While FT, GA, and SCRUB (Fig. 11B, D, and E) all yielded stable outputs, their prediction patterns diverged: FT and SCRUB were similar to the retrained JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 11 Fig. 12. Finding 5 Identifying the Elbow Layer via Layer-wise similarity analysis. The chart compares similarity of two models against the retrained model: Model (Original) and Model (RL). The Elbow Layer (e.g., layer3.1) is where retain class CKA is minimized, just before the sharp divergence for the forget class. This provides the rationale for our strategy of re-initializing these later, output-centric layers for efficient unlearning. Finding 6 The Attack Simulation view after unlearning of Fig. 13. bird. Samples near the cursor (with top-1 confidence greater than 0.8) exhibit abnormally high confidence for non-animal classan outcome rarely observed in predictions from the retrained model. This suggests potential vulnerability for MIAs. model (Fig. 11A), whereas GAs were not. Notably, RL and SalUn (Fig. 11C and F), both relying on random labeling, exhibited clear mismatch between predicted proportions and confidence for the forget class (i.e., the brightness contrast between the two triangles in the first-row cells). This mismatch highlights an issue of poor confidence calibration [55]; such model appears decisive due to high prediction proportions, while its low confidence values indicate internal uncertainty, leading to unreliable decisions in high-stakes domains such as medical diagnosis. 4) Feature Space Shifts (Finding 4): To understand how each method reshapes the feature space, they examined instance-level embeddings through the Embedding Space view (T4). They observed that FT and SCRUB generally shifted embeddings in ways similar to the retrained model (T2.2). However, other methods exhibited distinct behaviors. GA did not scatter the forget class embeddings into nearby clusters; instead, they dispersed throughout the entire feature space (Fig. 7A-left). This suggests that GA erased not only the intrinsic features of the forget class but also unintentionally removed features belonging to nearby samples from the retain classes. Consequently, GA appeared to over-forget certain retain classes samples, distributing them irregularly across the space (Fig. 7B-left). Meanwhile, labeling-based methods such as RL and SalUn maintained high accuracy, but their updates primarily affected the later layers. As seen in Fig. 2D, Model (RL) still formed distinct cluster for the forget class, indicating that underlying feature representations were largely preserved and only the final classification layers were significantly updated. 5) Layer-wise Representation Changes (Finding 5): To investigate where these behavioral differences manifest within the network architecture, they further analyzed the Layer-wise Similarity chart (T4). The analysis confirmed the tendency observed in the Embedding Space; as Fig. 5C shows, compared to Model (GA), the changes in Model (SalUn) were concentrated in the final layers. Beyond confirming the shallow update pattern, the chart also reveals transition boundary we term the Elbow Layer  (Fig. 12)  . By comparing the CKA similarity between the original (T2.1) and retrained models (T2.2), they identified the elbow as the layer where similarity for the retain classes is lowest just before similarity for the forget class begins to diverge sharply (e.g., layer3.1 in ResNet-18). This indicates that layers preceding the elbow learn general, foundational features, while subsequent layers specialize in more output-centric features required for class discrimination. This finding offers principled guideline for setting the value of in methods that selectively modify specific number of layers, such as EU-k and CF-k [28]. Motivated by the insight, they conducted an experiment: they froze all layers up to the Elbow Layer, re-initialized the subsequent layers, and then fine-tuned the model on the retain classes. This strategy resulted in approximately 30% faster convergence compared to standard fine-tuning baseline, confirming that the Elbow Layer serves as meaningful boundary for efficient unlearning. 6) Privacy Assessment (Finding 6): They employed the Attack Simulation view to assess each methods privacy beyond just the WCPS(T5). In the process, they discovered certain cases where unusually high confidence in the unlearned model made them more vulnerable to MIAs. For example, as shown in Fig. 13, after unlearning the bird class using GA, they identified multiple samples whose confidence was relatively low in the retrained model but abnormally high in Model (the unlearned model), suggesting they could be singled out by an attacker. In subsequent post-analysis of these high-risk samples, they found that while animalrelated features had been removed, certain non-animal features remained, leading the model to misclassify them as trucks with excessively high confidence. This suggests the need for subsequent step to recover the representations of the samples that were unintentionally damaged during the GA, thereby mitigating the privacy risks posed by these vulnerable misclassifications. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 12 TABLE ABLATION STUDY OF GU FOR TWO FORGET CLASSES, AUTOMOBILE AND DEER. TO REFLECT PRACTICAL EFFICIENCY IN CONSTRAINED SETTING, ALL VARIANTS RAN WITHIN 3% OF FULL-RETRAINING TIME. EACH ROW DEMONSTRATES HOW THE STEPS OF GU, GUIDED BY OUR VISUAL FINDINGS, INCREMENTALLY ENHANCE THE THREE MU PRINCIPLES. THE LAST ROW, SHOWING GU, ACHIEVES STRONG PERFORMANCE ACROSS ALL PRINCIPLES. IT YIELDS PARTICULARLY SIGNIFICANT IMPROVEMENT IN THE WCPS FOR THE AUTOMOBILE CLASS, DIFFICULT CASE WHERE THE RETRAINED MODEL CONSISTENTLY MISCLASSIFIES IT AS TRUCK WITH HIGH CONFIDENCE. Forget: automobile Forget: deer Method UA RA TUA TRA RT(s) WCPS UA RA TUA TRA RT(s) WCPS Retrain (Gold Standard) GA + FT (Baselines) GATuned + FT Re-Init + GATuned + FT Re-Init + GATuned + FTGuided (GU) 0.000 0.012 0.005 0.000 0.000 1.000 0.982 0.987 0.990 0. 0.000 0.018 0.006 0.000 0.000 0.954 0.921 0.928 0.933 0.938 3524.7 104.3 91.8 71.8 76.6 1.000 0.429 0.467 0.559 0.743 0.000 0.013 0.005 0.000 0.000 1.000 0.990 0.995 0.995 0. 0.000 0.014 0.004 0.000 0.000 0.952 0.935 0.942 0.943 0.944 3513.5 103.7 91.8 72.3 76.6 1.000 0.663 0.737 0.751 0.806 B. Improvement Stage: Developing Novel MU Method Based on the findings from the analysis stage, the experts implemented novel hybrid MU method, Guided Unlearning (GU). It consists of three stages: Warm-Up, Forgetting, and Recovery, where the stages respectively perform targeted reinitialization, tuned gradient ascent, and guided fine-tuning. The Warm-Up stage is performed once, while the Forgetting and Recovery stages are alternated for the given epochs. Warm-Up Stage. GU begins by copying the forget class dataset and relabeling its samples using the second-highest logit from the original model. This step aims to approximate the retrained models behavior (Finding 1), while alleviating the mismatch between predicted proportions and confidence that arises from random labeling (Finding 3). Additionally, motivated by the Elbow Layer the experts identified (Finding 5), the method re-initializes all subsequent high-level layers. This targeted reset would suppress predictions of the forget class, while earlier layers are preserved to retain essential lowlevel representations. single epoch of fine-tuning is then performed to stabilize the re-initialized layers; this mitigates initial loss spikes in the subsequent Forgetting stage. Forgetting Stage. To counter the tendency of label-only methods to alter only the final layers (Finding 4), this stage applies tuned gradient ascent (GATuned). Informed by Finding 2, we use large batch size and learning rate, accumulating all gradients for single update to minimize side effects on the retain classes. Recovery Stage. Subsequently, the recovery stage performs guided fine-tuning (FTGuided) using mixture of the relabeled forget class samples from the Warm-Up stage and the samples from the retain classes. By guiding the prediction with relabeled forget samples and recovering representations through fine-tuning on the retain classes, the method mitigates the overconfidence on irrelevant labels (Finding 6), thereby enhancing privacy and aligning the models prediction patterns closer to those of retrained model. We found GU outperforms the state-of-the-art method by achieving higher accuracy (Fig. 14A), competitive efficiency, and the highest WCPS of 0.913 among all tested methods (surpassing the previous high of 0.876 achieved by SCRUB). Furthermore, pairwise comparisons (T2.2) confirm that GU closely matches the retrained models behavior, specifically its prediction patterns (Fig. 14B), and embeddings (Fig. 14C). Fig. 14. Comparison of GU with other methods: (A) GU (Model B) achieves lower accuracy on the forget class (deer, green bar), and higher accuracy on most retain classes compared to SCRUB (Model A). (B) GU (Model B)s prediction patterns closely resemble the retrained model (Model A). (C) GU (Model B)s feature embeddings exhibit similar structure to the retrained model (Model A). The incremental performance improvements of each step are systematically validated in our ablation study under an efficiency-constrained setting (Table I). VI. EXPERT FEEDBACK In addition to the case study, we interviewed four MU researchers who did not participate in the design process (E1 E4), with each having over year of experience in the MU domain. Each interview session lasted 7090 minutes and included brief system introduction (20 minutes), hands-on session (up to 30 minutes), and concluding discussion on the systems utility, limitations, and potential research directions (up to 20 minutes)."
        },
        {
            "title": "Benefits of",
            "content": "the System. All researchers acknowledged the need for Unlearning Comparator in their research and agreed that its design aligns with the unique characteristics of the domain. Specifically, E2 recognized the importance of pairwise comparisons, stating that As machine unlearning research often involves continuous model comparisons, focusing on two methods at time would be more effective than JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 13 either examining all methods together or analyzing them one by one. E3 highlighted the systems alignment with actual research workflows and the utility, remarking that It seems to bring researchers close to completing their evaluation pretty quickly, covering roughly 80% of the necessary steps, and particularly highlighting the Attack Simulation view, adding that running actual attacks helps us identify vulnerable data points, which could inform more privacy-preserving MU methods. E4 pointed out the difficulty of managing the variety of metrics in MU evaluation purely in numeric form, noting that visualizing these decision factors in single integrated view would significantly reduce cognitive load. Suggestions for Improvement. E1 and E3, both working in unlearning of classification models, offered suggestions on extending the systems capabilities. E1 explained that it is difficult to monitor whether certain over-forgotten instances are being shifted to different classes, but the Embedding Space interactions made it simpler to track each samples changing predictions. Drawing from this, E1 suggested that visualizing embeddings from lower-level layers might offer useful insights. While we currently focus on the penultimate layer for efficient iterative workflow given the computational cost of processing high-dimensional lower-level layers, visualizing these layers could be valuable option as computational efficiency improves. Also, we note that such 2D projections of high-dimensional data should be interpreted with caution, as projections can sometimes distort the original high-dimensional structure. In terms of dataset scalability, E3 noted that the current system is optimized for datasets with ten classes and suggested that supporting more classes would facilitate dataset expansion and tasks like sub-class unlearning. We acknowledge this scalability challenge as promising future direction. VII. LIMITATIONS AND FUTURE WORK Additionally, we gathered insights from both the researchers and existing literature regarding our systems limitations and future research directions. Supporting Different MU Tasks. Our interface currently focuses on class-wise unlearning, but researchers have asked about broader scenarios such as multi-class unlearning, instance-wise unlearning, or unlearning in generative models. In particular, unlearning for generative models requires handling knowledge distributed across outputs rather than discrete classes. Extending the interface to handle these would pose new design challenges. Improving Privacy Evaluation. Quantifying privacy of MU methods remains challenging. Defining privacy score and accurately estimating it in practice is still an open problem; this includes the distribution-level indistinguishability approach we adopted. Also, while our system focuses on blackbox MIAs, we could consider scenarios where the attacker has access to the model weights (i.e., white-box attacks). Such an assumption would enable the evaluation of broader threats including model-inversion attacks [21] or attacks on the unlearning process itself through malicious unlearning requests [66]. Incorporating these scenarios may refine privacy evaluation and support more comprehensive assessments of MU methods. Evaluation Without Retrained Models. Our current approach assumes access to retrained model for comparison. In large-scale tasks such as LLM unlearning [16], however, retraining may be infeasible or prohibitively expensive. To address this concern, we could draw inspiration from the TOFU benchmark [67]. It asks GPT-4 to generate fake author profiles, fine-tunes pre-trained LLM on them as the original model, applies unlearning to produce the unlearned model, and uses the pre-trained LLM (without fine-tuning) as the retrained model. Exploring design spaces that evaluate MU methods without fully retrained model can be an important direction for future research. inspect class-, VIII. CONCLUSION We present Unlearning Comparator, visual analytics system designed to compare different MU methods, guided by our design study with MU researchers and the tasks identified therein. Our workflow helps researchers build and screen candidate models, instance-, and layer-level behavior, and perform privacy checks via attack simulation. Through case study with MU experts, we conducted visual analysis of prominent MU methods, deriving findings that informed novel MU method. This new method demonstrated improved adherence to the MU principles of accuracy, efficiency, and privacy. In addition, interviews with four MU researchers highlighted that pairwise comparisons reduced cognitive load and helped them detect critical samples more efficiently."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (RS2019-II190421, Artificial Intelligence Graduate School Program (Sungkyunkwan University)) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00221186). This work was partly supported by Institute for Information & communication Technology Planning & evaluation (IITP) grants funded by the Korean government MSIT: (RS-2022-II220688 and RS-2021II212068)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Cao and J. Yang, Towards making systems forget with machine IEEE, unlearning, in 2015 IEEE symposium on security and privacy. 2015, pp. 463480. [2] J. Rosen, The right to be forgotten, Stan. L. Rev. Online, vol. 64, p. 88, 2011. [3] C. J. Hoofnagle, B. Van Der Sloot, and F. Z. Borgesius, The european union general data protection regulation: what it is and what it means, Information & Communications Technology Law, vol. 28, no. 1, pp. 6598, 2019. [4] R. W. Singer and M. P. Goodyear, Ftc orders destruction of algorithms created from unlawfully acquired data, The Journal of Robotics, Artificial Intelligence & Law, vol. 4, 2021. [5] Y. Bengio et al., International AI Safety Report, arXiv preprint arXiv:2501.17805, 2025. [6] H. Touvron, T. Lavril, G. Izacard et al., LLaMA: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 14 [7] J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu, Model sparsity can simplify machine unlearning, Advances in Neural Information Processing Systems, vol. 36, pp. 51 58451 605, 2023. [8] C. Fan, J. Liu, Y. Zhang, E. Wong, D. Wei, and S. Liu, SalUn: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation, arXiv preprint arXiv:2310.12508, 2023. [9] X. F. Cadet, A. Borovykh, M. Malekzadeh, S. Ahmadi-Abhari, and H. Haddadi, Deep Unlearn: Benchmarking machine unlearning, arXiv preprint arXiv:2410.01276, 2024. [10] M. Chen, W. Gao, G. Liu, K. Peng, and C. Wang, Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 77667775. [11] S. Goel, A. Prabhu, A. Sanyal, S.-N. Lim, P. Torr, and P. Kumaraguru, Towards adversarial evaluations for inexact machine unlearning, arXiv preprint arXiv:2201.06640, 2022. [12] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, Certified data removal from machine learning models, in ICML, 2020, pp. 3832 3842. [13] A. Becker and T. Liebig, Evaluating machine unlearning via epistemic uncertainty, arXiv preprint arXiv:2208.10836, 2022. [14] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. Kankanhalli, Zeroshot machine unlearning, IEEE Transactions on Information Forensics and Security, vol. 18, pp. 23452354, 2023. [15] E. Triantafillou et al., NeurIPS 2023 - Machine Unlearning, https:// kaggle.com/competitions/neurips-2023-machine-unlearning, 2023, kaggle. [16] S. Liu, Y. Yao, J. Jia et al., Rethinking machine unlearning for large language models, Nature Machine Intelligence, pp. 114, 2025. [17] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, Membership inference attacks against machine learning models, in 2017 IEEE symposium on security and privacy (SP). IEEE, 2017, pp. 318. [18] M. Fredrikson, S. Jha, and T. Ristenpart, Model inversion attacks that exploit confidence information and basic countermeasures, in Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, 2015, pp. 13221333. [19] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, When machine unlearning jeopardizes privacy, in Proceedings of the 2021 ACM SIGSAC conference on computer and communications security, 2021, pp. 896911. [20] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, Membership inference attacks from first principles, in 2022 IEEE symposium on security and privacy (SP). IEEE, 2022, pp. 18971914. [21] H. Hu, S. Wang, T. Dong, and M. Xue, Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning, in 2024 IEEE Symposium on Security and Privacy (SP). IEEE, 2024, pp. 32573275. [22] J. Xu, Z. Wu, C. Wang, and X. Jia, Machine unlearning: Solutions and challenges, IEEE Transactions on Emerging Topics in Computational Intelligence, 2024. [23] T. T. Nguyen, T. T. Huynh, Z. Ren, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen, survey of machine unlearning, arXiv preprint arXiv:2209.02299, 2022. [24] H. Zhang, T. Nakamura, T. Isohara, and K. Sakurai, review on machine unlearning, SN Computer Science, vol. 4, no. 4, p. 337, 2023. [25] S. Sai, U. Mittal, V. Chamola, K. Huang, I. Spinelli, S. Scardapane, Z. Tan, and A. Hussain, Machine un-learning: An overview of techniques, applications, and future directions, Cognitive Computation, vol. 16, no. 2, pp. 482506, 2024. [26] A. Golatkar, A. Achille, and S. Soatto, Eternal sunshine of the spotless net: Selective forgetting in deep networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 93049312. [27] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, An empirical investigation of catastrophic forgetting in gradient-based neural networks, arXiv preprint arXiv:1312.6211, 2013. [28] M. Kurmanji, P. Triantafillou, J. Hayes, and E. Triantafillou, Towards unbounded machine unlearning, Advances in neural information processing systems, vol. 36, pp. 19571987, 2023. [29] H. Subramonyam and J. Hullman, Are we closing the loop yet? gaps in the generalizability of vis4ml research, IEEE Transactions on Visualization and Computer Graphics, vol. 30, no. 1, pp. 672682, 2023. [30] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau, Visual analytics in deep learning: An interrogative survey for the next frontiers, IEEE transactions on visualization and computer graphics, vol. 25, no. 8, pp. 26742693, 2018. [31] J. Wang, S. Liu, and W. Zhang, Visual analytics for machine learning: data perspective survey, IEEE transactions on visualization and computer graphics, vol. 30, no. 12, pp. 76377656, 2024. [32] A. Chatzimparmpas, K. Kucher, and A. Kerren, Visualization for trust in machine learning revisited: The state of the field in 2023, IEEE Computer Graphics and Applications, vol. 44, no. 3, pp. 99113, 2024. [33] C. Chen et al., unified interactive model evaluation for classification, object detection, and instance segmentation in computer vision, IEEE TVCG, vol. 30, no. 1, pp. 7686, 2023. [34] M. Kahng, P. Y. Andrews, A. Kalro, and D. H. Chau, ActiVis: Visual exploration of industry-scale deep neural network models, IEEE Transactions on Visualization and Computer Graphics, vol. 24, no. 1, pp. 8897, 2018. [35] Z. J. Wang, R. Turko, O. Shaikh, H. Park, N. Das, F. Hohman, M. Kahng, and D. H. P. Chau, CNN Explainer: Learning convolutional neural networks with interactive visualization, IEEE Transactions on Visualization and Computer Graphics, vol. 27, no. 2, pp. 13961406, 2020. [36] C. Yeh, Y. Chen, A. Wu, C. Chen, F. Viegas, and M. Wattenberg, AttentionViz: global view of transformer attention, IEEE Transactions on Visualization and Computer Graphics, vol. 30, no. 1, pp. 262272, 2023. [37] A. Cho, G. C. Kim, A. Karpekov, A. Helbling, Z. J. Wang, S. Lee, B. Hoover, and D. H. P. Chau, Transformer Explainer: Interactive learning of text-generative models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 28, 2025, pp. 29 62529 627. [38] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, Grad-CAM: Visual explanations from deep networks via gradient-based localization, in Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. [39] A. Boggust, V. Sivaraman, Y. Assogba, D. Ren, D. Moritz, and F. Hohman, Compress and compare: Interactively evaluating efficiency and behavior across ml model compression experiments, IEEE Transactions on Visualization and Computer Graphics, 2024. [40] X. Xuan, X. Zhang, O.-H. Kwon, and K.-L. Ma, VAC-CNN: visual analytics system for comparative studies of deep convolutional neural networks, IEEE Transactions on Visualization and Computer Graphics, vol. 28, no. 6, pp. 23262337, 2022. [41] R. Sevastjanova, E. Cakmak, S. Ravfogel, R. Cotterell, and M. ElAssady, Visual comparison of language model adaptation, IEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 11781188, 2022. [42] D. Jeon, W. Jeung, T. Kim, A. No, and J. Choi, An information theoretic evaluation metric for strong unlearning, 2024. [Online]. Available: https://arxiv.org/abs/2405. [43] S. Seo, D. Kim, and B. Han, Revisiting machine unlearning with dimensional alignment, in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025, pp. 32063215. [44] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images.(2009), 2009. [45] H. Xiao, K. Rasul, and R. Vollgraf, Fashion-MNIST: novel image dataset for benchmarking machine learning algorithms, arXiv preprint arXiv:1708.07747, 2017. [46] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [47] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [48] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ImageNet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248255. [49] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, Privacy risk in machine learning: Analyzing the connection to overfitting, in 2018 IEEE 31st computer security foundations symposium (CSF). IEEE, 2018, pp. 268282. [50] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes, ML-Leaks: Model and data independent membership inference attacks and defenses on machine learning models, arXiv preprint arXiv:1806.01246, 2018. [51] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. Kankanhalli, Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 6, 2023, pp. 72107217. JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX [52] M. Ding, R. Sharma, C. Chen, J. Xu, and K. Ji, Understanding fine-tuning in approximate unlearning: theoretical perspective, 2025. [Online]. Available: https://arxiv.org/abs/2410.03833 [53] T. Moreau, M. Massias, A. Gramfort, P. Ablin, P.-A. Bannier, B. Charlier, M. Dagreou, T. Dupre la Tour, G. Durif, C. F. Dantas et al., Benchopt: Reproducible, efficient and collaborative optimization benchmarks, Advances in Neural Information Processing Systems, vol. 35, pp. 25 40425 421, 2022. [54] T. maintainers and contributors, Torchvision: Pytorchs computer vision library, https://github.com/pytorch/vision, 2016. [55] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, On calibration of modern neural networks, in International conference on machine learning. PMLR, 2017, pp. 13211330. [56] F. Hohman, K. Wongsuphasawat, M. B. Kery, and K. Patel, Understanding and visualizing data iteration in machine learning, in Proceedings of the 2020 CHI conference on human factors in computing systems, 2020, pp. 113. [57] J. Huang, A. Mishra, B. C. Kwon, and C. Bryan, ConceptExplainer: Interactive explanation for deep neural networks from concept perspective, IEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 831841, 2023. [58] B. Alper, B. Bach, N. Henry Riche, T. Isenberg, and J.-D. Fekete, Weighted graph comparison techniques for brain connectivity analysis, in Proceedings of the SIGCHI conference on human factors in computing systems, 2013, pp. 483492. [59] J. Zhao, Z. Liu, M. Dontcheva, A. Hertzmann, and A. Wilson, Matrixwave: Visual comparison of event sequence data, in Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 2015, pp. 259268. [60] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, Similarity of neural network representations revisited, in International conference on machine learning. PMLR, 2019, pp. 35193529. [61] J. Bernstein, A. Vahdat, Y. Yue, and M.-Y. Liu, On the distance between two neural networks and the stability of learning, Advances in Neural Information Processing Systems, vol. 33, pp. 21 37021 381, 2020. [62] L. McInnes, J. Healy, and J. Melville, UMAP: Uniform manifold approximation and projection for dimension reduction, arXiv preprint arXiv:1802.03426, 2018. [63] L. Van der Maaten and G. Hinton, Visualizing data using t-SNE. Journal of machine learning research, vol. 9, no. 11, 2008. [64] M. Wattenberg, F. Viegas, and M. Hardt, Attacking discrimination with smarter machine learning, Google Research, vol. 17, 2016. [65] T. Shaik, X. Tao, H. Xie, L. Li, X. Zhu, and Q. Li, Exploring the landscape of machine unlearning: comprehensive survey and taxonomy, IEEE Transactions on Neural Networks and Learning Systems, vol. 36, no. 7, pp. 11 67611 696, 2025. [66] C. Zhao, W. Qian, R. Ying, and M. Huai, Static and sequential malicious attacks in the context of selective forgetting, Advances in Neural Information Processing Systems, vol. 36, pp. 74 96674 979, 2023. [67] P. Maini, Z. Feng, A. Schwarzschild, Z. C. Lipton, and J. Z. Kolter, TOFU: task of fictitious unlearning for llms, arXiv preprint arXiv:2401.06121, 2024. Jaeung Lee is an M.S. student in the Department of Computer Science and Engineering at Sungkyunkwan University. He received his B.S. degree in Computer Science and Engineering from Sungkyunkwan University in 2024. His research interests lie in visualization for machine learning, with particular focus on how interactive interfaces can support model evaluation and facilitate humancentered decision-making. Suhyeon Yu received the B.S. in computer science and engineering from Sungkyunkwan University in 2024. He is currently an M.S. student in the Department of Computer Science at Rice University. His interests include data visualization and cloud computing. Yurim Jang is an M.S. student in the Department of Artificial Intelligence at Sungkyunkwan University. She received her Bachelor of Science degree in Industrial Engineering from Hongik University in 2024. Her research interests are in machine unlearning and computer vision. Simon S. Woo is an Associate Professor with the the College of Computing and Informatics, Sungkyunkwan University. He received the B.S. degree in electrical engineering from the University of Washington, the M.S. degree in electrical and computer engineering from the University of California, San Diego, and the M.S. and Ph.D. degrees in computer science from the University of Southern California. His research interests include AI security and deepfake detection. Jaemin Jo received the B.S. and Ph.D. degrees in computer science and engineering from Seoul National University, Seoul, South Korea, in 2014 and 2020, respectively. He is currently an Associate Professor with the College of Computing and Informatics, Sungkyunkwan University, Korea. His research interests include human-computer interaction and large-scale data visualization."
        }
    ],
    "affiliations": [
        "Rice University, Houston, TX, USA",
        "Sungkyunkwan University, Suwon, Korea"
    ]
}