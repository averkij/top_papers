{
    "paper_title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
    "authors": [
        "Hanrong Ye",
        "Haotian Zhang",
        "Erik Daxberger",
        "Lin Chen",
        "Zongyu Lin",
        "Yanghao Li",
        "Bowen Zhang",
        "Haoxuan You",
        "Dan Xu",
        "Zhe Gan",
        "Jiasen Lu",
        "Yinfei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel \"Memory Pointer Prompting\" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding."
        },
        {
            "title": "Start",
            "content": "MM-EGO: TOWARDS BUILDING EGOCENTRIC MULTIMODAL LLMS Hanrong Ye1, Haotian Zhang2, Erik Daxberger2, Lin Chen2, Zongyu Lin3, Yanghao Li2, Bowen Zhang2, Haoxuan You2, Dan Xu1, Zhe Gan2, Jiasen Lu2, Yinfei Yang2 1CSE, HKUST 3UCLA 2Apple 4 2 0 2 ] . [ 1 7 7 1 7 0 . 0 1 4 2 : r Figure 1: We introduce foundation model for egocentric video understanding, contributing from three key perspectives: (a) data engine that can automatically transform human narrations into 7 million egocentric QA samples, (b) multimodal language model designed for egocentric video comprehension, and (c) the curation of challenging egocentric video understanding benchmark."
        },
        {
            "title": "ABSTRACT",
            "content": "This research aims to comprehensively explore building multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is lack of QA data for egocentric video understanding, we develop data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on humanannotated data. This is currently the largest egocentric QA dataset. Second, we contribute challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models ability in recognizing and memorizing visual details across videos of varying lengths. We introduce new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose specialized multimodal architecture featuring novel Memory Pointer Prompting mechanism. This design includes global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Study on egocentric videos explores how machines can see and understand the world from firstperson, self-centered perspective. Egocentric videos differ significantly from static-camera videos, such as movies or animations, both in terms of content and viewpoint. The content of egocentric videos primarily revolves around human daily activities. These videos typically share perspective similar to human vision, where the camera and viewpoint frequently move. As result of these characteristics, egocentric videos exhibit distinct data distribution compared to static-camera videos, which has motivated new area of research. In recent years, research interest in egocentric intelligence has been on the rise (Sigurdsson et al., 2018; Damen et al., 2018; Grauman et al., 2022; Work done during an internship at Apple. First Authors. Senior Authors. 1 Mangalam et al., 2023; Plizzari et al., 2024). This growing interest is driven by the rapid advancements in AR/VR headsets and robotics, where cameras capture long-form egocentric videos in manner akin to human vision. Research on egocentric videos will allow these devices to understand their surroundings and human intentions, fostering more advanced machine intelligence and improving the human-machine interaction experience, with immeasurable research and application potential. However, research on understanding egocentric videos remains in its early stages, with previous research primarily centered on specialized tasks such as story summarization (Lee et al., 2012), handobject relationship understanding (Cai et al., 2016), action classification (Cartas et al., 2017; Li et al., 2021), and temporal or spatial grounding (Grauman et al., 2022). In contrast, works focusing on developing more general egocentric video understanding model capable of complex understanding remain rare. Despite that video multimodal large language models (MLLMs) demonstrate strong video understanding and reasoning ability (Zhang et al., 2023; Wang et al., 2024; Lin et al., 2024; Zhang et al., 2024b), most of these works are unsuitable for egocentric video understanding from data, benchmark, and model design perspectives. (a) From data standpoint, although many MLLMs use some egocentric videos from ActivityNet (Yu et al., 2019), Ego4D (Grauman et al., 2022), and Charades (Sigurdsson et al., 2018) in their training, they have not been trained on large-scale egocentric video datasets, which inherently restricts their ability to comprehend lengthy first-person videos and accurately extract visual details. While Ego4D (Grauman et al., 2022) offers valuable human-annotated videos and labels for certain egocentric video understanding tasks, particularly episodic memory (which assesses models ability to retain visual details in such videos), its annotations are not structured for generating language responses, making them unsuitable for training MLLMs. Therefore, large-scale egocentric video QA corpus is still needed. (b) In terms of benchmarking, exisiting video QA benchmarks either focus on shorter videos such as EgoSchema (Mangalam et al., 2023) and QaEgo4D, which evaluate using around 3-minute and 8-minute videos, respectively or concentrate on Internet video content (e.g., Video-MME (Fu et al., 2024)). This creates notable gap in egocentric video understanding benchmarks that encompass videos ranging from seconds to an hour in length. (c) From model design perspective, previous video MLLMs have primarily addressed long videos in two ways. The first approach involves uniformly sampling limited number of video frames as visual input, as seen in Li et al. (2024); Lin et al. (2024). Despite its simplicity, this approach achieves better performance among open-source models on public video benchmarks (Fu et al., 2024), largely because its design ensures high training efficiency and good scaling properties. The second approach involves feeding large volume of visual tokens into the transformer backbone and employing engineering techniques, such as tensor parallelism and sequence parallelism (Xue et al., 2024; Zhang et al., 2024a), to facilitate training with millions of visual tokens in context. However, these longcontext transformers suffer from slow training speeds and small overall batch sizes, which hinder performance improvements given the constraints of computational resources and training time. Intuitively, even humans cannot remember every detail of an hour-long video. We believe more effective approach is to understand the video progressively: first get an overview of the entire video, then focus on specific details with particular questions in mind. Building on the observations mentioned above, we introduce MM-Ego, an egocentric MLLM designed to process and understand long egocentric videos. Our contributions are threefold: (i) Data. To scale training data for MLLMs with egocentric understanding ability, we develop an efficient data engine, using narration to egocentric QA strategy, to automatically synthesize large-scale egocentric QA dataset based on video narration data. Notably, rather than relying on existing vision-language models as labelers, we generate egocentric QAs based on the humanannotated fine-grained video clip narrations. This approach ensures that our data quality is not constrained by the limitations of existing vision-language labeling models. In this way, we create the first large-scale egocentric QA dataset, consisting of over 7 million egocentric QA samples that span video lengths from seconds to over an hour. This dataset enables the training of models to recognize and retain visual details from egocentric videos. (ii) Benchmark. To evaluate the MLLMs performance in understanding and memorizing visual details from egocentric videos, we propose the EgoMemoria benchmark. This challenging benchmark includes 7,026 multiple-choice questions for 629 egocentric videos ranging from 30 seconds to 1 hour. In the experiments on EgoMemoria, we further investigate the impact of inevitable language 2 Figure 2: Narration to Egocentric QA data engine. Given sequence of human-annotated video narrations, we instruct language model (GPT-4o) to generate egocentric understanding-related questions and answers, along with identifying the key frames necessary to answer those questions. biases across different models during evaluation and introduce debiased metric to more accurately assess the models true egocentric understanding capabilities. (iii) Model. For our MM-Ego model, we develop progressive approach to handle egocentric videos by introducing Memory Pointer Prompting method. It consists of two steps: global glimpse and fallback. In the global glimpse step, we extract compressed frame-level visual embeddings from the entire video to get global understanding. Then, we employ memory pointer embedding, designed to examine all compressed frame-level visual embeddings along with the question embeddings, to aid in identifying key visual embeddings in question-aware manner. In the following fallback step, the selected key visual embeddings, in higher-resolution form, are then used as final input to the LLM for processing and generation. This approach allows us to achieve global understanding of the entire video while also identifying and utilizing key visual information to answer questions related to visual details."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 NARRATION TO EGOCENTRIC QA DATA ENGINE As outlined in Section 1, high-quality egocentric QA pairs are lacking for training an MLLM with egocentric video understanding ability. To address this gap, we develop an innovative narration to egocentric QA data engine that automatically generates episodic memory-related QA samples based on human-annotated video clip narrations from the Ego4D dataset (Grauman et al., 2022) without the need for additional manual annotations. Our approach leverages over 3,000 hours of privacy-protected, de-identified egocentric videos accompanied by more than 3 million high-quality, human-created narrations. These fine-grained language descriptions provide rich resource for generating QA pairs. The workflow of the data engine is illustrated in Figure 2. By organizing sequential video clips {Clip 1, Clip 2, ..., Clip N} and their corresponding narrations {Narration 1, Narration 2, ..., Narration N} in proper chronological order, we create comprehensive narration paragraphs that describe entire video sequences. We then employ powerful text-only language model, i.e., GPT4-o, to generate diverse and confident QA pairs related to episodic memory based on these narration paragraphs. The language model is instructed to attach the index of the narration sentence upon which each QA pair is based. This indexing allows us to map each QA pair back to the corresponding time frames in the original videos, enabling the extraction of key frame information crucial for subsequent model training. Figure 3: Video length distribution in our egocentric QA dataset. 3 Applying this data engine to the extensive Ego4D dataset allows us to efficiently scale the creation of egocentric QA data. We partition the dataset into training and testing sets according to the official Ego4D episodic memory task. The egocentric QA dataset provides more than 7 million QA samples in 938K multi-turn conversations. The data encompasses videos of varying durations, ranging from 30 seconds to 1 hour, as illustrated in Figure 3. To ensure comprehensive coverage and prevent bias towards shorter videos, we balance the number of conversations across different video lengths in training. This is the first large-scale egocentric QA dataset featuring videos of such extended ranges of duration. Through these steps, our narration to egocentric QA data engine addresses the scarcity of largescale, high-quality egocentric QA data for egocentric scenes, and sets solid foundation for building MM-Ego, sophisticated egocentric MLLM, which we introduce in the following section."
        },
        {
            "title": "2.2 MM-EGO MODEL",
            "content": "Our modeling goal is to develop an MLLM for handling egocentric videos, which are lengthy and rich in visual details. On the one hand, frame-level information is necessary to capture the full content of the video, as skipping frames during sampling could result in significant loss of visual details. On the other hand, processing all visual tokens generated by the visual encoder is computationally challenging for the transformer model. For instance, if each image is encoded into 729 visual embeddings (tokens), the total number of visual embeddings for 300-frame video would be 218,700. However, most MLLMs are trained with context length of less than 10,000 tokens (Li et al., 2024). Taking these factors into account, we introduce the MM-Ego model, which is built for handling large volume of egocentric video frames while maintaining manageable computational costs within the transformer backbone. MM-Ego introduces an innovative Memory Pointer Prompting mechanism, which operates in two main steps: global glimpse and fallback. We will introduce the details of MM-Ego in the following sections. 2.2.1 VISUAL AND TEXTUAL EMBEDDING Given an input video and the question, the first step is to embed them into visual and textual embeddings separately for later processing. We begin by uniformly sampling the video into up to frames, where can be in the range of hundreds. Then, we extract per-frame visual feature maps from these frames using robust vision encoder, SigLIP-so400m (Zhai et al., 2023). Following the method outlined by Li et al. (2024), we apply 2-layer MLP to project the visual feature maps to the LLM embedding space and use average pooling to reduce the height and width of the visual feature maps by factor of two and flatten the height and width dimension, resulting in relatively high-resolution visual embeddings {Vi RT C, [1, ]} where is the embedding length and is the embedding dimension. For the textual embedding, since we use Qwen2 (Yang et al., 2024) as the LLM, we use its tokenizer and embedding layer to transform the input text into textual embedque RTqC, dings. For question q, we denote the corresponding textual question embedding as {Eq [1, Q]} where is the total number of questions and Tq is the embedding length of question q. 2.2.2 MEMORY POINTER PROMPTING As processing all high-resolution visual embeddings with the LLM is computationally difficult, we propose to identify key visual embeddings in question-aware manner and only send those selected embeddings to the subsequent LLM. Inspired by previous works on Pointer Networks (Vinyals et al., 2015; Merity et al., 2016), we propose Memory Pointer Prompting mechanism, which is illustrated in Figure 4. Memory Pointer Prompting consists of two steps during inference: global glimpse and fallback. In the global glimpse step, key visual embeddings are identified from all frame-level embeddings, guided by the context of the question. During the subsequent fallback step, the important visual embeddings are selected, and their higher-resolution versions are provided to the LLM transformer backbone for further processing and language response generation. Global Glimpse Step. We begin by compressing the visual embeddings through average pooling along the embedding length dimension, resulting in set of compressed visual embeddings {Ei vis R1C, [1, ]}. Next, we introduce learnable memory pointer prompt embedding R1C, 4 vis for [1, ], with the question embeddings E1 Figure 4: (a) Overview of the proposed Memory Pointer Prompting mechanism. Its inference consists of two steps: (1) Global Glimpse: We concatenate the compressed visual embeddings from all frames, denoted as Ei que and the memory pointer embedding P1. This combined embedding sequence is then input into the LLM. From the last layer, we extract embeddings and compute the dot product between the memory pointer embedding and all compressed visual embeddings to generate the correlation scores. The indices of the frames with the top scores are selected. During training, the correlation scores are supervised by ground-truth key frame indices via binary cross-entropy loss. (2) Fallback: The high-resolution visual embeddings corresponding to the selected indices are fed into the LLM along with the question embeddings for final processing and response generation. (b) Illustration of LLM input sequence during training. duplicate it times, yielding {Pi R1C, [1, Q]}, and concatenate the embeddings as follows: [E1 vis, vis, ..., EN vis, E1 que, P1]. Here = 1 as MLLMs generate answers for only one question at time. In this way, the question embedding is followed by pointer embedding, which will be used to identify key visual embeddings with knowledge of the question embedding. The entire embedding sequence is then fed into the LLM, from which we obtain the output embedding sequence of the final layer: [E 1 vis, 2 vis, ..., vis , 1 que, 1]. We extract and stack the processed visual embeddings {E matrix Evis RN C. We conduct softmax dot product operation between Evis and vis R1C, [1, ]} to obtain the 1: = Softmax(Evis 1T) RN . (1) Here is correlation score vector indicating the correlation between the question and each frame. Balancing Exploration and Exploitation. Our approach to selecting key visual embeddings parallels the principles of Bayesian Optimization (Frazier, 2018), where the objective function is expensive to evaluate. In such cases, its important to balance exploration (sampling in areas where the uncertainty is high) and exploitation (sampling in areas where the surrogate model predicts high performance). However, relying solely on the aforementioned Memory Pointer Prompting may lead to overemphasizing certain areas of interest, potentially undermining the exploration process. To mitigate this issue, we introduce perturbations into the score distribution by incorporating uniform sampling distribution. The probability vector of uniform sampling can be written as: ui = (cid:26)α if linspace(0, N, k), 0 otherwise. (2) Here α is an explore-exploit balancing parameter to adjust the probability distribution. We overlap the probability vector of uniform sampling and score matrix s: + u. (3) We then identify the top-k indices as the set {Si, [1, k]}. In this way, we find the key visual embeddings in question-aware manner. 5 Table 1: Distribution of videos and QA samples with different lengths. Class Minutes Videos QAs Short Medium Long 0.5-1 100 500 1-2 100 2-4 100 987 4-10 100 997 10-20 100 1715 20-40 100 1792 40-60 29 537 Sum - 629 7026 Table 2: Distribution of correct options in MCQs. Option Counts 1776 1751 1770 1729 B Fallback Step. During inference, as shown in Figure 4, with the set of indices {Si, [1, k]} for the selected visual embeddings, we now assemble the LLM input sequence as follows: [ VS1 , VS2 , ..., VSk (cid:124) (cid:125) (cid:123)(cid:122) Selected Top-k Visual Embeedings , E1 que]. As previously introduced, VS1, VS2, ..., VSk denote the selected top-k high-resolution visual embeddings, which provide more visual details than the compressed visual embeddings. This new embedding sequence is fed into the LLM to generate the final language response. In summary, the proposed Memory Pointer Prompting approach allows us to consider the full scope of video information while filtering out redundant data in the LLM transformer, ensuring computational efficiency. The new input serves as the final input of the LLM to generate the language response given the visual and textual information. Training Procedure. Given the novel design of MM-Ego, its training procedure is different from popular MLLMs (Liu et al., 2023). Specifically, let the answer embedding for question [1, Q] be denoted as Eq ans, then the input embedding sequence during the training process is represented as: [ E1 vis, E2 (cid:124) vis, ..., EN vis (cid:125) (cid:123)(cid:122) Compressed Visual Embeddings , E1 que, P1, ..., EQ que, PQ, VS1 , VS2 , ..., VSk (cid:124) (cid:125) (cid:123)(cid:122) Selected High-Res Visual Embeddings , E1 que, E1 ans, ..., EQ que, EQ ans]. We also provide simplified illustration (where = 1) of the input embedding sequence structure during training in Figure 4. Here, we begin by inputting the compressed visual embeddings for all frames, followed by the question embedding and memory pointer embedding. Next, we integrate the selected high-resolution visual embeddings (based on the ground-truth key frame labels), and finally, incorporate both the question and answer embeddings. Once the input sequence is prepared as outlined above, we can train MM-Ego similarly to traditional large language models. The compressed visual embeddings, question embedding, and memory pointer embeddings used as prefixes do not contribute to the language cross-entropy loss. When training on samples from our curated egocentric QA dataset where there are ground-truth key frame labels for each question, we compute the correlation score vector in the global glimpse step, and supervise it using binary cross-entropy loss. For training samples that lack ground-truth key frame labels, we omit the prefixes, which results in the traditional MLLM training process."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In the experiment section, we will first present new egocentric video understanding benchmark, specifically designed to assess episodic memory capabilities. Following this, we will perform comprehensive experiments to evaluate MM-Ego, utilizing both the newly introduced benchmark and existing public benchmarks. 3.1 EGOMEMORIA BENCHMARK To evaluate the performance of egocentric MLLMs, especially in terms of episodic memory ability, we propose new benchmark called EgoMemoria. Specifically, we generate memory-related questions and answers from human-annotated narrations in the validation set of the Ego4D dataset. To ensure diversity, for each video we only generate limited number of questions. We divide the videos into Figure 5: The most frequently occurring verbs and nouns in EgoMemoria. 6 Figure 6: EgoMemoria QAs visualization and prediction analysis of the global glimpse step. We find high consistency between the identified key frames and the questions, demonstrating the effectiveness of the proposed Memory Pointer Prompting method. The visualized correlation scores show distinct distributions for different questions given the same video, indicating its question-specific nature. The indicates that the selected frames are relevant to the questions. seven different length ranges: 0.5 to 1 min, 1 to 2 min, 2 to 4 min, 4 to 10 min, 10 to 20 min, 20 to 40 min, and 40 to 60 min. We aim to balance the number of samples in different video lengths. The distribution of videos and corresponding question-answer pairs (QAs) for each category is shown in Table 1. Furthermore, we group these video lengths into three broader categories: short (0.5 to 2 min), medium (2 to 20 min), and long (20 to 60 min). In total, we collect 629 videos with 7,026 questions. The most frequently occuring verbs and nouns in the questions are visualized in Figure 5. Since free-form answers are typically evaluated using closed-source LLM as judge, the evaluation can be inconsistent and subject to significant variance, especially due to model version updates. To ensure more reliable, standardized, and consistent performance evaluation, we convert the free-form answers into multiple-choice questions (MCQs), which helps reduce score instability. In practice, based on the free-form answer, we instruct ChatGPT to generate three additional choices that are plausible but incorrect, considering the original question and answer. We then randomize the order of these choices to achieve uniform distribution of correct options, as shown in Table 2, to minimize bias in option placement. We visualize some randomly sampled examples in Figure 6. 7 Table 3: Performance comparison and language bias analysis of different models on the EgoMemoria benchmark. Our MM-Ego model demonstrates the best performance both before and after excluding the language bias of different models. Method LLaVA-OV (Li et al., 2024) Short Medium Long Avg Ego SFT Short Medium Long Avg MM-Ego Short Medium Long Avg 70.24 Original Exclude LLaVA-OV Bias 56.44 Exclude Ego SFT Bias 55.75 Exclude MM-Ego Bias 47.41 Mean Debiased Accuracy (MDA) 53.20 64.94 49.64 49.27 42.11 47.01 61.19 65.45 79.06 44.83 50.30 66.37 45.21 50.08 61.73 35.22 41.58 50.60 41.76 47.32 59.56 76.34 64.15 59.59 46.39 56. 73.51 76.30 79.96 60.03 63.52 71.97 54.50 58.61 67.70 40.38 45.79 49.80 51.64 55.97 63.16 79.64 70.68 66.33 49.11 62.04 79.09 79.56 68.15 70.26 63.89 65.97 43.81 47.58 58.62 61."
        },
        {
            "title": "3.2 EXPERIMENTAL SETUP",
            "content": "Training Data. We employ joint image-video supervised fine-tuning (SFT) strategy. To enhance the models capability in understanding broader range of visual data, we combine our egocentric QA dataset with variety of multimodal datasets. We curate an SFT dataset mixture consisting of our egocentric QA dataset, Ego4D narration dataset (Grauman et al., 2022), LLaVA-NeXT SFT collection (including ChartQA (Masry et al., 2022), AI2D (Hiippala et al., 2021), DocVQA (Mathew et al., 2021), DVQA (Kafle et al., 2018), COCO (Lin et al., 2014)), ShareGPT4V (Chen et al., 2023a), synthdog-en (Kim et al., 2021)), ShareGPT-4o (Chen et al., 2023b), ALLaVA instruct (Chen et al., 2024a), ShareGPT4Video (Chen et al., 2024b), sherlock (Hessel et al., 2022), ScienceQA (Lu et al., 2022), NExT-QA (Xiao et al., 2021), and ActivityNet-QA (Yu et al., 2019). Implementation Details. The model is trained for one epoch with base learning rate of 1 105, using cosine scheduler. The batch size is set to 128. We sample maximum of 300 frames (N = 300) and select 32 visual embeddings in the proposed memory pointer prompting mechanism. By default, we set the explore-exploit balancing parameter α to 0.1. Pretrained Models. Our MM-Ego model is initialized from LLaVA-OV 7B (Li et al., 2024), state-of-the-art MLLM known for its good performance on general multimodal understanding tasks. Following the same architecture, we use the SigLip-so400M ViT (Zhai et al., 2023) as the visual encoder for embedding video frames and Qwen2-7B (Yang et al., 2024) as the LLM architecture. 3.3 MAIN RESULTS We first conduct experiments on our EgoMemoria benchmark, primarily comparing three models: LLaVA-OV (Li et al., 2024), its fine-tuned version using our MM-Ego SFT data mixture (referred to as Ego SFT), and our MM-Ego model, which incorporates the proposed Memory Pointer Prompting mentioned in Section 2.2.2. We show the EgoMemoria accuracy in the first row of Table 3. We observe significant improvement in the models performance on egocentric QAs after training on our MM-Ego data mixture, attributed to the rich egocentric knowledge provided by our curated egocentric QA training data. Moreover, leveraging the MM-Ego model architecture further enhances performance, thanks to the effective Memory Pointer Prompting mechanism. However, we notice that the original overall performance metrics are higher than anticipated, raising curiosity about the extent to which language bias contributes to the models accuracy. To answer this question, we conduct additional experiments aimed at eliminating these language biases. Specifically, we test the three model variants on the EgoMemoria benchmark without any visual inputs, identifying questions that could be correctly answered without videos as language-biased questions. Then, we evaluate the models performance on the subset of the benchmark without language-biased questions. For fairness, we apply this debiasing process across all three models so that they are evaluated on the same sets of data. We calculate the mean accuracy of the debiased variants, referred to as the Mean Debiased Accuracy (MDA). The results are presented in Table 3. As expected, after removing the language-biased questions, the accuracy of all three models drops significantly to more reasonable level. The performance decline is notably more pronounced in the Medium and Long classes compared to the Short class. For example, the average accuracy of LLaVA-OV across the three classes (short, medium, and long) drops from 65.45 to 47.32. The decrease in the Short class is 17.04, in the Medium class is 17.93, and in the Long class is 19.43. Despite this, we still observe improvements in MDA after training with SFT data generated by our MM-Ego data engine (+8.65) and applying our Memory Pointer Prompting method (+13.95). These results demonstrate the effectiveness of our approach even after considering language bias. 8 Table 4: Comparison with state-of-the-art video MLLMs. MM-Ego shows strong performance on egocentric understanding and competitive performance on Internet video understanding. Method EgoMemoria (MDA) Short Medium Long Avg EgoSchema Full Video-MME (w/o subs) Short Medium Long Entire 64.31 GPT-4o LLaVA-NeXT-Video-7B-DPO (Zhang et al., 2024b) 30.38 LLaVA-NeXT-Video-32B-Qwen (Zhang et al., 2024b) 43.78 53.20 LLaVA-OV 7B (Li et al., 2024) 63.16 MM-Ego (ours) 59.47 25.95 33.76 47.01 62.04 57.65 60.48 21.49 25.94 31.04 36.19 41.76 47.32 58.62 61.27 72.2 - 60.85 60.10 69.03 80.00 - - 69.30 67.60 70.30 - - 56.00 55. 65.30 - - 49.40 47.80 71.90 - 60.20 58.30 57.00 Table 5: MDA on EgoMemoria when inferring with different numbers of frames. Our MM-Ego model shows smaller relative drop on average when decreasing the number of sampled frames. Frames 32 16 8 4 Rel. Diff Short Medium Long Avg LLaVA-OV Ego SFT MM-Ego LLaVA-OV Ego SFT MM-Ego LLaVA-OV Ego SFT MM-Ego LLaVA-OV Ego SFT MM-Ego 53.20 52.68 50.76 50.43 5.20% 59.56 60.45 59.59 55.36 7.07% 63.16 63.82 62.22 62.30 1.36% 47.01 46.37 44.82 42.54 9.49% 56.71 55.99 54.55 52.08 8.16% 62.04 60.81 58.23 58.44 5.81% 41.76 40.12 39.41 38.88 6.89% 58.62 51.64 58.16 51.15 55.19 49.11 48.40 54.65 6.26% 6.77% 47.32 46.39 44.99 43.95 7.12% 55.97 55.86 54.42 51.95 7.19% 61.27 60.93 58.55 58.46 4.59% To better understand the capability of MM-Ego, we compare its performance with state-of-the-art video MLLMs on EgoMemoria and prevalent large-scale video QA benchmarks, including the long egocentric video understanding benchmark EgoSchema (Mangalam et al., 2023) and the Internetvideo-based long-video understanding benchmark Video-MME (Fu et al., 2024). The results are shown in Table 4. On EgoMemoria, GPT-4o is evaluated using 32 uniformly sampled frames from the videos, while other models follow their respective official inference settings. The MDA on EgoMemoria is computed using the debiased subsets used in Table 3. Notably, MM-Ego exhibits the highest performance on EgoMemoria, particularly in the Medium and Long classes. On the EgoSchema benchmark, our model achieves substantial performance gain of +8.18 over the previous state-of-the-art open-source model, underscoring the effectiveness of both our data and model design for egocentric understanding. Additionally, on the challenging Internet video understanding Video-MME benchmark, our model is on par with the leading model of similar parameter size. These results showcase MM-Egos capability in egocentric video understanding while preserving its general video comprehension abilities. 3.4 MODEL ANALYSIS Quantitative Analysis of Different Numbers of Frames. To evaluate the influence of sampling different numbers of frames for different models, we calculate the mean debiased accuracy (MDA) in Table 5. The relative performance drop from sampling 32 frames to sampling 4 frames is also calculated. As expected, all models exhibit decrease in performance with fewer sampled frames. Notably, MM-Ego exhibits smaller average performance drop when the number of frames is reduced due to its ability to identify key frames given lower computational budget. The relative performance drop in the short category is considerably smaller compared to the medium and long categories, likely because shorter videos require fewer frames to comprehend. Figure 7: MDA scores with different α values for explore-andexploit balancing. Qualitative Analysis of Memory Pointer Prompting. In Figure 6, we present qualitative analysis of the accuracy of Memory Pointer Prompting on EgoMemoria. We randomly select samples and visualize the key frames identified by the global glimpse step in Memory Pointer Prompting. The results show strong alignment between the questions and the selected frames. In failure cases, we observe that the issues are often due to the ambiguity of the questions, causing the model to struggle with accurately localizing the key visual embeddings. Furthermore, the visualized correlation scores during the global glimpse step show distinct patterns across various videos and questions, confirming its effectiveness in selecting key visual embeddings tailored to the specific questions. Quantitative Analysis of Explore-Exploit Balancing Parameter α. As discussed in Section 2.2.2, we design an explore-exploit balancing parameter α to fuse the uniform distribution and the sam9 Figure 8: Real-world conversation examples generated by MM-Ego. The input is 2-minute long egocentric video recorded using camera on an off-the-shelf wearable device. MM-Ego can accurately identify key visual details and provide correct answers to the users memory-related questions. pling probability computed by Memory Pointer Prompting. We illustrate MM-Egos performance with varying values of α in Figure 7. The results show that α = 0.1 achieves the best performance, while larger or smaller values of α tend to either over-explore or over-exploit. Conversation Examples by MM-Ego. In Figure 8, we show real-world demo of MM-Ego, where the input video is 2-minute long egocentric video captured by camera on an off-the-shelf wearable device (this video is not used in our dataset). MM-Ego is able to correctly answer the episodic memory-related questions given the egocentric video, despite the difference in data domain."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Multimodal Large Language Models. Recent advancements in Large Language Models (OpenAI, 2023; Touvron et al., 2023) have sparked significant interest in developing Multimodal Large Language Models (MLLMs) that combine the language understanding capabilities of LLMs with multi-modal perception abilities (Alayrac et al., 2022; Dai et al., 2023; Zhu et al., 2023; McKinzie et al., 2024). For video-based MLLMs, most works follow structure akin to image-based MLLMs. To handle the large volume of video frames, some methods reduce the number of frames (Zhang et al., 2023; Wang et al., 2024; Maaz et al., 2024; Xu et al., 2024), which results in the loss of many visual details. Others extend the LLMs context length by employing parallel techniques (Xue et al., 2024), but this often leads to low training efficiency. Unlike these approaches, our method preserves global awareness of the entire video, allows for attention to visual details, and is efficiently trainable. Egocentric Video Understanding. While the growing field of egocentric video understanding is still in its infancy, there have been many influential works. For comprehensive overview of egocentric vision please refer to Plizzari et al. (2024). On the data/benchmark side, representative works include Ego4D (Grauman et al., 2022), Ego-Exo4D (Grauman et al., 2024), and EPIC-KITCHENS100 (Damen et al., 2018). When also considering language, prior work on egocentric video-language benchmarks include QaEgo4D (Barmann & Waibel, 2022) and EgoSchema (Mangalam et al., 2023). For understanding long egocentric videos, prior modeling efforts include GroundVQA (Di & Xie, 2024), Encode-Store-Retrieve (Shen et al., 2023), and R-VLM (Xu et al., 2023). However, most previous works focus on classic video understanding tasks such as activity recognition and temporal grounding, and hence they do not involve language model. In contrast, we propose to develop multimodal LLM to tackle open-ended egocentric video-language understanding."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Limitation and Future Work. While MM-Ego demonstrates strong ability in egocentric understanding, there is still room for further improvement. On the data and benchmark side, we can introduce more diverse egocentric understanding corpus. For the model itself, we plan to enhance its capacity to process larger number of frames, such as at the order of thousands, to better handle longer or even always-on egocentric videos. Conclusion. In this paper, we make three key contributions towards the development of egocentric foundation model: the creation of the first large-scale egocentric QA training dataset, the introduction of novel model designed for effective long egocentric video comprehension, and the establishment of the EgoMemoria benchmark for assessing models ability to capture visual details from egocentric videos. We hope that these efforts will benefit further research on egocentric MLLMs."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our proposed method does not involve the creation or introduction of any new video content. All generated data is derived from publicly available, privacy-protected datasets (Grauman et al., 2022). The data is intended exclusively for academic research purposes and will not be used for any commercial applications. We have adhered to ethical standards by ensuring that no private or sensitive data has been used or compromised."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide detailed explanation of the data synthesis process in our data engine in Section 2.1. We also elaborate on our model design in Section 2.2.2. Additionally, we outline the implementation details, including the training hyperparameters in Section 3.2."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. Leonard Barmann and Alex Waibel. Where did leave my keys?-episodic-memory-based question answering on egocentric videos. In CVPR-W, 2022. Minjie Cai, Kris Kitani, and Yoichi Sato. Understanding hand-object manipulation with grasp types and object attributes. In Robotics: Science and Systems, volume 3, 2016. Alejandro Cartas, Juan Marın, Petia Radeva, and Mariella Dimiccoli. Recognizing activities of daily living from egocentric images. In Pattern Recognition and Image Analysis, 2017. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv, 2024a. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv, 2023a. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. arXiv, 2024b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv, 2023b. 11 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv, 2023. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In ECCV, 2018. Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In CVPR, 2024. Peter Frazier. tutorial on bayesian optimization. arXiv, 2018. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv, 2024. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1938319400, 2024. Jack Hessel, Jena Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: Dataset for Visual Abductive Reasoning. In ECCV, 2022. Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661 688, 2021. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document understanding transformer without ocr. arXiv, 2021. Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Discovering important people and objects for egocentric video summarization. In CVPR, 2012. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv, 2024. Yanghao Li, Tushar Nagarajan, Bo Xiong, and Kristen Grauman. Ego-exo: Transferring visual representations from third-person to first-person videos. In CVPR, 2021. 12 Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 2022. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2024. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS Datasets and Benchmarks Track, 2023. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proc. WACV, 2021. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv, 2016. OpenAI. ChatGPT: Optimizing language models for dialogue. https://openai.com/blog/ chatgpt, 2023. Accessed: 2023. Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Giovanni Maria Farinella, Dima Damen, and Tatiana Tommasi. An outlook into the future of egocentric vision. IJCV, pp. 157, 2024. Junxiao Shen, John Dudley, and Per Ola Kristensson. Encode-store-retrieve: Enhancing memory augmentation through language-encoded egocentric perception. arXiv, 2023. Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: large-scale dataset of paired third and first person videos. In arXiv, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv, 2023. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. NeurIPS, 28, 2015. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv, 2024. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In CVPR, 2021. Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, and Yan Lu. Retrieval-based video language model for efficient long video question answering. arXiv, 2023. 13 Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv, 2024. Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv, 2024. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering. In AAAI, 2019. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv, 2023. URL https://arxiv.org/abs/2306. 02858. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv, 2024a. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv, 2023."
        },
        {
            "title": "A MORE ANALYSIS OF MEMORY POINTER PROMPTING",
            "content": "To further assess the effectiveness of MM-Ego and the proposed Memory Pointer Prompting mechanism, we present additional visual results of key frame identification during the global glimpse step in Figure 9. MM-Ego demonstrates the capability to extract relevant visual information from large set of frames based on the given questions. Figure 9: More key frame identification results of the global glimpse step on EgoMemoria. We find high relevance between the identified key frames and the questions, demonstrating the effectiveness of the proposed Memory Pointer Prompting method. The indicates that the selected frames are relevant to the questions."
        }
    ],
    "affiliations": [
        "Apple",
        "CSE, HKUST",
        "UCLA"
    ]
}