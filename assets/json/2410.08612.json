{
    "paper_title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting",
    "authors": [
        "Purushothaman Natarajan",
        "Kamal Basha",
        "Athira Nambiar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 2 1 6 8 0 . 0 1 4 2 : r Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting Purushothaman Natarajan*, Kamal Basha, Athira Nambiar Department of Computational Intelligence, SRM Institute of Science and Technology, Kattankulathur, Tamil Nadu, 603203, India c30945@srmist.edu.in, c58527@srmist.edu.in, athiram@srmist.edu.in"
        },
        {
            "title": "Abstract",
            "content": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/ simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time to the best of our knowledge. Synth-SONAR achieves state-ofthe-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism. 1. Introduction Sound Navigation and Ranging (SONAR) technology is an essential component of underwater exploration and object detection, with extensive applications in areas such as anti-submarine warfare, mine detection, submarine navigation, and torpedo guidance. It serves both civilian and military purposes, playing crucial role in ensuring safety and operational efficiency in challenging underwater environments. Sonar operates by emitting sound waves that travel through water, reflect off objects, and are analyzed upon return to determine the location, size, and shape of underwater objects [46]. Sonar images are complex, consisting of the target, target shadow, and reverberation background regions [25]. Further, the underwater environment adds challenges like turbulence, noise, and low resolution, making underwater image analysis practically difficult [1]. Publicly available sonar datasets [16, 39, 53] often face challenges like low resolution, poor feature representation, and limited object diversity. Their scarcity is worsened by the need for expert labeling, security issues, and data sensitivity. To overcome these limitations, simulation-based studies [37, 40, 21] have been explored in the literature. However, these approaches still face limitations, including time-consuming manual modeling, the complexity of integrating various tools, and insufficient diversity in generated sonar data. [10], style transfer To this end, machine learning (ML)/ deep learning (DL) techniques have been employed for efficient sonar image synthesis [17, 47, 20, 18] in the recent years. Building on the success of generative AI (GenAI) in various fields e.g. medical imaging [41, 29], autonomous vehicles [51] similar approaches such as Generative Adversarial etc. [9] and Denoising Networks (GANs) Diffusion Probabilistic Models (DDPMs) [13] have been adopted in sonar image synthesis to tackle data scarcity and improve model accuracy. Nonetheless, significant challenges remain, including insufficient object diversity, poor fine-grained feature preservation, complexities such as target shadows and reverberation effects, and high computational demands for real-time sonar image synthesis. Additionally, there exists semantic gap between domain experts and machine learning models in interpreting and explaining the sonar characteristics in human-compliant way. In this paper, we propose novel sonar image synthesis framework i.e. Synth-SONAR to overcome the aforementioned limitations. Synth-SONAR generates high-quality, realistic sonar images by dint of advanced GenAI techniques i.e. text-conditioned diffusion models and GPT promting. The workflow of Synth-SONAR consists of three In the first phase, large-scale, diverse corpus phases. of sonar data by integrating publicly available sonar images, S3 Simulator [37], and style-injected sonar images captioned using CLIP [34] based vision-language models to capture high-level semantic features is generated. The next phase involves training Denoising Diffusion Probabilistic Model (DDPM) [13], fine-tuned with LoRA (LowRank Adaptation) [14] and integrated with GPT [4] based prompts to generate coarse-level sonar images. Finally, in the final phase, the coarse images are refined into finegrained outputs using domain-specific language instructions processed through Vision-Language Model (VLM), further enhanced with LoRA fine-tuning and GPT for content precision. Our approach achieves high degree of diversity and realism, as demonstrated through extensive qualitative and quantitative analysis, via metrics such as Frechet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Inception Score (IS). The key contributions of the paper are as follows: novel GenAI framework i.e.Synth-SONAR for sonar image synthesis, incorporating dual-stage textconditioned diffusion models for high-quality, multiresolution image generation. One of the most extensive and diverse sonar image datasets through the integration of multiple sources (real, simulated, and GenAI) and detailed annotations. An innovative approach that enhances image generation techniques by utilizing Denoising Diffusion Probabilistic Models (DDPM) combined with LoRA and GPT-based prompts for controlled and high-quality realistic sonar image synthesis, thus making our approach interpretable. 2. Related Work 2.1. Underwater Sonar image analysis Earlier sonar image analysis used traditional Machine learning (ML) techniques such as the Markov random field (MRF) model with the scale causal multigrid (SCM) algorithm [26] and undecimated discrete wavelet transform (UDWT) combined with PCA and k-means clustering [6]. Advancements in deep learning (DL) introduced methods like FS-UTNet [44], framework for underwater target detection using few-shot learning. Further, techniques such as RotNet, Denoising Autoencoders, and Jigsaw [33] facilitated learning representations for sonar image classification without large labeled datasets. EsonarNet [12], lightweight vision transformer network, is designed for efficient segmentation. The Global Context External-Attention Network (GCEANet) provides zero-shot classification in [3]. YOLOv7 improves high-precision object detection by integrating Swin-Transformer and Convolutional Block Attention Module (CBAM)[45]. EfficientNet is used as backbone for feature extraction in [2], which uses dualchannel attention mechanisms (SE and ECA) and modified BiFPN for multi-scale feature fusion. Additionally, DSA-Net for underwater object detection [23] used dual spatial attention network (DSAM), and Generalized Focal Loss (GFL) for optimized object detection. To enhance interpretability, LIME and SP-LIME have been employed [27] to make sonar image classification more transparent and understandable. 2.2. Synthetic Data Generation via Generative AI Synthetic data generation via GenAI addresses data scarcity using generative AI techniques like diffusion models [5], GANs [11], and VAEs [19]. CAD-based methods include using Unreal Engine (UE) [40] to create sonar images with diverse seabed conditions and objects, and the S3Simulator dataset [37], which leverages advanced simulation techniques, Segment Anything Model (SAM), and tools like SelfCAD and Gazebo for 3D modeling. Augmentation techniques feature the Seg2Sonar network [15], which uses spatially adaptive denormalization (SPADE), Skip-Layer channel-wise Excitation (SLE), and weight adjustment (WA) modules. GAN-based methods include an enhanced CycleGAN [54] model that improves underwater image contrast through depth-oriented attention mechanism, the CBL-sinGAN network [32] which combines sinGAN with Convolutional Block Attention Module (CBAM) for target image augmentation. Further, SIGAN [31] used multi-scale GAN for super-resolution of sonar images. Diffusion-based approaches are exemplified by method that uses diffusion models for synthetic image generation and augmentation in [48, 49] and an enhanced YOLOv7 model that integrates denoising-diffusion model [45], Vision Transformer (ViT) for high-precision object detection in side-scan sonar images. 2.3. Vision-Language Models (VLMs) and Their"
        },
        {
            "title": "Application in the Sonar Domain",
            "content": "Vision-Language Models (VLMs) represent significant advancement in the field of artificial intelligence by bridging the gap between visual and textual data [52]. Multimodels like CLIP [34], DALL-E [35], BLIP [22], FLAVA [42], and GIT [30] represent significant advancements by integrating visual and textual data, enhancing image generation and understanding through multimodal learning. While VLMs have made strides in various domains, their application to sonar data is still in its nascent stage, with only very Figure 1. Overall Architecture of the proposed Synth-SONAR sonar image synthesis framework. few works in the sonar domain e.g. VALE [28], which combines VLM techniques with sonar data for improved underwater environment analysis. 3. Methodology In this section, we provide detailed description of the proposed Synth-SONAR framework for generating sonar images. The overall architecture of the model is depicted in Fig. 1, which consists of three key phases. Phase 1 is the Data Acquisition Phase, as explained in Section 3.1. It entails the collection of real-world, CAD-simulated, and Gen-AI-generated images. Phases 2 and 3 utilize text-conditioned dual diffusion models and GPT-based prompting to synthesize both coarse and fine grained sonar images. In particular, Phase-2 customizes pre-trained diffusion models for generating sonar images as described in Section 3.2. Whereas, Phase-3 fine-tunes and generalizes the diffusion models for generating fine-grained sonar images as described in Section 3.3. 3.1. Phase-1: Data Acquisition Underwater sonar imagery is critical domain, wherein the data collection and processing of such large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section 4.1.1) or the CAD-based simulated dataset such as S3 simulator data (see Section 4.1.2). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section 3.1.1), as explained in the forthcoming section. Figure 2. Style Injection for sonar Image Synthesis. 3.1.1 Style Injection on Generated Images Style injection is technique in computer visions imageto-image tasks, that combines content and style features to generate new image, where the objective is to transform the content image (in our case, it is generated by the prompts from GPT) by infusing it with the stylistic elements of another image while preserving the original structure [7]. In this work, style injection is utilized to add more diversity to the real sonar data and to increase the availability of sonar data. Refering to Fig. 2, we leverage the generative capability of pre-trained large-scale model to generate content images and transfer sonar style information to the generated content images using pre-trained stable diffusion model, thereby resolving the issue of the traditional data-collection process. We employ Latent Diffusion Model (LDM) [36] to perform image synthesis by operating within lowdimensional latent space. This approach significantly reduces computational costs while maintaining focus on the semantic content of the data. For given image RHW 3, the encoder encodes the image into latent representation Rhwc, and the decoder reconstructs the image from this latent representation. The diffusion model is trained on the latent space z, where the task is to predict noise ϵ from the noised latent representation zt at given time step t. The corresponding training objective is: LLDM = Ez,ϵ,t (cid:2)ϵ ϵθ(zt, t, y)2 2 (cid:3) (1) where ϵ (0, 1) is the noise, is uniformly sampled from {1, . . . , }, is the conditioning variable (which could be style reference or text prompt), and ϵθ is neural network predicting the noise added to z. Generic Attention Mechanism: In Latent Diffusion Models (LDM), the attention mechanism is fundamental to both style injection in image-to-image tasks and text-toimage generation. The attention mechanism can be generically expressed as: = WQ(ϕ), = WK(ψ), = WV (ψ) (2) ϕout = Attn(Q, K, ) = softmax (cid:19) (cid:18) QK (3) where, query Q, key K, and value are the result of learned linear transformations via the projection layers WQ(), WK(), and WV (), respectively. The dimensionality of these projections is denoted by d, which is used in scaling the dot product between and in the attention mechanism. Also, ϕ represents the feature output from the residual block, while ψ is the conditioning variable, which can be the style features for image-to-image tasks or the text features for text-to-image tasks. Style Injection in Image-to-Image Tasks: For Style Injection, the conditioning variable ψ corresponds to the style features extracted from reference image. The goal of style injection is to blend the style features with the content features of the original image, transferring the style while maintaining the contents structure. The content features are represented as: Qc = WQ(ϕ) (4) where ϕ denotes the content features extracted from the image. The style features are extracted from the style image and projected into key and value pairs: Ks = WK(ϕs), Vs = WV (ϕs) (5) where ϕs represents the style features from the reference image. The attention mechanism used for injecting the style features into the content is given by: Qcs = γ Qc + (1 γ) Qcs out = Attn( Qcs ϕcs , , ) (6) (7) where γ is blending ratio that controls the amount of style injected. For sonar-specific style injection, referring to Fig. 2, the latent noise representations of the content and style images are denoted by zc and zs, respectively. These representations are obtained through the DDIM inversion process, which reconstructs both the content and style images into Gaussian noise at time step = . During the DDIM inversion process, we collect the query features Qc from the content and the key-value pairs from the style at each time step t. Once the inversion is completed, we initialize the stylized latent noise zcs by copying the content latent noise zc . To inject the style into the content, we blend the content and stylized queries using blending ratio γ from equation 6. The blended query Qcs is then passed through the attention mechanism along with the styles key and value features. By adjusting γ, we control the degree of style transfer, where higher γ retains more of the original content features, and lower γ strengthens the influence of the style features. This approach allows for precise control over the stylistic outcome, ensuring smooth transition between content preservation and style injection. , 3.2. Phase-2: Train and Generate sonar Images via DDPM and GPT-prompting Our objective is to generate sequence of sonar imIn ages that correspond to the given text conditioning. the case of text-to-image generation, referring to the equation 2, the conditioning variable ψ corresponds to the text features. The cross-attention mechanism aligns the textual description with the visual features to generate an image that matches the text prompt. Q2, K2, and V2 are analogous to the query, key, and value in the diffusion model explained earlier but are specific to Phase 2. In Phase 2, these components represent the following: Q2 = WQ(ϕ) represents the image features, K2 = WK(Text), and V2 = WV (Text) represent the text features. The attention mechanism for text-to-image generation is given by: ϕout = Attn(Q2, K2, V2) = softmax (cid:19) (cid:18) Q2K 2 V2 (8) where ϕout represents the output of the cross-attention layer, combining the image features with the text features. This interaction between the image and text features enables the generation of images conditioned on the provided text prompt. To train diffusion model from scratch, enormous computation and vast amount of training data are required. To mitigate the challenges of data scarcity and computation, we adopted LoRA [14] for fine-tuning. LoRA (LowRank Adaptation) applies to text-to-image tasks by efficiently fine-tuning models. Mathematically, LoRA updates the models weight matrices by adding low-rank decomposition = B, where Rmr and Rrn are low-rank matrices, where is the rank. LoRA updates the query, key, and value matrices in the following manner: 2 = Q2 + α WQ 2 = K2 + α WK2 2 = V2 + α WV2 (9) (10) (11) 2, 2 2, where, the LoRA-enhanced represent queries, keys, and values, where the original Q2, K2, and V2 are adjusted by adding the LoRA updates WQ2, WK2, WV2, scaled by factor α. The enhanced attention mechanism with LoRA is given by: out = Attn(Q ϕc 2, 2, 2 ) = softmax (cid:18) 2 2 d2 (cid:19) The attention mechanism, Attn(Q (12) 2 ), now uses the LoRA-enhanced queries, keys, and values to generate the final output ϕc out in text-to-image tasks. 2, 2, process involves optimizing the loss function for the diffusion model, where the loss is conditioned on both the image data and the text prompt: Lfine-tune = Ex,t,ϵ ϵ ϵθ(xt, tprompt)2(cid:105) (cid:104) (13) Here, xt represents the generated images, ϵθ is the noise predictor, and the fine-tuning process incorporates both the image data and sonar domain-specific textual instructions. This enhances the models ability to generate fine-grained sonar images based on more generalized prompts. 4. Experimental Setup 4.1. Dataset 4.1.1 Publicly Available Sonar Images In this work, publicly available datasets, specifically the Seabed Objects KLSG [16] and the sonar Common Target Detection Dataset (SCTD) [53], are utilized. The Seabed Objects KLSG dataset [16] includes 1,190 side-scan sonar images, featuring 385 shipwrecks, 36 drowning victims, 62 planes, 129 mines, and 578 seafloor images. Collected over ten years with the help of commercial sonar suppliers like Lcocean, Klein Martin, and EdgeTech, subset of 1,171 images (excluding mines) is publicly available for academic research. The sonar Common Target Detection Dataset (SCTD) 1.0 [53], developed by multiple universities, contains 596 images across 3 classes. Some sample images of the publicly available data are shown in Fig. 3. Seafloor Plane Ship Human 3.3. Phase-3: Tune & Generalize DDPM for Fine-"
        },
        {
            "title": "Grained sonar Image Generation",
            "content": "The fine-tuned model from Phase 2 can generate images only when the user provides custom object tag given during training. If the user fails to provide this tag, the model generates regular images instead of SONAR-specific images. To mitigate this limitation and generalize the model for generating fine-grained sonar images, we further train the model using the existing corpus of sonar images. In this phase, we generate images from the Phase 2 model using series of prompts obtained from GPT. These generated images, combined with domain-specific language instructions, are processed through Visual Language Model (VLM) to obtain low-level descriptions. These lowlevel descriptions are then fed back into GPT to generate high-level descriptions. The images and their corresponding low-level + high-level descriptions are subsequently used to fine-tune the stable diffusion model. The fine-tuning Figure 3. Samples from publicly available sonar images 4.1.2 S3 Simulator Images The S3Simulator [37] dataset is novel benchmark of simulated side-scan sonar images designed to overcome challenges in acquiring high-quality sonar data. Using advanced simulation techniques, the dataset accurately replicates underwater conditions and produces diverse synthetic sonar images, as illustrated in Fig. 4. Tools like the Segment Anything Model (SAM) and Gazebo are utilized for optimal object segmentation and visualization, enhancing the quality of data for AI model training in underwater object classification. 4.2. Evaluation Metrics We use Frechet Inception Distance (FID), Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio Ship Plane Manta Mine Cylindrical Mine Figure 4. Samples from S3 Simulator Dataset (PSNR), and Inception Score (IS) to evaluate the image-toimage generation task. FID(x, g) = µx µg2 + Tr(Σx + Σg 2(ΣxΣg)1/2) (14) where µx and µg are the means, and Σx and Σg are the covariance matrices of the real images and the generated images g. The term Tr denotes the trace of the matrix. FID measures the distance between the distributions of real and generated images in the feature space of pre-trained Inception model. 4.3. Implementation Details We train, develop, and experiment the proposed framework in stable diffusion 1.4 pre-trained on the LAION [38] dataset using NVIDIA RTX4000 with 20GB GPU, fine-tuned with sonar-specific images to tailor the synthesis to the target sonar domain. The diffusion model generates high-quality synthetic sonar images by conditioning on sonar-specific attribute styles in the case of image-to-image and domain-specific prompts in the case of text-to-image. The domain-specific prompts (low-level and high-level descriptions) are enhanced using GPT 3.5 Turbo at phase2 and in phase-3 VLM-LLaVA [24] and GPT-3.5 Turbo for improving the control over sonar image synthesis. For the classification task, we leverage transfer learning with several backbone models, including VGG16, ResNet50, DenseNet121, MobileNetV2, Xception, and InceptionResNetV2. The models pre-trained on large-scale ImageNet dataset [8] are adapted for sonar image classification with real dataset, synthetic dataset and combination of real and synthetic dataset. SSIM(x, y) = (2µxµy + c1)(2σxy + c2) + µ2 + c1)(σ + σ2 + c2) (µ2 (15) 5. Experimental Results where µx and µy represent the means of images and y, and σ2 σ2 are their variances, and σxy is the covariance between the two images. Constants c1 and c2 are used to avoid instability when the denominator is close to zero. SSIM assesses the perceived quality of images based on luminance, contrast, and structure. PSNR(x, y) = 10 log10 (cid:18) MAX2 (cid:19) MSE(x, y) (16) where MAX is the maximum possible pixel value of the image (e.g., 255 for an 8-bit image) and MSE(x, y) is the mean squared error between the images and y. PSNR is commonly used to measure the quality of reconstruction in image compression. IS(x) = exp (ExDKL(p(yx)p(y))) (17) where p(yx) is the conditional label distribution given an image x, and p(y) is the marginal label distribution. DKL represents the Kullback-Leibler divergence. IS evaluates the quality and diversity of generated images based on how well they are classified by pre-trained classifier. To evaluate the performance of the text-to-image generation task, R-Precision, Multimodal Distance [43], and Diversity can be used. However, since the sonar dataset lacks ground truth, the text-to-image sonar image generation model cannot be evaluated quantitatively; instead, qualitative analysis is facilitated. 5.1. Phase-1: Style-Injection Results 5.1.1 Style-Injection Quantitative Results The objective of style injection is to improve the quality and diversity of generated sonar images through style injection, ensuring that the generated images better reflect the unique characteristics of the dataset. To achieve this, style images are selected not randomly but via K-Means clustering on the real sonar dataset. One to three images from each cluster are chosen for style injection, which enhances the relevance and consistency of the stylized outputs. IS FID SSIM PSNR Class 1.05 15.022 0.300 4.13 Plane 1.04 10.759 0.392 6.15 Ship 1.12 12.410 0.452 1.12 Seafloor 1.07 12.730 0.381 3.8 Average Table 1. Image-to-Image Quantitative Metrics The generated images from Phase-1: Style Injection (Section 5.1.2) are quantitatively evaluated using FID, SSIM, PSNR, and IS scores, as shown in Table 1. Lower FID scores and higher SSIM and PSNR values indicate that the style injection method successfully maintains image quality, while the IS scores reflect the diversity of the generated images. From the results, the FID score is lowest for the Seafloor class (1.12), indicating the highest fidelity to the real data, while the Plane class has the lowest PSNR (15.022), suggesting relatively lower noise in image reconstruction. The SSIM values show that structural similarity is strongest for the Seafloor class (0.452), and the average across all classes indicates that the style injection improves both image quality and consistency. The IS scores across all classes are fairly similar, but they still indicate moderate level of diversity in the generated images. These results suggest that improvements in both image quality and diversity are due to effective style injection, which introduces stylistic variations while preserving key structural elements. 5.1.2 Style-Injection Qualitative Results The images generated using the style injection are depicted in Fig. 5. The content image is generated from the prompt developed using GPT, the actual image is from real sonar dataset, and the output is the stylized image from the proposed framework with the value of γ set to 0.5. The stylized output images retain the core structural elements necessary for accurate sonar image interpretation, such as the clear depiction of object boundaries and the representation of noise patterns that are characteristic of underwater acoustic imaging. This ensures that the model does not over-stylize the content, but instead enhances the aesthetic appeal while preserving the scientific accuracy of the generated outputs. Moreover, the generated images show clearer details and better feature separation, indicating that style injection with an attention mechanism helps the diffusion model distinguish foreground objects from background noise. This makes the images more similar to real sonar data while adding subtle artistic variations that enhance diversity. 5.1.3 Sonar Image Classification Model To support the quantitative measure of sonar image synthesis, classification task is carried out using the generated synthetic data. In particular, we developed classification models leveraging transfer learning on backbone models such as VGG16, ResNet50, DenseNet121, MobileNetV2, Xception, and InceptionResNetV2 With the real, synthetic, and real + synthetic dataset and the performance of the developed image classification model is assessed with real sonar dataset. Refer to Table 3 for the test accuracies of various models on real dataset. The model developed with fully synthetic dataset gave maximum accuracy of 79%, using the MobileNetV2 as base model. When using only real data, the DenseNet121 model reached the highest accuracy of 96%, and when using the real + synthetic dataset, DenseNet121 again achieved the best performance with 97%. This demonstrates that our approach can generate high-quality synthetic sonar data, and the inclusion of synthetic data slightly improves model performance on real datasets. CONTENT STYLE OUTPUT Figure 5. The Contents, Styles, and Stylized sonar Images (Outputs) in the style injection process Backbone Model VGG16 ResNet50 DenseNet121 MobileNetV2 Xception InceptionResNetV2 Real 95% 90% 96% 92% 95% 95% Synthetic 58% 29% 75% 79% 68% 48% Real + Synthetic 94% 82% 97% 95% 95% 94% Table 3. Test Accuracy on Real Dataset 5.2. Phase-2 & 3: Fine-Tuning Results Training or fine-tuning stable diffusion models typically demands extensive computational resources and large amounts of annotated data. However, due to computational constraints and limited access to experts for annotating generated images, we evaluated our framework as pilot study, utilizing dataset of 30 images per label. In total, the dataset consists of six different objects with 18 object variations across six seafloor environments used for model training. Image Prompt Low-Level Description High-Level Description Given the side scan sonar image and the caption: image of SH * on the AP *, where PL *, SH *, CYM*, ASF*, TCM* represent the objects in the image, and AS *, AP *, SEF* represent the background. The numbers following these abbreviations can range from one to five digits. Provide the following descriptions: low-level description focusing on simple details and objects visible in the image. high-level description interpreting the scene or conveying broader understanding based on the image and the given caption. The image shows an old shipwreck that has sunk into water near land, with the tail end pointing upwards. The sides are positioned halfway along the ships body, with the structure slightly off-center both horizontally and vertically. However, the ship is still identifiable as specific model type due to the SH33* designation. The image likely depicts shipwreck, designated SH33*, resting on seabed with the AP637* terrain as the background. The tail of the ship sticks out, indicating wreck. Table 2. Descriptions generated by the GPT in Phase 3. In this phase, images produced from Phase 2 are described using domain-specific prompt. The table includes four columns: (1) Image: the generated image from Phase 2, (2) Prompt: the domain-specific prompt used to generate the descriptions, (3) Low-Level Description: detailed description focusing on simple, observable details and objects in the image, and (4) High-Level Description: an interpretive description providing broader understanding of the scene depicted in the image. The abbreviations used in the prompt (e.g., PL *, SH *, CYM*, ASF*, TCM*, AS *, AP *, SEF*) represent various objects and background elements in the image, with numbers indicating different instances. Seafloor Mine Plane Ship γ = 0.9 γ = 0.75 γ = 0.5 γ = 0.3 Figure 6. Generated Samples from the fine-tuned model We qualitatively evaluated the performance of our model, with the generated images shown in Fig. 6 and Fig. 7. The model is trained using dual diffusion framework, comprising coarse and fine image generation phases. In the coarse phase (Phase-2), the model produces images, outlining the basic structure of the sonar scene using detailed and pre-determined prompts. As the training progresses into the fine phase (Phase-3), the model generates higher-resolution images using generic prompts and even multiple objects where applicable. The proposed hierarchical framework incorporates Visual Language Model (VLM) to generate descriptions that enhance the image generation process. As detailed in Table 2, the VLM supports GPT to provide both low-level descriptions (e.g., shape, size, texture of objects) and highlevel descriptions (e.g., object relationships and interactions with the environment) to refine the stable diffusion model. 5.3. Ablation study We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section 5.3.1, while the fine-tuning ablation study is detailed in Section 5.3.2. 5.3.1 Phase-1: Style-Injection (Image-to-Image) The blending ratio of content and style is controlled by the parameter γ. We conducted comparative study to examine how it impacts the quality of the image produced through style injection, as shown in Fig. 8. Figure 8. Style and Content Fidelity (γ); Lower values improve style fidelity but may reduce content fidelity. The generated images appear to be similar to each other. However, qualitative metrics such as SSIM and PSNR (refer to Table 4) were computed for images generated with γ = {0.9, 0.75, 0.5, 0.3} and found that as γ decreases, there is noticeable improvement in both SSIM and PSNR, indicating gradual enhancement in visual quality and fidelity. For our main model, we set γ = 0.5 as the default value, striking balance between preserving content and injecting stylistic information while avoiding excessive distortion or loss of essential content features. γ 0.9 0.75 0.5 0.3 SSIM PSNR 11.1372 0.2360 11.3949 0.2535 11.5774 0.2696 11.6498 0.2761 Table 4. SSIM & PSNR w.r.t. fidelity (γ) for the images in Fig. 8 5.3.2 Phase-2 & 3: Fine-Tuning (Text-toImage) The number of training steps significantly impacts the quality of the generated images. To verify the same, qualitative study on the outputs of the trained model (see Fig. 7) is carried out. It is observed that at lower step counts, the images tend to be coarse, with artifacts or incomplete object representations. Increasing the number of steps leads to sharper images with clearer object boundaries and better textures, allowing for more accurate and realistic depictions of underwater scenes. 500 1000 1500 3000 5000 7500 10000 12500 15000 Figure 7. Generated Images w.r.t training steps (Row-1: Number of Training steps, Row-2: Ship generated using the prompt image of SH34* ship on AP238* seabed, Row-3: Plane generated using the prompt Image of PL71* plane in sandy AS25* seabed, with broken tail and shadow extending to the front and left) For the ship category, the model started generating sonar images only after 3000 training steps. Between 5000 and 10000 training steps, the quality of the generated images was good. However, after 10000 training steps, the model failed to produce the expected output and instead generated multiple ships. In the context of planes, the given prompt Image of PL71* plane in sandy AS25* seabed, with broken tail and shadow extending to the front and left is relatively intricate. The model started generating sonar images after 1500 training iterations and consistently produced satisfactory outcomes until 5000 training iterations. However, after 6500 training iterations, the model yielded unsatisfactory outputs with multiple aircraft. In this setting, the output is influenced by the quality of the input dataset and the training stages. Due to computational constraints, advanced models like Stable Diffusion 2 and 3 were not explored in this study. The developed model has the capability to generate sonar images. However, it is uncertain how consistent the results are. In the experiments, it successfully produced images in 6 out of 10 trials, but sometimes the model hallucinates and leads to inaccurate responses. Some of such failure cases are shown in Fig. 9. Hence, in this pilot study, we develop proof of concept rather than comprehensive model. Figure 9. Failures from the trained model 5.4. State-of-the-art Comparison To the best of our knowledge, our work is the first work in text-conditioned image generation for underwatersonarimagery, and it lacks benchmark datasets, thereby hindering our ability to fully assess the models performance. However, to showcase the efficacy of the proposed architecture, we compared our work with existing similar imageto-image synthesis work and tabulated the results in the Table. 5. Our model outperforms the state-of-the-art models [50], [45],[48] in terms of image quality, with high SSIM & PSNR values and low FID value. Method Zhiwei et al.[50] Wen et al.[45] Yang et al.[48] Synth-SONAR (Ours) FID 138.56 0.2527 147.6 3. SSIM PSNR 11.1764 0.2512 - - 16.3 0.37 12.730 0.381 Table 5. State-of-the-art Comparison 6. Conclusion In this work, we present Synth-SONAR, novel framework for sonar image synthesis that leverages dual diffusion models and GPT prompting. Synth-SONAR overcomes traditional data collection challenges by creating large, diverse, and high-quality sonar dataset. The framework uniquely integrates Generative AI-based style injection with real and simulated data and utilizes dual textconditioning diffusion model to generate synthetic sonar images from both high-level (coarse) and low-level (detailed) prompts. This approach bridges the gap between textual descriptions and sonar image generation, achieving state-of-the-art performance in enhancing image diversity and realism. Future work could focus on expanding SynthSONAR across different underwater research applications. 7. Acknowledgements This work was partially supported by the Naval Research Board (NRB), DRDO, Government of India under grant number: NRB/505/SG/22-23."
        },
        {
            "title": "References",
            "content": "[1] Omar Almutiry, Khalid Iqbal, Shariq Hussain, Awais Mahmood, and Habib Dhahri. Underwater images contrast enhancement and its challenges: survey. Multimedia Tools and Applications, pages 126, 2024. 1 [2] PA Arjun, Suryanarayan, RS Viswamanav, Abhishek, and Anjali. Unveiling underwater structures: Mobilenet vs. efficientnet in sonar image detection. Procedia Computer Science, 233:518527, 2024. 2 [3] Zhongyu Bai, Hongli Xu, Qichuan Ding, and Xiangyue Zhang. Side-scan sonar image classification with zero-shot IEEE Transactions on Instrumentation and style transfer. and Measurement, 73:115, 2024. 2 [4] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 2 [5] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Li. survey on generative diffusion models. IEEE Transactions on Knowledge and Data Engineering, 2024. [6] Turgay Celik and Tardi Tjahjadi. novel method for sidesIEEE Journal of Oceanic can sonar image segmentation. Engineering, 36(2):186194, 2011. 2 [7] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87958805, 2024. 3 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [9] Leon Gatys. neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1 [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2 [12] Ju He, Hu Xu, Shaohong Li, and Yang Yu. Efficient sonarnet: Lightweight cnn grafted vision transformer embedding network for forward-looking sonar image segmentation. IEEE Transactions on Geoscience and Remote Sensing, pages 11, 2024. 2 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5 [15] Chao Huang, Jianhu Zhao, Hongmei Zhang, and Yongcan Yu. Seg2sonar: full-class sample synthesis method applied to underwater sonar image target detection, recognition, and segmentation tasks. IEEE Transactions on Geoscience and Remote Sensing, 62:119, 2024. 2 [16] Guanying Huo, Ziyin Wu, and Jiabiao Li. Underwater object classification in sidescan sonar images using deep transIEEE access, fer learning and semisynthetic training data. 8:4740747418, 2020. 1, 5 [17] Marija Jegorova, Antti Ilari Karjalainen, Jose Vazquez, and Timothy Hospedales. Full-scale continuous synthetic sonar data generation with markov conditional generative adversarial networks. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 31683174. IEEE, 2020. 1 [18] Yifan Jiang, Bonhwa Ku, Wanjin Kim, and Hanseok Ko. Side-scan sonar image synthesis based on generative adverIEEE sarial network for images in multiple frequencies. Geoscience and Remote Sensing Letters, 18(9):15051509, 2020. 1 [19] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [20] Sunmo Koo, Sangpil Youm, and Jane Shin. Cycle-gan-based synthetic sonar image generation for improved underwater classification. In Ocean Sensing and Monitoring XVI, volume 13061, pages 6983. SPIE, 2024. 1 [21] Sejin Lee, Byungjae Park, and Ayoung Kim. Deep learning from shallow dives: Sonar image generation and arXiv preprint training for underwater object detection. arXiv:1810.07990, 2018. 1 [22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 2 [23] Zikang Li, Zhuojun Xie, Puhong Duan, Xudong Kang, and Shutao Li. Dual spatial attention network for underwater object detection with sonar imagery. IEEE Sensors Journal, 24(5):69987008, 2024. 2 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [25] Max Mignotte, Christophe Collet, Patrick Perez, and Patrick Bouthemy. Three-class markovian segmentation of highresolution sonar images. Computer Vision and Image Understanding, 76(3):191204, 1999. 1 [26] M. Mignotte, C. Collet, P. Perez, and P. Bouthemy. Sonar image segmentation using an unsupervised hierarchical mrf model. IEEE Transactions on Image Processing, 9(7):1216 1231, 2000. 2 [27] Purushothaman Natarajan and Athira Nambiar. Underwater sonar image classification and analysis using limearXiv preprint based explainable artificial intelligence. arXiv:2408.12837, 2024. 2 [28] Purushothaman Natarajan and Athira Nambiar. Vale: multimodal visual and language explanation framework for image classifiers using explainable ai and language models, 2024. 3 [29] Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su imRuan, Qian Wang, and Dinggang Shen. Medical age synthesis with context-aware generative adversarial networks. In Medical Image Computing and Computer Assisted InterventionMICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20, pages 417425. Springer, 2017. [30] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. ImIn International conference on machine age transformer. learning, pages 40554064. PMLR, 2018. 2 [31] Chengyang Peng, Shaohua Jin, Gang Bian, and Yang Cui. Sigan: multi-scale generative adversarial network for underwater sonar image super-resolution. Journal of Marine Science and Engineering, 12(7):1057, 2024. 2 [32] Chengyang Peng, Shaohua Jin, Gang Bian, Yang Cui, and Meina Wang. Sample augmentation method for side-scan sonar underwater target images based on cbl-singan. Journal of Marine Science and Engineering, 12(3):467, 2024. 2 [33] Alan Preciado-Grijalva, Bilal Wehbe, Miguel Bande Firvida, and Matias Valdenegro-Toro. Self-supervised learning for sonar image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 14991508, June 2022. 2 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. 2 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 [37] Kamal Basha and Athira Nambiar. S3simulator: benchmarking side scan sonar simulator dataset for underwater image analysis, 2024. 1, 2, [38] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [39] Advaith Sethuraman, Anja Sheppard, Onur Bagoren, Christopher Pinnow, Jamey Anderson, Timothy Havens, and Katherine Skinner. Machine learning for shipwreck segmentation from side scan sonar imagery: Dataset and benchmark. The International Journal of Robotics Research, page 02783649241266853, 2024. 1 [40] Jane Shin, Shi Chang, Matthew Bays, Joshua Weaver, Thomas Wettergren, and Silvia Ferrari. Synthetic sonar image simulation with various seabed conditions for automatic target recognition. In OCEANS 2022, Hampton Roads, pages 18. IEEE, 2022. 1, 2 [41] Yasin Shokrollahi, Sahar Yarmohammadtoosky, Matthew Nikahd, Pengfei Dong, Xianqi Li, and Linxia Gu. comprehensive review of generative ai in healthcare. arXiv preprint arXiv:2310.00795, 2023. 1 [42] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: foundational language and vision In Proceedings of the IEEE/CVF Conalignment model. ference on Computer Vision and Pattern Recognition, pages 1563815650, 2022. 2 [43] Nitish Srivastava and Russ Salakhutdinov. Multimodal learning with deep boltzmann machines. Advances in neural information processing systems, 25, 2012. 6 [44] Qi Wang, YiXiao Zhang, Huanjin Wang, and Bo He. Multilevel feature representation framework with adaptive margin loss for few-shot sonar images classification of auvs. IEEE Transactions on Intelligent Vehicles, pages 113, 2024. 2 [45] Xin Wen, Feihu Zhang, Chensheng Cheng, Xujia Hou, and Guang Pan. Side-scan sonar underwater target detection: Combining the diffusion model with an improved yolov7 model. IEEE Journal of Oceanic Engineering, 49(3):976 991, 2024. 2, [46] Haesang Yang, Sung-Hoon Byun, Keunhwa Lee, Youngmin Choo, and Kookhyun Kim. Underwater acoustic research trends with machine learning: Active sonar applications. Journal of Ocean Engineering and Technology, 34(4):277 284, 2020. 1 [47] Zhiwei Yang, Jianhu Zhao, Yongcan Yu, and Chao Huang. sample augmentation method for side-scan sonar fullclass images that can be used for detection and segmentation. IEEE Transactions on Geoscience and Remote Sensing, 2024. 1 [48] Zhiwei Yang, Jianhu Zhao, Yongcan Yu, and Chao Huang. sample augmentation method for side-scan sonar fullclass images that can be used for detection and segmentation. IEEE Transactions on Geoscience and Remote Sensing, 62:111, 2024. 2, 9 [49] Zhiwei Yang, Jianhu Zhao, Hongmei Zhang, Yongcan Yu, and Chao Huang. side-scan sonar image synthesis method based on diffusion model. Journal of Marine Science and Engineering, 11(6):1103, 2023. 2 [50] Zhiwei Yang, Jianhu Zhao, Hongmei Zhang, Yongcan Yu, and Chao Huang. side-scan sonar image synthesis method based on diffusion model. Journal of Marine Science and Engineering, 11(6), 2023. 9 [51] Handi Yu and Xin Li. Intelligent corner synthesis via cycleconsistent generative adversarial networks for efficient validation of autonomous driving systems. In 2018 23rd Asia and South Pacific design automation conference (ASP-DAC), pages 915. IEEE, 2018. [52] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. IEEE Vision-language models for vision tasks: survey. Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [53] Peng Zhang, Jinsong Tang, Heping Zhong, Mingqiang Ning, Dandan Liu, and Ke Wu. Self-trained target detection of radar and sonar images using automatic deep learning. IEEE Transactions on Geoscience and Remote Sensing, 60:114, 2021. 1, 5 [54] Jianqun Zhou, Yang Li, Hongmao Qin, Pengwen Dai, Zilong Zhao, and Manjiang Hu. Sonar image generation by mfa-cyclegan for boosting underwater object detection of auvs. IEEE Journal of Oceanic Engineering, 49(3):905919, 2024."
        }
    ],
    "affiliations": [
        "Department of Computational Intelligence, SRM Institute of Science and Technology, Kattankulathur, Tamil Nadu, 603203, India"
    ]
}