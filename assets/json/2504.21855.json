{
    "paper_title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction",
    "authors": [
        "Qihao Liu",
        "Ju He",
        "Qihang Yu",
        "Liang-Chieh Chen",
        "Alan Yuille"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 5 8 1 2 . 4 0 5 2 : r ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction Qihao Liu1, Ju He1, Qihang Yu2, Liang-Chieh Chen2, Alan Yuille1 1Johns Hopkins University 2Independent Researcher https://revision-video.github.io/ equal advising Figure 1. By explicitly leveraging parameterized 3D physical model, ReVision enhances pre-trained video generation models (e.g., Stable Video Diffusion [6]) to produce high-quality videos with complex motion (row 1), enabling precise motion control (rows 2, 3) and accurate interactions (rows 4, 5). During inference, an optional target pose can be specified through rough sketch indicating the final position of body part (blue circles in rows 1, 3, 4) or by simple drag operation (blue arrows in row 2)."
        },
        {
            "title": "Abstract",
            "content": "In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, video diffusion model is used to generate coarse video. Next, we extract set of 2D and 3D features from the coarse video to construct 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms state-of-the-art video generation model with over 13B parameters on complex video generation by substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering promising solution for physically plausible video generation. 1 1. Introduction Video diffusion models have recently achieved remarkable success in producing high-quality, temporally coherent videos [6, 7, 24, 41]. This progress has been driven by advances in model architectures [39], substantial increases in model complexity, now reaching tens of billions of parameters [41], and the availability of large-scale, high-quality video datasets [8]. However, current models still struggle to generate videos that adhere to realistic physical principles, making it difficult to consistently achieve fine-grained motion control, complex movements, and coherent object interactions. Despite extensive efforts to enhance performance through larger models and higher-quality datasets, recent study [21] indicates that scaling model size and data alone is insufficient to fully capture the complexities of the physical world. On the other hand, human image animation models [16, 55, 67] offer valuable insights for addressing persistent challenges in video generation. Despite using smaller models and less data, these methods achieve consistent and precise video outputs with complex motions by following predefined 2D keypoint trajectories. This success suggests that incorporating well-defined motion prior can substantially reduce the learning complexity of video generative models, enabling them to generate coherent and lifelike motion. However, in general video generation tasks, such strong guiding signals are typically unavailable, limiting the direct applicability of these animation techniques to broader video generation scenarios. This raises critical question: Can we develop video generation framework that leverages the implicit motion information embedded in the generated videos as guidance to further enhance video quality? In this paper, we propose simple, plug-and-play video generation framework that explicitly optimizes physical knowledge in conditional video generation model through parameterized 3D representation, enabling the generation of complex object motions and interactions. The core of ReVision is to Regenerate Videos with explicit 3D physics representations, following an ExtractOptimizeReinforce pipeline. Specifically, to effectively leverage 3D knowledge without heavy retraining of the diffusion model while preserving its original ability to generate high-quality visual appearance, we design the pipeline in three stages. In the first stage, we employ video diffusion model (e.g., SVD [6]) to generate coarse video conditioned on the given input. In the second stage, we utilize parametric 3D models (i.e., SMPL-X [37] for humans, SMAL [49, 80] for animals, and depth-aware 2D bounding boxes [57, 69] for objects) to extract 3D shape and motion features from the coarse video. These 3D representations are subsequently optimized by the proposed Parameterized Physical Prior Model (PPPM), producing more accurate and natural 3D motion sequence. In the third stage, the refined 3D motion sequences are incorporated as additional conditioning inputs to reinforce the diffusion model, enabling it to regenerate the video with improved coherence and realism. Extensive qualitative results and human preference studies confirm that our model excels at generating complex motions and interactions. To demonstrate its applicability, we applied ReVision on Stable Video Diffusion (SVD) [6], substantially improving its ability to generate realistic and intricate motions. We further compare ReVision-SVD with HunyuanVideo [24], state-of-the-art open-source video generation model with 13 billion parameters, and demonstrate superior motion quality. Finally, on the particularly challenging dance generation task, our model outperforms state-of-the-art human image animation methods that rely on ground-truth pose sequences, surpassing them across all evaluation metrics. In summary, we make the following contributions: We demonstrate that optimizing physical knowledge of the generative models enhances their ability to generate complex motions and interactions, suggesting promising direction for improving video generation. We introduce ReVision, three-stage pipeline that significantly improves the motion quality of pre-trained video generation models by explicitly optimizing parameterized 3D physical knowledge extracted from generated videos. We propose PPPM, lightweight and robust parameterized physical prior model that effectively refines motion information in generated videos. 2. Related Work Video Generation. With the success of diffusion models in image generation [5, 10, 27, 46], driven by advancements in both generative modeling strategies [13, 26, 28, 31, 53] and model architectures [3, 29, 33, 39], video generation [4, 7, 14, 41, 52, 62, 70, 73, 77] has recently attracted significant attention. Parallel to text-to-video (T2V) generation, image-to-video (I2V) methods [1, 25, 36, 65, 74] generate videos from single starting frame. However, existing methods still struggle to handle complex motions and interactions, and often fail to maintain physical plausibility. To overcome these challenges, recent approaches incorporate additional conditions to enhance motion control in video generation. Common conditional inputs include text descriptions [9, 11, 17, 45, 72], which can further guide motion modeling. For example, MAGE [17] introduces spatially aligned motion anchor to blend motion cues from text, and SEINE [9] uses random-mask video diffusion model to create transitions guided by textual descriptions. Another popular condition is optical flow, where models [34, 35, 51] estimate rough flow from user-provided arrows or text to In contrast, ReVision guide complex motion generation. leverages implicit motion features already embedded in the generated video through 3D parameterized object represen2 tations. This allows it to directly extract, optimize, and reinforce accurate and reliable motion features from the generated video itself, resulting in precise motion sequences that enhance coherence and fidelity. Human Image Animation. Human image animation focuses on transferring motion from source human to target human by using ground-truth posture sequences, which can be represented as flow [63], keypoints [16, 55], or human part masks [67]. Extensive efforts have gone into extracting improved motion features. For example, MagicAnimate [67] leverages an off-the-shelf ControlNet [75] to obtain motion conditions, Hu et al. [16] introduce Pose Guider network to align pose images with noise latents, and Animate-X [55] utilizes both implicit and explicit pose indicators to generate motion and pose features. Such strong guidance enables high-quality video generation in human image animation, as each posture sequence directly dictates the synthesis of corresponding frames. However, in general video generation, the ground-truth dense guidance is typically unavailable, and there is usually no reference video for extracting motion sequence. To overcome this limitation, ReVision introduces three-stage process: it first extracts an implicit, rough motion sequence from the generated video, then refines it using the proposed PPPM, and finally leverages the refined motion to guide video regeneration. This approach provides effective guidance for video generation, significantly improving video quality. 3. Preliminary Latent Diffusion Model. Diffusion models [13] generate data through denoising process that learns probabilistic transformation. Latent diffusion models [46] move this process from pixel space to the latent space of Variational Autoencoder (VAE) [22]. Specifically, we consider the latent representation z0 of the input data. In the forward diffusion process, Gaussian noise is incrementally added to z0: q(ztzt1) = (cid:16) (cid:17) zt; (cid:112)1 γtzt1, γtI , (1) where zt represents the noisy latent representation at time step t, and γt is predefined noise schedule with (0, 1). As increases, the cumulative noise applied to z0 intensifies, gradually transforming zt closer to pure Gaussian noise. For simplicity, we can express the transformation from z0 to zt directly as: zt = αtz0 + 1 αt ϵ, (2) where αt = (cid:81)t i=1(1 γi) and ϵ (0, I). The latent diffusion model, parameterized by Θ, learns to reverse this noising process by taking zt as input and reconstructing the clean data with the objective: = ϵ ϵΘ(zt, t, c)2 2 , (3) 3 where is condition (e.g., text prompt) to guide the denoising process. Once the latent space is reconstructed, it is decoded via the VAE decoder. Video Latent Diffusion Model. We use SVD [6] as our base video diffusion model, which extends Stable Diffusion 2.1 [46] to video. The main architectural difference from image diffusion models is that SVD incorporates temporal UNet [48] by adding temporal convolution and (cross-) attention [58] layers after each corresponding spatial layer. 3D Human and Animal Mesh Recovery. We utilize the SMPL-X [37] and SMAL [79] parametric models to represent humans and animals, respectively. These models parameterize 3D meshes with pose parameters θ and shape parameters β. Additionally, SMPL-X model includes expression parameters ψ to capture facial expressions through blend shapes. Given these parameters, SMPL-X model is differentiable function that outputs posed 3D human mesh MSM LX (θh, βh, ψh) R104753, where pose θh R165, shape βh R10, and expression ψh R10. Similarly, SMAL model represents posed 3D animal mesh with MSM AL(θa, βa) R38893, where pose θa R105 and shape βa R41. In this project, we recover 3D meshes by fitting SMPL-X and SMAL to our data and to the generated videos. This approach provides accurate part labeling and incorporates spatial priors of human and animal bodies, improving upon direct 2D shape predictions. Additionally, it enables motion strength computation by measuring movement speed in 3D space, offering higher accuracy than 2D pixel-based predictions. 2.5D Parameterized Object Representation. Unlike humans and animals, there is no straightforward way to parameterize general objects in 3D space. Here, we represent objects in 2.5D by combining 2D bounding boxes [57], segmentation masks [40, 64], and depth estimation [69]. Specifically, given detected bounding box and segmentation mask, we first convert the mask into contour with 16 vertices. We then lift these 21 2D pixels (16 contour vertices, 4 bounding box corners, and the bounding box center) into 3D points using depth estimation. As result, each object is now represented as set of points po R213. 4. Method ReVision requires extending pre-trained video diffusion model to accept additional motion conditions as input. In Sec. 4.1, we describe how to adapt SVD into motionconditioned video generation model with minimal modifications. In Sec. 4.2, we introduce ReVision, three-stage video generation pipeline built upon the extended SVD, incorporating carefully designed Parameterized Physical Prior Model to provide accurate motion sequences as conditioning inputs. An illustration is provided in Fig. 2. Figure 2. Method overview. Given the motion-conditioned video generation model, ReVision operates in three stages. Stage 1: coarse video is generated based on the provided conditions (e.g., target pose, marked in blue, indicating the rough position of the yellow part in the last frame). Stage 2: 3D features from the generated coarse video are extracted and optimized using the proposed PPPM. Stage 3: The optimized 3D sequences are used to regenerate the video with enhanced motion consistency. Best viewed when zoomed in. ovals) to indicate the final positions of specific targets or parts (e.g., hand or arm). These user-friendly sketches are then converted into polygonal masks, similar to the part segmentation mask polygons used during training. Since polygon conversion introduces unavoidable errors, we assign confidence score of 0.5 in this case. Last, to preserve SVDs ability to generate videos without motion conditioning, the remaining 30% of training examples provide an empty part mask, with corresponding confidence score of 0. Note that all three settings use the same model architecture, introducing minimal modifications only to SVDs first convolutional block. This design enables fine-tuning of only the initial convolutional block and all temporal layers, eliminating the need to train SVD from scratch. Consequently, this fine-tuning strategy allows the extended SVD to generate videos conditioned on various types of motion sequence inputs while preserving its ability to generate videos based only on text and the first frame. 4.2. Proposed Method: ReVision Overview. As shown in Fig. 2, ReVision consists of three stages. In stage one (S1), we generate coarse video based on the provided conditions. In stage two (S2), we extract both 2D and 3D features from the coarse video and refine the 3D motion sequences through the proposed PPPM. In stage three (S3), we use the refined 3D motion sequences as strong conditioning, guiding the video generation model to regenerate the video, resulting in significantly higherquality output even for complex motions and interactions. S1: Coarse Video Generation. Given the first frame and an optional user-specified target motion in the final frame, we use the fine-tuned SVD model to generate the video. Since the generation relies only on the target motion in the final frame or an empty motion, rather than complete motion sequence, the resulting video often exhibits poor motion quality, leaving room for refinement. Therefore, we refer to this stage as coarse video generation. S2: Object-Centric 3D Optimization. Following coarse video generation, we utilize 3D parametric mesh models [32, 49, 80] for humans and animals, along with 2D Figure 3. Motion-conditioned video generation. We extend SVD for motion-conditioned video generation by introducing two additional conditioning channels to the original condition: (1) part segmentation mask derived from the 3D motion sequence, and (2) its corresponding confidence map. 4.1. Motion-Conditioned Video Generation Since SVD does not natively support motion-conditioned video generation, we extend its design to enable this capability, with focus on simplicity to preserve its original generation quality and minimize deviations from user-provided inputs. Concretely, we begin with pre-trained SVD and fine-tune it within carefully structured strategy. We concatenate two additional conditioning channels to the original condition: one for part-level segmentation mask derived from the 3D motion sequence, and another for confidence map indicating the reliability of the part mask, as illustrated in Fig. 3. We also design fine-tuning pipeline that integrates three scenarios with varying levels of part mask guidance, allowing the model to flexibly handle diverse inputs. We detail those three scenarios below. First, when the full motion sequence is provided (40% of training examples), the part-level mask is generated by merging all 2D part segmentation masks projected from 3D parametric mesh models. Since the motion sequence provides dense and precise control over video generation, we assign confidence score of 1 to the corresponding confidence map. Our experiments confirm that these 3Dprojected masks are more robust than existing part segmentation models. Second, when only the target pose is provided (30% of training examples), we convert the projected part segmentation masks into polygons. This aligns with users inference input, where they provide simple sketches (e.g., circles or 4 bounding boxes [57], segmentation masks [40, 71], and depth estimation [69] for general objects, to parameterize targets in 3D space. These parameters are extracted using series of off-the-shelf models (see Sec. 3). However, due to the poor motion quality and inconsistencies in the coarse video generated in S1, the 3D parameters extracted also suffer from instability and inconsistencies. To address this, we propose Parameterized Physical Prior Model (PPPM) to optimize the 3D motion sequence, based on the text information and the motion strength. PPPM first extracts text embeddings from the text description using pre-trained CLIP encoder [42]. Motion strength is computed from the differences in parametric 3D model parameters between adjacent frames, providing measure of motion speed. Since the 3D motion sequences are already parameterized as 2D vectors, PPPM employs series of transformer blocks to iteratively refine the motion sequence based on these conditioning inputs. Within each block, motion features undergo self-attention, followed by cross-attention with the conditioning inputs (text embeddings and motion strength) and feedforward network to generate the final output. Finally, the optimized 3D parameterized motion sequences are converted into 3D mesh sequences and projected into 2D as part segmentation masks and confidence maps, providing more accurate motion guidance. Architectural details can be found in Sec. 9 of Supp. To effectively train PPPM, we introduce small perturbations to ground-truth motion sequences during training. Three types of perturbations are randomly applied: (1) adding small random noise to the motion sequence, (2) shuffling the internal order of the sequence, and (3) dropping small segment while repeating the remaining segments to maintain the original length. Through this process, PPPM learns to denoise perturbations, improving its ability to recover smooth and robust motion sequences. S3: Fine-grained Video Generation. In the final stage, we regenerate the video using the same SVD model but with the improved motion sequence as additional conditioning. Unlike the coarse generation in stage one, which uses only the target pose in the last frame or no motion information, we now utilize the full motion sequence as part masks optimized in 3D space. With this stronger conditioning, the final output exhibits significantly improved motion consistency compared to the coarse video, as illustrated in Fig. 7. 5. Dataset Both the motion-conditioned video generation model and the parameterized physical prior model need to be finetuned (trained) on small yet high-quality video dataset with object-centric annotations. Existing large-scale video datasets [2, 8] mainly provide text-image pairs without detailed object-centric annotations. To address this limitation, we use suite of off-the-shelf models across various tasks to generate detailed 2D and 3D object-centric annotations. We annotate total of 20K videos from the Panda-70M [8] dataset. For each video, we provide frame-wise 2D bounding boxes, semantic masks, depth estimation maps, and 3D parametric mesh reconstructions for detected humans and animals. The details are outlined below. High-Quality Motion Videos Filtering. To start with, we use LLMs [68] and an open-vocabulary segmentation model [71] to curate high-quality motion videos. Specifically, the language model filters videos with evident motion based on their captions. Then, for each selected video, we equally sample 10 frames and apply the segmentation model to identify humans and animals. We evaluate each frame based on the predicted mask size and mask count. Then we retain videos where humans or animals occupy significant portion of the frame and where the count of humans does not exceed five in each frame. Object Detection and Depth Estimation. Based on the captions of the selected videos, we identify the objects mentioned and detect their bounding boxes [57] and instance masks [71]. Additionally, we apply Depth Anything V2 [69] to generate the depth maps of each frame. Human Videos Annotation. For videos containing humans, we focus on extracting 2D instance segmentation masks, 2D part masks, 2D face keypoints, 3D body pose and shape, and 3D hand pose. We begin by using YOLOV8 [57] to segment all humans in each frame, providing accurate human masks. Next, we apply state-of-the-art face keypoint detector, RTMPose [20], to predict facial keypoints for each detected human. Simultaneously, we use 4D-Human [12] and HaMeR [38] to estimate the 3D body and hand meshes. The resulting SMPL (body mesh) [32] and MANO (hand mesh) [47] parameters are then fit into unified SMPL-X [37] representation, which contains both human body and hand meshes. We then project the 3D SMPL-X human mesh onto 2D to obtain part masks, as each vertex in the SMPL-X mesh is labeled by body part. Finally, we project the face keypoints and 3D human mesh onto the instance mask, allowing us to compute the overlap between the projected keypoints, projected human mask, and detected 2D human mask. This overlap is quantified using an IoU score, which is used to filter out annotations with high errors. As result, for each video, we obtain annotations including human instance masks, 2D facial keypoints, 3D SMPL-X meshes for the body and hands, and 2D part-level segmentation masks. Animal Videos Annotation. We start by using Grounded SAM 2 [23, 43, 44] to segment animal masks in each frame. Next, we apply state-of-the-art camera estimation algorithm, VGGSfM [59], to optimize the cameras intrinsic and extrinsic parameters across the video. To ensure reliable camera estimate, we set thresholds on mean projection errors and mean track lengths, filtering out videos that do not 5 Figure 4. Qualitative results. ReVision enables the generation of high-quality videos with complex motions and interactions, such as running with dogs, picking up ball, and hitting tennis ball. It allows precise motion control over small body parts, such as arms and hands. Zoom in for better detail. Please check the videos on the project page. meet these criteria. We then use AnimalAvatar [50] initialized with Animal3D [66] to fit SMAL parameters. Each video is divided into segments of 10 consecutive frames, and AnimalAvatar is applied to each segment independently. This strategy helps mitigate the impact of outliers in camera predictions on the overall optimization quality. To ensure the accuracy of SMAL fitting, we impose thresholds on IoU and PSNR [15], filtering out video segments that do not meet our accuracy standards. Once accurate SMAL fittings are obtained, we follow similar pipeline to extract the desired annotations as used in human cases. 6. Experimental Results In this section, we first compare our method with Stable Video Diffusion (SVD) [6] and HunyuanVideo [24] in Sec. 6.1, demonstrating how our approach enhances SVD to enable more controllable and complex motion generation. Next, in Sec. 6.2, we compare our model with Human Image Animation methods, demonstrating that it achieves comparable performance even when given only the final pose instead of the full motion sequence. Finally, in Sec. 6.3, we conduct ablation studies to validate the effectiveness of our key components. Additional experimental results are provided in the Supp. 6.1. Image-to-Video Generation Experimental Setup. We use SVD [6] as the base video generation model and modify it by introducing two additional channels for conditional generation. We fine-tune SVD on our annotated dataset for 300K iterations with batch size of 64 and constant learning rate of 2 105. During training, we randomly sample 16-frame video clips with stride of 4 at resolution of 1024 576. To enable various control, we incorporate different conditioning strategies: 40% of video clips contain accurate part masks for each frame, 30% contain polygon mask for random parts in the final frame, and the remaining clips have no additional conditioning. User Study. To better compare our model with the baseline SVD and HunyuanVideo, we conduct user studies on Amazon MTurk to assess user preferences. Specifically, we generate 500 text descriptions of humans and animals engaged in daily activities using GPT-4o [18]. For the comparison with SVD, we use GPT-4o to generate five 16 : 9 images for each prompt, which are resized to 1024 576 as input. For the comparison with HunyuanVideo, we first use their released model to generate five videos at resolution of 1280 720 for each prompt, then extract the first frame of each video as the input image for our model to generate the corresponding video. For each image, we generate one video per model using the same random seed (42), resulting in total of 5, 000 video pairs. Each video pair is evaluated by three different users, leading to total of 15, 000 comparisons. Users are shown two videos side by side, generated by different models, with the order randomized. They are instructed to assess the videos based on Motion 6 ReVision-SVD (Ours) SVD SSIM PSNR LPIPS FVD 100 60 40 20 0 Motion Consistency Amount of Motion Motion Reality ReVision-SVD (Ours) HunyuanVideo 80 60 40 20 e e e e r r s 0 Motion Consistency Amount of Motion Motion Reality Figure 5. User preference comparison. Our model enhances the motion generation capability of the pre-trained SVD. It even surpasses HunyuanVideo, SOTA model with 13B parameters. These results highlight the effectiveness of our model in generating complex motions and interactions. SVD ReVision-SVD HunyuanVideo Time (s) 36 36 + 5 + 36 411 Table 1. Inference speed. We compare the average time required to generate 32-frame video for each method. Consistency, Amount of Motion, and Motion Realism. The results are reported in Fig. 5. Our model significantly enhances the motion generation capabilities of SVD, producing videos with superior motion quantity, consistency, and realism. Furthermore, it even surpasses HunyuanVideo, state-of-the-art video generation model with 13B parameters, in terms of motion quality. These results highlight the effectiveness of our model in generating complex motions and interactions. Qualitative Results. We provide samples of generated videos in Fig. 4. As shown in the figure, our approach, ReVision, produces realistic movements that closely follow user instructions. It also generates high-quality videos that involves complex motions and interactions, such as running with dogs, picking up ball, and hitting tennis ball. More visualizations are available in the supplementary videos. Inference Speed. We compare the inference speed of our model against two baselines in Tab. 1. Due to the limitation of SVD, we use all three models to generate video clip with 32 frames. Despite the three-step pipeline, the additional 3D detection and motion refinement modules add only 5 seconds to the inference time on single A100, making it significantly faster than SVD, which requires 36 seconds. More importantly, integrating these modules into SVD significantly improves its ability to generate intricate motions, making it comparable to or superior to state-ofDisco [61] MagicAnimate [67] Animate Anyone [16] Champ [78] VividPose [60] ReVision (w/ full) ReVision 0.668 0.714 0.718 0.802 0.758 0.842 - 29.03 29.16 29.56 29.91 29.83 30.05 - 0.292 0.239 0.285 0.234 0. 0.227 - 292.80 179.07 171.90 160.82 152.97 137.53 130.14 Table 2. Quantitative comparison for human dance generation. ReVision (w/ full) follows all baselines and takes the full motion sequences as condition, while ReVision generates videos with only the target pose from the final frame. the-art models like HunyuanVideo, which requires an average of 411 seconds. 6.2. Complex Motion Generation Experimental Setup. To demonstrate our models ability to generate videos with complex motion, we compare our approach with state-of-the-art human image animation models on the TikTok Dancing dataset [19], using the Disco [61] split. For compatibility with the SVD model architecture, all videos are cropped to 576 1024. We fine-tune the original SVD only on the training split for 30K iterations, with batch size of 8 and learning rate of 1 105. Evaluation Metrics. We follow baselines and report Peak Signal-to-Noise Ratio (PSNR) [15], Structural Similarity Index (SSIM) [63], and Learned Perceptual Image Patch Similarity (LPIPS) [76] to measure the visual quality of the generated results. We also report and Frechet Video Distance (FVD) [56] for video fidelity comparision. Experimental Results. We compare ReVision with human image animation methods in Tab. 2, where our approach achieves state-of-the-art performance across all metrics. Notably, we observe significant improvement in FVD, highlighting substantial gains in video generation quality. It is important to note that all baseline methods in this task rely on ground truth motion sequences, which are challenging to obtain in practical scenarios, thus limiting their applicability. In contrast, our method can generate realistic and high-quality videos using only the inference image and the target pose as input. 6.3. Ablation Study Herein, we conduct ablation studies to verify the effectiveness of the proposed designs. Parametric 3D Mesh. Previous human image animation models mainly rely on 2D pose sequences for each frame to provide motion information. However, this approach is not optimal for general video generation. As shown in Fig. 6, we compare the results of using parametric human mesh model [32] versus human keypoint model [54]. Our findings indicate that the human mesh model provides robust object-level prior, which significantly benefits general video 7 Figure 6. The parametric 3D mesh serves as an effective objectlevel prior, ensuring complete human body structures in the coarse video generated during the first stage. In the left two images, the human keypoint model fails to detect the missing right hand, which is accurately recovered by the parametric human mesh model. In the right two images, the human mesh model provides more accurate prior for both blurred hands. Object Consistency Motion Consistency Morphological Failure Rate Stage 1 Stage 3 12.4 87.6 4.0 96.0 83.5 14.3 Table 3. User preference studies for multi-stage generation. By explicitly using the 3D knowledge during generation, videos generated in stage 3 show significant improvements in object and motion consistency, while reducing the morphological failure rate. generation. Specifically, current video generation models often misinterpret the structure of humans and animals, occasionally producing unrealistic results, such as man with three arms, an example of morphological failure. This problem becomes more serious in complex motion generation. However, incorporating human and animal priors from 3D mesh models substantially mitigates these structural inaccuracies, enabling more accurate representations of targets. Multi-Stage Generation. To demonstrate the effectiveness of multi-stage generation, we select complex dance scenario and visualize outputs at different stages in Fig. 7: the coarse video from stage one, the optimized 3D mesh from stage two, and the re-generated video from stage three. We also present quantitative improvements of our multi-stage generation in Tab. 3, where 500 video pairs were evaluated by users on Amazon MTurk. Each pair was rated by three users, resulting in total of 1,500 evaluations. The results show that the video generation model alone still struggles to produce realistic videos with accurate motion. However, leveraging object-level priors from the parametric 3D mesh model and motion priors from our PPPM enables the generation of realistic 3D motion sequence aligned with the reference image, target pose, text prompt, and motion strength. Guided by this 3D motion sequence, Stage 3 generates realistic video with enhanced motion and visual quality. In addition, Fig. 7 shows that even when the first stage generates videos with significantly broken motions, the second stage can recover relatively smooth and reasonable motion sequence using the accurate first frame and target pose, ultimately ensuring successful final video generation. This demonstrates the robustness of our pipeline. 8 Figure 7. Video generated in stage 1 (S1) and stage 3 (S3). Although fine-tuned on dataset containing large motions, SVD still struggles to generate complex motion sequences. However, by incorporating the optimized 3D dense motion sequence from Stage 2, we significantly enhance both motion and visual quality and generate realistic videos with large motions in stage 3. 7. Limitations The proposed method has several remaining limitations. First, it relies on parametric 3D mesh models, requiring multiple off-the-shelf models for different object categories, though it adds only 5 seconds to the total inference time. Recent advances in 3D modeling, such as encoding 3D priors of general objects within single diffusion model [30], are paving the way for more general, efficient models that can be seamlessly integrated into our pipeline for high-quality video generation. Second, the model still struggles to generate high-quality details such as fingers and hands. Finally, while PPPM can generate realistic motion sequences beyond 32 frames, our current implementation, based on vanilla SVD, is limited by memory constraints (80GB RAM). However, recent methods have demonstrated longer video generation ability using pretrained diffusion models [9]. Exploring long-video generation with 3D knowledge remains future work. 8. Conclusion In this work, we introduced ReVision, three-stage framework for video generation that improves motion consistency by integrating 3D motion cues. Our approach leverages pretrained video diffusion model to generate coarse videos, refines 3D motion sequences through PPPM, and reconditions the generation process with these enhanced motions to improve fine-grained and complex motion generations. show that ReVision significantly outperEvaluations forms existing methods in motion fidelity and coherence."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy Campbell, and Sergey Levine. Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017. 2 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17281738, 2021. 5 [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. 2 [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 2 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2, 3, 6 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 2, 5 [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 2, 8 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2 [11] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [12] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 2, 3 [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [15] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, pages 23662369, 2010. 6, 7 [16] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2, 3, 7 [17] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation with text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1821918228, 2022. 2 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12753 12762, 2021. 7 [20] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Realtime multi-person pose estimation based on mmpose. arXiv preprint arXiv:2303.07399, 2023. 5 [21] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 2 [22] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. 3 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 5 [24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, [25] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal 9 video prediction from still images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 600 615, 2018. 2 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [27] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 2 [28] Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat Singh. Flowing from words to pixels: framework for cross-modality evolution. arXiv preprint arXiv:2412.15213, 2024. [29] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models and timedependent layer normalization. Advances in Neural Information Processing Systems, 37:133879133907, 2024. 2 [30] Qihao Liu, Yi Zhang, Song Bai, Adam Kortylewski, and Alan Yuille. Direct-3d: Learning direct text-to-3d generation on massive noisy 3d data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68816891, 2024. 8 [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [32] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, 2015. 4, 5, 7 [33] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 2 [34] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable animation of fluid elements in still images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36673676, 2022. 2 [35] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generIn Proceedings of ation with latent flow diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1844418455, 2023. 2 [36] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. Video generation from sinIn Proceedings of the IEEE/CVF gle semantic label map. Conference on Computer Vision and Pattern Recognition, pages 37333742, 2019. [37] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, In Proceedings of face, and body from single image. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097510985, 2019. 2, 3, 5 [38] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98269836, 2024. 5 [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2 [40] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segIn Proceedings of the IEEE/CVF Conference mentation. on Computer Vision and Pattern Recognition, pages 8533 8542, 2020. 3, 5 [41] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, et al. Movie gen: cast of media foundation models, 2024. 2 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 5 [43] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. [44] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 5 [45] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. 2 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution imIn Proceedage synthesis with latent diffusion models. ings IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [47] Javier Romero, Dimitrios Tzionas, and Michael Black. Embodied hands: Modeling and capturing hands and bodies together. arXiv preprint arXiv:2201.02610, 2022. 5 [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241, 2015. 3 [49] Nadine Rueegg, Shashank Tripathi, Konrad Schindler, Michael J. Black, and Silvia Zuffi. Bite: Beyond priors In Proceedings for improved three-d dog pose estimation. 10 IEEE Conferecne on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 4 [50] Remy Sabathier, Niloy Jyoti Mitra, and David Novotny. Animal avatars: Reconstructing animatable 3D animals from casual videos. ArXiv, abs/2403.17103, 2024. 6 [51] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion In ACM SIGGRAPH 2024 Conference Papers, modeling. pages 111, 2024. 2 [52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [53] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [54] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose esIn Proceedings of the IEEE/CVF Conference timation. on Computer Vision and Pattern Recognition, pages 5693 5703, 2019. 7 [55] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. 2, 3 [56] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [57] Rejin Varghese and Sambath. Yolov8: novel object detection algorithm with enhanced performance and robustness. In 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS), pages 16. IEEE, 2024. 2, 3, 5 [58] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [59] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep In Proceedings of the IEEE/CVF structure from motion. Conference on Computer Vision and Pattern Recognition, pages 2168621697, 2024. [60] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. arXiv preprint arXiv:2405.18156, 2024. 7 [61] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, ChungChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. 7 [62] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024. 2 [63] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. 3, 7 [64] Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, and Song Bai. General object foundation model for images and videos at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37833795, 2024. 3 [65] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 23642373, 2018. [66] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, et al. Animal3d: comprehensive dataset of 3d animal pose and shape. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90999109, 2023. 6 [67] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 2, 3, 7 [68] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 5 [69] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 2, 3, 5 [70] Mengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans, ProbabilisarXiv preprint Joshua Tenenbaum, and Pieter Abbeel. tic adaptation of text-to-video models. arXiv:2306.01872, 2023. [71] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. Advances in Neural Information Processing Systems, 36:3221532234, 2023. 5 [72] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 2 [73] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. 2 11 [74] Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, and Yunliang Jiang. Dtvnet: Dynamic time-lapse video generation via single still image. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 300315. Springer, 2020. 2 [75] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [76] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 586595, 2018. 7 [77] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 2 [78] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Qingkun Su, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image arXiv preprint animation with 3d parametric guidance. arXiv:2403.14781, 2024. 7 [79] Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, and Michael J. Black. 3d menagerie: Modeling the 3d shape and pose of animals. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5524 5532, 2017. 3 [80] Silvia Zuffi, Ylva Mellbin, Ci Li, Markus Hoeschle, Hedvig Kjellstrom, Senya Polikovsky, Elin Hernlund, and Michael J. Black. VAREN: Very accurate and realistic equine network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 4 ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary material includes the following additional information. Architectural details for PPPM. (Sec. 9) Quantitative ablations of multi-stage generation (Sec. 10) We also provide the generated videos used in all figures in the main paper, as well as additional videos demonstrating accurate motion control, in the supplementary videos. 9. Architectural details for PPPM To optimize the 3D motion sequence extracted from the coarse generated video, we propose the Parameterized Physical Prior Model (PPPM). As shown in Fig. 8, PPPM utilizes transformer architecture with self-attention, crossattention, and feedforward layers as its backbone. It takes the parameterized motion sequence of the coarse video as input and optimizes it based on the input text prompt and motion strength. 10. Improvement of Multi-stage Generation We briefly discuss the quantitative ablations of multi-stage generation in the main paper and provide additional experimental details and results here. To further demonstrate the improvements of our multi-stage approach, we conducted an additional user study on Amazon MTurk comparing videos from Stages 1 and 3. Unlike our previous study, which compared our method with SVD, this evaluation focuses on object consistency and motion consistency. We also report the percentage of videos containing incorrect human or animal structures (i.e., the morphological failure rate). We evaluate 500 video pairs, each rated by three different users, resulting in 1,500 total evaluations. The results are presented in Tab. 3 of the main paper. By optimizing with parametric 3D mesh, our approach significantly reduces incorrect human and animal structures, leading to substantial improvements in object and motion consistency. Figure 8. Architecture of the Parameterized Physical Prior Model"
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Johns Hopkins University"
    ]
}