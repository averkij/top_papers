{
    "paper_title": "Scaling Language-Free Visual Representation Learning",
    "authors": [
        "David Fan",
        "Shengbang Tong",
        "Jiachen Zhu",
        "Koustuv Sinha",
        "Zhuang Liu",
        "Xinlei Chen",
        "Michael Rabbat",
        "Nicolas Ballas",
        "Yann LeCun",
        "Amir Bar",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: \"Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?\" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 1 0 1 0 . 4 0 5 2 : r Scaling Language-Free Visual Representation Learning David Fan1,, Shengbang Tong1,2,, Jiachen Zhu1,2, Koustuv Sinha1, Zhuang Liu1,3, Xinlei Chen1, Michael Rabbat1, Nicolas Ballas1, Yann LeCun1,2, Amir Bar1,, Saining Xie2, 1FAIR, Meta, 2New York University, 3Princeton University equal contribution, equal advising Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data? We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning. Date: April 1, 2025 Project Page: https://davidfan.io/webssl/"
        },
        {
            "title": "1 Introduction",
            "content": "Visual representation learning has evolved along two distinct paths with different training approaches. Language-supervised methods such as Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021; Zhai et al., 2023) use paired image-text data to learn representations that are enriched with linguistic semantics. Self-Supervised Learning (SSL) methods (Zhang et al., 2016; Chen et al., 2020a; He et al., 2022; LeCun, 2022; Oquab et al., 2023) learn from images alone, without language. Despite SSL models outperforming languagesupervised models on classic vision tasks such as classification and segmentation (Oquab et al., 2023), they are less commonly adopted in recent multimodal large language models (MLLMs) (Liu et al., 2023a, 2024a; Agrawal et al., 2024; Tong et al., 2024a; Beyer et al., 2024; Li et al., 2024; AI@Meta, 2024). This difference in adoption is partially due to performance gap in visual question answering (see Figure 1), particularly for OCR & Chart interpretation tasks (Tong et al., 2024a; Shi et al., 2024). Beyond methodology differences, these approaches Figure 1 We compare the scaling behavior of visual SSL and CLIP on 16 VQA tasks from the Cambrian-1 suite under different data and model size regimes. Prior visual SSL methods achieved strong performance on classic vision tasks, but have underperformed as encoders for multimodal instruction-tuned VQA tasks. Our results show that with appropriate scaling of models and data, visual SSL can match the performance of language-supervised models across all evaluated domainseven OCR & Chart. have also been separated by data scale and distribution (Figure 1). CLIP models typically train on Figure 2 Visual SSL 2.0 changes. In this work, we adopt three improvements to the visual SSL pipeline: 1) Training on billion-scale web data, curated through the MetaCLIP pipeline, to move beyond conventional datasets; 2) Scaling model architecture from sub-billion parameter models to models exceeding 1 billion parameters; and 3) Incorporating VQA as complementary evaluation protocol to comprehensively assess visual features. These changes enable us to study visual SSL at larger scale and observe scaling trends previously unobserved in smaller-scale experiments. billion-scale image-text pairs from the web (Schuhmann et al., 2022; Chen et al., 2023; Xu et al., 2024b), while SSL methods use million-scale datasets such as ImageNet (Deng et al., 2009) or hundred-million scale data with ImageNet-like distributions (Ridnik et al., 2021; Oquab et al., 2023). In this work, we investigate fundamental question: Is language supervision necessary to pretrain visual representations for multimodal modeling? Rather than seeking to replace language-supervised approaches, we aim to understand the intrinsic capabilities and limitations of visual self-supervision at scale for multimodal applications. To conduct fair comparison, we train SSL models on the same billionscale web data used for state-of-the-art CLIP models specifically the MetaCLIP dataset (Xu et al., 2024b). This approach controls for data distribution differences when comparing visual SSL and CLIP. For evaluation, we primarily use visual question answering (VQA) as framework to evaluate SSL models across diverse set of capabilities at scale. VQA evaluation suites span vision-centric, visual reasoning, and OCR & Chart tasks, and have been shown to be more diverse testbed for assessing vision encoders (Tschannen et al., 2024; Wan et al., 2024; Fini et al., 2024; Tong et al., 2024a), reflecting the broader perception challenges found in real-world distributions. We adopt the evaluation suite proposed in Cambrian-1 (Tong et al., 2024a), which evaluates performance across 16 tasks spanning 4 distinct categories of VQA: General, Knowledge, OCR & Chart, and Vision-Centric. We train Web-SSL, family of visual SSL models ranging from 1 to 7 billion parameters, using the above setting for direct and controlled comparison to CLIP. As result of our empirical study, we contribute several insights: Visual SSL can match and even surpass languagesupervised methods for visual pretraining, on wide range of VQA taskseven on languagerelated tasks such as OCR & Chart understanding (Figure 3). Visual SSL scales well with respect to model capacity (Figure 3) and data (Figure 4), indicating that SSL has significant untapped potential. Visual SSL can maintain competitive traditional vision performance on classification and segmentation, even while improving at VQA (Figure 7). Training on higher ratio of images containing text is especially effective for improving OCR & Chart performance (Question 4). Exploring data composition is promising direction. This work serves as proof of concept that offers compelling vision-centric alternative to the recent CLIP-dominated trend, and opens new opportunities for future research. We plan to open-source our WebSSL vision models, and we hope to inspire the broader community to unlock the full potential of visual SSL in the multimodal era."
        },
        {
            "title": "2 From Visual SSL 1.0 to 2.0",
            "content": "In this section, we describe our experimental setup, which extends previous SSL works by (1) scaling dataset size to billion-scale images (Section 2.1), (2) scaling model size beyond 1B parameters (Section 2.2), and (3) evaluating vision models using open-ended VQA tasks (Section 2.3), in addition to 2 classic vision benchmarks such as ImageNet-1k (Deng et al., 2009) and ADE20k (Zhou et al., 2019)."
        },
        {
            "title": "2.1 Beyond ImageNet Pretraining",
            "content": "To study whether visual SSL can match the performance of CLIP, we start by adopting the same data that drove CLIPs success. We thus leverage the MetaCLIP dataset (Xu et al., 2024b,a), which has enabled the most successful open-source reproduction of CLIP to-date.1 We use 2 billion samples from MetaCLIP, which we refer to as MC-2B. We train SSL methods on only the images, and CLIP on the image-text pairs. This controls for data distribution and size as confounding variables, and enables fairer comparison of the pretraining methods themselves, while ensuring sufficient data diversity and scale."
        },
        {
            "title": "2.2 Scaling Up Vision Models to Billion Scale",
            "content": "Inspired by adWe can also increase model size. vancements in scaling language models (Brown et al., 2020; Kaplan et al., 2020; OpenAI, 2022), we train Vision Transformers (ViTs) with 1B, 2B, 3B, 5B, and 7B parameters, on only the images from MC-2B, to study the properties of larger-scale visual SSL models trained on web-scale data. We adapt ViT-g from Oquab et al. (2023) as ViT-1B, and define new configurations for ViT-2B to 7B  (Table 1)  ; see Appendix for model details."
        },
        {
            "title": "Model Width Depth Heads MLP",
            "content": "ViT-1B 1536 ViT-2B 2688 ViT-3B 3072 ViT-5B 3584 ViT-7B 4096 40 24 26 32 32 24 21 24 28 32 6144 10752 12288 14336 16384 Table 1 Model architecture details. For consistency, we denote ViT-g from Oquab et al. (2023) as ViT-1B."
        },
        {
            "title": "2.3 Multimodal LLMs as an Evaluation Proto-",
            "content": "col In addition to conventional evaluation protocols, such as ImageNet-1k linear probe, we also evaluate our vision encoders using VQA, flexible and robust evaluation protocol that reflects the diversity of realworld perceptual challenges (Tschannen et al., 2024; Tong et al., 2024a), as shown in Figure 2. Here, we study all vision encoders using the same controlled setting to ensure fair comparison. Specifically, 1The data used to train the original CLIP is closed-source. we use the same two-stage visual instruction tuning procedure and data as Cambrian-1 (Tong et al., 2024a). First, lightweight MLP adapter is added to project the vision encoder features into the same dimensionality as the LLM, and only this MLP adapter is trained. In the second stage, both the MLP adapter and LLM are finetuned. To enable controlled comparison, the vision encoder remains frozen in both stages, and all experiments use the same training recipe as well as Llama-3 8B Instruct (Touvron et al., 2023) backbone. We provide detailed training datasets and hyperparameters in Appendix A. We then report results on the Cambrian-1 (Tong et al., 2024a) evaluation suite, which is comprised of 16 VQA benchmarks spanning four established domains: General, Knowledge, OCR & Chart, and Vision-Centric. The average VQA performance is the average of the four subcategories. Each subcategory has 4 benchmarks and is equally weighted."
        },
        {
            "title": "3 Scaling Visual SSL",
            "content": "In this section, we explore the scaling behavior of visual SSL models with respect to both model and data size, as result of training on only images from MC-2B. We focus on DINOv2 (Oquab et al., 2023) as the visual SSL method in this section, and discuss MAE (He et al., 2022) in Section 4. In Section 3.1, we increase model size from 1B to 7B while keeping the training data fixed at 2 billion MC2B imagesunless otherwise denoted. We use the off-shelf training code and recipe for each method, and do not change the recipe for different model sizes in order to control for confounding variables. In Section 3.2, we shift our focus to scaling total data seen for fixed model size, and analyze how performance evolves as the number of images seen during training increases from 1 billion to 8 billion."
        },
        {
            "title": "3.1 Scaling Model",
            "content": "The intention of scaling model size is both to find the ceiling of visual SSL under this new data regime, and to identify any unique behavior that emerges in larger models. We thus pretrain DINOv2 ViT models, ranging from 1B to 7B parameters, using 2 billion unlabeled images at 224224 resolution from MC-2Bwithout highresolution adaptation (Oquab et al., 2023)to ensure fair comparison with CLIP. We refer to these models as Web-DINO throughout the paper. For controlled comparison, we also train CLIP models of the same sizes on the same data. 3 Figure 3 Scaling behavior of Web-DINO and CLIP ViTs trained on MC-2B. The x-axis shows model sizes from 1B to 7B parameters on log scale. We observe novel scaling behavior with Web-DINO models across all categories, with particularly pronounced improvements in the OCR & Chart and Vision-Centric domains as model size increases. In contrast, CLIP models demonstrate limited scaling benefits, with performance saturating at moderate model sizes. The two model families exhibit complementary strengths: CLIP models excel at OCR & Chart VQA, and Web-DINO models are superior at Vision-Centric VQA, while remaining competitive in all other categories. We evaluate each model with VQA and present the results in Figure 3. We will first discuss the overall performance trend and then turn to specific category performance. To the best of our knowledge, this is the first instance of vision encoder trained purely with visual self-supervision achieving performance parity with language-supervised encoders on VQAeven in the OCR & Chart category, which is traditionally considered to be highly text-dependent. Performance trend. We compare the performance trend as model capacity increases in Figure 3. WebDINOs Average, OCR & Chart, and Vision-Centric VQA performance improves nearly log-linearly with increasing model size, while General and Knowledge improve to smaller degree. In contrast, CLIPs performance in all VQA categories largely saturates after 3B parameters. This suggests that while smaller CLIP models may be more data-efficient, this advantage largely dissipates for larger CLIP models. The continual improvement from increasing Web-DINO model capacity also suggests that visual SSL benefits from larger model capacity, and that scaling visual SSL past 7B parameters is promising direction. Category-specific performance. In terms of categoryspecific performance, DINO also increasingly outperforms CLIP on Vision-Centric VQA and largely closes the gap with CLIP on OCR & Chart and Average VQA (Figure 3), as model size increases. At 5B parameters and above, DINO can exceed the Average VQA performance of CLIP, despite being trained solely on images and without language supervision. These results suggest that vision-only models, when trained on CLIP-distribution images, can develop strong visual features that are comparable to those of language-supervised vision encoders."
        },
        {
            "title": "3.2 Scaling Examples Seen",
            "content": "Previously, we focused on single-epoch training, where each of the 2B unique images in MC-2B is seen only once. Here, we investigate the impact of increasing the number of examples seen by training Web-DINO ViT-7B on data ranging from 1 billion to 8 billion images from MC-2B. As shown in Figure 4, General and Knowledge VQA performance improves incrementally with more examples seen, saturating at 4B and 2B examples respectively. Vision-Centric VQA performance improves sharply from 1B to 2B examples, and saturates beyond 2B examples. In contrast, OCR & Chart is the only category that shows consistent improvement with more examples seen. This suggests that as the model sees more data, it learns representation that is increasingly well-suited for text-related tasks, yet without marked degradation on other capabilities. Furthermore, when compared to CLIP model of the same size (ViT-7B), Web-DINO consistently outperforms CLIP on average VQA performance given the same number of samples seen (Figure 4). Notably, after seeing 8B samples, Web-DINO closes the performance gap with the CLIP model on OCR & Chart VQA tasks. This provides further evidence suggesting that visual SSL models have the potential to scale better than language-supervised models. Collectively, the results in Figure 3 and 4 indicate that as model size and examples seen increase, visual SSL learns features that are increasingly effective for VQA in general, but especially on OCR & Chart. Our results suggest that CLIP-based models do not hold an absolute advantage compared to visual SSL. In Section 4, we delve deeper into the underlying mechanisms driving this trend. 4 Figure 4 Scaling up examples seen when training Web-DINO-7B. Performance across different VQA categories as training data increases from 1B to 8B images. While General and Vision-Centric tasks show diminishing returns after 2B images, OCR & Chart tasks demonstrate continued improvement, contributing to steady gains in average performance. Further, Web-DINO consistently outperforms same-size (ViT-7B) CLIP models with different training samples seen. The x-axis plots training data size on log-scale."
        },
        {
            "title": "4 Scaling Analysis and Findings",
            "content": "In Section 3, we demonstrated that visual SSL models scale well with model size and training set size. These observations raise further questions about the generality and implications of these phenomena. To deepen our understanding, we investigate five key aspects, including whether scaling behavior extends to other vision-only models (Question 1), if SSL models also exhibit scaling behavior on smaller and more conventional data (Question 2), and whether SSL can retain competitive performance on classic vision tasks (Question 3). Additionally, we explore why scaling particularly enhances OCR & Chart performance (Question 4), and highlight emergent properties that arise via scaling visual SSL (Question 5). In this section, we provide detailed analysis of these findings. Question 1 Does the observed scaling behavior generalize to other visual SSL methods? In previous sections, we derived our findings from DINOv2, joint embedding visual SSL method. Here, we extend our analysis to masked modelling based visual SSL methodMasked Autoencoder (MAE) (He et al., 2022). We train MAE on MC-2B (denoted as Web-MAE) using ViT models ranging from 1B to 5B parameters and compare the results with Web-DINO models in Figure 5. Web-MAE models exhibit similar scaling behavior to Web-DINO models, with average VQA performance improving consistently as model size increases. Compared to joint embedding methods, Web-MAE models learn features that are particularly well-suited for OCR & Chart tasks but underperform in other domains. These results suggest that the scaling behavior observed in VQA tasks generalizes across different visual SSL methods. We also note that different visual SSL approaches learn distinct representations even when trained under the same conditions, as demonstrated by Web-MAEs OCR performance. Question 2 Does visual SSL exhibit similar scaling behavior on smaller scale conventional data, such as ImageNet? We pretrain Web-DINO 1B, 2B, and 3B models for 300 epochs on ImageNet-1k, conventional pretraining dataset for SSL, following the recipe from (Oquab et al., 2023). We compare these variants to those trained on MC-2B. We evaluate their downstream VQA performance and ImageNet-1k linear probing results. As shown in Figure 6, models pretrained on ImageNet-1k exhibit consistently inferior performance across all the metrics. Moreover, unlike models trained on MC-2B, those trained on ImageNet-1k do not improve with increasing model sizes. This highlights the importance of training visual SSL on more diverse and larger datasets. This echoes recent findings that increasing dataset sizes and diversity drive LLM scaling (Kaplan et al., 2020; Hoffmann et al., 2023; Chowdhery et al., 2022), and also that pretraining data distribution is critical to downstream performance (Liu and He, 2025). Question 3 How do scaled models perform on classic vision tasks? We evaluate Web-DINO models, ranging from 1B 5 Figure 5 Web-MAE trained on MC-2B. Web-MAE also exhibits consistent scaling behavior as model size increases. Notably, Web-MAE demonstrates better performance in OCR & Chart tasks, achieving higher accuracy than Web-DINO across all model sizes. Figure 6 Comparison of ImageNet-1k and MC-2B Pretraining. Increasing the diversity and scale of pretraining data improves model performance on VQA accuracy and ImageNet linear probing. Unlike MC-2B pretraining, training on ImageNet does not exhibit clear scaling trend. to 7B parameters, on classic vision benchmarks including linear probing on ImageNet-1k (Deng et al., 2009), semantic segmentation on ADE20K (Zhou et al., 2019), and depth estimation on NYUv2 (Silberman et al., 2012). Following the evaluation protocol of DINOv2 (Oquab et al., 2023), we freeze the vision encoder; see Appendix for details. As shown in Figure 7, Web-DINOs performance improves modestly with increasing model size. Web-DINO achieves strong performance across all benchmarks, outperforming MetaCLIP by significant margin and remaining competitive with off-shelf DINOv2, even outperforming it on ADE20K +ms. Note that the comparison with off-shelf DINOv2 is not exactly apples-to-apples, as we do not use high-resolution adaptation (Oquab et al., 2023), in order to maintain the same input resolution as CLIP. Additionally, the DINOv2 training data has higher correlation with these classic vision benchmarks, detailed further in Appendix E. These differences suggest that there remains considerable room for further improvement in our models classic vision performance. However, we observe that the scaling behavior in classic vision tasks is less pronounced compared to VQA. This finding, along with insights from previous work (Tong et al., 2024a; Fini et al., 2024; Naeem et al., 2024), reinforces the value of VQA as comprehensive vision model evaluation framework. While classic benchmarks remain important, VQA provides complementary view into model performance via offering diverse set of tasks that are grounded in real-world perceptual challenges. Question 4 Why does web-scale data improve OCR & Chart performance? In Section 3, we observed that increasing model size and examples seen leads to unprecedented improvements in OCR & Chart performance for visual SSL models. This is surprising since current off-the-shelf visual SSL methods are notably poor at OCR & Chart understanding compared to language-supervised models (Tong et al., 2024a; Shi et al., 2024). One possible explanation is that web-scale image datasets already contain degree of textual information. Unlike object-centric datasets such as ImageNet, images from the web often contain text (e.g. labels, signs, diagrams, etc.). Larger capacity and more data might aid visual SSL models to extract and leverage this textual information. To test this hypothesis, we apply an off-the-shelf MLLMSmolVLM2 (Allal et al., 2025)to identify images containing text. See Figure 8 for qualitative examples and Appendix for details. This results in 6 Figure 7 Performance of Web-DINO models on classic vision tasks. All models achieve strong performance across ImageNet1k classification, ADE20K segmentation, and NYU Depth estimation, and all tasks experience moderate improvements from increasing model size from 1B to 7B parameters. Web-DINO outperforms MetaCLIP (HF) and is competitive with DINOv2 (HF). (HF) denotes the largest official Hugging Face released version. VQA Evaluator Breakdown of OCR & Chart Tasks % of Method CLIP 2B Web-DINO 2B Web-DINO 2B 50.3% 53.4 (+2.6) 73.0 (+0.2) 51.7 (+4.6) 47.3 (+0.2) Web-DINO 2B MC-2B AVG 100% 53.0 100% 50.8 Knowledge 48.8 47.1 1.3% 53.7 (+2.9) 70.7 (-2.1) General 72.2 72.8 OCR Chart 36.1 26. Vision Centric 52.6 55.0 49.2 56.4 55.6 (-0.8) 33.2 (+6.4) 51.3 (+2.1) 23.0 (+4.0) 56.2 (-0.2) 40.4 (+13.6) 47.5 (+24.2) 29.4 (+13.8) 52.8 (+3.6) 32.0 (+13.0) ChartQA OCRBench TextVQA DocVQA 32.8 23.3 31.4 (+8.1) 32.9 15.6 27.3 (+11.7) 26.0 19.0 Table 2 Impact of data filtering on SSL model performance. We compare Web-DINO ViT-2B models trained on MC-2B with different levels of text filtering (full, 50.3%, and 1.3%) against CLIP ViT-2B trained on full MC-2B. OCR & Chart performance improves with progressively aggressive filtering, with the 1.3% filter achieving the best results. Despite receiving zero language supervision, SSL models can surpass CLIP in text-centric tasks while maintaining strong overall performance. outperforms even the language-supervised CLIP ViT2B trained on full data by +4.3% on OCR & Chart. Likewise, heavy filtering also improves Average VQA performance, outperforming the full data Web-DINO ViT-2B by +2.6% and even the full data CLIP ViT2B by +0.7%. This means that is it possible for visual SSL models to outperform CLIP models of the same size, with only fraction of the total data (in this case 1.3% of MC-2B). The improvement in OCR & Chart from training on heavily filtered data is particularly pronounced for ChartQA (+24.2%), OCRBench (+13.8%), and DocVQA (+13.0%), while performance remains competitive in all other categories. These results demonstrate that self-supervised visual models, when trained on images containing more text in them, can develop high-quality text understanding capabilities without language supervision. It suggests that data compositionrather than purely scale or language supervisionis crucial for developing strong OCR & Chart understanding abilities. Although it is not surprising that skewing the data in favor of OCR & Chart would improve OCR & Chart capabilities, it is surprising that simple data filtering can outperform language supervision on the full data. This simple proof of concept suggests that similar Figure 8 Examples of filtered MC-2B images. The Light filter (Middle) identifies images containing text, retaining 50.3% of the images. The Heavy filter (Right) identifies images explicitly containing charts and documents, retaining only 1.3% of MC-2B. two curated datasets: (i) Light filter: retains 50.3% of Web-DINO and contains images with any textual content. (ii) Heavy filter: retains 1.3% of MC-2B and contains images with charts, tables, or documents. We train Web-DINO ViT-2B models on these filtered datasets, with each experiment using 2 billion seen examples (meaning filtered datasets undergo multiple epochs). As shown in Table 2, the model trained on lightly filtered data outperforms the full data variant by +6.4% on OCR & Chart, while maintaining strong performance in other categories. The model trained on heavily filtered data performs better and 1B ViT-7B); and (3) seeing more training samples further enhances alignment (Web-DINO ViT-7B trained on 2B samples 8B samples). These findings suggest that as model size and, in particular, training samples scale, vision models naturally develop text-sensitive features and achieve strong alignment with LLMs and multimodal tasks, without explicit language supervision."
        },
        {
            "title": "5 The Web-SSL Model Family",
            "content": "Next, we analyze the overall best performing vision encoders using both VQA and classic vision benchmarks. In Table 3, we show the best results of our vision encoders against recent off-the-shelf vision encoders, in terms of VQA and classic vision tasks. For VQA, all vision encodersincluding off-the-shelf modelsare evaluated using the same visual instruction tuning setup detailed in Section 2.3, and mainly 224224 input resolution for the purpose of fair comparison. Because the goal is not to produce state-ofthe-art MLLM, we did not employ techniques such as unfreezing the vision encoder, resolution tiling (Liu et al., 2024b), and spatial visual aggregator (Tong et al., 2024a). For classic vision, we follow the evaluation procedure from Oquab et al. (2023) and evaluate linear probe performance on ImageNet-1k (Deng et al., 2009), ADE20K (Zhou et al., 2019), and NYU Depth v2 (Silberman et al., 2012). The input resolution differs between classic vision tasks, but each model tested uses the same exact settings from Oquab et al. (2023). We emphasize that the primary motivation is still to provide controlled insights. Performance at 224px. Web-DINO can outperform off-the-shelf MetaCLIP in both VQA and classic vision tasks. Web-DINO is even able to match the performance of SigLIP and SigLIP2 on VQA despite seeing 5 less data and receiving no language supervision. In general, Web-DINO outperforms all off-shelf language-supervised CLIP models at traditional vision benchmarks. Although our best Web-DINO model is 7B parameters, the results from Section 3.1 and Section 3.2 suggest that CLIP models saturate beyond moderate model and data sizes, while visual SSL improves progressively with increasing model and data size. Web-DINO also outperforms off-theshelf visual SSL methods, including DINOv2 (Oquab et al., 2023), in all VQA categories. Web-DINO is also competitive in traditional vision benchmarks. Performance beyond 224px. Next, we discuss the performance of higher resolution models. Following Figure 9 Alignment score between Web-DINO and LLMs. Moving from DINOv2 to Web-DINO improves the alignment between the image and the corresponding text representations obtained by LLMs. Increasing model size from 1B to 7B parameters shows gradual improvement, while training on larger data quantities (4B/8B samples) yields the most significant alignment gains. techniques may be used to help visual SSL bridge future gaps in other capabilities. Question 5 Why can SSL learn strong visual representations for multimodal modeling, without language supervision? Thus far, we have seen that visual SSL models can not only become competitive with CLIP models, but also that they can excel at tasks previously thought to require language. This raises an important question: why do vision-only models learn features that work well for multimodal models, even in the absence of language supervision? We hypothesize that SSL models learn features increasingly aligned with language as model size and examples seen increases. Following Huh et al. (2024), we evaluate intrinsic representational alignment by computing matching metric between the vision encoder and language model, using image-text pairs from the Wikipedia Captions dataset (Srinivasan et al., 2021). We use off-the-shelf DINOv2 (Oquab et al., 2023) and Web-DINO as vision encoders, and off-the-shelf Llama-3.1 8B and 70B (Touvron et al., 2023) as the language models, without any visual instruction tuning nor alignment procedure. As shown in Figure 9, we observe three key trends: (1) training on more diverse data (MC-2B) improves alignment with LLMs (DINOv2 ViT-1B WebDINO ViT-1B); (2) increasing the vision model size leads to slightly higher alignment (Web-DINO ViTModel MLLM Evaluator Classic Vision Tasks Method Pretrain Data Pretrain Samples Seen Language-Supervised Models SigLIP ViT-SO400M WebLI 45.0B SigLIP2 ViT-SO400M WebLI 45.0B MetaCLIP ViT-G MetaCLIP 12.8B Visual Self-Supervised Models MAE ViT-H ImageNet-1k I-JEPA ViT-H ImageNet-22k DINOv2 ViT-g LVD-142M 2.0B 0.9B 1.9B Web-DINO ViT-7B MC-2B 8.0B Res 384 224 384 224 224 518 224 378 518 e G e l K h & c n - s . K 0 2 . m 0 2 . k 1 ) ( . d ) ( 4 . d N 55.4 74.4 48.7 39.5 58.9 86. 36.5 38.0 0.607 0.525 60.0 76. 50.4 53.5 59.7 87.3 39.5 47. 0.582 0.438 56.3 74.4 50.7 42. 58.1 87.5 41.1 44.2 0.562 0. 62.0 76.6 51.9 58.4 61.0 88. 43.5 50.2 0.524 0.469 54.8 75. 48.2 37.3 58.4 86.4 38.0 46. 0.524 0.415 45.2 64.6 43.9 20. 51.7 76.6 33.3 30.7 0.517 0. 44.7 65.4 43.9 21.2 48.4 68. 31.6 34.6 0.548 0.520 47.9 70. 45.0 21.2 55.3 86.0 49.0 53. 0.344 0.298 55.2 74.5 48.0 39. 59.1 86.5 42.1 52.6 0.491 0. 57.4 73.9 47.7 50.4 57.7 86. 42.3 53.1 0.498 0.366 59.9 75. 48.2 55.1 60.8 86.4 42.6 52. 0.490 0.362 Table 3 Comparison with other vision models. Web-DINO ViT-7B achieves competitive performance with CLIP models on VQA without language supervision and surpasses them on traditional vision tasks. Compared to other self-supervised models like DINOv2, Web-DINO significantly narrows the performance gap with CLIP on VQA tasks, particularly excelling in OCR & Chart understanding. These results demonstrate that SSL can effectively produce strong visual representations for both multimodal and classic vision tasks. Oquab et al. (2023), we additionally fine-tune WebDINO for 20k steps. We do this for resolutions of 378 and 518, to compare against the higher-resolution off-shelf versions of SigLIP as well as DINO. See Appendix for training details. From 224 to 378 to 518 resolution, Web-DINO improves steadily at average VQA, with notable gains in OCR & Chart performance. Classic vision performance improves modestly with higher resolution. At 384 resolution, Web-DINO trails behind SigLIP. At 518 resolution, Web-DINO is largely able to bridge the gap. The results suggest that Web-DINO may benefit from further increasing high-resolution adaptation. et al., 2021; Caron et al., 2021; LeCun, 2022; Chen et al., 2022; Garrido et al., 2023), while masked modeling (Zhou et al., 2021; He et al., 2022; Wei et al., 2022; Fan et al., 2023; Assran et al., 2023; Woo et al., 2023; Bar et al., 2024; Bai et al., 2024; Carreira et al., 2024) learns by predicting masked visual inputs. Our work complements SSL research focused on pretraining algorithms, by taking off-the-shelf training code and training visual SSL at scale with controlled experimental setup. In Question 1, we show that the observed scaling behavior generalizes across both joint embedding and masked modeling SSL methods, and is likely not method-specific phenomena."
        },
        {
            "title": "6 Related Work",
            "content": "Visual self-supervised learning methods. Early visual SSL methods explored various pretext tasks for pretraining (Wang and Gupta, 2015; Doersch et al., 2015; Noroozi and Favaro, 2016; Zhang et al., 2016; Gidaris et al., 2018; Balestriero et al., 2023). More recently, research has converged on two primary approaches: joint embedding methods and masked image modeling. Joint embedding methods learn invariant features by aligning representations of different augmented views (He et al., 2019; Misra and Van Der Maaten, 2019; Chen et al., 2020a; Grill et al., 2020; Chen et al., 2020b; Chen and He, 2021; Chen Data used to train vision models. Both supervised (He et al., 2016; Xie et al., 2016; Dosovitskiy et al., 2021; Liu et al., 2022) and SSL vision models have traditionally relied on standard datasets such as MNIST (LeCun, 1998), CIFAR-10 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009; Ridnik et al., 2021). More recently, self-supervised methods have scaled to larger unlabeled datasets, such as YFCC (Thomee et al., 2016), LVD-142M (Oquab et al., 2023), and IG-3B (Singh et al., 2023); however, these methods still exhibit significant performance gap compared to language-supervised models on VQA. In contrast, language-supervised models (Radford 9 et al., 2021; Zhai et al., 2023; Sun et al., 2023, 2024; Xu et al., 2024b; Tang et al., 2025) leverage significantly larger image-text datasets, from WIT-400M (Radford et al., 2021) to billion-scale web data (Schuhmann et al., 2022; Fang et al., 2024; Xu et al., 2024b; Gadre et al., 2024), with some using up to 100B image-text pairs (Wang et al., 2025). Studies suggest that pretraining data distribution is more critical for downstream performance than specific training methodologies (Fang et al., 2022; Liu and He, 2025). Our work bridges these paradigms by pretraining SSL models on web-scale data. Through controlled experiments (Section 3 and 4), we show that (1) visual SSL models are sensitive to the training distribution, (2) increasing data diversity and quantity significantly improves performance on diverse range of VQA tasks, and (3) training on higher concentration of images containing text is highly effective for improving OCR & Chart understanding. Evaluating vision models. Classic works have primarily used image classification (LeCun, 1998; Krizhevsky et al., 2009; Deng et al., 2009; Bossard et al., 2014; Hendrycks et al., 2019, 2020) to evaluate learned representations. More recent SSL research has expanded evaluation to include image segmentation (Everingham et al., 2010; Cordts et al., 2016; He et al., 2017; Zhou et al., 2019), depth estimation (Silberman et al., 2012; Geiger et al., 2013; Song et al., 2015), and video classification (Soomro et al., 2012; Goyal et al., 2017a; Baruch et al., 2021). Languagesupervised models (Radford et al., 2021; Zhai et al., 2023), due to their two-tower encoder structure, commonly use zero-shot image classification to assess the quality of learned image and text features. Our work follows recent proposals (Naeem et al., 2024; Fini et al., 2024; Tong et al., 2024a) to evaluate vision encoders on broader range of VQA tasks (Goyal et al., 2017b; Yue et al., 2024a; Liu et al., 2024c; Fu et al., 2023; Tao and Xie, 2024; Yue et al., 2024b; xAI, 2024) using MLLMs. These VQA tasks complement traditional vision benchmarks by assessing visual features on more diverse range of real-world perceptual challenges. As shown in Section 3 and Section 4, we find that visual SSL trained on web-scale data learns representations that continue to improve on VQA benchmarks, andto lesser degreealso on traditional vision benchmarks."
        },
        {
            "title": "7 Limitations",
            "content": "In this work, we focus on training visual SSL models without using language. The main limitation of vision-only models, compared to language-supervised models, is that they do not support zero-shot image classification out of the box. However, by integrating visual SSL models into MLLM frameworks through instruction tuning, we show they can achieve impressive downstream performance across classification and other tasks. Another way to achieve zero-shot image classification is to use LiT-style adaptation (Zhai et al., 2022; Jose et al., 2024), but this is outof-scope for our work as we do not use language supervision. To focus on comparing the vision encoder, we fixed the base LLM for visual instruction tuning to Llama-3 8B Instruct (AI@Meta, 2024). We hypothesize that the findings using other LLM backbones would be similar, however this is not in scope for our work. Additionally, while we demonstrate that visual SSL scales well on MetaCLIP data, we leave the exploration of even larger and/or uncurated datasets to future work."
        },
        {
            "title": "8 Discussion",
            "content": "We show that large-scale visual encoders that are trained with self-supervised language-free objectives can produce high quality visual features for multimodal models. Our results echo the bitter lesson (Sutton, 2019) and suggest that imposing less supervisionincluding languageremains promising direction for advancing the field of computer vision. We hope our work will inspire further exploration of vision-only approaches, which will enable the construction of next generation vision models that excel at both traditional vision and modern multimodal capabilities."
        },
        {
            "title": "9 Acknowledgements",
            "content": "We thank Ellis Brown, John Nguyen, Junlin Han, Shengyi Qian, Tyler Zhu, Yuexiang Zhai, Druv Pai, Shusheng Yang, Jihan Yang, Muzi Tao, Boyang Zheng, and Anjali Gupta for reviewing this manuscript. We thank Hu Xu and the MetaCLIP paper authors for creating the MetaCLIP dataset. We thank Mido Assran, Mikael Henaff, Daniel Bolya, Hu Xu, Mark Ibrahim, Russ Howes, and Matthew Muckley for their insightful feedback. We thank Michaël Ramamonjisoa and Marc Szafraniec for their help with image segmentation and depth estimation evaluations. Lastly, we thank Ananya Saxena, Cody Olsen, Mack Ward, Maxwell Taylor, Kalyan Saladi, Dev Satpathy, Dinesh Kannappan, Xiaodong Ma, Jacob Kahn, Gabriel Synnaeve, and Shubho Sengupta for infrastructure support."
        },
        {
            "title": "References",
            "content": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 1 AI@Meta. Llama 3 model card. 2024. 1, 10, 16 Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric trainlanguage model. ing of small arXiv preprint arXiv:2502.02737, 2025. 6, 16 Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, 2023. 9 Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In CVPR, 2024. 9 Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023. Amir Bar, Florian Bordes, Assaf Shocher, Mido Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, and Yann LeCun. Stochastic positional embeddings improve masked image modeling. In ICML, 2024. 9 Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In NeurIPS, 2021. 10 Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 1 Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, 2014. 10 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 3 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. João Carreira, Dilara Gokay, Michael King, Chuhan Ignacio Rocco, Aravindh Mahendran, Zhang, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv preprint arXiv:2412.15212, 2024. 9 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020a. 1, 9 Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. 9 Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b. 9 Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual languageimage model. In ICLR, 2023. 2 Yubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun. Bag of image patch embedding behind the success of self-supervised learning. arXiv preprint arXiv:2206.08954, 2022. 9 Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive languageimage learning. In CVPR, 2023. 16 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arxiv 2022. arXiv preprint arXiv:2204.02311, 10:1, 2022. 5 Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 10 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 2, 3, 6, 8, 9, 10, 16, Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 9 11 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 9 Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In ICCV, 2017a. 10 Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 10 Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017b. David Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, and Xinyu Li. Motion-guided masking for spatiotemporal representation learning. In CVPR, 2023. 9 Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In ICML, 2022. 10 Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In ICLR, 2024. 10 Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Béthune, Zhe Gan, et al. Multimodal autoregressive pre-training of large vision encoders. arXiv preprint arXiv:2411.14402, 2024. 2, 6, 10 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. corr abs/2306.13394 (2023), 2023. 10, 20 Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2024. Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, and Yann Lecun. On the duality between contrastive and non-contrastive self-supervised learning. In ICLR, 2023. 9 Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 20 Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 10 Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 9 Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 9 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 10 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arxiv e-prints, art. In CVPR, 2019. 9 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1, 3, 5, 9 Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Xiaodong Song. Natural adversarial examples. 2021 ieee. In CVPR, 2019. 10 Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-ofdistribution generalization. 2021 ieee. In ICCV, 2020. Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. 20 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In NeurIPS, 2023. 5 Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 20 Minyoung Huh, Brian Cheung, Tongzhou Wang, and 12 Phillip Isola. The platonic representation hypothesis. In ICML, 2024. Cijo Jose, Théo Moutakanni, Dahyun Kang, Federico Baldassarre, Timothée Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Michaël Ramamonjisoa, Maxime Oquab, et al. Dinov2 meets text: unified framework for image-and pixel-level vision-language alignment. arXiv preprint arXiv:2412.16334, 2024. 10 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 3, 5 Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 9, 10 Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 9, 10 Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1): 162, 2022. 1, Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1 Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 20 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2023. 20 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. 20 Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. 20 Ishan Misra and Laurens Van Der Maaten. Self-supervised learning of pretext-invariant representations. in 2020 ieee. In CVPR, 2019. Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, and Federico Tombari. Silc: Improving vision language pretraining with selfdistillation. In ECCV, 2024. 6, 10 Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 9 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023a. 1 OpenAI. Chatgpt, 2022. 3 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024a. 1 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024b. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. 20 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024c. 10, 20 Zhuang Liu and Kaiming He. decades battle on dataset bias: Are we there yet? In ICLR, 2025. 5, 10 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, 2022. 9 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In TMLR, 2023. 1, 2, 3, 5, 6, 8, 9, 16, 17, 21 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 9, 10 Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. 2, 9 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset In for training next generation image-text models. NeurIPS, 2022. 2, 10, 16, 17 Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1, 6 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 6, 8, 10, 16, 20 Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus In Rohrbach. Towards vqa models that can read. CVPR, 2019. 20 Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. In ICCV, 2023. 9 Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In CVPR, 2015. 10 Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 10 Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipediabased image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 10 Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Evaclip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2024. 10 Richard Sutton. The bitter lesson."
        },
        {
            "title": "Incomplete Ideas",
            "content": "(blog), 2019. 10 Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, and David M. Chan. Tulip: Towards unified languageimage pretraining, 2025. Preprint. 10 Muzi Tao and Saining Xie. What does visual formal analysis of the worlds 500 most famous paintings tell us about multimodal LLMs? In The Second Tiny Papers Track at ICLR 2024, 2024. 10 Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59 (2):6473, 2016. 9 Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024a. 1, 2, 3, 6, 8, 10, 16, Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. 16 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024c. 20 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 8 Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. In NeurIPS, 2024. 2, 3 Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim Alabdulmohsin, Xiao Wang, André Susano Pinto, Andreas Steiner, Lucas Beyer, and Xiaohua Zhai. Locca: Visual pretraining with location-aware captioners. arXiv preprint arXiv:2403.19596, 2024. 2 Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015. Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, and Xiaohua Zhai. Scaling pre-training to one hundred billion data for vision language models. arXiv preprint arXiv:2502.07617, 2025. 10 Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, 2022. 9 Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In CVPR, 2023. 9 xAI. grok, 2024. 10, 20 Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016. 9 Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, et al. Altogether: Image captioning via re-aligning alt-text. arXiv preprint arXiv:2410.17251, 2024a. 14 Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024b. 2, 3, 10, 17, 21 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024a. 10, 20 Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. 10 Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1812318133, 2022. 10 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In ICCV, 2023. 1, Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In ECCV, 2016. 1, 9 Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 16 Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. 3, 6, 8, 10, 16, 20 Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021."
        },
        {
            "title": "A Implementation Details",
            "content": "Training. For training Web-DINO, Web-MAE, and CLIP models, we closely follow the existing opensource codebases: the official DINOv2 and MAE repositories, and the MetaCLIP codebase which builds on top of the OpenCLIP codebase (Cherti et al., 2023). We use Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) for distributed training of larger models. For Web-DINO and CLIP pretraining, we follow the exact recipe and hyperparameters from the original paper for their largest model. For MAE pretraining, we observe that training becomes more prone to divergence as model size increases. To mitigate this, we reduce the learning rate from 2.4e-3 to 1.6e-3 and extend the warmup period to 80K iterations. Table 4 provides summary of the pretraining hyperparameters."
        },
        {
            "title": "Batch Size Learning Rate Warmup",
            "content": "Web-DINO Web-MAE CLIP 3072 4096 32768 3.5e-4 1.6e-3 4e-4 100K 80K 2K Table 4 Hyperparameters for Web-DINO, Web-MAE and CLIP. VQA evaluation. For VQA evaluation, we follow Tong et al. (2024a,b) and use Cambrian-Alignment data for MLP projector training and Cambrian-7M for MLP and LLM fine-tuning. We finetune on top of Llama-3 8B Instruct (AI@Meta, 2024). The vision encoder is frozen throughout finetuning. We excluded LAION (Schuhmann et al., 2022) images from the Cambrian data to comply with safety standards. We first encode the images at the models original input resolution using the pretrained vision encoder. Next, we extract features from the final encoder layer. Following prior approaches (Tong et al., 2024a,b), we then resize the resulting token sequence to fixed length of 576 tokens through bilinear interpolation. This ensures consistency across evaluations despite variations in input image resolutions. We report configurations in Table 5. Classic vision evaluation. We follow the evaluation procedure in DINOv2 (Oquab et al., 2023) for all classic vision evaluation: linear probe on ImageNet1k (Deng et al., 2009), ADE20K (Zhou et al., 2019), and NYU Depth v2 (Silberman et al., 2012). For ImageNet-1k, we evaluate models with their pretrained image resolution; For ADE20K and NYU Depth v2, we use the settings from Oquab et al. (2023). For ADE20K, we follow DINOv2 and report the linear and +ms setting. For NYU Depth v2, we report lin. 1 and lin. 4. See the original paper for additional details. In Table 1, we defined the ViT Model architectures. architectures used in our study. To recap, we first borrowed the ViT-g architecture from Oquab et al. (2023) and named it ViT-1B for consistent notation. We then define 2B, 3B, 5B, and 7B architectures inspired by language model scaling. Specifically, the 2 - 7B architectures are wider than the 1B variant, inspired by language model recipes. Our 7B architecture is almost identical to the Llama-2 7B design, except for the patch embedding layer which is unique to ViTs. In Question 4, we introduced the Text filtering. Light and Heavy filters which retain 50.3% and 1.3% of MC-2B respectively. Specifically, we use small MLLM, SmolVLM2 (Allal et al., 2025), to identify images containing text, using prompts such as Does this image contain any readable text?. The intention is not to achieve perfect filtering, but rather to skew the data distribution in the general desired direction. See Figure 8 for visualization of the filtering process and some examples. This results in two curated datasets: (i) Light filter: Retains 50.3% of the original data, primarily consisting of images with some textual content. Prompt used: Does this image contain any readable text? Answer only yes or no. (ii) Heavy filter: Retains only 1.3% of the data, focusing mainly on charts and documents. Prompt used: Please think carefully before answering. Does this image contain charts, tables, or documents with readable text? Answer only yes or no."
        },
        {
            "title": "B Full Results",
            "content": "We include full results of all experiments presented in Section 3 and Section 4. B.1 Web-DINO Scaling up model sizes. We show quantitative results of scaling up the model under VQA evaluation in Table 6 and classic vision evaluation in Table 7. These are the numerical results for Section 3.1. Scaling up data sizes. We show quantitative results of scaling up the number of data seen with WebDINO ViT-7B on VQA evaluation in Table 8 and"
        },
        {
            "title": "Adapter\nLR WD BS",
            "content": "Llama-3 8B Instruct Cambrian Adapter Data Cambrian-7M 1.00e-5 0.0 512 4.00e-5 Instruction Tuning LR WD BS 512 0 Table 5 Hyperparameters for all VQA experiments. We exclude LAION (Schuhmann et al., 2022) from Cambrian data."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric M a A M Model Web-DINO ViT-1B 49.01 1731.52 65.37 69.92 62.40 72.58 35.33 12.30 64.28 19.20 9.40 47.41 17.00 37.33 57.12 64.80 63.16 Web-DINO ViT-2B 50.77 1760.80 68.98 71.29 62.89 73.67 31.77 15.90 67.06 23.30 15.60 49.20 19.00 38.00 57.38 65.85 64.41 Web-DINO ViT-3B 51.71 1757.27 68.04 71.84 63.19 73.57 33.00 14.40 67.32 25.68 17.10 50.45 20.00 42.66 56.86 69.49 65.83 Web-DINO ViT-5B 52.83 1840.81 70.01 72.39 63.56 75.06 32.11 12.40 67.77 26.96 22.10 50.64 21.00 44.66 57.64 67.75 69.16 Web-DINO ViT-7B 53.87 1823.76 68.98 73.02 64.22 74.61 35.11 14.00 69.43 28.80 23.59 51.10 22.00 48.00 59.34 69.96 68. V P D t C D 2 A Q e W A x c R a V M l 2 e - 3 e - Table 6 VQA Evaluation: Web-DINO trained on MC-2B with 2 billion images seen. classic vision evaluation in Table 9. These are the numerical results for Section 3.2. B.5 Baseline Models In Table 14, we provide full VQA results for the reference off-shelf models that we evaluated in Section 5. Scaling down training data. We show VQA evaluation results from training Web-DINO on less diverse dataImageNet-1k, in Table 10. These are the full results for scaling down training data experiments in Question 2. B.2 Web-MAE We show VQA evaluation results from scaling up MAE trained on MC-2B, in Table 11. These are the full results for Question 1. B.3 Scaled CLIP Models We show VQA evaluation results from scaling up MetaCLIP (Xu et al., 2024b) trained on MC-2B, in Table 12. These are the full results for Section 3.1. In contrast to visual SSL methods in Table 7 and Table 11, CLIP models do not exhibit clear scaling behavior. B.4 Text Filtered Models We provide full results for Question 4. As shown in Table 13, SSL models learn features particularly well-suited for OCR & Chart tasks when trained on datasets with higher concentration of text-rich images. This suggests that visual SSL is sensitive to the underlying training distribution and can be effectively steered toward specific downstream applications, such as OCR & Chart. 17 High Resolution Adaption of Web-"
        },
        {
            "title": "SSL",
            "content": "Following Oquab et al. (2023), we further fine-tune our model under higher resolution settings of 378378 and 518518 for 20k iterations. We use batch size of 2048 and correspondingly lower learning rate of 1.41e-5. All other parameters remain exactly the same as previously specified, including the learning rate warmup ratio, given the total of 10k iterations. We also provided detailed benchmark results of highresolution adaptation of Web-DINO in Table 15."
        },
        {
            "title": "D Evaluation",
            "content": "Table 16 lists evaluation benchmarks used and their purposes."
        },
        {
            "title": "E Pretraining Dataset Cards",
            "content": "For reference, in Table 17 we include the data composition of LVD-142M, which was used to train the off-shelf DINOv2 model (Oquab et al., 2023). LVD142M is carefully curated data mix closely aligned with downstream classic vision evaluation tasks. In comparison, we leverage MetaCLIP data, which is less curated and collected from 15 snapshots of CommonCrawl (CC). Vision Backbone Web-DINO ViT-1B Web-DINO ViT-2B Web-DINO ViT-3B Web-DINO ViT-5B Web-DINO ViT-7B IN1k lin. ADE20K lin. ADE20K +ms. NYUd lin. 1 () NYUd lin. 4 () 84.70 85.16 85.66 85.84 86.00 46.60 50.55 50.17 49.54 49.08 50.97 52.32 53.12 53.27 54. 0.364 0.351 0.348 0.378 0.380 0.345 0.335 0.328 0.335 0.339 Table 7 Classic Vision Evaluation: Web-DINO trained on MC-2B with 2 billion images seen."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric r P B I E G S s t U M t C 2 c R Q e V P M d W R 2 e - 3 e - Model Web-DINO ViT-7B (1B Data) 51.02 1785.97 68.12 72.54 63.60 73.87 32.88 12.70 66.58 23.60 15.20 49.04 19.00 43.33 57.12 68.35 61.08 Web-DINO ViT-7B (2B Data) 53.87 1823.76 68.98 73.02 64.22 74.61 35.11 14.00 69.43 28.80 23.59 51.10 22.00 48.00 59.34 69.96 68.58 Web-DINO ViT-7B (4B Data) 54.37 1827.12 71.39 72.61 63.53 72.73 34.00 18.90 67.09 35.12 30.00 53.19 24.00 45.33 55.94 69.68 65.00 Web-DINO ViT-7B (8B Data) 55.24 1811.05 71.30 72.14 64.04 72.43 35.66 15.20 68.52 35.52 36.40 56.53 29.00 46.00 57.90 70.53 62.08 Table 8 VQA Evaluation: Web-DINO ViT-7B trained on MC-2B with increased number of images seen. Vision Backbone Web-DINO ViT-7B (2B Data) Web-DINO ViT-7B (4B Data) Web-DINO ViT-7B (8B Data) IN1k lin. ADE20K lin. ADE20K +ms. NYUd lin. 1 () NYUd lin. 4 () 86.00 86.33 86.52 49.08 47.41 42.14 54.65 54.66 52.55 0.380 0.416 0.491 0.339 0.363 0.376 Table 9 Classic Vision Evaluation: Web-DINO ViT-7B trained on MC-2B with increased number of images seen."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric a A M Model Web-DINO ViT-1B 46.39 1704.30 59.27 66.43 60.12 71.29 32.77 18.70 63.40 17.56 4.90 44.93 14.00 32.00 52.41 62.81 56.41 Web-DINO ViT-2B 45.99 1666.01 60.13 66.64 60.19 68.71 34.88 12.10 62.07 18.60 4.39 45.55 14.00 32.66 52.67 62.07 57.83 Web-DINO ViT-3B 46.43 1729.40 60.56 66.99 60.24 70.50 31.88 11.70 62.30 17.52 4.80 45.18 15.00 31.33 53.20 62.77 62.50 Web-DINO ViT-5B 46.28 1661.25 59.27 67.24 61.10 69.41 31.55 10.90 61.46 18.72 4.60 45.53 15.00 34.00 53.07 64.57 61.08 c A a V E E M 2 A Q e - A x c R a V M l 2 e - 3 e Table 10 VQA Evaluation: Web-DINO trained on ImageNet-1k."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric r P B I E G S s t U M t C 2 c R Q e V P M d W R 2 e - 3 e C - Model Web-MAE ViT-1B 49.19 1736.22 62.02 68.38 60.05 73.27 33.11 12.90 63.92 23.60 16.40 47.84 18.00 36.66 52.81 70.42 60.83 Web-MAE ViT-2B 50.59 1700.16 63.57 69.21 60.93 72.48 32.22 15.50 64.44 29.00 23.20 48.78 20.00 38.00 55.16 67.98 63.91 Web-MAE ViT-3B 50.92 1723.85 64.69 69.71 60.94 72.13 34.33 13.50 65.70 30.92 24.60 48.92 20.00 37.33 54.64 64.15 66.91 Web-MAE ViT-5B 51.50 1710.13 65.12 70.13 61.10 72.63 32.66 13.90 65.67 33.80 26.50 49.60 21.00 38.00 53.72 66.69 67.91 Table 11 VQA Evaluation: Web-MAE trained on MC-2B."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric r P B I E G S s t U M t C 2 c R Q e V P M d W R 2 e - 3 e C - Model MetaCLIP ViT-1B 52.30 1813.70 68.90 69.45 60.35 74.07 33.55 12.70 64.41 33.20 34.59 52.15 26.00 37.33 52.15 65.47 61.83 MetaCLIP ViT-2B 53.03 1787.39 68.81 69.54 61.08 75.16 34.66 20.10 65.38 32.80 32.90 52.55 26.00 37.33 52.94 65.19 64.67 MetaCLIP ViT-3B 53.22 1873.67 68.72 70.33 61.85 77.29 32.77 11.80 66.35 32.16 34.40 54.58 26.00 35.33 55.55 65.57 65.08 MetaCLIP ViT-5B 52.52 1779.03 70.10 70.26 61.53 72.43 33.44 17.90 66.74 30.04 32.20 52.49 25.00 39.33 54.50 64.22 61.16 MetaCLIP ViT-7B 52.97 1827.80 69.93 69.47 61.33 74.91 35.55 16.80 65.15 32.12 32.10 52.07 25.00 39.33 54.11 65.08 63.16 Table 12 VQA Evaluation: MetaCLIP trained on MC-2B with 2 billion images seen."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric M a 2 Model 49.01 1731.52 65.37 69.92 62.40 72.58 35.33 12.30 64.28 19.20 9.40 47.41 17.00 37.33 57.12 64.80 63.16 Web-DINO ViT-1B (No Filter) Web-DINO ViT-1B (Light Filter) 50.73 1690.89 65.54 70.68 62.63 70.99 33.89 17.80 63.69 26.12 21.80 50.56 20.00 36.00 56.86 64.84 65.75 Web-DINO ViT-1B (Heavy Filter) 49.44 1593.79 61.40 65.34 59.53 71.19 31.33 14.90 64.83 36.92 24.09 50.09 27.00 21.33 53.20 66.53 63.66 Web-DINO ViT-2B (No Filter) 50.77 1760.80 68.98 71.29 62.89 73.67 31.77 15.90 67.06 23.30 15.60 49.20 19.00 38.00 57.38 65.85 64.41 Web-DINO ViT-2B (Light Filter) 53.38 1768.67 68.38 71.80 63.24 74.16 33.88 31.40 67.38 31.40 27.30 51.26 23.00 39.33 56.47 61.13 65.50 Web-DINO ViT-2B (Heavy Filter) 53.65 1743.56 65.29 69.28 61.19 74.86 32.22 14.50 67.42 47.48 29.40 52.80 32.00 40.00 54.50 65.85 64.50 M E M A a W a V M M t C e O t T V A r 2 e - 3 e - Table 13 VQA Evaluation: Web-DINO trained on text filtered MC-2B."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric r P B I E G S s t U M t C 2 c R Q e V P M d W R 2 e - 3 e C - Model CLIP Models 54.91 1860.58 72.93 70.96 62.22 77.88 36.88 15.00 67.32 35.60 33.40 55.10 29.00 41.33 53.46 68.53 65.91 MetaCLIP ViT-H224px 55.36 1807.30 72.76 71.83 62.68 76.74 35.44 14.00 68.65 33.08 40.20 56.61 28.00 47.33 56.99 66.42 64.66 SigLIP ViT-SO400M224px SigLIP ViT-SO400M384px 59.97 1892.16 73.71 73.00 63.80 77.83 33.88 20.00 69.78 54.24 46.40 63.53 50.00 46.00 58.43 67.37 66.91 SigLIP2 ViT-SO400M224px 56.32 1789.26 73.36 72.20 62.60 74.96 35.55 22.40 69.85 35.76 42.00 59.68 31.00 44.00 54.24 69.88 64.16 SigLIP2 ViT-SO400M384px 61.98 1895.70 74.57 72.24 64.81 79.27 36.33 19.90 72.24 59.68 52.90 67.15 54.00 49.33 54.77 70.73 69.00 SSL Models DINOv2 ViT-g224px DINOv2 ViT-g378px DINOv2 ViT-g518px I-JEPA ViT-H 224px MAE ViT-H224px 49.25 1785.25 64.86 70.89 62.89 72.03 32.11 12.40 62.37 17.96 5.50 47.06 15.00 47.33 56.33 65.92 66.08 47.94 1734.38 64.26 71.50 62.21 71.04 33.11 9.60 63.08 17.76 5.00 45.59 15.00 41.33 56.47 63.79 60.58 47.91 1694.08 62.45 70.64 62.87 71.29 33.55 11.80 63.37 18.32 5.10 46.27 15.00 37.33 56.60 65.36 61.83 44.78 1598.15 60.01 64.04 57.66 68.91 34.55 10.20 62.07 16.72 4.00 42.99 14.00 29.33 49.93 57.39 57.16 45.21 1697.06 56.87 56.41 60.51 70.74 32.11 11.50 61.30 17.40 5.50 45.38 14.00 27.33 53.46 61.19 64.75 Table 14 VQA Evaluation: Off-shelf CLIP and SSL models."
        },
        {
            "title": "Knowledge",
            "content": "OCR & Chart Vision-Centric r P B I E G S s t U M t C 2 c R Q e V D M d W R 2 e - 3 e - Model Web-DINO224px 55.24 1811.05 71.30 72.14 64.04 72.43 35.66 15.20 68.52 35.52 36.40 56.53 29.00 46.00 57.90 70.53 62.08 Web-DINO378px 57.43 1757.06 70.61 72.59 64.50 72.53 35.11 16.10 67.09 52.04 42.19 61.51 46.00 38.00 59.08 66.55 67.16 Web-DINO518px 59.91 1807.08 73.79 72.92 64.78 74.36 34.66 14.50 69.43 57.28 45.70 64.48 53.00 43.33 60.52 70.08 69.41 Table 15 VQA Evaluation: Web-DINO ViT-7B adapted to different resolution 19 Benchmark GQA SEED MME MMBench AI2D ScienceQA MathVista MMMU TextVQA DocVQA ChartQA OCRBench MMVP RealWorldQA CVBench-2D CVBench-3D ImageNet-1k ADE-20k NYU Depth v2 Depth Estimation Citation Eval Hudson and Manning (2019) General VQA Ge et al. (2023) General VQA Fu et al. (2023) General VQA Liu et al. (2024c) General VQA Hiippala et al. (2021) Knowledge VQA Lu et al. (2022) Knowledge VQA Lu et al. (2023) Knowledge VQA Knowledge VQA Yue et al. (2024a) OCR & Chart VQA Singh et al. (2019) OCR & Chart VQA Mathew et al. (2021) OCR & Chart VQA Masry et al. (2022) OCR & Chart VQA Liu et al. (2023b) Vision-Centric VQA Tong et al. (2024c) Vision-Centric VQA xAI (2024) Vision-Centric VQA Tong et al. (2024a) Vision-Centric VQA Tong et al. (2024a) Image Classification Deng et al. (2009) Image Segmentation Zhou et al. (2019) Table 16 List of benchmarks used Silberman et al. (2012) 20 FGVC-Aircraft / train Flowers-102 / train Food-101 / train Dataset / Split Task ImageNet-22k / classification ImageNet-22k / classification classification ImageNet-1k / train fine-grained classif. Caltech 101 / train fine-grained classif. CUB-200-2011 / train fine-grained classif. DTD / train1 fine-grained classif. fine-grained classif. fine-grained classif. fine-grained classif. Oxford-IIIT Pet / trainval fine-grained classif. fine-grained classif. fine-grained classif. segmentation segmentation segmentation depth estimation depth estimation depth estimation depth estimation retrieval retrieval retrieval retrieval retrieval retrieval retrieval Stanford Cars / train SUN397 / train1 Pascal VOC 2007 / train ADE20K / train Cityscapes / train Pascal VOC 2012 (seg.) / trainaug Mapillary SLS / train KITTI / train (Eigen) NYU Depth V2 / train SUN RGB-D / train Google Landmarks v2 / train (clean) Google Landmarks v2 / train (clean) AmsterTime / new AmsterTime / old Met / train Revisiting Oxford / base Revisiting Paris / base Images 14,197,086 14,197,086 1,281,167 3,030 5,994 1,880 3,334 1,020 75,750 3,680 8,144 19,850 2,501 20,210 2,975 1,464 1,434,262 23,158 24,231 4,829 1,580,470 1,580,470 1,231 1,231 397,121 4,993 6,"
        },
        {
            "title": "Retrieval\nas is\nsample\nsample\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\ncluster\nas is\ncluster\ncluster\ncluster\nas is\nsample\ncluster\ncluster\ncluster\ncluster\ncluster",
            "content": "Retrieved 56,788,344 40,997,344 2,630,000 1,300,000 1,580,000 1,170,000 1,060,000 21,670,000 2,750,000 7,220,000 18,950,000 1,010,000 20,720,000 1,390,000 10,140,000 3,700,000 10,850,000 4,870,000 6,321,880 960,000 830,000 62,860,000 3,680,000 3,660,000 Final 14,197,086 56,788,344 40,997,344 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,434,262 1,000,000 1,000,000 1,000,000 1,580,470 6,321,880 960,000 830,000 1,000,000 1,000,000 1,000,000 142,109,386 Table 17 LVD-142M Data Sources. In contrast to LVD-142M, which relies on highly curated data sources drawn from distributions closely aligned with various downstream evaluation tasks (see the table above from Oquab et al. (2023)), our data curation approach adopts the methodology from MetaCLIP (Xu et al., 2024b), utilizing web data collected from 15 snapshots of CommonCrawl (CC) spanning January 2021 through January 2023."
        }
    ],
    "affiliations": [
        "FAIR, Meta",
        "New York University",
        "Princeton University"
    ]
}