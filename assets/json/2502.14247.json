{
    "paper_title": "Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation",
    "authors": [
        "Jiayu Yang",
        "Taizhang Shang",
        "Weixuan Sun",
        "Xibin Song",
        "Ziang Cheng",
        "Senbo Wang",
        "Shenzhou Chen",
        "Weizhe Liu",
        "Hongdong Li",
        "Pan Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration. The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: https://github.com/Tencent/Tencent-XR-3DGen."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 2 7 4 2 4 1 . 2 0 5 2 : r Pandora3D: Comprehensive Framework for High-Quality 3D Shape and Texture Generation Jiayu Yang1*, Taizhang Shang1, Weixuan Sun1, Xibin Song1, Senbo Wang1, Shenzhou Chen1, Weizhe Liu1, Hongdong Li2, 1 Tencent XR Vision Labs, 2 The Australian National University Ziang Cheng1, Pan Ji"
        },
        {
            "title": "Abstract",
            "content": "This report presents comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs Variational Autoencoder (VAE) to encode implicit 3D geometries into latent space and diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration. The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: https://github.com/Tencent/Tencent-XR-3DGen. 1. Introduction Automated generation of high-quality digital 3D assets has drawn more and more attention in recent years. Digital 3D assets have become deeply ingrained in modern life and production. These assets vividly express the imaginations of creators across various fields, including gaming and film, bringing joy and creating immersive experiences for both players and audiences alike. Meanwhile, 3D assets also serve as essential building blocks in the domains of physical simulation and embodied AI, enabling machines and robots to understand the elements in the real world. However, the creation of 3D assets is far from simple; it is often complex, time-consuming, and expensive process. Taking text prompts or an image as input, the digital 3D asset production pipeline commonly involves stages of 3D shape generation and texture generation, each requiring high level of expertise and proficiency in digital content creation software. In this report, we present Pandora3D, framework designed for high-quality 3D shape and texture generation. The framework consists of two main components: 3D shape generation and texture generation. 3D Shape Generation: The 3D shape generation pipeline utilizes Variational Autoencoder (VAE) to encode implicit 3D geometries into latent space. diffusion network is then used to generate latents conditioned on input prompts, with modifications aimed at enhancing the models capacity. We also explore an alternative Artist-Created Mesh (AM) generation approach, which shows promising results for simpler geometries. Texture Generation: The texture generation process is multi-staged, starting with the generation of frontal images, followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. novel consistency scheduler is integrated into every stage of this process to ensure pixel-wise consistency among multi-view textures during inference, leading to seamless integration. *Equation contribution Project leader. The pipeline demonstrates effective handling of diverse input formats, leverages advanced neural architectures, and incorporates novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. 2. 3D Shape Generation 2.1. 3D Latent Space Diffusion The process begins by generating 3D shape from single image, multiple images, or text prompt. This involves the following steps: Variational Autoencoder (VAE): Compresses 3D geometries into latent space, enabling efficient representation and processing. Diffusion Network: Generates latent representations conditioned on the input prompts. This network is adapted from CLAY [43] / Craftsman [16] / LAM3D [4], with modifications to improve the capacity and performance of the model. 2.1.1 Efficient 3D Geometry Autoencoder For 3D geometry compression model, we build upon CraftsMan [16], which adopts structures introduced in 3DShape2VecSet [40] and Michelangelo [44]. Furthermore, we leverage the multi-resolution training strategy proposed in CLAY [43]. This approach encodes 3D geometry into latent space by progressively sampling additional points from 3D point cloud, which incrementally extends and refines the latent representation of the shape. Progressive sampling allows the model to focus on areas of higher geometric complexity, capturing both global structure and intricate details. The primary goal of our VAE is to generate expressive latent embeddings that effectively guide the diffusion process in subsequent stages. To enhance the efficiency of this process, we propose more advanced point sampling strategy. This method is designed to maximize the utility of the 3D point-cloud data by prioritizing points that contribute the most to capturing fine-grained features and spatial relationships. This enhancement not only increases the models capacity to handle large-scale data for improved scalability but also preserves the fine-grained details of 3D geometry. The design options of our VAE are illustrated in Fig. 1. We employ the model structure introduced in 3DShape2VecSet as our base model. This approach involves embedding the input point cloud, augmented with normal information RN 6, which is sampled from mesh , into latent code using learnable embedding function and cross-attention encoding module: = Îµ(X) = CrossAttn(q, osEmb(X)) , (1) where Rmd represents set of learnable queries that compress the sampled points into latent embedding. The cross-attention mechanism ensures effective integration of geometric and positional features into the latent space. The VAEs decoder is composed of successive self-attention layers followed by cross-attention layer. The cross-attention layer maps the latent embeddings back into 3D geometry, enabling reconstruction: D(Z, p) = CrossAttn(P osEmb(p), Self Attn(Z)) , (2) where denotes random query points in 3D space, these points query with the latent and output occupancy logits. This base VAE implementation is illustrated in Fig. 1 (A). Following the approach outlined in CLAY [43], we adopt multi-resolution training strategy to progressively upscale the models capacity. Specifically, we incrementally increase the number of sampled points from 4096 to 32768 while simultaneously extending the latent embedding dimensionality from 256 to 2048. This progressive training scheme gradually introduces more detailed input information to the model, enabling it to capture finer geometric details. At the same time, the expanded latent embedding length increases the models capacity to represent complex features. Together, these enhancements enrich the latent space, thereby providing more robust foundation for the subsequent diffusion model training. This multi-resolution approach ensures an efficient and scalable training process, optimizing both the models performance and its ability to generalize across diverse 3D geometries. Recall that the primary objective of our VAE is to generate expressive latent embeddings. While the previously mentioned approach progressively increases the number of sampling points, each object contains total of 500k points, leaving many points unsampled. This results in inevitable information loss, as not all geometric details are captured in the latent representation. Furthest point sampling [24] has the potential to mitigate this issue by selecting more representative points. However, this method is significantly slower compared to random sampling, making it less practical for large-scale training scenarios. This residual information loss can pose challenges during the diffusion process, as it may hinder the generation of high-quality latent embeddings. Consequently, the decoder is tasked with reconstructing fine-grained 3D details that might not be adequately represented in the latent embedding, potentially limiting the overall quality and fidelity of the reconstructed geometry. We have developed an enhancement to our Variational Autoencoder (VAE) model that allows it to operate without sampling, while still retaining all data points. straightforward approach might involve utilizing PointNet++ [24] to compress features from point cloud into few centroid points through the use of cascaded convolutional layers. However, this method demands substantial amount of memory, especially when managing point cloud data consisting of millions of points. To address this, our model optimizes the processing of large-scale point clouds more efficiently, reducing the memory burden without compromising the integrity and richness of the data. Alternatively, we opt to sample set of centroid points Rm6 and employ Q-former [15] style module to compress the raw point cloud data onto these centroids. The core component of the Q-former, the cross-attention mechanism, exhibits computational complexity O(2M d), where is the amount of centroids and is the size of the input point cloud and is feature dimension. Although utilizing memory-efficient attention method such as FlashAttention [5] helps, it remains resource-intensive and slow for processing large point clouds directly without sampling. To overcome these challenges, we propose the adoption of linear attention mechanism [26, 25] for implementing cross-attention within our Q-former module. The theoretical complexity of this approach is O(M d2 + d2). Given that >> >> d, the computational load of linear cross-attention is significantly reduced compared to traditional cross-attention methods. During the training of our VAE, we randomly select points from the original point clouds to serve as centroid points. These centroids, which have dimension larger than 6, act as queries in the Q-former and compress geometric information from the raw point cloud. Subsequently, these centroid points are processed by the VAE encoder to generate latent embeddings. This extended VAE is depicted in Fig. 1 (B). Similarly, we still adopt multi-reolustion training strategy to progressively increase the centroid points amount and latent embedding length to enlarge the latent embedding capacity. In addition, we empirically find that progressively increasing the training data amount can accelerate model convergence. Methods like 3DShape2VecSet and CLAY, which derive latents from sampled points of the original point cloud, inevitably suffer from detail loss. Our extension effectively addresses this issue of information loss and maximizes the utilization of high-resolution point clouds, thereby preserving more detailed and accurate representations. 2.1.2 Diffusion Pipeline The diffusion pipeline is illustrated in Fig. 2. We employ Multimodel Diffusion Transformer(MMDiT) [9] as our diffusion backbone, utilizing two pretrained models, specifically, CLIP-ViT-L/14 [27] as the global image feature extractor, and DinoInstead of employing DDPM, we utilize the flow matching schedule. V2-Large [20] for local image feature extraction. Following CLAY [43] methodology, the diffusion model is trained in coarse-to-fine manner. To enhance the image control effect, we use both global and local image feature as condition features of diffusion model. Global condition feature zglobal RL is extracted with ClIP vision encoder, meanwhile, local detail condition feature zlocal RL1024 is extracted with Dino vision encoder. The global condition and local condition are integrated into the diffusion model through MMDiT Block following Stable Diffusion 3 [9]. The diffusion model we use has 2.3B parameters and 28 layers MMDiT block. To enhance practical utility in 3D design workflows, we have extended our geometry generation framework to accept multi-view conditional inputs. This architectural advancement enables finer-grained geometric control through multi-view visual guidance. The system accommodates variable numbers of reference images (stochastically sampled from 1 to 4 views per instance) within unified architecture, eliminating requirement for fixed-size input configurations. All synthesized geometries maintain spatial alignment with the primary views coordinate system (defined by the first input image). Input Î¸i [0, 360). Multiview feature repreimages must be arranged in ascending azimuth order {Î¸1, Î¸2, . . . , Î¸n} where sentations are aggregated through ordered concatenation along the sequence dimension, preserving relative spatial-semantic correspondence across views. Accelerated convergence is achieved via progressive transfer learning, where parameters initialized from our single-view conditioned model undergo fine-tuning using multiview datasets while maintaining pretrained backbone weights during initial phases. Figure 1. 3D Geometry variational autoencoder. (A): Our base VAE for 3D geometry compression. (B) Extended VAE for efficient 3D geometry compression. Figure 2. Diffusion pipline. In the process of training diffusion model, the DinoV2, CLIP, and VAE Decoder components are kept frozen 2.2. Meshing and UV Unwrapping Once the 3D geometry is generated, it undergoes isosurface extraction, remeshing and UV unwrapping so that textureready triangle mesh is produced. 2.2.1 Isosurface extraction We perform modified version of marching cubes algorithm [17, 19] to efficiently extract watertight mesh from geometry tokens. Marching cubes traditionally require dense occupancy grid of occupancy values. Directly computing such dense grid with Eq. (2) incurs O(D3) time complexity that is prohibitively expensive at high resolutions D. To improve efficiency, we adopt coarse to fine strategy: starting from coarse grid resolution d0 D, we iteratively build sparse finer grid of resolution di+1 = 2di whose cells are subdivided from active cells in the coarser grid of resolution di that are close to the isosurface. This strategy ensures that most occupancy queries of Eq. (2) are confined within small margin around the isosurface and significantly reduces the number of queries required for isosurface extraction, achieving two to three orders faster mesh extraction. To guarantee watertight mesh, at the highest level dn = D, we expand the sparse active cells along the isosurface to eliminate holes and perform Lewiners topology check [14] to ensure manifoldness. We implement the sparse marching cubes as custom CUDA kernel function to maximize efficiency. 2.3. Remesh and UV unwrap The triangle meshes extracted from Marching cubes may contain poorly constructed elements such as collapsed faces or slivers. Furthermore, they often exhibit high face count that could create problems for downstream applications. We overcome these issues with an optional remeshing step using either an off-the-shelf quad-remesher1 or isotropic remeshing [21] followed by QEM triangle decimation [10]. In addition, we use the open source project UV-Atlas [45] for UV charting and packing. At this point, we obtain polygon mesh that is ready for texture generation. 2.4. Alternative Approach: Artist-Created Meshes Generation An alternative approach we explored involves directly generating the mesh, bypassing the initial generation of geometry as an implicit function followed by mesh extraction. This method effectively produces meshes with reasonable topology, akin to those crafted by artists for simple shapes. However, it encounters difficulties when applied to complex geometries, where maintaining structural integrity and topological accuracy becomes challenging. 2.4.1 Mesh Compression Direct regression of vertex coordinates results in substantial memory consumption, which consequently limits the number of faces the model can handle. To mitigate this issue, we adopt the methodology proposed by BPT [38], which involves compressing the original vertex coordinates using block index compression and patchified aggregation. Specifically, for vertex vi = (xi, yi, zi), the block-wise indexing (bi, oi) is formulated as follows: bi = (xiO) B2 + (yiO) + ziO , oi = (xi%O) O2 + (yi%O) + zi%O . (3) In this formulation, the symbols and % represent division without remainder and the modulo operation, respectively. This approach segments the coordinates along each axis into blocks, each of length O. To further enhance the compression ratio, we employ the patchified aggregation technique as described in [38]. This technique aggregates the faces connected to the same vertex into non-overlapping patch and utilizes dual-block indices to denote the starting point of patch. Consequently, the offset vocabulary is shared between the center vertex and the surrounding vertices. The center patch is formulated as follows: Pc = (b c, oc, b1, o1, b2, o2, . . . , bn, on) . (4) In this context, critical for accurately referencing the spatial configuration of the patch within the compressed data structure. and oc denote the blocking index and the offset index of the center patch, respectively. These indices are To achieve this, we initially convert vertex coordinates into discrete values with resolution of R. Subsequently, we encode the mesh information, including vertices and faces, into discrete token sequence. This sequence can be decoded back into mesh using the same technique. It is important to note that this encoding and decoding process is governed by predefined rules and does not involve any learnable parameters. 2.4.2 Autoregressive Model for Mesh Generation In this section, we describe the methodology for generating novel shapes from various modalities using the compression technique outlined in the preceding sections. Fig. 3 illustrates the pipeline of our approach. Initially, mesh is encoded into discrete token sequences utilizing the method detailed previously. Subsequently, decoder-only autoregressive model is employed to predict subsequent tokens based on preceding ones. To facilitate multi-modality condition control, pre-trained condition encoder network is utilized to encode condition information, such as images, text, and point clouds, into latent features. These features serve as the contextual input for the decoder-only model. The resulting token sequence can then be decoded back into the final mesh using mesh decoder. It is important to note that both the mesh encoder and mesh decoder are purely rule-based, as previously explained, and do not involve any learnable parameters. Fig. 4 visualizes the meshes generated by our model, which exhibit superior topology consistency with minimal number of faces. 1https://exoside.com/ Figure 3. Pipeline for Artist-Created Mesh Generation. Initially, meshes are encoded into discrete token sequences. These sequences are then processed through decoder-only autoregressive model that utilizes Transformer network architecture. To enforce multi-modality condition control, pretrained condition encoder network is employed. This network effectively integrates diverse modalities, ensuring that the generated meshes adhere to specified conditions. Figure 4. Example Meshes Generated by Our Artist-Created Mesh Generation Model. The meshes produced by our model demonstrate superior performance in maintaining topological consistency, showcasing the effectiveness of our approach in generating high-quality artistic meshes. 3. Texture Generation The proposed texture generation pipeline consists of several stages, each contributing to the generation of consistent and high-quality textures. Fig. 5 illustrates the texture generation pipeline. The pipeline begins with 3D mesh without texture. Below we introduce each stage in detail. 3.1. Frontal Image Generation If the input prompt is text, frontal image is initially generated conditioned on depth map derived from the 3D geometry. This process involves rendering the 3D mesh into depth map and utilizing depth-conditioned diffusion models [42] to produce the frontal image. Alternatively, if the input is an image, we integrate the IP-Adapter [39] and ControlNet [42] to generate the frontal image. As illustrated in Fig. 6, both text and image prompts are converted into geometry-aligned frontal image, which serves as the input for subsequent texture generation. Figure 5. Texture Generation Pipeline (input image and mesh from Trellis3D). Figure 6. Both textual and visual prompts are transformed into frontal image that is aligned with the frontal-view geometry. 3.2. Multi-view RGB Image Generation single-view to multi-view image generator creates multi-view RGB images conditioned on the multi-view position maps and the frontal image. Please note that normal and depth maps can also be used here. We first train multi-view image generator with network structure similar with Zero123++ [30], then, we combine the ControlNet [42] with Zero123++ [30] conditioned on the position (XYZ coordinate) maps, enabling the generation of position-aligned multi-view images. Whether the frontal image originates from text and depth or is provided as input, the multi-view image generator generates six multiview images (each 512 512) starting from random Gaussian noise, with geometric conditions and photometric modules. An example is shown in the first image in Fig. 7. 3.3. Multi-view PBR Image Generation After obtaining multi-view rgb image conditioned on multi-view position maps, we generate the corresponding multi-view PBR (Physically Based Rendering) image via image-2-image diffusion model [28]. Specially, taking multi-view rgb image as input, we train image-2-image diffusion models to generate corresponding multi-view PBR image. This stage includes generating multi-view albedo, metallic, and roughness maps (each sub image with resolution of 512512). Please kindly note that we train two models in multi-view PBR image generation process, where one model estimate multi-view albedo Figure 7. Multi-view RGB, albedo, metallic and roughness images. Figure 8. High-resolution albedo images. image, and one model estimate multi-view metallic and roughness maps. Examples of the generated albedo, metallic and roughness images are shown in Fig. 7. 3.4. High-Resolution Refinement To further enhance visual quality, we upscale the albedo multi-view images by several iterative upscaling steps. Firstly, we apply two steps of image repainting, utilizing pre-trained SDXL [22] model conditioned on albedo, depth, XYZ coordinate maps, and the frontal image to upscale the albedo multi-view images to resolutions of 768768 and 10241024, introducing finer details. We then use Real-ESRGAN [36] to further enhances the multi-view textures to generate detailed high-resolution albedo maps (3072 3072), see Fig. 8. 3.5. Pixel-Wise Consistency Enforcement The multi-view generation stages may produce inconsistencies at the pixel level across different views. To address this, we implement consistent scheduler similar to TexPainter [41], which enforces pixel-wise consistency. Specifically, the multi-view textures are firstly baked onto the 3D mesh, and Poisson system is solved to produce seamless textures. Then, multi-view images are re-rendered and resampled into each view, ensuring consistency across different views and lighting conditions. The resampled views are used as the updated z0, which is plugged into the noise scheduler of the diffusion model similar to [41]. 4. 3D Model Data Processing We collect assets (mostly 3D models) from multiple sources and preprocess them to be training compatible, including converting mesh geometry to discrete sampling of implicit functions, multi-view image generation, and PBR rendering. Our Figure 9. Data processing pipline. The procedures marked in red are one-off implementations, while the green-boxed elements demand tailored development according to the algorithmic modules deployed on the dataset, we thereby provide exampled steps for jobs described in Section 2 and Section 3.2. data processing pipeline is demonstrated in Fig. 9. We will detail each step in the processing pipeline in the following contents of this section. 4.1. Data Origins The main data sources are: Objaverse [7], which is large open-source 3D object dataset with more than 700k objects, we use roughly 600k of it; OXL i.e. Objaverse-xl [6], which is an extension of the previous dataset Objaverse [7], we use roughly 200k of it; ABO [3], we only use the 8k 3D objects in this dataset; BuildingNet [29], which contains 2k building models; HSSD [13], which contains roughly 13k object models; Toy4k [31], which contains roughly 4k object models. Some models downloaded from the Internet, for instance, polygone dataset2. 10k private data provided by our partners. 4.2. Filter Mesh We first filter mesh using the following mesh proprieties, which are modified from the standards used in MeshXL [2]: Mesh face number, we only use meshes which face number is between 500 and 80k; Material number, we ignore all meshes with more than 100 materials; material number is defined by total material number in blender3. Annotations of the dataset. We use annotations of Objaverse to remove scanned objects. Exampled images of scanned objects can be found in Fig. 10. Scanned objects are harmful to the overall 3D generative model training process in various ways: From (B) and (D) of Fig. 10 we notice that most scanned objects possess large number of fragmented faces; meshes containing such large number of faces can only use the automatic method of unwrapping the mesh, which requires extremely large texture images to maintain reasonable mesh appearance. Hence, its very slow in rendering. Some scanned objects do not possess an actual body, like (C) in Fig. 10. Its only thin layer and not suitable for training multiple view diffusion methods like what we discuss in Section 3. Objects cannot be of pure color i.e. pure red or pure blue. This can be checked by checking objects material graph in Blender. 2https://polygone.art/ 3https://www.blender.org/ Figure 10. Typical images of scanned objects in Objaverse dataset [7]. (A) and (C) are rendered images of the two objects; (B) and (D) are corresponding object demonstration in blender. 4.3. Format Conversion For convenience of usage, we convert all 3D mesh formats to OBJ4. This is because most processing pipelines that are not part of DCC software i.e. Digital Content Creation software, cannot support read full information of complex 3D formats like GLB5 and FBX6. If we wish to scale-up for future learning-based algorithms that need to directly read information from meshes, we have to convert 3D format to OBJ. However, directly converting 3D models to OBJ format often fails, mostly because default format conversion function in Blender cannot correctly deal with file path of texture images. We thus need some extra care of some certain file types. MAX to OBJ cannot be done directly, as MAX file is only supported by 3DSMax7 and OBJ exporting function in that software cannot correctly handle objects with multiple complex materials. This is because we use an extension of OBJ that supports PBR formats developed by Carla [8], which is not properly supported in 3DSMax. We thus first convert MAX files to FBX formats using 3DSMax, and convert FBX to OBJ using blender. GLB to OBJ can be done in blender, but to get correct textures extra care is needed in rebuilding material graph structure. This is because GLB specification has embedded texture images within mesh files. We first convert all GLB files to GLTF formats which extract texture files to disk; then we go through the material graph of GLTF format and rebuild connections in new mesh. PMX to OBJ can be done in blender using codes derived from Cats plugin8. PMX9 is often used by some creators of the anime industry. 4.4. Classify Mesh The propose of this step is to eliminate low-quality mesh as thoroughly as possible, and to mark object of distinctive types. This provides the following advantages: Low-quality mesh can disturb the overall training process. Labeling mesh with its class allows us to fine-tune diffusion model on data from certain domains. The filtering process descirbed in this section is modified from the process used in MeshXL [2], as shown in Fig. 11. After this process, we render 9 images surrounding the mesh, and use the HunYuan vision model10 to filter the mesh by these rendered images. Note that you can substitute this vision model with any other vLLM models, such as GPT-4V11. As shown 4https://en.wikipedia.org/wiki/Wavefront_.obj_file 5https://www.khronos.org/gltf/ 6https://www.autodesk.com/products/fbx/overview 7https://www.autodesk.com/products/3ds-max/overview 8https://github.com/absolute-quantum/cats-blender-plugin 9https://gist.github.com/felixjones/f8a06bd48f9da9a4539f 10https://cloud.tencent.com/document/product/1729/104753 11https://openai.com/index/gpt-4v-system-card/ Figure 11. Classify mesh using vLLM by making the vLLM model to describe the model using 9 rendered images. Some parts of the structured data (marked with red box) can be used to classify mesh; the aggregated full sentence can be used as the caption of the mesh. in Fig. 11, the text prompt guides the vLLM to describe the appearance of the object, and check if the object is of certain classes. Then, we use LLM [33] to convert unstructured text into structured format with structures similar to JSON. Labels in these structured texts can be used as the objects class. 4.5. Generate watertight mesh Generating watertight mesh, which is essential for 3D-DiT training, is generating an envelope tightly conforming to the models exterior geometry, rather than using both interior and exterior geometry. The latter, like Manifold [11, 12] and other works [23] that can provide similar results, is not suitable for 3D-DiT training. We use similar watertight mesh generation method from 3DShape2VecSet [40], which is adopted from Stutzs work [32] and used in occupancy networks [18]12. The method is based on TSDF fusion of group of depth map rendering around the object. However, this method will slightly vary the exterior shape of the mesh due to the following reasons: The method calculates closing of each depth map, which fills small gaps on depth image but remove some details. Distortion caused by such reason can be reduced by increasing the definition of depth image, as closing of the image is often calculated using fixed window size. The method applies bias that is half voxel size. Distortion caused by such reason can be reduced by high-definition SDF volume. However, increasing the definition of the SDF volume will lead to mesh with particularly large face number, which requires the pipeline to provide decimated mesh. Its worth noticing that most decimating methods in DCC software uses QEM triangle decimation [10], which will sometimes provide ill-formed faces i.e. the shape of the face may not be suitable for further developments. We therefore also recommend using ACVD [1, 35, 34] in the decimating step. We have also developed baking tool based on baking function in Blender to provide texture for watertight mesh. 4.6. Rendering and Sampling All meshes are normalized to tightly coupled (radius is 1) bounding sphere using Welzls algorithm [37]. We prefer not to use bounding boxes because arbitrarily rotating objects within them may cause the objects to extend beyond the confines of the box. Rendering is done in Blender using EEVEE13 renderer, while sampling is done using trimesh14. We sample three groups of points, each with 500k points, which is similar to sampling approach used by For 3D geometry compression model, we build upon CraftsMan [16], who adopts structures introduced in 3DShape2VecSet [40]: We perform uniformly random sampling within the circumscribed cube of the bounding sphere, yielding SPACE points. We sample group of points just on surface of the watertight mesh, yielding SURFACE points. We compute Gaussian curvature of each vertex on the mesh and use this curvature as importance of each area on the mesh: more points will be sampled on areas with higher curvature. 12We directly use implementation from https://github.com/autonomousvision/occupancy_networks/. 13https://docs.blender.org/manual/en/latest/render/eevee/introduction.html 14https://trimesh.org/ Figure 12. Visual results with color image as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures. We perform uniformly random sampling on the surface of the mesh, and add small bias to coordinates of each sampled point, yielding NEAR-SURFACE points. 5. Experiments Fig. 12 and Fig. 13 illustrate the results of 3D generation with the prompt and the image as input. As shown in these figures, which shows that our Pandora3D system could faithfully recover the shape and texture with fine-grained details and produce neat space without any floaters. 6. Conclusion In this report, we present Pandora3D, framework designed for high-quality 3D shape and texture generation. 3D shape generation and texture generation are proposed in Pandora3D. In specific, the 3D shape generation utilizes Variational Autoencoder (VAE) to encode implicit 3D geometries into latent space; then, diffusion network is used to generate latents conditioned on input prompts, with modifications aimed at enhancing the models capacity. Meanwhile, we also explore an alternative Artist-Created Mesh (AM) generation approach, which shows promising results for simpler geometries. The texture generation process is multi-staged, starting with the generation of frontal images, followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. novel consistency scheduler is integrated into every stage of this process to ensure pixel-wise consistency among multi-view textures during inference, leading to seamless integration. Experiment results demonstrate the effectiveness of Pandora3D handling of diverse input formats to produce high-quality 3D content."
        },
        {
            "title": "References",
            "content": "[1] Audette, Rivi`ere, Ewend, Enquobahrie, and Valette. Approach-guided controlled resolution brain meshing for fe-based interactive neurosurgery simulation. In Workshop on Mesh Processing in Medical Image Analysis, in conjunction with MICCAI 2011, pages 176186, 2011. 11 Figure 13. Visual results with prompt as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures. [2] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, et al. Meshxl: Neural coordinate field for generative 3d foundation models. arXiv preprint arXiv:2405.20853, 2024. 9, 10 [3] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2112621136, 2022. 9 [4] Ruikai Cui, Xibin Song, Weixuan Sun, Senbo Wang, Weizhe Liu, Shenzhou Chen, Taizhang Shang, Yang Li, Nick Barnes, Hongdong Li, et al. Lam3d: Large image-point-cloud alignment model for 3d reconstruction from single image. arXiv preprint arXiv:2405.15622, 2024. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 3 [6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 9 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 9, 10 [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 116. PMLR, 2017. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [10] Michael Garland and Paul Heckbert. Surface simplification using quadric error metrics. conference on Computer graphics and interactive techniques, pages 209216, 1997. 5, 11 In Proceedings of the 24th annual [11] Jingwei Huang, Hao Su, and Leonidas Guibas. Robust watertight manifold surface generation method for shapenet models. arXiv preprint arXiv:1802.01698, 2018. 11 [12] Jingwei Huang, Yichao Zhou, and Leonidas Guibas. Manifoldplus: robust and scalable watertight manifold surface generation method for triangle soups. arXiv preprint arXiv:2005.11621, 2020. 11 [13] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X. Chang, and Manolis Savva. Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation. arXiv preprint, 2023. 9 [14] Thomas Lewiner, Helio Lopes, AntËonio Wilson Vieira, and Geovan Tavares. Efficient implementation of marching cubes cases with topological guarantees. Journal of graphics tools, 8(2):115, 2003. 4 [15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. : Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 3 [16] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2, 11 [17] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 4 [18] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44604470, 2019. [19] Gregory M. Nielson. On marching cubes. IEEE Transactions on visualization and computer graphics, 9(3):283297, 2003. 4 [20] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [21] Nico Pietroni, Marco Tarini, and Paolo Cignoni. Almost isometric mesh parameterization through abstract domains. IEEE Transactions on Visualization and Computer Graphics, 16(4):621635, 2009. 5 [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 8 [23] Cedric Portaneri, Mael Rouxel-Labbe, Michael Hemmer, David Cohen-Steiner, and Pierre Alliez. Alpha wrapping with an offset. ACM Transactions on Graphics (TOG), 41(4):122, 2022. 11 [24] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 3 [25] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, et al. Transnormerllm: faster and better large language model with improved transnormer. 2023. 3 [26] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381, 2024. 3 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 7 [29] Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova, Melinos Averkiou, Andreas Andreou, Siddhartha ChaudIn Proceedings of the IEEE/CVF International huri, and Evangelos Kalogerakis. Buildingnet: Learning to label 3d buildings. Conference on Computer Vision, pages 1039710407, 2021. 9 [30] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 7 [31] Stefan Stojanov, Anh Thai, and James Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17981808, 2021. 9 [32] David Stutz and Andreas Geiger. Learning 3d shape completion under weak supervision. International Journal of Computer Vision, 128:11621181, 2020. [33] Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. 11 [34] Sebastien Valette and Jean-Marc Chassery. Approximated centroidal voronoi diagrams for uniform polygonal mesh coarsening. In Computer Graphics Forum, volume 23, pages 381389. Wiley Online Library, 2004. 11 [35] Sebastien Valette, Jean Marc Chassery, and Remy Prost. Generic remeshing of 3d triangular meshes with metric-dependent discrete voronoi diagrams. IEEE Transactions on Visualization and Computer Graphics, 14(2):369381, 2008. 11 [36] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19051914, 2021. 8 [37] Emo Welzl. Smallest enclosing disks (balls and ellipsoids). In New Results and New Trends in Computer Science: Graz, Austria, June 2021, 1991 Proceedings, pages 359370. Springer, 2005. 11 [38] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C. L. Philip Chen. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. 5 [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [40] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 2, 11 [41] Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, and Xifeng Gao. Texpainter: Generative mesh texturing with multi-view consistency. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 8 [42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 6, [43] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3 [44] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 2 [45] Kun Zhou, John Synder, Baining Guo, and Heung-Yeung Shum. Iso-charts: stretch-driven mesh parameterization using spectral analysis. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, pages 4554, 2004."
        }
    ],
    "affiliations": [
        "Tencent XR Vision Labs",
        "The Australian National University"
    ]
}