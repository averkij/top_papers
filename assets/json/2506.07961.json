{
    "paper_title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
    "authors": [
        "Peiyan Li",
        "Yixiang Chen",
        "Hongtao Wu",
        "Xiao Ma",
        "Xiangnan Wu",
        "Yan Huang",
        "Liang Wang",
        "Tao Kong",
        "Tieniu Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/"
        },
        {
            "title": "Start",
            "content": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models Peiyan Li1,2,3,, Yixiang Chen1,3, Hongtao Wu2,,, Xiao Ma2,, Xiangnan Wu1 Yan Huang1,3,4, Liang Wang1,3, Tao Kong2, Tieniu Tan1,3,5, 1CASIA 2ByteDance Seed 3UCAS 4FiveAges 5NJU Project Lead, Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within consistent 2D image space. In addition, we propose scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Page: https://bridgevla.github.io/ Corresponding Email: wuhongtao.123@bytedance.com; tnt@nlpr.ia.ac.cn 5 2 0 2 9 ] . [ 1 1 6 9 7 0 . 6 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Leveraging pre-trained vision-language models (VLMs) [14] for developing large vision-language-action (VLA) models has become promising method for learning generalizable and robust manipulation policies [59]. However, most VLA models only incorporate 2D image inputs and require extensive efforts on data collection. On the other hand, 3D robot policies leverage 3D structural priors in model design and demonstrate exceptional sample efficiency in learning complex 3D robot manipulation tasks [1014]. Can we develop unified 3D VLA model which combines the effectiveness of VLA models with the efficiency from 3D policies? Although there have been some works exploring integrating 3D information into VLMs for developing 3D VLA 1 Figure 1 Overview. BridgeVLA is novel 3D VLA model that aligns the input and output within unified 2D image space. It is pre-trained on object grounding using 2D heatmaps and fine-tuned on action prediction for 3D manipulation. Experiment results in both simulation and the real world show that it is able to learn 3D manipulation both efficiently and effectively. models [15, 16], these works typically convert actions into token sequences that do not have spatial structure and use next-token prediction to predict actions. This strategy fails to take advantage of the 3D structural priors as previous efficient 3D policies [1014] that align the observation input and action output into unified space, therefore leading to poor sample efficiency. Another significant challenge in developing 3D VLA models lies in the misalignment between the 3D inputs used in action fine-tuning and the 2D image inputs used in original VLM pre-training, causing large distributional shift from the original VLM pre-training. To tackle the challenges mentioned above, as inllustrated in Fig. 1, we present BridgeVLA, novel 3D VLA model that achieves remarkable sample efficiency and strong generalization capabilities. To ensure input alignment with the pre-trained VLM backbone, BridgeVLA transforms 3D point cloud observation into multiple 2D images captured from different orthographic projection views [13, 14]. To leverage the structural priors of the 3D input, BridgeVLA is trained to predict 2D heatmaps for translational action prediction. The 2D heatmaps, generated from the tokens corresponding to the projection images, share the same resolution as these images, aligning the input observations and output actions within unified spatial structure. Given that the original VLM is pre-trained to predict token sequences, which is incompatible with our VLAs 2D heatmap output, we also introduce scalable pre-training method, which trains the model to ground objects with heatmaps conditioned on text inputs. This pre-training method equips the VLM with the capabilities to predict heatmaps before downstream fine-tuning for policy learning. Overall, our design aligns the input and output within shared 2D space in both pre-training and fine-tuning. We perform extensive experiments in both simulation and the real world to evaluate the proposed method. Results show that BridgeVLA is able to learn 3D manipulation both efficiently and effectively. It outperforms state-of-the-art baseline methods in RLBench [17], improving the average success rate from 81.4% to 88.2%. In COLOSSEUM [18], it showcases strong performance in challenging generalization settings, boosting the success rate from 56.7% to 64.0%. In GemBench [19], it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, we evaluate on seven different settings, spanning from 2 visual perturbations to manipulating objects from unseen categories. BridgeVLA surpasses state-of-the-art method by 32% on average and demonstrates strong performance in generalizing to multiple out-of-distribution settings. Notably, BridgeVLA is able to achieve success rate of 96.8% on 10+ tasks using only 3 trajectories per task for training, highlighting its superb sample efficiency. In summary, the contributions of this paper are threefold: We introduce BridgeVLA, novel 3D VLA model that efficiently and effectively learns 3D robot manipulation with vision-language model via input-output alignment with 2D heatmaps. We propose scalable pre-training method to equip the model with the capability to predict heatmaps conditioned on text inputs via object grounding. We conduct extensive experiments in both simulation and real-world environments to thoroughly evaluate the proposed method. Results show that BridgeVLA outperforms state-of-the-art methods in both settings and achieves exceptional sample efficiency in real-robot experiments."
        },
        {
            "title": "2 Related Work",
            "content": "Language-Conditioned Visuomotor Policies. Most language-conditioned visuomotor policies employ transformers to process 2D visual inputs and directly generate 3D actions for manipulation [59, 2025]. In these works, leveraging pre-trained vision-language models (VLMs) for developing large vision-language-action (VLA) models has become popular for its effectiveness on learning complex manipulation [59]. However, such 2D image-based policies typically require significant efforts on data collection, often needing hundreds of trajectories per task to learn effectively. On the other hand, 3D manipulation policies hold great potential for efficient learning by taking advantage of the spatial structure inherent in the 3D inputs. popular line of works take as inputs point cloud data [11, 12, 2628]. For example, Act3D [12] proposes to create 3D feature cloud by lifting image features to the observation point cloud and predicts translational actions via classification for 3D points in the observation space. Another line of works utilize voxels to represent the observation space and predict translational actions within the voxel space, unifying the input observation and output actions within the same space [10, 29]. Recently, RVT [13] and RVT-2 [14] propose to leverage orthographic projection of 3D point clouds to convert 3D signals to 2D images to avoid high computational cost on processing 3D inputs. Different from the above methods, our method aims to unify the effectiveness of VLA models and the efficiency of 3D policies within single cohesive framework, combining the best of both worlds. 3D Vision-Language-Action (VLA) Models. While 2D VLA models have been extensively studied, 3D VLA models [15, 28, 30, 31] remain relatively under-explored. Zhen et al. [15] build 3D-VLA on top of large language model (LLM) and train the model to perform 3D reasoning, multi-modal goal generation, and robot planning. Lift3D [30] proposes to enhance 2D foundation models (e.g., DINOv2 [32]) with implicit and explicit 3D robotic representation for learning 3D manipulation policies. FP3 [28] leverages transformer to fuse the information from point clouds, proprioceptive states, and language instructions. PointVLA [31] utilizes VLM and point cloud encoder to process 2D images and 3D point clouds, respectively. The embeddings from both encoders are injected into an action expert for action prediction. SpatialVLA [16] introduces Ego3D position encoding to inject 3D information into 2D image observation and adaptive action grids to represent robot movement in more transferable way. Our method is different from the above methods in that it is designed in way to take advantage of the spatial structure of 3D inputs in action prediction. In addition, it bridges the gap between the 2D image inputs of pre-trained VLMs and the 3D inputs by projecting the 3D inputs into multiple 2D images instead of injecting 3D information into the VLMs. Such design enables it to simultaneously leverages the broad knowledge in the VLM backbone and the spatial structure priors embedded in 3D inputs. 3 Figure 2 Model Architecture. (a) 2D Heatmap Pre-training: we train BridgeVLA on 2D object detection datasets. The model takes as inputs an image and language describing the target object and outputs 2D heatmap which highlights regions of interest that correspond to the target object. Note that the bounding box shown here is for illustrative purposes only; it is not present in the image when input to the model. (b) 3D Action Fine-tuning: the model takes as inputs three orthographic projection images of 3D point cloud and language instruction. It outputs three 2D heatmaps, which highlight the position of the end-effector in the next keyframe across all three views. For the remaining action components, it uses an MLP to process the image feature tokens to predict the rotation action, gripper action, and collision flag of the next keyframe."
        },
        {
            "title": "3.1 Preliminaries\nBridgeVLA aims to learn a multi-task 3D robot manipulation policy π, which maps the observation o and a\nlanguage instruction l to an action a:",
            "content": "(1) i=1 1, ai π : (o, l) (cid:55) containing trajectories. And each trajectory We assume access to set of expert demonstrations = {τ i}N )}. contains language instruction and sequence of observation-action pairs, i.e., τ = {li, (oi The observation is one or multiple RGB-D images captured from one or multiple viewpoints. Following prior works [10, 12, 13], the action consists of 6-DoF end-effector pose SE(3), target gripper state {0, 1}, and collision flag {0, 1} of the next key frame. The collision flag indicates whether the motion planner should avoid collisions while moving towards the target pose. key frame typically captures important or bottleneck steps in trajectory (detailed in appendix B.1) [33]. BridgeVLA operates through an iterative process: 1) predicting the action at conditioned on the current observation ot and instruction l, 2) moving to the predicted next keyframe pose Tt using sampling-based motion planner [3436], 3) updating observation and repeating until task completion or reaching maximum step Hmax. As illustrated in Fig. 2, BridgeVLA employs dual-phase training recipe. During pre-training, it is trained to predict 2D heatmaps on object detection datasets. During fine-tuning, point clouds are projected into multiple 2D images as inputs to the VLM backbone. The model is trained to predict 2D heatmaps for estimating the translational action and other action components. This design aligns the input and output within shared 2D space in both pre-training and fine-tuning. 1), ..., (oi , ai 3.2 2D-Heatmap Pre-training The VLM backbone was originally pre-trained to predict token sequences without spatial structure. To equip it with the same ability to predict heatmaps as downstream policy learning, we introduce pre-training stage which trains the model to ground target objects via heatmaps. Concretely, we leverage the 120K object detection split of RoboPoint [37] as our pre-training dataset. For each image, we construct the ground-truth 4 heatmap gt from the bounding boxes of all objects of interest. Specifically, for each object, we construct probability map with spatial truncation:"
        },
        {
            "title": "H gt",
            "content": "i (x) = (cid:26) pi(x) 0 if pi(x) pmin otherwise (2) where = (u, v) denotes the pixel position, pi(x) = exp (cid:0)x (cid:98)xi2/2σ2(cid:1), (cid:98)xi is the center of the object bounding box, and pmin is probability threshold. For all the objects of interest, we fuse the probability map of all objects via averaging and normalization to obtain gt: gt(x) = Havg(x) xΩ Havg(x) (cid:80) , where Havg(x) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i="
        },
        {
            "title": "H gt",
            "content": "i (x) (3) where Ω denotes the pixel space. Please refer to Fig. 9 for samples of the ground-truth heatmaps. As illustrated in Fig. 2, we input an image along with the text prompt describing the objects of interest into the VLM backbone of BridgeVLA. In this paper, we employ PaliGemma [1] as the VLM backbone, which consists of SigLIP vision encoder [38] and Gemma transformer backbone [39]. During its pre-training, PaliGemma takes as input one or multiple 2D images together with prefix text (e.g., question about the image) and outputs suffix text (e.g., an answer to the question). While the model uses causal attention for predicting suffix text tokens, it adopts bidirectional attention for the image tokens and the prefix text tokens. This allows the image tokens to fuse information from the prefix text. To predict the heatmap, we first rearrange the output image tokens according to their patch positions to reconstruct the spatial feature grid. convex upsampling block [40] then converts the grid into heatmap with the same resolution as the input image. The model is trained with cross-entropy loss to predict heatmaps that localize the position of all objects of interest in the image. We emphasize that the proposed pre-training strategy outputs spatially aware 2D heatmap, in contrast to the conventional next-token-prediction used in prior works [15, 16]. Moreover, this approach is highly scalable, as it can, in principle, leverage any vision-language datasets that can be formulated as heatmap prediction tasks, such as keypoint detection and semantic segmentation. 3.3 3D Action Fine-tuning During fine-tuning, we first reconstruct point cloud of the scene from the RGB-D images captured from calibrated cameras. To align with the 2D image input of the VLM backbone, we render three orthographic projection images of the point cloud from three viewpoints (top, front, and right) and use these images as the input images for the VLM backbone as in RVT [13] and RVT-2 [14]. These images, along with the task instruction, are then fed into the pre-trained VLM backbone to generate heatmap for each of the three views. Importantly, we do not incorporate any additional information (e.g., robot states) during the VLM forward pass to minimize the distribution shift between pre-training and fine-tuning. For translational actions, we back-project the heatmaps of all three views to estimate the scores of all 3D point grids distributed uniformly across the robot workspace. The position of the 3D point with the highest score determines the end-effectors translation in the next keyframe. Similar to previous works [13, 14], we use Euler angles to represent rotational actions where each axis is discretized into 72 bins. To predict the rotation, binary gripper action, and collision avoidance flag, we integrate features from global and local contexts. For the global feature, max-pooling is applied to the output tokens of each inputted orthographic projection image, resulting in three tokens in total one for each view. For the local feature, we extract token from the heatmap peak of each view, also resulting in three tokens in total. All these tokens are concatenated and passed through MLP to predict the rotation action, gripper action, and collision avoidance flag. Following the approach in prior works [14, 29], BridgeVLA adopts coarse-to-fine refinement strategy for accurate action prediction. After the initial prediction on the original point cloud, we zoom in and crop the point cloud with cuboid centered at the predicted translation. second forward pass is performed on the cropped, zoomed-in point cloud. The predicted action from the second pass is used for execution."
        },
        {
            "title": "Overall",
            "content": "Task Success Rate (%)"
        },
        {
            "title": "Models",
            "content": "Avg. SR (%) Avg. Rank"
        },
        {
            "title": "Place\nWine",
            "content": "Image-BC (CNN) [10, 41] Image-BC (ViT) [10, 41] C2F-ARM-BC [10, 29] HiveFormer [42] PolarNet [26] PerAct [43] Act3D [12] RVT [13] 3D Diffuser Actor [11] RVT-2 [14] BridgeVLA (Ours)"
        },
        {
            "title": "Models",
            "content": "Image-BC (CNN) [10, 41] Image-BC (ViT) [10, 41] C2F-ARM-BC [10, 29] HiveFormer [42] PolarNet [26] PerAct [43] Act3D [12] RVT [13] 3D Diffuser Actor [11] RVT-2 [14] BridgeVLA (Ours) 1.3 1.3 20.1 45.3 46.4 49.4 65.0 62.9 81.3 81.4 9.3 9.7 8.7 6.9 6.5 6.3 4.3 4.4 2.5 2."
        },
        {
            "title": "1.9\nPut in\nDrawer\n8.0\n0.0\n4.0\n68.0\n32.0\n51.2±4.7\n90.0\n88.0±5.7\n96.0±3.6\n96.0±0.0\n99.2±1.8",
            "content": "0.0 0.0 24.0 52.0 36.0 55.24.7 92.0 52.02.5 96.02.5 100.00.0 100.00.0 Put in Safe 4.0 0.0 12.0 76.0 84.0 84.03.6 95.0 91.23.0 97.62.0 96.02.8 99.21.8 0.0 0.0 24.0 76.0 92.0 89.64.1 92.0 99.21.6 100.00.0 99.01.7 100.00.0 Screw Bulb 0.0 0.0 8.0 8.0 44.0 17.62.0 47.0 48.05.7 82.42.0 88.04.9 87.26.6 0.0 0.0 4.0 0.0 4.0 5.64.1 27.0 11.23.0 65.64.1 40.00.0 88.02.8 Slide Block 0.0 0.0 16.0 64.0 56.0 74.013.0 93.0 81.65.4 97.63.2 92.02.8 96.02.8 0.0 0.0 20.0 100.0 100.0 70.42.0 94.0 88.02.5 96.81.6 99.01.7 100.00.0 Sort Shape 0.0 0.0 8.0 8.0 12.0 16.84.7 8.0 36.02.5 44.04.4 35.07.1 60.87.7 0.0 0.0 20.0 52.0 84.0 88.05.7 93.0 71.26.9 89.64.1 74.011.8 100.00.0 Stack Blocks 0.0 0.0 0.0 8.0 4.0 26.43.2 12.0 28.83.9 68.33.3 80.02.8 76.88. 4.0 0.0 0.0 0.0 0.0 2.43.2 3.0 4.02.5 24.07.6 38.04.5 58.410.0 Stack Cups 0.0 0.0 0.0 0.0 8.0 2.42.0 9.0 26.48.2 47.28.5 69.05.9 81.63.6 0.0 0.0 8.0 80.0 40.0 44.87.8 80.0 91.05.2 93.64.8 95.03.3 88.02.8 Sweep to Dustpan 0.0 0.0 0.0 28.0 52.0 52.00.0 92.0 72.00.0 84.04.4 100.00.0 87.21."
        },
        {
            "title": "Push\nButtons",
            "content": "0.0 0.0 72.0 84.0 96.0 92.83.0 99.0 100.00.0 98.42.0 100.00.0 98.42.2 Turn Tap 8.0 16.0 68.0 80.0 80.0 88.04.4 94.0 93.64.1 99.21.6 99.01.7 92.83.3 Table 1 Results on RLBench. The Avg. Rank column reports the average rank of each method across all 18 tasks, where lower values indicate better overall performance. BridgeVLA achieves the best performance in 10 out of 18 tasks. The training loss during fine-tuning consists of four components: = Ltrans + Lrot + Lgripper + Lcollision (4) Similar to pre-training, Ltrans is cross-entropy loss that supervises the heatmap prediction for translational actions. The ground-truth heatmap for each orthographic view is the normalized single-object probability map defined in Eq. 2, where (cid:98)xi represents the projected pixel position of the ground-truth end-effector position in the next keyframe. As we discretize the Euler angles for rotation into bins, we also apply cross-entropy loss in Lrot to supervise rotation prediction. For gripper action and collision avoidance, we use the binary cross-entropy loss in Lgripper and Lcollision as supervision. To enhance geometric robustness, random rigid-body transformations are applied jointly to the point cloud and the ground-truth action during training. Additional training details can be found in Appendix A."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we perform extensive experiments in both simulation and the real world to evaluate the proposed method. Through the experiments, we aim to answer four questions: Q1: How effective does BridgeVLA learn 3D robot manipulation compared to state-of-the-art methods? Q2: Is BridgeVLA capable of learning efficiently from very limited data (e.g., 3 trajectories per task)? Q3: How robust is BridgeVLA on handling visual disturbances (e.g., distractors, background, and lighting)? Q4: How does BridgeVLA generalize to novel object-skill combinations and objects from previously unseen categories?"
        },
        {
            "title": "4.1.1 Experiments on RLBench",
            "content": "Setup. RLBench [17] implements tasks in CoppeliaSim [44] using Franka Panda robot mounted with parallel-jaw gripper. The observation contains four RGB-D images captured from four calibrated cameras"
        },
        {
            "title": "Overall",
            "content": "Success Rate (%) Models R3M-MLP[46] MVP-MLP[47] PerAct[43] RVT[13] RVT-2[14] BridgeVLA (Ours) Models R3M-MLP[46] MVP-MLP[47] PerAct[43] RVT[13] RVT-2[14] BridgeVLA (Ours) Avg. SR (%) Avg. Rank All Perturbations MO-COLOR RO-COLOR MO-TEXTURE RO-TEXTURE MO-SIZE 0.8 1.6 27.9 35.4 56."
        },
        {
            "title": "64.0\nRO-SIZE",
            "content": "0.0 0.0 29.3 40.5 53.4 1.5 61.7 0.8 5.71 5.0 3.71 3.28 1."
        },
        {
            "title": "1.07\nLight Color",
            "content": "1.0 1.6 29.1 34.0 58.0 1.1 69.7 1.2 0.6 0.8 7.2 6.4 15.6 0.8 18.7 2.2 Table Color 1.4 1.6 30.4 30.0 62.6 0.9 75.7 0.9 0.4 1.2 24.0 26.0 53.0 0.9 60.5 1.1 Table Texture 0.2 1.0 23.2 45.2 56.6 0.9 71.3 0.7 0.0 0.0 29.2 31.3 54.6 0.6 63.8 0.1 Distractor 1.6 3.8 27.1 18.8 60.8 0.5 51.8 1.5 0.0 0.4 28.8 44.8 59.7 0.7 63.5 1.5 Background Texture 1.2 2.2 33.5 46.4 68.7 1.1 74.8 1.0 0.0 0.0 17.71 41.1 56.7 1.4 68.4 3.3 RLBench 2.0 2.0 39.4 53.4 68.8 1.3 73.1 0.2 1.8 4.44 35.6 35.3 60.9 0.9 69.3 1.0 Camera Pose 0.8 2.6 36.3 42.2 64.4 0.5 73.8 0.3 Table 2 Results on the COLOSSEUM Benchmark. The table shows the success rates across 14 generalization settings. The Avg. Rank column reports the average rank of each method across all perturbations, where lower values indicate better overall performance. Compared to the state-of-the-art baseline, BridgeVLA improves the average success rate by 7.3%. positioned at the front, left shoulder, right should, and wrist. Following previous works [1014], we perform experiments on 18 tasks from RLBench. These tasks span 1) non-prehensile manipulation (e.g., slide block to target), 2) pick-and-place (e.g., stack cups), and 3) high-precision insertion (e.g., insert peg). Each task is provided with 100 expert demonstrations. And each demonstration is paired with language instruction and multiple keyframes. Models are evaluated via binary success rates over 25 trials per task, with maximum of 25 action steps per trial. Baselines. We compare BridgeVLA with multiple baselines. (1) Image-BC (CNN) and Image-BC (ViT) [41] are two 2D baseline methods which predict the actions directly from 2D images using CNN and ViT as the backbone, respectively. (2) C2F-ARM-BC [29] predicts the next keyframe action in the voxel space with coarse-to-fine strategy. (3) PerAct [10] also operates in the voxel space and predicts the action with perciever transformer [43]. (4) HiveFormer incorporates historical information using unified multi-modal transformer architecture. (5) PolarNet employs PointNext [45] to encode the 3D scene and predicts both heatmaps and offsets for all points to estimate translational actions. (6) Act3D [12] predicts the next keyframe action by selecting the point with the highest score from set of randomly sampled points in the workspace. (7) 3D Diffuser Actor [11] generates 3D trajectories via diffusion process conditioned on 3D observation and language instructions. (8) RVT [13] uses multi-view transformer to aggregate information from multiple orthographic views of the point cloud observation. (9) And RVT-2 [14], the current state-of-the-art method, further improves the precision of its prior via coarse-to-fine strategy. In total, we evaluate BridgeVLA five times to minimize statistical bias. The results are shown in Results. Table 1. BridgeVLA outperforms all the comparing baseline methods, achieving an average success rate of 88.2% and an average rank of 1.9 across all the 18 tasks, establishing new state of the art in this benchmark. These results address Q1, demonstrating the effectiveness of BridgeVLA in learning complex 3D manipulation tasks. We highlight that BridgeVLA outperforms the best baseline method by large margin in Insert Peg (88.0% vs 40.0%) and Sort Shape (60.8% vs 35.0%). These two tasks demand highly precise alignment between the peg and hole and the block and sorter, respectively. The high success rates of our method showcase its strong capabilities of learning precise manipulation which is highly desirable in many industrial applications. Among the 18 tasks, BridgeVLA performs the worst in Place Cups, despite surpassing all the comparing baseline methods. We hypothesize this is because the target keypoints are often occluded in all orthographic projection views, which makes learning and prediction more challenging. In the future, we plan to explore dynamically selecting the projection views for rendering to avoid this problem."
        },
        {
            "title": "Average",
            "content": "L1 Hiveformer [42] PolarNet [26] 3D Diffuser Actor [11] RVT-2 [14] 3D-LOTUS [19] 3D-LOTUS++ [19] BridgeVLA (Ours) 30.4 38.4 44.0 44.0 45.7 48.0 50.0 60.3 1.5 77.7 0.9 91.9 0.8 89.1 0.8 94.3 1.4 68.7 0.6 91.1 1.1 L2 26.1 1.4 37.1 1.4 43.4 2.8 51.0 2.3 49.9 2.2 64.5 0.9 65.0 1. L3 35.1 1.7 38.5 1.7 37.0 2.2 36.0 2.2 38.1 1.1 41.5 1.8 43.8 1.2 L4 0.0 0.0 0.1 0.2 0.0 0.0 0.0 0.0 0.3 0.3 17.4 0.4 0.0 0.0 Table 3 Results on GemBench. We show the average success rates on the four evaluation settings of GemBench. BridgeVLA establishes new state of the art on this benchmark, achieving an average success rate of 50.0%."
        },
        {
            "title": "4.1.2 Experiments on COLOSSEUM",
            "content": "Setup. To systematically evaluate the generalization capabilities of BridgeVLA, we further evaluate on the COLOSSEUM benchmark [18]. The COLOSSEUM benchmark is an extension to the RLBench benchmark. The model is trained on the data from the original RLBench benchmark but evaluated in environments spanning 12 axes of perturbations. These perturbations, which are unseen during training, encompass changes in object texture, color, and size, backgrounds, lighting, distractors and camera poses. In total, the COLOSSEUM creates 20,371 unique task perturbations instances to comprehensively evaluate the generalization capabilities of the model. Specifically, our evaluation includes three steps: 1) train the model with the original RLBench data without perturbations (100 trajectories per task) on 20 tasks, 2) evaluate each task over 25 trials per perturbation, 3) compute the average success rate of all evaluated tasks for every perturbation. Besides the 12 types of perturbations, we also evaluate on basic variations from the original RLBench (denoted as RLBench in Tab. 2), and more challenging setting which combines all the 12 types of perturbations (denoted as All Perturbations in Tab. 2). Baselines. We compare BridgeVLA with five baseline methods. R3M-MLP and MVP-MLP are two 2D methods that utilize pre-trained visual encoders to process observation images and an MLP for action prediction. Specifically, R3M-MLP uses R3M [46] that is pre-trained on large-scale egocentric human videos; MVP-MLP uses MVP [47] that is pre-trained on millions of in-the-wild data. Both visual encoders show strong adaptability on various robotics tasks in both simulation and the real world. We also compare with three 3D methods introduced in Sec. 4.1.1, i.e., PerAct [10], RVT [13], and RVT-2 [14]. Results. Results are shown in Tab. 2. BridgeVLA outperforms all the comparing baseline methods in terms of average success rate, significantly outperforming the best baseline method by 7.3%. Among all the 14 evaluated perturbations, our method ranks the best among all methods in 13 of them. These results address Q3, showcasing that BridgeVLA possesses strong robustness against visual perturbation. More detailed results can be found in Tab. 5 and 6."
        },
        {
            "title": "4.1.3 Experiments on GemBench",
            "content": "Setup. To further evaluate the generalization capabilities of BridgeVLA, we perform experiments on the GemBench benchmark. GemBench [19] is hierarchical generalization benchmark built on the RLBench simulator [17]. Its training set contains 16 tasks (31 variations) covering seven core action primitivespress, pick, push, screw, close, open, and stack/put. The test set consists of 44 tasks (92 variations), categorized into four increasingly challenging settings: L1 (Novel Placements): L1 consists of the original 16 tasks (31 variations). The object placements are randomized within the workspace. In addition, chromatic distractors are introduced to test the ability to handle additional visual complexity. L2 (Novel Rigid Objects): L2 involves 15 unseen tasks (28 variations) that require interaction with 8 novel rigid objects using learned primitives. The generalization capabilities are evaluated across two categories: novel object-color compositions and novel object shapes. 8 L3 (Novel Articulated Objects): L3 consists of 18 unseen tasks (21 variations) that involve interacting with articulated objects. It evaluates the generalization capabilities across three categories: novel action-part compositions, novel object instances, and novel object categories. L4 (Novel Long-Horizon Tasks): L4 includes 6 complex long-horizon tasks (12 variations) that require combining multiple actions to finish whole task. In total, we compare with six baseline methods. 3D-LOTUS [19] processes point cloud inputs Baselines. It showcases notable multithrough language-conditioned point cloud transformer architecture [48]. Its enhanced variant, 3D-LOTUS++ [19], integrates the tasking capabilities and high training efficiency. generalization capabilities of large-scale models into 3D-LOTUS with modular architecture consisting of three components: (1) LLM-based task planning [49], (2) VLM-based object grounding [50, 51], and (3) motion control inherited from 3D-LOTUS. We also compare with four methods introduced in Sec. 4.1.1, i.e., Hiveformer [42], PolarNet [26], 3D Diffuser Actor [11], RVT-2 [14] Results. Results are shown in Tab. 3 and more detailed results can be found in Appendix B.4. BridgeVLA consistently outperforms all the comparing baseline methods in terms of average success rate across the four evaluation settings. Notably, BridgeVLA achieves state-of-the-art results in both the L2 and L3 settings, demonstrating strong generalization capabilities, addressing Q4. However, similar to most baseline approaches, BridgeVLA exhibits limited performance in the L4 setting, where each task comprises multiple sub-tasks. In the future, we plan to explore leveraging large language models (LLMs) for long-horizon task decomposition and further improve the performance in such setting."
        },
        {
            "title": "4.2 Real-Robot Experiments",
            "content": "In this section, we perform real-robot experiments to validate the effectiveness of BridgeVLA in the Setup. real world. Our real-robot setup includes Franka Research 3 robot arm mounted with parallel-jaw gripper  (Fig. 3)  . static ZED 2i depth camera is used to provide the colored point cloud observation. In total, we evaluate on 13 tasks (see Tab. 11 for full list of tasks). These tasks ranges from simple pick-and-place to complex long-horizon tasks, requiring the robot to open drawer and put items into the drawer. Each task contains 3-9 keyframes (see Fig. 7 and 8 for visualization). For each task, we collect 10 expert trajectories for training. In total, we design 7 different settings to comprehensively evaluate our models performance. (1) Basic: The model is evaluated in environments that are similar to the training data. (2) Distractor: Distractor objects that are visually similar to at least one target object are added to the scene. (3) Lighting: The model is tested in visually distinct lighting condition in which the lights are turned off. (4) Background: Three different tablecloths are used to change the background. (5) Height: All objects for manipulation are placed on drawer that is 9.5cm high. (6) Combination: We combine objects and skills that are not paired together in the training datasets. That is, while the objects (e.g., red block and green plate) and skill (e.g., place in B) are seen during training, the instruction that pairs them together is novel (e.g., place the red block in the green plate). In total, we evaluate 13 novel object-skill combinations (Fig. 11 and 12). (7) Category: To test whether BridgeVLA is able to transfer the broad knowledge from pre-training to downstream policy learning, we evaluate on manipulating objects from categories that are unseen in the robot training data. In total, we test 7 novel objects  (Fig. 13)  . Distractor, Lighting, Background, and Height aim to evaluate the robustness against visual disturbances, while Combination and Category evaluate the generalization capabilities for unseen instructions. We compare with RVT-2 [14], the strongest baseline method in the simulation experiments (Sec. 4.1). In addition, we compare with an ablated variant of BridgeVLA, BridgeVLA w/o Pre-train which excludes the 2D-heatmap pre-training. This ablation study helps us understand the effectiveness of our proposed pre-training method. Results. Results are shown in Fig. 3. BridgeVLA outperforms both comparing baseline methods in six out of the seven settings. All the three methods perform well in the Basic setting. However, RVT-2 struggles 9 Figure 3 Real-Robot Experiments and Results. We use Franka Research 3 robot arm and ZED 2i camera to capture point clouds of the scene. To evaluate the models performance, we design 7 different settings including one basic setting and six generalization settings. Experimental results show that BridgeVLA outperforms the state-of-the-art baseline method RVT-2 [14] by an average of 32%. in all the four visual disturbance settings, while BridgeVLA performs better and is able to maintain high performance in Distractor and Background settings. These results addresses Q3, indicating that BridgeVLA is able to handle visual disturbance robustly. To assess the data efficiency of BridgeVLA, we also train the model with only 3 trajectories per task. Remarkably, despite the limited data, BridgeVLA achieves success rate of 96.8% in Basic, matching the performance achieved with 10 trajectories per task. This result underscores the data efficiency of the proposed method, directly addressing Q2. Detailed per-task results are provided in Appendix C.4. BridgeVLA w/o Pre-train is not able to generalize well in both language-related generalization settings. Using CLIP [52] as the text encoder, RVT-2 is able to outperform BridgeVLA w/o Pre-train in both settings. BridgeVLA achieves the best performance across the two generalization settings. It significantly surpasses the two baseline methods in Combination, highlighting its ability to understand language semantics. We hypothesize that the 2D-heatmap pre-training equips BridgeVLA with the ability to connect the semantics in language instructions with image observations in the heatmap space. These results address Q4 and highlight the effectiveness and importance of the proposed 2D-heatmap pre-training. Although our method outperforms baseline methods in the Category setting, its absolute success rate is not high. common failure mode is that the robot often ignores the target object and moves directly to the destination during pick-and-place manipulation. We believe this relatively low performance is not due to BridgeVLA forgetting the knowledge gained from pre-training, as it still predicts heatmaps accurately when provided with samples from the pre-training dataset after fine-tuning (see Fig. 4 and Appendix C.3). Instead, we hypothesize that the reduced performance stems from two factors: 1) The images in the pre-training dataset are mostly captured from third-person views, which differ significantly from the projection images in our robot data; 2) The pre-training task focuses solely on object localization, whereas manipulation involves predicting keypoints that do not correspond to an object. To address these issues, we plan to expand both the scale and diversity of the pre-training dataset and explore more expressive action-decoding methods to better leverage the preserved pre-training knowledge. 10 Figure 4 Prediction on Pre-training Data after Fine-tuning. To simulate the multi-view inputs during fine-tuning, we repeat each pre-training image three times and feed them into the fine-tuned model to generate heatmaps. Note that these samples are not cherry-picked. Additional samples can be found in Appendix C.3."
        },
        {
            "title": "5 Conclusions & Future Work",
            "content": "This paper has introduced BridgeVLA, novel and efficient 3D vision-language-action (VLA) model built on top of pre-trained vision-language model (VLM) [1]. Keys to our method are that (1) it converts 3D inputs to 2D images to align with the 2D image inputs of the pre-trained VLM; (2) it aligns the input observation and the output action to unified 2D image space via 2D heatmap prediction; (3) it adopts scalable pre-training method to equip the VLM with the capability to predict heatmaps before fine-tuning on action prediction. Extensive experiments show that the proposed method is able to learn 3D manipulation efficiently and effectively in both simulation and the real world. In the future, we plan to explore pre-training on more diverse tasks, including semantic segmentation and keypoint detection. We also want to incorporate more expressive action-decoding methods (e.g., diffusion [21]) into the framework to continue improving the policy performance."
        },
        {
            "title": "References",
            "content": "[1] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [2] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. [5] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, et al. Openvla: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning. [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control, 2024. URL https://arxiv. org/abs/2410.24164. [7] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [8] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [10] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785799. PMLR, 2023. [11] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. In 8th Annual Conference on Robot Learning, 2024. [12] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: 3d feature field transformers for multi-task robotic manipulation. In Conference on Robot Learning, pages 39493965. PMLR, 2023. [13] Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694710. PMLR, 2023. [14] Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. Rvt-2: Learning precise manipulation from few demonstrations. In RSS 2024 Workshop: Data Generation for Robotics, 2024. [15] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. [16] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. [17] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020. [18] Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, and Dieter Fox. The colosseum: benchmark for evaluating generalization for robotic manipulation. arXiv preprint arXiv:2402.08191, 2024. [19] Ricardo Garcia, Shizhe Chen, and Cordelia Schmid. Towards generalizable vision-language robotic manipulation: benchmark and llm-guided 3d policy. arXiv preprint arXiv:2410.01345, 2024. 12 [20] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [21] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2024. [22] Peiyan Li, Hongtao Wu, Yan Huang, Chilam Cheang, Liang Wang, and Tao Kong. Gr-mg: Leveraging partiallyannotated data via multi-modal goal-conditioned policy. IEEE Robotics and Automation Letters, 2025. [23] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [24] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. [25] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. [26] Shizhe Chen, Ricardo Garcia Pinel, Cordelia Schmid, and Ivan Laptev. Polarnet: 3d point clouds for languageguided robotic manipulation. In Conference on Robot Learning, pages 17611781. PMLR, 2023. [27] Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. M2t2: Multi-task masked transformer for object-centric pick and place. arXiv preprint arXiv:2311.00926, 2023. [28] Rujia Yang, Geng Chen, Chuan Wen, and Yang Gao. Fp3: 3d foundation policy for robotic manipulation. arXiv preprint arXiv:2503.08950, 2025. [29] Stephen James, Kentaro Wada, Tristan Laidlow, and Andrew Davison. Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1373913748, 2022. [30] Yueru Jia, Jiaming Liu, Sixiang Chen, Chenyang Gu, Zhilue Wang, Longzan Luo, Lily Lee, Pengwei Wang, Zhongyuan Wang, Renrui Zhang, et al. Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. arXiv preprint arXiv:2411.18623, 2024. [31] Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, and Yichen Zhu. Pointvla: Injecting the 3d world into vision-language-action models. arXiv preprint arXiv:2503.07511, 2025. [32] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [33] Edward Johns. Coarse-to-fine imitation learning: Robot manipulation from single demonstration. In 2021 IEEE international conference on robotics and automation (ICRA), pages 46134619. IEEE, 2021. [34] Ioan A. Şucan, Mark Moll, and Lydia E. Kavraki. The Open Motion Planning Library. IEEE Robotics & Automation Magazine, 19(4):7282, December 2012. https://ompl.kavrakilab.org. [35] James Kuffner and Steven LaValle. Rrt-connect: An efficient approach to single-query path planning. In Proceedings 2000 ICRA. Millennium conference. IEEE international conference on robotics and automation. Symposia proceedings (Cat. No. 00CH37065), volume 2, pages 9951001. IEEE, 2000. [36] David Coleman, Ioan Sucan, Sachin Chitta, and Nikolaus Correll. Reducing the barrier to entry of complex robotic software: moveit! case study. arXiv preprint arXiv:1404.3785, 2014. [37] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. [38] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 13 [39] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [40] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [41] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 9911002. PMLR, 2022. [42] Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia Pinel, Makarand Tapaswi, Ivan Laptev, and Cordelia Schmid. Instruction-driven history-aware policies for robotic manipulations. In Conference on Robot Learning, pages 175187. PMLR, 2023. [43] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. In International Conference on Learning Representations. [44] Eric Rohmer, Surya PN Singh, and Marc Freese. V-rep: versatile and scalable robot simulation framework. In 2013 IEEE/RSJ international conference on intelligent robots and systems, pages 13211326. IEEE, 2013. [45] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Advances in neural information processing systems, 35:2319223204, 2022. [46] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. [47] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. [48] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48404851, 2024. [49] AI@Meta. Llama 3 model card. 2024. [50] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36:7298373007, 2023. [51] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 14 Pretrain RLBench Finetune COLOSSEUM Finetune GemBench Finetune Real-robot Finetune learning rate optimizer batch size warmup steps 5e-5 AdamW 384 400 8e-5 AdamW 192 8e-5 AdamW 192 8e-5 AdamW 160 2e-5 AdamW 192 Table 4 Training hyperparameters for BridgeVLA"
        },
        {
            "title": "A Training",
            "content": "Detailed training configurations are summarized in Tab. 4. Throughout both pre-training and fine-tuning, we keep the SigLIP vision encoder and language token embeddings frozen. Computational Resources: 1. Pre-training: 8 NVIDIA A100 GPUs for 3,800 steps (2 hours) 2. RLBench fine-tuning: 48 NVIDIA H100 GPUs for 83,000 steps (20 hours) 3. COLOSSEUM fine-tuning: 48 NVIDIA H100 GPUs for 83,000 steps (20 hours) 4. GemBench fine-tuning: 40 NVIDIA A100 GPUs for 50 epochs (2.1 hours) 5. Real-world fine-tuning: 8 NVIDIA A100 GPUs for 300 epochs (1.5 hours)"
        },
        {
            "title": "B Simulation Experiments",
            "content": "B.1 Key frame Selection For all the simulation and real-robot experiments, we adopt the same key frame selection strategy as PerAct [10]. time step is labeled as key frame if (i) the robot is stationary, (ii) the gripper state changes, or (iii) the step is the final state of the episode. The robot is considered stationary when the absolute velocities of all joints fall below 0.1 rad/s. B.2 Data Following [10, 13, 14], we select 18 tasks from RLBench [17] to evaluate the performance of our method on complex manipulation tasks. These tasks are visualized in Fig. 5. To assess the generalization capability of BridgeVLA, we also evaluate on the COLOSSEUM benchmark [18] and GemBench [19]. The COLOSSEUM benchmark includes 20 basic tasks and 12 types of perturbations. These perturbations, which are unseen during training, encompass changes in object texture, color, and size, backgrounds, lighting, distractors and camera poses. The benchmark evaluates on all the 12 types of perturbations, setting with basic variations from the original RLBench, and more challenging setting which combines all the 12 types of perturbations. We visualize all perturbations except the one from the original RLBench in Fig. 6. For GemBench, the training set includes 16 tasks (31 variations) spanning seven fundamental action primitives (press, pick, push, screw, close, open, stack/put). The test set includes 44 tasks (92 variations) organized into four increasingly challenging settings. Unlike RLBench and COLOSSEUM, where demo augmentation is used, we train BridgeVLA using only keyframes from each trajectory without performing any demo augmentation in GemBench. 15 B.3 Detailed Results on COLOSSEUM For the COLOSSEUM results in Tab.2, we use the results of R3M-MLP [46], MVP-MLP [47], RVT [13], and PerAct [10] from the original COLOSSEUM paper [18]. For RVT-2 [14] and BridgeVLA, we perform our own training and evaluation process. We performed three test repetitions and report the average success rate and variance of BridgeVLA and RVT-2 for each task under different perturbations in Tab.5 and Tab.6, respectively. B.4 Detailed Results on GemBench We show per-task success rates on the four settings of GemBench in Tab. 7, 8, 9, 10. The results of baseline methods are sourced from [19]. In total, we evaluate on 5 random seeds to reduce statistical variance. And for every seed, we run 20 trials per task variation. Real-Robot Experiments C.1 Experiment Setup Fig. 3 illustrates our real-robot setting. The platform comprises 7-DoF Franka Research 3 manipulator with parallel-jaw gripper and ZED 2i stereo camera mounted on tripod for capturing point clouds of the workspace. We collect expert trajectories with kinestheic teaching approach. We first move the manipulator to keypoints of an expert trajectory and then play back the keypoints to record the observation and action at each keypoint. C.2 Generalization Settings We evaluate on total of six generalization settings: Distractor, Lighting, Background, Height, Combination, and Category. For Distractor, Lighting, Background, and Height, we visualize these settings in Fig. 10. We visualize the settings of Combination and Category in Fig. 11 and Fig. 12, respectively. In Distractor, we add distractor objects that are visually similar to at least one target object to the scene. In Lighting, we evaluate the model in novel lighting condition in which the lights are off. In Background, we use three different tablecloths to change the background. For Height, we elevate all objects for manipulation with drawer that is about 10cm high. Distractor, Lighting, Background, and Height aim to evaluate the robustness against visual disturbances. In Combination, we combine objects and skills that are not paired together in the training datasets. That is, while the object for manipulation and the manipulation skill are seen during training, the instruction that pairs them together is novel. The setting of Combination helps us evaluate whether the model is able to generalize across novel object-skill combinations. In Category, we want to evaluate whether BridgeVLA is able to manipulate objects from categories that are unseen in the robot training data. In total, we test 7 novel objects. C.3 Preservation of Object Grounding Capability after Fine-tuning We observe that even after fine-tuning on robot action data, BridgeVLA retains the object grounding capability learned during pre-training. We visualize its predictions on the pre-training dataset after fine-tuning in Fig. 14. It is important to note that the samples in Fig. 14 are not cherry-picked. BridgeVLA does not forget its pre-training knowledge after 3D action fine-tuning. C.4 Per-task Success Rate We showcase per-task success rates of BridgeVLA in the basic setting in Tab. 11. Notably, BridgeVLA achieves exceptionally high success rates even with only 3 trajectories per task, highlighting its superb sample efficiency. 16 Task Name n r o b r l L M - L R - T O - basketball_in_hoop 100.00. 4.03.3 94.71.9 96.00.0 84.05.7 close_box close_laptop_lid empty_dishwasher 100.00.0 72.00.0 94.71.9 100.00.0 11.115. 82.73.8 - - 0.00.0 0.00.0 1.31. 1.31.9 - - - T O - - - - S - Z - o t L l l e x l r a i 100.00.0 68.00.0 100.00. 100.00.0 100.00.0 37.31.9 t d r a 100.00.0 e e a a 100.00.0 100.00.0 93.31.9 67.914.6 - - 100.00.0 100.00.0 98.71.9 98.71.9 100.00.0 97.31. 100.00.0 89.38.2 92.00.0 97.33.8 82.76.8 96.03. 100.00.0 96.00.0 1.31.9 4.03.3 0.00.0 0.00. 0.00.0 0.00.0 0.00.0 1.31.9 1.31.9 0.00. get_ice_from_fridge 94.71.9 5.31.9 86.71.9 90.77.5 90.75. - 84.03.3 73.31.9 96.03.3 98.71.9 89.37. 56.08.6 94.71.9 96.03.3 98.71.9 hockey 57.35. 9.33.8 44.06.5 50.78.2 insert_onto_square_peg 93.33.8 23.32. 52.03.3 94.71.9 meat_on_grill move_hanger open_drawer 96.00. 9.31.9 32.00.0 88.05.7 37.33.8 2.73.8 26.73. 46.73.8 96.00.0 60.03.3 97.31.9 - place_wine_at_rack_location 88.05.7 17.313.6 82.75.0 89.37.5 - - - - - - 50.713.2 46.78. 65.35.0 45.31.9 64.08.6 53.31.9 20.03.3 56.05. 49.35.0 50.75.0 76.08.6 85.33.8 70.73.8 84.00. 88.03.3 88.03.3 44.011.8 86.71.9 77.35.0 96.00. - - - 100.00.0 - 90.71. - - - 100.00.0 92.06.5 90.71. 98.71.9 97.31.9 100.00.0 100.00.0 52.00.0 84.00. 52.05.7 52.05.7 33.35.0 42.71.9 24.00.0 88.03. 93.31.9 100.00.0 90.71.9 100.00.0 94.71.9 96.00. 92.06.5 93.33.8 90.73.8 90.75.0 97.31.9 88.03. 74.73.8 90.76.8 92.03.3 92.08.6 put_money_in_safe 94.71. 6.75.0 78.71.9 74.71.9 81.36.8 89.35.0 92.03. - 37.312.4 84.03.3 84.03.3 84.03.3 89.31. 86.78.2 86.71.9 reach_and_drag 100.00.0 0.00.0 89.33. 96.00.0 94.75.0 84.05.7 94.71.9 38.75.0 92.03. 88.05.7 78.73.8 28.08.6 100.00.0 100.00.0 94.73. scoop_with_spatula 96.03.3 6.71.9 94.71.9 93.31.9 85.33. 85.33.8 78.73.8 86.75.0 90.71.9 88.06.5 77.31. 20.05.7 90.76.8 89.31.9 93.31.9 setup_chess 10.71. 0.00.0 1.31.9 8.00.0 8.03.3 slide_block_to_target 100.00. 24.03.3 74.71.9 58.73.8 29.31.9 66.71.9 61.36. 8.05.7 16.05.7 93.31.9 85.33.8 94.73.8 - - - - 92.03.3 50.71.9 48.03. - 0.00.0 0.00.0 0.00.0 0.00.0 0.00. - - - - - - 13.31.9 - 44.03.3 - 90.71.9 0.00. - - - - - - 12.05.7 21.38.2 13.33.8 5.31.9 20.05.7 16.05. 4.03.3 100.00.0 100.00.0 98.71.9 84.09.8 100.00. 100.00.0 100.00.0 62.71.9 64.03.3 65.38.2 26.77. 73.38.2 64.014.2 72.08.6 61.39.4 65.31.9 54.78. 37.35.0 70.78.2 66.77.5 72.06.5 93.33.8 94.77. 96.03.3 96.03.3 96.00.0 88.03.3 100.00.0 0.00. 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. stack_cups straighten_rope turn_oven_on wipe_desk Task Mean 73.90. 18.72.2 60.51.1 63.80.1 63.51.5 68.43.3 69.31. 61.70.8 69.71.2 75.70.9 71.30.7 51.81.5 74.81. 73.10.2 73.80.3 Table 5 Success Rates of BridgeVLA under Different Perturbations of COLOSSEUM."
        },
        {
            "title": "Task Name",
            "content": "l g s t u P R C - O - T O - basketball_in_hoop 100.00.0 10.02.0 99.01.7 94.02.0 97.01.7 close_box close_laptop_lid empty_dishwasher 93.04.4 36.08.5 70.06.6 86.04. 40.00.0 89.03.3 - - 0.00.0 0.00. 1.01.7 0.00.0 - - - T O - - - - S - Z - r C i l l e x l r a i 100.00. 86.03.5 95.01.7 94.02.0 84.06.3 89.03.3 t d r a 100.00. n R P m 99.01.7 100.00.0 86.03.5 62.02. - - 99.01.7 97.01.7 91.04.4 93.03. 97.01.7 94.02.0 99.01.7 84.04.0 92.00.0 96.02. 89.05.2 99.01.7 87.03.3 92.00.0 0.00.0 0.00. 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. 0.00.0 0.00.0 get_ice_from_fridge 95.01.7 11.04.4 88.05. 77.05.2 89.01.7 - 78.03.5 79.03.3 83.05. 89.01.7 70.04.5 86.04.5 81.05.2 96.02.8 96.02. hockey insert_onto_square_peg meat_on_grill move_hanger open_drawer 19.04. 31.03.3 0.00.0 0.00.0 26.04.5 30.04.5 13.01. 35.09.5 100.00.0 89.01.7 100.00.0 100.00.0 91.05. 0.00.0 61.04.4 83.018.4 99.01.7 25.04.4 63.04. - place_wine_at_rack_location 96.04.9 28.06.3 74.04.5 98.02. - - - - - - 40.04.9 24.02.8 13.03.3 12.08.5 15.03.3 9.03. 10.06.0 14.02.0 17.03.3 19.03.3 32.02.8 33.38. 21.01.7 30.02.0 9.01.7 4.04.9 9.03.3 35.03. 35.01.7 23.04.4 - - - 100.00. - 92.00.0 - - - 99.01. 98.02.0 100.00.0 99.01.7 100.00.0 100.00.0 100.00. 55.05.9 69.05.9 29.05.2 92.02.8 94.02.0 87.04. 22.02.0 88.00.0 92.00.0 99.01.7 86.08.2 100.00. 95.01.7 95.01.7 93.05.2 87.03.3 90.06.6 81.07. 87.04.4 95.06.6 83.03.3 89.05.9 96.02.8 91.05. put_money_in_safe reach_and_drag scoop_with_spatula setup_chess 77.04.4 86.06. 89.05.2 3.01.7 9.01.7 0.00.0 2.03.5 0.00. slide_block_to_target 100.00.0 11.04.4 45.01.7 stack_cups straighten_rope turn_oven_on wipe_desk"
        },
        {
            "title": "Task Mean",
            "content": "45.03.3 22.03.5 55.06.6 73.03.3 69.01.7 - 56.02.8 70.04.5 72.06.3 82.06.6 79.03.3 77.08. 62.06.0 72.05.7 80.05.7 60.06.9 67.05.9 87.06. 55.04.4 68.02.8 76.02.8 71.05.2 61.06.6 88.02. 86.03.5 81.05.9 75.04.4 87.03.3 84.04.9 92.07. 94.04.5 83.05.9 54.02.0 79.05.2 74.06.0 83.05. 92.02.8 91.01.7 89.04.4 0.00.0 4.02.8 4.04. 35.05.2 0.00.0 47.05.9 66.011.5 0.00.0 25.03. 91.04.4 50.010.8 68.04.9 - - - - 97.01.7 45.05.9 66.010.0 - 0.00. 0.00.0 0.00.0 0.00.0 0.00.0 - - - - - - 17.07.1 - 23.04.4 - 83.01.7 0.00.0 - - - - - - 7.05.2 7.03. 9.07.1 14.04.5 14.03.5 16.08.9 9.03.3 84.04. 96.00.0 83.05.2 82.08.7 100.00.0 100.00.0 100.00. 18.02.0 16.04.0 13.09.5 19.07.7 24.02.8 43.09. 40.02.8 53.01.7 68.02.8 39.011.4 42.07.2 72.08. 69.06.6 75.04.4 95.03.3 97.01.7 95.03.3 96.00. 96.04.9 89.07.1 96.02.8 0.00.0 0.00.0 0.00. 0.00.0 0.00.0 0.00.0 0.00.0 67.81.5 15.60. 53.00.9 54.60.6 59.70.7 56.71.4 60.90.9 53.41. 58.01.1 62.60.9 56.60.9 60.80.5 68.71.1 68.81. 64.40.5 Table 6 Success Rates of RVT-2 under Different Perturbations of COLOSSEUM."
        },
        {
            "title": "Method",
            "content": "Hiveformer [42] PolarNet [26] 3D diffuser actor [11] RVT-2 [14] 3D-LOTUS [19] 3D-LOTUS++ [19] BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer [42] PolarNet [26] 3D diffuser actor [11] RVT-2 [14] 3D-LOTUS [19] 3D-LOTUS++ [19] BridgeVLA(Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer [42] PolarNet [26] 3D diffuser actor [11] RVT-2 [14] 3D-LOTUS [19] 3D-LOTUS++ [19] BridgeVLA (Ours) Avg. 60.31.5 77.60.9 91.90.8 89.00.8 94.33.5 68.70.6 91.11.1 Open Drawer+2 597.4 907.1 974.5 944.2 936.0 754.5 992.0 PutIn Cupboard+3 746.5 884.5 8211.5 806.1 7211.2 22.5 846.6 Close Fridge+ 964.2 992.2 1000.0 7711.0 963.7 950.0 992.0 Pick& Lift+0 864.2 929.1 992.2 992.2 992.0 976.0 992.0 PutMoney InSafe+0 853.5 934.5 955.0 938.4 943.7 226.8 799.7 Close Jar+15 6413.9 992.2 1000.0 974.5 1000.0 1000.0 984.0 Pick& Lift+2 926.7 847.4 992.2 982.7 1000.0 943.7 1000.0 PutMoney InSafe+ 882.7 955.0 982.7 968.5 992.0 164.9 863.7 Close Jar+16 922.7 992.2 1000.0 982.7 1000.0 992.0 1000.0 Pick& Lift+7 932.7 885.7 992.2 1000.0 992.0 935.1 982.5 Reach& Drag+14 375.7 992.2 1000.0 8510.0 992.0 943.7 965.8 CloseLaptop Lid+ Close Microwave+0 LightBulb In+17 LightBulb In+19 Open Box+0 903.5 953.5 992.2 7713.0 982.5 282.5 972.5 PickUp Cup+8 837.6 827.6 962.2 992.2 974.0 868.0 962.0 Reach& Drag+ 327.6 992.2 992.2 942.2 1000.0 628.7 974.0 887.6 982.7 1000.0 1000.0 984.0 875.1 855.5 PickUp Cup+9 6912.9 794.2 974.5 992.2 963.7 886.8 943.7 Slide Block+0 992.2 1000.0 1000.0 1000.0 1000.0 1000.0 1000.0 124.5 7212.5 855.0 935.7 847.4 5510.5 905.5 PickUp Cup+11 6119.8 7210.4 982.7 992.2 944.9 914.9 992.0 Slide Block+ 9112.4 00.0 894.2 376.7 1000.0 655.5 905.5 136.7 716.5 882.7 918.2 859.5 458.9 877.5 Push Button+0 8411.9 1000.0 982.7 1000.0 992.0 1000.0 1000.0 Stack Blocks+30 65.5 3410.8 887.6 885.7 945.8 865.8 778.1 44.2 3211.5 112.2 74.5 992.0 558.9 7610.2 Push Button+3 686.7 1000.0 964.2 1000.0 992.0 1000.0 984.0 Stack Blocks+ 74.5 309.4 856.1 932.7 916.6 204.5 874.0 Open Door+0 5315.2 698.9 964.2 984.5 772.5 799.7 7012.3 Push Button+4 877.6 992.2 982.7 1000.0 1000.0 1000.0 984.0 Stack Blocks+39 64.2 3612.9 895.5 8811.5 904.5 2813.6 857.8 Open Drawer+ 1512.2 6112.4 829.1 935.7 838.7 6812.5 865.8 PutIn Cupboard+0 348.2 527.6 855.0 888.4 895.8 12.0 746.6 Table 7 Per-task Success Rate on GemBench Level 1."
        },
        {
            "title": "Method",
            "content": "Avg. Push Button+13 Push Button+15 Push Button+17 Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours) 26.11.4 37.11.4 43.42.8 51.02.3 49.92.2 64.50.9 65.01.3 Stack Blocks+24 972.7 1000.0 8713.0 1000.0 992.0 992.0 1000.0 Stack Blocks+27 8510.0 1000.0 816.5 1000.0 1000.0 1000.0 1000.0 Stack Blocks+33 00.0 12.2 6613.9 184.5 138.1 229.3 6110.7 Reach& Drag+5 12.2 618.2 00.0 912.2 954.5 942.0 943. 44.2 22.7 822.7 5616.7 409.5 837.5 5113.2 Reach& Drag+7 00.0 106.1 646.5 896.5 1810.8 6412.4 963.7 00.0 68.2 5014.6 4513.7 695.8 593.7 798.6 PutCube InSafe+0 42.2 4014.1 32.7 65.5 255.5 375.1 32.5 882.7 857.9 609.4 1000.0 1000.0 992.0 1000.0 Slide Block+2 00.0 00.0 00.0 00.0 00.0 279.8 129.3 Pick&Lift Cylinder+ 785.7 936.7 992.2 982.7 888.7 912.0 982.5 Pick& Lift+14 216.5 34.5 94.2 477.6 32.5 943.7 749.7 Slide Block+3 00.0 00.0 00.0 12.2 00.0 53.2 34.0 Pick&Lift Star+0 737.6 888.4 4317.9 984.5 696.6 943.7 992.0 Pick& Lift+ Pick& Lift+18 94.2 12.2 189.1 299.6 188.7 963.7 894.9 Close Jar+3 86.7 00.0 00.0 84.5 339.3 953.2 00.0 Close Jar+4 PickUp Cup+10 307.1 4811.0 845.5 818.2 893.7 794.9 913.7 LightBulb In+1 PickUp Cup+ 2213.5 468.9 6011.7 599.6 788.7 899.7 903.2 LightBulb In+2 00.0 2010.6 2316.8 77.6 715.8 982.5 666.6 Pick&Lift Moon+0 00.0 825.7 825.7 775.7 904.5 963.7 884.0 Pick&Lift Toy+0 44.2 2211.5 5117.8 6814.4 244.9 569.7 668.6 PutIn Cupboard+7 00.0 178.4 6010.0 66.5 418.6 437.5 745.8 PutIn Cupboard+8 882.7 936.7 919.6 944.2 808.4 296.6 953. 874.5 903.5 309.4 788.4 963.7 712.0 935.1 00.0 00.0 00.0 00.0 00.0 12.0 00.0 00.0 00.0 34.5 00.0 00.0 00.0 00.0 PickUp Cup+13 2610.6 166.5 6213.0 729.7 577.5 8410.2 906.3 Lamp On+0 74.5 1410.8 77.6 00.0 00.0 22.0 74. Table 8 Per-task Success Rate on GemBench Level 2."
        },
        {
            "title": "Method",
            "content": "Avg. Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours) 35.11.7 38.51.7 37.02.2 36.02.2 38.11.1 41.51.8 43.81.2 Open Drawer2+0 5911.9 914.2 198.2 8111.9 903.2 705.5 656.3 OpenLaptop Lid+0 1000.0 1000.0 1000.0 935.7 1000.0 866.6 950.0 Close Door+0 00.0 00.0 00.0 12.2 00.0 12.0 00.0 Open Drawer3+0 3911.9 298.2 12.2 00.0 228.1 414.9 876.0 Open Microwave+0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 Close Box+0 12.2 00.0 00.0 22.7 588.1 298.6 12.0 OpenDrawer Long+0 788.4 8411.9 155.0 848.2 5613.9 724.0 598.6 PutMoney InSafe+2 00.0 12.2 24.5 00.0 00.0 138.1 22.5 Close Fridge2+0 349.6 785.7 972.7 726.7 369.7 932.5 955.5 OpenDrawer Long+1 824.5 885.7 3513.7 3910.8 3311.2 5210.8 348.0 Open Drawer+1 00.0 44.2 00.0 62.2 00.0 00.0 00. CloseLaptop Lid2+0 529.1 268.2 236.7 4214.0 5410.7 509.5 774.0 OpenDrawer Long+2 494.2 638.4 269.6 118.9 178.1 238.1 1810.3 Close Drawer+0 835.7 2911.9 667.4 788.4 878.1 695.8 5812.9 Close Microwave2+0 157.1 746.5 887.6 718.9 857.1 992.0 5410.2 OpenDrawer Long+3 5711.5 377.6 7912.9 756.1 756.3 785.1 858.4 Close Grill+0 4410.8 4211.5 6513.7 94.2 296.6 1913.9 3512.3 Table 9 Per-task Success Rate on GemBench Level 3. Open Door2+0 3211.5 336.7 867.4 796.5 426.8 5210.3 6810.8 Toilet SeatUp+0 64.2 22.7 00.0 75.7 00.0 85.1 65.8 Open Box2+0 53.5 238.4 679.8 56.1 116.6 168.0 744.9 Open Fridge+0 00.0 42.2 75.7 00.0 45.8 00.0 72."
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours)"
        },
        {
            "title": "Method",
            "content": "Hiveformer PolarNet 3D diffuser actor RVT-2 3D-LOTUS 3D-LOTUS++ BridgeVLA (Ours) Avg. 00.0 0.10.2 00.0 00.0 0.30.3 17.40.4 00.0 PutItems InDrawer+4 00.0 00.0 00.0 00.0 00.0 00.0 00.0 Push Buttons4+1 00.0 12.2 00.0 00.0 34.0 767.4 00.0 Push Buttons4+2 00.0 00.0 00.0 00.0 00.0 498.6 00.0 Tower4+ Tower4+3 00.0 00.0 00.0 00.0 00.0 1710.8 00.0 00.0 00.0 00.0 00.0 00.0 3013.4 00.0 Push Buttons4+3 00.0 00.0 00.0 00.0 00.0 378.1 00.0 Stack Cups+0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 TakeShoes OutOfBox+0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 Stack Cups+3 00.0 00.0 00.0 00.0 00.0 00.0 00.0 PutItems InDrawer+0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 PutAllGroceries InCupboard+0 00.0 00.0 00.0 00.0 00.0 00.0 00. PutItems InDrawer+2 00.0 00.0 00.0 00.0 00.0 00.0 00.0 Table 10 Per-task Success Rate on GemBench Level 4. 19 Figure 5 Visualization of 18 RLBench [17] Tasks. 20 Figure 6 Visualization of Perturbations in COLOSSEUM [18]."
        },
        {
            "title": "Task",
            "content": "3 trajectories 10 trajectories"
        },
        {
            "title": "Put the RedBull can in the top shelf\nPut the soda can in the bottom shelf\nPut the RedBull can in the bottom shelf\nPut the coke can in the top shelf\nPlace the red block in the blue plate\nPlace the orange block in the green plate\nPut the wolf in the upper drawer\nPlace the red block in the purple plate\nPlace the yellow block in the green plate\nPress sanitizer\nPut the zebra in the upper drawer\nPut the giraffe in the lower drawer\nPut the zebra in the lower drawer",
            "content": "5/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 4/5 5/5 Table 11 Per-task Success Rates of BridgeVLA in the Basic Setting. 22 Figure 7 Real-Robot Rollouts (I). Figure 8 Real-Robot Rollouts (II). 24 Figure 9 Visualization of Pre-training Data. We list some samples of pre-training data. For every sample, the left shows the original image; the middle shows the bounding boxes of the objects of interest; the right shows the ground-truth heatmap used for training. 25 Figure 10 Visualization of the Distractor, Lighting, Background, and Height settings. Figure 11 Visualization of the Combination Setting (I). During training, the manipulated objects and skills are seen, but their combinations are unseen. 27 Figure 12 Visualization of the Combination Setting (II). During training, the manipulated objects and skills are seen, but their combinations are unseen. 28 Figure 13 Visualization of the Category Setting. In total, we evaluate on 7 objects from novel categories that are unseen during training. Figure 14 Visualization of BridgeVLAs Prediction on Pre-training Dataset after Fine-tuning. To simulate the multi-view inputs during fine-tuning, we repeat the input image three times and feed them into the fine-tuned model to generate heatmaps. For each sample, the first row shows the input image; the second row shows the heatmap prediction; the third row shows the ground truth."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "CASIA",
        "FiveAges",
        "NJU",
        "UCAS"
    ]
}