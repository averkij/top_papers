{
    "paper_title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
    "authors": [
        "Kangning Zhang",
        "Wenxiang Jiao",
        "Kounianhua Du",
        "Yuan Lu",
        "Weiwen Liu",
        "Weinan Zhang",
        "Yong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 2 8 4 1 9 0 . 1 1 5 2 : r LOOPTOOL: CLOSING THE DATATRAINING LOOP FOR ROBUST LLM TOOL CALLS Kangning Zhang1,2 Wenxiang Jiao2 Kounianhua Du1,2 Yuan Lu2 Weiwen Liu1,(cid:66) Weinan Zhang1,(cid:66) Yong Yu1,(cid:66) 1Shanghai Jiao Tong University 2Xiaohongshu Inc. {zhangkangning, kounianhuadu, liuww, wnzhang, yyu}@sjtu.edu.cn wenxiangjiaonju@gmail.com, luyuan3@xiaohongshu.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on models specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the models mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) augmented with external tools have become powerful paradigm for solving complex tasks beyond pure text generation (Qu et al., 2025; Schick et al., 2023; Qin et al., 2023). By invoking APIs, querying databases, and interacting with computational engines, such agents can tackle diverse real-world scenarios (Chen et al., 2025b; Xie et al., 2024; Pan et al., 2025) with high efficiency and adaptability. The development of robust tool-use capabilities, however, hinges on access to accurate, large-scale, and well-aligned training data that matches the models current competencies (Liu et al., 2025). widely adopted approach in this domain involves constructing large-scale tool-calling datasets through automated synthesis pipelines (Qin et al., 2023; Liu et al., 2024; Tang et al., 2023; Liu et al., 2025; Prabhakar et al., 2025), followed by supervised fine-tuning (SFT) or reinforcement learning (Wang et al., 2025; Shao et al., 2024). Despite notable advances, they almost invariably adopt static design, wherein data generation and model training are executed as two separate, noninteractive processes. In these settings, the training data is generated priori without awareness of the evolving state of the model, causing wasted capacity on trivial cases already mastered while leaving harder, underrepresented cases unresolved. Furthermore, the model plays no role in guiding or influencing data generation. This inherent disconnect leads to persistent mismatch between the models learning needs and the fixed nature of the available training data, thereby constraining both the efficiency and effectiveness of post-training. This work was done while Kangning Zhang and Kounianhua Du were interns at Xiaohongshu Inc. 1The code is accessible in https://github.com/Rednote-DeepExperience/LoopTool. 1 Another major challenge in tool-use data generation lies in the trade-off between cost-efficiency and data quality. Many pipelines depend on large closed-source models (OpenAI, 2023) for data generation and evaluation. While these models are capable of producing high-fidelity tool-calling sequences, their use incurs high API costs and low generation efficiency, making frequent largescale data synthesis impractical. Replacing them with more accessible open-source models often introduces noisy annotations, including incorrect arguments, incomplete function calls, or outputs misaligned with task requirements. Such errors inject misleading learning signals and can undermine model generalization (Liu et al., 2025). To address the limitations of static, costly, and error-prone tool-use data pipelines, we propose LoopToolan automatic, model-aware data evolution framework that couples data synthesis and training in closed loop. LoopTool begins with an Automated Tool-Augmented Data Construction stage, where tool specifications are synthesized and combined with multi-agent dialogue generation to produce diverse seed corpus of realistic tool-oriented conversations. This corpus undergoes an initial GRPO-based (Shao et al., 2024; DeepSeek-AI et al., 2025) post-training round. Each iteration then integrates three synergistic modules. First, Greedy Capability Probing (GCP) queries the fine-tuned model on the seed corpus using greedy decoding, revealing mastered, borderline, and failure cases. The predicted tool calls are used for automated error analysis, allowing the pipeline to target challenging, underperforming cases. Second, Judgement-Guided Label Verification (JGLV) employs high-capacity open-source judge model, Qwen3-32B (Yang et al., 2025), to compare each prediction against its reference labelidentifying genuine model errors as well as cases where the model output surpasses the original annotation. Such model-better-than-label examples replace noisy labels, enabling systematic self-refinement and progressively purifying the supervision signal. Third, Error-Driven Data Expansion (EDDE) transforms verified failure cases into new, structurally similar but contextually diverse challenging samples. Augmented samples preserve the core functional challenge while introducing varied conditions, ensuring scenario diversity. Across iterations, LoopTool incorporates corrected annotations, diversified hard samples, and refined seeds into subsequent training rounds, creating dynamic curriculum attuned to the models evolving strengths and weaknesses. This process focuses learning on non-trivial, high-value opportunities while progressively mitigating noisy-label effects. To balance quality and cost, LoopTool unifies the roles of data generator and evaluation judge within single, open-source model, Qwen3-32B, eliminating reliance on expensive closed-source APIs while maintaining high data quality. Strikingly, despite being trained entirely on data generated and evaluated by Qwen3-32B, the final 8B-scale LoopTool model surpasses the 32B generator in tool-use performance, highlighting the amplifying effect of iterative, model-aware data refinement. In summary, our main contributions are: We present LoopTool, the first fully automatic, model-aware iterative framework that tightly couples data generation and model training for tool-augmented LLM learning. By continuously diagnosing model weaknesses and synthesizing error-targeted samples, it ensures the training data dynamically adapts to the models evolving capabilities. We incorporate Judge-Guided Label Verification (JGLV), module that uses judge model to compare model predictions with reference annotations and automatically correct label errors with superior model outputs, progressively purifying the dataset. We design Error-Driven Data Expansion (EDDE) to leverage failure cases as seeds for generating new, structurally similar yet diverse challenging samples. Using the open-source Qwen3-32B for both generation and evaluation, EDDE continuously enlarges the pool of high-value training instances while avoiding the expense and dependency of closed-source APIs. Leveraging fully open-source, self-contained data generation and refinement, an 8B model trained by LoopTool surpasses its 32B generator and achieves state-of-the-art performance on BFCL-v3 (Patil et al., 2025) and ACEBench (Chen et al., 2025a) among models of similar scale."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Tool-Augmented Large Language Models. Integrating large language models (LLMs) with external tools has proven effective in overcoming their inherent limitations (Qu et al., 2025).Such 2 Figure 1: The overall closed-loop automatic pipeline of LoopTool, which couples (a) GRPO optimization, (b) Greedy Capacity Probing, (c) Judgement-Guided Label Verification, and (d) ErrorDriven Data Expansion for iterative tool-use enhancement. integration enables API invocation (Shen et al., 2023; Qin et al., 2023), interaction with knowledge bases (Lazaridou et al., 2022; Chen et al., 2025b), code execution (Wang et al., 2024), and multimodal processing (Hu et al., 2024; Ma et al., 2024). Early efforts mainly relied on supervised fine-tuning (SFT) with human-labeled tool-use data, focusing on accurate tool selection and argument generation (Schick et al., 2023; Qin et al., 2023; Liu et al., 2024). Recent advances explore autonomous tool creation and dynamic invocation, enabling adaptation to unseen APIs without predefined schemas. Benchmarks such as tau-bench (Yao et al., 2024; Barres et al., 2025), BFCL (Patil et al., 2025), and ACEBench (Chen et al., 2025a) provide standardized evaluations across tool selection, argument generation, multi-step reasoning, and multi-turn tool calling. Synthetic Data Generation for Tool Use. The scarcity and cost of high-quality tool-use datasets have driven research into automated synthesis pipelines (Qin et al., 2023; Liu et al., 2025; 2024; Prabhakar et al., 2025). Methods include multi-agent simulation (Alvarez et al., 2024; Tang et al., 2024), modular task composition (Chen et al., 2025c), and graph-based queryfunction synthesis (Arcadinho et al., 2024; Yin et al., 2025). Our work builds on this line but differs by introducing fully automated, model-aware, iterative paradigm in which synthesis is guided by post-training diagnostics and refined via systematic error correction. Reinforcement Learning for Tool-Use Optimization. Reinforcement learning (RL) increasingly enhances LLM reasoning and decision-making (Ouyang et al., 2022; Rafailov et al., 2024; Meng et al., 2024; Shao et al., 2024). In tool-use settings, GRPO has shown strong performance (Qian et al., 2025; Zhang et al., 2025). We embed RL into an interleaved traingenerate loop, enabling the model to iteratively improve through exposure to prior failures and progressively refined supervision."
        },
        {
            "title": "3 AUTOMATED TOOL-AUGMENTED DIALOGUE CONSTRUCTION",
            "content": "Before initiating our iterative model-aware data evolution process, we require diverse and highquality seed dataset Dseed to support the first round of post-training. To this end, we introduce an Automated Tool-augmented Data Construction that synthesizes realistic function-calling interactions by combining curated APIs with simulated multi-agent conversations. While this stage is not the core innovation of our work, it establishes the essential foundation for the following iterations. 3.1 HIERARCHICAL DUAL-TREE GUIDED API SYNTHESIS Our tool set comprises both real-world APIs collected from public resources (Liu et al., 2025; 2024; Qin et al., 2023) and synthetically generated APIs produced via Hierarchical Dual-Tree method. For each application domain, we define two complementary hierarchical structures: (i) Context Tree encodes the topical scope and functional granularity of the domain, from coarse categories at the root to fine-grained specializations at the leaves; (ii) Constraint Tree specifies structural and 3 functional constraints for valid APIs, such as naming conventions, parameter types and counts, and output formats. To synthesize an API, we independently sample leaf path from each tree and merge the results into structured prompt for the LLM, ensuring that both functional intent and structural requirements are satisfied. Rule-based validation is subsequently applied to ensure conformity and semantic coherence. Concrete examples of Context and Constraint Trees are provided in Appendix D."
        },
        {
            "title": "3.2 MULTI-AGENT TOOL-USE DIALOG GENERATION",
            "content": "The dialog generation stage incorporates two components: the Multi-Agent Dialogue Simulation and Correctness Verification for quality control. Multi-Agent Dialogue Simulation. We populate the seed dataset by simulating tool-usage dialogues with four distinct roles: Planner Agent designs coherent conversation flows based on sampled subset of tools and target number of dialog turns. This planning phase ensures realistic task decomposition and natural progression toward tool use. User Agent interacts with the assistant according to the Planners high-level outline, generating new requests, clarifying requirements, or providing additional information such as missing parameters. Assistant Agent selects appropriate APIs from the assigned subset, extracts candidate parameters based on the dialog context, executes tool calls, or synthesizes responses for the user. Tool Agent processes the tool calls according to the given API definitions and produces simulated execution results. For certain domains, we integrate real executable backends to return authentic responses through actual code execution. The dialog proceeds turn-by-turn until the predefined conversation length is reached. Rule-based and LLM-based Verification. All generated dialogues undergo two-tier verification process. Rule-based verification checks API call syntax, parameter coverage, type matching, and adherence to schema definitions. LLM-based evaluation leverages an open-source judge model (Qwen3-32B) to holistically evaluate every tool call step for contextual appropriateness, logical consistency, and alignment with the users intent. Only dialogues satisfying both stages are admitted into the initial seed dataset."
        },
        {
            "title": "ITERATIVE MODEL TRAINING AND DATA AUGMENT",
            "content": "To overcome the limitations of static data generation and support dynamically adaptive model training, we develop an automated iterative framework for tool-augmented LLM learning as shown in Figure 1. LoopTool integrates the GRPO Optimization, Greedy Capability Probing, JudgementGuided Label Verification, and Error-Driven Data Expansion into unified closed loop. This iterative cycle enables the model to assess its own capabilities continuously, target its weaknesses, and refine the quality of supervision data."
        },
        {
            "title": "4.1 GRPO TRAINING FOR TOOL CALLING",
            "content": "Data Format. We construct an initial seed tool-calling dialogue dataset Dseed through the Automated Tool-Augmented Data Construction in Section 3. Each multi-turn dialog sample is transformed into multiple GRPO training samples, which consist of the tuple: (T , ct, ), where denotes the current turn in the dialogue, as single conversation may contain multiple sequential tool calls. denotes the set of available tools at the current step, ct represents the historical dialogue context, which can be either single-turn user query or multi-turn conversation. is the tool call step from the conversation corresponding to the last user query. The models output Ot include two structured components: reasoning trace wrapped within <think> . . . </think> and the predicted tool invocation at inside <tool call> . . . </tool call>. detailed specification of both the single-turn and multi-turn training formats is provided in Appendix E. Binary Reward Definition. To quantify the quality of model-generated tool calls, we adopt Binary Reward scheme, which serves as simple yet effective rule-based reward function. For given context ct and the model output at, the reward is defined as: r(T , ct, , at) = (cid:26)1, ToolMatch(at, ) otherwise 0, (1) 4 GRPO Optimization. Given the tool sets and historical dialogue ct, the policy πθ sample group of candidate response {O1 } from the old policy πθold and their corresponding rewards , . . . , rG are {r1 }. We optimizes the πθ through maximizing the following objective: , . . . , OG , t , r2 JGRPO(θ) = (T ,ct)D,{Oi t}G i=1πθold"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:2)min (cid:0)ρi tAi t, clip(ρi t, 1 ϵ, 1 + ϵ)Ai where ρi = πθ(Oi πθold(Oi ct, ) ct, ) , Ai = mean({r1 ri , r2 std({r1 , r2 , . . . , rG , . . . , rG }) }) (cid:1) β KL(πθ πold)(cid:3) , (2) ϵ is the PPO clipping parameter, and β controls the strength of the KL penalty."
        },
        {
            "title": "4.2 GREEDY CAPABILITY PROBING",
            "content": "GRPO-based post-training often assigns near-zero advantage values to both trivially solvable and prohibitively hard samples, resulting in negligible parameter updates despite non-trivial computational costs (Yu et al., 2025). To mitigate this inefficiency, we introduce Greedy Capability Probing (GCP)an offline diagnostic stage to identify samples of substantive learning value. Given the training set Dj in the j-th iteration, we perform deterministic greedy decoding with the current policy πθj on every instance. For each tool-call sample (T , ct, ), the model generates prediction at Ot via greedy search. If at = , the sample is provisionally considered mastered under the assumption that its label is correct. Otherwise, the quadruple (T , ct, ; at) is passed to Judgement-Guided Label Verification (JGLV) for correctness assessment. To further quantify sample difficulty, we compute sample-level perplexity (PPL) as: (cid:32) PPL(T ,ct) = exp 1 L (cid:88) i=1 log pθ(oi , ct, o1:i1) (3) (cid:33) where is the output length and oi denotes the i-th token in the output sequence. High perplexity indicates low model confidence and suggests that the sample resides near the decision boundary, making it more valuable for continued training. In subsequent iterations, GCP selectively retains subset of these high-PPL cases DHPPL into the next-turn iteration. 4.3 JUDGEMENT-GUIDED LABEL VERIFICATION To mitigate the impact of noisy synthetic annotations and integrate automatic label refinement directly into the iterative loop, we introduce Judgement-Guided Label Verification (JGLV)a structured evaluation stage that distinguishes genuine model failures from annotation errors. and model prediction at for every mismatched case (T , ct, ; at) identified by Greedy CaIn each iteration j, pability Probing, we organize the tool reference label implementainto an open-source LLMin our tion, Qwen3-32B (Yang et al., 2025)-which outputs categorical decision: yjudge {PRED WRONG, LABEL WRONG, BOTH CORRECT, BOTH WRONG} and formatted error analysis emessage. Based on the judgment results, we define two key subsets of the evolving dataset: the Prediction Wrong set and the Label Wrong Set. specifications , dialogue context ct, DP DLW = {(T , ct, = {(T , ct, ; at) yjudge = PRED WRONG} , at) yjudge = LABEL WRONG} (4) are retained for retraining in the next iteration. We replace the DP the dataset into DLR classified as BOTH CORRECT, we retain only those with high-PPL into DHPPL as BOTH WRONG are directly discarded to avoid propagating noisy supervision. with at to transform (Refer to Appendix for judgement prompt and detailed samples). For samples . Samples identified in DLW j Compared with approaches that rely on large language model to directly regenerate or correct labels, JGLV reframes annotation refinement as comparative judgment task, where the judge model 5 only determines which of two existing candidates better satisfies the task specification instead of producing new output from scratch. Moreover, by incorporating outputs from the evolving current policy into the judgment process, JGLV leverages the models progressively improving tool-calling competence to assist data refinement. As training advances, the policy increasingly produces valid and high-quality tool invocations, enabling the replacement of incorrect labels with superior model outputs. This synergy transforms label verification into self-reinforcing mechanism that continuously generates cleaner and more representative training data."
        },
        {
            "title": "4.4 ERROR-DRIVEN DATA EXPANSION",
            "content": "j DLR . For each error seed (T , ct, While GCP and JGLV effectively identify mismatched cases and correct noisy labels, reusing these instances without modification often yields marginal benefit (see Section 5.4), especially when failures arise from systematic weaknesses rather than incidental noise. To directly broaden the models coverage of challenging tool-use scenarios, we propose Error-Driven Data Expansion (EDDE)an augmentation strategy that transforms verified failure cases into structurally similar hard samples. In iteration j, EDDE operates on the union of the DM = DM , EDDE parses the following structured components: tool subset , dialog context ct, correct call acorrect , and error analysis emessage. The generator is instructed to produce new tool-calling samples that mirror the structural complexity of the error seed (e.g., similar argument, multi-step dependencies). To avoid excessive similarity among the augmented samples derived from the same error seed, we additionally introduce scenario diversification constraints sconstraint. Specifically, each generation prompt is enriched with varied situational contextssuch as alternative user goals, different domain-specific constraints, or modified environmental conditionswhile preserving the core challenge (Refer to Appendix for error generation prompt and new generated samples). All EDDE-generated samples are subjected to the same two-tier validation pipeline outlined in Section 3.2including rule-based and LLM-based evaluation. Samples passing both filters are collected into: DEE = Verify(cid:0)Generate(DES identified by JGLV: DES , wrong call awrong ; at) DES and DLR )(cid:1). t Integration into the Iterative Loop. At the end of iteration j, the training dataset for the next round is constructed by merging multiple sources identified during the current iteration: Dj+1 = DES DEE DHPPL DSeed-new (5) where DSeed-new is small untrained subset from the initial seed dataset Dseed. This merged dataset Dj+1 is then used in the subsequent GRPO training round, with the policy πθj serving as the initialization. The full iteration pipeline is summarized in the Algorithm 1."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Table 1: Comprehensive evaluation of the BFCL-v3 (last updated on 2025-06-14). FC denotes that the model is tailored for functional calling. The best results in each category are highlighted in bold, while the second-best are underlined. Rank Overall Acc Model Single-Turn Multi-Turn Hallucination Non-Live AST Acc Live Acc Overall Acc Relevance 1 2 3 4 5 6 7 8 9 10 11 19 20 78.45 76.43 74.93 73.57 72.04 71.71 70.42 70.32 69.25 68.89 68.73 66.34 65.19 xLAM-2-70b-fc-r (FC) xLAM-2-32b-fc-r (FC) LoopTool-8B (Ours) watt-tool-70B (FC) xLAM-2-8b-fc-r (FC) GPT-4o-2024-11-20 (FC) GPT-4o-2024-11-20 (Prompt) GPT-4.5-Preview-2025-02-27 (FC) Qwen3-32B (FC) GPT-4.1-2025-04-14 (FC) ToolACE-2-8B (FC) . . . 88.44 89.27 89.52 84.06 84.40 86.81 87.67 86.12 88.90 85.42 87.58 (Ranks 1218 omitted for brevity) 88.81 87.06 72.95 74.23 84.72 77.74 66.90 78.85 79.88 79.34 77.83 79.92 80.05 78.54 78.50 Qwen3-8B (FC) Qwen3-8B (FC, self-host) 75.00 67.12 50.88 58.87 69.12 50.00 43.00 45.38 43.12 40.50 37.00 33.00 31. 66.67 88.89 61.11 94.44 77.78 83.33 72.22 66.67 72.22 77.78 72.22 77.78 77.78 Irrelevance 78.91 76.74 87.67 76.32 64.34 81.31 85.36 83.64 75.79 85.95 90.11 79.08 78.74 5.1 EXPERIMENT SETUP Benchmarks. We evaluate LoopTool by training LLMs with our data generation pipeline, using the open-source Qwen3-8B (Yang et al., 2025) as the backbone under pure RL fine-tuning. Experiments Table 2: Comprehensive evaluation of ACEBench for English Data (last updated on 2025-07-21). LoopTool-8B (Ours) achieves the best result in the 8B scale. Model GPT-4o Gemini-2.5-Pro-05-06 Qwen-Max GPT-4o-Mini Gemini-1.5-Pro Claude-3-5-Sonnet Doubao-Pro-32k Kimi-k2-0711 Qwen2.5-Coder-32B-Instruct LoopTool-8B (Ours) ToolACE-2.5-Llama-3.1-8B DeepSeek-V3 Qwen2.5-72B-Instruct Qwen3-8B Llama-3.1-70B-Instruct Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct Atom Single-Turn Multi-Turn Similar API Perference Summary Closed-Source Large Language Models Normal Special Agent Overall 90.0 83.7 88.0 84.3 82.3 66.7 75.3 87.0 86.0 86.0 87.7 88.0 81.3 80.3 83.7 70.3 73. 78.0 73.5 75.0 73.5 73.0 64.0 58.0 78.5 73.5 76.0 75.5 77.5 74.5 68.5 71.5 57.0 63.5 68.0 61.0 61.0 59.0 61.0 46.0 52.0 80.0 72.0 74.0 74.0 74.0 58.0 70.0 78.0 58.0 82.0 72.0 72.0 68.0 54.0 Open-Source Large Language Models 62.0 59.0 58.0 62.0 63.0 64.0 52.0 61.0 49.0 52.0 70.0 76.0 74.0 74.0 76.0 76.0 70.0 74.0 62.0 70.0 74.0 72.0 78.0 66.0 78.0 80.0 58.0 66.0 58.0 58.0 82.5 75.1 79.7 76.4 75.7 62.2 66.3 78.9 77.4 78.0 78.3 80.3 76.8 70.9 75.6 62.8 66.6 92.7 90.7 74.0 76.7 77.3 72.7 50. 81.3 80.0 80.7 76.0 72.7 74.0 78.0 29.3 49.3 25.3 56.0 52.5 60.0 27.5 26.0 44.0 26.5 65.0 50.0 43.3 35.9 34.0 37.5 34.2 41.0 15.0 18.5 81.1 75.8 75.1 68.9 68.5 62.2 56.0 77.4 73.9 73.4 71.1 71.1 70.0 67.1 57.9 51.8 48.1 are conducted on two representative benchmarks: BFCL-v3 (Patil et al., 2025) and ACEBench (Chen et al., 2025a), which provide comprehensive, executable function-call tasks for assessing function invocation capability. We also perform ablation studies to examine the contribution of individual modules. Benchmark details and evaluation metrics are provided in Appendix B.1. Implementation Details. GRPO training is implemented with the open-source RL library Verl (Sheng et al., 2025), using batch size of 128 and learning rate of 1 106. Each iteration trains for two epochs, resetting optimizer parameters while initializing from the previous checkpoint. To promote exploration, the actor rollout temperature is fixed at 1.0, with both entropy coefficient and KL weight set to 0. We apply the Clip-Higher (Yu et al., 2025) strategy, increasing Ehigh from 0.2 to 0.28 to encourage generation of high-entropy, low-probability tokens. In EDDE, is set to 4. Full hyperparameters are listed in Appendix B.2. 5.2 OVERALL PERFORMANCE ANALYSIS Result on BFCL and ACEBench. We compare LoopTool-8B model with various representation models in BFCL (Patil et al., 2025) and ACEBench (Chen et al., 2025a). We adopt the official evaluation script and report the average accuracy across categories. The results are summarized in Table 1 and Table 2, respectively. On both BFCL-v3 and ACEBench leaderboards, LoopTool-8B achieves SOTA performance among all 8B-scale open-source models and exceeds several larger counterparts. In BFCL-v3  (Table 1)  , our model attains an overall accuracy of 74.93%, ranking third across all models and surpassing the original Qwen3-8B by +8.59 points, with the highest Single-Turn and Live execution accuracy. Remarkably, LoopTool-8B also outperforms the 32B-scale Qwen3 modelused as both the data generator and judge in our pipeline, demonstrating the capability amplification achieved through our model-aware iterative data evolution. On ACEBench  (Table 2)  , LoopTool-8B obtains an overall accuracy of 73.4%, improving over Qwen3-8B by +6.3 points and consistently delivering balanced gains across diverse evaluation categories. 5.3 ITERATIVE DETAILS AND ANALYSIS 5.3.1 ITERATIVE DATASET DISTRIBUTION Table 3: Distribution of samples across iterative datasets in our LoopTool framework. # Total 18304 18304 18304 18304 D1 D2 D3 D4 # DES # DEE 0 1919 (10.48%) 3386 (18.50%) 3731 (20.38%) 0 6566 (35.87%) 8066 (44.07%) 8169 (44.63%) # DHPPL 0 4187 (22.98%) 4036 (22.06%) 4996 (27.29%) # DSeed-new 18304 (100%) 5632 (30.77%) 2816 (15.38%) 1408 (7.69%) The initial seed dataset Dseed includes 28k tool call samples. The corpus Dj+1 at iteration + 1 is constructed from four primary sources as illustrated in Eq (5). DSeed-new means the untrained new seed samples randomly drawn from the seed dataset Dseed. In each iteration, we gradually 7 Figure 2: The Iterative Performance across four iterations evaluated in BFCL-v3. The left y-axis represents Category Acc (bar chart), while the right y-axis denotes Overall Acc (line chart).Overall w/o Iterations refers to the result obtained under the same number of iteration steps, where we train solely on the initial seed dataset Dseed. reduce the proportion of untrained seed samples, ensuring that each training round incorporates newly generalized queries, while gradually converging on increasingly challenging samples. The detailed data statistics are presented in Table 3. 5.3.2 PERFORMANCE ANALYSIS OF ITERATIVE TRAINING FRAMEWORK We evaluate the effectiveness of the iterative training paradigm against conventional static data generation. As shown in Figure 2, the proposed LoopTool framework delivers consistent gains in toolcalling accuracy across iterations. Starting from the initial model (Original), each iteration leverages the closed-loop data evolution to uncover and remedy model deficiencies, leading to steady improvements. In contrast, the static Overall w/o Iterations setting produces substantially smaller improvements. Without the injection of newly synthesized hard cases or label refinements, the model rapidly saturates on the limited supervision, exhausting the information content of Dseed. Improvements plateau by Iteration 2 and decline after Iteration 3, indicating overfitting and growing mismatch between the fixed training distribution and the models evolving inference behavior. 5.4 ABLATION STUDY Table 4: We conduct the corresponding ablation experiments in Iteration 2 and Iteration 3, employing the data variants of D2 and D3. Overall accuracy and per-category accuracy are reported. Configuration Overall Acc Non-Live Acc Live Acc Multi-Turn Acc Iteration 1 (D1) Iteration 2 (D2) w/o High-PPL w/o JGLV Remove EDDE HighPPL-Replace Error-Seed Repetition Iteration 3 (D3) w/o High-PPL w/o JGLV Remove EDDE HighPPL-Replace Error-Seed Repetition 71.20 73.00 72.31 71.30 71.50 72.50 72.38 74.34 73.50 72.61 73.12 73.28 73. 87.10 88.42 88.17 87.90 88.06 88.10 88.40 89.79 89.12 89.17 88.75 89.40 88.15 81.34 82.10 81.59 82.05 81.47 82.36 81.87 84.01 82.79 82.59 82.45 83.96 83.74 44.62 49.00 46.25 43.88 45.00 47.88 46.88 49.75 48.90 46.25 48.75 46.88 48.38 To assess the contributions of each key component in LoopTool, we perform ablation experiments on BFCL-v3. Specifically, we design the following variants: (i) w/o High-PPL: Replace DHPPL with randomly samples that the model predicted correctly; (ii) w/o JGLV: Skip verification and entirely; (iv) HighPPL-Replace: Replace DEE Figure 4: Scaling performance with different model sizes. Figure 3: The Prediction Accuracy of Error Seed across iterations. treat all mismatches (at = ) as model errors; keep original labels without refinement. (iii) Remove EDDE: Drop DEE with an equal number of and duplicate DES high-PPL samples selected via GCP; (v) Error-Seed Repetition: Remove DEE to match data scale. From the results in Table 4, several key observations can be made: From the results in Table 4, several key observations can be made: Importance of high-PPL samples. w/o High-PPL lead to consistent accuracy drops, especially in Multi-Turn cases. Even replacing EDDE samples with high-PPL ones (HighPPL-Replace) sustains performance close to full configurations, confirming that high-PPL casesthough previously predicted correctlylie near decision boundaries of current policy and drive further refinement, in line with recent works (Liang et al., 2025; Shang et al., 2025). Necessity of JGLV. Skipping verification (w/o JGLV) significantly degrades accuracy, confirming that noisy or erroneous labels can mislead training. Without label refinement, such errors persist and even propagate when used by EDDE to generate variants, exacerbating noise in subsequent iterations. Effectiveness of EDDE The three variants of w/o EDDE in both Iteration 2 and Iteration 3, result in consistent drops in overall accuracy. To further quantify the direct contribution of EDDEoriginated samples, we compare the three variants with full configuration, testing the accuracy exclusively on this subset of historically wrong cases, with results shown in Figure 3. The result illustrates that simply re-training the model on the original erroneous seeds is insufficient for the model to master these difficult cases effectively. In contrast, EDDE synthesizes structurally similar, error-informed variants that preserve the underlying challenges of the original failure cases while offering additional diversity. This targeted augmentation enables the model to acquire the relevant patterns more reliably, thereby improving its performance on the original hard seeds. 5.5 SCALING PERFORMANCE WITH MODEL SIZE We evaluate LoopTool across backbone models from 0.6B to 8B parameters, measuring BFCL-v3 accuracy over two training iterations (Figure 4). Larger models consistently achieve higher accuracy in both the initial (Iteration 1) and refined (Iteration 2) stages, with greater absolute improvements in the second iteration. Specifically, the 0.6B model gains only +0.70 points, whereas the 8B model achieves +1.80 points. This scaling trend stems from GRPO-based post-training, which depends on the models ability to discover correct tool-use trajectories during rollout exploration. Larger models tend to identify such trajectories earlier, thereby amplifying the benefits of iterative refinement. 5.6 GENERALIZATION ABILITY EVALUATION Table 5: Generalization benchmark performance comparison between vanilla Qwen3-8B and LoopTool-8B. Bold indicates the better score for each task. Model MMLU-redux IFEval LiveCodeBench Math-500 AIME24 AIME25 Qwen3-8B LoopTool-8B 87.72 87.37 83.30 84.70 42.31 46. 91.40 92.60 60.00 70.00 56.67 66.67 9 Beyond tool-use performance, we evaluate whether the LoopTool-8B model maintains or improves generalization to non-tool-related domains. We compare LoopTool-8B with the vanilla Qwen3-8B (Yang et al., 2025) across six representative benchmarks: MMLU-redux (Gema et al., 2025), IFEVAL (Zhou et al., 2023), LiveCodeBench (Jain et al., 2024), Math-500 (Lightman et al., 2023), AIME24 and AIME25 AIM. LoopTool-8B consistently matches or surpasses Qwen3-8B across all domains, with notable improvements in instruction-following (+1.40 on IFEval), code generation (+3.84 on LiveCodeBench), and mathematics (+1.20 on Math-500, and gains on both AIME sets. These results indicate that the proposed iterative, model-aware data refinement and training paradigm avoids overfitting to tool-calling tasks. Instead, it fosters improved general reasoning and problem-solving skills, enhancing the models capacity to generalize across diverse scenarios."
        },
        {
            "title": "6 CONCLUSION AND LIMITATION",
            "content": "We present LoopTool, fully automated, model-aware pipeline that integrates data synthesis, label refinement, and GRPO-based post-training into closed loop to enhance tool-augmented LLMs. This unified process yields progressively cleaner and more challenging data without dependence on costly closed-source APIs, leveraging single open-source model for both judgment and generation. Experiments show that our 8B-scale model trained with LoopTool surpasses its own 32B-scale generator, highlighting the amplifying effect of iterative, model-aware data evolution. Nonetheless, LoopTool currently operates as an offline iterative framework, in which training data evolution cannot occur concurrently with the models training process. LoopTool is also strictly serial per iteration, with subsequent rounds only beginning after the previous iteration finishes. Future work will explore online or streaming variants, as well as parallelized iteration schemes, to enable faster and more adaptive datamodel co-evolution."
        },
        {
            "title": "REFERENCES",
            "content": "AIME: AIME problems and solutions, 2025. URL https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. Hector Vargas Alvarez, Gianluca Fabiani, Nikolaos Kazantzis, Ioannis Kevrekidis, and Constantinos Siettos. Nonlinear discrete-time observers with physics-informed neural networks. Chaos, Solitons & Fractals, 186:115215, 2024. Samuel Arcadinho, David Aparıcio, and Mariana Almeida. Automated test generation to evaluate tool-augmented llms as conversational ai agents. arXiv preprint arXiv:2409.15934, 2024. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment, 2025. URL https://arxiv.org/abs/ 2506.07982. Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, and Wu Liu. Acebench: Who wins the match point in tool usage?, 2025a. URL https: //arxiv.org/abs/2501.12851. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025b. URL https://arxiv.org/ abs/2503.19470. Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional instruction tuning, 2025c. URL https://arxiv.org/abs/2410.12952. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao 10 Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile Van Krieken, and Pasquale Minervini. Are we done with MMLU? In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 50695096, Albuquerque, New Mexico, April 2025. Association for Computational LinISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.262. URL https: guistics. //aclanthology.org/2025.naacl-long.262/. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models, 2024. URL https://arxiv.org/abs/2406.09403. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/ 2403.07974. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering, 2022. URL https://arxiv.org/abs/2203.05115. Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, and Weizhu Chen. Beyond pass@1: Self-play with variational problem synthesis sustains rlvr, 2025. URL https://arxiv.org/abs/2508.14029. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng 11 Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling, 2025. URL https://arxiv.org/abs/2409.00920. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets, 2024. URL https://arxiv.org/ abs/2406.18518. Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. m&ms: benchmark to evaluate tool-use for multi-step multi-modal tasks, 2024. URL https://arxiv.org/ abs/2403.11085. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward, 2024. URL https://arxiv.org/abs/2405.14734. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Jingyu Pan, Guanglei Zhou, Chen-Chia Chang, Isaac Jacobson, Jiang Hu, and Yiran Chen. survey of research in large language models for electronic design automation, 2025. URL https: //arxiv.org/abs/2501.09655. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=2GmDdhBdDk. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, and Caiming Xiong. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay, 2025. URL https://arxiv.org/abs/ 2504.03601. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. URL https://arxiv. org/abs/2504.13958. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8), January 2025. ISSN 2095-2236. doi: 10.1007/s11704-024-40678-2. URL http://dx. doi.org/10.1007/s11704-024-40678-2. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. URL https://arxiv.org/abs/2302.04761. 12 Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, and Mao Yang. rstar2-agent: Agentic reasoning technical report, 2025. URL https://arxiv.org/ abs/2508.20722. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023. URL https: //arxiv.org/abs/2303.17580. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ March 2025. 3689031.3696075. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases, 2023. URL https://arxiv.org/abs/2306.05301. Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Tian Jin, Xiaowen Dong, Yanfeng Wang, and Siheng Chen. Synthesizing post-training data for llms through multi-agent simulation. arXiv preprint arXiv:2410.14251, 2024. Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy. Reinforcement learning enhanced llms: survey, 2025. URL https://arxiv.org/abs/2412.10400. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents, 2024. URL https://arxiv.org/abs/2402. 01030. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents, 2024. URL https://arxiv.org/abs/2402.01622. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/ 2406.12045. Fan Yin, Zifeng Wang, Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long Le, Kai-Wei Chang, Chen-Yu Lee, et al. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation. arXiv preprint arXiv:2503.07826, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao 13 Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning, 2025. URL https://arxiv.org/abs/2505.00024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https: //arxiv.org/abs/2311.07911. THE USE OF LARGE LANGUAGE MODELS (LLMS) In the research process, we employed the open-source Language model as both the Judge Model and the data Generator within our proposed LoopTool framework. During manuscript preparation, we used general-purpose LLMs exclusively for grammar checking, phrasing refinement, and clarity improvements in the English text. All conceptual contributions, experiment designs, analyses, and claims in this paper are the responsibility of the authors."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 BENCHMARKS BFCL The Berkeley Function-Calling Leaderboard (BFCL-V3) (Patil et al., 2025) constitutes broad and systematic framework designed to rigorously evaluate the function-calling proficiency of large language models (LLMs) across diverse spectrum of programming languages, application domains, and intricate real-world scenarios. The benchmark encompasses tasks ranging from multiple and parallel function invocations to multi-turn and multi-step function-call interactions. In total, BFCL-V3 comprises 4,951 test instances3,951 single-turn cases and 1,000 multi-turn samplescarefully curated to reflect dynamic, authentic use cases. The assessment methodology in BFCL incorporates several complementary metrics: Abstract Syntax Tree (AST) Evaluation: This metric examines the structural correspondence between the abstract syntax tree of the model-generated output, the ground-truth reference, and the formal function specification. It evaluates the correctness of function identification, the inclusion and accuracy of obligatory parameters, and the precision of both parameter types and associated values. Executable Function Evaluation: Here, the produced API call is executed, and its runtime output is compared directly against the expected ground-truth result, thereby measuring practical execution accuracy. Multi-turn State-based Evaluation: The evaluation focus on comparing the backend systems state after all function calls are executed at the end of each turn of the conversation. It capture the correctness of model executions that modify the internal state via write and delete. Multi-turn Response-based Evaluation: It compares the models execution path against the minimial viable execution result paths as labeled in ground truth. The minimial viable execution result paths refer to list of function calls that must be executed in order to produce desired response as user requests. Irrelevance: This criterion quantifies the models capacity to avoid generating function calls when presented with extraneous or unrelated user queries. The irrelevance score is determined by dividing the number of accurate non-function-call responses by the total test set size. Relevance: Relevance gauges the models adeptness at producing function calls that align contextually with the users query, irrespective of parameter value accuracy. This score is computed as the proportion of appropriate function-call responses within the entire evaluation set. 14 ACEBench ACEBench (Chen et al., 2025a) is designed to evaluate tool-use capabilities with finegrained categorization which could be divided into three primary categories: Normal, Special, Agent.Normal evaluates tool usage in basic scenarios;Special evaluates tool usage in situations with ambiguous or incomplete instructions;Agent evaluates tool usage through multi-agent interactions to simulate real-world, multi-turn dialogues: Normal Evaluation compares the models function call output with the ground truth using AST parsing. Special Evaluation mainly assesses the ability of model in problem identification. Specifically, the model must: (1) detect and alert missing parameters, (2) accurately locate erroneous parameters, and (3) recognize task-function mismatches. Agent Evaluation focus on the models proficiency in utilizing tools during human-agent interactions, employing gpt-4o as user simulator, incluing End-to-End Accuracy and Process Accuracy. B.2 HYPER-PARAMETERS Table 6: Configuration for Iterative GRPO training. Category Data Configuration Optimization Rollout Configuration Training & Logging Hyperparameter Train Batch Size: 128 Validation Batch Size: 128 Max Prompt Length: 4096 Max Response Length: 1024 Learning Rate: 1e-6 PPO Mini Batch Size: 128 KL Loss Used: False Entropy Loss Used: False Clip Ratio Low: 0.2 Clip Ratio High: 0.28 Rollout Mini Batch Size: 2 GPU Memory Utilization: 0.7 Number of Rollouts: 16 Save Frequency (epoch): 1 Test Frequency (epoch):"
        },
        {
            "title": "C THE ALGORITHM OF LOOPTOOL",
            "content": "We present the complete procedure of our LoopTool framework in Algorithm 1 , which couples GRPO-based post-training, Greedy Capability Probing (GCP), Judgement-Guided Label Verification (JGLV), and Error-Driven Data Expansion (EDDE) into unified closed-loop data evolution process."
        },
        {
            "title": "D THE EXAMPLE OF HIERARCHICAL DUAL SUBTREES",
            "content": "The example subtrees of the Context Tree and Constraint Tree are illustrated in Figure 5 and Figure 6, respectively."
        },
        {
            "title": "E THE TRAINING SAMPLE FOR GRPO",
            "content": "The Instruction Prompt used in all GRPO samples is illustrated in Figure 7. The Single-Turn and Multi-Turn samples are illustrated in Figure 8 and Figure 9. 15 Algorithm 1: LoopTool: Iterative Model-Aware Data Evolution Framework Input: Initial seed dataset Dseed from Automated Tool-Augmented Data Construction; Initial model parameters πθ0. Output: Final optimized tool-calling model πθJ after iterations. 1 Initialize: 1, D1 Subset(Dseed). 2 while do 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 // Step 1: GRPO-based Post-training Train policy πθj1 on Dj using GRPO in Eq.(2) with binary reward r(), obtaining updated parameters πθj . // Step 2: Greedy Capability Probing (GCP) foreach (T , ct, ) Dj do Generate at via deterministic greedy decoding from πθj ; if at = then Send (T , ct, t ; at) to JGLV for evaluation; Compute PPL(T ,ct) by Eq.(3) and retain high-PPL samples and at = into DHP ; // Step 3: Judgement-Guided Label Verification (JGLV) foreach mismatched case (T , ct, ; at) do Obtain judgement result yjudge {PRED WRONG, LABEL WRONG, BOTH CORRECT, BOTH WRONG} via Qwen3-32B; if yjudge = PRED WRONG then Add to DM ; else if yjudge = LABEL WRONG then at and add to DLR Replace j ; else if yjudge {BOTH CORRECT, BOTH WRONG} then Discard sample; // Step 4: Error-Driven Data Expansion (EDDE) DM Construct error seed set DES foreach error seed in DES do DLR ; Generate new samples with scenario diversification constraints; Validate generated set via rule-based + LLM-based evaluation to obtain DEE ; // Step 5: Dataset Update for Next Iteration Select untrained subset DSeed-new Construct next-round dataset by Eq.(5): Dseed; Dj+1 = DES DEE DHP j DSeed-new + 1; 23 24 return πθJ"
        },
        {
            "title": "F THE LABEL VERIFICATION PROMPT",
            "content": "The Prompt used in Judge-Guide Label Verification (JGLV) is concluded in Figure 10. Sample examples with yjudge = PRED WRONG and yjudge = REF WRONG are respectively presented in Figures 11 and 12."
        },
        {
            "title": "G THE ERROR GENERATION PROMPT AND NEW ERROR SAMPLES",
            "content": "The system and user prompts for Error-Driven Data Expansion (EDDE) are illustrated in Figures 13 and 14, respecitively. The generated sample case is shown in Figure 15. 16 Figure 5: The example subtree of Context Tree. Figure 6: The example subtree of Constraint Tree. Figure 7: The general current time and tool sets are placeholders. THE LEARNING CURVES IN ITERATIVE LEARNING instruction prompt employed in all GRPO samples. The variables Figure 8: The example of Single-Turn GRPO samples. 17 Figure 9: The example of Multi-Turn GRPO samples. Figure 10: The Prompt used in Judge-Guide Label Verification for Judgement Model. The red text corresponds to variables that are placeholders. Figure 11: The example with yjudge = PRED WRONG identified by JGLV. Figure 12: The example with yjudge = REF WRONG identified by JGLV. Figure 13: The system prompt for Error-Driven Data Expansion (EDDE). 19 Figure 14: The user prompt for Error-Driven Data Expansion (EDDE). Figure 15: The new sample generated by EDDE according to the error in the model response."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Xiaohongshu Inc."
    ]
}