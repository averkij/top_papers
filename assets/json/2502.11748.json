{
    "paper_title": "ILIAS: Instance-Level Image retrieval At Scale",
    "authors": [
        "Giorgos Kordopatis-Zilos",
        "Vladan Stojnić",
        "Anna Manko",
        "Pavel Šuma",
        "Nikolaos-Antonios Ypsilantis",
        "Nikos Efthymiadis",
        "Zakaria Laskar",
        "Jiří Matas",
        "Ondřej Chum",
        "Giorgos Tolias"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 4 7 1 1 . 2 0 5 2 : r ILIAS: Instance-Level Image retrieval At Scale Giorgos Kordopatis-Zilos Vladan Stojnic Anna Manko Pavel ˇSuma Nikolaos-Antonios Ypsilantis Nikos Efthymiadis"
        },
        {
            "title": "Zakaria Laskar",
            "content": "Jiˇrı Matas Ondˇrej Chum Giorgos Tolias VRG, FEE, Czech Technical University in Prague"
        },
        {
            "title": "Abstract",
            "content": "This work introduces ILIAS, new test dataset for InstanceLevel Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Largescale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/ 1. Introduction The ability to recognize and differentiate every unique object instance in the physical world represents one of the ultimate goals for foundation representation models [6, 45, 53, 87]. This work aims to assess this capability through the lens of instance-level image retrieval at very large scale. Instance-level image retrieval corresponds to searching for images of particular objects within large collections. All images of particular object form their own instance-level class. This is an important information retrieval task due to its numerous real-world applications in robotics [39, 56], e40 30 10 1 @ image-to-image text-to-image image-to-image (adapt) image-to-image re-ranking SigLIP+AMES SigLIP OpenCLIP OAI-CLIP AlexNet VGG DenseNet EffNet ViT 2014 2016 2018 2020 2022 Year Figure 1. Performance timeline on ILIAS. Curves indicate best performance in chronological order for image-to-image and textto-image retrieval, showing significant boost with the release of foundation models. Representations are linearly adapted via multi-domain learning on UnED [83]. Re-ranking with local descriptors achieves the best results by huge margin. commerce [85, 88], and cultural heritage [18, 61], to name just few. The task faces challenges because of the substantial variations among positive examples, such as illumination/viewpoint [28, 73] changes and background clutter [5, 40]. An additional difficulty is the high similarity among negatives, which is driven by the extremely fine granularity in the class definitions. It becomes even more challenging at real-world scale, where searching through millions or even billions of images requires handling an open-world setup with countless unseen objects spanning diverse and complex domains. Benchmarking instance-level retrieval under real-world challenges is currently limited by the lack of suitable datasets. Constructing dataset with instance-level class definitions necessitates huge development effort, reflected by the many shortcomings of existing datasets. Shortcomings exist in several key aspects, such as dataset size [77], domain diversity [51, 64], and ground-truth accuracy [78], which suffers from both false positives and false negatives. Popular datasets are typically limited to landmarks [51], and as dataset scale increases, ground-truth quality tends to decline [63, 78]. This is consequence of automating the 1 ground-truth creation process to facilitate scaling up. To address such limitations, we introduce the Instance-Level Image retrieval At Scale (ILIAS) evaluation dataset. The creation of our dataset has two key elements. First, query and positive images are manually captured to ensure challenging variations, covering 1,000 objects across diverse domains. Second, to expand the dataset size without ground-truth errors or additional annotation effort, we leverage key technique: distractor images, collected in 2014 from YFCC100M, are combined with query objects verified not to have publicly existed until after 2014. This distractor set includes 100 million images, two orders of magnitude larger than the largest existing dataset [51]. Notably, all images have permissive license, allowing us to ensure long-term online availability to the full extent. ILIAS includes both image and text queries. The latter is in the form of detailed descriptions of objects and their distinctive features. The dataset is designed to support future research in image-to-image and text-to-image retrieval for particular objects, and additionally serves as large-scale benchmark for evaluating representations of foundation vision and language models (VLM) [53, 87]. To facilitate faster experimentation, we provide mini, but challenging, version (5M) of the distractor set. We perform an extensive evaluation comparison, including many foundation image-to-image and text-to-image models, and establish comprehensive testbed that enables future comparisons. The provided evaluation includes retrieval with global image representation but also re-ranking techniques that use local representations [49, 55, 67] and query expansion [12, 52]. We observe the following: Performance of standard 10-year-old models, measured by mean Average Precision, is as low as 1.3%, while the best-performing model achieves 28.9%, as shown in Fig. 1. This points out the vast progress of representation models and the high challenging factors of ILIAS. VLMs are the top-performing models. Smaller (ViT-B) models trained/tested on large resolution (512/724) outperform larger models (ViT-L) trained/tested on small resolution (256/384). Using Universal Embedding Dataset (UnED) [83] to learn linear adaptation layer on top of frozen models improves performance of most models, making it candidate training set to couple with ILIAS. Notably, VLMs demonstrate the largest benefits, presumably because their training stage does not optimize image-to-image relations. In contrast to the current belief [59], local representation is key ingredient, while global representation, despite being efficient and compact, performs much lower. The performance gap between image-to-image and textto-image models is surprisingly small. Therefore, detailed text queries are reasonable proxy in the absence of image queries, even at the instance level. 2. Related work In this section, we review the related work in terms of existing datasets and benchmarks in the literature. Datasets. Tab. 1 presents the datasets from the image retrieval literature related to ILIAS. The datasets can be compared based on five main axes: (i) Class definition adopted. Many datasets [27, 42, 43, 51, 78] adopt strict definition very similar to ours, satisfying instance-level requirements. Others [3, 64, 77, 88] adopt more relaxed definition, where some minor variations are permitted, e.g. color changes in objects of the same class. Even more relaxed are the finegrained definitions [48, 85], where the object of the very same type is considered related, e.g. same product with dif- (ii) Domain of the dataset. Most datasets ferent variant. are tailored for specific domain. Landmarks are among the most popular domains [3, 27, 43, 51, 78]. Other domains include products [42, 48, 88] and fashion [37, 64]. Some datasets cover multiple domains, either being standalone [77] or bundle of repurposed datasets [58, 83]. (iii) Scale of database. Most of the datasets are small-scale, counting few thousand images [37, 77, 88]. Larger ones [51, 83, 85] expend slightly above million. None satisfies large-scale requirements. (iv) Noise in ground truth. Most datasets consist of clean annotations, except for few cases that contain inaccuracies, including false positives [78], i.e. images wrongly annotated as relevant, false negatives [42, 51], i.e. relevant images that have not been annotated, or the possibility of false positives [51]. (v) Availability. Most datasets are publicly available with permissive licenses, with few exceptions of partial [3, 27] or no [43, 85] availability. To this end, no publicly available dataset fits the strict instance-level definition, contains objects from multiple domains, ensures error-free labeling and is large scale. This gap is filled with ILIAS satisfying all the aforementioned requirements. Evaluation benchmarks. Benchmarking [62] tracks the progress in the field, which is even more necessary with the emergence of foundation models. Several benchmarks papers [32, 34, 89] exists in the instance-level retrieval literature, investigating the impact of learning scheme, post-processing, model ensembling, query expansion, and whitening. The most relevant benchmark to ILIAS is UnED [83] that combines existing datasets to create union that evaluates models performance across various domains. Due to its wide variety, UnED serves as the training dataset for linear adaptation. Regarding the evaluation of foundation models, the most common practice [16, 45, 68] is measuring classification performance on top of frozen models on ImageNet [14]. This is performed either with or without the training of classifier via linear probing or k-NN search. Furthermore, models are usually evaluated on dense prediction tasks [45] and several multiple-downstream single-domain tasks [2]. 2 datasets year objects query positives database gt class def. 2006 UKB [42] 2008 Holidays [27] 2011 Sculptures [3] 2015 INSTRE [77] 2015 SOP [64] 2016 InShop [37] 2018 R-Oxford [51] 2018 R-Paris [51] 2018 GLDv1 [43] GLDv2 [78] 2020 Product1M [88] 2021 2021 RP2K [48] 2021 GPR1200 [58] 2021 e-product [85] 2023 FORB [80] 2023 UnED [83] 2.5K 500 10 200 10K 500 70 1250 11.3K 60.5K 3.9K 14.2K 70 11 70 11 N/A 30K 1.1K 318 6.5K 392 1.2K 10.9K 12K 1.2K 206 10K N/A 13.9K 21K 241K ILIAS 2025 1,000 1,232 10K 991 3.1K 27.3K 60.5K 12.6K 5K 6.3K N/A 3.1K 40K 10.9K 12K N/A 4.5K 244K 4,715 online license FN 10K 1M Clean product IL IL landmark 3.1K Clean Partial IL sculpture multi 27.3K Clean Partial IL product 60.5K Clean Partial IL fashion 12.6K Clean Partial IL IL landmark IL landmark IL landmark IL landmark product product multi product planar multi 1M FN? 1M FN? 1.1M Clean 762K FP 40K Clean Partial IL FG IL+FG FG IL IL+FG domain bbox Fully Partially Partially Fully Fully Fully Fully Fully Partially 10.9K Clean 12K Mix 1.1M N/A 49.8K Clean 1.4M Mix N/A CC Flickr TC Flickr TC MIT License N/A Flickr TC, CC Flickr TC, CC Multiple Fully CC/ Public-domain N/A Fully Fully N/A Multiple Fully No N/A Snap Inc. Fully Multiple Fully 100M Clean IL multi Fully CC Table 1. Comparison with other instance-level datasets. Datasets are compared based on their size (object, query, positives, database), the accuracy of the ground truth (gt), type of class definition, domain, supplementary annotations (bbox) and accessibility (online, license). N/A: not available. FP/FN: false positives/negatives. FN?: possibility of false negatives. Mix: combination of clean and noisy datasets. IL: instance-level. FG: fine-grained. Partial IL: instance-level with subtle variations among same class objects. CC: Creative Commons. For VLMs, zero-shot classification and retrieval serve as the primary benchmarks [53, 81, 87], utilizing class text labels. In this work, we provide similar evaluation protocols tailored for instance-level retrieval. One can test the raw model capabilities or adapt for the instance-level task via linear adaptation on UnED. Text-to-image retrieval is also facilitated for the evaluation of VLMs. 3. ILIAS dataset 3.1. Composition and collection Instance-level class definition. Following an instancelevel class definition [51, 82], we consider all indistinguishable object instances of the real world to form their own class. Nevertheless, we add restriction to consider pair of images as relevant to each other only if there is view overlap. Other cases are explicitly not included in the dataset, contrasting the existing work [37, 64, 78]. Therefore, models should mostly rely on estimating the visual similarity and less on shortcuts through semantics. Overview. ILIAS supports both image-to-image (i2i) and text-to-image (t2i) retrieval and follows the standard setup for retrieval datasets, consisting of two main parts: (i) query images and text, and (ii) database (db) images. The objective is to rank positives db images relevant to the query at the top ranks. The collected objects cover wide range of categories and are not restricted to specific domains. An overview of some collected objects is provided in Fig. 2. Queries and positives are created/collected by group of collectors that are well-informed about the task objectives. In addition to positives, in the database, we include numerous distractors irrelevant (negative) images to the queries that make retrieval more challenging. Following previous work [51], adding large, uncurated set of random images achieves this. The larger the set, the higher the chances of hard negatives images with similar appearance or semantics to the queries. To this end, we select the YFCC100M [72] dataset to serve as the source of distractors due to its size and permissive license. Selected objects. Ensuring that distractor images include no false negatives cannot be performed in scalable or accurate way if one relies on human annotation or metadata. Instead, we take advantage of the fact that YFCC100M was crawled from Flickr in 2014. Hence, an object qualifies in ILIAS if it could not have appeared on Flickr before 2014. To verify this, we rely either on publicly available information, e.g. objects known to be created/manufactured after 2014, or on the collectors knowledge about the object not being publicly available. Additionally, we opt for objects with distinctive and unique features that set them apart from others within the same category. For example, we avoid recent smartphones that look like plain black screens or new objects with distinctive parts closely resembling older ones. Queries and positives. Query images depict the instance on clean or uniform background. When this is not feasible (e.g. buildings or statues), background blurring or cropping is applied. This is performed to avoid including background objects in the query that do not have corresponding positives in our ground truth information. Positives are images featuring the query object in challenging conditions, such as clutter, scale changes, occlusions, and partial views. Prior work [51] reveals that easy positives dominate performance metrics. Thus, we specifically opt for challenging cases that cannot be easily retrieved by the models. To avoid taking advantage of camera identification, most query and positive images are captured with at least two different camera models to introduce diversity. We also incorporate older camera models that are used in YFCC100M. Each text query con3 AP: 0 rank 8 rank rank >1000 rank >1000 AP: 19.1 rank 2 rank 8 rank rank >1000 AP: 0.1 rank 15 rank 47 rank 352 rank > AP: 38.3 rank 2 rank 4 rank 32 rank >1000 AP: 2. rank 4 rank 16 rank 22 rank 333 AP: 56.1 rank rank 3 rank 7 rank 9 AP: 10.3 rank 3 rank rank 79 rank 299 AP: 76.8 rank 1 rank 2 rank rank 24 Figure 2. Examples of query, positive and hard negatives within the distractor set. Average Precision per query and rank of the negatives and positives is reported using the best-performing SigLIP [87] model. Gray: queries. Green: positives. Red: distractors. sists of detailed and fine-grained textual description of an object. Descriptions are initially created by large language model prompted to provide highly detailed depictions of the object shown in query images. Generated descriptions are manually edited to fix errors, insufficient descriptions, or nuances of the model. Distractors. The YFCC100M dataset was chosen for the distractor set due to its large scale and diverse range of conIt consists of 100 million Flickr images, collected cepts. without specific filtering, aside from being shared under permissive CC-BY license. Bounding box annotation. We include supplementary bounding boxes that specify the precise location of objects in query and positive images. They provide statistics about the position and size of object areas, assist our analysis of the dataset challenges, and support future research in instance-level localization. Evaluation metric. Retrieval performance is evaluated via mean Average Precision (mAP), widely used metric in instance-level image retrieval [49, 50, 52]. Specifically, we adopt mAP@1k [78], which assesses the ranking of the top1k nearest neighbors for each query, treating any positive not ranked among the top-1k as not retrieved. We estimate the area under the curve using rectangles and not trapezoids. 3.2. Statistics Dataset size. The final ILIAS dataset includes 1,000 object instances captured in 5,947 images, of which 1,232 are queries and 4,715 are positives. Fig. 3a shows the distribution of positives per object. Also, 99,144,315 images from YFCC100M are downloaded. All images (queries, positives, distractors) are transferred through Flickr to ensure the same pre-processing. Taxonomy. hierarchical 3-level taxonomy is composed for ILIAS. All instances are assigned across one to three categories of different granularity levels. The taxonomy consists of 8 categories on the coarser level, 42 on the mid level, and 38 on the finer level. The categories are derived through manual labeling of the objects based on their semantic content. To form the coarser-level categories, we use domain definitions borrowed from prior work [37, 64, 78] to align with the literature, i.e. art, landmarks, products, clothing. We also define novel categories based on the objects that do not fit into any existing domain. The distribution of objects across categories is uneven, e.g. ranging from 168 and 162 for art and landmarks to 83 for products. Each midand finer-level category contains at least 4 instances. Note that taxonomy is given to provide statistics about the domains of objects and assist our analysis instead of being leveraged as ground truth. The distribution of taxonomy categories can be inferred by Fig. 5, and detailed figure is provided in the supplementary material. Bounding box analysis. total number of 6,117 bounding boxes are annotated for both queries and positives. Note that positives may display multiple objects of near identical appearance to the query; in such cases, bounding boxes are drawn on all indistinguishable objects. There are 235 images with more than one bounding box. Based on the annotated bounding boxes, we compute the area covered in the image by the object instances to derive its relative scale. Fig. 3c shows the distribution of the scale ratio for queries and positives. Most objects in queries cover the largest area of the images; while in the vast majority of positives, the object covers small area of less than half the image. It is result of the severe scale changes and partial views. Moreover, we use the Segment Anything Model (SAM) [31, 54] to extract object segments from positives. The number of detected segments outside the query objects bounding boxes is computed. This indicates clutter from other items in the positives. Fig. 3b shows the segment number distribution, with most images containing multiple segments due to clutter. 4 102 102 6 4 v s # 102 6 4 2 a i # positive query e # 2 0 0 0 0 0 30 20 200+ (a) 10 # positives 100 # segments (b) Figure 3. ILIAS statistics. (a) number of positives per object, (b) positive distribution by the SAM segment number outside the bounding box, (c) image distribution by the relative bounding box area to the image. 0.5 relative bbox area (c) 1 3.3. mini-ILIAS We provide small version of ILIAS, called mini-ILIAS, to facilitate quick experimentations. It consists of the query and positive images collected for ILIAS, and subset of the YFCC100M distractors. Instead of randomly subsampling YFCC100M, we construct challenging subset with the help of VLMs. We aim at selecting distractors displaying objects of similar categories as the query objects. We use the text category labels of the taxonomy as text queries. We also extend them with standard templates used for zero-shot recognition [53], which resulted in several thousands. T2i similarity between each text query and each distractor image is estimated. similarity score for each distractor is derived based on its maximum similarity over the text queries. We ensemble the scores of 3 VLMs to rank images. The top-5M ranked distractors compose the final mini-ILIAS. Our experiments indicate that this subset is significantly more challenging than random subset of the same size. 4. Benchmark methods We describe the methods and foundation models we evaluate on ILIAS, which are grouped according to their type of representations used for retrieval in global (i2i) representations, re-ranking with global (i2i) representations, reranking with local (i2i) representations, and text-to-image. detailed list of models, their performance, and implementation details are in the supplementary materials. Image-to-image retrieval with global representations. Global representation methods use image encoders to map images to global descriptors and rank db images based on cosine similarity. We evaluate legacy and recent foundation models, varying in architecture, descriptor dimensionality, training scheme, training data, and input resolution. Foundation models [6] are the models trained with training set on the scale of hundred million. Particularly, 23 CNN [22, 35, 38, 60, 69, 70, 90] and 41 ViT [15, 16] models, trained with supervision [29, 65, 75, 79], selfsupervision [8, 9, 23, 45], distillation [2, 57, 75], or visuallanguage alignment [10, 53, 68, 81, 87] are benchmarked. Most of the non-foundation models are trained on Ima5 geNet [14]. There are models trained on single specific domains [36, 47, 59], i.e. landmarks or products on GLDv2 or SOP. Universal models [2, 57, 83, 84] trained on multidomains or multi-task schemes are included. The full list of models and results is provided in supplementary materials. To mitigate the differences in training resolution, we use three widely-used resolutions, i.e. 384, 512, and 724 and resize images so that their larger dimension matches one of the three. The test resolution is defined to be one resolution above the one used for training, e.g. network trained with 224 or 384 is tested with 384 or 512, respectively. The vast majority of models achieve best performance under this rule. Similar behavior is observed in the literature [66, 74]. Linear adaptation for i2i retrieval. Pre-trained foundation models, as well as legacy models, are trained to extract representations that are applicable to various tasks; not all encoded features are directly relevant to instance-level retrieval. To adapt the representation to the task at hand, we propose to train single linear layer (projection) on top of frozen backbones. The recently introduced Universal Embeddings (UnED) dataset [83] is used for learning the linear adaptation. UnED contains images from 8 different domains with fine-grained and/or instance-level class annotation. In our experiments, the linear layer that converts the backbone output to 512D descriptor is trained on uniformly sampled subset of 1M images from UnED. The linear adaptation layer is trained with the UJCDS [83] method. Text-to-image retrieval. Text-to-image retrieval is performed using Vision-Language Models (VLMs) trained to align the two modalities. Retrieval is performed based on cosine similarity between the text query and db image descriptors that are extracted using the textual and visual encoder, respectively. We evaluate 17 VLM models. Re-ranking with global representations. Such methods rely on global descriptors for exhaustive search during the initial ranking, but also for second refinement stage that issues an new query. We experiment with αQE [52], the adaptive variant of average Query Expansion [12]. After the initial ranking, the descriptors of the top-ranked images are aggregated with the query via weighted average pooling. The weights are derived from the similarity to the query in the power of α. We dont have validation set; hence, we use fixed value α = 1. Re-ranking with local representations. These re-ranking methods rely on global descriptors for exhaustive search during the initial ranking but estimate query-to-db image similarity based on local descriptors for second refinement stage of the ranked list of images. We experiment with three methods: (i) Chamfer Similarity (CS) [4, 55] on the similarity matrix between local descriptors across the image pair. We use the asymmetric variant of CS with over db descriptors and sum over query descriptors. (ii) Spatial Verification (SP) [7, 17, 49], common re-ranking method where Legacy VLM DINOv2 multi-domain I 20 15 10 5 Legacy VLM DINOv2 multi-domain GLDv2-trained 15 10 5 0 20 10 5 0 Legacy VLM DINOv multi-domain SOP-trained 20 40 60 0 10 20 30 20 60 80 INSTRE GLDv2 SOP Figure 4. Comparison with other instance-level retrieval datasets via reporting mAP@1k. INSTRE: 27.3K db size, multi-domain. GLDv2: 762K db size, single-domain. SOP: 60.5K db size, single-domain. Different network types are color-coded. For GLDv2 and SOP, models fine-tuned on these domains with the corresponding training sets are highlighted. No linear adaptation is used. point correspondences are processed with RANSAC-like process and the number of inliers is used for re-ranking. (iii) AMES [67], recent transformer-based network to estimate the similarity between sets of local descriptors. Due to the scale of ILIAS database, we use only 100 binary local descriptors for each database image and 600 for the query image. Local descriptors are extracted using the base variant of DINOv2 with registers [13, 45] and selected based the local descriptor detector used in AMES [67]. Top-1k retrieved images are re-ranked. 5. Experiments We evaluate all the above models and methods, extracting useful insights regarding the factors that boost retrieval performance. ILIAS is compared with other existing datasets for instance-level image retrieval. We analyze the performance of selected models to break down the impact of different ILIAS attributes, such as domains, clutter, and scale. Unless stated otherwise, we use the large model variants with the largest resolution available, e.g. in our analysis we use SigLIP ViT-L trained with 384 resolution. 5.1. Comparison with other instance-level datasets In Fig. 4, ILIAS is compared with other instance-level retrieval datasets based on evaluation of the same models. Linear adaptation is not applied as parts of GLDv2 and SOP are included in the UnED dataset. Only for the sake of this comparison, and for no other experiment in this work, we use models fine-tuned on specific domains (in-domain models), i.e. on the training sets of SOP and GLDv2. INSTRE, which is also multi-domain, shows correlation to ILIAS, but its performance is saturated due to its small size. For single-domain datasets, in-domain models outperform others by large margin, with few exceptions1 Several multidomain models perform well on SOP since their training set is usually included in the training data. However, in-domain and multi-domain models face challenges on ILIAS, highlighting the diversity of our dataset. 1Note that DINOv2 includes part of GLDv2 in its training data. 5.2. Method comparison Image-to-image retrieval with global representations. Tab. 2 presents the performance of global descriptors models on ILIAS. Selected models are presented to highlight useful comparisons, while many other models are included in the supplementary material. The main factors that improve performance are the size of the training set, training resolution, and model architecture, which aligns with the literature. The impact of dataset size is apparent in various model combinations, e.g. CLIP with openai and laion2b. This is also pronounced by the dominance of foundation models. Training with large resolution brings significant gains and consistently improves mAP@1k. In some cases of SigLIP, smaller models trained with large resolutions outperform larger ones trained with small resolutions. For the models of the same resolution, it is common trend for larger model variants to bring corresponding performance gains. In general, VLMs perform the best. From non-VLMs, only DINOv2 and Unicom achieve competitive performance. Masked Image Modeling (MIM) and supervised models are not performing well. Our linear adaptation scheme is very effective, improving most models. The boost is more pronounced in the case of VLMs. possible explanation for such improvements is that image-to-image relations are not optimized during the training of VLMs. Text-to-image retrieval. Following results in Tab. 2, similar conclusions are derived for the t2i case. Retrieval performance improves with the scaling of the training data. The larger model achieves significantly better results, i.e. compare the base with large variants. Finally, it is noteworthy that the best performance achieved by SigLIP is very close to the i2i performance when no adaptation is used. Note that t2i includes 1k text queries in total, with one query per object, while i2i 1,232 image queries. Evaluation of mini-ILIAS selection. Tab. 3 shows performance on mini-ILIAS for five models with linear adaptation. The selected subset is significantly more challenging than random selection of 5M images. set of 26M random images matches the performance of mini-ILIAS. 6 image-to-image text-to-image model arch train dataset data size train res test res 5M 100M 100M 100M ResNet50 [22] DINO [9] ConvNext [38] OAI-CLIP [22] OpenCLIP [25, 38] OpenCLIP [25, 38] ViT [15, 65] EVA-MIM [16] ViT [15, 65] DINO [9] UDON-CLIP [84] OAI-CLIP [53] EVA-CLIP [68] MetaCLIP [81] DINOv2 [45] SigLIP [87] SigLIP [87] SigLIP [87] R50 R50 CN-L R50 CN-B CN-L ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B sup ssl sup vla vla vla sup ssl sup ssl sup vla vla vla ssl vla vla vla in1k in1k in1k openai laion2b laion2b in1k in22k in21k in1k uned openai merged2b 2pt5b lvd142m webli webli webli EVA-MIM [16] ViT-L ViT [15, 65] ViT-L EVA-MIM [16] ViT-L ViT-L OAI-CLIP [53] OpenCLIP [10, 25] ViT-L ViT-L Unicom [2] ViT-L OAI-CLIP [53] ViT-L DINOv2 [45] ViT-L EVA-CLIP [68] ViT-L MetaCLIP [81] ViT-L SigLIP [87] ViT-L SigLIP [87] in22k ssl in21k sup ssl merged38m openai vla laion2b vla dist laion400m openai vla lvd142m ssl merged2b vla 2pt5b vla webli vla webli vla 1M 1M 1M 400M 2B 2B 1M 142M 14M 1M 2.8M 400M 2B 2.5B 142M 10B 10B 10B 14M 14M 38B 400M 2B 400M 400M 142M 2B 2.5B 10B 10B 224 224 288 224 256 320 224 224 224 224 224 224 224 224 518 256 384 512 224 224 224 224 224 336 336 518 336 224 256 384 384 384 384 384 512 384 384 384 384 384 384 384 384 724 384 512 724 384 384 384 384 384 512 512 724 512 384 384 512 2.5 4.1 4.2 8.5 18.1 22.9 1.9 4.7 6.2 6.6 9.2 10.7 11.7 12.7 15.0 20.6 26.2 27.5 3.9 7.3 8.8 15.8 17.5 18.6 19.9 18.8 20.9 21.7 26.3 34. 1.8 2.9 2.9 6.0 14.0 18.3 1.3 3.2 4.4 4.8 6.7 7.9 8.7 9.4 12.1 16.7 21.5 23.0 2.7 5.3 6.1 11.9 13.7 14.6 15.2 15.3 16.0 16.9 21.8 28.9 1.7 2.9 2.2 3.2 7.9 9.6 1.0 2.1 3.0 3.7 5.9 4.2 5.9 6.6 11.5 11.5 15.6 16.6 1.5 4.6 4.7 7.0 9.4 13.9 9.4 15.3 10.9 11.7 15.2 19. - - - 1.5 4.6 8.1 - - - - - 1.6 2.5 4.9 - 7.5 11.0 11.1 - - - 4.6 7.0 - 5.8 - 7.2 9.2 12.8 18.1 5M - - - 2.3 7.0 11.5 - - - - - 2.7 4.4 7.6 - 10.3 14.4 14. - - - 6.7 9.4 - 8.4 - 10.6 13.1 16.4 22.2 Table 2. Performance comparison using mAP@1k on ILIAS and mini-ILIAS for global representation models for i2i and t2i. Comparison of model architecture (arch), training scheme (train), training data, and train/test resolution. indicates results with the linear adaptation. 5M and 100M correspond to the mini and full versions of the dataset, respectively. sup, ssl, dist, vla: supervised learning, self-supervised learning, distillation and vision-language alignment. R50, CN: ResNet50 and ConvNext. model 100M 5M-mini 5M-rand DINOv2 [45] EVA-CLIP [68] MetaCLIP [81] OpenCLIP [25, 38] SigLIP [87] 15.3 16.0 16.9 18.3 28.9 18.8 20.9 21.7 22.9 34.3 22.70.2 28.80.2 29.20.1 30.90.2 41.80.1 Table 3. challenging distractor subset for mini-ILIAS. mAP@1k evaluated for different distractor sets, 100M: the full dataset, 5M-mini: mini-ILIAS subset, 5M-rand: random subset. We report the mean and std of 3 randomly sampled subsets. indicates results with the linear adaptation. reranking global αQE1 [12, 52] αQE2 [12, 52] αQE5 [12, 52] CS [55] SP [49] AMES [67] SigLIP [87] SigLIP [87] mAP@1k oracle mAP@1k oracle 19.6 22.1 20.4 14.3 32.0 27.0 35.3 48. 44.7 40.8 34.9 48.7 48.7 48.7 28.9 33.7 31.5 23.5 38.3 34.3 45.8 56. 56.9 54.4 49.3 56.0 56.0 56.0 Table 4. Performance comparison for re-ranking methods. Oracle represents the performance of perfect re-ranking at the top-1k images. Top: query expansion with global descriptors. Bottom: re-ranking with local descriptors. : results with linear adaptation. Retrieval with re-ranking. Tab. 4 shows the performance of re-ranking methods applied on top of SigLIP with and without linear adaptation on ILIAS. Complementary to mAP@1k, an oracle-based top-1k re-ranking metric is reported as the upper bound of re-ranking method that processes the top 1k images. Local similarity estimated by hand-crafted or learned model proves to be very effective for re-ranking. Nevertheless, the oracle re-ranking performance indicates that there is lot more space for improvements. Re-ranking with QE is useful when the number of aggregated neighbors is low and drops below the baseline when the number of neighbors is increased. Interestingly, global re-reranking typically decreases oracle performance. 5.3. Analysis Performance per domain. Fig. 5 shows the performance per taxonomy categories. The taxonomy annotations allow fine-grained view of the results, which can possibly allow us to capture imbalanced improvements in future work. For example, DINOv2, despite being overall inferior to SigLIP, is outperforming it by far in categories like architecture, paper art, and sculptures or is quite similar in categories like public art and jewelry. This is possibly attributed to the curation and composition of the DINOv2 training set, which includes artwork and landmark datasets. 7 art landmark toys fashion decor technology media product 90 SigLIP+AMES SigLIP SigLIP SigLIP-text DINOv2 1 @ 70 30 10 digital(11) trading(29) playset(15) light(13) sport(6) publicart(110) sculpture(59) boardga e(10) appliances(21) accessory(14) accessory(46) painting(58) jew elry(34) ga ming(8) sign(20) gadget(7) paper odel(22) action figure(38) peripheral(10) vehicle(8) tattoo(9) stuffed(36) other(13) ultim edia(18) footw ear(14) textile(5) textile(8) outw ear(38) auto ation(30) architecture(32) decor(19) other(3) other(5) other(2) craft(10) container(18) tablew are(25) writingtool(10) product(21) recording(7) sta p(12) hygiene(10) perfu e(8) drink(31) book(48) sticker(22) other(3) other(4) Figure 5. Performance comparison per category. mAP@1k averaged over objects in the same mid-level taxonomy category, organized by their primary-level category size, with sorting within each group by SigLIP+AMES performance. Comparison between SigLIP with and without adaptation, SigLIP combined with AMES reranking, SigLIP t2i, and DINOv2. indicates results with the linear adaptation. w/ vs w/o linear adaptation (0.78) i2i vs t2i (0.51) global vs re-ranking w/ global (0.92) 100 global vs re-ranking w/ local (0.76) 100 area coverage 20 0 80 100 SigLIP-text (mAP@1k=18.1) 60 40 <7% 714% 1427% >27% (675) (630) (805) (794) <18 (698) 1833 (803) 3353 >53 (865) (825) number of segments 100 80 40 20 ) 6 . 9 1 = 1 @ ( g 0 20 40 60 80 100 SigLIP (mAP@1k=28.9) ) 6 . 9 1 = 1 @ m ( g 100 80 60 40 0 ) 9 . 8 2 = 1 @ ( g 80 40 20 0 ) 9 . 8 2 = 1 @ ( g 80 60 40 20 0 20 40 60 80 100 SigLIP+AMES (mAP@1k=45.8) 20 80 100 SigLIP+αQE1 (mAP@1k=33.7) 60 40 Figure 6. Performance comparison reporting AP per query for different approaches with SigLIP. Pearson correlation reported in parenthesis. indicates results with the linear adaptation. Per query comparisons. Fig. 6 shows the AP per query for various methods. Linear adaptation boosts most queries, i.e. performance drop only for 192 queries. Imageand textbased retrieval are not strongly correlated despite performing similarly, which is good evidence [71] for the effectiveness of model ensembles. Indeed, ensembling i2i and t2i by averaging similarities brings +6.1 improvement over i2i retrieval. Query expansion improves the queries with at least some positives at top positions, i.e. AP greater than 20. However, it harms many low-performing queries by aggregating descriptors irrelevant to the query. AMES, despite being trained on landmarks, significantly improves the vast majority of queries, many achieving perfect retrieval. Impact of clutter and scale in positives. To quantify the impact of background clutter and scale changes, Fig. 7 presents the performance for different groups of positives. Dealing with small objects and multi-object scenes form major weaknesses of existing models. Notably, t2i beats i2i without adaptation in small-scale groups. 8 SigLIP+AMES SigLIP SigLIP SigLIP-text 73. 53.5 39.8 29.5 60.7 41.6 60 40 20 51.7 34.9 18. 10.9 11.7 31.3 21.1 18.5 14. 6.7 5.0 3.6 30.3 22.9 43. 39.1 26.2 15.7 14.6 32.7 22. 14.5 14.0 19.7 12.6 11.9 60 40 20 1 @ Figure 7. Performance evaluation (mAP@1k) across different amounts of object area coverage and background clutter. Positives across all queries are jointly ranked based on coverage or clutter and split into 4 equal size groups. Queries with no positive in the corresponding group are discarded. No. of queries per group is in parentheses. indicates results with the linear adaptation. 6. Conclusions We introduce ILIAS and conduct an extensive evaluation of current foundational models and retrieval methods, highlighting that instance-level retrieval remains an unsolved problem. Our results indicate that off-the-shelf application of foundational models leaves considerable room for improvement, particularly in handling small objects and complex backgrounds. While specialized retrieval methods leveraging local descriptors are effective in these cases, their high memory and computational costs become impractical at the scale of ILIAS or beyond. ILIAS is designed to become standard benchmark for evaluating foundational representation models and retrieval methods, accommodating both global and local representations, and advancing the field of instance-level recognition. Acknowledgement. Work supported by the Junior Star GACR (no. GM 21-28830M), MSCA-PF (no. 101154126), JAC (no. CZ.02.01.01/00/22 010/0003405), CTU in Prague (no. SGS23/173/OHK3/3T/13). We thank VSB TUO, IT4Innovations National Supercomputing Center and Czech Republic for access to the LUMI supercomputer through MEYS and e-INFRA CZ (grant ID: 90254)."
        },
        {
            "title": "Supplementary materials",
            "content": "A. Implementation details Collection process. Queries and positives are created/collected by group of 16 collectors who are wellinformed about the task objectives. Most of the images consist of photographs taken by the collectors for the purpose of this work, while smaller part is downloaded from online repositories with permissive license. All collected images are manually filtered and curated by the authors. Regarding the selection of the objects, the collectors are advised to opt for objects with distinct, uncommon featuressuch as unique shapes, colors, or texturesthat set them apart within their category, i.e. prioritize items with rare modifications. As mentioned in the main paper, objects that are created or share parts with other objects created before 2014 do not qualify as query objects. Fig. illustrates some of the objects rejected during the selection process. Fig. Aa is the Kuggen building, whose construction finished in 2011. Fig. Ab is newly bought coaster that displays wellknown van Gogh painting. Fig. Ac is newly bought cutlery holder whose design is rather generic with no distinctive detail; hence, very similar (close to identical) objects may exist in YFCC100M. Furthermore, the collectors are provided with older camera models used in YFCC100M. This simulates similar camera distribution for the query and positive images with the distractors. Tab. shows the distribution of the most used cameras. Older-generation cameras are used for the majority of the collected images. The collectors are instructed to avoid using the same camera for both the query and the positives of an object to avoid any possible shortcuts learned by pre-trained models. Downloading and storing images. To acquire the YFCC100M [72], we download images based on the Flickr URLs provided by the original authors. Approximately 82M images are downloaded. The remaining images are downloaded from the AWS S3 data bucket provided by the authors. We opt for downloading the images from Flickr to ensure that identical preprocessing has been applied to the distractor dataset and the collected query and positive sets in ILIAS. The collected images in ILIAS are also uploaded to and downloaded from Flickr. We use the medium option to download all images, which resizes images to 500px based on their larger side. All images are stored with 90 JPEG compression quality with 4:4:4 chroma subsampling. Following [1, 20], white balancing is applied on all images. All personal details (e.g. human faces, license plates) that are displayed in the collected images of ILIAS are either blurred or cropped. mini-ILIAS composition. We consider the 88 text category labels from ILIAS taxonomy to generate text queries, manually expanded with 132 terms that are synonyms or fine- (a) (b) Figure A. Rejected objects. Example of objects that are disregarded during the selection process. (c) model Canon EOS 450D NIKON D3000 NIKON 3100 DiMAGE X1 Xiaomi Poco X5 Pro iPhone 14 Xiaomi Redmi Note 11 Pro Canon EOS 6D Mark II iPhone SE (3rd generation) NIKON 5300 Canon EOS 50D iPhone 14 Pro Canon PowerShot S5 IS ONEPLUS A6003 Canon EOS REBEL T2i year 2008 2009 2010 2005 2023 2022 2022 2017 2022 2013 2008 2022 2007 2018 2010 type images DSLR DSLR DSLR camera phone phone phone DSLR phone DSLR DSLR phone DSLR phone DSLR 770 585 443 286 275 237 210 208 195 144 141 122 118 110 102 Table A. Most frequently used camera models in ILIAS. Cameras used for more than 100 images are displayed. Information about release date and type of camera is provided. grained descriptions of the original labels. The collected labels are combined with 43 templates used in the original CLIP [53] to generate list of 9,976 text queries. Examples of the templates used are in Tab. B. We do not consider domain-specific templates. We use the large model variants of SigLIP, OpenCLIP, and EVA-CLIP to compute the ensemble text-image similarities between the text queries and each image of YFCC100M. Text query generation. We generate text queries using the GPT-4o [44]. The prompt displayed in Fig. is first provided to the LLM. Then, query image of one of the objects in ILIAS and its corresponding category is provided to the model to generate textual description. For object category, we use mid level category from taxonomy. If it is not available, we use the coarser level category. The generated text queries are manually edited by the authors to fix errors, insufficient descriptions, or nuances of the model. Global representations. For the implementation of global representation models, we rely on public resources available on PyTorch [46]. We use the timm2 and torchvision3 libraries that provide relevant code and weights for the majority of the models. For the models not included there, we use the relevant code from the official github repositories 2github.com/rwightman/pytorch-image-models 3github.com/pytorch/vision 9 close-up photo of the *. good photo of the *. photo of cool *. low resolution photo of the *. bad photo of the *. cropped photo of the *. photo of hard to see *. bright photo of *. photo of clean *. photo of dirty *. dark photo of the *. photo of my *. photo of the cool *. close-up photo of *. bright photo of the *. Table B. Examples of templates used for the text query generation for the creation of mini-ILIAS. The * symbol is replaced with taxonomy term. You are system generating descriptions of objects shown in an image. Provided with an image and category in which the item shown in the image belongs to, you will describe the main item that you see in the image, giving enough details to unambiguously describe the object. You can describe unambiguously what the item is and its material, color, and style if clearly identifiable. Please do not describe anything about the background. Figure B. Prompt used for the initial generation of text queries provided by the authors, i.e. [45]4, [9]5, [8]6, [23]7, [47]8, [29]9, [36]10, [59]11, [2]12, [57]13, [83]14, [84]15. Model weights that are not publicly available are provided to us by the original authors. For t2i, we use the image encoders from timm and the text encoders from huggingface16 and OpenCLIP17. We include only base and large model variants in our benchmark. Tab. and contain more in4github.com/facebookresearch/dinov2 5github.com/facebookresearch/dino 6github.com/facebookresearch/swav 7github.com/facebookresearch/moco-v3 8github.com/yash0307/recallatk_surrogate 9github.com/tjddus9597/hier-cvpr23 10github.com/sungonce/cvnet 11github.com/shihaoshao-gh/superglobal 12github.com/deepglint/unicom 13github.com/naver/unic 14github.com/nikosips/universal-image-embeddings 15github.com/nikosips/udon 16huggingface.co 17github.com/mlfoundations/open_clip formation, including model checkpoints. Regarding image preprocessing, following instance-level retrieval literature [36, 52, 59], the images are resized based on their largest side respecting their aspect ratio, i.e. isotropic rescaling. Image resolution is dictated by each models specifications together with the rule setting resolution one level higher than those used during training. This rule is empirically created based on experiments presented in Sec. B.1. We normalize the image tensors with the mean and standard deviation statistics according to model specifications. For all ViT-based models, bicubic interpolation of the position embeddings is performed. UNICOM [2] requires fixed-size tensors in the backbone output, which goes through projection head; hence, we use adaptive average pooling to fix the spatial dimensions of the output feature tensors. For UDON [84] and USCRR [83] models, we use the representation before projection due to the low dimensionality of the latter. For AlexNet [35] and VGG [60] models, we extract descriptors based on the feature maps of the last convolutional layer by applying GeM pooling [52]. For the rest of the models, the extraction process used in the original methods is employed. All global descriptors are ℓ2 normalized. Linear adaptation. The single linear adaptation layer is trained on 1M random subset of UnED [83]. The training follows the UJCDS [83] method that learns linear classifier on all classes in the UnED subset (191,513 classes). The classifier gets the ℓ2 normalized features output from the linear adaptation layer. During training, the Normalized Softmax loss [86] is minimized, and no balancing across UnED domains is performed. The linear layer and classifier are trained for 2 epochs with 128 batch size. We use Adam [30] optimizer with 10-3 learning rate and 10-6 weight decay. The scale of the Normalized Softmax loss is 16. Local representations. Following AMES [67], local descriptors are extracted based on the base variant of DINOv2 with registers [13, 45]. Local descriptors are selected based on their weights estimated by feature detector [7]. We employ the pre-trained re-ranking network trained on the corresponding descriptors. The local descriptor extraction, the pre-trained models, and inference configurations are provided by the original authors18. To ensure fair comparison between re-ranking methods, we use the same local descriptors for other methods but with different binarization. AMES consists of binarization layer initialized with ITQ [19, 33] and finetuned during model training. Hence, for Chamfer similarity [55] and Spatial verification [49], the descriptors are binarized with the same ITQ weights. Spatial verification is conducted with single-correspondence hypothesis of similarity transformations (lack of scale due to no multi-scale extraction) and local optimization [11] with affine transformations. Tentative inlier correspondences are extracted based on the nearest neighbor of each 18github.com/pavelsuma/ames 10 70 1 @ 50 30 0 OAI-CLIP EVA-CLIP OpenCLIP SigLIP DINOv2 MetaCLIP OpenCLIP SigLIP 1 10 number of distractors (in millions) Figure C. Impact of the number of distractors. mAP@1k of five models for varying db size. indicates results with the linear adaptation. and l: base and large model variants. query local descriptor, using threshold of 32 Hamming distance. Local similarity for re-ranking is estimated based on the number of inliers detected by RANSAC, with minimum threshold of 5 inliers. The final AMES similarity is an ensemble of local and global similarity, while CS and SP use only the similarity estimated with local descriptors. The ensemble, with its hyper-parameters tuned with the same validation process and set as in AMES, provides improvements for CS (from 38.3 to 41.8) which still under-performs AMES, but no improvements for SP. In the default settings, i.e. 100 binarized local descriptors for db images, the total memory requirements for storing local descriptors is 149GB, which is to be compared with 95GB needed for 512D global descriptors stored in half-precision. Note that we do not consider compression techniques for the global descriptors, which can decrease the memory footprint by an order of magnitude with an insignificant performance loss [21, 67]. B. Additional experiments Similar to the main paper, unless stated otherwise, we use the large ViT model variants with the largest resolution available, e.g. we use SigLIP ViT-L trained with 384 resolution. In the case of various architectures for the same method, we use the best-performing one, e.g. we use the large variant of ConvNext architecture for OpenCLIP. B.1. Additional analysis Impact of number of distractors. Fig. presents the performance of five models under varying numbers of distractors. Performance declines as more distractors are added; however, significantly increasing the datasets difficulty requires an exponential growth in the number of distractors. Notably, the ranking of models changes considerably when comparing performance with no distractors to that with 100M distractors. For example, DINOv2 demonstrates 11 model train res 224 EVA-CLIP [16, 68] MetaCLIP [81] OpenCLIP [25, 38] DINOv2 [45] SigLIP [87] SigLIP [87] EVA-CLIP [16, 68] MetaCLIP [81] OpenCLIP [25, 38] DINOv2 [45] SigLIP [87] SigLIP [87] 224 224 224 518 224 512 336 224 320 518 256 384 5.0 5.1 8.2 6.4 9.1 0.1 4.7 10.3 10.3 9.7 8.3 1.8 7.7 8.8 10.7 12.2 14.1 8.9 13.1 14.4 16.5 16.0 18.8 21.6 512 5.8 6.5 6.1 14.0 10.2 18.9 13.4 11.0 12.7 17.7 15.4 24.1 3.1 3.8 2.5 14.3 6.1 20.2 9.5 7.4 7.6 18.5 10.8 20.6 Table C. Impact of resolution. Performance (mAP@1k) by testing at different resolutions. The underline indicates the resolution selected for each model based on our rule. Linear adaptation is not used. Top: base models. Bottom: large models. number of segments area coverage (0-18] (18-33] (33-53] (53-558] (0.32-6.9] (6.9-13.5] (13.5-33] (27.3-100] r t p ] 0 1 - 0 [ ] 0 0 1 - 0 1 ( ] 0 0 0 1 - 0 0 1 ( ] + 0 0 0 1 ( 39.02 22.77 21.07 17.14 25. 28.12 25.94 20.59 24.17 26.46 25. 23.75 16.68 24.18 27.31 31.83 r i o ] 0 1 - 0 [ ] 0 0 1 - 0 1 ( ] 0 0 0 1 - 0 0 1 ( ] + 0 0 0 1 ( 7. 19.75 30.08 42.81 14.75 27.34 26. 30.94 22.17 26.32 26.48 25.04 40. 27.66 20.44 10.97 (a) (b) Figure D. Impact of clutter and area coverage. Percentage of images per ranking range based on SigLIP and grouped based on (a) clutter, i.e. number of segments detected by SAM, (b) scale, i.e. area of object bounding box in images. Column bins contain the same number of positives. Normalization per row is applied. strong robustness to distractor increases, ranking last with no distractors but surpassing two models at 100M distractors and reaching others. Also, several crossings between models are observed. Therefore, evaluation at large scale, provided by ILIAS, is important. Impact of image resolution. In Tab. C, we investigate the impact of resolution and validate the rule of using as test resolution one up from the training one. Linear adaptation is not used in this experiment. It is clear that the vast majority of models achieve the best performance following the imposed rule; test at resolution one level larger than the training resolution. Interestingly, SigLIP collapses when used with resolution much lower than the training one. Impact of background clutter. To quantify the impact of background clutter, we experiment with masking out areas outside object bounding boxes in the positives during descriptor extraction. This approach improves SigLIP performance from 28.9 to 62.4. Fig. Da also presents the impact of clutter, i.e. number of segments detected by SAM in positive image outside of the object bounding box, on the ranking of this positive. This experiment provides insight about the type of positives, according to clutter, that popuLegacy VLM DINOv2 multi-domain L 30 20 10 0 Legacy VLM DINOv2 multi-domain GLDv2-trained 30 20 0 20 40 60 80 10 20 30 Legacy VLM DINOv multi-domain SOP-trained 30 20 10 20 40 60 Figure E. Comparison with other instance-level retrieval datasets via reporting mAP@1k. Results with linear adaptation. INSTRE: 27.3K db size, multi-domain. GLDv2: 762K db size, single-domain. SOP: 60.5K db size, single-domain. Different network types are color-coded. For GLDv2 and SOP, models fine-tuned on these domains with the corresponding training sets are highlighted. INSTRE GLDv SOP 60 57.3 1 @ 50 40 20 10 44.3 49.2 48.5 30. 26.9 24.1 31.5 20.6 17.3 7. 47.5 32.6 37.2 31.0 29.9 16. SigLIP+AMES SigLIP SigLIP SigLIP-text DINOv2 43. 41.5 43.2 39.3 22.9 21.0 11. 26.6 24.7 19.1 17.4 14.8 17. 15.4 10.9 22.6 18.3 14.9 10. 22.3 21.0 19.8 8.9 technology toy product fashion household art landmark media Figure F. Performance comparison per primary category. mAP@1k averaged over objects in the same primary-level category size, sorted by SigLIP+AMES performance. Comparison between SigLIP with and without adaptation, SigLIP combined with AMES reranking, SigLIP t2i, and DINOv2. indicates results with the linear adaptation. late the top and bottom ranks. Positives with less clutter, i.e. low number of segments, are the most common in the higher ranks; while, positives with more clutter, i.e. high number of segments, are the most common in the lower ranks. Impact of object scale. Flowing the same strategy as above, but object bounding boxes are cropped and rescaled instead of masking, performance further improves to 69.4. However, although this does not reflect solely the impact of scale changes due to potential partial views and drastic viewpoint changes, it still gives good insight into the limitations of the current models regarding scale changes. Fig. Db presents the impact of relative scale, i.e. percentage of the bounding box area within the image area, on the ranking of positive. This experiment provides insight into the type of positives, according to relative image coverage, that populate the corresponding rank ranges. Positives where the object covers large area are the most common in the higher ranks; while, positives with small area coverage are the most common in lower. Multi-scale and multi-rotation. common approach to address scale variation is multi-scale feature extraction, as widely adopted in the literature [52, 59]. Applying multiscale extraction asymmetrically, i.e. only on the queries, yields an average 0.4 performance improvement across benchmarked models. SigLIP is marginally improved by 0.1. Multi-rotation is also tested in similar manner, which, however, leads to an average drop of 0.3. Yet, SigLIP is marginally improved by 0.2. Comparison with other datasets using linear adaptation. Fig. presents the performance of global representation models with linear adaptation. Similar conclusions derive as in the case without adaptation. Only SigLIP achieves competitive performance in SOP datasets out of the models not trained in-domain. Performance per domain. Fig. presents the average performance of objects grouped based on the coarser level of the taxonomy. Qualitative examples. Fig. and show examples of retrieved images based on i2i and t2i retrieval, respectively. B.2. Linear adaptation Comparison with other approaches. Tab. compares the proposed linear adaptation with other linear projection methods trained on UnED for three models. All methods project the off-the-shelf descriptors to 512D ones. The unsupervised PCA whitening (PCAw) [26] and the supervised learnable whitening (Lw) [52] approaches are evaluated. The proposed linear adaptation scheme achieves the best performance, typically with large margin. It is the only that does not drop off-the-shelf DINOv2 performance. Impact of multi-domain linear adaptation. Tab. illustrates the performance of several models with linear adaptation trained on the four largest single-domain datasets of UnED, as well as the entire UnED. Training on single domain typically increases the performance of VLMs, except in the case of Met, where performance drops dramatically. model labels DINOv2 OpenCLIP SigLIP no adaptation PCAw [26] Lw [52] ours - 15.3 14.8 14.0 15. 9.6 12.5 9.1 18.3 19.6 22.2 15.1 28. Table D. Performance comparison for linear adaptation via mAP@1k. Label requirement is indicated. Performance before adaptation is provided for reference. dataset domain DINOv2 OpenCLIP SigLIP no adaptation - GLDv2 [78] Food2k [41] Met [82] iNaturalist [76] landmarks food artworks natural world UnED [83] multi-domain 15. 14.6 12.6 14.7 14.2 15.3 9.6 14.2 13.6 5.1 16.3 18.3 19. 25.6 22.6 7.6 26.4 28.9 Table E. Performance comparison of singleand multi-domain linear adaptation. mAP@1k of models with linear adaptation trained on different dataset setups based on UnED. Performance before adaptation is provided for reference. DINOv2 OpenCLIP EVA-CLIP SigLIP MetaCLIP 30 25 20 15 10 1 @ m 64 128 256 512 descriptor dimensionality 1024 Figure G. Impact of descriptor dimensionality. mAP@1k of five models with the linear adaptation of various dimensionalities. DINOv2 performance decreases consistently with singledomain training. Nevertheless, the margin with multidomain training is significant, indicating that multi-domain training on the whole UnED is best suited for ILIAS. Impact of descriptor dimensionality. Fig. illustrates the performance of five models linearly adapted on UnED with varying descriptor dimensionalities. For all models, performance saturates at descriptor dimensionality of 256D, with only marginal improvements observed for most models beyond this point. Robustness. We conduct three independent runs using different random seeds to evaluate the robustness of the linear adaptation. Across five global descriptors, the proposed scheme exhibits strong robustness, with maximum standard deviation of 0.2 and minimum of 0 across runs. 400 600 100 200 50 1 @ m 40 10 50 20 30 global top-1k top-100 top-5k top-500 top-10k memory per image (KB) 101 Figure H. Impact of the re-ranking shortlist size and required memory for local descriptors. Text above each point denotes the number of local descriptors per DB image. The shortlist size is indicated in the legend. Results are with the linear adaptation. model DINOv2 + AMES OpenCLIP + AMES SigLIP + AMES ILIAS mini-ILIAS mAP@1k oracle mAP@1k oracle 15.3 27. 18.3 37.0 28.9 45.8 34.0 34.0 48.0 48.0 56.0 56.0 18.8 30. 22.9 39.2 34.3 48.9 41.8 41.8 56.3 56.3 63.9 63.9 Table F. Re-ranking on top of different global representations. mAP@1k and oracle re-ranking on ILIAS and mini-ILIAS. + indicates re-ranking with AMES. indicates results with the linear adaptation. B.3. Re-ranking with local representations Impact of top-M re-ranked images and number of local descriptors. Fig. illustrates the performance of SigLIP with re-ranking when an increasing number of re-ranked images and local descriptors, translated to memory per image, are used. Performance increases as both variables increase. In the default scenario of top-1k and 100 descriptors, the performance is 45.8, which requires 0.6sec per query and approximately 150GB of memory. In an unconstrained scenario, the top performance is 52.8, requiring 20sec and almost 900GB. Combination with various global representations. Tab. presents the performance with and without re-ranking on ILIAS and mini-ILIAS using various models for global representation. mAP@1k is improved by more than 10 when re-ranking is applied for all models and datasets. Qualitative examples. Fig. presents some queries with the largest AP improvement from re-ranking with AMES. Several cases of severe clutter, scale changes, and partial views are successfully retrieved with re-ranking. ILIAS INSTRE AP: 2.285.7 negs: 3390 negs: 3280 negs: 1730 0.6 0. 0.2 0.0 Figure I. Distribution of object bounding boxes in positives. C. Dataset extras Spatial location of objects in positives. Fig. illustrates the spatial location of the object in the positives. Center bias in ILIAS is much less prominent in comparison with INSTRE [77] dataset. Taxonomy. Fig. illustrated the defined categories for the three taxonomy levels. Query and positive examples. Fig. provides visual examples of the collected queries and positives of several query objects. Benchmarked models. Tab. and provide details and performance on ILIAS and mini-ILIAS of all models. D. Dataset hosting, sharing and license ILIAS is hosted in our servers in its entirety (i.e. collected images and the downloaded YFCC100M) to assert its longterm availability to the broader public. All collected images are shared under the permissive CC0 license. The downloaded images are distributed under their original license. All collectors have signed consent form for the distribution of their images under this license."
        },
        {
            "title": "Contributors acknowledgments",
            "content": "We want to thank Larysa Ivashechkina for her work on the annotation of object bounding boxes and masks and the initial filtering of text queries. We also want to thank all contributors for the collection of ILIAS dataset. List of contributors in alphabetical order: Aggeliki Tserota, Celeste Abreu, Christina Tserota, Dimitris Karageorgiou, Dmytro Mishkin, Eleni Karantali, Eva Tsiliakou, Kelly KordopatiZilou, Markos Zampoglou, Noa Garcia, Panagiotis Tassis, Paraskevas Kordopatis, Pavlos Alexantonakis, Ruslan Rozumnyi, Tereza Nejedla, Tomaˇs Jelınek, Yannis Kalantidis, Vasilis Alexiadis, Yankun Wu. AP: 19.5100.0 negs: 260 negs: 40 negs: 30 AP: 15.095. negs: 1690 negs: 900 negs: 420 AP: 23.4100.0 negs: 1430 negs: 8 negs: 00 AP: 0.675.0 negs: 3810 negs: 1930 negs: 1410 AP: 32.9100. negs: 1840 negs: 280 negs: 70 AP: 1.960.0 negs: 3060 negs: 244 negs: 110 AP: 30.888.8 negs: 9860 negs: 5020 negs: 60 AP: 26.783. negs: 3772 negs: 1050 negs: 680 AP: 11.065.7 negs: 7694 negs: 136 negs: 811 Figure J. Re-ranking with AMES. Queries with the most significant AP increase from re-ranking. The number of negatives ranked above positives is reported on top, as before after re-ranking."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Afifi and Michael Brown. What else can fool deep learning? addressing color constancy errors on deep neural network performance. In ICCV, 2019. 9 [2] Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. UNICOM: Universal and compact representation learning for image retrieval. In ICLR, 2023. 2, 5, 7, 10, 18 [3] Relja Arandjelovic and Andrew Zisserman. Smooth object retrieval using bag of boundaries. In ICCV, 2011. 2, 3 [4] Harry Barrow, Jay Tenenbaum, Robert Bolles, and Helen Wolf. Parametric correspondence and chamfer matching: Two new techniques for image matching. In Proceedings: Image Understanding Workshop, 1977. 5 [5] Wissam Bejjani, Wisdom Agboh, Mehmet Dogar, and Matteo Leonetti. Occlusion-aware search for object retrieval in clutter. In IROS, 2021. 1 [6] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv:2108.07258, 2021. 1, 5 [7] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep local and global features for image search. In ECCV, 2020. 5, [8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 5, 10, 18 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. ICCV, 2021. 5, 7, 10, 18 [10] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 5, 7, 18, 19 [11] Ondˇrej Chum, Jiˇrı Matas, and Josef Kittler. Locally optimized ransac. In GCPR, 2003. 10 [12] Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and Andrew Zisserman. Total recall: Automatic query expansion with generative feature model for object retrieval. In ICCV, 2007. 2, 5, [13] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 6, 10, 18 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 2, 5 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICML, 2021. 5, 7, 18 [16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EVA: Exploring the limits of masked visual represenIn CVPR, 2023. 2, 5, 7, 11, 18, tation learning at scale. 19 [17] Martin A. Fischler and Robert C. Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 1981. 5 [18] Cristopher Flagg and Ophir Frieder. Reconstruction of artifacts from digital image repositories. JOCCH, 2022. 1 [19] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: procrustean approach to learning binary codes for large-scale image retrieval. PAMI, 2012. [20] Rafael Gonzalez. Digital image processing. 2009. 9 [21] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Larlus. End-to-end learning of deep visual representations for image retrieval. IJCV, 2017. 11 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 5, 7, 18 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 5, 10, 18 [24] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In CVPR, 2017. 18 [25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 7, 11, 18, [26] Herve Jegou and Ondˇrej Chum. Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening. In ECCV, 2012. 12, 13 [27] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Hamming embedding and weak geometric consistency for large scale image search. In ECCV, 2008. 2, 3 [28] Tomas Jenicek and Ondrej Chum. No fear of the dark: Image In CVPR, retrieval under varying illumination conditions. 2019. 1 [29] Sungyeon Kim, Boseung Jeong, and Suha Kwak. HIER: Metric learning beyond class labels via hierarchical regularization. In CVPR, 2023. 5, 10, 18 [30] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 10 [31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In CVPR, 2023. 4 [32] Byungsoo Ko, Minchul Shin, Geonmo Gu, HeeJae Jun, Tae Kwan Lee, and Youngjoon Kim. benchmark on tricks for large-scale image retrieval. In arXiv:1907.11854, 2019. 2 [33] Giorgos Kordopatis-Zilos, Christos Tzelepis, Symeon Papadopoulos, Ioannis Kompatsiaris, and Ioannis Patras. DnS: 15 Distill-and-select for efficient and accurate video indexing and retrieval. IJCV, 2022. [34] Tarun Krishna, Kevin McGuinness, and Noel OConnor. Evaluating contrastive models for instance-based image retrieval. In ICMR, 2021. 2 [35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. 5, 10, 18 [36] Seongwon Lee, Hongje Seong, Suhyeon Lee, and Euntai Kim. Correlation verification for image retrieval. In CVPR, 2022. 5, 10, 18 [37] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In CVPR, 2016. 2, 3, 4 [38] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. CVPR, 2022. 5, 7, 11, 18, 19 [39] Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, and Kostas Bekris. Ovir-3d: Open-vocabulary 3d instance retrieval without training on 3d data. In CoRL, 2023. 1 [40] Claudio Michaelis, Matthias Bethge, and Alexander Ecker. One-shot segmentation in clutter. In ICML, 2018. 1 [41] Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. Large scale visual food recognition. PAMI, 2023. 13 [42] David Nister and Henrik Stewenius. Scalable recognition with vocabulary tree. In CVPR, 2006. 2, 3 [43] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In ICCV, 2017. 2, 3 [44] OpenAI. Gpt-4o system card. In arXiv:2410.21276, 2024. 9 [45] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 1, 2, 5, 6, 7, 10, 11, [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 9 [47] Yash Patel, Giorgos Tolias, and Jiˇrı Matas. Recall@k surrogate loss with large batches and similarity mixup. In CVPR, 2022. 5, 10, 18 [48] Jingtian Peng, Chang Xiao, and Yifan Li. RP2K: largescale retail product dataset for fine-grained image classification. In arXiv, 2020. 2, 3 [49] James Philbin, Ondˇrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In CVPR, 2007. 2, 4, 5, 7, 10 [50] James Philbin, Ondˇrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In CVPR, 2008. 4 [51] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondˇrej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In CVPR, 2018. 1, 2, [52] Filip Radenovic, Giorgos Tolias, and Ondˇrej Chum. Finetuning cnn image retrieval with no human annotation. PAMI, 2019. 2, 4, 5, 7, 10, 12, 13 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 5, 7, 9, 18, 19 [54] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In arXiv:2408.00714, 2024. 4 [55] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In CVPRW, 2014. 2, 5, 7, 10 [56] Taichi Sakaguchi, Akira Taniguchi, Yoshinobu Hagiwara, Lotfi El Hafi, Shoichi Hasegawa, and Tadahiro Taniguchi. Object instance retrieval in assistive robotics: Leveraging fine-tuned simsiam with multi-view images based on 3d semantic map. In IROS, 2024. 1 [57] Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, and Yannis Kalantidis. UNIC: Universal classification models via multi-teacher distillation. In ECCV, 2024. 5, 10, [58] Konstantin Schall, Kai Uwe Barthel, Nico Hezel, and Klaus Jung. GPR1200: benchmark for general-purpose contentbased image retrieval. In MMM, 2022. 2, 3 [59] Shihao Shao, Kaifeng Chen, Arjun Karpur, Qinghua Cui, Andre Araujo, and Bingyi Cao. Global features are all you need for image retrieval and reranking. In ICCV, 2023. 2, 5, 10, 12, 18 [60] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 5, 10, 18 [61] Ivan Sipiran, Patrick Lazo, Cristian Lopez, Milagritos Jimenez, Nihar Bagewadi, Benjamin Bustos, Hieu Dao, Shankar Gangisetty, Martin Hanik, Ngoc-Phuong Ho-Thi, et al. Shrec 2021: Retrieval of cultural heritage objects. Computers & Graphics, 2021. 1 [62] Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. Content-based image retrieval at the end of the early years. PAMI, 2000. 2 [63] Chull Hwan Song, Jooyoung Yoon, Taebaek Hwang, Shunghyun Choi, Yeong Hyeon Gu, and Yannis Avrithis. On train-test class overlap and detection for image retrieval. In CVPR, 2024. 1 16 [81] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. 3, 5, 7, 11, 18, 19 [82] Nikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos Tolias. The met dataset: In NeurIPS, 2021. 3, 13 Instance-level recognition for artworks. [83] Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Bingyi Cao, Mario Lipovsky, Pelin Dogan-Schonberger, Grzegorz Makosa, Boris Bluntschli, Mojtaba Seyedhosseini, Ondˇrej Chum, and Andre Araujo. Towards universal image embeddings: large-scale dataset and challenge for generic image representations. In ICCV, 2023. 1, 2, 3, 5, 10, 13, 18 [84] Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Andre Araujo, and Ondˇrej Chum. UDON: Universal dynamic online distillation for generic image representations. In NeurIPS, 2024. 5, 7, 10, [85] Jiangbo Yuan, An-Ti Chiang, Wen Tang, and Antonio Haro. eproduct: million-scale visual search benchmark to address product recognition challenges. In arXiv:2107.05856, 2021. 1, 2, 3 [86] Andrew Zhai and Hao-Yu Wu. Classification is strong baseline for deep metric learning. In BMVC, 2018. 10 [87] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 1, 2, 3, 4, 5, 7, 11, 18, 19 [88] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In ICCV, 2021. 1, 2, 3 [89] Liang Zheng, Yi Yang, and Qi Tian. Sift meets cnn: decade survey of instance retrieval. PAMI, 2017. 2 [90] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018. 5, 18 [64] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In arXiv, 2015. 1, 2, 3, [65] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. TMLR, 2021. 5, 7, 18 [66] Pavel Suma and Giorgos Tolias. Large-to-small image resolution asymmetry in deep metric learning. In WACV, 2023. 5 [67] Pavel Suma, Giorgos Kordopatis-Zilos, Ahmet Iscen, and Giorgos Tolias. AMES: Asymmetric and memory-efficient similarity estimation for instance-level retrieval. In ECCV, 2024. 2, 6, 7, 10, 11 [68] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv, 2023. 2, 5, 7, 11, 18, 19 [69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 5, 18 [70] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 5, [71] Ryan Theisen, Hyunsuk Kim, Yaoqing Yang, Liam Hodgkinson, and Michael Mahoney. When are ensembles really effective? In NeurIPS, 2024. 8 [72] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 2016. 3, 9 [73] Akihiko Torii, Relja Arandjelovic, Josef Sivic, Masatoshi Okutomi, and Tomas Pajdla. 24/7 place recognition by view synthesis. In CVPR, 2015. 1 [74] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve In Fixing the train-test resolution discrepancy. Jegou. NeurIPS, 2019. [75] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. 5, 18 [76] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In CVPR, 2018. 13 [77] Shuang Wang and Shuqiang Jiang. INSTRE: new benchmark for instance-level object retrieval and recognition. ACM TOMM, 2015. 1, 2, 3, 14 [78] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2 - large-scale benchmark for instance-level recognition and retrieval. In CVPR, 2020. 1, 2, 3, 4, 13 [79] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm. In arXiv:2110.00476, 2021. 5 [80] Pengxiang Wu, Siman Wang, Kevin Dela Rosa, and Derek Hu. FORB: flat object retrieval benchmark for universal image embedding. In NeurIPS, 2024. 17 checkpoint year cite repo arch train dims dataset data size train res test res 5M 100M 100M 2012 [35] alexnet.tv in1k 2014 [60] vgg16.tv in1k 2015 [22] resnet50.tv in1k 2015 [22] resnet101.tv in1k 2016 [24] densenet169.tv in1k 2017 [69] inception v4.tf in1k 2018 [90] nasnetalarge.tf in1k 2019 [70] tf efficientnet b4.ns jft in1k 2020 [15, 65] vit base patch16 224.augreg in1k 2020 [15, 65] vit base patch16 224.augreg in21k 2020 [15, 65] vit large patch16 224.augreg in21k 2020 [15, 65] vit large patch16 224.augreg in21k ft in1k 2020 [15, 65] vit large patch16 384.augreg in21k ft in1k 2021 [75] deit3 base patch16 224.fb in1k 2021 [75] deit3 large patch16 224.fb in1k 2021 [53] RN50.openai 2021 [53] vit base patch16 clip 224.openai 2021 [53] vit large patch14 clip 224.openai 2021 [53] vit large patch14 clip 336.openai 2021 [10, 25] vit large patch14 clip 224.laion2b 2021 [8] swav resnet50 2021 [9] dino resnet50 2021 [9] dino vitb16 2021 [23] moco v3 resnet50 2021 [23] moco v3 vitb 2022 [38] convnext base.fb in1k 2022 [38] convnext base.fb in22k 2022 [38] convnext large.fb in1k convnext large.fb in22k 2022 [38] 2022 [25, 38] convnext base.clip laion2b augreg convnext large mlp.clip laion2b ft soup 320 2022 [25, 38] recall 512-resnet50 recall 512-vit base patch16 224 in21k cvnet resnet50 cvnet resnet101 superglobal resnet50 superglobal resnet101 hier dino vits16 sop eva02 base patch14 224.mim in22k eva02 large patch14 224.mim in22k eva02 large patch14 224.mim m38m eva02 base patch16 clip 224.merged2b eva02 large patch14 clip 336.merged2b unicom vit base patch16 224 unicom vit large patch14 224 unicom vit large patch14 336 unicom vit base patch16 gldv2 unicom vit base patch16 sop uscrr 64-vit base patch16 clip 224.openai dinov2 vitb14 dinov2 vitl14 vit base patch16 siglip 224.webli vit base patch16 siglip 256.webli vit base patch16 siglip 384.webli vit base patch16 siglip 512.webli vit large patch16 siglip 256.webli vit large patch16 siglip 384.webli vit base patch16 clip 224.metaclip 2pt5b vit large patch14 clip 224.metaclip 2pt5b dinov2 vitb14 reg dinov2 vitl14 reg unic udon 64-vitb in21k ft in1k udon 64-vitb clip openai torchvision CNN torchvision CNN torchvision R50 torchvision R101 torchvision CNN torchvision CNN torchvision CNN timm timm timm timm timm timm timm timm github timm timm timm timm github github github github github timm timm timm timm timm timm github 2022 [47] github 2022 [47] github 2022 [36] github 2022 [36] github 2023 [59] github 2023 [59] github 2023 [29] timm 2023 [16] timm 2023 [16] timm 2023 [16] timm 2023 [16, 68] timm 2023 [16, 68] github 2023 [2] github 2023 [2] github 2023 [2] github 2023 [2] github 2023 [2] github 2023 [83] github 2023 [45] github 2023 [45] timm 2023 [87] timm 2023 [87] timm 2023 [87] timm 2023 [87] timm 2023 [87] timm 2023 [87] timm 2024 [81] 2024 [81] timm 2024 [13, 45] github 2024 [13, 45] github github 2024 [57] github 2024 [84] github 2024 [84] in1k 256 sup in1k sup 512 in1k sup 2048 in1k sup 2048 in1k sup 2048 in1k sup 1536 in1k sup 4032 in1k CNN sup+dist 1792 in1k 768 ViT-B sup in21k sup ViT-B 768 in21k sup 1024 ViT-L in1k sup 1024 ViT-L in1k sup 1024 ViT-L in1k ViT-B sup+dist 768 in1k ViT-L sup+dist 1024 opanai vla 1024 opanai 512 vla opanai 768 vla opanai 768 vla laion2b vla 768 in1k ssl 2048 in1k ssl 2048 in1k ssl 768 in1k ssl 2048 in1k ssl 768 in1k sup 1024 in22k sup 1536 in1k sup 1024 in22k sup 1536 laion2b 640 vla laion2b 768 vla sop 512 sup sop sup 512 gldv2 sup 2048 gldv2 sup 2048 gldv2 sup 2048 gldv2 sup 2048 sop 384 sup in22k ssl 768 in22k ssl 1024 ssl 1024 merged38m merged2b vla 512 merged2b 768 vla 768 laion400m dist 768 laion400m dist 768 laion400m dist gldv2 768 sup sop 768 sup uned 768 sup lvd142m ssl 768 lvd142m ssl 1024 webli 768 vla webli 768 vla webli 768 vla webli vla 768 webli vla 1024 webli vla 1024 2pt5b vla 768 2pt5b vla 1024 lvd142m 768 ssl lvd142m ssl 1024 in1k dist 1024 uned 768 sup uned 768 sup R50 ViT-B ViT-L ViT-L ViT-L R50 R50 ViT-B R50 ViT-B CN-B CN-B CN-L CN-L CN-B CN-L R50 ViT-B R50 R101 R50 R101 ViT-S ViT-B ViT-L ViT-L ViT-B ViT-L ViT-B ViT-L ViT-L ViT-B ViT-B ViT-B ViT-B ViT-L ViT-B ViT-B ViT-B ViT-B ViT-L ViT-L ViT-B ViT-L ViT-B ViT-L ViT-L ViT-B ViT-B 1M 1M 1M 1M 1M 1M 1M 1M 1M 14M 14M 1M 1M 1M 1M 400M 400M 400M 400M 2B 1M 1M 1M 1M 1M 1M 14M 1M 14M 2B 2B 60k 60k 1M 1M 1M 1M 60k 14M 14M 38M 2B 2B 400M 400M 400M 400M 400M 2.8M 142M 142M 10B 10B 10B 10B 10B 10B 2.5B 2.5B 142M 142M 1M 2.8M 2.8M 224 224 224 224 224 299 331 380 224 224 224 224 384 224 224 224 224 224 336 224 224 224 224 224 224 288 224 288 288 256 320 224 224 512 512 512 512 224 224 224 224 224 336 224 224 336 512 224 224 518 518 224 256 384 512 256 384 224 224 518 518 518 224 224 384 384 384 384 384 512 512 512 384 384 384 384 512 384 384 384 384 384 512 384 384 384 384 384 384 384 384 384 384 384 512 384 384 724 724 724 724 384 384 384 384 384 512 384 384 512 724 384 724 724 724 384 384 512 724 384 512 384 384 724 724 512 384 384 1.9 2.3 2.5 2.7 2.9 1.5 1.6 4.3 1.9 6.2 7.3 6.6 8.7 2.7 3.3 8.5 10.7 15.8 19.9 17.5 2.9 4.1 6.6 3.4 3.2 3.9 9.9 4.2 9.1 18.1 22.9 3.1 7.3 3.5 4.2 3.8 4.5 5.1 4.7 3.9 8.8 11.7 20.9 13.8 17.7 18.6 4.1 12.8 6.4 15.0 18.8 19.4 20.6 26.2 27.5 26.3 34.3 12.7 21.7 13.5 17.1 15.3 7.3 9. 1.3 1.6 1.8 1.8 2.0 1.0 1.0 2.9 1.3 4.4 5.3 4.7 6.4 1.8 2.4 6.0 7.9 11.9 15.2 13.7 2.1 2.9 4.8 2.6 2.3 2.7 7.6 2.9 6.9 14.0 18.3 2.1 5.3 2.6 3.1 2.8 3.2 3.6 3.2 2.7 6.1 8.7 16.0 11.1 13.8 14.6 3.3 9.9 4.3 12.1 15.3 15.7 16.7 21.5 23.0 21.8 28.9 9.4 16.9 10.7 13.6 11.7 5.3 6.7 1.5 2.3 1.7 1.9 2.4 1.1 1.0 2.6 1.0 3.0 4.6 3.6 5.3 1.2 1.5 3.2 4.2 7.0 9.4 9.4 1.7 2.9 3.7 2.6 1.9 2.0 6.4 2.2 6.6 7.9 9.6 1.6 5.0 2.9 3.0 3.4 3.4 3.3 2.1 1.5 4.7 5.9 10.9 11.0 13.8 13.9 3.0 9.1 3.8 11.5 15.3 11.2 11.5 15.6 16.6 15.2 19.6 6.6 11.7 9.4 12.7 8.9 5.5 5.9 Table G. Benchmarked model details and mAP@1k on ILIAS and mini-ILIAS for global representation models for i2i. Model details include the year of publication, repository used, architecture (arch), model descriptor dimensions (dims), training scheme (train), training data, and train/test resolution. 5M and 100M correspond to the mini and full versions of the dataset, respectively. For fine-tuned models, only the fine-tuning dataset is considered. Repo indicates the framework used to acquire model weights, i.e. torchvision, timm, or official github. indicates non-publicly available models provided by the original author. indicates results with the linear adaptation. sup, ssl, dist, vla: supervised learning, self-supervised learning, distillation, vision-language alignment. R50, R101, CN: ResNet50, ResNet101 and ConvNext. 18 checkpoint year cite repo arch dims dataset data size train res test res 5M 100M 2021 [53] RN50.openai 2021 [53] vit base patch16 clip 224.openai 2021 [53] vit large patch14 clip 224.openai vit large patch14 clip 336.openai 2021 [53] 2021 [10, 25] vit large patch14 clip 224.laion2b convnext base.clip laion2b augreg 2022 [25, 38] convnext large mlp.clip laion2b ft soup 320 2022 [25, 38] 2023 [16, 68] eva02 base patch16 clip 224.merged2b 2023 [16, 68] eva02 large patch14 clip 336.merged2b 2023 [87] vit base patch16 siglip 224.webli 2023 [87] vit base patch16 siglip 256.webli 2023 [87] vit base patch16 siglip 384.webli 2023 [87] vit base patch16 siglip 512.webli 2023 [87] vit large patch16 siglip 256.webli vit large patch16 siglip 384.webli 2023 [87] 2024 [81] vit base patch16 clip 224.metaclip 2pt5b 2024 [81] vit large patch14 clip 224.metaclip 2pt5b opanai R50 1024 oc opanai 512 timm+oc ViT-B opanai 768 timm+oc ViT-L opanai 768 timm+oc ViT-L laion2b 768 timm+oc ViT-L laion2b 640 timm+oc CN-B laion2b 768 timm+oc CN-L 512 merged2b timm+oc ViT-B 768 merged2b timm+oc ViT-L webli 768 timm+hf ViT-B webli 768 timm+hf ViT-B webli 768 timm+hf ViT-B webli timm+hf ViT-B 768 webli timm+hf ViT-L 1024 webli timm+hf ViT-L 1024 2pt5b timm+oc ViT-B 768 2pt5b timm+oc ViT-L 1024 400M 400M 400M 400M 2B 2B 2B 2B 2B 10B 10B 10B 10B 10B 10B 2.5B 2.5B 224 224 224 336 224 256 320 224 336 224 224 384 512 256 384 224 224 384 384 384 512 384 384 512 384 512 384 384 512 724 384 512 384 384 2.3 2.7 6.7 8.4 9.4 7.0 11.5 4.4 10.6 10.1 10.3 14.4 14.6 16.4 22.2 7.6 13. 1.5 1.6 4.6 5.8 7.0 4.6 8.1 2.5 7.2 7.1 7.5 11.0 11.1 12.8 18.1 4.9 9.2 Table H. Benchmarked model details and mAP@1k on ILIAS and mini-ILIAS for global representation models for t2i. Model details include the year of publication, repository used, architecture (arch), model descriptor dimensions (dims), training data, and train/test resolution. 5M and 100M correspond to the mini and full versions of the dataset, respectively. Repo indicates the framework used to acquire model weights, i.e. timm for the image encoders and huggingface (hf) or OpenCLIP (oc) for the text encoders. R50, CN: ResNet50 and ConvNext. 19 o ( 1 0 ) u ( 1 2 ) ) 6 3 ( w ) 6 ( ain e 2) (1 pin 0) (2 bag s x ( 9 ) ( 7 ) a a ste n et ( 7 ) ( 7 ) b t t f o a ( 9 ) ( 1 ) e ( 3 4 ) ) 8 3 ( w mural(75) statue (25) installation (7) n ( 2 9 ) i publicart(110) t a i ( 3 2 ) ) 2 ( i k ( 4 8 ) c ( 1 9 ) t n a ( 2 5 ) c s lig ht (1 3) ( 1 8 ) ( 1 4 ) ) 3 4 1 ( h h s l ( 9 ) landmark (162) ILIAS edia(93) ) 3 8 ( ct r t n l ( 7 ) sticker(22) (12) recording stamp (7) 1) (3 drin ) 1 2 ( f ) 0 1 ( t t ) 8 ( f ) 0 1 ( i (13) beer (7) tea ) 6 ( b ) 6 ( c o i ( 3 0 ) b c ( 2 2 ) p a e ( 2 1 ) r ( 6 ) 6) (4 ory c (10) (8) textile digital(11) craft paper odel(22) 8 ) ( 5 ai ti 8 ) 6 rt ( 1 toy(155) ti fi r e i a ( 3 8 ) r m n g ( 7 ( ) l e i ( 1 8 ) u u ( 5 9 ) ceramics-clay (29) sport (6) vehicle (8) boardgame(10) playset(15) trading(29) stuffed ( 6) coin(7) card(22) felting(10) plushie ( 6) ( 8 1 ) ) i ( 1 4 ) n o ( 7 ) o ( 1 5 ) Figure K. The ILIAS taxonomy with 3 level hierarchy. The number of objects is displayed for categories with more than 5 objects. The taxonomy is used to summarize the objects diversity and distribution and to report performance per category without affecting the ground truth, which is defined at the instance level. AP: 0.0 rank 1 rank 4 rank 37 rank 53 rank rank 81 rank >1000 rank >1000 AP: 0.0 rank 0 rank rank 19 rank 72 rank 85 rank >1000 rank >1000 rank > AP: 1.7 rank 0 rank 10 rank 21 rank 39 rank rank 75 rank 98 rank >1000 AP: 16.8 rank 0 rank rank 38 rank 55 rank 99 rank 92 rank 273 rank > AP: 19.9 rank 1 rank 3 rank 16 rank 26 rank rank 91 rank 202 rank 202 AP: 26.2 rank 10 rank rank 37 rank 41 rank 45 rank 90 rank >1000 rank > AP: 27.6 rank 0 rank 1 rank 3 rank 6 rank rank 9 rank 78 rank 197 AP: 28.7 rank 0 rank rank 6 rank 8 rank 21 rank >1000 rank >1000 rank > AP: 33.3 rank 0 rank 3 rank 41 rank 51 rank rank 99 rank >1000 rank >1000 AP: 38.1 rank 0 rank rank 2 rank 12 rank 46 rank 46 rank >1000 rank > AP: 50.0 rank 0 rank 1 rank 2 rank 4 rank rank 95 rank >1000 rank >1000 Figure L. Additional examples of queries, positives, and hard negatives within the distractor set based on i2i retrieval. Average Precision per query and rank of the negatives and positives are reported using SigLIP. Gray: queries. Green: positives. Red: distractors. 21 AP: 10. The image shows page from tear-off calendar. The page is yellow and features an illustration of pair of orange sneakers with white laces. The date 16 veresnia (September 16) is printed at the bottom in black text. The calendar is bound at the top with blue cover that has metal fasteners. AP: 45.8 The image shows small ceramic sculpture of lighthouse. The lighthouse features red and white horizontal stripes and blue top. Attached to the lighthouse is small building with brown roof. The sculpture is set on light-colored base. AP: 50.0 The sticker features an image of green, textured armchair with wooden legs. The chair is positioned next to tall cactus in pot. The sticker has holographic border with the text Generative AI by gettyimages at the bottom. AP: 6.1 This is white sock with yellow accents at the cuff, heel, and toe. It features pattern of small dog images along the entire length. At the top of the sock, there is logo of cat and dog, along with the text in Ukrainian Home for Rescued Animals. AP: 0.8 The image shows wooden Tower of Hanoi puzzle. It consists of three vertical pegs mounted on rectangular base. The central peg has series of wooden discs stacked in decreasing size from bottom to top. The puzzle is typically used to demonstrate recursive problem-solving techniques. AP: 19.3 The image shows crocheted textile with textured pattern. It features horizontal stripes in various colors, including yellow, orange, dark blue, gray, green, and light yellow. The texture appears to be bobble or popcorn stitch, giving it raised, bumpy appearance. AP: 25.3 The image shows colorful postcard featuring an illustration of people dancing in circle. The figures are depicted in vibrant clothing, with mosaic-like pattern on the ground and starry night sky in the background. The scene conveys sense of joy and celebration. AP: 13. This is stuffed toy resembling hedgehog. It has soft, beige body with teal nose and ears. The toy features colorful fabric spikes in shades of teal, pink, and orange, adding playful and vibrant touch. The eyes are black and round, giving it cute appearance. AP: 6.8 This is mural depicting large, detailed bee with realistic features, including its fuzzy body and translucent wings. The bee is set against background of abstract, geometric shapes and flowers in grayscale, creating striking contrast with the bees natural colors. The artwork combines realism... AP: 45.8 The image shows graphic card case designed to look like treasure chest with monstrous theme. It features skull with red eyes on top and sharp teeth lining the opening. Inside, graphics card is visible. The case has dark, weathered wood appearance with metal accents and small skull... rank 0 rank 4 rank 29 rank rank 199 rank >1000 rank 0 rank 2 rank 12 rank rank 80 rank 399 rank 0 rank 2 rank 3 rank rank 32 rank >1000 rank 0 rank 1 rank 2 rank rank 17 rank 18 rank 0 rank 123 rank 131 rank rank 197 rank >1000 rank 0 rank 1 rank 4 rank rank 46 rank >1000 rank 0 rank 2 rank 5 rank rank 194 rank >1000 rank 0 rank 5 rank 7 rank rank 24 rank 39 rank 1 rank 3 rank 56 rank rank 173 rank 683 rank 0 rank 1 rank 5 rank rank 20 rank 24 Figure M. Examples of text queries, positives, and hard negatives within the distractor set based on t2i retrieval. Average Precision per text query, and rank of the negatives and positives is reported using SigLIP. Gray: text queries. Green: positives. Red: distractors. 22 Figure N. Examples of collected query objects. Queries and multiple positives are displayed. Gray: queries."
        }
    ],
    "affiliations": [
        "VRG, FEE, Czech Technical University in Prague"
    ]
}