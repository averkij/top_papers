{
    "paper_title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
    "authors": [
        "Hengwei Bian",
        "Lingdong Kong",
        "Haozhe Xie",
        "Liang Pan",
        "Yu Qiao",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 4 8 0 8 1 . 0 1 4 2 : r Preprint. DYNAMICCITY: LARGE-SCALE LIDAR GENERATION FROM DYNAMIC SCENES Hengwei Bian1,2, Lingdong Kong1,3 Haozhe Xie4 Liang Pan1,, Yu Qiao1 Ziwei Liu4 1Shanghai AI Laboratory 3National University of Singapore 4S-Lab, Nanyang Technological University 2Carnegie Mellon University Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 80 6.4 meter3) and long sequential modeling (up to 128 frames), enabling diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io."
        },
        {
            "title": "ABSTRACT",
            "content": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting Work done during an internship at Shanghai AI Laboratory. Corresponding author. Project lead. 1 Preprint. versatile 4D generation applications, such as trajectoryand command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "LiDAR scene generation has garnered growing attention recently, which could benefit various related applications, such as robotics and autonomous driving. Compared to its 3D object generation counterpart, generating LiDAR scenes remains an under-explored field, with many new research challenges such as the presence of numerous moving objects, large-scale scenes, and long temporal sequences (Huang et al., 2021; Xu et al., 2024). For example, in autonomous driving scenarios, LiDAR scene typically comprises multiple objects from various categories, such as vehicles, pedestrians, and vegetation, captured over long sequence (e.g., 200 frames) spanning large area (e.g., 80806.4 meters3). Although in its early stage, LiDAR scene generation holds great potential to enhance the understanding of the 3D world, with wide-reaching and profound implications. Due to the complexity of LiDAR data, many efficient learning frameworks have been introduced for large-scale 3D scene generation. 3 (Ren et al., 2024b) utilizes hierarchical voxel diffusion model to generate large outdoor 3D scenes based on VDB data structure. PDD (Liu et al., 2023a) introduces pyramid discrete diffusion model to progressively generate high-quality 3D scenes. SemCity (Lee et al., 2024) resolves outdoor scene generation by leveraging triplane diffusion model. Despite achieving impressive LiDAR scene generation, these approaches primarily focus on generating static and single-frame 3D scenes with semantics, and hence fail to effectively capture the dynamic nature of outdoor environments. Recently, few works (Zheng et al., 2024; Wang et al., 2024) have explored 4D LiDAR generation. However, generating high-quality long-sequence 4D LiDAR scenes is still challenging and open problem (Nakashima & Kurazume, 2021; Nakashima et al., 2023). In this work, we propose novel 4D LiDAR generation framework, DynamicCity, enabling generating large-scale, high-quality dynamic LiDAR scenes. DynamicCity mainly consists of two stages: 1) VAE network for learning compact 4D representations, i.e., HexPlanes (Cao & Johnson, 2023; Fridovich-Keil et al., 2023); 2) HexPlane Generation model based on DiT (Peebles & Xie, 2023). VAE for 4D LiDAR. Given set of 4D LiDAR scenes, DynamicCity first encodes the scene as 3D feature volume sequence with 3D backbone. Afterward, we propose novel Projection Module based on transformer operations to compress the feature volume sequence into six 2D feature maps. In particular, the proposed projection module significantly enhances HexPlane fitting performance, offering an improvement of up to 12.56% mIoU compared to conventional averaging operations. After constructing the HexPlane based on the projected six feature planes, we employ an Expansion & Squeeze Strategy (ESS) to decode the HexPlane into multiple 3D feature volumes in parallel. Compared to individually querying each point, ESS further improves HexPlane fitting quality (with up to 7.05% mIoU gain), significantly accelerates training speed (by up to 2.06x), and substantially reduces memory usage (by up to relative 70.84% memory reduction). DiT for HexPlane. Using the encoded HexPlane, we use DiT-based framework for generating HexPlane, enabling 4D LiDAR generation. Training DiT with token sequences naively generated from HexPlane may not achieve optimal quality, as it could overlook spatial and temporal relationships among tokens. Therefore, we introduce the Padded Rollout Operation (PRO), which reorganizes the six feature planes into square feature map, providing an efficient way to model both spatial and temporal relationships within the token sequence. Leveraging the DiT framework, DynamicCity seamlessly incorporates various conditions to guide the 4D generation process, enabling wide range of applications including hexplane-conditional generation, trajectory-guided generation, command-driven scene generation, layout-conditioned generation, and dynamic scene inpainting. Our contributions can be summarized as follows: We propose DynamicCity, high-quality, large-scale 4D LiDAR scene generation framework consisting of tailored VAE for HexPlane fitting and DiT-based network for HexPlane generation, which supports various downstream applications. 2 Preprint. In the VAE architecture, DynamicCity employs novel Projection Module to benefit in encoding 4D LiDAR scenes into compact HexPlanes, significantly improving HexPlane fitting quality. Following, an Expansion & Squeeze Strategy is introduced to decode the HexPlanes for reconstruction, which improves both fitting efficiency and accuracy. Building on fitted HexPlanes, we design Padded Rollout Operation to reorganize HexPlane features into masked 2D square feature map, enabling compatibility with DiT training. Extensive experimental results demonstrate that DynamicCity achieves significantly better 4D reconstruction and generation performance than previous SoTA methods across all evaluation metrics, including generation quality, training speed, and memory usage."
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Object Generation has been central focus in machine learning, with diffusion models playing significant role in generating realistic 3D structures. Many techniques utilize 2D diffusion mechanisms to synthesize 3D outputs, covering tasks like text-to-3D object generation (Ma et al., 2024), image-to3D transformations (Wu et al., 2024a), and 3D editing (Rojas et al., 2024). Meanwhile, recent methods bypass the reliance on 2D intermediaries by generating 3D outputs directly in three-dimensional space, utilizing explicit (Alliegro et al., 2023), implicit (Liu et al., 2023b), triplane (Wu et al., 2024b), and latent representations (Ren et al., 2024b). Although these methods demonstrate impressive 3D object generation, they primarily focus on small-scale, isolated objects rather than large-scale, scene-level generation (Hong et al., 2024; Lee et al., 2024). This limitation underscores the need for methods capable of generating complete 3D scenes with complex spatial relationships. LiDAR Scene Generation extends the scope to larger, more complex environments. Earlier works used VQ-VAE (Zyrianov et al., 2022) and GAN-based models (Caccia et al., 2019; Nakashima et al., 2023) to generate LiDAR scenes. However, recent advancements have shifted towards diffusion models (Xiong et al., 2023; Ran et al., 2024; Nakashima & Kurazume, 2024; Zyrianov et al., 2022; Hu et al., 2024; Nunes et al., 2024), which better handle the complexities of expansive outdoor scenes. For example, (Lee et al., 2024) utilize voxel grids to represent large-scale scenes but often face challenges with empty spaces like skies and fields. While some recent works incorporate temporal dynamics to extend single-frame generation to sequences (Zheng et al., 2024; Wang et al., 2024), they often lack the ability to fully capture the dynamic nature of 4D environments. Thus, these methods typically remain limited to short temporal horizons or struggle with realistic dynamic object modeling, highlighting the gap in generating high-fidelity 4D LiDAR scenes. 4D Generation represents leap forward, aiming to capture the temporal evolution of scenes. Prior works often leverage video diffusion models (Singer et al., 2022; Blattmann et al., 2023) to generate dynamic sequences (Singer et al., 2023), with some extending to multi-view (Shi et al., 2023) and single-image settings (Rombach et al., 2022) to enhance 3D consistency. In the context of video-conditional generation, approaches such as (Jiang et al., 2023; Ren et al., 2023; 2024a) incorporate image priors for guiding generation processes. While these methods capture certain dynamic aspects, they lack the ability to generate long-term, high-resolution 4D LiDAR scenes with versatile applications. Our method, DynamicCity, fills this gap by introducing novel 4D generation framework that efficiently captures large-scale dynamic environments, supports diverse generation tasks (e.g., trajectory-guided (Bahmani et al., 2024), command-driven generation), and offers substantial improvements in scene fidelity and temporal modeling."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "HexPlane (Cao & Johnson, 2023; Fridovich-Keil et al., 2023) is an explicit and structured representation designed for efficient modeling of dynamic 3D scenes, leveraging feature planes to encode spacetime data. dynamic 3D scene is represented as six 2D feature planes, each aligned with one of the major planes in the 4D spacetime grid. These planes are represented as = [Pxy, Pxz, Pyz, Ptx, Pty, Ptz], comprising Spatial TriPlane (Chan et al., 2022) with Pxy, Pxz, and Pyz, and Spatial-Time TriPlane with Ptx, Pty, and Ptz. To query the HexPlane at point = (t, x, y, z), features are extracted from the corresponding coordinates on each of the six planes and fused into comprehensive representation. This fused feature vector is then passed through lightweight network to predict scene attributes for p. 3 Preprint. Figure 2: Pipeline of dynamic LiDAR scene generation. Our DynamicCity framework consists of two key procedures: (a) Encoding HexPlane with an VAE architecture (cf. Sec. 4.1), and (b) 4D Scene Generation with HexPlane DiT (cf. Sec. 4.2). Diffusion Transformers (DiT) (Peebles & Xie, 2023) are diffusion-based generative models using transformers to gradually convert Gaussian noise into data samples through denoising steps. The forward diffusion adds Gaussian noise over time, with noised sample at step given by xt = ϵ (0, I), where αt controls the noise schedule. The reverse diffusion, (xt using neural network ϵθ, aims to denoise xt to recover x0, expressed as: xt1 = 1 αt 1 αtϵ, αtx0 + 1 αtϵθ(xt, t)). New samples are generated by repeating this reverse process."
        },
        {
            "title": "4 OUR APPROACH",
            "content": "DynamicCity strives to generate dynamic 3D LiDAR scenes with semantic information, which mainly consists of VAE for 4D LiDAR encoding using HexPlane (Cao & Johnson, 2023; Fridovich-Keil et al., 2023) (Sec. 4.1), and DiT for HexPlane generation (Sec. 4.2). Given 4D LiDAR scene, i.e., dynamic 3D LiDAR sequence RT XY ZC, where , X, , Z, and denote the sequence length, height, width, depth, and channel size, respectively, the VAE first aims to encode an efficient 4D representation, HexPlane = [Pxy, Pxz, Pyz, Ptx, Pty, Ptz], which is then decoded for reconstructing 4D scenes with semantics. After obtaining HexPlane embeddings, DynamicCity leverages DiT-based framework for 4D LiDAR generation. Diverse conditions could be introduced into the generation process, facilitating range of downstream applications (Sec. 4.3). The overview of the proposed DynamicCity pipeline is illustrated in Fig. 2. 4.1 VAE FOR 4D LIDAR SCENES Encoding HexPlane. As shown in Fig. 3, the VAE could encode 4D LiDAR scene as HexPlane H. It first utilizes shared 3D convolutional feature extractor fθ() to extract and downsample features from each LiDAR frame, resulting in feature volume sequence Xtxyz RT XY ZC. To encode and compress Xtxyz into compact 2D feature maps of H, we propose novel Projection Module with multiple projection networks h(). To project high-dimensional feature input Xin RD1 as lower-dimensional feature output Xout Dn RD1 C, the projection network hSr() first reshapes Xin into 3-dimensional feature RSkSrC by grouping the dimensions into the two new dimensions, i.e., Sk the dimension that will be kept, and Sr the dimension that will be reduced, where Sk = D1 , and Sr = D1 . Afterward, hSr() utilizes transformer-based operation to project the Dn Dm D D2 Dm Dn D1 D2 r D2 SkSr 4 Preprint. Figure 3: VAE for Encoding 4D LiDAR Scenes. We use HexPlane as the 4D representation. fθ and gϕ are convolution-based networks with downsampling and upsampling operations, respectively. h() denotes the projection network based on transformer modules. RSkC, which is then reshaped to the expected lower-dimensional SkSr to Sk reshaped feature feature output Xout. Formally, the projection network is formulated as: D2 = hSr(X {D1 where their feature dimensions are added as the upscript for in and out, respectively. Dm Dn Dn {D1 }{D1 D2 D2 }C }C ) , (1) out in To construct the spatial feature planes Pxy, Pxz, and Pyz, the Projection Module first generate the XYZ Feature Volume Xxyz = ht(Xtxyz). Rather than directly access the heavy feature volume sequence Xtxyz, hz(), hy(), and hx() are applied to Xxyz for reducing the spatial dimensions of Xxyz along the z-axis, y-axis, and x-axis, respectively. The temporal feature planes Ptx, Pty, and Ptz are directly obtained from Xtxyz by simultaneously removing two spatial dimensions with hzy(), hxz(), and hxy(), respectively. Consequently, we could construct the HexPlane based on the encoded six feature planes, including Pxy, Pxz, Pyz, Ptx, Pty, and Ptz. Decoding HexPlane. Based on the HexPlane = [Pxy, Pxz, Pyz, Ptx, Pty, Ptz], we employ an Expansion & Squeeze Strategy (ESS), which could efficiently recover the feature volume sequence by decoding the feature planes in parallel for 4D LiDAR scene reconstruction. ESS first duplicates and expands each feature plane to match the shape of Xtxyz, resulting in the list of six feature volume sequences: {X Pxy txyz, Pyz txyz}. Afterward, ESS squeezes the corresponding six expanded feature volumes with Hadamard Product: txyz, Pyz txyz, Pty txyz, Pty txyz, Pxz txyz, Pxz txyz, Ptx txyz, Ptx txyz, Ptz txyz, Ptz {X Pxy txyz} . txyz = (cid:89) (2) Hadamard Subsequently, the convolutional network gϕ() is employed to upsample the volumes for generating dense semantic predictions Q: = gϕ(Concat(X txyz, PE(Pos(X txyz)))) , (3) where Concat() and PE() denote the concatenation and sinusoidal positional encoding, respectively. Pos() returns the 4D position of each voxel within the 4D feature volume txyz. Optimization. The VAE is trained with combined loss LVAE, including cross-entropy loss, Lovász-softmax loss (Berman et al., 2018), and Kullback-Leibler (KL) divergence loss: LVAE = LCE(Q, Q) + αLLov(Q, Q) + βLKL(H, (0, I)) , (4) 5 Preprint. Figure 4: Padded Rollout Figure 5: Condition Injection for DiT where LCE is the cross-entropy loss between the input and prediction Q, LLov is the Lovászsoftmax loss, and LKL represents the KL divergence between the latent representation and the prior Gaussian distribution (0, I). Note that the KL divergence is computed for each feature plane of individually, and the term LKL refers to the combined divergence over all six planes. 4.2 DIFFUSION TRANSFORMER FOR HEXPLANE training the VAE, 4D semantic scenes can be embedded as HexPlane = After [Pxy, Pxz, Pyz, Ptx, Pty, Ptz]. Building upon H, we aim to leverage DiT (Peebles & Xie, 2023) model Dτ to generate novel HexPlane, which could be further decoded as novel 4D scenes (see Fig. 2(b)). However, training DiT using token sequences naively generated from each feature plane of HexPlane could not guarantee high generation quality, mainly due to the absence of modeling spatial and temporal relations among the tokens. Padded Rollout Operation. Given that the feature planes of HexPlane may share spatial or temporal dimensions, we employ the Padded Rollout Operation (PRO) to systematically arrange all six planes into unified square feature map, incorporating zero paddings in the uncovered corner areas. As shown in Fig. 4, the dimension of the 2D square feature map is ( ), which minimizes dX the area for padding, where dX , dZ, and dT represent the downsampling rates along the X, Z, and axes, respectively. Subsequently, we follow DiT to first patchify the constructed 2D feature map, converting it into sequence of = (( + )/p)2 tokens, where is the patch size, chosen dT dX so each token holds information from one feature plane. Following patchification, we apply the frequency-based positional embeddings to all tokens similar to DiT. Note that tokens corresponding to padding areas are excluded from the diffusion process. Consequently, the proposed PRO offers an efficient method for modeling spatial and temporal relationships within the token sequence. + dZ + dZ + dT Conditional Generation. DiT enables conditional generation through the use of Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). To incorporate conditions into the generation process, we designed two branches for condition insertion (see Fig. 5). For any condition c, we use the adaLN-Zero technique from DiT, generating scale and shift parameters from and injecting them before and after the attention and feed-forward layers. To handle the complexity of image-based conditions, we add cross-attention block to better integrate the image condition into the DiT block. 4.3 DOWNSTREAM APPLICATIONS Beyond unconditional 4D scene generation, we explore novel applications of DynamicCity through conditional generation and HexPlane manipulation. 6 Preprint. Table 1: Comparisons of 4D Scene Reconstruction. We report the mIoU scores of OccSora (Wang et al., 2024) and our DynamicCity framework on the CarlaSC, Occ3D-Waymo, and Occ3D-nuScenes datasets, respectively, under different resolutions and sequence lengths. Symbol denotes score reported in the OccSora paper. Other scores are reproduced using the official code. Dataset #Classes Resolution #Frames OccSora (Wang et al., 2024) Ours (DynamicCity) CarlaSC (Wilson et al., 2022) Occ3D-Waymo (Tian et al., 2023) Occ3D-nuScenes (Tian et al., 2023) 10 10 10 10 11 11 17 17 1281288 1281288 1281288 1281288 20020016 20020016 20020016 20020016 20020016 4 8 16 32 16 32 32 32 41.01% 39.91% 33.40% 28.91% 36.38% 13.70% 13.51% 13.41% 27.40% 79.61% (+38.6%) 76.18% (+36.3%) 74.22% (+40.8%) 59.31% (+30.4%) 68.18% (+31.8%) 56.93% (+43.2%) 42.60% (+29.1%) 40.79% (+27.3%) 40.79% (+13.4%) Table 2: Comparisons of 4D Scene Generation. We report the Inception Score (IS), Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and the Precision (P) and Recall (R) rates of SemCity (Lee et al., 2024), OccSora (Wang et al., 2024), and our DynamicCity framework on the CarlaSC and Occ3D-Waymo datasets, respectively, in both the 2D and 3D spaces. Dataset Method #Frames Metric2D Metric3D IS2D FID2D KID2D P2D R2D IS3D FID3D KID3D P3D R3D CarlaSC (Wilson et al., 2022) Occ3D-Waymo (Tian et al., 2023) OccSora Ours OccSora Ours 16 2.492 2.498 1.926 1.945 25.08 10.95 82.43 7.138 0.013 0. 0.094 0.003 0.115 0.008 2.257 0.238 0.066 2.331 0.227 0.014 3.129 0.617 0.096 3.206 1559 354.2 3140 1806 52.72 19. 12.20 77.71 0.380 0.151 0.460 0.170 0.384 0.001 0.494 0.026 First, we showcase versatile uses of image conditions in the conditional generation pipeline: 1) HexPlane: By autoregressively generating the HexPlane, we extend scene duration beyond temporal constraints. 2) Layout: We control vehicle placement and dynamics in 4D scenes using conditions learned from birds-eye view sketches. To manage ego vehicle motion, we introduce two numerical conditioning methods: 3) Command: Controls general ego vehicle motion via instructions. 4) Trajectory: Enables fine-grained control through specific trajectory inputs. Inspired by SemCity (Lee et al., 2024), we also manipulate the HexPlane during sampling to: 5) Inpaint: Edit 4D scenes by masking HexPlane regions and guiding sampling with the masked areas. For more details, kindly refer to Sec. A.5 in the Appendix."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL DETAILS Datasets. We train the proposed model on the 1Occ3D-Waymo, 2Occ3D-nuScenes, and 3CarlaSC datasets. The former two from Occ3D (Tian et al., 2023) are derived from Waymo (Sun et al., 2020) and nuScenes (Caesar et al., 2020), where LiDAR point clouds have been completed and voxelized to form occupancy data. Each occupancy scene has resolution of 200 200 16, covering region centered on the ego vehicle, extending 40 meters in all directions and 6.4 meters vertically. The CarlaSC dataset (Wilson et al., 2022) is synthetic occupancy dataset, with scene resolution of 128 128 8, covering region 25.6 meters around the ego vehicle, with height of 3 meters. Implementation Details. Our experiments are conducted using eight NVIDIA A100-80G GPUs. The global batch size used for training the VAE is 8, while the global batch size for training the DiT is 128. Our latent HexPlane is compressed to half the size of the input in each dimension, with the latent channels = 16. The weight for the Lovász-softmax and KL terms are set to 1 and 0.005, respectively. The learning rate for the VAE is 103, while the learning rate for the DiT is 104. 7 Preprint. Figure 6: Dynamic Scene Generation Results. We provide unconditional generation scenes from the 1st, 8th, and 16th frames on Occ3D-Waymo (Left) and CarlaSC (Right), respectively. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples. Evaluation Metrics. The mean intersection over union (mIoU) metric is used to evaluate the reconstruction results of VAE. For DiT, Inception Score, FID, KID, Precision, and Recall are calculated for evaluation. Specifically, we follow prior work (Lee et al., 2024; Wang et al., 2024) by rendering 3D scenes into 2D images and utilizing conventional 2D evaluation pipelines for assessment. Additionally, we train the 3D Encoder to directly extract features from the 3D data and calculate the metrics. For more details, kindly refer to Sec. A.2 in the Appendix. 5.2 4D SCENE RECONSTRUCTION & GENERATION Reconstruction. To evaluate the effectiveness of the proposed VAE in encoding the 4D LiDAR sequence, we compare it with OccSora (Wang et al., 2024) using the CarlaSC, Occ3D-Waymo, and Occ3D-nuScenes datasets. As shown in Tab. 1, DynamicCity outperforms OccSora on these datasets, achieving mIoU improvements of 38.6%, 31.8%, and 43.2% respectively, when the input number of frames is 16. These results highlight the superior performance of the proposed VAE. Generation. To demonstrate the effectiveness of DynamicCity in 4D scene generation, we compare the generation results with OccSora (Wang et al., 2024) on the Occ3D-Waymo and CarlaSC datasets. As shown in Tab. 2, the proposed method outperforms OccSora in terms of perceptual metrics in both 2D and 3D spaces. These results show that our model excels in both generation quality and diversity. Fig. 6 and Fig. 15 show the 4D scene generation results, demonstrating that our model is capable of generating large dynamic scenes in both real-world and synthetic datasets. Our model not only exhibits the ability to generate moving scenes with static semantics shifting as whole, but it is also capable of generating dynamic elements such as vehicles and pedestrians. Applications. Fig. 7 presents the results of our downstream applications. In tasks that involve inserting conditions into the DiT, such as command-conditional generation, trajectory-conditional generation, and layout-conditional generation, our model demonstrates the ability to generate reasonable scenes and dynamic elements while following the prompt to certain extent. Additionally, the inpainting method proves that our HexPlane has explicit spatial meaning, enabling direct modifications within the scene by editing the HexPlane during inference. 8 Preprint. Figure 7: Dynamic Scene Generation Applications. We demonstrate the capability of our model on diverse set of downstream tasks. We show the 1st, 8th, and 16th frames for simplicity. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples. 5.3 ABLATION STUDIES We conduct ablation studies to demonstrate the effectiveness of the components of DynamicCity. VAE. The effectiveness of the VAE is driven by two key innovations: Projection Module and Expansion & Squeeze Strategy (ESS). As shown in Tab. 3, the proposed Projection Module substantially improves HexPlane fitting performance, delivering up to 12.56% increase in mIoU compared to traditional averaging operations. Additionally, compared to querying each point individually, ESS enhances HexPlane fitting quality with up to 7.05% mIoU improvement, significantly boosts training speed by up to 2.06x, and reduces memory usage by substantial 70.84%. HexPlane Dimensions. The dimensions of HexPlane have direct impact on both training efficiency and reconstruction quality. Tab. 4 provides comparison of various downsample rates applied to the original HexPlane dimensions, which are 16 128 128 8 for CarlaSC and 16 200 200 16 for Occ3D-Waymo. As the downsampling rates increase, both the compression rate and training efficiency improve significantly, but the reconstruction quality, measured by mIoU, decreases. 9 Preprint. Table 3: Ablation Study on VAE Network Structures. We report the mIoU scores, training time (second-per-iteration), and training-time memory consumption (VRAM) of different Encoder and Decoder configurations on CarlaSC and Occ3D-Waymo, respectively. Note that ESS denotes Expansion & Squeeze. The best and second-best values are in bold and underlined. Encoder Decoder CarlaSC Occ3D-Waymo mIoU Time (s) VRAM (G) mIoU Time (s) VRAM (G) Average Pooling Average Pooling Projection Projection Query ESS Query ESS 60.97% 68.02% 68.73% 74.22% 0.236 0.143 0.292 0.205 12.46 4. 13.59 5.92 49.37% 55.72% 61.93% 62.57% 1.563 0.758 2.128 1.316 69.66 20. 73.15 25.92 Table 4: Ablation Study on HexPlane Downsampling (D.S.) Rates. We report the compression ratios (C.R.), mIoU scores, training speed (seconds per iteration), and training-time memory consumption on CarlaSC and Occ3D-Waymo. The best and second-best values are in bold and underlined. D.S. Rates CarlaSC dT dX dY dZ C.R. mIoU Time (s) VRAM (G) Occ3D-Waymo C.R. mIoU Time (s) VRAM (G) 1 1 2 2 1 2 2 4 1 2 2 4 1 1 2 2 5.78% 84.67% 1.149 0.289 17.96% 76.05% 0.205 23.14% 74.22% 0.199 71.86% 65.15% 21.63 8.49 5.92 4. Out-of-Memory 38.42% 63.30% 1.852 0.935 48.25% 62.37% 0.877 153.69% 58.13% >80 32.82 24.9 22.30 Table 5: Ablation Study on Organizing HexPlane as Image Tokens. We report the Inception Score (IS), Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and the Precision (P) and Recall (R) rates on CarlaSC. The best values are highlighted in bold. Method Metric2D Metric3D IS2D FID2D KID2D P2D R2D IS3D FID3D KID3D P3D R3D Direct Unfold 2.496 205.0 0.248 0.000 0.000 2. 9110 723.7 0.173 0.043 Vertical Concatenation 2.476 12.79 0. 0.191 0.042 2.305 623.2 26.67 0.424 0.159 Padded Rollout 2. 10.96 0.002 0.238 0.066 2.331 354.2 19.10 0.460 0. To achieve the optimal balance between training efficiency and reconstruction quality, we select downsampling rate of dT = dX = dY = dZ = 2. Padded Rollout Operation. We compare the Padded Rollout Operation with different strategies for obtaining image tokens: 1) Direct Unfold: directly unfolding the six planes into patches and concatenating them; 2) Vertical Concat: vertically concatenating the six planes without aligning dimensions during the rollout process. As shown in Tab. 5, Padded Rollout Operation (PRO) efficiently models spatial and temporal relationships in the token sequence, achieving optimal generation quality."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present DynamicCity, framework for high-quality 4D LiDAR scene generation that captures the temporal dynamics of real-world environments. Our method introduces HexPlane, compact 4D representation generated using VAE with Projection Module, alongside an Expansion & Squeeze Strategy to enhance reconstruction efficiency and accuracy. Additionally, our Masked Rollout Operation reorganizes HexPlane features for DiT-based diffusion, enabling versatile 4D scene generation. Extensive experiments demonstrate that DynamicCity surpasses state-of-the-art methods in both reconstruction and generation, offering significant improvements in quality, training speed, and memory efficiency. DynamicCity paves the way for future research in dynamic scene generation. 10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Matthias Nießner. Polydiff: Generating 3d polygonal meshes with diffusion models. arXiv preprint arXiv:2312.11417, 2023. 3 Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, and David B. Lindell. Tc4d: Trajectory-conditioned text-to-4d generation. arXiv preprint arXiv:2403.17920, 2024. 3 Maxim Berman, Amal Rannen Triki, and Matthew Blaschko. The lovász-softmax loss: tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 44134421, 2018. 5 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. 3 Lucas Caccia, Herke van Hoof, Aaron Courville, and Joelle Pineau. Deep generative modeling of lidar data. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 50345040, 2019. Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1162111631, 2020. 7, 15 Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 130141, 2023. 2, 3, 4 Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1612316133, 2022. 3 Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 30753084, 2019. 16 Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pp. 1634416359, 2022. Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1247912488, 2023. 2, 3, 4 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6, 17 Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu, Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic segmentation via dynamic shifting networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):34803495, 2024. 3 Qianjiang Hu, Zhimin Zhang, and Wei Hu. Rangeldm: Fast realistic lidar point cloud generation. In European Conference on Computer Vision, pp. 115135, 2024. Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised representation learning for 3d point clouds. In IEEE/CVF International Conference on Computer Vision, pp. 65356545, 2021. 2 Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 dynamic object generation from monocular video. arXiv preprint arXiv:2311.02848, 2023. 3 11 Preprint. Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic scene generation with triplane diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2833728347, 2024. 2, 3, 7, 8, Yuheng Liu, Xinke Li, Xueting Li, Lu Qi, Chongshou Li, and Ming-Hsuan Yang. Pyramid diffusion for fine 3d large scene generation. arXiv preprint arXiv:2311.12085, 2023a. 2 Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. In International Conference on Meshdiffusion: Score-based generative 3d mesh modeling. Learning Representations, 2023b. 3 Zhiyuan Ma, Yuxiang Wei, Yabin Zhang, Xiangyu Zhu, Zhen Lei, and Lei Zhang. Scaledreamer: Scalable text-to-3d synthesis with asynchronous score distillation. In European Conference on Computer Vision, pp. 119, 2024. 3 Kazuto Nakashima and Ryo Kurazume. Learning to drop points for lidar scan synthesis. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 222229, 2021. 2 Kazuto Nakashima and Ryo Kurazume. Lidar data synthesis with denoising diffusion probabilistic models. In IEEE International Conference on Robotics and Automation, pp. 1472414731, 2024. 3 Kazuto Nakashima, Yumi Iwashita, and Ryo Kurazume. Generative range imaging for learning scene priors of 3d lidar data. In IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 12561266, 2023. 2, 3 Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, and Cyrill Stachniss. Scaling diffusion models to real-world 3d lidar scene completion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1477014780, 2024. 3 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32: 80268037, 2019. 17 William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. 2, 4, 6 Haoxi Ran, Vitor Guizilini, and Yue Wang. Towards realistic scene generation with lidar diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1473814748, 2024. 3 Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 3 Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024a. 3 Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 42094219, 2024b. 2, Sara Rojas, Julien Philip, Kai Zhang, Sai Bi, Fujun Luan, Bernard Ghanem, and Kalyan Sunkavall. Datenerf: Depth-aware text-based editing of nerfs. arXiv preprint arXiv:2404.04526, 2024. 3 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. 3 Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 12 Preprint. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2015. 16 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-tovideo generation without text-video data. In International Conference on Learning Representations, 2022. 3 Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 3 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24462454, 2020. 7, 15 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182826, 2015. Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In European Conference on Computer Vision, pp. 685702, 2020. 16 Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang Zhao. Occ3d: large-scale 3d occupancy prediction benchmark for autonomous driving. In Advances in Neural Information Processing Systems, volume 36, pp. 6431864330, 2023. 7, 15, 20, 22, 23 Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, and Jiwen Lu. Occsora: 4d occupancy generation models as world simulators for autonomous driving. arXiv preprint arXiv:2405.20337, 2024. 2, 3, 7, 8, 19, 27 Joey Wilson, Jingyu Song, Yuewei Fu, Arthur Zhang, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, and Maani Ghaffari. Motionsc: Data set and network for real-time semantic mapping in dynamic environments. IEEE Robotics and Automation Letters, 7(3):84398446, 2022. 7, 15, 19, 20, 21, 24, 25, 26, 27 Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024a. 3 Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024b. Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Ultralidar: Learning compact representations for lidar completion and generation. arXiv preprint arXiv:2311.01448, 2023. 3 Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, and Qingshan Liu. 4d contrastive superflows are dense 3d representation learners. In European Conference on Computer Vision, pp. 5880, 2024. 2 Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, and Changjun Jiang. Lidar4d: Dynamic neural fields for novel space-time view lidar synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 51455154, 2024. 2, 3 Vlas Zyrianov, Xiyue Zhu, and Shenlong Wang. Learning to generate realistic lidar point clouds. In European Conference on Computer Vision, pp. 1735, 2022. 3 Preprint."
        },
        {
            "title": "APPENDIX",
            "content": "In this appendix, we supplement the following materials to support the findings and conclusions drawn in the main body of this paper. Additional Implementation Details A.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 DiT Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Model Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Classifier-Free Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Downstream Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Quantitative Results B.1 Per-Class Generation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Qualitative Results C.1 Unconditional Dynamic Scene Generation . . . . . . . . . . . . . . . . . . . . . . C.2 HexPlane-Guided Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Layout-Guided Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Command- & Trajectory-Guided Generation . . . . . . . . . . . . . . . . . . . . . C.5 Dynamic Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 Comparisons with OccSora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Potential Societal Impact & Limitations D.1 Societal Impact D.2 Broader Impact . . . . . . D.3 Known Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Public Resources Used E.1 Public Datasets Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Public Implementations Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 17 17 17 19 20 20 22 23 26 27 28 28 28 29 29 29 14 Preprint."
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "In this section, we provide additional implementation details to assist in reproducing this work. Specifically, we elaborate on the details of the datasets, DiT evaluation metrics, the specifics of our generation models, and discussions on the downstream applications. A.1 DATASETS Our experiments primarily utilize two datasets: Occ3D-Waymo (Tian et al., 2023) and CarlaSC (Wilson et al., 2022). Additionally, we also evaluate our VAE on Occ3D-nuScenes (Tian et al., 2023). The Occ3D-Waymo dataset is derived from real-world Waymo Open Dataset (Sun et al., 2020) data, where occupancy sequences are obtained through multi-frame fusion and voxelization processes. Similarly, Occ3D-nuScenes is generated from the real-world nuScenes (Caesar et al., 2020) dataset using the same fusion and voxelization operations. On the other hand, the CarlaSC dataset is generated from simulated scenes and sensor data, yielding occupancy sequences. Using these different datasets demonstrates the effectiveness of our method on both real-world and synthetic data. To ensure consistency in the experimental setup, we select 11 commonly used semantic categories and map the original categories from both datasets to these 11 categories. The detailed semantic label mappings are provided in Tab. 6. Table 6: Summary of Semantic Label Mappings. We unify the semantic classes between the CarlaSC (Wilson et al., 2022), Occ3D-Waymo (Tian et al., 2023), and Occ3D-nuScenes (Tian et al., 2023) datasets for semantic scene generation. Class Building Barrier Other Pedestrian Pole Road Ground Sidewalk Vegetation Vehicle CarlaSC Building Barrier, Wall, Guardrail Other, Sky, Bridge, Rail track, Static, Dynamic, Water Occ3D-Waymo Occ3D-nuScenes Building - Manmade Barrier General Object General Object Pedestrian Pedestrian Pole, Traffic sign, Traffic light Sign, Traffic light, Pole, Construction Cone Pedestrian Traffic cone Road, Roadlines Ground, Terrain Sidewalk Vegetation Vehicle Road - Sidewalk Vegetation, Tree trunk Vehicle Bicyclist, Bicycle, Motorcycle Drivable surface Other flat, Terrain Sidewalk Vegetation Bus, Car, Construction vehicle, Trailer, Truck Bicycle, Motorcycle Bicycle - Occ3D-Waymo. This dataset contains 798 training scenes, with each scene lasting approximately 20 seconds and sampled at frequency of 10 Hz. This dataset includes 15 semantic categories. We use volumes with resolution of 200 200 16 from this dataset. CarlaSC. This dataset contains 6 training scenes, each duplicated into Light, Medium, and Heavy based on traffic density. Each scene lasts approximately 180 seconds and is sampled at frequency of 10 Hz. This dataset contains 22 semantic categories, and the scene resolution is 128 128 8. Occ3D-nuScenes. This dataset contains 600 scenes, with each scene lasting approximately 20 seconds and sampled at frequency of 2 Hz. Compared to Occ3D-Waymo and CarlaSC, Occ3D-nuScenes has fewer total frames and more variation between scenes. This dataset includes 17 semantic categories, with resolution of 200 200 16. 15 Preprint. A.2 DIT EVALUATION METRICS Inception Score (IS). This metric evaluates the quality and diversity of generated samples using pre-trained Inception model as follows: IS = exp (cid:0)EQpg [DKL(p(yQ) p(y))](cid:1) , where pg represents the distribution of generated samples. p(yQ) is the conditional label distribution given by the Inception model for generated sample Q. p(y) = (cid:82) p(yQ)pg(Q) dQ is the marginal distribution over all generated samples. DKL(p(yQ) p(y)) is the Kullback-Leibler divergence, defined as follows: (5) DKL(p(yQ) p(y)) = p(yiQ) log . (6) (cid:88) p(yiQ) p(yi) Fréchet Inception Distance (FID). This metric measures the distance between the feature distributions of real and generated samples: FID = µr µg2 + Tr (cid:16) Σr + Σg 2(ΣrΣg)1/2(cid:17) , (7) where µr and Σr are the mean and covariance matrix of features from real samples. µg and Σg are the mean and covariance matrix of features from generated samples. Tr denotes the trace of matrix. Kernel Inception Distance (KID). This metric uses the squared Maximum Mean Discrepancy (MMD) with polynomial kernel as follows: KID = MMD2(ϕ(Qr), ϕ(Qg)) , (8) where ϕ(Qr) and ϕ(Qg) represent the features of real and generated samples extracted from the Inception model. MMD with polynomial kernel k(x, y) = (xy + c)d is calculated as follows: MMD2(X, ) = 1 m(m 1) (cid:88) i=j k(xi, xj) + 1 n(n 1) (cid:88) i=j k(yi, yj) 2 mn (cid:88) i,j k(xi, yj) , (9) where = {Q1, . . . , Qm} and = {y1, . . . , yn} are sets of features from real and generated samples. Precision. This metric measures the fraction of generated samples that lie within the real data distribution as follows: Precision = 1 N (cid:88) i=1 (cid:0)(fg µr)Σ1 (fg µr) χ2(cid:1) , (10) where fg is generated sample in the feature space. µr and Σr are the mean and covariance of the real data distribution. I() is the indicator function. χ2 is threshold based on the chi-squared distribution. Recall. This metric measures the fraction of real samples that lie within the generated data distribution as follows: Recall = 1 (cid:88) j=1 (cid:0)(fr µg)Σ (fr µg) χ2(cid:1) , (11) where: fr is real sample in the feature space. µg and Σg are the mean and covariance of the generated data distribution. I() is the indicator function. χ2 is threshold based on the chi-squared distribution. 2D Evaluations. We render 3D scenes as 2D images for 2D evaluations. To ensure fair comparisons, we use the same semantic colormap and camera settings across all experiments. pre-trained InceptionV3 (Szegedy et al., 2015) model is used to compute the Inception Score (IS), Fréchet Inception Distance (FID), and Kernel Inception Distance (KID) scores, while Precision and Recall are computed using pre-trained VGG-16 (Simonyan & Zisserman, 2015) model. 3D Evaluations. For 3D data, we trained MinkowskiUNet (Choy et al., 2019) as an autoencoder. We adopt the latest implementation from SPVNAS (Tang et al., 2020), which supports optimized sparse convolution operations. The features were extracted by applying average pooling to the output of the final downsampling block. Preprint. A.3 MODEL DETAILS General Training Details. We implement both the VAE and DiT models using PyTorch (Paszke et al., 2019). We utilize PyTorchs mixed precision and replace all attention mechanisms with FlashAttention (Dao et al., 2022) to accelerate training and reduce memory usage. AdamW is used as the optimizer for all models. We train the VAE with learning rate of 103, running for 20 epochs on Occ3D-Waymo and 100 epochs on CarlaSC. The DiT is trained with learning rate of 104, and the EMA rate for DiT is set to 0.9999. VAE. Our encoder projects the 4D input into HexPlane, where each dimension is compressed version of the original 4D input. First, 3D CNN is applied to each frame for feature extraction and downsampling, with dimensionality reduction applied only to the spatial dimensions (X, , Z). Next, the Projection Module projects the 4D features into the HexPlane. Each small transformer within the Projection Module consists of two layers, and the attention mechanism has two heads. Each head has dimensionality of 16, with dropout rate of 0.1. Afterward, we further downsample the dimension to half of its original size. During decoding, we first use three small transpose CNNs to restore the dimension, then use an ESS module to restore the 4D features. Finally, we apply 3D CNN to recover the spatial dimensions and generate point-wise predictions. Diffusion. We set the patch size to 2 for our DiT models. The Waymo DiT model has hidden size of 768, 18 DiT blocks, and 12 attention heads. The CarlaSC DiT model has hidden size of 384, 16 DiT blocks, and 8 attention heads. A.4 CLASSIFIER-FREE GUIDANCE Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(xc) and unconditional generation p(x), and guidance during sampling is provided by the following equation: ˆxt = (1 + w) ˆxt(c) ˆxt() , (12) where ˆxt(c) is the result conditioned on c, ˆxt() is the unconditioned result, and is weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved. A.5 DOWNSTREAM APPLICATIONS This section provides comprehensive explanation of five tasks to demonstrate the capability of our 4D scene generation model across various scenarios. HexPlane. Since our model is based on Latent Diffusion Models, it is inherently constrained to generate results that match the latent space dimensions, limiting the temporal length of unconditionally generated sequences. We argue that robust 4D generation model should not be restricted to producing only short sequences. Instead of increasing latent space size, we leverage CFG to generate sequences in an auto-regressive manner. By conditioning each new 4D sequence on the previous one, we sequentially extend the temporal dimension. This iterative process significantly extends sequence length, enabling long-term generation, and allows conditioning on any real-world 4D scene to predict the next sequence using the DiT model. We condition our DiT by using the HexPlane from frames earlier. For any condition HexPlane, we apply patch embedding and positional encoding operations to obtain condition tokens. These tokens, combined with other conditions, are fed into the adaLN-Zero and Cross-Attention branches to influence the main branch. Layout. To control object placement in the scene, we train model capable of generating vehicle dynamics based on birds-eye view sketch. We apply semantic filtering to the birds-eye view of the input scene, marking regions with vehicles as 1 and regions without vehicles as 0. Pooling this binary image provides layout information as tensor from the birds-eye perspective. 17 Preprint. The layout is padded to match the size of the HexPlane, ensuring that the positional encoding of the birds-eye layout aligns with the XY plane. DiT learns the correspondence between the layout and vehicle semantics using the same conditional injection method applied to the HexPlane. Command. While we have developed effective methods to control the HexPlane in both temporal and spatial dimensions, critical aspect of 4D autonomous driving scenarios is the motion of the ego vehicle. To address this, we define four commands: STATIC, FORWARD, TURN LEFT, and TURN RIGHT, and annotate our training data by analyzing ego vehicle poses. During training, we follow the traditional DiT approach of injecting class labels, where the commands are embedded and fed into the model via adaLN-Zero. Trajectory. For more fine-grained control of the ego vehicles motion, we extend the commandbased conditioning into trajectory condition branch. For any 4D scene, the XY coordinates of the trajectory traj RT 2 are passed through an MLP and injected into the adaLN-Zero branch. Inpaint. We demonstrate that our model can handle versatile applications by training conditional DiT for the previous tasks. Extending our exploration of downstream applications, and inspired by (Lee et al., 2024), we leverage the 2D structure of our latent space and the explicit modeling of each dimension to highlight our models ability to perform inpainting on 4D scenes. During DiT sampling, we define 2D mask RXY on the XY plane, which is extended across all dimensions to mask specific regions of the HexPlane. At each step of the diffusion process, we apply noise to the input Hin and update the HexPlane using the following formula: Ht = Ht + (1 m) Hin , (13) where denotes the element-wise product. This process inpaints the masked regions while preserving the unmasked areas of the scene, enabling partial scene modification, such as turning an empty street into one with heavy traffic. 18 Preprint."
        },
        {
            "title": "B ADDITIONAL QUANTITATIVE RESULTS",
            "content": "In this section, we present additional quantitative results to demonstrate the effectiveness of our VAE in accurately reconstructing 4D scenes. B.1 PER-CLASS GENERATION RESULTS We include the class-wise IoU scores of OccSora (Wang et al., 2024) and our proposed DynamicCity framework on CarlaSC (Wilson et al., 2022). As shown in Tab. 7, our results demonstrate higher IoU across all classes, indicating that our VAE reconstruction achieves minimal information loss. Additionally, our model does not exhibit significantly low IoU for any specific class, proving its ability to effectively handle class imbalance. Table 7: Comparisons of Per-Class IoU Scores. We compared the performance of OccSora (Wang et al., 2024), and our DynamicCity framework on CarlaSC (Wilson et al., 2022) across 10 semantic classes. The scene resolution is 1281288. The sequence lengths are 4, 8, 16, and 32, respectively. d B r B h a s P P R o l d n a e c V Method mIoU Resolution: 128 128 8 Sequence Length: 4 OccSora Ours Improv. 41.009 79.604 38.861 76.364 10.616 31.354 6.637 68.898 19.191 93.436 21.825 87. 93.910 98.617 61.357 87.014 86.671 95.129 15.685 68.700 55.340 88.569 38. 37.503 20.738 62.261 74.245 66.137 4. 25.657 8.458 53.015 33.229 Resolution: 128 128 8 Sequence Length: 8 OccSora Ours Improv. 39.910 76.181 33.001 70.874 3.260 50.025 5.659 52.433 19.224 87. 19.357 85.866 93.038 97.513 57.335 83.074 85.551 93.944 30.899 58.626 51.776 81. 36.271 37.873 46.765 46.774 68.734 66. 4.475 25.739 8.393 27.727 29.722 Resolution: 128 128 8 Sequence Length: OccSora Ours Improv. 33.404 74.223 19.264 66.852 2.205 51.901 3.454 49. 11.781 79.410 9.165 82.369 92.054 96.937 50.077 84.484 82.594 94.082 18.078 58. 45.363 78.134 40.819 47.588 49.696 46.390 67. 73.204 4.883 34.407 11.488 40.139 32. Resolution: 128 128 8 Sequence Length: 32 OccSora Ours Improv. 28.911 59.308 16.565 52.036 1.413 25. 0.944 29.382 6.200 56.811 4.150 57.876 91.466 94.792 43.399 78.390 78.614 89. 11.007 46.080 35.353 62.234 30.397 35.471 24.108 28. 50.611 53.726 3.326 34.991 11.341 35. 26.881 19 Preprint."
        },
        {
            "title": "C ADDITIONAL QUALITATIVE RESULTS",
            "content": "In this section, we provide additional qualitative results on the Occ3D-Waymo (Tian et al., 2023) and CarlaSC (Wilson et al., 2022) datasets to demonstrate the effectiveness of our approach. C.1 UNCONDITIONAL DYNAMIC SCENE GENERATION First, we present full unconditional generation results in Fig. 8 and 9. These results demonstrate that our generated scenes are of high quality, realistic, and contain significant detail, capturing both the overall scene dynamics and the movement of objects within the scenes. Figure 8: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. 20 Preprint. Figure 9: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of total of 16 consectutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. 21 Preprint. C.2 HEXPLANE-GUIDED GENERATION We show results for our HexPlane conditional generation in Fig. 10. Although the sequences are generated in groups of 16 due to the settings of our VAE, we successfully generate long sequence by conditioning on the previous one. The result contains 64 frames, comprising four sequences, and depicts T-intersection with many cars parked along the roadside. This result demonstrates strong temporal consistency across sequences, proving that our framework can effectively predict the next sequence based on the current one. Figure 10: HexPlane-Guided Generation Results. We provide qualitative examples of total of 64 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. 22 Preprint. C.3 LAYOUT-GUIDED GENERATION The layout conditional generation result is presented in Fig. 11. First, we observe that the layout closely matches the semantic positions in the generated result. Additionally, as the layout changes, the positions of the vehicles in the scene also change accordingly, demonstrating that our model effectively captures the condition and influences both the overall scene layout and vehicle placement. Figure 11: Layout-Guided Generation Results. We provide qualitative examples of total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. Preprint. C.4 COMMAND- & TRAJECTORY-GUIDED GENERATION We present command conditional generation in Fig. 12 and trajectory conditional generation in Fig. 13. These results show that when we input command, such as \"right turn,\" or sequence of XY-plane coordinates, our model can effectively control the motion of the ego vehicle and the relative motion of the entire scene based on these movement trends. Figure 12: Command-Guided Scene Generation Results. We provide qualitative examples of total of 16 consectutive frames generated under the command RIGHT by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. 24 Preprint. Figure 13: Trajectory-Guided Scene Generation Results. We provide qualitative examples of total of 16 consectutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. 25 Preprint. C.5 DYNAMIC INPAINTING We present the full inpainting results in Fig. 14. The results show that our model successfully regenerates the inpainted regions while ensuring that the areas outside the inpainted regions remain consistent with the original scene. Furthermore, the inpainted areas seamlessly blend into the original scene, exhibiting realistic placement and dynamics. Figure 14: Dynamic Inpainting Results. We provide qualitative examples of total of 16 consectutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. 26 Preprint. C.6 COMPARISONS WITH OCCSORA We compare our qualitative results with OccSora (Wang et al., 2024) in Fig. 15, using similar scene. It is evident that our result presents realistic dynamic scene, with straight roads and complete objects and environments. In contrast, OccSoras result displays unreasonable semantics, such as pedestrian in the middle of the road, broken vehicles, and lack of dynamic elements. This comparison highlights the effectiveness of our method. Figure 15: Comparisons of Dynamic Scene Generation. We provide qualitative examples of total of 16 consectutive frames generated by OccSora (Wang et al., 2024) and our proposed DynamicCity framework on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. Preprint. POTENTIAL SOCIETAL IMPACT & LIMITATIONS In this section, we elaborate on the potential positive and negative societal impact of this work, as well as the broader impact and some potential limitations. D.1 SOCIETAL IMPACT Our approachs ability to generate high-quality 4D LiDAR scenes holds the potential to significantly impact various domains, particularly autonomous driving, robotics, urban planning, and smart city development. By creating realistic, large-scale dynamic scenes, our model can aid in developing more robust and safe autonomous systems. These systems can be better trained and evaluated against diverse scenarios, including rare but critical edge cases like unexpected pedestrian movements or complex traffic patterns, which are difficult to capture in real-world datasets. This contribution can lead to safer autonomous vehicles, reducing traffic accidents, and improving traffic efficiency, ultimately benefiting society by enhancing transportation systems. In addition to autonomous driving, DynamicCity can be valuable for developing virtual reality (VR) environments and augmented reality (AR) applications, enabling more realistic 3D simulations that could be used in various industries, including entertainment, training, and education. These advancements could help improve skill development in driving schools, emergency response training, and urban planning scenarios, fostering safer and more informed society. Despite these positive outcomes, the technology could be misused. The ability to generate realistic dynamic scenes might be exploited to create misleading or fake data, potentially undermining trust in autonomous systems or spreading misinformation about the capabilities of such technologies. However, we do not foresee any direct harmful impact from the intended use of this work, and ethical guidelines and responsible practices can mitigate potential risks. D.2 BROADER IMPACT Our approachs contribution to 4D LiDAR scene generation stands to advance the fields of autonomous driving, robotics, and even urban planning. By providing scalable solution for generating diverse and dynamic LiDAR scenes, it enables researchers and engineers to develop more sophisticated models capable of handling real-world complexity. This has the potential to accelerate progress in autonomous systems, making them safer, more reliable, and adaptable to wide range of environments. For example, researchers can use DynamicCity to generate synthetic training data, supplementing real-world data, which is often expensive and time-consuming to collect, especially in dynamic and high-risk scenarios. The broader impact also extends to lowering entry barriers for smaller research institutions and startups that may not have access to vast amounts of real-world LiDAR data. By offering means to generate realistic and dynamic scenes, DynamicCity democratizes access to high-quality data for training and validating machine learning models, thereby fostering innovation across the autonomous driving and robotics communities. However, it is crucial to emphasize that synthetic data should be used responsibly. As our model generates highly realistic scenes, there is risk that reliance on synthetic data could lead to models that fail to generalize effectively in real-world settings, especially if the generated scenes do not capture the full diversity or rare conditions found in real environments. Hence, its important to complement synthetic data with real-world data and ensure transparency when using synthetic data in model training and evaluation. D.3 KNOWN LIMITATIONS Despite the strengths of DynamicCity, several limitations should be acknowledged. First, our models ability to generate extremely long sequences is still constrained by computational resources, leading to potential challenges in accurately modeling scenarios that span extensive periods. While we employ techniques to extend temporal modeling, there may be degradation in scene quality or consistency when attempting to generate sequences beyond certain length, particularly in complex traffic scenarios. 28 Preprint. Second, the generalization capability of DynamicCity depends on the diversity and representativeness of the training datasets. If the training data does not cover certain environmental conditions, object categories, or dynamic behaviors, the generated scenes might lack these aspects, resulting in incomplete or less realistic dynamic LiDAR data. This could limit the models effectiveness in handling unseen or rare scenarios, which are critical for validating the robustness of autonomous systems. Third, while our model demonstrates strong performance in generating dynamic scenes, it may face challenges in highly congested or intricate traffic environments, where multiple objects interact closely with rapid, unpredictable movements. In such cases, DynamicCity might struggle to capture the fine-grained details and interactions accurately, leading to less realistic scene generation. Lastly, the reliance on pre-defined semantic categories means that any variations or new object types not included in the training set might be inadequately represented in the generated scenes. Addressing these limitations would require integrating more diverse training data, improving the models adaptability, and refining techniques for longer sequence generation."
        },
        {
            "title": "E PUBLIC RESOURCES USED",
            "content": "In this section, we acknowledge the public resources used, during the course of this work. E.1 PUBLIC DATASETS USED nuScenes1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 nuScenes-devkit2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 Waymo Open Dataset3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Waymo Dataset License CarlaSC4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License Occ3D5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License E.2 PUBLIC IMPLEMENTATIONS USED SemCity6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown OccSora7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 MinkowskiEngine8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License TorchSparse9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License SPVNAS10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License spconv11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 1https://www.nuscenes.org/nuscenes 2https://github.com/nutonomy/nuscenes-devkit 3https://waymo.com/open 4https://umich-curly.github.io/CarlaSC.github.io. 5https://tsinghua-mars-lab.github.io/Occ3D. 6https://github.com/zoomin-lee/SemCity. 7https://github.com/wzzheng/OccSora. 8https://github.com/NVIDIA/MinkowskiEngine. 9https://github.com/mit-han-lab/torchsparse. 10https://github.com/mit-han-lab/spvnas. 11https://github.com/traveller59/spconv."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "National University of Singapore",
        "S-Lab, Nanyang Technological University",
        "Shanghai AI Laboratory"
    ]
}