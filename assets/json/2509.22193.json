{
    "paper_title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance",
    "authors": [
        "Nicolas Boizard",
        "Hippolyte Gisserot-Boukhlef",
        "Kevin El-Haddad",
        "Céline Hudelot",
        "Pierre Colombo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 9 1 2 2 . 9 0 5 2 : r WHEN DOES REASONING MATTER? CONTROLLED STUDY OF REASONINGS CONTRIBUTION TO MODEL PERFORMANCE. Nicolas Boizard1,5 Hippolyte Gisserot-Boukhlef2,5 Kevin El Haddad1,4 Celine Hudelot Pierre Colombo3,5 1Diabolocom 2Artefact Research Center 5MICS, CentraleSupelec, Universite Paris-Saclay 3Equall 4ISIA Lab, University of Mons Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on synthetic data distillation framework to conduct large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks. Correspondence: nicolas.boizard@centralesupelec.fr Website: https://huggingface.co/when-does-reasoning-matter Date: September 26,"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) that generate explicit Chains of Thought (CoT) have rapidly become defining paradigm. The research community is releasing increasingly capable reasoning models, which consistently outperform standard Instruction Fine-Tuned (IFT) counterparts at test time, especially on math, coding, and other reasoning-heavy tasks DeepSeek-AI (2025); OpenAI (2024); Mistral-AI (2025). Despite rapid progress, we still lack clarity on when explicit reasoning is most beneficial. Both prior evidence and our findings (Figure 1) point to highly task-dependent picture: reasoning yields substantial gains on math and coding benchmarks where multi-step problem solving is essential (Zhu et al., 2024), but provides only limited improvements on simpler factual or classification tasks (Liu et al., 2024). As Figure 1 shows, these gains concentrate on reasoning-intensive (e.g., gsm8k, aime) and open-ended tasks, while benefits on general multiple-choice tasks are much smaller or inconsistent. Figure 1: Task sensitivity to reasoning. Reasoning helps most on open-ended and math tasks; gains are limited or inconsistent on general multiple-choice tasks. X-axis: extra-token factor when switching from IFT to reasoning. Y-axis: accuracy gain (%). Meanwhile, the scaling dynamics of reasoning models pose further challenges. Small models often struggle to absorb the reasoning depth of large teachers unless traces are carefully adapted (Li et al.). Conversely, at larger scales, reasoning appears to unlock performance plateaus that IFT models When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. cannot surpass, as shown by frontier efforts such as OpenAIs o1 reasoning series (OpenAI, 2024) and open-source counterparts like Qwen (Qwen-Team, 2025) and Mistrals Magistral line (MistralAI, 2025). While these works emphasize headline results, they dont systematically disentangle confounding factors such as model scale or training and inference budget, leaving practitioners with little concrete guidance. The goal of this paper is to bridge these gaps by providing unified, controlled view of reasoning versus IFT. More broadly, we aim to clarify the design choices shaping reasoning models: Which tasks consistently benefit from reasoning, how do these gains vary with model scale, and how are they balanced against training and inference costs relative to standard IFT? Challenges. Addressing this question is highly challenging, requiring controlled experimental setup specifically designed to isolate performance drivers such as data domain, model capacity, and inference budget. Our approach. We investigate this matter with large-scale, fully controlled distillation setup that holds data and capacity constant while varying the supervision format (IFT vs. reasoning). single teacher produces paired answers (IFT and reasoning) to the same prompts (Appendix D), enabling like-for-like comparisons across model sizes and domains. Contributions. This paper makes three main contributions: controlled reasoning testbed for disentangling confounders. We present large-scale distillation framework that isolates the effect of supervision format (IFT vs. reasoning) across different model sizes and data domains. This design removes major confounders and enables clean attribution of performance. Using 1.6M IFT-reasoning pairs for training and evaluating over 12 benchmarks (amounting to 70k H100 GPU-hours), we map reasonings impact across model scale, task family (math vs. general), and answer format (multiple-choice vs. open-ended). Actionable guidance for practitioners. Reasoning reliably breaks IFT performance plateaus, often matching models several times larger ( 3), whereas IFT remains reliably cost-efficient path for both training and inference ( 4). In nutshell, reasoning is beneficial when task and scale justify the extra compute, whereas larger IFT model is preferable otherwise. Open resources. We release all code and paired training datasets (IFT and reasoning outputs for the same inputs) to enable reproducibility and future controlled studies on reasoning."
        },
        {
            "title": "2 EXPERIMENTAL SETUP",
            "content": "Frontier research initiatives highlight reasoning models performance but often do not disentangle the underlying sources of improvement, due to opaque data mixtures and shifting supervision schemes. We move the needle by isolating reasoning itself. Using single teacher that generates paired IFT and reasoning answers to the same prompts, we assess performance across model scales and data domains. This controlled setup enables clean attribution of performance to reasoning while sidestepping the cost of RL pipelines (Mistral-AI, 2025; Qwen-Team, 2025). 2.1 FORMALIZATION Preliminaries. We adopt the standard prompt-based generation setting, where causal language model fθ : Ω RΩ maps an input text sequence to unnormalized logit scores for next-token prediction. Here, Ω = {ω1, . . . , ωΩ} is the vocabulary and Ω its Kleene closure.1 We define the generation mechanism Gτ,p such that Gτ,p(fθ) : Ω Ω represents the recursive generation process of fθ under temperature τ 0 and nucleus-sampling parameter ]0, 1]. For convenience, we denote this process by gθ. Intuitively, given question x, gθ(x) corresponds to the answer generated by model fθ. 1Ω is the set of all sequences written with elements in Ω. Formally, Ω = (cid:83) i=0 Ωi. When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Distillation procedure. We consider student model fθS : Ω RΩ and teacher model fθT : Ω {0, 1} RΩ. Let gθS : Ω Ω and gθT : Ω {0, 1} Ω be the generation function of the student and teacher models, respectively. The teacher differs from the student in that it accepts an additional binary input {0, 1} indicating whether reasoning mode is enabled (r = 1) or disabled (r = 0). Given collection of input questions = {xi}N i=1, we construct synthetic dataset = {(xi, gθT (xi, ri))}N i=1, where ri {0, 1} specifies whether reasoning is enabled for sample i. The distilled student model can be written as TH (fθS , D), where TH denotes the causal training procedure that updates student fθS on the teacher-generated dataset under hyperparameters H."
        },
        {
            "title": "2.2 DISTILLATION PROTOCOL",
            "content": "Teacher models (fθT ). For data generation, we employ state-of-the-art open-weight mixture-ofexperts model, Qwen3-235B-A22B (Qwen-Team, 2025), which includes configurable flag that enables or disables reasoning mode. Student models (fθS ). We distill knowledge into five Qwen2.5 base models ranging from 0.5B to 14B parameters: Qwen-2.5-0.5B, *-1.5B, *-3B, *-7B and *-14B (Yang et al., 2024a; Qwen-Team, 2024). These untuned base checkpoints are chosen from family distinct from the teachers, reducing pretraining overlap and inductive biases. Input questions (X). We consider two regimes that reflect common deployment scenarios. (1) General-purpose training: starting from base student, we distill general teacher capabilities using input questions from the 7M core subset of the Infinity-Instruct dataset (Li et al., 2025). These questions cover multiple domains, including general knowledge, commonsense Q&A, coding, and math, and are denoted by Xgeneral. (2) Math-centric training: starting from either base or general-distribution-trained student, we distill knowledge on specific domain. We decide to focus on mathematics, as it is common reasoning domain. Input questions, Xmath, are drawn from the Llama-Nemotron-Post-Training-Dataset (Bercovich et al., 2025). Data generation (D). For each set of input questions {Xgeneral, Xmath}, we generate answers under both = 0 (IFT) and = 1 (reasoning). Formally, DIF = {(x, gθT (x, 0)) X} and DR = {(x, gθT (x, 1)) X}. For reasoning generations, we sample with temperature τ = 0.6 and nucleus parameter = 0.95, while for IFT we use τ = 0.7 and = 0.8.2 In total, to ensure sufficient convergence during model training, we generate 1.6M answer pairs: 1.3M for the general-domain setting and 300K for the math-centric scenario. Training (T ). All student models are trained exclusively on synthetic data produced by the teacher; no reinforcement learning is involved. To control the impact of supervision format, we vary the fraction of reasoning versus IFT instances. Let Xρ be subset of prompts such that Xρ ρX, with ρ [0, 1] denoting the reasoning ratio. We then construct Dρ = {(x, y) (x, y) DR, Xρ} and Dρ IF = {(x, y) (x, y) DIF , Xρ}, and train on their union Dρ = Dρ R. We evaluate ρ {0, 0.25, 0.5, 0.75, 1} under two settings: (1) sequential training (Tseq), where models are first trained on IFT and then reasoning data, and (2) mixed training (Tmix), where both are combined from the start. We also study domain-specific adaptation, where general-domain students are further aligned on math-centric data.3 IF Dρ 2.3 EVALUATION METHODOLOGY Benchmarks. For comprehensive assessment, we evaluate models on suite of 12 benchmarks covering both general-purpose and mathematical reasoning, across Multiple-Choice (MC) and Open-Ended (OE) formats. For general-purpose MC tasks, we use winogrande (Keisuke et al., 2020), openbookqa (Mihaylov et al., 2018), and mmlu-misc. For general-purpose OE tasks, we use squad (Rajpurkar et al., 2016), coqa (Reddy et al., 2019), and ifeval (Zhou et al., 2023). In the mathematical domain, MC benchmarks include aqua-rat (Ling et al., 2017), mmlu-math 2Generation parameters were sampled according to the Qwen3-235B-A22B model recommendations. 3Training hyperparameters are further discussed in Appendix B. 3 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. (Hendrycks et al., 2021), and mmlu-pro-math (Wang et al., 2024), while OE benchmarks include gsm8k (Cobbe et al., 2021b), math-500 (Lightman et al., 2023), and aime (of Problem Solving, 2025). Additional details on task prompting are provided in Appendix D. Inference parameters. We apply standard decoding with temperature τ = 1.0 and nucleussampling parameter = 1.0. To mitigate the limited instruction-following capability of base student models, we evaluate them in three-shot setting, whereas distilled models are evaluated in zeroshot setting to directly measure distilled behaviors. LLM as judge. To ensure consistent and reliable evaluation across tasks, we use Llama-3 1-Nemotron-Ultra-253B-v1 (Bercovich et al., 2025) as judge model (Zheng et al., 2023; Gu et al., 2024; Saha et al., 2025), with sampling parameters τ = 0.7 and = 0.95. Using high-capacity judge alleviates the issue of smaller student models producing poorly structured answers, which can make automatic correctness assessment difficult, by accurately interpreting and scoring their outputs. Additional details on judge prompting are provided in Appendix D."
        },
        {
            "title": "3 MODEL PERFORMANCE ANALYSIS",
            "content": "We analyze how downstream performance shifts under different training design choices. Specifically, we vary the supervision format (IFT vs. reasoning) across different model scales and data domains (general vs. math). This setup allows us to disentangle the contribution of reasoning traces from confounding factors, to map where reasoning provides reliable gains, and show how these dynamics interact with model size and task type. 3.1 MAIN RESULTS Figure 2 presents overall results on the impact of model scale, training data format, and distribution on downstream performance in simple mono-phasic setup, where student models are trained on single data distribution using single data format. Figure 2: Downstream performance of mono-phasic models. Results are shown for the teacher model and base students, as well as for models trained with IFTand reasoning-style data on both general and math-centric domains. 4 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Reasoning data boosts downstream performance in general distribution training, especially as model scale increases. Student models trained on general data distribution with reasoning globally achieve higher accuracies across benchmarks compared to those trained with IFT. Specifically, on General-OE, Math-OE, and Math-MC tasks, reasoning enables 3B students to match or closely approach the accuracy of 14B IFT models, demonstrating robust accuracy gains from reasoning. An exception occurs on General-MC tasks, where reasoning provides less consistent benefits, and IFT data remains competitive for models under 1.5B parameters, suggesting that smaller models struggle to exploit reasoning data on less reasoning-intensive tasks. Math-centric training helps large models on the most reasoning-intensive tasks. Similar to general-distribution training, the benefits of reasoning on math-centric data increase with model scale, though they exhibit distinct patterns across task categories. For non-math downstream tasks (General-MC and General-OE), reasoning data provides an advantage over the IFT format only for larger models (7B and above for General-MC, 14B for General-OE). In contrast, on mathematical tasks, the advantage of reasoning data over IFT emerges at lower scales (around 1.5B). Notably, math-specialized reasoning models achieve comparable performance to general-distribution training once model size exceeds 3B for math tasks, 7B for General-MC, and 14B for General-OE, despite using only quarter of the training samples (300K versus 1.3M). Overall, this suggests that while larger models gain the most from math reasoning traces, smaller models should continue to additionally rely on general-distribution training to maximize performance across tasks, even over domain-specific distributions. 3.2 IMPACT OF MIXING IFT AND REASONING DATA Motivated by the strong performance of reasoning models, we further investigate their effectiveness by varying the proportion of reasoning instances in the general training mix. Specifically, we examine potential synergies between IFT and reasoning under both the sequential and mixed approaches (Tseq and Tmix, respectively; see 2), and subsequently analyze scaling behaviors in sequential training relative to the reasoning ratio and model size. Figure 3: Comparison of sequential and mixed training scenarios across varying reasoning ratios. The accuracy gap relative to the IFT baseline (0% ratio) is shown with solid lines, while the average answer length (in tokens) is reported with dashes. Results are averaged over all student sizes. Mixed training exhibits moderate IFT-reasoning synergies. We motivate our analysis of mixed training by the hypothesis that models can acquire reasoning abilities while retaining the conciseness of IFT-style answers. Figure 3 confirms that, for math tasks, mixed training with 2550% reasoning ratio significantly outperforms pure IFT while keeping responses concise, indicating some IFTreasoning synergy. However, mixed training exhibits pronounced instability, as evidenced by higher variance in accuracy across reasoning ratios (most notably on General-OE). Additionally, models tend to transition abruptly into reasoning mode once reasoning instances exceed 50% of the training mix, suggesting that they adopt reasoning-style outputs whenever the majority of training data is reasoning-focused. In consequence, we focus on the sequential setting for the remainder of this study, leaving stabilization of mixed-style training and consistent exploitation of its potential benefits to future work. 5 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Figure 4: Impact of the reasoning ratio on downstream performance. Results show the accuracy gap relative to the IFT baseline (0% reasoning ratio) in the sequential training scenario, where models are first trained on IFTand then on reasoning-style data. Sequentially combining IFT and reasoning yields no accuracy gains. Consistent with prior work (Mistral-AI, 2025), Figure 4 shows that cold-start training with IFT data (ratios of 25%, 50%, and 75%) does not boost performance. The sole exception is the 0.5B model on General-MC tasks, where IFT-only achieves the highest accuracy. Open-ended tasks benefit the most of reasoning. Varying the reasoning ratio reveals two distinct patterns depending on the downstream task family (Figure 4). For multiple-choice tasks, accuracy plateaus as the reasoning ratio increases (25% for General-MC and 75% for Math-MC), indicating limited benefit from further reasoning-based training. In contrast, for open-ended tasks, especially Math-OE, accuracy continues to rise with higher reasoning ratios across all student sizes, suggesting headroom for extended reasoning training. 3.3 DOMAIN-SPECIFIC ADAPTATION Figure 5: Downstream performance of models trained sequentially on general and math-centric data. Results show the accuracy gap relative to mono-phasic general-domain IFT models (General-IFT in Figure 2). Mono-phasic reasoning models are included as baselines. 6 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. In this subsection, building on established training practices, we study bi-phasic strategies in which models are further trained on targeted domain starting from checkpoints pretrained on generaldistribution data (Bolton et al., 2024; Alves et al., 2024; Shao et al., 2024a; Yang et al., 2024b). IFT adaptation of reasoning model provides no benefit. Applying IFT alignment on model that has already performed general-reasoning training results in performance that is at best comparable to two-stage IFT, and often worse for smaller models (Figure 5). We observe no positive interaction between reasoning and subsequent IFT adaptation; in some cases, performance even declines relative to general-reasoning models, consistent with the findings reported in 3.2. Domain-specific alignment yields performance gains at larger model scales. Math-centric adaptation can yield significant performance gains, but only under specific conditions. Models with 1.5B parameters and above, particularly when initialized from general-distribution reasoning checkpoint fine-tuned on math-centric distribution, achieve the strongest results on mathematical tasks. Under the same setup, models beyond 3B parameters not only match the performance of exclusively math-specialized models but also maintain their non-specific reasoning capabilities (Shenfeld et al., 2025), demonstrating an ideal balance between improved in-domain results and robust general-purpose abilities. In contrast, models below 1.5B parameters exhibit signs of catastrophic forgetting (Kirkpatrick et al., 2017) under the same adaptation regime, with 0.5B student even experiencing global drop in performance, indicating insufficient capacity to solve challenging reasoning tasks."
        },
        {
            "title": "4 ACCURACY-EFFICIENCY TRADE-OFF ANALYSIS",
            "content": "Reasoning outputs are typically longer than IFT responses, making both training and inference more expensive. In this section, we move beyond raw accuracy to analyze the accuracyefficiency tradeoff. All results are reported for general-distribution training from base checkpoints. 4.1 TRAINING EFFICIENCY We first contextualize accuracy relative to training compute (Figure 6). In sequential distillation setup, we vary the proportion of reasoning instances to examine the trade-offs between performance and training cost in FLOPs. Accounting details are provided in Appendix C. Figure 6: Accuracy versus training FLOPs for models trained with IFT (0%), reasoning-style data (100%), and sequential reasoning ratios of 25%, 50%, and 75%. The Pareto frontier (black dashed lines) highlights efficient configurations, while those that lie in the red-shaded area are suboptimal. IFT is an efficient training strategy. Across all tasks, IFT models follow the Pareto frontier, indicating that scaling model size rather than incorporating reasoning-based training is reliable approach to achieve performance gains without substantially increasing training costs. Reasoning models reach training efficiency as scale increases. IFT models exhibit an earlier performance plateau compared to models trained with reasoning data, suggesting that additional 7 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. gains could be obtained by integrating reasoning into the training mix. In fact, reasoning models ( 25% reasoning ratio) achieve Pareto optimality at larger scales, with some variation across downstream tasks (e.g., 0.5B for General-OE and 7B for General-MC). Intermediate reasoning ratios achieve Pareto-optimal trade-offs. Models trained with 100% reasoning ratio never reach the Pareto frontier. While sufficiently large models may benefit from improved performance, this comes at the cost of significantly heavier training. In contrast, intermediate ratios (25%, 50%, or 75%) consistently lie on the Pareto frontier, offering controlled performance gains without incurring excessive training cost. This pattern suggests that practitioners should either scale model size or prefer moderate reasoning ratios to optimize the accuracy-efficiency trade-off. 4."
        },
        {
            "title": "INFERENCE EFFICIENCY",
            "content": "In this subsection, we adopt the perspective of user leveraging the models for generation purposes. Training is treated as an offline cost, and we evaluate accuracy with respect to inference FLOPs (Figure 7). Figure 7: Accuracy versus inference FLOPs for models trained with IFT (0% reasoning ratio) and reasoning-style (100% reasoning ratio) data. The purple-dotted and blue-dashed lines indicate the accuracy-FLOPs interpolated curves for IFT and reasoning, respectively (further details in Appendix E). The red-shaded region highlights configurations that are Pareto-suboptimal. IFT is always Pareto-optimal. Consistent with the observations in 4.1, IFT models lie on the Pareto frontier across tasks, indicating that increasing model size reliably yields Pareto-optimal gains in inference efficiency. Reasoning becomes Pareto-optimal at larger scales. Trends in the Pareto plots reveal that all reasoning models approach the Pareto frontier as model size increases, with patterns varying depending on the task, while IFT models tend to plateau earlier. This trend is particularly notable for models above 7B, suggesting the benefits of reasoning-based scaling beyond this size. Confirming this hypothesis would require experiments with models larger than 14B parameters, which we leave for future work for practical reasons. Open-ended tasks benefit more from reasoning than multiple-choice. Building on the findings in 3.2, which show that open-ended tasks gain the most in accuracy from reasoning, we further observe that they also incur smaller relative increases in inference cost compared to multiple-choice tasks. Specifically, switching from IFT to reasoning on open-ended tasks results in an approximate 7 increase in inference cost, whereas for General-MC tasks the increase is around 1015 (see further details in Appendix F, Figure 11). These results support the idea that certain tasks are inherently more reasoning-sensitive, as characterized in Figure 1. Longer generations tend to be incorrect. To gain further insights into inference efficiency, we analyze evaluation-time reasoning traces and find strong positive correlation between answer length and error rate (Figure 8). In Appendix (Figure 9), we test budgeted decoding abstention mechanism that halts generation once fixed token budget is reached. While this policy reduces inference FLOPs, it substantially decreases accuracy, shifting performance off the Pareto frontier. 8 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Figure 8: Answer length analysis across student sizes and correctness in reasoning models. Vertical bars indicate average answer lengths for each task category, while the black line shows the corresponding downstream accuracies."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Instruction tuning and reasoning. Instruction Fine-Tuning (IFT) has been the standard recipe for aligning LLMs with human instructions (Wei et al., 2022b; Ouyang et al., 2022; Chung et al., 2022). Chain-of-Thought (CoT) extended this paradigm by supervising intermediate reasoning steps, yielding strong gains on arithmetic, symbolic, and commonsense reasoning benchmarks (Rajani et al., 2019; Nye et al., 2021; Cobbe et al., 2021a; Wei et al., 2022a; Kojima et al., 2022). These findings sparked new wave of reasoning-centric models from both frontier labs and the open-source community. However, most reports highlight aggregate improvements without disentangling when and why reasoning helps, gap our work addresses. Reinforcement learning for reasoning. Recent frontier efforts extend beyond supervised traces, using Reinforcement Learning (RL) to refine reasoning strategies. Methods such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017), and GRPO (Shao et al., 2024b) optimize reasoning trajectories with outcome-based rewards, such as correctness of derivations or code executability (OpenAI, 2024; DeepSeek-AI, 2025; Mistral-AI, 2025). While effective, these methods are compute-heavy and opaque about the precise drivers of performance gains. By contrast, our fully supervised distillation setup isolates reasoning signals without RL, enabling clearer attribution. Knowledge distillation. Knowledge Distillation (KD) transfers capabilities from strong teachers to smaller students (Buciluundefined et al., 2006; Hinton et al., 2015b). Beyond representation-based KD, text-based distillation has become central for reasoning: large teacher models generate either IFTor reasoning-style traces that guide student learning (Kim & Rush, 2016; Zhou & Chiam, 2023; Hsieh et al., 2023; He et al., 2024). This approach reduces the cost of expensive RL while preserving the performance (DeepSeek-AI, 2025; Qwen-Team, 2025; Mistral-AI, 2025). Yet, prior studies largely focus on showcasing empirical gains rather than dissecting the taskand scale-dependent trade-offs. Our contribution is to turn this distillation pipeline into controlled testbed, stripping away confounders."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Through large-scale, distillation-based controlled study, we characterize scenarios when reasoning yields the greatest benefits, showing how its effectiveness depends on model scale, task type, and computational cost. While classical IFT models remain reliably Pareto-optimal baseline, reasoning consistently delivers substantial gains on open-ended and reasoning-intensive tasks above the 7B-parameter scale, enabling models to break past the performance plateaus of IFT. These results suggest that reasoning signals are not just redundant supervision but complementary resource that grows in value with scale, pointing toward hybrid approaches that harness reasoning capabilities alongside IFTs conciseness. 9 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Environmental and compute considerations. This work provides an in-depth analysis of scenarios where enabling reasoning capabilities in models is beneficial, as well as where it may not be. In an era where practitioners often prioritize accuracy above all else, we contextualize performance relative to both training and inference costs, offering guidance to avoid excessive computational overhead across different use cases. Responsible use of LLMs. In preparing this manuscript, we occasionally used suggestions from LLMs (GPT-5) to guide improvements in clarity, grammar, and overall readability. All scientific content, including experimental design, codebase, data analysis, results, and interpretations, is independently developed by the authors. LLMs are not involved in generating, modifying, or interpreting any experimental results, nor in producing code or analyses. Their use is strictly limited to selectively refining language to ensure clear and effective communication of our research."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have taken every effort to ensure the reproducibility of our experiments. All training and evaluation procedures are described in detail, including the base models, datasets, and all relevant training and generation hyperparameters. To further facilitate replication, we release all project artifacts, including trained models, data generation scripts, training scripts, and evaluation code. 10 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance."
        },
        {
            "title": "REFERENCES",
            "content": "Duarte M. Alves, Jose Pombal, Nuno M. Guerreiro, Pedro H. Martins, Joao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, Jose G. C. de Souza, and Andre F. T. Martins. Tower: An open multilingual large language model for translationrelated tasks, 2024. URL https://arxiv.org/abs/2402.17733. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning, 2025. URL https: //arxiv.org/abs/2504.03380. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. Nicolas Boizard, Kevin El Haddad, Celine Hudelot, and Pierre Colombo. Towards cross-tokenizer distillation: the universal logit distillation loss for llms. arXiv preprint arXiv:2402.12030, 2024. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte M. Alves, Andre Martins, Ayoub Hammal, Caio Corro, Celine Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, Gabriel Hautreux, Joao Alves, Kevin El-Haddad, Manuel Faysse, Maxime Peyrard, Nuno M. Guerreiro, Patrick Fernandes, Ricardo Rei, and Pierre Colombo. Eurobert: Scaling multilingual encoders for european languages, 2025. URL https://arxiv.org/abs/2503.05500. Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, and Christopher D. Manning. Biomedlm: 2.7b parameter language model trained on biomedical text, 2024. URL https://arxiv.org/abs/2403.18421. Cristian Buciluundefined, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 06, pp. 535541, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933395. doi: 10.1145/1150402.1150464. URL https://doi.org/ 10.1145/1150402.1150464. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, 11 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021a. URL https://arxiv. org/abs/2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera Lucia Raposo, Sofia Morgado, and Michael Desa. Saullm7b: pioneering large language model for law, 2024. URL https://arxiv.org/abs/ 2403.03883. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Aniket Didolkar, Nicolas Ballas, Sanjeev Arora, and Anirudh Goyal. Metacognitive reuse: Turning recurring llm reasoning into concise behaviors. arXiv preprint arXiv:2509.13237, 2025. Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Antonio Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, Joao Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, Francois Yvon, Andre F. T. Martins, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Croissantllm: truly bilingual french-english language model, 2025. URL https://arxiv.org/ abs/2402.00786. Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Celine Hudelot, and Pierre Colombo. Towards trustworthy reranking: simple yet effective abstention mechanism. arXiv preprint arXiv:2402.12997, 2024a. Hippolyte Gisserot-Boukhlef, Ricardo Rei, Emmanuel Malherbe, Celine Hudelot, Pierre Colombo, Is preference alignment always the best option to enhance llm-based and Nuno Guerreiro. translation? an empirical analysis. arXiv preprint arXiv:2409.20059, 2024b. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan Huang, Shaoqing Lu, Ding Liang, and Mingjie Zhan. Teacherlm: Teaching to fish rather than giving the fish, language modeling likewise, 2024. URL https://arxiv.org/abs/2310.19019. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural networks. arXiv preprint arXiv:1503.02531, 2015a. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015b. URL https://arxiv.org/abs/1503.02531. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 12 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023. URL https://arxiv.org/abs/2305.02301. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. Winogrande: An adIn Thirty-Fourth AAAI Conference on Artificial versarial winograd schema challenge at scale. Intelligence, 2020. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 13171327, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139. URL https://aclanthology. org/D16-1139/. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13): ISSN 1091-6490. doi: 10.1073/pnas.1611835114. URL http: 35213526, March 2017. //dx.doi.org/10.1073/pnas.1611835114. Takeshi Kojima et al. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. Jijie Li, Li Du, Hanyu Zhao, Bo wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin. Infinity instruct: Scaling instruction selection and synthesis to enhance language models, 2025. URL https://arxiv.org/abs/2506.11116. Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners, 2025. URL https://arxiv. org/abs/2502.12143. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL, 2017. Ryan Liu, Jiayi Geng, Addison Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas Griffiths. Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. arXiv preprint arXiv:2410.21333, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. Pedro Henrique Martins, Joao Alves, Patrick Fernandes, Nuno Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte Alves, Jose Pombal, Nicolas Boizard, et al. Eurollm9b: Technical report. arXiv preprint arXiv:2506.04079, 2025. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. Mistral-AI. Magistral, 2025. URL https://arxiv.org/abs/2506.10910. 13 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114. Art of Problem Solving. AoPS Wiki, 2025. URL https://artofproblemsolving.com/wiki/index.php/American_ Invitational_Mathematics_Examination. Accessed: 2025-09-02. American invitational mathematics examination. OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Qwen-Team. Qwen2.5: party of foundation models, September 2024. URL https:// qwenlm.github.io/blog/qwen2.5/. Qwen-Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning, 2019. URL https://arxiv.org/ abs/1906.02361. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Profor machine comprehension of text. ceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264. Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. doi: 10.1162/tacl 00266. URL https://aclanthology.org/Q19-1016. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. Learning to plan & reason for evaluation with thinking-llm-as-a-judge. arXiv preprint arXiv:2501.18099, 2025. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a. URL https://arxiv.org/abs/ 2402.03300. 14 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024b. URL https://arxiv.org/abs/ 2402.03300. Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler, 2024. URL https://arxiv.org/abs/2408. 13359. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 61056114. PMLR, 2019. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. Jason Wei et al. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022a. Jason Wei et al. Finetuned language models as zero-shot learners. arXiv preprint arXiv:2109.01652, 2022b. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https: //arxiv.org/abs/2311.07911. Tianxun Zhou and Keng-Hwee Chiam. Synthetic data generation method for data-free knowledge distillation in regression neural networks. Expert Systems with Applications, 227:120327, October 2023. ISSN 0957-4174. doi: 10.1016/j.eswa.2023.120327. URL http://dx.doi.org/10. 1016/j.eswa.2023.120327. Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. Distilling mathematical reasoning capabilities into small language models. Neural Networks, 179:106594, 2024. 15 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance."
        },
        {
            "title": "A DISCUSSION",
            "content": "Several avenues remain for extending our understanding of the conditions under which reasoning distillation is most effective. Future work could test the approach beyond math-centric and generalpurpose tasks, for instance on domain-specific reasoning challenges such as code (Rozi`ere et al., 2024), legal reasoning (Colombo et al., 2024) or additional languages (Faysse et al., 2025; Martins et al., 2025). Examining training dynamics such as convergence behavior (Hoffmann et al., 2022) with respect to dataset size, or assessing larger student models, may help explore potential gains from additional scaling. Other promising avenues include replicating our controlled setup in other scenarios such as reinforcement learning (Schulman et al., 2017; 2015; Shao et al., 2024b), teacher-student logits distillation (Hinton et al., 2015a; Boizard et al., 2024), or exploring alternative techniques beyond SFT, such as preference-based optimization (Rafailov et al., 2024; Xu et al., 2024; Gisserot-Boukhlef et al., 2024b) and data filtering (Boizard et al., 2025; Bae et al., 2025)."
        },
        {
            "title": "B TRAINING HYPERPARAMETERS",
            "content": "All training runs are performed for single epoch with global batch size of 262,144 tokens across 16 H100 GPUs. The learning rate follows Warmup-Stable-Decay (WSD) schedule (Shen et al., 2024) (150-step linear warmup, constant plateau, and 300-step linear decay to 10% of the peak value), using the AdamW fused optimizer (Loshchilov & Hutter, 2019). Peak learning rates are selected via grid search over {2105, 1105, 7106, 5106, 3106, 1106}. We list in Table 1 the peak learning rates used for student distillation across all models and both data formats (reasoning and IFT). Notably, reasoning-based distillation generally benefits from slightly higher learning rates than IFT. Model Reasoning IFT Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B 2e-5 1e-5 7e-6 5e-6 3e1e-5 7e-6 5e-6 3e-6 1e-6 Table 1: Peak learning rates selected for each student model and training data format."
        },
        {
            "title": "C FLOPS COMPUTATION",
            "content": "In this section, we present the methodology used to compute both training and inference FLOPs, following the approach proposed by Hoffmann et al. (2022). C.1 NOTATIONS We introduce the following notations for FLOPs computations: : vocabulary size dmodel : hidden dimension of the model dff : dimension of feed-forward layers : number of attention heads Nl : number of transformer layers : sequence length lp : prompt length lg : generation length Ns : number of training samples 16 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. C.2 TRAINING FLOPS The following formulas compute the FLOPs for model training, assuming batch size of 1. It is reasonable to assume that the FLOPs are largely independent of the batch size. FLOPsforward = 2 dmodel (cid:125) (cid:124) + (cid:0)6 d2 (cid:124) (cid:123)(cid:122) embeddings + 4 dmodel dff Nl (cid:125) (cid:123)(cid:122) feed-forward (cid:124) + 2 dmodel (cid:125) (cid:123)(cid:122) output logits (cid:124) model + 2 l2 dmodel + 3 l2 + 2 l2 dmodel + 2 model (cid:123)(cid:122) attention FLOPstraining step = 3 FLOPsforward FLOPstraining = Ns(cid:88) i= FLOPstraining step(i) (cid:1) Nl (cid:125) (1) (2) (3) C. INFERENCE FLOPS FLOPsinference and The following formulas compute the FLOPs for model FLOPsinference with cache correspond to single-token generation. FLOPsinference with cache assumes that past token keys and values are stored in memory and do not need to be recomputed. inference. FLOPsinference = 2 lp dmodel (cid:125) (cid:124) (cid:123)(cid:122) embeddings + (cid:0)6 lp d2 (cid:124) model + 2 l2 dmodel + 3 l2 dmodel + 2 lp d2 model + 2 l2 (cid:123)(cid:122) attention (cid:1) Nl (cid:125) + 4 lp dmodel dff Nl (cid:125) (cid:123)(cid:122) feed-forward (cid:124) + 2 dmodel (cid:124) (cid:125) (cid:123)(cid:122) output logits FLOPsinference with cache = 2 dmodel (cid:124) (cid:125) (cid:123)(cid:122) embedding + (cid:0)6 d2 (cid:124) model + 2 lp dmodel + 3 lp + 2 lp dmodel + 2 model (cid:123)(cid:122) attention (4) (cid:1) Nl (cid:125) (5) + 4 dmodel dff Nl (cid:125) (cid:123)(cid:122) feed-forward (cid:124) + 2 dmodel (cid:125) (cid:124) (cid:123)(cid:122) logits FLOPsmulti-token inference = FLOPsinference(lp) + lg (cid:88) i=1 FLOPsinference with cache(lp + i) (6) 17 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance."
        },
        {
            "title": "D PROMPTING DETAILS",
            "content": "D.1 DATA GENERATION D.1.1 INFINITY-INSTRUCT Question Answer lottery games Lotto 6/49 is one of three in national Canada. Launched on June 12, 1982, Lotto 6/49 was the first nationwide Canadian lottery game to allow players to choose their own numbers. Previous games, national such as the Olympic Lottery, Loto Canada and Superloto used pre-printed numbers on tickets. Lotto 6/49 led to the gradual phase-out of that type of lottery game in Canada. paragraph the Based above can we conclude that Canadas Lottery 6/49 was very popular in the 70s? on the answer to know that the question What are isolators found in microwave antennas made of? is in necessary condition for the aforementioned reciprocity property is that the materials in the antenna and transmission medium are linear and reciprocal. Most materials used in antennas meet these conditions, but some microwave antennas use high-tech components such as isolators and circulators, made of nonreciprocal materials such as ferrite. These can be used to give the antenna different behavior on receiving than it has on transmitting, which can be useful in applications like radar. Can you tell me what it is? IFT: No, we cannot conclude that Canadas Lottery 6/49 was very popular in the 70s based on the paragraph. The text states that Lotto 6/49 was launched on June 12, 1982, which is after the 1970s, so the statement is incorrect. Reasoning: <think> Okay, lets see. The user is asking if we can conclude that Canadas Lotto 6/49 was very popular in the 70s based on the given paragraph. First, need to check the dates mentioned. . . . The answer should state that the conclusion is incorrect because the game started in 1982, which is after the 70s.</think> No, we cannot conclude that Canadas Lottery 6/49 was very popular in the 70s based on the paragraph. The text explicitly states that Lotto 6/49 was launched on June 12, 1982, which is in the early 1980s, not the 1970s. The paragraph also notes that earlier national lottery games (e.g., Olympic Lottery, Loto Canada, Superloto) used pre-printed numbers and were phased out after Lotto 6/49s introduction. There is no information provided about the popularity of any lottery games in the 1970s, nor was Lotto 6/49 even in existence during that decade. The conclusion is therefore unsupported. IFT: The isolators found in microwave antennas are made of nonreciprocal materials such as ferrite. Reasoning: <think> Okay, lets see. The user is asking about what isolators found in microwave antennas are made of. They provided passage that should contain the answer. First, need to read through the passage carefully. The key part here is where it mentions isolators and circulators. The sentence says, made of nonreciprocal materials such as ferrite. So the answer should be ferrite. Wait, but let me make sure. The passage starts by talking about reciprocity in antennas, but then introduces isolators and circulators as exceptions. It says these components are made of nonreciprocal materials. The example given is ferrite. So yes, the answer is definitely ferrite. should check if there are any other materials mentioned, but the text only lists ferrite as an example. The user might be looking for that specific term. No other materials are named here. So the answer should be ferrite.</think> Answer: The isolators found in microwave antennas are made of ferrite, nonreciprocal material. Table 2: Examples of generated data from the Infinity-Instruct dataset, showing pairs of IFT and reasoning answers for common query. We report shortened reasoning traces with . . . due to their length. 18 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. D.1.2 LLAMA-NEMOTRON-POST-TRAINING-DATASET Question Answer Solve the following math problem. Make sure to put the answer (and only the answer) inside boxed{}. Evaluate the limit (cid:18) 10 + 1 lim (cid:19)3n+1 Solve the following math problem. Make sure to put the answer (and only the answer) inside boxed{}. Find the number of integer solutions to the equation xyz = 2003. IFT: We are asked to evaluate the limit: limn (cid:17) 10 (cid:17) = 1 1: Simplify the base expression n10 1 (cid:16) 1+ (cid:16) 1 1+ 10 1 . n+1 = . . . (cid:16) n10 n+1 (cid:17)3n+1 . Step (3 + 1 boxed{eˆ{-33}} ) = 33. Now exponentiate: = e33. Final Answer: Total characters: 1,417 (cid:1)3n+1 Reasoning: <think> Okay, so need to evaluate the limit as of (cid:0) n10 . Hmm, lets see. remember that limits involving expressions raised to variables can sometimes be solved using the exponential function and n+1 . . . (cid:16) (cid:17) 3n+1 Thus: ln limn(3n + 1) n+1 = 11 3 = 33. Exponentiating both sides: = e33. Final Answer: boxed{eˆ{-33}} = 11 limn 11 n+1 Total characters: 11,225 IFT: We are asked to find the number of integer solutions to the equation xyz = 2003. Step 1: Prime Factorization. First, factor 2003: 2003 17 = 117.823 (not divisible by 17). Try dividing by small primes up to Eventually, we find: boxed{12}. . . . 2003 44.76. Total characters: 2,806 Reasoning: <think> Okay, so need to find the number of integer solutions to the equation xyz = 2003. Hmm, let me start by understanding what exactly the problem is asking for. We need all ordered triples of integers (x, y, z) such that when you multiply them together, you get 2003. Thus, only the above 12 combinations are valid. Final Answer: boxed{12}. . . . Total characters: 26,747 Table 3: Examples of generated data from the Llama-Nemotron-Post-Training-Dataset, showing pairs of IFT and reasoning answers for common query. We report shortened answers with . . . and their respective character counts. 19 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. D.2 EVALUATION PROMPTS Benchmark gsm8k math 500 aime mmlu math mmlu pro math aqua rat winogrande openbookqa squad mmlu misc coqa ifeval Instruction Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Solve the following math problem. Make sure to put the answer (and only answer) inside boxed{}. Given sentence with blank ( ) and two possible options, choose the option that correctly fills the blank so that the sentence makes the most logical sense. Make sure to put the answer (and only answer) inside boxed{}. Select the option that best completes the scenario based on everyday reasoning about cause and effect. Make sure to put the answer (and only answer) inside boxed{}. Read the passage and answer the question by selecting the text span from the passage that best answers it. Make sure to put the answer (and only answer) inside boxed{}. Answer the following multiple-choice question by selecting the option that best fits the correct knowledge. Make sure to put the answer (and only answer) inside boxed{}. Read the passage and answer the question by selecting the text span from the passage that best answers it. Make sure to put the answer (and only answer) inside boxed{}. Answer the following instruction. Table 4: Instruction prompts used for answer generation across evaluation benchmarks. 20 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. D."
        },
        {
            "title": "JUDGING PROMPTS",
            "content": "Benchmark Default Instruction You will be given Question, User Answer (only its ending is shown due to length), and Ground Truth. Your task is not to answer the question, but to say if the user answer is equivalent in meaning to the ground truth. First, extract the final result from both the User Answer and the Ground Truth Answer, based on the Question. Then, compare the two final results and determine whether they convey the same meaning. If they are equivalent, respond with boxed{yes}. If they are not equivalent, or if the User Answer does not contain valid answer, respond with boxed{no}. Question: {question} User Answer: {answer} ifeval Ground Truth: {truth} You will be given an Instruction and User Answer (only its ending is shown due to length). Your task is not to answer the Instruction, but to determine whether the User Answer follows all the formal requirements stated in the Instruction. If the User Answer contains thinking process, you should ignore it and only focus on the final answer. identify every explicit requirement in the Instruction (e.g., no comFirst, mas, maximum word count, required word occurrences, formatting rules). Then, compare the User Answer against these requirements. If all requirements are satisfied, respond with boxed{yes}. If any requirement is violated, respond with boxed{no}. Question: {question} User Answer: {answer} Table 5: Instruction prompts used for LLM-based answer assessment. Default instructions are applied across all benchmarks, except for ifeval. 21 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance."
        },
        {
            "title": "E DETAILS ON PARETO INTERPOLATION",
            "content": "In 4.2, we show Pareto plot of accuracy versus inference cost for IFT and reasoning models. To predict the impact of further model scaling on downstream accuracy, we fit saturating growth interpolation function to the observed data points (Tan & Le, 2019; Kaplan et al., 2020). The objective function is defined as: (x) = α + β(1 exp(γxδ)), where denotes the number of FLOPs and (x) gives the interpolated accuracy. The parameters are subject to the constraints α, β > 0, α + β not exceeding the teachers accuracy, γ > 0, and 0 < δ 1. Intuitively, (0) = α corresponds to the minimum achievable performance on the benchmark (a random model with 0 FLOPs), while limx (x) = α + β represents the maximum performance. The parameters γ and δ control the curvature of the interpolated curve. The function is fitted by minimizing the mean absolute error."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "F.1 GENERATION EARLY-STOPPING Figure 9: Inference-cost impact of generation early stopping for IFT and reasoning models. Each model is evaluated at five maximum-length thresholds, corresponding to the 0th, 25th, 50th, 75th, and 100th answer length percentiles. The Pareto frontier is indicated by black dashed lines. In Figure 9, we leverage the observation that incorrect answers are typically longer to design simple early-stopping strategy, stopping generation once specified answer length threshold is reached. For each model, we evaluate five thresholds corresponding to the 0th, 25th, 50th, 75th, and 100th answer length percentiles. We find that this straightforward strategy does not shift the Pareto frontier, as the reduction in inference cost comes at the expense of notable drop in accuracy. Nevertheless, investigating more advanced approaches, such as calibration-based abstention methods (Gisserot-Boukhlef et al., 2024a) or behavior-conditioned inference (Didolkar et al., 2025), to reduce unnecessary generation costs represents promising direction for future research. F.2 INCREASING MAXIMUM GENERATION LENGTH Figure 10: Impact of increasing maximum generation length (from 16,384 to 32,768 tokens) on downstream performance across mmlu-math, math-500, and aime. Interestingly, Figure 10 shows that certain mathematical tasks benefit from increased generation length in the reasoning setting. In this experiment, models are allowed to generate up to 32,768 22 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. tokens, compared to the 16,384-token length used during training. This provides insight into why simple early-stopping strategies may fail, as some tasks require more tokens to produce correct answers. It also demonstrates that reasoning models can extrapolate well beyond the lengths on which they are trained, behavior that could be further explored in future work. F."
        },
        {
            "title": "INFERENCE COST SCALING TRENDS",
            "content": "Figure 11: Inference FLOPs versus student model size for IFT and reasoning-style training. Points indicate the average inference FLOPs for each task category, while the curves show the corresponding log-linear scaling trends. In Figure 11, we fit log-linear curves to inference FLOPs as function of model size across task categories, assuming power-law relationships of the form = αxβ. The corresponding scaling coefficients are reported in each subplot. For General-OE, Math-MC, and Math-OE, the exponents β are closely aligned (βIFT βRea + 0.10), slightly favoring βRea. This is consistent with Figure 8, where reasoning answers shorten slightly faster than IFT answers as model size increases. In contrast, for General-MC tasks, reasoning models display larger scaling coefficients than IFT models, indicating that the higher computational cost, combined with only marginal performance gains, limits the improvement observed on these tasks. TASK-LEVEL RESULTS Figure 12, Figure 13, Figure 14, Figure 15,Figure 16, Figure 17 and Figure 18 present the task-level versions of the aggregated results shown in Figure 2, Figure 3, Figure 4, Figure 5, Figure 6, Figure 7 and Figure 8, respectively. Figure 12: Task-level downstream performance of mono-phasic models. When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Figure 13: Task-level comparison of sequential and mixed training scenarios across varying reasoning ratios. Figure 14: Task-level impact of the reasoning ratio on downstream performance. Figure 15: Task-level downstream performance of math-adapted models. 24 When Does Reasoning Matter? Controlled Study of Reasonings Contribution to Model Performance. Figure 16: Task-level accuracy versus training FLOPs for models trained with IFT (0%), reasoningstyle data (100%), and sequential reasoning ratios of 25%, 50%, and 75%. Figure 17: Task-level accuracy versus inference FLOPs for models trained with IFT and reasoningstyle data. Figure 18: Task-level answer length analysis across student sizes and correctness in reasoning models."
        }
    ],
    "affiliations": [
        "Artefact Research Center",
        "Diabolocom",
        "Equall",
        "ISIA Lab, University of Mons",
        "MICS, CentraleSupelec, Universite Paris-Saclay"
    ]
}