{
    "paper_title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "authors": [
        "Kaican Li",
        "Lewei Yao",
        "Jiannan Wu",
        "Tiezheng Yu",
        "Jierun Chen",
        "Haoli Bai",
        "Lu Hou",
        "Lanqing Hong",
        "Wei Zhang",
        "Nevin L. Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 5 4 7 8 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "INSIGHT-O3: EMPOWERING MULTIMODAL FOUNDATION MODELS WITH GENERALIZED VISUAL SEARCH Kaican Li1, Lewei Yao2, Jiannan Wu2, Tiezheng Yu2, Jierun Chen2, Haoli Bai2, Lu Hou2, Lanqing Hong2, Wei Zhang2, Nevin L. Zhang1 1Hong Kong University of Science and Technology, 2Huawei"
        },
        {
            "title": "ABSTRACT",
            "content": "The ability for AI agents to think with images requires sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-BENCH, new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-BENCH features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-BENCH. To make progress, we propose INSIGHT-O3, multi-agent framework consisting of visual reasoning agent (vReasoner) and visual search agent (vSearcher) for which we introduce the task of generalized visual search locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present multimodal LLM purpose-trained for this task via reinforcement learning. As plugand-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on wide range of benchmarks. This marks concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3."
        },
        {
            "title": "INTRODUCTION",
            "content": "Thinking with images is an important and very useful skill for multimodal agents (OpenAI, 2025c). The skill rests on two crucial and fundamental cognitive abilities: reasoning and perception. Recent efforts at developing such skill based on open models mainly focus on the perception component, e.g., searching for particular object or figure in natural images and then answering simple visual query about them (Wu & Xie, 2024; Shen et al., 2024; Li et al., 2025; Zhang et al., 2025b; Su et al., 2025a; Zheng et al., 2025; Wang et al., 2025c; Zhu et al., 2025b; Wang et al., 2025a; Lai et al., 2025). While this feature is useful, it is still far from being able to handle many real-world tasks that require deeper and more abstract reasoning. Typical examples include extracting information from complex reports and navigating through intricate maps. Solving these tasks often require both organized reasoning and focused attention to visual details scattered across an image (or images). Currently, the reasoning capability of open multimodal models is still relatively weak in comparison with frontier proprietary models (Yue et al., 2024a; Yuan et al., 2025; Hao et al., 2025). This makes it very difficult to replicate the kind of reasoning-driven image-thinking behavior demonstrated by OpenAI o3 (OpenAI, 2025c). In this work, we take concrete step towards building such an intelligent system with open models. First, we propose new multimodal benchmark, O3-BENCH, to help better evaluate the general capability of multimodal models to think with images. Complementary to most of the existing benchmarks which only deal with object attributes and spatial relations in natural images (Wu & Xie, 2024; Wang et al., 2025g; Lai et al., 2025; Wang et al., 2025a), O3-BENCH consists of set of high-quality, reasoning-oriented questions on images of high information density. The questions involve real-world tasks such as map navigation and cross-chart/diagram analysis that are highly challenging even for frontier systems like OpenAI o3. Compared with benchmarks like Equal contribution. Corresponding author (lzhang@cse.ust.hk)."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: multi-step visual reasoning example of INSIGHT-O3 on O3-BENCH. For clarity, the internal reasoning processes are omitted. More examples can be found in Appendix D.2. MME-RealWorld (Zhang et al., 2024b), O3-BENCH is significantly harder, requiring the evaluated system to collect detailed visual information from multiple distinct image areas while performing complex, interleaved reasoning using the information collected in the process. To make substantive progress on O3-BENCH, we introduce multi-agent framework, INSIGHT-O3, that comprises visual reasoning agent (vReasoner) and visual search agent (vSearcher). The former is responsible for high-level reasoning and general image understanding, while the latter is to help vReasoner locate specific regions of interest and collect the visual information therein. As such, INSIGHT-O3 reduces the burden of single agent, allowing us to build an o3-like system via divide-and-conquer. This kind of specialization has been shown to work in prior art (Dayan & Hinton, 1992; Zeng et al., 2023; Shen et al., 2023; Li et al., 2023a; Hong et al., 2023; Castrejon et al., 2024). In this work, we focus on vSearcher and how it should interact with given vReasoner. Different from current practices (Wu & Xie, 2024; Lai et al., 2025) for natural images and discrete object references, we aim to solve generalized visual search, where the input image can be arbitrary, e.g., map, poster, or screenshot; and the referring description may specify relational, fuzzy, or conceptual region, e.g., the area to the left of the wooden chair, and the chart showing the companys revenue in the last decade, rather than specific object or figure. Such fuzzy descriptions are more in line with how humans reason and direct their attention to general region of interest. To address this broader challenge, we present InSight-o3-vS, vSearcher model specialized in generalized visual search through reinforcement learning. InSight-o3-vS combines multimodal understanding with spatial reasoning to localize regions described in completely free-form language. The"
        },
        {
            "title": "Preprint",
            "content": "name of our model, InSight-o3-vS, reflects its dual role: providing deeper insight into multimodal semantics while bringing the target region in sight through precise localization. Our model empowers existing multimodal foundation models (as vReasoners) in plug-and-play fashion, significantly improving the performance of frontier models across wide range of benchmarks, e.g., from 39.0% to 61.5% on O3-BENCH for GPT-5-mini (OpenAI, 2025a), and from 80.1% to 87.6% on V-Bench for Gemini-2.5-Flash (Comanici et al., 2025). To summarize, we make the following key contributions in this work: We propose new benchmark, O3-BENCH, to evaluate complex, reasoning-oriented visual tasks. This benchmark features challenges like map navigation and cross-chart analysis, which require collecting information from multiple image areas and performing interleaved reasoning, making it significantly harder than existing benchmarks. We introduce INSIGHT-O3, multi-agent framework that divides the task of thinking with images between high-level reasoning agent (vReasoner) and visual search agent (vSearcher). This divide-and-conquer design greatly simplifies the complex interleaved reasoning, allowing us to build o3-like systems that surpass OpenAI o3 across variety of benchmarks. We present InSight-o3-vS, specialized vSearcher model that excels at generalized visual search. It is designed to be plug-and-play component that empowers existing multimodal foundation models, demonstrably and significantly improving the performance of frontier systems on wide range of benchmarks including O3-BENCH."
        },
        {
            "title": "2 RELATED WORK",
            "content": "We provide brief overview of the most relevant related work in this section. For more comprehensive discussion, please refer to Appendix A. Multimodal benchmarks. Classical multimodal benchmarks (Goyal et al., 2017; Saikh et al., 2022; Liu et al., 2023; Ge et al., 2024) mainly test coarse image-level or salient-attribute recognition, where modern MLLMs are near-saturated (Bai et al., 2025b; Wang et al., 2025e). Recent multimodal reasoning benchmark split into (i) cognition-centric STEM benchmarks (Lu et al., 2023; Yue et al., 2024a;b) that emphasize multi-step/world-knowledge reasoning but use visually simple images, and (ii) perception-centric datasets (Wu & Xie, 2024; Zhang et al., 2024b; Lai et al., 2025) that require fine-grained recognition in high-resolution, text-rich scenes yet often limited to singleregion lookups. Motivated by the think with images paradigm (OpenAI, 2025c), O3-BENCH jointly evaluates search/localization and higher-level reasoning on high-information-density charts and maps, requiring cross-region evidence aggregation via interleaved, multi-hop reasoning. Multimodal reasoning models. Reinforcement learning (RL) has long been used to align model behavior with human preferences (Schulman et al., 2017). DeepSeek-R1 applies group relative policy optimization (GRPO) (Guo et al., 2025a; Shao et al., 2024b), reliably eliciting planning, reflection, and long chain-of-thought reasoning under simple rewards. Building on this idea, recent multimodal models (Yang et al., 2025c) adopt GRPO-style training and report strong gains, while cascaded RL stages (e.g., InternVL3.5 (Wang et al., 2025e), Keye-VL1.5 (Yang et al., 2025a)) further push reasoning, approaching proprietary models. There are also pioneering work utilizing Python code execution to call various vision tools to solve tasks via divide-and-conquer or to help with reasoning (Gupta & Kembhavi, 2023; Surıs et al., 2023; Ke et al., 2024). Nevertheless, most multimodal reasoners remain text-centric, overlooking the distinctive demands of visual reasoning. Visual search models. Visual search is core multimodal capability, requiring active region perception for fine-grained understanding. Early methods relied on external detectors or scripted workflows to localize regions and triggered tools via instruction tuning, leading to rigid outputs and typically single-round search (Wu & Xie, 2024). The think with images paradigm (OpenAI, 2025c) internalizes zoom/crop operations and has inspired end-to-end search (DeepEyes (Zheng et al., 2025)), synthetic warm-starts (Pixel-Reasoner (Su et al., 2025a)), and multi-turn RL (Minio3 (Lai et al., 2025)). Nonetheless, most systems still emphasize finding single region in natural images, with limited support for multi-hop reasoning. We broaden this scope by decoupling visual search from visual reasoning and enabling multi-region search on arbitrary images."
        },
        {
            "title": "3 O3-BENCH",
            "content": "We conceptualize thinking with images as an iterative perception-reasoning process. Perception focuses on searching and localizing task-relevant visual details, while reasoning needs to organize these cues into structured facts and performs higher-order inference (e.g., planning, arithmetic, use of world knowledge) to complete the task. These two critical skills should be executed effectively and cooperate tightly to achieve strong performance. Existing benchmarks (Wu & Xie, 2024; Zhang et al., 2024b; Lai et al., 2025) primarily emphasize perception, where their questions hardly require multi-step reasoning and thus induce short reasoning chains. To bridge this gap, we introduce O3-BENCH, benchmark that jointly assesses high-resolution perception and multi-hop visual reasoning. O3-BENCH is designed with two principles: High resolution & high information density. Images are large, high-resolution, cluttered, and information-dense, making evidence gathering genuinely non-trivial. Multi-hop solution paths. Questions require decomposing the goal, retrieving evidence from multiple regions, and composing it via intermediate steps before answering. To instantiate these principles, O3-BENCH comprises two complementary domains: (1) Composite charts. Each image contains multiple heterogeneous charts (e.g., bar/line/pie/tables). Our crafted questions demand cross-chart retrieval (series, axes, units), lightweight calculations (differences, ratios, aggregates), and consistency checks (scale, legend, time ranges) to derive the final answer. (2) High-resolution digital maps. The images typically include map along with auxiliary components such as legends and building indices. We meticulously design questions that require visual search for targets (e.g., matching symbols, categories, or toponyms) and spatial reasoning about relations and routes (e.g., proximity constraints or shortest paths), conditioned on the provided context. Overall, O3-BENCH comprises 204 images (117 charts, 87 maps) and 345 QA samples (163 for charts, 182 for maps) in total. The majority of samples fall into the more challenging map category, underscoring our prioritization of complex visual perception and multi-hop reasoning. The questions of O3-BENCH are multi-choice questions with six choices and one correct answer. Among the six choices, there are four distractors that appear in the image or look similar to the correct one. We also include an option as No Right Choice if there are no correct options provided. Below, we present the construction process of O3-BENCH. For other details about O3-BENCH, see Appendix B.1. 3.1 SOURCE DATA COLLECTION Chart. The chart images in O3-BENCH are curated from the Diagram and Table subset from MME-RealWorld (Zhang et al., 2024b) and the Internet. To ensure high information density, we run layout detection model, PP-DocLayout plus-L (Cui et al., 2025), on the candidate images and only keep those with at least 8 detected layouts. As result, 256 of 2,539 images (from MME-RealWorld) that contain sufficient number of sub-figures and rich recognizable texts are left. Map. We manually collect high-resolution digital maps from the Internet via keyword search. We center on the venue-level maps that require reading the provided legend/index and visually locating entities within the image to answer the question. We exclude all the country-, state-, or city-scale cartography that could be potentially answered with world knowledge. Through this process, we end up with 87 high-density map images spanning the categories over bus routes, campus, park, etc. 3.2 ANNOTATION PIPELINE After the collection and initial filtering process, all images then undergo further manual screening to ensure clarity and completeness of key visual cues (e.g., axes, units and legends). Next, we combine automated machine pre-annotation with human verification and authoring to generate the question-answer (QA) instances. The detailed process of data annotation is presented as follows. Machine pre-annotation. To relieve the burden of human annotators and increase the data diversity, we first apply three-step automated data pipeline to generate five questions for each image. (1) Layout detection. We divide the high-resolution images into several structured layouts (e.g., tables, charts, legends) using PP-DocLayout plus-L (Cui et al., 2025). For map images, we review"
        },
        {
            "title": "Preprint",
            "content": "the predictions, correct erroneous regions, and supplement missing areas via manual annotation. (2) Information extraction. For each layout, we prompt Qwen2.5-VL-32B (Bai et al., 2025b) to produce detailed caption for the layout and extract OCR text from it. In addition, we obtain global context by generating caption and extracting the OCR text for the full image. (3) Automated question synthesis. For each image, we provide the layout set (with captions and OCR texts) and the global context to GPT-5 (OpenAI, 2025a) to generate five questions (with answers and explanations) that compose evidence from the provided layout regions. Note that we do not provide the full image to GPT-5, compelling it to focus on region-level details and encourages multi-hop composition. More details about the whole pre-annotation process can be found in Appendix B.2. (1) Filtering and validation. Annotators start by discarding ill-posed or lowHuman annotation. quality machine-generated QAs (e.g., those with factual inconsistencies, ambiguous prompts, or spurious multi-hop reasoning). For the retained QAs, annotators verify that the six-option set contains exactly one unambiguous correct answer and confirm that the explanation faithfully, step by step, justifies the choice. The annotators also ensures that the target layouts are relevant to answering the question; these layouts are either derived from the explanation or added via manual annotation. (2) Human-authored questions. For information-dense images, machine-generated QAs often contain logical errors, wrong answers, or missed visual details. These QAs are reworked or completely rewritten by the annotators, adhering to our design principles: requiring fine-grained detail retrieval and multi-hop reasoning. Each QA includes detailed explanation to aid verification and have exactly one unambiguous correct choice among the six. Difficulty filtering and secondary review. We evaluate all candidate items with three strong proprietary MLLMs, i.e., GPT-5-mini (OpenAI, 2025a), Gemini-2.5-Flash (Comanici et al., 2025) and Doubao-Seed-1.6 (Bytedance, 2025), using the same evaluation prompt. We discard any items solved by all three models to ensure difficulty. Subsequently, independent reviewers (distinct from the original annotators) conducts cross-verification: final pass over the QAs and the explanations to confirm factual correctness, clarity, and formatting consistency. Finally, we confirm with experiments that attention to visual details is vital to good performance on O3-BENCH (see Appendix B.3). 4 INSIGHT-O3 In the previous section, we introduced O3-BENCH, meticulously-crafted benchmark that require problem-solving systems to have both good reasoning and perception capabilities, as well as the ability to integrate them in natural, synergetic manner. Recent approaches towards such systems mostly build upon single MLLM agent which handles both reasoning and perception workloads within single context window (Su et al., 2025a; Zheng et al., 2025; Lai et al., 2025). While this is reasonable for tasks primarily focusing on either reasoning or perception, the agent may struggle when the workloads are heavy and intertwined. To address the issue, we propose INSIGHT-O3, two-agent system that largely decouples the aforementioned burden by visual reasoning agent (vReasoner) and visual search agent (vSearcher). The former specializes in high-level, abstract reasoning (with some general image understanding), while the latter is mainly responsible for locating detailed visual information and presenting them to vReasoner. For instance, given question, vReasoner first decomposes the question via reasoning, and, if needed, issues relevant image region descriptions to vSearcher; vSearcher then localizes the requested evidence (with help from tools like image cropping) and returns it for subsequent rounds until final answer is produced. This process is illustrated in Figure 2(a). However, jointly training both agents in system like INSIGHT-O3 is notoriously difficulttheir objectives differ yet are highly interdependent, causing difficulties such as credit assignment across calls and non-stationary updates when both policies learn. Additionally, in our case, even the frontier open MLLMs, e.g., Qwen2.5-VL (Bai et al., 2025b), tend to produce overly concise replies (Lai et al., 2025). To avoid overcomplication, we consider simpler, more manageable setting in this paper. Specifically, we delegate higher-order reasoning at training time to strong external model (e.g., GPT-5mini as vReasoner) and focus on training vSearcher to cooperate with the given vReasoner effectively. This separation helps simplify optimization and improve data efficiency. Our experiment shows that well-trained vSearcher can significantly improve the performance of wide range of vReasoners as plug-and-play callable agent. Furthermore, the resulting system may help syn-"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Training pipeline. We use hybrid RL algorithm to train vSearcher. (a) In the in-loop component, vReasoner generates visual search tasks on-the-fly during training as it tries to answer user query. We use vReasoners feedback and final answer correctness as supervision (denoted by dashed arrows) for vSearcher. (b) In the out-of-loop component, we use pre-generated descriptions with ground-truth bounding boxes, allowing us to train vSearcher efficiently via IoU supervision. thesize multi-turn supervised-finetuning (SFT) traces with interleaved reasoning and visual search, paving the way for larger, potentially unified model. While prior works (Su et al., 2025a; Jiang et al., 2025a) have explored role-playing to construct similar SFT traces, the data quality is often not very high as the role-playing agents are only loosely coordinated. In this respect, our approach helps strength this coordination with reinforcement learning (RL), as detailed next. 4.1 TRAINING ALGORITHM We propose hybrid sub-agent RL algorithm that consists of an in-loop component and an out-ofloop component to train vSearcher (see Figure 2). In the out-of-loop component, we pre-generate region descriptions with predefined bounding boxes, allowing us to train vSearcher very efficiently via direct IoU supervision. In the in-loop component, we use real descriptions generated on-thefly during training by vReasoner. Compared with pre-generated tasks, these dynamically generated tasks are more aligned in nature with the tasks that vSearcher will see during inference time. = I[ntool > 0] Reward design. For the out-of-loop RL, we use the following reward function for vSearcher: (λformat rIoU), where ntool is the number of tool calls made by vSearcher, rformat { is the format reward, (0, 1) is the IoU reward (λformat and λIoU are weighting coefficients for the rewards). In and rIoU Eq.(1), the IoU reward rIoU encourages vSearcher to propose an accurate region that matches the description. In addition, we encourage vSearcher to use image cropping1 at least once to verify that the returned region matches the given region description. For every predicted box and the groundtruth box b, we define the IoU reward as rIoU = max (0, 1) controls the reward threshold. Boxes with an IoU less than α are not rewarded. rformat + λIoU 0, IoU(b, b) α) where α 0, 1 /( (1) α { } } 0, 1 } For the in-loop RL component, we replace the IoU reward rIoU in Eq.(1) with pseudo IoU reward . We obtain ˆrIoU by asking vReasoner to rate each vSearchers prediction, with the ˆrIoU { rating criterion being whether the prediction is relevant to the assigned task and can help vReasoner answer the user query. The rating is binary score indicating if the prediction is helpful. However, as this rating is not always reliable (vReasoner may sometimes make mistakes), we further 0, 1 { } 1For simplicity, we only allow vSearcher to use the most essential image cropping tool. Our framework, however, does not impose such constraint. It is easy to incorporate other kinds of tools for vSearcher to use."
        },
        {
            "title": "Preprint",
            "content": "incorporate outcome supervision as safeguard. Let of vReasoner is correct. The pseudo IoU reward is defined as ˆrIoU = I[s = = 1]. { } 0, 1 stand for whether the final answer Advantage estimation. We follow GRPO (Shao et al., 2024b) to estimate the advantages for the out-of-loop RL component. As for the in-loop component, we normalize the rewards with respect to the global mean and standard deviation instead of the group mean and standard deviation since the concept of group no longer exists for the dynamically generated tasks. Formally, the advantage of an output token ot at time step is computed as ˆAt = [r mean(r)]/std(r), where for the out-of-loop component, = i=1 with being the group size; and for the in-loop component, ri } = i=1 with being the total number of visual search tasks generated on-the-fly by vReasoner. } ri { { Objective function. The objective function we use is based on GRPO (Shao et al., 2024b), with some modifications (e.g., global advantage estimations) to incorporate the in-loop RL component. Given policy model πθ, the old policy πθold, and reference policy πref, the objective function for batch of vSearcher outputs (including both in-loop and out-of-loop ones) is defined as J(θ) = (cid:88)"
        },
        {
            "title": "1\nM",
            "content": "1 oi oi (cid:88) (cid:110) min (cid:104) γt(θ) ˆAt, clip (γt(θ), ϵ, 1 + ϵ) ˆAt (cid:105) βDKL[πθ (cid:111) , πref] (2) t= i=1 where γt(θ) = πθ(oi,tq,oi,<t) (oi,tq,oi,<t) is the importance ratio of the output token oi,t given query and all previous output tokens oi,<t including tool-response tokens. During training, we mask the loss for tool-response tokens as they are not generated by the policy model. πθold 4.2 TRAINING DATA CONSTRUCTION As high-resolution, information-dense images with challenging questions are difficult to collect, we construct training data by synthesizing collages (for in-loop RL) and generating pseudo visual search targets (for out-of-loop RL). The source data are mostly from existing VQA training datasets, which follow largely different distribution from evaluation benchmarks we consider in our experiments. In-loop RL data. key criterion for in-loop RL data is that they must be difficult enough to incentivize visual search; otherwise vReasoner could simply answer on its own and vSearcher would receive no reward. To raise search difficulty and ensure meaningful credit assignment, we build image collages by stitching multiple low-to-medium-resolution images into canvas. We construct collages from filtered combination of Visual CoT (Shao et al., 2024a) and the training data (Wu & Xie, 2024), where each item provides QA pair and target bounding box. For each collage, we choose one target image (carrying the QA) and add several filler images as distractors. After difficulty filtering, we obtain 15,303 hard problems that vReasoner must rely on vSearcher to solve reliably. For more construction details and visualizations of the data, see Appendix C.1. Out-of-loop RL data. We use InfographicVQA (Mathew et al., 2022) as the image source of the out-of-loop RL data. Most InfographicVQA images have high information density and feature more organic and diverse layouts than collages. We detect layout components in the source images with PP-DocLayout plus-L (Cui et al., 2025). The candidate layout boxes are filtered and further processed, resulting in 10,186 high-quality layout boxes. We then use GPT-5-nano to generate concise, high-level region descriptions for each box, as illustrated in Figure 2(b.1). Through prompting, we make GPT-5-nano mimic the style it would use when invoking vSearcher in the in-loop setting. This process yields set of (image, region description, bbox) which enables the out-of-loop RL. More construction details and visualizations are provided in the Appendix C.2."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "In our main experiments, we train Qwen2.5-VL-7B-Instruct (Bai et al., 2025b) as vSearcher under GPT-5-mini-2025-08-07 (OpenAI, 2025a) as vReasoner for balanced efficiency and reasoning capability. The resulting vSearcher, named InSight-o3-vS, is evaluated under various vReasoners including Gemini-2.5-Flash (Comanici et al., 2025). For comparison, we evaluate these vReasoners normally as standalone models and as vReasoner with the untrained Qwen2.5-VL-7B-Instruct as"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison with frontier models/systems. All models/systems are evaluated under their default configurations unless specified otherwise. Performance of open models are mostly cited from the literature (Wu & Xie, 2024; Zheng et al., 2025; Wang et al., 2025a; Lai et al., 2025). Other results are averaged over 3 trials, except MME-RWLite (single-trial). Small-size numbers indicate performance gaps between vReasoners w/ and w/o access to vSearchers. For more comprehensive benchmark results on O3-BENCH, see Appendix B.6. HR-Bench4K Tree-Bench VProbeHard MME-RWLite O3-Bench Average Model/System LLaVA-OV-7B InternVL3.5-8B Qwen2.5-VL-7B Qwen3-VL-8B Qwen3-VL-32B Pixel Reasoner DeepEyes Mini-o3 OpenAI o3 GPT-4o + Qwen2.5-VL-7B + InSight-o3-vS GPT-5-nano + Qwen2.5-VL-7B + InSight-o3-vS V-Bench 70.9 64.0 75.5 86.4 86. 86.3 83.3 88.2 76.4 68.6 75.2 +6.6 80.4 +11.8 64.0 70.1 +6.1 75.1 +11.1 62.0 64.5 68.2 78.9 81.1 74.0 73.2 77.5 74.3 65.1 69.7 +4.6 76.2 +11. 60.6 67.3 +6.7 72.3 +11.7 72.0 83.2 +11.2 86.7 +14.7 85.7 +13.7 73.8 80.6 +6.8 86.9 +13.1 86.2 +12.4 GPT-5-mini + Qwen2.5-VL-7B + InSight-o3-vS + InSight-o3-vS Gemini-2.5-Flash# + Qwen2.5-VL-7B + InSight-o3-vS + InSight-o3-vS Gemini-2.5-Flash + Qwen2.5-VL-7B + InSight-o3-vS + InSight-o3-vS Trained with Gemini-2.5-Flash as vReasoner. # Image-size constraint set to 1280 72.8 76.3 +3.5 80.8 +8.0 85.5 +12.7 80.1 80.9 +0.8 87.6 +7.5 88.3 +8. 75.0 76.7 +1.7 80.2 +5.2 82.7 +7.7 83.5 79.0 4.5 82.3 1.2 83.0 0.5 37.3 40.5 37.0 48.3 48.2 28.8 37.5 - 52.3 47.4 45.9 1.6 49.5 +2. 45.4 45.7 +0.3 47.7 +2.3 54.6 53.1 1.5 54.1 0.5 55.0 +0.4 48.9 51.3 +2.4 52.1 +3.2 52.6 +3.7 49.9 49.1 +0.8 50.1 +0.2 53.6 +3.7 13.4 11.0 23.9 31.6 28.6 28.8 35.1 48.0 23. 26.4 15.4 11.0 25.5 1.1 21.7 18.2 3.5 31.4 +9.7 26.4 37.7 +11.3 41.2 +14.6 39.6 +13.2 17.9 16.7 1.2 19.8 +1.9 26.4 +8.5 39.6 31.4 8.2 36.2 3.4 38.3 1.3 48.5 48.0 46.7 53.0 54. - - - 55.2 51.2 44.6 6.6 50.1 1.1 47.7 44.9 2.8 48.4 +0.7 56.1 58.1 +2.0 59.0 +2.9 58.4 +2.3 55.6 50.9 4.7 55.1 0.5 56.1 +0.5 56.5 52.0 4.5 56.3 0.2 56.4 0. 20.2 24.3 27.4 43.6 60.4 - 27.0 29.1 40.8 28.0 29.5 +1.5 36.4 +8.4 26.5 25.3 1.2 34.6 +8.1 39.0 47.5 +8.5 61.5 +22.5 59.9 +20.9 49.8 47.9 1.9 58.0 +8.2 61.1 +11. 60.4 55.6 4.8 69.7 +9.3 68.3 +7.9 42.1 42.1 46.5 57.0 59.8 - - - 54.0 47.8 46.7 1.1 53.0 +5.2 44.3 45.3 +1.0 51.6 +7.3 53.7 55.4 +1.7 64.9 +11.2 64.1 +10. 53.4 53.3 0.1 57.7 +4.3 60.7 +7.3 61.7 58.0 3.7 63.7 +2.0 64.7 +3.0 1280px, roughly the maximum supported size for OpenAI models/systems via API. vSearcher. We use the default configuration for proprietary models/systems (per official API) except from setting image detail to high2. More implementation details can be found in Appendix D.1. Evaluation datasets. We evaluate range of open and proprietary models/systems on the following benchmarks: (1) Natural-image benchmarks: V-Bench (Wu & Xie, 2024), Tree-Bench (Wang et al., 2025a), and VisualProbe-Hard (Lai et al., 2025). (2) Mixed benchmarks: HR-Bench (Wang et al., 2025f) and MME-RealWorld (Zhang et al., 2024b). For efficient evaluation, we use the lite version of MME-RealWorld, which has 1,919 questions, still much heavier than the other benchmarks. (3) Our O3-BENCH. More information about the benchmarks can be found in Appendix G. 5.1 MAIN RESULTS Cross-domain performance improvement for frontier models. As shown in Table 1, INSIGHTO3 significantly improves frontier models such as GPT-5-mini and Gemini-2.5-Flash on most benchmarks. On average, the performance of GPT-5-mini has improved by 20.9% (relatively) with the help of our vSearcher (InSight-o3-vS). In particular, the accuracy of GPT-5-mini on O3-BENCH has improved from 39.0% to 61.5%. Meanwhile, INSIGHT-O3 also significantly outperforms their pre-RL counterparts, i.e., vReasoner + Qwen2.5-VL-7B, across all the benchmarks. The results suggest that InSight-o3-vS is able to generalize out-of-distribution across various domains since its training data distribution is distinct from the evaluation data distributions. To gain more insight on how INSIGHT-O3 improves the baselines, see Appendix D.3 for comparative analysis. 2When image detail is high, OpenAI scales down oversize images to 12801280px, as per OpenAI API (https://platform.openai.com/docs/guides/images-vision#calculating-costs). This is the maximum supported image resolution for OpenAI models/systems via API. Gemini-2.5-Flash API does not impose such constraint, so we use much larger, 35003500px budget, which is ample for the task."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance of Gemini-2.5-Flash (+ InSight-o3-vS) under different maximum training/test image resolutions. All results are averaged over 3 trials, except for MME-RWLite (which is based on single trial). Small-size numbers indicate performance gaps between settings with and without vSearcher. Trained with Gemini-2.5-Flash as vReasoner. Train res. Test res. V-Bench HR-Bench4K Tree-Bench VProbeHard MME-RWLite O3-Bench Average - 12802 35002 - 12802 35002 12802 12802 12802 35002 35002 72.8 85.5 +12.7 85.3 +12.5 80.1 87.8 +7.7 88.3 +8.2 75.0 82.7 +7.7 81.3 +6.3 83.5 84.3 +0.8 83.0 0.5 48.9 52.6 +3.7 53.1 +4.2 49.9 52.1 +2.2 53.6 +3. 17.9 26.4 +8.5 22.6 +4.7 39.6 39.6 +0.0 38.3 1.3 55.6 56.1 +0.5 55.1 0.5 56.5 56.4 0.1 56.4 0.1 49.8 61.1 +11.3 58.8 +9.0 60.4 67.8 +7.4 68.3 +7. 53.3 60.7 +7.4 59.4 +6.1 61.7 64.7 +3.0 64.7 +3.0 Table 3: Ablation study on reward design and advantage estimation. All results are averaged over 3 trials. Small-size numbers indicate performance changes w.r.t. the proposed setting. Table 4: Sensitivity analysis w.r.t. max. input resolution of vSearcher. #vSearch is the number of vSearcher calls made by vReasoner per QA. Setting V-B. VPHard O3-B. Avg. Max. pixels V-B. O3-B. #vSearch Proposed w/o tool cond. w/o feedback w/o outcome w/o GN 86.9 86.4 0.5 86.5 0.4 86.9 +0.0 87.3 +0.4 41.2 39.3 1.9 37.1 4.1 38.7 2.5 36.8 4.4 61.5 60.6 0.9 58.1 3.4 60.9 0.6 61.3 0.2 63.2 62.1 1.1 60.6 2.6 62.2 1.0 61.8 1.4 0.8M 1.6M 3.2M 6.4M 12.8M 85.3 86.7 89.4 86.4 86. 56.0 60.5 62.3 61.9 61.5 2.82 2.75 2.69 2.66 2.58 Generalization under different vReasoners. We observe that InSight-o3-vS, which was trained as sub-agent under GPT-5-mini, generalizes under other vReasoner models as well. As shown in Table 1, InSight-o3-vS improves the performance of much smaller model, GPT-5-nano, from 21.7% to 31.4% on VisualProbe-Hard, from 26.5% to 34.6% on O3-BENCH, and from 44.3% to 51.6% overall. Under Gemini-2.5-Flash (a different model family), the advantage remains significant, showing about 710% lead over the baselines on V-Bench and O3-BENCH. We have also explored training InSight-o3-vS under Gemini-2.5-Flash instead of GPT-5-mini, and observed similar generalization (see + InSight-o3-vS rows in Table 1). In few cases where InSight-o3-vS fails to improve the performance of the vReasoner, e.g., GPT-4o and Gemini-2.5-Flash on VisualProbeHard, we see sharp decrease in performance as we allow these models to call Qwen2.5-VL-7B. This suggests that these models are relatively weak at tool calling and multi-turn reasoning. In Appendix D.4, we present typical failure cases of INSIGHT-O3; we find that even GPT-5-mini (despite the good performance) still makes lot of mistakes. Performance gaps on O3-BENCH. Interestingly, on O3-BENCH, we observe that Gemini-2.5Flash has huge edge over GPT-5-mini (when they have no access to vSearcher), but on the other benchmarks, the edge is not so prominentGemini-2.5-Flash is even slightly worse than GPT-5mini on Tree-Bench. This suggests that O3-BENCH is indeed quite different from the other benchmarks, and Gemini-2.5-Flash is particularly good at solving the kind of tasks in O3-BENCH on its own. Notably, with InSight-o3-vS, GPT-5-mini is able to drastically reduce the gap (from 21.4% to 8.2%) with Gemini-2.5-Flash, demonstrating the importance of thinking with images for addressing O3-BENCH, and also highlighting the effectiveness of our approach. Effect of input image resolution. Comparing the results of Gemini-2.5-Flash in Table 1 under different maximum input image resolutions, we see that much higher resolution offers clear advantages. However, the improvement brought by vSearcher is less when vReasoner can see clearer. In addition, we find that training under one resolution and evaluating under another seems to have little impact on the performance (see Table 2). Meanwhile, the input image resolution for vSearcher has less impact. Table 4 shows the performance of GPT-5-mini + InSight-o3-vS under varying maximum image resolution of InSight-o3-vS during evaluation. We can see that InSight-o3-vS is not sensitive to the resolution, maintaining decent performance on V-Bench and O3-BENCH even when the maximum image resolution is only 0.8M (25% of that during training). When the resolution is low, the average number of vSearcher calls is relatively high. This is expected as low-resolution images often obscure fine details, making it harder for vSearcher to locate the targets."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Training dynamics of INSIGHT-O3. The rightmost chart, # of vReasoner calls, shows the average number of times vReasoner calls vSearcher per QA. For fair comparison, the reward curves are plotted under the same setting (w/o feedback) for all the settings."
        },
        {
            "title": "5.2 ABLATION STUDY",
            "content": "Hybrid RL training. Table 5 shows the results of ablating the in-loop and the out-of-loop sub-agent RL components. Without the in-loop RL component, training is much faster (80%+ reduction in time per training step) but the final performance is worse on average. Dropping the out-of-loop RL component also hurts the performance; moreover, the training time increases due to more in-loop training. As mentioned in Section 4.2, the two RL components use different training data. The better performance of hybrid RL training can be partly explained by the combined use of the training data. Another contributing factor is the combination of two different sources of supervision (high-level vReasoner feedback and low-level IoU with groundtruth boxes). Overall, combining the two components leads to the best result. Table 5: Ablation study on hybrid RL training. I. and O. stand for the in-loop and outof-loop RL components, respectively. T/step is the average time per training step. All results are averaged over 3 trials. Small-size numbers indicate performance changes w.r.t. the untrained baselines. I. O. V-B. VPHard O3-B. T/step a - 5 - i - 5 - 70.1 74.9 +4.8 73.7 +3.6 74.5 +4.4 80.6 86.4 +5.8 84.8 +4.2 86.9 +6. 18.2 23.9 +5.7 25.1 +6.9 27.4 +9.2 37.7 39.0 +1.3 41.2 +3.5 41.2 +3.5 25.3 27.9 +2.6 31.5 +6.2 32.4 +7.1 - 846s 130s 693s 47.5 - 59.6 +12.1 1223s 58.8 +11.3 105s 61.5 +14.0 941s In Table 3, we compare the setting we proposed in Reward design and advantage estimation. Section 4.1 on GPT-5-mini + InSight-o3-vS with the following ablated variants: w/o tool cond. drops the tool condition in the reward function; w/o feedback removes vReasoner feedback, only using outcome supervision for pseudo IoU reward; w/o outcome is the opposite of w/o feedback; and w/o GN drops the global normalization for advantage estimation. The originally proposed setting outperforms all the variants with small average lead on the three benchmarks. The training dynamics under these settings are shown in Figure 3. As vSearcher learns to better locate the regions described by vReasoner, we observe that both the out-of-loop localization IoU and the in-loop vReasoner accuracy improve. The non-monotonic # of vReasoner calls shows two RL phases of INSIGHT-O3: vSearcher first learns to obey the formatting instructions, and then learns to localize more accurately (so vReasoner could solve the same problem with less vSearcher calls). Although we encourage vSearcher to use the image-cropping tool, we find the average tool call count often ends up close to 1. There are two underlying reasons for this behavior. First, as mentioned by Zheng et al. (2025); Lai et al. (2025), Qwen2.5-VL-7B-Instruct is often reluctant to call the tool, and does not seem to know how to use the tool properly. Second, vReasoner usually describe rough region that is not very hard for vSearcher to locate, so the tool has little utility for vSearcher."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced O3-BENCH, high-information-density benchmark that jointly evaluates visual localization and multi-hop reasoning. To advance the research on this challenging benchmark, we proposed INSIGHT-O3, multi-agent framework that decomposes the think with images workflow into high-level reasoning (vReasoner) and visual search (vSearcher). We focus on the training of vSearcher via reinforcement learning to seamlessly cooperate with vReasoner. The specialized InSight-o3-vS can be used as plug-and-play component for existing multimodal foundation models and helps significantly improve the performance of frontier models."
        },
        {
            "title": "REFERENCES",
            "content": "Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. 54 Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 126135, 2017. 53 Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. 29, 43, 47, 54 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. 3, 5, 7, 19, 21, 29, 43, 47 Bytedance. Doubao-Seed-1.6. https://console.volcengine.com/ark/region: ark+cn-beijing/model/detail?Id=doubao-seed-1-6, 2025. 5, 29 Lluis Castrejon, Thomas Mensink, Howard Zhou, Vittorio Ferrari, Andre Araujo, and Jasper Uijlings. Hammr: Hierarchical multimodal react agents for generic vqa. In NeurIPS 2024 Workshop on Compositional Learning: Perspectives, Methods, and Paths Forward, 2024. 2, 19 Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025a. 19 Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025b. 19 Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025. 20 Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 5, 7, 19, Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. URL https://arxiv.org/abs/2507.05595. 4, 7, 21 Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, et al. Multi-agent collaboration via evolving orchestration. arXiv preprint arXiv:2505.19591, 2025. 19 Peter Dayan and Geoffrey Hinton. Feudal reinforcement learning. Advances in neural information processing systems, 5, 1992. 2 Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025. URL https://arxiv.org/abs/2503.17352."
        },
        {
            "title": "Preprint",
            "content": "Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 19 Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. 20 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 19 Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3, 19 Google. Gemini 3. https://blog.google/products/gemini/gemini-3/, 2025. 29 Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. 3, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. 3, 19, 47 Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. 19 Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1495314962, 2023. 3, 20 Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. 19 Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. In Forty-second International Conference on Machine Learning, 2025. 1 Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for In The Twelfth International Conference on Learning multi-agent collaborative framework. Representations, 2023. 2, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. 19,"
        },
        {
            "title": "Preprint",
            "content": "Zhipeng Hou, Junyi Tang, and Yipeng Wang. Halo: Hierarchical autonomous logic-oriented orchestration for multi-agent llm systems. arXiv preprint arXiv:2505.13516, 2025. 19 Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025. 19 Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 20 Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. 19 Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. Llvip: visible-infrared paired dataset for low-light vision. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 34963504, 2021. 53 Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang. Vlm-r3: Region recognition, reasoning, and refinement for enhanced multimodal chain-of-thought. arXiv preprint arXiv:2505.16192, 2025a. 6, 20 Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, and Lei Zhang. Rex-thinker: Grounded object referring via chain-of-thought reasoning. arXiv preprint arXiv:2506.04034, 2025b. 20 Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, and Hamid Rezatofighi. Hydra: hyper agent for dynamic compositional visual reasoning. In European Conference on Computer Vision, pp. 132149. Springer, 2024. 3, 20 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. 1, 2, 3, 4, 5, 8, 10, 19, 20, 52 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 29 Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo: training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 90989108, 2025. 1, 20 Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023a. 2, 19 Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, et al. Coda: real-world road corner case dataset for object detection in autonomous driving. In European conference on computer vision, pp. 406 423. Springer, 2022. 53 Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b."
        },
        {
            "title": "Preprint",
            "content": "Jiaying Liu, Dong Liu, Wenhan Yang, Sifeng Xia, Xiaoshuai Zhang, and Yuanying Dai. comprehensive benchmark for single image compression artifact reduction. IEEE Transactions on image processing, 29:78457860, 2020. 53 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 3, 19 Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246, 2025. 20 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 3, 19, 54 Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. 7 Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Minheng Ni, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Wangmeng Zuo, and Lijuan Wang. Point-rft: Improving multimodal reasoning with visually grounded reinforcement finetuning. arXiv preprint arXiv:2505.19702, 2025. 20 OpenAI. GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. 29 OpenAI. GPT-5. https://openai.com/index/introducing-gpt-5/, 2025a. 3, 5, 7, 19, 21, 29, 33, 45, 46, 52 OpenAI. GPT-5.2. https://openai.com/index/introducing-gpt-5-2/, 2025b. 29 OpenAI. OpenAI o3 and o4-mini. https://openai.com/index/introducing-o3and-o4-mini/, 2025c. 1, 3, 19, 20, 29 Yi Peng, Peiyu Wang, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, et al. Skywork r1v: Pioneering multimodal reasoning with chain-ofthought. arXiv preprint arXiv:2504.05599, 2025. 19 Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236, 2024. 20 Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms, 2024. URL https: //arxiv.org/abs/2407.01509. 54 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 19 Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2024. URL https://arxiv.org/abs/2405. 14573. Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Mykel Kochenderfer, Chiho Choi, and Behzad Dariush. Rank2tell: multimodal driving dataset for joint importance ranking and reasoning. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 75137522, 2024."
        },
        {
            "title": "Preprint",
            "content": "Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. 3, 19 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3, 19 Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR, 2024a. 7, 20, 28 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. 3, 7, 19 Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025a. 19 Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo arXiv preprint Skywork-r1v3 technical report. Hao, Peiyu Wang, Jianhao Zhang, et al. arXiv:2507.06167, 2025b. 19 Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. 2, 19, 20 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025a. 1, 3, 5, 6, 20 Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025b. 20 Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, et al. Fair1m: benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 184:116130, 2022. 53 Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1188811898, 2023. 3, 20 Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025a. 1, 8, 19, 52 Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025b. Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025c. 1,"
        },
        {
            "title": "Preprint",
            "content": "Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 19, 54 Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025d. 19 Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3.5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025e. 3, 19, 29, 47 Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025f. 8, 53 Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025g. 1, 19 Ye Wang, Qianglong Chen, Zejun Li, Siyuan Wang, Shijie Guo, Zhirui Zhang, and Zhongyu Wei. Simple o3: Towards interleaved vision-language reasoning. arXiv preprint arXiv:2508.12109, 2025h. Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, et al. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. arXiv preprint arXiv:2507.05255, 2025. 19 Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. 1, 2, 3, 4, 7, 8, 19, 20, 28, 52 Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, 2024. 19 LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/ 2506.03569. 19 Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl 1.5 technical report. arXiv preprint arXiv:2509.01563, 2025a. 3, 19 Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025b. 19 Qinhong Yang, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Lu Yuan, Gang Hua, and Nenghai Yu. Hq-50k: large-scale, high-quality dataset for image restoration. arXiv preprint arXiv:2306.05390, 2023. 53 Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025c. 3,"
        },
        {
            "title": "Preprint",
            "content": "Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, et al. Mme-reasoning: comprehensive benchmark for logical reasoning in mllms. arXiv preprint arXiv:2505.21327, 2025. 1 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024a. 1, 3, 19 Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. 3, 19 Andy Zeng, Maria Attarian, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, et al. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations, 2023. 2, 19 Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Bjorn Stenger, Wei Liu, Hongdong Li, and Ming-Hsuan Yang. Benchmarking ultra-high-definition image super-resolution. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1476914778, 2021. 53 Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024a. Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: hierarchical multi-agent framework for general-purpose task solving. arXiv e-prints, pp. arXiv 2506, 2025a. 19 Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025b. 1, 20 Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024b. 2, 3, 4, 8, 19, 53 Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu arXiv preprint Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv:2508.11630, 2025c. 20 Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling. arXiv preprint arXiv:2507.07998, 2025. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 1, 3, 5, 8, 10, 20, 51 Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. 54 Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025a. 19 Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large language models with active perception via grpo. arXiv preprint arXiv:2505.21457, 2025b. 1,"
        },
        {
            "title": "Preprint",
            "content": "Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and tracking meet drones challenge. IEEE transactions on pattern analysis and machine intelligence, 44(11):73807399, 2021."
        },
        {
            "title": "A ADDITIONAL DISCUSSION ON RELATED WORK",
            "content": "A.1 MULTIMODAL BENCHMARKS Classical multimodal benchmarks (Goyal et al., 2017; Hudson & Manning, 2019; Gurari et al., 2018; Saikh et al., 2022; Li et al., 2023b; Fu et al., 2023; Liu et al., 2023; Ge et al., 2024) primarily focus on coarse image-level understanding or target at salient-object attributes, on which current multimodal models (Bai et al., 2025b; Wang et al., 2025e; Yang et al., 2025a) show near-saturated performance. With growing attention to multimodal reasoning, more challenging benchmarks have emerged, which could be categorized as two groups. (1) Cognition-centric benchmarks. STEM (science, technology, engineering, and mathematics) benchmarks (Lu et al., 2023; Wang et al., 2024; Zhang et al., 2024a; Yue et al., 2024a;b) evaluate the models multi-step reasoning, integration of world knowledge, and complex calculations to solve scientific problems, whereas the accompanying images are generally straightforward to interpret. (2) Perception-centric benchmarks. These benchmarks (Wu & Xie, 2024; Wang et al., 2025g; Zhang et al., 2024b; Lai et al., 2025) require fine-grained perception in high-resolution images and strong OCR recognition on text-rich scenes. Nevertheless, many questions become routine once the model precisely localizes the target region, allowing for single-glance solutions. Though the recent TreeBench (Wang et al., 2025a) evaluates second-order reasoning over object spatial transformations, depth ordering, and etc, it still centers on single region in natural images, leaving cross-region evidence aggregation largely underexplored. With the emergence of the think with images paradigm (OpenAI, 2025c), we argue that well-designed benchmarks should evaluate the joint perceptual and cognitive skills. Our proposed O3-BENCH fills the research gap by meticulously constructing hard questions on high-information density images (e.g., composite graphs, maps), therefore requiring models to gather information from multiple, spatially distinct regions and to perform complex, interleaved reasoning. A.2 MULTIMODAL REASONING MODELS Reinforcement learning (RL) (Schulman et al., 2017; Rafailov et al., 2023; Hu et al., 2025) has long been used to align the response of large language models (LLMs) and multimodal LLMs (MLLMs) with human preferences. Recently, DeepSeek-R1 (Guo et al., 2025a) creatively applied group relative policy optimization (GRPO) (Shao et al., 2024b) to LLMs, estimating the mean and variance of advantages across response groups under simple reward signal. This strategy reliably elicits behaviors such as planning, thinking, and self-reflection, enabling long chain-of-thought (CoT) reasoning and moving toward more general-purpose reasoning capabilities. Building on this success, several works (Huang et al., 2025; Yang et al., 2025c; Meng et al., 2025; Chen et al., 2025a; Wang et al., 2025b; Shen et al., 2025a; Chen et al., 2025b; Deng et al., 2025; Wei et al., 2025; Peng et al., 2025; Wang et al., 2025d; Shen et al., 2025b) explore cold-start initialization and GRPO-based RL training for multimodal models (e.g., Qwen2.5-VL (Bai et al., 2025b)) and report substantial gains on scienceand math-oriented benchmarks. Concurrently, InternVL3.5 (Zhu et al., 2025a; Wang et al., 2025e) and Keye-VL1.5 (Yang et al., 2025b;a) further leverage cascaded, iterative RL stages to push the frontier of reasoning ability, achieving performance competitive with proprietary models (OpenAI, 2025a; Comanici et al., 2025). Nevertheless, current multimodal reasoning models (Xiaomi, 2025; Hong et al., 2025; Guo et al., 2025b; Du et al., 2025) still focus on text-centric reasoning, neglecting the distinctive demands of visual reasoning in multimodal scenarios. A.3 HIERARCHICAL AGENT FRAMEWORKS AND TOOL-USING MULTIMODAL AGENTS Recent works have shown that hierarchical collaboration among specialized agents can significantly improve performance on complex tasks. Socratic Models (Zeng et al., 2023) and HuggingGPT (Shen et al., 2023) demonstrate early examples of LLM-based orchestration over expert models via language. More structured frameworks like CAMEL (Li et al., 2023a), MetaGPT (Hong et al., 2023), and HAMMR (Castrejon et al., 2024) explore role-based or modular specialization with coordinated task decomposition. Others, including AutoGen (Wu et al., 2024), HALO (Hou et al., 2025), Puppeteer (Dang et al., 2025), and AgentOrchestra (Zhang et al., 2025a), further extend this paradigm with explicit planning hierarchies, adaptive execution, and learned orchestration, consistently outperforming flat-agent baselines across diverse domains."
        },
        {
            "title": "Preprint",
            "content": "There are also recent works exploring equipping multimodal models with tool-use and programmatic reasoning to tackle complex visual tasks. VisProg (Visual Programming) (Gupta & Kembhavi, 2023) and ViperGPT (Surıs et al., 2023) are two pioneering approaches that use large language models to generate and execute code for orchestrating vision modules. Beyond static program generation, other agents use LLMs as high-level controllers. For example, HuggingGPT (Shen et al., 2023) demonstrates an LLM (ChatGPT) orchestrating numerous specialized models (for vision, language, etc.) More recently, HYDRA (Ke et al., 2024) introduces dynamic multi-stage framework for visual reasoning: it integrates an LLM-based planner and reasoner with reinforcement learningbased controller that adapts the sequence of operations via feedback loops, yielding more reliable step-by-step reasoning. These tool-augmented systems highlight the power of combining learned vision-language models with external modules or code execution to improve flexibility and compositional reasoning. In contrast, our work (INSIGHT-O3) targets complementary gap by introducing dedicated visual search agent that can be invoked by reasoning agents to locate finegrained, conceptually described regions within images. This specialized capability, absent in prior tool-using frameworks, allows an INSIGHT-O3-enabled system to pinpoint relevant visual details based on free-form descriptions, thereby enhancing multimodal reasoning with more precise visual understanding. A.4 VISUAL SEARCH MODELS Visual search is an important functionality in the multimodal domain, requiring the models to perform active perception over regions of interest (RoIs) for fine-grained visual understanding. Early approaches (Wu & Xie, 2024; Shao et al., 2024a; Qi et al., 2024; Hu et al., 2024; Li et al., 2025) rely on external tools or predefined workflows for region localization and use instruction tuning to trigger tool use. These models exhibit rigid output patterns and typically support only single round of visual search, which limits their effectiveness in complex scenes. Recently, the milestone OpenAI o3 (OpenAI, 2025c) established the think with images paradigm, in which image manipulations (e.g., zooming, cropping) are internalized as intrinsic capabilities, enabling imagetext interleaved reasoning. The community has rapidly turn the attention to the promising field. DeepEyes (Zheng et al., 2025) exploits the inherent grounding ability of MLLMs and incentivizes visual search via end-to-end reinforcement learning. Pixel-Reasoner (Su et al., 2025a) improves search accuracy by warm-start instruction tuning on synthesized data with error-induced self-correction trajectories. Mini-o3 (Lai et al., 2025) introduces an over-turn masking technique during RL to encourage multiturn interaction, markedly enhancing reasoning adaptability and diversity. Other lines of work (Zhao et al., 2025; Zhang et al., 2025c; Liu et al., 2025) resort to write codes for executing multiple image manipulations (e.g., cropping, rotation, enhancement), pointing to an open-ended toolkit for visual reasoning. Although effective, recent advanced methods (Zhu et al., 2025b; Fan et al., 2025; Zhang et al., 2025b; Su et al., 2025b; Ni et al., 2025; Cheng et al., 2025; Jiang et al., 2025a;b; Wang et al., 2025c;h) still largely prioritize locating single region on natural images, which sidelines the models capacity for reasoning. This paper broadens the capability scope of visual search models by decoupling visual reasoning and visual search agents, allowing the receiving of any images and searching of multiple distinct regions. ADDITIONAL INFORMATION ON O3-BENCH B.1 BENCHMARK STATISTICS Figure 4: Distribution of layout numbers in O3-BENCH. Figure 5: Resolution distribution."
        },
        {
            "title": "Preprint",
            "content": "We summarize the benchmark statistics from following three aspects. (1) Distribution of layouts. The benchmark features 8.7 layouts and 2.4 target layouts for each sample on average, indicating high information density and the need for multi-step reasoning. And the layout distribution by category is displayed in Figure 4. It can be seen that chart images typically exhibit larger set of total layouts, whereas map images require more target layouts for reasoning. (2) Distribution of resolution. We collect high-resolution imagery in O3-BENCH. As shown in Figure 5, most images have side lengths in the 2K5K range, while some images reach up to 10K pixels on the longer side, yielding high information density. On average, image height and width are 3,967 and 4,602 pixels, respectively. (3) Distribution of options. We randomly shuffle options AE, ensuring an approximately uniform distribution of correct-answer positions. And small portion (7.2%) of samples use option (No Right Choice) as correct answer, which compels models to aggregate evidence across the entire image and determine that none of the other options is valid. B.2 DETAILS OF MACHINE PRE-ANNOTATION (1) Layout detection. We first divide the high-resolution images into several structured layouts (e.g., tables, charts, legends). We use PP-DocLayout plus-L (Cui et al., 2025) to detect layout bounding R4 denotes layout coordinates. i=1, where li li boxes in the image and construct the set } For chart images, we directly use the detector outputs. For map images, we review the predictions, correct erroneous regions, and supplement missing areas via manual annotation. = { (2) Information extraction. For each detected layout li in image I, we obtain the cropped image Ili according to its coordinates. We then prompt Qwen2.5-VL-32B (Bai et al., 2025b) to produce detailed caption ci and extract OCR text oi for Ili, thereby forming the layout triplet τi = (Ili, ci, oi). = i=1. In addition, we obtain global context And then we aggregate all region triplets into = (cg, og). The prompts we by generating caption and OCR text for the full image, denoted as use are provided in Appendix E.1. τi } { (3) Automated question synthesis. We provide the layout set to GPT5 (OpenAI, 2025a) and explicitly prompt it to generate five questions that compose evidence from multiple regions. For each question, GPT-5 must produce six options (AF) with exactly one correct answer, and option is reserved for No Right Choice. It also need to supply step-by-step explanation that interprets the reasoning chain. It is noted that we do not provide the full image to GPT-5, which compels the model to focus on region-level details and encourages multi-hop composition. The prompts we use are provided in Appendix E.2. and the global context B.3 MORE EXPERIMENTAL RESULTS FOR O3-BENCH We present additional results for O3-BENCH in this section. Because our annotation pipeline supplies the target layouts most relevant to each question, we can provide these region crops alongside the original full image at test time. We evaluate GPT-5-Mini (OpenAI, 2025a) and Qwen2.5-VL-7B (Bai et al., 2025b), with results shown in Table 6. Both models exhibit significant performance gains when given the additional target layouts, underscoring the need for models to actively locate task-critical regions and perform interleaved visual reasoning. B.4 VISUALIZATION OF O3-BENCH Table 6: Ablation on target layouts in O3-BENCH. Model O3-BENCH chart map overall GPT-5-mini + target layouts Qwen2.5-VL-7B + target layouts 34.4 74.2 30.9 39. 43.2 61.5 24.4 31.9 39.0 67.5 27.4 35.4 In Figures 611, we present six representative visualizations (four map items and two chart items), showing how O3-BENCH couples high-resolution perception with multi-step reasoning. Each annotation includes: (i) the multiple-choice question and answer; (ii) highlighted target layouts that mark the regions consulted along the solution path; and (iii) concise, ordered explanation that composes the evidence, which allows readers to verify the answer quickly."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Example from O3-BENCH (Map-1). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Example from O3-BENCH (Map-2). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Example from O3-BENCH (Map-3). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Example from O3-BENCH (Map-4). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Example from O3-BENCH (Chart-1). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Example from O3-BENCH (Chart-2). Each annotation comprises six-choice QA and brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see."
        },
        {
            "title": "Preprint",
            "content": "B.5 COMPARISON WITH MME-REALWORLD (CHART) The chart images of O3-BENCH are mostly selected from the Diagram & Table subset of MMERealWorld. The original questions from MME-RealWorld are relatively simple, usually focusing on single value in chart, which do not require any kind of multi-hop reasoning. For example, the original question of MME-RealWorld for the chart we show in Figure 10 is: What is the cost inflation rate in the General Settings section of the General Assumptions table? In comparison, the new question in O3-BENCH is: If the Condo units Purchase Cost is depreciated evenly over its stated operation period, what percentage does this depreciation amount account for of its Avg. Annual Rental Fee (annualized based on the Year 10 Avg. Daily Rental Fee and 30 days/month)? As illustrated in Figure 10, answering this question requires piecing together detailed information from three different tables through multi-step reasoning and arithmetic. Overall, the questions of O3-BENCH (chart) are much more challenging than the MME-RealWorld counterparts. This can also be seen from the following statistics: The average accuracy of GPT-5-mini on O3-BENCH (chart) is about 38.2%, whereas on MMERealWorld (chart), the accuracy is about 82.4%. The average number of vSearch steps of InSight-o3 on O3-BENCH (chart) is about 3.1, whereas on MME-RealWorld (chart), the number is about 1.1. The average multi-turn response length (including reasoning tokens) of GPT-5-mini vReasoner on O3-BENCH (chart) is about 1942.3 characters, whereas on MME-RealWorld (chart), the average response length is about 730.0 characters. B.6 FULL BENCHMARK RESULTS Table 7 shows our full benchmark results of frontier multimodal models/systems on O3-BENCH. For OpenAI models/systems, oversize input images are resized to 1280 1280px (this is roughly the maximum supported size per OpenAI API, as mentioned in the main paper) and image detail is set to high. For other models/systems, oversize input images are resized to 3500 3500px (this translates to about 16K tokens/image for Qwen2.5-VL). All models/systems are given 16K tokens/response budget (including reasoning tokens), which should be more than enough to solve the problems in O3-BENCH. Incomplete responses beyond this budget are considered wrong without checking. Open models are evaluated via self-hosted vLLM instances. Proprietary models are evaluated via official APIs. Our evaluation code can be found at https://github.com/m-Just/ InSight-o3."
        },
        {
            "title": "C TRAINING DATA CONSTRUCTION DETAILS",
            "content": "C.1 IN-LOOP RL DATA Our collage sources come from the training split of Visual CoT (Shao et al., 2024a) and (Wu & Xie, 2024). We first filter both datasets by target bounding box size, retaining items with area(bbox)/area(image) < 0.04. From Visual CoT, we keep all Chart/OCR-centric subsets (dude, cub, textvqa, docvqa, infographicsvqa, sroie, vsr, textcap), and treat the naturalimage subsets (flickr30k, gqa, openimages, v7w) together with as separate stream due to lower QA reliability (e.g., weaker questionimage alignment and non-unique answers). To ensure stable RL rewards, we filter this stream with an MLLM check using Qwen2.5-VL-7B and GPT-5nano under deterministic prompt. An item is retained only if both models return correct answer; otherwise it is discarded, including ambiguous or poorly aligned cases. After this pipeline, we retain 100K items as the source pool for collage synthesis. D"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Benchmark of frontier multimodal models/systems on O3-BENCH. Default model/system settings are used unless stated otherwise. All results are averaged over 3 random trials. LLaVA-OneVision-7B (Li et al., 2024) 21.13.2 19.44.3 20.23."
        },
        {
            "title": "Overall",
            "content": "InternVL3.5-8B (Wang et al., 2025e) InternVL3.5-30B-A3B (Wang et al., 2025e) GLM-4.6V (Hong et al., 2025) Qwen2.5-VL-7B-Instruct (Bai et al., 2025b) Qwen2.5-VL-32B-Instruct (Bai et al., 2025b) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-8B-Thinking (Bai et al., 2025a) Qwen3-VL-30B-A3B-Instruct (Bai et al., 2025a) Qwen3-VL-30B-A3B-Thinking (Bai et al., 2025a) Qwen3-VL-32B-Instruct (Bai et al., 2025a) Qwen3-VL-32B-Thinking (Bai et al., 2025a) Qwen3-VL-235B-A22B-Instruct (Bai et al., 2025a) Qwen3-VL-235B-A22B-Thinking (Bai et al., 2025a) GPT-4o (OpenAI, 2024) GPT-5-nano (OpenAI, 2025a) GPT-5-mini (OpenAI, 2025a) GPT-5 (OpenAI, 2025a) GPT-5.2 (OpenAI, 2025b) OpenAI o3 (OpenAI, 2025c) Gemini-2.5-Flash#(Comanici et al., 2025) Gemini-2.5-Flash (Comanici et al., 2025) Gemini-2.5-Pro (Comanici et al., 2025) Gemini-3-Flash (Google, 2025) Gemini-3-Pro-Preview (Google, 2025) 26.22.5 24.53. 22.70.7 21.21.7 24.31.1 22.82.5 51.52.2 38.52.9 44.62.4 30.91.8 35.41. 54.40.3 49.12.2 49.31.4 51.11.5 73.71.3 52.43.1 73.41.9 57.31.2 22.10.9 19.22.3 34.43.5 30.90.8 31.92.3 27.81.3 46.61.3 61.81.2 67.32.5 68.12.6 67.72.0 24.41.1 33.51.2 33.94.3 33.00.9 32.11.9 36.81.2 48.52.1 40.51.4 53.82.0 47.82.0 33.31.0 33.33.9 43.22.0 52.60.7 39.02.7 52.42. 52.63.0 59.21.8 63.72.5 69.03.4 69.63.6 27.40.3 34.41.0 43.60.4 40.60.7 40.20.4 43.61.3 60.41.7 46.11.3 63.10.8 52.30.8 28.00.8 26.53.1 39.00.6 42.30.0 35.72.3 40.80.9 49.81.4 60.40.5 65.42.5 68.61.6 68.72.7 doubao-seed-1-6-250615 (Bytedance, 2025) 55.41.5 48.54.4 51.82.7 INSIGHT-O3 (w/ GPT-4o) INSIGHT-O3 (w/ GPT-5-nano) INSIGHT-O3 (w/ GPT-5-mini) INSIGHT-O3 (w/ Gemini-2.5-Flash) 34.40.7 35.32.2 67.31.4 75.62.0 38.30.8 34.11.6 56.42.1 64.43. 36.40.2 34.61.9 61.50.4 69.70.7 # Image-size constraint set to 12801280px, roughly the maximum supported size for OpenAI models/systems via API. Given the filtered source pool , we synthesize collage-style training images around one primary target (the image the model should attend to) and auxiliary fills (other images used to occupy remaining space and control background complexity). The full procedure is presented in Algorithm 1. Specifically, we sample and grid-quantize canvas, then determine feasible target scale by intersecting global bounds with bbox-to-canvas cap and minimum short-edge constraint after light aspect jitter (Steps 12). We plan & place the target using fit-then-shrink heuristic with single enlarge-canvas fallback (Steps 34). Remaining area is panelized into grid-aligned regions under simple aspect/size guards (Step 5). Panels are then filled (largest-first) by sampling images using usage-aware weights (favoring less-frequently used candidates) that also roughly match panel aspect; when needed, we apply light center crop and bounded scaling (Step 6). If packing remains incomplete after brief extra fill pass, we resample from the canvas; otherwise we finalize the collage (Steps 78). To avoid ambiguity when querying the target image, we annotate each collage tile with an ID and include this ID in the question as reference. Figure 12 shows representative visualizations of synthesized collages. The canvas is sized so that the target box occupies only tiny fraction of the canvas area, enforcing area(bbox)/area(canvas) < 0.0002. We filter out items that vReasoner can already solve without calling vSearcher using pass@3 check (three attempts; any success leads to removal)."
        },
        {
            "title": "Preprint",
            "content": "D Algorithm 1 Target-and-Fill Collage Synthesis (High-level) Require: Metadata table (image path, Wsrc, Hsrc, object bbox), target image , grid G, min short edge , canvas area/aspect ranges [Amin, Amax] and [amin, amax], target scale bounds [λmin, λmax], target aspect jitter τtgt, fill jitter τfill, fill scales [λfill max], panel aspect range [ARpanel 104, placement retries R, max attempts and placements max], max effective source area Seff max, bbox coverage cap ρcap= Ensure: Canvas { 1: Precompute target meta. From t, read Wsrc, Hsrc, bbox ratio ρsrc; set min , ARpanel target, fills min, λfill = } (WsrcHsrc) (cid:112) min(cid:0)1, max/(WsrcHsrc)(cid:1)2 Seff 2: for attempt = 1 to do 3: , and rsrc Wsrc/Hsrc. obtain (W, H). Step 1 Sample canvas. Draw Acanvas Step 2 Compute feasible target scale interval. [Amin, Amax], rejection loop [amin, amax]; snap to grid to Bounds: λ [λmin, λmax]; occupancy: λS Choose by log-jittering rsrc within λS/r) min(λSr, . (cid:112) Acanvas; bbox: ρsrcλS Acanvas τtgt and raise the lower bound on λ so that ρcap. Let Iλ be the intersection of the above constraints; if Iλ = , optionally enlarge Acanvas once reject restart at Step 1 Iλ (e.g., midpoint); set w=(cid:112)(λS)r, and recompute; if still empty, continue Step 3 Plan target box. Pick λ h=(cid:112)(λS)/r; snap (w, h) to multiples of G. Step 4 Place target. Best-fit on the free list (grid-aligned). If no fit, iteratively shrink (w, h) and update λ=wh/S, keeping feasibility in Iλ, up to retries; if still not placed, optionally enlarge canvas once and re-plan; if it fails, continue restart at Step 1 Step 5 Normalize free space. Recursively split free regions into grid-aligned panels max] and minimal size. subject to aspect ARP min , ARpanel [ARpanel reject Step 6 Fill panels. For each panel (largest-first), sample fill image aspect band around ARP ; if needed, center-crop to ARP ; scale with λfill place. [λfill within min, λfill τfill max] and Step 7 Resample if needed. continue Step 8 Finalize. return 10: 11: end for 12: return None , If residual free space remains after one extra fill pass, restart at Step 1 reject no feasible collage after attempts 4: 5: 6: 7: 8: 9: C.2 OUT-OF-LOOP RL DATA For PP-DocLayout plus-L, we use the following configuration: {\"threshold\": \"layout nms\": True, \"layout merge bboxes mode\": \"union\"}. 0.01, To construct meaningful visual search targets from the boxes produced by PP-DocLayout plus-L, we start by dropping boxes of trivial layout classes such as header and footer, keeping only text, image, table, chart, figure_title, and paragraph_title boxes. We then drop boxes that are too large as they usually merge disparate things together. Boxes with area larger than quarter of the whole image area are dropped, except for charts which are usually clean and we use much higher threshold (0.8) for them. Next, we merge boxes that are very close to each other, measured by the effort required to enclose them in one box. The effort is computed as 1 - summed_area / min_enclosing_area. We start with the boxes that require the least effort to merge, and stop until the required effort reaches threshold. For figure_title and paragraph_title boxes, the threshold is 0.15, while for other boxes, it is 0.1. This helps merging small, auxiliary boxes such as figure titles and chart legends with their closest neighbors in the vicinity, avoiding truncating important context and preventing trivial search targets dominating"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Example of synthesized collage for the in-loop RL. Multiple low-resolution images are stitched to raise visual density. The blue dashed box highlights the target tile; the magenta box marks the target bbox. Remaining tiles are distractors."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Examples of InfographicVQA images with pre-generated layout boxes and region descriptions for the out-of-loop RL."
        },
        {
            "title": "Preprint",
            "content": "the dataset. We skip merge if the new box would be too large (box-to-image area ratio more than 0.25) or have an extreme aspect ratio (not within 1:5 and 5:1). We do not merge charts and tables. After merging the boxes, we drop (1) charts/tables enclosing other charts/tables, (2) unmerged images that do not contain any text or titles, (3) unmerged titles, (4) unmerged texts, (5) boxes that are too small (box-to-image area ratio less than 0.001), and (6) boxes with extreme aspect ratios (not within 1:5 and 5:1). These boxes often contain little information (e.g., icons, short texts). In the end, an image may still have multiple visual search targets; they are treated as separate data entries. To generate region descriptions for the visual search targets obtained earlier, we first draw red box around the target on the image, and then prompt GPT-5-nano as follows: [SYSTEM] You are visual assistant. Your goal is to help the user to locate the region indicated by the red bounding box in an image. When the user asks you to describe the region, you must follow the following rules: - Keep it super simple and short as if you cant see clearly what is in the region. - Dont mention any details, specific content, or small text in the region. - Use concise, visually grounded targets (e.g., chart, an object, text block, distinct area). - Optionally include approximate location (e.g., top-left of the image, bottom-right of the big chart, center column). - Optionally include the title of the region (e.g., the table about XXX, the section titled XXX). - Avoid non-visual or ordinal references (e.g., \"the third largest bar\", \"the second rows number\"). - Dont mention the red bounding box. Output format: region_description={...} [USER] Describe the region in the red bounding box. In Figure 13, three examples of the final data are shown. Note that unlike collages, these examples are not stitched together; they are simply displayed side-by-side to save space. ADDITIONAL INFORMATION ON INSIGHT-O3 D.1 INSIGHT-O3 IMPLEMENTATION DETAILS 3.2M pixels (4K tokens/image) during trainThe maximum image resolution of vSearcher is set to ing, and 12.8M pixels (16K tokens/image) during evaluation. Oversize images are downsampled to meet the constraint. We allow both vReasoner and vSearcher to make at most 6 sub-agent/tool calls during both training and evaluation. Image crops returned by sub-agent/tool calls are obtained from original images, and then resized if they exceed the size limit. For vSearcher, we use maximum response length (including results returned by sub-agent/tool calls) of 9K and 32K tokens (with sampling temperature 1 and 0) for training and evaluation, respectively. Other hyperparameters include: training batch size 24, rollout number 8, learning rate 106, KL loss coefficient 0.01, reward weights λformat = 0.2, λIoU = 0.8, and IoU reward threshold α = 0.25. The composition of in-loop/out-of-loop training data is 1:1. We train vSearcher fully on-policy for 150 steps. We freeze the vision tower and the adapter of Qwen2.5-VL-7B-Instruct during the whole training process. We use GPT-5-nano (OpenAI, 2025a) for evaluating answer correctness. Our code is based on verl (Sheng et al., 2024). The prompts we use can be found in Appendix F. D.2 MORE VISUALIZATIONS OF INSIGHT-O3 REASONING PROCESS Figure 14 and 15 show examples of inference process of INSIGHT-O3 (GPT-5-mini as vReasoner). The vReasoner issues natural-language target descriptions; the vSearcher localizes evidence and returns them. Across few rounds, the pair composes multi-step evidence and produces the final answer, demonstrating that INSIGHT-O3-VS plugs in cleanly and supports effective reasoning."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Qualitative example 1 with GPT-5-mini as vReasoner and INSIGHT-O3-VS as vSearcher. The reasoner requests venue-level cues (e.g., legend/index lookups); the searcher returns localized regions and snippets, iterating to correct answer."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Qualitative example 2 with GPT-5-mini as vReasoner and INSIGHT-O3-VS as vSearcher. The reasoner requests venue-level cues (e.g., legend/index lookups); the searcher returns localized regions and snippets, iterating to correct answer."
        },
        {
            "title": "Preprint",
            "content": "D.3 COMPARATIVE ANALYSIS BETWEEN INSIGHT-O3 AND BASELINES In Figure 1619, we compare the behavior of INSIGHT-O3 (GPT-5-mini + InSight-o3-vS) with two baselines: (1) GPT-5-mini and (2) GPT-5-mini + Qwen2.5-VL-7B. This comparative analysis is based on three examples of O3-BENCH. We rate each crop returned by vSearcher on three levels: High-quality crops tightly enclose the visual search targets and the relevant context. Medium-quality crops contain the visual search targets but include too much context. Low-quality crops miss or truncate the visual search targets or relevant context. In the most basic setting where vReasoner does not have access to vSearcher, it often uses similar reasoning patterns as follows to reach its conclusion: first locate . . . then look for . . . There is . . . Therefore . . . (see the top parts of Figure 17-19). During this process, vReasoner often hallucinates and makes factual errors about what it sees, suggesting that it does not really see the relevant visual details clearly but still pretends so anyway. see . . . With the vanilla Qwen2.5-VL-7B vSearcher, vReasoner is able to take closer looks at regions of interest and makes less factual errors (see the bottom part of Figure 17 and the middle part of Figure 18-19). However, the vanilla vSearcher is often unreliable, returning inaccurate/wrong crops to vReasoner or simply concluding that the target is not in the image, usually after minimal amount of (sometimes none) reasoning. In such cases, vReasoner would eventually give up and resort to its own perception after multiple failed visual search attempts, leading to wrong final answers. In comparison, our visual search agent, InSight-o3-vS, would usually first reason about the visual search request and then crop the candidate region to verify before returning it to vReasoner. For example, in Figure 16, InSight-o3-vS first reasons about what the bounding box should cover: Based on the description, the right section of the map includes the legend/index and the \"Catering venues\" list with numbered cafes. The legend/index is located at right of the map, and the \"Catering venues\" list is further down, under the \"Catering venues\" heading. The bounding box should cover these areas. Then, after viewing the cropped region, it concludes: Based on the tool response, the right section of the map showing the legend/index and the \"Catering venues\" list with numbered cafes is already covered by the bounding box provided. Therefore, no further zooming is necessary. In some cases, InSight-o3-vS is able to correct an initial bad crop after reviewing the crop: Based on the response, the zoomed-in area does not match the intended area around grid B4. The bounding box needs to be adjusted to better capture the car parks and nearby cafe icons and labels. The returned crops are usually medium-to-high-quality crops as shown in Figure 16. From these cases, we can see that vSearcher mostly helps vReasoner by (i) precisely locating the regions requested by vReasoner and (ii) presenting them to vReasoner and directing its attention to those regions, thereby reducing hallucination and facilitating evidence-based reasoning. D.4 FAILURE CASES OF INSIGHT-O3 Figure 20-21 show typical failure cases of INSIGHT-O3. In the first three failure cases, vSearcher (InSight-o3-vS) provided the correct crops for the search targets but vReasoner (GPT-5-mini) answers incorrectly due to its own errors, e.g., ignoring visual evidence due to internal knowledge bias, and jumping to conclusion without examining key visual information. This suggests that existing frontier models, even the proprietary ones, are still not very good at thinking with images in complex scenarios. The last failure cases is due to vSearcher failing to understand the structure of the map and/or the relatively complicated region descriptions involving multiple visual cues. Finally, we note that, at least on O3-BENCH, most wrong answers are due to vReasoner, not vSearcher."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Example 1 (MAP case 1) of complete reasoning trace of our model (GPT-5-mini as vReasoner and INSIGHT-O3-VS as vSearcher). The vReasoner iteratively guides vSearcher through natural-language region descriptions. INSIGHT-O3-VS precisely retrieves high-quality image crops that match the described regions, effectively supporting vReasoner in locating the target area and producing the correct answer."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Example 1 (MAP case 1, continuation of Fig. 16): reasoning traces of GPT-5-mini and GPT-5-mini + Qwen2.5-VL-7B-Instruct. GPT-5-mini exhibits misperception and reasoning drift, while Qwen2.5-VL-7B-Instruct frequently fails to follow vReasoners instructions, producing lowquality crops misaligned with the described regions. Consequently, both baselines yield incorrect answers."
        },
        {
            "title": "Preprint",
            "content": "Figure 18: Example 2 (MAP case 2): reasoning traces of our model, GPT-5-mini, and GPT-5-mini + Qwen2.5-VL-7B-Instruct. Our INSIGHT-O3-VS accurately follows vReasoners instructions and returns high-quality crops aligned with the described regions, leading to correct answer. In contrast, Qwen2.5-VL-7B fails to return valid crop in the final reasoning round, resulting in an incorrect answer."
        },
        {
            "title": "Preprint",
            "content": "Figure 19: Example 3 (CHART case): reasoning traces of our model, GPT-5-mini, and GPT-5-mini + Qwen2.5-VL-7B-Instruct. INSIGHT-O3-VS effectively follows vReasoners guidance and retrieves high-quality crops that fully capture the described regions. In contrast, Qwen2.5-VL-7B returns only partial crops in Round 2 and, in the Round 3, fails to produce valid crop as requested by vReasoner, incorrectly concluding that the target region is absent, which leads to an incorrect answer."
        },
        {
            "title": "Preprint",
            "content": "Figure 20: Failure cases 1 & 2 of INSIGHT-O3 (GPT-5-mini + InSight-o3-vS)."
        },
        {
            "title": "Preprint",
            "content": "Figure 21: Failure cases 3 & 4 of INSIGHT-O3 (GPT-5-mini + InSight-o3-vS)."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Performance of Qwen3-VL-32B and INSIGHT-O3 with Qwen3-VL-32B vReasoner. vReasoner vSearcher Qwen3-VL-32B - Qwen3-VL-32B Qwen2.5-VL-7B (before RL) Qwen3-VL-32B Qwen2.5-VL-7B (after RL) V-Bench VisualProbe-Hard O3-Bench 28.6 21.7 36.8 60.4 48.5 61. 86.0 69.1 90.1 D.5 INSIGHT-O3 WITH QWEN3-VL VREASONER Apart from closed-source proprietary models, we also experiment with open models like Qwen3VL (Bai et al., 2025a) as the vReasoner of INSIGHT-O3. As shown in Table 8, INSIGHT-O3 (the last row) outperforms both the base Qwen3-VL-32B and the combination of Qwen3-VL-32B (vReasoner) and Qwen2.5-VL-7B (vSearcher) without RL. Although the advantage of INSIGHT-O3 on O3-BENCH is relatively small, we note that as open models continue to advance in tool use and general reasoning, the performance of INSIGHT-O3 with open vReasoners will further improve. PROMPTS FOR O3-BENCH E.1 PROMPTS FOR INFORMATION EXTRACTION For both the full image and cropped images, we feed them into Qwen2.5-VL-32B (Bai et al., 2025b) for information extraction using the same prompt as below. ### System You are acting as **precise visual information extractor**. Given ONE image, you must (1) identify the image type, (2) write **comprehensive, strictly factual** caption, and (3) extract **complete OCR text** when present (with special handling for tables). Follow the rules **exactly** and return the output in the three sections shown under ** Output Format**. --- ### Global Principles 1) **No hallucinations.** Describe only what is visible. If something is unclear, write [ illegible] or [partially obscured: ...]. 2) **Be exhaustive.** Do not omit small text, legends, tick labels, footnotes, watermarks, axis titles, subtitles, panel labels (e.g., (a), (b)), or figure notes. 3) **Preserve fidelity.** Copy punctuation, capitalization, diacritics, signs (+, -), units, and spacing **exactly**. Do not normalize numbers or rewrite text. 4) **Reading order.** When listing text outside of tables, use **top-to-bottom, left-toright** order. 5) **Language.** OCR text must remain in the **original language(s)**. The caption should be in English unless the visible UI/page language is clearly not English; in that case, keep captions in that language. Do **not** translate OCR unless the image itself contains translation. 6) **No extra sections.** Output **only** the three required sections and nothing else. --- ### Image Type Identification (Section 1) - Classify the image using one or more of the following types (multiple allowed if appropriate): chart, table, document/text page, diagram/flowchart, map, UI/screenshot, form, invoice/receipt, poster/flyer, scientific figure (multi-panel), natural scene, legend, infographic, other (specify). --- ### Detailed Caption (Section 2) Write **dense, structured** caption that covers all critical elements. Use clear, objective language and organize logically (left->right, top->bottom; or foreground-> background). Include the relevant sub-guidelines:"
        },
        {
            "title": "Preprint",
            "content": "**A. Charts / Plots / Scientific Figures** - State the figure title (if present), chart type(s), axes titles, **units**, tick labels, gridlines, data series, markers/line styles, **legend and color/shape mappings**, annotations, error bars, trend lines, and notable extrema/patterns (peaks, troughs, monotonic trends, outliers). - If multi-panel: identify panel labels (a), (b), ... and summarize each panel in order . - Mention any insets, callouts, footnotes, or sources. **B. Tables / Forms / Receipts / Documents** - Summarize what the table/document contains (topics/fields), approximate dimensions (e.g., 12 rows 6 columns), header rows, merged cells, checkboxes, stamps, signatures, page numbers, and footers/footnotes. - Call out key sections (headings, lists, paragraphs), logos, and seals. **C. Maps** - Report title, compass/north arrow, scale/scale bar, coordinate grid or lat/long, boundaries, regions, routes/lines with **color-to-meaning mapping** (from legend), symbols/ icons (e.g., hospitals, stations), labels for places/roads, insets, and any zoning/heat color ramp. - Include legend content (categories and their visual encodings). **D. UI / Screenshots** - App/site name, window title, menus/toolbars, visible controls (buttons, toggles, checkboxes, dropdowns, search fields), selected/disabled states, scroll position, timestamps, status bars, notifications, dialogs, visible file paths, and version strings. **E. Natural / Real-World Scenes** - Enumerate salient objects, text on signs/labels, relative positions (e.g., red sign above the doorway\"), counts for repeated items, conditions (day/night, indoor/outdoor), and visible brands/logos. > Do **not** invent interpretations or causal explanations. Keep to what is visually supported. --- ### OCR Extraction (Section 3) Extract **all visible text**. Follow these rules: **General OCR Rules** - Use **natural reading order** (top->bottom, left->right). - Preserve original line breaks and spacing. - If text is repeated (e.g., in watermark), list it once and note (repeats) if necessary. - If character/word is uncertain, write it as [illegible] or [?] without guessing. **Tables (very important)** - When an area is table, extract it immediately using **Markdown table syntax**. - Preserve the **exact** row/column structure and header rows; if cell has line breaks, use <br> or n. - For merged cells, repeat the visible text in each affected cell and note (merged) once after the table. - If multiple tables exist, label them sequentially as Table 1, Table 2, ... in the order they appear. **Documents / Text Pages / Forms** - Extract headings, paragraphs, lists, captions, footnotes, headers/footers exactly as shown. Maintain indentation and list markers. **Charts / Maps / Diagrams / UI / Natural Scenes** - Extract **all textual elements** present: titles, subtitles, axis labels, tick labels, legend entries, series labels, annotations, callouts, map labels (places/roads/lines), UI labels (menu items, buttons, tooltips), signs, badges, and watermarks. If **no text** is present, write None. --- ### Output Format (return EXACTLY this Markdown structure; no code fences) ## Image Identification <one or more types from the allowed list; add brief justification if mixed> ## Detailed Caption <dense, strictly factual caption covering all visible elements per rules above> ## Extracted OCR (if any)"
        },
        {
            "title": "Preprint",
            "content": "<EITHER: full Markdown tables + remaining text in reading order; OR: all non-table text in reading order; write \"None\" if no text> E.2 PROMPTS FOR CONSTRUCTING O3-BENCH For chart images, we use the following prompt for GPT-5 (OpenAI, 2025a) to automatically generate QA instances. You are an expert assessment-item author who designs rigorous **multi-hop visual-reasoning ** questions to benchmark \"think-with-images\" abilities on **dense diagrams, charts, tables , and schematics**. The goal is to generate items so challenging that todays strongest MLLMs score **<= 50%** without external tools. You will receive: ### INPUTS 1. **GLOBAL_OCR** - OCR or caption text describing the entire image/page. {GLOBAL_OCR} 2. **GLOBAL_CAPTION** - caption text describing the entire image/page. {GLOBAL_CAPTION} 3. **LAYOUTS** - list of cropped regions. Each layout contains: - layout_id - unique numeric ID (for your internal reference only). - caption_or_ocr - OCR or descriptive caption of the cropped region. {LAYOUTS} > **Important constraint:** > In the **question** and **options**, you must use only natural labels/text present in the GLOBAL_OCR or GLOBAL_CAPTION or caption_or_ocr. > **Never** mention layout_id, region, crop, panel, box, or any similar tokens. Layouts are **reference-only** to help you construct questions; they may be cited in the ** explanation** but not in the question or options. --- ## TASK Create **3-5 independent multiple-choice questions** that each requires **>=2 distinct visual hops** across layouts or global text. These must be **multi-step items directly**, not derived from single-step seeds. All facts must be grounded in the provided OCR/captions only. --- ## QUESTION-DESIGN RULES 1. **Multi-hop inference (required)** Each question must integrate information from at least **two different layouts** or from global + local text. Valid patterns include: - Cross-table lookup & join (match category in Layout to code/key in Layout B, then filter by condition). - Table <-> chart alignment (map series/labels from one layout to another). - Diagram <-> table mapping (use schematic labels to query corresponding rows/values). - Temporal alignment (identify when threshold is crossed in one chart, then fetch related info from another). - Proportions/ratios/ranks (compute shares from one region and compare with targets in another). - Exceptions/constraints (apply footnote conditions from one layout before interpreting values elsewhere). 2. **Analytical realism** Situate each question in plausible scenario (finance, science, education, operations, product metrics, etc.) while remaining strictly grounded in the provided OCR/captions. 3. **Difficulty control** Questions should require careful scanning, cross-referencing, and light calculations ( differences, ratios, ranking). Avoid items that can be answered at glance. 4. **Units, scales, rounding** Always follow the units/scales given in the OCR. If rounding is necessary, state the rounding rule in the **explanation**. 5. **Ambiguity guardrails** Ensure exactly **one correct choice** among A-E, unless (\"No right choice\") is correct. Adjust conditions to avoid ties. 6. **Label fidelity** Copy text exactly as provided (case, spelling, diacritics). Never use external/world knowledge."
        },
        {
            "title": "Preprint",
            "content": "--- ## ANSWER-OPTION RULES 1. Provide **exactly six** options, one per line, labeled A. ... F. 2. F. must always be exactly No right choice. 3. Place the correct answer randomly among A.-E.; <=10% of items may have as the correct answer. 4. Distractors must be plausible and drawn from actual text/numbers in the OCR/captions. 5. Make options mutually exclusive no meta-options like \"All of the above\". --- ## EXPLANATION RULES - Provide **step-by-step chain**: Step 1: ..., Step 2: ..., etc. - Explicitly cite which layouts were used as [layout X]. - Show all computations (e.g., \"(132 - 95) / 95 = 0.389 = 38.9% [layout 4]\"). --- ## OUTPUT FORMAT Return **only** the following JSON array - no extra text, no markdown outside the code block, no commentary: json [ {{ \"question\": \"...\", \"options\": \"A. ...nB. ...nC. ...nD. ...nE. ...nF. No right choice\", \"answer\": \"C\", \"explanation\": \"Step 1: ... [layout 2]nStep 2: ... [layout 5]nStep 3: ...\" }}, ... ] For map images, we use the following prompt for GPT-5 (OpenAI, 2025a) to automatically generate QA instances. You are an expert evaluation-item author who designs rigorous **multi-hop visual-reasoning ** questions to benchmark \"think-with-images\" abilities on **maps** (bus/metro networks, terminals, malls, festivals, parks, etc.). Items should be challenging enough that todays strongest MLLMs achieve **<= 50%** accuracy. You will receive: ### INPUTS 1. **GLOBAL_OCR** - OCR or caption text describing the entire map. {GLOBAL_OCR} 2. **GLOBAL_CAPTION** - caption text describing the entire map. {GLOBAL_CAPTION} 3. **LAYOUTS** - list of cropped map regions. Each layout contains: - layout_id - unique numeric ID (for your internal reference only). - caption_or_ocr - OCR or descriptive caption of the cropped region. {LAYOUTS} > **Hard requirement:** In the **question** and **options**, you must use only natural labels/text found in GLOBAL_OCR or GLOBAL_CAPTION or caption_or_ocr. > Do **not** mention layout_id, region, crop, box, panel, or similar. Layouts are **reference-only** for your reasoning; you may cite them in the **explanation**, but never in the question or options. --- ## TASK Generate **3-5 independent multiple-choice questions**. Each question must require **>=2 distinct reasoning hops** that combine information across different layouts or between global and local OCR. All facts must be image-grounded. --- ## 1) QUESTION-DESIGN RULES 1. **Multi-hop reasoning (mandatory).** Examples of valid hops: - Legend <-> line color <-> stop/zone. - Grid index <-> label <-> adjacency. - Level/floor marker <-> facility <-> inset. - Route <-> timetable <-> destination. - Symbol <-> restriction <-> path feasibility."
        },
        {
            "title": "Preprint",
            "content": "- Distance/scale <-> number of segments <-> travel time. 2. **Image/OCR grounded only.** Do not use external/world knowledge. 3. **Diversity.** Vary question styles (routing, conditional reachability, transfer logic, adjacency/containment, count/compare, scale-based). 4. **Difficulty target.** Avoid \"at-a-glance\" items. Require cross-checking, counting, or lightweight calculation. 5. **Label fidelity.** Copy map labels exactly (case, spelling, diacritics). 6. **Uniqueness.** Ensure exactly **one correct answer** among A-E, unless (\"No right choice\") is deliberately correct. 7. **Units & scale.** If computing length/time/segments, use the maps own scales, symbols, or counts. --- ## 2) ANSWER-OPTION RULES 1. Provide **exactly six** options labeled A. ... F.. 2. F. must always be No right choice. 3. Normally, the correct answer is among A-E; only rarely (<10%) should be correct. 4. Distractors must be plausible, drawn from real map text/numbers, and mutually exclusive. 5. No meta-options (\"All of the above\"). --- ## 3) EXPLANATION RULES - Provide **step-by-step reasoning chain**. - Explicitly cite layouts used as [layout X]. - Make hops and computations explicit (e.g., \"Count 5 stops along Red Line [layout 3] and compare to 4 stops in Zone [layout 5]\"). --- ## 4) OUTPUT FORMAT Return **only** the following JSON array-no extra commentary or markdown: json [ {{ \"question\": \"...\", \"options\": \"A. ...nB. ...nC. ...nD. ...nE. ...nF. No right choice\", \"answer\": \"B\", \"explanation\": \"Step 1: ... [layout 1]nStep 2: ... [layout 4]nStep 3: ...\" }}, ... ] E.3 PROMPTS FOR EVALUATION For proprietary models/systems, we adopt the following thinking prompt (Guo et al., 2025a) as their system prompt. The prompt is used across all benchmarks except O3-Bench. conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within < think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. On O3-Bench, we use the models default system prompts as they usually lead to better performance. On other benchmarks, the performance difference is negligible (mostly within 1%) except only that GPT-5-nano performs much worse on VisualProbe-Hard when the default system prompt is used (accuracy dropping from 21.7% to 16.0%). For open models, e.g., InternVL3.5 (Wang et al., 2025e), Qwen2.5-VL (Bai et al., 2025b), and Qwen3-VL models (Bai et al., 2025a), we use their default system prompts since they are relatively sensitive to the system prompt. We note that the thinking prompt above often leads to suboptimal performance of Qwen3-VL models."
        },
        {
            "title": "Preprint",
            "content": "F PROMPTS FOR INSIGHT-O3 F.1 VREASONER PROMPTS We use the following prompts for vReasoner during training. [SYSTEM] You are visual assistant. Your goal is to answer question based on an image. First, think step by step to identify which visual facts you need from the image to answer the question. If the visual information is insufficient or unclear, call the visual search tool by providing concise region description: <tool_call>region_description={...}</tool_call> The tool will search the image and return cropped view of the target region. You may repeat this process until you have enough evidence to answer confidently. The tool is not always precise -- evaluate its output critically. If it looks incorrect or off-target, refine your description and try again. Region description guidance: - Use concise, visually grounded targets (e.g., chart, an object, text block, distinct area) - Optionally include approximate location (e.g., top-left, bottom-right, center) - Avoid non-visual or ordinal references (e.g., \"the third largest bar\", \"the second rows number\") - Describe only one region per tool call; do not request multiple regions in single description Output format: - Put your reasoning process inside <think>...</think>. - Immediately after </think>, output your assessment of the most recent tool result (if any ) formatted as <tool_feedback>helpful/unhelpful</tool_feedback>. This should indicate whether the result returned by the previous tool call is relevant to your prior region description and helpful to answering the question. If it misses the key information you are looking for, it is unhelpful. If no previous tool result exists ( e.g., the first turn), output <tool_feedback>NA</tool_feedback>. - Immediately after </tool_feedback>, do exactly one of: 1) Call the tool; or 2) Provide the final answer (no tool call) -- include the result in boxed{...}. Do not mix tool calls and answers in the same turn. - If you need to call the tool, provide the region description using the exact format < tool_call>region_description={...}</tool_call>. You must strictly follow the output format, otherwise your answer will be judged as wrong. multi-turn format example: Assistant: <think>{your step-by-step analysis; decide if more detail is needed}</think> <tool_feedback>NA</tool_feedback> <tool_call>region_description={concise, visually grounded target (optionally with location) }</tool_call> User: [Zoomed-in image + guidance (e.g., \"Based on your description, here is the zoomed-in image. Please continue your analysis; you may call the tool again or provide your final answer if sufficient.\")] Assistant: <think>{updated analysis based on the zoomed-in view; decide whether to refine or answer}</ think> <tool_feedback>unhelpful</tool_feedback> <tool_call>region_description={next concise target (optionally with location)}</tool_call> (Repeat the User -> Assistant pattern as needed until enough evidence is gathered.) Assistant (final turn): <think>{final reasoning; explain why the available visual evidence is sufficient}</think> <tool_feedback>helpful</tool_feedback> Answer: boxed{...} [USER] {question}<image> [ASSISTANT] ... [USER] Based on your description, here is the zoomed-in image."
        },
        {
            "title": "Preprint",
            "content": "Please continue your analysis. After the analysis, state your assessment of the previous tool result using <tool_feedback>helpful/unhelpful</tool_feedback>, then do one of the following: - Call the tool again if you believe more visual detail is needed; or - Provide your final answer if the current information is sufficient. <image> [ASSISTANT] ... [USER] Based on your description, here is the zoomed-in image. You have reached the limit for using the visual tool and cannot call it again. In this turn, after reasoning step by step, output your assessment of the previous tool result using exactly <tool_feedback>helpful/unhelpful</tool_feedback>, and then, based on the available information, provide your final answer using the required format. <image> [ASSISTANT] ... In case that vSearcher is unable to find region that matches the region description provided by vReasoner, we use the following user prompt to notify vReasoner. [USER] The visual searcher could not locate the requested target in the image based on your description. Please adjust or refine your region description (for example, refer to larger, clearly visible area) and continue your analysis. Think first, then state your assessment of the previous tool result using <tool_feedback>helpful/unhelpful</tool_feedback>. Finally, do exactly one of the following: - Call the tool again with revised description; or - Provide your final answer if the current information is sufficient. Occasionally, vReasoner may fail to follow the format instructions. When this happens, we use the following user prompt to ask vReasoner to generate new response: [USER] In your previous response, neither tool call nor final boxed answer was detected, or you didnt output your assessment of the previous tool result in the correct format. Think first, and then include your assessment of the previous tool result using exactly < tool_feedback>helpful/unhelpful</tool_feedback> (or <tool_feedback>NA</tool_feedback> if there is no previous result). Finally, do exactly one of the following: - If you still need more visual detail, call the tool using the exact format: <tool_call>region_description={...}</tool_call> - Otherwise, provide the final answer now and include the result in boxed{...}. During inference, we do not ask vReasoner to provide any feedback to the tool. The prompts are slightly simplified as follows: [SYSTEM] You are visual assistant. Your goal is to answer question based on an image. First, think step by step to identify which visual facts you need from the image to answer the question. If the visual information is insufficient or unclear, call the visual search tool by providing concise region description: <tool_call> region_description={...} </tool_call> The tool will search the image and return cropped view of the target region. You may repeat this process until you have enough evidence to answer confidently. The tool is not always precise -- evaluate its output critically. If it looks incorrect or off-target, refine your description and try again. Region description guidance: - Use concise, visually grounded targets (e.g., chart, an object, text block, distinct area) - Optionally include approximate location (e.g., top-left, bottom-right, center) - Avoid non-visual or ordinal references (e.g., \"the third largest bar\", \"the second rows number\")"
        },
        {
            "title": "Preprint",
            "content": "- Describe only one region per tool call; do not request multiple regions in single description Output format: - Put your reasoning process inside <think>...</think>. - When you need to call the tool, you need to provide the region description using the format <tool_call>region_description={...}</tool_call>. - Immediately after each </think>, do exactly one of: 1) Call the tool; or 2) Provide the final answer (no tool call) -- include the result in boxed{...}. Do not mix tool calls and answers in the same turn. You must strictly follow the output format, otherwise your answer will be judged as wrong. multi-turn format example: Assistant: <think>{your step-by-step analysis; decide if more detail is needed}</think> <tool_call> region_description={concise, visually grounded target (optionally with location )} </tool_call> User: [Zoomed-in image + guidance (e.g., \"Based on your description, here is the zoomed-in image. Please continue your analysis; you may call the tool again or provide your final answer if sufficient.\")] Assistant: <think>{updated analysis based on the zoomed-in view; decide whether to refine or answer}</ think> <tool_call> region_description={next concise target (optionally with location)} </tool_call > (Repeat the User -> Assistant pattern as needed until enough evidence is gathered.) Assistant (final turn): <think>{final reasoning; explain why the available visual evidence is sufficient}</think> Answer: boxed{...} [USER] {question}<image> [ASSISTANT] ... [USER] Based on your description, here is the zoomed-in image. Please continue your analysis. You may: - Call the tool again if you believe more visual detail is needed; or - Provide your final answer if the current information is sufficient. <image> [ASSISTANT] ... [USER] Based on your description, here is the zoomed-in image. You have reached the limit for using the visual tool and cannot call it again. In this turn, based on the available information, provide your final answer using the required format. <image> [ASSISTANT] ... When vSearcher cant find the target region, we use the following user prompt to notify vReasoner: [USER] The visual searcher could not locate the requested target in the image based on your description. Please adjust or refine your region description (for example, refer to larger, clearly visible area) and continue your analysis. You may: - Call the tool again with revised description; or - Provide your final answer if the current information is sufficient. When vReasoner fails to follow the format instruction described in the system prompt, we the following user prompt to ask vReasoner to generate new response:"
        },
        {
            "title": "Preprint",
            "content": "[USER] In your previous response, neither tool call nor final boxed answer was detected. Please do exactly one of the following: - If you still need more visual detail, call the tool using the exact format: <tool_call>region_description={...}</tool_call> - Otherwise, provide the final answer now and include the result in boxed{...}. F.2 VSEARCHER PROMPTS We use the following prompts for vSearcher during both training and evaluation after training. The prompts are adapted from DeepEyes (Zheng et al., 2025). For the last turn, we notify vSearcher in the user prompt that it has reached tool call limit. [SYSTEM] You are helpful assistant. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\":\"function\",\"function\":{\"name\":\"image_zoom_in_tool\",\"description\":\"Zoom in on specific region of an image by cropping it based on bounding box (bbox) and an optional object label.\",\"parameters\":{\"type\":\"object\",\"properties\":{\"bbox_2d\":{\"type\":\"array\",\"items \":{\"type\":\"number\"},\"minItems\":4,\"maxItems\":4,\"description\":\"The bounding box of the region to zoom in, as [x1, y1, x2, y2], where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.\"},\"label\":{\"type\":\"string\",\"description\":\"The name or label of the object in the specified bounding box (optional).\"}},\"required\":[\"bbox\"]}}} </tools> # How to call tool Return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> Example: <tool_call> {\"name\": \"image_zoom_in_tool\", \"arguments\": {\"bbox_2d\": [10, 20, 100, 200], \"label\": \"the apple on the desk\"}} </tool_call> [USER] <image> Locate {target}. Think first, call image_zoom_in_tool if needed, then answer with the bbox coordinates in [ x1, y1, x2, y2] format (or [0, 0, 0, 0] if you cant locate it). Format strictly as: < think>...</think> answer> (otherwise) <tool_call>...</tool_call> (if tools needed) <answer>[x1, y1, x2, y2]</ [ASSISTANT] ... [USER] <tool_response><image></tool_response> Think first, call image_zoom_in_tool if needed, then answer with the bbox coordinates in [ x1, y1, x2, y2] format (or [0, 0, 0, 0] if you cant locate it). Format strictly as: < think>...</think> answer> (otherwise) <tool_call>...</tool_call> (if tools needed) <answer>[x1, y1, x2, y2]</ [ASSISTANT] ... [USER] <tool_response><image></tool_response> You have reached the tool call limit and cannot call tools anymore. Think first, call image_zoom_in_tool if needed, then answer with the bbox coordinates in [ x1, y1, x2, y2] format (or [0, 0, 0, 0] if you cant locate it). Format strictly as: < think>...</think> <answer>[x1, y1, x2, y2]</answer> [ASSISTANT] ..."
        },
        {
            "title": "Preprint",
            "content": "Without RL, Qwen2.5-VL-7B has difficulty following the instructions given in the above prompts. So, for evaluating vReasoner + Qwen2.5-VL-7B, we use the following simple prompt: [SYSTEM] You are helpful assistant. [USER] Given an image and region description, locate the region that best matches the description and output its bounding box coordinates as [x_min, y_min, x_max, y_max]. If the target cannot be found, output [0, 0, 0, 0]. Region description: {target} Now, output the coordinates in format [x_min, y_min, x_max, y_max]: <image> [ASSISTANT] ... F.3 PROMPTS FOR ANSWER VERIFICATION For answer verification, we use GPT-5-nano (OpenAI, 2025a) and feed it with the following prompt. You are given an image-based question, the ground truth (GT) answer, and models answer. Compare the models answer with the GT answer: - If the models answer matches the GT answer visually or semantically, reply with <correct >. - If it doesnt match, or if uncertain, reply with <wrong>. Only reply with <correct> or <wrong>, no explanations. Question: {question} GT Answer: {gt_answer} Model Answer: {model_answer}"
        },
        {
            "title": "G EVALUATION BENCHMARKS",
            "content": "G.1 NATURAL-IMAGE BENCHMARKS V-Bench (Wu & Xie, 2024) is designed to test models ability to attend to high-resolution, detail-rich images. V-Bench consists of 191 challenging natural images (sourced from the SA1B Segment Anything dataset (Kirillov et al., 2023)) and focuses on two fine-grained visual tasks: attribute recognition (identifying specific object attributes like color or material) and spatial relationship reasoning (determining the relative positions of objects). By requiring accurate visual grounding of small details, V-Bench exposes the limitations of models that rely on coarse image understanding. Tree-Bench (Wang et al., 2025a), like V-Bench, uses high-quality, object-dense natural images (drawn from the SA-1B dataset) to evaluate fine-grained visual reasoning. However, Tree-Bench places additional emphasis on traceable evidence and complex reasoning. Each of its 405 visual question-answer pairs is annotated with bounding-box evidence for the correct answer, and many questions require second-order reasoning about object interactions or spatial hierarchies rather than simple identification. VisualProbe (Lai et al., 2025) pushes visual reasoning to an even harder regime. It features high-resolution images with very small target objects and many distractors, making it super challenging and necessitating iterative, trial-and-error search by the model. VisualProbe is organized into easy, medium, and hard subsets; VisualProbe-Hard denotes the toughest set of questions (106 in total) that often cannot be solved in single glance. Compared to V-Bench and Tree-Bench, VisualProbe-Hard scenarios demand an active visual search strategy: the model may need to zoom in on different regions or sequentially explore the image to find relevant details."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Comparison of O3-BENCH and related benchmarks. Benchmark # of QAs Image domains V-Bench Tree-Bench VisualProbeHard HR-Bench4K 191 405 106 200 MME-RealWorld 29K O3-Bench 345 natural (100%) natural (100%) natural (100%) natural (89%), chart (5%), map (6%) natural (>60%), chart (25%), map (6%), etc. chart (47%), map (53%) Layout/target boxes Detailed explanations Multi-hop Avg. resolution height) (width 2246 2152 4944 1583 1615 3980 3503 2708 1844 3967 Roughly 200 distinct QA pairs. The original paper reported 800 but most are the same questions with scrambled options. Table 10: Comparison of O3-BENCH and related benchmarks on chart & map. Benchmark Avg. resolution height) (width HR-Bench4K MME-RealWorld O3-Bench 4032 1875 4602 Based on 500 random samples. 2509 1269 3967 Avg. acc. of GPT-5-mini Avg. # of vSearch steps 79.6 83.8 39.0 2.3 1.0 2.9 G.2 MIXED BENCHMARKS HR-Bench (Wang et al., 2025f) is benchmark deliberately designed to evaluate models on ultra high-resolution images (up to 4K-8K pixels). It addresses key limitation of prior multimodal tests (which max out at 2K resolution) by presenting tasks that cannot be solved with downsampled images. HR-Bench is split into two sub-task categories: (1) Fine-grained Single-instance Perception (FSP), with tasks like identifying detailed attributes of single object, reading text via OCR, or responding to visual prompts on an image; and (2) Fine-grained Cross-instance Perception (FCP), which includes more complex multi-object challenges such as analyzing maps, interpreting charts/graphs, and assessing spatial relationships among multiple items. Each sub-task contains 100 queries on 8K-resolution images (with downsampled 4K version also provided for efficiency). 2000 MME-RealWorld (Zhang et al., 2024b) is large-scale, comprehensive benchmark that evaluates models across wide spectrum of real-world visual tasks. It comprises 13,366 high-quality images (average 1500 resolution) and 29,429 QA pairs, spanning 43 distinct task types grouped into five real-world scenarios, curated from various datasets (Agustsson & Timofte, 2017; Liu et al., 2020; Zhang et al., 2021; Yang et al., 2023; Li et al., 2022; Sun et al., 2022; Sachdeva et al., 2024; Zhu et al., 2021; Jia et al., 2021). These scenarios cover diverse applications such as autonomous driving (e.g., interpreting traffic scenes), video surveillance (e.g., counting vehicles in an overhead street video), remote sensing (e.g., identifying and counting tiny objects in satellite maps), sports and entertainment (e.g., reading scoreboard in broadcast image), and others. MME-RealWorld is notable as the largest fully human-annotated multimodal benchmark to date. Given the benchmarks scale, we employ the official MME-RealWorld lite version3 for efficiency, which uses subset of 50 samples per task to speed up evaluation without significantly altering the task distribution."
        },
        {
            "title": "Preprint",
            "content": "G.3 COMPARISON OF O3-BENCH AND RELATED BENCHMARKS 3967 of ours vs 1875 In Table 9 and Table 10, we compare O3-BENCH with related benchmarks and highlight their key differences. Compared with the most closely related MME-RealWorld, we note that on the overlapping domains (i.e. chart & map): (i) the average image resolution of O3-BENCH is significantly higher (4602 1269 of MME-RealWorld); (ii) the average accuracy of GPT5-mini on O3-BENCH is much lower (39.0% of ours vs 83.8% of MME-RealWorld); and (iii) the average number of visual search steps produced by InSight-o3 for O3-BENCH is 2.9 that of MMERealWorld. In addition, O3-BENCH provides layout boxes and detailed explanations for each QA pair, while most of the benchmarks do not. The explanations can help the community easily verify the correctness of the answers. These differences show that O3-BENCH is of exceptional quality and much harder to solve than the other related benchmarks. Apart from quality, the scale of O3-BENCH is on par with most of the fine-grained perception multimodal understanding benchmarks, e.g., V-Bench (191), Tree-Bench (405), VisualProbe-Hard (106), HR-Bench-4K (200), and commonly-used benchmarks in other multimodal research areas; to list few: Math: MathVision test-mini (304) (Wang et al., 2024), MathVista test-mini (1K) (Lu et al., 2023); VQA: RealWorldQA4 (765); Embodiment/Spatial Understanding: ERQA (400) (Abeyruwan et al., 2025), RefSpatial-Bench (277) (Zhou et al., 2025); Agent: MIA Bench (400) (Qian et al., 2024), OSWorld (389) (Xie et al., 2024), AndroidWorld (116) (Rawles et al., 2024); Fine-grained Perception: V-Bench. These benchmarks were used to evaluate the performance of Qwen3-VL (Bai et al., 2025a). As with O3-BENCH, these benchmarks are relatively small mainly because of the difficulty in data collection. Nevertheless, they have served the timely purpose of evaluating frontier models in the respective fast-developing areas. O3-BENCH only focuses on composite charts and digital maps for two main reasons. First, they are representative of most use cases of thinking with images in the digital domain (as opposed to the natural domain). More specifically, composite charts represent structured images (with clear delineations between different layout regions) and often require more abstract reasoning (e.g., computing the difference of two quantities). On the other hand, digital maps represent images with less structure and more organic layouts. They usually require more visual reasoning (e.g., finding the shortest route from location to B). Together, we argue that O3-BENCH is generally sufficient for evaluating the thinking-with-image capability of current multimodal models in the digital domain. As for the natural domain, there are already number of high-quality benchmarks for thinking with natural images (some are listed in Table 9). O3-BENCH precisely fills the gap of existing benchmarks while striking balance between evaluation efficiency and generality. 3https://huggingface.co/datasets/yifanzhang114/MME-RealWorld-Lite 4https://huggingface.co/datasets/xai-org/RealworldQA"
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Huawei"
    ]
}