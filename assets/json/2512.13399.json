{
    "paper_title": "Differentiable Evolutionary Reinforcement Learning",
    "authors": [
        "Sitao Cheng",
        "Tianle Li",
        "Xuhan Huang",
        "Xunjian Yin",
        "Difan Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 9 9 3 3 1 . 2 1 5 2 : r a"
        },
        {
            "title": "DIFFERENTIABLE EVOLUTIONARY REINFORCEMENT\nLEARNING",
            "content": "Sitao Cheng1, Tianle Li2, Xuhan Huang3, Xunjian Yin4, Difan Zou2 1University of Waterloo 3The Chinese University of Hong Kong, Shenzhen 4Duke University 2The University of Hong Kong sitao.cheng@uwaterloo.ca tianleli@connect.hku.hk xuhanhuang@link.cuhk.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The design of effective reward functions presents central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, Meta-Optimizer evolves reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the meta-gradient of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention. We release our code and model at https://github.com/sitaocheng/DERL. Figure 1: Illustration of DERL and performance of Meta-Reward. Left: Overview of DERL versus traditional approaches. In DERL, Meta-Optimizer generates parameterized Meta-Reward to guide policy model evolution. Crucially, the validation performance serves as feedback signal to update the Meta-Optimizer via policy gradients, establishing differentiable, closed-loop optimization process. DERL eliminates the need for heuristic design or expensive human annotation. Right: Performance comparison of outcome reward, avg reward (i.e., average over atomic primitives) and Meta-Reward. Our Meta-Reward consistently outperforms all baselines in different tasks, demonstrating the effectiveness of DERL. *Equal Contribution"
        },
        {
            "title": "Work in Progress",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "In reinforcement learning (RL), the efficacy of autonomous agents hinges fundamentally on the quality of the reward signalthe critical lens through which an agent perceives its environment and guides learning toward desirable behaviors (Schulman et al., 2017; Shao et al., 2024; Team et al., 2025). However, crafting an optimal reward function remains persistent bottleneck (Skalse et al., 2022). While manual reward engineering can be effective, it is notoriously brittle and prone to reward hacking, where agents exploit specification flaws to maximize scores without achieving the intended goal (Amodei et al., 2016; Yan et al., 2025). In complex reasoning tasks, outcomebased signals (e.g., success/failure binary flags) are often too sparse to drive efficient learning over long horizons. Consequently, the field has leaned heavily on human-in-the-loop paradigms, such as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022), which rely on extensive human annotations to train dense reward model (shown in the top left of Figure 1). Yet, as highlighted by Suttons The Bitter Lesson (Sutton, 2019), strategies dependent on specific human priors are ultimately less scalable than general methods that leverage computation to learn directly from experience. To transcend the scalability limits of human dependency, studies have pivoted toward the automatic optimization of agent configurations, including reward functions, prompts, and other hyperparameters. Early attempts employed genetic algorithms to evolve these configurations via stochastic mutations (Such et al., 2017; Jaderberg et al., 2017b; 2019) or another prompted agent (Zhang et al., 2025a; Novikov et al., 2025). In this context, evolution typically refers to derivative-free optimization where population of agents undergoes perturbations and selection based on fitness. critical limitation, however, is that they are predominantly non-differentiable. These approaches treat the agent configuration as black box, relying on mutations without explicitly capturing the causal relationship between change in the configuration and the resulting shift in agent performance. This inability to learn non-arbitrary structurethe relationship between the reward function and agent performancerenders these methods sample-inefficient and difficult to scale, as they are forced to blindly traverse the optimization landscape without exploiting the intrinsic structural logic that drives improvement. When human experts tune system, they possess an intuitive meta-gradientthe consciousness that modifying specific reward parameter will likely yield specific behavioral shift (Knox & Stone, 2009). We hypothesize that the Large Language Models (LLMs) are able to capture this meta-gradient to update their own parameters, thereby generating progressively better reward functions. To this end, we propose Differentiable Evolutionary Reinforcement Learning (DERL), framework that enables the autonomous discovery of optimal objectives. As shown in the lower left of Figure 1, unlike traditional reward design, DERL is fully differentiable in its meta-optimization: it utilizes feedback from the performance of the policy model to update the weights of MetaOptimizer (a trainable LLM) via policy gradients, rather than just manipulating context. DERL features bi-level evolutionary process: an inner-loop where the policy model evolves based on generated Meta-Reward, and an outer-loop where the Meta-Optimizer itself evolves by learning from the validation performance of the inner policy. While DERL is generalizable to any agent configurations, we focus this work on Reward Modeling, since it represents the critical yet challenging component to optimize, acting as the primary driver of agent behavior. We demonstrate an implementation with Group Relative Policy Optimization (GRPO) (Shao et al., 2024) in Figure 2. The technical realization of DERL addresses two core challenges: defining tractable action space and establishing valid supervisory signal for the Meta-Optimizer. First, instead of generating arbitrary text from scratch, which introduces vast search space, our Meta-Optimizer constructs rewards by composing atomic primitives, like tool-augmented agents (Qin et al., 2023; Huang et al., 2024). These primitives are modular, executable functional blocks that are easy to obtain (e.g., format checkers, partial goal verifiers) that serve as structured search space. This design ensures expressivenessallowing the inclusion of outcome rewards and logical constraintswhile constraining the model to learn structural logic rather than struggling with text parsing. Second, to alleviate human labor, DERL utilizes the validation performance of the inner-loop policy as the direct feedback signal. By observing how different reward structures impact the policys final performance, the Meta-Optimizer approximates the gradient of task success, learning to generate Meta-Rewards with increasingly dense and actionable feedback signals via Reinforcement Learning."
        },
        {
            "title": "Work in Progress",
            "content": "We empirically validate DERL across three distinct domains: Robotic Agents (ALFWorld (Shridhar et al., 2020)), Scientific Simulation (ScienceWorld (Wang et al., 2022)), and Mathematical Reasoning (GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021)). Experimental results demonstrate that DERL not only generalizes effectively across these diverse tasks but also consistently outperforms methods relying on sparse outcome rewards or human-designed heuristics. Notably, DERL achieves new state-of-the-art (SOTA) performance on robotic and scientific benchmarks, showing exceptional robustness in Out-of-Distribution (O.O.D.) scenarios. Further analysis of the evolutionary process reveals that the Meta-Optimizer successfully captures the meta-gradient: as training progresses, the generated Meta-Rewards evolve to encode the intrinsic structure of the tasks, demonstrating self-exploratory capability that aligns with the true gradient of optimization. Our contributions are summarized as follows: We introduce Differentiable Evolutionary Reinforcement Learning (DERL), bi-level optimization framework that automates the discovery of reward functions. Unlike traditional evolutionary methods that treat agent configuration as black box, DERL enables the Meta-Optimizer to capture the gradient between reward structures and task performance, allowing it to update its own parameters to generate increasingly effective Meta-Rewards. We introduce novel Meta-Optimizer architecture that constructs rewards by composing atomic primitivesmodular, executable functionsrather than generating arbitrary text. By utilizing the validation performance of the inner-loop policy as supervisory signal, we formulate reward generation as reinforcement learning problem, eliminating the need for human annotation while ensuring distinct, logical search space. We validate DERL across diverse domains, including robotic agent, scientific simulation, and mathematical reasoning. DERL achieves state-of-the-art performance on robotic and scientific benchmarks (i.e., ALFWorld, ScienceWorld), demonstrating superior robustness in O.O.D. scenarios. Further analysis confirms that our Meta-Optimizer successfully evolves to capture the intrinsic structure of tasks, progressively refining the reward signal without human intervention."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Agentic Evolution of LLMs The capabilities of LLMs have recently shifted towards agentic systems that encompass complex planning, tool usage, and self-correction mechanisms (Wu et al., 2024; Qin et al., 2023). However, the deployment of LLMs as agents is critically bottlenecked by their reliance on human-engineered configurations (e.g., prompts, workflow, codes, reward functions, etc), which are non-scalable and brittle (Sutton, 2019; Sarkar et al., 2025). While evolutionary algorithms attempt to optimize agent configurations, they operate in discrete, black-box manner by permutation (Jaderberg et al., 2017b; Chen et al., 2025; Fang et al., 2025) or prompted agent (Yin et al., 2024; Novikov et al., 2025; Zhang et al., 2025a; Shao et al., 2025), relying only on sparse final fitness scores and failing to exploit the rich information embedded in the training dynamics (Gao et al., 2025). Our DERL introduces parameterized Meta-Optimizer that enables gradient-guided search for optimal configurations. Leveraging the validation performance of the optimizee as reward signal, DERL validates that the meta-gradient can be captured by the evolutionary process, moving beyond human priors toward scalable, computation-driven optimization mechanism. Learning to Learn Learning to learn, a.k.a. meta-learning, focuses on developing models or algorithms that can rapidly adapt to new tasks by leveraging experience gained from other related tasks (Vilalta & Drissi, 2002; Xu et al., 2018). It essentially automates the traditional manual processes, e.g., hyper-parameter optimization, the selection of appropriate learning algorithms (Andrychowicz et al., 2016). Recent work has extended this concept to RL, where meta-learner is designed to optimize the inner-loop learning process of an RL agent (Bello et al., 2017; Agarwal et al., 2019; Xu et al., 2020; Oh et al., 2020; Anonymous, 2025). However, as reward modeling is demanding, whether the optimization of reward signal can be learned by meta-model is understudied. Our DERL is the first to formalize the automated reward search as bi-level meta-optimization problem with LLMs, leveraging principles from meta-learning to optimize meaningful reward function. Reward Modeling The success of LLM alignment hinges on the reward function providing feedback for agent evolution through reinforcement learning (Schulman et al., 2017). This is complicated"
        },
        {
            "title": "Work in Progress",
            "content": "by the dilemma between sparse, objective outcome reward (Shao et al., 2024; Tang et al., 2025) and dense, but expensive, human-annotated reward (such as those used in RLHF) (Wang et al., 2025; Ouyang et al., 2022). Recent studies seek open-ended reward by training model on large-scale LLM-annotated web-crawled data (Ma et al., 2025b; Zhang et al., 2024; Ma et al., 2024). Others design heuristic rewards which requires complex manual coordination and may even degrade performance if naively combined (Zhang et al., 2025b; Wei et al., 2025; Yu et al., 2025; Yan et al., 2025). To address these challenges, our DERL employs Meta-Optimizer to automatically generate reward functions without relying on external human preference data."
        },
        {
            "title": "3 DIFFERENTIABLE EVOLUTION REINFORCEMENT LEARNING",
            "content": "We first introduce the general formulation of the Differentiable Evolutionary Reinforcement Learning (DERL) framework, which models automated reward design as bi-level optimization process. Subsequently, we detail its specific instantiation, focusing on reward parameterization and the algorithms implementation in the inner and outer loops. 3.1 BI-LEVEL EVOLUTIONARY TRAINING To reduce human dependency in RL for Large Language Models, automated reward design has emerged as critical research direction. prevalent methodological paradigm is to treat the target function parameterization as configuration to be optimized via evolutionary search, strategy widely adopted across various automated design tasks (Romera-Paredes et al., 2024). However, conventional implementations of this paradigm primarily rely on genetic algorithms driven by stochastic mutations (Chen et al., 2023; Jaderberg et al., 2017a) or heuristic optimization via prompted agents (Ma et al., 2024; 2025a). Crucially, these approaches function as zero-order optimizers; they are inherently blind to the underlying optimization landscape and fail to capture the metagradient. This limitation results in significant sample inefficiency, analogous to the performance disparity between random grid search and gradient descent in function optimization. Mitigating such inefficiencies necessitates reformulating the discrete evolutionary search over reward configurations into continuous, differentiable optimization process. This transformation enables the capture and utilization of meta-gradients to guide the search. Drawing inspiration from Bello et al. (2017), who employed RL to discover optimization algorithms, we propose DERL: bilevel evolutionary framework. In this framework, the outer level (loop) consists of Meta-Optimizer ψ trained via RL to generate the reward configuration ϕ (which instantiates the reward function Rϕ), while the inner level (loop) optimizes policy model θ under the provided reward function Rϕ. The overall training process is illustrated in Figure 2. Specifically, the bi-level optimization problem is formulated as follows: Inner-loop (Policy Model Optimization) Given reward configuration ϕ generated by the outer loop, the inner-loop policy θ is optimized to maximize the expected parameterized reward: where represents the training dataset and τ denotes the trajectories sampled from the policy model. inner ϕ (θ) = ExD,τ πθ(x)[Rϕ(τ )], (1) Outer-loop (Meta-optimizer Evolution) The objective of the outer loop is to optimize the MetaOptimizer ψ to generate reward configurations that yield high-performing inner policies. Building upon the findings of Bello et al. (2017), who demonstrated that reinforcement learning could effectively discover novel optimization algorithms, we postulate that similar data-driven paradigm can automate the discovery of complex reward functions. Accordingly, we employ RL to train the Meta-Optimizer ψ. Formally, the Meta-Optimizer acts as generator policy πψ(ins), taking the task instruction ins as context and outputting configuration ϕ. This configuration instantiates the Meta-Reward Rϕ used in Equation 1. Once the inner policy converges to an optimal θ under Rϕ, it is evaluated against the performance metric Perf() (e.g., accuracy calculated over ground-truth) on validation set. We utilize this performance score as the outer feedback signal (i.e., the reward) for the Meta-"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 2: Bi-level evolutionary training for DERL. Blue Block: The evolution of Meta-Optimizer ψ with generated Meta-Rewards (i.e., rollouts). Taking fixed task instruction as input, the Meta-Optimizer updates the parameter Φ of the Meta-Reward with the signal from validation performance v. Green Block: The inner-loop training for policy model θi with Meta-Reward Rϕi by GRPO. We evaluate the validation performance for each θi as the reward of Rϕi, making it differentiable signal for the Meta-Optimizer to evolve through reinforcement learning. Optimizer ψ. Consequently, the Meta-Optimizer aims to maximize the following bi-level objective: outer(ψ) = Eϕπψ(ins)[Perf(θ)], (2) s.t. θ = arg max θ inner ϕ (θ). This bi-level training framework transforms the discrete evolutionary search over reward function into continuous, differentiable optimization over the meta-policy parameters ψ. Unlike genetic algorithms that rely on heuristic, stochastic mutations (blindly searching the space), DERL leverages parameterized policy πψ to generate reward configurations. By applying policy gradients to ψ, we effectively learn the search direction, transforming the zero-order reward search into first-order optimization of the generator. 3.2 INSTANTIATION OF DERL In this section, we detail the specific instantiation of the DERL framework, focusing on the symbolic reward parameterization and the algorithmic implementations for both the inner and outer loops. Reward Parameterization central challenge in the evolution of Meta-Optimization lies in defining the specific output configuration (which serves as the reward function for the inner loop) and the corresponding feedback signal used to update the Meta-Optimizer. Traditional reward design methods typically leverage heuristic scalar functions. These are often sparse and necessitate significant manual effort to analyze validation failures (Shao et al., 2024). Alternatively, some approaches employ learned reward models (e.g., LLM-as-a-Judge) to address brittleness and scalability. However, training such models incurs high cost in human annotation. To bridge this gap, we directly parameterize the reward function structure. Instead of predicting scalar reward value, the MetaOptimizer generates reward configuration ϕa symbolic definition that dictates how to evaluate the inner-loop agent. This formulation not only eliminates the need for expensive human annotation to train reward model but also enables the system to automatically evolve complex, non-linear evaluation criteria, thereby significantly reducing human dependency. To ensure structured yet expressive search space, we define the reward function as symbolic composition of atomic primitives = {g1, g2, . . . , gk}. These primitives are functions that evaluate specific aspects of models generation, o. Crucially, many primitives require not only the generation but also context variable, C, which contains information like the input question and ground-truth answer a. Example primitives include checking final outcome correctness (e.g., comparing to a), adherence to formatting rules in C, or process heuristics like step counts. Based on these primitives, the Meta-Optimizer ψ does not predict scalar reward directly. Instead, it predicts the structure and weights (ϕ) that combine these signals through mathematical composition. The"
        },
        {
            "title": "Work in Progress",
            "content": "reward function is therefore defined as: Rϕ(o, C) = Func(g1(o, C), . . . , gk(o, C); ϕ) (3) where Func() represents the symbolic execution of the configuration ϕ, which involves applying weights and mathematical operators (e.g., summation, logical conditions) to the primitive outputs. The merits of design are threefold: 1) Coverage of large and continuous search space for exploration. Equation 3 is super-set of the standard outcome reward and takes any other factors into account, ensuring search space which is more informative than the sparse outcome signal. 2) Structural reasoning on varying aspects. The meta-optimizer can focus on considering various aspects of the problem instead of processing tedious textual output. 3) Extensibility. The framework effectively decouples the definition of atomic signals from their utilization during meta-optimization. This separation renders the system agnostic to the specific choice of G, thereby facilitating seamless generalization to new tasks. One can incorporate diverse array of potential discriminatorsranging from rigorous constraints to task-specific heuristics, or even noisy and potentially detrimental signals. The evolutionary process then acts as an automated filter, identifying the optimal composition and combining these factors without requiring manual verification of their individual efficacy. Inner-loop (Policy Model Optimization) We utilize Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as the algorithm for the inner loop (illustrated in the green block of Figure 2). The objective is defined as: (cid:34) qP (D) i=1πθold {oi}G (q) 1 (cid:88) i=1 min (cid:16) πθ(oi q) πθold (oi q) Ai, clip(cid:0) πθ (oiq) (oiq) , 1 ϵ, 1 + ϵ(cid:1) Ai πθold (cid:17) β DKL(πθπref ) (cid:35) , (4) where, for each question from the training set D, we sample group of outputs {oi}G i=1 from the old policy πθold . The models πθ and πθold are the current and previous policies, respectively. The terms ϵ and β are hyper-parameters for the clipping range and the KL-divergence penalty against reference policy πref , with DKL detailed in Shao et al. (2024). Crucially, Ai represents the groupwise advantage derived from the parameterized reward function within each group. For each output oi in group, generated with respect to given context C, the reward ri is computed. The advantage is then defined as: Ai = ri mean({rj}G std({rj}G j=1) j=1) , where ri = Rϕ(oi, C). (5) To investigate the impact of model plasticity and optimization efficiency, we implement two distinct initialization strategies for the inner loop: 1) Standard Initialization (DERL): In this setting, the policy model θ is re-initialized from the base model at the beginning of each inner loop. This ensures that the performance of θ is solely attributable to the efficacy of the current reward configuration ϕ, providing an unbiased evaluation signal to the Meta-Optimizer. 2) Population-based Variant (DERL-Pop.): In the first inner-loop, we initialize the policy model from scratch. In later innerloop training, we initialize the policy model as the model with the best validation performance from the last loop. This is similar to traditional evolutionary training where the policy model is evolved based on different training configurations (Shao et al., 2025; Jaderberg et al., 2017b), but our MetaOptimizer captures the meta-gradient in differentiable way. Moreover, the bi-level evolutionary training demonstrates dynamic Meta-Reward signal. For fair comparison, we make sure that the total training step for DERL-pop.s inner-loop policy model remains the same as standard DERL. This also saves computation because the outer-loop evolves more frequently. Outer-loop (Meta-optimizer Evolution) We similarly employ GRPO to optimize the outer loop (blue block in Figure 2). In each iteration, the Meta-Optimizer πψ samples group of reward configurations Φ = {ϕ1, ϕ2, . . . , ϕn}, where denotes the rollout size. Each configuration ϕi is used to instantiate reward function Rϕi for training corresponding inner-loop policy θi. Upon completion of the inner-loop training, we evaluate each policy θi on held-out validation set to compute the performance score vi = Perf(θi), using the pass@1 accuracy metric: 1 I(fθ(q) = a), Perf(θ) = (cid:88) (6) (q,a)V"
        },
        {
            "title": "Work in Progress",
            "content": "where fθ(q) denotes the output generated by the trained policy πθ using deterministic decoding, denotes ground truth, and I() denotes indicator function. The resulting validation scores {v1, . . . , vn} serve as the feedback signals (outer rewards) for the corresponding configurations {ϕ1, . . . , ϕn}. We then compute the group-wise advantage (same as the inner loop) to update the Meta-Optimizer parameters ψ. To ensure the stability of this symbolic generation process, we implement three constraints: 1) Cold Start: We apply Supervised Fine-Tuning (SFT) to the Meta-Optimizer on small set of valid format examples to initialize the policy. 2) Constrained Decoding: During generation, we enforce tokenlevel constraints to ensure the validity of tokens in the output ϕ. 3) Validity Penalty: In rare cases where generated ϕi is mathematically ill-defined (e.g., resulting in execution errors), we assign penalty reward vi = 0 to suppress such generations. Crucially, this formulation establishes closed-loop computation graph. By utilizing the validation performance as scalar feedback signal for the generated configuration, we enable the meta-gradient to be estimated and applied to the generator. This effectively propagates the non-differentiable validation signal back to the Meta-Optimizer parameters ψ, allowing the system to learn the optimal search direction in the reward landscape. An illustration of this gradient flow through the bi-level optimization process is provided in Appendix B.1. The differentiable nature of DERL offers profound implications for scalability and automation. Unlike non-differentiable paradigms that often necessitate manual supervision or heuristic signal design to guide the search, our framework establishes direct, gradient-based optimization path between downstream performance and reward configuration. This allows the Meta-Optimizer to be trained directly via autonomous interaction, eliminating the bottleneck of human intervention. Consequently, DERL enables the autonomous synthesis of dense feedback signals from sparse outcome-based signals, effectively resolving the scalability challenge in complex domains where manual reward engineering is infeasible or prohibitively expensive. We empirically verify the effectiveness of this framework in Section 4. Our evaluation starts with proof-of-concept experiment to showcase the frameworks ability to navigate the reward landscape, followed by extensive experiments validating its generalization to various real-world applications."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We validate our DERL framework across three diverse domains that necessitate complex reasoning, i.e., Robotic Agents, Scientific Simulation and Mathematical Reasoning. We introduce benchmarks and baselines in Section 4.1. We instantiate atomic primitives and setups in section 4.2. 4.1 EXPERIMENT SETUPS Robotic Agent We evaluate the effectiveness of DERL on multi-round robotic agent task, i.e., ALFWorld (Shridhar et al., 2020). This benchmark requires learning to complete embodied household tasks using natural language or visual observations. To rigorously assess the generalization capability, inspired by Zhang et al. (2025b), we divide the task into three levels difficulty based on the distribution shift of training and testing data: L0 (in-distribution, seen): trained on all 6 task types and evaluated on seen variants; L1 (in-distribution, unseen): trained on all 6 task types but evaluated on unseen variants; L2 (out-of-distribution), trained only on 4 task types and evaluated on the remaining 2 unseen types. We compare DERL with standard reinforcement learning baselines: 1) GRPO w/ Outcome Reward: GRPO with binary outcome rewards. 2) GRPO w/ Avg Reward: As we introduce the atomic primitives, common practice is to calculate the weighted sum over all functions. To demonstrate DERLs exploration of an optimal reward structure over the search space, we compare our MetaReward with the average weighted sum. 3) GiGPO (Feng et al., 2025), introducing two-level structure for finer-grained credit assignment. 4) RLVMR (Zhang et al., 2025b), with structured verifiable process reward, which is the previous state-of-the-art method. We do not compare with LLM-based reward models due to their high resource intensity, particularly since ground-truth outcome signals are readily available for our target domains."
        },
        {
            "title": "Work in Progress",
            "content": "Table 1: Success rates of DERL and different RL baselines on ALFWorld and ScienceWorld, based on three levels of generalization difficulty (Section 4.1). The base model is Qwen2.5-1.5B-Instruct. The GRPO baselines are run by ourselves. Other baseline results are from Zhang et al. (2025b). Bold and underline denotes the best and second best performance. Our method, DERL, outperforms all baselines in all difficulty levels across all benchmarks, achieving state-of-the-art performance. Method GRPO w/ Outcome Reward GRPO w/ Avg Reward GiGPO RLVMR DERL DERL-pop. ALFWorld ScienceWorld L0 76.6 88.1 86.7 89.1 91.0 91.8 L1 71.1 85.4 83.2 87.9 89.1 88. L2 29.7 30.5 48.0 56.3 65.0 76.4 L0 21.1 37.9 25.8 46.9 47.7 98. L1 13.7 31.3 15.2 34.4 43.0 95.3 L2 10.9 18.0 4.7 26.5 30.1 31. Scientific Simulation For science domain, we adopt ScienceWorld (Wang et al., 2022), an interactive text environment at the level of standard elementary school science curriculum, testing the agents scientific reasoning abilities. To ensure consistent evaluation of generalization, we apply the three levels of difficulty as in the Robotic agent tasks (i.e., L0, L1, and L2). We compare DERL against the same set of baseline methods as for Robotic Agent task. Mathematical Reasoning For math domain, we evaluate DERL on two established benchmarks: GSM8K (Cobbe et al., 2021) for grade-school math and MATH (Hendrycks et al., 2021) for advanced competition-level problems. We vary the training data by using either the MATH training set, which contains more difficult maths problems, or the training data combining the MATH and GSM8k, which contains both easy and hard problems. We benchmark against set of baselines varying in reward structure: 1) Outcome: standard binary outcome-based reward; 2) Outcome + Format: the outcome rewards augmented with format reward; and 3) Avg Reward: the average reward over all individual atomic primitives. 4.2 IMPLEMENTATION DETAILS Robotic Agent and Scientific Simulation We introduce four atomic primitives to construct the search space for Meta-Reward. The first is the binary outcome reward, others are captured from three stages of process reward of the interaction trajectory, inspired by Zhang et al. (2025b). Specifically, we compute the average reward over the first, middle and last third of steps of the interaction trajectory, respectively. This straightforward design incentivizes the model to attend to distinct temporal phases of the task. For instance, given six-step interaction with step-wise reward sequence of [1, 0, 1, 1, 0, 0], the atomic primitives corresponding to the three temporal segments yield values of 0.5, 1, and 0, respectively. We implement DERL with GRPO through the VeRL framework (Sheng et al., 2024). For outerloop, we set number of rollouts to 8. Other hyper-parameters remain the same as default. For innerloop, we employ Qwen2.5-1.5B-Instruct as the base policy with cold start (which is the same as other baselines). We set epoch to 40 for ALFWorld and 80 for ScienceWorld, respectively. After obtaining the optimal Meta-Reward, we train the policy model (from scratch) using this reward function for 100 steps, the same as RLVMR, whereas other baselines are trained for 150 steps. The Meta-Optimizer achieves convergence in approximately ten and five outer-loop iterations for ALFWorld and ScienceWorld, respectively. For DERL-pop., we train the outer-loop for 10 rounds and set the training epochs for inner-loops to 10 on the ALFWorld task. On the ScienceWorld task, we train the outer-loop for 3 rounds and the inner-loop for 33 rounds, to ensure the consistency of the total training epochs for the inner-loop. We finally report the success rate on the test set, i.e., whether the model can ultimately complete the task."
        },
        {
            "title": "Work in Progress",
            "content": "Table 2: Performance comparison on mathematical reasoning benchmarks. We report the accuracy of Qwen-2.5-3B on different training data using different reward functions. Our DERL outperforms all baselines, including the outcome reward, outcome reward with format reward, and reward using the average over atomic primitives. All experiments are run under the same configuration. Reward Function Training Data GSM8k MATH Outcome MATH+GSM8k Outcome + Format MATH+GSM8k MATH+GSM8k Avg Reward MATH Outcome MATH Outcome + Format MATH Avg Reward DERL DERL-pop. DERL DERL-pop. MATH+GSM8k MATH+GSM8k MATH MATH 82.6 86.4 86.5 82.9 83.9 83.6 87.0 87.6 83.2 84.1 58.8 55.9 55.8 59.1 56.8 54.9 60.2 60.2 60.5 60.9 Mathematical Reasoning For mathematical reasoning, we construct reward space with four straightforward atomic signals: 1) Binary outcome reward; 2) Format reward that verifies if the answer is enclosed in boxed{}; 3) Step-by-step reward, identifying whether the output contains CoT tokens like step1, first, etc; 4) Soft outcome reward that credits the presence of the ground truth anywhere in the output, which would benefit when the model indeed knows how to answer but generates in wrong format. For outer-loop, we keep all training configurations the same as other tasks. For inner-loop, we adopt Qwen-2.5-3B as the base policy model. We train for 10 epochs and enforce time limit of 3.5 hours for any inner-loop training. With these settings, the Meta-Optimizer achieves convergence in approximately 8 outer-loop iterations. We then take the optimal Meta-Reward to train the base policy for 15 epochs for fair comparison with all baselines. For DERL-pop., we train each inner-loop for 2 epochs and report the testing result after the seventh outer-loop iteration for fair comparison. We evaluate the exact match of the ground truth answer in the test set. 4.3 RESULTS We evaluate DERL across three domains to address two research questions: 1) Can DERL discover reward functions better than heuristics signals? 2) Does the learned Meta-Reward generalize better to out-of-distribution (O.O.D.) scenarios in complex reasoning tasks? 4.3.1 ROBOTIC AGENT AND SCIENTIFIC SIMULATION Table 1 presents the performance comparison of DERL with all baselines on both the Robotic Agent and Scientific Simulation benchmarks, based on three levels of generalization difficulty. State-of-the-Art Performance. DERL achieves state-of-the-art success rates across all difficulty levels on both benchmarks. This indicates that the Meta-Optimizer effectively explores the function space to discover Meta-Rewards that drive policy improvement beyond standard outcome signals. Notably, our population-based instantiation, DERL-pop., further demonstrates exceptional performance, achieving 91.8% on ALFWorld (L0) and 98.2% on Science World (L0). This shows that by initializing the inner-loop policy with the best-performing model from previous generations, the Meta-Optimizer can effectively adapt the reward signal dynamically as the policy model evolves, creating curriculum-like effect that accelerates convergence and elevates the performance ceiling. Appendix details the training dynamics of DERL-pop. Robustness in Out-of-Distribution (O.O.D.) Scenarios. critical limitation of heuristic rewards is their brittleness under distribution shifts. As shown in the L2 (O.O.D.) columns, standard baselines falter significantly. For instance, while GRPO w/ Avg Reward improves in-distribution (L0) performance by approximately 10% over GRPO w/ Outcome Reward, it fails to translate this gain"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 3: Optimization dynamics on ALFWorld, GSM8k and MATH Benchmark. The plots illustrate the training trajectories of the Meta-Optimizer, where the horizontal axis represents the number of training steps in the outer-loop. The blue and orange lines represent the average validation and testing performance over Meta-Reward (i.e., rollouts), respectively. The results show that DERL optimizes and converges as the training step increases, without overfitting. to the O.O.D. setting (showing only 0.8% improvement). This suggests that the straightforward reward summation encourages overfitting rather than genuine reasoning. In contrast, our DERL substantially improves the O.O.D. robustness, achieving 65.0% and 30.1% success rates on ALFWorld and Science World, respectively. This more than doubles the performance of the outcome reward baseline. Takeaway: The Meta-Reward captures the intrinsic structure of the task, enabling generalization to unseen scenarios where heuristic combinations fail. 4.3.2 MATHEMATICAL REASONING Overcoming the Limits of Static Heuristics. Mathematical reasoning presents unique challenge where the binary outcome reward is already strong, albeit sparse, signal. Table 2 illustrates the performance on GSM8K and MATH benchmark. We observe that naively incorporating auxiliary signals (e.g., GRPO with Outcome + Format or Avg Reward) often degrades performance on the more difficult MATH dataset (dropping from 58.8% to 55.8%), likely due to reward hacking or distraction from the core reasoning path (e.g., prioritizing formatting over correct reasoning). However, our DERL successfully navigates this pitfall. By autonomously optimizing the reward structure without any human effort, DERL outperforms all baseline reward functions, including the strong outcome reward (e.g., 60.2% vs. 58.8% on MATH), with the population-based instantiation DERL-pop. further improving the performance. This demonstrates DERLs ability to navigate the delicate trade-off between signal density and signal fidelity. Takeaway: Even in domains with strong ground-truth signals, DERL discovers non-trivial reward compositions that provide denser feedback without introducing the noise associated with manual heuristic design."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Having demonstrated the empirical superiority of DERL, we now investigate the internal mechanisms driving these improvements. We focus on two key aspects: the optimization dynamics of the outer-loop and the structural evolution of the generated reward functions. 5.1 OPTIMIZATION DYNAMICS critical question is whether the Meta-Optimizer genuinely learns progressive optimization strategy or merely performs random search over the function space. To investigate this, we visualize the training trajectories (i.e., how the Meta-Optimizer evolves over training steps) on ALFWorld, GSM8K and MATH benchmarks in Figure 3. We do not analyze on ScienceWorld because the Meta-Optimizer converges faster in this task. The horizontal axis represents the outerloop update steps, while the vertical axis represents the performance over different tasks. Specifically, the performance is defined as the average validation accuracy of the inner-loop policies Θ = {θ1, θ2, ..., θn} trained with generated Meta-Reward."
        },
        {
            "title": "Work in Progress",
            "content": "We observe consistent, monotonic upward trend in both the average training and testing accuracy of the inner-loop policies as the outer-loop steps progress. Specifically, we find that the trend in mathematical reasoning is more stable as the Meta-Optimizer recognizes that such verifiable task is mainly driven by the outcome reward. For agent tasks, outer-loop optimization involves more exploration than exploitation, ultimately enabling robust evolution. Crucially, the concurrent rise in training and validation performance provides empirical verification that the Meta-Optimizer is not overfitting to the specific instances in the outer-loop training set. Instead, it successfully approximates the meta-gradient of task success. By leveraging the validation performance as supervisory signal, the Meta-Optimizer iteratively refines the reward policy, generating increasingly effective feedback signals that guide the inner-loop agent toward higher performance."
        },
        {
            "title": "5.2 EVOLUTION DYNAMICS OF REWARD STRUCTURES",
            "content": "To elucidate the specific characteristics of the learned rewards, we analyze the structural composition of the Meta-Reward generated throughout the evolutionary process, i.e., the evolution dynamics. We categorize the generated combinations of atomic primitives into three distinct structural types based on their mathematical properties. We denote the atomic primitives as g1, g2, g3, and g4. We define Stable Structures as those that adopt linear combinations or normalization mechanisms. g1 For example, linear additions (e.g. 0.5 g1 + 0.8 g2) or division operations (e.g., g2+1 ) that act similarly to sigmoid functions bound the output range. These structures mirror robust designs in deep learning, preventing numerical explosion while retaining sufficient expressivity to guide the agent. Conversely, we identify Unstable Structures, which predominantly feature unbounded products without normalization. typical example is chain of sequential multiplications (e.g. g1 (g2 + 0.2) g3). Such structures create severe veto mechanism: if any single atomic signal approaches zero, the entire reward vanishes, leading to high variance and unstable gradient update. Finally, Invalid Structures refer to mathematically adversarial forms, such as assigning negative coefficients to positive signals (e.g., (g1 + 0.5 g2)), which penalize desirable behaviors and offer no optimization utility. Figure 4 tracks the distribution of these structural categories over the course of training on ALFWorld. The evolutionary trajectory reveals compelling natural selection phenomenon driven by the Meta-Gradient. In the early exploration phase, Unstable Structures appear frequently as the optimizer explores the search space. However, as training progresses, we observe sharp decline in their prevalence. Simultaneously, the proportion of Stable Structures exhibits strong upward trend, eventually becoming the dominant form. This dynamic suggests that the Meta-Optimizer effectively acts as an evolutionary filter for mathematical robustness. Without explicit human programming or constraints, our DERL implicitly discovers that consistent, bounded, and numerically stable rewards are prerequisite for effective policy optimization, favoring these structures to maximize the long-term validation performance of the agent. Figure 4: Evolution dynamics of reward structures on ALFWorld. We visualize the proportion of Stable Structures and Unstable Structures over outerloop steps. The consistent upward trend of stable structures demonstrates the Meta-Optimizers selection preference for mathematical robustness."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced Differentiable Evolutionary Reinforcement Learning (DERL), bi-level evolutionary training framework that automates the discovery of reward functions for autonomous agents. By parameterizing the reward structure as composition of atomic primitives and treating the validation performance of the inner-loop policy as supervisory signal, DERL bridges the gap between black-box evolutionary heuristics and gradient-based optimization. Our approach enables the Meta-Optimizer to capture the meta-gradient of task success, allowing it to progressively learn"
        },
        {
            "title": "Work in Progress",
            "content": "dense, actionable feedback signals without reliance on expensive human annotations or sparse outcome flags. We validate DERL across three diverse domains: robotic agent, scientific simulation, and mathematical reasoning. Empirical results demonstrate that our DERL consistently outperforms standard reinforcement learning baselines and human-designed heuristics. Most notably, DERL exhibits superior generalization capabilities in out-of-distribution (O.O.D.) scenarios, achieving state-of-theart performance on the ALFWorld and Science World benchmarks. Furthermore, our analysis of the evolution dynamics reveals that the Meta-Optimizer naturally converges toward numerically stable and robust reward structures, effectively filtering out volatile signal combinations. This confirms that DERL is not merely search algorithm, but mechanism for discovering the intrinsic structural logic required for effective agent training."
        },
        {
            "title": "LIMITATIONS AND FUTURE WORK",
            "content": "While DERL demonstrates significant promise, several limitations remain that outline important directions for future research. Computational Cost. The primary bottleneck of our framework is the computational expense associated with the bi-level optimization structure. Since every update to the Meta-Optimizer requires the training of multiple inner-loop policies (rollouts), the process is resource-intensive compared to standard single-level RL. detailed cost analysis is conducted in Appendix C. We already provided DERL-pop. with higher efficiency and better performance. Future work could explore the integration of lightweight proxy tasks or more sample-efficient outer-loop algorithms (e.g., REINFORCE++) to approximate the meta-gradient with reduced compute. Dependency on Atomic Primitives. The expressivity of the discovered reward functions is currently bounded by the set of atomic primitives defined in the search space. While our selection of primitives (e.g., format checks, partial goal verifiers) are proved effective for the studied domains, the Meta-Optimizer cannot invent entirely new functional capabilities outside of this predefined grammar. Expanding the search space to include more granular or semantically rich primitivespotentially extracted automatically from task descriptionsremains an open challenge. Long-Horizon Credit Assignment. Although DERL improves upon sparse outcome rewards, the generated Meta-Rewards are still evaluated based on the final validation performance of the policy. In tasks with extremely long horizons or deceptive intermediate goals, the signal propagation from the final metric back to the specific reward parameters may still suffer from attenuation. Investigating intermediate meta-supervision signals could further enhance the stability and efficiency of the evolutionary process."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In International conference on machine learning, pp. 130140. PMLR, 2019. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent, 2016. URL https://arxiv.org/abs/1606.04474. Anonymous. Temperature as meta-policy: Adaptive temperature in LLM reinforcement learning. In Submitted to The Fourteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=AoTHU2OmS6. under review. Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search with reinforcement learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,"
        },
        {
            "title": "Work in Progress",
            "content": "volume 70 of Proceedings of Machine Learning Research, pp. 459468. PMLR, 2017. URL http://proceedings.mlr.press/v70/bello17a.html. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, SymXuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. bolic discovery of optimization algorithms. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 9a39b4925e35cf447ccba8757137d84f-Abstract-Conference.html. Xingwu Chen, Tianle Li, and Difan Zou. Reshaping reasoning in llms: theoretical analysis of rl training dynamics through pattern selection, 2025. URL https://arxiv.org/abs/2506. 04695. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407, 2025. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, and Yuzhong Qu. QueryAgent: reliable and efficient reasoning framework with environmental feedback based self-correction. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50145035, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.274. URL https://aclanthology.org/2024. acl-long.274/. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks, 2017a. URL https:// arxiv.org/abs/1711.09846. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017b. Max Jaderberg, Wojciech Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil Rabinowitz, Ari Morcos, Avraham Ruderman, et al. Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859865, 2019."
        },
        {
            "title": "Work in Progress",
            "content": "W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The tamer framework. In Proceedings of the fifth international conference on Knowledge capture, pp. 916, 2009. Michel Ma, Takuma Seno, Kaushik Subramanian, Peter R. Wurman, Peter Stone, and Craig Sherstan. Automated reward design for gran turismo, 2025a. URL https://arxiv.org/abs/ 2511.02094. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025b. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward deIn The Twelfth International Conference on Learning sign via coding large language models. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=IEduRUO55F. Alexander Novikov, Ngˆan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. Junhyuk Oh, Matteo Hessel, Wojciech Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. Advances in Neural Information Processing Systems, 33:10601070, 2020. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nat., 625(7995):468475, 2024. doi: 10.1038/ S41586-023-06924-6. URL https://doi.org/10.1038/s41586-023-06924-6. Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio Leon Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, et al. Evolution strategies at the hyperscale. arXiv preprint arXiv:2511.16652, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel Finlayson, David Sontag, et al. Dr tulu: Reinforcement learning with evolving rubrics for deep research. arXiv preprint arXiv:2511.19399, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020."
        },
        {
            "title": "Work in Progress",
            "content": "Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017. Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, and Benyou Wang. Calm before the storm: Unlocking native reasoning for optimization modeling, 2025. URL https: //arxiv.org/abs/2510.04204. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Ricardo Vilalta and Youssef Drissi. perspective view and survey of meta-learning. Artificial intelligence review, 18(2):7795, 2002. Ruoyao Wang, Peter Jansen, Marc-Alexandre Cˆote, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader? arXiv preprint arXiv:2203.07540, 2022. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571. Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, et al. Truthrl: Incentivizing truthful llms via reinforcement learning. arXiv preprint arXiv:2509.25760, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, 2024. Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. Advances in neural information processing systems, 31, 2018. Zhongwen Xu, Hado van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and David Silver. Meta-gradient reinforcement learning with an objective discovered online. Advances in Neural Information Processing Systems, 33:1525415264, 2020. Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, and Jie Fu. Re:form reducing human priors in scalable formal software verification with rl in llms: preliminary study on dafny, 2025. URL https://arxiv.org/abs/2507.16331. Xunjian Yin, Xinyi Wang, Liangming Pan, Li Lin, Xiaojun Wan, and William Yang Wang. odel agent: self-referential agent framework for recursive self-improvement. arXiv preprint arXiv:2410.04444, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Openended evolution of self-improving agents. arXiv preprint arXiv:2505.22954, 2025a. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh arXiv preprint Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv:2408.15240, 2024."
        },
        {
            "title": "Work in Progress",
            "content": "Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844, 2025b."
        },
        {
            "title": "A PRELIMINARY EXPERIMENTS",
            "content": "In this section, we present preliminary experiment designed to investigate the capacity of our proposed Meta-Reward mechanism to simulate and potentially enhance standard outcome-based rewards. Specifically, we utilize computational graph parameterized by small set of weights to derive reward function, which is then utilized to train the inner-loop model. A.1 EXPERIMENTAL SETUP Architecture. As illustrated in Figure 5, the Meta-Optimizer is implemented via graph neural network representing general computational graphs. The Meta-Reward function, parameterized by ϕt, is represented by set of twelve learnable weights (e.g., wadd, wsub, wmul, . . . ) distributed across the computational nodes. The set of atomic primitives used in this graph remains consistent with those described in the main body of this paper (Section 3). Training Protocols. We explore two distinct strategies to evolve the Meta-Optimizer: Supervised Fine-Tuning (SFT): In this setting, the Meta-Optimizer is trained to directly regress the ground truth outcome reward. The objective is to minimize the divergence between the generated Meta-Reward and the standard outcome signal (0.0 or 1.0). Reinforcement Learning (RL): We formulate the optimization of weights as an RL problem. For each computation step, we sample operations based on the distribution of within each node. If the resulting Meta-Reward aligns with the ground truth outcome reward (i.e., I(Meta-Reward = Outcome)), reward of 1.0 is assigned to the current configuration of w; otherwise, the reward is 0.0. Model and Data. To generate diverse set of trajectories for training, we perform inference on the training sets of the GSM8K and MATH benchmarks using Qwen2.5-3B-Instruct. These trajectories serve as the basis for optimizing the Meta-Optimizer. Once Meta-Reward is learned, it is frozen and used to train the inner-loop policy model (Qwen2.5-3B). The inner-loop training settings are identical to the baseline configuration to ensure fair comparison. Table 3: Performance comparison on mathematical reasoning benchmarks. We compare the preliminary results of Meta-Reward with different baselines. The Meta-Optimizer utilize 12-parameter graph optimizer to shape the reward signal Reward Function GSM8K MATH Outcome Avg Reward Meta-Reward (SFT) Meta-Reward (RL) 82.6 86.5 85.5 83. 58.8 55.8 62.9 59.9 A.2 RESULTS AND ANALYSIS Table 3 presents the performance comparison between the standard outcome reward, an average reward baseline (same as the main experiments in Section 4), and our proposed Meta-Reward (SFT and RL). The results indicate that the Meta-Reward, despite being parameterized by only 12 weights, effectively discovers reward function that outperforms the sparse outcome reward. Notably, on the challenging MATH dataset, the Meta-Reward (SFT) achieves significant improvement over the Outcome baseline (62.9% vs. 58.8%). These findings suggest that the Meta-Reward mechanism avoids overfitting to the rigid binary outcome signal. Analogous to an educational setting, using binary outcome reward is akin to instructing student solely to score 100 points, which provides sparse and high-variance signal. In"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 5: Demonstration of the training loop our differentiable evolutionary reward. We adopt GRPO as an example RL algorithm for the inner-loop. In the outer-loop, we leverage graph neural network to represent general computational graphs and obtain the final reward function Φt parameterized by ϕt = {wadd, wsub, ...} through differentiable optimization. contrast, our approachconstrained by the computational graph structureencourages the model to learn generalized heuristic. Although this is not an explicit process reward annotated by humans, the optimization process discovers an implicit, dense reward function that guides the model more effectively toward the correct reasoning path than the raw outcome signal alone."
        },
        {
            "title": "B GRADIENT PROPAGATION ON DERL",
            "content": "In this section, we elaborate on the information flow between the meta-optimizer (parameterized by ψ) and the inner-loop policy model (the optimizee, parameterized by θ). distinct feature of our DERL framework is the preservation and utilization of Meta-Gradient information, which allows the optimizer to explicitly learn from the validation performance of the optimizee. B.1 GRADIENT PROPAGATION FLOW The interaction between the optimizer and the optimizee unfolds as bi-level optimization process, as illustrated in Figure 6. Let vt denote the validation performance (or evaluation metric) of the policy model θt at step t. The optimization process consists of two coupled loops: Inner Loop (Optimizee): The policy model updates its parameters θt1 θt based on the guidance of the Meta-Reward Rϕt provided by the optimizer. The optimizer then generates the update instructions (parameterized by ϕt) conditioned on its current state ψt. Outer Loop (Optimizer): The meta-optimizer evolves ψt1 ψt by maximizing the expected future validation performance of the optimizee. Crucially, the update of the optimizer ψ is driven by the gradient of the validation performance, denoted as outer(ψt). This term represents the meta-gradient: it quantifies the sensitivity of the optimizees performance with respect to the optimizers parameters. By backpropagating the signal"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 6: Illustration of Gradient Propagation in bi-level evolutionary training loop. The top row (Orange) represents the trajectory of the optimizee θ, updated via instructions ϕ derived from the optimizer. The bottom row (Green) represents the evolution of the Meta-Optimizer ψ. The blue nodes vt denote the validation performance evaluation. Unlike static methods, our framework computes the meta-gradient outer(ψt) (vertical arrows), allowing the optimizer to update its own parameters ψ to explicitly maximize the optimizees performance. from the validation performance through the update step to ψ, our DERL establishes direct feedback loop. B.2 META-GRADIENT PROPAGATION COMPARED WITH PREVIOUS EVOLUTION The core innovation of our framework lies in the end-to-end differentiability of the optimization trajectory, or how the feedback signal vt is utilized. In traditional reinforcement learning or promptbased optimization methods, the optimizer ψ is often treated as static entity (e.g., fixed prompted agent or random perturbation generator). In such cases, the dependency chain is broken, and the meta-gradientthe gradient of the validation performance with respect to the optimizers parametersis lost. In contrast, our approach treats ψ as learnable entity. We explicitly compute the gradient flow from the evaluation metric back to the optimizer parameters. As depicted in the bottom flow of Figure 6, the optimizer updates its own parameters ψ to maximize the expected future validation performance of the optimizee: ψt = ψt1 + η outer(ψt1) (7) This derivation highlights that updating ψ is fundamentally learning the meta-gradient. Unlike prior works where the optimizer is fixed (resulting in ϕ ψ = 0 or undefined), our DERL framework maintains differentiable (or estimable) path. This enables the Meta-Optimizer to iteratively improve the reward structure ϕ driven by direct performance feedback. In doing so, DERL serves as foundational proof-of-concept for completely autonomous, self-improving frameworks."
        },
        {
            "title": "C COMPUTATIONAL COST ANALYSIS",
            "content": "In this section, we provide detailed breakdown of the computational costs associated with the bi-level evolutionary training framework (DERL) and discuss potential strategies for efficiency improvements. C.1 COMPUTATIONAL BREAKDOWN The training process consists of an outer-loop (Meta-Optimizer evolution) and an inner-loop (Policy Model evolution). We incorporate parallelism in most parts of DERL to improve GPU untilization. The computational cost is as summarized as follows: The training process consists of an outer-loop (Meta-Optimizer evolution) and an inner-loop (Policy Model evolution). We incorporate parallelism in most parts of DERL to improve hardware utilization. The computational cost is summarized as follows:"
        },
        {
            "title": "Work in Progress",
            "content": "Inner-Loop Latency (The Bottleneck). The primary computational bottleneck lies in the innerloop, where the policy model θi evolves (e.g., interacts with the environment) using the Meta-Reward Ri. Since our outer-loop utilizes the GRPO algorithm (Shao et al., 2024), the Meta-Optimizer generates distinct Meta-Rewards (as rollouts) per step. To mitigate latency, we implement fully parallelized architecture similar to standard GRPO: Parallel Execution: All rollouts (where = 8 in our experiments) are evaluated simultaneously, with each inner-loop allocated dedicated set of compute resources. Each inner-loop for each task finally consumes similar computation resources. Malformed Rewards: Meta-Rewards that fail to compile or produce valid computation graphs are immediately terminated, assigned reward of 0.0, and incur zero training cost (though the system waits for concurrent inner-loops to complete before updating the outerloop). Time Budgeting: significant challenge in Meta-Reward discovery is that certain reward functions may incentivize excessively long reasoning chains, increasing training costs unpredictably. To address this, we impose strict computational budget. For mathematical reasoning tasks, we set maximum floating-point operation cap approximately 1.3 the cost of standard binary-outcome training. If training exceeds this threshold, the process is halted, and the latest checkpoint is saved for evaluation. For other tasks, we rely on fixed number of inner epochs, as the action space is more controllable. Evaluation Cost. Following the inner-loop, we evaluate the validation performance to calculate the advantage for the Meta-Optimizer. We utilize vLLM for high-throughput inference. By parallelizing the evaluation across all rollouts, the validation phase incurs negligible latency compared to training. Outer-Loop Update. The Meta-Optimizer utilizes lightweight 0.5B parameter model. Updating this model using the collected rollout data (n = 8) is computationally negligible, taking only minutes to complete. Based on the parallelization strategy described above, the wall-clock time for one complete outerloop iteration is determined by the slowest successful inner-loop trial plus evaluation and update overhead. Ttotal max(Tinner) + Teval + Tupdate C.2 RESOURCE ESTIMATION To contextualize the resource requirements of DERL, we compare its cost against the baseline of training single inner-loop policy model. Let Cinner denote the computational cost required to train one standard inner-loop. The total computational cost for standard DERL, which runs for Eouter outer-loop epochs with parallel rollouts per step, can be estimated as: CDERL Eouter Cinner This cost scales linearly with the number of outer-loop iterations required for the Meta-Optimizer to converge. In contrast, for DERL-pop, we simplify the process selecting the best reward function from single population generation. In this setting, the total cost is significantly reduced to: CDERL-pop Cinner Consequently, DERL-pop offers more efficient alternative, consuming way less wall-clock time while still benefiting from population-based exploration. We utilized high-memory data center accelerators to accommodate the memory requirements of the parallel inner-loop training. C.3 EFFICIENCY IMPROVEMENTS AND FUTURE WORK In our current implementation, we utilized relatively large number of rollouts (n = 8) and full inner-loop training protocol to empirically verify that LLMs can effectively learn meta-gradients through Reinforcement Learning. However, our preliminary experiments (demonstrated in Section A) suggest that simple parameterizations (e.g., 12 parameters) can also yield competitive results."
        },
        {
            "title": "Work in Progress",
            "content": "Figure 7: Training dynamics of DERL-population. We present the training dynamics of DERLpop. and GRPO w/ Avg Reward to demonstrate the superiority of the population method. This observation points toward promising direction for future work: reducing the heavy computational burden of the inner-loop by adopting lightweight RL algorithms, such as REINFORCE++. By simplifying the inner-loop requirements or using proxy tasks, the reliance on massive parallel resources can be significantly reduced, making the evolution of Meta-Rewards accessible to broader range of computational budgets. Evaluation Cost. Following the inner-loop, we evaluate the validation performance to calculate the advantage for the Meta-Optimizer. We utilize vLLM for high-throughput inference. By parallelizing the evaluation across all rollouts, the validation phase typically requires only few minutes. Outer-Loop Update. The Meta-Optimizer itself utilizes lightweight 0.5B parameter model. Updating this model using the collected rollout data (n = 8) is computationally negligible, taking only minutes to complete. TRAINING DYNAMICS OF DERL-POP. Figure 7 illustrates the training dynamics of DERL-pop. and the comparison with the GRPO w/ Avg Reward baseline. The experiments are based on the L0 difficulty of the ScienceWorld task. For GRPO w/ Avg Reward, we train for full 100 steps. For DERL-pop., we train the inner layers for only 33 steps each time, and then the next round of training is based on the best-performing model from the previous round, rather than starting from scratch. It demonstrate that in the first 33 steps, the two models perform on par with each other. However, starting from the second outerloop of DERL-pop., it starts to surpass the baseline. When exceeding 66 steps, DERL-pop further shows significantly better results. This showcases how the dynamic nature of DERL-pop.s reward function surpasses standard fixed reward function. EXAMPLES OF OUTER-LOOP EVOLUTION We demonstrate the detailed training dynamic of DERL by showcasing each Meta-Reward explored in the outer-loop on ALFWorld (L2) in Table 4. We show four outer-loop iterations. We observe that there may be lot of low-quality meta-rewards in the early stages, but DERL can quickly learn high-quality rewards."
        },
        {
            "title": "Work in Progress",
            "content": "Table 4: Evolution of Meta-Reward structures and their corresponding reward across outer-loop training steps on ALFWorld. Step Meta Reward Reward 0 2 3 g1 (g2 1)/2 + (g3 + 1) (g4 1) 2/3 g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4)))) 0.5 (g2 + 0.5 (g3 + g1 + 0.01 (g2 0.001) + 0.0001 (g3 0.0001) + 0.000005 (g4 g1 0.5 + 0.2 (g2 + 0.1) 0.3/2 + 0.4 (g3 0.1) + 0.1 (g4 g1 (g2 + (g3 1.0) (g4 0.0)) g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4 + 0.5))) g1 + 0.5 (g2 + 0.2 (g3 + 0.1 (g4 + 0.05)))) g1 (g2 + 2 (g3/3)) + (g4 4 (g2 1)) 2.0 (0.5 (g1 0.1)) + (0.5 (g2 0.1)) + (0.5 (g3 0.1)) + (0.5 g1 (g2 + (g3 (g4 (g2 + (g3 (g4 (g3 (g4 (g3 (g4 (g3 (g1 + 0.5 (g2 + 0.3 (g3 0.2 (g4 + 0.1 1))) + 0.1 1)/1.2 g1 (g2 1) (1 0.5) + 0.5 2 (g3 1) (1 0.25) + 0.25 g1 + (g2 (g3/2)) (g4/3) g1 (g2 + 0.5) + 0.2 (g3 0.3 + 0.1) 0.1 (g4 0.2 + 0.5) g1 + 2 (g2 + 3 (g3 2)) 0.1 + 4 0.3 5 (g4 + 1) g1 (g2 1)/2 + (g3 1)/3 + (g4 1)/4 + 3.0 g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4)))) g1 + 0.5 (g2 0.5) + 0.3 (g3 0.05) + 0.2 (g4 + 0.05) 0.2 g1 + (g2 (g3/2)) (g4/4) g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4 1)))) g1 + 0.01 + 0.0001 (g2) + 0.000001 (g3) + 0.00000001 (g4) g1 (g2 + (g3 (g4/2))) + (g1 (g2 + (g3 (g4/2)))) 0.5 g1 0.99 + 0.01 (g2 + 0.99) + 0.005 (g3 + 0.99) + 0.0005 (g4 + g1 + 0.5 (g2 0.5) + 0.2 (g3 0.5) + 0.1 (g4 0.5) g1 + (g2/2.0) (g3 0.1) + (g4 0.05) g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4 + 0.5))) g1 + 0.05 (g2 + 0.05 (g3 + 0.05 (g4 + 0.05))) g1 + 0.5 (g2/2.0) + 0.1 (g3 2.0) + 0.25 (g4/4.0) g1 + 0.5 (g2 + 0.4 (g3 + 0.2 (g4 + 0.1))) g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4 + 0.5))) g1 + 0.5 (g2 + 0.5 (g3 + 0.5 (g4 + 0.5))) g1 + (g2 (g3/2)) (g4/3) 0 0 0.8496 0.8789 0.0234 0.8848 0 0.8945 0.7793 0 0.0020 0 0.8594 0.8477 0.8984 0.0098 0 0 0.8438 0 0.8652 0.8867 0.8477 0.8906 0.8438 0.875 0.8242 0.8632 0.8496 0.8926 0.8789 0."
        }
    ],
    "affiliations": [
        "Duke University",
        "The Chinese University of Hong Kong, Shenzhen",
        "The University of Hong Kong",
        "University of Waterloo"
    ]
}