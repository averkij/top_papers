{
    "paper_title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
        "Chengsong Huang",
        "Tong Zheng",
        "Langlin Huang",
        "Jinyuan Li",
        "Haolin Liu",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers."
        },
        {
            "title": "Start",
            "content": "RelayLLM: Efficient Reasoning via Collaborative Decoding Chengsong Huang1 Tong Zheng2 Langlin Huang1 Jinyuan Li1 Haolin Liu3 Jiaxin Huang1 1Washington University in St. Louis 2University of Maryland {chengsong, h.langlin, ljinyuan, jiaxinh}@wustl.edu tzheng24@umd.edu 3University of Virginia srs8rh@virginia.edu 6 2 0 2 ] . [ 1 7 6 1 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deploying Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via special command, effectively \"relaying\" the generation process. We introduce two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic helpseeking. Empirical results across six benchmarks demonstrate that RelayLLM improves the average accuracy from 42.5% to 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering 98.2% cost reduction compared to performance-matched random routers. Our code is available at https:// github.com/Chengsong-Huang/RelayLLM."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning and problem-solving (Comanici et al., 2025; Yang et al., 2025a; Achiam et al., 2023). However, their deployment is often constrained by high computational costs and latency. In contrast, Small Language Models (SLM) are resource-efficient options but typically struggle with hard reasoning tasks due to their limited capacity (Kaplan et al., 2020). 1 Figure 1: Results are averaged across six mathematical benchmarks. The Random Router baseline randomly directs questions to either the small or large model. The Perfect Router baseline directs only questions SLM cannot solve to large model. The x-axis represents the Call Ratio (percentage of tokens generated by the teacher model), and the y-axis denotes the average accuracy. This trade-off has motivated the development of collaborative systems that combine the reasoning capabilities of LLMs with the efficiency of smaller models (Hakimov et al., 2025). Existing approaches to different-sized model collaboration often rely on cascading or routing mechanisms, where router determines the difficulty of query and directs it to either small or large model (Ding et al., 2024; Hu et al., 2024; Ong et al., 2024). While effective to some extent, these methods typically operate at coarse granularity by offloading the entire generation task to the large model once query seems difficult. This all-or-nothing strategy leads to significant computational waste, as the small model often possesses the competence to handle the majority of the reasoning steps, requiring expert intervention only at specific critical positions (Lin et al., 2024; Ruan et al., 2025). To address these problems, we propose RelayLLM, novel framework for efficient reasoning via token-level collaborative decoding (Shen et al., 2024) without an additional controller. Unlike static routers, RelayLLM empowers the small model to act as both problem solver and an active controller that dynamically requests assistance only when necessary. Inspired by tool-use agents (Wölflein et al., 2025; Zheng et al., 2025a), we introduce an interleaved generation process where the small model generates special command token (<call>) to pause its own generation and invoke the large model for specified number of tokens. The small model then receives the experts guidance and resumes reasoning, effectively relaying the output process between models. We propose two-stage framework to equip the small model with this strategic delegation capability to train the RelayLLM. We first employ supervised warm-up phase to teach the model the syntactic structure of calling commands. This is followed by reinforcement learning stage using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) training, where we design context-aware reward that guides the model to balance independence with necessary help-seeking, penalizing both wasted costs and avoidable errors. Empirical results on six benchmarks demonstrate the effectiveness of our approach. As illustrated in Figure 1, RelayLLM achieves an average accuracy of 49.52% on six benchmarks, largely recovering the performance gap between the small model and the large one. Remarkably, this gain is achieved with minimal cost, as RelayLLM invokes the large model for only 1.07% of the total generated tokens. Furthermore, in comparison to resource-equivalent Random Router, RelayLLM yields substantial 6.9% accuracy improvement. These results confirm that RelayLLM effectively identifies critical reasoning steps for expert intervention, reducing token costs by 98.2% compared to performance-matched router. Surprisingly, evaluations in teacher-free setting reveal that the model internalizes effective reasoning patterns during collaboration, enabling it to surpass baselines on easier benchmarks even without expert assistance. pensive Large Language Model (LLM), denoted as ML. Given an input query x, the system aims to generate high-quality response through dynamic collaboration. Different from standard cascading methods that simply offload the entire remaining task to larger model, we define an interleaved generation process where the small model acts as both reasoner and central controller. The process operates as follows: 2.1 Small Model Generation By default, MS generates tokens autoregressively based on the current context history as normal language model. MS is augmented with special control capability: it can actively request assistance by generating specific command pattern: Ccmd(n) = <call> </call> where denotes string concatenation, and Z+ represents the number of tokens required from the larger language model. 2.2 Large Model Intervention When this trigger pattern is detected, the generation by MS pauses. The current context (including query and tokens generated by SLM) is forwarded to ML. Crucially, to maintain compatibility with the large models standard input distribution, we strip the special command tokens (Ccmd) from the context provided to ML. The large model then generates the next tokens (or stops early if an endof-sequence [EOS] token is reached), providing high-quality continuation or reasoning step. 2. Iterative Relay of LLM and SLM After ML completes the generation, control is returned to the small model. The context is updated with the new tokens generated by ML added into the existing context. Unlike the large model, MS retains the full history, including its own generated command tokens (<call>n</call>). This allows MS to maintain trace of its active delegation decisions. With the updated context, MS resumes generation, digesting the experts guidance to continue the reasoning process."
        },
        {
            "title": "3 RelayLLM Training",
            "content": "As illustrated in Fig. 2, we consider hybrid inference setting involving primary, resourceefficient Small Language Model (SLM), denoted as MS, and powerful but computationally exTo enable the small model MS to determine when and how long to invoke the LLM, we propose twostage framework including supervised warm-up phase followed by reinforcement learning phase. 2 Figure 2: Overview of the RelayLLM framework.(Left) Collaborative Inference: The Small Language Model acts as central controller. During generation, it can actively trigger an intervention by generating <call> command. The Large Language Model then generates the specified number of tokens, after which control returns to the SLM to complete the reasoning. (Right) Difficulty-Aware Reward Design: During GRPO training, we sample group of rollouts (both with and without LLM intervention) and classify the query difficulty into three distinct scenarios: Solvable, Teacher-Dependent, and Unsolvable, to guide the policy optimization.Distinct reward designs are applied to each scenario to align the models behavior with the optimal strategy. 3.1 Cold Start via Supervised Warm-up via = min(nsample, Lrem). The small model MS may not naturally generate the command pattern Ccmd if we train it directly with reinforcement learning. We employ supervised warm-up phase as cold start to initialize the model with the necessary structural knowledge. We construct synthetic dataset Dwarm to teach the model the calling command without inducing distribution shifts. To prevent distribution shifts of the small model MS, we first generate base sequences by sampling directly from vanilla MS rather than relying on external corpora. This ensures that the training contexts align perfectly with the small models own distribution. Within these self-sampled sequences, we insert command tokens at random indices at the token level. By avoiding rigid restrictions to sentence or paragraph boundaries, we enable the model to learn to trigger assistance at the precise moment reasoning gap occurs during the inference time. This data construction strategy ensures effective on-policy training data to prevent data distribution shift of the small model, while simulating various calling scenarios. Furthermore, we explicitly simulate varying degrees of reliance on the expert model by synthesizing delegation lengths across multiple orders of magnitude. We randomly sample the target length nsample = 10k, where {1, . . . , 9} and {0, . . . , 3}. To guarantee validity, the final request length is clipped to the available response We fine-tune MS on this constructed dataset using standard cross-entropy loss. While we acknowledge theoretical discrepancy between this training setup (where tokens subsequent to the control command are from model itself) and the inference phase (where tokens are generated by ML), this supervised warm-up is essential. It focuses primarily on adapting the model to output valid executable commands (Ccmd). This creates stable starting point for the subsequent reinforcement learning stage, where MS will be trained to request tokenlevel assistance from ML and take advantage of their feedback. 3.2 Policy Refinement with Reinforcement Learning In this stage, we employ Reinforcement Learning to align the models behavior with our core objective: maximizing response quality while strictly minimizing collaborative cost. We proceed by detailing our optimization framework. 3.2.1 GRPO Training with RLVR We leverage Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to refine the policy πθ of MS by adopting the Reinforcement Learning with Verifiable Reward (RLVR) paradigm (Lambert et al., 2024). This paradigm is well-suited for domains where the quality of generated responses can be deterministically verified. In its standard form, RLVR employs rule-based verifier 3 : {0, 1} that assigns binary rewards. This binary scheme is effective for tasks with unambiguous success criteria, such as mathematical problemsolving. In our setting, we design corresponding rule-based reward to verify model-generated relations, described in Sec. 3.2.3. To optimize the policy using these rewards, GRPO samples group of outputs {oi}G i=1 for each query from the old policy πθold and evaluates them relative to the group average. The training objective is formulated as: JGRPO(θ) = EqD (cid:34) 1 (cid:88) (cid:35) (Mi βDKL) , i=1 (1) where DKL = DKL(πθ πref ) is the regularization term. The surrogate objective Mi is computed as min(ρiAi, clip(ρi, 1 ϵ, 1 + ϵ)Ai), where ρi = (oiq) . The advantage Ai is derived from the πθ(oiq) πθold group-normalized rewards defined below: Ai = ri mean({rj}) std({rj}) + εstab , (2) where εstab is small constant for stability. This formulation encourages the model to generate responses that outperform the group average. 3.2.2 Data Filtering Since our method leverages the large model to generate reasoning paths or feedback, it is essential to identify the subset of data where such intervention is useful. If the large model consistently fails to solve query, calling it during training yields no positive gain. Therefore, we preprocess the dataset to filter out instances that are too hard for the large model. We sample 10 responses per query and only preserve those with pass rate of 50%. This step ensures that the training data lies in the competence boundary of the large model and the responses can contribute effectively. We provide an ablation study on this filtering mechanism in Table 3. 3.2.3 Reward Design We formulate the optimization objective using two distinct reward signals, simple reward and our designed difficulty-aware reward. Let be the response, be the final answer parsed from y, be the ground truth, and ρ(y) [0, 1] be the call ratio (the ratio of large-model generated tokens to the total response length.). 4 Simple Reward. We define straightforward reward to encourage both accuracy and efficiency: rsimple(y) = 1(a = g) ρ(y). (3) where 1() is the indicator function, thus the responses are scored by their correctness and penalized by the cost of calling the expert model. Difficulty-Aware Reward. To capture the relative difficulty of each query, we define the reward based on the collective performance of the sampled group G. We categorize each query into three scenarios based on its difficulty (correctness of responses in G). As illustrated in Figure 2 (Right), we provide concrete examples of how these rewards are assigned across different categories. Scenario 1: Student-Solvable (Encouraging Independence). This scenario applies when the student model is capable of solving the query independently, without assistance from the large model. This scenario is identified if there exists at least one sample in the group that answers correctly without invoking the large model. In this case, calling the teacher is deemed redundant. Consequently, we assign boosted bonus (r = 1.5) for independent success to promote efficiency and independence, while dependent success (ρ(y) > 0) still receives the simple reward rsimple in Eq.(3), and incorrect responses receive zero reward. Scenario 2: Teacher-Dependent (Penalizing Stubbornness). This scenario represents challenging queries where correct answers appear only in samples that call the large model. Here, the small models independent reasoning is insufficient. To discourage blind guessing, we impose penalty on samples that fail to call the teacher (r = 1.0 when ρ(y) = 0). Conversely, effective expert calling that leads to correct answer is rewarded with rsimple. Scenario 3: Teacher-Unsolvable (Incentivizing Exploration). This scenario occurs when no sample in yields the correct answer, indicating that the query is extremely difficult or the teachers guidance was ineffective. Rather than providing zero training signal for all responses, we assign small exploration reward (r = ρ(y)) to samples that attempted to call the large model. This reinforces the tendency to seek help in highly uncertain situations. This piecewise design aligns the policy with an optimal strategy: solve independently when possible, seek help when necessary, and avoid costly errors."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup 4.1.1 Models To evaluate the effectiveness of RelayLLM, we utilize the Qwen3 model family (Yang et al., 2025a) due to its consistent architectural scaling and strong performance across various sizes. We select Qwen3-0.6B and Qwen3-1.7B as our primary small language models (MS) to investigate how our framework scales with model capacity at the sub-2B parameter level. For the teacher model (ML), we utilize Qwen3-8B. Selecting model from the same model family ensures that the generation style, token distribution, vocabulary and tokenizer are more consistent, making collaboration more stable. To optimize training and inference efficiency, we consistently run the models in non-thinking mode. 4.2 Evaluation Setup To evaluate the effectiveness of RelayLLM, we conduct experiments on six reasoning benchmarks, and compare our approach against the standard GRPO baseline. We also add CITER (Zheng et al., 2025b), token-level routing method as the baseline method which requires an additional controller. The benchmarks include Minerva (Lewkowycz et al., 2022), MATH-500 (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), Olympiad-Bench (He et al., 2024), AIME-2024, and AIME-2025. We use GPT-4o-mini as semantic judge to verify the models output against the ground truth (Zhao et al., 2025). For the high-difficulty AIME datasets, we report the avg@32 metric to ensure robust evaluation. For other benchmarks, we report standard accuracy (pass@1) using greedy decoding. 4.3 Training Details We conduct our experiments using the DAPO dataset (Yu et al., 2025a). Our implementation is built upon the EasyR1 framework (Zheng et al., 2025c) using its default hyperparameter configurations (shown in App. E). All models are trained for single epoch to ensure fair comparison. Regarding data usage, the GRPO baseline is trained on the full dataset, whereas our method utilizes the filtered subset as described in Section 3.2.2. To enable efficient interaction with the large model, we serve the teacher model via the vLLM inference engine (Kwon et al., 2023). We implement the switching mechanism as stop sequence in the sampling parameters: when the model generates the calling command token, generation halts, and the system invokes the teacher model via the API. 4.4 Main Results As presented in Table 1, RelayLLM demonstrates superior trade-off between reasoning capability and inference efficiency. First, our method achieves substantial performance improvements across all benchmarks while maintaining negligible collaborative cost (less than 1% token overhead). For instance, on the challenging Minerva benchmark, the Qwen3-0.6B model with Difficulty-Aware-Reward improves from base score of 15.81% to 23.53%, representing relative improvement of approximately 48.8% while invoking the large model for only 0.77% of the total tokens. Compared to CITER, our method demonstrates superior performance despite CITERs more computationally intensive design. CITER relies on an external MLP to estimate score every token, which introduces substantial latency and computational overhead. In contrast, RelayLLM achieves better results with significantly more efficient mechanism at the cost of only several addtional tokens. Second, comparing optimization strategies, the Difficulty-Aware-Reward mechanism outperforms the Simple-Reward in performance, with marginal increase in token consumption. For the Qwen3-1.7B model, the Difficulty-Aware-Reward strategy achieves higher average accuracy of 49.52% compared to 49.30% for the SimpleReward, which correlates with slight increase in the average call ratio from 0.43% to 1.07%, suggesting that the difficulty-based signal better incentivizes the model to seek help in complex scenarios. Finally, RelayLLM effectively bridges the capability gap between small and large models using minimal tokens. Remarkably, the Qwen3-1.7B (Difficulty-Aware) recovers approximately 60% of the performance gap between the base SLM (42.50%) and the expert model (54.12%), highlighting that sparse, strategic intervention at critical reasoning steps is sufficient to unlock significant portion of the teacher models potential."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we conduct series of in-depth analyzes to better understand the behavior and effectiveness of RelayLLM framework. 5 Table 1: Performance comparison on six benchmarks. We compare the effectiveness of RelayLLM using Qwen30.6B and Qwen3-1.7B as student models across different methods: the standard Base model, the GRPO-tuning baseline, CITER and RelayLLM (Simple-Reward and Difficulty-Aware-Reward). The Qwen3-8B teacher model performance is provided for reference. We report avg@32 for the challenging AIME datasets and standard pass@1 (greedy decoding) for all other benchmarks. The Avg. Call Ratio denotes the percentage of tokens generated by the teacher model during the collaborative inference process. The best results within each model group are highlighted in bold. Model Minerva MATH500 GSM8K Olympiad AIME25 AIME24 Average Avg. Call Ratio Qwen3-0.6B Base Model GRPO CITER RelayLLM (Simple) RelayLLM (Difficulty-Aware) Qwen3-1.7B Base Model GRPO CITER RelayLLM (Simple) RelayLLM (Difficulty-Aware) Qwen3-8B 15.81 17.65 19.29 20.96 23.53 33.82 35.66 38.63 43.01 43.75 48.16 54.00 58.60 58.80 60.20 60. 74.60 75.60 80.24 83.40 81.40 83.20 64.82 65.50 67.78 69.14 71.95 82.64 81.73 82.26 86.13 86.28 93.63 26.22 29.04 29.60 32.15 32. 43.11 45.04 51.20 51.56 55.70 56.89 1.04 5.42 5.93 7.19 6.15 8.75 10.73 11.96 13.44 12.71 17.92 1.15 3.23 3.24 3.85 3. 12.08 15.62 16.58 18.23 17.29 24.90 27.17 29.91 30.77 32.25 33.04 42.50 44.06 46.81 49.30 49.52 54.12 0.98% 0.31% 0.77% 1.34% 0.43% 1.07% 100% 5.1 RelayLLM Generalizes to Unseen Reasoning Domains To verify the generalization capability of RelayLLM, we extended our evaluation to general reasoning domains that were unseen during training. Although our model was trained exclusively on the mathematical DAPO dataset, we tested it on three diverse benchmarks out of the math domain: Big-Bench Hard (BBEH) (Kazemi et al., 2025), MMLU-Pro (Wang et al., 2024), and SuperGPQA (Du et al., 2025). As shown in Table 2, RelayLLM consistently outperforms baseline methods despite the domain shift. For instance, using Qwen3-1.7B, our method achieves 59.03% on MMLU-Pro, significantly surpassing the GRPO baseline (49.76%) and CITER (53.38%). These results demonstrate that our framework effectively help SLM have generalized help-seeking behavior; even when facing unfamiliar inputs, the SLM successfully recognizes its knowledge gaps and invoke the LLM, leading to substantial performance gains in out-of-distribution tasks. 5.2 Ablation Study To investigate the distinct contribution of each component in RelayLLM , we conducted an ablation study using the Qwen3-1.7B model in Table 3. Data filtering prevents wasteful calls where teacher models fail. We show that removing the data filtering mechanism results in tripled call ratio with decreased accuracy; this confirms that Table 2: Performance comparison on reasoning and general knowledge benchmarks. We evaluate the effectiveness of RelayLLM using Qwen3-0.6B and Qwen3-1.7B as student models across different settings. The Qwen38B performance is provided for reference. The best results within each model group are highlighted in bold. Model BBEH MMLU-Pro SuperGPQA Qwen3-0.6B Base Model GRPO CITER RelayLLM (Simple) RelayLLM (Difficulty-Aware) Qwen3-1.7B Base Model GRPO CITER RelayLLM (Simple) RelayLLM (Difficulty-Aware) Qwen3-8B 7.19 7.82 8.16 8.32 8.56 9.91 10.89 11.67 12.67 12.46 15.31 30.03 32.15 33.12 35.61 35. 46.90 49.76 53.38 58.76 59.03 66.46 17.22 19.91 20.34 21.35 20.88 24.46 26.01 28.25 29.85 29.93 36.21 filtering out queries intractable for the teacher is crucial to avoid cost that yield no performance gain. Filtering out some too hard data also save time and resources during the training stage. Encouraging independence reduces reliance on teacher model and improves efficiency. We remove the independence encouraging (where we boosted correctness reward from 1 to 1.5 for solvable queries), and this causes the call ratio to spike to 4.10%. This demonstrates that specifically rewarding the independent success is crucial to prevent the model from becoming over-reliant on the expert LLM for tasks it could solve alone. 6 Table 3: Ablation study on data filtering and reward design strategies using Qwen3-1.7B. w/o Data Filtering denotes training on the unfiltered dataset including teacher-failed queries. w/o Indep. Incentive removes the bonus reward (from 1.5 to 1) for independent success (Scenario 1). w/o Explor. Reward removes the exploration reward (from ρ to 0) for seeking help in unsolvable queries (Scenario 3). Method RelayLLM w/o Data Filtering w/o Indep. Incentive w/o Explor. Reward Avg. Acc. (%) Call Ratio (%) 49.52 48.76 49.34 47.56 1.07 3.30 4.10 0.65 Table 4: Evaluation of intrinsic reasoning capability. We disable the teacher during inference (w/o Teacher) by masking the call tokens. Hard refers to AIME24 and AIME25, while Easy refers to the remaining. Method Easy (%) Hard (%) GRPO Baseline 59.51 13.18 RelayLLM (Simple) Standard Inference w/o Teacher 66.03 61.12 RelayLLM (Difficulty-Aware) Standard Inference w/o Teacher 66.78 60.26 15.84 13.13 15.00 11.93 Exploration reward effectively increases accuracy. When we remove the exploration reward (for unsolvable queries), this leads to significant accuracy drop to 47.56%, indicating that the exploration reward is necessary to encourage the model to call for help from teacher models in highly uncertain scenarios. 5. Intrinsic Reasoning Capability To investigate whether RelayLLM improves the students inherent reasoning or merely learns to offload tasks, we evaluate the models in Teacher-Free setting by strictly forbidding invocations during inference (implemented via bad_words=[<call>, </call>] when inference). Results in Table 4 reveal three key insights. First, on Easy datasets, even without teacher access, RelayLLM (Simple-Reward) achieves 61.12%, surpassing the GRPO baseline. This suggests that the student model has successfully learn from the reasoning ability of the expert model during the collaborative training process. On Harder datasets, removing the teacher leads to notable performance drop (e.g., DifficultyAware-Reward falls from 15.00% to 11.93%), confirming that for complex tasks, the model remains heavily dependent on expert intervention. Third, comparing reward schemes, the Simple-Reward variant demonstrates stronger capabilities than the Difficulty-Aware-Reward variant. This aligns with our previous observation that Difficulty-AwareReward encourages higher call ratio, leading to stronger dependency on the teacher, whereas Simple-Reward retains more independence. 7 Table 5: Comparison between dynamic length prediction (RelayLLM) and fixed delegation length strategies. Note that Fixed-k does not simply denote inference truncation; these models were retrained with the constraint to always request tokens, ensuring they learned optimal policies for those specific lengths. Method Avg. Acc. (%) Call Ratio (%) Fixed-20 Fixed-100 Fixed-500 RelayLLM 49.41 49.56 51.17 49.52 1.32 2.87 5. 1.07 5.4 Dynamic Token-Length Calling Minimizes Computational Cost We investigate whether dynamically predicting the calling length requested from the large model is superior to using rigid, pre-defined lengths. To ensure fair evaluation, we retrained separate variations of the student model where the call command is hard-constrained to fixed token count {20, 100, 500} during both training and inference. As shown in Table 5, RelayLLM demonstrates superior efficiency compared to these specialized fixed-length models. Specifically, compared to the Fixed-100 model, RelayLLM achieves similar accuracy but reduces the call ratio from 2.87% to 1.07%. This indicates that while the fixed-length model is forced to consume set budget even for simple queries, RelayLLM effectively learns to request just enough tokens to bridge the reasoning gap, thereby minimizing computational waste without compromising performance. We provide more detailed results in Appendix B. 5.5 Distributional Alignment To determine whether the student model has acquired generalized reasoning capabilities or merely overfitted to the specific patterns and words of the small models, which can be broadly categorized into two directions. The first direction involves speculative reasoning. Bachmann et al. (2025) and Pan et al. (2025) employ judge mechanisms or verifiers to validate small model outputs, effectively acting as dynamic routers. Similarly, large model guidance is leveraged to enhance small model reasoning specifically at inference time (Yang et al., 2025b; Zhang et al., 2025). The second direction focuses on collaborative decoding (Shen et al., 2024) with interleaved generation via an additional controller. Shen et al. (2024); Sun et al. (2024b) proposed learning joint policy for multiple models. Strategic intervention is further investigated by Li et al. (2025a); Feng et al. (2025b); Fu et al. (2025) to explore thought spaces efficiently. 6.2 RL for LLM Reasoning Reinforcement learning has recently emerged as pivotal technique for augmenting LLM reasoning, demonstrating broad success ranging from traditional mathematical and code generation tasks (Guo et al., 2025; Wang et al., 2025a) to intricate multi-modal challenges (Huang et al., 2025c; Wang et al., 2025b; Li et al., 2025b) and structured data environments (Shi et al., 2025; Tang et al., 2025). To support these diverse applications and enable complex behaviors like RelayLLM, concurrent research is actively refining methodologies through novel training paradigms, such as self-play (Liu et al., 2025; Huang et al., 2025b; Yu et al., 2025b), alongside developing more robust algorithmic techniques exemplified by DAPO (Yu et al., 2025a), VAPO (Yue et al., 2025), and high-entropy guided optimization (Dai et al., 2025; Wang et al., 2025d; Zhou et al., 2025)."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented RelayLLM, addressing the inefficiency of \"all-or-nothing\" offloading in routing systems. By treating the large model as an on-demand tool rather than fallback generator, our approach demonstrates that small models can handle the vast majority of reasoning steps if supported at specific critical positions. The success of our GRPO-based training strategy confirms that help-seeking behaviors can be effectively learned and optimized. Our results show that RelayLLM not only outperforms resource-equivalent random routers by 6.9% but also achieves comparable reasoning accuracy to larger models with negligible computation. Figure 3: Impact of teacher model size on student performance. We evaluate two student models (DifficultyAware) across six benchmarks. The x-axis represents the size of the teacher model used during the collaborative inference process, ranging from \"None\" (the same as Sec. 5.3, prevent model to call teacher model) to 14B. The reported scores are averaged across all six datasets. LLM in training time, we performed cross-LLM evaluation by substituting the training teacher with different models in the inference phase. The results, illustrated in Figure 3, reveal two critical insights. First, consistency with the training LLM yields optimal performance. The accuracy peaks when the inference teacher matches the training teacher, reaching 49.52% for the 1.7B student. Notably, replacing it with larger model results in slight performance decline. This indicates that the distribution shift between the training and inference teachers can outweigh the benefits of the larger models superior reasoning capabilities. Second, even employing relatively weak teacher that is weaker than itself (e.g., 0.6B or 1.7B) consistently outperforms the None baseline. This suggests that the trained model has become accustomed to the presence of external assistance, adapting its generation dynamics to effectively leverage such interventions rather than relying solely on its intrinsic capabilities. Furthermore, excluding the distribution shift at 8B, which is we used in training, there is positive correlation between teacher size and student performance, confirming that the student effectively leverages the stronger reasoning signals provided by more capable experts."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Model Collaboration Model collaboration (Feng et al., 2025a) ranges from weight-level merging (Wortsman et al., 2022; Huang et al., 2023) and logits-level ensembling (Liu et al., 2024; Li et al., 2023) to text-level interaction. Recent research has focused on navigating the efficiency trade-off between large and"
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Dongfu Jiang (University of Waterloo) for his helpful insights and discussions on tool use LLM. This research was supported in part by the NVIDIA Academic Grant Program and WashU Ignite Interdisciplinary Grants."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Gregor Bachmann, Sotiris Anagnostidis, Albert Artsiom Pumarola, Markos Georgopoulos, Judge decoding: Faster Sanakoyeu, et al. 2025. speculative sampling requires going beyond model alignment. arXiv preprint arXiv:2501.19309. Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, et al. 2021. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next arXiv preprint generation agentic capabilities. arXiv:2507.06261. Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, et al. 2025. Cde: Curiosity-driven exploration for efficient reinforcement learning in large language models. arXiv preprint arXiv:2509.09675. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, et al. 2024. Hybrid llm: Cost-efficient and quality-aware query routing. arXiv preprint arXiv:2404.14618. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al. 2025. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739. Shangbin Feng, Wenxuan Ding, Alisa Liu, Zifeng Wang, Weijia Shi, et al. 2025a. When one llm drools, multi-llm collaboration rules. arXiv preprint arXiv:2502.04506. Shangbin Feng, Wenhao Yu, Yike Wang, Hongming Zhang, Yulia Tsvetkov, et al. 2025b. Dont throw away your pretrained model. arXiv preprint arXiv:2510.09913. Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, et al. 2025. R2r: Efficiently navigating divergent reasoning paths with small-large model token routing. arXiv preprint arXiv:2505.21600. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Sherzod Hakimov, Roland Bernard, Tim Leiber, Karl Osswald, Kristina Richert, et al. 2025. The price of thought: multilingual analysis of reasoning, performance, and cost of negotiation in large language models. arXiv preprint arXiv:2510.08098. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Annual Meeting of the Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In Proc. of ICLR. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, et al. 2024. Routerbench: arXiv benchmark for multi-llm routing system. preprint arXiv:2403.12031. Chengsong Huang, Langlin Huang, and Jiaxin Huang. 2024. Divide, reweight, and conquer: logit arithmetic approach for in-context learning. arXiv preprint arXiv:2410.10074. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025a. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, et al. 2023. Lorahub: Efficient crosstask generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, et al. 2025b. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, et al. 2025c. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, et al. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, et al. 2025. Big-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26473 26501. 9 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, et al. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Nathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et al. 2024. Tülu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, et al. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1228612312. Ziheng Li, Hengyi Cai, Xiaochi Wei, Yuchen Li, Shuaiqiang Wang, et al. 2025a. Efficient thought space exploration through strategic intervention. arXiv preprint arXiv:2511.10038. Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, et al. 2025b. Self-rewarding visionlanguage model via reasoning decomposition. arXiv preprint arXiv:2508.19652. Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, et al. 2024. Critical tokens matter: Tokenlevel contrastive estimation enhances llms reasoning capability. arXiv preprint arXiv:2411.19943. Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, et al. 2024. Tuning language models by proxy. arXiv preprint arXiv:2401.08565. Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, et al. 2025. Spiral: Self-play on zerosum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119. Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. 2024. Non-myopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, et al. 2024. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665. Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, et al. 2025. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891. Zhiwen Ruan, Yixia Li, He Zhu, Yun Chen, Peng Li, et al. 2025. Enhancing large language model reasoning via selective critical token fine-tuning. arXiv preprint arXiv:2510.10974. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. 2024. Learning to decode collaboratively with multiple language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1297412990. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, et al. 2025. Mobilegui-rl: Advancing mobile gui agent through reinforcement arXiv preprint learning in online environment. arXiv:2507.05720. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024a. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290. Hao Sun, Jiayi Wu, Hengyi Cai, Xiaochi Wei, Yue Feng, et al. 2024b. Adaswitch: Adaptive switching between small and large agents for effective cloud-local collaborative learning. arXiv preprint arXiv:2410.13181. Yinxu Tang, Chengsong Huang, Jiaxin Huang, and William Yeoh. 2025. Unirel-r1: Rl-tuned llm reasoning for knowledge graph relational question answering. Preprint, arXiv:2512.17043. Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, et al. 2025a. To code or not to code? adaptive tool integration for math language models via expectation-maximization. arXiv preprint arXiv:2502.00691. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, InFangzhen Lin, et al. 2025b. Vl-rethinker: centivizing self-reflection of vision-language modarXiv preprint els with reinforcement learning. arXiv:2504.08837. Jikai Wang, Juntao Li, Lijun Wu, and Min Zhang. 2025c. Efficient reasoning for llms through speculative chain-of-thought. arXiv preprint arXiv:2504.19095. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, et al. 2025d. Beyond the 80/20 rule: Highentropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. 10 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, et al. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Yaowei Zheng, Shenzhi Wang Junting Lu, Zhangchi Feng, and Yuwen Xiong Dongdong Kuang. 2025c. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/ EasyR1. Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, et al. 2025. Evolving language models without labels: Majority drives selection, novelty promotes variation. arXiv preprint arXiv:2509.15194. Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelovic, and Jakob Nikolas Kather. 2025. Llm agents making agent tools. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609226130. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, et al. 2022. Model soups: averaging weights of multiple finetuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. 2025b. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, et al. 2025a. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, et al. 2025b. Guided self-evolving llms with minimal human supervision. arXiv preprint arXiv:2512.02472. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, et al. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Haozhen Zhang, Tao Feng, and Jiaxuan You. 2025. Router-r1: Teaching llms multi-round routing and aggregation via reinforcement learning. In The Thirtyninth Annual Conference on Neural Information Processing Systems. Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, et al. 2025. One token to fool llm-as-a-judge. volume abs/2507.08794. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, et al. 2025a. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980. Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, et al. 2025b. Citer: Collaborative inference for efficient large language model decoding with token-level routing. arXiv preprint arXiv:2502.01976."
        },
        {
            "title": "A Case Study",
            "content": "Here we show two case studies from Qwen3-1.7B (Difficulty-Aware) to illustrate how invoking LLM can help solve the questions in Figure 4."
        },
        {
            "title": "Strategies",
            "content": "In the main text, we demonstrated that RelayLLM achieves comparable average accuracy to fixed 100-token delegation strategy while consuming significantly fewer tokens. Table 6 presents the granular performance breakdown across all six mathematical benchmarks. We observe distinct trends across datasets of varying difficulty: Efficiency on Standard Benchmarks. On standard reasoning datasets such as MATH500 and GSM8K, RelayLLM matches the performance of the Fixed-100 strategy almost exactly (e.g., 81.40% on MATH500 for both methods). However, it achieves this parity with drastically lower call ratio (1.07% vs. 2.87%). This indicates that for the majority of queries in these datasets, the student model only requires short, precise interventions rather than long-context guidance. Adaptability on Complex Benchmarks. The advantage of dynamic prediction becomes more evident on the challenging Minerva benchmark. RelayLLM outperforms the Fixed-20 significantly (43.75% vs. 39.71%), suggesting that our model correctly identifies the need for longer generation lengths when facing harder problems. While the Fixed-500 strategy yields the highest accuracy on Minerva (44.49%) and AIME, this marginal gain comes at prohibitive cost: it incurs 5.37% call ratio, which is over 5 the computational overhead of RelayLLM."
        },
        {
            "title": "C Prompt Templates",
            "content": "This section presents the exact prompt templates used for the models. C.1 Inference Prompt Solver Prompt Template System Message: Please reason step by step, and put your final answer within boxed{}. User Message: {problem_statement} Note: {problem_statement} is placeholder for the actual math problem. C.2 GPT-4o-mini Judge Prompt To programmatically evaluate the correctness of answers on mathematical benchmarks where the final answer can be complex (e.g., simplified expressions), we use GPT-4o-mini as judge. The exact prompt and configuration used for this evaluation are detailed below. Configuration for GPT-4o as Judge Model: gpt-4o Temperature: 0.1 System Message: You are math answer checker. User Message Template: Hi, there is an answer: {answer}, and the ground truth answer is: {response}, please check whether the answer is correct or not, and return the **only** Yes or No. Note: {answer} is placeholder for the model-generated solution, and {response} is the ground-truth answer from the benchmark."
        },
        {
            "title": "D Other Related Work",
            "content": "RelayLLM also align with efficient reasoning, include efficient chain-of-thought methods (Wang et al., 2025c; Huang et al., 2025a), speculative bestof-N decoding (Sun et al., 2024a), in-context learning methods (Huang et al., 2024) and non-myopic generation (Ma et al., 2024). 12 Case Study: list of positive integers has the following properties: The sum of the items in the list is 30. The unique mode of the list is 9. The median of the list is positive integer that does not appear in the list itself.Find the sum of the squares of all the items in the list. We are given list of **positive integers** with the following properties: 1. **Sum of the items is 30.** 2. **The unique mode is 9.** 3. **The median is positive integer that does not appear in the list.** We are to find the **sum of the squares of all the items** in the list. ### Step 1: Understand the properties - **Mode is 9**: This means 9 appears more times than any other number in the list. - **Unique mode is 9**: No other number appears as frequently as 9. - **Median is positive integer that does not appear in the list**: So the median is some number m, and is not in the list. - **Sum of the items is 30**. ### Step 2: Consider the structure of the list Lets denote the list as a1, a2, . . . , an, sorted in non-decreasing order. #### Key observations: - The **median** is the middle value. If is odd, it is the n+1 2 -th element. If is even, it is the average of the two middle elements. - Since the **median is positive integer that does not appear in the list**, it must be **greater than the largest number in the list** (if the list has only one element, its trivial, but well assume > 1). - The **mode is 9**, so 9 appears more times than any other number. ### Step <call> 300 </call> 3: Try small values of $ $ Let test small values of $ $ to find valid list that satisfies all conditions . #### Try $ = 5 $ ( odd ): - Then the ** median is the 3 rd element **. - Suppose the list is $ [a , , , , d] $ , where $ $ is the median . - $ $ is not in the list . - The ** mode is 9** , so 9 must appear at least twice , and more than any other number . Let try to construct such list : Try $ [9 , 9, 1, 1, 10] $ ( sum = 9+9+1+1+10 = 30) Check the properties : - ** Sum = 30 ** - ** Mode = 9** ( appears twice ) - but are there other numbers with same frequency ? - 1 appears 3 times -> ** mode is 1** , not 9 correct Try $ [9 , 9, 1, 2, 9] $ -> sum = 9+9+1+2+9=30 - Mode is 9 (3 times ) , 1 and 2 once -> correct - Median is 1 ( the middle number ) -> 1 is in the list incorrect Try $ [9 , 9 (end call) , 1, 2, 8] $ -> sum = 9+9+1+2+8 = 29 incorrect Try [9, 9, 1, 3, 8] -> sum = 30 - Mode is 9 - Median is 1 (not in the list) - 1 is not in the list So this is valid list: [9, 9, 1, 3, 8] Check sum: 9 + 9 + 1 + 3 + 8 = 30 Check mode: 9 appears twice, others once -> **unique mode is 9** Check median: middle element is 3 3 is not in the list ### Step 4: Compute the sum of squares Compute 92 + 92 + 12 + 32 + 82 = 81 + 81 + 1 + 9 + 64 = 236 236 Figure 4: qualitative case study showcases that our model invoke LLM to show how to validate the final answer, then solve the problem by itself. Table 6: Full performance breakdown of different delegation length strategies across six benchmarks. Fixed-k indicates models retrained with fixed call length of tokens. We report mean@32 for AIME datasets (AIME24, AIME25) and pass@1 for others. Best accuracy values are bolded, and the most efficient call ratio is highlighted. Method Minerva MATH500 GSM8K Olympiad AIME25 AIME24 Average Call Ratio Fixed-20 Fixed-100 Fixed-500 RelayLLM 39.71 40.44 44. 43.75 81.40 81.40 81.80 81.40 86.96 86.50 88.25 86.28 55.74 56.78 57. 55.70 14.11 14.42 15.05 12.71 18.54 17.81 19.48 17.29 49.41 49.56 51. 49.52 1.32% 2.87% 5.37% 1.07%"
        },
        {
            "title": "E Hyperparameter",
            "content": "We utilize the Group Relative Policy Optimization (GRPO) algorithm for post-training. The model is optimized using AdamW with constant learning rate of 1 106 and weight decay of 1 102. The global batch size is set to 32. To support extensive reasoning steps required for complex mathematical problems, we configure the maximum sequence length to allow for long Chain-of-Thought (CoT) generation. Specifically, the maximum prompt length is set to 4096 tokens, and the maximum response length is extended to 8192 tokens. For the GRPO specific configurations, we set the group size = 8, meaning that 8 outputs are sampled for each prompt to estimate the baseline. To ensure training stability and prevent the policy from deviating excessively from the reference model, we apply KL divergence coefficient β of 0.01. During the rollout phase, we use sampling temperature of 1.0 to encourage diverse reasoning paths. Table 7: Hyperparameter settings for GRPO training. Hyperparameter Value Optimizer Learning Rate Weight Decay Global Batch Size AdamW 1 106 1 102 32 Max Prompt Length Max Response Length Temperature Group Size (G) KL Coefficient (β) 4096 8192 1.0 8 0."
        }
    ],
    "affiliations": [
        "University of Maryland",
        "University of Virginia",
        "Washington University in St. Louis"
    ]
}