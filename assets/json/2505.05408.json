{
    "paper_title": "Crosslingual Reasoning through Test-Time Scaling",
    "authors": [
        "Zheng-Xin Yong",
        "M. Farid Adilazuarda",
        "Jonibek Mansurov",
        "Ruochen Zhang",
        "Niklas Muennighoff",
        "Carsten Eickhoff",
        "Genta Indra Winata",
        "Julia Kreutzer",
        "Stephen H. Bach",
        "Alham Fikri Aji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 0 4 5 0 . 5 0 5 2 : r Crosslingual Reasoning through Test-Time Scaling Zheng-Xin Yong Jonibek Mansurov2 Niklas Muennighoff3 Carsten Eickhoff4 Genta Indra Winata5 M. Farid Adilazuarda2 Ruochen Zhang1 Julia Kreutzer6 1Brown University Stephen H. Bach1 Alham Fikri Aji2 2MBZUAI 3Stanford University 6Cohere Labs 5Capital One contact.yong@brown.edu 4University of TÃ¼bingen"
        },
        {
            "title": "Abstract",
            "content": "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLMs CoTs are naturally predominantly English, they consistently follow quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts."
        },
        {
            "title": "Introduction",
            "content": "Scaling up compute at test-time can maximize model performance and output quality [1, 2, 3, 4], but it has been understudied in multilingual settings. In particular, reasoning language models (RLMs), such as Deepseeks r1 [5] and OpenAIs o1 or o3 models [6, 7], strongly benefit from added inference compute to their long chain-of-thoughts (long CoTs) [8]. However, this advantage has primarily been explored in English contexts, such as in recent work that combined small-scale reasoning finetuning with scaled up thinking tokens at test time [9, 10]. State-of-the-art RLMs rely on reasoning training data that contain long CoTs, which is currently most available for English [11]. Thus, these RLMs are English-centric [9, 12, 13, 14, 15, 11]. Given that their base models are often multilingual models such as Qwen models [16], does reasoning finetuning in English give them multilingual reasoning abilities? In this work, we investigate how much test-time compute can improve multilingual reasoning abilities of English-centric RLMs. In particular, our research questions are as follows: Core contributors. See Appendix for further details. Preprint. Under review. RQ1. Crosslingual test-time scaling: How effective is test-time scaling of English-centric RLMs on multilingual reasoning tasks? (Section 4) RQ2. Language-mixing behaviors: What kind of language-mixing patterns do English-centric RLMs exhibit when they interact with non-English prompts? (Section 5) RQ3. Language forcing: How well do English-centric RLMs perform when being forced to think in non-English languages? (Section 6) RQ4. Cross-domain generalization: How well does crosslingual reasoning generalize beyond the original STEM domain, such as humanities and social sciences? (Section 7) We experiment with s1 models [9] as our English-centric RLMs for crosslingual generalization study. They are multilingual Qwen2.5-Instruct models [16] supervised finetuned on 1k training samples of English STEM reasoning tasks and achieve state-of-the-art performance on English math reasoning benchmarks [9]. Our most significant contributions are as follows: 2 1. We provide evidence that larger models benefit from crosslingual test-time scaling, which contrasts with contemporary work [17] that reports early negative findings based on 1.5B models. Crosslingual test-time scaling is not only effective for both high-resource and low-resource languages, but it also allows an RLM to outperform models twice its size on multilingual math reasoning tasks. 2. We report dominant language-mixing pattern where RLMs quote non-English phrases related to the question prompts in quotation marks in the thinking process. This quote-and-think pattern suggests that models multilingual capability to parse and understand questions enables crosslingual generalization of English reasoning finetuning. 3. We discover an effective strategy to control the reasoning language of RLMs, and we find that forcing RLMs to think in high-resource languages yields substantially better reasoning performance than in low-resource languages. Furthermore, the long CoTs for high-resource languages are more token-efficient at test time. 4. We show that while reasoning finetuning may benefit tasks in certain domains, we do not observe consistent benefits of scaling up thinking tokens for non-STEM tasks that are outside of the STEM domain of reasoning finetuning data. In domains requiring cultural commonsense knowledge, test-time scaling can even hurt performance. Our work shows that test-time scaling of English-centric RLMs can serve as strong multilingual reasoning baseline. Furthermore, for English-centric reasoning finetuning, it is advisable to finetune models through data-efficient training (such as with s1s data [9]) to minimize catastrophic forgetting of multilingual capability. We recommend letting the English-centric RLMs reason in high-resource languages such as English and Chinese for optimal performance and inference-compute efficiency. Future work is needed for enabling RLMs to generalize to out-of-domain tasks or to reason in low-resource languages."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Reasoning language models (RLMs) Recent advancements of reasoning language models (RLMs) such as OpenAI-o1 [6, 7] and DeepSeek-R1 [5] builds on LLMs capability to perform intermediate reasoning steps, which is commonly referred to as chain-of-thought reasoning [18]. Prior work demonstrates that these intermediate computation steps can significantly improve the correctness for final answer outputs [18, 19, 20, 21, 22]. Furthermore, extending the lengths of these computation steps, thereby creating long chain-of-thoughts (long CoTs), can allow the model to backtrack on incorrect reasoning steps and self-correct its final answer [8, 23, 5, 12, 24]. In our work, we focus on RLMs with long CoTs capability, which is an emerging research area. These models are created through distilling long English-only reasoning chains from larger RLMs [25, 10, 26, 27, 28] to finetune multilingual pretrained models like Llama [29] or Qwen models [16]. There is limited understanding how pretrained models multilingual capability enables crosslingual reasoning of long CoTs, which our work aims to address. 2We release our code and artifacts at https://github.com/BatsResearch/crosslingual-testtime-scaling. 2 Test-time scaling and s1 Test-time scaling is new scaling paradigm where more computation budget is allocated for LLMs at inference time before committing to an answer [1]. One example of test-time scaling is scaling up model generation length, particularly for RLMs with long CoTs capability [1, 30, 6, 5]. In particular, the s1 work [9] demonstrates the effectiveness of simple test-time scaling recipe: reasoning finetuning on small amount of training data with long CoTs (specifically 1k samples distilled from larger RLMs such as DeepSeek-R1) and scaling up inference budget at test time. Through test-time scaling of 32B-parameter model, the authors achieve the state-of-the-art mathematical reasoning performance, and their models even rival industry-grade RLMs such as o1-mini [6]. Nonetheless, similar to aforementioned RLMs literature, exploration of test-time scaling paradigm mostly evaluate on English math benchmarks [1, 11, 4, 15, 3, 9]. Here, our work focuses on understanding how effective test-time scaling of English-centric RLMs, specifically s1 models, in multilingual settings for various domains. Multilingual reasoning Multilingual reasoning encompasses the ability of language models to perform complex reasoning tasks across different languages. Early work has demonstrated that chainof-thought prompting in English can significantly improve performance on multilingual mathematical reasoning tasks [31], which suggests that LLMs might rely on dominant languages like English as pivot language for complex reasoning. Follow-up work explores several strategies such as translating the multilingual queries to English [32, 33, 34], aligning of latent representation spaces [35, 36] and reasoning outputs [37, 38] across languages, or expanding language coverage of reasoning training data [39]. Our work focuses on understanding how controlling the length of long CoTs and their reasoning language at test time affects multilingual reasoning. One similar work [17] experimented with controlling generation lengths of finetuned DeepSeek-R1-1.5B [5] but reported negative results: increasing thinking tokens leads to minimal performance gains for mathematical reasoning in nonEnglish languages. We believe that their negative findings are due to constrained model parameters, as we show that larger models can benefit from crosslingual test-time scaling."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Models We use s1 models [9] as our English-centric RLMs. In particular, we work with the s1.1 variants, which are multilingual Qwen2.5-Instruct models finetuned on 1k English-only reasoning data generated by DeepSeek-R1. We choose s1 models for two reasons: (1) their English mathematical reasoning capability reaches state-of-the-art performance, and (2) the models and the training data are fully open-sourced. We experiment with s1 models at different scalesspecifically with 1.5B, 3B, 7B, 14B, and 32B parameters. Budget forcing Budget forcing refers to techniques for controlling inference budget for long CoTs [9], which can be done in two ways: (1) truncation, which cuts off long CoTs after they reach maximum thinking tokens, or (2) extrapolation, which adds tokens such as Wait at the end of CoTs to force the model continue reasoning. In our extrapolation setup, we experiment with adding Wait only once since we do not observe significant performance gains from lengthening CoTs. Evaluation data For research questions RQ1, RQ2, and RQ3, we use the Multilingual Grade School Math (MGSM) benchmark [31]. It contains 250 grade-school math problems from the GSM8K dataset that have been manually translated into ten languagesBengali (bn), German (de), Spanish (es), French (fr), Japanese (ja), Russian (ru), Swahili (sw), Telugu (te), Thai (th), and Mandarin Chinese (zh). We check for 8-gram overlap between s1s training samples and MGSM (including the English subset) following standard practice [40], and we observe no traintest overlap. For RQ4, addressed in section 7, we evaluate on three different cross-domain benchmarks: 1. Global-MMLU [41]: multilingual translated version of the original MMLU dataset [42] that spans different subject topics, including STEM, business, humanities, medical, social sciences, etc. Additionally, it also contains annotations that identify whether the question is culturally agnostic or specific, allowing for more fine-grained analysis. 2. Food ORiented cultural commonsense Knowledge (FORK) [43]: manually-curated set of commonsenseQA-style [44] questions in English for probing cultural biases and assumptions on food-related customs around the world. 3 Figure 1: Crosslingual test-time scaling of s1 and Qwen models on the MGSM benchmark (excluding English) across different model sizes. In subfigure (a) we enforce hard limit of maximum thinking token, and in (b) we measure their inference FLOP compute for Pareto frontier analysis. measures the absolute difference between average accuracy scores at 0.5k and 8k maximum thinking tokens. Dash lines indicate the best few-shot prompting baseline performance of Qwen. 3. COPAL-ID [45]: manually-curated datasets following COPAs causal reasoning format [46] in Indonesian languages to evaluate causal reasoning with Indonesian cultural nuances. We use the lm-evaluation-harness library [47] as the main evaluation framework.3. We evaluate with greedy decoding and report task accuracy, which is equivalent to pass@1. Baselines Qwen2.5-Instruct models, which are the base models of s1, are our main baseline models. For MGSM, we apply two prompting strategies. The first is zero-shot prompting, and the second is few-shot CoT prompting using the provided eight CoT examples in MGSM train set. For the latter, we follow [31] and experiment with prompting with few-shot CoT in English (EN-CoT) and in the same language as input prompt (native-CoT). In addition, for MGSM, we benchmark against prior state-of-the-art models [48, 37, 49, 35, 50] and comparable English-centric RLMs such as R1-distilled-Qwen models [5] open-sourced by DeepSeek. For cross-domain benchmarks, we simply compare against zero-shot prompting of Qwen2.5-Instruct."
        },
        {
            "title": "4 Crosslingual Test-Time Scaling",
            "content": "In RQ1, we explore test-time scaling in zero-shot crosslingual setting, where English-centric reasoning models are applied to math problems in different languages. These models were finetuned from multilingual pretrained models in the same domain, therefore carrying high potential for crosslingual transfer. 4.1 Effectiveness of Crosslingual Test-Time Scaling Crosslingual generalization of reasoning training and test-time scaling We report two main obsevations from Figure 1 (a). First, we observe that s1 outperforms Qwens few-shot prompting baseline across languages in MGSM (excluding English) when given high inference thinking budget. Second, crosslingual test-time scaling is effective for models with 3B parameters and above, with s1 at 14B size having the largest accuracy gain of +9.4% when maximum inference budget increases from 0.5k to 8k thinking tokens. We want to highlight that sufficient model capacity is necessary for effective crosslingual test-time scaling, as test-time scaling only yields minimal benefits at 1.5B 3For cross-domain benchmarks, we use the configuration generate_until tasks instead of the default multiple_choices setting for cross-domain benchmarks and extract answers from generations using GPT4o-mini (see Appendix E.1). For Global-MMLU, we use the default subset of the benchmark and pick 7 languages that overlap with the MGSM benchmark. 4 Table 1: MGSM performance comparison against 14B-sized s1 model with maximum 8k thinking tokens. We report the language-breakdown accuracy from cited papers if available; otherwise, we reproduce using their open-sourced models without any inference budget constraint. We report the average length of the generations (avg. len) and the relative accuracy difference (green text) between s1-14B under extrapolation budget forcing and its baseline Qwen2.5-14B-Instruct. We bold both s1 performance and baseline models that outperform s1. es Models avg len AVG bn sw de ru en zh th ja fr te Qwen2.5-14B-Instruct [16] + 8-Shot EN-CoT [31] + 8-Shot Native-CoT [31] s1-14B (truncation) s1-14B (extrapolation) Relative accuracy difference (%) MetaMath-13B [52] MetaMathOctopus-13B [37] MAPO-DPO-13B [37] SLAM-13B [49] MetaMath-LB-15B [35] MetaMath-LB-20B [35] R1-Distill-Qwen-14B [5] R1-Distill-Qwen-32B [5] Gemma-3-12B-it [50] Gemma-3-27B-it [50] Qwen3-14B [50] 413.1 316.5 365.2 1912.9 2352.3 529.8 545.8 552.4 101.5 93.2 93. 1030.7 1353.8 238.2 461.7 1575.2 74.0 77.2 79.2 82.0 82.8 +11.9% 77.6 75.2 77.2 84.8 86.8 +11.9% 82.0 87.6 88. 92.8 92.4 +12.7% 77.6 86.0 87.2 88.4 86.4 +11.3% 67.6 68.4 68.4 85.2 83.2 +23.1% 70.4 76.8 76. 83.6 83.2 +18.2% 76.4 76.4 75.6 86.8 88.8 +16.2% 40.4 45.6 46.8 55.6 57.2 +41.6% 50.8 52.0 53. 59.6 58.0 +14.2% 6.8 41.6 54.7 45.6 50.0 52.8 66.0 77.6 55.6 64.8 85.2 64.4 60.1 69.5 62.8 63.6 64.0 77.2 82.8 74.4 83.2 83.6 70.4 66.8 70.5 71.2 67.6 66. 83.6 85.2 83.2 88.4 94.8 63.6 61.1 70.6 67.6 63.2 60.4 80.4 85.6 81.2 84.0 88.4 65.2 60.8 71.3 65.2 61.6 64.0 74.4 79.6 64.8 72.4 87.2 47.6 57.3 69.0 54.0 42.0 45. 78.4 83.2 74.0 79.2 77.6 60.0 59.1 68.2 64.4 60.0 58.8 82.4 84.8 74.8 83.2 94.0 11.6 50.9 62.9 46.4 41.6 49.2 22.4 38.0 71.2 78.0 63.6 0.8 3.6 4.0 2.4 36.4 47. 22.4 14.4 73.2 76.0 80.0 78.8 79.2 80.4 85.2 84.8 +7.6% 4.8 52.1 64.7 47.6 52.8 53.6 74.8 82.4 78.4 84.4 86.8 84.0 84.4 83. 86.4 87.6 +4.3% 50.8 53.1 68.2 58.8 48.0 52.4 79.6 85.6 79.2 84.4 85.2 70.9 73.5 74.1 80.9 81.0 +14.2% 40.5 51.5 61.2 53.3 53.5 55. 67.4 72.7 73.6 79.8 84.2 size. The model only experiences +1.8% accuracy gain with performance peak at 4000 maximum thinking tokens. Our findings contrast the conclusion drawn by recent work [17] that test-time scaling may not generalize as effectively to multilingual tasks. Our results suggest that the limitation observed by [17] is due to their usage of 1.5B models in their experiments, instead of an inherent weakness of testtime scaling. Furthermore, [17] posit that multilingual generalization of test-time scaling would occur for significantly larger models with at least 70B parameters, but we discover substantially smaller parameter threshold at 3B parameters, above which models consistently benefit from crosslingual test-time scaling. Pareto frontier analysis Figure 1 (b) illustrates the performance-efficiency trade-off across different sizes of the s1 model family on MGSM. We follow prior test-time scaling work [1, 51] and compute the inference cost using the approximation FLOPs = 2N Dinference where represents model parameters and Dinference the total number of tokens generated at inference time, and we average across different languages. The figure further demonstrates the model capacity constraint on test-time scaling: The 32B and 14B models can reach accuracy levels that are simply unattainable for the smaller models, no matter how much test-time compute is allocated to those smaller models. Even at their maximum performance ceiling, the 3B and 7B models cannot match what the larger models achieve with moderate computational resources. While we observe accuracy-to-computation tradeoffs (i.e., better performance comes with using larger models and higher test-time compute), the 14B model offers compelling compromise by achieving above 80% accuracy with substantially lower inference FLOPs than the 32B model, representing \"sweet spot\" on the Pareto frontier for practical applications. 4.2 Performance Comparison on MGSM Benchmark Comparison against Qwen baselines. Table 1 shows that with cross-lingual test-time scaling, s1 gains substantial accuracy increase as compared to different baselines with Qwen2.5 models. Furthermore, crosslingual test-time scaling benefits both high-resource and low-resource languages. For instance, fr receives significant +23.1% relative accuracy increase, whereas swthe worstperforming language for the base model Qwenreceives +41.6% relative accuracy improvement. Lastly, we observe similar performance for both truncation and extrapolation budget forcing strategies. This is because s1 models are already generating extensive reasoning chains so further lengthening the CoTs have minimal benefits. Similar trends of language-specific improvements are also observed in other s1 model sizes (Appendix B). 5 Box 1. Example of quote-and-think language-mixing pattern demonstrated by s1 where non-English phrases are mentioned in quotation marks and used for reasoning. Question: ã­ã¼ããä½æããã«ã¯éè²ã®ç¹ç¶­ã2å·»åç½è²ã®ç¹ç¶­ããã®ååç¨ ããå¿è¦ãããã¾ãå¨ä½ã§ä½å·»å¿è¦ã§ãã (To make robe, you need 2 rolls of blue fiber and half of the white fiber. How many rolls are needed total?) s1s Thinking: Okay, lets see. ... The problem says \"ç½è²ã®ç¹ç¶­ããã®ååç¨ãã\" which is white fibers the half amount. \"ãã®\" refers to the blue fibers, right? So half of the blue fibers, which is 2 rolls, so half is 1. So 2 + 1 is 3. Yeah, that seems right. Wait, sometimes these problems can be tricks, but dont think so. If it had said \"ç½è²ã®ç¹ç¶­ ã2å·»åã¨ãã®åå\" that would be different, but no. Its \"éè²ã®ç¹ç¶­ã2å·»åç½è²ã® ç¹ç¶­ããã®åå\". So white is half of blue. So calculation is straightforward. Blue 2, white 1, total 3. ... Comparison against state-of-the-art models. Table 1 shows that crosslingual test-time scaling of s1 models can serve as strong multilingual baseline for MGSM, as it outperforms all prior state-of-the-art models that involve finetuning on multilingual data such as MetaMath, MAPO, and [52, 37, 49, 35]. We believe this is because these prior studies use Llama as their base models, which generate significantly shorter reasoning traces and lack sophisticated reasoning behaviors such as verification and backtracking compared to Qwen models [23]. Surprisingly, 14B-sized s1 can even outperform recent state-of-the-art reasoning models twice its size, namely DeepSeeks R1-Distill-Qwen-32B [5] and Googles Gemma-3-27B-it [50]. We observe that R1-Distill-Qwen has substantially poorer performance on sw and te, suggesting that their 800k samples of English and Chinese training data [5] leads to catastrophic forgetting of lower-resource languages. In contrast, s1 is only trained with 1k English samples for only 5 epochs [9], which leads to minimal forgetting and better crosslingual generalization. While the multilingual Gemma-3 models outperform s1 on low-resource languages, probably due to these languages being incorporated during reasoning finetuning, its performance gap against s1 on high-resource languages may be attributed to the shorter reasoning thinking time. Qwen3 [53] is the most performant model, probably due to its long reasoning capability and extensive multilingual training data."
        },
        {
            "title": "5 Language-Mixing Behaviors",
            "content": "We notice that s1 models can mix languages in their CoT reasoning under inference scaling. Given lack of systematic study of language mixing behaviors in crosslingual reasoning in prior work [5, 11], we present an analysis of linguistic behaviors of s1 in multilingual math reasoning tasks to address RQ2. In particular, we are interested in two sub-questions of RQ2. First, how does the dominant languagethe primary language of generated response, also known as matrix languagechange after English-centric reasoning finetuning? Second, what are the language-mixing patterns exhibited by s1 in its reasoning outputs? We focus on four languages, namely ja, ru, th, and zh, and we refer our readers to Appendix C.1 for detailed methodology. 5.1 Dominant Language in Model Outputs Figure 2 shows that s1 and its base model Qwen use entirely different dominant languages in their overall responses in multilingual settings. Qwen models generate responses in the same language as the questions, especially for zero-shot and native CoT prompting settings. Surprisingly, even in crosslingual input settings where the 8-shot CoT reasoning samples are provided in English, Qwen models still stick to generating outputs in the same language as the question. In contrast, s1 generates reasoning and final answers with English as the dominant language, which suggests that finetuning on only 1k English reasoning data can sufficiently change model behavior from multilingual to English-dominant. Similar behaviors are observed for models at smaller parameter sizes (Figure 8). 6 Figure 2: Proportion of dominant languages in models entire responses when queried with multilingual math questions. same indicates that the response language is the same as query language. Figure 3: Breakdown of language-mixing patterns in s1s reasoning. Percentage indicates the probability of sentence being English only, quoting non-English phrases (quote-and-think), entirely being in different language (intersentential), or mixing different languages within the same sentence (intrasentential). 5.2 Language-Mixing Patterns During Reasoning Dominant mixing pattern Figure 3 shows that s1s reasoning is predominantly in English for all four languages of study. Specifically, at least 92.5% of the sentences in s1s CoTs are in English only and do not mix languages. However, in the remaining cases when s1 mixes languages during reasoning, it primarily follows sophisticated pattern to which we refer as quote-and-think. Particularly, s1 will first quote certain words or phrases, often from the input question, and then interpret their meanings and implications during its thinking process. This is demonstrated by the quoted phrase ç½è²ã®ç¹ç¶­ããã®ååç¨ãã and s1s literal translation white fibers the half amount in Box 1. In linguistics, this type of language-mixing is known as foreign-language quotation [54]. This language-mixing behavior happens due to crosslingual generalization of the quoting-and-thinking reasoning characteristic in s1s English finetuning data (see Appendix C.4). We want to emphasize that the quote-and-think pattern goes beyond simple translation. As demonstrated in Box 1, s1 builds upon the extracted phrase and synthesizes new multilingual setting where if the question had asked ç½è²ã®ç¹ç¶­ã2å·»åã¨ãã®åå (two and half rolls of white fiber) it would have arrived at different answer. Here, the model shows an understanding of how the syntactic structure in Japanese affects the semantic meaning of the math problem, which suggests that s1 is genuinely parsing and reasoning about the mathematical relationships expressed in Japanese and not merely translating the content to English before processing. This suggests that the multilingual capability of the base models is preserved for natural language understanding and allows s1 to reason about what it has understood about the question. Other language-mixing patterns Figure 3 shows that, compared to other languages, Russian exhibits intersentential language-mixingmixing of sentences of different languagesthe most, where English sentences are switched into Russian sentences and then back to English in the middle of the reasoning. This is undesirable [5] because it can create confusion for users expecting coherent response in single language and thus impacting readability. For intrasentential languagemixingmixing of words of different languages in single sentencewe perform deeper analysis (see Appendix C.5) and report that at least 70% of cases resemble quote-and-think as the non-English phrases are extracted from the question prompt, but no quotation marks were provided. This further confirms that quote-and-think is the dominant pattern of language mixing in s1s reasoning process, even when not explicitly marked, and represents natural strategy the model has developed to handle multilingual mathematical reasoning."
        },
        {
            "title": "6 Language Forcing",
            "content": "When multilingual user interacts with LLMs, it is natural to expect the LLMs to respond in the language consistent with the users query. Therefore, in RQ3, we are interested in understanding if 7 Table 2: Performance comparison of different language forcing strategies where we force s1 to reason in the same language as the question with 8k maximum thinking tokens. Languages are categorized into high-resource (HRL: de, en, es, fr, ru, ja, zh) and low-resource (LRL: bn, sw, te, th) groups. We use or to indicate if the language forcing method outperforms or underperforms the baseline of thinking in English. Model Method s1-32B s1-14B s1-7B Baseline translated_wait prefix system combined Baseline translated_wait prefix system combined Baseline translated_wait prefix system combined Accuracy HRL 91.2 91.3 91.3 90.3 90.5 90.3 90.6 90.7 89.5 89.0 86.9 85.8 84.0 84.8 84.5 LRL 80.8 81.7 75.7 79.3 73.4 73.9 75.2 66.9 73.8 65.4 53.6 53.6 44.2 52.7 44.4 ALL 87.4 87.8 85.6 86.0 84.3 84.4 85.0 82.0 83.8 80.4 74.8 74.1 69.5 73.1 69.9 Language Compliance LRL ALL HRL 0.0 19.4 96.2 47.7 98. 0.1 25.4 96.4 35.4 97.5 0.0 27.1 89.8 70.5 96.0 0.0 30.4 99.4 71.4 99.7 0.2 26.5 99.6 53.0 99.9 0.0 41.0 96.9 96.6 98.0 0.0 0.2 90.7 6.2 96. 0.0 0.3 89.2 4.6 93.1 0.0 2.7 77.3 24.9 92.6 we can perform language forcingcontrolling an English-centric RLM to generate reasoning in particular languageand if the difference in reasoning language affects performance. 6.1 Methodology: Language Forcing Techniques We experiment with the following language forcing techniques to control s1s reasoning language: Translated Wait (translated_wait): Building upon extrapolation budget forcing strategy that explicitly extends reasoning traces [9], once the model finishes English reasoning, we append translated Wait token as an intervention strategy to force the model to switch language and continue reasoning in our chosen language. Prefix (prefix): We appended prefix string translation-equivalent of Okay, let me try to figure this out. at the beginning of the reasoning generation in order to guide the models generation in our chosen language. We also apply the translated_wait strategy and append the translated Wait token. System Prompt (system): We use system prompt to control the language use in model generation. Specifically, we translate the system prompt You are helpful assistant.4 into our chosen language and add the translation-equivalent of the instruction You must think and answer only in {language}. Combined (combined): This method uses all the techniques above to maximize control over the models reasoning language. We experiment with language forcing on the MGSM benchmark with = 11 languages. Given question in language (i.e., query language), we force the model to reason in language (i.e., reasoning language). We compute two evaluation metrics: task accuracy and language compliance. The former follows the setup described in Section 3, whereas the latter measures the proportion of tokens generated in the intended reasoning language relative to tokens produced in other languages. In other words, language compliance quantifies the effectiveness of s1 in following explicit language constraints. To only measure the effect of reasoning language and eliminate the confounding factors such as introduction of prefix string, our baseline is to let s1 models think in its dominant language English using the combined setup. 4We remove the part of You are Qwen, created by Alibaba Cloud. because English proper nouns like Qwen and Alibaba do not have translation equivalents in many non-English languages. 8 Table 3: Performance scores across different reasoning languages given query language. We use 11 color codes to rank each row to highlight the high- (blue) and low-performing (red) reasoning language given query language. We also bold the best-performing reasoning language. Lastly, we use to indicate the average accuracy when the reasoning language is the same as query language (i.e., average of the diagonals). Query Language bn de en es fr ja ru sw te th zh bn 79.2 88.4 93.2 86.4 87.2 79.2 89.2 45.6 53.2 80.8 85.2 de 85.2 89.2 94.4 92.4 87.2 84.8 91.2 58.8 56.4 88.4 86.8 AVG 78.9 83. en 86.8 90.4 94.4 93.6 88.4 83.6 92.4 59.6 60.0 89.2 89.6 84.4 6.2 In-Language Forcing Reasoning Language ja ru fr 81.6 90.8 94.8 92.4 88.0 85.6 93.6 55.6 60.0 91.2 86.8 81.2 90.0 94.4 90.8 89.6 82.0 92.0 47.6 57.2 87.2 88.8 83.6 87.6 93.2 93.2 88.4 84.8 92.4 48.4 55.2 87.2 90.8 es 84.4 88.8 95.2 93.6 87.2 81.6 89.6 55.2 56.4 88.4 87.2 sw 62.4 75.6 84.0 76.6 72.8 71.6 77.6 44.4 34.8 66.4 73.6 te 75.6 78.4 84.0 82.8 77.6 74.0 80.8 32.4 54.4 69.2 77.2 th 80.8 88.0 94.8 90.0 87.2 85.6 90.0 45.2 53.6 86.4 86.0 zh 81.2 89.6 96.8 90.8 88.0 83.6 91.2 52.0 52.8 88.8 89.2 82.6 83.7 81. 82.3 67.3 71.5 80.7 82.2 Range (max - min) 24.4 15.2 12.8 17.0 16.8 14.0 16.0 27.2 25.2 24.8 17.2 81.2 In-language forcing refers to the setting where the reasoning language matches the query language, i.e., = k. Here, we focus on analyzing if we should perform in-language forcing for English-centric RLMs or let models reason in their natural reasoning language (i.e., English for s1). We showcase an example of s1 thinking in Japanese due to language forcing in Appendix D.1. Reasoning in high-resource languages As shown in Table 2, reasoning in high-resource languages (HRLs) achieve similar scorestypically within 1-2 points difference compared to baselineregardless of language forcing strategy. One possible explanation is that these languages are highly represented within pretraining of Qwen, which allows for effective crosslingual transfer from English reasoning finetuning. Reasoning in low-resource languages Table 2 highlights that, in contrast to HRLs, for lowresource languages (LRLs), combined strategy substantially underperforms the baseline. This suggests that English reasoning finetuning does not transfer to LRLs as effectively, and thus s1 is relatively incapable of reasoning in LRLs. Strategies that permit mix of English and the target languageparticularly translated_wait and systemoutperform strict in-language approaches such as combined. This is because s1 can lean on its English reasoning ability while incorporating the target language, which appears to be beneficial in scenarios with limited language-specific training data. For example, in (te) and (sw), the 14B and 32B models using translated_wait consistently outperform those using prefix or combined strategies by 45 accuracy points (see Appendix D.3 for detailed results). Trade-off between accuracy and language compliance Table 2 shows that we can successfully control the reasoning language of RLMs in their long CoTs, but that usually come at cost of task performance. For instance, for both 32Band 14B-sized s1 models, the combined strategy achieves near 100% language compliance for both HRLs and LRLs, but it yields the worst accuracy compared to other language forcing strategies. For 7B-sized models, we observe that we need combination of all strategies (i.e., combined) to achieve high language compliance. This suggests that it is non-trivial to control the reasoning language, as different choices of language forcing strategy can significantly affect how consistently s1 adhere to our specified reasoning language. 6.3 Crosslingual Language Forcing Given our success in controlling the reasoning language of s1 with combined strategy, we explore if there is particular language that is best served as reasoning language for s1. Particular, for each 9 Figure 4: Language and domain breakdown for Global-MMLU benchmark. Dashed lines indicate the performance of zero-shot prompting of Qwen-32B-Instruct models. query language , we force the model to all possible languages, resulting in an exhaustive query-reasoning language-pair analysis.5 Performance comparison of reasoning languages Table 3 shows that reasoning in HRLs such as en, fr, or de yield similarly high performance (the accuracy difference is within 1 to 2 points), with English being the most performant reasoning language and French being the close second. We discover two surprising findings: first, even though the Qwen2.5 base model is highly pretrained in Chinese [16], it is not necessarily the best reasoning language, even when the question is asked in zh; second, neither reasoning in en nor in query language necessarily yields the best performancequite the contrary, even reasoning in languages that are usually less represented in pretraining data [55] such as ru and th can achieve the best performance for query languages in other families such as ja. Lastly, we observe that languages that are considered as slightly less-resourced [55] such as th and bn still achieve nearly 80% overall accuracy, but further lower-resourced languages such as sw or te result in substantially lower overall accuracy. Inference cost analysis Our analysis of inference costs across reasoning languages in Figure 5 reveals significant negative correlation (-0.811) between token count and mathematical problem-solving accuracy. Reasoning in LRLs not only underperform their HRL counterpart (with accuracy below 80%), but they also demand substantially more computational resources at test-timeoften exceeding 3,000 tokens in their long CoTs for bn, sw, and te. This token efficiency disparity at test-time translates directly to computational costs; for instance, reasoning in Swahili requires approximately 3.5 times more compute than French for the same tasks. This is very likely due to the well-known tokenization disparity for LRLs, where tokenized lengths for an equivalent sentence in different languages are much larger for LRLs [56, 57]. Figure 5: MGSM accuracy against number of thinking tokens in s1 models outputs in different reasoning languages. 5This analysis is is computationally heavy, so we only focus on 14B-sized s1 models. 10 Figure 6: Effects of thinking time for s1 models on different domains of Global-MMLU benchmark (subfigure (a)) and cultural commonsense knowledge (FORK) and reasoning (COPAL-ID) benchmarks (subfigure (b)). Similar to Figure 4, we added dashed lines as zero-shot prompting of Qwen-32B as baselines in (b). Choice of query language Table 3 sheds light on whether we should translate inputs into HRLs such as English for reasoning tasks, which has proven to be an effective strategy [32, 33]. Our results are consistent with prior work: merely translating the question from Swahili to French can boost the accuracy from 59.6 to 90.8 even when the model reasons in Frencha language that s1 is not trained to reason in. Besides, based on the range column, which measures difference between the best and worst reasoning languages for particular query language, the model is less sensitive to query language in HRLs than in LRLs as exhibited by the smaller range. In other words, querying s1 in HRLs increases the models consistency in achieving the same correct answer with different reasoning languages."
        },
        {
            "title": "7 Cross-Domain Generalization",
            "content": "Since s1 models obtain strong crosslingual math performance with English-only training, natural question to ask is whether such generalization extends to other non-math domains that may require knowledge recall or cultural reasoning. We address this research question RQ4 using the cross-domain benchmarks described in Section 3. 7.1 In-domain generalization STEM domains Figure 6 (a) shows that performance on STEM subject domain in Global-MMLU improves with test-time scaling of thinking tokens (cyan line). Furthermore, s1 outperforms its base model Qwen2.5-32B-Instruct (black dashed line for cyan blocks in Figure 4) by large margin, especially for bn and sw where both gain +19.8% (56.5% 76.1%) and +13.1% (82.6% 95.7%) accuracy respectively. Given that s1 training data includes OlympicArena dataset [58] that encompasses various STEM subject knowledge such as biology and astronomy, our results here further corroborates Section 4 findings on strong crosslingual in-domain generalization. 7.2 Out-of-domain generalization non-STEM domains We report two main observations. First, Figure 4 shows that the benefits of reasoning finetuning are domain-specific. For instance, business and social sciences domains gain slight improvements, but the medical domain experiences up to nearly 10% decrease in accuracy even for HRLs such as en and fr compared to the baseline Qwen model. Second, we report minimal cross-domain generalization of test-time scaling from Figure 6 (a). Domains such as medicine do not benefit from scaling up thinking tokens, as increasing maximum thinking tokens from 0.5k 11 Figure 7: Performance comparison between s1 and zero-shot prompting of Qwen baseline for culturally-agnostic (C-Agnostic) and culturally-specific (C-Specific) questions in different domains. Results are average across languages. to 4k tokens merely improves accuracy by only +0.8% (73.0% 73.8%), and further scaling to 8000 thinking tokens even reduces accuracy by 2.0% (73.0% 71.0%). Out of all non-STEM domains, business benefits the most from test-time scaling (+3.2%), but the accuracy gain still lags behind STEM domain (+11.5%) by huge margin. Cultural-specific knowledge and reasoning For cultural commonsense knowledge and reasoning benchmarks, we observe similar findings that there is minimal benefits of test-time scaling of s1. Figure 6 (b) shows that while reasoning finetuning improves overall model performance over Qwen baselines (dashed lines), scaling up test-time thinking compute does not improve performance. In fact, for the English FORK benchmark, increasing thinking tokens leads to substantially poorer performance. This is also known as overthinking [59] where reasoning models expend excessive compute in their long CoTs and lead to worse performance [60, 61, 62]. As for Global MMLU, which contains additional annotations for whether questions belong to culturespecific or culture-agnostic types, Figure 7 shows that there is no obvious pattern explaining which type benefits the most from the s1-training. For the STEM category, the improvements are almost equivalent. However, in other domains, we observe inconsistent performance change. For instance, we observe that performance drops in the humanities subject mostly belong to the culture-specific type while the opposite for the medical domain. We also experiment with language forcing (using the combined strategy from Section 6) to see if reasoning in the same language as the question helps with performance, but we observe minimal and often inconsistent performance gains from language forcing (see Appendix E.2 for more details)."
        },
        {
            "title": "8 Discussion and Future Work",
            "content": "Data-efficient English reasoning finetuning One notable finding from our work is that s1 remains capable of generating text in different languages and experiences minimal catastrophic forgettinga phenomenon where the model loses its ability to generate fluent text in other languages after languagespecific supervised finetuning [63, 64]. In contrast, R1-Distill-Qwen baseline experiences significant catastrophic forgetting for low-resource languages. This suggests that data-efficient finetuning with small number of reasoning finetuning steps (s1 is only trained with 1k English samples for 5 epochs) is advisable for English-centric reasoning finetuning to preserve multilingual capability. Reasoning finetuning with multilingual data Given the limitations of crosslingual test-time scalingpoor reasoning in low-resource languages and in cross-domain settingsone potential solution would be to curate multilingual reasoning training data with wide language and domain coverage. However, the current practice of generating multilingual reasoning training data simply through translation technology [39, 17] is insufficient, as it is well-established that translation models including LLMs still suffer from poor cultural alignment and Western-centric bias [65, 66] as well as poor translation performance with low-resource languages [67, 68, 69]. Future work 12 should systematically explore the effectiveness of multilingual augmentation techniques such as back-translation or synthetic data generation [70, 71]. Reasoning in low-resource languages (LRLs) Our results indicate poor performance of using LRLs as reasoning language. Furthermore, unfairness in tokenization for LRLs [56] leads to significantly higher inference costs for test-time scaling of RLMs. This disparity creates accessibility barriers for multilingual communities, as deploying reasoning models in LRLs becomes prohibitively expensive. Future work should focus on developing more equitable tokenization strategies for reasoning across diverse languages [72, 73, 74]. Crosslingual generalization for small RLMs Our observation of successful crosslingual test-time scaling happening at 3B parameter threshold corroborates contemporary work that shows small models below 3B parameters struggle to learn from long CoTs [75] and generalize in multilingual settings [17]. While there are ongoing work creating English-centric RLMs smaller than 3B size [28], we argue that future work on small RLMs should benchmark on multilingual reasoning tasks rather than solely on English data. This would not only advance the development of multilingual reasoning models but also help identify factors that enable crosslingual reasoning."
        },
        {
            "title": "9 Conclusion",
            "content": "Our work studies the effectiveness of test-time scaling of English-centric reasoning language models for multilingual math reasoning, and we perform pareto frontier analyses of different model sizes and inference thinking compute. Furthermore, we document the language-mixing patterns naturally exhibited by reasoning models and report the novel quote-and-think pattern. We also investigate the effects of the choice of reasoning languages as well as query languages, demonstrating that successful generalization of English reasoning finetuning to other high-resource languages. Finally, we observe minimal cross-domain generalization of test-time scaling, thus highlighting the need for future work to explore more robust and domain-agnostic reasoning strategies, particularly for multilingual applications."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank Nihal Nayak and Peilin Yu from BATS research group at Brown University for helpful discussions. Disclosure: Stephen Bach is an advisor to Snorkel AI, company that provides software and services for data-centric artificial intelligence."
        },
        {
            "title": "References",
            "content": "[1] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [2] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher RÃ©, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [3] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. [4] Noam Levi. simple model of inference scaling laws. arXiv preprint arXiv:2410.16377, 2024. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [6] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 13 [7] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, April 2025. [8] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [9] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [10] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [11] Akash Ghosh, Debayan Datta, Sriparna Saha, and Chirag Agarwal. The multilingual mind : survey of multilingual reasoning in language models, 2025. [12] Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling. arXiv preprint arXiv:2501.11651, 2025. [13] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [14] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. In ICLR, 2024. [15] Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682, 2025. [16] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [17] Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407, 2025. [18] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [19] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, Vancouver, Canada, July 2017. Association for Computational Linguistics. [20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [21] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. [22] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations, 2024. 14 [23] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [24] Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. Evolving deeper llm thinking. arXiv preprint arXiv:2501.09891, 2025. [25] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. [26] Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoningdistillation, 2025. Accessed: 2025-01-22. [27] Sathwik Tejaswi Madhusudhan, Shruthan Radhakrishna, Jash Mehta, and Toby Liang. Millions scale dataset distilled from r1-32b. https://huggingface.co/datasets/ServiceNow-AI/R1-DistillSFT, 2025. [28] Shangshang Wang, Julian Asilis, Ãmer Faruk AkgÃ¼l, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. Tina: Tiny reasoning models via lora. arXiv preprint, April 2025. [29] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [30] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. [31] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023. [32] Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual In Houda prompting: Improving zero-shot chain-of-thought reasoning across languages. Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 26952709, Singapore, December 2023. Association for Computational Linguistics. [33] Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question translation training for better multilingual reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 84118423, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [34] Hyunwoo Ko, Guijin Son, and Dasol Choi. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap. arXiv preprint arXiv:2501.02448, 2025. [35] Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. LangBridge: Multilingual reasoning without multilingual supervision. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75027522, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [36] Zixian Huang, Wenhao Zhu, Gong Cheng, Lei Li, and Fei Yuan. Mindmerger: Efficiently boosting LLM reasoning in non-english languages. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 15 [37] Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. MAPO: Advancing multilingual reasoning through multilingual-alignment-as-preference optimization. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001510027, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [38] Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, and Jiajun Zhang. Language imbalance driven rewarding for multilingual self-improving. In The Thirteenth International Conference on Learning Representations, 2025. [39] Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246, 2023. [40] Andy Zhang, Kevin Klyman, Yifan Mai, Yoav Levine, Yian Zhang, Rishi Bommasani, and Percy Liang. Language model developers should report train-test overlap. arXiv preprint arXiv:2410.08385, 2024. [41] Shivalika Singh, Angelika Romanou, ClÃ©mentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation, 2024. [42] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [43] Shramay Palta and Rachel Rudinger. FORK: bite-sized test set for probing culinary cultural biases in commonsense reasoning models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 99529962, Toronto, Canada, July 2023. Association for Computational Linguistics. [44] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [45] Haryo Wibowo, Erland Fuadi, Made Nityasya, Radityo Eko Prasojo, and Alham Aji. COPALID: Indonesian language reasoning with local culture and nuances. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 14041422, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [46] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages 9095, 2011. [47] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [48] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. 16 [49] Yuchun Fan, Yongyu Mu, YiLin Wang, Lei Huang, Junhao Ruan, Bei Li, Tong Xiao, Shujian Huang, Xiaocheng Feng, and Jingbo Zhu. SLAM: Towards efficient multilingual reasoning via selective language alignment. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics, pages 94999515, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. [50] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [51] Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. [52] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. [53] Qwen Team. Qwen3: Think deeper, act faster, 4 2025. 2036 words, 10 min read. [54] Philippe De Brabanter. Foreign-language quotations and code-switching: The grammar behind. In ESSE Conference (European Society for the Study of English), 2004. [55] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online, July 2020. Association for Computational Linguistics. [56] Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. Language model tokenizers introduce unfairness between languages. Advances in neural information processing systems, 36:3696336990, 2023. [57] Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah A. Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of In The 2023 Conference on Empirical Methods in Natural commercial language models. Language Processing, 2023. [58] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1920919253. Curran Associates, Inc., 2024. [59] Ryan Liu, Jiayi Geng, Addison Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas Griffiths. Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. arXiv preprint arXiv:2410.21333, 2024. [60] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025. [61] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [62] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [63] Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina. BLOOM+1: Adding language support to BLOOM for zero-shot prompting. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1168211703, Toronto, Canada, July 2023. Association for Computational Linguistics. [64] Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. In The Twelfth International Conference on Learning Representations, 2024. [65] Pushpdeep Singh, Mayur Patidar, and Lovekesh Vig. Translating across cultures: LLMs for intralingual cultural adaptation. In Libby Barak and Malihe Alikhani, editors, Proceedings of the 28th Conference on Computational Natural Language Learning, pages 400418, Miami, FL, USA, November 2024. Association for Computational Linguistics. [66] Jonathan RystrÃ¸m, Hannah Rose Kirk, and Scott Hale. Multilingual!= multicultural: Evaluating gaps between multilingual capabilities and cultural alignment in llms. arXiv preprint arXiv:2502.16534, 2025. [67] Nathaniel Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. ChatGPT MT: Competitive for high- (but not low-) resource languages. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 392418, Singapore, December 2023. Association for Computational Linguistics. [68] Sara Court and Micha Elsner. Shortcomings of LLMs for low-resource translation: Retrieval and understanding are both the problem. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the Ninth Conference on Machine Translation, pages 13321354, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [69] Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, TegawendÃ© BissyandÃ©, and Jacques Klein. Is llm the silver bullet to low-resource languages machine translation? arXiv preprint arXiv:2503.24102, 2025. [70] Chenxi Whitehouse, Monojit Choudhury, and Alham Fikri Aji. LLM-powered data augmentation for enhanced cross-lingual performance. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 671686, Singapore, December 2023. Association for Computational Linguistics. [71] Zheng Xin Yong, Cristina Menghini, and Stephen Bach. LexC-gen: Generating data for extremely low-resource languages with large language models and bilingual lexicons. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1399014009, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [72] Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1314213152, Singapore, December 2023. Association for Computational Linguistics. [73] HyoJung Han, Akiko Eriguchi, Haoran Xu, Hieu Hoang, Marine Carpuat, and Huda Khayrallah. Adapters for altering LLM vocabularies: What languages benefit the most? In The Thirteenth International Conference on Learning Representations, 2025. 18 [74] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291306, 2022. [75] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. [76] Constanza Fierro, Negar Foroutan, Desmond Elliott, and Anders SÃ¸gaard. How do multilingual models remember? investigating multilingual factual recall mechanisms. arXiv preprint arXiv:2410.14387, 2024. [77] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Background and Related Work 3 Experimental Setup 4 Crosslingual Test-Time Scaling"
        },
        {
            "title": "4.2 Performance Comparison on MGSM Benchmark . . . . . . . . . . . . . . . . . .",
            "content": "5 Language-Mixing Behaviors"
        },
        {
            "title": "5.1 Dominant Language in Model Outputs . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5.2 Language-Mixing Patterns During Reasoning . . . . . . . . . . . . . . . . . . . . 6 Language Forcing 6.1 Methodology: Language Forcing Techniques . . . . . . . . . . . . . . . . . . . . 6.2 In-Language Forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Crosslingual Language Forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Cross-Domain Generalization 7.1 In-domain generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Out-of-domain generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Discussion and Future Work 9 Conclusion Authors Contributions Further Details on Crosslingual Test-Time Scaling Further Details on Language-Mixing Behaviors C.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Annotation for Language-Mixing Patterns in s1 Reasoning . . . . . . . . . . . . . C.3 Dominant Language in 14B-Sized Model Outputs . . . . . . . . . . . . . . . . . . C.4 Quote-And-Think Pattern in s1s Training Data . . . . . . . . . . . . . . . . . . . C.5 Fine-Grained Analysis of s1-32Bs Intrasentential Language Mixing . . . . . . . . Further Details on Language Forcing D.1 Example of CoTs after Language Forcing . . . . . . . . . . . . . . . . . . . . . . D.2 Language Compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 In-Language Budget Forcing Reasoning . . . . . . . . . . . . . . . . . . . . . . . 1 2 3 4 4 6 6 7 7 8 9 11 11 11 12 22 22 22 22 23 23 24 24 24 24 Further Details on Cross-Domain Generalization E.1 Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Language Forcing for GlobalMMLU . . . . . . . . . . . . . . . . . . . . . . . . . 25 25"
        },
        {
            "title": "Appendix",
            "content": "A Authors Contributions Zheng-Xin Yong led the project and ran most of the experiments. Muhammad Farid Adilazuarda and Jonibek Mansurov worked on language forcing. Ruochen Zhang worked on cross-domain generalization with GlobalMMLU. Niklas Muennighoff trained s1 models at different model sizes. Alham Fikri Aji, Stephen Bach, Julia Kreutzer, Genta Indra Winata, and Niklas Muennighoff advised the project at the early stage. All authors contributed to the paper writing. Further Details on Crosslingual Test-Time Scaling Table 4, Table 5, Table 6, and Table 7 shows the s1 performance against Qwen baselines on the MGSM benchmark. Relative accuracy difference measures the relative gains for s1 under extrapolation budget forcing compared to zero-shot prompting Qwen models (first row), except for Table 7 where the relative accuracy is measured for s1 under truncation budget forcing. Table 4: MGSM performance comparison against 32B-sized s1 model with maximum 8k thinking tokens. Models zh AVG avg len bn sw en ru de th ja es te fr Qwen-32B-Instruct + 8-Shot EN-CoT + 8-Shot Native-CoT s1-32B (truncation) s1-32B (extrapolation) Relative accuracy difference (%) 365.6 264.7 179.9 1682.1 2610.1 82.0 82.0 82.4 89.2 89.2 +8.8% 79.6 80.4 78.0 86.8 88.8 +11.6% 84.0 89.6 90.0 94.8 93.2 +11.0% 80.0 84.8 87.2 90.0 89.6 +12.0% 69.2 66.8 66.8 84.8 83.6 +20.8% 80.8 85.2 80.8 84.8 84.8 +5.0% 77.2 77.6 74.0 91.2 88.8 +15.0% 54.0 56.8 57.2 66.8 70.8 +31.1% 56.4 55.6 59.6 65.6 68.4 +21.3% 84.4 84.8 87.2 87.2 88.0 +4.3% 84.0 84.8 88.0 88.8 88.0 +4.8% 75.6 77.1 77.4 84.5 84.8 +12.2% Table 5: MGSM performance comparison against 7B-sized s1 model with maximum 8k thinking tokens. Models zh AVG avg len bn sw de ru en th ja es fr te Qwen-7B-Instruct + 8-Shot EN-CoT + 8-Shot Native-CoT s1-7B (truncation) s1-7B (extrapolation) Relative accuracy difference (%) 537.7 537.8 480.1 3767.1 4363.5 59.2 62.0 65.2 65.2 70.8 +19.6% 69.2 72.4 74.4 82.8 84.0 +21.4% 78.0 86.4 90.4 88.8 90.4 +15.9% 72.8 78.8 76.4 86.0 83.6 +14.8% 66.4 62.4 65.2 82.0 84.4 +27.1% 67.2 68.0 71.6 78.8 74.8 +11.3% 71.2 76.8 68.8 86.4 84.4 +18.5% 13.6 15.2 18.4 21.6 19.2 +41.2% 33.2 33.2 20.8 38.8 36.4 +9.6% 68.8 70.4 69.6 80.0 78.4 +14.0% 79.6 76.8 76.8 83.6 82.8 +4.0% 61.7 63.9 63.4 72.2 71.7 +16.2% Table 6: MGSM performance comparison against 3B-sized s1 model with maximum 8k thinking tokens. Models zh AVG avg len bn sw de en ru th es ja te fr Qwen-3B-Instruct + 8-Shot EN-CoT + 8-Shot Native-CoT s1-3B (truncation) s1-3B (extrapolation) Relative accuracy difference (%) 1023.3 281.3 1657.0 4813.3 5367.1 37.6 48.0 36.0 56.8 55.2 +46.8% 58.8 67.6 63. 66.8 65.6 +11.6% 74.0 79.2 80.0 82.0 81.6 +10.3% 66.0 71.2 70.8 74.4 76.4 +15.8% 54.4 65.2 58. 69.6 71.6 +31.6% 54.8 58.0 52.0 60.4 60.8 +10.9% 64.8 70.4 62.0 72.4 74.4 +14.8% 9.2 12.4 9. 10.4 9.6 +4.3% 7.6 14.8 9.6 16.8 20.0 +163.2% 56.8 60.4 59.2 68.8 68.8 +21.1% 68.4 68.4 70. 72.0 74.0 +8.2% 50.2 56.0 51.9 59.1 59.8 +19.1% Further Details on Language-Mixing Behaviors C.1 Methodology To filter out language-mixed sentences, we first identify the dominant language, also known as matrix language, of the generated response using the state-of-the-art language identification library lingua. Then, we use the NLP library stanza to perform sentence segmentation according to the matrix language and obtain individual sentences. Finally, we use lingua to annotate the language label of each sentence and of each individual word token in the sentence. 22 Table 7: MGSM performance comparison against 1.5B-sized s1 model with maximum 8k thinking tokens. We didnt run extrapolation budget forcing since without it, s1 already generates extremely long CoTs. Models zh AVG avg len bn sw en ru de th es ja fr te Qwen-1.5B-Instruct + 8-Shot EN-CoT + 8-Shot Native-CoT s1-1.5B (truncation) Relative accuracy difference (%) 2991.7 1100.3 1729.9 8227. 10.4 21.6 14.0 35.6 46.4 44.4 27.6 +165.4% 51.2 +43.8% 66.0 70.0 71.6 66.8 +1.2% 52.8 58.0 52.4 41.2 55.2 41.6 31.2 37.2 34.4 43.6 51.6 39.2 2.0 2.8 3.2 1.2 6.4 1. 31.2 41.2 33.2 62.8 +18.9% 56.0 +35.9% 43.2 +38.5% 55.6 +27.5% 1.6 -20.0% 6.4 +433.3% 46.4 +48.7% 56.0 52.0 54.0 58.8 +5.0% 33.7 40.2 35.4 43.3 +28.5% We classify language-mixing patterns into three categories: (1) quote-and-think, where words or phrases in foreign language are quoted in quotation marks; (2) intersentential, where the entire sentence is in language entirely different from generation dominant language, and (3) intrasentential, where words, phrases or clauses of different languages are present in the same sentence. We refer our readers to Appendix C.2 for our annotation procedures. We focus on four languages, namely Japanese (ja), Russian (ru), Thai (th), and Mandarin Chinese (zh), as they are readily supported by the libraries stanza and lingua. We avoid languages with Latin scripts due to their possible shared vocabulary with English and results in incorrect language classification. For instance, the German word also (therefore) in the sentence Sie isst 3 Eier zum FrÃ¼hstÃ¼ck und verwendet 4 Eier fÃ¼r Muffins, also verwendet sie insgesamt 3 + 4 = 7 Eier pro Tag. was misclassified as English word. C.2 Annotation for Language-Mixing Patterns in s1 Reasoning We collect the language label with the highest probability assigned to the entire sentence, and we label sentence belonging to intersentential language-mixing for s1 if the sentence is non-English, as the dominant language of s1s overall output is English. We then check language labels for individual word tokens. If there are mixing of different languages within the same sentence, and quotation marks are present around the non-English words or phrases, then the sentence is assigned with the quote-and-think label. Otherwise, if quotation marks are not present, the sentence is assigned with the intrasentential label. C.3 Dominant Language in 14B-Sized Model Outputs Figure 8 shows the dominant language distribution in model outputs when MGSM questions are asked in Japanese (ja), Russian (ru), Thai (th), and Mandarin Chinese (zh). Figure 8: Proportion of dominant languages used by 14B-sized models responses when queried with Japanese (ja), Russian (ru), Thai (th), and Mandarin Chinese (zh) languages. same indicates that the response language is the same as query language. C.4 Quote-And-Think Pattern in s1s Training Data Among 1k English training samples of s1 models, 68.3% of the samples exhibit the quote-and-think pattern, among which at least half of them involves directly copying from the question prompts. This suggests that the quote-and-think language-mixing pattern is due to crosslingual transfer of the 23 original s1 models learned behavior of quoting phrases from question prompts during its long CoTs thinking process. C.5 Fine-Grained Analysis of s1-32Bs Intrasentential Language Mixing We perform human annotations on the intrasententially language-mixed sentences during reasoning and classify if each sentence belongs to one of the following categories: (1) extract-and-explain, where the non-English phrases are taken directly from the original input prompt but without quotation marks given (this resembles quote-and-think but no quotation marks are generated around the nonEnglish phrases); (2) insertional code-switching, where non-English lexical items (usually nouns) are inserted into the morphosyntactic frame of the English sentence (an example would be want to eat nasi goreng where nasi goreng is Malay word for fried rice), and (3) clause-level codeswitching, where switching between two languages within single sentence structure at clausal level (an example would be want to go to the library dan bersedia untuk peperiksaan where dan bersedia untuk peperiksaan is Malay clause for and prepare for the exam.) Figure 9 demonstrates the distribution of each category, with extract-and-explain being the dominant language-mixing patterns. Figure 9: Breakdown of s1-32Bs finegrained intrasentential language mixing patterns."
        },
        {
            "title": "D Further Details on Language Forcing",
            "content": "D.1 Example of CoTs after Language Forcing Box 2 show the example of CoTs generated by s1 from the combined language-forcing strategy. This example contrasts Box 1 where Box 1 showcases s1s natural CoTs without language forcing. Note that we still observe the sophisticated quote-and-think pattern where here, English phrases are quoted for reasoning (the dominant language is Japanese). Specifically, in that sentence where quote-and-think occurs, it is translated to if this question was in English, it would be how many rolls are needed in total? D.2 Language Compliance Detailed results for language compliance can be found in Tables 8, 9 and 10 for 32B, 14B and 7B, respectively. D. In-Language Budget Forcing Reasoning Detailed results for in-language budget forcing reasoning performance on MGSM are presented in Table 11. 24 Box 2. Example of CoTs prdocued from combined language forcing strategy. Question: ã­ã¼ããä½æããã«ã¯éè²ã®ç¹ç¶­ã2å·»åç½è²ã®ç¹ç¶­ããã®ååç¨ ããå¿è¦ãããã¾ãå¨ä½ã§ä½å·»å¿è¦ã§ãã (To make robe, you need 2 rolls of blue fiber and half of the white fiber. How many rolls are needed total?) s1s Thinking: ããè§£ãã¦ã¿ããåé¡ã¯ã­ã¼ããä½ãããã«å¿è¦ãªç¹ç¶­ã®å·» æ°ãè¨ç®ãããã®ã ã­éè²ã®ç¹ç¶­ã2å·»å¿è¦ã§ç½è²ã®ç¹ç¶­ã¯ãã®ååã¤ã¾ ã1å·»å¿è¦ã ã ããåè¨ã§3å·»ãã§ãå¾ã£ã¦åé¡æãããä¸åº¦ç¢ºèªããã ã­ ã¼ ã ã ä½ æ ãã ã«ã¯ é è² ã® ç¹ ç¶­ ã2å·» å ç½ è² ã® ç¹ ç¶­ ãã ã®å å ç¨ ã ãå¿è¦ãããã¾ãå¨ä½ã§ä½å·»å¿è¦ã§ããããã éè²ã2å·»ç½è²ããã®å åã¤ã¾ãéè²ã®ååã¯1å·»ã ããç½è²ã¯1å·»å¿è¦ã ããåè¨3å·»ããã§ãã ã®ããªã§ãããããã¦éè²ã®2å·»ã¨ç½è²ã®ååãè¶³ãã¨2 + 0.5 = 2.5å·»ã£ã¦æ å³ãããªãã®ã ãããã§ãåé¡ã®è¨ãåããå°ãããããããª å¾ ã£ã¦ å æ ãã ãä¸ åº¦ é è² ã® ç¹ ç¶­ ã2å·» å ç½ è² ã® ç¹ ç¶­ ãã ã®å å ç¨ ããã¨ããã®ã¯éè²ã2å·»ããã¦ç½è²ããã®ååã¤ã¾ãéè²ã®å åãªã®ãããã¨ã2å·»ã®ååãªã®ãæ¥æ¬èªã®ææ³ã§èããã¨ãã®å åã¯ç´åã®æ°è©2å·»åãæãã¦ããå¯è½æ§ãé«ãã¤ã¾ãéè²2å·»ç½ è²ã¯2å·»ã®ååãªã®ã§1å·»ã ããåè¨3å·»ã§ãããéè²ã®ç¹ç¶­ã2å·»åç½ è²ã®ç¹ç¶­ããã®ååã¨ããã®ãéè²ã2å·»ç½è²ãéè²ã®ååã¤ã¾ã1å·» ã¨ããæå³ãªãåãã3å·»çµå±åãçµæã«ãªãã®ã ã ã ã« å é¡ ã å¨ ä½ ã§ ä½ å·» å¿ è¦ ã§ ã ã ã¨ è ã ã¦ ã ã ã® ã§ å¿ è¦ ãª ç· å·»æ°ãèããã¦ããã®ã ããã¤ã¾ãéè²2å·»ç½è²1å·»åè¨3å·»ãããç­ã ã ã¨æãããã§ããããã®åé¡ãè±èªã ã¨how many rolls are needed in total? ã ã¨ä»®å®ããã¨3 rolls ãç­ã ã§ãããããã¦éè²2å·»ç½è²ååã¤ã¾ ã2.5å·»ã¨ããè§£éãããã§ãåé¡æã®æ¥æ¬èªã®è¡¨ç¾ããããã¨éè²2å·» ç½è²1å·»åè¨3å·»ãå¦¥å½ã ã¨æã ... Box 3. GlobalMMLU Eval Prompt Template System: Your task is to extract the answer choice from the Response field. Do not attempt to answer the question in the Question field yourself. Task Prompt: Your task is to extract the answer (A, B, C, or D) from the generated response based on the question and the option choices. Question: {question} Answer choices: {answer_options} Response: {response} Further Details on Cross-Domain Generalization E.1 Evaluation Details For GlobalMMLU, we use GPT-4o-mini to parse the final answer since their formats can vary in the reasoning traces. We set max_tokens to 2 to check if the extracted answer contains any of the four options (A, B, or D). See the prompts used for evaluation in Box 3. 25 Strategy Reasoning Language avg len Language Distribution Language Compliance Table 8: Results for s1-32B. translated_wait prefix system combined bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh 3073.3 2405.3 1833.1 2401.5 2379.7 2515.1 2601.1 2611.4 3821.3 1894.7 1776.3 ENGLISH: 99.6, BENGALI: 0.4 ENGLISH: 51.6, GERMAN: 48.4 ENGLISH: 100.0 ENGLISH: 88.4, SPANISH: 11.6 ENGLISH: 90.8, FRENCH: 8.8, CHINESE: 0.4 ENGLISH: 70.8, JAPANESE: 29.2 ENGLISH: 90.4, RUSSIAN: 9.2, CHINESE: 0.4 ENGLISH: 100.0 ENGLISH: 99.6, TELUGU: 0.4 ENGLISH: 99.6, CHINESE: 0.4 ENGLISH: 94.4, CHINESE: 5.6 ENGLISH: 100.0 SPANISH: 99.6, ENGLISH: 0.4 FRENCH: 100.0 JAPANESE: 98.8, ENGLISH: 1.2 3320.0 BENGALI: 98.8, ENGLISH: 1.2 1747.2 GERMAN: 98.8, CHINESE: 1.2 1729.7 2790.6 1822.9 2321.6 1564.2 RUSSIAN: 98.8, ENGLISH: 0.8, CHINESE: 0.4 SWAHILI: 91.2, ENGLISH: 8.4, JAPANESE: 0.4 3083.9 TELUGU: 95.2, ENGLISH: 4.8 6912.7 2126.3 THAI: 77.6, CHINESE: 20.4, ENGLISH: 2.0 1150.1 CHINESE: 99.6, ENGLISH: 0.4 ENGLISH: 90.4, BENGALI: 9.6 2693.8 1979.3 GERMAN: 100.0 ENGLISH: 100.0 1728.3 ENGLISH: 69.6, SPANISH: 30.4 2241.6 ENGLISH: 68.4, FRENCH: 31.2, CHINESE: 0.4 2346.0 JAPANESE: 99.2, ENGLISH: 0.8 1805.7 ENGLISH: 59.6, RUSSIAN: 39.2, CHINESE: 1.2 2180.0 ENGLISH: 98.8, SWAHILI: 0.8, CHINESE: 0.4 2721.2 ENGLISH: 93.6, TELUGU: 6.4 3869.3 1930.3 ENGLISH: 90.8, THAI: 8.0, CHINESE: 1.2 1162.1 CHINESE: 100. 3507.6 BENGALI: 99.6, CHINESE: 0.4 1845.2 GERMAN: 100.0 ENGLISH: 100.0 1582.6 SPANISH: 100.0 2604.3 FRENCH: 99.2, CHINESE: 0.8 1726.5 2127.4 JAPANESE: 100.0 1523.6 RUSSIAN: 98.8, ENGLISH: 0.4, GERMAN: 0.4, CHINESE: 0.4 3161.1 7036.0 2043.3 1188.7 CHINESE: 100.0 SWAHILI: 98.4, ENGLISH: 1.6 TELUGU: 97.6, ENGLISH: 2.4 THAI: 90.8, CHINESE: 8.8, GERMAN: 0.4 0.4 48.4 100.0 11.6 8.8 29.2 9.2 0.0 0.4 0.0 5.6 98.8 98.8 100.0 99.6 100.0 98.8 98.8 91.2 95.2 77.6 99.6 9.6 100.0 100.0 30.4 31.2 99.2 39.2 0.8 6.4 8.0 100.0 99.6 100.0 100.0 100.0 99.2 100.0 98.8 98.4 97.6 90.8 100. E.2 Language Forcing for GlobalMMLU We further study if forcing s1 to reason in the same language as the question helps with performance, as different languages may have different knowledge recall mechanism [76], and retrieving the right knowledge will help with MMLU-style questions [77]. Following the findings in Section 6, we use the combined language forcing strategy that yields the highest language compliance in multilingual generations. Table 13 shows marginal performance improvements for reasoning language in HRLs with Latin scripts, such as German (de), English (en) and French (fr). We want to highlight that the performance gain from language forcing is inconsistent: for instance, in-language reasoning for German business subject questions improves accuracy by around 3% but decreases accuracy for humanities subject by nearly 5%. For Chinese, even though it is one of the highly represented languages during pretraining of the base model Qwen, reasoning in Chinese introduces minor performance degradation even for STEM domain. For the even lower-resourced languages like Bengali (bn) and Swahili (sw), language forcing leads to performance degradationthere is around 6% decrease on average across domains. In general, the patterns for non-math-specific domains are similar that reasoning in HRLs would be more beneficial compared to LRLs; nonetheless the latter could be more friendly in user-facing setting and could potentially help with easier verification for more cultural-specific tasks. 26 Strategy Reasoning Language Table 9: Results for s1-14B. avg len Language Distribution Language Compliance translated_wait prefix system combined bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh 2457.1 1940.1 1638.2 1966.1 2062.6 2162.1 1937.9 3044.8 3852.4 1767.3 1438.9 ENGLISH: 100.0 ENGLISH: 76.4, GERMAN: 23.6 ENGLISH: 100.0 ENGLISH: 94.4, SPANISH: 5.6 ENGLISH: 95.2, FRENCH: 4.4, CHINESE: 0.4 ENGLISH: 86.8, JAPANESE: 13.2 ENGLISH: 98.8, RUSSIAN: 0.8, CHINESE: 0.4 ENGLISH: 99.6, SWAHILI: 0.4 ENGLISH: 98.8, CHINESE: 0.4, TELUGU: 0.8 ENGLISH: 100.0 ENGLISH: 62.0, CHINESE: 38. 4570.1 BENGALI: 92.8, ENGLISH: 7.2 1621.8 GERMAN: 100.0 ENGLISH: 100.0 1572.2 SPANISH: 99.6, ENGLISH: 0.4 2343.2 FRENCH: 99.6, ENGLISH: 0.4 1354.3 2194.3 JAPANESE: 99.2, ENGLISH: 0.8 1470.3 RUSSIAN: 98.8, CHINESE: 0.8, ENGLISH: 0.4 4442.6 6041.8 2448.6 1149.3 CHINESE: 100.0 SWAHILI: 85.2, ENGLISH: 14.4, TAGALOG: 0.4 TELUGU: 90.4, ENGLISH: 9.2, CHINESE: 0.4 THAI: 88.4, CHINESE: 10.8, ENGLISH: 0.8 2196.2 ENGLISH: 99.2, BENGALI: 0.8 1643.4 GERMAN: 92.8, ENGLISH: 7.2 ENGLISH: 100.0 1664.3 ENGLISH: 84.4, SPANISH: 15.6 1766.7 ENGLISH: 91.6, FRENCH: 8.4 1841.7 2036.4 JAPANESE: 17.2, ENGLISH: 82.8 1631.3 RUSSIAN: 49.2, ENGLISH: 50.8 2984.6 3942.8 1775.4 1295.8 ENGLISH: 98.8, SWAHILI: 1.2 ENGLISH: 91.6, TELUGU: 8.4 ENGLISH: 91.6, THAI: 8.0, CHINESE: 0.4 ENGLISH: 12.0, CHINESE: 88.0 4823.5 BENGALI: 98.0, ENGLISH: 2.0 1479.8 GERMAN: 100.0 ENGLISH: 100.0 1553.2 SPANISH: 100.0 2503.3 FRENCH: 100.0 1269.8 1941.4 JAPANESE: 100.0 1577.6 RUSSIAN: 99.6, CHINESE: 0.4 SWAHILI: 88.4, ENGLISH: 11.6 4525.5 TELUGU: 93.6, ENGLISH: 6.4 6046.9 2244.8 THAI: 92.4, CHINESE: 7.2, ENGLISH: 0.4 1118.8 CHINESE: 100.0 0.0 23.6 100.0 5.6 4.4 13.2 0.8 0.4 0.8 0.0 38. 92.8 100.0 100.0 99.6 99.6 99.2 98.8 85.2 90.4 88.4 100.0 0.8 92.8 100.0 15.6 8.4 17.2 49.2 1.2 8.4 8.0 88.0 98.0 100.0 100.0 100.0 100.0 100.0 99.6 88.4 93.6 92.4 100.0 27 Strategy Reasoning Language Table 10: Results for s1-7B. avg len Language Distribution Language Compliance translated_wait prefix system combined bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh bn de en es fr ja ru sw te th zh 3355.8 2490.5 2050.4 3156.8 2544.1 3381.9 2742.3 5381.5 5232.8 2654.8 1471.1 CHINESE: 99.2, ENGLISH: 0.8 ENGLISH: 100.0 ENGLISH: 68.0, GERMAN: 32.0 ENGLISH: 100.0 ENGLISH: 83.6, SPANISH: 16.4 ENGLISH: 90.4, FRENCH: 9.2, CHINESE: 0.4 ENGLISH: 72.0, JAPANESE: 28.0 ENGLISH: 96.0, RUSSIAN: 2.8, CHINESE: 1.2 ENGLISH: 100.0 ENGLISH: 97.6, TELUGU: 2.4 ENGLISH: 89.6, THAI: 8.4, CHINESE: 2. ENGLISH: 100.0 SPANISH: 99.2, CHINESE: 0.4, ENGLISH: 0.4 FRENCH: 99.2, CHINESE: 0.8 JAPANESE: 99.6, ENGLISH: 0.4 3814.5 BENGALI: 91.6, ENGLISH: 7.2, CHINESE: 1.2 2183.4 GERMAN: 98.0, CHINESE: 1.6, ENGLISH: 0.4 2405.3 3468.3 1712.5 4976.8 2242.7 RUSSIAN: 82.4, CHINESE: 13.6, ENGLISH: 4.0 SWAHILI: 55.2, ENGLISH: 43.6, TAGALOG: 1.2 7653.5 TELUGU: 82.4, ENGLISH: 17.2, CHINESE: 0.4 6649.8 THAI: 80.0, CHINESE: 17.6, ENGLISH: 2.4 3239.8 1675.4 CHINESE: 100.0 ENGLISH: 96.4, BENGALI: 3.2, CHINESE: 0.4 ENGLISH: 100.0 SPANISH: 100.0 FRENCH: 99.6, CHINESE: 0.4 JAPANESE: 100.0 3508.5 2093.8 GERMAN: 97.2, CHINESE: 2.8 2721.8 4099.5 1872.1 2513.9 2166.5 RUSSIAN: 79.2, CHINESE: 9.2, ENGLISH: 11.6 5310.8 5250.4 3438.9 1528.5 CHINESE: 100.0 ENGLISH: 100.0 ENGLISH: 96.8, TELUGU: 3.2 THAI: 93.2, CHINESE: 6. 4094.8 BENGALI: 97.6, CHINESE: 0.8, ENGLISH: 1.6 2046.7 GERMAN: 100.0 ENGLISH: 100.0 2857.6 SPANISH: 100.0 3907.6 FRENCH: 99.6, CHINESE: 0.4 1657.1 JAPANESE: 100.0 4804.9 2450.9 RUSSIAN: 86.4, CHINESE: 13.2, ENGLISH: 0.4 7393.2 6403.0 3609.9 1734.0 CHINESE: 100.0 SWAHILI: 81.6, ENGLISH: 18.4 TELUGU: 98.4, ENGLISH: 1.6 THAI: 92.8, CHINESE: 7.2 0.0 32.0 100.0 16.4 9.2 28.0 2.8 0.0 2.4 8.4 99.2 91.6 98.0 100.0 99.2 99.2 99.6 82.4 55.2 82.4 80.0 100.0 3.2 97.2 100.0 100.0 99.6 100.0 79.2 0.0 3.2 93.2 100.0 97.6 100.0 100.0 100.0 99.6 100.0 86.4 81.6 98.4 92.8 100. Table 11: Performance comparison of different language forcing strategies across multiple model sizes and languages on fixed 8k thinking tokens. Languages are categorized into high-resource (HRL: de, en, es, fr, ru, ja, zh) and low-resource (LRL: bn, sw, te, th) groups. th en ALL HRL LRL Method Model bn sw zh de ru es ja fr te s1.1-32B s1.1-14B s1.1-7B Baseline translated_wait prefix system combined Baseline translated_wait prefix system combined Baseline translated_wait prefix system combined 90.8 91.2 85.2 87.6 82.8 86.8 85.6 81.2 84.0 81.2 72.0 69.2 64.0 71.6 60.8 90.8 90.4 90.4 90.0 89.2 90.4 90.0 90.4 88.8 90. 87.6 84.0 82.8 84.0 84.0 96.0 94.8 95.6 96.4 95.2 94.4 96.8 95.2 95.2 93.2 92.4 93.2 93.6 90.8 92.8 93.2 93.2 92.8 91.2 91.6 93.6 93.6 92.0 91.2 92. 88.8 89.2 86.8 92.0 88.0 89.6 89.2 90.4 86.8 88.8 88.4 86.8 90.4 87.2 86.4 83.2 87.2 87.2 82.8 83.6 87.6 89.2 84.8 85.2 85.2 83.6 85.2 82.8 82.8 82. 82.4 76.4 68.0 74.4 72.8 93.2 92.0 94.0 92.8 92.8 92.4 92.4 92.8 91.2 90.4 88.0 87.2 85.6 87.6 86.4 72.4 73.2 65.2 71.2 58.4 59.6 63.2 44.4 58.8 36. 24.0 24.0 14.4 25.6 14.8 68.4 70.8 63.6 67.2 63.2 60.0 61.2 55.2 62.0 54.4 36.8 37.2 24.0 36.8 27.6 91.6 91.6 88.8 90.8 89.2 89.2 90.8 86.8 90.4 89. 81.6 84.0 74.4 76.8 74.4 88.0 90.0 90.8 89.6 90.4 89.6 89.2 91.2 90.4 88.4 86.0 83.2 84.0 82.0 83.6 87.4 87.8 85.6 86.0 84.3 84.4 85.0 82.0 83.8 80. 74.8 74.1 69.5 73.1 69.9 91.2 91.3 91.3 90.3 90.5 90.3 90.6 90.7 89.5 89.0 86.9 85.8 84.0 84.8 84.5 80.8 81.7 75.7 79.3 73.4 73.9 75.2 66.9 73.8 65. 53.6 53.6 44.2 52.7 44.4 28 Table 12: Number of average thinking tokens for each reasoning langauge when the MGSM task questions are asked in particular query language. Query Language bn de en es fr ja ru sw te th zh bn de en es 4,559 3,838 3,429 3,736 3,868 3,994 4,105 5,958 5,761 4,166 4,017 1,974 1,642 1,239 1,388 1,562 1,623 1,392 2,945 3,341 1,687 1,650 2,190 1,910 1,467 1,779 1,886 2,028 1,713 2,955 3,528 2,000 1, 3,473 2,838 2,253 2,512 2,886 2,627 2,695 3,801 4,160 2,801 2,521 Reasoning Language ja ru fr sw te th zh AVG 1,640 1,159 1,075 1,225 1,218 1,366 1,170 2,144 2,473 1,300 1,365 2,276 1,794 1,625 1,665 2,103 2,033 2,007 4,114 3,574 1,993 1,822 1,977 1,617 1,341 1,467 1,577 1,616 1,469 3,108 3,337 1,756 1, 4,900 3,510 3,431 3,600 4,077 3,957 4,055 3,867 5,934 4,107 4,233 5,461 5,073 5,154 5,323 5,033 5,334 4,818 6,836 6,277 5,247 5,344 2,852 2,025 1,703 1,875 2,218 2,285 2,132 3,976 3,879 2,279 2,009 1,710 3,001 1,327 2,430 1,075 2,162 1,133 2,336 1,263 2,517 1,333 2,563 1,098 2,423 2,705 3,855 2,557 4,074 1,241 2,597 1,117 2,496 1,505 2,585 AVG 4, 1,858 2,108 2,960 1,466 2,273 1, 4,151 5,445 2,475 Table 13: Model performances comparison with and without using combined strategy. Green cells show positive gains compared to no-forcing setting and red for the opposite. Model Method Domain bn de en fr ja sw zh Languages none s1.1-32B combined STEM business humanities medical other social sciences AVG STEM business humanities medical other social sciences AVG 95.65 74.14 69.61 66.67 76.79 71.57 75.74 89.13 67.24 67.65 63.89 66.07 64.71 69.78 93.48 79.31 83.33 77.78 87.50 82.35 83.96 95.65 82.76 78.43 83.33 80.36 84.31 84.14 93.48 91.38 88.24 80.56 87.50 89.22 88. 97.83 87.93 87.25 80.56 94.64 88.24 89.41 95.65 82.76 81.37 72.22 83.93 83.33 83.21 97.83 81.03 79.41 72.22 87.50 81.37 83.23 93.48 89.66 79.41 69.44 85.71 80.39 83.02 86.96 82.76 74.51 77.78 76.79 82.35 80.19 76.09 46.55 45.10 52.78 66.07 47.06 55. 67.39 32.76 49.02 44.44 64.29 41.18 49.85 93.48 81.03 80.39 77.78 80.36 84.31 82.89 91.30 86.21 79.41 75.00 76.79 80.39 81."
        }
    ],
    "affiliations": [
        "Brown University",
        "Capital One",
        "Cohere Labs",
        "MBZUAI",
        "Stanford University",
        "University of TÃ¼bingen"
    ]
}