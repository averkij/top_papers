{
    "paper_title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "authors": [
        "HyperAI Team",
        "Yuchen Liu",
        "Kaiyang Han",
        "Zhiqiang Xia",
        "Yuhang Dong",
        "Chen Song",
        "Kangyu Tang",
        "Jiaming Xu",
        "Xiushi Feng",
        "WenXuan Yu",
        "Li Peng",
        "Mingyang Wang",
        "Kai Wang",
        "Changpeng Yang",
        "Yang Li",
        "Haoyu Lu",
        "Hao Wang",
        "Bingna Xu",
        "Guangyao Liu",
        "Long Huang",
        "Kaibin Guo",
        "Jinyang Wu",
        "Dan Wu",
        "Hongzhen Wang",
        "Peng Zhou",
        "Shuai Nie",
        "Shande Wang",
        "Runyu Shi",
        "Ying Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 5 0 4 1 . 2 1 5 2 : r HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices HyperAI Team, Xiaomi Corporation"
        },
        {
            "title": "Abstract",
            "content": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs. To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within unified framework, enabling dynamic switching between visual branches under shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference."
        },
        {
            "title": "Introduction",
            "content": "In recent years, multimodal large language models (MLLMs)such as GPT-4o, Gemini [Gemini Team et al., 2023, 2024, Google DeepMind, 2025a,b,c,d,e], Claude [Anthropic, 2024, 2025a,b,c], QwenVL [Bai et al., 2023, Wang et al., 2024a, Bai et al., 2025a], and InternVL [Zhu et al., 2025, Wang et al., 2025]have achieved rapid progress, delivering impressive breakthroughs in cross-modal understanding, visual reasoning, optical character recognition (OCR), and open-world perception. At the same time, the demand for multimodal capabilities on on-device platforms is growing rapidly. However, these models are primarily designed for cloud-based inference; their complex architectures and extremely large parameter scales make it challenging to run them efficiently on on-device environments, such as smartphones and tablets, where compute and memory budgets are highly constrained. On-device models provide strong privacy protection for user data and significantly reduce cloud inference costs, making the performanceefficiency trade-off of on-device MLLMs central concern for major device manufacturers. As data quality improves and model architecture design matures, the densing law of LLMs [Yao et al., 2024] continues to rise, enabling small-parameter models to achieve strong general capabilities. This trend provides practical foundation for on-device deployment of large models. Recently, compact MLLMs such as Qwen3 VL 2B [Bai et al., 2025a] and InternVL 2B [Zhu et al., 2025, Wang et al., 2025] have demonstrated competitive performance. Nevertheless, critical bottleneck remains: these models typically rely on standard Vision Transformer (ViT) encoders, which suffer from quadratic complexity. When processing high-resolution inputs, which is essential in typical on-device scenarios such as user interface (UI) understanding and grounding, they are prone to high memory consumption and noticeable inference latency during on-device deployment. Attempts to mitigate this, such as Apples FastVLM [Vasu et al., 2025], often resort to reducing visual tokens via aggressive convolutional downsampling, but this frequently comes at the cost of substantial performance degradation. Figure 1: OpenCompass Average Score vs. Model Parameters. We compare HyperVL with open-source models released after October 2024, based on OpenCompass benchmarks. HyperVL (marked by the star) achieves superior performance comparable to larger models while maintaining compact parameter footprint. To this end, we introduce HyperVL, an efficient multimodal large language model tailored for ondevice inference. Specifically, we adopt an image-tiling strategy that divides high-resolution inputs into smaller patches that can be encoded independently, thereby reducing peak memory consumption. Building on this foundation, we further reduce on-device inference costs through two key techniques. First, the Visual Resolution Compressor (VRC) adaptively predicts the optimal visual encoding resolution based on the image information density, avoiding unnecessary high-resolution computation and reducing visual encoding latency. Second, we apply Dual Consistency Learning (DCL) on ViT encoders of different capacities so that they can seamlessly connect to shared LLM, enabling dynamic switching between lightweight and high-accuracy visual paths depending on task types, latency budget, or device compute capability. In summary, our contributions are as follows: We propose HyperVL, multimodal large language model designed for efficient on-device inference. As shown in Fig. 1, under comparable model sizes, HyperVL achieves state-ofthe-art results across multiple authoritative multimodal benchmarks and outperforms existing general models on on-device scenarios, marking meaningful capability breakthrough for real-world on-device applications. We significantly improve visual encoding efficiency through dual consistency learning for ViTs and the Visual Resolution Compressor. The Compressor is plug-and-play and can be directly integrated into any pretrained MLLM, providing general acceleration method for on-device multimodal inference. We systematically validate on-device inference performance on real hardware. We quantify the reduction in latency and memory consumption on commercial mobile devices, providing comprehensive system-level demonstration of an efficient multimodal inference pipeline optimized for on-device deployment."
        },
        {
            "title": "2 Related Work",
            "content": "Small-scale Multimodal Models. In recent years, multimodal large language models (MLLMs) have achieved remarkable progress across understanding and generation tasks. However, much 2 of this progress has been driven by scaling laws [Kaplan et al., 2020] where increasing parameter countsfrom tens of billions to over trillioncontinues to push performance ceilings but also leads to prohibitive training and deployment costs, making such models challenging to run on-device. Consequently, both academia and industry have begun exploring smaller, more energy-efficient multimodal models. Representative work includes the Qwen-VL [Bai et al., 2023, 2025b,a] and InternVL families [Chen et al., 2024a, 2025a, Zhu et al., 2025, Wang et al., 2025], which provide variants ranging from 1B to 4B parameters and significantly reduce computational complexity while maintaining strong multimodal reasoning capabilities. Nonetheless, their ViT backbones still exhibit high memory consumption and long inference latency during on-device execution, particularly under high-resolution visual inputs. FastVLM [Vasu et al., 2025] improves ViT inference efficiency by compressing visual tokens using CNNs, though this design introduces noticeable performance degradation. Several manufacturers have successfully deployed small-scale models on real devices via joint hardwaresoftware optimization. AndesVL [Jin et al., 2025] proposes system-level optimization pipeline and utilizes ARM-level kernel inference optimization to enable stable execution of 3B model within 2 GB of memory. Although these works demonstrate substantial progress in small-model design and efficient inference, visual encoder remains the primary bottleneck for on-device deployment. Efficiency Improvements for On-device MLLMs. Improving the efficiency of multimodal large language models has attracted increasing attention, leading researchers to pursue optimization from model design to system-level deployment. To address the computational burden introduced by visual inputs, visual token compression has emerged as an important direction. For example, MiniCPM-V [Yu et al., 2025] employs Perceiver Resampler, which uses single layer of cross-attention to compress hundreds of visual tokens into small, fixed number of feature vectors, significantly reducing the sequence length processed by the language model. Cui et al. [2025] introduces visual consistency learning, allowing the model to represent images with different numbers of visual tokens depending on their semantic complexity. Moreover, knowledge distillation [Xu et al., 2024, Yang et al., 2025a, Wang et al., 2023] has been further developed in multimodal settings, enabling student model to absorb richer visual understanding and cross-modal alignment knowledge by mimicking the intermediate representations or outputs of stronger teacher model. These algorithmic optimizations, combined with techniques such as model quantization (QAT/PTQ) [Liu et al., Xiao et al., 2023, Liu et al., 2024a], operator fusion in inference frameworks, collectively form comprehensive technical stack supporting efficient on-device multimodal inference."
        },
        {
            "title": "3 HyperVL",
            "content": "3.1 Model Architecture As illustrated in Figure 2, HyperVL comprises four core modules: visual resolution compressor, two visual encoders of different sizes, vision-language projector, and shared large language model (LLM). Detailed descriptions of each module are provided below: Visual Resolution Compressor. We observe that high-resolution images are not invariably requiredlarger images significantly increase the inference latency and memory footprint of on-device ViT, while in some scenarios, low-resolution images can already support high-quality responses. To address this, we design visual resolution compressor that predicts the compression ratio based on the information density of the input image, scaling the image down to 10%100% of its original size. The scaled image is then fed into the visual encoder. Visual Encoders. We adopt two ViT variants of different sizesSigLIP2-Large and SigLIP2-Base [Tschannen et al., 2025] as visual encoders, which consist of 24 layers (300M parameters) and 18 layers (93M parameters), respectively. To handle images with arbitrary aspect ratios and resolutions, we integrate the AnyRes [Zhang et al., 2024a] approach: it preserves the original aspect ratio of the input image through minimal scaling and applies additional padding if necessary. The processed image is then tiled into non-overlapping patches. Both ViT encoders share the same LLM and can be flexibly switched between different downstream tasks. Vision-language Projector. To map visual representations to the input space of the LLM, we employ two-layer MLP as the projector, aligning the output of the ViT with the embedding layer of the LLM. To reduce the length of visual tokens fed into the LLM, we use pixel shuffle to compress the token length to 1/4 of its original size. Figure 2: The overall architecture of HyperVL mainly includes visual resolution compressor, two visual encoders of different sizes, vision-language projector, and shared LLM. Large Language Model (LLM). Considering the balanced size and superior performance, we adopt the Qwen3 1.7B model [Yang et al., 2025b] as the backbone LLM. Table 1: Configuration of HyperVL Model. Model Name #Param (B) Vision Encoder Language Model HyperVL HyperVL ViTL 1.8 2.0 SigLIP2-Base Qwen3-1.7B SigLIP2-Large Qwen3-1.7B 3.2 Dual Consistency Learning To accommodate the computational constraints of diverse on-device platforms, we design dualbranch vision-language model architecture. The model incorporates two visual encoders of different sizes while sharing an single LLM. During on-device execution, the system can dynamically switch between branches based on task types or device capabilities, thereby balancing performance and resource consumption. Specifically, input images can be processed by either visual encoder to extract visual features, which are then projected into semantic space aligned with textual representations before being fed into the language model for text generation or multimodal reasoning. This design aims to ensure that, whether using the small branch or the larger branch, the model can produce semantically consistent outputs with comparable performance. To this end, we employ semantic consistency training strategy. Through an alternating dual-branch training mechanism, both visual encoders learn robust semantic mappings. In addition, teacherstudent distillation approach is applied to align the output distribution of the lightweight branch with that of the high-precision branch. Dual-Branch Alternating Training. The model is optimized using step-wise alternating training strategy, in which two branches are activated alternately throughout training. This alternating optimization allows the model to learn stable and consistent semantic mapping space under different visual representations. Semantic Consistency Distillation. To further reduce the semantic distribution gap between the two branches, we introduce Semantic Consistency Distillation mechanism. Specifically, the larger branch is treated as the teacher model, generating teacher distribution pθl() for given input. Simultaneously, the smaller branch serves as the student model, producing the prediction distribution 4 pθb (). By imposing temperature-smoothed KL divergence constraint on the two distributions, the student branch gradually approximates the semantic representations of the teacher branch during training, achieving consistent cross-branch semantic alignment. The loss function for this process is defined as: (cid:16) LKD = KL πθs (yiy<i, I) (cid:13) (cid:13) (cid:13) πθl (yiy<i, I) (cid:17) (1) Note that this loss is only computed on text tokens, skipping image tokens, to ensure the effectiveness and robustness of the distillation signal. 3.3 Visual Resolution Compressor Figure 3: Architecture of the Visual Resolution Compressor. The top panel illustrates the data construction pipeline used for training the compressor, while the lower shows the inference process. The bottom panel depicts the inference workflow, where the lightweight compressor dynamically predicts the optimal resolution to accelerate processing. In multimodal large language models, input resolution governs not only the cost of visual encoding but also directly affects the computational complexity of LLM and the KV-cache memory footprint by determining the number of visual tokens. Higher resolutions capture more fine-grained visual information but incur substantially higher compute and memory costs during both training and inference. In contrast, tasks with low visual information density (e.g., object classification) can be accurately solved under lower input resolutions. Motivated by this, we propose lightweight, plug-and-play Resolution Compressor designed to automatically predict the optimal compression ratio for input images. Its primary objective is to substantially reduce the number of visual tokens while preserving task performance, thereby lowering the overall computational overhead. The design of the component follows three key principles: (1) On-device deployability: lightweight architecture with constrained computation and storage overhead, compatible with mobile or embedded devices; (2) Plug-and-play capability: The module is independent of specific MLLM architectures and can be directly integrated into any pre-trained MLLM without additional architectural modifications; (3) Fine-grained controllability: Providing multi-level compression ratios to better balance computational overhead and performance. Compared with ViCO [Cui et al., 2025], which only supports two fixed compression ratios (0.25 and 1), the proposed visual resolution compressor enables finer-grained compression ratio selection (0.11.0), allowing more precise control over visual encoding resolution and computational cost. Data Construction. To construct supervised data for training the compressor, we first calculate the ground-truth compression ratio for each image in the training samples. The design principle is to select the smallest compression ratio that does not introduce noticeable degradation. As illustrated in Figure 3, the specific procedure is as follows: For each training sample Di = {I, Tp, Ta}i (where denotes the image, Tp the prompt text, and Ta the ground-truth response), we compress the image at multiple ratios to obtain set of compressed 5 variants {Iα}, where α {0.1, 0.2, . . . , 1.0}. Each image-text pair {Iα, Tp, Ta} is then fed into reference MLLM, from which we compute the cross-entropy loss. We measure the relative loss ratio by comparing each compressed loss against the loss of the uncompressed input. Specifically, using the baseline loss L(TaI1.0, Tp), we define (α) = L(TaIα, Tp) L(TaI1.0, Tp) (2) which quantifies the relative increase in loss caused by compression. We then identify the maximum acceptable compression ratio α = max{α (α) ϵ} (3) where ϵ is predefined loss tolerance threshold. This α is used as the supervision signal for the compressor to predict the optimal compression ratio for each image. Compressor Training. To satisfy the computational and latency constraints of on-device deployment, the compressor is designed as lightweight, low-parameter model. We adopt MobileNet [Qin et al., 2024] as the visual backbone to reduce computational complexity and memory footprint, while preserving task performance. As the model requires fixed resolution of r, we adjust the original image and the corresponding maximum acceptable compression ratio accordingly: α = α r2 (4) where and represent the width and height of the original image, respectively. The training data {Iresize, α} is then prepared, where Iresize denotes the input image resized to r. The resized image is passed through the visual backbone to extract features, followed by learnable weighted pooling layer and multi-layer perceptron (MLP) that predicts the scaling factor ˆα. The compressor is trained to predict the optimal scaling ratio of the input image, using the Mean Squared Error (MSE) as the loss function. Module Integration. During inference, the compressor first predicts the normalized scaling factor ˆα from the resized input Iresize and then restores it to the corresponding compression ratio for the original image, as illustrated in Figure 3. This ratio is subsequently applied to the original high-resolution image. This process enables adaptive balancing of computational load and task performance on on-device platforms: for complex scenarios, the compressor preserves the original high-resolution image to maintain visual fidelity; for semantically simple or low-information-density scenarios, it selects higher compression ratio to substantially reduce computational and memory overhead. Importantly, the compressor is agnostic to the MLLM backbone and can be integrated as standalone plug-in into any model. 3.4 Training Data This section describes the construction of the multimodal training data used in this study. We first introduce the composition of the training corpus across different task categories, and then present the data pipeline used for data processing and organization. 3.4.1 Overview To support the comprehensive capabilities of multimodal large models in visual understanding, cross-modal reasoning, and generation tasks, this study constructs large-scale multimodal training corpus covering image caption, visual question answering (VQA), OCR, document understanding, grounding and counting, GUI interaction, STEM, and text-only tasks. The training corpus integrates combination of open-source and synthetic data to balance task diversity and scenario generality. In particular, several large-scale open-source datasetssuch as Infinity-MM [Gu et al., 2025], FineVision [Wiedmann et al., 2025], LLaVA-One-Vision-1.5 [An et al., 2025], LLaVAR [Zhang et al., 2024b], WIT [Srinivasan et al., 2021], MMInstruct [Liu et al., 2024b], ShareGPT4V [Chen et al., 2024b], and ShareGPT4o [Chen et al., 2025b]are comprehensive multimodal resources that span multiple task categories, including image captioning, VQA, instruction-following, and dialogue. These datasets provide rich visual and textual annotations, enabling the model to learn cross-modal understanding and reasoning in variety of contexts. Throughout training, we dynamically adjust the proportions of different data modalities and task types across stages to balance training efficiency, 6 model stability, and multi-task generalization. The specific data sources described below focus on task-specific datasets that complement the comprehensive multi-task datasets introduced above. Image Caption. Image captioning data is used to train the models visual-to-linguistic generation ability, covering general scenarios and fine-grained descriptions. Task-specific datasets include Wukong [Gu et al., 2022], Laion [Webster et al., 2023], CoCo-Caption [Chen et al., 2015], TextCap [Sidorov et al., 2020], and OpenImages-Caption [Kuznetsova et al., 2020]. Notably, we re-annotate original images using captioning modelsespecially for the WIT dataset, where we leverage its original rich text as contextual information to incorporate more comprehensive world knowledge. Visual Question Answering (VQA). VQA data is designed to improve the models capabilities in visual reasoning, knowledge-based QA, and multi-turn dialogue, encompassing open-domain, knowledge-driven, and complex reasoning tasks. These datasets exhibit significant diversity in question types, reasoning depth, and knowledge dependency, ensuring the models ability to transition from perception to cognition. Task-specific open-source datasets, such as GQA [Hudson and Manning, 2019], TallyQA [Acharya et al., 2019], A-OKVQA [Schwenk et al., 2022], Objects365 [Shao et al., 2019], IconQA [Lu et al., 2022a], and VisDial [Das et al., 2017], are used to provide examples of visual reasoning and dialogue scenarios. Optical Character Recognition (OCR). OCR capability is core foundational skill for MLLMs. To enhance the models ability to recognize and understand text in images, our data covers multiple languages and scenarios (natural scenes, documents, invoices, and tables). The data is highly diverse in font styles, languages, and scene complexity, supporting generalization in cross-lingual text understanding. Additionally, data augmentation (e.g., cropping, rotation, background replacement, and blurring) is applied to improve the models generalization and robustness.Representative opensource datasets include Laion-COCO [Schuhmann et al., 2022], Wukong-OCR [Gu et al., 2022], SynthDoG-EN [NAVER Clova Information Extraction, 2023a], SynthDoG-ZH [NAVER Clova Information Extraction, 2023b], LSVT [Sun et al., 2019], EATEN [Guo et al., 2019], and ChineseText-Recognition [Yu et al., 2022], providing annotated samples across multiple scripts and domains. Document Understanding. Document understanding data is used to train the models ability to parse, interpret, and reason over structured and unstructured documents, including forms, receipts, tables, and handwritten notes. The datasets cover range of scenarios such as text layout analysis, entity recognition, and document-level question answering, aiming to improve both visual and semantic comprehension of documents. Selected open-source datasets, such as DUDE [Van Landeghem et al., 2023], UniMER-1M [Wang et al., 2024b], and IAM-line [Teklia, 2023], provide representative examples of diverse document types and annotation styles, supporting the models generalization across structured and unstructured document understanding tasks. Grounding and Counting. Grounding data is annotated with single or multiple bounding boxes. Basic datasets are selected from public sources such as Visual Genome [Krishna et al., 2017], RefCOCOs [Yu et al., 2016], Flickr30k-Entities [Plummer et al., 2015], and Groma [Ma et al., 2024]. These datasets are filtered and categorized into four types: object referencing, regional captioning, referred entity recognition, and grounded image captioning. We design multiple prompt templates for model instruction and unify all coordinates into relative coordinate system of [0, 1000]. Graphical User Interface (GUI) Understanding. To enhance models understanding and navigation capabilities in real-world GUI environments, we construct large-scale, multi-scenario GUI dataset covering key tasks such as icon recognition, interface element analysis, region localization, interaction reasoning, and agent action execution. The data are sourced from open-source UI icon libraries, real-world multi-platform usage scenarios, and automated task collection, encompassing variety of interface types, including mobile, web, and desktop applications. Representative open-source datasets include AITW [Rawles et al., 2023], AndroidControl [Li et al., 2024], AutoGUI [Li et al., 2025], RicoSCA [Li et al., 2020], and WaveUI [AgentSea, 2024], providing diverse examples of GUI layouts and interaction patterns. By aligning icons with their functional semantics, the dataset enables models to learn mappings between visual appearance features of controls and interaction intents, gradually extending from element recognition to comprehensive page structure understanding and interaction reasoning. STEM (Science, Technology, Engineering, Mathematics). STEM data is used for scientific reasoning, mathematical formula understanding, and professional knowledge QA, covering disciplines such as mathematics, physics, finance, medicine, and chemistry. It aims to strengthen the models 7 logical reasoning and knowledge-based QA capabilities. In our corpus, we incorporate selected open-source STEM datasets, for example ART500K [HKUST DeepArt Group, 2023], BIOMEDICA [Lozano et al., 2025], and ScienceQA [Lu et al., 2022b], which offer variety of domain-specific questions and reasoning tasks. Text-Only Instruction Data. To maintain the text capabilities of the multimodal model, we incorporate certain proportion of text pre-training data and instruction-following data. Model performance changes are monitored through instruction-based evaluations. 3.4.2 Data Pipeline Figure 4: Data pipeline. We adopt three-stage data governance pipeline consisting of data preparation and categorization, data cleaning and normalization, and data quality filtering and mixed packaging, enabling large-scale construction of high-quality multimodal training samples with strong controllability and consistency. To facilitate the construction of large-scale, high-quality multimodal training samples, HyperVL designs and implements systematic data governance pipeline that spans data collection, cleaning, filtering, categorization, and deduplication. As illustrated in Figure 4, the pipeline is organized into three stages: (i) data preparation and categorization, (ii) data cleaning and normalization, and (iii) data quality filtering and mixed packaging. Data Preparation and Categorization. The HyperVL multimodal corpus comprises both general task data and domain-specific task data, stored in standardized formats such as tar, parquet, json, and jsonl. General data typically contains heterogeneous task types and requires automatic task partitioning after decompression and structural unification. To enable fine-grained sampling control and improve data controllability, we develop detailed task categorization system using task decompositionrecombination strategy, in which categorization is decomposed into two parallel subtasks: (i) content-based visual recognition and (ii) query-driven intent inference from textual instructions, with the final task label obtained through logical recombination of the two predictions. This design achieves stable categorization accuracy exceeding 96% on complex imagetext samples, substantially improving sampling controllability and distribution balance in downstream training. In contrast, domain-specific data (e.g., OCR and GUI grounding) is inherently organized by task type and can be directly forwarded to subsequent processing stages. Data Cleaning and Normalization. This stage aims to transform raw multimodal corpora collected from diverse sources into unified samples. We first perform automated quality inspection and remove samples with corrupted or blurred images, missing or misaligned text, incomplete questionanswer pairs, trailing duplication or gibberish, and abnormal or inconsistent fields, thereby ensuring structural integrity and semantic consistency. Next, all remaining samples are converted into unified HyperVL JSONL format, retaining essential fields such as image paths, input text, output text, task labels, and metadata to ensure cross-dataset 8 consistency. Dual-modal deduplication is conducted over the entire corpus, followed by explicit trainingtest decontamination to prevent evaluation data leakage. For image collections exceeding billions of samples and exhibiting high source heterogeneity, we further deploy an efficient image deduplication system based on Self-Supervised Copy Detection (SSCD) [Pizzi et al., 2022]. Unlike traditional perceptual hashing methods such as pHash [Zauner, 2010], SSCD encodes each image into 512-dimensional deep feature representation and supports large-scale similarity search via Elasticsearch-based vector retrieval. This feature-level matching enables robust detection of semantic duplicates under cross-dataset, cross-style, and cross-resolution variations, significantly improving deduplication accuracy and robustness. Data Quality Filtering and Mixed Packaging. On the cleaned and deduplicated corpus, HyperVL conducts multi-dimensional data quality assessment followed by automated refinement. We design unified quality evaluation framework spanning six primary dimensionsimagetext alignment, linguistic quality, image quality, factual consistency, sample complexity, and complianceresulting in twelve quantifiable sub-indicators. The evaluation procedure integrates metric normalization with type-adaptive weighting, assigning importance to indicators across data categories, and performs joint assessment via rule-based heuristics and the Qwen3-VL model, ensuring both robustness and flexibility. Samples identified as severely low quality are automatically re-annotated in an iterative manner to improve overall data reliability. Finally, HyperVL applies hierarchical sampling and weighted mixing based on task categories and quality tiers, producing structured clean JSONL training sets and corresponding YAML configuration files for multi-stage training. 3.5 Training Pipeline We divide the pre-training of the multimodal model into three phases: the vision-language alignment phase, the knowledge enhancement phase, and the multi-task training phase. The parameter configurations and data details for each phase are presented in Table 2. Table 2: Overview of HyperVL training stages. Stages Alignment Knowledge Enhancement Multi-Task Training Dataset Caption Caption + VQA + OCR + Interleaved image + Grounding + STEM + Text Multitask data Tokens Sequence Length Learning Rate Trainable Parameters 82.8B 4096 5e-4 Adapter 214.6B 8192 5e-5 ViT + Adapter + LLM 55.1B 8192 2e-5 ViT + Adapter + LLM Stage 1: Vision-Language Alignment. The visual encoders adopt parameters from two variants of SigLIP2 (Base and Large). Compared to larger alternatives such as Qwen2VL-ViT 675M and InternVL 300M, their structure and size are more suitable for mobile deployment. During this stage, the parameters of the ViT and LLM are frozen, while the adapter parameters are randomly initialized. We train the adapter using image-text captioning data with learning rate of 5e-4. The primary goal of this stage is to align the visual encoder with the language model. Stage 2: Knowledge Enhancement. The second stage involves joint vision-language pre-training. We unfreeze the parameters of the ViT and LLM and perform full-parameter pre-training with relatively low learning rate. In this stage, the training data includes multi-type, multi-task imagetext data and text-only data, enabling the model to acquire extensive visual knowledge. During pre-training on this data, we only compute the loss for text tokens while excluding the loss for image tokens. The maximum sequence length used in this stage is 8192. Stage 3: Multi-Task Training. To further enhance the models general capabilities and reasoning ability, we select high-quality multi-task data, including STEM, mathematics, reasoning, and synthetic data. Existing studies have shown that high-quality reasoning-oriented pre-training datasets can moderately improve general reasoning performance, while synthetic data yields significant benefits across various downstream tasks. The synthetic data here includes short Chain-of-Thought (CoT) and long CoT sequences, covering both text and multimodal reasoning tasks. In this stage, the maximum sequence length is increased to 8192, and the model is trained on approximately 23M high-quality data. Ultimately, this stage enhances the models ability to handle complex, multi-step reasoning processes."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 Evaluation Setting To comprehensively evaluate the models performance in multimodal understanding, reasoning, and generation, we conduct systematic evaluation on both public and internal benchmarks. Our model is compared against state-of-the-art multimodal baselines with comparable parameter scales to ensure fair and meaningful comparisons. For public benchmarks, we adopt the OpenCompass open-source evaluation framework, VLMEvalKit, which provides standardized and reproducible evaluation protocol. GPT-4o is employed as the judge model for automated answer evaluation and consistency checking, ensuring the objectivity and reliability of the reported results. 4.2 Public Benchmarks The public benchmark evaluation framework encompasses four core leaderboards, spanning multimodal reasoning, OCR and document understanding, comprehensive multimodal capabilities, and hallucination detection. The evaluated benchmarks are detailed as follows: 4.2.1 Multimodal Reasoning and Mathematics Models are evaluated for cross-modal reasoning and mathematical problem-solving capabilities under this leaderboard, including: MMMU_val [Yue et al., 2024] multimodal expert-level reasoning benchmark covering over 30 disciplines such as science, engineering, medicine, and finance, emphasizing cross-modal knowledge integration and professional-level visual understanding. MMMU_Pro_V [Yue et al., 2025] robust extension of MMMU featuring vision-only question embedding, enhanced option diversity, and strict filtering to eliminate text-only solvable items. MathVista [Lu et al., 2024] small-scale benchmark for visual mathematics, focusing on formula understanding, geometric reasoning, and reasoning in mixed text-image mathematical scenarios. 4.2.2 OCR, Chart, and Document Understanding In this leaderboard, models are evaluated on text recognition, chart parsing, and structured document understanding, including: AI2D [Kembhavi et al., 2016] diagram reasoning dataset based on educational scenarios, testing models understanding of explanatory images and conceptual relationships through flowcharts and structure diagrams. ChartQA [Masry et al., 2022] Evaluates models understanding and reasoning over visual charts, including bar charts, line charts, and pie charts, with emphasis on structured information extraction and numerical reasoning. TextVQA [Singh et al., 2019] Focuses on recognition and question answering for text embedded in visual scenes, testing OCR and semantic understanding together. DocVQA [Mathew et al., 2021] visual question answering benchmark on scanned documents and PDFs, assessing structured document understanding and information extraction. InfoVQA [Mathew et al., 2022] Evaluates document information extraction and QA in highly structured scenarios such as financial statements, invoices, and contracts. 10 OCRBench [Liu et al., 2024c] general OCR benchmark covering natural scenes, scanned documents, and receipts, focusing on robustness to complex fonts, multilingual text, and noisy environments. ChineseOCRBench [SWHL, 2024] Chinese-focused high-difficulty OCR benchmark, covering multiple fonts, diverse scenarios, complex layouts, and handwritten text. SEEDBench [Li et al., 2023a] large-scale multimodal benchmark featuring 19K multiple-choice questions across 12 spatial and temporal understanding dimensions, designed for rigorous evaluation of generative multimodal large language models. 4.2.3 Comprehensive Multimodal tasks This evaluation setting measures model performance across integrated multimodal tasks, including: MME [Fu et al., 2025] comprehensive benchmark comprising 14 perception and cognition subtasksmeasuring vision-language understanding through manually crafted, concise instructionanswer pairs to enable fair and quantitative evaluation of multimodal large language models. MMBench_CN [Liu et al., 2024d] comprehensive benchmark featuring diverse, quality-controlled questions, rigorous evaluation strategies, and standardized multiple-choice formats for robust assessment of multimodal large language models. MMBench v1.1 (EN/CN) [Liu et al., 2024d] comprehensive multimodal benchmark assessing image understanding, description, reasoning, and multilingual question answering. MMVet [Yu et al., 2024] visual reasoning benchmark emphasizing evidence consistency and factual accuracy, testing models ability to avoid hallucinations across cross-image facts and visual details. MMStar [Chen et al., 2024c] multimodal instruction-following benchmark for complex visual scenarios, covering reasoning, localization, relationship understanding, and multi-step chain-of-task reasoning. MMMB_CN [Sun et al., 2025] multilingual multimodal benchmark comprising 12,000 multiplechoice questions across 15 categories and 6 languages, designed to rigorously evaluate cross-lingual visionlanguage understanding in multimodal large language models. In this work, we utilize the MMMB_CN subset to specifically assess the Chinese language capability of our model. MME-RealWorld_CN [Zhang et al., 2025] high-resolution multimodal benchmark designed to evaluate multimodal large language models in complex real-world scenarios, featuring diverse tasks that challenge fine-grained perception, reasoning, and robustness under visually demanding conditions. 4.2.4 Hallucination Evaluation Model tendencies toward visual and factual hallucinations are examined through the following benchmarks: HallusionBench [Guan et al., 2024] Diagnoses factual hallucinations in visual scenes, focusing on unsupported inferences and incorrect visual details. POPE [Li et al., 2023b] Tests robustness in determining object existence through contrastive questionanswering, measuring visual hallucination rates. CRPE [Wang et al., 2024c] Evaluates factual consistency and reasoning fidelity in complex visual-text scenarios, particularly for content fabrication in reasoning chains. 4.3 Internal Benchmarks While public benchmarks provide standardized assessment of general capabilities, they often fail to capture the complexities encountered in real-world deployments. To evaluate the practical utility of our model, we construct specialized benchmark tailored to internal scenarios, encompassing four core tasks: user intent recognition and recommendation, image-text creation, UI understanding and structured parsing, and image relevance ranking. 11 4.3.1 Intent Recognition and Recommendation The benchmark evaluates multimodal models ability to parse diverse visual information from device screenshotsincluding UI structures, text, and interaction statesto generate concise search queries that reflect the users potential intent. Evaluation focuses on deep semantic understanding and latent intent extraction, which underpin downstream features such as proactive search recommendations. The assessment measures the models proficiency in interpreting UI visual structures, extracting search intent, and producing relevant, actionable queries under strict constraints. rigorous evaluation protocol is employed: generated query is considered successful only if it simultaneously satisfies six criteriatopic relevance, factual accuracy, logical soundness, linguistic fluency, recommendation value, and safety compliance. Failure in any single criterion results in overall rejection, ensuring high standards for deployment readiness. 4.3.2 Image-Text Creation Model performance is assessed on the ability to generate high-quality text closely aligned with both the application context (e.g., social media editors, review interfaces) and user-uploaded images. The task covers common content-generation and review-related application pages, including posting interfaces on platforms such as Xiaohongshu, WeChat, Weibo, Douyin, Kuaishou, and Dianping, as well as product review pages on platforms such as Dianping and Taobao. Performance is evaluated using four-dimensional framework comprising topic relevance, multimodal alignment, content quality, and style consistency, ensuring that the generated text is semantically accurate, visually grounded, and strictly adherent to user-defined stylistic constraints. 4.3.3 UI Understanding and Structured Parsing The benchmark assesses the models capability to comprehend complex mobile interfaces and structurally extract key information, with focus on order detail pages. Given raw screenshot, the model performs joint analysis of textual content, visual layout, and semantic relationships to identify critical fields without reliance on predefined templates. Structured outputs are produced for downstream applications. Performance is evaluated using Field Accuracy, defined as the ratio of correctly extracted fields to the total number of valid fields across the dataset: FieldAccuracy = (cid:80)N i=1 Ci i=1 Mi (cid:80)N (5) where Ci represents the number of correctly extracted fields in sample i, and Mi represents the total number of ground-truth valid fields in that sample. 4.3.4 Image Relevance Ranking This task employs multimodal large model to perform fine-grained relevance assessment on candidate images retrieved during the initial coarse-retrieval stage. The primary objective is to filter out irrelevant or low-quality results and prioritize the most semantically pertinent images, thereby maximizing the overall precision of the final search results. The model accepts user query and set of candidate images as input. For each (Query, Image) pair, the model performs deep cross-modal interaction modeling to compute fine-grained relevance score. The candidate set is then re-ranked based on these scores to produce an optimized retrieval list. Evaluation relies on Precision@10, Precision, and the F1 Score. Precision@10 measures the proportion of relevant images within the top-10 results of the re-ranked list: Precision@10 = Relevant Results in Top-K 10 (6) Precision evaluates the overall accuracy of relevance predictions (treating the task as binary classification of relevant vs. irrelevant), calculated as: Precision = True Positives (TP) True Positives (TP) + False Positives (FP) (7) 12 Benchmarks Qwen2-VL 2.2B Qwen3-VL 2.1B Ovis2 2.5B InternVL3 2.1B InternVL3.5 2.3B SAIL-VL1.5 2.5B SAIL-VL2 2.7B AndesVL 2.4B HyperVL 1.8B HyperVL ViTL 2.0B Opencompass Scores Opencompassavg 56.4 63.5 62. 63.9 69.1 66.9 69.1 64.6 64. 66.1 MMMU MMMUProV MathVista AI2DTEST ChartQATEST TextVQAVAL DocVQA InfoVQA OCRBench ChineseOCRBench SEEDBenchIMG MME MMBench_DEVCN MMBench_V1.1EN MMBench_V1.1CN MM-Vet MMStar MMMBcn MME-RealWorldCN 41.1 19.9* 43 74.7 73.5 79.7 90.1 65.5 809 77.5* 71.6* 1872 73.5 72.2 72.2 49.5 48 74.2 44.3* 53.4 28.1* 61.3 76.9 77.6* 79.8* 93.3 72.4 858 78.0* 74.8* 2004.2* 76.28 77.8 75.5* 43.3 58.3 78.0* 55.2* Multimodal Reasoning and Mathematics 41.4* 18.5* 64* 48.6 17.5* 57.0 59.0 23.6* 71.8 42.9 16.4* 67.3 OCR, Chart, and Document Understanding 79.7* 79.5* 78.1* 89.6* 69.7* 868* 81.8* 73.3* 1406.7* 72.2* 71.7* 72.1* 44.4* 54.7* 78.9* 52.3* 78.7 80.2 77.0 88.3 66.1 835 76.7* 74.6* 78.8 80.7 76.5 89.4 70.8 836 66.2* 63.8* 83.7 84.8 82.0 91.6 71.8 885 63.0* 74.2* Comprehensive Multimodal Tasks 2221.2 78.4 78.0 78.6 62.2 60.7 78.3 47.5* 2123.0 45.5* 76.6 44.4* 71.7 62.7 77.7 35.9* Hallucination Evaluation 1837.9* 71.7* 78.5 70.8* 61.4 62.8 79.3* 47.9* 47.7 22.2* 71.1 83.0 85.4* 82.1* 93.1 73.5* 895 76.2* 77.3* 2156.1* 81.1* 77.0 80.1* 68.7 64.1 82.6* 59.5* 46.1 22.0* 64. 80.1 87.4 79.9 94.2 74.2 846 88.3* 73.5* 2081.0 77.8* 77.3 77.3 52.0 60.0 78.6* 56.7* 43.4 23.9 66.2 81.8 83.8 75.7 91.3 70.2 830 80.4 74.1 2070.7 79.2 76.5 76.0 56.3 59.2 80.5 54.4 44.6 23.8 65. 83.1 85.7 78.8 92.2 74.6 859 84.3 75.3 2114.7 79.64 77.4 77.1 59.0 61.1 79.3 52.9 41.7 68.3* 86.4* HallBench CRPE POPE Table 3: Comparison of HyperVL with other models with similar parameter scales on diverse benchmarks, which are grouped into 4 domains. Data marked with * are from our evaluation, while others are from their original papers or the OpenCompass leaderboard. 60.8* 64.9* 87.5* 51.7 75.2 86.7* 49.8 61.1* 84.5* 51.4 73.7* 88.8* 51.8 73.0 87.9 42.5 71.5 89.6 48.6 75.6 87.2 49.2 64.5 87. 51.5 62.7 88.9 F1 Score represents the harmonic mean of Precision and Recall, providing balanced view of the models retrieval performance: F1 = 2 Precision Recall Precision + Recall (8) 4.4 Experimental Results 4.4.1 Public Benchmarks Table 3 presents comprehensive comparison between HyperVL and representative visionlanguage models with similar parameter scales. The two rightmost columns report the performance of HyperVL and HyperVL ViTL, both trained under the same architectural framework. Despite its compact scale, HyperVL achieves an OpenCompass average score of 64.5, which is competitive with several 2B3B models. Notably, it demonstrates consistent strengths in OCR, chart, and document understanding tasks, achieving 83.8 on ChartQA, 81.8 on AI2D, and 91.3 on DocVQA. These benchmarks emphasize the models ability to capture fine-grained visual details and structured visual information, which are critical for real-world multimodal applications. Scaling the model to HyperVL ViTL leads to stable and consistent improvements across most benchmarks, raising the OpenCompass average score to 66.1. Performance gains are observed not only in OCR-related tasks but also in comprehensive multimodal benchmarks such as MME and MMBench, indicating that the proposed framework scales smoothly with increased capacity. In the multimodal reasoning and mathematics domain, HyperVL achieves MathVista score of 66.2, outperforming some models of similar scale and remaining competitive with larger counterparts. On more challenging benchmarks such as MMMU, HyperVL maintains solid baseline performance, suggesting balanced reasoning capability despite the limited parameter budget. Furthermore, both HyperVL variants exhibit stable hallucination-related performance on HallBench, with scores comparable to models of larger scale. This indicates that reducing model size does not lead to noticeable degradation in reliability or safety-related behavior. Overall, the results demonstrate that HyperVL offers well-balanced trade-off between model capacity and performance. The base variant emphasizes parameter efficiency and strong visual 13 understanding, while the large variant further improves general multimodal capability with minimal additional cost. These characteristics make HyperVL practical and effective solution for lightweight yet general-purpose multimodal modeling. 4.4.2 Internal Benchmarks Benchmarks UI Understanding and Structure Parsing Image Relevance Ranking Intent Recognition and Recommendation Image-Text Creation Qwen2-VL Qwen3-VL Ovis2 2.5B 2.2B 2.1B InternVL3 2.1B InternVL3.5 2.3B SAIL-VL2 AndesVL HyperVL 2.4B 1.8B 2.7B 80. 48.6 92.7 21.2 91.7 78.1 83. 48.6 44.1 46.2 91.7 91.7 90. 28.9 37.7 45.4 70.6 44.3 80. 43.9 92.7 50.5 89.3 38.6 86. 49.4 95.0 46.1 84.2 51.5 94. 49.8 Table 4: Comparison of HyperVL with other models on internal benchmarks. The highest scores are marked in bold and the second highest are underlined. To assess practical utility in real-world deployment, we evaluate several models on our internal benchmarks covering four critical business scenarios. As shown in Table 4, despite having the smallest parameter size (1.8B), HyperVL demonstrates superior capability in high-level semantic understanding and creative generation, significantly outperforming larger competitors. Our model maintains strong competitiveness in fundamental visual perception and structured processing tasks. In Intent Recognition and Recommendation, HyperVL achieves an impressive 94.0, outperforming Qwen3-VL and InternVL3, proving its reliability in capturing latent user intent. While optimizing for high-level reasoning, it maintains solid baseline in UI Understanding and Structure Parsing. Although slightly lower than other models, its performance effectively balances structural perception with advanced cognitive capabilities, ensuring robust support for downstream interactive applications. HyperVL demonstrates exceptional performance in tasks requiring multimodal alignment and creativity. In the challenging Image-Text Creation task, our model ranks first with score of 49.8, establishing substantial lead over Qwen3-VL and outperforming the strong baseline AndesVL. This highlights its exceptional proficiency in generating context-aware, high-quality content. Additonally, in Image Relevance Ranking, HyperVL ranks first, surpassing specialized models like SAIL-VL2, which confirms its precision in fine-grained visual-semantic matching essential for search and recommendation systems. 4.4.3 Ablation Studies Methods AI2DTEST ChartQATEST TextVQAVAL DocVQA OCRBench Ch_OCR MMStar HyperVL ViTL HyperVL + DCL 83.1 76.8 81.8 +5. 85.7 81.2 83.8 +2.6 78.8 75.5 75.7 +0.2 92.2 90.8 91.3 +0. 859 808 830 +22.0 84.3 79.8 80.4 +0.6 61.1 58.1 59.2 +1. Table 5: Impact dual consistency learning. +DCL means that HyperVL model is trained with dual consistency learning. Ch_OCR refers to ChineseOCRBench. Effectiveness of Dual Consistency Learning. To validate the efficacy of the proposed Dual Consistency Learning (DCL) strategy, we compare the DCL-enhanced model against the baseline HyperVL and the large-scale teacher variant (HyperVL ViTL). As detailed in Table 5, the integration of DCL yields substantial performance gains across all benchmarks. Notably, the model achieves significant improvement of +5.0 on AI2D and +2.6 on ChartQA, alongside remarkable +22.0 point increase on OCRBench. These results strongly suggest that our strategy effectively facilitates the transfer of fine-grained visual knowledge, enabling the lightweight model to bridge the performance gap with larger vision encoders. 14 Effectiveness of Visual Resolution Compressor. We evaluate the Visual Resolution Compressor (VRC) on its ability to balance computational cost with model performance under strict on-device constraints. With marginal overhead of only 2 ms, the VRC is virtually cost-free compared to standard vision encoders yet delivers significant throughput improvements. Experimental results in Table 6 confirm its efficacy: HyperVL achieves 20.2% reduction in visual tokens while maintaining 98.7% of the baseline performance. The benefits are even more pronounced on Qwen3-VL-2B, which sees 29.3% token reduction with 98.4% performance retention. Furthermore, the VRC demonstrates content-adaptive capabilities. It intelligently adjusts compression levels based on image complexity: aggressively compressing high-redundancy inputs like DocVQA (49.5%63.4% reduction), while preserving full resolution for detail-dense tasks like ChartQA ( 2%3% reduction) to ensure readability. For mixed scenarios like OCRBench, it adopts balanced strategy ( 30%). In summary, the VRC provides stable and efficient trade-off, accelerating inference for large inputs without compromising accuracy in detail-sensitive applications. Model HyperVL Baseline + VRC Compression Ratio HyperVL ViTL Baseline + VRC Compression Ratio Qwen3-VL-2B Baseline + VRC Compression Ratio AI2DTEST ChartQATEST TextVQAVAL DocVQA OCRBench Ch_OCR MMStar Avg. 81.8 81.1 14.9% 83.1 83.1 14.9% 76.9 76.9 14.4% 83.8 83.8 3.2% 85.7 85.5 3.2% 77.6 77.6 2.2% 75.7 75.4 4.0% 78.8 78.7 4.0% 79.8 77.82 36.0% 91.3 87.9 49.5% 92.2 88.9 49.5% 93.3 88.9 63.4% 830 820 30.9% 859 846 30.9% 858 831 35.0% 80.4 80.0 27.5% 84.3 82.7 27.5% 78.0 76.8 46.5% 59.2 57.9 11.3% 100% 98.7% 20.2% 61.1 61.7 11.3% 100% 99.1% 20.2% 58.3 58.8 7.9% 100% 98.4% 29.3% Table 6: Performance and token compression ratio of the visual resolution compressor across different models and benchmarks. Ch_OCR refers to ChineseOCRBench. 4.4.4 On-Device Evaluation Figure 5: Efficiency comparison of on-device ViTs cross different image resolutions on Qualcomm 8750 Platform. Left: memory consumption (MB) comparison of diffenet ViTs. Right: inference latency (ms) comparison of different ViTs. Efficiency Analysis. To evaluate the feasibility of running MLLMs on resource-constrained ondevice platforms, we conducted comparative analysis of inference latency and memory overhead between HyperVL, HyperVL ViTL, and the baseline Qwen3-VL-2B. The results demonstrate the significant advantages of our architecture in terms of scalability and resource efficiency. As illustrated in Figure 5, HyperVL demonstrates superior inference efficiency and linear scalability compared to the baseline. While Qwen3-VL-2B suffers from dramatic increase in latency as the image resolution increases. HyperVL maintains highly efficient processing rate. This represents an approximate 12.9 speedup, ensuring real-time responsiveness even for high-resolution inputs. 15 Crucially, regarding resource consumption, unlike the dynamic memory growth of Qwen3-VL (peaking at 1354 MB) HyperVL overall maintains constant memory usage regardless of input complexity, achieving 6.8 reduction in peak memory overhead. The superior efficiency of HyperVL stems from its alignment with the hardware characteristics of Qualcomm NPUs. Standard ViTs using QNN deployment require static allocation of intermediate buffers; as resolution increases, large attention matrices exceed the limited on-chip VTCM, triggering frequent, costly data swaps with external DDR and causing exponential latency growth. In contrast, our serial processing strategy fundamentally alters this scaling law. By processing small, fixed-size chunks, we ensure that intermediate activations consistently fit within the high-speed VTCM. This minimizes memory bandwidth bottlenecks and maintains constant memory footprint, ensuring that inference latency scales linearly rather than exponentially with image resolution. Quantization AI2DTEST ChartQATEST TextVQAVAL DocVQA InfoVQA OCRBench Ch_OCR floating point w4a 81.8 80.6 83.8 83.2 75.7 73.5 91.3 91.2 70.2 68.1 830 80.4 79.8 Table 7: Performance comparison of quantization. Quantization Accuracy Analysis. To validate the feasibility of on-device deployment, we evaluate the models performance under W4A16 quantization (4-bit weights, 16-bit activations). As shown in Table 7, our model demonstrates exceptional robustness to low-bit precision. Notably, on DocVQA, the quantized model retains virtually all of its FP16 performance (91.3 91.2), exhibiting negligible drop of only 0.1 points. Similarly, in ChartQA, the performance remains highly stable (83.8 83.2). Even on aggregate benchmarks like OCRBench, the model maintains 98.2% of its original capability (830 815). This result confirms that the proposed quantization scheme achieves highly favorable trade-off, significantly reducing memory bandwidth requirements while preserving accuracy."
        },
        {
            "title": "5 Conclusion",
            "content": "We present HyperVL, an efficient multimodal large language model designed for on-device inference. By addressing the bottleneck of high-resolution visual encoders through image tiling, visual resolution compressor, and dual consistency learning, HyperVL achieves state-of-the-art performance on multiple multimodal benchmarks while significantly reducing latency and memory usage on mobile devices. The model demonstrates strong generalization for on-device tasks such as UI understanding and parsing, intent recommendation, and image-text creation. Future work includes exploring adaptive token sparsity and attention pruning to further improve efficiency, extending the model to video and interactive scenarios, and incorporating user-adaptive personalization to enhance relevance and resource-aware performance in real-world mobile applications."
        },
        {
            "title": "References",
            "content": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Google DeepMind. Gemini 2.0 is now available to everyone. https://blog.google/ technology/google-deepmind/gemini-model-updates-february-2025/, Feb 2025a. Accessed: 2025-12-XX. Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/, Mar 2025b. Accessed: 2025-12-XX. 16 Google DeepMind. Gemini 2.5 pro. https://deepmind.google/models/gemini/pro/, 2025c. Accessed: 2025-12-XX. Google DeepMind. Introducing gemini 3: our most intelligent model yet. https://blog.google/ products/gemini/gemini-3/, Nov 2025d. Accessed: 2025-12-XX. Google DeepMind. Gemini 3 pro. https://deepmind.google/en/models/gemini/pro/, 2025e. Accessed: 2025-12-XX. Anthropic. Introducing the next generation of claude 3 model family. https://www.anthropic. com/news/claude-3-family, Mar 2024. Accessed: 2025-12-XX. Anthropic. Introducing claude 4: Opus 4 and sonnet 4. https://www.anthropic.com/news/ claude-4, May 2025a. Accessed: 2025-12-XX. Anthropic. Claude opus 4.1. https://www.anthropic.com/news/claude-opus-4-1, Aug 2025b. Accessed: 2025-12-XX. Anthropic. Claude sonnet 4.5. https://www.anthropic.com/news/claude-sonnet-4-5, Sep 2025c. Accessed: 2025-12-XX. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024a. URL https://arxiv.org/abs/2409.12191. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025a. URL https://arxiv.org/abs/2511.21631. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, and Gen Luo. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency, 2025. URL https://arxiv.org/abs/2508.18265. 17 Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone, 2024. URL https://arxiv.org/abs/ 2408.01800. Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1976919780, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025b. URL https://arxiv.org/abs/2502.13923. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025a. URL https: //arxiv.org/abs/2412.05271. Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, and Haonan Lu. Andesvl technical report: An efficient mobile-side multimodal large language model, 2025. URL https://arxiv.org/abs/2510.11496. Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, Bokai Xu, Junbo Cui, Yingjing Xu, Liqing Ruan, Luoyuan Zhang, Hanyu Liu, Jingkun Tang, Hongyuan Liu, Qining Guo, Wenhao Hu, Bingxiang He, Jie Zhou, Jie Cai, Ji Qi, Zonghao Guo, Chi Chen, Guoyang Zeng, Yuxuan Li, Ganqu Cui, Ning Ding, Xu Han, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe, 2025. URL https://arxiv.org/abs/2509.18154. Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, and Wenhai Wang. Vico: training strategy towards semantic aware dynamic high-resolution. arXiv preprint arXiv:2510.12793, 2025. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. Chuanpeng Yang, Yao Zhu, Wang Lu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. Survey on knowledge distillation for large language models: methods, evaluation, and application. ACM Transactions on Intelligent Systems and Technology, 16(6):127, 2025a. 18 Tiannan Wang, Wangchunshu Zhou, Yan Zeng, and Xinsong Zhang. Efficientvlm: Fast and accurate vision-language models via knowledge distillation and modal-adaptive pruning. In Findings of the association for computational linguistics: ACL 2023, pages 1389913913, 2023. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: Llm quantization with learned rotations. In The Thirteenth International Conference on Learning Representations. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International conference on machine learning, pages 3808738099. PMLR, 2023. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 467484, 2024a. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding. Localization, and Dense Features, 6, 2025. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024a. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025b. URL https://arxiv.org/abs/2505.09388. Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, et al. Mobilenetv4: Universal models for the mobile ecosystem. In European Conference on Computer Vision, pages 7896. Springer, 2024. Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, Zhenchong Hu, Bo-Wen Zhang, Jijie Li, Dong Liang, Yingli Zhao, Songjing Wang, Yulong Ao, Yiming Ju, Huanhuan Ma, Xiaotong Li, Haiwen Diao, Yufeng Cui, Xinlong Wang, Yaoqi Liu, Fangxiang Feng, and Guang Liu. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data, 2025. URL https://arxiv.org/abs/2410.18558. Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andrés Marafioti. Finevision: Open data is all you need, 2025. URL https://arxiv.org/abs/2510.17269. Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, and Jiankang Deng. Llava-onevision-1.5: Fully open framework for democratized multimodal training, 2025. URL https://arxiv.org/abs/2509.23661. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding, 2024b. URL https://arxiv.org/abs/2306.17107. 19 Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021. Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, and Jifeng Dai. Mminstruct: high-quality multimodal instruction tuning dataset with extensive diversity. Science China Information Sciences, 67(12), December 2024b. ISSN 1869-1919. doi: 10.1007/s11432-024-4187-3. URL http: //dx.doi.org/10.1007/s11432-024-4187-3. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024b. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025b. Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b, 2023. URL https://arxiv.org/abs/2303.12733. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015. URL https://arxiv.org/abs/1504.00325. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In European conference on computer vision, pages 742758. Springer, 2020. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision (IJCV), 2020. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 80768084, 2019. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146162. Springer, 2022. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning, 2022a. URL https://arxiv.org/abs/2110.13214. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326335, 2017. 20 Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Benjamin Trom, and Romain Beaumont. LAION-COCO: 600m synthetic captions from laion2b-en. https://laion.ai/ blog/laionâĂŚcoco/, 2022. Accessed: YYYY-MM-DD. NAVER Clova Information Extraction. Synthdog-en dataset. https://huggingface.co/ datasets/naver-clova-ix/synthdog-en, 2023a. NAVER Clova Information Extraction. Synthdog-zh dataset. https://huggingface.co/ datasets/naver-clova-ix/synthdog-zh, 2023b. Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15571562. IEEE, 2019. He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding. Eaten: Entity-aware attention for single shot visual text extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 254259. IEEE, 2019. Haiyang Yu, Jingye Chen, Bin Li, Jianqi Ma, Mengnan Guan, Xixi Xu, Xiaocong Wang, Shaobo Qu, and Xiangyang Xue. Benchmarking chinese text recognition: Datasets, baselines, and an empirical study, 2022. URL https://arxiv.org/abs/2112.15093. Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1952819540, 2023. Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: universal network for real-world mathematical expression recognition, 2024b. URL https://arxiv.org/abs/2404.15254. Teklia. Iam-line dataset. https://huggingface.co/datasets/Teklia/IAM-line, 2023. Accessed: 2025-12-15. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual In European Conference on tokenization for grounding multimodal large language models. Computer Vision, pages 417435. Springer, 2024. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. Hongxin Li, Jingfan Chen, Jingran Su, Yuntao Chen, Li Qing, and Zhaoxiang Zhang. Autogui: Scaling gui grounding with automatic functionality annotations from llms. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1032310358, 2025. 21 Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020. AgentSea. Waveui. https://huggingface.co/datasets/agentsea/wave-ui, 2024. HKUST DeepArt Group. Art500k: large-scale dataset for artistic text recognition. https: //deepart.hkust.edu.hk/ART500K/art500k.html, 2023. Accessed: 2025-12-15. Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Anita Rau, Austin Wolfgang Katzer, et al. Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1972419735, 2025. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022b. Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. selfsupervised descriptor for image copy detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1453214542, 2022. Christoph Zauner. Implementation and benchmarking of perceptual image hash functions. Masters thesis, Upper Austria University of Applied Sciences, Hagenberg Campus, 2010. URL https://www.phash.org/docs/pubs/thesis_zauner.pdf. Masters thesis, includes the pHash perceptual hashing library. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513415186, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310.02255. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 22 Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024c. SWHL. Chineseocrbench: benchmark dataset for chinese ocr evaluation. https://huggingface. co/datasets/SWHL/ChineseOCRBench, 2024. Accessed: 2025-02-05. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023a. URL https://arxiv.org/ abs/2307.16125. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024d. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. URL https://arxiv.org/abs/2308.02490. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024c. Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, and Han-Jia Ye. Parrot: Multilingual visual instruction tuning, 2025. URL https://arxiv.org/abs/2406.02539. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans?, 2025. URL https://arxiv.org/abs/2408.13257. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023b. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In European Conference on Computer Vision, pages 471490. Springer, 2024c."
        },
        {
            "title": "A Contributor",
            "content": "Yuchen Liu*, Kaiyang Han*, Zhiqiang Xia*, Yuhang Dong*, Chen Song*, Kangyu Tang*, Jiaming Xu, Xiushi Feng, WenXuan Yu, Li Peng, Mingyang Wang, Kai Wang, Changpeng Yang, Yang Li, Haoyu Lu, Hao Wang, Bingna Xu, Guangyao Liu, Long Huang, Kaibin Guo, Jinyang Wu, Dan Wu, Hongzhen Wang, Peng Zhou, Shuai Nie, Shande Wang, Runyu Shi, Ying Huang * Equal contribution. Project leader, # xujiaming1@xiaomi.com"
        },
        {
            "title": "B Qualitative Examples",
            "content": "In this section, we present collection of qualitative examples that illustrate the breadth of capabilities supported by HyperVL. These examples span variety of multimodal reasoning and perception tasks, including mathematical computation, structured information extraction, geometric reasoning, interface-level planning, image description with character recognition, common-sense inference, and knowledge-based understanding. Table 8 provides an example of mathematical computation derived from visual input, demonstrating the models ability to perform systematic numerical reasoning based on images containing equations or quantitative information. Table 9 presents an example of chart content extraction, where the model effectively interprets structured visual representations such as plots and diagrams. Table 10 illustrates an example of geometric reasoning, showing the models capacity to integrate diagrammatic cues with symbolic reasoning. Table 11 offers an example of GUI understanding and planning, highlighting how the model interprets graphical user interface elements and generates coherent action plans. Table 12 demonstrates an example of image description combined with character extraction, reflecting the models ability to simultaneously comprehend visual scenes and transcribe embedded text. Table 13 provides an example of common-sense reasoning grounded in real-world scenarios. Finally, Table 14 showcases an example of knowledge-based reasoning in the artistic domain, where HyperVL interprets an artwork and exhibits both its grasp of art-specific knowledge and its capacity for extended contextual associations. 24 Prompt: 请解决这个问题 Model Response: 好的我们来一步步解决这个方程 **原方程** + 3 4 1 = 2x 1 6 **步骤1消除分母** 为了消去分母我们可以找到4 和6 的最小公倍数LCM4 和6 的最小公倍数是12将方程 两边同时乘以12以消除分母 12 (cid:18) + 3 (cid:19) 1 = 12 2x 1 6 **步骤2简化方程** 计算左边 12 12 1 = 3(x + 3) 12 + 3 4 3x + 9 12 = 3x 3 计算右边 12 = 2(2x 1) 2x 1 6 4x 现在方程变为 3x 3 = 4x 2 **步骤3移项** 将含有x 的项移到一边常数项移到另一 边从两边减去3x 3 = 2 两边同时加上2 3 + 2 = = 1 **步骤4验证解** 将x = -1 代入原方程进行验证左边 右边 1 + 3 4 1 = 2 4 1 = 1 2 1 = 1 2 2(1) 1 6 = 2 1 6 = 3 = 1 2 两边相等验证正确 **最终答案** = 1 Table 8: Mathematical reasoning results. Prompt: Whats the increase of the step therpy between 2005 to 2013? Please calculate it step by step. Model Response: To calculate the increase of step therapy between 2005 and 2013, we need to subtract the percentage of step therapy in 2005 from the percentage in 2013. Percentage of step therapy in 2005: 27% Percentage of step therapy in 2013: 67% Increase = (Percentage in 2013 - Percentage in 2005) Increase = (67% - 27%) Increase = 40% Therefore, the increase of step therapy between 2005 and 2013 is 40%. Table 9: Visual chart interpretation and data extraction. Prompt: If the ABCDE shape is combination of rectangle and an equilateral triangle, the perimeter of the ABCDE shape is 84, the BCFG shape is combination of rectangle and semi-circle, the perimeter of the BCFG shape is 46 and the area of the CIJF square is 36, compute the length of the AB side of the ABCDE shape. Assume π = 3.14. Round computations to 2 decimal places. Model Response: The area of the CIJF square is 36, so the length of the CF side 36 = 6. The perimeter of the BCFG shape is 46 and the is length of the CF side is 6, so 2 OtherSide + 6 + 63.14 2 = 46. So 2 OtherSide = 46 6 63.14 2 = 46 6 9.42 = 30.58. Therefore, the length of the BC side is 30.58 2 = 15.29. The side of the equilateral triangle in the ABCDE shape is equal to the side of the rectangle with length 15.29 so the shape has two rectangle sides with equal but unknown lengths, one rectangle side with length 15.29, and two triangle sides with length 15.29. The perimeter of the ABCDE shape is 84 so 2 nknownSide + 3 15.29 = 84. So 2 nknownSide = 84 45.87 = 38.13, and the length of the AB side is 38.13 2 = 19.07. Therefore the final answer is 19.07. 2 = 46 6 18.84 Table 10: An example of geometric reasoning. 26 Prompt: 怎样购买手机最划算 Model Response: 根 据 图 片 中 的 信 息 购 买 手 机 可 以 享 受手 机 至 高 优 惠200元的活动具体优惠金额和购买渠道可以在年终 盛典部分查看此外还可以通过手机选购指南获取更 多购买建议和优惠信息 Table 11: GUI understanding and task planning performed. 27 Prompt: 详细描述一下这张图片 Model Response: 这张图片展示了一张名为浙江美食合集的海报背景为 黄色海报上展示了多种浙江特色菜肴的图片和名称这 些菜肴被排列成五行每行有五道菜 第一行从左到右分别是 - 叫花鸡 - 东坡肉 - 干菜焖肉 - 火瞳神仙鸡 - 白鸟朝凤 第二行从左到右分别是 - 油爆大虾 - 干炸响铃 - 南肉春笋 - 金玉满堂 - 咸菜大汤黄鱼 第三行从左到右分别是 - 龙井虾仁 - 蜜汁火方 - 三丝敲鱼 - 宋嫂鱼羹 - 冰糖甲鱼 第四行从左到右分别是 - 荷叶粉蒸肉 - 拔丝蜜桔 - 嘉兴粽子 - 油焖春笋 - 温州猪脏粉 第五行从左到右分别是 - 砂锅鱼头豆腐 - 宁波汤圆 - 千张包子 - 西湖莼菜汤 - 西湖醋鱼 每 道 菜 的 图 片 都 清 晰 地 展 示 了 菜 肴 的 外 观 旁 边 配 有 相应的中文名称整体设计简洁明了突出了浙江美食的 多样性和特色 Table 12: Image description and text extraction. 28 Prompt: 图中实际有几辆汽车 Model Response: 图中实际上只有一辆汽车这辆车被设计成悬浮在空中 看起来像是有两辆车但实际上只有一辆车 Table 13: Common-sense reasoning on real-world scenarios. Prompt: Please introduce the author of this painting Model Response: The painting youve shown is modern interpretation of Frida Kahlos self-portraits, specifically drawing inspiration from her iconic style and themes. Frida Kahlo was Mexican artist known for her self-portraits, which often depicted her physical and emotional pain, as well as her cultural heritage. Her works frequently included vibrant colors, symbolic imagery, and blend of realism and surrealism. In this particular piece, the artist has incorporated elements such as butterflies, monkey, cat, and hummingbird, which are common motifs in Kahlos work. The background features lush green leaves, adding to the organic and natural feel of the painting. Table 14: Knowledge-based visual reasoning."
        }
    ],
    "affiliations": [
        "Xiaomi Corporation"
    ]
}