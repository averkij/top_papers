{
    "paper_title": "Autoregressive Image Generation with Masked Bit Modeling",
    "authors": [
        "Qihang Yu",
        "Qihao Liu",
        "Ju He",
        "Xinyang Zhang",
        "Yang Liu",
        "Liang-Chieh Chen",
        "Xi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/"
        },
        {
            "title": "Start",
            "content": "Qihang Yu 1 Qihao Liu 1 Ju He 1 Xinyang Zhang 1 Yang Liu 1 Liang-Chieh Chen 2 * Xi Chen 1 * 6 2 0 2 9 ] . [ 1 4 2 0 9 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves new state-of-theart gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/ 1. Introduction Visual generative models have driven remarkable progress across wide range of computer vision tasks (Wang et al., 2023; Team, 2024; Cui et al., 2025; Deng et al., 2025; Wiedemer et al., 2025; Agarwal et al., 2025). central component of these systems is visual tokenization, which compresses high-dimensional pixel inputs into compact latent representations. Operating on these latent tokens, generative model learns the underlying image distribution to synthesize 1Amazon FAR (Frontier AI & Robotics) 2Work done while at FAR. *: Equal advising. Correspondence to: Qihang Yu <yuqiha@amazon.com>. Preprint. February 10, 2026. Figure 1. The proposed BAR achieves superior quality-cost trade-off (generation FID vs. throughput) on ImageNet-256. high-fidelity visual content. Depending on quantization and regularization strategies, visual tokenization and generation pipelines can be broadly categorized into discrete and continuous approaches. Each paradigm offers distinct advantages: discrete tokenizers align naturally with language modeling, making them suitable for native multimodal large language models (Team, 2024; Cui et al., 2025), whereas continuous tokenizers excel at modeling raw visual signals and preserving fine-grained details. Despite progress in both directions, continuous tokenizers, typically with diffusion models, remain dominant in visual generation (Rombach et al., 2022; Peebles & Xie, 2023; Li et al., 2024; Zheng et al., 2025b). This dominance is largely attributed to their higher information capacity, which enables superior reconstruction fidelity and higher ceiling for generation (Li et al., 2024; Wang et al., 2025b). In this work, we investigate the performance gap between discrete and continuous pipelines. Our key observation is that this gap is not intrinsically caused by the nature of the representations, but is instead largely associated with differences in the compression rates used in practice. To make this comparison explicit, we unify both paradigms under common metric: the number of bits used to represent the latent space. From this unified perspective, we find that the commonly observed inferior performance of discrete tokenizers is largely attributable to their substantially higher compression ratios, which lead to severe information loss. Empirically, we show that allocating more bits per token (equivalent to scaling up the codebook size) allows discrete tokenizers to match, and in some cases surpass, their continAutoregressive Image Generation with Masked Bit Modeling ple visual inputs along spatial dimensions while expanding channel dimensions, thereby providing more compact and structured representation space that is well suited for diffusion-based generation. While large body of work has focused on diffusion model architectures (Peebles & Xie, 2023; Bao et al., 2023; Gao et al., 2023; Liu et al., 2024; Wang et al., 2025a), denoising trajectories (Lipman et al., 2022; Ma et al., 2024; Liu et al., 2025), and prediction objectives (Li et al., 2024; Ren et al., 2024; 2025; He et al., 2025), SD-VAE (Rombach et al., 2022) has remained the de facto standard VAE backbone in most studies. More recently, increasing attention has been paid to enriching the semantic content of VAE latent spaces, either by incorporating off-the-shelf models (Yao et al., 2025) or by using frozen encoders as tokenizers (Zheng et al., 2025b). There are also works (Hoogeboom et al., 2024; Li et al., 2025; Li & He, 2025) that explore tokenizer-free diffusion models operating in pixel space. Discrete Visual Tokenization and Generation. Building on the foundation of VQGAN (Esser et al., 2021), substantial body of work has focused on quantizer, the core component of discrete pipelines. One stream of research aims to enhance the utilization and training dynamics of vanilla vector quantization with learnable codebooks (Yu et al., 2022; Zheng & Vedaldi, 2023; Zhu et al., 2024). Conversely, other approaches abandon learnable codebooks entirely in favor of lookup-free quantizers (Mentzer et al., 2024; Yu et al., 2024a; Zhao et al., 2025). Notably, while these approaches tokenize images into bit tokens, they primarily emphasize the benefits of lookup-free quantization, and do not exploit this bit-level structure to redefine the generation targets. Among these studies, the most closely related works are MaskBit (Weber et al., 2024) and Infinity (Han et al., 2025). MaskBit (Weber et al., 2024) adopts LFQ (Yu et al., 2024a) as the tokenizer and directly feeds bit tokens into the generator. However, it still predicts codebook indices rather than bits during generation, which limits scalability with respect to codebook size, similar to standard discrete generative models. Infinity (Han et al., 2025) scales to extremely large codebook sizes (264) using BSQ (Zhao et al., 2025) and directly generates images from bits. Nevertheless, it relies heavily on the VAR generator (Tian et al., 2024) and an external bit-corrector as post-processing module. In contrast, the proposed BAR framework is compatible with arbitrary autoregressive formulations and generates bit tokens correctly in fully self-contained manner, enabled by the proposed masked bit modeling head. 3. Method 3.1. Background We begin by introducing the visual tokenization process. visual tokenizer, whether discrete or continuous, can Figure 2. Best discrete and continuous generator comparison. uous counterparts in reconstruction quality. While increasing the codebook size narrows the reconstruction performance gap, it poses significant challenge for generative modeling. Discrete generators are typically trained with cross-entropy objectives, and large vocabularies substantially increase both computational and statistical complexity. In particular, scaling the codebook size makes training prohibitively memory-intensive (Han et al., 2025) and increasingly difficult to optimize (Yu et al., 2024a). To address this, we propose replacing the standard linear prediction head with lightweight bit generation mechanism. Instead of classifying over massive vocabulary, our method predicts discrete tokens by progressively generating their constituent bits. This design effectively accommodates unbounded vocabulary sizes and consistently improves generation performance, particularly as the codebook size scales. In summary, discrete tokenizers can serve as competitive visual compressors relative to their continuous counterparts, and that discrete generators can outperform diffusion models in generation fidelity while achieving faster convergence and higher sampling throughput. Building on these findings, we propose masked Bit AutoregRessive modeling (BAR), strong discrete visual generation framework that challenges the prevailing dominance of continuous pipelines. BAR establishes new state of the art: with only 415M parameters, it achieves gFID of 1.13 on ImageNet-256 (Deng et al., 2009), surpassing prior discrete models while being 3.68 faster than leading continuous approaches (Zheng et al., 2025b). Additionally, our efficient variant matches the performance of one-step model MeanFlow (Geng et al., 2025) with 2.94 speedup. Finally, our best-performing variant attains gFID of 0.99, setting new benchmark across both discrete and continuous paradigms. 2. Related Work Continuous Visual Tokenization and Generation. Continuous visual tokenization and generation pipelines typically consist of two main components: variational autoencoder (VAE) (Kingma & Welling, 2014) and diffusion model (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020). VAEs are autoencoders trained with specific regularization on the latent space (e.g., KL regularization (Rombach et al., 2022)). They usually downsamAutoregressive Image Generation with Masked Bit Modeling be viewed as an autoencoder (Hinton & Salakhutdinov, 2006) equipped with an information bottleneck (Kingma & Welling, 2014; Esser et al., 2021; Mentzer et al., 2024). Structurally, it consists of three key components: an encoder Encoder, bottleneck module Bottleneck, and decoder Decoder. The nature of the bottleneck distinguishes the two paradigms: discrete bottleneck maps latent features to entries in finite codebook, whereas continuous bottleneck typically employs dimensionality reduction coupled with regularization, such as the KL-divergence penalty. Given an input image RHW 3, where and denote the image height and width, respectively, the encoder first maps the image to dense feature map L: = Encoder(I), (1) where is the encoded feature with spatial shape f . This feature map is then processed by the bottleneck module Bottleneck to yield the latent representation X. This step imposes paradigm-specific constraintssuch as quantization for discrete models or KL-regularization for continuous ones. Finally, the decoder Decoder reconstructs the image ˆI from these latents: = Bottleneck(L), ˆI = Decoder(X). (2) In practice, each latent token is encouraged to follow structured distribution (e.g., discrete or Gaussian), which facilitates subsequent generative modeling by making the latent space easier to model and sample from. Figure 3. unified view for comparing discrete and continuous tokenizers. By measuring information capacity in bits, we enable direct comparison. The continuous tokenizer MAR-VAE (Li et al., 2024) outperforms the discrete tokenizer LlamaGen-VQ (Sun et al., 2024) in reconstruction quality, result directly attributable to its substantially higher bit allocation. discrete tokenizer with codebook size C, its bit budget is1: Bdiscrete = f log2 C. (3) Conversely, for continuous tokenizer with latent channel dimension D, the bit budget is: Bcontinuous = f 16D, (4) where the constant factor 16 reflects mixed-precision training, with each latent channel represented using 16 bits. While bit budget defines the nominal capacity, the effective information content may be lower due to dead codebook entries or distributional regularization. This metric facilitates the direct comparison shown in Fig. 3. 3.2. Benchmarking Discrete and Continuous Tokenizers 3.3. Discrete Tokenizers Beat Continuous Tokenizers The primary distinction between discrete and continuous tokenizers lies in the design of the bottleneck. Discrete tokenizers typically rely on codebook lookup with hard assignments to discretize latent features, whereas continuous tokenizers impose bottlenecks through dimensionality reduction combined with regularization losses. This fundamental difference makes direct comparison between the two paradigms nontrivial. In practice, discrete tokenizers are commonly characterized by their codebook size (Esser et al., 2021; Yu et al., 2022), while continuous tokenizers are often compared based on the dimensionality of their latent representations (Rombach et al., 2022; Li et al., 2024). To enable unified and fair comparison across these paradigms, we evaluate both tokenizers using common metric: the Bit Budget (B). This metric quantifies the total information capacity allocated to the latent space, serving as proxy for the nominal compression ratio. Formally, consider an input image of height and width , processed by tokenizer with spatial downsampling factor . For Equipped with the Bit Budget metric, we conduct systematic evaluation of existing discrete and continuous tokenizers. As shown in Fig. 4, we observe distinct separation between the two paradigms: discrete tokenizers generally exhibit worse reconstruction quality while using substantially fewer bits. This discrepancy in compression ratio is non-negligible and can largely account for the inferior reconstruction performance observed in discrete methods. Crucially, we identify convergence trend: as we increase the number of bits allocated to the latent space, the performance of discrete tokenizers progressively improves, narrowing the gap with continuous tokenizers. This observation prompts critical investigation: Is the perceived inferiority of discrete tokenizers intrinsic to the quantization bottleneck, or is it merely consequence of insufficient bit allocation? 1We mainly discuss the most common single-scale and singlecodebook tokenizers, whereas the formulation can be easily generalized to other cases such as multi-scale (Tian et al., 2024) or multi-codebook (Qu et al., 2025; Ma et al., 2025a). 3 Autoregressive Image Generation with Masked Bit Modeling approaches. 3.4. Discrete Autoregressive Models Beat Diffusion While scaling the codebook size effectively resolves the reconstruction bottleneck (as established in the preceding subsection), it introduces new, critical impediment to generative modeling: the vocabulary scaling problem. Standard autoregressive models face prohibitive computational cliff as vocabularies expand. Projecting highdimensional hidden states onto vocabulary of millions (220) or billions (230) of entries renders the final linear prediction head intractable in terms of both memory and compute. Consequently, prior works typically cap codebook sizes at 218 (262144), accepting ceiling on reconstruction fidelity to preserve trainability. Furthermore, even when hardware permits, learning reliable categorical distribution over such vast space is statistically difficult, leading to sharp degradation in generation quality (Yu et al., 2024a). We empirically validate this limitation by training models with standard linear prediction head across different codebook sizes. The model works fine with limited codebook size but stops at 18 bits (corresponding to vocabulary sizes 262144); beyond this range, training becomes unaffordable under typical GPU memory constraints. We also experimented with bits-based head that predicts the bit representation of target discrete token instead of the index over entire vocabulary (Han et al., 2025). While this approach enables training with large codebook sizes, it consistently yields inferior performance across vocabulary sizes and suffers from severe degradation as the vocabulary scales. Prediction Head as Bit Generator. To overcome this, we disentangle the generator into two distinct functional components: an Autoregressive Transformer, which captures global structure via causal attention, and Prediction Head, which projects latent embeddings onto specific discrete codes. This separation is critical: as codebook sizes scale, the autoregressive transformer remains computationally invariant; the entire burden of the exponential vocabulary growth is absorbed exclusively by the prediction head. Unlike prior approaches relying on linear or bit-based projection, we propose paradigm shift: rather than treating token prediction as massive classification task, we formulate it as conditional generation task. We introduce Masked Bit Modeling (MBM) Head, which generates the target discrete token via an iterative, bit-wise unmasking process conditioned on the autoregressive transformers output. The proposed prediction head is lightweight, typically requiring only small number of additional forward passes to decode discrete token. Formulation. Let denote the autoregressive transformer (Vaswani et al., 2017). Given causal prefix of Figure 4. Scaling BARs discrete tokenizer (BAR-FSQ) with Bit Budget. Standard discrete methods (green circles) historically lag behind continuous baselines (blue circles) primarily due to restricted bit allocation. By systematically scaling the codebook size, BAR-FSQ (red curve) demonstrates that discrete tokenizers reconstruction performance is not inherently bounded; it matches and further surpasses continuous reconstruction fidelity with increased bit budget, challenging the assumption that continuous latent spaces are required for high-fidelity reconstruction. To address this, we examine the effect of scaling the codebook size to approach the bit budget of continuous tokenizers. Since classical Vector Quantization (VQ) with learnable codebooks becomes computationally infeasible at extreme scales (e.g., 2256), we adopt the FSQ quantizer (Mentzer et al., 2024)2. This allows us to scale smoothly without auxiliary quantization or entropy losses (Chang et al., 2022; Yu et al., 2024a; Zhao et al., 2025). For simplicity, we fix the number of latent tokens to 256 (i.e., downsampling ratio of = 16 for 256 256 images) and use 1 bit per channel in the FSQ quantizer. We then vary the latent channel dimension from 10 to 12, 14, 16, 18, 32, 64, 128, and 256, corresponding to codebook sizes of 210, 212, 214, 216, 218, 232, 264, 2128, and 2256, respectively. The results, shown by the BAR-FSQ curve in Fig. 4, demonstrate that reconstruction quality improves consistently with codebook size. Notably, when the bit budget increases beyond certain point, the discrete tokenizer achieves competitive or superior fidelity. For instance, at budget of 65536 bits, our discrete tokenizer attains an rFID of 0.33, outperforming the SD-VAE (rFID 0.62). Furthermore, discrete tokenizers demonstrate superior efficiency in budget utilization. With only 16384 bits, we achieve comparable performance (rFID 0.50). This indicates that discrete method yields highly expressive representations even under strict constraints, leading to our core discovery: The main performance bottleneck of discrete tokenizer lies in an insufficient bit budget, while scaling up codebook size enables discrete tokenization outperform continuous 2The discussion here can easily generalize to other bit quantization such as LFQ (Yu et al., 2024a) or BSQ (Zhao et al., 2025). 4 Autoregressive Image Generation with Masked Bit Modeling Figure 5. Overview of the proposed BAR framework. We decompose autoregressive visual generation into two stages: context modeling and token prediction. (a) For context modeling, we employ an autoregressive transformer to generate latent conditions via causal attention. For the subsequent token prediction stage, we contrast our method with two baselines: (b) standard linear head predicts logits over the full codebook. While effective for small vocabularies (< 218), it fails to scale to larger sizes due to computational bottlenecks. (c) bit-based head predicts bits directly; while scalable, it results in inferior generation quality. (d) The proposed Masked Bit Modeling (MBM) head generates bits via progressive unmasking mechanism conditioned on the autoregressive transformers output. Unlike the baselines, MBM achieves both exceptional scalability and superior generation quality. At inference, the next token is not selected via single sampling step but is generated through progressive bitwise unmasking schedule (Chang et al., 2022). As illustrated in Fig. 5, this design offers two key advantages. First, in terms of scalability, decomposing the token into its constituent bits bypasses the need for monolithic softmax over the entire vocabulary, reducing memory complexity from O(C) to O(log2 C), where = 2k is the codebook size. Second, regarding robustness, the bit-wise masking acts as strong regularizer that consistently improves generation quality. As result, the MBM head yields superior trade-off between reconstruction (rFID) and generation (gFID) across all codebook scales, as shown in Fig. 6. Discussion. Unlike standard linear heads, which are constrained by fixed computational costs and memory requirements that scale linearly with vocabulary size, the proposed masked bit modeling head facilitates discrete generation with arbitrarily large vocabularies. Our results demonstrate consistent improvements over baselines, with the advantage becoming more pronounced at larger codebook sizes. This improved scaling behavior stems from the models capacity to flexibly allocate more computation per token via progressive unmasking, mechanism analogous to the iterative denoising process in diffusion models. Figure 6. Reconstruction and generation quality as function of BAR tokenizers vocabulary size. Unlike the linear head, the proposed masked bit modeling head scales to arbitrary codebook sizes. Furthermore, it achieves superior reconstruction generation trade-off compared to the bit head. discrete tokens {x1, x2, . . . , xi1}, where each token is represented by k-bit binary code, the autoregressive transformer maps the input to sequence {z1, z2, . . . , zi1}. Specifically, for the prediction of the i-th token, we have: zi1 = F({x1, x2, . . . , xi1}), (5) We utilize zi1 as condition to predict the next token xi via masked bit modeling head parameterized by θ: ˆxi = Gθ (cid:0)Maskbit(xi) zi1, M(cid:1), (6) where Maskbit() randomly masks subset of bits in xi by replacing them with special mask token, and denotes the masking ratio. 4. Experimental Results 4.1. Implementation Details During training, we optimize cross-entropy loss between the predicted token ˆxi and the ground-truth token xi: = 1 (cid:88) i= CrossEntropybit(xi, ˆxi), (7) where is the sequence length, and CrossEntropybit applies the loss in bit-wise manner. Tokenizer. We build the discrete tokenizer using FSQ (Mentzer et al., 2024), incorporating several modern design choices from recent works (Weber et al., 2024; Tian et al., 2024; Lu et al., 2025; Zheng et al., 2025b) to better align with contemporary training recipes. Specifically, we initialize the encoder from SigLIP2-so400M (Tschannen et al., 2025) and apply an L2 loss against the original 5 Autoregressive Image Generation with Masked Bit Modeling Table 1. Scaling BAR-FSQ codebook size (C) with different prediction heads. Unlike linear and bit-based baselines, the proposed masked bit modeling (MBM) scales to arbitrary codebook sizes while delivering superior generation quality. bits 10 12 14 18 32 64 1024 rFID 1.25 4096 1.13 16384 1.06 1.02 262144 0.93 4.29 109 0.70 1.84 0.50 prediction head linear bit MBM linear bit MBM linear bit MBM linear bit MBM linear bit MBM linear bit MBM linear bit MBM w/o CFG w/ CFG gFID 2.80 10.77 3.10 2.70 14.52 2.10 2.60 14.57 1.71 2.90 23.11 1.68 3.45 21.81 1.77 OOM 45.11 2.37 OOM 73.67 2.60 IS 171.6 107.6 180.6 180.3 100.1 207.9 192.4 96.9 240.8 182.7 71.8 231.6 172.9 78.7 228.5 OOM 49.6 197.8 OOM 30.7 213. gFID 1.73 2.63 1.48 1.67 3.24 1.25 1.52 2.85 1.22 1.63 3.83 1.19 1.91 3.67 1.20 OOM 5.81 1.37 OOM 10.97 1.67 IS 238.7 213.9 271.2 248.9 213.9 268.3 263.0 224.5 292.2 253.2 215.0 282.3 241.0 223.0 281.1 OOM 204.3 292.1 OOM 183.0 295.0 CLIP features to encourage semantic alignment in the latent space (Zheng et al., 2025a). For the decoder, we use ViTL (Dosovitskiy et al., 2021) model trained from scratch, and we employ frozen DINO model (Caron et al., 2021; Sauer et al., 2023; Tian et al., 2024; Zheng et al., 2025b) as the discriminator. The final training objective combines L1, L2, perceptual (Zhang et al., 2018), Gram loss (Lu et al., 2025) and GAN losses (Goodfellow et al., 2014). The training is conducted for 40 epochs for ablation studies. For final models, we finetune the decoder for 40 more epochs. Generator. We build upon the state-of-the-art discrete autoregressive generation model RAR (Yu et al., 2025a). In addition, we augment the model with several architectural components commonly used in recent diffusion-based generators (Yao et al., 2025; Li & He, 2025; Zheng et al., 2025b), including RoPE (Su et al., 2024), SwiGLU (Shazeer, 2020), RMSNorm (Zhang & Sennrich, 2019), and repeated class conditioning (Li & He, 2025). The masked bit modeling head employs 3-layer SwiGLU with adaLN (Ba et al., 2016; Peebles & Xie, 2023), which is lightweight and incurs only marginal extra cost. All training hyperparameters strictly follow the original RAR configuration. Training is conducted for 400 epochs with batch size of 2048. Sampling. We sample 50000 images for FID computation using the evaluation code from (Dhariwal & Nichol, 2021). When classifier-free guidance is employed, we adopt simple linear guidance schedule (Chang et al., 2023). 4.2. Ablation Studies We study the impact of different designs based on BAR-B, supported by results both without and with classifier-free guidance (CFG) (Ho & Salimans, 2022) for comprehensive analysis of how different designs affect performance. Different Prediction Heads. As shown in Tab. 1, the linear head performs reasonably well when the codebook size is small, but it does not scale to large codebook sizes: when the vocabulary reaches 232, training is no longer feasible within reasonable resource budget. Although the bits head (Han et al., 2025) partially alleviates the computational bottleneck by making generation with large vocabularies affordable, its generation quality is significantly inferior. Without CFG, all bits-head variants yield gFID values > 10, and even with CFG, performance remains poor with gFID > 2.6, indicating substantial degradation in generation quality. Besides, its performance degrades as vocabulary scales. In contrast, the proposed masked bit modeling head not only scales naturally to arbitrary codebook sizes, but also consistently yields superior generation performance. Even with large codebook of size 232, it achieves gFID of 1.37, approaching state-of-the-art performance. Masking Ratio Sampling Strategy. We evaluate different masking ratio sampling strategies during training in Tab. 2a, specifically comparing arccos (Besnier & Chen, 2023), uniform, and logit-normal sampling (Esser et al., 2024). In contrast to typical Masked Image Modeling (MIM) generative models (Chang et al., 2022; Besnier & Chen, 2023; Yu et al., 2024b; Weber et al., 2024), which often favor tail-heavy distributions (e.g., arccos), BAR does not require such skewing. Instead, simple uniform sampling performs remarkably well. Overall, BAR demonstrates robustness across all strategies, achieving competitive generation quality in each case. We adopt logit-normal sampling as the default, as it yields slight performance advantage, particularly for larger codebook sizes. Prediction Head Size. We summarize the impact of prediction head capacity in Tab. 2b, varying both the number of layers and the hidden width. We observe consistent improvements in generation quality without CFG as the prediction head capacity increases, while the gains become less pronounced when CFG is applied. Interestingly, the benefits of larger prediction head are more substantial for larger codebook sizes, suggesting that predicting discrete tokens from larger space is inherently more challenging and therefore benefits from stronger generative prediction head. Sampling Strategy. We ablate sampling strategies in Tab. 2c along two dimensions: the number of sampling steps and the bit unmasking schedule. Increasing the number of sampling steps from 2 to 3 yields significant improvement in generation quality, while further increasing the steps to 5 or 6 provides only marginal gains. We also evaluate back-loading bit unmasking schedule and find that it improves performance with CFG, but slightly degrades performance without CFG, where uniform unmasking schedule remains preferable. Efficient Generation with Token-Shuffling. The proposed MBM head offers an additional advantage by enabling efficient visual generation trading off sequence length and bits per token using patch size, similar to prior practices (Rom6 Autoregressive Image Generation with Masked Bit Modeling Table 2. Ablation studies on BAR design. The rows labeled with gray color indicate our choices for final models. (a) Impact of masking strategy during training. BAR demonstrates robustness across different masking strategies. bits masking strategy 10 12 16 arccos uniform logit-normal arccos uniform logit-normal arccos uniform logit-normal arccos uniform logit-normal w/o CFG w/ CFG FID 4.02 3.04 3.10 2.33 2.11 2.10 1.89 1.78 1.68 2.66 2.48 2.37 IS 162.6 183.4 180.6 192.5 206.4 207.9 218.1 225.4 231.6 192.0 209.0 197.8 FID 1.88 1.47 1.48 1.38 1.27 1.25 1.46 1.22 1.19 1.48 1.38 1. IS 229.3 275.8 271.2 295.9 268.7 268.3 311.8 293.7 282.3 287.8 281.9 292.1 (b) Scaling codebook size with different head sizes. Increasing the head size improves performance, particularly for larger vocabularies. However, these benefits offer diminishing returns when ClassifierFree Guidance (CFG) is applied. bits 10 12 32 head size (#layers width) 3 1024 3 1536 3 2048 6 2048 3 1024 3 1536 3 2048 6 2048 3 1024 3 1536 3 2048 6 2048 3 1024 3 1536 3 2048 6 2048 w/o CFG w/ CFG FID 3.29 3.14 3.10 2.87 2.16 2.07 2.10 2.02 1.85 1.79 1.68 1.63 2.68 2.44 2.37 2.10 IS 176.0 179.9 180.6 187.8 206.2 195.3 207.9 198.2 216.3 222.5 231.6 233.8 198.7 205.8 197.8 210. FID 1.52 1.51 1.48 1.48 1.26 1.25 1.25 1.27 1.21 1.20 1.19 1.19 1.45 1.41 1.37 1.31 IS 284.0 260.7 257.9 265.4 286.3 280.1 268.3 279.5 304.0 291.2 282.3 293.2 282.9 278.6 292.1 290.1 (c) Impact of sampling strategy. More steps advances results, while back-loading schedule further improves with CFG. bits bits unmasking schedule [8, 8] [5, 5, 6] [4, 4, 4, 4] [3, 3, 3, 3, 4] [2, 2, 3, 3, 3, 3] [2, 2, 5, 7] w/o CFG w/ CFG FID 3.73 1.95 1.68 1.64 1.64 1.81 IS 191.1 218.2 231.6 235.8 230.0 214.2 FID 1.55 1.20 1.19 1.23 1.22 1. IS 272.5 291.3 282.3 301.1 307.2 289.2 bach et al., 2022; Peebles & Xie, 2023; Ma et al., 2025b). By shuffling from tokens to bits (for example, flattening and concatenating the bits of neighboring tokens), the effective token sequence length can be significantly reduced, enabling more efficient generation. As shown in Tab. 2d, BAR provides flexible mechanism to trade off generation quality and computational cost by balancing computation between the autoregressive transformer and the masked bit modeling head. Specifically, BAR-B can downsample the latent space by 2 (named BAR-B/2 with patch size 2), resulting in 4 fewer tokens, while incurring only modest degradation in performance (from 1.68 to 2.24 without CFG and from 1.19 to 1.35 with CFG). Consequently, sampling throughput increases substantially, from 24.9 images per second to 150.3 images per second. More aggressive downsampling leads to BAR-B/4 (patch size 4), which further improves the sampling speed to 445.5 images per second. 4.3. Main Results We report BAR results against state-of-the-art methods on the ImageNet-1K benchmarks at resolutions 256 256. For all results reported, we use the official ADM scripts (Dhariwal & Nichol, 2021) to ensure fair comparison. ImageNet 256256. We summarize the results in Tab. 3. We observe that BAR-B, despite having only 415M parameters, achieves substantially better performance than prior state-of-the-art discrete generation methods. In particular, (d) Efficient BAR. Sampling are based on uniform schedules with 4 bits unmasking steps per token for all methods. BAR enables better accuracy-cost trade-off. patch size BAR-B BAR-B/2 BAR-B/4 token 256 64 16 bits per token 16 64 256 w/o CFG FID 1.68 2.24 3.50 IS 231.6 217.3 212. FID 1.19 1.35 2.34 w/ CFG IS 282.3 293.4 274.7 images / sec 24.9 150.3 445.5 BAR-B uses only one quarter of the model size of RAR (415M vs. 1.5B), yet attains significantly higher generation quality (gFID 1.13 vs. 1.48). It also outperforms other discrete approaches by clear margin, including VAR (1.13 vs. 1.92) and LlamaGen (1.13 vs. 2.18). Moreover, BAR-B already surpasses state-of-the-art diffusion models based on continuous pipelines. Specifically, BAR-B outperforms xAR, which is approximately 3 larger in model size, achieving gFID of 1.13 compared to 1.24. Despite its compact size, BAR-B exceeds the performance of several strong diffusion-based models, including DDT (1.13 vs. 1.26), VA-VAE (1.13 vs. 1.35), and MAR (1.13 vs. 1.55). Compared to the concurrent work RAE, the two methods achieve comparable performance at gFID 1.13. Scaling BAR-B to larger model yields BAR-L, which further improves performance and significantly outperforms all prior methods, both discrete and continuous, achieving new state-of-the-art gFID of 0.99. Notably, BAR-L not only sets new record under classifier-free guidance (0.99 vs. 1.13 for RAE), but also establishes new best result without guidance (1.42 vs. 1.51 for RAE). Sampling Speed. We compare BAR with state-of-the-art methods in terms of sampling speed in Tab. 4. Notably, the efficient variants of BAR achieve an excellent trade-off between generation quality and sampling efficiency. BAR-B/2, with gFID of 1.35, not only outperforms all efficient generation methods such as PAR (gFID 2.29) and VAR (gFID 7 Autoregressive Image Generation with Masked Bit Modeling Table 3. ImageNet-1K 256 256 generation results. We report metrics with and without classifier-free guidance. BAR only adopts simple linear guidance schedule, with no need for auto-guidance (Karras et al., 2024) from an external model that is used by other state-of-the-art methods (Zheng et al., 2025b). epochs #params FID generation@256 w/o guidance generation@256 w/ guidance Prec. Rec. Prec. Rec. FID IS IS method pixel space ADM (Dhariwal & Nichol, 2021) JiT (Li & He, 2025) SiD2 (Hoogeboom et al., 2024) continuous tokens DiT (Peebles & Xie, 2023) SiT (Ma et al., 2024) DiMR (Liu et al., 2024) FlowAR (Ren et al., 2024) MDTv2 (Gao et al., 2023) MAR (Li et al., 2024) VA-VAE (Yao et al., 2025) REPA (Yu et al., 2025b) DDT (Wang et al., 2025a) xAR (Ren et al., 2025) RAE (Zheng et al., 2025b) 350 600 1280 1400 1400 800 400 1080 800 80 800 80 800 80 400 800 80 800 discrete tokens MaskGIT (Chang et al., 2022) 300 Open-MAGVIT2 (Luo et al., 2024) 350 300 LlamaGen (Sun et al., 2024) 800 TiTok (Yu et al., 2024b) 350 VAR (Tian et al., 2024) 1080 MAGVIT-v2 (Yu et al., 2024a) 1080 MaskBit (Weber et al., 2024) RAR (Yu et al., 2025a) 400 400 BAR-B (ours) 80 BAR-L (ours) 554M 10.94 101.0 - - 2B - - - 675M 675M 9.62 675M 8.61 3.56 1.1B - 1.9B 676M - 943M 2.35 4.29 2.17 7.90 5.78 6.62 6.27 - 2.16 1. 675M 675M 839M 1.1B - 177M 6.18 - 1.5B 3.1B 9.38 287M 4.44 2.0B 307M 3.65 305M 1.5B 415M 1.64 1.71 1. 1.1B - - 121.5 131.7 - - - 227.8 - 205.6 122.6 158.3 135.2 154.7 - 214.8 242.9 182.1 - 112.9 168.2 - 200.5 - - 230.4 224.3 236.2 0.69 - - 0.67 0.68 - - - 0.79 - 0.77 0.70 0.70 0.69 0.68 - 0.82 0. - - 0.69 - - - - - 0.80 0.80 0.79 0.63 - - 0.67 0.67 - - - 0.62 - 0.65 0.65 0.68 0.67 0.69 - 0.59 0.63 - - 0.67 - - - - - 0.62 0.63 0.65 3.94 1.82 1.38 2.27 2.06 1.63 1.65 1.58 1.55 - 1.35 - 1.29 1.52 1.26 1.24 - 1. - 2.33 2.18 1.97 1.92 1.78 1.52 1.48 1.13 1.15 0.99 215.8 292.6 - 278.2 270.3 292.5 296.5 314.7 303.7 - 295.3 - 306.3 263.7 310.6 301.6 - 262.6 - 271.8 263.3 281.8 323.1 319.4 328.6 326.0 289.0 288.7 296.9 0.83 0.79 - 0.83 0.82 0.79 0.83 0.79 0.81 - 0.79 - 0.79 0.78 0.79 0.83 - 0. - 0.84 0.81 - 0.82 - - 0.80 0.77 0.77 0.77 0.53 0.62 - 0.57 0.59 0.63 0.60 0.65 0.62 - 0.65 - 0.64 0.63 0.65 0.64 - 0.67 - 0.54 0.58 - 0.59 - - 0.63 0.66 0.66 0.69 Table 4. Sampling throughput (including de-tokenization process). All are benchmarked using single H200, with float32 precision. BAR only uses KV-cache without further optimization. method PAR-4 (Wang et al., 2025c) VAR (Tian et al., 2024) MeanFlow (Geng et al., 2025) BAR-B/4 (ours) BAR-B/2 (ours) MAR (Li et al., 2024) VA-VAE (Yao et al., 2025) DDT (Wang et al., 2025a) xAR (Ren et al., 2025) RAE (Zheng et al., 2025b) BAR-B (ours) BAR-L (ours) #params FID images / sec 2.29 3.1B 2.0B 1.92 676M 2.20 416M 2.34 415M 1.35 943M 1.55 675M 1.35 675M 1.26 1.24 1.1B 839M 1.13 415M 1.13 0.99 1.1B 4.92 8.08 151.48 445.48 150.52 1.19 1.51 1.62 2.03 6.62 24.33 10.65 1.92), but also achieves substantially faster sampling speeds, with 30.59 and 18.64 speedups over PAR and VAR, respectively. Even when compared to single-step diffusion models such as MeanFlow, BAR-B/2 demonstrates superior generation quality (gFID 1.35 vs. 2.20) at comparable sampling speed (150.52 vs. 151.48 images per second). The most efficient variant, BAR-B/4, achieves generation quality comparable to MeanFlow (gFID 2.34 vs. 2.20), while producing samples 2.94 faster. In more performance-oriented comparisons, BAR-B achieves state-of-the-art visual quality while being 20.45, 16.11, 15.02, 11.99, and 3.68 faster than MAR, VAVAE, DDT, xAR, and RAE, respectively. Notably, the bestperforming variant BAR-L not only sets new state-ofthe-art record with gFID 0.99, but also maintains clear advantage in sampling speed, achieving 8.95 speedup over MAR, 5.25 over xAR, and 1.61 over RAE. 5. Conclusion In this paper, we presented unified and fair comparison between discrete and continuous visual tokenizers. We showed that differences in compression ratio, as measured by the number of bits allocated to the latent space, constitute dominant factor underlying the observed performance differences between discrete and continuous tokenizers. When operating under comparable bit budgets, discrete tokenizers can match or even outperform their continuous counterparts. Building on this analysis, we introduced novel generative prediction head that models discrete tokens by generating their bit representations. This design enables efficient and effective discrete generation with arbitrarily large vocabularies, overcoming key limitation of prior discrete generative models. As result, the proposed masked bit autoregressive modeling framework establishes new state of the art, substantially outperforming both existing discrete methods and strong continuous baselines. 8 Autoregressive Image Generation with Masked Bit Modeling Impact Statement. This work advances the field of visual generation by demonstrating that discrete tokenizers can match or surpass continuous approaches when given sufficient information capacity, while achieving faster sampling speeds and more efficient training. By making high-quality image generation more computationally accessible through such methods, this research could democratize generative AI for researchers with limited resources and reduce the environmental impact of large-scale generation tasks. However, the improved quality and efficiency of these models also amplify concerns around potential misuse for creating deepfakes, spreading misinformation, or generating harmful content at scale. These advances underscore the critical importance of developing robust detection methods, implementing responsible access controls, and establishing clear ethical guidelines for deployment."
        },
        {
            "title": "References",
            "content": "Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In CVPR, 2023. Besnier, V. and Chen, M. pytorch reproduction of masked generative image transformer. arXiv preprint arXiv:2310.14400, 2023. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In CVPR, 2022. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K., Freeman, W. T., Rubinstein, M., et al. Muse: Text-to-image generation via masked generative transformers. In ICML, 2023. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS, 2021. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked diffusion transformer is strong image synthesizer. In ICCV, 2023. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. NeurIPS, 2014. Han, J., Liu, J., Jiang, Y., Yan, B., Zhang, Y., Yuan, Z., Peng, B., and Liu, X. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In CVPR, 2025. He, J., Yu, Q., Liu, Q., and Chen, L.-C. Flowtok: Flowing seamlessly across text and image tokens. arXiv preprint arXiv:2503.10772, 2025. Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. science, 2006. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 2020. Hoogeboom, E., Mensink, T., Heek, J., Lamerigts, K., Gao, R., and Salimans, T. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. Karras, T., Aittala, M., Kynkäänniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. NeurIPS, 2024. 9 Autoregressive Image Generation with Masked Bit Modeling Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In ICLR, 2014. Li, T. and He, K. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. NeurIPS, 2024. Li, T., Sun, Q., Fan, L., and He, K. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, Q., Zeng, Z., He, J., Yu, Q., Shen, X., and Chen, L.- C. Alleviating distortion in image generation via multiresolution diffusion models. NeurIPS, 2024. Liu, Q., Yin, X., Yuille, A., Brown, A., and Singh, M. Flowing from words to pixels: noise-free framework for cross-modality evolution. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 27552765, 2025. Lu, J., Song, L., Xu, M., Ahn, B., Wang, Y., Chen, C., Dehghan, A., and Yang, Y. Atoken: unified tokenizer for vision. arXiv preprint arXiv:2509.14476, 2025. Luo, Z., Shi, F., Ge, Y., Yang, Y., Wang, L., and Shan, Y. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Ma, C., Jiang, Y., Wu, J., Yang, J., Yu, X., Yuan, Z., Peng, B., and Qi, X. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025a. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In ECCV, 2024. Ma, X., Sun, P., Ma, H., Tang, H., Ma, C.-Y., Wang, J., Li, K., Dai, X., Shi, Y., Ju, X., et al. Token-shuffle: Towards high-resolution image generation with autoregressive models. arXiv preprint arXiv:2504.17789, 2025b. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: Vq-vae made simple. In ICLR, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Qu, L., Zhang, H., Liu, Y., Wang, X., Jiang, Y., Gao, Y., Ye, H., Du, D. K., Yuan, Z., and Wu, X. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In CVPR, 2025. Ren, S., Yu, Q., He, J., Shen, X., Yuille, A., and Chen, L.- C. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. Ren, S., Yu, Q., He, J., Shen, X., Yuille, A., and Chen, L.-C. Beyond next-token: Next-x prediction for autoregressive visual generation. ICCV, 2025. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Sauer, A., Karras, T., Laine, S., Geiger, A., and Aila, T. Stylegan-t: Unlocking the power of gans for fast largescale text-to-image synthesis. In ICML, 2023. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 2024. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. NeurIPS, 2017. 10 Autoregressive Image Generation with Masked Bit Modeling Wang, S., Tian, Z., Huang, W., and Wang, L. Ddt: arXiv preprint Decoupled diffusion transformer. arXiv:2504.05741, 2025a. Zhao, Y., Xiong, Y., and Krähenbühl, P. Image and video tokenization with binary spherical quantization. ICLR, 2025. Zheng, A., Wen, X., Zhang, X., Ma, C., Wang, T., Yu, G., Zhang, X., and Qi, X. Vision foundation models as effective visual tokenizers for autoregressive image generation. NeurIPS, 2025a. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025b. Zheng, C. and Vedaldi, A. Online clustered codebook. In ICCV, 2023. Zhu, L., Wei, F., Lu, Y., and Chen, D. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. NeurIPS, 2024. Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., and Huang, T. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Wang, Y., Lin, Z., Teng, Y., Zhu, Y., Ren, S., Feng, J., and Liu, X. Bridging continuous and discrete tokens arXiv preprint for autoregressive visual generation. arXiv:2503.16430, 2025b. Wang, Y., Ren, S., Lin, Z., Han, Y., Guo, H., Yang, Z., Zou, D., Feng, J., and Liu, X. Parallelized autoregressive visual generation. In CVPR, 2025c. Weber, M., Yu, L., Yu, Q., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. Maskbit: Embedding-free image generation via bit tokens. TMLR, 2024. Wiedemer, T., Li, Y., Vicol, P., Gu, S. S., Matarese, N., Swersky, K., Kim, B., Jaini, P., and Geirhos, R. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Yao, J., Yang, B., and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion tokenizer is key to visual generation. In ICLR, 2024a. Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024b. Yu, Q., He, J., Deng, X., Shen, X., and Chen, L.-C. Randomized autoregressive visual generation. In ICCV, 2025a. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. ICLR, 2025b. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In CVPR, 2022. Zhang, B. and Sennrich, R. Root mean square layer normalization. NeurIPS, 2019. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 11 Autoregressive Image Generation with Masked Bit Modeling A. Appendix Table 7. Detailed hyper-parameters for final BAR models. config value training hyper-params optimizer learning rate weight decay optimizer momentum total batch size learning rate schedule ending learning rate total epochs warmup epochs annealing start epoch annealing end epoch precision max grad norm dropout rate attn dropout rate class label dropout rate training GPUs training time AdamW 4e-4 0.03 (0.9, 0.96) 2048 cosine decay 1e-5 400 100 200 300 bfloat16 1.0 0.0 (B) / 0.2 (L) 0.0 (B) / 0.2 (L) 0.1 16 H200 (B) / 32 H200 (L) 20 hrs (B) / 32 hrs (L) guidance schedule temperature guidance scale bits unmasking schedule sampling hyper-params w/ CFG linear 2.5 (B) / 3.0 (L) 5.0 (B) / 5.3 (L) [2,2,5,7] sampling hyper-params w/o CFG temperature bits unmasking schedule 2.0 (B) / 2.4 (L) [4,4,4,4] Table 8. ImageNet-1K 512 512 generation results. We report metrics with classifier-free guidance. BAR only adopts simple linear guidance schedule, with no need for auto-guidance (Karras et al., 2024) from an external model that is used by other stateof-the-art methods (Zheng et al., 2025b). Due to computational constraints, we only train the model for 200 epochs. #params FID IS method 227M 26.52 66.8 VQGAN (Esser et al., 2021) MaskGiT (Chang et al., 2022) 227M 7.32 156.0 675M 3.04 240.8 DiT (Peebles & Xie, 2023) DiMR (Liu et al., 2024) 525M 2.89 289.8 2.3B 2.63 303.2 VAR (Tian et al., 2024) 675M 2.08 274.6 REPA (Yu et al., 2025b) 608M 1.70 281.5 xAR (Ren et al., 2025) 1.5B 1.66 295.7 RAR (Yu et al., 2025a) 675M 1.28 305.1 DDT (Wang et al., 2025a) 839M 1.13 259.6 RAE (Zheng et al., 2025b) 1.1B 1.09 311.1 BAR-L (ours) D. Visualization on Generated Samples We provide visualization results in Fig. 7, Fig. 8, Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, and Fig. 18. The supplementary material includes the following additional information: Sec. provides the detailed hyper-parameters for the final BAR-FSQ and BAR models. Sec. provides additional experimental results including BARs results on the ImageNet-512 benchmark. Sec. provides more visualization samples of BAR models. B. Hyper-parameters for Final BAR Models Table 5. Architecture configurations of BAR. We follow prior works scaling up ViT (Dosovitskiy et al., 2021; Zhai et al., 2022) for different configurations. model BAR-B BAR-L depth width mlp 3072 768 5120 1280 24 32 heads 12 16 #params 415M 1108M Table 6. Detailed hyper-parameters for final BAR-FSQ models. config value training hyper-params optimizer learning rate weight decay optimizer momentum total batch size learning rate schedule ending learning rate total epochs warmup epochs precision max grad norm perceptual loss weight clip l2 loss weight gram loss weight discriminator loss weight discriminator kicks in epoch reconstruction loss weight training GPUs training time AdamW 1e-4 0.0 (0.9, 0.999) 256 cosine decay 1e-5 40 2 bfloat16 1.0 1.0 0.7 10.0 0.05 20 1.0 8 H200 13 hrs The BAR model configuration is detailed in Tab. 5. We list the detailed training hyper-parameters and sampling hyper-parameters for all BAR-FSQ, BAR models in Tab. 6 and Tab. 7, resptively. C. More Experimental Results We provide additional experimental results on ImageNet512 in Tab. 8, where BAR demonstrates clear advantages over other methods. 12 Autoregressive Image Generation with Masked Bit Modeling Figure 7. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 1: goldfish, Carassius auratus. Figure 8. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 33: loggerhead, loggerhead turtle, Caretta caretta. Figure 9. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 90: lorikeet. 13 Autoregressive Image Generation with Masked Bit Modeling Figure 10. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 107: jellyfish. Figure 11. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 207: golden retriever. Figure 12. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 250: Siberian husky. 14 Autoregressive Image Generation with Masked Bit Modeling Figure 13. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 417: balloon. Figure 14. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 562: fountain. Figure 15. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 928: ice cream, icecream. 15 Autoregressive Image Generation with Masked Bit Modeling Figure 16. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 933: cheeseburger. Figure 17. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 971: bubble. Figure 18. Visualization samples from BAR models. BAR is capable of generating high-fidelity image samples with great diversity. class idx 980: volcano."
        }
    ],
    "affiliations": [
        "Amazon FAR (Frontier AI & Robotics)"
    ]
}