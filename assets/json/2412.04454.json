{
    "paper_title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
    "authors": [
        "Yiheng Xu",
        "Zekun Wang",
        "Junli Wang",
        "Dunjie Lu",
        "Tianbao Xie",
        "Amrita Saha",
        "Doyen Sahoo",
        "Tao Yu",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 4 5 4 4 0 . 2 1 4 2 : r AGUVIS: UNIFIED PURE VISION AGENTS FOR AUTONOMOUS GUI INTERACTION Yiheng Xu Zekun Wang Tianbao Xie Amrita Saha Doyen Sahoo Tao Yu Caiming Xiong University of Hong Kong Salesforce Research {yhxu,tyu}@cs.hku.hk cxiong@salesforce.com https://aguvis-project.github.io Junli Wang Dunjie Lu"
        },
        {
            "title": "ABSTRACT",
            "content": "Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce AGUVIS, unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct largescale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that AGUVIS surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We will open-source all datasets, models, and training recipes to facilitate future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "Graphical User Interfaces (GUIs) are cornerstone of human-computer interaction, providing structured yet intuitive platform for users to accomplish tasks across various digital environments: website, desktop, and mobile devices (Deng et al., 2023; Zhou et al., 2024; Xie et al., 2024; Rawles et al., 2024b). Automating GUI operations through autonomous agents can revolutionize productivity by enabling seamless task execution on various applications using existing human-centric tools. Moreover, this approach lays the groundwork for advanced AI systems that can interact with and learn from rich digital environments in ways that mirror human behavior. To effectively perform GUI tasks autonomously, GUI agent requires three core competencies: understanding, grounding, and planning & reasoning. For GUI understanding, the agent must first comprehend high-resolution and complex interfaces designed for human users, enabling it to grasp the context and perform subsequent reasoning tasks. GUI grounding involves mapping natural language instructions to visual observations of the interface. For planning and reasoning, the agent must synthesize and analyze the current multimodal observations of the environment with previous observations and action histories, enabling it to generate coherent and effective next steps to ultimately achieve the task goal. Although recent advances in large vision-language models (LVLMs) (OpenAI, 2024; Reid et al., 2024; Li et al., 2024a; Wang et al., 2024a) have significantly enhanced the ability of AI systems to interpret complex visual interfaces, there remain critical challenges in grounding and reasoning specifically tailored for GUI tasks. We identify three primary challenges that must be addressed to advance the capabilities of GUI agents: Equal contribution Corresponding authors. Work was partially done during YXs internship at Salesforce. 1 Enhancing Pure Vision Framework. Previous approaches (Gur et al., 2024; Kim et al., 2023; Deng et al., 2023; Zhou et al., 2024; Xie et al., 2024) predominantly focus on mapping natural language instructions to textual representations of GUIs, such as HTML or accessibility trees. This method presents several limitations. Firstly, GUIs are inherently visual, and leveraging image-based representations aligns more closely with human cognitive processes. Secondly, textual representations can vary widely across different environments, complicating the generalization of the model and limiting the availability of consistent training data. Finally, these textual representations are often verbose and complex, leading to increased inference times compared to more compact image encodings (Figure 2). By unifying observations across platforms as images and grounding instructions to image coordinates, GUI agents can generalize more effectively across diverse environments. Unification Across GUI Environments. The action spaces and control APIs for GUI interactions vary significantly across diverse environments, particularly when the observations are textual. Even within the same platform, the action space can differ greatly. This heterogeneity limits the amount of training data available for each environment, impeding the development of model that can generalize effectively across different platforms and scale further. unified action space that abstracts these environmental differences is crucial for creating robust and adaptable GUI agents. Previous work (Chen et al., 2024b; Zeng et al., 2024) has attempted to unify digital agent data across diverse environments, such as combining GUI, game, and CLI interfaces for joint training. However, these interfaces do not share the same interaction logic. In contrast, GUIs on desktop, web, and mobile platforms naturally share similar human-computer interaction (HCI) logic. This commonality facilitates their unification, enabling consistent visual observations and action spaces that mutually benefit both visual grounding and reasoning. Integrating Planning and Reasoning with Grounding. Current methodologies (Zheng et al., 2024a) often depend on the reasoning capabilities of closed-source large language models (LLMs) (OpenAI, 2024) to plan the completion of GUI tasks or, alternatively, train agents to make direct action decisions through grounding without an explicit reasoning process. This dichotomy results in either lack of grounding abilities or lack of comprehensive reasoning abilities. Recently, some works (Gou et al., 2024; Lu et al., 2024) attempt to use closed-source LLMs with specialized GUI grounding models together and communicate with natural language instruction to utilize both abilities. However, on the one hand, natural language communication between the two models usually results in information loss. On the other hand, most importantly, this approach is not further scalable to solve GUI interaction since grounding has been improved close to the upper bound with data synthesis, and most remaining problems are planning related. However, the GUI planning and reasoning ability of closed-source LLMs cannot be further improved. To address these challenges, we introduce unified framework for GUI agents that harmonizes pure vision observation and consistent action spaces across diverse environments. Our approach leverages vision-based grounding to improve generalization and reduce inference costs while employing standardized action space with plugin system to facilitate consistent learning and interaction across various platforms. After unified GUI grounding training stage, we demonstrate that unified augmented datasets can effectively build model capable of executing complex GUI grounding instructions on various platforms. In addition, we integrate explicit visual planning and reasoning into the same model, enabling autonomous navigation and interaction within complex digital environments. Since existing GUI agent trajectories do not fully support these demands, we have unified the existing planning datasets on different platforms and constructed large-scale, pure vision, crossplatform, multi-step dataset of agent trajectories, featuring comprehensive multimodal reasoning and grounding. Through extensive experiments across various scenarios, we demonstrate the effectiveness of our approach in advancing the state-of-the-art for pure vision-based autonomous GUI agents. To our knowledge, this is the first model that can autonomously complete tasks in real-world online environments without relying on higher reasoning abilities from closed-source models. Our contributions are as follows: We introduce unified pure vision framework for building generalizable GUI agents that operate with vision-based observations and plugin-enabled action system, enhancing cross-platform adaptability. We develop comprehensive data pipeline that unifies existing GUI grounding annotations and integrates explicit planning and reasoning. This enables the construction of large-scale datasets for grounding and multi-step agent trajectory datasets across platforms. Starting with VLM, we present two-stage training processfirst for GUI grounding, followed by planning and reasoningresulting in AGUVIS, the first cross-platform autonomous GUI agent capable of performing complex tasks independently without relying on closed-source models. All data, models, and training resources will be open-sourced."
        },
        {
            "title": "2.1 PROBLEM FORMULATION",
            "content": "We model the autonomous GUI agents interaction with the environment as Partially Observable Markov Decision Process (POMDP), characterized by the tuple (S, A, O, T, O). In this formulation, represents the set of possible states of the environment, denotes the set of actions the agent can take, and refers to the set of observations the agent can receive. The state transition function, : [0, 1], defines the probability of transitioning from one state to another given an action, while the observation function, : AO [0, 1], specifies the probability of receiving particular observation given state and an action. At each time step t, the agent receives an image observation ot from the GUI environment, reasons and generates an inner monologue (Huang et al., 2022) based on its previous actions and observations. This inner monologue consists of three components: natural language description of the current observation (dt), internal reasoning (ht) based on the high-level goal G, the observation description dt, and previous thoughts ht1, and finally, low-level action instruction (ainstr ) in natural language that specifies the next action. The agent then executes the action at based on the instruction ainstr , receives new observation ot+1, and repeats this process until it either achieves the goal or reaches terminal state. 2.2 UNIFIED PURE VISION FRAMEWORK Figure 1: Overview of the two-stage training paradigm for autonomous GUI agents. In this work, we propose to unify observation and action space via pure vision and pyautogui commands with pluggable action system  (Table 9)  . For observation, pure vision does not require the model to understand different UI source codes of the interfaces of different platforms, such as HTML of the webpage, and accessibility tree of desktop and mobile operating systems, which can help improve the generalization. Meanwhile, pure vision can reduce the input token length. Generally, the input length of accessibility tree observation is 6k tokens (Xie et al., 2024), and HTML is 4k tokens (Figure 2), depending on the complexity of the interface. Compared with relatively long input, the token cost of image observation does not vary across different interfaces but only depends on model design, which in our case is 1200 tokens for 720p image observation. For unified action space, we choose the widely used standard pyautogui action space with pluggable action system. This library leverages the high-level programming language Python to replicate and replay various human inputs into computers through code, allowing us to construct universal and complete representation of actions. We show the action space in Table 9. We use pyautogui commands to unify basic GUI operations of all platforms including web, desktop, and 3 mobile. Over this action space, an agent model can then learn to generate actions in order to control GUI without any action space description. While mouse and keyboard inputs form the core of GUI interactions, they are not comprehensive. Certain platforms require additional actions. For example: (1) specific actions on mobile platforms such as swiping; (2) shortcuts that efficiently perform series of actions like opening apps; (3) communication actions such as providing answers or terminating after completion. To address these extended requirements, we introduce pluggable action system. This system allows us to expand the action space by aligning new actions with the existing pyautogui commands where possible. For actions that cannot be directly mapped, the pluggable system provides the flexibility to incorporate them with detailed action descriptions. This enables the model to generalize effectively to environments where new actions are introduced. By combining pure vision observations with unified action space and flexible pluggable system, our framework enables the training of single model that can operate across diverse platforms. This setup not only simplifies the training process but also ensures the model can generalize and adapt to novel environments and tasks."
        },
        {
            "title": "2.3 THE AGUVIS COLLECTION",
            "content": "GUI agent trajectories are low-resource data source compared with its challenges. This is because the observation and action space vary across different environments even on the same platform. Fortunately, GUI environments share the same operation logic and similar action space. We can efficiently unify existing data to scale the training set. Therefore, we propose THE AGUVIS COLLECTION, large-scale GUI agent training dataset collected and augmented with existing GUI agent data. This data collection consists of two splits: grounding split  (Table 10)  and planning & reasoning split  (Table 11)  , corresponding to the two important GUI abilities. Template-augmented Grounding Data. Vision-based grounding requires the model to ground the natural language intent to the image observation with coordinates. On one hand, there are several previous works that have built datasets on different platforms, including natural language instructions and corresponding target elements. We collected and unified them into pyautogui commands format. On the other hand, we found that there are many datasets proposed for user interfaces on different platforms that contain large amount of metadata, including the positions of all text/icons/widgets in the current interface. Using this type of data we constructed templates for pyautogui actions. We randomly generated grounding data pairs through these templates to train models to ground these elements based on images. This operation greatly expanded the data scale. VLM-augmented Planning & Reasoning Trajectories. High-quality GUI agent trajectories contain several key components: high-level goal, sequence of interleaved observations, natural language reasoning, and grounded actions. Existing approaches typically rely on human annotation to collect these trajectories (Deng et al., 2023; Rawles et al., 2024b; Li et al., 2024c). Most of the agent trajectory data contains high-level goals, observations, and grounded actions. However, the intermediate reasoning process and low-level action instructions are not included. This makes it difficult for existing data to train agents to perform chain-of-thought or inner monologue reasoning to help the model plan the next action, resulting in poor agent performance. To augment the agent trajectories with detailed reasoning and low-level action instructions, we employ vision-language model (VLM) to generate the inner monologue for each step in the trajectory. Specifically, for each time step t, given the high-level goal G, the current image observation ot, and the grounded action at, we prompt the VLM to produce the inner monologue components: observation description dt, thoughts ht, and low-level action instruction ainstr . To assist the VLM in generating accurate and contextually relevant monologues, we highlight the target element associated with the grounded action at on the image observation ot. This visual cue helps the model focus on the relevant part of the interface. Additionally, we include the previous low-level action instructions ainstr , ainstr t1 to provide the VLM with the action history, ensuring continuity 2 and coherence in the generated reasoning. , . . . , ainstr The prompting strategy is carefully crafted to guide the VLM in generating inner monologues that are predictive and goal-oriented, without relying on hindsight or revealing future actions. By simulating the agents thought process in first-person perspective, we encourage the generation of 4 actionable instructions that align with the high-level goal and current observation. This approach results in large-scale dataset of agent trajectories enriched with detailed reasoning and instructions."
        },
        {
            "title": "2.4 MODEL ARCHITECTURE",
            "content": "Unlike grounding agents that rely on structured UI representations (such as accessibility trees) as their textual input, vision-based grounding requires the model to map intents directly to visual observations. This means the model needs to encode high-resolution images while preserving their original aspect ratios. Recent advances in VLMs have made these capabilities possible. We choose Qwen2-VL (Wang et al., 2024b) as our starting VLM. It uses NaViT as an image encoder with native dynamic resolution support (Dehghani et al., 2023). Unlike its predecessor, Qwen2-VL can now process images of any resolution, dynamically converting them into variable number of visual tokens. To support this feature, ViT is modified by removing the original absolute position embeddings and introducing 2D-RoPE (Su et al., 2024) to capture the two-dimensional positional information of images. Based on these unique features, Qwen2-VL is highly suitable for GUI agents needs. It can encode high-resolution images of any ratio with relatively fewer image token costs. Therefore, we chose Qwen2-VL as our starting VLM to build our GUI agent. LLaVA-OneVision (Li et al., 2024a) is another suitable VLM as it also supports high-resolution any ratio image encoding, although its image token cost is relatively higher than Qwen2-VL. We also apply our data recipe and training strategy to LLaVA and show that our framework is modelindependent and generally works for high-resolution VLMs details are shown in Section 4.2.. 2.5 TRAINING PARADIGM We begin with Vision-Language Model (VLM) that possesses advanced image understanding capabilities, and the training process is divided into two main stages: Grounding Training and Planning & Reasoning Training. Each stage utilizes distinct data split from our THE AGUVIS COLLECTION to progressively enhance the VLMs abilities. Stage 1: Grounding Training In this stage, we focus on enabling the model to understand and interact with objects within single GUI screenshot. GUI environments typically feature multiple interactable objects within single screenshot, generating large volume of grounding data but leading to shorter, less diverse interaction sequences, which can limit training efficiency. We train our model with grounding packing strategy where multiple instruction-action pairs are bundled into single image, resulting in single-image-multiple-turn format. This technique allows the model to process several grounding examples from one screenshot, reducing redundant training overhead while retaining high level of grounding performance. This approach significantly accelerates training by maximizing the use of each image without compromising accuracy. To equip our model with the capability for GUI understanding and grounding, which serves as the foundation for subsequent planning and reasoning, we conducted this training stage. Upon completing Stage 1 training, the model is referred to as AGUVIS-G. Stage 2: Planning & Reasoning Training Building on the foundation of AGUVIS-G, the second stage introduces more complex decision-making and reasoning processes. This phase is designed to teach the model how to execute multi-step tasks by reasoning through agent trajectories that vary in complexity and environments, encompassing diverse reasoning modes. Thanks to our detailed inner monologue trajectory data, we implement reasoning mixture approach, where the model is exposed to various levels of cognitive complexity, from straightforward low-level action instructions to full inner monologues that include observation descriptions, thoughts, and detailed action plans. By dynamically adjusting the complexity of these trajectories, we train the model to be adaptable, fostering step-by-step reasoning and high-level decision-making abilities. This diversity in reasoning ensures that the model can handle wide range of tasks with nuanced understanding and precision. After this stage, the fully trained model is called AGUVIS, which can be employed in both offline and online GUI tasks across diverse environments."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "To evaluate the effectiveness of GUI agent models on various platforms, we conduct experiments on several GUI benchmarks: GUI Grounding Evaluation and Offline/Online GUI Agent Evaluation."
        },
        {
            "title": "3.1 GUI GROUNDING EVALUATION",
            "content": "Table 1: Comparison of various planners and grounding methods on ScreenSpot across various device and input modalities. The top part of table shows the results on original instructions evaluation setting while the bottom part shows results on self-plan evaluation setting. Best results are in bold. Planner Grounder Mobile Desktop Web Text Icon/Widget Text Icon/Widget Text Icon/Widget - 22.6 GPT-4 20.2 GPT-4o 67.0 CogAgent 78.0 SeeClick 75.5 Qwen2-VL UGround 82.8 AGUVIS-G-7B 88.3 GPTGPT-4o SeeClick OmniParser UGround SeeClick UGround AGUVIS-7B AGUVIS-72B 76.6 93.9 90.1 81.0 93. 95.6 94.5 24.5 24.9 24.0 52.0 60.7 60.3 78.2 55.5 57.0 70.3 59.8 76.9 77.7 85.2 20.2 21.1 74.2 72.2 76.3 82.5 88. 68.0 91.3 87.1 69.6 92.8 93.8 95.4 11.8 23.6 20.0 30.0 54.3 63.6 70.7 28.6 63.6 55.7 33.6 67. 67.1 77.9 9.2 12.2 70.4 55.7 35.2 80.4 85.7 40.9 81.3 85.7 43.9 88.7 88.3 91.3 8.8 7.8 28.6 32.5 25.7 70.4 74. 23.3 51.0 64.6 26.2 68.9 75.2 85.9 Avg 16.2 18.3 47.4 53.4 55.3 73.3 81.8 48.8 73.0 75. 52.3 81.4 84.4 89.2 ScreenSpot. We first assess the performance of GUI grounding, which is foundational capability of GUI agent models. Following previous work (Cheng et al., 2024; Gou et al., 2024), we evaluate models on ScreenSpot (Cheng et al., 2024). This dataset encompasses variety of grounding instructions tailored for mobile, desktop, and website platforms, and is assessed under two distinct settings: (1) Original Instructions: models perform grounding actions directly following the original instructions; and (2) Self-plan: models are required to generate plans in natural language based on the original instructions before executing grounding actions. The performance illustrated in Table 1 demonstrates that AGUVIS exhibits impressive GUI grounding capabilities under two settings across various platforms. We observe that with the proposed grounding training, AGUVIS-G-7B significantly outperforms existing models with the original instructions, suggesting that AGUVIS has strong universal GUI grounding capability. After training on high-quality planning trajectory data, AGUVIS shows strong planning capability and outperforms previous models that rely on external closed-source LLMs (like GPT-4o). Moreover, further scaling parameters, AGUVIS-72B achieves state-of-the-art performance, attaining an average score of 89.2. 3.2 OFFLINE GUI AGENT EVALUATION Multimodal-Mind2Web. We utilize Multimodal-Mind2Web (Zheng et al., 2024a) for evaluating the offline planning capabilities of GUI agents on websites, which builds on the original Mind2Web (Deng et al., 2023). We compare with previous work including closed LLMs taking text-only (Deng et al., 2023) or SoM as inputs (Zheng et al., 2024a) and recent prue vision-based agent models. Following previous work (Cheng et al., 2024; Gou et al., 2024), AGUVIS only use the GUI screenshot as observation. We report element accuracy (Ele.Acc), Operation F1 (Op.F1), and step success rate (Step SR). As shown in Table 2, AGUVIS consistently achieves superior performance, with notable improvement in Step SR (+51.9% averaged), indicating enhanced reasoning capabilities regarding planning. 6 Table 2: Performance comparison on Multimodal Mind2Web across different settings. We report element accuracy (Ele.Acc), Operation F1 (Op.F1), and step success rate (Step SR). Best results are in bold. means the textual HTML code as inputs. means the GUI images as inputs. More explanation about result source in Appendix D.2 Obs. Planner Grounder + GPT-3.5 Choice Choice GPT-4 GPT-4 GPT-4 Choice SoM I - - SeeClick CogAgent SeeClick GPT-4o GPT-4V OmniParser GPT-4o UGround AGUVIS-7B AGUVIS-72B Cross-Task Cross-Website Cross-Domain Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F Step SR 19.4 40.8 46.4 29.6 23.8 54.2 32.1 42.4 47.7 64.2 69. 59.2 63.1 73.4 - - - - 87.6 - 89.8 90.8 16.8 32. 40.2 20.3 - - - 39.4 - 60.4 64.0 14.9 30.2 38.0 20. 15.3 50.0 33.1 41.0 46.0 60.7 62.6 56.5 61.0 67.8 - - - - 84.8 - 88.1 88.6 14.1 27.0 32.4 13.9 - - - 36.5 - 54.6 56.5 25.2 35.4 42.4 27.0 16.2 54.7 33.5 45.5 46.6 60.4 63. 57.9 61.9 69.3 - - - - 85.7 - 89.2 88.5 24.1 29. 36.8 23.7 - - - 42.0 - 56.6 58.2 AndroidControl. We assess the planning performance of GUI agent models on mobile devices using AndroidControl (Li et al., 2024d). Following the setting in Li et al. (2024d), we randomly sample 500 step-actions to create subset, and we report the step accuracy on out-of-domain (OOD) data within both high-level and low-level tasks. The high-level task setting necessitates that the model plans and executes actions, whereas the low-level task setting requires the model to simply adhere to human-labeled instructions for executing the next-step action. We compare with baselines that take textual accessibility tree or images as GUI observations. Table 3 shows that AGUVIS achieves the best performance under both settings. Table 3: Step Accuracy of out-of-domain (OOD) data on AndroidControl under high-level tasks and low-level tasks. Best performance is in bold. Acc.Tree means the textual accessibility tree. Observation Planner Grounder Step Accuracy High-Level Low-Level Acc. Tree GPT-4-Turbo Choice PaLM 2S (Specialized) Choice Image Image GPT-4-Turbo GPT-4-Turbo GPT-4o GPT-4o SeeClick UGround SeeClick UGround AGUVIS-7B AGUVIS-72B 42.1 58.5 39.4 46.2 41.8 48. 61.5 66.4 55.0 77.5 47.2 58.0 52.8 62.4 80.5 84.4 3.3 ONLINE GUI AGENT EVALUATION Beyond offline planning, we test AGUVIS on real-time interaction benchmarks: Mind2WebLive (Pan et al., 2024b), AndroidWorld (Rawles et al., 2024a) and MobileMiniWob (Rawles et al., 2024b). We introduce each benchmark below and more details are shown in D. Mind2Web-Live. Mind2Web-Live is dynamic dataset in real web-based environment derived from the original Mind2Web. The benchmark evaluates whether each required step within task has been completed and uses the task success rate (Task SR) as the reported metric. AndroidWorld. AndroidWorld is benchmark operating on an Android virtual environment, capable of dynamically instantiating with randomly generated parameters to generate unique tasks for automatic evaluation. To assess the pure vision agent models, we follow the instructions in Rawles et al. (2024b), installing Pixel 6 phone simulator on our computers to serve as the experimental 7 Table 4: Task Success Rate (SR) and efficiency costs on Mind2WebLive. USD Efficiency is calculated by dividing the models total inference cost in USD by the number of successful steps. Inputs Planner Grounder Task SR USD Efficiency Choice GPT-4-Turbo GPT-4o Choice Llama-3.1-405B Choice Choice Llama-3.1-70B Choice GPT-3.5-turbo GPT-4-Turbo GPT-4o GPT-4o UGround UGround AGUVIS-7B AGUVIS-72B HTML Image Image 21.1 22.1 24.0 20.2 17.3 23.1 19.2 24.0 27. - 0.142 0.174 0.031 0.092 - - 0.106 0.012 Figure 2: Comparison of Input Tokens per Step and USD Efficiency in GUI Interaction. The bar chart shows the input tokens required per step during GUI interactions, while the line graph illustrates USD Efficiency for all models. S s o p 4,000 3,000 2,000 1,000 HTML Image n fi S GPT-4o GPT-3.5 AGUVIS-72B Table 5: Task Success Rates (SR) on AndroidWorld and MobileMiniWob. Best results are in bold. Input AXTree Planner Grounding AndroidWorldSR MobileMiniWobSR Choice GPT-4-Turbo Gemini 1.5 Pro Choice 30.6 19.4 59.7 57.4 Image + AXTree GPT-4-Turbo Gemini 1.5 Pro SoM SoM Image Image GPT-4-Turbo GPT-4o GPT-4o UGround UGround AGUVIS-7B AGUVIS-72B 25.4 22.8 31.0 32.8 37.1 26.1 67.7 40.3 - - 55.0 66. Table 6: Success rate on the OSWorld benchmark in screenshot-only setting Planner Grounding Task SR GPT-4o GPT-4V Gemini-Pro-1.5 GPT-4o GPT-4o SoM AGUVIS-7B AGUVIS-72B 5.03 5.26 5.40 4.59 11.07 10.26 environment. The AndroidWorld benchmark incorporates fully automated task-level evaluation system that automatically assesses whether state has successfully completed designated task. MobileMiniWob. MobileMiniWob is the instantiation of 92 tasks from MiniWob++ (Zheng et al., 2024b) in AndroidWorld environment. Thus, we adopt the same observation and action space utilized in AndroidWorld and use real-time evaluation function to determine task success rate. In our online experiments, we explore two distinct configurations. The first configuration employs GPT-4o as the planner, collaborating with our AGUVIS-7B, which serves as the grounder. The second setup utilizes our AGUVIS-72B in dual role, acting as both the planner and the grounder. We compare the performance of these configurations with existing SOTA methods that use GPT-4(o) models as planners. Unlike existing methods that rely on Set-of-Mark (SoM) or textual HTML/AXTree information, AGUVIS uses only screenshots as observations and is restricted to pyautogui actions in all environments: We set the screenshot viewport to resolution of 1280 720 and disabled all actions based on HTML/AXTree selection. As shown in Table 4 and Table 5, when incorporating the GPT-4o as planner, AGUVIS-7B outperforms existing work in task success rate across various benchmarks. We further adopt our AGUVIS72B both as the planner and grounder, achieving the best performance on Mind2Web-Live and MobileMiniWob, which demonstrates the advantage potential of employing purely visual agent models for autonomous GUI interactions. By employing AGUVIS-72B as both the planner and the grounder, we achieve the best performance on Mind2Web-Live and MobileMiniWob. This underscores the advantages of utilizing unified purely visual agent model for autonomous GUI interactions. Furthermore, we observe that our model demonstrates significant advantage in terms of efficiency costs compared to both closed-source and open-source models (as discussed below), demonstrating that there is considerable potential for applying purely visual agents in real-world online scenarios. 8 Table 7: Ablation on AGUVIS-7B on MM-Mind2Web and AndroidControl benchmarks. We report the step success rate. We provide more comprehensive ablation in Appendix E.1 Settings ScreenSpot Multimodal-Mind2Web AndroidControl Cross-Task Cross-Website Cross-Domain High-Level Low-Level AGUVIS-7B (a) w/o Stage 2 (b) w/o Stage 1 (c) w/o Stage 1 & 2 (d) w/o Inner Monologue 84.4 81.8 77.4 55. 79.3 58.5 50.9 59.7 50.9 55.4 55.4 45.2 55.3 44.9 53.7 54.8 45.3 56.8 47. 54.9 61.5 58.0 58.8 59.1 60.3 80.5 75.6 79.8 59.2 69."
        },
        {
            "title": "4.1 ABLATION",
            "content": "To assess the impact of each stage in the training pipeline of AGUVIS, we conduct ablation experiments. Specifically, we evaluate the performance of the following variants: (a) model trained without the second stage (planning training), referred to as AGUVIS-G-7B, and (b) base model, Qwen2-VL (Wang et al., 2024a), without both stages of our specialized training. We report the results of these ablations on two key benchmarks, Multimodal-Mind2Web and AndroidControl, focusing on the step success rate as the evaluation metric  (Table 7)  . The findings show clear decline in performance when either training stage is omitted. Notably, omitting the second stage (planning and reasoning) has more significant negative effect on the models step success rate, indicating that planning training is critical for enhancing the agents ability to handle complex GUI tasks. 4.2 GENERAZATION ON OTHER VLM BACKBONE Table 8: Performance of AGUVIS based on LLaVAOneVision backbone. We report the average score on ScreenSpot and the step success rate of each split in Multimoda-Mind2Web. These results demonstrate that our framework and data recipe are model independent and the planning stage can largely improve the performance of both grounding and planning ability. Models ScreenSpot Average MM-Mind2Web Task Website Domain Previous SOTA AGUVISOV-G-7B AGUVISOV-7B 73.3 70.0 81.2 39.4 43.4 55.3 36.5 39.0 50.0 42.0 40.7 50.8 Figure 3: Error analysis on Screenspot dataset under the self-plan setting. Ambiguous Error Grounding Error Planning Bonus Self Plan Enforced Plan 1 0.8 0.6 0.4 0.2 0 In our experiments, we also implement version of AGUVIS based on another typical VLM LLaVAOneVision (Li et al., 2024a), named AGUVISOV-7B, to explore the generalizability of AGUVIS. We report the average score of ScreenSpot and the step success rate of Multimoda-Mind2web. These results demonstrate that our framework and data recipe are model-independent and the planning training stage can largely improve the performance of both grounding and planning ability. 4.3 EFFICIENCY We investigate the efficiency costs of AGUVIS on the online planning benchmark Mind2Web-Live. Following Pan et al. (2024a), we adopt the USD Efficiency Score to evaluate the efficiency of our model in completing tasks. Specifically, this Score is calculated as the total dollar cost of tokens used by the model to complete all tasks in the dataset divided by the total Success Steps. lower USD Efficiency Score indicates that the model requires fewer USD to complete successful step. In addition to the USD Efficiency Score, we calculated the number of tokens consumed during the completion of the whole dataset divided by the total number of steps taken by agent models. This reflects the average number of tokens consumed per step. 9 As shown in Figure 2, AGUVIS significantly reduces the efficiency costs by reducing 93% USD costs and 70% input tokens per step compared to GPT-4o, which indicates considerable potential for applying purely visual agents in practical applications."
        },
        {
            "title": "4.4 ERROR ANALYSIS",
            "content": "We conduct an error analysis of AGUVIS on 50 samples from the ScreenSpot dataset under the selfplan setting to understand the impact of planning on performance. As shown in Figure 3, our findings reveal that 40% of errors are due to ambiguous instructions that could refer to multiple grounding targets, while the remaining 60% are grounding errors. We observe that in these error cases, the model tends to perform direct grounding action rather than planning explicitly before acting. Notably, when we enforce planning by prompting the agent model to generate low-level instructions before execution, it resolved 20% of the grounding errors. This suggests that while the agent model possesses strong grounding capabilities, there remains significant potential for improvement in effectively leveraging planning and reasoning. These insights highlight opportunities for future work, including improving instruction clarity through the agent model itself, developing adaptive planning mechanisms, and refining training data to include more diverse planning scenarios. Addressing these aspects could further enhance our GUI agent models robustness on various tasks and environments."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 BENCHMARKS AND DATASETS FOR GUI AGENT Recent advancements in autonomous GUI agents have led to the development of numerous benchmarks and datasets. Web-based benchmarks such as Mind2Web (Deng et al., 2023), WebArena (Zhou et al., 2024; Koh et al., 2024a), WebLINX (L`u et al., 2024), WorkArena (Drouin et al., 2024) and WebCanvas (Pan et al., 2024b) focus on evaluating agents performance in web environments. For desktop and mobile platforms, datasets like OSWorld (Xie et al., 2024), WindowsAgentArena (Bonatti et al., 2024), AitW (Rawles et al., 2024b), AitZ (Zhang et al., 2024b), AMEX (Chai et al., 2024), GUI-Odyssey (Lu et al., 2024) and AndroidControl (Li et al., 2024b) have been introduced to assess agents capabilities across different operating systems and device types. Cross-platform datasets such as ScreenSpot (Cheng et al., 2024), OmniACT (Kapoor et al., 2024), GUICourse (Chen et al., 2024a), and CRAB (Xu et al., 2024a) aim to provide comprehensive evaluation frameworks spanning multiple devices and interfaces. Evaluations on specialized applications have also emerged, such as WonderBread (Wornow et al., 2024)s focus on business process management tasks and Spider-2V (Cao et al., 2024)s on data science and engineering workflows. In this work, we extensively test benchmarks under both online and offline task settings to thoroughly evaluate and demonstrate the models planning and grounding capabilities. 5.2 MODELS AND APPROACHES FOR GUI AGENT In parallel with dataset development, significant progress has been made in creating more capable GUI agents. Models like WebGPT (Nakano et al., 2021), Lemur (Xu et al., 2024b), AgentLumos (Yin et al., 2024), CogAgent (Hong et al., 2024), AutoWebGLM (Lai et al., 2024) and xLAM (Zhang et al., 2024a) have demonstrated improved performance in web navigation tasks. Auto-GUI (Zhang & Zhang, 2024), AppAgent (Zhang et al., 2023), and ScreenAgent (Niu et al., 2024) propose novel approaches for direct GUI interaction without relying on application-specific APIs. SearchAgent (Koh et al., 2024b) introduces an inference-time search algorithm to enhance multi-step reasoning and planning in interactive web environments. These advancements collectively contribute to developing more sophisticated and capable GUI agents, pushing the boundaries of whats possible in automated task completion across various digital platforms."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduced AGUVIS, unified pure vision-based framework for building autonomous GUI agents that operate across diverse platforms. By only leveraging vision-based observations and consistent action space, AGUVIS addresses the key challenges of GUI grounding, 10 planning, and reasoning. Our framework unifies and augments existing datasets, enabling more effective cross-platform generalization while reducing inference costs. Extensive experiments demonstrate that AGUVIS outperforms existing methods in both offline and online GUI tasks, showcasing the first fully autonomous pure vision GUI agent capable of completing real-world tasks without reliance on closed-source models. We will open-source all data, models, and training recipes to facilitate future research in this exciting domain."
        },
        {
            "title": "REFERENCES",
            "content": "Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera Arcas. Uibert: Learning generic multimodal representations for UI understandIn Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, ing. 2021. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Fender C. Bucker, Lawrence Jang, and Zack Hui. Windows agent arena: Evaluating multi-modal os agents at scale. 2024. URL https://api. semanticscholar.org/CorpusID:272600411. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, and Tao Yu. Spider2-v: How far are multimodal agents from automating data science and engineering workflows? ArXiv preprint, 2024. URL https: //arxiv.org/abs/2407.10956. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. ArXiv preprint, 2024. URL https://arxiv.org/abs/2407.17490. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. ArXiv preprint, 2024a. URL https://arxiv.org/abs/2406.11317. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. In Findings of the Association for Computational Linguistics, 2024b. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing GUI grounding for advanced visual GUI agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.505. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Luˇcic, and Neil Houlsby. Patch pack: Navit, vision transformer for any aspect ratio and resolution, 2023. URL https://arxiv.org/ abs/2307.06304. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, 2017. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems, 2023. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Leo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024. URL https://arxiv.org/abs/2403.07718. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Uground: Navigating the digital world as humans do: Universal visual grounding for gui agents, 2024. URL https://github.com/OSU-NLP-Group/UGround/blob/ gh-pages/static/papers/UGround_paper.pdf. Preprint. Izzeddin Gur, Hiroki Furuta, Austin V. Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. In International Conference on Learning Representations, 2024. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. ArXiv preprint, 2022. URL https://arxiv.org/abs/ 2207.05608. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. ArXiv preprint, 2024. URL https://arxiv.org/ abs/2402.17553. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. In Advances in Neural Information Processing Systems, 2023. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024a. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. ArXiv preprint, 2024b. URL https://arxiv.org/abs/2407.01476. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. ArXiv preprint, 2024. URL https://arxiv.org/ abs/2404.03648. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv preprint, 2024a. URL https://arxiv.org/abs/2408.03326. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. ArXiv preprint, 2024b. URL https://arxiv.org/abs/2406.03679. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents, 2024c. URL https: //arxiv.org/abs/2406.03679. Wei Li, William W. Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. ArXiv preprint, 2024d. URL https://arxiv.org/abs/2406.03679. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020a. URL https://aclanthology.org/2020.acl-main.729. Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020b. URL https://aclanthology.org/2020. emnlp-main.443. 12 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. ArXiv preprint, 2024. URL https://arxiv.org/abs/ 2406.08451. Xing Han L`u, Zdenˇek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multiturn dialogue. ArXiv preprint, 2024. URL https://arxiv.org/abs/2402.05930. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. URL https://arxiv.org/abs/2408.00203. Microsoft. Playwright for python documentation. https://playwright.dev/python/, 2024. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv preprint, 2021. URL https://arxiv.org/ abs/2112.09332. Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: vision language model-driven computer control agent, 2024. URL https://arxiv.org/abs/2402.07945. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, and Zhengyang Wu. Webcanvas: Benchmarking web agents in online environments. ArXiv preprint, 2024a. URL https://arxiv.org/abs/2406. 12373. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. ArXiv preprint, 2024b. URL https://arxiv.org/abs/2406.12373. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, URL https://proceedings.neurips.cc/paper/2019/hash/ Canada, 2019. bdbca288fee7f92f2bfa9f7012727740-Abstract.html. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, 2020. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2024a. URL https://arxiv.org/abs/2405. 14573. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 2024b. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, 2024. URL https://arxiv.org/abs/2403.05530. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv preprint, 2024a. URL https://arxiv.org/ abs/2409.12191. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv preprint, 2024b. URL https://arxiv.org/abs/2409. 12191. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingfaces transformers: State-of-the-art natural language processing. ArXiv preprint, 2019. URL https://arxiv.org/abs/1910.03771. Michael Wornow, Avanika Narayan, Ben Viggiano, Ishan S. Khare, Tathagat Verma, Tibor Thompson, Miguel Angel Fuentes Hernandez, Sudharsan Sundar, Chloe Trujillo, Krrish Chawla, Rongfei Lu, Justin Shen, Divya Nagaraj, Joshua Martinez, Vardhan Agrawal, Althea Hudson, Nigam H. Shah, and Christopher Re. Do multimodal foundation models understand enterprise workflows? benchmark for business process management tasks. ArXiv preprint, 2024. URL https://arxiv.org/abs/2406.13264. Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. Webui: dataset for enhancing visual ui understanding with web semantics. ACM Conference on Human Factors in Computing Systems (CHI), 2023. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. ArXiv preprint, 2024. URL https: //arxiv.org/abs/2404.07972. Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, Philip H. S. Torr, Bernard Ghanem, and G. Li. Crab: Cross-environment agent benchmark for multimodal language model agents. ArXiv preprint, 2024a. URL https://arxiv.org/abs/2407.01511. Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. Lemur: Harmonizing natural language and code for language agents. In International Conference on Learning Representations, 2024b. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 2024. China. Xiaoyan Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. ArXiv preprint, 2023. URL https://arxiv.org/abs/2312.13771. 14 Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Awalgaonkar, Rithesh Murthy, Eric Hu, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, and Caiming Xiong. xlam: family of large action models to empower ai agent systems. 2024a. URL https://api.semanticscholar.org/CorpusID:272424184. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. ArXiv preprint, 2024b. URL https://arxiv.org/abs/2403.02713. Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. In Findings of the Association for Computational Linguistics, 2024. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024a. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024b. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In International Conference on Learning Representations, 2024."
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A AGUVIS Unified Design A.1 Details of Action Space in AGUVIS . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Pluggable Functions: Mobile Environments as An Example . . . . . . . . . . . . . Data Curation of THE AGUVIS COLLECTION B.1 Detailed Source Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Prompt for Augmenting Planning & Reasoning Trajectories . . . . . . . . . . . . . B.3 Human Study on Augmented Data . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.1 Qualitative Human Study . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.2 Failure Cases Under Noisy Training Data . . . . . . . . . . . . . . . . . . AGUVIS Training C.1 Training Example Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Benchmarks D.1 GUI Grounding Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Offline GUI Agent Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Online GUI Agent Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3.1 Prompts for using GPT-4o as Planning Model . . . . . . . . . . . . . . . . Analysis E.1 Training Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.1 Training Strategy Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.2 Data Strategy Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Planning Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.1 Prompts for self-planning and enforced planning mode. . . . . . . . . . . . E.2.2 Planning Bounus Examples . . . . . . . . . . . . . . . . . . . . . . . . . E.3 AGUVIS Trajectories Examples on Online Evaluation . . . . . . . . . . . . . . . . E.3.1 Mind2Web-Live Case: AGUVIS-72B as Planner and Grounder . . . . . . . E.3.2 Mind2Web-Live Case: GPT-4o as Planner and AGUVIS-7B as Grounder . . E.3.3 AndroidWorld Case: AGUVIS-72B as Planner and Grounder . . . . . . . . E.3.4 AndroidWorld Case: GPT-4o as Planner and AGUVIS-7B as Grounder . . . E.4 Case of AGUVIS Generalization in Real-World Scenarios . . . . . . . . . . . . . . 17 17 18 18 18 19 19 21 21 22 22 22 24 25 28 28 29 30 30 31 32 33 34 35"
        },
        {
            "title": "A AGUVIS UNIFIED DESIGN",
            "content": "A.1 DETAILS OF ACTION SPACE IN AGUVIS In this section, we introduce our unified action space of our pure vision agent framework AGUVIS. As shown in Table 9, we use default standard pyautogui actions with pluggable actions as the action space of AGUVIS, which ensures the agent models universality across environments as well as its flexibility in the specific environment. Table 9: Default standard pyautogui actions with pluggable actions."
        },
        {
            "title": "Basic\nActions",
            "content": "pyautogui.moveTo(x, y) pyautogui.click(x, y) pyautogui.write(text) pyautogui.press(enter) pyautogui.hotkey(ctrl, c) pyautogui.scroll(200) pyautogui.dragTo(x, y) browser.select option(x, y, value) mobile.swipe(from, to) mobile.home() mobile.back() mobile.open app(name) terminate(status) answer(text) ... Pluggable Actions ... A.2 PLUGGABLE FUNCTIONS: MOBILE ENVIRONMENTS AS AN EXAMPLE In the mobile environment, we provide the following pluggable functions for Aguvis, along with their corresponding descriptions as shown in Figure A.2. Pluggable Functions for AGUVIS You are GUI agent. You are given task and screenshot of the screen. You need to perform series of pyautogui actions to complete the task. You have access to the following functions: - {\"name\": \"mobile.home\", \"description\": \"Press the home button\"} - {\"name\": \"mobile.back\", \"description\": \"Press the back button\"} - { \"name\": \"mobile.long_press\", \"description\": \"Long press on the screen\", \"parameters\": { \"type\": \"object\", \"properties\": {\"x\": {\"type\": \"number\", \"description\": \"The coordinate of the long press\"}, \"y\": {\"type\": \"number\", \"description\": \"The coordinate of the long press\"}}, \"required\": [\"x\", \"y\"] } } - { \"name\": \"mobile.open_app\", \"description\": \"Open an app on the device\", \"parameters\": { \"type\": \"object\", 17 \"properties\": {\"app_name\": {\"type\": \"string\", \"description\": \"The name of the app to open\"}}, \"required\": [\"app_name\"] } } - { \"name\": \"terminate\", \"description\": \"Terminate the current task and report its completion status\", \"parameters\": { \"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"success\"], \"description\": \"The status of the task\"}}, \"required\": [\"status\"] } } - { \"name\": \"answer\", \"description\": \"Answer question\", \"parameters\": { \"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\", \"description\": \"The answer to the question\"}}, \"required\": [\"answer\"] } }"
        },
        {
            "title": "B DATA CURATION OF THE AGUVIS COLLECTION",
            "content": "B.1 DETAILED SOURCE DATASET STATISTICS We present the detailed statistical information of all training datasets utilized in both the grounding and planning & reasoning stages. The statistics are shown in Table 10 and Table 11, respectively. Table 10: The grounding split of THE AGUVIS COLLECTION. Each example in this split consists of single-step trajectory. Data source Platform Instruction #Trajectory Website SeeClick (Cheng et al., 2024) Website GUIEnv (Chen et al., 2024a) Website GUIAct (Chen et al., 2024a) Website WebUI (Wu et al., 2023) Widget Captioning (Li et al., 2020b) Mobile Mobile RicoSCA (Li et al., 2020a) Mobile UI RefExp (Bai et al., 2021) Mobile RICO Icon (Deka et al., 2017) Desktop & Website OmniACT (Kapoor et al., 2024) Augmented Augmented Original Augmented Original Original Original Augmented Original Total 271K 328K 67K 57K 101K 173K 16K 16K 7K 1.036M B.2 PROMPT FOR AUGMENTING PLANNING & REASONING TRAJECTORIES Prompt for GPT-4o generating planning & reasoning data Goal: {goal} Previous Actions: {previous_actions} Given the current screenshot and the next ground truth action labeled as `{current_action_instruction}`, the action commands is: 18 ```json {action_commands} ``` This element is highlighted in red bounding box in the image. Describe the situation in detail, focusing on the goal and current observation. Ensure your reasoning aligns with the goal and the labeled action, but avoid using the labeled action or the highlighted bounding box as reasoning support, as they represent hindsight rather than predictive insight. Conclude with clear, actionable instruction in one sentence. Aim to reason through the task as if solving it, rather than simply reflecting on the labeled outcome. Use the first-person perspective to represent the annotator's thought process. We use GPT-4o as the foundational model to augment our integrated agent trajectory. In this stage, goal represents the target of the trajectory, previous actions is stack of all past low-level instructions, current action instruction refers to the low-level instruction corresponding to the current action in the dataset, and action commands is the representation of the current action in the form of PyAutoGUI code within the dataset. B.3 HUMAN STUDY ON AUGMENTED DATA B.3.1 QUALITATIVE HUMAN STUDY Based on our findings that our Augmented Planning and Reasoning Data improves the performance of Aguvis, we conducted qualitative study on augmented data. From the VLM-augmented data, we selected 90 samples for human study and evaluated them according to specific criteria. We determined that for augmented data to be considered successful, it must: Match the action type and action target elements of the ground truth, Correctly describe the steps intention, Establish clear connection between the steps intention and the overall goal, Assist the agent in successfully completing the task. Among the sampled data, we found that 86.7% demonstrated intermediate reasoning that aligned with the ground truth actions and the overall goals action intention. The remaining 7.8% cases were influenced by dataset noise (irrelevant or unnecessary actions within the task), and 5.5% cases were due to misinterpretations of the action intention under clean data. B.3.2 FAILURE CASES UNDER NOISY TRAINING DATA We analyzed error cases in the generated data and identified several issues. Specifically, we found that unnecessary actions in the training data can lead to the VLM failing to establish connection Table 11: The planning & reasoning split of THE AGUVIS COLLECTION. Data source Platform Inner Monologue Avg. Steps #Trajectory MM-Mind2Web (Zheng et al., 2024a) Website Website GUIAct (Chen et al., 2024a) Website MiniWoB++ (Zheng et al., 2024b) Mobile AitZ (Zhang et al., 2024b) Mobile AndroidControl (Li et al., 2024d) Mobile GUI Odyssey (Lu et al., 2024) Mobile AMEX (Chai et al., 2024) Mobile AitW (Rawles et al., 2024b) Total Generated Generated Generated Original Original Generated Generated Generated 7.7 6.7 3.6 6.0 5.5 15.3 11.9 8. 1,009 2,482 2,762 1,987 13,594 7,735 2,991 2,346 35K 19 between these extra actions and the overall goal, ultimately resulting in incorrect reasoning and planning. While these redundant actions do not compromise the trajectorys overall completeness or correctness, they do introduce challenges for the VLM in generating accurate planning. Figure 4: Examples of augmented planning and reasoning data generated by GPT-4o. The position of the mouse in the image represents the ground truth click position in the training data."
        },
        {
            "title": "C AGUVIS TRAINING",
            "content": "C.1 TRAINING EXAMPLE SCHEMA Training Data Schema of Stage 1 Grounding Prompt <im_start>system You are GUI agent. You are given task and screenshot of the screen. You need to perform series of pyautogui actions to complete the task.<im_end> <im_start>user <vision_start><image_pad><vision_end> Please generate the next move according to the ui screenshot, instruction and previous actions. Instruction: {overall_goal} Previous actions: {previous_actions} <im_end> Generation <im_start>assistant<recipient>os Action: {pyautogui function} <diff_marker> Training Data Schema of Stage 2 Planning Prompt <im_start>system You are GUI agent. You are given task and screenshot of the screen. You need to perform series of pyautogui actions to complete the task.<im_end> <im_start>user <vision_start><image_pad><vision_end> Please generate the next move according to the ui screenshot, instruction and previous actions. Instruction: {overall_goal} Previous actions: {previous_actions} <im_end> Generation <im_start>assistant<recipient>all Observation: {Observation} Thought: {Planning} Low-level Instruction: {Low-level Instruction} <im_end> <im_start>assistant<recipient>os Action: {pyautogui function} <diff_marker> AGUVIS introduces novel explicit planning and reasoning training framework that differs from existing approaches. We illustrate these differences with visual examples in Figure 5. While existing training datasets utilize trajectory data to fine-tune agents, these approaches often involve agents directly outputting action commands (e.g., via pyautogui), bypassing the generation of observations, thoughts, and low-level instructions in natural language that correspond to actions. To elicit the reasoning and planning capabilities of vision-language models and provide the model with richer context for action generation, we scale up training datasets that explicitly require the model to output 21 reasoning and planning steps. Moreover, this approach enhances the interpretability of computer-use agents behavior, laying solid foundation for future research. C.2 TRAINING DETAILS For AGUVIS based on the Qwen2-VL backbone, we set the maximum pixels for each image to 1280 720 to achieve better trade-off between performance and efficiency1. Following the SFT strategy in Wang et al. (2024a), we freeze the ViT parameters during training. For AGUVIS based on the LLaVA-OneVision backbone, we adopt the anyres strategy, which splits high-resolution images into multiple patches following (Li et al., 2024a). The maximum sequence length of tokens is set to 8192 for all models. We use Adam optimizer (Loshchilov & Hutter, 2019) for both grounding and planning & reasoning training stages and employ cosine learning rate scheduler with warm-up ratio of 3% steps. In the grounding stage, we introduce grounding packing strategy to enhance training efficiency. We conduct an ablation study using the grounding data of website platform to investigate the strategy effectiveness. We observe that it reduces overall GPU hours from 6 hours to 1 hour. Moreover, this strategy even marginally improve the performance of ScreenSpot website split from 73.3 to 76.8. We train AGUVIS with batch size of 128 for 1 epoch in each stage. The peak learning rate is set to 1e-5 for AGUVIS-7B and 5e-6 for AGUVIS-72B. Our codebase is based on Pytorch (Paszke et al., 2019) and Huggingface Transformers (Wolf et al., 2019). During training, we utilize the strategies of DeepSpeed optimization (Rajbhandari et al., 2020), BF16 format and gradient checkpointing to save GPU memory. We train AGUVIS on cluster of H100-80G GPUs: AGUVIS-7B uses 8 nodes and completes the grounding training within 5 hours and planning & reasoning training within 1 hour. AGUVIS-72B uses 16 nodes and completes the grounding training within 30 hours and planning & reasoning training within 6 hours."
        },
        {
            "title": "D EVALUATION BENCHMARKS",
            "content": "In this section, we introduce more details of evaluation benchmarks used in our work. D.1 GUI GROUNDING EVALUATION ScreenSpot. ScreenSpot (Cheng et al., 2024)is typical benchmark designed specifically for GUI visual grounding, consisting of 1.2K single-step instructions and coordinates of the target elements. This dataset encompasses variety of grounding instructions tailored for mobile, desktop, and website platforms, and categorizes element types into text and icons/widgets. The benchmark is assessed under two distinct settings: (1) Original Instructions: models perform grounding actions directly following the original instructions; and (2) Self-plan: models are required to generate plans in natural language based on the original instructions before executing grounding actions. D.2 OFFLINE GUI AGENT EVALUATION Multimodal-Mind2Web. We utilize Multimodal-Mind2Web (Zheng et al., 2024a) for evaluating the offline planning capabilities of GUI agents on websites, which builds on the original Mind2Web (Deng et al., 2023). We report element accuracy (Ele.Acc), Operation F1 (Op.F1), and step success rate (Step SR). In Table 2 for Multimodal Mind2Web (Zheng et al., 2024a), we only report element accuracy for SeeClick (Cheng et al., 2024) and CogAgent (Hong et al., 2024). This is because the original SeeClick and CogAgent models were evaluated on Mind2Web (Deng et al., 2023), not Multimodal Mind2Web, making the examples misaligned and incomparable. Therefore, we referenced the results from UGround (Gou et al., 2024), where they report the element accuracy of the SeeClick and CogAgent models on Multimodal Mind2Web, striving to comprehensively present all previously representative methods. 1During preliminary experiments, we observe that increasing the maximum pixels to 1920 1080 does not yield significant improvements on ScreenSpot performance. 22 Figure 5: Compared to the schema of exisiting gui agent data (left), the schema of AGUVIS planning & reasoning data (right) includes explicit reasoning process with informative natural language previous action context. 23 AndroidControl. Following the setting in Li et al. (2024d), we randomly sample 500 step-actions from AndroidControl full test set to create subset, and we report the step accuracy on out-ofdomain (OOD) data within both high-level and low-level tasks. The high-level task setting necessitates that the model plans and executes actions, whereas the low-level task setting requires the model to simply adhere to human-labeled instructions for executing the next-step action. D.3 ONLINE GUI AGENT EVALUATION Mind2Web-Live. We adopt Mind2Web-Live (Pan et al., 2024b) to evaluate GUI agents online planning, derived dynamic data set from Mind2Web, comprising 104 real-time interactive web tasks. It evaluates whether each required step within task has been successfully completed and uses the task success rate (Task SR) as the reported metric. The original Mind2Web-Live is built with WebCavas (Pan et al., 2024a), which is text-based agent framework. To better accommodate the unified observation and action space of pure vision models, we utilize BrowserGym (Drouin et al., 2024) as the evaluation environment for online web tasks which provide support for pure visionbased agent models. BrowserGym is browser testing environment built on the Playwright (Microsoft, 2024) engine. We incorporate all Mind2Web-Live tasks and evaluation into BrowserGym, involving registering all Mind2Web-Live tasks, setting up the entry points for these tasks, and porting the Mind2Web-Live evaluation functions to BrowserGym. As Mind2Web-Live is text-based benchmark, we have to adapt its evaluation function to suit our pure vision-based model. To achieve this, we introduce the two modifications following: For the Mind2Web-Live benchmarks click verification, we adapt our coordinate-based approach by comparing the ground truth CSS selectors bounding box (when available) with our click coordinates, as we cannot directly identify HTML elements. Similarly, for input validation, we retrieve and compare the value of the ground truth input element (if present) with the expected value, circumventing the need for precise HTML element identification based on CSS selectors. The Mind2Web-Live environment relies on real-world websites, many of which implement detection systems for automated browser testing and reCAPTCHA challenges. These factors created difficulties during evluation on the Mind2Web-Live dataset, resulting in lower task success rate (Task SR). Specifically, we observed the following websites to have significant issues with automation detection: kohls. Model using the search functionality on the Kohls website through Playwright directly results in 502 Bad Gateway error. target. We are unable to open targets job website using Playwright due to network connection error. united. We are unable to open united website using Playwright due to network connection error. In addition to the websites that were consistenly prone to failure, several other sites intermittently blocked our Playwright access during testing. In total, we encountered 18 network errors and 6 reCAPTCHA tasks that the model was unable to complete, preventing our model from scoring on these 24 tasks. AndroidWorld. AndroidWorld (Rawles et al., 2024b) is benchmark operating on an Android virtual environment, capable of dynamically instantiating with randomly generated parameters to generate unique tasks for automatic evaluation. It spans 20 real-world applications, encompassing 116 diverse tasks. To assess the pure vision agent models, we follow the instructions in Rawles et al. (2024b), installing Pixel 6 phone simulator on our computers to serve as the experimental environment. The benchmark incorporates fully automated task-level evaluation system that automatically assesses whether state has successfully completed designated task. The AndroidWorld environment supports optional inputs such as Set-of-Mark (SoM) and textual AXTree information, which most multimodal models currently rely on to complete tasks. However, we solely use raw screenshots as the observation input and restrict the model to coordinate-level actions and basic mobile functions. 24 MobileMiniWob. MobileMiniWob (Rawles et al., 2024b) is the instantiation of 92 tasks from MiniWob++ (Zheng et al., 2024b) in the AndroidWorld environment. Thus, we adopt the same observation and action space used in AndroidWorld and use real-time evaluation function to determine task success. D.3.1 PROMPTS FOR USING GPT-4O AS PLANNING MODEL In all online experiments, we employed two settings: GPT-4o as the planner, AGUVIS-7B as the grounder, and AGUVIS-72B as both the planner and grounder. For experiments where AGUVIS72B served as both the planner and grounder, the prompt was straightforward: we only needed to provide AGUVIS-72B with single prompt at each step, and it could independently handle reasoning, planning, and grounding. We use prompt for forcing plan to improve AGUVIS-72Bs performance on the online experiments, as illustrated in Appendix E.2. In the GPT-4o + AGUVIS-7B setting, the situation was more complex. Two key challenges needed to be addressed: making GPT-4os planning usable by AGUVIS-7B and determining which actions required AGUVIS-7B for grounding. To address these challenges, we modified GPT-4os prompts based on Mind2Web-Live (BrowserGym) and AndroidWorld to enable it to delegate grounding actions to AGUVIS-7B when necessary and to share its planning outputs with AGUVIS-7B. Specifically, we append <im start>assistant<recipient>allnThought:{GPT-4o Thought}nAction:{GPT-4o Low-level Instruction} to the end of the prompt and therefore let AGUVIS-7B generate grounding actions based on GPT-4os response. Table 12: Prompt used for the planning model in Mind2Web-Live, modified from the prompt in (Drouin et al., 2024) Instructions Review the current state of the page and all other information to find the best possible next action to accomplish your goal. Your answer will be interpreted and executed by program, make sure to follow the formatting instructions. Goal: {Goal} Observation of current step Current URL: {URL} History of interaction with the task: {History} Action Space 8 different types of actions are available. noop(wait ms: float = 1000) Description: Do nothing, and optionally wait for the given time (in milliseconds). send msg to user(text: str) Description: Sends message to the user. scroll(delta x: float, delta y: float, relative: bool = False) Description: Scroll horizontally and vertically. Amounts in pixels, positive for right or down scrolling, negative for left or up scrolling. Dispatches wheel event. fill(element: str, value: str) Description: Fill out form field. It focuses the element and triggers an input event with the entered text. It works for <input>, <textarea>, and [contenteditable] elements. The element parameter represents the semantic information of the element you want to fill. click(element: str, button: Literal[left, middle, right] = left) Description: Click an element. The element parameter represents the semantic information of the element you want to click. dblclick(element: str, button: Literal[left, middle, right] = left) Continued on the next page 25 Table 12 Continued from the previous page Instructions Review the current state of the page and all other information to find the best possible next action to accomplish your goal. Your answer will be interpreted and executed by program, make sure to follow the formatting instructions. Description: Double click an element. The element parameter represents the semantic information of the element you want to double click. hover(element: str) Description: Hover over an element. The element parameter represents the semantic information of the element you want to hover over. keyboard press(key: str) Description: Press combination of keys. Accepts the logical key names that are emitted in the keyboardEvent.key property of the keyboard events: Backquote, Minus, Equal, Backslash, Backspace, Tab, Delete, Escape, ArrowDown, End, Enter, Home, Insert, PageDown, PageUp, ArrowRight, ArrowUp, F1 - F12, Digit0 - Digit9, KeyA - KeyZ, etc. You can alternatively specify single character youd like to produce such as or #. Following modification shortcuts are also supported: Shift, Control, Alt, Meta. Only single action can be provided at once. Example: fill(comment text area, This is an example) Note: you are on mac so you should use Meta instead of Control for Control+C etc. Table 13: Prompts used for the planning model in AndroidWorld, modified from the prompt in (Rawles et al., 2024a) Instruction You are an agent who can operate an Android phone on behalf of user. Based on users goal/request, you may - Answer back if the request/goal is question (or chat message), like user asks What is my schedule for today?. - Complete some tasks described in the requests/goals by performing actions (step by step) on the phone. When given user request, you will try to complete it step by step. At each step, you will be given the current screenshot and history of what you have done (in text). Based on these pieces of information and the goal, you must choose to perform one of the action in the following list (action description followed by the JSON format) by outputing the action in the correct JSON format. - If you think the task has been completed, finish the task by using the status action with complete as goal status: {action type: status, goal status: complete} - If you think the task is not feasible (including cases like you dont have enough information or can not perform some necessary actions), finish by using the status action with infeasible as goal status: {action type: status, goal status: infeasible} - Answer users question: {action type: answer, text: answer text} - Click/tap on an element on the screen. Please describe the element you want to click using natural language. {action type: click, target: target element description}. - Long press on an element on the screen, similar with the click action above, use the semantic description to indicate the element you want to long press: {action type: long press, target: target element description}. - Type text into text field (this action contains clicking the text field, typing in the text and pressing the enter, so no need to click on the target field to start), use the semantic description to indicate the target text field: {action type: input text, text: text input, target: target element description} Continued on the next page 26 Table 13 Continued from the previous page - Press the Enter key: {action type: keyboard enter} - Navigate to the home screen: {action type: navigate home} - Navigate back: {action type: navigate back} - Scroll the screen or scrollable UI element in one of the four directions, use the same semantic description as above if you want to scroll specific UI element, leave it empty when scroll the whole screen: {action type: scroll, direction: up, down, left, right, element: optional target element description} - Open an app (nothing will happen if the app is not installed): open app, app name: name} - Wait for the screen to update: {action type: wait} {action type: Guidelines Here are some useful guidelines you need to follow: General: - Usually there will be multiple ways to complete task, pick the easiest one. Also when something does not work as expected (due to various reasons), sometimes simple retry can solve the problem, but if it doesnt (you can see that from the history), SWITCH to other solutions. - Sometimes you may need to navigate the phone to gather information needed to complete the task, for example if user asks what is my schedule tomorrow, then you may want to open the calendar app (using the open app action), look up information there, answer users question (using the answer action) and finish (using the status action with complete as goal status). - For requests that are questions (or chat messages), remember to use the answer action to reply to user explicitly before finish! Merely displaying the answer on the screen is NOT sufficient (unless the goal is something like show me ...). - If the desired state is already achieved (e.g., enabling Wi-Fi when its already on), you can just complete the task. Action Related: - Use the open app action whenever you want to open an app (nothing will happen if the app is not installed), do not use the app drawer to open an app unless all other ways have failed. - Use the input text action whenever you want to type something (including password) instead of clicking characters on the keyboard one by one. Sometimes there is some default text in the text field you want to type in, remember to delete them before typing. - For click, long press and input text, the target element description parameter you choose must based on VISIBLE element in the screenshot. - Consider exploring the screen by using the scroll action with different directions to reveal additional content. - The direction parameter for the scroll action can be confusing sometimes as its opposite to swipe, for example, to view content at the bottom, the scroll direction should be set to down. It has been observed that you have difficulties in choosing the correct direction, so if one does not work, try the opposite as well. Text Related Operations: - Normally to select certain text on the screen: (i) Enter text selection mode by long pressing the area where the text is, then some of the words near the long press point will be selected (highlighted with two pointers indicating the range) and usually text selection bar will also appear with options like copy, paste, select all, etc. (ii) Select the exact text you need. Usually the text selected from the previous step is NOT the one you want, you need to adjust the range by dragging the two pointers. If you want to select all text in the text field, simply click the select all button in the bar. - At this point, you dont have the ability to drag something around the screen, so in general you can not select arbitrary text. Continued on the next page 27 Table 13 Continued from the previous page - To delete some text: the most traditional way is to place the cursor at the right place and use the backspace button in the keyboard to delete the characters one by one (can long press the backspace to accelerate if there are many to delete). Another approach is to first select the text you want to delete, then click the backspace button in the keyboard. - To copy some text: first select the exact text you want to copy, which usually also brings up the text selection bar, then click the copy button in bar. - To paste text into text box, first long press the text box, then usually the text selection bar will appear with paste button in it. - When typing into text field, sometimes an auto-complete dropdown list will appear. This usually indicating this is enum field and you should try to select the best match by clicking the corresponding one in the list."
        },
        {
            "title": "E ANALYSIS",
            "content": "E.1 TRAINING ABLATION E.1.1 TRAINING STRATEGY ABLATION To further demonstrate the contribution of Stage 1, Stage 2, and their combination to model training, we conducted an ablation study. Specifically, we designed five experimental settings on AGUVISQWEN2-VL and AGUVISLLAVA-OV: Stage 1 Stage 2 corresponds to the staged configuration AGUVIS used in our paper, where Stage 1 is followed by Stage 2 sequentially. Stage 1 + Stage 2 represents joint training setup, where two stages are combined into training process. w/o Stage indicates the absence of the respective stage in the setting. Note that for the setting of removing Stage 2 (w/o Stage 2 or w/o Stage 1 & 2), the models are fine-tuned on the corresponding task-specific dataset for planning tasks. From the first two rows in Table 14, it can be observed that the differences between models trained with Staged Training and Joint Training setups are relatively minor. However, clear trend emerges: models trained using the Joint Training setup perform better on GUI grounding tasks but exhibit inferior performance on datasets requires planning ability such as MM-Mind2Web and AndroidControl High-level. This trend implies grounding data in Stage 1 is more abundant, dominating the optimization process and biasing the model toward grounding tasks. In contrast, the data in Stage 2, which combines planning and grounding, is of higher quality and better aligned with the agents deployment scenarios. This rationale underpins our decision to position Stage 2 later in the training sequence. Moreover, it is observed that compared to AGUVISQWEN2-VL trained through both Stage 1 and Stage 2, the model trained with only Stage 2 data maintains similar performance on MM-Mind2Web and AndroidControl but exhibits notable decline in GUI grounding performance on ScreenSpot. This suggests that the stability on Mind2Web and AndroidControl can be attributed to Qwen2VLs pretraining on natural image grounding. However, the diverse image and domain requirements of the ScreenSpot GUI grounding test set highlight the necessity of extensive and varied grounding training from Stage 1. This training is essential for improving the grounding performance required for cross-platform GUI agent model. To verify this analysis, we conduct the same ablation study on the LLaVA model, as shown in Table 15. From the results, we can see that the original LLaVA did not undergo extensive natural image grounding training during the training process, making it insufficient for LLaVA to excel when only Stage 1 or Stage 2 is conducted. When both Stage 1 and Stage 2 are performed, LLaVA can be significantly improved, even surpassing previous SOTA results. This validates the above analysis and further demonstrates that our method is model-agnostic and universally applicable to popular VLMs like Qwen2-VL and LLaVA. 28 Table 14: Ablation study of AGUVISQWEN2-VL on training strategy. Settings ScreenSpot Multimodal-Mind2Web AndroidControl Cross-Task Cross-Website Cross-Domain High-Level Low-Level Stage 1 2 Stage 1 + 2 w/o Stage 2 w/o Stage 1 w/o Stage 1 & 2 84.4 85.0 81.8 77.4 55.3 58.5 56.1 50.9 59.7 50.9 55.4 53.1 45.2 55.3 44.9 54.8 55.6 45.3 55.8 47. 61.5 59.2 58.0 58.8 59.1 80.5 80.9 75.6 79.8 59.2 Table 15: Ablation study of AGUVISLLAVA-OV on training strategy. Settings ScreenSpot Multimodal-Mind2Web AndroidControl Cross-Task Cross-Website Cross-Domain High-Level Low-Level Stage 1 2 w/o Stage 2 w/o Stage 1 w/o Stage 1 & 2 81.2 70.0 71.3 3.8 55.3 43.4 42.5 33.8 50.0 39.0 40.3 30. 50.8 40.7 42.8 32.4 60.7 54.9 61.4 50.4 82.4 65.6 80.5 50.0 E.1.2 DATA STRATEGY ABLATION To investigate the impact of different device domain datasets within unified action space, we designed three settings on the MM-Mind2Web dataset: (1) training with the complete dataset comprising both Web and Mobile data, (2) training using only the Web data, and (3) fine-tuning exclusively on the MM-Mind2Web dataset. All three experiments include fine-tuning on the MM-Mind2Web dataset. Table 16: Ablation Study of The Impact of Mobile Data on MM-Mind2Web Model Training Data MM-Mind2Web Cross-Task Cross-Website Cross-Domain AGUVISQWEN2-VL AGUVISLLAVA-OV Web + Mobile (Stage 2 Equivalent) Web Only Mind2Web Only Web + Mobile (Stage 2 Equivalent) Web Only Mind2Web Only 58.5 53.1 50.9 55.3 44.9 43.4 55.4 50.3 44.9 50.0 43.5 39. 54.8 52.2 47.7 50.8 42.1 40.7 Table 17: Ablation Study of the Impact of Inner Monologue AGUVIS ScreenSpot Multimodal-Mind2Web AndroidControl Cross-Task Cross-Website Cross-Domain High-Level Low-Level AGUVIS AGUVIS w/o IM 84.4 79.3 58.5 55.4 55.4 53. 54.8 54.9 61.5 60.3 80.5 69.1 The experimental results, presented in the Table 16, demonstrate that training AGUVIS with both Web and Mobile data consistently outperforms the setting trained exclusively on MM-Mind2Web. This performance gain underscores the contribution of Mobile data to enhancing cross-device domain generalization in the Web domain, validating the effectiveness of our cross-platform data. In addition, we conducted ablation study on the role of incorporating inner monologue (IM) in training. The result shown in Table 17 demonstrated clear performance gain from inner monologue. This gain can be attributed to two key factors: the use of inner monologue enables the model to elicit reasoning about the current step while also serving as context to facilitate more effective planning for subsequent steps. Additionally, incorporating low-level instructions from the training data improves the accuracy of the models action execution, as demonstrated in both the Screenspot and AndroidControl low-level tasks. E.2 PLANNING ANALYSIS E.2.1 PROMPTS FOR SELF-PLANNING AND ENFORCED PLANNING MODE. In Appendix C.1, we present the training data schema for Stage 1 and Stage 2. We use the special token <recipient> along with os or all to control whether the message content is an inner monologue or pyautogui action command. Thanks to this design, we can use <recipient> during the inference phase to control the content generated by the model. In the Enforced Plan Setting, we employ the <recipient>allnThought prompt to compel the model to generate planning phase following this. While in the self-plan setting, we do not add any word after <recipient>, so the model can choose to generate os to directly produce pyautogui command, or generate all to first create natural language reasoning and then generate pyautogui command. Prompt Template For Self-plan <im_start>system You are GUI agent. You are given task and screenshot of the screen. You need to perform series of pyautogui actions to complete the task.<im_end> <im_start>user <vision_start><image_pad><vision_end>Please generate the next move according to the ui screenshot, instruction and previous actions. Instruction: {goal} Previous actions: {previous_actions} <im_end> <im_start>assistant<recipient> Prompt Template For Enforced Plan <im_start>system You are GUI agent. You are given task and screenshot of the screen. You need to perform series of pyautogui actions to complete the task.<im_end> <im_start>user <vision_start><image_pad><vision_end>Please generate the next move according to the ui screenshot, instruction and previous actions. Instruction: {overall_goal} Previous actions: {previous_actions} <im_end> <im_start>assistant<recipient>all Thought: 30 E.2.2 PLANNING BOUNUS EXAMPLES Figure 6: Self-plan examples on different environments. 31 E.3 AGUVIS TRAJECTORIES EXAMPLES ON ONLINE EVALUATION E.3.1 MIND2WEB-LIVE CASE: AGUVIS-72B AS PLANNER AND GROUNDER Figure 7: Example of AGUVIS-72B as planner and grounder executing Mind2Web-Live task. Due to space limitations, we present here the trajectory generated guided by Thought. 32 E.3.2 MIND2WEB-LIVE CASE: GPT-4O AS PLANNER AND AGUVIS-7B AS GROUNDER Figure 8: Example of GPT-4o as planner and AGUVIS-7B as grounder executing Mind2Web-Live task. E.3.3 ANDROIDWORLD CASE: AGUVIS-72B AS PLANNER AND GROUNDER Figure 9: Example of AGUVIS-72B as planner and grounder executing AndroidWorld task. Due to space limitations, we present here the trajectory generated guided by Thought. 34 E.3.4 ANDROIDWORLD CASE: GPT-4O AS PLANNER AND AGUVIS-7B AS GROUNDER Figure 10: Example of GPT-4o as planner and AGUVIS-7B as grounder executing AndroidWorld task. E.4 CASE OF AGUVIS GENERALIZATION IN REAL-WORLD SCENARIOS Figure 11: Case of AGUVIS generalization in real-world scenarios: closing cookie pop-ups, which is an out-of-domain situation."
        }
    ],
    "affiliations": [
        "Salesforce Research",
        "University of Hong Kong"
    ]
}