{
    "paper_title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "authors": [
        "Xu Guo",
        "Fulong Ye",
        "Xinghui Li",
        "Pengqi Tu",
        "Pengze Zhang",
        "Qichao Sun",
        "Songtao Zhao",
        "Xiangwang Hou",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 5 2 4 1 0 . 1 0 6 2 : r DreamID-V: Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer Xu Guo1,, Fulong Ye2,, Xinghui Li2,, Pengqi Tu2, Pengze Zhang2, Qichao Sun2, Songtao Zhao2,, Xiangwang Hou1,, Qian He2 1Tsinghua University, 2Intelligent Creation Lab, ByteDance Equal contribution, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Video Face Swapping (VFS) requires seamlessly injecting source identity into target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks. Date: January 6, 2026 Project Page (Demo, Codes, Models): https://guoxu1233.github.io/DreamID-V/"
        },
        {
            "title": "Introduction",
            "content": "Face swapping aims to generate an image or video that combines the identity of source face with the attributes (such as background, pose, expression, lighting) from target image or video. This technique has sparked considerable research interest, due to its significant potential for practical applications in film production, creative design and privacy protection. Unlike Image Face Swapping (IFS), Video Face Swapping (VFS) presents more challenges, as it introduces additional critical constraints on temporal identity continuity, pose consistency, and environment preservation. Existing studies on Image Face Swapping (IFS), such as [12, 47], have achieved remarkable success in maintaining identity similarity and preserving attributes. However, directly applying these IFS methods frame-by-frame to video sequences often leads to significant challenges in temporal consistency, resulting in noticeable flickering and jittering artifacts. Recently, the rapid advancement of diffusion-based video 1 Figure 1 Showcase of DreamID-V. DreamID-V robustly handles challenging scenarios, e.g., complex expressions, animation, large angles, occlusions, and small faces. generation models has greatly propelled the development of Video Face Swapping (VFS). While methods like VividFace [37], DynamicFace [41], HiFiVFS [5], and CanonSwap [26] have improved the coherence and generation quality of VFS, their capabilities in terms of identity similarity and attribute preservation still lag behind those of state-of-the-art IFS models. The fundamental difference between IFS and VFS lies in the dynamic nature of video, which requires consistent preservation of motion and expression across frames. This observation inspires us to explore whether we can bridge the gap between image and video domains by supplementing these dynamic signals, thereby harnessing the strengths of IFS to significantly boost VFS performance. Building on these insights, we propose comprehensive framework comprising novel data pipeline and customized architecture to enhance VFS performance significantly. Our proposed data curation pipeline, SyncID-Pipe, seamlessly transfers the superiority of IFS to the video domain. Specifically, the pipeline pretrains pose-driven First-Last-Frame video generation model, which we term the Identity-Anchored Video Synthesizer (IVS). The IVS employs an adaptive pose attention mechanism to inject pose information into First-Last-Frame video foundation models. This enables the model to generate video consistent with the 2 content of the given start and end frames and the actions specified by the pose sequence. Subsequently, we combine it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Furthermore, to enhance data reliability, we propose an expression adaptation strategy to achieve effective expression transfer, incorporating an enhanced background recomposition mechanism to ensure strict environment alignment in the paired videos. Building upon paired data, we develop DreamID-V, the first video face swapping framework based on Diffusion Transformer (DiT) models [31], achieving high-similarity and superior-coherence results. We first introduce Modality-Aware Conditioning (MC) mechanism that discriminatively injects conditions from multiple modalities, enabling condition decoupling and feature fusion. Furthermore, we design novel Synthetic-toReal Curriculum learning strategy to strengthen visual realism while maintaining identity similarity. To further enhance the preservation of facial dynamics under challenging scenarios, we develop an Identity-Coherence Reinforcement Learning (IRL) mechanism, which significantly improves model robustness in complex motions. Through the aforementioned improvements, our approach enables effective video face swapping across diverse scenarios, as illustrated in Fig. 1. Due to the limited video-face-swapping benchmarks, we introduce IDBenchV, comprehensive benchmark encompassing wide spectrum of videos with varying head poses, facial expressions, and lighting conditions. We conduct extensive evaluations on IDBench-V, demonstrating that DreamID-V achieves clear advantages over state-of-the-art methods, both quantitatively and qualitatively. Notably, our proposed framework exhibits exceptional versatility and can be seamlessly adapted to various swap-related tasks. Overall, our contributions are summarized as follows. Technology. 1) We develop SyncID-Pipe, which seamlessly transfers the superiority of IFS to VFS, effectively boosting the video face swapping. 2) We propose DreamID-V, the first video face swapping framework based on DiT. 3) We introduce IDBench-V, comprehensive benchmark tailored for the video face swapping task. Significance. 1) DreamID-V demonstrates superior generation performance compared to state-of-the-art methods. 2) We present comprehensive study of the VFS taskincluding data, model, and benchmark. 3) Our proposed framework shows remarkable versatility and can be flexibly adapted to various swap-related tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Video Foundation Model. The development of diffusion models [14] has significantly advanced video foundation model research. Early latent diffusion methods [3, 11, 15] extended Text-to-Image models with U-Net architectures to the video domain by incorporating temporal modules such as 3D convolutions and temporal attention. Rencently, the emergence of Diffusion Transformer(DiT) [32]-based methods for video generation has exhibited superior performance in quality and consistency. These methods [19, 25, 27, 40, 46] employ powerful scaling transformers to generate longer and higher-quality videos. In addition to common Text-to-Video and Image-to-Video models, growing number of keyframe interpolation models [9] have emerged (e.g., First-Last-Frame models), paving the way for various downstream video generation tasks. Face Swapping. Early image face swapping primarily focused on GAN-based models, such as FSGAN [29, 30], FaceShifter [21], HifiFace [42], and SimSwap [4]. More recently, with the rapid development of diffusion models, several diffusion-based image face swapping models have emerged, including DiffFace [18], DiffSwap [49], FaceAdapter [12], ReFace [1], and DreamID [47], all of which have achieved promising results. Compared to image face swapping, video face swapping is still in its nascent stages. VividFace [37] models video face swapping as conditional inpainting task and proposes the first diffusion-based framework. DynamicFace [41] incorporates precise and disentangled facial conditions for flexible and accurate control. HiFiVFS [42] introduces an additional attribute extraction module to capture fine-grained attribute features. CanonSwap [26] proposes canonical space, performing face swapping within this space before projecting back to the real domain. While these works have improved the temporal consistency and quality of generated results, their performance in terms of identity similarity and attribute preservation remains largely unsatisfactory due to lack of explicit supervision. In contrast, we significantly advance video face swapping by leveraging the superiority of image face swapping to construct explicit supervision. 3 Figure 2 Overview of SyncID-Pipe. We pre-train the Identity-Anchored Video Synthesizer and combine it with the Image Face Swapping model to construct Bidirectional Quadruplet Pair data."
        },
        {
            "title": "3 Methodology",
            "content": "We propose comprehensive framework to boost Video Face Swapping (VFS) by harnessing the prowess of Image Face Swapping (IFS). We first introduce novel data curation pipeline SyncID-Pipe, which constructs bidirectional ID quadruplets to bridge the gap between VFS and IFS (Sec. 3.1). Building upon paired data, we develop DreamID-V, the first Diffusion Transformer (DiT)-based video face swapping framework employing core Modality-Aware Conditioning module to discriminatively inject conditions from multiple modalities (Sec. 3.2). To further enhance visual realism and identity consistency under challenging scenarios, we design Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy during training (Sec. 3.3). Moreover, our proposed framework exhibits exceptional versatility (Sec. 3.4)."
        },
        {
            "title": "3.1 SyncID-Pipe",
            "content": "IFS has demonstrated better performance in identity and attribute preservation compared to VFS. The fundamental difference between IFS and VFS lies in the dynamic nature of video, which requires consistent preservation of motion and expression across frames. This observation inspires us to explore bridging the gap between image and video domains by supplementing dynamic signals, thereby leveraging the strengths of IFS to significantly boost VFS performance. 3.1.1 Identity-Anchored Video Synthesizer Building on this insight, we introduce simple yet effective Identity-Anchored Video Synthesizer (IVS) to generate pair data for explicitly supervised training. As shown in Fig. 2, the IVS is trained to reconstruct portrait video Vr by leveraging its extracted pose sequence p. This is achieved by conditioning First-LastFrame video foundation model (FLF2V) [9] on the initial and final frames of Vr, along with its pose sequence p. Such reconstruction-based training enables large-scale video pre-training. To minimize modifications to the foundation model and facilitate seamless integration of its pre-trained motion priors with the face swapping framework (detailed in Sec. 3.2), we introduce an Adaptive Pose-Attention mechanism to inject motion information. Adaptive Pose-Attention. We employ lightweight Pose Guider composed of several simple convolutional layers to extract pose features and align them with the dimension of the latent feature. To ensure precise spatiotemporal alignment between the pose sequence and the noisy latent video, we reuse the Rotary Position Embedding (RoPE) [38] indices from the noisy latents for the pose condition, which can help maintain the alignment. In each DiT block, we introduce two trainable linear layers with latent feature Z. Formally, the output of the Pose-Attention Znew is: and to align pose features Znew = Softmax (cid:19) (cid:18) QK V + λ Softmax (cid:18) Q(K) (cid:19) V, (1) where = ZWq, = ZWk, and = ZWv are derived from the frozen DiT layers, while = PW and = PW are from our trainable pose-adapter layers. The hyperparameter λ controls the strength and flexibility of incorporating pose features. We train IVS by collecting large-scale portrait dataset and extracting pose sequences. The model is optimized using Flow Matching [23]. Through the aforementioned training, IVS can generate video consistent with the content of the given keyframes and the actions specified by the pose sequence. This video is subsequently utilized for constructing bidirectional ID quadruplets."
        },
        {
            "title": "3.1.2 Bidirectional ID Quadruplets Construction",
            "content": "Leveraging the identity preservation and dynamic attribute controllability of our designed IVS model, we effectively bridge the gap between IFS and VFS. Building upon this capability, we construct bidirectional ID quadruplet training data, anchored by IFS, to enable explicit supervision. As illustrated in Fig. 2, for source image-video pair (Ir, Vr) with identity ID and target image (Ig) with identity ID B, we first utilize state-of-the-art IFS model [47] to transfer ID onto the first and last frames of Vr. This process yields high-quality reference frames (Iref 1, Iref 2). Subsequently, these reference frames, along with the retargeted pose sequence, are fed into the pre-trained IVS module to synthesize Vg, video of ID B. The resulting bidirectional ID quadruplet is formatted as {Ir, Vr, Ig, Vg}, where {Ir, Vg, Vr} constitutes the forward-generated paired data, and {Ir, Vr, Vg} represents the backward-real paired data. To further enhance the diversity and practicality of the training data, we implement the following strategies: Source Data Curation. To ensure the robustness of DreamID-V across diverse and challenging scenarios, we carefully curate source videos encompassing varied makeup styles, extreme lighting conditions, and other adversarial settings. Furthermore, we incorporate talking-head datasets to enhance the ability of the model to preserve subtle facial expressions and accurate lip-synchronization. Expression Adaptation. Considering that simply using the pose sequence of the source video to drive the generation of the target video leads to identity-expression entanglement and thus causes identity leakage, we use an expression adaptation module to decouple identity and expression, allowing for high-quality expression transfer. During inference, 3D face reconstruction model [43] extracts identity coefficient from Ig, and the expression and pose coefficients from each Vr frame. We then recombine identity of Ig with expression and pose of each Vr frame to reconstruct new 3D face model. Projecting this model yields retargeted facial landmarks, which replace the original ones in the pose sequence for final inference. Enhanced Background Recomposition. Static keyframe-driven IVS often produces videos with background inconsistencies relative to the source videos, particularly with significant background motion. To enhance the applicability of the model in real-world dynamic scenarios, we design an Enhanced Background Recomposition module. As shown in Fig. 2, for forward-generated paired data {Ir, Vg, Vr}, we first extract foreground masks from Vr and Vg using SAM2 [33]. We then utilize MinimaxRemover [33] to remove the foreground from Vr, yielding clean background video Vbg. Subsequently, the foreground from Vg is pasted back into Vbg to form enhanced video . feathering operation is then applied at the foreground edges to achieve smooth and natural blending. Crucially, in the final paired data obtained this way, the supervision video remains the real video Vr. This prevents the model from learning artifacts introduced during the background pasting process. By specifically augmenting these data, we aim to improve the capabilities of the model in background preservation."
        },
        {
            "title": "3.2 DreamID-V Framework",
            "content": "VFS aims to seamlessly transfer identity information from source face to target video, while preserving attributes of the target video, such as pose, expression, and background. core challenge in designing such model involves effectively injecting and simultaneously disentangling and fusing different types of information, 5 Figure 3 Overview of DreamID-V framework. We design customized injection mechanisms for Spatio-Temporal Context, Structural Guidance, and Identity Information, respectively. including identity and attributes. To address this, we develop DreamID-V, the first video face swapping framework based on Diffusion Transformer (DiT) models, as illustrated in Fig. 3. Central to this framework is the Modality-Aware Conditioning (MC) mechanism, designed to efficiently inject multiple conditions, tailored to their respective needs. 3.2.1 Modality-Aware Conditioning. To make conditions disentangled from each other, the MC mechanism decomposes the conditions for video face swapping into three distinct types. The details are as follows: Spatio-Temporal Context Module. To provide the contextual reference information to be retained (e.g., background, lighting), we inject both the reference video and the dilated face mask into the model. Since these conditions must align precisely with the latent noise in both spatial and temporal dimensions, we concatenate them with latent of source video along the channel dimension. Structural Guidance Module. To achieve fine-grained preservation of dynamic attributes, we incorporate the pose condition as structural guidance. To this end, we employ the Pose-Attention mechanism and initialize corresponding parameters with pre-trained model described in Sec. 3.1.1, which fully leverages the prior from the IVS model. The strategy enables efficient structural control while avoiding disruption to the high-level features being processed by the DiT. In contrast to the context and structural information, identity condition Identity Information Module. represents high-level semantic features and requires comprehensive spatiotemporal interaction. We first employ dedicated ID encoder to encode the reference identity into ID embeddings. And then we concatenate them with the latent noise after patchification along the token dimension, which enables full interaction with latent features through the DiTs inherent attention mechanism. In summary, the spatio-temporal context module provides required attribute information and leverages the mask to indicate facial regions to the model. The structural guidance module further enhances motion attributes, improving the preservation of exaggerated movements and fine-grained expressions. Finally, the identity information module efficiently injects identity-related features."
        },
        {
            "title": "3.3 DreamID-V Training Pipeline",
            "content": "A significant challenge in training video face swapping models involves balancing identity similarity, attribute preservation, visual realism, and ensuring temporal consistency of identity. To mitigate the issue, we introduce Sync-to-Real Curriculum mechanism along with an Identity-Coherence Reinforcement Learning strategy. The training process is divided into the following stages: Synthetic Training. In our constructed bidirectional ID quadruplet data {Ir, Vr, Ig, Vg}, {Ir, Vg, Vr} represents the forward-generated paired data, while {Ig, Vr, Vg} represents the backward-real paired data. In this initial stage, we train our model using the forward-generated paired data. Since Vg is synthesized by the IVS module, it remains distributionally consistent with our underlying video foundation model. (as further elaborated in the supplementary material). This alignment significantly accelerates model convergence and enables the attainment of higher identity similarity, yielding superior similarity compared to direct training with backward-real paired data. Real Augmentation Training. The previous stage yields VFS model with high identity similarity, however, its realism and background preservation remain limited due to the fact that the forward-generated paired data is model-generated and inherently lacks fidelity in background and realism. Therefore, we introduce real augmentation training stage to fine-tune the model, which uses backward-real paired data {Ig, Vr, } augmented by the Enhanced Background Recomposition strategy introduced in Sec. 3.1.2. Experiments show that this stage enables the model to maintain strong identity similarity while achieving excellent background. Identity-Coherence Reinforcement Learning. While the aforementioned mechanisms establish robust baseline, temporal identity consistency remains challenge in complex scenarios, particularly in videos with significant motions. We observe that identity similarity fluctuates, remaining high in frontal views and mild movements but degrading considerably during profile views or intense actions. To specifically address this final-mile problem, as shown in Fig. 3, we introduce the Identity-Coherence Reinforcement Learning (IRL) mechanism, inspired by [8, 24]. The core insight behind IRL is to incentivize the model to focus its learning capacity on difficult frames. We formalize this as policy optimization problem, where the models generative process is treated as policy, πθ, that aims to produce video frames with maximal identity fidelity. Consequently, frames with low identity fidelity are inherently more valuable as learning signals, as they represent the greatest potential for policy improvement. Instead of learning complex Q-function via temporal difference updates in some methods [22, 36], we leverage the specific nature of the video face-swapping task to define an efficient and explicit Q-value. Given state representing the conditional inputs and an action ˆx0 corresponding to the generated video frame, we define the Q-value as: Q(y, ˆx0) = 1 cos(E(ˆx0), E(It)) + δ , (2) where E() denotes the feature embedding extracted by [6], cos(, ) is the cosine similarity, It is the target identity image, and δ is small constant for numerical stability. As implemented in our pipeline, we first perform full sampling pass without backpropagation to generate video and compute the Q-value for each frame. These frame-wise Q-values are then averaged to obtain Qc for each VAE-encoded chunk. These Qc are then used as weights for the flow matching loss [23] during the IRL training step: LIRL(θ) = (cid:88) c=1 Et,ϵ Qc (zc ϵ) vθ ((1 t)zc + tϵ, t, y)2(cid:105) (cid:104) , (3) where the loss is summed over all VAE-encoded chunks in the video. By dynamically re-weighting the loss, our IRL mechanism steers the model to refine identity preservation in difficult segments, significantly reducing temporal flickering."
        },
        {
            "title": "3.4 DreamID-V Versatility\nNotably, our proposed DreamID-V extends beyond face swapping task. By constructing explicit paired data\nfor various human-centric swapping tasks—such as outfit, accessory, and hairstyle swapping—simply through",
            "content": "7 Figure 4 Qualitative comparisons with state-of-the-art methods. Please zoom in for more details. replacing the Image Face Swapping (IFS) model with general-purpose image editing model (e.g., Nano banana [2]) within the SyncID-Pipe, DreamID-V can be extended to wider range of swap category tasks. We will detail this extensibility in Sec. 4.5."
        },
        {
            "title": "4.1 Setup\nIDBench-V. We introduce IDBench-V, a new comprehensive benchmark for video face swapping. The\nbenchmark comprises 200 real-world source video-target image pairs, covering a diverse range of challenging\nscenarios. These include small faces, extreme head poses, severe occlusions, complex and dynamic expressions,\nand cluttered multi-person scenes. IDench-V provides a rigorous and holistic platform for evaluation in\nreal-world usage scenarios.",
            "content": "Implementation Details. We choose OpenHumanVid [20] as the training set and subsequently filter it based on ID similarity to create paired videos of the same identity. Refer to Sec. A.2.2 for more details. Baselines. We conduct comparisons against existing face swapping state-of-the-art (SOTA) models on IDBench-V. For image face swapping, we compare with FSGAN [30], REFace [1], Face-Adapter [12], and DreamID [47], noting that these image-based methods achieve video face swapping by processing frames individually. For video face swapping, we evaluate against Stand-In [45] and CanonSwap [26]. Due to the unavailability of open-source code for VividFace [37] and DynamicFace [41], we perform qualitative comparison using videos from their respective demos in Sec. A.5. Evaluation Metrics We evaluate the performance of various video face swapping methods across three key 8 Method Identity Consistency Attribute Preservation Video Quality ID-Arc ID-Ins ID-Cur Variance Pose Expression Background Subject FVD Smoothness FSGAN [30] REFace [1] Face-Adapter [12] DreamID [47] Stand-In [45] CanonSwap [26] Ours 0.435 0.472 0.440 0.616 0.403 0.397 0.659 0.466 0.471 0.496 0.702 0.403 0.431 0. 0.441 0.474 0.450 0.664 0.367 0.407 0.688 0.0069 0.0191 0.0081 0.0058 0.0057 0.0030 0.0029 7.415 5.102 5.156 3.013 19.819 2.430 2.446 3.463 2.785 3.037 2.930 2.995 2.477 2.430 0.904 0.909 0.942 0.940 0.931 0.950 0.951 0.919 0.913 0.945 0.951 0.951 0.954 0. 6.582 7.084 3.460 3.108 3.368 2.176 2.243 0.982 0.988 0.988 0.989 0.982 0.991 0.992 Table 1 Quantitative comparisons with baseline methods. dimensions: Identity Consistency, Attribute Preservation, and Video Quality. For Identity Consistency, we employ ArcFace [6], InsightFace [39], and CurricularFace [16] to compute ID similarity. To quantify temporal stability, we additionally calculate the variance of these frame-wise similarities. Attribute Preservation is assessed by evaluating the fidelity of pose and expression transferred from the driving video.(details are provided in the supplementary material) Furthermore, we incorporate three metrics from VBench [17]: background consistency, subject consistency, and motion smoothness, which respectively evaluate the consistency of the background, the primary subject, and the overall motion. Finally, for Video Quality, we evaluate perceptual video quality in unpaired scenarios using the Fréchet Video Distance (FVD) [10] with ResNext [44] feature extractor."
        },
        {
            "title": "4.2 Quantitative Comparisons\nMetric Evaluation. As shown in Tab. 1, DreamID-V comprehensively outperforms state-of-the-art models in\nterms of identity similarity metrics. Regarding attribute preservation, DreamID-V is optimal across almost\nall metrics, with only a slight inferiority to CanonSwap in terms of pose. It is worth noting that CanonSwap,\ndue to its very low identity similarity, results in minimal alteration to the original video, thereby exhibiting\ngood attribute preservation and video quality. Our proximity to CanonSwap in attribute preservation\ndemonstrates our model’s excellent capability in this regard, while its identity similarity is significantly\nhigher than CanonSwap. This superiority is attributed to our ID quadruplet effectively transferring the high\nidentity similarity from DreamID to VFS. Interestingly, owing to the effectiveness of our training strategy,\nour identity similarity even slightly surpasses that of DreamID. In terms of video quality, our model also\nachieves outstanding results, showing substantial improvements compared to IFS models such as REFace,\nFace-Adapter, and DreamID. This collectively demonstrates that our model not only achieves high identity\nsimilarity and robust attribute preservation but also generates high-quality videos.",
            "content": "User Study. We invited 19 volunteers to conduct human evaluation of the models on IDBench. Each sample was rated across three dimensions: Identity Similarity, Attribute Preservation, and Video Quality, with scores ranging from 1-5. As presented in Tab. 2, show that our model achieved the best performance across all metrics, thereby demonstrating the superior capabilities of our model."
        },
        {
            "title": "4.3 Qualitative Analysis",
            "content": "Method ID Sim Attr Quality REFace [1] Face-Adapter [12] DreamID [47] Stand-In [45] Canonswap [26] Ours 1.45 2.17 3.78 2.45 1.99 3.85 2.15 2.93 3.89 1.60 3.91 4.22 1.11 1.14 3.06 2.91 3.42 4. Table 2 User study of DreamID-V. We conduct qualitative comparison with VFS methods CanonSwap and Stand-In, as well as IFS methods Face-Adapter and DreamID. As illustrated in Fig. 4, DreamID-V demonstrates excellent performance across identity similarity, expression preservation, background preservation, and occlusion. In the first two cases presented in the first row, our method demonstrates superiority in identity similarity compared to other approaches, consistently across both male and female subjects. Specifically, our identity similarity is significantly better than that of Face-Adapter, Stand-In, and CanonSwap. Perceptually, the identity similarity is close to DreamID, however, we observe that our IVS module, by incorporating dynamic expression information, leads to superior expression performance compared to DreamID. Stand-In, which utilizes an inpainting approach for face swapping, introduces substantial 9 Figure 5 Ablation studies of DreamID-V. alterations to the original video. In the second row, the left case highlights the robust performance of our model under occlusion, outperforming all other models. The right case showcases the superiority of our model in handling complex expressions."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "To demonstrate the effectiveness of our proposed method, we conduct the following ablation studies: a). Directly training using the self-reconstruction inpainting-based approach (w/o Quadruplet); b). Training exclusively with backward-real paired data (w/o ST); c). Training solely with forward-generated paired data (w/o RAT); d). Training without the Identity-Coherence Reinforcement Learning stage (w/o IRL). As shown in Tab. 3 a), Traditional inpainting-based method yields significantly lower identity similarity, demonstrating the effectiveness of SyncID-Pipe to construct explicitly supervised data to bridge the gap between VFS and IFS. b). As demonstrated in Fig. 5 a), the w/o ST setting yields higher realism but lower identity similarity (i.e., good FVD score but poor ID-Arc). Conversely, w/o RAT achieves superior identity similarity but at the cost of realism (i.e., good ID-Arc score but poor FVD). In contrast, our Sync-to-real training strategy(line w/o IRL) ultimately strikes good balance, maintaining high identity similarity while preserving realism. Furthermore, the IRL mechanism significantly enhances identity similarity under complex motions, leading to noticeable improvement. comparison between lines (d) and (e) reveals that IRL not only improves ID-Arc to some extent but also substantially reduces variance, which represents the consistency of inter-frame similarity. As depicted in Fig. 5 b), the top and bottom frames present profile views, while the middle frame shows frontal view. Without IRL, the model performs well on frontal views but poorly on profile views. However, IRL substantially boosts identity similarity in profile views, as exemplified by the topmost frame. Method ID-Arc Variance Pose Expression FVD a) w/o Quadruplet b) w/o ST c) w/o RAT d) w/o IRL e) Ours 0.510 0.604 0.657 0.631 0.659 0.0036 0.0035 0.0042 0.0041 0.0029 2.468 2.742 2.557 2.687 2.446 2.432 2.445 2.443 2.488 2.430 2.242 2.145 3.845 2.206 2. Table 3 Ablation study of DreamID-V."
        },
        {
            "title": "4.5 Versatility",
            "content": "As shown in Fig. 6, by expanding training data, our model can be extended to wider range of human-centric swapping tasks, including accessory, outfit, headphone, and hairstyle swapping. 10 Figure 6 Versatility of our methods. Please zoom in for more details."
        },
        {
            "title": "5 Conclusion",
            "content": "This work presents comprehensive framework for Video Face Swapping (VFS). Our SyncID-Pipe data pipeline effectively transfers the superiority of image face swapping to video, enabling DreamID-Vthe first DiT-based model for VFSto achieve superior performance on our proposed comprehensive benchmark IDBench-V. The method demonstrates strong versatility and provides systematic solution for high-fidelity VFS."
        },
        {
            "title": "References",
            "content": "[1] Sanoojan Baliah, Qinliang Lin, Shengcai Liao, Xiaodan Liang, and Muhammad Haris Khan. Realistic and efficient face swapping: unified approach with diffusion models. arXiv preprint arXiv:2409.07269, 2024. [2] Nano Banana. Gemini 2.5 flash image. https://aistudio.google.com/models/gemini-2-5-flash-image, 2025. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. Simswap: An efficient framework for high fidelity face swapping. In MM 20: The 28th ACM International Conference on Multimedia, 2020. [5] Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, and Chengjie Wang. Hifivfs: High fidelity video face swapping. arXiv preprint arXiv:2411.18293, 2024. [6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [7] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In IEEE Computer Vision and Pattern Recognition Workshops, 2019. [8] Shutong Ding, Ke Hu, Zhenhao Zhang, Kan Ren, Weinan Zhang, Jingyi Yu, Jingya Wang, and Ye Shi. Diffusionbased reinforcement learning via q-weighted variational policy optimization. Advances in Neural Information Processing Systems, 37:5394553968, 2024. [9] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [10] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in fréchet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72777288, 2024. [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [12] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with fine-grained id and attribute control. arXiv preprint arXiv:2405.12970, 2024. [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [16] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59015910, 2020. [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [18] K. Kim, Y. Kim, S. Cho, J. Seo, J. Nam, K. Lee, S. Kim, and K. Lee. Diffface: Diffusion-based face swapping with facial guidance. 2022. 12 [19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [20] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. arXiv preprint arXiv:2412.00115, 2024. [21] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Advancing high fidelity identity swapping for forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50745083, 2020. [22] Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. [24] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [25] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [26] Xiangyang Luo, Ye Zhu, Yunfei Liu, Lijian Lin, Cong Wan, Zijian Cai, Shao-Lun Huang, and Yu Li. Canonswap: High-fidelity and consistent video face swapping via canonical space modulation. arXiv preprint arXiv:2507.02691, 2025. [27] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [28] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):25792605, 2008. [29] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGAN: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE International Conference on Computer Vision, pages 71847193, 2019. [30] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGANv2: Improved subject agnostic face swapping and reenactment. IEEE, 2022. [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [33] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. [34] Nataniel Ruiz, Eunji Chong, and James M. Rehg. Fine-grained head pose estimation without keypoints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2018. [35] Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 13 [37] Hao Shao, Shulun Wang, Yang Zhou, Guanglu Song, Dailan He, Shuo Qin, Zhuofan Zong, Bingqi Ma, Yu Liu, and Hongsheng Li. Vividface: diffusion-based hybrid framework for high-fidelity video face swapping. arXiv preprint arXiv:2412.11279, 2024. [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [39] The InsightFace Team. Insightface products. https://insightface.ai/products. [40] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [41] Runqi Wang, Sijie Xu, Tianyao He, Yang Chen, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, and Yao Hu. Dynamicface: High-quality and consistent video face swapping using composable 3d facial priors. arXiv preprint arXiv:2501.08553, 2025. [42] Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang, Jilin Li, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Hififace: 3d shape and semantic prior guided high fidelity face swapping. arXiv preprint arXiv:2106.09965, 2021. [43] Zidu Wang, Xiangyu Zhu, Tianshuo Zhang, Baiqin Wang, and Zhen Lei. 3d face reconstruction with the geometric guidance of facial part segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16721682, 2024. [44] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016. [45] Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, and Chen Li. Stand-in: lightweight and plug-and-play identity control for video generation. arXiv preprint arXiv:2508.07901, 2025. [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [47] Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, and Xinglong Wu. Dreamid: High-fidelity and fast diffusion-based face swapping via triplet id group learning. arXiv preprint arXiv:2504.14509, 2025. [48] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, and Bingyue Peng. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. [49] Wenliang Zhao, Yongming Rao, Weikang Shi, Zuyan Liu, Jie Zhou, and Jiwen Lu. Diffswap: High-fidelity and controllable face swapping via 3d-aware masked diffusion. CVPR, 2023."
        },
        {
            "title": "A Appendix",
            "content": "In the supplementary material, the sections are organized as follows: We provide the details of Flow Matching in Sec. A.1. We provide more details regarding parameters, datasets, inference, evaluation metrics and user study in Sec. A.2. We provide the details of our proposed benchmark IDBench-V in Sec A.3 We provide the distribution visualization of synthetic data and real data in Sec. A.4. We provide more comparisons with baselines, more qualitative results in Sec. A.5. We provide Ethical Considerations in Sec. A.6 A.1 Preliminary The Diffusion Transformer (DiT) [32] model employs transformer as the denoising network to refine the diffusion latent. Our method inherits the video diffusion transformers trained using Flow Matching [23], which conducts the forward process by linearly interpolating between noise and data in straight line. At the time step t, latent zt is defined as: zt = (1 t)z0 + tϵ, where z0 is the clean video, and ϵ (0, 1) is the Gaussian noise. The model is trained to directly regress the target velocity: LFM = Et,z0,ϵ (z0 ϵ) vθ(zt, t, y)2(cid:105) (cid:104) , (4) where vθ refers to the diffusion model output and denotes condition. A.2 Implementation Details A.2.1 Inference Details To fully leverage the models enhanced capabilities and maximize identity fidelity, strong Classifier-Free Guidance (CFG) [13] is required. In the context of video face swapping, it is natural to leverage the guidance vector to steer the generation towards high degree of identity similarity with the target. This guidance vector, d, is defined as the difference between the velocity predictions with and without the target identity condition, Cid: (5) = vθ(zt, Cpose, Cref, Cid) vθ(zt, Cpose, Cref, ), where Cpose is the pose sequence, Cref is the concatenation of the source video and its mask, and denotes the null identity condition. However, we observe that naively applying conventional CFG often introduces pernicious trade-off: while identity similarity increases, it frequently leads to oversaturation and unrealistic artifacts. Motivated by prior work on guidance modification [35, 48], we propose ID Guidance Purification (IDGP). Our method decomposes the guidance vector into parallel and orthogonal components relative to the normalized conditional prediction ˆvcond: = d, ˆvcondˆvcond, = d, (6) (7) where vcond = vθ(zt, Cpose, Cref, Cid) and ˆvcond = vcond/vcond. We empirically find that the orthogonal component, d, is the primary contributor to the aforementioned artifacts. IDGP, therefore, adjusts the composition of the guidance by differentially re-weighting these components to create purified guidance vector, dIDGP: dIDGP = α + d, (8) 1 α where α > 1 is hyperparameter that simultaneously amplifies the identity-preserving parallel signal and suppresses the artifact-inducing orthogonal signal. Finally, the purified guidance dIDGP is applied to the conditional prediction with an overall guidance scale s: voutput = vcond + dIDGP, (9) 15 Figure 7 Demo of User study. This process effectively purifies the guidance signal, enabling strong identity preservation without sacrificing realism. A.2.2 Detailed Parameters We use the AdamW optimizer with constant learning rate of 1.0 105 for all training stages. The IVS module is trained on 1000 hours of video data. The Synthetic Training stage utilizes 100 hours of IVS-generated video as GT, followed by the Real Augmentation Training stage with 150 hours of real and synthetic data. Finally, the IRL stage is conducted on 10 hours of data selected for high ID variance. The training process for DreamID-V commences with 50k iterations with global batch size of 16 on exclusively synthetic ground truth (GT) data Vg to rapidly establish strong baseline for ID similarity. Subsequently, the model is fine-tuned for an additional 80k iterations with global batch size of 32 on hybrid dataset of real GT Vr and synthetic GT Vg to enhance photorealism. The training regimen culminates in the application of our IRL, where the model is further refined on curated subset of data from the previous stagesspecifically, samples exhibiting high variance in ID similarity. A.2.3 Evaluation Metrics We evaluate the performance of various video face swapping methods across three key dimensions: Identity Consistency, Attribute Preservation, and Video Quality. We employ ArcFace [6], InsightFace [39], and CurricularFace [16] to extract face embeddings and compute the cosine similarity with the target identity image. Additionally, we calculate the variance of these frame-wise similarities to quantify temporal stability. Regarding Attribute Preservation, we assess the fidelity of pose and expression transferred from the driving video. This is achieved by computing the L2 distance between the generated frames and the driving frames in terms of head pose, estimated by HopeNet [34], and expression coefficients, extracted via Deep3DFaceRecon [7]. A.2.4 User Study Fig. 7 illustrates the interface of our user study. Each evaluator was presented with source video, reference image, and six anonymized videos generated by different models. Evaluators were instructed to rate each generated video across three dimensionsidentity similarity, attribute preservation, and video qualityusing 1-to-5-point scale for each dimension. We recruited 19 evaluators, and their final scores were averaged to obtain the overall evaluation. Figure 8 Examples of IDBench-V. A.3 IDBench-V Details To address the lack of benchmark for Video Face Swapping (VFS), we introduce comprehensive benchmark, IDBench-V, which consists of 200 videos paired with meticulously selected ID images. As shown in Fig. 8, our collected videos span diverse categories, including small faces, extreme head poses, severe occlusions, complex and dynamic expressions, and cluttered multi-person scenes. A.4 T-SNE Visualization To validate our claim that synthetic videos (Vg) are inherently distribution-aligned with our DiT-based architecture, we analyze their latent space representations. We sampled 300 videos respectively from three domains: real source videos (Vr), synthetic videos (Vg), and outputs from the base DiT model. Each video was encoded into latent vector using pre-trained VAE, and the resulting representations are visualized in 2D using t-SNE [28]. As shown in Figure 9, the visualization reveals clear structural relationship. The distribution of synthetic videos (Vg, pink) demonstrates high degree of overlap with that of the base models output (blue), forming cohesive cluster. In contrast, the real source videos (Vr, green) occupy more distinct and dispersed region of the latent space. This provides strong visual evidence that our IVS module generates data that is already well-aligned with the models target distribution. This inherent alignment explains the accelerated convergence and improved performance observed when training with synthetic data, as it effectively narrows the domain gap from the outset. A.5 More Visual Results In this section, we provide more visual results of DreamID-V. Fig. 10 and Fig. 13 present our inference results, Fig. 12 and Fig. 13 show comparisons with baselines. As DynamicFace [41] and ViVidFace [37] do not have publicly available source codes, we extract some cases from their websites for comparison. The design of our Figure 9 T-SNE Visualization of Latent Space Distributions. SyncID-Pipe seamlessly inherits the capabilities of image face swapping, enabling high-fidelity results in diverse scenarios such as cartoon styles and under complex lighting. Moreover, the Synthetic-to-Real Curriculum learning strategy allows DreamID-V to maintain high photorealism while preserving exceptional identity similarity. Furthermore, our Identity-Coherence Reinforcement Learning (IRL) ensures that DreamID-V maintains high similarity even under challenging conditions, including large poses and extreme expressions. comparison with closed-source models further underscores the superior identity similarity achieved by DreamID-V. A.6 Ethical Considerations DreamID-V produces high-fidelity, temporally consistent face-swapped videos that could be mis-used to create non-consensual deepfakes or disinformation. To mitigate these risks we release the model under click-through license that explicitly prohibits malicious, privacy-violating or misleading applications and require users to obtain explicit consent from any identifiable individual before publication. 18 Figure 10 More qualitative results I. 19 Figure 11 More qualitative results II. 20 Figure 12 More qualitative comparisons I. Figure 13 More qualitative comparisons II."
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance",
        "Tsinghua University"
    ]
}