{
    "paper_title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning",
    "authors": [
        "Jie He",
        "Richard He Bai",
        "Sinead Williamson",
        "Jeff Z. Pan",
        "Navdeep Jaitly",
        "Yizhe Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 2 9 5 6 8 1 . 1 1 5 2 : r CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning Jie He1,2, Richard He Bai1, Sinead Williamson1, Jeff Z. Pan2, Navdeep Jaitly1, Yizhe Zhang1 1Apple 2University of Edinburgh Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrievalgeneration optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), unified framework that performs embedding-based compression and joint optimization in shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via single language modeling loss, with gradients flowing through both modules using differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines. Code: https://github.com/apple/ml-clara Correspondence: Jie He: j.he@ed.ac.uk; Yizhe Zhang: yizhe_zhang@apple.com Date: November 27,"
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG) has become powerful paradigm for enhancing large language models (LLMs) across diverse NLP tasks (Lewis et al., 2020; Gao et al., 2024; Li et al., 2024b; Wu et al., 2024; Abootorabi et al., 2025). By incorporating external evidence, RAG mitigates key weaknesses of LLMs such as hallucination (Ayala & Bechard, 2024) and knowledge obsolescence (Lau et al., 2025). Most RAG systems suffer from fundamental structural issue: retrieval and generation are optimized separately. Retrievers select documents based on surface-level similarity, while generators produce answers without providing feedback about what information is truly needed (Shi et al., 2025). This disjoint design leads to two intertwined challenges. (1) Optimization. Because document selection is discrete, gradients cannot flow from the generator back to the retriever (Sachan et al., 2021; Lin et al., 2024), hindering joint training and preventing the retriever from aligning with the generators task objective. (2) Efficiency. Dense retrievers rank documents in embedding space, whereas generators still consume raw text, resulting in an architectural mismatch. This mismatch yields (i) inconsistent representation spaces that prevent end-to-end optimization, (ii) redundant text processing that increases inference cost (Merth et al., 2024) and causes context overflow (Leng et al., 2024; Yue et al., 2025), and (iii) duplicated encoding for both retrieval and generation. Even if gradients could flow jointly, these inefficiencies would persist due to the lack of shared latent space. Our Key Insight: Shared Continuous Representations. Towards tackling this issue, we propose unified framework that performs retrieval and generation over shared continuous document representations as shown in Fig 1. Instead of maintaining separate embeddings and raw text, we encode documents once into compact Work done at Apple. 1 Figure 1 (a) During training, we first pretrain the compressor to encourage it to retain only essential information. Next, we perform offline compression of the documents. After that, we encode the query using the query reasoner, retrieve the compressed document representations for generation, and use only the final next-token prediction loss to jointly update both the query reasoner and the generator. (b) An example from the inference stage: the tokens represent key clue words related to the question. When we decode the continuous query embedding, we find that it contains information not present in the original query, indicating that it has learned some of the intermediate reasoning keywords. memory-token representations that serve both purposes. central motivation behind this design is that supervised retrieval training typically relies on relevance-labeled data, which is scarce and often domain-specific. To overcome this limitation, we propagate the next-token prediction (NTP) loss from the generator to the retriever, providing weakly supervised signal that naturally adapts retrieval to downstream generation objectives. This mechanism allows the retriever to learn which documents truly enhance answer generation rather than relying on surface-level similarity. Moreover, continuous representations and joint optimization are inherently complementary: continuous encodings make the retrieval process differentiable, while joint training aligns both modules within shared semantic space optimized for reasoning. This unified design resolves both challenges simultaneously. Optimization-wise, continuous representations enable differentiable top-k selection via Straight-Through estimation, allowing generator gradients to update the retriever directly through gradient descent rather than inefficient RL sampling. Efficiency-wise, shared encodings remove redundant computation and drastically reduce context length, enabling fully end-to-end optimization and inference within the same representation space. To realize this vision, we present CLaRa (Continuous Latent Reasoning), joint retrievalgeneration framework built on shared compressed representations. In Stage I, we propose SCP (Salient Compressor Pretraining), which enhances semantic fidelity by constructing QA pairs that emphasize salient document content beyond surface reconstruction. In Stage II, CLaRa performs end-to-end joint training of the query encoder and answer generator under unified next-token prediction loss, with differentiable top-k selection via Straight-Through estimation. Theoretically, we show this unified objective yields valid gradients for retriever learning without explicit labels. We evaluate CLaRa on four single-hop and multi-hop QA benchmarks with Mistral-7B and Phi-4B. Results show that SCP produces semantically rich compressed representations, and CLaRa achieves state-of-the-art retrieval and generation performanceoutperforming both supervised and unsupervised baselines, and even surpassing full-text fine-tuned models on several tasks."
        },
        {
            "title": "2 SCP: Salient Compressor Pretraining",
            "content": "Previous methods (Louis et al., 2025a; Cheng et al., 2025) typically use token-level reconstruction loss to learn doc representation. However, the learned representation may waste limited capacity/budget on token-by-token reconstruction which might be trivial. Also, the raw representation learned in such way might not digest the document exhaustively. To enable the model to focus on abstracting and digesting semantically informative representations, we first synthesize pre-training data that highlights salient information. Based on this data, we train compression framework, where compressor learns to retain merely essential semantics (Figure 2). 2 Figure 2 Overview of the SCP (Salient Compressor Pretraining) framework. It includes (a) synthetic data construction for pretraining, (2) compressor training using the pretraining data."
        },
        {
            "title": "2.1 Guided Data Synthesis for Semantic Preservation",
            "content": "We first construct synthetic dataset where salient information is explicitly exposed through QA and paraphrasing. This way, the compressor later learns to identify, digest and retain the semantic core of the text by deeply processing the raw token-level information. As shown in Figure 2 (a), our synthetic data generation pipeline consists several steps: (1) salient information elicitation via QA and paraphrase generation, (2) automatic verification of coverage, and (3) regeneration of missing content. Salient Information Elicitation. Using 2M sampled Wikipedia-2021 documents (Izacard et al., 2023), locally deployed LLM (Qwen-32B ) generates three complementary forms of supervision: Simple QA: each pair captures single fact, encouraging fine-grained factual retention. To avoid redundancy, the model is guided to extract distinct atomic facts that have not been covered by previous questions. This helps ensure broad coverage of salient information while keeping each question focused. Example: Q: In which plant family is Trichocladus crinitus classified? A: Trichocladus crinitus belongs to the plant family Hamamelidaceae (the witch-hazel family). Complex QA: each pair integrates multiple facts, promoting relational reasoning and higher-level abstraction. During generation, the model is encouraged to connect previously unlinked facts, which reduces repetition and increases coverage of relational information. Example: Q: Which player joined Newport County on 2 October 2014 and made his debut as second-half substitute in 10 defeat at Oxford United on 4 October? A: The player is James Loveridge. Paraphrase: paraphrased documents reorder sentences structure, thus altering the surface form of the text while preserving its core semantics. Learning such mapping from original text to paraphrased text through an information bottleneck of continuous representation will enable the learned representation to focus on the semantics. QA pairs distill fact-centric supervision as they tell the model which details are essential for answering meaningful questions. Paraphrases, in contrast, demonstrate expression, level compactness, how to rephrase the same content more efficiently. Together, they form complementary signals: factual grounding and linguistic compactness. Verification and Regeneration. Each document and its generated outputs (QA pairs or paraphrases) are verified by the same LLM for factual consistency and information coverage. When missing information is detected, 3 the LLM reviews both the original text and existing QA pairs to generate additional ones capturing uncovered facts, iteratively up to ten rounds. Samples failing final coverage criteria are excluded. This iterative check ensures the model only learns from fully covered, factually faithful pairs."
        },
        {
            "title": "2.2 Compressor Pretraining\nFollowing PISCO (Louis et al., 2025a), we adopt a shared base model equipped with multiple LoRA adapters\nfor modular control, where each adapter corresponds to a distinct function (compression or generation) as\nillustrated in Figure 2 (b).",
            "content": "Compression and Generation. Given document di = {t1, . . . , tm}, we append learnable memory tokens (m1, . . . , ml) and activate only the compressor LoRA θc. The final-layer hidden states of the memory tokens form the compressed representation: Mi = LLMθc([t1, . . . , tm, m1, . . . , ml])[m+1 : m+l]. The compressed vector Mi is concatenated with an instruction to form = [I; Mi]. During pretraining, corresponds to general text-generation tasks (e.g., QA or paraphrasing), and later is replaced with task-specific prompts during instruction tuning. Only the generator LoRA θg is trained via cross-entropy loss: LCE(θc, θg) = (cid:88) (cid:88) (di,I,R ) t=1 log pθg (cid:0)a i,t I, Mi, i,<t (cid:1). (2.1) Compression Alignment. To ensure that the compressed representation faithfully reflects the semantics of the original document, we encourage their latent representations to remain aligned. Intuitively, the memory tokens should summarize the same semantic space as the document tokens, rather than drifting to unrelated regions. Therefore, we minimize the mean squared error (MSE) between the averaged hidden states of document tokens and memory tokens: The total training loss is: LMSE = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 di (cid:88) tdi ht 1 (cid:88) j=1 hmj (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 . Ltotal = LCE + λLMSE, (2.2) (2.3) where λ balances semantic alignment and generative quality. Instruction Tuning. The pretrained compressor is for general purpose. To adapt the compressor for downstream QA and also obtain an answer generator that can comprehend the continuous document representation, we optionally performed an additional instruction finetuning training. Note that this step is not necessary. It is for evaluation and ablation purpose, and we show the comparison in our experiments. To achieve this, we use downstream training datasets in which the retrieved documents are paired with task instructions. These documentinstruction pairs form the input to the model, while the output is reference response generated by teacher model conditioned on the same retrieved documents and instructions. Similar to the compressor pretraining stage, we jointly finetune the LoRA adapters of both the compressor and the answer generator during this instruction-tuning process."
        },
        {
            "title": "3 CLaRa: Retrieval and generation joint training",
            "content": "While the compressor distills documents into compact representations, key challenge is how to retrieve and leverage these representations effectively for downstream reasoning. Conventional RAG systems train the retriever and generator separately, often resulting in mismatch between what retrieval considers relevant and what generation truly needs, as well as two isolated representation spaces. To address this, we propose 4 Figure 3 CLaRa end-to-end training: update query reasoner (θqr) and generator (θg) via language modeling loss using candidate documentquestionanswer triples. CLaRa, which unifies retrieval and generation by training both within single pretrained LLM through differentiable retrieval module. Achieving end-to-end training, however, requires retrieval space that is both stable and computationally manageablefull documents are far too large to be re-indexed throughout optimization. To solve this, we use the compressor trained in SCP to produce high-quality compact representations that remain stable even when the rest of the model updates. By retrieving over these frozen compressed vectors, CLaRa can support end-to-end optimization of retrieval and generation using only shared cross-entropy loss, without requiring relevance-labeled data. Framework Overview As shown in Figure 3, each of the document is compressed into dense embedding Mi = θc(ti) using pretrained compressor θc. The compressor remains frozen to allow offline document encoding. We then train query reasoner (θqr), LoRA adapter initialized from θc, to represent queries in the same space and with the same number of memory tokens as document representations. Through next-token prediction (NTP) training, θqr learns not only to encode query intent but also to anticipate relevant document content, enhancing retrieval and answer generation. For example, given Which city hosted the first modern Olympic Games? , an embedding-based retriever may miss first or modern, whereas the NTP-trained query reasoner favors documents mentioning Athens 1896, which better satisfies retrieval with reasoning needs. We use cosine similarity to determine the relevance between query and documents: si = cos(q, Mi), = 1, . . . , D. (3.1) The top-k document compressed embeddings {M1, . . . , Mk} are concatenated with and fed to the generator (activated by θg), which produces the final answer. During training, both θqr and θg are updated via the unified language modeling loss: LCLaRa(θq, θg) = (cid:88) t= log pθg (cid:0)a (cid:12) (cid:12) Q, M(1:k), <t (cid:1) , (3.2) 1, . . . , where = (a R) denotes the reference output. Importantly, this allows the retriever (implicitly represented by θqr) to learn through weak supervision from the generation objective, without explicit reranking labels. Finding real supervised data might be challenging, and our method is data free as it relies only on downstream next token prediction objective to reason on how to retrieve the doc that maximize the likelihood of downstream generation, thus is more flexible and adaptive. Differentiable Top-k Selection In the CLaRa framework, retrieval and generation are trained jointly, yet their connection is mediated by the top-k selection of relevant documents. However, this discrete operation introduces broken gradient problem: the generators supervision cannot propagate back to inform the retriever why certain documents should be preferred over others. To address this issue, we introduce top selector via Straight-Through (ST) estimator, which conceptually acts as soft lens preserving the discrete retrieval behavior during inference while allowing smooth gradient feedback during training (see Algorithm 1 in Appendix for details). 5 Given cosine similarities = [s1, . . . , sD], temperature τ , and masking for previously selected items, the soft and hard selections are defined as: Zsoft[b, j, :] = softmax (cid:18) sb + log(maskb + ε) τ (cid:19) , Zhard[b, j, i] = (cid:40) 1, 0, if = arg maxi Zsoft[b, j, i], otherwise, (3.3) (3.4) and the final objective that combines the hard and soft representations through ST estimator is: = Zhard + (cid:0)Zsoft SG(Zsoft)(cid:1), where SG() denotes the stop-gradient operator. This maintains discrete behavior in the forward pass while enabling differentiable training through Zsoft. The aggregated top-k document representation is then computed as: (3.5) M(k) = ZM, where RBDd is the matrix of all candidate embeddings with denoting the batch size, the number of candidate documents, and the dimensionality of each document representation, typically defined as the product of the number of memory tokens and the hidden dimension of the underlying LLM. (3.6) Theoretical Justification: Gradient Coupling Analysis We provide theoretical justification for why learning from NTP may yield stronger and more stable learning signals for the θqr. Note that θqr can also be pre-trained independently using contrastive learning over positive and negative documentquery pairs (d+, q) and (d, q), following the DPR framework (Zhou & Chen, 2025). For query and document d, the retrieval score sxd defines p(dx) = exp(sxd/τ ) exp(sxj/τ ) (cid:80) , p(yx) = (cid:88) p(dx)p(yx, d), = log p(yx). When reranking and generation share same representations, p(yx, d) depends on sxd, allowing generator gradients to flow into the reranker. With shared embeddings = (cid:80) πd(s)zd and Straight-Through estimator for top-k, the gradient is sxd = 1 p(yx) (cid:20) p(dx)(cid:0)p(yx, d) p(yx)(cid:1) + p(yx, r) τ (cid:21) p(dx)g(zd r) , where = log p(yx, r). Intuitively, the gradient coupling allows the retriever to receive two complementary Interpretation: learning signals. First, it is rewarded for ranking the correct documents higher through probabilistic alignment between p(dx) and p(yx, d). Second, it is guided to represent documents in way that facilitates the generators reasoning via representation-level feedback from the gradient g. Together, these dual signals stabilize trainingrather than oscillating between strong retrieval and weak generation, both modules progressively align within shared semantic space, ensuring consistent retrievalgeneration behavior (see Appendix for details). Case Study: Query Reasoner θqr To probe the information embedded within the query reasoner θqr, we adopt the logit lens analysis technique (nostalgebraist, 2020). For each memory embedding, we project it through the LLMs output head and record the top-50 tokens with the highest logits as topic tokens. We then aggregate and filter these decoded tokens to remove trivial elements such as punctuation or special symbols. As shown in Table 1, for the query How many yards did the nephew of Ivory Lee Brown get during his 2004 true freshman season?, the query embeddings decoded from the reasoner include the token NFL, Oklahoma, despite the fact that this word does not appear in the question itself. Interestingly, this token also occurs in the corresponding positive document and serves as crucial clue for answering the question. This finding indicates that our end-to-end optimization enables the query reasoner to implicitly encode reasoning-relevant knowledge aligned with the gold evidence, thus enhancing retrieval accuracy and semantic alignment compared to baseline systems. 6 Analysis of Decoded Tokens from Query Reasoner via Logit Lens Question: How many yards did the nephew of Ivory Lee Brown get during his 2004 true freshman season? Reasoned topics from query representation: Truly, Nep, IV, four, yards, NFL, Oklahoma, Ned, Neil, Howard, Kentucky... Retrieved Documents: [1]...Adrian Lewis Peterson (born March 21, 1985) is an American football running back for the New Orleans Saints of the National Football League (NFL). He played college football at Oklahoma and was drafted by the Minnesota Vikings seventh overall in the 2007 NFL Draft. Peterson set the NCAA freshman rushing record with 1,925 yards as true freshman during the 2004 season... [2]...Ivory Lee Brown (born August 17, 1969) is former professional American football running back in the National Football League and World League of American Football. He played for the Phoenix Cardinals of the NFL and the San Antonio Riders of the WLAF. Brown is the uncle of Minnesota Vikings running back Adrian Peterson... ... Answer: 1,925 yards Table 1 Analysis of Decoded Tokens from Query Reasoner via Logit Lens. The highlighted tokens (red) denote the new information reasoned by the query reasoner, while (blue) denotes key evidence for solving this multihop task."
        },
        {
            "title": "Models",
            "content": "CR NQ"
        },
        {
            "title": "Musique",
            "content": "2Wiki"
        },
        {
            "title": "Average",
            "content": "Autocompressor Mistral-7B w/o BGE retrieval Mistral-7B w/ BGE retrieval Phi4-mini w/o BGE retrieval Phi4-mini w/ BGE retrieval llmlingua-2 SCP-Mistral-7B SCP-Phi4-mini coconum pcc pisco SCP-Mistral-7B SCP-Phi4-mini xrag SCP-Mistral-7B SCP-Phi4-mini Autocompressor Mistral-7B w/ BGE retrieval Phi4-mini w/ BGE retrieval llmlingua-2 SCP-Mistral-7B SCP-Phi4-mini coconum pcc pisco SCP-Mistral-7B SCP-Phi4-mini xrag SCP-Mistral-7B SCP-Phi4-mini 1x 1x 1x 1x 1x 4x 4x 4x 16x 16x 16x 16x 16x 128x 128x 128x 1x 1x 1x 4x 4x 4x 16x 16x 16x 16x 16x 128x 128x 128x"
        },
        {
            "title": "Normal",
            "content": "17.24 35.01 54.58 18.77 48.14 47.53 57.05+9.52 53.31+5.78 24.12 31.38 54.39 55.56+1.17 51.96-2.43 32.35 53.36+21.01 43.09+10.74 14.61 27.55 42.94 21.10 37.78 37.05 45.09+8.04 42.36+5.31 21.48 22.29 41.94 43.72+1.78 40.86-1.08 25.16 41.37+16.21 33.92+8."
        },
        {
            "title": "Oracle",
            "content": "29.47 71.64 66.10 63.99 76.50+12.51 73.67+9.68 25.61 49.62 73.44 75.48+2.04 73.17-0.27 42.60 69.96+27.36 60.44+17.84 19.24 70.77 64.06 52.42 73.81+21.39 72.41+19.99 21.72 34.56 66.53 70.79+4.26 70.26+3.73 30.21 62.09+31.88 51.52+21.31 3.81 5.38 8.94 4.05 8.11 9.02 10.34+1.32 8.73-0.29 3.52 3.43 10.09 10.55+0.46 8.61-1.48 3.64 10.26+6.62 6.87+3.23 7.16 45.72 37.07 27.47 46.26+18.79 40.13+12.66 3.64 18.25 33.80 43.15+9.35 38.39+4.59 7.03 30.86+23.83 19.28+12.25 19.89 38.45 44.24 30.26 35.11 44.35 46.94+2.59 45.22+0.87 24.48 19.47 44.88 46.00+1.12 44.27-0.61 28.79 46.40+17.61 43.70+14.91 26.74 68.83 52.69 53.92 70.48+16.56 64.22+10.30 24.63 27.56 60.45 66.16+5.71 63.15+2.70 30.94 59.08+28.14 50.29+19. 13.89 26.6 37.67 18.55 32.28 34.49 39.86+5.37 37.40+2.91 18.40 19.14 37.83 38.96+1.13 36.42-1.42 22.48 37.85+15.37 31.90+9.42 20.65 64.24 54.98 49.45 66.76+17.31 62.61+13.16 18.90 32.50 58.55 63.90+5.35 61.24+2.69 27.70 55.50+27.80 45.38+17.68 Table 2 Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the absolute performance change () of our method under different compression rates relative to its corresponding best baseline performance. CR denotes compression rate."
        },
        {
            "title": "4.1 Experimental setup\nDatasets Following prior work Shi et al. (2025), we evaluate both the compressor and the end-to-end\nframework on the full development sets of four widely used question answering benchmarks: NQ (Kwiatkowski\net al., 2019), HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and 2WikiMultihopQA (Ho\net al., 2020).",
            "content": "7 Baselines For compressor evaluation, we benchmark against both classical and recent methods, including AutoCompressor (Chevalier et al., 2023), XRAG(Cheng et al., 2025), COCOM (Rau et al., 2025), PCC (Dai et al., 2025), LLMLingual-2 (Pan et al., 2024), and PISCO (Louis et al., 2025a). For reranking, we compare with BM25, BGE-Reranker (Chen et al., 2023), RankZephyr-7B (Pradeep et al., 2023), Setwise (Zhuang et al., 2024), and Rank-R1 (Zhuang et al., 2025). End-to-end QA results are evaluated against representative RAG systems, including prompt-based (GenGround (Shi et al., 2024), In-Context RAG), retrieval-optimized (ReComp (Xu et al., 2024), DPA-RAG (Dong et al., 2025)), fine-tuned LLMs (Self-RAG (Asai et al., 2024), Retrobust (Yoran et al., 2024), ChatQA (Liu et al., 2025)), and jointly optimized models (DDR-RAG (Li et al., 2024a), DRO (Shi et al., 2025)). Unlike all baselines operating on raw text, our method is the first to jointly optimize reranking and generation directly over compressed representations. Full experimental settings are provided in Appendix B. Below, we summarize the key findings, while the complete set of additional experiments can be found in the Appendix, including pretraining data analysis (App.C & D), training process analysis (App.E), fidelity and grounding evaluations (App.F), as well as further module analyses (App.G)."
        },
        {
            "title": "4.2 Evaluation of Compression Effectiveness\nWe evaluate our document compressor under two settings: Normal and Oracle. In the Normal setting,\nthe model retrieves the top-5 documents from Wikipedia-2021 for each query. In the Oracle setting, the\nannotated positive document is included among the top-5 to isolate compression quality from retrieval noise.",
            "content": "Table 2 summarizes results across compression ratios. For full results, please refer to table 6 Our method consistently outperforms all baselines. Compared to the best soft compression model PISCO, our model achieves average gains of 1.13% (Normal) and 5.35% (Oracle); over the hard compression baseline LLMLingual-2, improvements reach 5.37% and 17.31%, highlighting stronger semantic preservation. Surprisingly, our model exceeds the text-based w/ BGE retrieval baseline using uncompressed documents, with average gains of 2.36% on Mistral-7B and 6.36% on Phi-4-min. This implies that well-trained soft compression can retain essential reasoning information while substantially reducing input length. This may be because the compressed representations filter out irrelevant content and focus the generator on the reasoning-relevant context, leading to better generalization than raw text inputs. While performance declines at extreme compression (beyond 32 in Oracle), the drop remains moderate under Normal conditions due to weaker document relevance."
        },
        {
            "title": "4.3 Joint Training Results",
            "content": "For end-to-end learning, we evaluate our model under both Normal and Oracle settings. In the Normal setup, each query retrieves the top-20 documents from Wikipedia-2021; the Oracle setup adds annotated positives to the 20-document pool to isolate generation quality from retrieval noise. We compare two initialization strategies for joint rerankinggeneration training: (i) from the compression pretraining checkpoint, and (ii) from the instruction-tuned compressor. Results are shown in Table 3, with full results in Table 7 in Appendix. Under the Normal setting, performance remains stable across compression ratios, peaking at 1632. As 4 might be harder to optim w/ NTP, CLaRa-Mistral-7B with 16x surpasses the text-based DROMistral-7B, improving F1 from 51.0151.41 on NQ and 43.6547.18 on 2Wiki. In the Oracle setting, performance rises notablyF1 exceeds 75% on both NQ and HotpotQAshowing that joint optimization effectively exploits accurate retrieval. Instruction-tuned initialization outperforms pretraining-based initialization under Normal conditions, especially on NQ and HotpotQA, indicating stronger alignment between compression and answering. However, the gap narrows in the Oracle setting, suggesting initialization matters less when retrieval is reliable. Overall, CLaRa demonstrates robust and scalable performance across retrieval qualities and compression ratios."
        },
        {
            "title": "4.4 Retrieval performance\nWe evaluate our method on the document reranking task to assess retrieval effectiveness under the Oracle\nsetting, where positive documents are guaranteed in the candidate set, allowing accurate computation of",
            "content": ""
        },
        {
            "title": "Models",
            "content": "CR NQ F1 EM HotpotQA F1 EM"
        },
        {
            "title": "Musique",
            "content": "2Wiki"
        },
        {
            "title": "Average",
            "content": "F1 EM F1 EM F1 EM GenGround* In-context RAG* RECOMP* DPA-RAG* RetRobust* ChatQA* Self-RAG* DDR-RAG* DRO-Mistral-7B* CLaRa-Mistral-7B (Pretrianing-initialized) CLaRa-Mistral-7B (Instruction-initialized) CLaRa-Mistral-7B (Pretrianing-initialized) CLaRa-Mistral-7B (Instruction-initialized) 1x 1x 1x 1x 1x 1x 1x 1x 1x 4x 16x 32x 4x 16x 32x 4x 16x 32x 4x 16x 32x 42.31 44.69 42.67 44.31 43.82 34.54 31.63 28.76 51.01 40.62 41.75 40.68 48.21 50.89 49.72 77.80 73.81 72.03 75.63 71.54 69. Prompting-based Method 24.36 20.11 41.27 37.14 44.71 41.27 Retrieval tuning 42.72 40.53 38.72 37.15 24.96 20.36 40.60 38. 37.47 37.29 LLM Fine-tuning 37.03 23.64 29.74 35.59 33.40 16.30 40.54 44.60 27.30 18.16 17.05 21.50 End-to-end optimization 10.57 25.32 14.53 15.36 15.32 17.49 18.01 16. 31.71 40.37 29.54 33.72 31.26 35.12 36.67 34.85 35.44 47.87 39.53 44.37 41.84 45.93 47.62 45.73 40.74 42.41 31.21 32.24 31.36 38.16 41.02 39."
        },
        {
            "title": "Oracle data setting",
            "content": "70.52 65.74 63.65 67.64 63.29 65.17 77.66 69.57 70.91 69.66 71.17 68.87 64.83 56.76 57.07 56.92 57.54 55.20 41.59 31.15 33.40 33.19 30.77 28.87 20.77 16.78 17.34 18. 18.11 16.64 9.43 13.54 21.36 6.16 6.99 6.66 8.11 8.44 7.82 30.33 21.18 22.22 22.42 20.56 18.45 42.58 41.02 38.26 39.66 39.11 31.90 27. 38.40 43.65 42.59 43.47 43.23 47.18 44.66 42.57 73.20 65.90 66.32 73.86 60.37 64.38 39.61 38.51 32.17 39.02 38.65 26.80 23.52 35.44 42.12 38.49 39.50 38.98 43.11 40.48 38. 69.14 61.31 61.12 69.74 55.73 59.32 38.49 36.77 37.15 36.22 35.41 32.02 26.94 28.29 41.96 34.32 36.24 35.27 39.70 40.30 38.71 67.56 60.11 60.66 63.08 58.46 57. 35.56 32.62 31.43 32.98 32.34 25.12 19.75 30.36 36.56 26.35 28.11 27.06 31.12 31.65 30.24 58.70 51.25 51.02 54.18 49.28 49.53 Table 3 End-to-End QA Performance. * indicates results reported from the DRO paper. CR denotes compression rate. The highest scores are shown in bold, and the second-best ones are underlined. Overall, our method achieves comparable performance while reducing the required context length by 16. Recall@k.. To compare supervision levels, we introduce fully supervised retriever baseline, Sup-Instruct, which fine-tunes the Query Reasoner via contrastive learning with annotated positive and negative documents. In contrast, our method trains the retriever in weakly supervised manner, only using the next token prediction loss from the downstream generation. Notably, our method does not rely on any supervised data of annotated document relevance labels. As shown in Fig.4 (full results in Table8), CLaRa-Mistral-7B initialized from pretraining consistently outperforms its instruction-tuned version, indicating that instruction tuning, while improving answer generation, biases the model toward localized evidence at the cost of global semantics crucial for retrieval. Remarkably, under the pretraining-initialized setup, CLaRa even surpasses the fully supervised SupInstruct using ground-truth relevance labels. On HotpotQA (compression ratio 4), it achieves Recall@5 of 96.21%, exceeding the strongest supervised baseline BGE-Reranker (85.93%) by +10.28%. Despite relying solely on weak generation supervision, CLaRa presumably captures deep semantic correlations between queries and documents and adapts to the downstream scenarios, achieving retrieval quality on par with or surpassing fully supervised models."
        },
        {
            "title": "5 Ablation Study",
            "content": "Pretraining Data Mix Each document in our setup is paired with two output types: (i) QA-style questionanswer pairs and (ii) paraphrased documents. To assess the impact of data composition, we vary pretraining objectives and report results in Table 4 and 20. For both Mistral-7B and Phi4-mini, using either SimpleQA or Paraphrase alone already outperforms the no-pretraining baseline, showing that factual reasoning and paraphrastic rewriting both enrich compressed representations. Combining multiple QA types (SimpleQA+ComplexQA) or adding paraphrases (SimpleQA+ComplexQA+Para) achieves the best performance, confirming that diverse objectives enhance semantic coverage and generalizationespecially under the Oracle setting, where high-quality retrieval amplifies pretraining benefits. 9 Figure 4 Retrieval performance (Recall@1/3/5) on the Mistral-7B model across different reranking methods under compression ratios = 4 and various initialization settings on NQ and HotpotQA datasets. Supdenotes models trained with labeled data using contrastive learning for the reranker. -Pretrain denotes experiments conducted using the model checkpoint obtained after pretraining, while -Instruct denotes experiments conducted using the model checkpoint obtained after instruction tuning. Models Data composition NQ HotpotQA Musique 2Wikiqa Average Mistral-7B Phi4-mini No-pretrain SimpleQA Para SimpleQA+ComplexQA SimpleQA+ComplexQA+Para No-pretrain SimpleQA Para SimpleQA+ComplexQA SimpleQA+ComplexQA+Para Oracle 70.01 72.66+2.65 73.86+3.85 74.34+4.33 73.77+3.76 65.54 68.70+3.16 67.90+2.36 69.33+3.79 69.90+4.36 61.13 66.41+5.28 68.64+7.51 69.31+8.18 69.51+8.38 60.32 64.60+4.28 64.72+4.40 65.15+4.83 65.32+5.00 29.00 35.29+6.29 36.86+7.86 36.70+7.70 38.31+9.31 27.31 30.41+3.10 31.11+3.80 31.15+3.84 31.77+4.46 57.43 61.22+3.79 63.22+5.79 63.71+6.28 64.54+7.11 56.39 57.46+1.07 58.67+2.28 57.94+1.55 58.52+2. 54.39 58.90+4.51 60.64+6.25 61.02+6.63 61.53+7.14 52.39 55.29+2.90 55.60+3.21 55.89+3.50 56.38+3.99 Table 4 Effect of pretraining data composition on instruction-tuning performance under Oracle (gold context) settings under the 32 compression ratio. We report the absolute score change () for each pretraining data setting relative to the No-Pretrain baseline. Effect of MSE Loss We analyze the effect of the mean-squared error (MSE) loss in Eq.2.2, which aligns compressed and original document representations. As shown in Table 5 and 21, the improvements are modest (0.30.6 points on average) but consistent across datasets, confirming that the MSE loss facilitates semantic preservation during compression. To provide qualitative perspective, we visualize 4K document embeddings and their corresponding compressed representations using t-SNE (Figure 8 in Appendix). Without the MSE loss, the two distributions are clearly separated, reflecting weak correspondence between the memory-token and document spaces. When the MSE loss is applied, the compressed embeddings exhibit strong overlap with the original document representations, demonstrating that the alignment objective effectively enforces semantic consistency between embedding spaces."
        },
        {
            "title": "Models",
            "content": "CR NQ"
        },
        {
            "title": "HotpotQA Musique",
            "content": "2Wikiqa"
        },
        {
            "title": "Average",
            "content": "Mistral-7B w/ mse Mistral-7B w/ mse 32x 32x 128x 128x 74.65 73.77-0.88 71.24 69.96-1."
        },
        {
            "title": "Oracle",
            "content": "69.05 69.51+0.46 62.26 62.09-0.17 37.32 38.31+0.99 29.29 30.86+1.57 62.98 64.54+1.56 57.87 59.08+1.21 61.00 61.53+0.53 55.16 55.50+0.34 Table 5 Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32, 128) and oracle retrieval settings."
        },
        {
            "title": "6.1 Embedding-based/Soft Compression",
            "content": "Recent studies have leveraged LLMs to compress lengthy RAG documents into continuous embeddings for QA tasks (Chevalier et al., 2023; Ge et al., 2024; Mu et al., 2023; Xiao et al., 2025; Dai et al., 2025; Kuratov et al., 2025). Generally, they shorten contexts using continuous representations but are trained independently of LLMs and do not support retrievalgeneration co-optimization. Cheng et al. (2025) propose projection module mapping each document to single-vector representation while freezing encoder and decoder parameters, achieving high compression but losing fine-grained semantics essential for RAG. Louis et al. (2025a) introduce PISCO, which replaces documents with variable memory-token representations and jointly trains the encoder and decoder for tighter coupling between compression and generation. While they suggest pretraining offers limited gains with sufficient instruction data, our results show more targeted pretraining objective can still yield richer and more informative representations. The most related work, Louis et al. (2025b), jointly trains query-aware compression model that also functions as retriever. However, requiring re-compression per query contradicts the goal of reusable, query-independent representations and increases latency. In contrast, our approach enables efficient, fully label-free retriever learning."
        },
        {
            "title": "6.2 End-to-End Optimization for Retrieval and Generation",
            "content": "Reinforcement learning approaches (Shi et al., 2025) allow joint optimization but are unstable and computationally heavy, still relying on raw text. Differentiable reranking (Huang et al., 2025) enables gradient-based selection via Gumbel-softmax but likewise processes full documents at every step, leaving the representation mismatch and context length issues unresolved. As motivated earlier, joint training of retrieval and generation in RAG systems is hindered by the nondifferentiability of discrete document selection. In typical QA pipelines, the retriever reorders retrieved documents before generation (Yu et al., 2024; Dong et al., 2024), but discrete sampling operations prevent gradient backpropagation. In contrast, our framework uniquely combines compression and joint training: by employing length-flexible compressed vectors in shared latent space, we enable efficient differentiable selection while drastically reducing context length. Optimized solely through the generators language modeling loss, CLaRa ensures consistent traininginference alignment and efficient end-to-end learning without explicit retrieval supervision."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we address the challenge of compressing documents into high-quality implicit representations to enhance the performance of retrieval-augmented generation (RAG) systems that rely on document embeddings for question answering. To this end, we design multiple pretraining objectives that leverage LLM prompting to construct diverse supervision signals, including QA pairscovering both simple and compositional reasoningand paraphrased documents, encouraging the compressor to retain essential semantic information. We further introduce an efficient end-to-end training framework that unifies document representations across the reranking and generation stages, leading to substantial improvements in retrieval accuracy and answer quality. Extensive experiments on multiple QA benchmarks demonstrate that embedding-based contextual compression not only reduces input length and computation cost but also bridges the gap between retrieval and generation, enabling more unified and semantically coherent RAG paradigm."
        },
        {
            "title": "Limitations",
            "content": "Compressor Generalization. The current compressor is pretrained exclusively on Wikipedia data. To improve generalizability, future work may incorporate domain-adaptive pretraining objectives and leverage more diverse corpora (e.g., code datasets (Wang et al., 2025)) to enhance representation robustness across modalities and domains. 11 Reasoning over Compressed Representations. While our study does not primarily focuses on reasoning based on compressed representations, the compact and semantically dense nature of compressed representations makes them promising candidate for integration into reasoning-oriented or agentic RAG frameworks, such as Search-R1 (Jin et al., 2025). natural next step is to investigate whether such representations can function as efficient reasoning memory in multi-hop or planning-based RAG systems. Model Size. Our experiments are conducted on medium-scale models (Mistral-7B and Phi-4B). Larger models may produce higher-quality document representations that better capture semantic nuances. An open question for future work is whether there exists model size threshold beyond which compressed representations surpass raw text in supporting understanding and generation. Generalization of Implicit Representations. This work focuses on leveraging unified representations for both understanding and generation within the RAG framework. Given the growing interest toward unified models that jointly perform comprehension and generation (Zhang et al., 2025), extending compression-based methods to broader taskssuch as tool learning (Qu et al., 2025)offers promising direction for developing more general-purpose, reasoning-capable systems. Besides, linking implicit understanding with implicit reasoning (Hao et al., 2025; Kang et al., 2025) would be an interesting direction."
        },
        {
            "title": "References",
            "content": "Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: comprehensive survey on multimodal retrieval-augmented generation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1677616809, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.861. URL https://aclanthology.org/2025.findings-acl.861/. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=hSyW5go0v8. Orlando Ayala and Patrice Bechard. Reducing hallucination in structured outputs via retrieval-augmented generation. In Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pp. 228238, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-industry.19. URL https://aclanthology.org/2024.naacl-industry.19/. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023. Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. xrag: extreme context compression for retrieval-augmented generation with one token. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/. Yuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao. Pretraining context compressor for large language models with embedding-based memory. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2871528732, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1394. URL https://aclanthology.org/2025.acl-long.1394/. Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Ji-Rong Wen, and Zhicheng Dou. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. In THE WEB CONFERENCE 2025, 2025. https://openreview.net/forum?id=2ZaqnRIUCV. 12 Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. Dont forget to connect! improving rag with graph-based reranking, 2024. URL https://arxiv.org/abs/2405.18414. Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation, 2025. URL https://arxiv.org/abs/2309.15217. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. URL https://arxiv.org/abs/2312.10997. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=uREj4ZuGJE. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language model to reason in continuous latent space. 2025. https://openreview.net/forum?id=tG4SgayTtk. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580/. Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, and Zhouhan Lin. Gumbel reranking: Differentiable end-to-end reranker optimization. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 71427161, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.354. URL https://aclanthology.org/ 2025.acl-long.354/. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24(1), January 2023. ISSN 1532-4435. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. https://openreview.net/forum?id=Rwhi91ideu. Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, and Lianhui Qin. Ladir: Latent diffusion enhances llms for text reasoning, 2025. URL https://arxiv.org/abs/2510.04573. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into single vector and back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1932319339, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. URL https://aclanthology.org/2025.acl-long.948/. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, MingWei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/. Kwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, and Xiaojun Cheng. Reading between the timelines: Rag for answering diachronic questions, 2025. URL https://arxiv.org/abs/2507.22917. Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. Long context rag performance of large language models, 2024. URL https://arxiv.org/abs/2411.03538. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, et al. Rag-ddr: Optimizing retrieval-augmented generation using differentiable data rewards. arXiv preprint arXiv:2410.13509, 2024a. 13 Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, and Chenyan Xiong. RAG-DDR: Optimizing retrieval-augmented generation using differentiable data rewards. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=Pnktu2PBXD. Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context LLMs? comprehensive study and hybrid approach. In Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 881893, Miami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.66. URL https://aclanthology.org/2024.emnlp-industry.66/. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrievalaugmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=22OTbutug9. Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: surpassing gpt-4 on conversational qa and rag. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. Maxime Louis, Hervé Déjean, and Stéphane Clinchant. PISCO: Pretty simple compression for retrieval-augmented generation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1550615521, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.800. URL https://aclanthology.org/2025.findings-acl.800/. Maxime Louis, Thibault Formal, Hervé Dejean, and Stéphane Clinchant. Oscar: Online soft compression and reranking, 2025b. URL https://arxiv.org/abs/2504.07109. Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting: improving and In Proceedings of the 41st International Conference on Machine accelerating retrieval-augmented generation. Learning, ICML24. JMLR.org, 2024. Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. nostalgebraist. interpreting GPT: the logit lens, Aug 2020. URL https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens. LessWrong blog post. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 963981, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.57. URL https://aclanthology.org/2024.findings-acl.57/. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankzephyr: Effective and robust zero-shot listwise reranking is breeze!, 2023. URL https://arxiv.org/abs/2312.02724. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool learning with large language models: survey. Front. Comput. Sci., 19(8), January 2025. ISSN 2095-2228. doi: 10.1007/s11704-024-40678-2. URL https://doi.org/10.1007/s11704-024-40678-2. David Rau, Shuai Wang, Hervé Déjean, Stéphane Clinchant, and Jaap Kamps. Context embeddings for efficient answer generation in retrieval-augmented generation. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, WSDM 25, pp. 493502, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400713293. doi: 10.1145/3701551.3703527. URL https://doi.org/10.1145/3701551.3703527. Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. https://openreview.net/forum?id=5KWmB6JePx. Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. Generate-thenground in retrieval-augmented generation for multi-hop question answering. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics 14 (Volume 1: Long Papers), pp. 73397353, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.397. URL https://aclanthology.org/2024.acl-long.397/. Zhengliang Shi, Lingyong Yan, Weiwei Sun, Yue Feng, Pengjie Ren, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, Maarten de Rijke, and Zhaochun Ren. Direct retrieval-augmented optimization: Synergizing knowledge selection and language models, 2025. URL https://arxiv.org/abs/2505.03075. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.org/2022.tacl-1.31/. Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, and Daniel Fried. Coderag-bench: Can retrieval augment code generation?, 2025. URL https://arxiv.org/abs/2406.14497. Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue. Retrieval-augmented generation for natural language processing: survey. CoRR, abs/2407.13193, 2024. URL https://doi.org/10.48550/arXiv.2407.13193. Zilin Xiao, Qi Ma, Mengting Gu, Chun cheng Jason Chen, Xintao Chen, Vicente Ordonez, and Vijai Mohan. Metaembed: Scaling multimodal retrieval at test-time with flexible late interaction, 2025. URL https://arxiv.org/abs/2509.18095. Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=mlJLVigNHp. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models In The Twelfth International Conference on Learning Representations, 2024. robust to irrelevant context. https://openreview.net/forum?id=ZS4m74kZpH. Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. RankRAG: Unifying context ranking with retrieval-augmented generation in LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. https://openreview.net/forum?id=S1fc92uemC. Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=FSjIrOm1vz. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: EvalIn International Conference on Learning Representations, 2020. uating text generation with bert. https://openreview.net/forum?id=SkeHuCVFDr. Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Unified multimodal understanding and generation models: Advances, challenges, and opportunities, 2025. URL https://arxiv.org/abs/2505.02567. Jiawei Zhou and Lei Chen. Optimizing retrieval for rag via reinforced contrastive learning, 2025. URL https: //arxiv.org/abs/2510.24652. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, pp. 3847, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657813. URL https://doi.org/10.1145/3626772.3657813. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Rank-r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.06034. Weronika Łajewska, Momchil Hardalov, Laura Aina, Neha Anna John, Hang Su, and Lluís Màrquez. Understanding and improving information preservation in prompt compression for llms, 2025. URL https://arxiv.org/abs/2503.19114. 15 Gradients for Non-shared vs. Shared Representations in RAG Step 1: Let sxd be the retrieval score for query and document d, and let p(d x) = exp(sxd) dC exp(sxd) , (cid:80) p(y x) = (cid:88) dC p(d x) p(y x, d), = log p(y x). (A.1) Step 2: product rule inside the sum. sxd (cid:16) (cid:88) p(dx) p(yx, d) (cid:17) (cid:88) = p(dx) sxd (cid:123)(cid:122) softmax Jacobian (cid:124) (cid:125) p(yx, d) + p(dx) p(yx, d) sxd . (cid:88) (A.2) Step 3: softmax Jacobian. For p(dx) = es (cid:80) xd esxj ,"
        },
        {
            "title": "Therefore",
            "content": "p(dx) sxd = p(dx)(cid:0)1[d = d] p(dx)(cid:1). p(dx) sxd (cid:88) p(yx, d) = (cid:88) p(dx)(cid:0)1[d = d] p(dx)(cid:1) p(yx, d) = p(dx) p(yx, d) p(dx) (cid:88) p(dx) p(yx, d) = p(dx) (cid:0)p(yx, d) p(yx)(cid:1). Step 4: put together. Plugging equation A.5 into equation A.2 and then equation A.1 gives sxd = 1 p(yx) p(dx)(cid:0)p(yx, d) p(yx)(cid:1) (cid:125) (cid:123)(cid:122) (cid:124) (I) probability path + (cid:88) p(dx) p(yx, d) sxd (cid:124) (cid:123)(cid:122) (II) representation/generation path (cid:125) (A.3) (A.4) (A.5) (A.6) . If the generators conditional p(yx, d) depends on sxd only when Step 5: common simplification (assumption). = (e.g., each conditional uses its own selected document; non-shared case gives it zero), then the second sum reduces to single term: p(dx) p(yx, d) sxd (cid:88) = p(dx) p(yx, d) sxd . Under this widely-used assumption, equation A.6 becomes sxd = (cid:104) 1 p(y x) p(d x)(cid:0)p(y x, d) p(y x)(cid:1) (cid:125) (cid:123)(cid:122) (cid:124) (I) probability path + p(d x) p(y x, d) sxd (cid:123)(cid:122) (II) representation/generation path (cid:125) (cid:124) (cid:105) . (A.7) If the generator conditions on mixture = (cid:80) Remark (more general shared-conditioning). p(dx) p(yx,d) p(yx, d) shares the same r), then p(yx,d) forms are consistent; the boxed formula corresponds to the per-document conditional view. is the same for all and (cid:80) sxd sxd πj(s) zj (so every = p(yx,r) . Both sxd Term (II) is present if the generators conditional p(y x, d) depends (directly or indirectly) on sxd. 16 Case A: Non-shared representations (retriever = generator) Here the generator consumes raw tokens or an independent encoder, hence p(y x, d) does not depend on sxd: Plugging into equation A.7 gives the complete gradient: p(y x, d) sxd = 0. sxd = 1 p(y x) p(d x) (cid:0)p(y x, d) p(y x)(cid:1). (A.8) (A.9) This expression already accounts for the softmax coupling via p(y x) and is more accurate than writing only 1 p(y x) p(y x, d) p(d x)/sxd. Case B: Shared representations (retriever = generator) When retriever and generator share embeddings, p(y x, d) depends on sxd through the generators conditioning vector. common differentiable conditioning is πj = exp(sxj/τ ) ℓ exp(sxℓ/τ ) (cid:80) , = (cid:88) jC πj zj, p(y x, d) p(y x, r), (A.10) where τ > 0 is the temperature, zj are document embeddings, and is fed to the generator. Let log p(y x, r) (cid:88) = log p(yt y<t, x, r) . sxd (cid:88) = πj sxd zj = 1 τ πd (zd r). Using the softmax Jacobian, By chain rule, p(y x, d) sxd = p(y x, r) r sxd = p(y x, r) τ πd g(cid:0)zd r(cid:1). Substituting equation A.13 into equation A.7 yields the full shared-representation gradient: sxd = 1 p(y x) (cid:104) p(d x) (cid:0)p(y x, d) p(y x)(cid:1) + p(r x) p(y x, r) τ πd g(cid:0)zd r(cid:1)(cid:105) . (A.11) (A.12) (A.13) (A.14) If the forward pass uses hard top-k selection (argmax/indices) but the backward Straight-through (ST) note. pass adopts the softmax gradient (ST estimator), then formulas equation A.12equation A.14 remain the correct backpropagation rules (with π computed from the scores for the backward pass). Optional: Cosine-similarity score backpropagation"
        },
        {
            "title": "If the score is cosine similarity",
            "content": "sxd = qzd zd , 17 (A.15) j= taken 0BD (1) Hard selection: rj arg maxi s(:, i) on unmasked candidates Algorithm 1 Differentiable Top-k Selection with Straight-Through Estimator in CLaRA 1: Input: Similarity scores RBD, temperature τ , number of selections 2: Output: Selection tensor RBkD and top-k indices {rj }k 3: s/ max(τ, 106) 4: Initialize Zhard, Zsoft 0BkD, 5: for = 1 to do 6: 7: 8: 9: 10: 11: 12: 13: end for 14: (3) Straight-through estimator: Zhard + (Zsoft SG(Zsoft)) 15: return (Z, {rj }k logitsj + log(mask + ε) pj softmax(logitsj ) Zsoft[:, j, :] pj taken min(taken + Zhard[:, j, :], 1) (2) Soft selection: mask 1 SG(taken) Zhard[:, j, rj ] j=1) then the required Jacobians are sxd = 1 zd (cid:18) zd sxd (cid:19) q2 zd , sxd zd = 1 zd (cid:18) sxd (cid:19) zd zd2 ."
        },
        {
            "title": "Hence",
            "content": "L = (cid:88) dC sxd sxd , zd = sxd sxd zd . (A.16) (A.17) Num. Avg.pairs Avg.inp Avg.out Simple QA Complex QA Paraphrase Doc 2,000,000 7.80 95.56 158.18 2,000,000 4.62 95.56 253.90 1,966,291 1.00 95.56 108. Table 9 Pretraining data statistics for SCP. The table reports the total number of training examples (Num.), average number of generated QA pairs or documents (Avg.pairs), average input document length (Avg.inp) and average generated text length (Avg.out) for Simple QA, Complex QA, and Paraphrased Documents.."
        },
        {
            "title": "Evaluation Data Size",
            "content": "Nature Question HotpotQA MusiQue 2WikiMultiHopQA 58,622 90,185 168,745 167,454 6,489 7,384 2,417 12,576 Table 10 Statistics of experimental datasets."
        },
        {
            "title": "B Detailed experimental setup",
            "content": "B.1 Datasets The pretraining corpus consists of 2M documents and their corresponding 2M SimpleQA sets, 2M ComplexQA sets, and 2M paraphrased documents. Detailed statistics on data composition and distribution are provided in Table 9. During the instruction tuning stage of compression learning, we use question data from COCOM (Rau et al., 2025) , which contains 453k questions. We employ the Mistral-7B model and retrieve the top-5 most"
        },
        {
            "title": "Models",
            "content": "CR NQ"
        },
        {
            "title": "Musique",
            "content": "2Wiki"
        },
        {
            "title": "Average",
            "content": "Autocompressor xrag coconum pcc llmlingua-2 pisco Mistral-7B w/o BGE retrieval Mistral-7B w/ BGE retrieval SCP-Mistral-7B Phi4-mini w/o BGE retrieval Phi4-mini w/ BGE retrieval SCP-Phi4-mini Autocompressor xrag coconum pcc llmlingua-2 pisco Mistral-7B w/ BGE retrieval SCP-Mistral-7B Phi4-mini w/ BGE retrieval SCP-Phi4-mini"
        },
        {
            "title": "Normal",
            "content": "17.24 32.35 24.12 31.38 47.53 54.39 35.01 54.58 57.05+2.47 55.56+0.98 54.64+0.06 54.18-0.40 53.36-1.22 52.84-1.74 18.77 48.14 53.31+5.17 51.96+3.82 49.30+1.16 45.72-2.42 43.09-5.05 42.73-5.41 14.61 25.16 21.48 22.29 37.05 41.94 27.55 42.94 45.09+2.15 43.72+0.78 43.52+0.58 42.17-0.77 41.37-1.57 40.00-2.94 21.10 37.78 42.36+4.58 40.86+3.08 38.62+0.84 35.75-2.03 33.92-3.86 34.02-3."
        },
        {
            "title": "Oracle",
            "content": "29.47 42.60 25.61 49.62 63.99 73.44 71.64 76.50+4.86 75.48+3.84 73.77+2.13 71.90+0.26 69.96-1.68 68.82-2.82 66.10 73.67+7.57 73.17+7.07 69.90+3.80 64.72-1.38 60.44-5.66 60.12-5.98 19.24 30.21 21.72 34.56 52.42 66.53 70.77 73.81+3.04 70.79+0.02 69.51-1.26 66.22-4.55 62.09-8.68 59.93-10.84 64.06 72.41+8.35 70.26+6.20 65.32+1.26 57.79-6.27 51.52-12.54 51.54-12.52 1x 128x 16x 16x 4x 16x 1x 1x 4x 16x 32x 64x 128x 256x 1x 1x 4x 16x 32x 64x 128x 256x 1x 128x 16x 16x 4x 16x 1x 4x 16x 32x 64x 128x 256x 1x 4x 16x 32x 64x 128x 256x 3.81 3.64 3.52 3.43 9.02 10.09 5.38 8.94 10.34+1.40 10.55+1.61 10.55+1.61 10.17+1.23 10.26+1.32 10.38+1.44 4.05 8.11 8.73+0.62 8.61+0.50 7.70-0.41 6.50-1.61 6.87-1.24 6.87-1.24 7.16 7.03 3.64 18.25 27.47 33.80 45.72 46.26+0.54 43.15-2.57 38.31-7.41 34.96-10.76 30.86-14.86 26.19-19.53 37.07 40.13+3.06 38.39+1.32 31.77-5.30 23.54-13.53 19.28-17.79 19.61-17. 19.89 28.79 24.48 19.47 44.35 44.88 38.45 44.24 46.94+2.70 46.00+1.76 46.58+2.34 47.03+2.79 46.40+2.16 46.31+2.07 30.26 35.11 45.22+10.11 44.27+9.16 43.71+8.60 43.96+8.85 43.70+8.59 43.75+8.64 26.74 30.94 24.63 27.56 53.92 60.45 68.83 70.48+1.65 66.16-2.67 64.54-4.29 61.55-7.28 59.08-9.75 56.50-12.33 52.69 64.22+11.53 63.15+10.46 58.52+5.83 53.11+0.42 50.29-2.40 50.33-2.36 13.89 22.48 18.40 19.14 34.49 37.83 26.6 37.67 39.86+2.19 38.96+1.29 38.82+1.15 38.39+0.72 37.85+0.18 37.38-0.29 18.55 32.28 37.40+5.12 36.42+4.14 34.83+2.55 32.98+0.70 31.90-0.38 31.84-0.44 20.65 27.70 18.90 32.50 49.45 58.55 64.24 66.76+2.52 63.90-0.34 61.53-2.71 58.66-5.58 55.50-8.74 52.86-11.38 54.98 62.61+7.63 61.24+6.26 56.38+1.40 49.79-5.19 45.38-9.60 45.40-9.58 Table 6 Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the absolute performance change () of our method under different compression rates relative to its corresponding w/ retrieval setting. CR denotes compression rate. similar documents from the Wikipedia-2021 corpus using dense retrieval. Given each query and its retrieved documents, the model is prompted to generate the corresponding answer, which serves as the gold target for instruction tuning. For end-to-end training, we use the training set of each benchmark individually, except for MuSiQue. Since MuSiQue is more challenging and difficult to converge when trained alone, we construct its training set by combining the training samples from HotpotQA, 2Wiki, and MuSiQue. For each query, we first obtain its positive documents, and then retrieve additional documents from the corpus using the BGE-large-en-v1.5 model until we collect total of 20 candidates. This ensures that the gold answer remains inferable from at least one of the selected documents during end-to-end optimization. Table 10 summarizes the data statistics."
        },
        {
            "title": "Models",
            "content": "CR"
        },
        {
            "title": "Retrieval Mode",
            "content": "NQ F1 EM HotpotQA EM F"
        },
        {
            "title": "Musique",
            "content": "F1 EM 2Wiki F1 EM Prompting-based Method 44.71 41. 42.31 44.69 40.60 38.07 41.27 37.14 24.36 20.11 20.77 16.78 42.58 41. 39.61 38."
        },
        {
            "title": "Retrieval tuning",
            "content": "42.67 44.31 37.47 37.29 42.72 40.53 38.72 37.15 24.96 20.36 17.34 18. 38.26 39.66 32.17 39.02 LLM Fine-tuning GenGround* In-context RAG* RECOMP* DPA-RAG* RetRobust* ChatQA* Self-RAG* DDR-RAG* DRO-Mistral-7B* CLaRa-Mistral-7B CLaRa-Phi4-mini CLaRa-Mistral-7B CLaRa-Phi4-mini 1x 1x 1x 1x 1x 1x 1x 1x 1x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x"
        },
        {
            "title": "Oracle",
            "content": "37.03 23.64 29.74 43.82 34.54 31.63 40.54 44.60 27.30 End-to-end optimization 35.44 47.87 28.76 51.01 40.74 42.41 Pretraining-initialized 40.62 41.75 40.68 41.58 42.04 42.90 77.80 73.81 72.03 68.18 68.66 66.85 39.69 31.93 30.70 28.88 29.26 29.92 61.07 65.09 62.35 58.51 56.13 54.58 31.21 32.24 31.36 31.38 31.78 32.58 70.52 65.74 63.65 59.56 59.25 57.17 30.41 23.38 21.78 19.91 19.85 20.53 52.15 55.96 52.07 47.08 44.52 43.24 39.53 44.37 41.84 41.62 42.26 41.32 77.66 69.57 70.91 67.64 66.51 63.43 37.10 37.21 37.14 34.98 34.73 34.10 59.82 57.87 51.06 55.47 52.68 51.62 Instruction-tuned-initialized"
        },
        {
            "title": "Oracle",
            "content": "48.21 50.89 49.72 50.91 51.41 50.57 75.63 71.54 69.75 68.17 66.95 65.60 41.86 42.17 39.14 36.91 36.26 37.58 55.53 58.62 61.15 57.63 56.26 54.55 38.16 41.02 39.88 41.07 41.27 40.39 67.64 63.29 65.17 59.04 57.61 55.65 31.96 32.61 29.45 27.09 26.34 27.48 45.94 48.90 50.45 46.62 44.77 43.09 45.93 47.62 45.73 45.68 44.63 43.02 69.66 71.17 68.87 66.64 64.09 61.79 39.44 42.77 42.59 38.90 36.39 35.84 55.28 56.47 56.31 52.39 50.74 50.00 35.59 33.40 16.30 18.16 17.05 21.50 18.11 16.64 9. 39.11 31.90 27.33 38.65 26.80 23.52 31.71 40.37 10.57 25.32 13.54 21.36 38.40 43. 35.44 42.12 29.54 33.72 31.26 31.12 31.78 30.44 64.83 56.76 57.07 53.22 52.30 49.08 27.33 27.22 26.99 25.07 24.95 24.62 46.99 45.26 38.33 41.62 38.49 37.77 35.12 36.67 34.85 34.74 33.88 32.26 56.92 57.54 55.20 52.87 54.63 47.74 29.32 32.00 31.47 28.02 26.44 25.73 43.24 43.45 43.13 38.94 37.68 36.78 14.53 15.36 15.32 14.78 15.53 15.44 41.59 31.15 33.40 28.43 28.44 27.44 15.20 14.30 13.26 13.31 13.07 13.07 25.87 21.09 20.98 23.89 20.74 21.45 17.49 18.01 16.83 16.76 15.75 16.02 33.19 30.77 28.87 27.30 26.11 27.67 15.70 15.84 15.55 14.08 14.70 13.66 25.96 23.07 21.28 22.38 22.27 20.92 6.16 6.99 6.66 6.16 6.37 6.41 30.33 21.18 22.22 17.42 16.67 16.92 6.08 4.84 4.39 4.55 4.01 4.22 15.76 11.25 10.92 12.99 9.85 9. 8.11 8.44 7.82 7.65 7.03 6.99 22.42 20.56 18.45 16.96 15.97 17.05 5.59 6.08 5.71 4.88 5.42 4.92 15.14 12.49 11.29 11.63 11.79 11.01 42.59 43.47 43.23 42.64 41.80 41.96 73.20 65.90 66.32 62.53 64.82 62.96 38.43 40.03 38.15 37.74 36.41 35.98 56.84 55.75 50.68 53.90 49.97 48.46 47.18 44.66 42.57 40.34 40.55 40.10 73.86 60.37 64.38 60.98 62.34 59.40 37.63 36.69 41.47 39.52 37.14 36.26 55.57 56.85 51.21 48.11 47.64 46.85 38.49 39.50 38.98 38.40 37.37 37.60 69.14 61.31 61.12 57.02 58.97 57.35 34.26 35.62 33.82 33.57 32.23 31.61 52.12 50.41 45.41 48.26 44.11 42.63 43.11 40.48 38.41 35.91 36.12 35.77 69.74 55.73 59.32 55.59 56.64 53.75 33.40 32.47 36.68 34.98 32.85 32.11 50.18 51.57 45.70 42.83 42.31 41.66 Table 7 End-to-End QA Performance. * indicates that the results are reported from the DRO paper. CR means compression rate. 20 Models BM25 BGE-reranker RankZephyr-7B Setwise Rank-R1 Sup-Mistral-7B CLaRa-Mistral-7B Sup-Phi4-mini CLaRa-Phi4-mini Sup-Mistral-7B CLaRa-Mistral-7B Sup-Phi4-mini CLaRa-Phi4-mini CR 1x 1x 1x 1x 1x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x R@1 6.57 11.18 16.00 12.60 8.95 31.57 30.66 30.19 29.88 29.02 27.54 32.62 28.45 28.06 27.69 28.17 25.33 24.22 28.16 27.66 27.40 27.45 25.67 8.58 20.94 27.63 29.17 29.63 27.50 28.33 28.20 27.56 25.70 25.05 25.15 24.66 23.66 21.54 20.77 20.07 19.39 22.38 23.85 23.72 22.34 22.80 22.27 3.98 8.15 16.94 17.63 21.90 19. NQ R@3 21.89 33.56 40.20 38.20 30.60 62.34 61.05 60.89 60.38 59.25 56.67 63.71 58.97 59.68 59.11 59.64 55.62 52.18 57.82 57.96 57.76 57.08 54.20 28.21 48.89 58.37 60.08 60.54 57.90 58.52 57.24 56.70 54.11 53.12 52.61 55.27 52.43 49.50 49.52 47.19 46.91 49.29 52.04 52.09 49.46 50.64 49.31 17.02 25.82 42.64 44.29 50.34 45.89 R@1 25.12 28.53 38.00 30.40 24.87 HotpotQA R@3 48.86 67.91 63.70 60.30 56.56 R@5 62.09 85.93 75.90 74.60 73.06 Pretraining-initialized 82.84 80.45 83.80 84.46 83.76 83.81 96.21 87.85 90.32 89.54 84.70 83.34 78.94 79.18 80.30 81.34 81.28 78.62 60.01 64.38 70.00 78.44 75.93 75. 46.33 44.00 45.69 45.92 44.79 44.50 47.07 42.01 43.84 43.17 40.70 39.68 39.85 42.18 41.66 41.40 41.69 40.82 18.38 23.90 28.64 36.25 37.94 37.10 71.76 69.66 73.76 74.35 73.76 73.89 90.90 77.80 81.32 79.35 73.60 71.38 66.57 67.66 69.12 70.39 70.47 67.54 42.90 49.17 55.48 65.67 63.96 63.30 R@5 35.99 47.78 52.40 52.10 46.72 74.96 73.30 73.93 73.53 72.33 70.00 76.38 71.88 73.21 72.79 73.30 69.79 66.15 71.13 71.69 72.01 70.61 67.85 44.61 64.34 72.37 73.83 74.02 71.58 Instruction-tuned-initialized 71.96 69.57 69.58 66.80 65.73 65.27 69.82 66.79 65.13 64.71 61.98 62.66 62.81 65.97 65.43 63.40 63.30 62.15 31.76 40.84 58.30 59.33 65.14 60. 42.40 42.24 44.88 44.94 45.14 44.60 25.73 35.85 35.78 33.65 32.24 29.71 39.91 40.92 41.53 40.93 40.47 39.76 13.19 18.00 31.30 32.09 30.53 31.18 65.93 67.91 71.02 73.32 74.56 73.95 52.20 67.69 65.94 63.93 61.83 56.66 66.12 65.97 67.81 68.95 67.99 65.61 33.04 41.31 58.02 59.98 55.15 56.05 77.40 79.33 81.54 84.24 85.18 85.18 68.63 81.13 80.11 78.47 77.36 71.58 77.59 77.75 79.44 80.57 80.36 78.39 48.40 57.10 71.88 74.35 68.30 69.80 R@1 13.00 18.32 21.04 18.50 16.77 32.35 29.44 31.34 31.32 29.42 29.13 30.45 22.57 28.22 24.96 20.74 19.58 26.13 28.48 27.87 28.11 27.62 27.03 10.56 15.07 16.43 16.38 17.67 17.67 27.87 26.80 29.29 29.02 28.11 28.69 18.06 18.95 16.75 16.13 13.77 15.84 26.11 26.47 27.41 26.17 27.05 26.54 7.60 7.82 13.71 15.93 12.81 12. Musique R@3 29.12 42.45 40.61 40.30 38.49 50.30 45.07 48.20 46.91 45.97 46.33 59.99 44.34 56.19 49.13 41.29 38.57 42.16 43.75 42.92 44.27 42.85 41.45 26.81 30.79 34.31 33.36 34.90 34.13 45.47 42.33 43.86 44.21 43.18 44.32 37.67 37.21 36.93 34.13 29.70 31.92 41.34 42.17 41.40 42.17 41.82 41.06 21.09 18.24 29.99 32.42 27.30 26.19 R@5 39.13 54.13 50.65 51.60 51.68 60.02 54.63 57.86 55.93 55.21 56.04 72.46 58.83 70.19 62.37 54.86 51.33 51.27 53.55 51.16 53.46 51.41 49.93 40.08 42.49 46.55 46.23 46.39 44.98 55.39 50.68 52.22 53.28 52.40 53.25 50.27 50.71 50.48 46.41 41.93 43.57 50.39 51.07 50.23 51.14 50.21 48.39 31.03 28.19 40.74 44.22 37.66 37. R@1 15.23 22.21 33.40 26.20 24.27 42.38 40.03 42.08 42.13 40.65 39.99 34.37 32.04 32.36 32.84 31.80 30.81 37.43 39.96 39.11 38.27 37.85 36.78 17.05 21.16 30.30 27.74 30.79 31.79 39.90 39.22 42.21 42.09 42.27 41.89 28.51 12.85 17.14 17.40 19.43 20.12 37.79 38.52 39.79 38.47 38.95 37.68 20.40 21.16 29.85 29.77 25.54 27.54 2Wiki R@3 37.06 54.95 61.55 57.99 59.02 74.51 65.48 68.09 68.26 65.61 64.87 68.08 67.12 64.96 63.45 63.41 56.56 62.27 55.62 61.05 61.67 63.81 60.32 38.03 46.83 58.89 53.69 58.53 60.53 62.02 59.77 55.47 54.68 58.47 52.64 57.74 32.17 39.59 38.10 42.62 42.26 61.89 53.51 51.25 49.72 50.31 49.09 39.25 40.80 53.24 56.59 47.53 51. R@5 51.40 68.32 74.69 71.61 77.02 85.02 77.27 79.41 79.52 76.97 76.22 79.13 82.50 81.01 79.40 79.48 72.59 73.49 66.99 71.51 73.13 74.89 71.13 52.63 64.10 75.98 69.63 74.64 76.90 74.35 70.38 66.56 66.03 69.94 63.77 73.00 47.90 55.75 51.61 57.31 56.29 72.96 64.79 60.99 58.88 59.78 56.86 52.00 51.96 66.04 71.59 61.28 64.10 Table 8 Retrieval performance (Recall@1/3/5) on the Mistral-7B and Phi-4-mini model across different reranking methods under various compression ratios (CR) and initialization settings on four QA datasets. Supdenotes models trained with labeled data using contrastive learning for the reranker. 21 B.2 Models For document retrieval, we employ BGE-large-en-v1.51 as the retriever for coarse ranking. Unless otherwise specified, we adopt Mistral-7B-Instruct-v0.22 as the default backbone for all experiments. Additionally, we evaluate the proposed method on Phi-4-mini-instruct3 to assess its generalization across different model families. On top of the backbone model, we implement three LoRA modules: compressor, query reasoner, and generation module. B.3 Evaluation Metrics Following previous studies (Cheng et al., 2025; Louis et al., 2025a), we evaluate the compressor using the Cover Exact Match (ACC) metric, which measures whether the ground-truth answer is included in the generated output. For the reranker, we report Recall@k (k {1, 3, 5}), defined as the proportion of positive documents appearing within the top-k ranked results. For the generation model, we adopt two standard QA metrics: Exact Match (EM) and F1. The EM score measures the percentage of predicted answers that exactly match the gold answers, while the F1 score computes the token-level overlap between predictions and references, reflecting the harmonic mean of precision and recall."
        },
        {
            "title": "Hyperparameter",
            "content": "LR Scheduler Optimizer Epochs LoRA Layers (r) LoRA Rank (r) LoRA Dropout LoRA Alpha LoRA Rank (r) Warmup Ratio Max Gradient Norm Documents Max Tokens Compression learning λ Batch Size Learning Rate (LR) End-to-end learning Batch Size Learning Rate (LR)"
        },
        {
            "title": "Value",
            "content": "cosine AdamW 1 all-linear 16 0.1 32 16 0.03 1.0 256 0.1 128 1 104 32 5 106 Table 11 Hyperparameter settings used in our experiments. B.4 Implementation Details Table 11 summarizes the hyperparameters used for all LoRA modules and training stages. Specifically, we employ separate configurations for the compression learning, and end-to-end training phases. During end-to-end learning, both the query reasoner and the generator are initialized from the compressor-trained checkpoints. Following Shi et al. (2025), for each query x, we first retrieve the top-20 documents from the corpus using BGE-large-en-v1.5, obtain their corresponding compressed representations, and then pass them along with the query into the query reasoner to identify the top-k (k = 5) ranked documents, which are subsequently fed into the generator. For corpus preprocessing, each document is segmented into chunks of 256 tokens. We extensively evaluate our model under different compression ratios ρ {4, 16, 32, 64, 128, 256}, where the number of memory tokens is computed as 256/ρ. All experiments are conducted on 8 100 H100 GPUs. Unless otherwise stated, all training runs are performed for single epoch. 1https://huggingface.co/BAAI/bge-large-en-v1.5 2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 3https://huggingface.co/microsoft/Phi-4-mini-instruct 22 B.5 Baselines In this section, we provide detailed descriptions of all baseline methods used for comparison under different experimental settings. We categorize them into three groups: (1) compression baselines, (2) retrieval and reranking baselines, and (3) end-to-end QA baselines. B.5.1 Compression Baselines AutoCompressor. (Chevalier et al., 2023) This method segments long document into chunks, appends <Sum> token at the end of each chunk, and trains the model to produce fixed number of summary vectors. During training, the model is fine-tuned with standard language modeling cross-entropy loss, with stop-gradient applied to past summary vectors. At inference time, the model first compresses and then reuses the summaries, achieving efficient long-context reasoning at significantly reduced cost. XRAG. (Cheng et al., 2025) XRAG treats retrieved document embeddings as an additional retrieval modality, mapping them into the language models representation space via lightweight projection layer. This enables retrieval-augmented generation with as few as single document token. XRAG adopts two-stage training strategy: (1) Paraphrase Pretraining to align document embeddings with textual semantics, and (2) ContextAware Instruction Tuning with self-distillation to optimize retrieval utilization. Only the projection layer is trained, while both the retriever and language model remain frozen, achieving compression ratios up to 178. COCOM. (Rau et al., 2025) COCOM maps each retrieved document into compact sequence of context embeddings (e.g., compressing hundreds of tokens into 4128 embeddings), which reduces input length and accelerates generation. It jointly trains compressor and generator with two objectives: (i) an auto-encoding reconstruction loss to preserve semantic information, and (ii) conditional generation loss to ensure highquality answers from compressed contexts. The framework also supports multi-document compression and cross-document fusion, and offers lightweight variant (COCOM-light) using BERT as the compressor. PCC. (Dai et al., 2025) PCC consists of an encoder and transformer-based converter. The encoder extracts compact semantic representations, while the converter adjusts their dimensionality and semantics through two MLP layers so that the compressed memory can be directly fed into any LLM. The model is pretrained (with the LLM frozen) using combination of auto-encoding reconstruction and auto-regressive completion tasks to retain generation-relevant information. Domain-specific fine-tuning is then performed on limited data for RAG QA, ICL reasoning, and dialogue tasks. LLMLingua-2. (Pan et al., 2024) LLMLingua-2 constructs large-scale extractive compression dataset using GPT-4-generated high-fidelity summaries. It formulates compression as token-level binary classification problem (keep or remove), where bidirectional Transformer encoder (e.g., XLM-RoBERTa) estimates the retention probability of each token. Tokens are ranked by their probabilities to achieve 25 compression while maintaining semantic completeness. PISCO. (Louis et al., 2025a) PISCO introduces trainable memory tokens appended to the document, jointly fine-tuned with LoRA adapters to compress text by up to 1/16 of its original length. It employs sequencelevel knowledge distillation (SKD) from teacher-generated answer sequences to ensure consistency between compressed and uncompressed outputs. B.5.2 Retrieval and Reranking Baselines BM25. classical lexical retrieval method that scores each document based on term frequency, inverse document frequency, and document length normalization. BGE-Reranker. (Chen et al., 2023) recent large-scale, general-purpose reranker that directly predicts the relevance score between query and each candidate document, used to reorder initial retrieval results. 23 RankZephyr. (Pradeep et al., 2023) 7B-parameter open-source reranker distilled in two stages from RankGPT3.5 and RankGPT-4. It integrates variable-window training, input order shuffling, and teacher-guided ranking data, achieving robust performance under varying document counts and ranking conditions. During inference, RankZephyr performs iterative sliding-window ranking using prompt-decoder style generation. Setwise. (Zhuang et al., 2024) Unlike pairwise reranking, Setwise compares multiple candidate documents in single inference step, greatly reducing LLM calls and prompt length. It leverages classical sorting algorithms (e.g., heap or bubble sort) and directly estimates relevance probabilities from model logits, avoiding step-by-step list generation. Rank-R1. (Zhuang et al., 2025) reinforcement learning-based reranking framework that enhances LLM reasoning capabilities for document ranking. Built upon the Setwise ranking paradigm, it introduces explicit reasoning instructions before answer generation and optimizes the model via Group Relative Policy Optimization (GRPO). The model is trained only with queries and relevance labels, and receives reward signals based on prediction correctness and format compliance. B.5.3 End-to-End QA Baselines GenGround. (Shi et al., 2024) This method decomposes complex questions into sub-questions using the models internal knowledge, then refines preliminary answers via retrieved documents for evidence grounding. It further introduces Instructional Grounding Distillation (IGD), which distills grounding trajectories from ChatGPT into smaller open models such as Mistral-7B. In-Context RAG. Selects the top-k retrieved documents using the BGE Reranker and feeds them as context to the LLM for direct answer generation. ReComp. (Xu et al., 2024) ReComp retrieves relevant documents and compresses them into concise, queryrelated summaries via either an extractive or generative compressor. These summaries are then used as context for answer generation. Training jointly optimizes both retriever and compressor, allowing selective retrieval when documents are unhelpful. DPA-RAG. (Dong et al., 2025) This method introduces preference-aligned retrieval and generation. It first constructs preference data by analyzing LLM responses under various retrievals and then aligns both reranker and generator through hybrid of point-wise, pair-wise, and contrastive training objectives. Improves robustness of RAG systems through two mechanisms: (i) using an RetRobust. (Yoran et al., 2024) NLI model to filter irrelevant retrieved texts, and (ii) fine-tuning with mixed relevant/irrelevant retrieval samples so that the model learns when to utilize or ignore retrieval information. ChatQA. (Liu et al., 2025) context-augmented instruction-tuned model that integrates multi-source conversational and instruction data to enhance reasoning and refusal capabilities. It also fine-tunes dense retriever on multi-turn QA data, replacing traditional query rewriting modules. Incorporates reflection tokens (e.g., need retrieval?, retrieved relevant?, supSelf-RAG. (Asai et al., 2024) ported answer?) so the model can self-assess and adaptively decide when to retrieve external knowledge. Training combines GPT-4generated annotated data with self-reflective labeling to enable dynamic retrieval and self-critique during inference. RAG-DDR. (Li et al., 2025) Employs Differentiable Data Rewards (DDR) to achieve fully end-to-end optimization of RAG systems. It uses rollout-based system rewards and aligns retrieval and generation through Direct Preference Optimization (DPO). 24 DRO. (Shi et al., 2025) Models document ordering as latent variable and alternates between inference and optimization using variational EM framework. The E-step estimates document order distributions via importance sampling, while the M-step jointly updates the selector and generator based on weighted likelihood maximization."
        },
        {
            "title": "C Pretraining Data Quality",
            "content": "To ensure the quality of the constructed pretraining data, we conducted manual evaluation. We randomly sampled 200 examples for each output type, resulting in total of 600 samples, which were independently assessed by one of the authors. The evaluation results indicate that, thanks to our rigorous filtering process, almost all generated samples successfully cover the key information contained in the source documents. Only 21 instances exhibited mild hallucinations, where the model introduced information not present in the original text. This demonstrates that the synthesized data are of high factual and semantic quality, providing reliable foundation for compression pretraining. Models Corpus size NQ Mistral-7B Phi4-mini Mistral-7B Phi4-mini 0.5M 1M 2M 0.5M 1M 2M 0.5M 1M 2M 0.5M 1M 2M 53.38 54.82 54.64 48.82 48.40 49.30 70.33 74.08 73.77 68.31 69.41 69. 10.30 10.63 10.55 7.78 7.73 7.70 HotpotQA Musique Normal 41.40 43.71 43.52 38.53 38.47 38.62 Oracle 62.47 68.88 69.51 64.41 64.42 65.32 29.16 38.97 38.31 29.25 31.32 31.77 2Wikiqa Average 46.67 46.90 46.58 43.57 43.82 43. 57.97 63.91 64.54 58.22 58.00 58.52 37.94 39.02 38.82 34.67 34.61 34.83 54.98 61.46 61.53 55.05 55.79 56.38 Table 12 Instruction tuning performance of Mistral-7B and Phi4-mini models under different pretraining corpus sizes (0.5M, 1M, 2M). Results are reported on four QA datasets under both Normal and Oracle retrieval settings with fixed compression ratio (CR = 32)."
        },
        {
            "title": "D Pretraining Data Scaling",
            "content": "To investigate how the number of pretraining samples affects the performance of the compressor, we train models with varying amounts of pretraining data and assess their performance after instruction tuning on four QA datasets. The results are illustrated in Table 12. We observe that enlarging the pretraining corpus generally leads to consistent performance improvements across all datasets and both retrieval settings. For instance, under the Normal setting, the Mistral-7B model improves its average score from 37.94 to 39.02 as the corpus size increases from 0.5M to 1M, while the performance remains stable when further scaled to 2M. similar trend can be observed in the Oracle setting, where the model achieves an average gain of over 6 points when moving from 0.5M to 1M, indicating that additional pretraining data enhances the compressors ability to preserve more task-relevant information. For the smaller Phi4-mini model, the improvements are relatively modest, suggesting that model capacity may constrain the benefits of scaling pretraining data. Overall, these findings demonstrate that moderate expansion of pretraining data contributes positively to downstream QA performance, while extremely large pretraining sets bring diminishing returns."
        },
        {
            "title": "E Training Curves",
            "content": "Figures 5 present the validation loss curves during the compression pretraining stage across different compression ratios. clear trend emerges: as the compression ratio increases, the validation loss rises for both models. This effect is more pronounced for Phi4-mini, where losses at ratios of 128 and 256 diverge sharply. In contrast, 25 Figure 5 Validation loss curves during the compression pretraining stage under different compression ratios (CR) on the Phi-4-mini (left) and Mistral-7B (right) models. Figure 6 Validation trends of recall and evaluation loss during the end-to-end training stage under different compression ratios (CR) on the NQ (top) and Musique (bottom) datasets. Mistral-7B exhibits relatively uniform loss gaps across compression ratios. We hypothesize that this difference arises because of capacity. Phi4-mini, with fewer parameters, has limited representational ability. At very high compression levels (e.g., CR=128), excessive information loss leads to semantic degradation and steep rise in validation loss. Figure 6 presents the validation curves during end-to-end training on the NQ and Musique datasets. Recall scores consistently increase while evaluation losses steadily decrease, indicating stable and effective optimization. Higher compression ratios generally yield lower recall and higher loss, mirroring the trends observed during the compression pretraining stage."
        },
        {
            "title": "F More Analysis",
            "content": "F.1 Effect of Freezing the Compressor and Query Reasoner We investigate the effect of limiting the fine-tuning scope to the generator module while freezing both the compressor and query reasoner. Specifically, we examine two representative compression settings, CR=32 and CR=128, and compare model performance when only the generator is fine-tuned during both the instruction tuning and end-to-end QA training stages. The results are shown in Table 13 and 14. During the compression learning and instruction tuning stages, we observe that fine-tuning the compressor alongside the generator brings only marginal improvements. For example, under the Normal setting, the average gain of full finetuning over generator-only tuning is less than 2.0% across most datasets. Considering that in Section 4.4, the instruction-tuned compressor tends to degrade retrieval performance due to its focus on answer-centric representations, promising future direction is to explore how to effectively extract task-relevant information from compressed representations without directly fine-tuning the compressor itself. In contrast, during the 26 end-to-end learning stage, fine-tuning the query reasoner proves to be more beneficial. trainable retrieval module enables the model to identify more relevant documents and provide stronger contextual grounding for the generator. For instance, under the Oracle setting with compression ratio of 32, the F1 score of Mistral-7B improves from 52.54% to 70.91% when jointly fine-tuning both the query reasoner and generator. This highlights the crucial role of query reasoner in enhancing overall QA performance within our unified training framework. Models CR NQ HotpotQA Musique Normal 2Wiki Average Generator-only Mistral Mistral Phi Phi Full finetune Mistral Mistral Phi Phi Generator-only mistral mistral phi phi Full finetune mistral mistral phi phi 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 52.26 50.72 45.91 38.83 54.64 53.36 49.30 43. 72.78 66.93 65.65 52.87 73.77 69.96 69.90 60.44 42.66 40.10 37.70 32.30 43.52 41.37 38.62 33.92 Oracle 67.48 59.66 62.76 47.51 69.51 62.09 65.32 51. 10.43 9.14 6.95 6.50 10.55 10.26 7.70 6.87 34.38 25.94 27.60 17.38 38.31 30.86 31.77 19.28 45.79 45.52 42.97 42.53 46.58 46.40 43.71 43. 60.89 58.19 56.46 48.98 64.54 59.08 58.52 50.29 37.78 36.37 33.39 30.04 38.82 37.85 34.83 31.90 58.88 52.68 53.12 41.68 61.53 55.50 56.38 45. Table 13 Instruction tuning results of Mistral and Phi models under different fine-tuning scopes (generator-only vs. finetune-both), retrieval modes (Normal vs. Oracle), and compression ratios (CR = 32, 128) on four QA datasets. Models CR HotpotQA EM F1 Normal 2Wiki EM Generator-only Mistral Mistral Phi4 Phi4 Full finetune Mistral Mistral Phi4 Phi4 Generator-only Mistral Mistral Phi4 Phi4 Full finetune Mistral Mistral Phi4 Phi4 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 32x 128x 38.40 38.26 32.91 31.42 41.84 42.26 37.14 34.73 Oracle 52.54 51.60 45.91 39.46 70.91 66.51 51.06 52.68 28.24 28.26 23.51 21. 31.26 31.78 26.99 24.95 40.11 39.06 34.20 28.06 57.07 52.30 38.33 38.49 39.93 41.34 35.99 35.54 43.23 41.80 38.15 36.41 45.43 44.64 40.33 37. 66.32 64.82 50.68 49.97 35.80 37.29 32.14 31.32 38.98 37.37 33.82 32.23 40.98 40.22 36.12 32.57 61.12 58.97 45.41 44.11 Table 14 End-to-end QA performance of Mistral and Phi models under different fine-tuning scopes (generator-only vs. finetune-both), retrieval modes (Normal vs. Gold), and compression ratios (CR = 32, 128) on HotpotQA and 2WikiQA datasets. F.2 Retrieval number generalization We further explore the impact of varying the number of retrieved documents (top-k) during testing in our end-to-end training framework. During training, the model is consistently trained with the top-5 retrieved documents, while at test time, we vary from 1 to 10 to examine the models sensitivity to retrieval size. The results are presented in Figure 7. As shown in the figure, the F1 score generally exhibits rapid 27 Figure 7 Performance of varying the number of retrieved documents (k) during testing on different QA datasets. increase followed by gradual decline as increases. However, the performance drop remains relatively small, indicating that our trained query reasoner and generator demonstrate good generalization with respect to the number of retrieved documents during inference. F.3 Effect of Query Reasoner Initialization We evaluate the effect of initializing the query reasoner with the pretrained compressor parameters versus random initialization on the HotpotQA and 2Wiki datasets, as shown in Table 15. The results demonstrate that compressor-initialized models consistently outperform their randomly initialized counterparts across all settings. This performance gain (e.g., from 66.84%70.91% F1 and 62.68%66.32% F1 on HotpotQA and 2Wiki, respectively) indicates that the pretrained compressor provides strong prior for learning effective query reasoning representations, as it already encodes semantic relationships between queries and document content during the compression pretraining stage. Model Mistral-7B w/ Compressor Init. Mistral-7B w/ Compressor Init. Mistral-7B w/ Compressor Init. Mistral-7B w/ Compressor Init. CR Retrieval Mode 32x 32x 32x 32x 128x 128x 128x 128x Normal Normal Oracle Oracle Normal Normal Oracle Oracle HotpotQA EM F1 29.12 39.48 31.26 41.84 52.91 66.84 57.07 70.91 27.38 37.25 31.78 42.26 48.37 62.06 52.30 66.51 2Wiki F1 39.90 43.23 62.68 66.32 38.55 41.80 60.63 64.82 EM 35.79 38.98 57.55 61.12 34.69 37.37 54.87 58.97 Table 15 End-to-End QA Performance with Randomly Initialized vs. Compressor-Initialized Query Reasoner F.4 Efficiency Analysis We evaluate the inference efficiency of our framework under different compression ratios. Specifically, for each query, we retrieve 20 candidate documents, compress them into 5 document representations using the compressor, and then generate the final answer based on these 5 compressed representations and the query. The average inference time for each stage is reported in Table 16. All timing statistics are measured on single NVIDIA H100 GPU with 80GB memory. As shown in the results, decoding with compressed representations takes only about 40% of the time required when using full-text documents. Although compressing 20 documents is relatively time-consuming, this step can be performed offline; hence, it does not affect real-time inference latency during query answering. This makes the overall computational cost acceptable for practical deployment. We also observe that for the Mistral model, compression time tends to decrease as the compression ratio increases, while both decoding and query retrieval times remain relatively stable across different compression settings. 28 Models Mistral-7B Mistral-7B Mistral-7B Mistral-7B Mistral-7B Mistral-7B Mistral-7B Phi4-mini Phi4-mini Phi4-mini Phi4-mini Phi4-mini Phi4-mini Phi4-mini CR Pure text 4x 16x 32x 64x 128x 256x Pure text 4x 16x 32x 64x 128x 256x Compression Time Query Time Decoding Time 1092.29 922.85 904.22 893.76 876.99 835.87 674.78 574.46 561.17 604.89 594.55 789.49 99.69 94.17 92.16 95.14 95.24 90.76 94.34 89.53 84.35 85.33 91.73 99.47 1290.57 532.73 502.78 514.75 521.09 518.41 521.03 870.29 342.05 343.01 358.04 354.77 360.23 354.87 Table 16 Average inference time (in milliseconds) for compression, retrieval, and decoding across different compression ratios (CR) on Mistral-7B and Phi4-mini models."
        },
        {
            "title": "G Fidelity and Grounding Analysis",
            "content": "In this section, we aim to understand how much essential information is retained in our compressed representations, and to what extent the generated answers remain grounded to the input documents and queries after both compression learning and end-to-end training. G."
        },
        {
            "title": "Information Preservation",
            "content": "During compression representation pretraining, we include paraphrasing objective that allows the generation model to reconstruct the original text from the compressed representation. We consider two evaluation settings: (1) unseen data, consisting of positive documents of downstream QA tasks that were not used in pretraining, and (2) seen data, where we randomly sample 4,000 documents from the pretraining corpus. We evaluate the reconstruction quality using several metrics: BERTScore (Zhang* et al., 2020) (which measures semantic similarity between texts), ROUGE-1 and ROUGE-L (which capture lexical overlap), and following Łajewska et al. (2025), we also compute the entity preservation ratio, which measures the proportion of entities from the input text that are preserved in the reconstructed text4. The results are shown in Table 17. We observe that our model achieves high BERTScore of nearly 90%, which remains stable across different compression ratios. This indicates that the compressed representations successfully retain most of the semantic information from the original text. For ROUGE-1, ROUGE-L, and entity preservation, the model also maintains relatively high scoresover 50% on average. We further observe that as the compression ratio increases, the lexical overlap and entity preservation metrics gradually decline, suggesting that fewer memory tokens make it harder to reconstruct the exact surface form of the original text. However, the consistently high semantic similarity scores imply that the key meaning is preserved. This phenomenon may indicate that when using fewer memory tokens, the model tends to generate paraphrased expressions to maintain the original semantics. We leave further exploration of this linguistic compression behavior for future work. 4Entity extraction is performed using the SpaCy library."
        },
        {
            "title": "Models",
            "content": "Mistral-7B Phi4-mini CR 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x BERT 90.67 90.63 90.56 90.28 89.84 89.19 90.93 90.77 90.36 89.53 88.26 88.13 Seen Data R-L R-1 40.12 55.88 40.33 56.12 40.10 56.21 38.86 55.54 36.56 54.12 33.12 51.75 42.16 58.48 41.49 58.20 39.40 57.04 35.40 54.27 29.65 49.30 29.10 49. Entity 54.78 54.78 53.91 51.45 47.75 42.12 57.86 56.28 52.38 45.28 34.98 34.27 BERT 91.45 91.43 91.39 91.24 91.00 90.51 91.70 91.66 91.42 90.84 89.61 89.52 Unseen Data R-L R-1 44.09 59.74 44.10 59.97 43.98 60.28 43.42 60.09 42.48 59.61 39.59 57.89 45.10 62.00 45.31 62.20 44.34 61.71 41.72 60.20 35.58 55.68 35.22 55.27 Entity 60.04 59.88 59.33 58.26 55.75 52.38 63.14 62.22 59.64 54.47 43.89 43.61 Table 17 Evaluation of information preservation under different compression ratios (CR) on seen and unseen documents using BERTScore, ROUGE, and entity preservation. NQ Musique 2Wiki Average Models CR Retrieval Mistral-7B Phi4-mini Mistral-7B Phi4-mini 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x 4x 16x 32x 64x 128x 256x Normal Oracle Normal Oracle Normal"
        },
        {
            "title": "Oracle",
            "content": "Faith 81.57 75.85 72.65 67.49 65.99 64.74 86.73 81.13 79.34 74.43 74.63 71.13 79.45 74.99 66.58 59.66 60.26 57.65 82.51 81.00 75.42 68.72 63.49 61.36 54.67 52.13 49.33 50.60 49.03 50.90 69.55 69.57 65.97 63.90 68.57 65.60 49.50 41.57 41.07 35.98 38.47 37.50 65.13 66.45 60.20 57.35 55.30 52.80 HotpotQA Fc Fc Pretraining-initialized Faith Faith 11.80 13.58 11.86 10.85 10.35 12.34 18.72 19.19 16.31 16.63 12.60 16.49 10.06 12.84 11.64 9.42 10.57 8.71 18.79 17.55 16.72 15.60 14.84 11.60 8.39 8.97 7.34 8.29 8.67 7.42 9.42 9.74 8.16 8.07 10.40 9.63 7.47 8.13 5.74 8.36 8.18 7.78 8.32 8.81 10.20 7.66 8.99 9.80 67.42 62.50 61.40 57.95 56.50 53.68 83.75 83.16 81.87 78.39 77.17 73.69 62.46 61.45 59.22 50.44 51.83 45.57 82.88 81.73 74.79 71.63 66.46 64.48 55.80 49.64 51.35 46.55 44.52 40.79 67.67 63.89 65.27 55.89 55.14 51.35 50.57 51.61 45.44 39.52 35.08 33.79 60.67 63.38 54.49 49.55 44.17 44.01 Instruction-tuned-initialized 12.68 13.12 12.25 11.84 11.77 11.81 23.86 18.85 21.91 18.60 16.19 16.80 12.65 9.82 8.58 11.37 8.06 9.34 14.57 13.28 13.70 13.18 12.80 11.98 36.15 39.91 36.97 36.71 39.58 38.75 68.23 60.83 61.72 56.46 57.86 56.30 35.57 32.59 30.10 29.71 29.52 25.10 52.32 52.31 45.89 48.79 42.43 46. 39.69 39.13 37.64 36.92 36.40 40.02 71.58 67.79 63.60 62.81 60.98 63.81 41.02 30.75 26.85 25.25 26.75 27.89 59.84 62.90 53.13 52.66 51.05 47.28 30.21 37.68 36.08 35.35 36.52 34.95 67.94 60.25 59.77 57.47 55.75 55.46 28.13 27.70 28.64 28.57 30.23 26.48 43.67 46.57 46.27 47.40 42.34 45.35 Fc Faith Fc Faith Fc 9.57 10.34 8.18 10.00 9.50 7.87 13.60 11.05 10.79 10.09 10.47 10.22 11.13 10.04 9.03 8.53 9.47 7.38 11.02 10.54 10.30 7.51 8.38 10.76 11.08 12.55 10.29 10.52 10.56 11.41 37.92 27.60 30.38 21.99 21.43 20.74 13.56 8.21 8.97 11.03 8.25 9.03 22.90 18.58 17.42 18.28 17.51 14.93 56.45 51.02 53.30 44.44 43.26 40.88 80.16 73.96 71.60 67.61 61.62 56.03 51.05 48.59 45.16 39.79 34.72 32.73 76.81 69.61 61.67 56.36 48.50 53.23 5.50 11.13 12.93 13.50 12.13 15.73 38.43 39.90 33.40 27.40 29.40 28.40 6.37 7.87 10.63 10.13 11.03 10.80 18.37 21.87 25.30 18.63 18.67 23.50 5.15 5.04 7.30 5.16 4.54 4.85 6.26 4.12 3.71 4.30 4.00 4.67 5.27 5.17 3.69 4.13 4.61 5.71 4.19 3.03 3.26 2.63 2.07 3. 38.97 36.17 40.87 34.24 35.57 37.67 64.61 60.19 56.57 52.27 55.38 52.97 34.37 35.37 31.36 34.87 31.00 31.74 48.38 47.04 43.71 43.99 44.84 40.64 65.31 59.75 59.67 54.11 52.57 50.02 79.58 75.54 74.52 69.08 67.14 63.05 54.69 59.16 54.10 47.35 45.48 42.44 75.72 73.93 66.59 61.56 55.65 55.77 25.76 28.52 27.65 27.82 27.36 28.35 49.95 47.14 45.26 41.84 42.48 41.57 24.16 21.74 22.23 21.51 21.95 21.03 35.43 37.04 36.37 41.13 32.28 33.41 8.73 9.48 8.67 8.57 8.27 8.12 12.00 11.03 9.74 9.77 9.37 10.25 8.48 9.05 7.52 7.61 8.20 7.39 10.58 9.98 10.12 8.35 8.57 8.82 31.47 31.94 31.44 29.60 30.53 31.96 60.59 54.10 53.07 48.38 48.91 48.46 31.13 26.73 24.32 25.21 23.88 23.44 45.86 45.21 40.04 40.93 38.96 37.29 Table 18 Grounding evaluation of Mistral-7B and Phi4-mini models under different initialization settings (pretraininginitialized vs. instruction-tuned-initialized), retrieval modes (Normal vs. Oracle), and compression ratios (CR). Metrics include Faithfulness (Faith) and Factual Correctness (Fc) across four QA datasets. 30 G.2 Grounding Analysis We further evaluate the grounding quality between the generated answers and the compressed document representations under both compression evaluation and end-to-end evaluation settings. We adopt the RAGAs (Es et al., 2025) package, which implements the LLM-as-a-Judge paradigm for assessing generation quality. Two key metrics are used: faithfulness, which measures whether the generated answer is faithful to the provided context and relevant to the query, and factual correctness, which evaluates whether the answer is factually supported by the context. We employ GPT-4o-mini as the judging model. The results are presented in Table 18. For the compression evaluation, our model achieves consistently high faithfulness scores, particularly when positive documents are included, indicating that the model generates answers more closely aligned with the query. However, the factual correctness scores are comparatively lower, consistent with findings reported in Łajewska et al. (2025). We hypothesize that this is because, after instruction tuning, the generation model tends to produce longer and more elaborative answers, occasionally introducing tokens that do not appear in the original context. We also observe decreasing trend in faithfulness as the compression ratio increases and model size decreases. For the end-to-end evaluation, the model demonstrates strong performance across both metrics. In particular, under the Mistral-7B model with compression ratio of 4 and the top-20 retrieval setting containing positive documents, faithfulness and factual correctness reach 49.95 and 60.59, respectively. The higher factual correctness is likely due to the use of short gold answers during training, which encourages the model to directly copy relevant words or phrases from the retrieved documents. Other observed trends are consistent with those in the compression evaluation results."
        },
        {
            "title": "H Paraphrase Case Study",
            "content": "To better understand the nature of information captured by our compressed representations and the query reasoner, we analyze how effectively the compressed representations preserve key semantic information. As illustrated in Table 19, the generations conditioned on compressed representations largely preserve the essential semantic content of the original documents, while substantially altering surface expressions such as sentence structure and word order. This observation is consistent with our pretraining objective, which encourages the model to encode core semantic meaning rather than memorize lexical sequences. The models ability to reconstruct paraphrased variants of the original text demonstrates that the learned compressed representations successfully capture high-level semantic knowledge in compact form. (a) Without MSE (b) With MSE Figure 8 t-SNE visualization of document representations and compressed (memory token) representations of Mistral7B under the compression ratio of 32. 31 Examples of Original and Paraphrased Documents Generated from Compressed Representations Example 1 Original Document: This document about SMK Bukit Kepayang. 1) Tuan Haji Mat Rohani Bin Kamar was the first Principal of the School. (2004 - 2008); 2) Madam Hajah Sadariah Binti Haji Md. Jali is the current Principal of the school. (2008-2014). Madam Hajah Sadariah achieved the title Master Principle in the year 2011 due to outstanding performance and achievements in that year. Paraphrased Document: From 2004 to 2008, Tuan Haji Mat Rohani Bin Kamar served as the Principal of SMK Bukit Kepayang. He was succeeded by Madam Hajah Sadariah Binti Haji Md. Jali, who led the school until 2014. In 2011, she was honored with the title of Master Principal for her exceptional achievements. Example 2 Original Document: This is document about Sikorsky SH-60 Seahawk and the U.S. 5th Fleet. The ALMDS is sensor system designed to detect, classify, and localize floating and near-surface moored mines in littoral zones, straits, and choke points. The system is operated from an MH-60S, which gives it countermine role traditionally handled by the MH-53E Sea Dragon, allowing smaller ships the MH-53E cant operate from to be used in the role. The ALMDS beams laser into the water to pick up reflections, then produces video image for ground personnel to determine if the object is mine. The MH-60S will utilize the BAE Systems Archerfish. Paraphrased Document: The ALMDS system, designed to detect and classify floating and near-surface mines in strategic locations such as straits, littoral zones, and choke points, is operated from an MH-60S helicopter. This platform enables the U.S. Navys 5th Fleet to perform countermine mission that was previously the responsibility of the MH-53E Sea Dragon. The MH-60S is equipped with the BAE Systems Archerfish system, which plays key role in the process. The ALMDS emits laser pulse into the water, which reflects. Example 3 Original Document: This document about Pinewild Womens Championship. The Pinewild Womens Championship was golf tournament on the LPGA Tour, played only in 1995. It was played at the Pinewild Country Club of Pinehurst in Pinehurst, North Carolina. Rosie Jones was the winner, beating Dottie Pepper on the first hole of sudden-death playoff. Paraphrased Document: In 1995, the Pinewild Womens Championship took place as single-year event on the LPGA Tour. The competition was held at the Pinewild Country Club of Pinehurst, located in Pinehurst, North Carolina. Rosie Jones emerged victorious, securing the title by defeating Dottie Pepper in sudden-death playoff on the first hole. Table 19 Examples of Original and Paraphrased Documents generated from compressed representations."
        },
        {
            "title": "I Prompts",
            "content": "Figures 916 illustrate the prompts used during the data synthesis process. Specifically, we employ different prompting strategies for (1) generating QA pairs, (2) producing paraphrased documents, (3) validating information completeness, and (4) completing missing information. Additionally, Figure 17 shows the prompt template used by the generation model to answer questions based on the compressed document representations."
        },
        {
            "title": "Prompt for Simple Question Generation",
            "content": "You are given document delimited by <doc> and </doc>. Your job is to read the given document and generate comprehensive set of multi-hop questions that fully cover all the key information in the text. <doc> <INSERT DOCUMENT HERE> </doc> Question Requirements: You should generate as many questions as necessary to fully cover all the key facts in the document. (1) Each question must be self-contained, meaning it should be understood by the user without seeing the document. (2) Each question must cover only one or at most two distinct key pieces of information. (3) The questions must be non-overlapping no two questions should target the same piece of information. (4) The questions should be simple factual recall only do not require inference, reasoning, or summarization. (5) Your output should be list of self-contained, non-overlapping factual questions that together comprehensively cover all the key information in the document. There are some examples: {3 demonstrations} Your output should be JSON object with the following format: { \"Question1\": \"...\", \"Question2\": \"...\", ..., \"QuestionN\": \"...\" } Figure 9 Prompt used for simple question generation. 33 Models Data composition Mistral-7B Phi4-mini No-pretrain SimpleQA Para SimpleQA+ComplexQA SimpleQA+ComplexQA+Para No-pretrain SimpleQA Para SimpleQA+ComplexQA SimpleQA+ComplexQA+Para NQ Normal 53.03 53.84+0.81 54.52+1.49 55.48+2.45 54.64+1.61 48.10 48.56+0.46 48.65+0.55 49.47+1.37 49.30+1.20 HotpotQA Musique 2Wikiqa Average 40.63 42.20+1.57 43.05+2.42 43.00+2.37 43.52+2.89 37.65 38.91+1.26 38.41+0.76 38.88+1.23 38.62+0.97 9.68 10.26+0.58 10.51+0.83 10.67+0.99 10.55+0.87 7.61 8.19+0.58 7.74+0.13 8.03+0.42 7.70+0.09 46.64 46.68+0.04 46.41-0.23 46.39-0.25 46.58-0.06 44.68 43.70-0.98 44.11-0.57 43.96-0.72 43.71-0.97 37.50 38.25+0.75 38.62+1.12 38.88+1.38 38.82+1.32 34.51 34.84+0.33 34.73+0.22 35.08+0.57 34.83+0.32 Table 20 Effect of pretraining data composition on instruction-tuning performance under Normal (top-5 retrieval) settings under the 32 compression ratio. We report the absolute score change () for each pretraining data setting relative to the No-pretrain baseline. Models CR NQ HotpotQA Musique 2Wikiqa Average Mistral-7B w/ mse Mistral-7B w/ mse 32x 32x 128x 128x 54.25 54.64+0.39 52.98 53.36+0.38 Normal 43.11 43.52+0.41 41.32 41.37+0.05 9.85 10.55+0.70 10.22 10.26+0.04 45.84 46.58+0.74 46.23 46.40+0. 38.26 38.82+0.56 37.69 37.85+0.16 Table 21 Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32, 128) and normal retrieval settings."
        },
        {
            "title": "Prompt for Complex Question Generation",
            "content": "You are given document delimited by <doc> and </doc>. Your job is to generate set of MULTI-HOP questions that, taken together, comprehensively cover the documents key information. <doc> <INSERT DOCUMENT HERE> </doc> Question Requirements: 1) Self-contained: Every question must be understandable without viewing the document. 2) Multi-hop only: Each question must require at least TWO independent pieces of evidence from DIFFERENT parts of the document (e.g., different paragraphs/sections/tables/items). If question can be answered from single sentence or data point, REJECT it. 3) Non-overlapping: No two questions may target the same fact or the same combination of facts. Each question must have unique reasoning path and evidence combination. 4) Coverage: Produce as many questions as needed to cover ALL key facts in the document. Prefer many small, precise multi-hop questions over few large ones. 5) Focus: Each question should target ONE multi-hop objective, typically integrating 23 facts (bridging, comparison, aggregation, temporal/causal linking, entityattribute joining, etc.). Do NOT bundle multiple unrelated sub-questions. 6) Verifiability: The answer to each question must be derivable SOLELY from the document, with no external knowledge or subjective judgment. 7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints, quantities, and identifiers where relevant. 8) No explanations: Do NOT include rationales, steps, or referencesONLY output the questions as JSON. 9) You can generate 2-hops, 3-hops, 4-hops, etc. questions. QUESTION TEMPLATES (use as patterns, adapt as needed) - Bridging: \"Which satisfies BOTH condition mentioned in [context A] AND condition mentioned in [context B]?\" - Comparison: \"Considering [pivot], which of or meets [criterion] when combining details from [source 1] and [source 2]?\" - Aggregation: \"When combining [quantity/info] from section with [quantity/info] from section B, which single entity matches [combined constraint]?\" - Temporal/Causal: \"Based on the timeline described in parts and B, which event/entity fulfills [temporal/causal relation]?\" There are some examples: {3 demonstrations} Input FORMAT: Document: <INSERT DOCUMENT HERE> OUTPUT FORMAT Return ONLY JSON object with keys \"Question1\", \"Question2\", ..., \"QuestionN\". Example (structure only): { \"Question1\": \"...\", \"Question2\": \"...\", \"Question3\": \"...\", \"QuestionN\": \"...\" } Figure 10 Prompt used for complex question generation."
        },
        {
            "title": "Prompt for Answer Generation",
            "content": "You are factual answering assistant. Your task is to read the provided document and answer the given question **based only on the information explicitly stated in the document**. Please output the answer as short as possible. Requirements: - Your answer must be based solely on the content of the document. - Do not use prior knowledge or make assumptions beyond the document. - If the document does not contain the answer, respond with: \"The document does not contain this information.\" - The answer should be concise, factual, and complete. Input Format: Document: <INSERT DOCUMENT TEXT HERE> Question: <INSERT QUESTION HERE> Output Format: Answer: <YOUR ANSWER HERE> Figure 11 Prompt used for factual answer generation."
        },
        {
            "title": "Prompt for QA Validation",
            "content": "You are fact-checking assistant. Your task is to verify whether the given answer to question is **fully supported by the provided document**. Instructions: - Read the document carefully. - Read the question and the provided answer. - Determine whether the answer is correct **based solely on the information in the document**. - The answer must be **complete**, **factually correct**, and **not contain any information that is not in the document**. If the answer is fully correct and supported by the document, respond with: \"Correct\" If the answer is partially correct, incomplete, or includes unsupported information, respond with: \"Incorrect\" Input Format: Document: <INSERT DOCUMENT HERE> Question: <INSERT QUESTION HERE> Answer: <INSERT ANSWER HERE> Output Format: {{\"Judgment\": \"Correct\" / \"Incorrect\"}} Figure 12 Prompt used for QA validation."
        },
        {
            "title": "Prompt for Supplementary Simple QA Generation",
            "content": "You are given document and set of existing question-answer pairs. Your task is to carefully compare the information covered in the QA pairs against the document and generate additional questions that cover any key information not yet addressed. Requirements: - Only generate questions for key facts present in the document that are **not already covered** in the existing QA pairs. - Do **not** repeat or rephrase the information in the existing question answer pairs. - Each question should cover **only one or two distinct key pieces of information**. - Each question must be self-contained, meaning it should be understood by the user without seeing the document. - All questions should require **simple factual recall only**, with no inference or reasoning. There are some examples: {3 demonstrations} Input Format: Document: <INSERT DOCUMENT HERE> Existing QA: <INSERT EXISTING QA HERE> Output Format: Return your generated new supplementary questions in the following JSON format: { \"Number of Supplementary Questions\": N, \"Question1\": \"...\", \"Question2\": \"...\", ..., \"QuestionN\": \"...\" } If all key information is already covered and no supplementary questions are needed, output an empty JSON object: { \"Number of Supplementary Questions\": 0 } Figure 13 Prompt used for supplementary Simple QA generation."
        },
        {
            "title": "Prompt for Supplementary Complex QA Generation",
            "content": "You are given document and set of existing question-answer pairs. Your task is to carefully compare the information covered in the QA pairs against the document and generate additional MULTI-HOP questions that cover any key information not yet addressed. Requirements: - Only generate questions for key facts present in the document that are **not already covered** in the existing question answer pairs. - Do **not** repeat or rephrase information which can be found in the existing question answer pairs. Question Requirements: 1) Self-contained: Every question must be understandable without viewing the document. 2) Multi-hop only: Each question must require at least TWO independent pieces of evidence from DIFFERENT parts of the document (e.g., different paragraphs/sections/tables/items). If question can be answered from single sentence or data point, REJECT it. 3) Non-overlapping: No two questions may target the same fact or the same combination of facts. Each question must have unique reasoning path and evidence combination. 4) Coverage: Produce as many questions as needed to cover ALL key facts in the document. Prefer many small, precise multi-hop questions over few large ones. 5) Focus: Each question should target ONE multi-hop objective, typically integrating 23 facts (bridging, comparison, aggregation, temporal/causal linking, entityattribute joining, etc.). Do NOT bundle multiple unrelated sub-questions. 6) Verifiability: The answer to each question must be derivable SOLELY from the document, with no external knowledge or subjective judgment. 7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints, quantities, and identifiers where relevant. 8) No explanations: Do NOT include rationales, steps, or referencesONLY output the questions as JSON. 9) You can generate 2-hops, 3-hops, 4-hops, etc. questions. QUESTION TEMPLATES (use as patterns, adapt as needed) - Bridging: \"Which satisfies BOTH condition mentioned in [context A] AND condition mentioned in [context B]?\" - Comparison: \"Considering [pivot], which of or meets [criterion] when combining details from [source 1] and [source 2]?\" - Aggregation: \"When combining [quantity/info] from section with [quantity/info] from section B, which single entity matches [combined constraint]?\" - Temporal/Causal: \"Based on the timeline described in parts and B, which event/entity fulfills [temporal/causal relation]?\" Input Format: Document: <INSERT DOCUMENT HERE> Existing QA: <INSERT EXISTING QA HERE> Output Format: Note, do not repeat or paraphrase existing questions. Instead, generate new multi-hop questions for the missing information, and put the new questions in JSON format: { \"Number of Supplementary Questions\": N, \"Question1\": \"...\", ..., \"QuestionN\": \"...\" } If all key information is already covered and no supplementary questions are needed, output an empty JSON object: { \"Number of Supplementary Questions\": 0 } Figure 14 Prompt used for supplementary complex QA generation."
        },
        {
            "title": "Prompt for Document Paraphrasing",
            "content": "You are given document. Your task is to paraphrase the document in way that: (1) **Restructure extensively** **Try your best to break down the structure of the original document**, do not keep the same paragraphing, ordering, or sentence flow. Reorganize ideas, shuffle the order, merge or split sentences, and restructure arguments. (2) **Preserve meaning with absolute accuracy** ensure that ALL key information and semantics of the original document are retained. Do not omit any factual details, numbers, dates, or specific information. (3) **Avoid direct copying** no sentence should remain identical; re-express ideas in fresh way using synonyms and varied sentence structures. (4) **CRITICAL: Add no new information** the paraphrased document cannot introduce: - New facts, interpretations, or context not explicitly stated - Organizational names or affiliations not mentioned in the original - Explanatory details or background information - Your own analysis or conclusions about the content (5) **Maintain the originals voice and perspective** if the original uses commands (\"followers shall...\"), dont change it to descriptions (\"the organization promotes...\"). Preserve the documents intended tone and format. (6) **Verify factual relationships** when paraphrasing complex information involving multiple entities, dates, or cause-and-effect relationships, double-check that you maintain the correct connections and chronology. (7) **Use varied vocabulary** employ synonyms and alternative expressions while maintaining precision. (8) **Preserve completeness** if the original mentions specific numbers, dates, names, or measurements, include them in your paraphrase (even if reworded). (9) **Maintain coherence** the paraphrased version should read as natural and fluent writing. **WARNING: Do not assume context.** If document mentions \"the Samaj\" without identifying what organization this refers to, do not assume its \"Brahma Samaj\" or any other specific group. Work only with what is explicitly stated. Produce paraphrased version that keeps the meaning and all factual details but has significantly altered structure and wording. Here are some examples of paraphrased documents. {10 demonstrations} Input: <document> Figure 15 Prompt used for document paraphrasing."
        },
        {
            "title": "Prompt for Paraphrase Validation",
            "content": "You are given two texts: Original Document the source text containing the key information. Paraphrased Document rewritten version of the original. Your task is to check whether the paraphrased document fully preserves all the key information from the original document, without adding any new information. Guidelines: (1) Key information means the essential facts, arguments, data, and main ideas of the original. (2) The paraphrased document must: (2.1) Contain all the key information from the original. (2.2) Not omit any important point. (2.3) Not introduce information that is absent in the original. (2.4) Preserve meanings without distortion or contradiction. (2.5) Differences in style, sentence structure, or wording are acceptable as long as the meaning is preserved. (3) Output format: (3.1) Answer Yes if the paraphrased document completely retains all key information and introduces no new information. (3.2) Answer No if any key information is lost, altered, or if extra/unwarranted information appears. Dont output any explanation. Input: Original Document: [insert original doc] Paraphrased Document: [insert paraphrased doc] Output: Yes / No Figure 16 Prompt used for paraphrase validation. Prompt for Document-based QA You are helpful assistant. Your task is to extract relevant information from provided documents and to answer to questions as briefly as possible. Background: {docs} Question:{question} Figure 17 Prompt used for document-based question answering."
        }
    ],
    "affiliations": [
        "Apple",
        "University of Edinburgh"
    ]
}