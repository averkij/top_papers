{
    "paper_title": "Hybrid Latent Reasoning via Reinforcement Learning",
    "authors": [
        "Zhenrui Yue",
        "Bowen Jin",
        "Huimin Zeng",
        "Honglei Zhuang",
        "Zhen Qin",
        "Jinsung Yoon",
        "Lanyu Shang",
        "Jiawei Han",
        "Dong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 5 4 8 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Hybrid Latent Reasoning via Reinforcement Learning",
            "content": "Zhenrui Yue1, Bowen Jin1, Huimin Zeng1, Honglei Zhuang2, Zhen Qin2, Jinsung Yoon2, Lanyu Shang3, Jiawei Han1, Dong Wang1 1University of Illinois Urbana-Champaign, 2Google, 3LMU {zhenrui3,bowenj4,huiminz3,lshang3,hanj,dwang24}@illinois.edu, {hlz,zhenqin,jinsungyoon}@google.com, lanyu.shang@lmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have introduced latent reasoning as promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RLbased hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledgeand reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning."
        },
        {
            "title": "Introduction",
            "content": "Latent reasoning has emerged as compelling alternative to traditional autoregressive reasoning methods in large language models (LLMs) [8, 35, 39]. In contrast to the conventional chain-of-thought (CoT) [43, 17, 10], which relies on the discrete decoding and sampling process, latent reasoning enables LLMs to reason internally with continuous hidden representations from the previous steps. For instance, Coconut [11] achieves latent reasoning by utilizing the models last hidden state as continuous thought, feeding it back as input embeddings to the next reasoning step, thereby matching the performance of CoT on reasoning-intensive tasks. To show the difference between the autoregressive generation and latent reasoning, we compare both approaches in Figure 1. Nevertheless, existing methods in latent reasoning utilize extensive CoT traces for training. That is, CoT trajectories are required to learn informative latent representations. An example is CODI [35], which adopts self-distillation to train on discrete CoT tokens and transfers learnt features into continuous thoughts. Although recurrent latent reasoning removes the need for CoT data, it relies on training multi-block LLM from scratch to reason internally [8]. Moreover, these methods employ tailored training paradigms for latent representation learning, incurring high training costs Preprint. Under review. Figure 1: Comparison between discrete reasoning (left) and latent reasoning (right). Unlike the autoregressive sampling process in discrete reasoning, latent reasoning incorporates hidden representations from previous steps to enhance reasoning performance (between <think> and </think>). and overlooking the inherent reasoning capabilities of LLMs [11, 8, 34]. For example, Coconut [11] requires multi-stage training on CoT steps, which not only increases training compute but also delays the models acquisition of complete reasoning chains [35]. Furthermore, we find that latent reasoning is often incompatible with LLMs due to the discrepancy between output hidden states and input embeddings (as we show Section 4.3). That is, feeding hidden states into the next decoding step degrades generation quality (e.g., repetition, incoherence), causing difficulties in adapting LLMs for latent reasoning. Therefore, an ideal latent reasoning method should capitalize on pretrained LLMs generalizability by seamlessly integrating continuous representations, preserving LLMs interpretability while mitigating CoT-dependent extensive training for broader applicability. To this end, we introduce hybrid reasoning policy optimization (HRPO), novel hybrid latent reasoning optimization framework based on reinforcement learning (RL). HRPO unifies policy learning with latent reasoning, thereby utilizing the LLMs intrinsic reasoning patterns without relying on CoT trajectories. To preserve the generative capabilities while encouraging the model to reason in the continuous space, HRPO introduces gating mechanism to gradually incorporate hidden state representations from previous steps into sampled token embeddings. The gating mechanism is initially configured in way that the inputs come predominantly from the sampled tokens. As training progresses, the gate learns to incorporate richer, more informative features from previous hidden states for improved internal reasoning. Since the sampling operation introduces stochasticity, HRPO rollouts can be performed like standard RL methods, with hybrid outputs (tokens and latent representations) stored in the rollout buffer for policy updates. For optimization, HRPO leverages simple outcome-based reward and employs the hybrid rollout buffer to calculate log probabilities, enabling policy gradient updates that adaptively integrate both token-level and latent representations. By bridging discrete and continuous reasoning, HRPO provides scalable and training-efficient solution that unlocks latent reasoning in existing LLMs. As result, HRPO enhances the adaptability of latent reasoning and leads to superior performance on both knowledgeand reasoning-intensive tasks. We highlight our contributions in the following1: We introduce HRPO, the first reinforcement learning-based approach for hybrid reasoning, empowering LLMs to autonomously develop latent reasoning capabilities. We design gating mechanism to preserve LLMs generative abilities, which starts by prioritizing sampled token embeddings and, through RL-driven updates, progressively incorporates the continuous representations. By leveraging the LLMs inherent reasoning patterns through HRPO, we mitigate the need for chain-of-thought annotations and expensive multi-stage training, offering an efficient and scalable alternative to existing latent reasoning methods. To show the efficacy of the proposed hybrid latent reasoning, we evaluate on multiple knowledge and reasoning benchmarks and show that it outperforms existing models and latent reasoning baselines, demonstrating consistent performance gains across diverse scenarios. 1Our code can be found at https://github.com/Yueeeeeeee/HRPO. 2 In addition, we provide insights into RL-based training of latent reasoning models and present intriguing reasoning patterns emerging from HRPO."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Latent Reasoning Early research in latent reasoning focuses on analyzing the latent space computation within transformer models [2, 47]. For example, Biran et al. [2] study multi-hop reasoning and show that back-patch features from later layers can improve performance on challenging queries. Alternatively, latent representations can be used to construct informative features as in-context demonstrations to enhance few-shot performance at test-time [45, 52]. In particular, Xu et al. [45] exploit latent skills to select in-context examples for reasoning-intensive tasks. Different from this line of work, hidden reasoning is also proposed to improve generative capabilities by incorporating latent variables into language modeling [8, 20]. For instance, Geiping et al. [8] propose depth-recurrence language model that injects latent variables and iteratively processes them to derive the final states used for decoding. Similarly, special tokens (e.g. <pause>) are inserted to allocate extra test-time compute for internal reasoning, leading to improvements across diverse scenarios [9, 29]. Pfau et al. [29] argue that filler tokens act as intermediate reasoning steps in multi-token computations, yielding measurable performance gains on parallelizable problems. Furthermore, implicit reasoning methods transform explicit, token-level reasoning trajectories into internal reasoning to enhance efficiency or accuracy [6, 7]. For instance, CODI [35] employs self-distillation to framework to align explicit and implicit reasoning tokens for improved performance. Concurrent to our work, hidden reasoning approaches [11, 34, 36] leverage previous output hidden states as next input embeddings, enabling compact yet informative internal reasoning. Nonetheless, the majority of existing methods require processed traces and extensive training. In contrast, we focus on hybrid latent reasoning through reinforcement learning to exploit the inherent generation capabilities of LLMs. 2.2 Reinforcement Learning Reinforcement learning (RL) is paradigm where an agent interacts with an environment, receives feedback, and learns to make decisions that maximize cumulative rewards over time [37]. Recently, RL has been introduced to improve language models by learning from implicit human feedback (RLHF) [28]. Such fine-tuning typically employs policy gradient algorithms and their variants like REINFORCE [38]. To reduce variance, actor-critic methods like A2C [26] are proposed to compute learnt baseline and leverage advantage estimates for better training dynamics. Similarly, proximal policy optimization (PPO) [32] introduces clipped surrogate objective to bound policy updates, thereby achieving training stability and robustness to hyperparameter choices. Parallel to these approaches, direct preference optimization (DPO) [31] is introduced to directly optimize language models using pairwise human preference comparisons. DPOs simpler variant such as SimPO [25] further mitigates the need of reference models. Despite DPOs efficiency, online RL methods remain preferred for their consistent superior performance [44]. Recently, reinforce leaveone-out (RLOO) [1] proposes REINFORCE-style RL that generates multiple responses and utilizes the mean reward of the other responses as baseline. Similarly, group relative policy optimization (GRPO) [33] and REINFORCE++ [16] compute baselines from group-level or batch-level reward scores across candidate completions, and thus reduce memory overhead while maintaining accuracy and stability for complex tasks. In this work, we design novel online RLdriven approach to incentivize hybrid latent reasoning by progressively incorporating hidden states into LLM inputs, thereby providing richer representations for improved reasoning performance."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Hybrid Reasoning with Gating We first describe our notation and settings for hybrid latent reasoning. For input query = [x1, x2, . . . , xt] and its corresponding token embeddings = [e1, e2, . . . , et], we describe the raw hidden states from the LLM output at step with ˆht, namely: ˆH = [ˆh1, ˆh2, . . . , ˆht] = Transformer(E), (1) 3 Figure 2: Hybrid reasoning with gating (left) and hybrid reasoning policy optimization (right). During rollouts, the reasoning trajectory is generated hybridly with both discrete tokens and latent features, and for policy update, we compute the HRPO loss using the hybrid rollout buffer to update the model. in which Transformer denotes the transformer model (i.e., decoder layers), ˆH represents the finallayer hidden states produced by the Transformer. With the LM head (Head), the next output token ˆxt+1 can be sampled from the output distribution over the vocabulary via: ˆxt+1 softmax(Head(ˆht)). (2) However, hidden states often lie outside the models token embedding manifold, which degrades generation quality when fed directly. To avoid this, we project ˆht back into the embedding space to ensure the inputs conform to the models learned distribution. Specifically, we use the output probabilities pt+1 to compute weighted interpolation over the vocabulary: ht+1 = pt+1 pt+1 , with pt+1 = softmax( Head(ˆht) τ ), (3) in which τ is the temperature and We denotes the embedding matrix of the LLM. In other words, we compute the next input embedding as weighted sum of all token embeddings, with weights given by pt+1. In addition, pt+1 is normalized to preserve the scale and variance of the output vector. This sampling-free mapping ensures differentiability and aligns the projected embedding with the models native input space, thus leading to improved training dynamics (see Section 4.3). While interpolated embeddings preserve semantic continuity, directly feeding ht+1 as the next token input removes stochasticity and injects noise from irrelevant tokens, causing degraded generation within RL rollouts. As such, we design hybrid approach for latent reasoning by gradually imposing hidden state representations into the sampled token embeddings with gating mechanism. Drawing on gated recurrence models [5, 27], we formulate the gating mechanism as: rt = σ(Waˆet+1 + ba), it = σ(Wxˆet+1 + bx), at = exp(c softplus(Λ) rt), at ˆet+1 + (cid:112)1 a2 ˆet+1 (cid:26) et+1 = (it ht+1) (4) think, think, et+1 is the resulting hybrid input for the next step, ˆet+1 denotes the embedding of the sampled discrete token ˆxt+1, whereas ht+1 is the projected hidden states as in Equation (3). The gates rt and it leverages sigmoid function σ to control the blending, at scales ˆet+1, is fixed scaling constant, and Λ is learnable vector. Note that hybrid reasoning only applies during the reasoning phase (i.e., think), while the final answer is still generated via standard autoregressive decoding, as we show in Figure 2 (left). By initializing at 1 (see Appendix A), the inputs first draw predominantly from the sampled token embeddings, thereby effectively preserving the LLMs generative capabilities. As the training progresses, the value range of at converges to an optimum range and thus incorporates informative features from both hidden representations and sampled tokens. 4 Overall, our hybrid reasoning approach projects hidden states into the embedding space via weighted interpolation. Moreover, the sampling steps preserve stochasticity for effective reinforcement learning. We employ plug-and-play gating mechanism that initially prioritizes sampled token embeddings while gradually integrating latent signals, providing richer inputs for subsequent reasoning. 3.2 Hybrid Reasoning Policy Optimization (HRPO) Rather than relying on strong supervision, we optimize the policy model via hybrid rollouts using reinforcement learning (RL), fully harnessing LLMs native reasoning capabilities. Inspired by recent RL advances such as group relative policy optimization (GRPO) [33], we introduce hybrid reasoning policy optimization (HRPO), an efficient RL-driven framework that enable LLMs to fuse discrete tokens with continuous representations for hybrid reasoning. As illustrated in Figure 2 (right), the proposed HRPO optimizes the policy (parameterized by θ) to maximize the expected reward for input drawn from dataset and the sampled hybrid outputs (discrete tokens) and (hidden representations): max θ E(x,y)D,(ˆy,H)πθ(x)[r(a, y)], (5) where is simple outcome-based reward function and denotes the ground truth answer (i.e., it outputs 1 for correct prediction in and 0 otherwise). The rewards are computed solely on the discrete tokens within the answer span. To obtain an unbiased, low-variance advantage for hybrid latent reasoning, we generate hybrid rollouts per input query and compute the advantages by standardizing the rewards within the group (i.e., for the i-th response, the advantage is calculated by ˆAi = rimean([r1,r2,...,rg]) ). Consequently, the policy gradients can be estimated with: std([r1,r2,...,rg]) θJHRPO(θ) = ExD,{(yi,Hi)}g i=1πθ(x) 1 (cid:88) i= 1 yi yi (cid:88) t=1 θ log πθ(yi,tx, yi,<t, Hi,<t) ˆAi,t βθDKL[πθπref ], (6) where πref denotes the reference model and KL-divergence acts as regularizer, controlled by hyperparameter β. This objective follows simple REINFORCE-style formulation, fusing discrete token inputs with continuous hidden representations across the reasoning span via the introduced gating mechanism. The hybrid trajectories that yield higher returns are assigned larger advantage estimates, encouraging policy updates to increase the log probabilities of their subsequent reasoning tokens. For the KL divergence term, we compute log probabilities using solely token IDs for πref , as we find it more effective in preserving training stability. Different from PPO / GRPO objectives, we omit the likelihood ratio and directly use raw log probabilities in Equation (6) because ratio clipping is rarely encountered under our conservative learning schedule. Furthermore, since the hidden representations are directly tied to the parameters θ, each trajectory should only be used for single gradient update; attempting to reuse iteven with importance samplingviolates the on-policy constraints. As such, our HRPO implementation remains lightweight, strictly on-policy and could be seamlessly combined with further RL optimizations. In summary, the proposed HRPO framework unifies hybrid latent reasoning under simple RL objective that fully leverages LLMs intrinsic reasoning capabilities. During rollouts, the decoding process progressively fuses discrete and continuous representations through learnable gate, preserving coherence while exploiting hidden states. For policy updates, HRPO derives advantages directly from outcome rewards and performs policy gradient steps with KL regularization. As result, HRPO incentivizes LLMs to dynamically integrate sampled tokens with latent representations, delivering stable and efficient on-policy hybrid reasoning training without separate value function."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate HRPO on both knowledgeand reasoning-intensive tasks: (1) open-domain & multi-hop knowledge-intensive question answering (Knowledge); and (2) science, technology, engineering or mathematics (STEM) benchmarks. The experimental results are reported as follows. 5 Table 1: Evaluation performance of various larger LLMs and trained models on open-domain and multi-hop QA benchmarks. The table reports exact match scores based on top-3 retrieved documents on five datasets: NQ, TriviaQA, HotpotQA, 2WikiMQA and Bamboogle. The upper block reports results for several RAG baselines using the larger Qwen 2.5 7B LLM, while the lower two blocks evaluate smaller Qwen models (1.5B and 3B) trained with different strategies. NQ TriviaQA HotpotQA 2WikiMQA Bamboogle Average QA CoT IRCoT Search-o1 RAG SFT RAG PPO GRPO HRPO (Ours) SFT RAG PPO GRPO HRPO (Ours) 0.134 0.048 0.224 0.151 0.349 0.094 0.288 0.327 0.293 0.364 0.249 0.348 0.356 0.381 0.378 Qwen2.5-7B-Instruct 0.183 0.092 0.133 0.187 0.299 0.250 0.111 0.149 0.176 0.235 Qwen2.5-1.5B-Instruct 0.129 0.228 0.256 0.202 0.273 0.210 0.203 0.242 0.213 0.276 Qwen2.5-3B-Instruct 0.186 0.255 0.304 0.308 0.316 0.248 0.226 0.293 0.303 0.318 0.408 0.185 0.478 0.443 0.585 0.193 0.477 0.527 0.480 0.553 0.292 0.544 0.563 0.570 0.593 0.120 0.232 0.224 0.296 0. 0.024 0.072 0.184 0.120 0.216 0.112 0.080 0.240 0.272 0.296 0.219 0.134 0.242 0.251 0.335 0.130 0.254 0.307 0.261 0.337 0.217 0.291 0.351 0.367 0.380 4.1 Evaluation on Knowledge Benchmarks We first evaluate HRPO on five open-domain and multi-hop question answering (QA) datasets: Natural Questions (NQ), TriviaQA, HotpotQA, 2WikiMultiHopQA (2WikiMQA) and Bamboogle [14, 19, 21, 30, 48]. For each query, we use the E5 embedding model [42] to retrieve the top-3 Wikipedia documents as context (details presented in Appendix A). Following [18], we merge the NQ and HotpotQA training sets to train HRPO models, and evaluate it on each datasets evaluation split. The exact match results of HRPO and baselines (including supervised fine-tuning (SFT), retrieval augmented generation (RAG) [22] and RL-based PPO [32] and GRPO [33]) for the 1.5B and 3B Qwen2.5 Instruct models [46] are presented in Table 1. We also include comparisons to several QA and RAG baselines using the larger Qwen2.5-7B-Instruct as backbone, including: direct inference (QA), chain-of-thought (CoT) [43], interleaving retrieval with CoT (IRCoT) [41], Search-o1 [23] and RAG [22]. For each block in Table 1, we mark the best performance in bold for clarity. Across all knowledge benchmarks, HRPO delivers the strongest exact match (EM) scores with smaller Qwen models and rivals the much larger 7B baselines. In particular, we observe: (1) HRPO reaches 0.380 EM with Qwen2.5-3B, outperforming the strongest 7B RAG baseline by 4.5%. Similarly, HRPO with the smaller 1.5B backbone scores an average of 0.337, achieving consistent gains and surpassing PPO by 3.0%. (2) HRPO consistently outperforms other RL-based methods. For example, HRPO with both the 1.5B and 3B backbones surpasses the strongest RL baseline by 3.0% and 1.3% respectively; the only dataset both models perform similarly is NQ. (3) Interestingly, GRPO underperforms PPO by 4.6% on the 1.5B backbone but outperforms it by 1.6% on the 3B model, likely consequence of sparser rewards and limited sampled trajectories with smaller model. (4) RLbased methods perform on par with the best-performing RAG baseline, with HRPO delivering the largest performance gainsparticularly on terse, incomplete queries (NQ) and multi-hop questions (2WikiMQA)while yielding modest improvements on one-hop datasets like TriviaQA. Overall, these results demonstrate that combining retrieval augmentation with hybrid latent reasoning yields state-of-the-art knowledge performance under computation constraints, establishing HRPO as competitive alternative to both RL-based learning methods and larger retrieval augmented LLMs. 6 Table 2: Evaluation performance of various larger LLMs and trained models on STEM benchmarks. The table presents accuracy scores on five datasets: GSM8k, MATH, MATH500, MMLU-ST and ARC-C. The upper block reports results for several few-shot baseline LLMs 7B, while the lower two blocks evaluate smaller Qwen models (1.5B and 3B) trained with different strategies. GSM8k MATH MATH500 MMLU-ST ARC-C Average DeepSeekMath-7B Gemma-2-9B Qwen2.5-7B MAmmoTH2-7B MAmmoTH2-8B SFT Distilled CoT PPO GRPO HRPO (Ours) SFT Distilled CoT PPO GRPO HRPO (Ours) Larger LLMs (Size 7B) 0.362 0.377 0.498 0.367 0.358 0.346 0.364 0.464 0.396 0.732 Qwen2.5-1.5B-Instruct 0.300 0.503 0.507 0.502 0.518 0.302 - 0.518 0.524 0.536 Qwen2.5-3B-Instruct 0.348 0.575 0.597 0.602 0.613 0.360 - 0.604 0.604 0.630 0.565 0.651 0.723 0.624 0. 0.403 - 0.566 0.562 0.569 0.454 - 0.582 0.601 0.590 0.642 0.707 0.854 0.684 0.704 0.560 0.706 0.694 0.711 0.720 0.670 0.799 0.819 0.834 0.845 0.678 0.682 0.637 0.817 0. 0.602 - 0.715 0.737 0.742 0.474 - 0.811 0.814 0.820 0.519 0.556 0.635 0.578 0.652 0.433 - 0.600 0.607 0.617 0.461 - 0.682 0.691 0.700 4.2 Evaluation on STEM Benchmarks We also evaluate the performance of the proposed HRPO on the reasoning-intensive STEM datasets: GSM8k, MATH, MATH500, MMLU-STEM (MMLU-ST) and ARC-Challenge (ARC-C) [4, 13, 24, 12, 3]. Table 2 reports the performance of HRPO alongside fine-tuned baselines (SFT, SFT with distilled CoT from QwQ [40]) and RL baselines (PPO [32] and GRPO [33]) on the Qwen 2.5 1.5B and 3B Instruct models [46]. In addition, we select several larger LLMs ( 7B in size) using few-shot CoT for comparison [46, 33, 49]. For GSM8k, we train on the training split, and for MATH and MATH500, we train on the MATH training split. For MMLU-ST and ARC-C, we train on the merged auxiliary MMLU and ARC-C training sets. Distilled CoT is only available for GSM8k and MATH due to dataset size constraints. We also highlight the best scores in each block in bold. Across the five STEM benchmarks, HRPO delivers the strongest results with compact Qwen backbones and could match the performance of much larger LLMs. Our key observations are: (1) SFT underperforms compared to distilled CoT and RL-based methods, suggesting the efficacy of RL with verifiable rewards on reasoning-intensive tasks. (2) With the 3B backbone, HRPO achieves an average accuracy of 0.700, matching the best 7B baseline on four of the datasets. Even the 1.5B HRPO averages at 0.617, outperforming the 7B leader on MATH by 2.0%. (3) At 1.5B, HRPO improves on the strongest alternative GRPO with notable boosts on MATH and MATH500 (1.6% and 1.2%), whereas the average gain narrows at 3B, implying that HRPO is more beneficial for smaller models. (4) HRPO registers the highest accuracies recorded for sub-7B models on MATH (0.613) and MATH500 (0.630), demonstrating the value of RL-based hybrid reasoning on challenging benchmarks. Taken together, these results show that hybrid latent reasoning unlocks the power of much larger LLMs in compact backbones, proving the effectiveness of the proposed HRPO. 4.3 Analysis of HRPO Different Strategies for Latent Reasoning. We compare different strategies to compute latent representations. Specifically, we use three methods to integrate hidden states into RL and train the 1.5B Qwen model on the MATH dataset. These variants are: (1) hidden states, which use the final layer hidden states as the next input; (2) interpolation, which employs interpolated embeddings 7 as defined in Equation (3); and (3) HRPO, our hybrid latent reasoning in Equation (4). We visualize the exponential moving average (EMA) of rewards along with the GRPO baseline in Figure 3. Due to the mismatch between hidden states and embeddings, using hidden states degrades generation and yields nonsensical rollouts with zero reward. Although interpolation performs similar to HRPO for the first few hundred steps, the rewards eventually collapse and only slowly recover, likely because interpolation introduces excessive noise. We also provide direct comparison between HRPO and latent reasoning methods in Appendix B. Overall, our approach achieves superior training dynamics with faster convergence while maintaining stability comparable to GRPO, highlighting the efficacy of our hybrid design choice in HRPO. Figure 3: Reward on MATH for Qwen-2.5-1.5B using different latent reasoning strategies. Figure 4: Hidden ratio with varying rmin in exp(c softplus(Λ)) and learning rate. We visualize the hidden ratio and completion length for training runs with rmin from [0.95, 0.98, 0.99]. Ratio of Latent Representations. We track how the balance between discrete tokens and continuous latent representations shifts as LLMs learn to reason hybridly. Here, we train Qwen 1.5B on the knowledge task and visualize both the mean hidden ratios (i.e., (cid:112)1 a2 ) and completion lengths (along with GRPO) in Figure 4. Across all runs, the hidden ratio increases steadily, even as the learning rate tapers off toward the end of training under cosine schedule. In addition, completion lengths increase during the initial phase and later decline across all methods, with the drops most significant in HRPO. Furthermore, setting rmin = 0.95 leads to an interesting behavior where completion lengths substantially decreasean effect not seen in the other variants. This may be because the hidden representations effectively capture historical context, thereby shortening completions while maintaining or even improving performance (see Table 3). As such, hybrid latent reasoning could be particularly effective when leveraging contextual information for reasoning. Table 3: Impact of Λ-initialization on HRPOs performance across knowledge and STEM tasks. Init Range Knowledge NQ TriviaQA HotpotQA 2WikiMQA Bamboogle Average [0.95 - 0.999] [0.98 - 0.999] [0.99 - 0.999] 0.364 0.336 0.336 0.553 0.553 0.534 0.273 0.263 0. 0.264 0.276 0.275 0.184 0.216 0.216 0.328 0.329 0.324 Init Range STEM GSM8k MATH MATH500 MMLU-ST ARC-C Average [0.95 - 0.999] [0.98 - 0.999] [0.99 - 0.999] 0.705 0.703 0. 0.516 0.509 0.518 0.536 0.532 0.526 8 0.569 0.563 0.567 0.735 0.732 0.742 0.612 0.608 0. Figure 5: Sensitivity analysis for temperature τ in Equation (3). We visualize the reward and completion length for training runs with different temperature selected from [0.3, 0.5, 0.7, 0.9]. Initialization of Λ for Gating. Beyond hidden ratio, we examine how the initialization of Λwhich control the balance between latent features and token embeddingsaffects HRPO performance. Specifically, we initialize exp(c softplus(Λ)) from [rmin, 0.999] and report the results on Qwen 1.5B in Table 3, where lowering rmin yields higher initial hidden ratio. For the knowledge domain, performance improves as rmin decreases: the best average performance occurs at rmin = 0.98, and most individual datasets peak at rmin = 0.95. In contrast, the STEM benchmarks display bimodal trend: performance rises when rmin is either lower or higher, but drops for the intermediate range [0.98, 0.999]. This pattern implies that the model profits from emphasizing either explicit token trajectories or latent representations, whereas mid-level mix is sub-optimal. In summary, our results show that knowledge tasks benefit from lower rmin, whereas optimal performance for STEM tasks arises from leaning toward either explicit token trajectories or latent representations. Sensitivity of τ on Hybrid Reasoning. We further investigate the impact of temperature τ on HRPO: lower τ values reduce noise but overemphasize top tokens, whereas larger τ spreads probability mass across more tokens. We explore τ {0.3, 0.5, 0.7, 0.9} and present the rewards and completion lengths of the 1.5B Qwen model on MMLU in Figure 5. The left panel indicates that τ = 0.3 and τ = 0.5 converge faster and reach the highest reward plateau, outperforming higher values (τ 0.7) and showing the benefits of smaller τ . Interestingly, the right panel reveals that both smaller and larger τ values shorten completion lengths, while τ = 0.5 and τ = 0.7 maintain longer generations. This may be because lower τ sharpens token distribution, yielding confident latent vector that lets HRPO finish quickly. In contrast, higher τ flattens the distribution and enhances informativeness, prompting the policy to extract answers in shorter rollouts. Overall, we find HRPO to be robust across varuing τ selections, only completion length varies noticeably. Further analysis is in Appendix B. Figure 6: Example cross-lingual reasoning (English-Chinese) and its translation for HRPO. Hybrid Latent Reasoning Patterns. Finally, we highlight several intriguing reasoning patterns that emerge from HRPO. First, the hybrid outputs show readable trajectories by interpreting the tokens even without any CoT supervision. Second, HRPO exhibits cross-lingual patterns in some completions, fluidly integrating tokens from different languages, suggesting that latent representations can generalize across linguistic boundaries (see Figure 6). Moreover, the hybrid reasoning process often delivers compact yet accurate responses to simple or factual queries, where the model requires fewer decoding steps thanks to the richer context encoded in the hidden representations. These emergent patterns indicate that hybrid latent reasoning can improve both interpretability and efficiency over existing latent reasoning approaches. Further qualitative examples can be found in Appendix C."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose hybrid reasoning policy optimization (HRPO), novel reinforcement learning (RL) framework that unifies discrete token sampling with continuous latent representations through learnable gating mechanism. By gradually incorporating hidden features into sampled token embeddings, HRPO incentivizes LLMs to refine their reasoning strategies hybridly. Extensive evaluations on knowledge and STEM benchmarks demonstrate that HRPO outperforms both SFT and RL baselines, achieving consistent gains across diverse scenarios. Moreover, our analysis reveals that HRPO not only ensures stable hybrid latent reasoning but also triggers intriguing reasoning patterns, showing the HRPOs potential and providing insights for further work in latent reasoning."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [2] Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024. [3] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [5] Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [6] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. [7] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. [8] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. [9] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 10 [12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [14] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, 2020. [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [16] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [19] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. [20] Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, et al. Scalable language models with posterior inference of latent thought vectors. arXiv preprint arXiv:2502.01567, 2025. [21] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [23] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [24] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [25] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [26] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 19281937. PmLR, 2016. [27] Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 2667026698. PMLR, 2023. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [29] Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. [30] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. [35] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. [36] DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275, 2025. [37] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [38] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. [39] Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts. arXiv preprint arXiv:2502.08524, 2025. [40] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [41] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, 2023. [42] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 12 [44] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. arXiv preprint arXiv:2404.10719, 2024. [45] Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xian Wu, Peter Stone, and Yanjun Qi. Lars: Latent reasoning skills for chain-of-thought reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 36243643, 2024. [46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [47] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024. [48] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [49] Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 37:9062990660, 2024. [50] Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, and Dong Wang. Evidencedriven retrieval augmented response generation for online misinformation. arXiv preprint arXiv:2403.14952, 2024. [51] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. [52] Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, and Jianfeng Gao. Vector-icl: In-context learning with continuous vector representations. arXiv preprint arXiv:2410.05629, 2024."
        },
        {
            "title": "A Implementation",
            "content": "For hybrid latent reasoning, our plug-and-play component is by design compatible with any LLM architecture. We initialize its linear layers with uniform distribution from [1/(cid:112)H, 1/(cid:112)H], where denotes the hidden state dimension. The gating parameter Λ is selected such that the quantity ac = exp(c softplus(Λ)) is drawn uniformly from [rmin, 0.999], with the scalar constant fixed at = 8 [5]. Tuning rmin adjusts the initial fraction of hidden states involved in hybrid reasoning; larger value increases the proportion of sampled token embeddings and can be helpful for enhancing generation quality during the initial training phase. Similarly, the temperature hyperparameter τ in Equation (3) can be tuned for optimal task performance, although HRPO remains robust across wide range of τ values. To efficiently train the LLMs with HRPO, we patch the models with optimized kernel implementations2 and employ low-rank adaptation (LoRA) [15]. The default choice of hyperparameters are reported in Table 4 for HRPO experiments. Algorithm Epochs Optimizer Optimizer Momentum Weight Decay Learning Rate Learning Rate (Linear in Equation (4)) Learning Rate (Λ in Equation (4)) HRPO β Max Gradient Norm Gradient Accumulation Step Group size in HRPO Total Train Batch Size LR Scheduler Warmup Ratio Precision (WA) Table 4: Experiment hyperparameter settings. HRPO 1 AdamW 8bit β1, β2 = 0.9, 0.99 0.1 5e-6 1e-4 1e-3 0.005 0.1 4 4 / 8 32 / 64 Cosine with Warmup 0.1 BF16-mixed LoRA Modules LoRA Rank LoRA α query, key, value, dense 32 64 The hyperparameters are selected empirically to balance efficiency and performance, and thanks to HRPOs lightweight design and additional optimizations, our framework can run on single GPU across diverse tasks. Additionally, we apply larger weight-decay coefficient to (1) enhance HRPO training stability and (2) encourage the gating towards incorporating more latent representations (since smaller positive Λ values increase the hidden ratio (cid:112)1 a2 ). For simpler knowledge tasks and GSM8k, we fix the HRPO group size at 4, which already delivers strong performance. For more challenging benchmarks, namely MATH, MATH500, MMLU-ST and ARC-C, we instead generate 8 hybrid completions for each query. As for prompt and completion lengths, we select them empirically based on our observations, and the selected values are summarized in Table 5. Table 5: Experiment prompt / completion lengths. Prompt / Completion Length for Knowledge Tasks Prompt / Completion Length for GSM8k Prompt / Completion Length for MATH & MATH500 Prompt / Completion Length for MMLU-ST & ARC-C 2048 / 512 512 / 512 512 / 1024 512 / 512 For both training and evaluation, we build each prompt by prepending system message that directs the LLM to perform step-by-step internal reasoning before generating its final answer. The user query is then appended, and the entire input is formatted with the model chat template. Different from 2https://github.com/unslothai/unsloth prior work [10, 18], we adopt the minimalist delimiter #### to separate the models hybrid reasoning span from its final answer. This is because the delimiter tokenizes as single unit, adding no length overhead while providing clear signal to switch from hybrid latent reasoning to autoregressive answer generation. We also penalize repeated occurrences of the delimiter within the completion (by assigning 0 reward regardless answer correctness) to prevent the model from early termination of hybrid reasoning. We illustrate full prompts for different type of tasks, showing the system message and example queries in Figure 7, Figure 8 and Figure 9, respectively. Example Prompt for Knowledge Tasks The user asks question, The assistant first thinks about the The final answer is provided after the #### tag, i.e., <im_start>system conversation between User and Assistant. and the assistant solves it. reasoning process in the mind and then provides the user with the answer. {reasoning process} #### {answer}.<im_end> <im_start>user Context (which may or may not be relevant): Clyde River (New South Wales)::::Clyde River (New South Wales) The... Barwon River (New South Wales)::::River and Weir River (part of... Taponga River::::Taponga River The Taponga River, an inland... Question: What direction does the river that Austrolebias bellotti are found in flow?<im_end> <im_start>assistant Figure 7: Example prompt for knowledge tasks, contexts are partially omitted due to space constraints. Example Prompt for GSM8k / MATH / MATH500 The user asks question, The assistant first thinks about the The final answer is provided after the #### tag, i.e., <im_start>system conversation between User and Assistant. and the assistant solves it. reasoning process in the mind and then provides the user with the answer. {reasoning process} #### {answer}.<im_end> <im_start>user Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<im_end> <im_start>assistant Figure 8: Example prompt for GSM8k / MATH / MATH500 in HRPO. For each question in our knowledge-intensive QA setup, we embed the query with E5 embedding model [42]. The entire English Wikipedia 2020 dump is pre-encoded with the same model, after which we perform approximate nearest neighbor (ANN) search and select the three highest-scoring documents. These top-3 passages are concatenated to form the external context fed to the LLM, as illustrated in Figure 7. In our evaluation, we generate tokens using greedy decoding and compute latent representations according to Equation (3), thereby ensuring the reproducibility of our results. For outcome-based reward and evaluation settings on knowledge tasks, we report exact match scores on val / test splits following [50, 51, 18]. For mathematical (GSM8k, MATH and MATH500) and multiple-choice datasets (MMLU-ST and ARC-C), we follow [49] for post-processing and scoring. 15 Example Prompt for MMLU-ST / ARC-C The user asks question, The assistant first thinks about the The final answer is provided after the #### tag, i.e., <im_start>system conversation between User and Assistant. and the assistant solves it. reasoning process in the mind and then provides the user with the answer. {reasoning process} #### {answer}.<im_end> <im_start>user One person is pushing with Question: Two people are pushing car. force of 450 and the other person is pushing with force of 300 N. What information is needed to determine the net force applied to the car by the people? Options: A. the direction of the road B. the direction of the forces C. the weight of the two people D. the weight of the automobile<im_end> <im_start>assistant Figure 9: Example prompt for MMLU-ST / ARC-C in HRPO."
        },
        {
            "title": "B Additional Results",
            "content": "Comparison to Latent Reasoning Methods. In addition to strong RL methods such as PPO and GRPO in our main experiments, we also benchmark the proposed HRPO against additional latent reasoning baselines. Specifically, we evaluate HRPO, Coconut and CODI on the GSM8K and MATH reasoning datasets, all using the 1.5B Qwen backbone. For Coconut, we train with its augmented CoT data (no MATH split is available), whereas for CODI we adopt the original datasets CoT trajectories. The results are reported in Table 6. We observe: (1) HRPO achieves the best accuracy on both datasets, with 9.42% and 23.63% respective gains over the best performing latent reasoning baseline CODI. (2) Even compared to distilled CoT from significantly larger model QwQ, HRPO still scores consistent improvements on both datasets, showing the effectiveness of our hybrid latent reasoning. (3) Coconut lags behind on GSM8k, indicating limitations of latent reasoning by compressing CoT tokens, whereas CODI improves substantially with CoT SFT but still trails Distilled CoT and HRPO. Overall, HRPO achieves the best performance against all baselines, demonstrating its consistent advantages over CoT distillation and prior latent reasoning methods. Table 6: Performance comparison of HRPO against alternative latent reasoning methods and distilled CoT baseline. Coconut CODI Distilled CoT HRPO GSM8k MATH GSM8k MATH GSM8k MATH GSM8k MATH Accuracy 0.315 - 0.658 0. 0.706 0.503 0.720 0.518 Sensitivity Analysis for Λ and τ . In addition to the results reported in Table 3, we further present the performance of various Λ initializations on the Qwen 3B model, as shown in Table 7. Our observations echo the same trends on the 1.5B backbone: smaller initial rmin consistently benefits both knowledge and STEM tasks. Moreover, performance peaks when rmin is selected either lower or higher, and drops slightly within the intermediate range of [0.98, 0.999]. We also examine the sensitivity of the τ hyperparameter used to construct the interpolated embeddings and present the corresponding results for both backbone models in Table 8. The training rewards and completion lengths for GSM8k, MATH and the knowledge tasks are shown in Figure 10, Figure 11 and Figure 12. We note that choosing τ in the range of 0.5 0.7 offers reliable balance of efficiency and accuracy, as the performance often peaks around this interval for both backbone models. Overall, we find that Table 7: Impact of Λ-initialization on HRPOs performance for the Qwen 3B backbone. Init Range Knowledge NQ TriviaQA HotpotQA 2WikiMQA Bamboogle Average [0.95 - 0.999] [0.98 - 0.999] [0.99 - 0.999] 0.845 0.842 0.838 0.613 0.600 0.606 0.622 0.614 0.630 0.576 0.585 0.590 0.820 0.813 0.817 0.695 0.691 0. Init Range STEM GSM8k MATH MATH500 MMLU-ST ARC-C Average [0.95 - 0.999] [0.98 - 0.999] [0.99 - 0.999] 0.367 0.378 0.375 0.593 0.588 0.584 0.316 0.311 0.309 0.311 0.298 0. 0.296 0.296 0.288 0.377 0.374 0.375 HRPO benefits from smaller initial rmin, which outperforms larger rmin settings and highlights the value of latent representations for complex reasoning. Moreover, HRPO is robust to the choice of τ , where the performance scores remain stable with only minor fluctuations at the extremes. Table 8: Impact of τ on HRPOs performance for both backbone models. Model τ GSM8k MATH MATH500 MMLUST ARC-C NQ TQ HotpotQA 2Wiki Bamboogle Qwen 1.5B Qwen 3B 0.3 0. 0.7 0.9 0.3 0.5 0.7 0. 0.717 0.518 0.522 0.561 0.735 0.320 0.524 0.263 0.276 0.216 0.720 0.516 0.536 0.569 0.741 0.336 0.534 0.260 0.272 0.216 0.705 0.507 0.532 0.559 0.742 0.317 0.553 0.252 0.264 0. 0.694 0.514 0.524 0.567 0.724 0.364 0.553 0.273 0.244 0.176 0.842 0.597 0.622 0.577 0.820 0.378 0.588 0.311 0.318 0.296 0.841 0.606 0.614 0.590 0.817 0.375 0.593 0.316 0.311 0. 0.845 0.613 0.622 0.574 0.809 0.373 0.578 0.309 0.297 0.296 0.833 0.599 0.630 0.580 0.808 0.363 0.578 0.306 0.293 0.280 Figure 10: Reward and completion length for training runs with different temperature values on GSM8k using the Qwen 1.5B backbone. Additional Analysis for Λ Initialization. We further provide an expanded analysis of how varying rmin in the initialization of Λ affects training dynamics with the larger Qwen 3B backbone. Figures Figure 13, Figure 14, Figure 15 and Figure 16 plot the reward and completion length curves for the knowledge tasks, GSM8k, MATH and MMLU-ST / ARC-C respectively. Overall, our findings here echo the observations in Section 4.3: different rmin values exhibit similarly high training stability and preserve the LLMs generative capabilities, but selecting smaller rmin (i.e., larger initial hidden ratio) generally accelerates convergence and shortens generated completions. Nevertheless, these benefits are less pronounced for the 3B backbone than for the 1.5B counterpart, which we attribute to the fewer update steps and trainable parameters in HRPO. In summary, our analysis shows 17 Figure 11: Reward and completion length for training runs with different temperature values on MATH using the Qwen 1.5B backbone. Figure 12: Reward and completion length for training runs with different temperature values on knowledge tasks using the Qwen 1.5B backbone. that HRPO preserves stable training dynamics and effectively leverages LLMs intrinsic reasoning patterns across rmin values; moreover, choosing smaller rmin further enhances convergence and yields shorter generated sequences, which can be especially beneficial for smaller-scale LLMs."
        },
        {
            "title": "C Qualitative Analysis",
            "content": "To further highlight HRPOs reasoning patterns, we present additional qualitative examples. Each example provides the reasoning trace by decoding the sampled tokens from the hybrid reasoning process, and we include both successful and erroneous cases across different tasks in the following. The correct examples are provided in Figure 17, Figure 18, Figure 19, Figure 20, Figure 21, where as the mistakes are provided in Figure 22, Figure 23, Figure 24, Figure 25, Figure 26, we show the raw strings and omit the options / contexts in the examples due to space constraints. From these examples, we identify four reasoning patterns that can lead to correct answers: (1) Purely English reasoning with coherent trajectories (Figs. Figure 17 and Figure 18), pattern commonly observed in LLM reasoning outputs. (2) Predominantly English reasoning punctuated by rare tokens (e.g., %n rather than n), as shown in Figure 19). (3) Cross-lingual reasoning that interweaves multiple languages (English and Chinese in Figure 20). (4) Reasoning with many uncommon tokens and atypical steps, yet still arriving at the correct answer (Figure 21). These latter three patterns are rarely observed in standard reasoning LLMs but are more prevalent in HRPO trained models, demonstrating that HRPO can enhance reasoning by leveraging LLMs intrinsic generative capabilities across different languages and token types, thereby delivering improvements across diverse scenarios. As for reasoning errors, we also identify several common patterns: (1) Cross-lingual mistakes arising from limited parametric or contextual knowledge, as in Figure 22 and Figure 23. (2) Correct answers that violate the predefined format and thus receive zero score (Figure 24). (3) Repetitive loops that continue until the response hits the maximum completion length (Figure 25). (4) Cross-lingual reasoning that is nonetheless truncated by the length limit (Figure 26). Overall, these patterns indicate that, while HRPO effectively integrates discrete and latent representations in its internal reasoning process, it may be further enhanced through refined output formatting (e.g., potentially with format 18 Figure 13: Reward and completion length for training runs with varying initial rmin on knowledge tasks using the Qwen 3B backbone. Figure 14: Reward and completion length for training runs with varying initial rmin on GSM8k using the Qwen 3B backbone. reward), extended optimization schedules with conservative learning, increased model parameters, and longer context / generation capabilities, pointing to promising directions for future research. 19 Figure 15: Reward and completion length for training runs with varying initial rmin on MATH using the Qwen 3B backbone. Figure 16: Reward and completion length for training runs with varying initial rmin on MMLU-ST / ARC-C using the Qwen 3B backbone. Figure 17: Correct reasoning example 1 in HRPO. Figure 18: Correct reasoning example 2 in HRPO. 20 Figure 19: Correct reasoning example 3 in HRPO. Figure 20: Correct reasoning example 4 in HRPO. Figure 21: Correct reasoning example 5 in HRPO. Figure 22: Mistaken reasoning example 1 in HRPO. Figure 23: Mistaken reasoning example 2 in HRPO. Figure 24: Mistaken reasoning example 3 in HRPO. Figure 25: Mistaken reasoning example 4 in HRPO. 22 Figure 26: Mistaken reasoning example 5 in HRPO."
        }
    ],
    "affiliations": [
        "Google",
        "LMU",
        "University of Illinois Urbana-Champaign"
    ]
}