{
    "paper_title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "authors": [
        "Dohun Lee",
        "Chun-Hao Paul Huang",
        "Xuelin Chen",
        "Jong Chul Ye",
        "Duygu Ceylan",
        "Hyeonho Jeong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 6 9 2 6 1 . 1 0 6 2 : r Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory Dohun Lee1,2,* Chun-Hao Paul Huang1 Xuelin Chen1 Jong Chul Ye2 Duygu Ceylan1 Hyeonho Jeong1 1Adobe Research 2KAIST Figure 1. Memory-V2V enables iterative video-to-video editing with long-term visual memory, producing videos that remain consistent with all previously edited videos. (a) and (b) illustrate results for video novel view synthesis and text-guided long video editing, respectively. Colored boxes in (a) highlight novel-view regions that must remain consistent across generations. Note that each editing iteration performs independent denoising. Please refer to our project page for video results: https://dohunlee1.github.io/MemoryV2V"
        },
        {
            "title": "Abstract",
            "content": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of In this multi-turn setting, current video ediinteraction. tors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, 1 Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate MemoryV2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. 1. Introduction Videos are rapidly becoming dominant medium of modern communication and expression, spanning domains from entertainment to robotics simulation. With the advent of largescale generative models, users increasingly expect video editing tools that enable attribute-level control allowing modification of subjects [1, 26, 28], motion [1, 23, 26], or viewpoint [3, 41, 56]. Furthermore, real-world video workflows are inherently iterative where users frequently refine outputs over several interactions which we term as multiturn video editing. While recent video editing frameworks demonstrate impressive results in single-pass interactions, they often fail to maintain cross-consistency across sequential edits. For example, as shown in Fig.1(a), state-of-the-art video novel view synthesis model, ReCamMaster [3], successfully rerenders the input video from multiple target viewpoints, yet the novel-view regions across iterations remain inconsistent. Similarly, in Fig.1(b), when segments of long video are iteratively edited using LucyEdit [1], each segment individually follows the editing prompt; but the overall appearance drifts. To address these challenges, we introduce MemoryV2V, framework that equips pretrained video-to-video diffusion models with explicit visual memory. Unlike prior models that lack the ability to recall previous edits, Memory-V2V integrates lightweight memory modules into existing architectures and finetunes them on standard videoto-video data, enabling consistent, multi-turn video editing. Designing memory-guided editing framework requires several key capabilities. First, we need compact way to encode past generations into meaningful representations. To this end, we begin by comparing several candidate representations in the context of multi-turn video novel view synthesis, where maintaining 3D consistency is critical. Specifically, we evaluate three different encoders including recurWork done during internship at Adobe. rent 3D reconstruction model [45], encoder-decoder-based state-of-the-art novel view synthesis model [27], and the video VAE [30] used by diffusion-based video generators. We find that the latter provides the best balance between fidelity and efficiency. Given an effective representation, straightforward approach to enable multi-turn consistency is to condition the model on the encoded representations of all the previously edited videos. However, this quickly becomes infeasible: the cost scales with sequence length, and redundant or irrelevant frames can even degrade the generation quality [55]. Hence, we introduce retrieval mechanism to identify which past edits are relevant to the current editing iteration. Memory-V2V uses an efficient conditioning strategy that integrates the retrieved information without overwhelming computation or introducing redundancy. In particular, we introduce learnable dynamic tokenizers which tokenize the retrieved videos with varying kernel sizes based on their relevance to the current edit. We further inspect the importance of the resulting tokens by computing per-frame attention responsiveness score. We demonstrate that instead of discarding low-responsive tokens, further compressing them in carefully selected blocks of the video diffusion model yields consistent editing results while reducing the computational complexity. We conduct experiments on two representative video-tovideo editing tasks: video novel view synthesis and textguided long video editing. Memory-V2V achieves strong cross-iteration consistency in both tasks while preserving or even improving single-turn editing quality compared to the baselines. Our contributions can be summarized as follows: We introduce and define the task of consistent multi-turn video editing which aims to achieve cross-consistency across multiple editing iterations in real-world video editing workflows. We propose Memory-V2V, the first framework that extends existing video editing diffusion transformers with explicit visual memory. This is achieved by efficient taskspecific retrieval mechanisms and learnable dynamic tokenizers and compression layers. We evaluate Memory-V2V on challenging video editing tasks including iterative video novel view synthesis and long video editing where Memory-V2V demonstrates strong cross-iteration consistency and editing quality. 2. Related Work Video novel view synthesis. Given monocular video of dynamic scene, video novel view synthesis aims to generate plausible videos captured from unseen camera trajectories, while preserving the underlying 3D structure and temporal dynamics [44]. Due to the scarcity of largescale paired multi-view video datasets, prior works have focused on test-time optimization or scene-specific overfit2 Figure 2. Overview of Memory-V2V. (a) From an external cache of previously edited videos, only the top-k most relevant videos are retrieved and used as memory inputs to ensure cross-iteration consistency. (b) Dynamic tokenizers allocate more tokens to highly relevant videospreserving fine details while maintaining an efficient overall token budget. (c) Adaptive token merging reduces latency and FLOPs by compressing less informative frames based on their attention-based responsiveness to the target query. ting of existing monocular video generators [24, 33, 59]. Recently, approaches such as ReCamMaster [3] leverage large-scale synthetic 4D datasets rendered from simulation engines [13, 17] to finetune text-to-video diffusion models into multi-view video-to-video models. Another line of work [24, 41, 46, 56] employs point-cloud renderings, which, though incomplete, approximate the desired camera trajectories as geometric proxies. Video diffusion models then use these renderings as spatial guidance to synthesize complete and geometrically consistent videos. While some works iteratively refine point clouds using generated videos to improve static scene novel view synthesis [41, 58], they fail to generalize to dynamic scenes with non-rigid motion. In lack of an explicit memory mechanism, when these video-to-video methods are applied iteratively, the generated outputs fail to maintain 3D consistency for regions that were not visible in the input video. Our work introduces an explicit visual memory into pre-trained video-to-video diffusion models such as ReCamMaster [3]. Text-guided video editing. Early text-driven video editing approaches can be broadly divided into two categories [1]. The first relies on inference-time solutions, often using deterministic inversion with test-time optimization to align edits with text prompts [6, 16, 23, 48]. The second conditions video diffusion models on explicit visual cues such as depth maps or optical flow fields [10, 11, 18, 21]. While effective in constrained settings, they often introduce temporal artifacts and struggle to generalize to flexible, opendomain editing scenarios. Recently, instruction-based foundational video-to-video editing models [1, 26, 28, 38, 39] have emerged, jointly conditioning on source videos and text instructions. These models deliver precise, high-fidelity edits while preserving subject identity and motion. However, these models are constrained by limited temporal context window, so they cannot process videos exceeding their temporal window. naive workaround is to split long video into shorter segments and edit each independently, but as shown in Fig. 1(b), this approach leads to significant appearance inconsistencies across segments. In this work, we cast long video editing as multiturn video editing problem and demonstrate that equipping aforementioned video-to-video diffusion models [1] with our memory mechanism enables precise long-form video editing with consistent appearance and motion, even when video segments are denoised independently. Long video generation with memory or context. Although modern video models generate visually impressive results, they inherently lack 3D and long-term consistency due to their 2D frame-based representations and limited temporal context [43]. To address this, recent works first reformulate full-sequence video diffusion models into autoregressive generators [7, 19, 42, 54], then incorporate mem3 ory modules that capture information from past generations. One line of work maintains an external cache of previous generations and conditions the current generation by retrieving subset of them, either as raw RGB frames [31, 50, 55] or as hidden states [5, 57]. Alternatively, other methods aim to compress the long-term context into compact forms such as latent states [12, 37, 64], semantic video tokens [25, 36, 61], or visionlanguage model features [8]. Unlike prior literature, we tackle long-term memory in the context of video editing, where the model must selectively maintain consistency across multiple edited videos and user-input video rather than continuous frame stream. Our approach combines retrieval and compression: past videos are first retrieved based on relevance, then adaptively compressed, preserving more detail for more relevant clips. 3. Memory-V2V 3.1. Overview Existing video-to-video diffusion models are single-turn video editing models, which are trained to generate an output video conditioned on single source video and auxiliary editing signals, approximating the conditional distribution p(x y, c), where is the output video, is the user-provided source video, and denotes additional conditions like text or camera pose. In contrast, our multi-turn video editing task requires incorporating prior editing history by allowing to include both the current input video and previously edited videos, enabling the model to maintain cross-edit consistency across iterations. Memory-V2V achieves this through hybrid retrieval and compression strategy, built upon an effective memory representation. In the following sections, we first detail our framework in the context of video novel view synthesis. We consider ReCamMaster [3], pretrained video DiT model capable of single-turn video novel view synthesis, as our base model. We begin with proof-of-concept experiment to identify suitable memory encoders (Sec. 3.2). Next, we introduce the core components of Memory-V2V which include an efficient video retrieval mechanism that effectively fetches only relevant past videos from an external cache and dynamically tokenizes them and learnable compressor that effectively processes the retrieved information (Sec. 3.4). Finally, we extend Memory-V2V to text-guided, long-form video editing by formulating it as multi-turn video editing task (Sec. 3.5). 3.2. Proof-of-Concept: Ideal Context Encoder Maintaining semantic consistency and persistency when editing long videos in chunks or in multi-turn editing setup requires to encode prior editing results as additional context which can then be utilized as constraints for the next Figure 3. Comparison of different memory encoders on twoturn novel view synthesis. The red-colored box depicts the novel region which are expected to be consistent between x1 and x2. editing round. Hence, an important design choice is how the context is represented. We devise simple experiment setup to evaluate the representation capability of different potential context encodings. We limit our experiment to the novel-view synthesis task in constrained two-turn editing setup using ReCamMaster as the base model. In particular, given an input video y, in the first turn we generate video x1 from new camera trajectory ccam . Using an arbitrary encoder E, we extract the context E(x1) from x1 and in the second turn, we generate video x2 from camera trajectory ccam , conditioning on both the original source video as well as the encoded context E(x1). If E(x1) has sufficient representation power, we expect that the novel content generated in x1 and x2 to be consistent. 2 We experiment with three different choices of pretrained context encoders for the novel view synthesis task: (i) recurrent 3D reconstructor model CUT3R [45] (ii) state of the art novel view synthesis network LVSM [27], and (iii) video VAE [30]. For each setting, encoded representations E(x1) are patchified to match the dimensionality of the DiTs video tokens, allowing interaction within the selfattention layers. During finetuning, we freeze the context encoder and train only the patchification layer and the ReCamMaster model to accept additional memory context (see the supplementary for details). Moreover, during training, we randomly sample multiple videos of the same scene: one as the original source video and another as the source of contextual memory x1. 4 We provide examples from each of the context encodings in Fig. 3. We observe that the state tokens from CUT3R or LVSM fall short for encoding detailed appearance in newly generated regions. In contrary, using the same video latent space achieves the best performance in terms of quality and consistency of the results. Hence, we adopt this option for the context representation and focus on how to efficiently retrieve and process the context representations obtained from multiple past generations as will be discussed next. 3.3. Video Retrieval-based Dynamic Tokenization After each editing iteration j, we store the latent video E(xj) RF HW in an external cache Ω, indexed by its corresponding camera trajectory ccam . The latent videos temporal and spatial dimensions are denoted by F, H, , and is the latent channel size, and the context encoder corresponds to the VAE encoder. At the i-th editing iteration (i>j>0), our goal is to produce new video xi that remains consistent with both the user-input video and the previously generated videos in Ω = {(E(xj), ccam ) 0 < < i}, while accurately following the new target camera trajectory ccam . Since the total number of cached frames in Ω increases rapidly over multiple editing turns, it is infeasible to condition the pretrained video-to-video DiT model on the entire editing history. We argue that only subset of the previously edited videos is necessary to maintain spatial and temporal consistency. To this end, we sort all cached latent videos in Ω and dynamically retrieve set of the top-k most relevant ones, denoted as Ω. i Video FOV retrieval. In case of video novel-view synthesis, we determine the relevance via our proposed VideoFOV retrieval algorithm, which quantifies the geometric overlap between the field-of-view (FOV) of the target camera trajectory ccam and those of cached videos. Specifically, given target camera trajectory ccam = {ccam i,t 1 }, where is the number of frames, we place unit sphere at the first camera position ccam i,1 and uniformly sample viewing directions on its surface 1. For each frame along the trajectory, sampled point is marked as visible if it lies within the projected image bounds and has positive depth after being transformed to the cameras local coordinate system. The per-frame FOV Fframe(ccam i,t ) is defined as the set of visible sampled points, and the video-level FOV Fvideo(ccam ) is obtained by taking the union of all framelevel FOVs: Fvideo(ccam ) = (cid:91) t=1 Fframe(ccam i,t ) . (1) Given target trajectory ccam and cached trajectory Figure 4. Long-video editing as multi-turn video editing with Memory-V2V. (a) We extend target videos from existing video editing dataset for Memory-V2V training. (b) During inference, the external cache Ω stores the editing history as sourcetarget video pairs. At the i-th editing turn, relevant memory videos are retrieved based on the similarity between source video segments. ccam , we define two complementary FOV similarity metrics: soverlap(ccam , ccam ) = scontain(ccam , ccam ) = Fvideo(ccam Fvideo(ccam Fvideo(ccam j ) Fvideo(ccam ) Fvideo(ccam ) Fvideo(ccam ) Fvideo(ccam ) ) ) , . (2) The final relevance score is weighted combination: s(ccam , ccam ) = λ soverlap(ccam , ccam where we set λ=0.5. )+(1λ) scontain(ccam , ccam (3) ), All cached videos in Ω are then ranked by s(ccam ), and the top-k highest-scoring videos are selected as the retrieved video set Ω. The full algorithm and implementation details are provided in the supplementary material. , ccam Dynamic tokenization. Unlike long video generation [31, 50], where the memory context typically consists of small number of frames (<10), the multi-turn video editing treats entire videos as retrieval units. As iterations accumulate, the total number of past conditioning frames can easily reach hundreds 2, making direct encoding computationally prohibitive. To address this, we propose to further tokenize the conditional videos dynamically based on the relevance of each video. Our goal is to apply multiple learnable tokenizers to each retrieved video. In practice, we train tokenizers with spatio-temporal compression factors (f hw) of 122, 144, and 188, where the 122 tokenizer processes the user-input video, the 144 tokenizer processes the top-3 most relevant retrieved videos, and the 1In all experiments, we set = 64, 800. 2For video novel view synthesis, single video contains 81 frames. 5 Figure 5. Qualitative results for multi-turn video novel view synthesis. Compared to baselines, Memory-V2V (Ours) generates videos from new camera trajectories while maintaining consistency across all previously generated novel regions (e.g., red and blue areas). 188 tokenizer processes the remaining ones. This adaptive tokenization preserves fine-grained details for the most relevant videos while keeping the total token count managable. 3.4. Adaptive Token Merging While retrieval-based dynamic tokenization effectively allocates the token budget, the quadratic complexity of the DiTs self-attention still leads to high computational cost as the token sequence length increases. Fortunately, recent studies [49, 60, 63] reveal that DiT attention maps are inherently sparse, with only small subset of entries contributing meaningfully to the output. Motivated by this, we further enhance computational efficiency by adaptively merging unresponsive tokens before the attention operation. This adaptive token merging strategy avoids redundant computation while preserving the essential context information. Frame-level token responsiveness. Previous studies [5, 22, 51, 62] show that the spatially averaged attention features (e.g., query or key features) serve as strong proxy for measuring how much each frame contributes to the models prediction. Formally, given query, key, and value matrices Q, K, RN within DiT block, we represent all tokens belonging to frame with single aggregated vector Kt obtained by spatially averaging the key features: Kt = 1 It (cid:88) iIt Ki, (4) where It denotes the set of token indices for frame t. Next, we estimate the responsiveness of each frame by computing its maximum attention response to the target queries Qtgt: Rt = max qQtgt (softmax (cid:18) t (cid:19) ). (5) The responsiveness score Rt quantifies the degree to which frame influences the current generation step. higher Rt indicates that at least one target query token strongly attends to that frame, implying it contains essential visual cues, whereas frames with low Rt can be safely compressed without degrading generation quality. Adaptive token merging. Empirically, we find that completely discarding low-importance tokens leads to noticeable degradation in generation quality (see Fig. 8). Instead, we introduce learnable convolutional operator Cθ that adaptively merges tokens belonging to frames with low responsiveness. Given set of tokens {Xi}iIt from frame with low responsiveness score Rt, the operator fuses them into compact representation: Xt = Cθ({Xi}iIt), Xt Nt D, (6) where is the spatial-temporal reduction factor. We scale proportionally to the number of conditional videos, as 6 Table 1. Pearson/Spearman correlations and Bottom-k overlap (k=50%) evaluating consistency of frame responsiveness across transformer blocks. Correlations measure linear and rank-order alignment. Bottom-k overlap reflects whether low-responsive frames remain consistently uninformative across layers. Table 3. Quantitative comparison results for text-guided long video editing. The higher the better for all metrics. Subject Consistency Background Consistency Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness DINO-F CLIP-F LucyEdit (Ind) LucyEdit (FIFO) Memory-V2V (Ours) 0.8683 0.8737 0.9326 0.9026 0.9042 0.9233 0.4601 0.4527 0.4950 0.6429 0.5598 0.6759 0.9844 0.9844 0. 0.9915 0.9919 0.9939 0.6856 0.6784 0.8019 0.8225 0.8198 0.8741 Pearson Spearman ρ Bottom-k overlap (%) Block 1 vs 230 Block 11 vs 1230 Block 21 vs 22 0.608 0.137 0.723 0.115 0.753 0.144 0.506 0.267 0.657 0.139 0.683 0.126 0.730 0.160 0.758 0.120 0.793 0.114 Table 2. Quantitative comparison results for multi-turn video novel view synthesis. Multi-view Consistency Camera Accuracy Visual Quality 1st Iter. vs 2nd Iter. 1st Iter. vs 3rd Iter. 2nd Iter. vs 3rd Iter. Avg. Score RotErr TransErr Subject Consistency Imaging Quality Temporal Flickering Motion Smoothness 0.1254 TrajCrafter [56] 0.1665 ReCam (Ind) [3] ReCam (AR) [3] 0.1181 Memory-V2V w/ CUT3R 0.1875 0.1966 Memory-V2V w/ LVSM 0.1168 Memory-V2V (Ours) 0.2110 0.1982 0.1985 0.2345 0.2255 0.1525 0.2090 0.2031 0.1290 0.2444 0.2467 0. 0.1818 0.1892 0.1485 0.2221 0.2229 0.1357 3.66 1.97 - 1.71 1.90 1.65 57.44 24.23 - 18.10 17.87 13.47 0.9452 0.9483 - 0.9479 0.9421 0.9494 0.7385 0.7186 - 0.7208 0.7199 0.7242 0.9661 0.9765 - 0.9722 0.9720 0. 0.9880 0.9931 - 0.9931 0.9929 0.9933 larger memory context introduces higher redundancy and thus benefits from stronger compression. Block selection for token merging. To determine where to apply token merging within the DiT architecture, we analyze the stability of responsiveness scores Rt across transformer blocks. In 30-block DiT, we group layers into early (110), middle (1120), and late (2130) stages and measure how well the first block in each stage (1, 11, 21) correlates with subsequent blocks. As shown in Tab. 1, the first block (Block 1) shows weak correlation with the rest (Blocks 230), meaning early merging risks discarding frames that later become important. By contrast, responsiveness stabilizes in mid and late stages (Blocks 10 and 20), where low-importance frames remain consistently uninformative. This suggests that applying token merging at these points preserves essential information while effectively reducing redundancy. Hence, we apply adaptive token merging at Block 10 and 20, where representations are sufficiently mature for stable and efficient compression. 3.5. Extension to Text-guided Long Video Editing We show that Memory-V2V can be applied to other video editing tasks such as text-guided long video editing. We assume access to pretrained single-turn text-based video editing model, LucyEdit [1], as our base model in this experiment. Given long user-input video typically exceeding 200 frames, which is significantly longer than the temporal context window of the base model ( 81 frames), we reformulate the task as memory-aware iterative editing. We first divide the long source video into shorter segments y1, . . . , yT that fit within the base models temporal window and iteratively edit each segment. When editing the i-th source segment yi, the top-k most relevant edited segments are retrieved from an external cache Ω = {E(xj) 0 < < i}, which stores all past edited segments as video latents. Unlike camera-conditioned video Table 4. Quantitative ablational studies on each component. Time (s) MEt3R Subject Consistency Aesthetic Quality Imaging Quality Motion Smoothness Dynamic Tokenization Only 980.80 0.2234 + Video Retrieval + Adaptive Token Merging + All (Full Model) 965.95 661.31 648.5 0.2169 0.2344 0. 0.9368 0.9338 0.9361 0.9351 0.5632 0.7307 0.5632 0.5626 0.5636 0.7288 0.7287 0. 0.9917 0.9918 0.9921 0.9919 novel view synthesis where each generated video xj can naturally be indexed by its camera pose ccam for retrieval, text-conditioned editing poses unique challenge. Text instructions ctext are often too ambiguous or coarse, making text-based similarity unreliable. To overcome this, we instead index and retrieve edited videos based on their corresponding source video segments: for given source segment yi to edit, we compute feature similarities using DINOv2 embeddings [35] across all previous source segments {yj 0<j<i} and retrieve the edited videos E(xj) whose corresponding sources yj are most similar to yi (see supplementary for details). The retrieved videos are then dynamically tokenized where adaptive token merging is also applied when generating the edited segment xi. Once all segments are iteratively edited, they are stitched together to form single output video (see Fig. 4(b) for overview). 4. Experiments 4.1. Implementation Details Video novel view synthesis. Starting from the single-turn video novel view synthesis model ReCamMaster [3], we finetune it for iterative novel view synthesis using synthetic multi-camera video dataset containing 10 synchronized videos per scene, each captured from distinct camera trajectories. The model is trained with the rectified flow matching loss [14] for 2K steps on 32 A100 GPUs with total batch size of 32. We finetune the self-attention layers, MLP projector, and camera encoder from the base model, together with the newly introduced dynamic tokenizers and adaptive token compressors. During training, 16 videos are randomly sampled at each iteration to serve as memory videos Ω. To ensure all tokenizers with varying kernel sizes are trained without reliance on the other, we randomly use one of the tokenizer for the 50% of the training time, while for the other 50% we use mix of them. Moreover, adaptive token merging is enabled with 50% probability, using compression factor randomly sampled from [0.3, 0.7]. Text-guided long video editing. We extend the singleturn, instruction-driven video editing model LucyEdit [1]. We finetune it to take both the source video and previ7 Figure 6. Text-guided long video editing results by iterative multi-turn editing. In contrast to (iterative) LucyEdit [1], Memory-V2V (Ours) consistently adds the same hat to woman (left) and transforms the dark door into the same white door (right) across all segments. Figure 7. Ablation on video retrieval and token merging. ously edited (history) videos as input. Training is conducted on 56K samples [28] filtered from the publicly available Senorita-2M dataset [66]. Each sample in Senorita contains triplet of text instruction, source video, and target video. However, since both the source and target clips in Senorita are relatively short, we employ an off-the-shelf video extension model [61] to temporarily extend the target videos, using these extended segments as the memory videos during training (see Fig. 4(a)). The model is trained with the rectified flow matching loss [14] for 1K steps on 32 A100 GPUs with total batch size of 32. 4.2. Memory-V2V for Video Novel View Synthesis In this experiment, we evaluate on 40 publicly available videos [3, 34, 47, 56], each serving as user input. We compare Memory-V2V against ReCamMaster (ReCam) [3] and TrajectoryCrafter (TrajCrafter) [56]. For ReCamMaster, two inference modes are evaluated: (1) ReCam (Ind), Figure 8. Comparison between merging and discarding tokens. which reuses the same user-input video as the source for every iteration, and (2) ReCam (AR), which uses the previous output as the next input in sequential manner. We also include two ablation variants of our model: MemoryV2V w/ CUT3R and Memory-V2V w/ LVSM, which employ CUT3R [45] and LVSM [27] based memory encoders, respectively. Qualitative comparisons are shown in Fig. 5, and quantitative results in Tab. 2. For cross-iteration consistency, we adopt MEt3R [2], which evaluates 3D coherence across multiple generated views. For each input video, we iteratively generate three outputs with highly overlapping camera trajectories (e.g., pan-left and orbit-right) and measure pairwise consistency across the 1st2nd, 1st3rd, and 2nd3rd iterations. For visual quality and camera accuracy, we report VBench metrics [20] and report rotation & translation errors. We observe that as the number of iterations increase, the cross-consistency significantly drops for the base models (TrajCrafter and ReCam (Ind)). While ReCam (AR) preserves consistency between subsequent iterations (e.g., 2nd and 3rd), it fails to keep other iterations (e.g., 1st Through task-specific retrieval and learnable compression, Memory-V2V efficiently integrates visual memory into existing video-to-video models. On both novel view synthesis and long video editing, it achieves strong cross-iteration consistency while improving original capacity, advancing toward truly iterative, memory-aware video editors. Acknowledgements. We thank Minguk Kang and Sihyun Yu for their insightful discussions at the early stage of the project. Figure 9. Computational cost analysis. Adaptive token merging reduces FLOPs and latency by over 30%, with efficiency gains further increasing as the number of memory videos grows. and 3rd) consistent. As discussed earlier, while CUT3Rand LVSM-based encoders struggle to preserve appearance consistency, Memory-V2V results in consistent generations across all iterations while successfully preserving the visual quality and improving camera adherence of the base model. 4.3. Memory-V2V for Long Video Editing We evaluate the long video editing performance on 50 videos from the Senorita validation set [66]. We compare against two variants of LucyEdit [1]: LucyEdit (Ind), which edits each video segment independently, and LucyEdit (FIFO), which follows FIFO-Diffusions diagonal denoising to process consecutive frames with increasing noise levels, simulating autoregressive generation [29]. We report visual quality and consistency metrics in Tab. 3, using VBench and cross-frame DINO/CLIP similarity metrics [35, 40]. Memory-V2V consistently outperforms baselines across all these metrics. Qualitative results in Fig. 6 show that our method achieves geometrically and visually consistent edits across long video sequences (>200 frames). 4.4. Ablation Study We first ablate the effects of main components of MemoryV2V, as summarized in Tab. 4 and Fig. 7. Dynamic tokenization combined with video retrieval improves crossiteration consistency, while adaptive token merging notably reduces computational cost. As shown in Fig. 7, retrieval guided by VideoFOV effectively preserves long-term consistency even after multiple iterations (e.g., between the 1st and 5th generations), whereas token merging maintains efficiency without visible degradation. We further analyze the computational cost in Fig. 9. We also compare adaptive token merging with token discarding baseline, where the same low-responsive tokens are either merged or removed. In Fig. 8, merging preserves appearance and motion continuity, whereas discarding introduces artifacts, indicating that merged tokens retain essential semantic information. 5. Conclusion We presented Memory-V2V, memory-augmented framework that enables consistent multi-turn video editing. 9 Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is organized as follows: Sec. introduces the rectified flow matching loss used to train Memory-V2V. Sec. provides additional implementation details and results for the context encoder experiments. In Sec. C, we describe the retrieval mechanisms for video novel view synthesis and text-guided long video editing, respectively. Moreover, in Sec. D, we detail the training and inference setups, including RoPE design and camera conditioning strategies. Sec. presents computational analysis, and Sec. includes further qualitative results of MemoryV2V. Finally, Sec. discusses limitations, failure cases, and future directions. full set of video results is available on our project page (see the included index.html). A. Preliminary: Rectified Flow Matching Recently, flow-based training frameworks [9, 15, 32, 65] have gained attention for their ability to progressively transform samples from prior distribution to the target data distribution through series of linear interpolations. Specifically, we define velocity field vt(x) of flow ψt(x) : [0, 1] Rd Rd that satisfies ψt(x0) = xt and ψ1(x0) = x1. Here, the ψt is uniquely characterized by flow ODE: dψt(x) = vt(ψt(x))dt (7) In particular, linear conditional flow defines the flow as xt = ψt(x1x0) = (1 t)x0 + tx1. Then, we can compute the velocity field vt(xtx0) = ψt(ψ1 (xtx0)x0) = x1 x0, leading to the following conditional flow matching loss: LRF = Et, x0, ϵN (0,I) (cid:104) (x0 ϵ) vθ(xt, t)2(cid:105) . (8) During inference, the predicted velocity is utilized to guide the transformation of an initial noise sample towards the target data distribution through reverse integration process. This approach enables efficient and controlled generation of high-quality samples. B. Context Encoder Experiment Details Recurrent 3D reconstructor CUT3R. CUT3R [45] is designed to predict the 3D point representation rom video stream by maintaining recurrent geometric state. Given an input frame It, the model first extracts feature map using ViT encoder, Ft = Encoder(It). Figure 10. Additional comparison of different memory encoders on two-turn novel view synthesis. The red-colored box depicts the novel region which are expected to be consistent between x1 and x2. To aggregate geometry over time, these features are passed to decoder along with the previous state st1, producing , st = Decoder(Ft, st1), where the updated state st R768768 accumulates 3D cues across frames that function as the models internal geometric memory. 3D point map is then predicted from . To evaluate whether this recurrent state can serve as memory representation, we sequentially update st over video frames, project the state via learnable MLP into the transformer latent space, and condition the diffusion model through dedicated branch (following the multimodal conditioning strategy used in SD3 [14]). The branch shares self-attention with the backbone while using separate modulation and feed-forward layers. In specific, we fine-tune for 2K steps with batch size of 32. However, the diffusion transformer backbone fails to effectively leverage the CUT3R state: generations remain unstable and lack coherent geometry, as can be seen in Fig. 3 and Fig. 10. These observations further align with our reconstruction analysis. Following CUT3Rs original procedure, we at10 Figure 11. Reconstruction results using CUT3R state s. Figure 13. Sphere sampling and Field-of-View visualization. where is the rendered view. We evaluate whether the latent state could serve as meaningful memory representation. Similar to the CUT3R setup, we project into the DiT feature dimension via learnable MLP and inject it through dedicated conditioning branch that shares self-attention layers with the main model but uses separate modulation and feed-forward layers. The model is trained for 2K steps with batch size 32. However, as illustrated in Fig. 3 and Fig. 10, conditioning on provides little benefit. While LVSMs decoder can exploit for its own renderer, the representation lacks the transferable, fine-grained geometric cues needed to guide diffusion-based generator. Consequently, the conditioned synthesis is unstable and does not improve multi-turn consistency, suggesting that the LVSM latent space is not structured for effective memory conditioning. C. Video Retrieval Algorithm Details C.1. Retrieval for Video Novel View Synthesis To compute the VideoFOV retrieval score, we first uniformly sample dense set of points on unit sphere centered at the target camera position (see Fig. 13(a)). In specific, we generate latitudelongitude grid with: Nθ = 180, Nϕ = 360, resulting in total of = NθNϕ = 64,800 sampled directions. The sampling index set is defined as: = {(u, v) {0, . . . , Nθ1}, {0, . . . , Nϕ1}} , (9) where each pair (u, v) uniquely maps to 3D point on the sphere. Fig. 13(b) illustrates how the sampled spherical points are tested for visibility given camera intrinsics and extrinsics. During training, intrinsics are taken directly from the dataset; during inference, we adopt fixed intrinsic matrix from the training set. To ensure consistent evaluation, each frames camera pose is represented as relative transform with respect to the first frame of the target video. Each sampled point (u, v) is then projected to the image plane. point is considered visible if it falls within Figure 12. Novel view synthesis results using LVSM state z. tempted to reconstruct scenes using only the recurrent state and ray map. As shown in Fig. 11, the resulting geometry is coarse and blurry, lacking meaningful fine-grained detail. This suggests that the recurrent state encodes limited and lossy structural information. As result, when used as conditioning signal, it fails to provide sufficiently informative guidance for generationleading to the degraded performance observed in our CUT3R-conditioned model. 3D Novel view synthesis model LVSM. LVSM [27] synthesizes novel views by jointly encoding input images and their corresponding Plucker ray embeddings. Given an input image Ii and Plucker embedding Pi, the model projects their concatenation into shared feature space: xi = Linear([Ii, Pi]) Rd. These tokens, together with an initial latent token e, are processed by the encoder, x1, x2, . . . , xn, = Encoder(x1, x2, . . . , xn, e), producing latent state z. For novel view rendering, the decoder conditions on the Plucker embedding of the target camera rays and predicts z, = Decoder(z, q), 11 Figure 14. Example video retrieval results using Alg. 1. Figure 15. Example video retrieval results using Alg 2. image bounds with valid (positive) depth value. The perframe field of view (FOV) is defined as: Fframe(ccam i,t ) = {(u, v) (u, v) projects inside the image} . (10) We accumulate these visibility sets across all frames to compute the video-level FOV, and then evaluate the relevance score using Eq. (1), (2), and (3). The complete retrieval workflow is provided in Alg. 1. Example retrieval results using the proposed VideoFOV algorithm are shown in Fig. 14. The top row displays the target (query) video, while subsequent rows present retrieved videos ranked by descending relevance. C.2. Retrieval for Text-guided Long Video Editing For text-guided long video editing, retrieval is performed by selecting previously edited segments whose corresponding source segments are most visually similar to the current input segment. Segment-level features are computed by extracting DINOv2 [35] embeddings for all frames within segment and averaging them into single descriptor. Cosine similarity between descriptors is then used to rank relevance. To maintain temporal coherence between consecutive segments in the output, the most recently edited segment is always included in the retrieved set. The full procedure is provided in Alg. 2. Example retrieval results are shown in Fig. 15, where semantically similar segments consistently rank highest. D. Additional Training and Inference Details D.1. Video Novel View Synthesis Positional embedding (RoPE) design. Video diffusion models typically embed the spatiotemporal structure of videos using 3D Rotary Position Embeddings (3D-RoPE), where each token is assigned positional encoding derived from its spatial coordinates (x, y) and its temporal frame index t. Since all frames share the same spatial resolution, the spatial RoPE indices remain fixed across the video. In contrast, the temporal RoPE indices are bounded by the maximum video length seen during training. When the model is required to generate videos longer than this training horizon or incorporate additional conditioning videos during multiturn editing, it must extrapolate to unseen temporal positions, which often leads to temporal drift or inconsistencies. To resolve this issue, we first formalize the three types of videos involved in multi-turn video editing. The target video is the video being generated in the current iteration and has temporal length . The user-input video is an externally provided video used to guide generation. The memory videos consist of previously generated videos that are cached and reused as conditioning inputs. Unlike the target or user-input videos, the number of memory videos may grow unboundedly across editing iterations, requiring positional encoding scheme that can support an expanding conditioning set without encountering unseen RoPE in12 Algorithm 1: VideoFOV Retrieval: top-k memory selection by spherical FOV overlap Algorithm 2: DINOv2-based Segment Similarity Ranking Input : Target video with per-frame cameras }T {K , , t=1; candidate memory videos {vi}N i=1 with per-frame cameras t}Ti {K , Ri t=1; sphere radius r; number of sphere samples ; balance λ [0, 1]; top-k. t, ti Output: Ω = top-k videos ranked by FOV score. 1 Function RelativePose(R, t, Rref , tref ): R R return (R, t) ref ref (t tref ) 3 4 7 9 10 5 Function InFOV(p, K, R, t): 6 xc R(p t) if (xc)z 0 then return false K(cid:0)xc/(xc)z pixels return inside image bounds (cid:1) // World camera // Project to 11 Function VideoFOV({Kt, Rt, tt}T t=1, = {pm}M m=1): for 1 to do for 1 to do if InFOV(pm, Kt, Rt, tt) then {pm} return 12 14 15 16 17 18 Reference the targets first frame as origin 19 (Rref , tref ) (R 1, 1) 20 Relativize all frames of and each vi via RelativePose() 21 Sample = {pm}M m=1 uniformly on the sphere of radius 22 VideoFOV({K 23 for 1 to do 24 Fi VideoFOV({K , , t }, S) , Ri t, ti t}, S) 25 soverlap Fi Fi Fi si λ soverlap + (1 λ) scontain scontain 27 28 return Ω top-k videos by si dices. To provide stable positional structure across these heterogeneous video sources, we assign each category disjoint range of temporal RoPE indices. For target video of 13 Input: Target segment Xtar with frames {F tar }Tk k=1 with frames }Ttar t=1 ; previous segments {Xk}N {F (k) DINO backbone fθ; flag enforce recent first. Output: Sorted indices π; similarity scores t=1; {simk}N 1 Function DESCRIPTOR(X): k=1. 2 3 Let contain frames {Ft}T 1 t=1 fθ(Ft) return (cid:80)T t=1 5 Function SIM(di, dj): di, dj di2 dj2 sim 7 return sim 11 8 dtar DESCRIPTOR(Xtar) 9 for 1 to do 10 dk DESCRIPTOR(Xk) simk SIM(dtar, dk) 12 π argsort({simk}, descending) 13 if enforce recent first then Move index to the front of π 14 15 return π, {simk}N k= length , we define τ tgt = {0, 1, . . . , 1}, τ usr = {T, + 1, . . . , 2T 1}, τ mem = {2T, 2T + 1, . . . , 3T 1}. (11) (12) (13) Each token at spatial location (x, y) and temporal index receives positional encoding given by RoPE(x, y, t) = [RoPEx(x), RoPEy(y), RoPEt(t)], (14) where is sampled from τ tgt, τ usr, or τ mem depending on the video type. Because this multi-video type configuration does not naturally arise during training, we adopt mixed training strategy that enables the model to correctly interpret and utilize the hierarchical RoPE layout during inference. When both the user-input and memory RoPE ranges are active, we perturb the memory-video tokens with Gaussian noise to prevent the model from overfitting to artifacts that may accumulate across iterations. Specifically, for memory token zmem, we use zmem = zmem + ϵ, ϵ (0, σ2I). (15) This encourages the model to prioritize the cleaner and more reliable user-input video. Complementarily, we disable the user-input RoPE range with probability during training, forcing the model to rely solely on τ mem. This RoPE dropout enables memory videos to serve as an independent conditioning signal. Through this formulation and training procedure, the model learns coherent and scalable RoPE structure that remains stable even as memory videos accumulate. The resulting system supports multi-turn video generation by enabling controlled information flow from the user-input video to the target video while mitigating the risk of error propagation through the memory video sets. Camera conditioning strategy. To incorporate viewpoint geometry into the generative process, we condition the feature representations on the camera extrinsics associated with each video in the conditioning set. For video consisting of frames, we denote its camera representation by camv (34), where each 3 4 block corresponds to the rigid transformation [R t] describing the rotation and translation of the camera for an individual frame. These parameters are then projected into the feature space through fully connected layer Ec(). In ReCamMaster [3], target camera camt information is injected uniformly across all features. Let Fo denote the output feature of the spatial attention layer and Fi denote the input feature to the subsequent 3D self-attention layer. The baseline formulation applies residual update of the form Fi = Fo + Ec(camt), (16) implicitly assuming that all conditioning features are derived from single video captured under single camera trajectory. However, within our memory-driven framework, each memory video is generated under its own distinct camera extrinsics. This means that the conditioning set does not share unified viewpoint, and the model must interpret the geometric context of each video individually. To achieve this, we assign to every feature token the camera embedding corresponding to the specific video from which it originates. If (v) denotes feature coming from video v, and camv is the associated camera trajectory, the camera-aware conditioning becomes Figure 16. RoPE assignments to mitigate train-inference gap. camera motion and rotation, enabling more stable and coherent viewpoint reasoning during multi-turn video generation. D.2. Text-guided Long Video Editing Dataset construction. Existing video editing datasets [4, 66] are not suited for long video editing, as they contain only short clips with limited temporal context. To enable training of Memory-V2V in this setting, we construct long-form dataset by temporally extending video editing pairs from Senorita-2M [66]. Senorita-2M provides stable local editing pairswhere foreground edits minimally affect the backgroundbut each clip is only 33 frames long, which is insufficient for long-horizon editing. To address this limitation, we apply an off-the-shelf generative video extension model, FramePack [61]. For each target clip, we retain the original 33 frames and generate 200 additional frames via forward temporal extension, resulting in 233frame sequence. The extended portion serves as memory during training; at each iteration, we randomly sample segments from this extended portion to function as memoryconditioning videos. Positional embedding (RoPE) design. Long-video editing follows the similar RoPE strategy used for video novel view synthesis. For each target segment of frames, we allocate disjoint temporal RoPE index ranges to distinguish token groups. Specifically, the target segment is assigned as τ tgt = {0, 1, . . . , 1}, (v) = (v) + Ec(camv) . (17) while the immediately preceding segment is assigned as By embedding camera trajectories on per-video basis in this manner, the model becomes capable of distinguishing heterogeneous viewpoints present across the target, userinput, and memory videos. This explicit geometric conditioning improves the models ability to handle complex τ prev = {T, + 1, . . . , 2T 1}, and other remaining segments are assigned as τ mem = {2T, 2T + 1, . . . , 3T 1}. 14 Figure 17. Computational cost analysis. Our method reduces FLOPs and latency by over 90%, with efficiency gains further increasing as the number of memory videos grows. Figure 18. Failure case. Memory-V2V exhibits difficulties when the input long video contains multiple shots with large scene transitions. G. Additional Discussion The dataset used to train Memory-V2V for text-guided long video editing consists exclusively of continuous, singleshot videos, without abrupt scene transitions. As result, Memory-V2V struggles when applied to real long-form content containing multiple shots, where substantial visual discontinuities occur at shot boundaries (see Fig. 18). In such cases, the model may incorrectly propagate objects or textures from the preceding shot into the next (e.g., hand or accessory reappearing), even if the high-level semantics remain similar. Additionally, the target videos used during training are extended using generative video model. These generated extensions exhibit mild flickering at the junction between real and synthesized segments due to temporal inconsistencies, slight changes in tone, or blur artifacts. When such imperfect segments are stored in memory, these deviations accumulate over repeated denoising steps, especially in long sequences, ultimately leading to visible appearance drift. We believe this limitation can be addressed by training with multi-shot datasets and higher-quality long-video data pairs, and we leave these directions to future work. Moreover, to further enhance the interactivity of Memory-V2V, future work could integrate it with diffusion distillation or autoregressive generation frameworks [7, 5254]. This hierarchical indexing preserves strong continuity with the most recent segment while still providing broader historical context from earlier memory clips. Mitigating training-inference gap. During training, as shown in Fig. 16 (a), the target video is temporally extended using generative model; consequently, the target frames appear earlier in the sequence, while the extended (memory) frames appear later. RoPE indices are therefore assigned in chronological order from target to memory frames. However, during inference, the memory frames correspond to previously generated results, which naturally occur before the target frames in time. If RoPE indices are assigned in the same forward order as shown in Fig. 16 (b), this mismatch leads to clear training-inference gap. To eliminate this discrepancy, we reverse the RoPE assignment for memory frames during inference, as illustrated in Fig. 16 (c). By flipping the RoPE index order, the positional structure of the conditioning sequence becomes consistent with the ordering used during training, thereby preventing the traininginference mismatch. E. Additional Computational Analysis As shown in Fig. 17, dynamic tokenization strategy yields over 90% reduction in FLOPs and latency compared to the setting where all memory videos are uniformly tokenized (e.g., with 122 spatiotemporal kernel) and processed with full attention. In addition, adaptive token merging provides further 30% reduction by removing redundant memory tokens. Consequently, even when conditioning on large number of memory videos, the inference time remains comparable to novel-view synthesis using only the user input video. F. Additional Qualitative Results Additional results for multi-turn video novel view synthesis can be found in Fig. 19 and Fig. 20, while further results for text-guided long video editing are provided in Fig. 21. 15 Figure 19. Additional qualitative results for multi-turn video novel view synthesis. Refer to our project page for video results. Figure 20. Additional qualitative results for multi-turn video novel view synthesis. Refer to our project page for video results. 16 Figure 21. Additional qualitative results for text-guided long video editing. Refer to our project page for video results."
        },
        {
            "title": "References",
            "content": "[1] Decart AI. Open-weight text-guided video editing, 2025. https://platform.decart.ai/. 2, 3, 7, 8, 9 [2] Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60346044, 2025. 8 [3] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 2, 3, 4, 7, 8, 14 [4] Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, and Qifeng Chen. Scaling instruction-based video editing with highquality synthetic dataset. arXiv preprint arXiv:2510.15742, 2025. 14 [5] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. 4, [6] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. 3 [7] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 3, 15 [8] Nan Chen, Mengqi Huang, Yihao Meng, and ZhenLonganimation: Long animation generaarXiv preprint dong Mao. tion with dynamic global-local memory. arXiv:2507.01945, 2025. 4 [9] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. 10 [10] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv e-prints, pages arXiv2305, 2023. [11] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flowguided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. 3 [12] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1770217711, 2025. 4 [13] Unreal Engine. Unreal engine. Retrieved from Unreal Engine: https://www. unrealengine. com/en-US/what-is-unrealengine-4, 2018. 3 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 7, 8, 10 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 10 [16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 3 [17] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. [18] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7621 7630, 2024. 3 [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 3 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 8 [21] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zeroshot grounded video editing using text-to-image diffusion models. arXiv preprint arXiv:2310.01107, 2023. 3 [22] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similar score distillation for zero-shot video editing. In European Conference on Computer Vision, pages 358376. Springer, 2024. 6 [23] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adapIn Proceedings of tion for text-to-video diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92129221, 2024. 2, 18 [24] Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reanglea-video: 4d video generation as video-to-video translation. arXiv preprint arXiv:2503.09151, 2025. 3 [25] Jiaxiu Jiang, Wenbo Li, Jingjing Ren, Yuping Qiu, Yong Guo, Xiaogang Xu, Han Wu, and Wangmeng Zuo. Lovic: Efficient long video generation with context compression. arXiv preprint arXiv:2507.12952, 2025. 4 [26] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3 [27] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. 2, 4, 8, 11 [28] Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video arXiv editing and generation with in-context preprint arXiv:2509.20360, 2025. 2, 3, 8 learning. [29] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. Advances in Neural Information Processing Systems, 37:8983489868, 2024. 9 [30] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 4 [31] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video szicene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025. 4, 5 [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 10 [33] YOU Meng, Zhiyu Zhu, LIU Hui, and Junhui Hou. Nvssolver: Video diffusion model as zero-shot novel view synIn The Thirteenth International Conference on thesizer. Learning Representations, 2024. 3 [34] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7, 9, 12 [36] Wenqi Ouyang, Zeqi Xiao, Danni Yang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Tokensgen: Harnessing condensed tokens for long video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1819718206, 2025. 4 [37] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. 4 [38] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 3 [39] Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Instructvid2vid: Controllable video and Yueting Zhuang. In 2024 IEEE editing with natural language instructions. International Conference on Multimedia and Expo (ICME), pages 16. IEEE, 2024. 3 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [41] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 2, 3 [42] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel URL Rolling diffusion models, 2024. Hoogeboom. https://arxiv. org/abs/2402.09470, 2024. 3 [43] HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025. 3 [44] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. 2 [45] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perIn Proceedings of the ception model with persistent state. Computer Vision and Pattern Recognition Conference, pages 1051010522, 2025. 2, 4, 8, 10 [46] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, and Mohit Bansal. Epic: Efficient video camera control learning with precise anchor-video guidance. arXiv preprint arXiv:2505.21876, 2025. [47] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 8 [48] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76237633, 2023. 3 [49] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, 19 tion. arXiv preprint arXiv:2504.12626, 2(3):5, 2025. 4, 8, 14 [62] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. [63] Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Faster video diffusion with trainable sparse attention. arXiv e-prints, pages arXiv2505, 2025. 6 [64] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. 4 [65] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 10 [66] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. 8, 9, 14 Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 6 [50] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: LongarXiv term consistent world simulation with memory. preprint arXiv:2504.12369, 2025. 4, [51] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84668476, 2024. 6 [52] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 15 [53] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [54] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv eprints, pages arXiv2412, 2024. 3, 15 [55] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. 2, 4 [56] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 2, 3, 7, 8 [57] Sihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, Jose Lezama, Irfan Essa, David Ross, and Jonathan Huang. Malt diffusion: Memory-augmented latent transformers for any-length video generation. arXiv preprint arXiv:2502.12632, 2025. [58] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [59] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2050 2062, 2025. 3 [60] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. 6 [61] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video genera-"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "KAIST"
    ]
}