{
    "paper_title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces",
    "authors": [
        "Souhail Hadgi",
        "Luca Moschella",
        "Andrea Santilli",
        "Diego Gomez",
        "Qixing Huang",
        "Emanuele Rodol√†",
        "Simone Melzi",
        "Maks Ovsjanikov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional subspaces the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 8 2 5 0 . 3 0 5 2 : r Escaping Platos Cave: Towards the Alignment of 3D and Text Latent Spaces Souhail Hadgi1 Diego Gomez1 Luca Moschella2,* Qixing Huang4 Andrea Santilli2 Emanuele Rodol`a2 Simone Melzi3 Maks Ovsjanikov 1Ecole polytechnique 2Sapienza University of Rome 3University of Milano-Bicocca 4The University of Texas at Austin"
        },
        {
            "title": "Abstract",
            "content": "Recent works have shown that, when trained at scale, unimodal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional subspaces the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish baseline for posttraining alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations. 1. Introduction Recent advances in multi-modal learning, particularly in vision-language models such as CLIP [49], have sparked interest in extending these successes to the 3D domain. Most current approaches primarily focus on training 3D encoders through triplet-based learning with pre-trained 2D vision and language encoders [34, 65, 70], leveraging new large-scale datasets such as Objaverse [10, 11], and showing promising *Currently at Apple. results in tasks such as zero-shot shape recognition. While CLIP [49] was trained with explicit alignment objectives between text and image representations, recent work has observed that even when trained independently, vision and text encoders tend to exhibit significant similarities [22, 39]. In particular, the learned latent spaces of pure (uni-modal) text and vision encoders have similar proximity structure [22] and can be aligned relatively easily after training with small number of known anchor correspondences [3, 39, 45]. Furthermore, the degree of similarity in learned features across modalities strongly correlates with the quality of performance in various downstream tasks. One prominent interpretation of these results is that given sufficient scale of training data and model complexity, different representations are converging to shared underlying structure of the physical world. This has given rise to the Platonic Representation Hypothesis [22], where different representations are viewed as projections of reality on particular modalities. At the same time, as the physical world is inherently (at least) 3-dimensional, natural question is how the structure of uni-modal vision or text encoders relates to the features learned directly from 3D data. This question raises several challenges: first, large-scale 3D datasets have only recently become available [10, 11]; second, most existing 3D foundation models are trained with explicit alignment objectives with respect to frozen 2D and text encoders, which limits the utility of post-training comparison. Finally, there is lack of universally agreed-upon architectures and training objectives for 3D data, and many commonly-used architectures tend to have limited generalization power [63]. In this work, we initiate the first study on the relation between 3D and language representations. We formulate the task of post-training alignment between range of 3D and text encoders and study the accuracy and utility of several alignment strategies. Our first insight is that when trained on pure 3D data with self-supervised objectives, 3D encoders tend to lead to representations that align only very weakly to text-based representations. We believe that this observation already 1 Figure 1. visual overview of the proposed approach. is illustrated. From left to right: We begin with two distinct input collectionsone consisting of 3D shapes and the other of textual prompts. In the blue box, independent, frozen uni-modal encoders map each modality into separate, high-dimensional latent spaces, shown in the red box. dimensionality reduction procedure is applied to project these learned spaces into low-dimensional subspaces, represented in the green box. Finally, an alignment method registers the two low-dimensional subspaces, enabling cross-modal applications such as shape retrieval, with examples depicted in the yellow box. highlights the difficulty of uni-modal training and sheds light on the scarcity of pure 3D foundation models. Additionally, we reveal that while 3D and text latent spaces are not naturally aligned, effective cross-modal translation can be achieved through subspace projection and alignment. Our key insight is that by identifying and operating in correlated subspaces, we can improve the latent space alignment of 3D and text encoders without the need for expensive joint training. To achieve this, we introduce simple but effective approach that combines Canonical Correlation Analysis (CCA) and existing alignment approaches such as affine translation [38] and local CKA [39]. This approach and our subsequent analysis extend previous works aimed at comparing learned feature spaces, but shows that careful subspace alignment can reveal subtle but important similarities, which are otherwise obscured in global comparisons. We provide visual overview of the proposed approach in Fig. 1. To summarize, our main contributions are as follows: 1. We extend the analysis of vision-text uni-modal latent space alignment to 3D uni-modal encoders and text, highlighting the limited similarity between these latent spaces and the low efficacy of current alignment approaches. 2. We propose an efficient approach for cross-modal alignment between text and 3D features that combines CCA for subspace selection and existing alignment methods. This approach improves alignment performance with minimal computational overhead, as demonstrated through experiments in matching and retrieval tasks. Our method establishes baseline for 3D-text cross-modal understanding, providing an alternative to explicit multimodal pretraining for cross-modal tasks. 3. We observe complementary structure between the spanned subspaces and the original feature spaces, enabling distinction between geometric and semantic representations. 2. Related Work Multi-modal representation learning. Multimodal representation learning has surged in recent years, driven by the success of image-language models [26, 32, 49, 53] that enable seamless cross-modal applications. These models serve as the backbone for tasks spanning from text-based image retrieval to generating high-quality visuals from natural language prompts. By aligning visual and linguistic features in shared latent space, these models have paved the way for advanced interactions between modalities, setting the stage for applications where visual and textual information are jointly processed and understood. Building on these advancements, vision-language models have recently been adapted for 3D point cloud representation, where 3D-image-text triplets [34, 64, 65, 70] enable contrastive pre-training. These models leverage powerful techniques such as momentum contrast [20] to align representations across modalities, allowing point cloud encoders to be pre-trained in multimodal context. Additionally, tech2 niques that apply vision-language models directly to 2D projections of point clouds [69, 71] have expanded cross-modal applications to 3D, including zero-shot shape classification, where models trained on 2D-image-text data demonstrate strong performance on 3D-related tasks. In the domain of multimodal synthesis, recent efforts in 2D and 3D model generation have leveraged diffusion models to tackle complex generation tasks. These models use robust priors, often trained on vast datasets, which allow them to create high-fidelity outputs in scenarios such as novel view synthesis and realistic 3D reconstructions from single RGB images [35, 66]. By combining synthetic data with diffusion-based priors, these frameworks achieve impressive zero-shot generalization and geometry-consistent 3D synthesis, opening new avenues for applications where generating realistic 3D content from limited information is critical. Representational similarity. The study of representation similarity across neural networks has seen significant rise in interest, spurred largely by seminal works originating in neuroscience and computational cognitive science. These fields have long been invested in understanding the nature and alignment of cognitive representations, providing foundational basis that has influenced the current trajectory in machine learning [44, 57]. Based on these insights, various works [2, 33, 42, 43, 45, 46, 60] provide evidence of an intrinsic connection between independently trained networks; the similarity is especially notable among large-scale models, with works such as [41, 42, 56] exploring the phenomenon. In the computer vision and pattern recognition areas, the line of work on similarity-based representations, pioneered by Duin and Pekalska [47], has also been seminal. This line of research, which examines data through the lens of similarity rather than feature attributes, has provided robust frameworks for classification and clustering [6], enabling models to generalize across patterns and variations in complex datasets. Although mostly empirical in nature, these observations find theoretical support in the study of harmonics in neural networks weights [40], Independent Component Analysis [23, 24, 27, 51] and Independent Mechanism Analysis [14, 15, 55]. These works suggest that, when capturing the same underlying data generative factors, deep learning models may converge towards similar structures despite their complexity and non-linearity. Latent space alignment. More recently, range of approaches has been developed to align latent spaces within the same modality [4, 8, 13], as well as across different modalities [38, 59, 68], particularly between visual and textual domains. Techniques such as Procrustes analysis [61] and several similarity metrics [28, 54, 58, 62], including centered kernel alignment (CKA) [9, 29, 39], have proven instrumental in aligning representations. These methods offer various strategies for quantifying similarity between feature spaces, allowing us to examine cross-modal interactions at deeper level. HHowever, these methods often focus on aligning entire latent spaces, potentially missing meaningful similarities confined to specific subspaces. Our approach builds upon CCA [21, 50], pivotal tool in pattern recognition and multi-view learning applications [17, 52]. This technique identifies maximally correlated subspaces, enabling more refined alignment across modalities by isolating core, mutually relevant components. Leveraging CCA, alignment methods can extend into complex domains, such as connecting 3D and textual latent spaces, as we demonstrate in the following. 3. Method We compare the similarity between latent (feature) spaces of various 3D and text encoders, introducing new approach that builds on existing alignment methods to improve their effectiveness. Below, we outline the tools used to measure and align these latent spaces. 3.1. Preliminaries Centered Kernel Alignment. CKA is similarity measure frequently used in recent studies [9, 29] to compare representations in neural network feature spaces. Given feature matrices Rnp and Rnq, we apply kernel functions and to obtain kernel representations = k(X, X) Rnn and = l(Y, Y) Rnn. CKA is defined as: CKA(K, L) = HSIC(K, L) (cid:112)HSIC(K, K)HSIC(L, L) (1) where HSIC is the Hilbert-Schmidt Independence Criterion [16] and can be written as HSIC(K, L) = 1 (n 1)2 tr(KHLH) , (2) where = 1 11. Canonical Correlation Analysis. CCA [21, 50] is statistical method that finds linear projections of two datasets, maximizing their correlation in shared latent space. Formally, given two sets of zero-centered variables Rnd1 and Rnd2, CCA seeks two projection matrices WX Rd1k and WY Rd2k that map and into common k-dimensional space, maximizing the correlation between the projections XWX and YWY. The optimization can be expressed as: max WX,WY corr(XWX, YWY) , (3) where corr(, ) represents the correlation between the projected variables. 3 3.2. Alignment approaches Recent developments in latent space alignment have introduced various methods. In this work, we examine both the affine transformation approach in [38] and the CKA-based matching approach from [39], observing their limited effectiveness in 3D-text latent space alignment, and propose method to address these limitations. Latent Space Translation through Affine Transformation [38]. It is possible to estimate an affine transformation that maps one latent space onto another latent space such that (x) = Rx + b, . To compute , we assume access to an anchor subset consisting of groundtruth paired samples from both latent spaces. These anchors enable us to determine by optimizing through gradient descent or, if = 0 by minimizing the least squares error. It assumes that and share the same dimensionality and are normalized to zero mean and unit variance. These constraints can be enforced by zero-padding the smaller latent space. Local CKA-based retrieval and matching [39]. The CKA metric computed between two sets of features is sensitive to ordering and maximized when ground-truth pairs are aligned, this insight can be used to match unseen data by including them in well-aligned anchors set. Formally, starting from aligned set of features XA and YA, we compute for query pair (xq, yq) its local CKA defined as: and WYA Rqk. All samples are then projected into this reduced space: Xr = XWXA , Yr = YWYA . (5) RnAk. to the reduced anchors as In particular, we refer RnAk and Yr Xr Our experiments show that projecting 3D and text latent spaces into lower-dimensional, shared subspace improves alignment by isolating features that are highly correlated across modalities Alignment of projected latent spaces. Given the projected latent spaces, we can apply either of the previously described alignment methods in the reduced space. For the affine transformation approach, we learn mapping between Xr and Yr using the projected anchor pairs Xr to optimize the transformation parameters and b: A, Yr (Xr) = RXr + b, (6) Alternatively, we can employ the local CKA-based matching q, yr approach in the projected space. For query pair (xr q), we compute its local CKA using the projected anchor sets: localCKA(xr q, yr q) = CKA(K[Xr A,xr ], K[Yr A,yr ]), (7) localCKA(xq, yq) = CKA(K[XA,xq], K[YA,yq]) , (4) 4. Experimental setup where [X, x] denotes the column-wise concatenation of matrix and vector x. Local CKA calculates similarity in pairwise manner, accounting for anchor alignment while being sensitive to ordering among query pairs. We introduce all possible query pairs, where correctly matched pairs exhibit the highest local CKA. 3.3. Proposed method. Our goal is to reliably align the latent spaces of pre-trained 3D and text encoders. Given dataset of caption-point cloud pairs, each embedded in the corresponding feature space, and represented as matrices Rnp and Rnq respectively, we first select subset of anchor pairs that will guide our translation process, denoted as XA RnAp and YA RnAq. Building upon the mathematical foundations introduced in Section 3.2, we develop pipeline that combines dimensionality reduction with the previously introduced alignment methods. Common Subspace Projection. In our work, we show that 3D and text latent spaces can effectively be aligned in lowdimensional connected subspaces. We begin by applying CCA to the anchor pairs to identify shared k-dimensional subspace (with < p, q) that connects point cloud and text latent spaces. This yields projection matrices WXA Rpk 4.1. Pre-training Dataset Prior works in point cloud pre-training, particularly within uni-modal frameworks, have relied heavily on ShapeNet [5], dataset of 51,300 annotated 3D synthetic shapes spanning 55 categories. ShapeNet has been instrumental in advancing foundational methods, yet it remains limited by the relatively narrow scope of categories. With the release of Objaverse [10], which includes over 800,000 shapes across diverse realworld categories, new standard for large-scale representation learning in the 3D domain has emerged. Objaverses extensive shape diversity makes it ideal for both uni-modal and multi-modal learning. Despite these advantages, there is lack of works that explore uni-modal pre-training specifically on Objaverse. 4.2. Encoders We explore both multi-modal and uni-modal 3D and text encoders across varying levels of model complexity. Multi-modal 3D Encoders. For multi-modal pre-training, we use OpenShape [34] pre-trained models, trained on point cloud, image, and text triplets of Objaverse with contrastive pre-text task to align 3D encoders with frozen CLIP encoders. OpenShapes pipeline also includes enhanced captions, generated by applying Blip [31] on images and GPT-4 [1] for 4 Method 3D Encoder Affine + Subspace Projection Affine + Subspace Projection Affine Affine Affine Affine + Subspace Projection (Ours) Affine + Subspace Projection (Ours) Affine + Subspace Projection (Ours) Local CKA Local CKA Local CKA Local CKA + Subspace Projection (Ours) Local CKA + Subspace Projection (Ours) Local CKA + Subspace Projection (Ours) PointBert SparseConv PointBert SparseConv Pointnet PointBert SparseConv Pointnet PointBert SparseConv Pointnet PointBert SparseConv Pointnet RoBERTa Matching accuracy Top-5 retrieval Matching accuracy Top-5 retrieval Matching accuracy Top-5 retrieval BERT CLIP Multi-modal 3D encoder 85.6 87. Uni-modal 3D encoder 23.6 34.4 21.8 42.2 45.6 36.6 15.2 13.6 18.0 60.19 56.9 53.0 67.6 67.6 15.8 11.0 18.4 30.8 21.4 25.2 5.8 3.4 6.6 29.4 19.0 26.8 55.2 57.2 7.8 6.0 8.0 23.2 19.2 22.4 1.8 1.79 2.4 17.0 15.8 18. 75.8 75.6 15.6 20.0 10.2 28.4 16.8 20.8 1.4 1.6 2.0 42.4 34.0 40.2 45.0 46.8 6.4 4.2 9.6 15.6 15.8 16.6 1.0 0.6 1.0 15.0 10.0 14.3 70.6 69.2 13.4 16.2 12.2 18.2 15.0 16.0 4.39 3.8 5.0 37.2 30.8 Table 1. Matching and retrieval performance across 3D and text encoders using different alignment approaches. We use 30,000 anchors for subspace projection and affine transformation approaches, and 1,000 anchors for local CKA. query set of 500 is uniformly sampled, with results averaged over 3 different seeds. The subspace dimension is fixed at 50. SparseConv corresponds to MinkowskiNet. Our approach (Ours) consistently demonstrates improved matching and retrieval performance, with multi-modal 3D encoders setting the upper bound for performance. Additional top-k retrieval metrics are provided in the supplementary. caption refinement. To evaluate the effect of different architectures, we include both transformer-based model, PointBERT [67], and sparse convolution-based model, MinkowskiNet [7], each representing distinct approaches in processing 3D data. Uni-modal 3D Encoders. For the uni-modal setup, we pretrain PointBERT on Objaverse using its original pretext tasks, which include masked point reconstruction and uni-modal contrastive loss to encourage robust shape representation. We also explore two additional architectures: MinkowskiNet model and simpler architecture in PointNet [48], each pre-trained using shape-level contrastive learning method [19]. This approach contrasts different partial views of input shapes. Across all 3D encoders in this setup, we fix the latent dimension at 512 to maintain consistency in representation space comparisons. Text encoders. We use the text encoder from OpenCLIP ViT-bigG-14 [25], chosen to match the multi-modal encoders used in OpenShape. Additionally, we examine alignment with purely uni-modal text encoders by including BERT [12] and RoBERTa [36], popular encoders that enable analysis of uni-modal text-3D representation alignment. Across all pre-training setups, parameters are kept consistent to facilitate direct comparisons. 4.3. Downstream tasks We evaluate our alignment approach on Objaverse-LVIS [18], human-verified test subset of Objaverse that contains 1,156 object categories. This subset is specifically reserved for evaluation and is unseen during pre-training. The captions are generated with Cap3D [37], which provides enhanced descriptive text for each 3D shape. Our evaluation framework consists of two main tasks: matching and retrieval. The matching task involves finding the correct permutation of captions for perfect matching given shuffled set of query images and their corresponding captions; we utilize the linear sum assignment approach to perform this task [30]. For the retrieval task, the model must identify the correct 3D object from the query set based on text caption. These tasks are particularly effective in measuring the cross-modal capabilities of encoders, and have been evaluated in prior Vision-Text studies [39]. While our main results emphasize the matching and top-5 retrieval tasks, we provide comprehensive evaluation of top-1 and top-10 retrieval metrics in the supplementary. 5. Results 5.1. Are 3D and Text Latent Spaces similar ? To evaluate the inherent similarity between 3D and text feature spaces without alignment, we compute linear CKA scores for both unimodal and multimodal encoders. The results are shown in Fig. 2. Comparison of 3D-Text and Vision-Text alignment. Prior work [22] reports CKA alignment values ranging from approximately 30% to 48% between different uni-modal vision and text encoders (see Figure 13 in [22]). In stark contrast, we find that the default alignment between 3D and text latent spaces is significantly weaker, with maximum score of 0.12 observed for the uni-modal PointBERT and CLIP pair. This substantial gap underscores key insight: unlike vision and text encoders, 3D encoders did not converge to structures similar to text. Alignment favors multi-modality. We observe that 3D multi-modal encoders demonstrate the highest CKA score with text encoders, particularly with CLIPs text encoder. 5 Figure 2. Linear CKA scores between text and 3D encoders without alignment. SpConv corresponds to MinkowskiNet. Higher scores reflect stronger alignment between encoder pairs, with the strongest alignment observed between multi-modal 3D encoders and CLIP text encoder due to their shared training on aligned representations. Uni-modal 3D encoders show significantly lower alignment with text encoders, though slightly higher with the CLIP text encoder. This behavior is expected, given that 3D multi-modal encoders are explicitly trained to align with CLIP latent spaces, which share the same textual modality. Even among unimodal 3D encoders, alignment with the CLIP text encoder is notably higher (e.g. 0.12 for PointBERT-CLIP) compared to alignment with uni-modal text encoders like RoBERTa (e.g., 0.04 for PointBERT-RoBERTa). This suggests that the visual understanding embedded in CLIPs text encoder extends beyond image and text domains to include 3D representations, despite the lack of explicit alignment during pre-training. 5.2. Latent Space Alignment results We evaluate the performance of the alignment approaches outlined in Sec. 3 using matching and retrieval tasks, and analyze their effectiveness in aligning 3D and text encoders. Unless otherwise specified, we fix the subspace dimension to = 50 and the number of anchors to 30, 000. For downstream tasks, we uniformly sample query set of size 500 and average results over 3 different seeds. For the affine transformation approach, we present results for the text-to3D direction, noting that similar performance is achieved in the 3D-to-text direction as seen in Fig. 3. Existing Approaches Enable Limited Alignment We first assess whether previous successful approachesaffine transformation and local CKAcan achieve meaningful 3D-text alignment. As shown in Tab. 1, both methods yield modest alignment improvements over the unaligned baseline, where unimodal 3D encoders start near zero in matching Figure 3. Linear CKA scores between text and 3D encoders after affine translation. SpConv refers to MinkowskiNet. Affine translation results in similar increase in similarity for both 3D-totext and text-to-3D directions. and retrieval tasks. These results suggest small, yet notable, alignment shift. However, even with this improvement, alignment remains significantly lower than the alignment achieved in vision-text benchmarks [39]. This finding hints at the limits of uni-modal 3D encoders in achieving similar Vision-Text alignment performance, and might necessitate different approach to possibly achieve notable alignment. Importance of subspace projection (Ours). While aligning entire latent spaces of 3D and text encoders achieved some success, results remained well below vision-text alignment benchmarks. Motivated by this limitation, we propose aligning lower-dimensional subspaces, based on the hypothesis that 3D and text representations might intersect within shared latent subspace. Using validation set, we report in Fig. 4 the impact of subspace dimension, obtained through our CCA approach (see Sec. 3.3) combined with the affine transformation on the top-5 retrieval accuracy of uni-modal PointBert. Notably, while affine alignment alone yields better performance at higher dimensions, our subspace projection approach significantly outperforms it when the subspace dimension is reduced. This finding indicates that alignment quality is optimized within carefully chosen lowerdimensional spaces, reinforcing our hypothesis that 3D-text alignment is more successful within targeted subspaces. The results in Tab. 1 further validate this approach, showing substantial increase in matching and retrieval performance across all 3D and text encoder pairs, outperforming both alignment approaches when they operate on the original latent space. 3D encoder complexitys low impact on alignment. As anticipated, multi-modal 3D encoders reach the highest alignment performance across all metrics, effectively setting an upper bound. Interestingly, however, PointBERT outperforms the more complex SparseConv among uni-modal 3D encoders, suggesting that increasing model complexity alone does not guarantee improved alignment in 3Dtext tasks. Surprisingly, even PointNeta relatively simple modelachieves similar alignment scores, showing that factors other than model complexity may play pivotal role in 3D-text alignment. This observation contrasts with vision-text alignment results [22, 39], where complex models typically leverage large datasets more effectively. Our findings thus indicate distinctive aspect of 3D-text alignment: model simplicity does not impede, and may even aid, the interpretability and compatibility of learned features for cross-modal alignment. Figure 4. Impact of subspace dimensionality on retrieval performance. Comparison of three approaches: our proposed CCA + affine translation method (blue), affine translation without subspace projection (red), and baseline feature space alignment without transformation (orange). Results are shown for the uni-modal PointBERT and CLIP text encoder, with generalizations to other encoders provided in the supplementary. Figure 5. Effect of anchor set size on retrieval performance. Validation set results with fixed subspace dimension = 50 show that retrieval performance improves as the anchor subset size increases but eventually reach plateau. Results are shown for the uni-modal PointBERT and CLIP text encoder, with generalizations to other encoders provided in the supplementary. have alignment techniques Different different strengths. Our experiments reveal that different alignment techniques offer complementary strengths across tasks. For instance, affine transformation proves particularly effective for matching tasks across all 3D encoders, while local CKA shows superior performance in top-5 retrieval accuracy. This suggests that while some methods excel in precision tasks, others might better capture broader semantic nuances, making them more suitable for retrieval. Together, these observations imply that hybrid approach could leverage the unique strengths of each method, opening up promising directions for future cross-modal applications. Scaling of our approach. In Fig. 5, we explore the scalability of our approach by analyzing how performance responds to increasing the number of anchors. Notably, our subspace projection method scales effectively with anchor count, reaching plateau before requiring the full dataset (over 800,000 shapes). This scalability highlights the approachs efficiency in learning robust 3D-text mappings with limited subset of anchor pairs. However, we observe leveling off in performance gains beyond certain anchor count, likely due to the constraints imposed by the low-dimensional 7 Figure 6. Pearson correlation between shape query chamfer distances and pairwise distances in the projected text latent subspace. We observe higher Pearson correlation in the projected text latent subspace with optimal subspace dimension. Results are shown for the uni-modal PointBERT and CLIP text encoders. subspace. This suggests that while our approach is computationally efficient and data-efficient, its reliance on fixed lower-dimensional subspace could limit its adaptability to larger, more diverse datasets. yields notably high correlation with Chamfer distances, signaling notable increase in the sensitivity of the text encoder to geometric properties when projected into this specific subspace. Increased semantic understanding of the 3D latent subspace. To assess the improved semantic capacity of the 3D subspace, we conducted qualitative analysis of shape retrieval performance in both the reduced and the original latent spaces  (Fig. 7)  . Given set of query shapes, we observe that the reduced subspace retrieves shapes with stronger semantic similarity (e.g., retrieving animals for query sheep shape), while the original latent space primarily favors geometric resemblance (e.g retrieving stacked square shapes when querying 3D model of CPU). This shift suggests that our subspace projection method enables the 3D encoder to capture semantic relationships that are absent in the full latent space. This effectively enables cross-modal applications and explains its higher matching and retrieval performance. 6. Conclusion, Limitations and Future Work In this work, we present the first study investigating latent space alignment between 3D and text pre-trained encoders. Building on the hypothesis that these modalities share semantic connections within lower-dimensional subspaces, we propose an effective approach combining CCA projection with affine transformation estimation to translate between modalities latent spaces. Our empirical results show that optimal cross-modal performance is achieved through lowdimensional subspace projection, and our method successfully improves alignment across diverse 3D and text unimodal encoders. While CLIP-based multi-modal encoders establish performance upper bounds, we enable significant cross-modal capabilities in uni-modal encoders previously limited to single-modality tasks. We also demonstrate that semantic understanding can be extracted from geometry-aware latent spaces of uni-modal 3D encoders. Although our work focused on Objaverse, which is the first large-scale 3D dataset, it would be interesting to consider how scaling on Objaverse-XL [11] would affect the alignment quality between 3D and text encoders. Moreover, in this work we do not distinguish object-level vs. scene-level annotations, and decomposing objects or scenes into their composing blocks could shed light onto the compositionality of the learned representations. Finally, the subspace alignment method that we introduce in this work can be broadly applicable to other representations as well. In the future, we plan to use it to investigate the limitations of alignment observed in other representations. In particular, even when trained at significantly higher data scales, images and text representations do not align perfectly [22], and it would be interesting to reveal the unique and complementary nature of different modalities via subspace analysis. Figure 7. Shape retrieval comparison between original and reduced latent spaces. For given query shape, we retrieve the closest match based on cosine similarity. Results demonstrate higher semantic understanding in the reduced 3D latent subspace compared to the original latent space. Shown for uni-modal PointBERT and CLIP. 5.3. Geometries vs. semantics. Our quantitative evaluation shows that low-dimensional subspace projection significantly improves latent space alignment in the 3D-Text setting. To better understand the characteristics of these subspaces spanned relative to the original spaces, we analyze the increase in semantic and geometric knowledge within the projected spaces. Increased geometric awareness of the text latent subspace. To quantify the increase in geometric awareness within the projected text subspace, we calculate the Pearson correlation between the Chamfer distances of query set of 500 shapes and the pairwise distances between feature vectors within the text subspace. We then compare these results to those obtained from the original text latent space. Results in Fig. 6 show that the optimal subspace dimension 8 Acknowledgements Parts of this work were supported by the ERC Starting Grant 758800 (EXPROTEA), ERC Consolidator Grant 101087347 (VEGA), ANR AI Chair AIGRETTE, as well as gifts from Ansys and Adobe Research. This work was also supported by the Galileo 2022 fellowship from the Universit`a Italo Francese/ Universite Franco Italienne (UIF/UFI) within the project G22 4 titled Multimodal Artificial Intelligence for 3D shape analysis, modeling and applications."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 4 [2] Lisa Bonheme and Marek Grzes. How do variational autoencoders learn? insights from representational similarity. arXiv preprint arXiv:2205.08399, 2022. 3 [3] Irene Cannistraci, Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, and Emanuele Rodol`a. Bootstrapping parallel anchors for relative representations. In The First Tiny Papers Track at ICLR 2023, Tiny Papers at ICLR 2023, 2023. 1 [4] Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca, and Emanuele Rodol`a. From bricks to bridges: Product of invariances to enhance latent space communication. In The Twelfth International Conference on Learning Representations, 2024. 3 [5] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An informationrich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 4 [6] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Cazzanti. Similarity-based classification: Concepts and algorithms. Journal of Machine Learning Research, 10(27): 747776, 2009. [7] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30753084, 2019. 5 [8] Donato Crisostomi, Irene Cannistraci, Luca Moschella, Pietro Barbiero, Marco Ciccone, Pietro Lio, and Emanuele Rodol`a. In From charts to atlas: Merging latent spaces into one. NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations, 2023. 3 [9] MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and Eugene Belilovsky. Reliability of cka as similarity measure in deep learning. arXiv preprint arXiv:2210.16156, 2022. 3 [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. 2023 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1314213153, 2022. 1, 4 [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 1, 8 [12] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [13] Marco Fumero, Marco Pegoraro, Valentino Maiorca, Francesco Locatello, and Emanuele Rodol`a. Latent functional maps: spectral framework for representation alignment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3 [14] Shubhangi Ghosh, Luigi Gresele, Julius von Kugelgen, Independent Michel Besserve, and Bernhard Scholkopf. mechanism analysis and the manifold hypothesis, 2023. 3 [15] Luigi Gresele, Julius Von Kugelgen, Vincent Stimper, Bernhard Scholkopf, and Michel Besserve. Independent mechanism analysis, new concept? In Advances in Neural Information Processing Systems, 2021. 3 [16] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with hilbertschmidt norms. In International conference on algorithmic learning theory, pages 6377. Springer, 2005. 3 [17] Chenfeng Guo and Dongrui Wu. Canonical correlation analysis (cca) based multi-view learning: An overview. arXiv preprint arXiv:1907.01693, 2019. [18] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 5 [19] Souhail Hadgi, Lei Li, and Maks Ovsjanikov. To supervise or not to supervise: Understanding and addressing the arXiv preprint key challenges of 3d transfer learning. arXiv:2403.17869, 2024. 5 [20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual reparXiv preprint resentation learning. arxiv e-prints, art. arXiv:1911.05722, 2019. 2 [21] Harold Hotelling. Relations between two sets of variates. In Breakthroughs in statistics: methodology and distribution, pages 162190. Springer, 1992. 3 [22] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. 1, 5, 7, 8 [23] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA. In Advances in Neural Information Processing Systems. Curran Associates, Inc. [24] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, pages 859868. PMLR. 3 [25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal 9 Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 5 [26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. 2 [27] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational Autoencoders and Nonlinear ICA: Unifying Framework. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, pages 22072217. PMLR. [28] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. Similarity of neural network models: survey of functional and representational measures, 2024. 3 [29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 35193529. PMLR, 2019. 3 [30] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. 5 [31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. 4 [32] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965 10975, 2022. 2 [33] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543, 2015. 3 [34] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in neural information processing systems, 36, 2024. 1, 2, [35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. 2023. 3 [36] Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019. 5 [37] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. 5 [38] Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, and Emanuele Rodol`a. Latent space translation via semantic alignment. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 4 [39] Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel OConnor. Do vision and language encoders represent the world similarly? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1433414343, 2024. 1, 2, 3, 4, 5, 6, [40] Giovanni Luca Marchetti and Christopher Hillar. Harmonics of learning: Universal fourier features emerge https : / / synthical . in invariant networks. com / article / 03032df5 - f9c5 - 43a6 - 8253 - d4af1d67e0d4, 2023. 3 [41] Raghav Mehta, Vƒ±tor Albiero, Li Chen, Ivan Evtimov, Tamar Glaser, Zhiheng Li, and Tal Hassner. You only need good embeddings extractor to fix spurious correlations, 2022. 3 [42] Tomas Mikolov, Quoc Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013. 3 [43] Ari S. Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks with canonical correlation. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, page 57325741, Red Hook, NY, USA, 2018. Curran Associates Inc. 3 [44] Luca Moschella. Latent communication in artificial neural networks, 2024. [45] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodol`a. Relative representations enable zero-shot latent space communication. In The Eleventh International Conference on Learning Representations, 2023. 1, 3 [46] Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodola, and Francesco Locatello. Asif: Coupled data turns unimodal models to multimodal without training. Advances in Neural Information Processing Systems, 36:1530315319, 2023. 3 [47] E. Pekalska and Robert Duin. Automatic pattern recognition by similarity representations. Electronics Letters, 37:159 160, 2001. 3 [48] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652660, 2017. 5 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2 [50] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. Advances in neural information processing systems, 30, 2017. [51] Geoffrey Roeder, Luke Metz, and Durk Kingma. On Linear Identifiability of Learned Representations. In Proceedings of the 38th International Conference on Machine Learning, pages 90309039. PMLR. 3 10 VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 574591. Springer, 2020. 1 [64] Le Xue, Mingfei Gao, Chen Xing, Roberto Martƒ±n-Martƒ±n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11791189, 2023. 2 [65] Le Xue, Ning Yu, Shu Zhang, Artemis Panagopoulou, Junnan Li, Roberto Martƒ±n-Martƒ±n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, et al. Ulip-2: Towards scalable multimodal pre-training for 3d understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2709127101, 2024. 1, 2 [66] Yuxuan Xue, Xianghui Xie, Riccardo Marin, and Gerard. Pons-Moll. Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models. 2024. [67] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, and Jie Zhou. Pre-training 3d point cloud transformers with masked point modeling. 2022 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1929119300, 2021. 5 [68] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1810218112, 2022. 3 [69] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85528562, 2022. 3 [70] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. 1, 2 [71] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng, Shanghang Zhang, and Peng Gao. Pointclip v2: Adapting clip for powerful 3d open-world learning. arXiv preprint arXiv:2211.11682, 3(4), 2022. 3 [52] Jan Rupnik and John Shawe-Taylor. Multi-view canonical correlation analysis. In Conference on data mining and data warehouses (SiKDD 2010), pages 14, 2010. [53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [54] Mahdiyar Shahbazi, Ali Shirali, Hamid Aghajan, and Hamed Nili. Using distance on the riemannian manifold to compare representations in brain and in models. NeuroImage, 239: 118271, 2021. 3 [55] Joanna Sliwa, Shubhangi Ghosh, Vincent Stimper, Luigi Gresele, and Bernhard Scholkopf. Probing the robustness of independent mechanism analysis for representation learning. In UAI 2022 Workshop on Causal Representation Learning, 2022. 3 [56] Gowthami Somepalli, Liam Fowl, Arpit Bansal, Ping YehChiang, Yehuda Dar, Richard Baraniuk, Micah Goldblum, and Tom Goldstein. Can neural nets learn the same model twice? investigating reproducibility and double descent from the decision boundary perspective. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1368913698, 2022. 3 [57] Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. OConnell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Muller, Mariya Toneva, and Thomas L. Griffiths. Getting aligned on representational alignment, 2023. 3 [58] Shuai Tang, Wesley J. Maddox, Charlie Dickens, Tom Diethe, and Andreas Damianou. Similarity of neural networks with gradients, 2020. [59] Thomas Theodoridis, Theocharis Chatzis, Vassilios Solachidis, Kosmas Dimitropoulos, and Petros Daras. CrossIn 2020 modal variational alignment of latent spaces. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 41274136, 2020. 3 [60] Ivan Vulic, Sebastian Ruder, and Anders S√∏gaard. Are all good word vector spaces isomorphic? arXiv preprint arXiv:2004.04070, 2020. 3 [61] Chang Wang and Sridhar Mahadevan. Manifold alignment using procrustes analysis. In Proceedings of the 25th international conference on Machine learning, pages 11201127, 2008. 3 [62] Alex Williams, Erin Kunz, Simon Kornblith, and Scott Linderman. Generalized shape metrics on neural representations. In Advances in Neural Information Processing Systems, 2021. 3 [63] Saining Xie, Jiatao Gu, Demi Guo, Charles Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised preIn Computer training for 3d point cloud understanding. Escaping Platos Cave: Towards the Alignment of 3D and Text Latent Spaces"
        },
        {
            "title": "Supplementary Material",
            "content": "different pairs of text and 3D encoders, as we observe in Fig. 9. This consistency suggests that the alignment of latent spaces depends on comparable amount of data, with limited scalability to very large datasets. This supplementary document provides additional details and results to complement the main material. Specifically, we first extend the matching and top-5 downstream results from Table 1 in the main paper by including top-1 and top-10 retrieval scores in App. A. Then, in App. B, we analyze the impact of chosen subspace dimensions and anchor counts across other pairs of text and 3D encoders. These additional results provide more comprehensive understanding of our alignment approach and its performance across different setups. A. Downstream results We extend our evaluation by including top-1 and top-10 retrieval metrics, which complement the matching and top-5 results presented in the main paper by offering additional perspectives. As shown in Tab. 2, these results emphasize the consistency of our findings: the combination of local CKA and our proposed subspace projection method consistently achieves superior performance in retrieval tasks, whereas the affine approach demonstrates better results in matching tasks  (Table 1)  . This highlights that method performance can vary significantly depending on the downstream task, reflecting the distinction between overall assignment accuracy (matching) and query-specific precision (top-1 retrieval). Among uni-modal 3D encoders, PointBERT performs best. Meanwhile, CLIP continues to excel as the most effective text encoder, which shows its generalizability across modalities. B. Additional ablations Dimensionalitys impact on alignment. We generalize the dimension analysis to additional pairs of text and 3D encoders in Fig. 8, extending the findings presented in the main paper. The results confirm that our method consistently achieves better alignment in low-dimensional subspaces across all evaluated pairs, which reaffirms the importance of dimensionality reduction to enable our subspace projection approach. The optimal subspace dimension is often consistent across different encoders, but exceptions are observed. For example, MinkowskiNet exhibits improved performance at higher dimensions (e.g. 200 vs. 50), which shows that encoders have representations that might align differently. This variability highlights that the ideal subspace dimension for balancing geometric and semantic features, while being low, is not fixed but encoder-dependent. Number of anchors impact on alignment. We find that aligning representations using our subspace projection approach generally requires similar number of anchors across 12 Method 3D Encoder CLIP Top-1 retrieval Top-10 retrieval RoBERTa Top-1 retrieval Top-10 retrieval BERT Top-1 retrieval Top-10 retrieval Affine + Subspace Projection Affine + Subspace Projection Affine Affine Affine Affine + Subspace Projection (Ours) Affine + Subspace Projection (Ours) Affine + Subspace Projection (Ours) Local CKA Local CKA Local CKA Local CKA + Subspace Projection (Ours) Local CKA + Subspace Projection (Ours) Local CKA + Subspace Projection (Ours) PointBert SparseConv PointBert SparseConv Pointnet PointBert SparseConv Pointnet PointBert SparseConv Pointnet PointBert SparseConv Pointnet 56.4 58. 9.8 10.6 7.0 18.0 13.0 14.0 5.4 3.4 4.2 30.0 21.2 23.79 Multi-modal 3D encoder 90.8 93.6 Uni-modal 3D encoder 37.2 46.2 30.2 57.4 58.0 44.8 24.4 23.59 28.19 70.8 64.0 62.6 38.8 46. 42 8.0 3.4 10.8 8.2 7.0 0.2 0.2 0.0 17.8 14.79 15.8 81.8 86.2 22.2 29.4 22.0 36.6 29.4 35.8 4.0 3.2 3.59 54.4 42.4 49.2 32.4 37.2 3.4 3.2 3.0 7.6 5.2 6.6 0.8 0.6 1.0 13.6 11.4 14.6 78.8 79. 22.6 20.4 20.0 25.8 21.0 23.8 7.19 6.4 8.0 51.0 41.4 45.4 Table 2. Top-1 and top-5 retrieval accuracy across 3D and text encoders using different alignment approaches. We use 30,000 anchors for subspace projection and affine transformation approaches, and 1,000 anchors for local CKA. query set of 500 is uniformly sampled, with results averaged over 3 different seeds. The subspace dimension is fixed at 50. SparseConv corresponds to MinkowskiNet. Our approach (Ours) consistently demonstrates improved retrieval performance, with multi-modal 3D encoders setting the upper bound for performance. 13 (a) CLIP and PointBert (b) CLIP and MinkowskiNet (c) CLIP and PointNet (d) RoBERTa and PointBert (e) RoBERTa and MinkowskiNet (f) RoBERTa and PointNet (g) BERT and PointBert (h) BERT and MinkowskiNet (i) BERT and PointNet Figure 8. Impact of subspace dimensionality on retrieval performance. Comparison of two approaches: our proposed CCA + affine translation method (blue) and affine translation without subspace projection (red). Each plot corresponds to pair of Text Encoder and 3D Encoder. Optimal downstream performance is obtained with low-dimensional subspace projection, although the exact dimension differs from encoder to another. 14 (a) CLIP and PointBert (b) CLIP and MinkowskiNet (c) CLIP and PointNet (d) RoBERTa and PointBert (e) RoBERTa and MinkowskiNet (f) RoBERTa and PointNet (g) BERT and PointBert (h) BERT and MinkowskiNet (i) BERT and PointNet Figure 9. Effect of anchor set size on retrieval performance. Validation set results with fixed subspace dimension = 50 show that retrieval performance improves as the anchor subset size increases but eventually reach plateau. Each plot corresponds to pair of Text Encoder and 3D Encoder. Anchors scalability is constant across different encoders settings."
        }
    ],
    "affiliations": [
        "Ecole polytechnique",
        "Sapienza University of Rome",
        "The University of Texas at Austin",
        "University of Milano-Bicocca"
    ]
}