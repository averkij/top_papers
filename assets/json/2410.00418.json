{
    "paper_title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
    "authors": [
        "Guy Ohayon",
        "Tomer Michaeli",
        "Michael Elad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . e [ 1 8 1 4 0 0 . 0 1 4 2 : r POSTERIOR-MEAN RECTIFIED FLOW: TOWARDS MINIMUM MSE PHOTO-REALISTIC IMAGE RESTORATION Guy Ohayon, Tomer Michaeli, Michael Elad TechnionIsrael Institute of Technology {ohayonguy@cs,tomer.m@ee,elad@cs}.technion.ac.il"
        },
        {
            "title": "ABSTRACT",
            "content": "Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize weighted sum of distortion loss (e.g., MSE) and perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to high-quality image using rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on variety of image restoration tasks. Our codes are available at https://github.com/ohayonguy/PMRF."
        },
        {
            "title": "INTRODUCTION",
            "content": "Photo-realistic image restoration (PIR) is the task of reconstructing visually appealing images from degraded measurements (e.g., noisy, blurry). This is long-standing research problem with diverse applications in mobile photography, surveillance, remote sensing, medical imaging, and more. PIR algorithms are commonly evaluated by distortion measures (e.g., PSNR, SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018)), which quantify some type of discrepancy between the reconstructed images and the ground-truth ones, and by perceptual quality measures (e.g., FID (Heusel et al., 2017), KID (Binkowski et al., 2018), NIQE (Mittal et al., 2013), NIMA (Talebi & Milanfar, 2018)), which are intended to predict the extent to which the reconstructions would look natural to human observers. Since distortion and perceptual quality are typically at odds with each other (Blau & Michaeli, 2018), the core challenge in PIR is to achieve minimal distortion without sacrificing perceptual quality. common way to approach this task is through posterior sampling (Bendel et al., 2023; Chung et al., 2023; Daras et al., 2024; Kawar et al., 2021a;b; 1 Illustration of Figure 1: the distortionperception tradeoff, where distortion is measured by MSE. Many photo-realistic image restoration methods aim for posterior sampling. Theoretically, this approach achieves perfect perceptual index (p ˆX = pX ) but its MSE is twice the MMSE. In contrast, we aim for the estimator ˆX0 that minimizes the MSE under perfect perceptual index constraint (Eq. (3)), which typically achieves smaller MSE than posterior sampling. Figure 2: Visual results of PMRF (our method) on the CelebA-Test blind face image restoration data set. Our algorithm produces sharp and visually appealing details while maintaining incredibly low distortion according to variety of measures simultaneously. See Table 1. Table 1: Quantitative evaluation of state-of-the-art blind face image restoration algorithms on the CelebA-Test benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Our method achieves the best FID, KID, PSNR and SSIM, and the second or third best scores in the rest of the perceptual quality and distortion measures. visual comparison is provided in Figure 2 and Figure 5 in the appendix. Perceptual Quality Distortion Method FID KID NIQE Precision PSNR SSIM LPIPS Deg LMD DOT 100.2 0.0914 6.462 0.1600 21.32 0.6636 0.4756 43.87 2.876 RestoreFormer++ 41.15 0.0290 4.187 42.30 0.0301 4.405 53.16 0.0425 4.649 41.79 0.0297 3.693 46.77 0.0346 4.169 46.72 0.0350 4. RestoreFormer CodeFormer VQFRv1 VQFRv2 GFPGAN DiffBIR DifFace BFRffusion 59.06 0.0509 6.084 38.43 0.0258 4.288 41.53 0.0301 4.966 PMRF (Ours) 37.46 0.0257 4.118 0.6877 0.7010 0.6940 0.6593 0.6590 0. 0.5643 0.7413 0.6623 0.7073 25.31 24.62 25.15 24.07 23.23 24.99 25.39 24.80 26.21 0.6703 0.3441 29.63 2.043 0.6460 0.3655 32.13 2.299 0.6700 0.3432 37.28 2.470 0.6446 0.3515 35.75 2.429 0.6412 0.3624 44.38 3.053 0.6774 0.3643 36.05 2.443 0.6536 0.3878 32.94 2.006 0.6726 0.3999 45.79 2.965 0.6917 0.3619 30.98 1. 26.37 0.7073 0.3470 30.67 2.030 2022; Man et al., 2023; Murata et al., 2023; Ohayon et al., 2021; Saharia et al., 2022; 2023; Song et al., 2023; Wang et al., 2023a; Zhu et al., 2023). Specifically, letting and denote the random vectors corresponding to the ground-truth image and its degraded measurement, respectively, posterior sampling generates reconstruction ˆX by sampling from pX ). This solution is appealing as it theoretically guarantees perfect perceptual index1 (p ˆX = pX ). Interestingly, however, the Mean Squared Error (MSE) that this solution achieves is not the minimal possible under the perfect perceptual index constraint. Indeed, the MSE achieved by posterior sampling is precisely twice the Minimum MSE (MMSE) that can be achieved without constraint on the perceptual index (Blau & Michaeli, 2018). This is while the minimal MSE achievable under perfect perceptual index constraint is typically strictly smaller (Blau & Michaeli, 2018; Freirich et al., 2021), as illustrated in Figure 1. Throughout this paper, we denote by ˆX0 the estimator that minimizes the MSE under perfect perceptual quality constraint. Its formal definition is provided in Section 2.2. (such that ˆX = pX 1Formally, the perceptual index of ˆX is defined as the statistical divergence between ˆX and pX . 2 Another common way to solve PIR tasks is to train model by minimizing weighted sum of distortion loss (e.g., MSE) and GAN loss (Goodfellow et al., 2014; Gu et al., 2022; Ledig et al., 2017; Wang et al.; 2018; 2021; 2022; 2023b; Yang et al., 2021; Zhang et al., 2021; Zhou et al., 2022). As explained by Blau & Michaeli (2018), this is principled way to traverse the distortion-perception tradeoff, where the GAN loss coefficient acts as Lagrange multiplier that controls the desired perceptual index. Thus, in principle, one can approximate ˆX0 by selecting sufficiently large such coefficient. Despite the elegance of this approach, diffusion methods that aim for posterior sampling tend to perform better in practice, both in terms of distortion and perceptual quality (see Table 1), implying that current GAN-based methods fail to approximate ˆX0. Such shortcoming can be partially attributed to the fact that GANs are extremely difficult to optimize, especially when the GAN loss coefficient is significantly larger than that of the distortion loss. In this paper, we propose Posterior-Mean Rectified Flow (PMRF), straightforward framework to directly approximate ˆX0. Interestingly, Freirich et al. (2021) proved that ˆX0 can be constructed by first predicting the posterior mean ˆX := E[XY ], and then optimally transporting the result to the ground-truth image distribution (see Section 2.2 for formal explanation). Motivated by this result, PMRF first approximates the posterior mean by using model that minimizes the MSE between the reconstructed outputs and the ground-truth images. Then, we train rectified flow model (Liu et al., 2023) to predict the direction of the straight path between corresponding pairs of posterior mean predictions and ground-truth images. Given degraded measurement at test time, PMRF solves an ODE using such flow model, with the posterior mean prediction set as the initial condition. As we explain in Section 3, PMRF approximates the desired estimator ˆX0, aiming for solution that minimizes the MSE under perfect perceptual index constraint. Our paper is organized as follows. In Section 2 we provide the necessary background and set mathematical notations. In Section 3 we describe our proposed method, and provide intuition via theoretical results and toy example with closed-form solutions. In Section 4 we discuss related work. In Section 5 we demonstrate the utility of PMRF on variety of face image restoration tasks, including denoising, super-resolution, inpainting, colorization, and blind restoration. We show that PMRF sets new state-of-the-art on several benchmarks in the challenging blind face image restoration task, and is either on-par or outperforms previous frameworks in the rest of the tasks. Finally, in Section 6 we conclude our work and discuss its limitations."
        },
        {
            "title": "2 BACKGROUND",
            "content": "We adopt the Bayesian perspective for solving inverse problems (Davison, 2003; Kaipio & Somersalo, 2005), where natural image is regarded as realization of random vector with probability density function pX . The degraded measurement (e.g., noisy or low-resolution image) is realization of random vector , which is related to via the conditional probability . Given degraded measurement y, an image restoration algorithm generates density function pY (y), such that ˆX adheres to the Markov chain ˆX prediction ˆx by sampling from ˆX (i.e. and ˆX are statistically independent given ). 2.1 DISTORTION AND PERCEPTUAL INDEX Image restoration algorithms are typically evaluated by their average distortion E[(X, ˆX)], where (x, ˆx) is some distortion measure that quantifies the discrepancy between and ˆx, and the expectation is taken over the joint distribution pX, ˆX . Common examples for (x, ˆx) are the absolute error ˆx1, the squared error ˆx2, and LPIPS (Zhang et al., 2018). Moreover, as the goal in PIR is to produce reconstructions that would look natural to humans, PIR algorithms are also evaluated by perceptual quality measures. The ideal way to evaluate perceptual quality is to assess the ability of humans to distinguish between samples of ground-truth images and samples of reconstructed ones. This is typically done by conducting experiments where human observers vote on whether the generated images are real or fake (Dahl et al., 2017; Denton et al., 2015; Guadarrama et al., 2017; Iizuka et al., 2016; Isola et al., 2017; Salimans et al., 2016; Zhang et al., 2016; 2017). However, such experiments are too costly and impractical for optimizing models. practical and sensible alternative to quantify the perceptual quality is via some perceptual index d(pX , ˆX ), where d(, ) is statistical divergence between probability distributions (e.g., KullbackLeibler, Wasserstein) (Blau & Michaeli, 2018). Quantifying the perceptual index for high-dimensional distributions 3 is both statistically and computationally intractable, so it is common to resort to approximations. Popular examples include the Frechet Inception Distance (FID) (Heusel et al., 2017) and the Kernel Inception Distance (KID) (Binkowski et al., 2018)."
        },
        {
            "title": "2.2 OPTIMAL ESTIMATORS FOR THE SQUARED ERROR DISTORTION",
            "content": "Due to the distortion-perception tradeoff (Blau & Michaeli, 2018), it has become common practice to compare image restoration algorithms on the distortion-perception plane, where the goal is to obtain optimal estimators with the lowest possible distortion given prescribed level of perceptual index. This goal can be formalized by the distortion-perception function (Blau & Michaeli, 2018), D(P ) = min ˆX E[(X, ˆX)] s.t. d(pX , ˆX ) P. (1) Perhaps the most common points of interest on D(P ) are D() and D(0), where the first point corresponds to the estimator achieving minimal average distortion under no constraint, and the second corresponds to the estimator achieving minimal average distortion under perfect perceptual index constraint. Considering the squared error distortion, these points are defined by E[X ˆX2] and E[X ˆX2] s.t. ˆX = pX , (2) (3) min ˆX min ˆX It is well-known that the unique solution to Problem (2) is the posterior mean respectively. ˆX := E[XY ], which typically produces overly-smooth reconstructions (Blau & Michaeli, 2018). Therefore, in PIR tasks, it is more appropriate to aim for the solution to Problem (3). Interestingly, Freirich et al. (2021) proved that solution to Problem (3) can be obtained by solving the optimal transport problem pU,V arg min E[U 2], pU ,V Π(pX ,p ˆX ) (4) ) := {pU ,V : pU = pX , pV = ˆX } is the set of all joint probabilities pU ,V where Π(pX , ˆX with marginals pU = pX and pV = ˆX . Namely, the optimal solution to Problem (3) can be constructed as follows: Given degraded measurement y, first predict the posterior mean ˆx = E[XY = y], and then sample from pU (ˆx), which is the optimal transport plan from ˆX to pX . Similarly to Freirich et al. (2021), we denote such solution to Problem (3) by ˆX0. As discussed before, one of the most common and appealing solutions for PIR tasks is the estimator ˆX that samples from the posterior distribution pX . While such an estimator always attains perfect perceptual index (Blau & Michaeli, 2018), its MSE is typically larger than that of ˆX0 (Blau & Michaeli, 2018; Freirich et al., 2021) (see Figure 1). In other words, to design an algorithm with minimal MSE under perfect perceptual index constraint, one should often not resort to posterior sampling, but rather to solving Problem (3). This is our goal in this paper. Lastly, one may wonder whether sampling from pX instead of using the optimal transport plan from Equation (4) may also be effective in terms of MSE. However, in Appendix A.1 we prove that such an approach leads to precisely the same MSE as sampling from the posterior. , such that ˆX = pX ˆX 2.3 FLOW MATCHING AND RECTIFIED FLOWS Flow matching. Flow matching algorithms (Albergo & Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023) are generative models defined via the ODE dZt = v(Zt, t)dt, (5) where is often called vector field, and Zt is some forward process such that pZ0 is the source distribution, from which we can easily sample (e.g., isotropic Gaussian noise), and pZ1 is the target distribution from which we aim to sample (e.g., natural images). In principle, one can generate samples from the target distribution pZ1 by solving Equation (5), where samples from the source distribution pZ0 are set as the initial conditions for the ODE solver. Nevertheless, given particular forward process Zt, there are possibly many different vector fields that satisfy Equation (5). The goal in flow matching is to somehow find an appropriate vector field with desirable practical and theoretical properties, e.g., where the solution to Equation (5) is unique. 4 Algorithm 1: Posterior-Mean Rectified Flow (PMRF) Training Stage 1: Solve ω arg minω (cid:2)X fω(Y )2(cid:3) Stage 2: Solve θ arg minθ // Zt := tX + (1 t)(fω (Y ) + σsϵ), where is sampled from [0, 1]. (cid:2)(X Z0) vθ(Zt, t)2(cid:3) Inference (using Eulers method with steps to solve the ODE) Sample ϵ (0, I) ˆx fω (y) + σsϵ for 0, . . . , 1 do vθ (ˆx, ) ˆx ˆx + 1 Return ˆx // is the given degraded measurement Rectified flow. Rectified flow (Liu et al., 2023) is flow matching algorithm defined via the particular forward process Zt = tZ1 + (1 t)Z0, which connects samples from pZ1 and pZ0 with straight lines. Here, Z0 and Z1 can be statistically independent, as is typically the case when learning flow model from Gaussian noise to image data, but they can also have any joint distribution pZ0,Z1 . This forward process clearly adheres to the ODE dZt = (Z1 Z0)dt, where Z1 Z0 is the corresponding vector field. However, this is not practical generative model, since it requires knowing the destination realization of Z1 at any time step < 1 (i.e., the solution is not causal). To solve this issue, Liu et al. (2023) offer instead to use the vector field (6) vRF(Zt, t) = E[Z1 Z0Zt], which is causal, and generates the target distribution if the solution to Equation (5) exists and is unique when adopting such vector field (Theorem 3.3 in (Liu et al., 2023)). Interestingly, solving the ODE in Equation (5) with vRF often approximates the optimal transport map from the source distribution to the target one, especially when the process is repeated several times (i.e., reflow) or when pZ1,Z0 is close to the optimal transport plan between pZ0 and pZ1 (Liu et al., 2023; Tong et al., 2024). To learn vRF, one can simply train model vθ by minimizing the loss (7) (cid:90) 0 (cid:2)(Z1 Z0) vθ(Zt, t)2(cid:3) dt, (8) where the expectation is taken over the joint distribution pZ1,Z0 (Liu et al., 2023)."
        },
        {
            "title": "3 POSTERIOR-MEAN RECTIFIED FLOW",
            "content": "We now describe our proposed algorithm, which we coin Posterior-Mean Rectified Flow (PMRF) (Algorithm 1). Our method consists of two simple training stages. First, we train model fω to predict the posterior mean by minimizing the MSE loss, ω = arg min ω (cid:2)X fω(Y )2(cid:3) . (9) Note that this training stage can often be skipped, whenever there exists an off-the-shelf algorithm that attains sufficiently small MSE (high PSNR) in the desired restoration task. In the second stage, we train rectified flow model vθ (a vector field) to solve (cid:90) 1 θ = arg min θ 0 (cid:2)(X Z0) vθ(Zt, t)2(cid:3) dt, (10) where Zt := tX + (1 t)Z0. Here, Z0 := fω (Y ) + σsϵ, where ϵ (0, I) is statistically independent of and X, and σs is hyper-parameter that controls the level of the Gaussian noise added to the posterior mean prediction. As shown by Albergo et al. (2023), adding such noise is critical when the source and target distributions lie on low and high dimensional manifolds, respectively. Specifically, it alleviates the singularities resulting from learning deterministic mapping 5 between such distributions. Note, however, that adding noise to fω (Y ) may harm the MSE of the reconstructions produced by PMRF, and so σs should be taken to be sufficiently small. To explain why PMRF approximates the desired estimator ˆX0, we prove an important proposition and demonstrate it on simple example with closed-form solutions. Specifically, let ˆZt = vRF( ˆZt, t)dt with ˆZ0 = Z0 (11) be the ODE in PMRF, where vRF(z, t) = E[X Z0Zt = z] and ˆZt is the random vector generated by PMRF at time step [0, 1]. In Appendix A.2 we prove the following: Proposition 1. Suppose that σs = 0, and let us assume that the solution of the ODE in Equation (11) exists and is unique. Then, (a) ˆZ1 attains perfect perceptual index (p ˆZ1 (b) The MSE of ˆZ1 cannot be larger than that of the posterior sampler. (c) If the distribution of (X ˆX )Zt = zt is non-degenerate for almost every zt supp pZt and = pX ). [0, 1], then the MSE of ˆZ1 is strictly smaller than that of the posterior sampler. Note that assumptions (a) and (b) are the same as the ones in (Liu et al., 2023), so they are not more limiting. Whether assumption (c) holds depends on the nature of the restoration task. For example, if can be reconstructed from with zero error (i.e., pX (y) is Dirac delta function for almost every y), then ˆX = 0 almost surely and assumption (c) does not hold. Yet, this is not an interesting setting as the degradation is not invertible in most practical scenarios. To gain intuition into more common scenario, consider the following example from (Blau & Michaeli, 2018): Example 1. Let = +N , where (0, 1) and (0, σ2 ) are statistically independent and σN > 0. Then, the MSE of ˆX0 is strictly smaller than that of the posterior sampler. Moreover, when σs = 0, all the assumptions in Proposition 1 hold, and we have ˆZ1 = ˆX0 almost surely. See Appendix A.3 for the proof of Example 1. This example shows that PMRF not only outperforms posterior sampling, but may even coincide with the desired estimator ˆX0 in certain cases."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Before moving on to demonstrate the effectiveness of our approach, it is instructive to note the difference between our PMRF method and existing techniques that may superficially seem similar. Diffusion and flow-based posterior samplers. Diffusion or flow-based image restoration algorithms often attempt to sample from the posterior distribution by training conditional model that takes (or some function of , like ˆX ) as an additional input (Lin et al., 2024; Zhu et al., 2024). Some works avoid training conditional model for each task separately, and rather modify the sampling process of trained unconditional diffusion model (Chung et al., 2023; Kawar et al., 2022). In Section 5.2 we perform controlled experiment on various inverse problems, which shows that our PMRF method consistently outperforms posterior samplers with the same architecture. Flow from degraded image. Some diffusion/flow models are trained on corresponding pairs of ground-truth images and degraded measurements (Albergo et al., 2023; Delbracio & Milanfar, 2023; Li et al., 2023). In this approach, the idea is to obtain high-quality image by solving an ODE/SDE with the degraded measurement set as the initial condition. For example, Albergo et al. (2023) trained rectified flow model for the forward process Zt = tX + (1 t)Y , where is an upsampled version of such that it matches the dimensionality of X. These algorithms are closely related to PMRF, in the sense that they learn to transport an intermediate signal (instead of pure noise) to the ground-truth image distribution. Yet, they have two critical disadvantages compared to PMRF. First, the flow models design is not agnostic to the type of degradation, as the degraded signals can have varying dimensionalities or lie in different domain than that of the ground-truth images (e.g., in MRI image reconstruction). Thus, the task of the flow model may be harder than necessary, as it needs to translate signals from one domain to another. On the other hand, in PMRF the flow model always operates in the image domain, where the dimensionalities of the source and target signals are the same. Second, the theoretical motivation for flowing from is not clear, 6 at least from reconstruction performance standpoint (e.g., distortion). In contrast, the theoretical motivation underlying PMRF is clear: it approximates ˆX0, which achieves the minimal possible MSE under the constraint of perfect perceptual index. As we show in Section 5.2, PMRF always either outperforms or is on-par with the solution that flows from (see Figure 4). Methods that aim for ˆX0 directly. To the best of our knowledge, Deep Optimal Transport (DOT) (Adrai et al., 2023) is the only existing method that, like PMRF, attempts to approximate ˆX0 directly. Specifically, DOT approximates the desired optimal transport map (Equation (4)) via linear transformation in the latent space of variational auto-encoder (VAE) (Kingma & Welling, 2014). This transformation is computed in closed-form using the empirical means and covariances (in latent space) of the source distribution (that of the posterior mean predictions) and the target distribution (that of the ground-truth images), under the assumption that both are Gaussian. This method is computationally efficient, but the use of VAE imposes performance ceiling. Moreover, the optimal transport in DOT occurs in latent space and assumes that the source and target distributions are Gaussians, unlike Equation (4) which occurs in pixel space and does not make such an assumption. In contrast, PMRF does not use VAE, and approximates the optimal transport directly in pixel space. In Section 5 we show that PMRF significantly outperforms DOT (see Figure 4)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 BLIND FACE IMAGE RESTORATION We train PMRF to solve the challenging blind face image restoration task, and compare its performance with leading methods. As in previous works (e.g., (Wang et al., 2021)), we use the FFHQ data set (Karras et al., 2019) with images of size 512 512 to train our model. Similarly to previous works, we adopt complex and random degradation process to synthesize the degraded images, = [(X kσ) +Nδ]JPEGQ where denotes convolution, kσ is Gaussian blur kernel of size 41 41 and variance σ2, is bilinear down-sampling by factor R, Nδ is white Gaussian noise of variance δ2, and []JPEGQ is JPEG compression-decompression with quality factor Q. Similarly to (Yue & Loy, 2024), we synthesize the degraded images by sampling σ, R, δ and uniformly from [0.1, 15], [0.8, 32], [0, 20], and [30, 100], respectively. See Appendix B.1 for additional implementation details. (12) , 5.1.1 EVALUATION SETTINGS For evaluation, we consider the common synthetic CelebA-Test benchmark, as well as the realworld data sets LFW-Test (Huang et al., 2008; Wang et al., 2021), WebPhoto-Test (Wang et al., 2021), CelebAdult-Test (Wang et al., 2021), and WIDER-Test (Zhou et al., 2022). CelebA-Test consists of 3,000 high-quality images taken from the test partition of CelebA-HQ (Karras et al., 2018), and the degraded images were synthesized by Wang et al. (2021). For the real-world data sets, the degradations are unknown and there is no access to the clean ground-truth images. We compare our performance with DOT (Adrai et al., 2023) and leading blind face restoration models, including BFRffussion (Chen et al., 2024), DiffBIR (Lin et al., 2024), DifFace (Yue & Loy, 2024), CodeFormer (Zhou et al., 2022), GFPGAN (Wang et al., 2021), VQFRv1 and VQFRv2 (Gu et al., 2022), RestoreFormer and RestoreFormer++ (Wang et al., 2022; 2023b). Notably, these restoration methods also use the degradation model from Equation (12), though the ranges of σ, R, δ, and differ across methods. The ranges we choose, those from (Yue & Loy, 2024), are the most severe among all the compared methods. For example, the range of we use is [0.8, 32], whereas Wang et al. (2021) use [1, 8]. Thus, PMRF attempts to solve more difficult restoration task than some of the compared methods. In the following experiments, we use = 25 flow steps in PMRF (Algorithm 1). Refer to Appendix B.2 for an evaluation of additional values of K, and to Appendix B.3 for the implementation details of DOT. 5.1.2 RESULTS ON CELEBA-TEST For the CelebA-Test benchmark, we measure the perceptual quality by FID (Heusel et al., 2017), KID (Binkowski et al., 2018), NIQE (Mittal et al., 2013), and Precision (Kynkaanniemi et al., 2019), 7 and measure the distortion by the PSNR, SSIM (Wang et al., 2004), and LPIPS (Zhang et al., 2018). Similarly to previous works (Gu et al., 2022; Wang et al., 2021), we also compute the identity metric Deg (using the embedding angle of ArcFace (Deng et al., 2019)) and the landmark distance LMD. Both of these can be considered as distortion measures, as they quantify some type of discrepancy between each reconstructed image and its ground-truth counterpart. The results are reported in Table 1. Notably, PMRF outperforms all other methods in FID, KID, PSNR, and SSIM, achieves the second best scores in NIQE, Precision and Deg, and the third best scores in LPIPS, and LMD. Interestingly, no other method attains such consensus in performance like PMRF, namely, where none of the measures are significantly compromised compared to the state-of-the-art. For example, while DifFace achieves the highest Precision, it attains worse LMD, Deg, LPIPS, SSIM, and PSNR compared to the third best method in each of these metrics. This demonstrates that PMRF produces robust reconstructions, in the sense that it does not over-fit particular perceptual quality or distortion measures, but rather achieves high performance in all of them simultaneously. Visual results are provided in Figure 2 and in Figure 5 in the appendix."
        },
        {
            "title": "5.1.3 RESULTS ON REAL-WORLD DEGRADED IMAGES",
            "content": "Evaluating the distortion for real-world degraded images is impossible, as there is no access to the ground-truth images. Consequently, previous works conduct only perceptual quality evaluation (e.g., FID) on real-world data sets such as WIDER-Test and LFW-Test. Yet, high perceptual quality alone is clearly not indicative of reconstruction performance (to attain high perceptual quality, one may simply ignore the inputs and generate samples from pX ). Thus, we consider measure which indicates the Root MSE (RMSE) and allows ranking algorithms according to their (approximate) RMSE, without access to the ground-truth images. Specifically, for any estimator ˆX it holds that E[X ˆX2] E[ ˆX (Y )2] + m, (13) where (Y ) ˆX is an approximation of the true posterior mean predictor ˆX , and is constant that does not depend on ˆX (see Appendix for an explanation). Thus, the square root of E[ ˆX (Y )2], which we denote by IndRMSE, indicates the true RMSE. We utilize the posterior mean predictor trained by (Yue & Loy, 2024)2 as , and compute the IndRMSE of all the evaluated algorithms on the LFW-Test, WebPhoto-Test, CelebAdult-Test, and WIDER-Test data sets. As before, we evaluate perceptual quality by FID, KID, NIQE, and Precision. In Figure 3 we provide visual results on inputs from the WIDER-Test data set, and compare the algorithms on distortionperception plane (IndRMSE vs. FID). DOT is not plotted as it achieves far worse FID compared to other methods. Our algorithm attains the best (smallest) IndRMSE on all data sets, while achieving on-par perceptual quality compared to the state-of-the-art. This indicates that PMRF achieves superior distortion on such real-world data sets, while not compromising perceptual quality. In the appendix, we report the rest of the perceptual quality measures in Tables 7 to 10, provide visual results in Figures 6 to 8, and also report the performance of DOT. 5.2 COMPARING PMRF WITH PREVIOUS FRAMEWORKS IN CONTROLLED EXPERIMENTS One may wonder whether the performance of PMRF is attributed to the framework itself (Algorithm 1), or, maybe it is attributed to the model architecture, the rectified flow training approach, the chosen hyper-parameters, etc. Could we have done better by training flow to sample from the posterior, or by adopting the approach of (Albergo et al., 2023) and flow directly from ? Here, we conduct controlled study where we demonstrate that the high performance of PMRF is indeed attributed to the proposed framework itself (Algorithm 1). Specifically, we consider the image denoising, super-resolution, inpainting, and colorization tasks, where we train PMRF and several baseline methods on the same grounds. In each task we train two conditional rectified flow models, where one is conditioned on the degraded measurement (we call this method flow conditioned on ), and the other is conditioned on the posterior mean predictor fω (Y ) (we call this method flow conditioned on ˆX ). The first model represents posterior sampling methods, and the second model allows for fair comparison of model capacity with PMRF (since PMRF is comprised of fω (Y ) and flow model). In fact, theoretically speaking, the second approach achieves precisely the same 2Importantly, the exact same posterior mean predictor model (and weights) is also used by other methods such as DifFace and DiffBIR, so this is fair evaluation. 8 Figure 3: Real-world face image restoration. Top: Qualitative results on inputs from the WIDERTest data set. Bottom: Comparison on the distortion-perception plane (IndRMSE vs. FID), where IndRMSE indicates the RMSE of each method (the true distortion cannot be computed as there is no access to the ground-truth images). Our algorithm outperforms all other methods in IndRMSE, while achieving on-par perceptual quality compared to the state-of-the-art. MSE as the posterior sampler (see Appendix A.1), and is often used in practice (e.g., in (Lin et al., 2024; Zhu et al., 2024)). In addition, we train an unconditional rectified flow model, where the forward process is defined as Zt = tX + (1 t)Z0, Z0 = + σsϵ, ϵ (0, I), and is the up-scaled version of the degraded measurement such that it matches the dimensionality of (we call this method flow from ). This method represents the frameworks in (Albergo et al., 2023; Delbracio & Milanfar, 2023; Li et al., 2023), which we discuss in Section 4. All of the models are trained with the same hyper-parameters as PMRF, using the same architecture, learning rate, weight decay, number of training epochs, etc. Moreover, for PMRF and flow conditioned on ˆX method, we use the exact same architecture and weights for fω (Y ). To clarify the differences between the mathematical formulations of the baseline methods, in Table 11 in the appendix we summarize the definitions of the training loss and the forward process of all methods. Moreover, in Algorithms 2 to 4 we disclose pseudo-code for the training and inference procedures of the baseline methods. While DOT is not flow method, we still evaluate its performance as it is related to PMRF. In Figure 4 we compare the algorithms on the distortion-perception plane (RMSE vs. FID), using = 100 flow steps for each flow algorithm. We clearly see that for the inpainting and colorization tasks, PMRF dominates all other methods, achieving notably smaller RMSE without compromising FID. This demonstrates that PMRF achieves our desired goal, which is to attain low distortion without compromising on perceptual quality. For the denoising task, we observe that PMRF and flow from attain similar performance, and both dominate the posterior sampling approaches. We hypothesize that, in some tasks (e.g., denoising), flowing from may be as effective as PMRF in terms of approximating ˆX0. To demonstrate this, we prove in Appendix C.4 that flowing from is optimal in the toy problem in Example 1 (just like PMRF). Yet, our experiments demonstrate that PMRF generally leads to better performance compared to previous frameworks. To assess the 9 Figure 4: controlled experiment comparing PMRF (our method) with several baseline methods, where the models are trained with the same architecture, hyper-parameters, etc. (see Section 5.2). Top: Qualitative comparison of PMRF and the baseline methods on several tasks. Bottom: Quantitative comparison on the distortion-perception plane (RMSE vs. FID). DOT is not flow model, but rather another approach that attempts to approximate ˆX0 (like PMRF). These experiments demonstrate that PMRF is either superior or is on-par with previous frameworks (i.e., posterior sampling or flowing from ) on variety of image restoration tasks. See Section 5.2 for more details. effectiveness of each method given different inference time constraints, in Figure 9 in the appendix we vary the number of flow inference steps for each method. Interestingly, we observe that PMRF is still either on-par or dominates the other methods for any given number of inference steps. These results further demonstrate that the superior performance of PMRF is attributed to our framework itself, rather than to the chosen hyper-parameters. See Appendix for more details, and refer to Figures 10 to 13 in the appendix for visual comparisons."
        },
        {
            "title": "6 CONCLUSION AND LIMITATIONS",
            "content": "The goal in this paper was to design an algorithm that directly approximates ˆX0, which is the estimator that minimizes the MSE under perfect perceptual index constraint (Equation (3)). To achieve this goal, we introduced Posterior-Mean Rectified Flow (PMRF), simple yet highly effective image restoration algorithm that outperforms previous frameworks (e.g., posterior sampling, flow from , and GAN-based methods) in variety of image restoration tasks. As we explained in Section 3, PMRF alleviates the issues resulting from solving the ODE by adding Gaussian noise to the posterior mean predictions. We note that the noise level σs should be carefully tuned, as taking it to be too large or too small may cause the MSE or the perceptual quality of PMRF to degrade, respectively. While the flow from method (Algorithm 4) suffers from the same limitation (though it does not provide theoretical guarantee on the MSE, like PMRF), this may be considered disadvantage of PMRF compared to posterior sampling methods (e.g., Algorithm 2), which do not require such hyper-parameter. Moreover, we proved in Proposition 1 that, under some conditions, PMRF is guaranteed to achieve smaller MSE than the posterior sampler. However, as in (Liu et al., 2023), one could argue that the assumptions in Proposition 1 may be too limiting in some cases. Finally, we 10 did not provide experiments on general-content images, as this requires training significantly larger models (Crowson et al., 2024). However, we believe that our experiments demonstrate the strength and potential of the PMRF approach, as we showcased its superiority on five different tasks, including the highly challenging blind face image restoration problem."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our codes are available at https://github.com/ohayonguy/PMRF. We provide all the explanations and checkpoints necessary to reproduce our results, including training, inference, and the computation of the distortion and perceptual quality measures in Section 5. Besides our code, our paper discloses all the implementation details required to reproduce the results, including architecture details, training hyper-parameters, etc. Refer to Sections 5.1 and 5.2 and appendices and for implementation details, and to Table 12 in the appendix for summary of our training hyper-parameters."
        },
        {
            "title": "REFERENCES",
            "content": "Theo Adrai, Guy Ohayon, Michael Elad, and Tomer Michaeli. Deep optimal transport: practical algorithm for photo-realistic image restoration. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 6177761791. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/c281c5a17ad2e55e1ac1ca825071f991-Paper-Conference.pdf. Michael S. Albergo, Mark Goldstein, Nicholas M. Boffi, Rajesh Ranganath, and Eric VandenEijnden. Stochastic interpolants with data-dependent couplings. arXiv, 2023. URL https: //arxiv.org/abs/2310.03725. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=li7qeBbCR1t. Matthew Bendel, Rizwan Ahmad, and Philip Schniter. regularized conditional GAN for posterior sampling in image recovery problems. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=z4vKRmq7UO. Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=r1lUOzWCW. Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Xiaoxu Chen, Jingfan Tan, Tao Wang, Kaihao Zhang, Wenhan Luo, and Xiaochun Cao. Towards real-world blind face restoration with generative diffusion prior. IEEE Transactions on Circuits and Systems for Video Technology, pp. 11, 2024. doi: 10.1109/TCSVT.2024.3383659. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=OnD9zGAGT0k. Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=WRIn2HmtBS. Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. 11 Giannis Daras, Hyungjin Chung, Chieh-Hsin Lai, Yuki Mitsufuji, Peyman Milanfar, Alexandros G. Dimakis, Chul Ye, and Mauricio Delbracio. survey on diffusion models for inverse problems. 2024. URL https://giannisdaras.github.io/publications/ diffusion_survey.pdf. A. C. Davison. Statistical Models. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2003. doi: 10.1017/CBO9780511815850. Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=VmyFF5lL3F. Featured Certification. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 46854694, 2019. doi: 10.1109/CVPR.2019.00482. Emily Denton, Soumith Chintala, and Rob Fergus. tive image models using laplacian pyramid of adversarial networks. N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett formation Processing Systems, volume 28. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2015/file/ aa169b49b583a2b5af89203c2b78c67c-Paper.pdf. Inc., 2015. Deep generaIn C. Cortes, InURL (eds.), Advances in Neural szlam, arthur Dror Freirich, Tomer Michaeli, and Ron Meir. in wasserstein space. and J. Wortman Vaughan (eds.), Advances the distortion-perception In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. tradeoff Information Processin Neural Liang, ing Systems, volume 34, pp. 2566125672. Curran Associates, URL Inc., 2021. https://proceedings.neurips.cc/paper_files/paper/2021/file/ d77e68596c15c53c2a33ad143739902d-Paper.pdf. theory of Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2014/ 2014. file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf. Generative adversarial nets. Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. In ComVqfr: Blind face restoration with vector-quantized dictionary and parallel decoder. puter Vision ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVIII, pp. 126143, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 9783-031-19796-3. doi: 10.1007/978-3-031-19797-0 8. URL https://doi.org/10.1007/ 978-3-031-19797-0_8. Sergio Guadarrama, Ryan Dahl, David Bieber, Jonathon Shlens, Mohammad Norouzi, and Kevin In British Machine Vision Conference 2017, Murphy. Pixcolor: Pixel recursive colorization. BMVC 2017, London, UK, September 4-7, 2017. BMVA Press, 2017. URL https://www. dropbox.com/s/wmnk861irndf8xe/0447.pdf. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf. Gary B. Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled Faces in the Wild: Database forStudying Face Recognition in Unconstrained Environments. In Workshop on Faces in Real-Life Images: Detection, Alignment, and Recognition, Marseille, France, October 2008. Erik Learned-Miller and Andras Ferencz and Frederic Jurie. URL https://inria.hal. science/inria-00321923. 12 Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification. ACM Transactions on Graphics (Proc. of SIGGRAPH 2016), 35(4), 2016. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017. Jari Kaipio and Erkki Somersalo. Statistical and Computational Inverse Problems. Springer, Dordrecht, 2005. doi: 10.1007/b138659. URL https://cds.cern.ch/record/1338003. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 43964405, 2019. doi: 10.1109/CVPR.2019.00453. Bahjat Kawar, Gregory Vaksman, and Michael Elad. Stochastic image denoising by sampling from the posterior distribution. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pp. 18661875, 2021a. doi: 10.1109/ICCVW54120.2021.00213. Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochastically. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2175721769. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/ paper/2021/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf. Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Advances in Neural Information Processing Systems, 2022. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6114. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. ImIn H. Wallach, (eds.), AdInc., URL https://proceedings.neurips.cc/paper_files/paper/2019/ proved precision and recall metric for assessing generative models. H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett vances in Neural Information Processing Systems, volume 32. Curran Associates, 2019. file/0234c510bc6d908b28c70ff313743079-Paper.pdf. Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. PhotoIn 2017 IEEE realistic single image super-resolution using generative adversarial network. Conference on Computer Vision and Pattern Recognition (CVPR), pp. 105114, 2017. doi: 10.1109/CVPR.2017.19. Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-to-image translation with brownian bridge diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19521961, 2023. Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pp. 18331844, October 2021. Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv, 2024. URL https://arxiv.org/abs/2308.15070. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. 13 Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=XVjTT1nw5z. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Sean Man, Guy Ohayon, Theo Adrai, and Michael Elad. High-perceptual quality jpeg decoding via posterior sampling. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 12721282, 2023. doi: 10.1109/CVPRW59228.2023.00134. Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Making completely blind image qualIEEE Signal Processing Letters, 20(3):209212, 2013. doi: 10.1109/LSP.2012. ity analyzer. 2227726. Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. GibbsDDRM: partially collapsed Gibbs sampler for solving blind inverse problems with denoising diffusion restoration. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2550125522. PMLR, 2329 Jul 2023. URL https://proceedings.mlr. press/v202/murata23a.html. Guy Ohayon, Theo Adrai, Gregory Vaksman, Michael Elad, and Peyman Milanfar. High perIn 2021 IEEE/CVF Interdoi: ceptual quality image denoising with posterior sampling cgan. national Conference on Computer Vision Workshops (ICCVW), pp. 18051813, 2021. 10.1109/ICCVW54120.2021.00207. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, SIGGRAPH 22, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393379. doi: 10.1145/3528233.3530757. URL https: //doi.org/10.1145/3528233.3530757. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):47134726, 2023. doi: 10.1109/TPAMI.2022.3204461. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/ paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf. Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9_gsMA8MRKQ. Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image Processing, 27(8):39984011, 2018. doi: 10.1109/TIP.2018.2831899. Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=CD9Snc73AW. Expert Certification. 14 Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW). Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In The European Conference on Computer Vision Workshops (ECCVW), September 2018. Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with In The IEEE Conference on Computer Vision and Pattern Recognition generative facial prior. (CVPR), 2021. Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. The Eleventh International Conference on Learning Representations, 2023a. Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. Restoreformer: Highquality blind face restoration from undegraded key-value pairs. 2022. Zhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, and Ping Luo. Restoreformer++: Towards real-world blind face restoration from undegraded key-value paris. 2023b. Tao Yang, Peiran Ren, Xuansong Xie, , and Lei Zhang. Gan prior embedded network for blind face restoration in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Zongsheng Yue and Chen Change Loy. Difface: Blind face restoration with diffused error contracIEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 115, 2024. doi: tion. 10.1109/TPAMI.2024.3432651. Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In IEEE International Conference on Computer Vision, pp. 47914800, 2021. Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In ECCV, 2016. Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela Lin, Tianhe Yu, and Alexei Efros. Real-time user-guided image colorization with learned deep priors. ACM Transactions on Graphics (TOG), 9(4), 2017. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. In NeurIPS, 2022. Yixuan Zhu, Wenliang Zhao, Ao Li, Yansong Tang, Jie Zhou, and Jiwen Lu. Flowie: Efficient image enhancement via rectified flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1322, June 2024. Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (NTIRE), 2023."
        },
        {
            "title": "A SUPPLEMENTARY EXPLANATIONS FOR PMRF",
            "content": "A.1 PROOF THAT CONDITIONING ON ˆX ACHIEVES THE SAME MSE AS POSTERIOR SAMPLING Proposition 2. Let ˆX be the estimator which, given any degraded measurement y, first predicts (ˆx)3. Then, the MSE of ˆX the posterior mean ˆx = E[XY = y] and then samples from pX equals twice the MMSE, which is the MSE attained by the posterior sampler. ˆX Proof. The MSE of ˆX is given by E[X ˆX 2] = E[X ˆX 2] + E[X ˆX 2], (14) where this equality follows from Lemma 2 in (Freirich et al., 2021) (Appendix B.1). By the definition of ˆX we have ˆX , ˆX = pX, ˆX , so E[X ˆX 2] = E[X ˆX 2]. Substituting this result into Equation (14), we get E[X ˆX 2] = 2E[X ˆX 2]. (15) (16) Namely, ˆX attains precisely the same MSE as the posterior sampler, which is equal to twice the MMSE (Blau & Michaeli, 2018). Thus, in theory, one should not expect to improve the MSE of conditional diffusion/flow model by supplying ˆX as condition instead of . A.2 PROOF OF PROPOSITION 1 For completeness, we first restate Proposition 1 and then provide its proof. Proposition 1. Suppose that σs = 0, and let us assume that the solution of the ODE in Equation (11) exists and is unique. Then, (a) ˆZ1 attains perfect perceptual index (p ˆZ1 (b) The MSE of ˆZ1 cannot be larger than that of the posterior sampler. (c) If the distribution of (X ˆX )Zt = zt is non-degenerate for almost every zt supp pZt and = pX ). [0, 1], then the MSE of ˆZ1 is strictly smaller than that of the posterior sampler. Proof. We first prove (a) and (b) assuming that the solution for the ODE in Equation (11) exists and is unique for σs = 0. Then, we will prove (c) by also assuming that the distribution of (X ˆX )Zt = zt is non-degenerate for almost every zt and [0, 1]. From Theorem 3.3 in (Liu et al., 2023) we have ˆZt ˆZ1 Next, without additional assumptions, we will prove (b) by showing that = pZ1 = pX , i.e., PMRF attains perfect perceptual index when σs = 0. This proves (a). = pZt for every [0, 1]. This implies that E[ ˆZ1 ˆX 2] E[X ˆX 2], (17) which will imply that the MSE of ˆZ1 can only be smaller than that of the posterior sampler. Since σs = 0, we have Z0 = ˆX + σsϵ = ˆX . Following similar arguments to those in the proof of 3Note that ˆX is posterior sampler which is conditioned on ˆX . Thus, Algorithm 3 represents such an algorithm, which is one of the baseline methods we evaluate in Section 5.2. Theorem 3.5 in (Liu et al., 2023), it holds that E[ ˆZ1 ˆX 2] = = = (cid:90) 1 0 (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) 0 (cid:20)(cid:90) 1 (cid:90) 1 2(cid:35) (cid:13) (cid:13) vRF( ˆZt, t)dt (cid:13) (cid:13) 2(cid:35) (cid:13) (cid:13) vRF(Zt, t)dt (cid:13) (cid:13) (cid:21) vRF(Zt, t)2 dt 0 (cid:20)(cid:90) 1 0 (cid:20)(cid:90) 1 (cid:13) (cid:13) 2 E[X ˆX Zt] (cid:13) (cid:13) (cid:13) (cid:13) (cid:21) dt (cid:21) E[X ˆX 2Zt]dt 0 (cid:104) E[X ˆX 2Zt] (cid:105) dt E[X ˆX 2]dt = = (cid:90) 1 0 (cid:90) 1 0 (18) (19) (20) (21) (22) (23) (24) = E[X ˆX 2], (25) where Equation (18) follows from the definition of ˆZ1 and ˆX , Equation (19) follows from the fact that ˆZt = pZt, Equation (20) follows from Jensens inequality, Equation (21) follows from the definition of vRF(Zt, t), Equation (22) follows from Jensens inequality, Equation (23) follows from the linearity of the integral operator, and Equation (24) follows from the law of total expectation. Thus, we have E[ ˆZ1 ˆX 2] E[X ˆX 2]. Combining this result with Lemma 2 from (Freirich et al., 2021) (Appendix B.1), we conclude that E[X ˆZ12] = E[X ˆX 2] + E[ ˆZ1 ˆX 2] 2E[X ˆX 2], (26) where the left hand side is the MSE of PMRF, and the right hand side is the MSE of the posterior sampler, which always equals twice the MMSE (Blau & Michaeli, 2018). Finally, to prove (c), let us further assume that (X ˆX )Zt = zt is non-degenerate random vector for every zt supp pZt and [0, 1]. Thus, the inequality in Equation (22) becomes strict (from Jensens inequality for strictly convex functions), and hence we have E[ ˆZ1 ˆX 2] < E[X ˆX 2]. Combining this result with Lemma 2 from (Freirich et al., 2021) (Appendix B.1), we conclude that E[X ˆZ12] < 2E[X ˆX 2]. (27) Namely, the MSE of ˆZ1 (left hand side) is strictly smaller than that of the posterior sampler (right hand side). A.3 PROOF OF THE RESULTS IN EXAMPLE 1 From (Blau & Michaeli, 2018; Freirich et al., 2021), we know that ˆX0 in Example 1 attains MSE that is strictly smaller than that of the posterior sampler (assuming that σN > 0). Specifically, the closed-form solution of ˆX0 in Example 1 is given by (Freirich et al., 2021): Moreover, in this example, it is well known that the posterior mean E[XY ] is given by ˆX0 = 1 (cid:112)1 + σ Y. ˆX = 1 1 + σ2 Y. Next, we will prove that: (a) All the assumptions in Proposition 1 hold. (b) ˆZ1 = ˆX0 almost surely. 17 (28) (29) Proof of (a). Since σs = 0, we have vRF(Zt, t) = E[X ˆX Zt] and Zt = tX + (1 t) ˆX . Below, we show that Cov(X ˆX , Zt) = , and σ2 1 + σ2 1 1 + σ2 + . Var(Zt) = t2 σ2 1 + σ2 Since ˆX and Zt are jointly Gaussian4, we have vRF(Zt, t) = E[X ˆX Zt] = E[X ˆX ] + Cov(X ˆX , Zt) Var(Zt) (Zt E[Zt]) = = = Cov(X ˆX , Zt) Var(Zt) σ2 1+σ2 Zt, Zt + 1 1+σ2 t2 σ2 1+σ2 tσ2 1 + t2σ2 Zt, (30) (31) (32) (33) where Equation (32) follows from the fact that E[X ˆX ] = 0 and E[Zt] = 0. One can verify that the solution of ˆZt = vRF( ˆZt, t)dt for any initial condition ˆZ0 = is unique and is given by (cid:113) ˆZt = (34) To show that the distribution of (X ˆX )Zt = zt is non-degenerate for almost every zt and [0, 1], note that 1 + t2σ2 . Var(X ˆX ) = Cov(X ˆX , ˆX ) = Cov(X, X) 2Cov(X, ˆX ) + Cov( ˆX , ˆX ) (cid:18) = 1 2Cov X, 1 1 + σ2 (cid:19) + Cov (cid:18) 1 1 + σ2 (cid:19) Y, 1 1 + σ2 Y 1 (1 + σ2 )2 Cov(Y, ) Cov(X, ) + + 1 1 + σ2 = 1 = 1 = 1 2 1 + σ2 2 1 + σ2 1 1 + σ2 = σ2 1 + σ2 . (35) Thus, for any > 0, and assuming σN > 0, the correlation between ˆX and Zt is given by (cid:113) Cov(X ˆX , Zt) Var(Zt)Var(X ˆX ) = (cid:114)(cid:16) σ2 1+σ2 t2 σ2 1+σ2 + 1+σ2 (cid:17) (cid:16) σ2 1+σ2 (cid:17) = = tσN t2σ2 + 1 1 1 + 1 t2σ2 < 1. (36) 4X ˆX and Zt can be written as linear transformation of (X, ), which are jointly Gaussian random variables. Thus, ˆX and Zt are jointly Gaussian. 18 Namely, the correlation between ˆX and Zt is strictly smaller than 1 for every (0, 1]. Moreover, for = 0 the correlation between ˆX and Zt clearly equals zero, so such correlation is smaller than 1 for every [0, 1]. This implies that the distribution of (X ˆX )Zt = zt is nondegenerate for almost every zt and [0, 1], and so all the assumptions in Proposition 1 hold. To prove Equations (30) and (31), first note that Cov(X, ˆX ) = Cov( ˆX , ˆX ) = 1 1+σ2 Cov(X, ˆX ) Cov( ˆX , ˆX ) = 0. Thus, , and so Cov(X ˆX , Zt) = Cov(X ˆX , tX + (1 t) ˆX ) = t(Cov(X, X) Cov(X, ˆX )) + (1 t)(Cov(X, ˆX ) Cov( ˆX , ˆX )) (cid:18) = 1 (cid:19) 1 1 + σ2 = σ2 1 + σ2 , and, Var(Zt) = Cov(Zt, Zt) = Cov(tX + (1 t) ˆX , tX + (1 t) ˆX ) = t2Cov(X, X) + 2t(1 t)Cov(X, ˆX ) + (1 t)2Cov( ˆX , ˆX ) 1 1 + σ2 1 1 + σ2 = t2 + (2t 2t2 + 1 2t + t2) = t2 + (2t(1 t) + (1 t)2) = t2 + (1 t2) = t2 σ2 1 + σ2 + 1 1 + σ2 1 1 + σ2 . (37) (38) Proof of (b). The proof follows directly from Equation (34). Specifically, for the initial condition ˆZ0 = ˆX , we have ˆZ1 = = = (cid:113) (cid:113) 1 + σ2 ˆX 1 + σ2 1 1 + σ2 1 (cid:112)1 + σ2 = ˆX0. (39) (40) Thus, in Example 1, PMRF with σs = 0 coincides with the desired optimal estimator ˆX0. A.4 REFLOW (OPTIONAL) be the random vector generated by PMRF (Algorithm 1), where ˆZ To potentially improve the MSE of PMRF further, one may conduct reflow procedure (Liu et al., 2023), where sequence of flow models are trained, and the flow model at index + 1 learns to flow from the source distribution to the distribution generated by the flow model at index k. Specifically, let ˆZ k+1 1 replaces the role of 1 in Algorithm 1 and ˆZ 0 1 = (Z0 remains unchanged). Thus, from Theorem 3.5 in (Liu et al., 2023), we have E[c( ˆZ k+1 1 Z0)], which implies the reflowing may only improve the MSE of PMRF, and hence improve the approximation of the desired optimal transport map (Equation 4). We leave this possibility for future work. 1 Z0)] E[c( ˆZ"
        },
        {
            "title": "B SUPPLEMENTARY DETAILS AND EXPERIMENTS IN BLIND FACE IMAGE",
            "content": "RESTORATION Unfortunately we do not compare with FlowIE (Zhu et al., 2024), as the checkpoints in the official repository of this method seem to not work at the moment. Note that FlowIE is conditional method that utilizes ControlNet (similarly to DiffBIR), so it is not similar to our PMRF algorithm. B."
        },
        {
            "title": "IMPLEMENTATION DETAILS OF PMRF",
            "content": "During training, we only use random horizontal flips for data augmentation. We use the SwinIR (Liang et al., 2021) model trained by Yue & Loy (2024) as the posterior mean predictor fω in Algorithm 1, and use σs = 0.1. This model was trained using the same synthetic degradation as in Equation (12), with the same ranges for σ, R, δ, and we mentioned in Section 5.1. The SwinIR models weights are kept frozen during the vector fields training stage, and the same weights are utilized during inference as well. The vector field vθ is HDiT model (Crowson et al., 2024), which we train from scratch. As in (Crowson et al., 2024), we sample uniformly from [0, 1] using stratified sampling strategy. The vector field is trained for 3850 epochs using the AdamW 4, (β1, β2) = (0.9, 0.95), and optimizer (Loshchilov & Hutter, 2019), with learning rate of 5 10 2 (as in (Crowson et al., 2024)). In the last 350 epochs, we reduce the learning weight decay of 10 rate gradually, multiplying it by 0.98 at the end of every epoch. The training batch size is set to 256 and is kept fixed. We compute the exponential moving average (EMA) of the models weights, using decay of 0.9999. The EMA weights of the model are then used in all evaluations. Our model is trained using bfloat16 mixed precision. summary of the vector field training hyper-parameters is provided in Table 12. B.2 VARYING THE NUMBER OF FLOW STEPS IN PMRF In Tables 2 to 6 we evaluate the performance of PMRF for various choices of (the number of inference steps in Algorithm 1). As expected, increasing generally improves the perceptual quality while harming the distortion. B.3 DETAILS OF DOT We use the official codes of DOT (Adrai et al., 2023) as provided by the authors. This method performs optimal transport between the source and target distributions in latent space, using the closed-form solution for the optimal transport map between two Gaussians. As in (Adrai et al., 2023), we use the VAE (Kingma & Welling, 2014) of stable-diffusion (Rombach et al., 2022). For computing the latent empirical mean and covariance of the target distribution, we provide to the code the first 1000 images from FFHQ, with images of size 512 512 (the default is 100 images, so using 1000 images instead ensures that the performance of DOT is not compromised, as explain by Adrai et al. (2023)). For computing the latent empirical mean and covariance of the source distribution, we randomly synthesize degraded images according to Equation (12) from the first 1000 images in FFHQ, and reconstruct each image using the SwinIR model with the pre-trained weights from (Yue & Loy, 2024) (the same weights we use in PMRF). Given degraded image at test time, the code of Adrai et al. (2023) first predicts the posterior mean using the SwinIR model, encodes it to latent space, optimally transports the result using the pre-computed empirical means and covariances, and finally uses the decoder to obtain the reconstructed image. B.4 COMPUTATION OF FID, KID, AND PRECISION For each data set and algorithm, the FID, KID, and Precision are computed between the entire FFHQ 512 512 training set, and the reconstructed images produced for the degraded images in the test data set (as in previous works). For example, for the evaluations on the CelebA-Test data, this means that the FID is computed between the 70,000 FFHQ images, and the 3,000 CelebA-Test reconstructed images. 20 SUPPLEMENTARY DETAILS ON SECTION 5.2 C.1 DEGRADATIONS The degraded images in each task in the controlled experiments are synthesized according to the following degradations: 1. Denoising: We apply additive white Gaussian noise with standard deviation 0.35. 2. Super-resolution: We use the 8 bicubic down-sampling operator, and add Gaussian noise with standard deviation 0.05. 3. Inpainting: We randomly mask 90% of the pixels in the ground-truth image, and add Gaussian noise with standard deviation 0.1. 4. Colorization: We average the color channels in the ground-truth image (with weight of 1 3 for each color channel), and add Gaussian noise with standard deviation 0.25. C."
        },
        {
            "title": "IMPLEMENTATION DETAILS OF THE FLOW METHODS",
            "content": "Training. For all restoration tasks in Section 5.2, the models are trained on the FFHQ data set with images of size 256 256 (we down-sample the original 1024 1024 images to 256 256). Unlike in the blind face image restoration experiments, where the model is trained on images of size 512 512, here we choose to use smaller image resolution to save computational resources and achieve shorter training times. During training, we only use random horizontal flips for data augmentation. Choice of σs. As expected, we observe that using σs = 0 in both PMRF (Algorithm 1) and the flow from method (Algorithm 4) leads to blurry results with small MSE and large FID. Thus, for fair comparison, we use the same value of σs > 0 in both methods. For the denoising task we use σs = 0.025, and for the rest of the tasks (inpainting, colorization, and super-resolution), we use σs = 0.15. Posterior mean predictor. The posterior mean predictor fω is 4.4M parameters SwinIR model6 which we train from scratch for each task. In all tasks, this model is trained for 1000 epochs, with 4, (β1, β2) = fixed batch size of 256, using the AdamW optimizer with learning rate of 5 10 (0.9, 0.95), without weight decay, and without learning rate scheduling. When utilizing this model in the flow process (e.g., in PMRF), we use the EMA weights computed with decay of 0.9999. Vector field. Similarly to Appendix B.1, the vector field is HDiT model. The time in Algorithms 1 and 2 to 4 is sampled from [0, 1] using stratified sampling strategy. For all baseline methods and PMRF, we train the vector field for 1000 epochs, use fixed batch size of 256, adopt 4, (β1, β2) = (0.9, 0.95), and weight decay the AdamW optimizer with learning rate of 5 10 2. As in (Crowson et al., 2024), we do not apply learning rate scheduling. Finally, we use of 10 the EMA weights for evaluation, using decay of 0.9999. summary of the hyper-parameters is provided in Table 12. Evaluation. We test all models on the CelebA-Test data set, with images of size 256 256. The FID of each method is computed between the entire FFHQ 256 256 training set, and the images produced by the algorithm for the synthesized CelebA-Test degraded images. The MSE is computed between the reconstructed images and the corresponding ground-truth images. C.3 DETAILS OF DOT. We use DOT (Adrai et al., 2023) similarly to Appendix B.3, using images of size 256 256 instead of 512512, and adopting the official codes of the authors. For the source distribution, we randomly 5Note that the optimal value of σs depends on the severity of the restoration task. For example, in mild image denoising task, the posterior mean ˆX may already be close to the ground-truth image, so σs should be smaller compared to case where the noise is severe. 6We use the official code for the SwinIR architecture from https://github.com/JingyunLiang/ SwinIR. Implementation details and hyper-parameters are provided in our code. 21 synthesize degraded images according to the degradation of each task (Appendix C.1) from the first 1000 images in FFHQ, reconstruct each image using the SwinIR model we trained for each task (the same weights we use in PMRF), and finally compute the empirical mean and covariance of the reconstructions in latent space. C.4 PROVING THAT FLOW FROM IS ALSO OPTIMAL IN EXAMPLE 1 In Section 5.2 we show that, for the denoising task, PMRF and flow from are on-par in terms of both perceptual quality and MSE. To provide intuition for this result, we show that flow from leads to the desired estimator ˆX0 in Example 1 (just like PMRF does). Specifically, as in Example 1, suppose that (0, 1), (0, σ2 ), σN > 0, and = X+N . In flow from with σs = 0 we have Zt = tX + (1 t)Y , and thus vRF(Zt, t) = E[X Zt]. Below, we show that Cov(X Y, Zt) = (t 1)σ2 Var(Zt) = σ2 (t2 2t + 1) + 1. , and Hence, vRF(Zt, t) = E[X Zt] = E[X ] + Cov(X Y, Zt) Var(Zt) (Zt E[Zt]) = = Zt Cov(X Y, Zt) Var(Zt) (t 1)σ2 σ2 (t2 2t + 1) + 1 Zt, (41) (42) (43) (44) where Equation (43) holds since E[X ] = 0 and E[Zt] = 0. One can verify that the solution of ˆZt = vRF( ˆZt, t)dt for any initial condition ˆZ0 = is given by (t2 2t + 1) + 1 (cid:112)σ2 (45) ˆZt = (cid:112)1 + σ . Namely, we have ˆZ1 = 1 (cid:112)1 + σ2 Y = ˆX0, (46) where the last equality follows from Equation (28). It follows that flow from is also optimal in Example 1, just like PMRF. Demonstrating Equations (41) and (42) is straightforward. We have Cov(X Y, Zt) = Cov(X Y, tX + (1 t)Y ) = tCov(X, X) + (1 t)Cov(X, ) tCov(X, ) (1 t)Cov(Y, ) = + (1 t) (1 t)(1 + σ2 ) = (t 1)σ2 , (47) and Var(Zt) = Cov(tX + (1 t)Y, tX + (1 t)Y ) = t2Cov(X, X) + 2t(1 t)Cov(X, ) + (1 t)2Cov(Y, ) = t2 + 2t(1 t) + (1 t)2(1 + σ2 = t2 + 2t 2t2 + (1 2t + t2)(1 + σ2 ) + 2t(1 1 σ2 = t2(1 2 + 1 + σ2 = t2σ2 + σ2 2tσ2 + 1 = σ2 ) ) + 1 + σ2 N (t2 2t + 1) + 1. ) (48) 22 INDICATOR RMSE (INDRMSE) DERIVATION The MSE of any estimator ˆX can always be written as E[X ˆX2] = E[ ˆX ˆX 2] + E[X ˆX 2] = E[ ˆX ˆX 2] + m, (50) where ˆX = E[XY ] is the MMSE estimator, Equation (49) follows from Lemma 2 in (Freirich et al., 2021) (Appendix B.1), and is some constant that does not depend on ˆX. Thus, if (Y ) ˆX , we have (49) E[X ˆX2] E[ ˆX (Y )2] + m, (51) (cid:113) E[ ˆX (Y )2] may be used as an indicator for so vestigate the effectiveness of this measure. (cid:113) E[X ˆX2]. Future works should in23 Table 2: Varying the number of flow steps in PMRF (Algorithm 1) on the CelebA-Test blind face image restoration benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Increasing the number of steps improves the perceptual quality while hindering the distortion. These results are expected due to the distortion-perception tradeoff."
        },
        {
            "title": "Distortion",
            "content": "K FID KID NIQE Precision PSNR SSIM LPIPS Deg LMD 3 5 10 25 50 100 81.81 63.77 44.39 37.46 36.63 36.57 0.0811 0.0581 0.0342 0.0257 0.0244 0.0240 8.9012 7.4568 5.2648 4.1179 3.8492 3.7311 0.2820 0.4563 0.6427 0.7073 0.7050 0.7010 27.668 27.498 27.017 26.373 26.028 25. 0.7669 0.7601 0.7388 0.7073 0.6896 0.6787 0.3582 0.3401 0.3314 0.3470 0.3591 0.3662 31.41 30.80 30.49 30.67 30.89 31.06 2.0340 2.0294 2.0215 2.0303 2.0409 2.0409 Table 3: Varying the number of flow steps in PMRF (Algorithm 1) on the LFW-Test blind face image restoration benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Increasing the number of steps generally improves the perceptual quality while hindering the IndRMSE. These results are expected due to the distortion-perception tradeoff. 3 5 10 25 50 100 FID KID NIQE Precision IndRMSE 78.2331 64.3121 51.9845 49.3151 49.5581 49. 0.0692 0.0524 0.0387 0.0366 0.0375 0.0377 8.2315 6.8733 4.9896 4.0028 3.7126 3.6242 0.3477 0.5143 0.6546 0.6692 0.6826 0.6710 3.3934 3.8008 4.8648 6.1382 6.7960 7.2004 Table 4: Varying the number of flow steps in PMRF (Algorithm 1) on the WIDER-Test blind face image restoration benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Increasing the number of steps generally improves the perceptual quality while hindering the IndRMSE. These results are expected due to the distortion-perception tradeoff. 3 5 10 25 50 100 FID KID NIQE Precision IndRMSE 85.0361 65.2563 42.5002 41.2685 41.4446 42. 0.0704 0.0451 0.0179 0.0160 0.0174 0.0183 9.9988 8.4650 5.5677 4.0726 3.6953 3.5704 0.2742 0.5381 0.7144 0.7144 0.6845 0.6907 5.3486 5.7665 7.1134 9.2164 10.3403 11.0674 Table 5: Varying the number of flow steps in PMRF (Algorithm 1) on the WebPhoto-Test blind face image restoration benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Increasing the number of steps generally improves the perceptual quality while hindering the IndRMSE. These results are expected due to the distortion-perception tradeoff. 3 5 10 25 50 100 FID KID NIQE Precision IndRMSE 128.7858 113.4734 91.3677 81.0642 78.7174 79. 0.0996 0.0782 0.0484 0.0347 0.0324 0.0313 9.1626 7.5893 5.4199 4.2402 3.9512 3.7990 0.3907 0.5553 0.6413 0.6462 0.6265 0.5602 3.2961 3.7371 4.8369 6.3098 7.0159 7.6887 24 Table 6: Varying the number of flow steps in PMRF (Algorithm 1) on the CelebAdult-Test blind face image restoration benchmark. Red, blue and green indicate the best, the second best, and the third best scores, respectively. Increasing the number of steps generally improves the perceptual quality while hindering the IndRMSE. These results are expected due to the distortion-perception tradeoff. 3 5 10 25 50 100 FID KID NIQE Precision IndRMSE 122.8780 113.7837 105.7426 102.8914 102.1454 102.0568 0.0551 0.0426 0.0319 0.0293 0.0276 0.0279 6.6818 5.5810 4.4119 3.7367 3.5609 3.4878 0.3944 0.4444 0.6111 0.5500 0.6278 0.5944 3.7339 4.3313 5.4908 6.7145 7.3004 7.7286 Table 7: Quantitative evaluation of blind face restoration algorithms on the LFW-Test data set. Method FID KID NIQE Precision IndRMSE SwinIR ( Posterior mean) 87.34 0.0808 DOT RestoreFormer++ RestoreFormer CodeFormer VQFRv1 VQFRv2 GFPGAN DiffBIR DifFace BFRffusion 97.09 50.80 49.04 52.82 51.31 51.16 47.59 40.97 46.48 50. 0.0891 0.0386 0.0355 0.0387 0.0399 0.0378 0.0308 0.0234 0.0329 0.0377 PMRF (Ours) 49.32 0.0366 8. 5.705 3.911 4.168 4.484 3.590 3.761 4.554 5.738 4.024 4.963 4.003 0.2513 0.1806 0.6330 0.6674 0.6756 0.6014 0.6154 0.6400 0.5804 0.7411 0. 0.6692 0 26.24 9.429 12.21 9.534 11.26 16.15 9.842 9.105 11.33 7.210 6.138 Table 8: Quantitative evaluation of blind face restoration algorithms on the WIDER-Test data set. Method FID KID NIQE Precision IndRMSE SwinIR ( Posterior mean) 91.96 0.0780 DOT RestoreFormer++ RestoreFormer CodeFormer VQFRv1 VQFRv2 GFPGAN DiffBIR DifFace BFRffusion 82.15 45.41 50.23 39.27 44.21 38.70 41.28 35.87 37.38 56. 0.0618 0.0209 0.0251 0.0138 0.0192 0.0157 0.0182 0.0114 0.0131 0.0307 PMRF (Ours) 41.27 0.0160 10. 7.633 3.759 3.894 4.164 3.055 3.995 4.450 5.659 4.383 4.647 4.073 0.1649 0.4082 0.6505 0.6505 0.7227 0.5959 0.6381 0.7876 0.6361 0.7856 0. 0.7144 0 14.900 14.466 14.200 12.185 17.042 16.368 11.840 11.106 10.418 11.759 9.2164 Table 9: Quantitative evaluation of blind face restoration algorithms on the WebPhoto-Test data set. Method FID KID NIQE Precision IndRMSE SwinIR ( Posterior mean) 132.1 0.1022 DOT RestoreFormer++ RestoreFormer CodeFormer VQFRv1 VQFRv2 GFPGAN"
        },
        {
            "title": "DiffBIR\nDifFace\nBFRffusion",
            "content": "125.6 75.60 77.80 84.17 75.57 83.52 88.43 92.82 80.05 84.83 0.0865 0.0291 0.0334 0.0406 0.0312 0.0411 0.0494 0.0541 0.0341 0.0388 PMRF (Ours) 81. 0.0347 9.638 7.397 4.080 4.460 4.709 3.608 4.620 4.941 6.069 4.405 5.612 4.240 0. 0.3071 0.6143 0.6265 0.6830 0.5774 0.5848 0.6781 0.5307 0.7273 0.5872 0.6462 0 20.69 18.43 11.55 8.952 12.53 14.48 9.240 9.152 10.31 7. 6.310 Table 10: Quantitative evaluation of blind face restoration algorithms on the CelebAdult-Test data set. Method FID KID NIQE Precision IndRMSE SwinIR ( Posterior mean) 143.80 0.0811 DOT RestoreFormer++ RestoreFormer CodeFormer VQFRv1 VQFRv2 GFPGAN DiffBIR DifFace BFRffusion 208.54 103.81 103.96 111.62 105.59 104.72 109.19 109.74 98.780 103.06 0.1634 0.0313 0.0315 0.0427 0.0336 0.0337 0.0395 0.0411 0.0243 0.0290 PMRF (Ours) 102. 0.0293 7.477 6.018 4.006 4.320 4.544 3.756 3.999 4.423 5.650 3.901 4.702 3.737 0. 0.0444 0.5167 0.5556 0.5722 0.5944 0.6056 0.5111 0.5000 0.6833 0.6056 0.5500 0 44.24 11.43 14.97 10.49 11.14 18.51 11.90 9.853 12.66 8. 6.715 Figure 5: Comparison with state-of-the-art blind face restoration methods on inputs from the CelebA-Test data set. Our method produces high perceptual quality while achieving lower distortion overall. Zoom in for best view. 26 Figure 6: Qualitative results on the real-world LFW-Test data set. Our algorithm produces reconstructions with either better or on-par perceptual quality compared to the state-of-the-art, while maintaining very high consistency with the input measurements. Zoom in for best view. Figure 7: Qualitative results on the real-world WebPhoto-Test data set. Our algorithm produces reconstructions with either better or on-par perceptual quality compared to the state-of-the-art, while maintaining very high consistency with the input measurements. Zoom in for best view. Figure 8: Qualitative results on the real-world CelebAdult-Test data set. Our algorithm produces reconstructions with either better or on-par perceptual quality compared to the state-of-the-art, while maintaining very high consistency with the input measurements. Zoom in for best view. Figure 9: controlled experiment comparing PMRF with previous methodologies, where we vary the number of steps in each algorithm (Algorithms 1 and 2 to 4). Specifically, we use {5, 10, 20, 50, 100}, where larger marker size corresponds to larger value of K. See Section 5.2 for more details. Table 11: comparison of the forward process and training loss of PMRF and the baseline methods from Section 5.2. For the flow from algorithm, we have = for all tasks besides superresolution. For the super-resolution task, we up-scale using nearest-neighbor interpolation. Forward process Flow training loss PMRF (Ours) Flow cond. on Flow cond. on ˆX Flow from minθ Zt = tX + (1 t)Z0 Z0 = fω (Y ) + σsϵ ϵ (0, I) Zt = tX + (1 t)Z0 minθ Z0 (0, I) Zt = tX + (1 t)Z0 minθ Z0 (0, I) Zt = tX + (1 t)Z0 Z0 = + σsϵ ϵ (0, I) minθ (cid:82) 1 0 (cid:82) 1 0 (cid:82) 1 0 (cid:82) 1 0 (cid:2)(X Z0) vθ(Zt, t)2(cid:3) dt (cid:2)(X Z0) vθ(Zt, t, )2(cid:3) dt (cid:2)(X Z0) vθ(Zt, t, fω (Y ))2(cid:3) dt (cid:2)(X Z0) vθ(Zt, t)2(cid:3) dt 28 Algorithm 2: Flow conditioned on Training Solve θ arg minθ // Zt := tX + (1 t)Z0, Z0 (0, I). is sampled uniformly from [0, 1]. (cid:2)(X Z0) vθ(Zt, t, )2(cid:3) Inference (using Eulers method with steps to solve the ODE) Initialize ˆx (0, I) for 0, . . . , 1 do vθ (ˆx, ˆx ˆx + 1 , y) // is the given degraded measurement Return ˆx Algorithm 3: Flow conditioned on ˆX Training Stage 1: Solve ω arg minω (cid:2)X fω(Y )2(cid:3) Stage 2: Solve θ arg minθ // Zt := tX + (1 t)Z0, Z0 (0, I). is sampled uniformly from [0, 1]. (cid:2)(X Z0) vθ(Zt, t, fω (Y ))2(cid:3) Inference (using Eulers method with steps to solve the ODE) Initialize ˆx (0, I) for 0, . . . , 1 do vθ (ˆx, ˆx ˆx + , fω (y)) Return ˆx // is the given degraded measurement Algorithm 4: Flow from Training (cid:2)(X Z0) vθ(Zt, t)2(cid:3) Solve θ arg minθ Z0 = + σsϵ, ϵ (0, I), and is the up-scaled version of that matches the dimensionality of X. [0, 1]. is sampled uniformly from // Zt := tX + (1 t)Z0, Inference (using Eulers method with steps to solve the ODE) // is the up-scaled version of the degraded Initialize ˆx (y, Iσ2 ) measurement for 0, . . . , 1 do vθ (ˆx, ) ˆx ˆx + Return ˆx 29 Figure 10: Visual results on the image colorization task from Section 5.2. Our method outperforms all baselines for any number of inference steps K. Zoom in for best view. 30 Figure 11: Visual results on the image denoising task from Section 5.2. Our method is on-par with flow from , and outperforms the posterior sampling methods for any number of inference steps K. Zoom in for best view. Figure 12: Visual results on the image inpainting task from Section 5.2. Our method outperforms all baselines for any number of inference steps K. Zoom in for best view. 32 Figure 13: Visual results from Section 5.2 on the image super-resolution task. Our method is onpar with flow from , and outperforms the posterior sampling methods for any number of inference steps K. Zoom in for best view. 33 Table 12: Training hyper-parameters of the HDiT architecture (Crowson et al., 2024). We use this architecture as the vector field vθ in PMRF (Algorithm 1), and also in the baseline methods described in Section 5.2 (Algorithms 2 to 4). Hyper-parameter Blind face restoration (Section 5.1) Controlled experiments (Section 5.2) Parameters Training Epochs Batch Size Image Size Precision Training Hardware Training Time Patch Size Levels (Local + Global Attention) Depth Widths Attention Heads (Width / Head Dim) Attention Head Dim Neighborhood Kernel Size Mapping Depth Mapping Width Optimizer Learning Rate Learning Rate Scheduler AdamW betas AdamW eps Weight Decay EMA Decay 160M 3850 256 512512 bfloat16 mixed 16 A100 40GiB 12 days 4 2 + 1 (2,2,8) (256,512,1024) (4, 8, 16) 64 7 1 768 AdamW 4 5 10 Multi-step, last 350 epochs (0.9, 0.95) 10 10 0.9999 2 121M 1000 256 256256 bfloat16 mixed 4 L40 48GiB 2.5 days 4 1 + 1 (2,11) (384,768) (6,12) 64 7 1 768 AdamW 4 5 10 Not applied (0.9, 0.95) 10 10 0.9999 8"
        }
    ],
    "affiliations": [
        "Technion Israel Institute of Technology"
    ]
}