{
    "paper_title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation",
    "authors": [
        "Zhaojian Yu",
        "Yilun Zhao",
        "Arman Cohan",
        "Xiao-Ping Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce self-invoking code generation, a new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with a base problem and a related, more complex problem. They must solve the base problem and then utilize its solution to address the more complex one. This work features three key contributions. First, we propose a general recipe for generating more challenging versions of existing benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on self-invoking code generation. Second, from the analysis of experimental results over twenty LLMs on our benchmarks, we have two important observations: (i) Most LLMs excel in traditional code generation benchmarks like HumanEval and MBPP, but their performance declines on self-invoking tasks. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. (ii) On self-invoking code generation task, the instruction-tuned models demonstrate only marginal improvements compared to the base models. Third, we disclose the types of failure modes that exist in our evaluation results. All these results underscore the need for further advancements in self-invoking code generation tasks and provide a new direction for future research on enhancing LLMs' code reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation Zhaojian Yu1 Yilun Zhao2 Arman Cohan2 Xiao-Ping Zhang1 1Tsinghua University 2Yale University github.com/CodeEval-Pro/CodeEval-Pro 4 2 0 2 0 ] . [ 1 9 9 1 1 2 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce self-invoking code generation, new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with base problem and related, more complex problem. They must solve the base problem and then utilize its solution to address the more complex one. This work features three key contributions. First, we propose general recipe for generating more challenging versions of existing benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on self-invoking code generation. Second, from the analysis of experimental results over twenty LLMs on our benchmarks, we have two important observations: (i) Most LLMs excel in traditional code generation benchmarks like HumanEval and MBPP, but their performance declines on selfinvoking tasks. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. (ii) On self-invoking code generation task, the instruction-tuned models demonstrate only marginal improvements compared to the base models. Third, we disclose the types of failure modes that exist in our evaluation results. All these results underscore the need for further advancements in self-invoking code generation tasks and provide new direction for future research on enhancing LLMs code reasoning capabilities."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated significant progress in various code-related tasks including code generation (Roziere et al., 2023; Zhang et al., 2023; Ni et al., 2024), program repair (Xia et al., 2022; Jin et al., 2023), and code translation (Zhu et al., 2022), etc. Traditional human-annotated benchmarks such as HumanEval Figure 1: The overview of self-invoking code generation in HumanEval Pro and MBPP Pro. Given base problem and related, more complex problem, they are required to solve the base problem and use its solution to address the complex problems. (Chen et al., 2021) and MBPP (Austin et al., 2021) have been widely adopted to evaluate the code generation abilities of LLMs, providing standardized evaluation protocols for assessing their performance on code-related tasks. However, these existing benchmarks primarily focus on isolated, single-function code generation, which represents only subset of the challenges encountered in realworld software development scenarios. To evaluate LLMs under more realistic problemsolving scenarios, BigCodeBench (Zhuo et al., 2024) presents benchmark that comprises of complex and practical problems requiring LLMs to use multiple function calls from diverse libraries. While BigCodeBench highlights the use of external function calls, it falls short in assessing LLMs reasoning ability to generate and invoke their own generated functions in problem-solving. CRUXEval (Gu et al., 2024) assesses LLMs code reasoning by predicting function inputs and outputs. However, the direct input and output prediction does not involve explicit code generation. In practical software engineering contexts, developers must not only write code but also comprehend, modify, and utilize existing code to solve more complex problems. Hence, the ability to understand and subsequently leverage ones own generated code, namely self-invoking code generation (Figure 1), plays an important role for LLMs to leverage their reasoning capabilities to code generation that current benchmarks fail to capture. Therefore, we present HumanEval Pro and MBPP Pro, two expanded versions of the traditional HumanEval and MBPP benchmarks to evaluate LLMs on self-invoking code generation task. As illustrated in Figure 1, HumanEval Pro and MBPP Pro extend beyond simple code generation by introducing self-invoking problems which requires LLMs to solve the base problem and invoke their self-generated code to solve more complex problem. By evaluating LLMs on self-invoking code generation task, HumanEval Pro and MBPP Pro provide useful and important probe to better understand the programming capabilities of LLMs. The capability of self-invoking code generation also facilitates LLMs to tackle difficult tasks with greater autonomy and effectiveness. To obtain HumanEval Pro and MBPP Pro, we propose general recipe for constructing self-invoking code generation benchmarks by building upon existing datasets. First, we use Deepseek-V2.5 (DeepSeek-AI, 2024) to generate self-invoking problems based on the original problems in HumanEval and MBPP. These problems are designed to be more complex than the base problems and closely related to them, ensuring progressive reasoning and coherent code invocation. Second, we generate the candidate solution and test inputs for each problem. Third, we execute the code of candidate solution to generate output and use the assert command in Python to build test cases. In the third stage, human experts are assigned to manually review each problem and continuously modify and execute the code of solutions to ensure that all canonical solutions could correctly solve the problem and cover the test cases. To verify the reproducibility of our benchmark construction approach, we further construct BigCodeBench-Lite Pro, new self-invoking problems set derived from BigCodeBench (Zhuo et al., 2024). On Bigcodebench-Lite Pro, LLMs show consistent performance trend with HumanEval Pro and MBPP Pro, which emphasizes the generalizability of our construction pipeline. Therefore, our benchmark construction approach can also be extended to adapt other code generation benchmarks, particularly as the capabilities of LLMs advance and older benchmarks become obsolete. Through extensive evaluation of various LLMs, we uncover significant disparity between traditional code generation and self-invoking code generation capabilities. Our findings reveal that while frontier LLMs excel at generating individual code snippets, they often struggle to effectively utilizing their own generated code for solving more complex problems. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro, demonstrating the challenges inherent in self-invoking code generation. From the comparison between instruction-tuned models and their base models, we found that instruction-tuned models are less efficient on self-invoking code generation than traditional code generation task. Furthermore, our detailed statistics of failure cases in HumanEval Pro and MBPP Pro also reflect the shortcomings of LLMs in self-invoking code generation, thereby providing complementary insights on real-world coding capabilities of LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Recent advances in LLMs have demonstrated remarkable capabilities in code generation and understanding. This section reviews the current landscape of code-related benchmarks and LLMs. Benchmarks for Code Generation The evaluation landscape for Code LLMs has evolved significantly. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) serve as fundamental benchmarks, focusing on Python function completion tasks with test-driven evaluation. Several benchmarks have expanded code evaluation benchmarks to encompass multiple programming languages (Zheng et al., 2023; Athiwaratkun et al., 2022), complex tasks like program repair (Haque et al., 2022; Jiang et al., 2023; Muennighoff et al., 2024; Xia et al., 2024), dynamic problem sets (Jain et al., 2024), and code reasoning through code summarization (Barone and Sennrich, 2017; Hasan et al., 2021) and simFigure 2: The overview of benchmark construction. An example is shown in Figure 8. We summarize the entire benchmark construction process as follows: (1) Self-invoking problem Generation: We use Deepseek-V2.5 to generate the self-invoking problems, as well as their candidate solutions and test inputs. (2) Solutions Generation: We execute the generated solution with the test inputs in controlled Python environment to obtain ground truth outputs. (3) Test Cases Generation: We employ an iterative method involving Python execution check and manual review to ensure that all test cases pass successfully. The final execution results are then used to construct complete test cases with assert command. ulated execution (Gu et al., 2024). To evaluate LLMs in professional software engineering, benchmarks like SWE-Bench (Jimenez et al., 2023), EvoCodeBench (Li et al., 2024), RepoBench (Liu et al., 2023), and GoogleCodeRepo (Shrivastava et al., 2023) focus on real-world tasks, code evolution, and repository-level challenges. These benchmarks collectively drive the advancement of LLMs, providing valuable insights into their strengths and limitations. Our benchmarks introduce novel selfinvoking code generation task, which addresses gaps left by existing benchmarks. This addition provides more holistic framework to evaluate LLMs on leveraging their reasoning capabilities to code generation. Moreover, our benchmark construction method could also push existing benchmarks forward to accommodate more complex and challenging code-related tasks. LLMs for Code Generation The development of foundation models specifically designed for code generation has seen significant progress. CodeX (Chen et al., 2021) pioneered this direction by finetuning GPT models on code-specific data. Subsequent models like CodeGeeX (Zheng et al., 2023) and CodeLLaMA (Roziere et al., 2023) further advanced the field by incorporating multilingual code understanding and generation capabilities. StarCoder (Li et al., 2023), DeepseekCoder (Zhu et al., 2024) and Qwen2.5-Coder (Hui et al., 2024) demonstrated the importance of high-quality code data curation and specialized architecture designs. Building upon these models, researchers have explored instruction-tuning approaches using GPT-4 or GPT-3.5 as teachers. Notable examples include WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2024), WaveCoder (Yu et al., 2024), OpenCodeInterpreter (Zheng et al., 2024), and ReflectionCoder (Ren et al., 2024). These models have achieved impressive performance on standard code generation benchmarks through enhanced data diversity and instruction complexity."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "To facilitate meaningful comparison between self-invoking code generation and traditional code generation, we have crafted two new benchmarks, HumanEval Pro and MBPP Pro. These benchmarks are extensions of the original HumanEval and MBPP, requiring the model to solve both the base problem and more complex self-invoking problem. In addressing the self-invoking problems, LLMs are required to apply the solutions they have independently generated for the base problem. This evaluation of self-invoking code generation offers deeper insights into the programming capabilities of LLMs, extending beyond the scope of singleproblem code generation. The benchmark construction process, illustrated in Figure 2, will be discussed in detail in the following subsections."
        },
        {
            "title": "3.1 Self-invoking Problem Generation",
            "content": "To ensure that all benchmarks are permissively licensed, we employ one of the state-of-the-art (SoTA) open-source models, DeepSeek-V2.5, to create new problems and solutions derived from the original HumanEval and MBPP datasets. Two main guidelines is established for self-invoking problems generation to rigorously evaluate LLMs. 1) Complexity Enhancement: The self-invoking problems should introduce additional programming challenges while preserving the core functionality of the original problems. This ensures that successful solutions require both understanding of the original code and ability to extend it appropriately. 2) Semantic Relevance: The self-invoking problems should maintain sufficient semantic similarity to their original counterparts to enable meaningful self-invoking code generation process. Appendix F.1 presents the prompt for self-invoking problem generation."
        },
        {
            "title": "3.2 Solution Generation",
            "content": "In self-invoking problem generation process, the candidate solution and test inputs are generated simultaneously with the self-invoking problem. However, when dealing with self-invoking problems, these generated solutions are often flawed, which can lead to execution errors during the verification process, thereby highlighting significant challenge in maintaining the accuracy and effectiveness of these test cases. Therefore, as shown in Figure 2, we propose method to iteratively execute the solution code with test inputs and obtain expected outputs correctly. For the execution errors, the authors manually analyze these errors and modify the solutions to ensure that the final solution can cover all the test cases comprehensively. The manual review process involves (1) identifying the root causes of the errors, (2) making necessary adjustments to the code or algorithm, and (3) re-evaluating the solution against the entire set of test cases to confirm its correctness and completeness. Table 1 shows that our rigorous verification process ensures the high quality of our benchmarks."
        },
        {
            "title": "3.3 Test Cases Generation",
            "content": "After obtaining the self-invoking problem and its candidates solution, critical challenge is ensuring the reliability of the test cases (with both test inputs and expected execution outputs) to validate the the generated solutions. Despite the apparent simplicity of using the same LLM context to generate both problems and test cases, CRUXEval (Gu et al., 2024) results show that even leading models like GPT-4 achieve only 63.4% pass@1 rate in test output prediction. This suggests that using models like GPT-4 to directly generate test cases for problems will lead to many inaccurate evaluation results. Our iterative verification method effectively addresses this challenge. By combining Iteration HumanEval Pro (%) MBPP Pro (%) Round 1 Round 2 Round 3 64.0 98.8 100.0 84.7 99.7 100.0 Table 1: Pass@1 (%) of candidate solutions across different iteration rounds for canonical solution and test case generation with human manual review. Python execution checks with manual reviews, we ensure that all test cases accurately assess solution correctness and achieves 100% pass@1 under correct implementation conditions. Furthermore, we categorize the common execution errors that occur during test case generation into four main types: variable type mismatches, index out of bounds, invalid input handling, and edge case failures. To obtain the high-quality self-invoking problem solutions, we adopt main remediation strategies including: (1) implementing input validation, (2) adding type checking, (3) handling edge cases explicitly, and (4) refining problem specifications when necessary. Beyond basic execution correctness, we also verify the self-invoking problem and solutions in the following aspects: (1) logical consistency between problem statements and test cases, (2) coverage of essential edge cases, and (3) alignment with original problem objectives."
        },
        {
            "title": "4 Experiments",
            "content": "We present results of proprietary models and opensource models on HumanEval Pro and MBPP Pro: Qwen-2.5-Coder (Base and Instruct, 1.5B, 7B, 33B) (Hui et al., 2024), DeepseekCoder (Base and Instruct) (Guo et al., 2024), DeepseekCoderV2 (DeepSeek-AI, 2024), Yi-Coder-9B (Base and Instruct) (01.AI, 2024), OpenCoder (Base and instruct) (Huang et al., 2024), Magicoder-S-DS6,7B (Wei et al., 2024), WaveCoder-Ultra-6.7B (Yu et al., 2024), Codestral-22B (Mistral, 2024), GPT3.5 (Ouyang et al., 2022), GPT-4o (OpenAI, 2024a), Claude-3.5-sonnet (Anthropic, 2024) and o1-mini (OpenAI, 2024b). To facilitate reproducibility, the HuggingFace checkpoints of all open-source models and API name of proprietary models are provided in Appendix C. Our prompts for evaluation is shown in Appendix F.2. Following previous work (Chen et al., 2021), We use the pass@k (Chen et al., 2021) score as the evaluation metric of HumanEval Pro and MBPP Pro. We use greedy decoding strategy to generate solutions for all open-source models and set"
        },
        {
            "title": "Model",
            "content": "Params HumanEval (+) HumanEval Pro (1-shot) (0-shot) MBPP (+)"
        },
        {
            "title": "MBPP Pro",
            "content": "(0-shot) (1-shot) o1-mini GPT-4o GPT-4-Turbo Claude-3.5-sonnet - - - - Deepseek-V2.5 DeepseekCoder-V2-instruct - 21/236B Qwen2.5-Coder-1.5B-base Qwen2.5-Coder-1.5B-instruct DeepseekCoder-6.7B-base DeepseekCoder-6.7B-instruct Magicoder-S-DS-6.7B WaveCoder-Ultra-6.7B Qwen2.5-Coder-7B-base Qwen2.5-Coder-7B-instruct OpenCoder-8B-base OpenCoder-8B-instruct Yi-Coder-9B-base Yi-Coder-9B-chat Codestral-22B-v0. DeepseekCoder-33B-base DeepseekCoder-33B-instruct Qwen2.5-Coder-32B-base Qwen2.5-Coder-32B-instruct LLaMA3-70B-instruct 1.5B 1.5B 6.7B 6.7B 6.7B 6.7B 7B 7B 8B 8B 9B 9B 22B 33B 33B 32B 32B 70B"
        },
        {
            "title": "Proprietary Models",
            "content": "97.6 (90.2) 90.2 (86.0) 90.2 (86.6) 92.1 (86.0) 76.2 75.0 72.0 72.6 Open-source Models 90.2 (83.5) 90.2 (84.8) 43.9 (36.6) 70.7 (66.5) 49.4 (39.6) 78.6 (71.3) 76.8 (70.7) 78.6 (69.5) 61.6 (53.0) 88.4 (84.1) 66.5 (63.4) 83.5 (78.7) 53.7 (46.3) 85.4 (74.4) 81.1 (73.2) 56.1 (47.6) 79.3 (75.0) 65.9 (60.4) 92.7 (87.2) 81.7 (72.0) 73.8 77.4 37.2 33.5 35.4 55.5 54.3 54.9 54.9 65.9 39.0 59. 42.7 59.8 59.1 49.4 56.7 61.6 70.1 60.4 84.8 77.4 76.2 79. 76.8 82.3 39.6 37.8 36.6 61.6 56.7 59.8 56.1 67.1 42.1 54.9 50.0 64. 65.9 49.4 62.8 67.1 80.5 64.6 93.9 (78.3) 86.8 (72.5) 85.7 (73.3) 91.0 (74.6) 87.6 (74.1) 89.4 (76.2) 69.2 (58.6) 69.2 (59.4) 70.2 (51.6) 74.9 (65.6) 75.7 (64.4) 74.9 (63.5) 76.9 (62.9) 83.5 (71.7) 79.9 (70.4) 79.1 (69.0) 78.3 (64.6) 81.5 (69.3) 78.2 (62.2) 74.2 (60.7) 80.4 (70.1) 83.0 (68.2) 90.2 (75.1) 82.3 (69.0) 68.3 70.9 69.3 66.4 71.2 71.4 48.4 42. 50.5 57.1 58.7 60.1 61.4 64.8 52.4 57.9 60.3 64.8 63.8 59.0 64. 67.7 69.8 63.5 81.2 80.2 73.3 76.2 77.5 76.5 51.3 43.7 55.0 58.2 64.6 64. 68.0 69.8 53.7 61.4 61.4 71.7 71.2 65.1 68.3 73.3 77. 70.4 Table 2: Main result of different models on HumanEval Pro and MBPP Pro. More results is shown in Appendix A. Figure 3: Performance Comparison: HumanEval Pro (and MBPP Pro) vs. HumanEval (and MBPP). temperature=0.2 for all API-models. For all previous benchmarks, we use the reported results whenever available; otherwise, we evaluate using the EvalPlus codebase (Liu et al., 2024). Table 2 presents the pass@1 scores of HumanEval Pro and MBPP Pro alongside those of other relevant benchmarks, including HumanEval, HumanEval+, MBPP, and MBPP+ (Liu et al., 2024), highlighting the following salient observations: 1) Most LLMs have 10% to 15% absolute performance drop on self-invoking code generation benchmarks. 2) Large size open-source LLMs have comparable performance with proprietary LLMs on self-invoking benchmarks. Notably, DeepseekCoder-V2-instruct achieves 77.4% on HumanEval Pro, surpassing the score of all propriFigure 4: HumanEval (or MBPP) scores against the results on HumanEval Pro and MBPP Pro (HumanEval+ and MBPP+). We presents the comparison between base model and instruct model. etary LLMs. 3) Most instruction-tuned models have less improvements on self-invoking code generation benchmarks (e.g., HumanEval Pro) than traditional benchmarks (e.g.,HumanEval). For instance, Qwen2.5Coder-32B-instruct have 26.8% absolute improvement on HumanEval compared to Qwen2.5Coder-32B-base (from 65.9% to 92.7%) but only 8.5% on HumanEval Pro (from 61.6% to 70.1%). Appendix also presents the evaluation results for different values with the sampling generation strategy. Section 4 provides detailed analysis for these results."
        },
        {
            "title": "5 Analysis",
            "content": "Frontier LLMs still face challenges in selfinvoking code generation. Table 2 and Figure 3 present the comparison between HumanEval Pro (or MBPP Pro) and HumanEval (or MBPP). As shown in Table 2, while 1-shot prompting improves model performance on HumanEval Pro and MBPP Pro, the pass@1 scores achieved on these datasets remain notably lower compared to their counterparts on the original HumanEval and MBPP benchmarks. This performance gap indicates that although current LLMs excel at direct code generation tasks, they struggle to maintain comparable performance when tasked with selfinvoking code generation for complex problems. Notably, even the SoTA reasoning model o1-mini, that achieves an impressive 96.2% pass@1 on HumanEval, demonstrates significant performance degradation when tackling more complex problems, as evidenced by its lower 76.2 pass@1 score on HumanEval Pro under zero-shot setting."
        },
        {
            "title": "5.1 Base Model vs Instruct Model",
            "content": "Currently, the training of LLMs is typically divided into two stages: pre-training stage that relies (a) Qwen2.5-Coder-7B-base (b) Qwen2.5-Coder-32B-base (c) Qwen2.5-Coder-7B-instruct (d) Qwen2.5-Coder-32B-instruct Figure 5: The confusion matrix of different models. We use (Failed, Passed) to indicate samples that fail in HumanEval Pro (or MBPP Pro) but pass in HumanEval (or MBPP). on self-supervised learning, and subsequent supervised fine-tuning stage based on <instruction, response> pairs. Previous studies (Luo et al., 2023; Hui et al., 2024; Wei et al., 2024) have shown that the instruction-based supervised fine-tuning stage can significantly enhance the code generation capabilities of base models on traditional benchmarks. For example, as shown in Table 2, Qwen2.5-Coderinstruct 7B started with the Qwen2.5-Coder-7B base model and improved the HumanEval pass@1 score from 61.6% to 88.4%. There remains new curiosity about whether these instruction-tuned models still show such significant improvements under new problem solving scenario. In this section, we explore this through our new benchmarks. The instruction-tuned models demonstrate only marginal improvements compared to the base models on self-invoking code generation. In Figure 4, we plot the previous reported HumanEval (or MBPP) scores against the results on HumanEval Pro and MBPP Pro (HumanEval+ and MBPP+). From the Figure 4, we have an interesting finding: When observing the correlation between HumanEval (or MBPP) and HumanEval Pro (or MBPP Pro), we see that the orange dot (indicates base model) is always to the upper left of the blue dot (indicates instruction-tuned model). However, for the comparison between HumanEval (or MBPP) and HumanEval+ (or MBPP+), the blue dot is always distributed to the upper of orange dot (even in line on HumanEval vs HumanEval+). Overall, this suggests that while instruction-based fine-tuning significantly improves performance on simpler benchmarks like HumanEval (+) (or MBPP (+)), its efficiency diminishes for more complex self-invoking code generation tasks. On the other hand, base models like Qwen2.5-Coder-base and Deepseek-Coder-base have higher Ratio = pass@k on HumanEval Pro (or MBPP Pro) pass@k on HumanEval (or MBPP) (1) than instruct models, which indicates that they have elevated training potential on self-invoking code generation task."
        },
        {
            "title": "Different Models",
            "content": "From Table 2, we observe that most LLMs have score gap between direct code generation and selfinvoking code generation tasks. To better understand the correlation and overlap between these two kinds of tasks, we compare the number of problems passed and failed in HumanEval Pro and MBPP Pro with their corresponding base problems in HumanEval and MBPP. Figure 5 presents an array of confusion matrix over problems, highlighting the following salient observations: Most LLMs are proficient in code generation tasks but struggle with generating code"
        },
        {
            "title": "AssertionError\nNameError\nValueError\nIndexError\nTypeError\nOther Errors",
            "content": "Failing to pass the test cases. The code includes undefined variables. Unaware of the value of variables Array out of bounds Incorrect variable type usage. KeyError, SyntaxError, ZeroDivisionError, IndentationError, etc. Examples in Appendix G.1 Examples in Appendix G.2 Examples in Appendix G.3 Examples in Appendix G.4 Examples in Appendix G.5 Table 3: The execution error types and their descriptions in our evaluation results."
        },
        {
            "title": "Model",
            "content": "GPT-4o DeepseekV2.5 Qwen2.5-Coder-32B-ins Qwen2.5-Coder-7B-ins"
        },
        {
            "title": "CoT HE Pro MBPP Pro",
            "content": "75.0 78.0 73.8 74. 70.1 72.0 65.9 71.3 70.9 70.9 71.2 71.4 69.8 70.1 64.8 64. Table 4: The execution error types and their descriptions in our evaluation results. Figure 6: Error types of GPT-4o with and without CoT reasoning on HumanEval Pro. that can self-invoke effectively. Although some SoTA LLMs such as Qwen2.5-Coder-32B-instruct successfully solve 90% of base problems on the original HumanEval and MBPP benchmarks, over 25% of problems still fail on more challenging HumanEval Pro and MBPP Pro benchmarks with self-invoking code generation (as shown in the top right of each subfigure in Figure 5). This suggests that the drop in the models scores on HumanEval Pro and MBPP Pro is largely due to its lower accuracy in generating self-invoking code compared to direct code generation. The instruction-tuned model does not significantly outperform the base model in selfinvoking code generation task. From the confusion matrices of the base model and the instruct model in Figure 5, we can observe trend: the instruction-tuned model typically has significantly higher number of (Passed, Passed) instances compared to the base model. However, for samples that pass the base problems but fail in HumanEval Pro and MBPP Pro, i.e., (Failed, Passed), the instruct model does not demonstrate notable improvement. This observation underscores our argument in Section 5.1: current instruction-based fine-tuning approaches are insufficiently effective for more complex selfinvoking code generation tasks."
        },
        {
            "title": "5.3 Chain-of-Thought Prompting",
            "content": "To evaluate the impact of the models reasoning ability, we evaluated the performance of GPT-4o, DeepseekV2.5, Qwen2.5-Coder-instruct (7B and 32B) with and without Chain-of-Thought (CoT) prompting (Wei et al., 2022) on HumanEval Pro and MBPP Pro. The full prompt we use is shown in Appendix F.2. For CoT prompting, we used the greedy decoding strategy for generation to align the results before. As shown in Table 4, after applying CoT, the pass@1 of the selected models on HumanEval Pro witnesses significant improvement. Notably, the accuracy of GPT-4o increases from 75.0% to 78.0%. On MBPP Pro, although the model does not show significant improvement, it still maintains its original performance level, indicating that CoT can enhance the accuracy of model-generated code to notable degree. CoT could help Code LLMs to generate more reliable code when scheduling across multiple code-related problems. To further study which aspects of code LLM can be improved by CoT, we use Python to run the code generated by GPT4o with and without CoT, and present the number of all error types that occurred in Figure 6. We have two main observations: (1) With CoT prompting, the AssertionError number decreases from 28 to 24. This indicates that CoT prompting enables the model to generate code that more frequently passes test cases. (2) The NameError number de-"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT4-Turbo Claude-3.5-sonnet DeepseekV2.5 Qwen2.5Coder-1.5B-base Qwen2.5Coder-1.5B-instruct OpenCoder-8B-base OpenCoder-8B-instruct DeepseekCoder-6.7B-base DeepseekCoder-6.7B-instruct WaveCoder-Ultra-6.7B Magicoder-S-DS-6.7B Yi-Coder-9B Yi-Coder-9B-Chat Qwen2.5Coder-7B-base Qwen2.5Coder-7B-instruct DeepseekCoder-33B-base DeepseekCoder-33B-instruct Qwen2.5Coder-32B-base Qwen2.5Coder-32B-instruct Codestral-22B QwQ-32B-preview BCB-Lite Pro (%) 64.9 61.4 73.7 80.7 50.9 50.9 56.1 75.4 59.6 56.1 61.4 50.9 57.9 66.7 59.6 64. 71.9 80.7 68.4 80.7 78.9 86.0 52.6 52.6 50.9 50.9 15.8 10. 10.5 22.8 35.1 35.1 26.3 33.3 21.1 31.6 38.6 35.1 38.6 43.9 49.1 52. 54.4 59.6 Table 5: Passing rate (%) of LLMs on BigCodeBench (BCB)-Lite and BCB-Lite-Pro. dataset example of BCB-Lite-Pro is shown in Appendix G.6. generating self-invoking code."
        },
        {
            "title": "6.1 BigCodeBench-Lite Pro Benchmark",
            "content": "To study self-invoking code generation on wider range of programming problems, we construct BigCodeBench-Lite Pro, small self-invoking code generation benchmark derived from BigCodeBench (Zhuo et al., 2024). We first construct the BigCodeBench-Lite benchmark by selecting 57 problems with solve rate between 50% and 70% from BigCodeBench1. For each examples in BigCodeBench-Lite, we then curate the corresponding self-invoking problem as well as test cases, following the same procedure described in Section 3. After further filtering by human experts, BigCodeBench-Lite Pro contains 57 self-invoking programming problems from different topics."
        },
        {
            "title": "6.2 Results Analysis",
            "content": "We evaluate set of LLMs on BigCodeBench-Lite Pro. Table 5 presents the results (pass@1) of vari1We use reported statistics in https://huggingface.co/ datasets/bigcode/bigcodebench-solve-rate. Figure 7: Statistics of error type across different LLMs on HumanEval Pro and MBPP Pro. We sum up all kinds of errors on the two benchmarks. Exact number is shown in Appendix H. creases, which indicates that CoT prompting helps the model produce more self-contained code snippets and reduces the use of undefined variables. These findings highlight that CoT prompting could help LLMs to generate more accurate and reliable solution on self-invoking code generation task."
        },
        {
            "title": "5.4 Error Analysis",
            "content": "In order to further understand the failure modes across different LLMs, we analyze the errors encountered in code generated by different LLMs for HumanEval Pro and MBPP Pro problems and categorize them by error type. The result is shown in Figure 7. Primarily, AssertionErrors constitute the primary source of errors for all models on selfinvoking code generation task, which suggests that the majority of errors are still due to failing test cases. Secondly, the NameErrors, which is often caused by the undefined variable or function, contribute significantly to the error rate. This suggests that despite the function infomation being provided in the prompt, many functions still fail to generate the correct function header. This may indicate that the LLM has issues with understanding or correctly utilizing the provided information. Finally, we also found that some TypeErrors and ValueErrors accounted for relatively small proportion of errors, which shows that LLM still has some deficiencies in handling variable types and usage when ous Proprietary and Open-source LLMs, highlighting the following observations: (1) Although the base problems we selected has solving rate of between 50% and 70% on BigCodeBench, only small number of models in Table 5 have passing rate of more than 50% on BigCodeBench-Lite Pro. This highlights the difficulty of the self-invoking code generation task. (2) The instruction-tuned models still demonstrate marginal improvements (sometimes decrease) compared to base models, which also reinforces our argument in Section 5.1."
        },
        {
            "title": "7 Conclusion",
            "content": "We present HumanEval Pro, MBPP Pro as well as BigCodeBench-Lite Pro, series of benchmarks to evaluate LLMs on self-invoking code generation task where the LLMs are employed to solve the base problem and use its solution to address more complex problems. Through extensive evaluation of over 20 LLMs, we found that while these models have made significant progress in traditional code generation tasks, they still struggle with more complex self-invoking code generation tasks. Furthermore, we provide extensive comparison and analysis between existing instruct model and base model. HumanEval Pro and MBPP Pro are positioned to serve as valuable benchmarks for code-related evaluations and to inspire future LLM development by shedding light on current model shortcomings and encouraging innovation in training methodologies."
        },
        {
            "title": "Limitations",
            "content": "In this paper, we present HumanEval Pro and MBPP Pro, series of benchmarks evaluate LLMs on self-invoking code generation task. One limitation is that the programming languages of our benchmarks only includes Python due to the intrinsic limitation of original HumanEval and MBPP. Secondly, although the models have shown shortcomings in the self-invoking problem, the diversity of existing self-invoking problems in HumanEval Pro and MBPP Pro is still subject to the constraints of the original problems. Hence, future work should pay more attention to more diverse and multi-lingual self-invoking problem benchmarks."
        },
        {
            "title": "References",
            "content": "01.AI. 2024. Meet yi-coder: small but mighty llm for code. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Antonio Valerio Miceli Barone and Rico Sennrich. 2017. parallel corpus of python functions and documentation strings for automated code documentation and code generation. arXiv preprint arXiv:1707.02275. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. DeepSeek-AI. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434. Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. 2024. Cruxeval: benchmark for code reasoning, understanding and execution. In Forty-first International Conference on Machine Learning. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming arXiv preprint the rise of code intelligence. arXiv:2401.14196. Md Mahim Anjum Haque, Wasi Uddin Ahmad, Ismini Lourentzou, and Chris Brown. 2022. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed Mehrab, Md Mahim Anjum Haque, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal, and Rifat Shahriyar. 2021. Codesc: large code-description parallel dataset. arXiv preprint arXiv:2105.14220. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. 2024. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint. Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. arXiv preprint arXiv:2302.05020. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. Inferfix: End-to-end program repair with llms. arXiv preprint arXiv:2303.07263. Jia Li, Ge Li, Xuanming Zhang, Yunfei Zhao, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, and Yongbin Li. 2024. Evocodebench: An evolving code generation benchmark with domain-specific evaluations. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36. Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. Repobench: Benchmarking repository-level arXiv preprint code auto-completion systems. arXiv:2306.03091. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. arXiv preprint arXiv:2306.08568. Mistral. 2024. Codestral. Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2024. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, Dragomir Radev, Arman Cohan, and Arman Cohan. 2024. L2CEval: Evaluating language-to-code generation capabilities of large language models. Transactions of the Association for Computational Linguistics, 12:13111329. OpenAI. 2024a. Gpt-4o. OpenAI. 2024b. Openai o1 system card. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Aojun Zhou, Junting Pan, and Hongsheng Li. 2024. Reflectioncoder: Learning from reflection sequence for enhanced one-off code generation. Preprint, arXiv:2405.17057. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pages 3169331715. PMLR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning. Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang. 2024. Top leaderboard ranking = top coding proficiency, always? evoeval: Evolving coding benchmarks via llm. arXiv preprint. Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2022. Practical program repair in the era of large pre-trained language models. arXiv preprint arXiv:2210.14179. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2024. Wavecoder: Widespread and versatile enhancement for code large language models by instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51405153. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. survey on language models for code. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658. Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, and Chandan Reddy. 2022. Xlcost: benchmark dataset for cross-lingual code intelligence. arXiv preprint arXiv:2206.08474. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877."
        },
        {
            "title": "C Model Information",
            "content": "D Comparison between HumanEval (Pro), MBPP (Pro) and BigCodeBench-Lite (Pro) Discussion about Self-invoking Problems and Solutions"
        },
        {
            "title": "F Prompts",
            "content": "F.1 Prompts for Benchmark Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Prompts for Evaluation . . . . . Examples of Different Error Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1 Examples of AssertionError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Examples of NameError . . G.3 Examples of ValueError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Examples of IndexError . G.5 Examples of TypeError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.6 An Example of BigCodeBench-Lite Pro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "H Error Statistics across Different Models",
            "content": "14 14 15 16 17 18 18 18 18 19 20 22 23"
        },
        {
            "title": "Model",
            "content": "HumanEval Pro (0-shot) MBPP Pro (0-shot) LLaMA-3.1-8B-base LLaMA-3.1-8B-instruct LLaMA-3.1-70B-base LLaMA-3.1-70B-instruct Qwen-2.5-72B-base Qwen-2.5-72B-instruct QwQ-32B-preview LLaMA-3.3-70B-instruct Mistral-Large-instruct-2411 25.0 45. 40.9 60.4 62.2 68.9 72.0 67.1 75.0 36.5 53.7 57.4 63.8 65.3 68. 67.5 64.6 69.3 Table 6: Results of Other LLMs on HumanEval Pro and MBPP Pro (greedy decoding)."
        },
        {
            "title": "Model",
            "content": "DeepseekCoder-6.7B-base DeepseekCoder-6.7B-instruct Magicoder-S-DS-6.7B WaveCoder-Ultra-6.7B DeepseekCoder-33B-base DeepseekCoder-33B-instruct Qwen2.5-Coder-7B-base Qwen2.5-Coder-7B-instruct OpenCoder-9B-base OpenCoder-9B-instruct Yi-Coder-9B-base Yi-Coder-9B-chat Codestral-22B Qwen2.5-Coder-32B-base Qwen2.5-Coder-32B-instruct QwQ-32B-preview HumanEval Pro pass@1 pass@5 pass@10 MBPP Pro pass@1 pass@5 pass@10 38.0 55.9 55.1 55.7 49.4 59.1 51.8 65. 44.5 59.8 47.9 59.7 59.5 62.4 69.2 70.9 50.9 64.1 62.7 61.4 60.8 68. 62.1 72.5 56.2 68.5 59.0 66.4 66.2 70.3 72.3 77.7 54.7 66.5 65.1 63. 65.2 71.3 66.2 75.0 59.9 70.8 61.9 67.9 67.7 72.2 73.3 79. 51.6 55.2 57.7 58.2 59.1 63.4 61.3 64.2 54.8 58.1 59.6 65.0 63. 67.6 70.6 67.0 60.4 62.6 64.9 64.4 67.2 70.6 69.9 70.5 62.9 63.7 67.7 69. 67.7 75.0 74.7 73.0 63.1 64.9 67.2 66.3 69.3 72.9 72.3 72.6 65.0 65. 69.7 71.2 68.9 76.9 76.0 74.5 Table 7: The results of different models on HumanEval Pro and MBPP Pro . We generate 20 samples for each problems with random sampling strategy where temperature is set to 0.2 and top_p is set to 0.95."
        },
        {
            "title": "B Example in Benchmark Construction",
            "content": "Figure 8: An example of self-invoking problems in HumanEval Pro"
        },
        {
            "title": "API Name",
            "content": "O1-mini GPT-4o GPT-4-Turbo Claude-3.5-sonnet Deepseek-V2.5 o1-mini-2024-09-12 gpt-4o-2024-08-06 gpt-4-turbo-2024-04-09 claude-3-5-sonnet-20241022 deepseek-chat"
        },
        {
            "title": "HuggingFace URL",
            "content": "DeepseekCoder-V2-instruct Qwen2.5-Coder-1.5B-base Qwen2.5-Coder-1.5B-instruct DeepseekCoder-6.7B-base DeepseekCoder-6.7B-instruct Magicoder-S-DS-6.7B WaveCoder-Ultra-6.7B Qwen2.5-Coder-7B-base Qwen2.5-Coder-7B-instruct OpenCoder-8B-base OpenCoder-8B-instruct Yi-Coder-9B-base Yi-Coder-9B-chat Codestral-22B-v0.1 DeepseekCoder-33B-base DeepseekCoder-33B-instruct Qwen2.5-Coder-32B-base Qwen2.5-Coder-32B-instruct LLaMA3-70B-instruct QwQ-32B-Preview LLaMA3.1-8B-base LLaMA3.1-8B-instruct LLaMA3.1-70B-base LLaMA3.1-70B-instruct Qwen2.5-72B-base Qwen2.5-72B-instruct https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B https://huggingface.co/microsoft/wavecoder-ultra-6.7b https://huggingface.co/Qwen/Qwen2.5-Coder-7B https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct https://huggingface.co/infly/OpenCoder-8B-Base https://huggingface.co/infly/OpenCoder-8B-Instruct https://huggingface.co/01-ai/Yi-Coder-9B https://huggingface.co/01-ai/Yi-Coder-9B-Chat https://huggingface.co/mistralai/Codestral-22B-v0.1 https://huggingface.co/deepseek-ai/deepseek-coder-33b-base https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct https://huggingface.co/Qwen/Qwen2.5-Coder-32B https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct https://huggingface.co/Qwen/QwQ-32B-Preview https://huggingface.co/meta-llama/Llama-3.1-8B https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct https://huggingface.co/meta-llama/Llama-3.1-70B https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct https://huggingface.co/Qwen/Qwen2.5-72B https://huggingface.co/Qwen/Qwen2.5-72B-Instruct Table 8: The corresponding API names and HuggingFace model URLs for the evaluated models are listed in Table 2. Comparison between HumanEval (Pro), MBPP (Pro) and BigCodeBench-Lite (Pro) Figure 9: Comparison between HumanEval Family, MBPP Family and BigCodeBench-Lite Family. Discussion about Self-invoking Problems and Solutions We analyze the complexity comparison between base problem and its self-invoking counterpart by examining the line count of their canonical solutions. The line count serves as proxy for the complexity of each problem. By comparing the number of lines required to solve the base problem with those needed for the self-invoking version, we gain insight into how the introduction of self-invocation affects the overall complexity. Generally, self-invoking problems, which often involve recursion or similar constructs, may require more lines of code to handle additional logic and edge cases, thereby increasing the complexity. This comparison helps in understanding the additional computational and conceptual challenges introduced by self-invocation. Figure 10: Complexity comparison between base problem and self-invoking problem. We use the line count of the canonical solution for both the base problem and the self-invoking problem as measure of the problems complexity."
        },
        {
            "title": "F Prompts",
            "content": "F.1 Prompts for Benchmark Construction We set the prompt in our benchmark construction as follows: Prompt for benchmark construction: Ill give you raw programming question and its solution, please generate new problem that requires multiple calls to the raw question to solve, and generate the solution in new_solution. Please return with json format including 3 keys: new_problem,new_solution, test_input, Ill use json.loads() to transform it to dict type. To solve new_problems, new_solution should include the multiple function calls of raw question. So new_problems will be not only related problem but also more complex problem than raw problem. raw problem: {raw problem} raw solution: {raw solution} F.2 Prompts for Evaluation We set the prompt in our evaluation as follows: Prompt of 0-shot: You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions. Write solution of python file to the following problems, the solution of the second problem requires single or multiple calls to the first @@ Instruction {base problem} {self-invoking problem} @@ Response Prompt of 1-shot: You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions. Write solution of python file to the following problems, the solution of the second problem requires single or multiple calls to the first solution @@ Instruction {base problem} {self-invoking problem} {example} @@ Response Prompt with Chain-of-Thought: You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions. Write solution of python file to the following problems, the solution of the second problem requires single or multiple calls to the first @@ Instruction {base problem} {self-invoking problem} @@ Response Lets think step by step."
        },
        {
            "title": "G Examples of Different Error Types",
            "content": "G.1 Examples of AssertionError Listing 1: AssertionError in HumanEval/24-generated by Qwen2.5Coder-7B-base"
        },
        {
            "title": "1 Status : AssertionError",
            "content": "2 3 Traceback : assert largest_common_divisor ([15 , 30 , 45]) == 15 Explanation : expected 15 but got 5. 1 def largest_divisor (n: int ) -> int : 2 4 5 6 7 8 \"\"\" For given number , find the largest number that divides evenly , smaller than >>> largest_divisor (15) 5 \"\"\" for in range (n -1 , 0, -1) : if % == 0: return return 10 11 def largest_common_divisor ( numbers : list ) -> int : \"\"\" For given list of integers , find the largest number that divides each of the numbers in the list evenly , smaller than the smallest number in the list >>> largest_common_divisor ([15 , 30 , 45]) 15 \"\"\" smallest = min ( numbers ) for in range ( smallest -1 , 0, -1) : if all (n % == 0 for in numbers ) : return return 1 12 13 14 15 17 18 19 20 21 22 23 assert largest_common_divisor ([15 , 30 , 45]) == 15 24 assert largest_common_divisor ([15 , 30 , 45]) == 15 25 assert largest_common_divisor ([10 , 20 , 30]) == 10 26 assert largest_common_divisor ([7 , 14 , 21]) == 7 27 assert largest_common_divisor ([16 , 32 , 64]) == 16 28 assert largest_common_divisor ([9 , 18 , 27]) == G.2 Examples of NameError Listing 2: NameError in HumanEval/14-generated by Qwen2.5Coder-7B-instruct"
        },
        {
            "title": "1 Status : NameError",
            "content": "2 3 Traceback : assert all_prefixes_for_list ([ abc , def ]) == [[ , ab , abc ], [ , de , def ]] Explanation : The expected function name is all_prefixes_for_list , but all_prefixes_of_strings is generated though this information is given in prompt . 1 from typing import List 2 def all_prefixes ( string : str ) -> List [ str ]: 3 \"\"\" Return list of all prefixes from shortest to longest of the input string >>> all_prefixes ( abc ) [ , ab , abc ] \"\"\" return [ string [: i] for in range (1 , len ( string ) + 1) ] 4 5 6 7 8 9 10 def all_prefixes_of_strings ( strings : List [ str ]) -> List [ List [ str ]]: \"\"\" Return list of lists where each sublist contains all prefixes of the corresponding string in the input list , sorted from shortest to longest . If the input list is empty , return an empty list . >>> all_prefixes_of_strings ([ abc , def , ghi ]) [[ , ab , abc ], [d , de , def ] , [ , gh , ghi ]] \"\"\" return [ all_prefixes (s) for in strings ] 11 12 14 15 16 17 18 19 assert all_prefixes_for_list ([ abc , def ]) == [[ , ab , abc ], [ , de , def ]] 20 assert all_prefixes_for_list ([ ]) == [[]] 21 assert all_prefixes_for_list ([]) == [] 22 assert all_prefixes_for_list ([ single ]) == [[ , si , sin , sing , singl , single ]] G.3 Examples of ValueError Listing 3: ValueError in HumanEval/153-generated by O1-mini 1 Status : ValueError Traceback : 2 3 >>> assert Strongest_Extensions ({ Slices : [ SErviNGSliCes , Cheese , StuFfed ], my_class : [ AA , Be , CC ]}) == [ Slices . SErviNGSliCes , my_class . AA ] 4 >>> for class_name , extensions in classes_with_extensions 5 Explanation : ValueError : too many values to unpack ( expected 2) 1 2 def Strongest_Extension ( class_name , extensions ): 3 4 5 6 7 9 10 11 12 \"\"\" Determines the strongest extension for given class based on the strength calculation ."
        },
        {
            "title": "The strength of an extension is calculated as the number of",
            "content": "uppercase letters minus the number of lowercase letters in the extension name . If multiple extensions have the same strength , the first one in the list is chosen . Parameters : - class_name ( str ): The name of the class . 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 - extensions ( List [ str ]) : list of extension names . Returns : - str : string in the format \" ClassName . StrongestExtensionName \". Example : >>> Strongest_Extension ( my_class , [ AA , Be , CC ]) my_class . AA \"\"\" if not extensions : return f\"{ class_name }. None \" def calculate_strength ( extension ) : cap = sum (1 for char in extension if char . isupper () ) sm = sum (1 for char in extension if char . islower () ) return cap - sm strongest_extension = extensions [0] max_strength = calculate_strength ( strongest_extension ) for ext in extensions [1:]: strength = calculate_strength ( ext ) if strength > max_strength : strongest_extension = ext max_strength = strength return f\"{ class_name }.{ strongest_extension }\" 39 40 def Strongest_Extensions ( classes_with_extensions ): 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 \"\"\" Determines the strongest extensions for list of classes . For each class , it finds the strongest extension based on the strength calculation . If class has no extensions , it is represented as \" ClassName . None \". Parameters : - classes_with_extensions ( List [ Tuple [ str , List [ str ]]]) :"
        },
        {
            "title": "A list where each element is a tuple containing a class name",
            "content": "and list of its extensions . Returns : - List [ str ]: list of strings in the format \" ClassName . StrongestExtensionName \". Example : >>> Strongest_Extensions ([ ... ... ... ... ]) [ my_class . AA , Slices . SErviNGSliCes , EmptyClass . None ] ( my_class , [ AA , Be , CC ]) , ( Slices , [ SErviNGSliCes , Cheese , StuFfed ]) , ( EmptyClass , []) 62 63 64 65 66 68 69 \"\"\" result = [] for class_name , extensions in classes_with_extensions : if extensions : strongest = Strongest_Extension ( class_name , extensions ) else : strongest = f\"{ class_name }. None \" result . append ( strongest ) return result 70 71 assert Strongest_Extensions ({ Slices : [ SErviNGSliCes , Cheese , StuFfed ], my_class : [ AA , Be , CC ]}) == [ Slices . SErviNGSliCes , my_class . AA ] 72 assert Strongest_Extensions ({ Empty : [] , Single : [ ]}) == [ Empty . None , Single .A ] 73 assert Strongest_Extensions ({ Mixed : [ aA , Bb , Cc ], AllCaps : [ AA , BB , CC ]}) == [ Mixed . aA , AllCaps . AA ] 74 assert Strongest_Extensions ({ NoExtensions : []}) == [ NoExtensions . None ] 75 assert Strongest_Extensions ({}) == [] # Check for empty input G.4 Examples of IndexError Listing 4: IndexError in HumanEval/73-generated by GPT-4o 1 Status : IndexError Traceback : 2 3 >>> assert smallest_change_2d ([[1 ,2 ,3 ,5 ,4 ,7 ,9 ,6] , [1 ,2 ,3 ,4 ,3 ,2 ,2] , [1 ,2 ,3 ,2 ,1]]) == 7 4 >>> column = [ arr [ row ][ col ] for row in range ( rows )] Explanation : IndexError : list index out of range 5 1 2 def smallest_change ( arr ): 3 5 6 7 8 9 11 12 13 \"\"\" Given an array arr of integers , find the minimum number of elements that need to be changed to make the array palindromic . palindromic array is an array that is read the same backwards and forwards . In one change , you can change one element to any other element . \"\"\" = len ( arr ) changes = 0 for in range (n // 2) : if arr [i] != arr [n - - 1]: changes += return changes 14 15 def smallest_change_2d ( arr ): 16 \"\"\" 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 Given 2D array of integers , find the minimum number of elements that need to be changed to make each row and each column of the array palindromic . palindromic row or column is row or column that is read the same backwards and forwards . In one change , you can change one element to any other element . \"\"\" if not arr or not arr [0]: return 0 rows = len ( arr ) cols = len ( arr [0]) # Calculate changes needed for each row row_changes = sum ( smallest_change ( row ) for row in arr ) # Calculate changes needed for each column col_changes = 0 for col in range ( cols ): column = [ arr [ row ][ col ] for row in range ( rows )] col_changes += smallest_change ( column ) return row_changes + col_changes 37 38 assert smallest_change_2d ([[1 ,2 ,3 ,5 ,4 ,7 ,9 ,6] , [1 ,2 ,3 ,4 ,3 ,2 ,2] , [1 ,2 ,3 ,2 ,1]]) == 7 39 assert smallest_change_2d ([[1 ,2 ,3 ,4] , [4 ,3 ,2 ,1] , [1 ,2 ,2 ,1] , [1 ,2 ,3 ,4]]) == 8 40 assert smallest_change_2d ([[1 ,2 ,3 ,2 ,1] , [1 ,2 ,3 ,2 ,1] , [1 ,2 ,3 ,2 ,1] , [1 ,2 ,3 ,2 ,1]]) == 0 G.5 Examples of TypeError Listing 5: TypeError in MBPP/164-generated by DeepseekCoder-V2-instruct 1 Status : TypeError Traceback : 2 3 >>> assert total_and_average_volume ([{ radius : 3, height : 5} , { radius : 4, height : 6}]) == (147.6548547187203 , 73.82742735936014) 4 >>> total_volume += volume_cone (r , h) 5 >>> return ( math . pi * **2 * ) / 3 6 7 Explanation : TypeError : unsupported operand type (s) for ** or pow () : str and int 1 # Write function to find the volume of cone . 2 import math 3 def volume_cone (r ,h): return ( math . pi * **2 * h) / 3 4 5 6 # Given list of cones with their respective radii and heights , write function to calculate the total volume of all cones and the average volume per cone . 7 8 def total_and_average_volume ( cones ) : 9 10 11 12 14 total_volume = 0 for cone in cones : , = cone total_volume += volume_cone (r , h) average_volume = total_volume / len ( cones ) return total_volume , average_volume 15 16 assert total_and_average_volume ([{ radius : 3, height : 5} , { radius : 4 , height : 6}]) == (147.6548547187203 , 73.82742735936014) 17 assert total_and_average_volume ([{ radius : 1, height : 2}]) == (2.0943951023931953 , 2.0943951023931953) G.6 An Example of BigCodeBench-Lite Pro Listing 6: The outputs of GPT-4o on BigCodeBench-Lite Pro (BigCodeBench/355). Traceback ( most recent call last ): File \" evalpro / result / GPT -4 o/ bigcodebench_lite_pro / log / santized_results / case_20 / gen_0 . py \" , line 76 , in < module > new_solution ( points ) File \" evalpro / result / GPT -4 o/ bigcodebench_lite_pro / log / santized_results / case_20 / gen_0 . py \" , line 65 , in new_solution voronoi_plot_2d ( Voronoi ( subset ) , ax = ax , show_vertices = False , line_colors =f C{i} , point_size =2) File \" _qhull . pyx \" , line 2677 , in scipy . spatial . _qhull . Voronoi . __init__ File \" _qhull . pyx \" , line 352 , in scipy . spatial . _qhull . _Qhull . __init__ 1 2 3 4 6 7 8 scipy . spatial . _qhull . QhullError : QH6214 qhull input error : not enough points (1) to construct initial simplex ( need 4) 1 import numpy as np 2 from scipy . spatial import Voronoi , voronoi_plot_2d 3 import matplotlib . pyplot as plt 4 def task_func ( points , seed =0) : \"\"\" Calculate the Voronoi diagram for number of points in 2D and plot it . Note : this function will raise errors when input is invalid , for example wrong type or shape . Jittering is applied prior to plotting . Parameters : - points ( np . ndarray ): numpy ndarray of shape ( n_points , 2) with the coordinates of the points . - seed ( int ): Random seed for reproducibility . Defaults to 0. 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 Returns : tuple ( vor , ax ): tuple containing : - vor ( Voronoi ): Voronoi object representing the Voronoi diagram of the points . - ax ( Axes ): The axes of the plotted Voronoi diagram . \"\"\" if points . shape [1] != 2: raise ValueError (\" Input points should have shape ( n_points , 2) \") np . random . seed ( seed ) jittered_points = points + np . random . normal ( scale =0.01 , size = points . shape ) vor = Voronoi ( jittered_points ) fig , ax = plt . subplots () voronoi_plot_2d ( vor , ax =ax , show_vertices = False , line_colors = blue , point_size =2) plt . show () return vor , ax 32 33 def new_solution ( points ): 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 \"\"\" Given set of points in 2D space , calculate and plot the Voronoi diagram for each subset of points obtained by dividing the original set into three equal parts . Additionally , overlay the Voronoi diagrams of these subsets on single plot to visualize their combined effect . Parameters : - points ( np . ndarray ): numpy ndarray of shape ( n_points , 2) with the coordinates of the points . Returns : None \"\"\" if len ( points ) < 3: raise ValueError (\" Need at least 3 points to divide into three subsets \") # Sort points array for reproducibility points = np . array ( sorted ( points , key = lambda x: (x [0] , [1]) )) # Divide the points into three equal subsets n_points = len ( points ) part_size = n_points // subsets = [ points [i* part_size :( +1) * part_size ] for in range (3) ] # Create plot for overlay 58 59 60 62 63 64 65 66 fig , ax = plt . subplots () # Plot the Voronoi for each subset for , subset in enumerate ( subsets ) : # Note : Here task_func is assumed to return the ax , which will be reused for plotting voronoi_plot_2d ( Voronoi ( subset ) , ax =ax , show_vertices = False , line_colors =f C{i} , point_size =2) plt . title (\" Overlay of Voronoi Diagrams for the Three Subsets \") plt . show () 67 68 # Test case 1: Basic test with 9 points 69 points = np . array ([[0 , 0] , [0 , 1] , [1 , 0] , [1 , 1] , [2 , 2] , [2 , 3] , [3 , 3] , [3 , 4] , [4 , 4]]) 70 new_solution ( points ) 71 72 # Test case 2: Test with exactly 3 points 73 points = np . array ([[0 , 0] , [1 , 1] , [2 , 2]]) new_solution ( points ) 75 76 # Test case 3: Test with random points ensuring at least 9 points 77 points = np . random . rand (9 , 2) 78 new_solution ( points )"
        },
        {
            "title": "H Error Statistics across Different Models",
            "content": "Model O1-mini GPT-4o DeepseekCoder-V2-instruct DeepseekV2.5 Qwen2.5-Coder-32B-instruct Qwen2.5-Coder-7B-instruct Claude-3.5-sonnet LLaMa-3-70B-instruct Codestral-22B OpenCoder-8B-base OpenCoder-8B-instruct Qwen2.5Coder-1.5B-base Qwen2.5Coder-7B-base Qwen2.5Coder-32B-base Yi-Coder-9B Yi-Coder-9B-Chat GPT-4-Turbo DeepseekCoder-33B-base DeepseekCoder-33B-instruct DeepseekCoder-6.7B-base DeepseekCoder-6.7B-instruct Magicoder-S-DS WaveCoder-Ultra-6.7B Dataset"
        },
        {
            "title": "HumanEval Pro\nMBPP Pro\nAll",
            "content": "HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All HumanEval Pro MBPP Pro All"
        },
        {
            "title": "OtherError",
            "content": "Error type 27 89 116 28 82 110 26 79 105 30 82 112 32 89 36 93 129 30 87 117 44 100 144 45 102 147 47 114 161 42 118 56 117 173 45 99 144 39 90 129 48 92 140 47 96 143 33 91 55 108 163 49 101 150 59 128 187 46 107 153 49 107 156 51 113 8 15 23 11 17 28 7 12 19 8 18 26 12 16 28 8 14 11 28 39 10 12 22 13 16 29 43 43 86 15 22 37 25 37 15 21 36 15 17 32 31 37 68 12 19 31 8 18 26 16 23 14 16 30 24 25 49 15 30 45 11 21 32 12 20 32 3 6 2 4 6 1 4 5 2 1 3 2 3 5 3 3 6 1 3 3 2 5 3 3 6 0 2 2 2 3 5 7 3 10 3 1 3 2 5 2 1 3 1 1 2 3 1 4 2 5 7 2 2 4 3 7 4 4 8 6 2 8 2 2 4 1 2 3 1 1 1 3 4 1 3 4 2 1 3 2 3 5 1 1 2 2 2 3 1 4 3 2 5 1 1 2 1 4 5 4 3 7 3 2 5 3 8 3 2 5 1 1 2 2 1 3 2 1 3 4 3 4 2 6 4 2 6 3 4 7 0 4 4 0 5 5 1 7 2 4 6 1 4 5 6 18 24 0 6 6 2 14 16 2 12 5 14 19 5 11 16 9 14 23 5 16 21 1 7 8 3 12 3 11 14 1 5 6 3 8 11 4 10 14 6 14 20 2 17 5 20 25 4 8 12 0 4 4 0 1 1 1 3 4 0 1 1 1 2 1 2 3 2 2 4 4 8 12 1 3 4 2 6 2 4 6 5 21 26 2 6 8 2 4 6 5 5 10 0 4 0 0 0 5 10 15 0 6 6 9 14 23 2 2 4 0 4 2 4 6 All 39 120 159 41 110 151 37 108 145 43 109 50 114 164 56 133 189 45 127 172 65 138 203 67 137 204 100 181 67 159 226 103 196 299 74 146 220 63 122 185 94 150 244 66 133 46 116 162 83 155 238 71 136 207 106 187 293 73 162 235 75 156 74 151 225 Table 9: Error type of Different Models on HumanEval Pro and MBPP Pro."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Yale University"
    ]
}