{
    "paper_title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
    "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Ziyu Guo",
        "Yanwei Li",
        "Yu Qi",
        "Xinyan Chen",
        "Liuhui Wang",
        "Jianhan Jin",
        "Claire Guo",
        "Shen Yan",
        "Bo Zhang",
        "Chaoyou Fu",
        "Peng Gao",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 2 6 9 0 . 2 0 5 2 : r MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency Dongzhi Jiang 1 , Renrui Zhang 1 , Ziyu Guo 2 , Yanwei Li 3 , Yu Qi 4 , Xinyan Chen 1 Liuhui Wang 5 , Jianhan Jin 6 , Claire Guo 7 , Shen Yan 3 , Bo Zhang 8 Chaoyou Fu 6 , Peng Gao 8 , Hongsheng Li"
        },
        {
            "title": "6 NJU 7 CUHK (Shenzhen)\n8 Shanghai AI Laboratory\n{dzjiang,renruizhang}@link.cuhk.edu.hk",
            "content": "Core contribution Project lead Equal contribution Project Page: https://mmecot.github.io/ Abstract Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks systematic assessment and in-depth investigation. In this paper, we introduce MMECoT, specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at fine-grained level. Leveraging curated high-quality data and unique evaluation strategy, we conduct an in-depth analysis of state-of-theart LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as foundation for advancing multimodal reasoning in LMMs. Figure 1: Chain-of-Thought Performance of Leading LMMs in MME-CoT. Our evaluation suite assesses LMMs using three novel metrics that yield six distinct scores. Results reveal that current open-source models, including those with reflection capabilities, still lag behind closed-source models like GPT-4o and Kimi k1.5 in key aspects of chainof-thought reasoning. 1. Introduction The emergence of Chain-of-Thought (CoT) (Wei et al., 2022) in Large Language Models (LLMs) has demonstrated promising advances in reasoning capabilities, exemplified by the recent OpenAI o1 (OpenAI, 2024a) and DeepSeekR1 (Guo et al., 2025a). By engaging in more deliberate, stepwise reasoning process before reaching final answer, this methodology presents an effective solution in tackling complex scenarios. 1 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 2: An Overview of MME-CoT. Our benchmark contains comprehensive CoT evaluation suite with three novel aspects and meticulously curated dataset encompassing six categories. In parallel, the multimodal extensions of LLMs, termed Large Multimodal Models (LMMs), have demonstrated remarkable proficiency across diverse visual domains, e.g., general image recognition (Zhang et al., 2023; Zhu et al., 2023; OpenAI, 2023; Zhang et al., 2024a), temporal video understanding (Li et al., 2023; Chen et al., 2023), and 3D geometry perception (Guo et al., 2024b; Xu et al., 2023; Guo et al., 2023; Jia et al., 2024). However, to what extent and how much CoT reasoning can benefit multimodal challenges still remains an open question. Although some previous efforts (Zhang et al., 2024c; Yu et al., 2023; Zhang et al., 2024d; Guo et al., 2025b) have been made to evaluate the CoT capabilities of LMMs, their examination is insufficiently systematic and thorough, limiting our understanding of multimodal reasoning and its further development. To bridge this gap, we propose MME-CoT, comprehensive and specialized benchmark for evaluating the CoT reasoning skills within LMMs (Figure 2). Our benchmark spans six fundamental domains: math, science, OCR, logic, space-time, and general scenes, encompassing broad range of CoT-relevant scenarios. Unlike the simplistic metrics used in previous studies, MME-CoT introduces rigorous evaluation framework that delves into the fine-grained CoT process of LMMs, assessing reasoning quality, robustness, and efficiency. Specifically, we address three critical research questions as follows: 2 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency 1. Is each intermediate CoT step logically valid and faithful without hallucination? The outcome-oriented evaluation paradigm, where most current benchmark adapts, omits the scenario where the model reaches the correct answer through flawed logic or random guess. This causes an illusion of inflated reasoning capabilities in the model. To delve into the reasoning process, we introduce two interpretable metrics to evaluate the Quality of CoT: 1) Recall, which quantifies reasoning informativeness by measuring the proportion of ground-truth solution steps appearing in the response; 2) Precision, which measures faithfulness by evaluating how many of the generated steps are accurate. 2. Does CoT interfere with perception tasks, and to what extent does it enhance reasoning tasks? While existing studies primarily focus on the performance improvements CoT brings to reasoning tasks, they often overlook whether CoT could inadvertently disrupt the models ability to solve perception tasks that require minimal reasoning. To this end, we present the first investigation into the Robustness of CoT in LMMs. Our benchmark incorporates two task categories (perception and reasoning), and employs two distinct prompting strategies (direct answer and step-by-step) to assess two metrics: 1) Stability, which examines whether CoT negatively impacts the models performance on direct perception tasks; 2) Efficacy, which measures the extent to which CoT enhances the models performance on complex reasoning tasks. 3. How can we assess the efficiency of CoT in long reasoning process? Recent o1-like models have distinguished themselves by employing excessively long CoT and reflection steps. This raises critical trade-off question: does this approach strike an optimal balance between accuracy and computational cost? To investigate this, we present the first study on the Efficiency of CoT in LMMs. We evaluate efficiency using two key metrics: 1) Relevance Rate, which assesses the proportion of generated content that contributes to answering the question. 2) Reflection Quality, which analyzes whether each reflection step drives the question towards correctness. when applying CoT on the perception tasks. This significantly impedes the applicability of models using CoT reasoning as default practice. Moreover, for CoT efficiency, we notice that not all steps within the long CoT are related to answering the question, and the model could be distracted by the image content, especially when handling general scenes, space-time, and OCR tasks. Around 30% to 40% of reflection steps fail to help answer questions, pointing out critical issues of current models reflection capabilities. The contributions of this paper are summarized as follows: The MME-CoT benchmark is curated, covering comprehensive scope of six multimodal reasoning scenarios. The data collection and annotation process undergoes rigorous human verification, aiming to provide the community with high-quality evaluation dataset for multimodal reasoning. We identify critical issues in existing benchmarks, and introduce thorough evaluation suite specialized for multimodal CoT reasoning, which meticulously examines the reasoning quality, robustness, and efficiency. We conduct extensive experiments and analysis on state-of-the-art LMMs with reasoning capabilities. We summarize our observations and insights, hoping to inspire future advancements of reasoning performance. 2. Dataset Curation 2.1. Data Composition and Categorization. MME-CoT composes 6 major domains with 17 subcategories, as visualized in Fig. 3. Different from textual reasoning questions, the extra visual input significantly enriches the scope of the visual reasoning questions. With the image input, the model needs to frequently visit the image for relevant information according to current reasoning progress. Describing the image area of interest becomes crucial part of the CoT process. Thus, in addition to complex problems demanding rigorous logic, commonsense scenarios also pose challenging reasoning problem, as shown in the general scenes in Fig. 2. To maintain focus on the reasoning process, we exclude questions that require complex domain-specific theorems or specialized knowledge. Through our systematic evaluation and analysis, we discover that the fine-grained reflection capability greatly enhances the CoT quality, e.g., QVQ achieves F1 Score of 62.0%, largely surpassing Qwen2-VL-72B by 6.8%. Kimi k1.5 beats GPT-4o and achieves the best quality. As for the robustness, we surprisingly find that most models are interfered with by CoT on the perception tasks, implying harmful overthinking behavior. The worst case happens in InternVL2.5-8B, where we witness 6.8% degradation In addition, to evaluate CoT robustness detailed in Section 3.2, we incorporate variety of perception tasks along with the reasoning tasks in the benchmark. The reasoning tasks contain questions that demand multi-step logical inference, while the perception tasks consist of questions that primarily test visual recognition abilities or require very minimal reasoning. Existing benchmarks often conflate these two types of tasks, with perception and reasoning questions frequently appearing within the same categories. MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency"
        },
        {
            "title": "Total questions",
            "content": "- Reasoning questions Multiple-choice questions Free-form questions - Perception questions Multiple-choice questions Free-form questions"
        },
        {
            "title": "Total key step annotation",
            "content": "- Total inference conclusions - Average inference conclusions - Total image captions - Average image captions Reference image caption item Average reference caption"
        },
        {
            "title": "Number",
            "content": "1,130 837 (74.1%) 431 406 293 (25.9%) 275 18 3,865 2,667 3.2 1,198 1.4 1,579 1.9 2,380 808 271 477 15 41.2 1.2 Figure 3: Category and Subcategory Distribution of MME-CoT. Table 1: Key Statistics of MME-COT. To address this, we implement two-stage classification approach combining both model-based and human assessment. Initially, we leverage LMMs to guide the preliminary categorization by comparing their performance with and without CoT prompting. We employ GPT-4o (OpenAI, 2024b) and Qwen2-VL-7B (Wang et al., 2024b) to answer questions using both direct and CoT approaches. Superior performance with CoT indicates reasoning-dominant subcategory, while comparable or inferior CoT performance suggests either perception-focused content or insufficient model reasoning capabilities. The results are shown in Appendix B.2. Subsequently, expert annotators review individual questions to finalize their classification. In total, MME-CoT contains 1,130 questions with 3,865 key step annotation. The detailed statistics of data compositions are shown in Table 1. Please refer to Appendix for more details about the distribution of data sources. 2.2. Data Annotation and Review To facilitate CoT evaluation, we provide key steps annotation and reference image captions for all the reasoning questions. Key steps are defined as those that must be done to reach the correct answer. For efficient annotation, we first employ GPT-4o to generate the answer rationale and image captions. For the rationale, we provide both questions and ground truth answers to the model, which yields more accurate rationales compared to question-only prompting. Annotators are then asked to provide key intermediate steps with the help of GPT-4os responses. For cases where GPT-4o fails to generate reasonable rationales, annotators develop solutions independently. The intermediate steps fall into two categories: inference conclusion and image caption. Note that the final answer is also included as concluding inference. All the steps are reduced to the simplest form, retaining only core conclusions and relevant visual element descriptions. Notably, for problems with multiple solutions, annotators are required to provide all possible methods. For reference captions, we also ask annotators to verify and correct the details. 3. CoT Evaluation Strategy Existing benchmarks only focus on evaluating the final answer of the questions, leaving the whole chain of thoughts unvisited. We argue that the CoT process reflects reasoning capability from multiple aspects, serving as crucial medium to understand LMMs thinking pattern and deficiency. Here, we present the first holistic CoT evaluation suite to facilitate comprehensive understanding of the LMMs reasoning abilities. We detail the evaluation of correctness in Section 3.1, stability and efficacy in Section 3.2, and reflection quality in Section 3.3. 3.1. CoT Quality Evaluation Existing methods typically rely on state-of-the-art LLMs or LMMs to directly evaluate Chain-of-Thought reasoning based on self-defined criteria, using only the final answer as reference (Hao et al., 2024; Zhang et al., 2024c). We identify two primary issues with the strategy. First, the scoring process only attends to the logical validity of each step, omitting the helpfulness evaluation. Second, there is large number of complex visual reasoning questions that even the scoring model cannot solve. It is unreasonable for the scoring model to judge another models reasoning process on these questions without knowing the ground truth solution process. Therefore, building upon our annotated key steps and reference image captions, we leverage two interpretable metrics to evaluate the CoT correctness: recall and precision (Figure 5). The two metrics respectively attend to the two aspects of the CoT correctness: informativeness and accuracy. We denote the key steps as = I, where = {c1, ..., cM } includes key inference conclusions 4 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 4: Illustration of Step Partition. We instruct GPT-4o to divide each step into three categories: image caption, background information, or logical inference. The step partition result is later used to perform step-wise reasoning evaluation. We focus on evaluating the image caption and logical inference steps, which are the keys to visual reasoning. and = {i1, ..., iN } includes key image captions. Recall. We prompt GPT-4o (OpenAI, 2024) to determine whether each key step occurs in the models CoT response. Then we calculate the ratio of the matched key steps Smatched against all the annotated key steps: RecallC = , RecallI = (cid:12) (cid:12)S matched k (cid:12) (cid:12) , k0 = arg max (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Ck0 (cid:12) matched Ck0 (cid:12) (cid:12)S k0 (cid:12) matched k0 (cid:12) (cid:12) (cid:12) . Recall = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)I k0 (cid:12) matched k0 , (1) (2) (3) where denotes the kth method of the problem. Intuitively, recall measures how many informative steps are reached by the model. From another perspective, this metric also strictly examines the processs rigorousness toward reaching the correct answer, eliminating the probability of random guessing. For questions with multiple methods, we compute the recall on the most matched method. image caption, and background information. The logical inference step draws an intermediate or final conclusion based on the previously obtained information. The image caption step depicts elements of interest in the image. The background information step states external knowledge or question information. Visual reasoning can be primarily characterized as an interleaved sequence of image captions and logical inferences, so we focus on measuring precision for these two key step types. We assess the correctness of logical inference steps (CP ) and image caption steps (I ) using two criteria: 1. If the step exists in S, the step is correct. 2. If the step is logically correct or faithfully depicts the image based on the annotations, the step is also correct. Thus, we compute precision as: PrecisionC = (cid:12) (cid:12)CP correct CP (cid:12) (cid:12) , PrecisionI = Precision = (cid:12) (cid:12)CP correct CP correct (cid:12) (cid:12)I correct (cid:12) (cid:12) , (cid:12) (cid:12) (4) (5) Precision. We first instruct GPT-4o to partition the prediction into sequence of steps P, as shown in Fig. 4. Each step is categorized into one of three classes: logical inference, Intuitively, precision evaluates the faithfulness of each step, considering all the possible reasoning output. Finally, we calculate the F1 score as the metric of CoT quality. 5 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 5: Illustration of CoT Quality Evaluation. For recall, we prompt GPT-4o to match each key step annotation in the prediction. For precision, GPT-4o is instructed to split the prediction into steps and determine the correctness of all the image caption and logical inference steps. 3.2. CoT Robustness Evaluation Here, we perform the first investigation on the robustness of CoT in visual reasoning. The effectiveness of CoT on reasoning tasks has been verified in many works (Wei et al., 2022; OpenAI, 2024a). However, how CoT impacts visual perception tasks or tasks requiring minimal reasoning still remains unknown. Despite the neglect, this question bears great importance. In real-world applications, what task is given is unknown in advance. Whether the model should perform CoT to solve the task is difficult to determine. In fact, there exists no golden standard to determine which question can benefit from CoT so far (Sprague et al., 2024). Instead of trying to define this criterion, we examine the performance of CoT across all kinds of tasks, both reasoning and perception. We argue that an ideal CoT process should assist in reasoning and not interfere with pure perception. Therefore, it can be applied for any tasks. Based on this, we propose to evaluate two metrics of CoT: stability and efficacy (Figure 6). We leverage two kinds of prompts: the direct prompt (DIR) and the CoT prompt (COT). The direct prompt asks the model to directly provide the final answer, while the CoT prompt instructs the model to perform step-by-step reasoning and finally give the answer. To directly compare the performance difference caused by these two prompts, we conduct the direct evaluation, which only judges the correctness of the final answer, i.e., accuracy. We instruct GPT-4o mini (OpenAI, 2024) to extract the final answer, and then compare it with the ground truth answer, following the two-step procedure introduced in (Zhang et al., 2024c). Stability. We define the performance difference of the two prompts on the perception tasks as the stability score: Stability = AccP COT AccP DIR. (6) Intuitively, applying the CoT prompt to perception tasks should not degrade performance compared with the direct prompt. Thus, model with stable CoT should be not less than 0. Otherwise, the models thinking process demonstrates inconsistency and harm. The overthinking process pushes over the original correct judgment. 6 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 6: Illustration of CoT Robustness Evaluation. We compare the performance of applying CoT prompt and direct prompt on two types of tasks: perception and reasoning. The stability score measures whether CoT interferes with perception, while the efficacy score assesses the performance gain of CoT on reasoning tasks. Figure 7: Illustration of CoT Efficiency Evaluation. For relevance rate, we partition the prediction into steps and determine if it is relevant by GPT-4o. For reflection quality, we prompt GPT-4o to identify the reflection steps by common indicators and judge the validity of the reflection. The definitions of relevance and validity are included. Efficacy. Similarly, the performance difference of the two prompts on the reasoning tasks is defined as the score: Efficacy = AccR COT AccR DIR. (7) Intuitively, CoT facilitates stepwise thinking and therefore benefits answering reasoning tasks. The difference reflects how much CoT can enhance reasoning. 3.3. CoT Efficiency Evaluation Models like o1 generate extremely long thinking processes with reflection and verification of current steps and outcomes. We perform the first exhaustive analysis of the CoT efficiency of visual reasoning with two carefully designed metrics (Figure 7): in the image for answering the question, but it still generates detailed description of other objects. This irrelevant information provides no helpful information to work out the answer. In the meantime, this extra content slows down the generation speed. Similar to the calculation of precision, we employ the same method to partition the prediction into steps. Then, we instruct GPT-4o to determine all the relevant steps Prelevant. The step is considered relevant only when the majority of its content works towards solving the question. We first compute the raw relevance rate and then apply scaling factor to amplify the differences between models. Let rx denote the raw relevance rate: (cid:12) (cid:12) (cid:12) (cid:12)I (cid:12) (cid:12)CP (cid:12) (cid:12) rC = relevant CP , rI = = Prelevant . relevant , (8) (9) Relevance Rate. Although the long reasoning content allows for deeper thinking, it may also introduce large amount of irrelevant information. As shown in the bottom left of Fig. 7, the model has identified the critical element Then, the final relevance rate Relevance Ratex is defined as: Relevance Ratex = rx α 1 α , C, I, (10) 7 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Table 2: Evaluation Results of Three Aspects of CoT in MME-COT. We mark the highest score of each metric in red . denotes unreliable results due to the refusal to answer directly. CoT Quality CoT Robustness CoT Efficiency Precision Image Conclusion Recall Image Conclusion Avg. Score Stability CoT Perception Direct Perception Efficacy CoT Reasoning Direct Reasoning Avg. Score Relevance Rate Image Conclusion Reflection Quality"
        },
        {
            "title": "Model",
            "content": "Mulberry LLaVA-OV-7B LLaVA-CoT LLaVA-OV-72B MiniCPM-V-2.6 InternVL2.5-8B Qwen2-VL-7B F1 Score 27.4 30.9 34.9 36. 39.8 41.1 42.1 InternVL2.5-8B-MPO 43.0 InternVL2.5-78B-MPO 52.7 Qwen2-VL-72B Virgo-72B QVQ-72B 56.2 60.8 62.0 59.1 50. 53.9 57.3 57.3 60.0 61.6 60.4 73. 77.3 79.5 80.2 74.1 47.2 75. 43.4 63.4 52.4 61.0 60.8 68.4 67. 71.6 73.9 GPT-4o Kimi k1.5 64.0 64. 85.4 92.0 73.3 78.1 53.8 43. 46.2 50.6 45.4 50.8 49.3 49.9 63. 70.3 72.7 77.5 81.4 89.8 17. 22.2 25.8 26.6 30.5 31.3 32. 33.4 41.1 44.2 49.2 50.5 26.5 24. 35.8 29.5 47.5 40.4 46.6 44.9 53. 57.1 60.5 60.1 51.2 49.3 64. 62.9 17.1 23.2 24.4 27.4 26. 30.6 30.5 31.8 39.1 42.2 47.7 48. 49.9 47.9 Open-source LMMs 3.5* -3.4 0.4* -0.2 -3.5 -3.0 -4.0 0.6 0.2 -2. -2.3* -1.8* 4.4* -3.8 1.4* 0. -4.8 -6.8 -3.1 0.3 -2.0 -6.5 -1.7* -3.1* 42.3 46.1 51.5 61.1 59. 57.3 60.1 62.5 68.3 68.9 74.1 69. Closed-source LMMs 2.1 1.4* -1.0 2.9* 71. 65.7 37.9 49.8 50.2 60.8 64. 64.2 63.1 62.1 70.3 75.4 75.8 72. 72.0 62.9 2.6* -3.0 -0.6* -0. -2.2 0.9 -4.8 0.9 2.4 2.4 -2.9* -0.4* 5.1 0.0* 18.6 16.4 24. 27.6 26.2 30.3 26.0 28.8 38.0 38. 41.8 41.0 40.6 40.0 16.0 19. 25.0 28.2 28.3 29.4 30.8 27.9 35. 36.2 44.7 41.3 89.5 91.5 94. 95.4 92.8 98.4 94.9 94.7 95.3 96. 75.3 67.9 79.0 83.0 88.1 90. 85.7 96.8 89.8 89.3 90.6 92.9 90. 83.7 50.8 72.1 69.2 83.7 74. 93.0 80.3 84.0 82.9 86.0 79.8 63. 35.5 40.0 96.0 82.2 92.0 92. 82.4 82.2 95.4 93.6 96.2 98. 97.6 98.9 98.8 96.4 98.2 98.7 95. 95.1 99.1 97.2 100 100 100 100 100 100 100 100 60.6 61.7 100 72.2 where = corresponds to the overall relevance rate, and we take α as 0.8. Reflection Quality. The superior reasoning ability could be largely attributed to the reflection and verification process. However, our analysis reveals that not all reflective steps contribute meaningfully to finding correct answers. We identify distinct failure patterns in the reflection process. Some reflective steps mislead the reasoning by introducing new errors or incorrect assumptions, while others are redundant, simply echoing previous conclusions without contributing new insights. To account for failure reflection scenarios, we propose to measure the validity of the reflection. We define valid reflection as either correctly pointing out the previous mistakes or verifying the previous conclusion with new insight. Otherwise, the reflection only slows down the reasoning. To instruct GPT-4o to determine all the valid reflection steps R, we list set of common indicators of the start of the reflection, such as Wait and Alternatively, and illustrate the definition of valid reflection. For all the valid reflection steps Rvalid, the reflection quality is computed as: Reflection Quality = Rvalid . (11) 4. Experiments In this section, we conduct systematic evaluation of stateof-the-art models on MME-COT. We first detail the experiment setup in Section 4.1. Then in Section 4.2, we report the quantitative results and provide valuable insights derived from our analysis. 4.1. Experiment Setup Evaluation Models. We select top-performing LMMs for comprehensive CoT evaluation. We test earlier models such as LLaVA-OneVision (7B, 72B) (Li et al., 2024a), Qwen2-VL (7B, 72B) (Qwen Team, 2024), MiniCPM-V2.6 (Yao et al., 2024b), and InternVL2.5 (8B) (Chen et al., 2024b), which are not trained for the reasoning capability. We also include GPT-4o (OpenAI, 2024b) as strong baseline model. Besides, we test recent models targeting reasoning, including LLaVA-CoT (11B) (Xu et al., 2024), Mulberry (8B) (Yao et al., 2024a), InternVL2.5-MPO (8B, 78B) (Wang et al., 2024c). Finally, we evaluate LMMs with reflection capabilities, including both closed-source models like Kimi k1.5 (Team et al., 2025) and open-source implementations such as QVQ-72B (Team, 2024) and Virgo72B (Du et al., 2025). Note that we sample 150 questions from MME-COT to evaluate Kimi k1.5, due to the access limitations. The sample comprises 115 reasoning and 35 perception questions. Implementation Details. We define the CoT prompt as: Please generate step-by-step answer, include all your intermediate reasoning process, and provide the final answer at the end. and the direct prompt as: Please directly provide the final answer without any other output. We only calculate recall of image observation and logical inference on questions where key inference conclusion or image observation exists. We employ GPT-4o mini for the direct evaluation and GPT-4o for all other criteria. For hyperparameters, we follow the settings in VLMEvalKit (Duan et al., 2024). 8 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Table 3: Evaluation Results of Three Aspects of CoT in Each Category in MME-COT. Best performance is marked in red . denotes unreliable results due to the refusal to answer directly."
        },
        {
            "title": "General Scenes",
            "content": "Space-Time"
        },
        {
            "title": "Mulberry",
            "content": "LLaVA-OV-7B LLaVA-CoT LLaVA-OV-72B MiniCPM-V-2.6 InternVL2.5-8B Qwen2-VL-7B InternVL2.5-8B-MPO 33.9 41.8 38.2 41.8 47.1 43.8 46. 47.2 InternVL2.5-78B-MPO 47.9 Qwen2-VL-72B Virgo-72B QVQ-72B GPT4o 51.9 60.5 62.6 62.3 4.3 -6.2 -2. -2.3 3.2 -6.4 -3.4 2.9 0. -2.9 0.5 -1.5 -1.7 76.0 81.8 89. 98.9 87.7 87.1 79.3 94.3 89. 88.9 91.0 86.9 96.2 18.2 23.8 33. 29.0 49.3 50.7 51.7 51.8 55. 59.7 59.6 58.2 66.3 1.0 -6.7 2. -0.9 -14.4 -8.9 -11.8 -0.2 -2. -5.3 -3.8 -2.5 5.5 38.4 24.8 68. 43.6 71.1 99.1 73.0 74.6 91. 86.7 86.0 57.7 64.7 26.7 44.1 37. 40.8 63.7 44.7 65.9 59.6 72. 77.6 79.9 76.9 83.3 6.6 -0.2 0. -1.7 -4.9 -4.1 0.9 -1.0 2. 2.5 -1.0 -1.4 -1.0 26.4 42.7 77. 84.2 62.0 98.9 86.2 81.5 73. 81.7 82.1 52.6 82.1 29.1 27.4 35. 38.4 32.9 40.9 34.0 37.4 50. 49.6 59.6 61.4 60.8 87.9 97.3 91. 98.7 95.2 98.0 97.9 93.4 95. 97.8 90.3 92.7 98.8 29.1 28.5 36. 35.4 29.5 40.8 34.6 39.0 48. 53.6 55.5 57.7 64.1 91.9 95.1 93. 95.7 90.4 97.1 95.0 95.6 97. 99.0 98.7 95.9 97.4 13.9 12.2 14. 18.4 16.9 19.5 18.4 20.9 24. 40.0 39.6 44.6 27.2 99.1 98.0 97. 82.3 93.7 96.8 76.7 79.9 87. 88.0 88.2 94.9 92.0 4.2. Quantitative Results We conduct extensive experiments on various LMMs with our proposed CoT evaluation suite. The main results are presented in Table 2 and Table 3. We begin by analyzing the overall performance and then highlight key findings. Overall Results. In Table 2, we present the overall performance of three CoT evaluation perspectives with specific metrics. To provide comprehensive understanding, we report precision, recall, and relevance for both logical inference and image caption steps. For robustness, we provide the direct evaluation result on the perception and reasoning tasks, with either CoT or direct prompt. We employ the average value of the stability and efficacy as the final robustness metric. Notably, we define the reflection quality as 100 on models incapable of reflection. For CoT quality, Kimi k1.5 achieves the highest F1 score. Open-source models with larger sizes consistently demonstrate better performance, highlighting the scalability of LMMs. Notably, Qwen2-VL-72B outperforms all other open-source models without reflection, even surpassing InternVL2.5-78B-MPO, which is specifically enhanced for reasoning. Analysis reveals that GPT-4o achieves superior performance across all recall metrics, while Kimi k1.5 demonstrates the highest scores in precision evaluations. For CoT robustness, Mulberry obtains the highest average score. However, when we look into its output, we find it still generates lengthy rationales despite receiving direct prompt. Even worse, the direct prompt seems to be an out-of-distribution input for Mulberry, frequently leading to nonsensical outputs. Further analysis of other models predictions reveals that LLaVA-CoT, Virgo, QVQ, and Kimi k1.5 similarly neglect the direct prompt, instead generating extended rationales before answering. Consequently, their robustness scores may be misleading. Once again, GPT-4o achieves the highest robustness score. Among open-source models, only InternVL2.5-MPO, in both its 8B and 78B variants, attains positive robustness score. Finally, for CoT efficiency, InternVL2.5-8B obtains the maximum relevance of 98.4%, suggesting its consistent focus on questions. Now, we summarize our key observations as follows: Models with reflection largely benefit CoT quality. As shown in Table 2, the F1 scores of the two models with reflection capability most closely approach GPT-4o. After specifically fine-tuning for the reasoning capabilities from Qwen2-VL-72B, QVQ surpasses its base model by 5.8%. Notably, although QVQ generates longer CoT sequences than Qwen2-VL-72B, QVQs precision still exceeds Qwen2VL-72B by 2.9%, indicating superior accuracy in each reasoning step. Kimi k1.5 also surpasses the previous state-ofthe-art model GPT-4o, obtaining the highest CoT quality. Long CoT does not necessarily cover key steps. Despite high precision in long CoT models, the informativeness of each step is not guaranteed. We observe that the recall trend among GPT-4o, QVQ, and Virgo does not align with their CoT Rea. performance (i.e., their final answer accuracy on the reasoning tasks under the CoT prompt). Specifically, while both Virgo and QVQ outperform GPT-4o in direct evaluation, they lag behind in recall. This suggests that long CoT models sometimes reach correct answers while skipping intermediate steps, which contradicts the principle of stepwise reasoning and warrants further investigation. CoT impairs perception task performance in most models. Surprisingly, most models exhibit negative stability scores, indicating that CoT interferes with perception tasks. The most significant degradation occurs in InternVL2.58B, where performance drops by 6.8%. This reveals inconsistency and potential overthinking in current models, 9 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency presenting significant barrier to adopting CoT as the default answering strategy. Among models that provide direct answers, only LLaVA-OV-72B and InternVL2.5-8B-MPO achieve modest positive score of 0.3%. More parameters enable models to grasp reasoning better. We find that models with larger parameter counts tend to achieve higher efficacy scores. This pattern is evident across LLaVA-OV, InternVL2.5-MPO, and Qwen2-VL. For instance, while Qwen2-VL-7B shows 4.8% decrease in performance when applying CoT to reasoning tasks, its larger counterpart, Qwen2-VL-72B, demonstrates 2.4% improvement. This discrepancy suggests that models with more parameters could better grasp the reasoning ability under the same training paradigm. Long CoT models may be more susceptible to distraction. Long CoT models may demonstrate lower relevance scores compared to other models. They frequently generate content unrelated to solving the given question, corresponding to their relatively low recall scores compared to direct evaluation, like QVQ. Although few models with short CoT, like Mulberry and LLaVA-OV-7B, also obtain low relevance rate, we find that it is because these models may keep repeating words when dealing with specific type of questions, resulting in irrelevant judgment. The fine-grained metric reveals that models tend to lose focus when describing images, often producing exhaustive captions regardless of their relevance to the question. From Table 3, we find that this phenomenon prevails in general scenes, space-time, and OCR tasks. This behavior can significantly slow inference by generating substantial irrelevant content. Teaching long CoT models to focus on question-critical elements represents promising direction for future research. Reflection often fails to help. While reflection is key feature of long CoT models for answer verification, both QVQ and Virgo achieve reflection quality scores of only about 60%, indicating that approximately 40% of reflection attempts fail to contribute meaningfully to answer accuracy. Even for the closed-source model Kimi k1.5, over 25% reflection steps are also invalid. This substantial failure rate compromises efficiency by potentially introducing unnecessary or distracting steps before reaching correct solutions. Future research should explore methods to reduce these ineffective reflections to improve both efficiency and quality. 4.3. Error Analysis In this section, we analyze error patterns in the LMM reflection process. An effective reflection should either correct previous mistakes or validate correct conclusions through new insights. We examined 200 model predictions from QVQ and identified four distinct error types that hinder proFigure 8: Distribution of Reflection Error Types. We identify four types of error: ineffective reflection, incompleteness, repetition, and interference. ductive reflection. These patterns are illustrated in Fig. 10 and their distribution is shown in Fig. 8. The four major error types are: Ineffective Reflection. The model arrives at an incorrect conclusion and, upon reflecting, continues to make incorrect adjustments. This is the most common error type and is also witnessed most frequently. Incompleteness. The model proposes new analytical approaches but does not execute them, only stopping at the initial thought. The reflection slows down the inference process without bringing any gain. Repetition. The model restates previous content or methods without introducing new insights, leading to inefficient reasoning. Interference. The model initially reaches correct conclusion but, through reflection, introduces errors. Understanding and mitigating these errors is crucial for improving the reliability of LMM reflection mechanisms. The analysis provides the opportunity to focus on solving specific error types to enhance the overall reflection quality. 5. Conclusion In this paper, we have introduced MME-CoT, comprehensive benchmark designed to evaluate Chain-of-Thought reasoning in Large Multimodal Models. Our dataset comprises six categories to cover most scenarios of visual reasoning tasks. To gain thorough understanding of the reasoning process, we design novel CoT evaluation suite with three metrics. Our systematic evaluation obtains useful insights into the issues within the current state-of-the-art Large Multimodal Models. We identify critical flaws in all the tested 10 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency open-source models. As the field continues to evolve, MMECoT stands as valuable tool for measuring progress and identifying areas for improvement in the development of more sophisticated multimodal AI systems. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Computer Vision and Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al. Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. Chen, Q., Qin, L., Zhang, J., Chen, Z., Xu, X., and Che, W. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. In Proc. of ACL, 2024a. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024c. Du, Y., Liu, Z., Li, Y., Zhao, W. X., Huo, Y., Wang, B., Chen, W., Liu, Z., Wang, Z., and Wen, J.-R. Virgo: preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 11198 11201, 2024. Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. ICML 2024, 2024. Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023. Guo, Z., Zhang, R., Chen, H., Gao, J., Gao, P., Li, H., and Heng, P.-A. Sciverse. https://sciverse-cuhk.github.io, 2024a. URL https://sciverse-cuhk.github. io/. Guo, Z., Zhang, R., Zhu, X., Tong, C., Gao, P., Li, C., and Heng, P.-A. Sam2point: Segment any 3d as videos in zero-shot and promptable manners. arXiv preprint arXiv:2408.16768, 2024b. Guo, Z., Zhang, R., Tong, C., Zhao, Z., Gao, P., Li, H., and Heng, P.-A. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025b. Hao, S., Gu, Y., Luo, H., Liu, T., Shao, X., Wang, X., Xie, S., Ma, H., Samavedhi, A., Gao, Q., et al. Llm reasoners: New evaluation, library, and analysis of stepby-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Jia, Y., Liu, J., Chen, S., Gu, C., Wang, Z., Luo, L., Lee, L., Wang, P., Wang, Z., Zhang, R., et al. Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. arXiv preprint arXiv:2411.18623, 2024. Jiang, D., Zhang, R., Guo, Z., Wu, Y., Lei, J., Qiu, P., Lu, P., Chen, Z., Song, G., Gao, P., et al. Mmsearch: Benchmarking the potential of large models as multimodal search engines. arXiv preprint arXiv:2409.12959, 2024. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Golovneva, O., Chen, M., Poff, S., Corredor, M., Zettlemoyer, L., Fazel-Zarandi, M., and Celikyilmaz, A. Roscoe: suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919, 2022. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b. 11 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International Conference on Machine Learning, pp. 1288812900. PMLR, 2022. Sprague, Z., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., and Durrett, G. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. ECCV 2024, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, 2023. Lu, P., Bansal, H., Xia, T., Liu, J., yue Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. ArXiv, abs/2310.02255, 2023. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Team, Q. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/blog/ qvq-72b-preview/. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Wang, F., Fu, X., Huang, J. Y., Li, Z., Liu, Q., Liu, X., Ma, M. D., Xu, N., Zhou, W., Zhang, K., et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. OpenAI. URL gpt-4v-system-card. GPT-4V(ision) 2023. https://openai.com/research/ system card, OpenAI. Gpt-4o mini: inhttps://openai.com/index/ telligence. gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. advancing cost-efficient Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wang, W., Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Zhu, J., Zhu, X., Lu, L., Qiao, Y., and Dai, J. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024c. Wang, W., Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Zhu, J., Zhu, X., Lu, L., Qiao, Y., et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024d. Wang, Z., Xia, M., He, L., Chen, H., Liu, Y., Zhu, R., Liang, K., Wu, X., Liu, H., Malladi, S., Chevalier, A., Arora, S., and Chen, D. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024e. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xu, G., Jin, P., Li, H., Song, Y., Sun, L., and Yuan, L. Llava-cot: Let vision language models reason step-bystep, 2024. URL https://arxiv.org/abs/2411. 10440. OpenAI. Introducing openai o1, 2024., 2024a. URL https://openai.com/o1/. OpenAI. Hello gpt-4o. https://openai.com/ index/hello-gpt-4o/, 2024b. Peng, T., Li, M., Zhou, H., Xia, R., Zhang, R., Bai, L., Mao, S., Wang, B., He, C., Zhou, A., et al. Chimera: Improving generalist model with domain-specific experts. arXiv preprint arXiv:2412.05983, 2024. Prasad, A., Saha, S., Zhou, X., and Bansal, M. Receval: Evaluating reasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703, 2023. Qwen Team. Qwen2-vl. 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar. org/CorpusID:231591445. 12 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin, D. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. fine-tuning of large language models with zero-initialized In The Twelfth International Conference on attention. Learning Representations, 2024b. URL https:// openreview.net/forum?id=d4UiXAHN2W. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Gao, P., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024c. Zhang, R., Wei, X., Jiang, D., Zhang, Y., Guo, Z., Tong, C., Liu, J., Zhou, A., Wei, B., Zhang, S., et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024d. Zhang, Y., Bai, H., Zhang, R., Gu, J., Zhai, S., Susskind, J., and Jaitly, N. How far are we from intelligent visual deductive reasoning? In COLM, 2024e. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., and Fan, Z. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Yao, H., Huang, J., Wu, W., Zhang, J., Wang, Y., Liu, S., Wang, Y., Song, Y., Feng, H., Shen, L., et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024a. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024b. Ying, K., Meng, F., Wang, J., Li, Z., Lin, H., Yang, Y., Zhang, H., Zhang, W., Lin, Y., Liu, S., et al. Mmtbench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Yue, X., Zheng, T., Ni, Y., Wang, Y., Zhang, K., Tong, S., Sun, Y., Yu, B., Zhang, G., Sun, H., Su, Y., Chen, W., and Neubig, G. Mmmu-pro: more robust multidiscipline multimodal understanding benchmark, 2024. URL https://arxiv.org/abs/2409.02813. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang, L., Li, C., et al. Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949, 2023. Zhang, R., Han, J., Liu, C., Zhou, A., Lu, P., Qiao, Y., Li, H., and Gao, P. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In ICLR 2024, 2024a. Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. LLaMA-adapter: Efficient MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency"
        },
        {
            "title": "Appendix Overview",
            "content": "Section A: Related Work. Section B: More Dataset Details. Section C: Error Analysis. Section D: More Qualitative Examples. Section E: Evaluation Prompts. A. Related Work A.1. Large Multimodal Models The field of multimodal (Radford et al., 2021; Li et al., 2022; OpenAI, 2023; 2024b) AI has experienced extraordinary growth, particularly through the development of Large Multimodal Models (LMMs) (Liu et al., 2023; Zhu et al., 2023; Lin et al., 2023; Qwen Team, 2024). These models build upon the achievements of Large Language Models (LLMs) (Touvron et al., 2023; Yang et al., 2024) and advanced vision models (Radford et al., 2021), expanding their capabilities to process multiple kinds of visual input (Li et al., 2024b; Guo et al., 2023; Li et al., 2023). Closed-source models, such as OpenAIs GPT-4o (OpenAI, 2024b), have demonstrated exceptional capabilities in visual understanding and reasoning. However, their closed-source nature creates barriers to widespread adoption and further development by the broader research community. In response, significant progress has been made in developing open-source alternatives. Early approaches like LLaVA (Liu et al., 2023), LLaMA-Adapter (Zhang et al., 2024b), and MiniGPT-4 (Zhu et al., 2023) established foundation by combining frozen CLIP models for image encoding with LLMs, enabling multimodal instruction tuning. Subsequent developments through projects such as InternVL2 (Chen et al., 2024c), Qwen2-VL (Qwen Team, 2024), SPHINX (Gao et al., 2024; Lin et al., 2023), and MiniCPM-V (Yao et al., 2024b) have expanded these capabilities by incorporating more diverse visual instruction datasets and broadening application scenarios. Recently, with the introduction of o1 (OpenAI, 2024a), the field of LMMs has also focused on enhancing the reasoning capability. (Wang et al., 2024d) introduces mixed preference optimization with automatically constructed data. (Yao et al., 2024a) proposes to leverage collective knowledge from multiple models to identify effective reasoning paths. Besides, several works (Team, 2024; Du et al., 2025) have demonstrated the ability to replicate behaviors similar to o1 models, particularly regarding multi-step CoT reasoning with iterative self-reflection and verification processes. A.2. Reasoning Evaluation Several methods have been developed to evaluate reasoning in natural language processing, including ROSCOE (Golovneva et al., 2022) and ReCEval (Prasad et al., 2023), which assess reasoning chains across multiple dimensions such as correctness and informativeness. However, these approaches are limited to text-only scenarios and do not address the unique challenges present in visual reasoning tasks. Furthermore, the emergence of long chain-of-thought (CoT) reasoning has introduced additional considerations, such as output efficiency and reflection quality, which existing evaluation methods do not adequately address. On the other hand, various multimodal benchmarks have been developed to assess reasoning abilities across specific domains. Current exploration of visual reasoning predominantly focuses on the mathematics (Zhang et al., 2024d; Peng et al., 2024) domains. MathVista (Lu et al., 2023) provides comprehensive collection of mathematical problems that assess mathematical and logical reasoning abilities. Building on this, MathVerse (Zhang et al., 2024c) introduces new benchmark by eliminating redundant textual information to evaluate whether LMMs can accurately interpret graphical representations. OlympiadBench (He et al., 2024) further raises the complexity bar by incorporating challenging Olympiad-level mathematics and physics problems. Despite these advances in specialized domains, broader applications such as general-scene reasoning remain relatively unexplored. Recent developments have begun to expand beyond purely scientific reasoning. For instance, M³CoT (Chen et al., 2024a) and SciVerse (Guo et al., 2024a) incorporate commonsense tasks alongside scientific reasoning and knowledge-based assessment in the multimodal benchmark. However, most existing benchmarks focus solely on evaluating final answers while overlooking the intermediate steps, thus providing limited insights into the process through which models arrive at their conclusions. 14 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency B. More Dataset Details B.1. Data Source Distribution We visualize the data source distributions in our benchmark, which consists of 15 sets, including MathVerse (Zhang et al., 2024c), MMMUPro (Yue et al., 2024), OlympiadBench (He et al., 2024), MMT-Bench (Ying et al., 2024), MuirBench (Wang et al., 2024a), ml-rpm-bench (Zhang et al., 2024e), MMSearch (Jiang et al., 2024), CharXiv (Wang et al., 2024e), and SciVerse (Guo et al., 2024a). Figure 9: Data Source Distribution of MME-CoT. 15 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency B.2. Preliminary Categorization Result Table 4: Accuracy of MMT-Bench for different subcategories. ACT: Action Understanding; AUT: Attribute Similarity; CNT: Cartoon Understanding; CIM: Counting; DOC: Diagram Understanding; EMO: Difference Spotting; HAL: Geographic Understanding; IIT: Image-Text Matching; IRT: Ordering; IQT: Scene Understanding; MEM: Visual Grounding; MIA: Visual Retrieval; OCR: Object Recognition; PLP: Physical Layout Prediction; RRE: Relationship Extraction; TMP: Temporal Reasoning; VCP: Visual Comprehension; VCR: Visual Coherence Reasoning; VGR: Visual Generation; VIL: Visual Identification; VPU: Visual Prediction Understanding; VRE: Visual Reasoning Evaluation."
        },
        {
            "title": "File Name",
            "content": "T U C"
        },
        {
            "title": "M\nC",
            "content": "I"
        },
        {
            "title": "O\nM\nE",
            "content": "L I I I"
        },
        {
            "title": "M\nE\nM",
            "content": "I"
        },
        {
            "title": "R\nC\nO",
            "content": "P R T V"
        },
        {
            "title": "R\nG\nV",
            "content": "L V"
        },
        {
            "title": "U\nP\nV",
            "content": "E GPT4o-cot 0.60 0.60 0.44 0.67 0.79 0.30 0.71 0.50 0.63 0.10 0.85 0.60 0.77 0.36 0.76 0.48 0.86 0.80 0.49 0.48 0.82 0.85 GPT4-direct 0.53 0.60 0.44 0.67 0.81 0.23 0.69 0.33 0.66 0.25 0.80 0.43 0.78 0.42 0.78 0.36 0.89 0.85 0.41 0.37 0.85 0.85 0.53 0.61 0.34 0.65 0.77 0.53 0.74 0.40 0.31 0.20 0.78 0.58 0.60 0.43 0.69 0.43 0.85 0.90 0.54 0.35 0.79 0.81 Qwen2-VL-7B-cot Qwen2-VL-7B-direct 0.49 0.67 0.40 0.78 0.75 0.52 0.73 0.43 0.31 0.10 0.78 0.55 0.60 0.54 0.69 0.40 0.85 0.85 0.67 0.38 0.85 0.82 Table 5: Accuracy of MUIRBench for different subcategories. AU: Action Understanding; AS: Attribute Similarity; CU: Cartoon Understanding; CO: Counting; DU: Diagram Understanding; DS: Difference Spotting; GU: Geographic Understanding; ITM: Image-Text Matching; OR: Ordering; SU: Scene Understanding; VG: Visual Grounding; VR: Visual Retrieval."
        },
        {
            "title": "File Name",
            "content": "AU GPT4o-cot 0.48 GPT4o-direct 0.45 0.38 Qwen2-VL-7B-cot Qwen2-VL-7B-direct 0.39 AS 0.57 0.62 0.51 0.47 CU 0.55 0.59 0.42 0. CO 0.75 0.50 0.43 0.41 DU 0.82 0.88 0.43 0.40 DS 0.64 0.62 0.27 0. GU 0.59 0.55 0.21 0."
        },
        {
            "title": "ITM",
            "content": "OR 0.82 0.86 0.55 0.51 0.38 0.33 0.13 0.13 SU 0.88 0.74 0.69 0.67 VG 0.56 0.38 0.37 0.31 VR 0.70 0.77 0.28 0.20 Table 6: Accuracy of OlympiadBench for the mathematics and physics subcategories."
        },
        {
            "title": "Physics",
            "content": "GPT4o-cot GPT4o-direct Qwen2-VL-7B-cot Qwen2-VL-7B-direct 0.25 0.07 0.05 0.07 0.04 0.03 0.01 0.01 16 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency C. Error Analysis We showcase the examples of the identified error types of reflection in Fig. 10. Figure 10: Examples of Reflection Error Types. 17 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency D. More Qualitative Examples Figure 11: Examples of Precision and Recall Evaluation. 18 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 12: Examples of Precision and Recall Evaluation. 19 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 13: Examples of Precision and Recall Evaluation. 20 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 14: Examples of Precision and Recall Evaluation. 21 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 15: Examples of Precision and Recall Evaluation. 22 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 16: Examples of Precision and Recall Evaluation. 23 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 17: Examples of Precision and Recall Evaluation. 24 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 18: Examples of Relevance Rate Evaluation. 25 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 19: Examples of Relevance Rate Evaluation. 26 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 20: Examples of Relevance Rate Evaluation. 27 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Figure 21: Examples of Reflection Quality Evaluation. 28 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency E. Detailed Evaluation Setup E.1. CoT Quality Evaluation Prompts"
        },
        {
            "title": "Recall Evaluation Prompt",
            "content": "You are an expert system to verify solutions to image-based problems. Your task is to match the ground truth middle steps with the provided solution. INPUT FORMAT: 1. Problem: The original question/task 2. Solution of model 3. Ground Truth: Essential steps required for correct answer MATCHING PROCESS: You need to match each ground truth middle step with the solution: Match Criteria: - The middle step should exactly match in the content or is directly entailed by certain content in the solution - All the details must be matched, including the specific value and content - You should judge all the middle steps for whether there is match in the solution OUTPUT FORMAT: [ ] { } \"step_index\": textless integertextgreater, \"judgment\": \"Matched\" \"Unmatched\" ADDITIONAL RULES: 1. Only output the JSON array with no additional information. 2. Judge each ground truth middle step in order without omitting any step. Here are the problem, answer, solution, and ground truth middle steps: [Problem] {question} [Answer] {answer} [Solution] {solution} [Ground Truth Information] {gt annotation} 29 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency"
        },
        {
            "title": "Precision Evaluation Prompt",
            "content": "# Task Overview Given solution with multiple reasoning steps for an image-based problem, reformat it into well-structured steps and evaluate their correctness. # Step 1: Reformatting the Solution Convert the unstructured solution into distinct reasoning steps while: - Preserving all original content and order - Not adding new interpretations - Not omitting any steps ## Step Types 1. Logical Inference Steps - Contains exactly one logical deduction - Must produce new derived conclusion - Cannot be just summary or observation 2. Image Observation Steps - Pure visual observations - Only includes directly visible elements - No inferences or assumptions 3. Background Information Steps - External knowledge or question context - No inference process involved ## Step Requirements - Each step must be atomic (one conclusion per step) - No content duplication across steps - Initial analysis counts as background information - Final answer determination counts as logical inference # Step 2: Evaluating Correctness Evaluate each step against: ## Ground Truth Matching For image observations: - Key elements must match ground truth observations For logical inferences: - Conclusion must EXACTLY match or be DIRECTLY entailed by ground truth ## Reasonableness Check (if no direct match) Step must: - Premises must not contradict any ground truth or correct answer - Logic is valid - Conclusion must not contradict any ground truth - Conclusion must support or be neutral to correct answer ## Judgement Categories - Match: Aligns with ground truth - Reasonable: Valid but not in ground truth MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency - Wrong: Invalid or contradictory - N/A: For background information steps # Output Requirements 1. The output format must be in valid JSON format without any other content. 2. For highly repetitive patterns, output it as single step. 3. Output maximum 40 steps. Always include the final step that contains the answer. Here is the json output format: ## Output Format [ { } ] \"step_type\": \"image observationlogical inferencebackground information\", \"premise\": \"Evidence (only for logical inference)\", \"conclusion\": \"Step result\", \"judgment\": \"MatchReasonableWrongN/A\" Here is the problem, and the solution that needs to be reformatted to steps: [Problem] {question} [Solution] {solution} [Correct Answer] {answer} [Ground Truth Information] {gt annotation} E.2. CoT Efficiency Prompt"
        },
        {
            "title": "Relevance Rate Evaluation Prompt",
            "content": "# Task Overview Given solution with multiple reasoning steps for an image-based problem, evaluate the relevance to get solution (ignore correct or wrong) of each step. # Step 1: Reformatting the Solution Convert the unstructured solution into distinct reasoning steps while: - Preserving all original content and order - Not adding new interpretations - Not omitting any steps ## Step Types 1. Logical Inference Steps - Contains exactly one logical deduction 31 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency - Must produce new derived conclusion - Cannot be just summary or observation 2. Image Description Steps - Pure visual observations - Only includes directly visible elements - No inferences or assumptions 3. Background Information Steps - External knowledge or question context - No inference process involved ## Step Requirements - Each step must be atomic (one conclusion per step) - No content duplication across steps - Initial analysis counts as background information - Final answer determination counts as logical inference # Step 2: Evaluating Relevancy relevant step is considered as: 75% content of the step must be related to trying to get solution (ignore correct or wrong) to the question. IMPORTANT NOTE: Evaluate relevancy independent of correctness. As long as the step is trying to get to solution, it is considered relevant. Logical fallacy, knowledge mistake, inconsistent with previous steps, or other mistakes do not affect relevance. logically wrong step can be relevant if the reasoning attempts to address the question. The following behaviour is considered as relevant: i. The step is planning, summarizing, thinking, verifying, calculating, or confirming an intermediate/final conclusion helpful to get solution. ii. The step is summarizing or reflecting on previously reached conclusion relevant to get solution. iii. Repeating the information in the question or give the final answer. iv. relevant image depiction should be in one of following situation: 1. help to obtain conclusion helpful to solve the question later; 2. help to identify certain patterns in the image later; 3. directly contributes to the answer v. Depicting or analyzing the options of the question is also relevant. vi. Repeating previous relevant steps are also considered relevant. The following behaviour is considered as irrelevant: i. Depicting image information that does not related to what is asking in the question. Example: The question asks how many cars are present in all the images. If the step focuses on other visual elements like the road or building, the step is considered as irrelevant. ii. Self-thought not related to what the question is asking. iii. Other information that is tangential for answering the question. # Output Format [ ] { } \"step_type\": \"image observationlogical inferencebackground information\", \"conclusion\": \"A brief summary of step result\", \"relevant\": \"YesNo\" # Output Rules 32 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Direct JSON output without any other output Output at most 40 steps Here is the problem, and the solution that needs to be reformatted to steps: [Problem] {question} [Solution] {solution}"
        },
        {
            "title": "Reflection Quality Evaluation Prompt",
            "content": "Heres refined prompt that improves clarity and structure: # Task Evaluate reflection steps in image-based problem solutions, where reflections are self-corrections or reconsideration of previous statements. # Reflection Step Identification Reflections typically begin with phrases like: - But xxx - Alternatively, xxx - Maybe should - Let me double-check - Wait xxx - Perhaps xxx It will throw doubt of its previously reached conclusion or raise new thought. # Evaluation Criteria Correct reflections must: 1. Reach accurate conclusions aligned with ground truth 2. Use new insights to find the mistake of the previous conclusion or verify its correctness. Invalid reflections include: 1. Repetition - Restating previous content or method without new insights 2. Wrong Conclusion - Reaching incorrect conclusions vs ground truth 3. Incompleteness - Proposing but not executing new analysis methods 4. Other - Additional error types # Input Format [Problem] {question} [Solution] {solution} 33 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency [Ground Truth] {gt annotation} # Output Requirements 1. The output format must be in valid JSON format without any other content. 2. Output maximum 30 reflection steps. Here is the json output format: ## Output Format [ ] { } \"conclusion\": \"One-sentence summary of reflection outcome\", \"judgment\": \"CorrectWrong\", \"error_type\": \"N/ARepetitionWrong ConclusionIncompletenessOther\" # Rules 1. Preserve original content and order 2. No new interpretations 3. Include ALL reflection steps 4. Empty list if no reflections found 5. Direct JSON output without any other output E.3. Direct Evaluation Prompt"
        },
        {
            "title": "Answer Extraction Prompt",
            "content": "You are an AI assistant who will help me to extract an answer of question. You are provided with question and response, and you need to find the final answer of the question. Extract Rule: [Multiple choice question] 1. The answer could be answering the option letter or the value. You should directly output the choice letter of the answer. 2. You should output single uppercase character in A, B, C, D, E, F, G, H, (if they are valid options), and Z. 3. If the meaning of all options are significantly different from the final answer, output Z. [Non Multiple choice question] 1. Output the final value of the answer. It could be hidden inside the last step of calculation or inference. Pay attention to what the question is asking for to extract the value of the answer. 2. The final answer could also be short phrase or sentence. 3. If the response doesnt give final answer, output Z. Output Format: Directly output the extracted answer of the response. {In Context Examples} Question: {question} Answer: {response} 34 MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency Your output:"
        },
        {
            "title": "Answer Scoring Prompt",
            "content": "You are an AI assistant who will help me to judge whether two answers are consistent. Input Illustration: [Standard Answer] is the standard answer to the question. [Model Answer] is the answer extracted from models output to this question. Task Illustration: Determine whether [Standard Answer] and [Model Answer] are consistent. Consistent Criteria: [Multiple-Choice questions] 1. If the [Model Answer] is the option letter, then it must completely matches the [Standard Answer]. 2. If the [Model Answer] is not an option letter, then the [Model Answer] must completely match the option content of [Standard Answer]. [Nan-Multiple-Choice questions] 1. The [Model Answer] and [Standard Answer] should exactly match. 2. If the meaning is expressed in the same way, it is also considered consistent, for example, 0.5m and 50cm. Output Format: 1. If they are consistent, output 1; if they are different, output 0. 2. DIRECTLY output 1 or 0 without any other content. {In Context Examples} Question: {question} [Model Answer]: {extract answer} [Standard Answer]: {gt answer} Your output:"
        }
    ],
    "affiliations": []
}