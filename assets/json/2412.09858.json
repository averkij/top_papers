{
    "paper_title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning",
    "authors": [
        "Charles Xu",
        "Qiyang Li",
        "Jianlan Luo",
        "Sergey Levine"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), a method that leverages reinforcement learning to generate high-quality training data for finetuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40% higher success rates while generalizing better to new tasks. We also provide a detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers a promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers. Videos and code can be found on our project website https://generalist-distillation.github.io"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 8 5 8 9 0 . 2 1 4 2 : r RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Charles Xu1, Qiyang Li1, Jianlan Luo1 and Sergey Levine1 1Department of EECS, UC Berkeley, Project Leads December 2024 Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), method that leverages reinforcement learning to generate high-quality training data for finetuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40% higher success rates while generalizing better to new tasks. We also provide detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers. Videos and code can be found on our project website https://generalist-distillation.github.io/. 1. Introduction Recent advances in robotic foundation models have demonstrated impressive capabilities in understanding and executing diverse manipulation skills (Collaboration et al., 2024; Brohan et al., 2023b;a; Team et al., 2024; Kim et al., 2024; Black et al., 2024; Wen et al., 2024; Liu et al., 2024; Cheang et al., 2024). By leveraging Internet-scale pretraining and grounding with robot actions, these models can achieve zero-shot and few-shot generalization across various domains. Deploying these models typically requires fine-tuning them with task-specific data to adapt to the target task or domain. The quality of this fine-tuning data is therefore critical to the performance of the resulting policies. While human teleoperation is common and accessible source for such data, human demonstrations often contain inconsistencies in execution quality and style. These variations make it challenging for foundation models to learn robust policies, as they must cope with imperfections and inconsistencies inherent in human demonstrations. This challenge affects all robotic tasks but becomes particularly pronounced in scenarios requiring precise control and dexterity, such as contact-rich manipulation. These tasks demand fine-grained, reactive control to succeed, making the quality and consistency of demonstration data even more crucial for effective policy learning. To tackle this challenge, we propose Reinforcement Learning Distilled Generalist (RLDG), simple yet effective method that leverages reinforcement learning to generate high-quality training data for robotic foundation models. While directly finetuning foundation models with reinforcement learning is possible in principle, it presents significant challenges including optimization instability, computational costs, and potential catastrophic forgetting of pre-trained capabilities, which makes it largely an open problem. Instead, our key insight is that RL agents can autonomously generate high-quality trajectories through reward maximization, making them better suited for fine-tuning generalist policies compared to human demonstrations. The approach is straightforward: we first train vision-based manipulation policies using sample-efficient real-world RL frameworks (Luo et al., 2024b;d) until convergence, then collect data from these policies to fine-tune robotic foundation models. This procedure is simple and flexible, offering several benefits. First, it provides an automated approach to generate large amounts of high-quality training data without requiring the effort of human teleoperation, which is particularly valuable since autonomous RL training is significantly more cost-effective than collecting Corresponding authors: Jianlan Luo(jianlanluo@berkeley.edu), Charles Xu(xuc@berkeley.edu) RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Figure 1: RLDG improves generalist robot policies like OpenVLA and Octo by training with specialist RL policies and using them to generate high-quality fine-tuning datasets. It has the flexibility to distill knowledge from multiple RL policies trained on individual narrowly scoped tasks into single generalist. It can also be applied to the most critical sub-task of long-horizon manipulation task, improving the success rate at the bottleneck\" while leveraging human demonstrations on parts of the task where it suffices. human demonstrations. Second, by combining the optimization capabilities of RL with the strong generalization of foundation models, RLDG produces policies that achieve superior performance while generalizing to novel scenarios. Finally, RLDG provides valuable solution for complex multi-stage tasks by using RL data to address the bottleneck\" step that hinders the performance of the overall task. Through extensive experiments across multiple manipulation tasks with well-defined reward functions, we demonstrate that generalist policies like OpenVLA (Kim et al., 2024) and Octo (Team et al., 2024) achieve superior performance when finetuned with RL data compared to human demonstrations, specifically for tasks where RL can learn effective controllers. For precise manipulation tasks such as tight-fitting connector insertions presented in Fig. 3, RLDG achieves 30% higher success rates on average. This performance gap widens further when evaluating generalization: policies trained with RLDG demonstrate significantly better transfer to novel scenarios, with on average 50% higher success rates. Notably, as we will show in Section 4, achieving comparable performance to RLDG would require 6-10x more human demonstrations. For complex tasks such as precise insertion, RLDG can achieve perfect success rates (100%), while policies trained on human demonstrations plateau at 90% even with significantly more data. Our key contributions include introducing RLDG, simple yet effective method that leverages reinforcement learning to generate high-quality training data for fine-tuning pre-trained robotic foundation models, providing an automated alternative to human demonstrations. Through extensive experiments on robotic manipulation tasks, we demonstrate that RLDG achieves 30-50% higher success rates compared to conventional fine-tuning with human demonstrations while requiring 6-10x less data. Additionally, we show that RLDG can be flexibly combined with human demonstrations in multistage tasks, enabling better overall performance by using RL data for critical phases while maintaining the benefits of human demonstrations for other phases. Our results suggest promising direction for robotic learning: using reinforcement learning as an automated source of high-quality training data for foundation models. This synergy enables more capable robotic systems that can both execute skills precisely and generalize effectively to new scenarios through natural language instructions while reducing reliance on human demonstration data collection. 2. Related Work Our work bridges reinforcement learning and foundation models for robotics through policy distillation. By combining these approaches, we develop general technique for training robust robotic policies 2 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning that leverage both the performance of RL policies and the flexibility of foundation models. Thus, we survey related work across these three key areas and examine their intersections. Foundation Models for Robotics. Recent advances in vision-language foundation models have enabled the development of generalist robotic policies that can understand and execute diverse tasks through natural language instructions. Such models (Brohan et al., 2023b;a; Kim et al., 2024; Team et al., 2024; Black et al., 2024; Wen et al., 2024; Liu et al., 2024; Cheang et al., 2024; Driess et al., 2023) leverage large-scale pretraining on Internet-scale vision-language data followed by finetuning on robot demonstrations. While these approaches show impressive generalization capabilities across wide range of tasks, our experiments demonstrate that they often struggle with precise manipulation tasks that require careful alignment and contact-rich interactions (see Section 4). This challenge comes from fundamental limitations in the demonstration-based learning approach - human demonstrations, while diverse and adaptable, often lack the precision and repeatability needed for contact-rich manipulation tasks. RLDG addresses this limitation by complementing the semantic understanding of foundation models with the robust behaviors learned through reinforcement learning, enabling precise manipulation while maintaining the flexibility and generalization capabilities of foundation models. Reinforcement Learning for Robotic Manipulation. Reinforcement learning has been successfully applied to learn complex robotic manipulation skills in the real world through direct interaction with the environment (Luo et al., 2024b;d; Rajeswaran et al., 2017; Levine et al., 2016; Hu et al., 2024b; Johannink et al., 2019; Hu et al., 2024a; Rajeswaran et al., 2018; Schoettler et al., 2020). Prior work has demonstrated RLs effectiveness in learning challenging tasks like precision insertion (Luo et al., 2021; Zhao et al., 2022; Luo et al., 2019; 2018), multi-stage assembly (Gupta et al., 2021), and dexterous in-hand manipulation (Hu et al., 2024b). key advantage of RL is its ability to discover optimal action distributions through trialand-error exploration, leading to more robust and efficient policies compared to pure imitation learning (Luo et al., 2024a). However, RL policies typically struggle to generalize beyond their training distributions, requiring separate policies to be trained for 3 each task variant or environmental condition. However, while RL policies can achieve exceptional performance on specific tasks, scaling RL to effectively handle the massive datasets used in foundation model training remains challenging. This is primarily due to computational and algorithmic difficulties in scaling up value function learning and policy optimization to process such large quantities of diverse data. As result, RL approaches often struggle to match the broad generalization capabilities demonstrated by foundation models trained on Internet-scale datasets. RLDG bridges this gap by combining the strengths of both approaches - using RL to learn optimal behaviors for specific challenging tasks, then distilling these precise capabilities into foundation models while preserving their broad generalization abilities. Policy Distillation and Knowledge Transfer. The idea of distilling multiple specialized policies into single more general policy has been explored extensively in the RL and robotics literature (Levine and Abbeel, 2014), including methods that use RL to distill into general-purpose neural networks (Rusu et al., 2015; Parisotto et al., 2015), methods that employ bidirectional constraints between specialists and generalists (Teh et al., 2017; Ghosh et al., 2018), and methods that focus on continual learning (Rusu et al., 2016; Schwarz et al., 2018). These approaches have demonstrated that careful distillation can preserve the essential behavioral characteristics of expert policies while potentially adding beneficial properties like improved generalization or reduced computational requirements. While prior work has explored policy distillation in various contexts, our work introduces two key innovations: (1) we show that distilling RL policies into foundation models that leverage large-scale pre-training yields better results than training from scratch, and (2) we demonstrate that for precise manipulation tasks, using RL-generated data for finetuning foundation models produces superior performance compared to using human demonstrations, even when high-quality demonstrations are available. Together, these findings establish RLDG as practical approach for enhancing foundation models with specialized RL capabilities while maintaining their broad generalization abilities. RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning 3. Reinforcement Learning Distilled"
        },
        {
            "title": "Generalist",
            "content": "Reinforcement Learning Distilled Generalist (RLDG) is simple yet effective method for enhancing generalist policy performance through the distillation of specialized RL policies. In our system, we train RL policies for individual tasks and then use these policies to generate training data that can be used to finetune single generalist robotic manipulation policy, such as OpenVLA (Kim et al., 2024) or Octo (Team et al., 2024). Although specialized RL policies can achieve high performance on specific tasks, they often lack zero-shot generalization and robustness to disturbances. Conversely, generalist policies excel at generalization but can struggle to achieve high performance when trained on human demonstrations, for example due to suboptimal data or modality mismatches between human demonstrators and robot policies (e.g., different viewpoints, memory, and task knowledge). RLDG bridges this gap through knowledge distillation, resulting in more performant generalists compared to finetuning on human demonstrations, while demonstrating stronger generalization capabilities compared to the original RL policies. This distillation approach through data generation with RL is agnostic to both the choice of RL algorithm and generalist policy architecture, making it flexible to any model choice. Furthermore, it offers flexibility to train and collect data with separate RL policies trained on multiple narrowly scoped tasks (such as one policy for each connector in the Connector Insertion task). We can also elect to train RL on the bottleneck\" segments of long-horizon task that require the most precision and benefit the most from RL-generated data, while leaving the less critical parts for humans to demonstrate. This simplifies the RL training complexity, improves data diversity for better generalist performance, and avoids training RL on unnecessarily long-horizon tasks. 3.1. Online RL Training We can formulate each robotic task as Markov Decision Process (MDP), where the state ùë†ùë° consists of RGB images and proprioceptive information, and actions ùëéùë° represent desired end-effector movements. The policy objective ùúã(ùëéùë°ùë†ùë°) is to maximize the expected discounted return: ùêΩ (ùúã) = ùîº ùë†0ùúå0 ùëéùë° ùúã(ùëéùë° ùë†ùë° ) ùë†ùë°+1ùëÉ(ùë†ùë°+1ùë†ùë° ,ùëéùë° ) ùëá [ ùë°=0 ùõæ ùë°ùëÖ(ùë†ùë°, ùëéùë°) ] (1) where ùúå0 defines the initial robot configurations, ùëÉ represents the systems transition dynamics, and ùëÖ ‚Ñù is reward function encoding the task objectives. While RLDG is algorithm-agnostic, we implement RLDG using HIL-SERL (Luo et al., 2024d) motivated by its sample efficiency and high performance for learning precise real-world manipulation skills from pixel input. It incorporates human interventions with RLPD (Ball et al., 2023) to efficiently learn visuomotor policies that consistently achieve 100% success rate by maximizing (1). 3.2. Experience Collection After training RL experts for each of the tasks provided to RLDG, we collect high-quality fine-tuning dataset by rolling out the converged policies. Since we transfer knowledge from RL into the generalist policy only through this data, we have the flexibility to mix experience from multiple sources. For tasks that involve separate RL policies per manipulation object like Connector Insertion, we rolled out each policy and constructed balanced fine-tuning dataset consisting of equal number of episodes per object. In cases where RL is only trained on segment of the task like FMB Assembly, we combine the RL rollouts with human demonstrations for the remainder of the task. 3.3. Generalist Policy Finetuning Robot generalist models are often pre-trained on diverse large-scale datasets before being fine-tuned to improve performance its performance on particular task while preserving the generalization capabilities form the diverse pre-training. In RLDG, we use the data collected as described above to fine-tune these generalist models. Specifically, suppose we have pre-trained policy ùúã0, we fine-tune it with taskspecific dataset ùê∑(ùë†ùë° ,ùëéùë° ) with the following supervised learning objective: (ùúÉ) = ùîº(ùë†ùë° ,ùëéùë° )[log ùúãùúÉ(ùëéùë°ùë†ùë°)] (2) We showcase the efficacy of our method by finetuning two pre-trained robot generalist models using different action parametrization. OpenVLA. OpenVLA (Kim et al., 2024) is 7Bparameter vision-language-action model built on Llama 2 (Touvron et al., 2023). It takes single image as observation input along with language instruction. It predicts 7-dimensional actions which 4 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning are discretized into 256 bins per dimension autoregressively using the standard cross-entropy loss. To fine-tune the model on our RL-generated dataset, we use the public model weights pre-trained on 970 thousand Open X-Embodiment dataset (Collaboration et al., 2024) and apply Low Rank Adaptation (LoRA) (Hu et al., 2022), popular parameterefficient fine-tuning method. Octo. Octo is another open-source generalist robotic policy, designed to adapt to diverse sensory inputs and action spaces efficiently. Different from OpenVLA, Octo predicts continuous actions with diffusion head, which excels at modeling multimodal distributions, helpful for imitating human demonstrations (Chi et al., 2024). To predict an action, the transformer backbone takes in the tokenized observation and goal, then outputs readout embedding ùëí, which is used to condition the denoising process trained on the standard DDPM objective (Ho et al., 2020). We take the pre-trained Octo-Base model, remove its secondary image tokenizer, and mask out the image goal to match our input modalities, and directly fine-tune the remainder of the network on our RL-generated dataset. 4. Experiment and Results Our experiments aim to evaluate RLDG in terms of its ability to improve over both imitation learning methods for training generalist policies (in terms of performance), and its ability to improve over more specialized RL policies (in terms of generalization). We use test suite of tasks that require precise and delicate manipulation, and thus are particularly challenging for imitation learning methods. Specifically, we focus on two main questions: (1) Is the RLDG approach for training generalists using data from RL more effective than the conventional approach of training on demonstration data? (2) Is the generalist policy that results from RLDG training more effective at generalizing than the RL policies used to generate the training data? 4.1. Experimental Setup and Tasks Our robot setup for all experiments is shown in Fig. 2. The arm tracks end-effector commands with 1kHz low-level impedance controller. Data collection, RL, and Octo policies command actions at 10Hz, while OpenVLA runs at 4Hz due to inference speed limitations. The action space for all policies is 6-dimensional end-effector delta pose in the wrist Figure 2: We use Franka Emika Panda arm with parallel jaw gripper teleoperated by 3Dconnexion SpaceMouse device. There is single RealSense D405 camera mounted on the robots wrist for image observations. frame and 1 binary gripper action for tasks that involve grasping. The RL policys observation space consists of single 128 128 wrist RGB image along with end-effector pose, velocity, and wrench measurements. For the generalist policies, we fine-tune only using the wrist camera image as input. We evaluate RLDG on four real-world manipulation tasks that present distinct challenges. These include high-precision contact-rich tasks that typically challenge generalist models, pick-and-place tasks where we show RLDG can further improve performance, and multi-stage assembly tasks that leverage RLDGs ability to compose skills. Through these tasks, we also evaluate the methods ability to generalize to unseen configurations. Connector Insertion. This task requires inserting various electronic connectors into their matching ports, which requires sub-millimeter precision and dexterity to deal with the intricate contact dynamics during alignment. We train separate RL policies and use them to collect data on USB, Ethernet, and VGA connectors before distilling them into single generalist policy. We also use Type-C, HDMI, Display Port, and 3-pin XLR connectors to evaluate the policys zero-shot generalization performance. Pick and Place. We also test our method on pickand-place task, where the robot grasps an object from randomized location and places it in bowl. To test generalization, we also evaluate on an unseen scenario by replacing the training object and background as shown in Fig. 3. With this experiment, we aim to demonstrate RLDGs effectiveness 5 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Figure 3: Illustrations of tasks used to evaluate RLDG. (A) Precise Connector Insertion includes three training objects and four unseen test objects for evaluating policy generalization. (B) Pick and Place involves an unseen scenario that tests the policys visual robustness to different backgrounds and objects. (C) FMB Insertion involves inserting pre-grasped object in moving board while (D) FMB Assembly starts with the object on the table and involves an additional grasping phase. on tasks that generalist policies are often used for and benchmarked on. FMB Insertion. We also use the single object insertion task of FMB (Luo et al., 2024c), common and reproducible benchmark for comparing robotic manipulation methods. This task involves inserting pre-grasped object into matching opening with 1.5ùëöùëö tolerance at randomized positions. We utilize this task primarily for our analysis experiments in Section 5. FMB Single Object Assembly. This task stems from the FMB Single-Object Multi-Stage Assembly, which adds grasping phase on top of the FMB Insertion task above. We use this multi-stage task to demonstrate RLDGs ability to enhance overall task performance by distilling RL data for the precision-critical insertion phase while using human demonstrations for the grasping and transport phases. More details on the experiment tasks and training procedure can be found in Appendices and B. 4.2. RLDG vs. Conventional Fine-tuning In this section, we seek to answer Question 1 by comparing generalist policies fine-tuned using RLDG and standard generalist fine-tuning via imitation learning. For each task, we fine-tune OpenVLA and Octo on RL-generated data as described in Sec. 3, and on expert human demonstrations. For fair comparison, we use the same task setup, training configuration, observation and action space, and the number of successful episodes for both methods. The only difference is the source of the data (RL vs. human). We aim to evaluate whether RL policies are better source of training data for generalist models than conventional human demonstrations in terms of resultant policy performance. Success rate. We present the success rate of each policy and method in Fig. 4. On each task, both OpenVLA and Octo fine-tuned with RL-generated data consistently achieved higher success rates than their counterparts trained with human demonstrations, in both seen and unseen evaluation scenarios. On the precise FMB Insertion and Connector Insertion tasks, where we anticipated the generalist to benefit the most from higher quality training data, OpenVLA with RLDG saw 33% and 23% higher success rates, respectively, compared to the baseline. The benefit of RLDG is equally pronounced for Octo, where it improved the success rate by 10% and 6 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Figure 4: Success rate comparison of OpenVLA and Octo policies fine-tuned with RLDG versus conventional methods using human demonstrations. Both generalists trained with RLDG consistently outperform their counterparts trained with the same number of successful expert human demonstrations in both training and unseen scenarios. 37%, respectively, although the overall success rate is lower than OpenVLA. We found that RLDG also improved the success rate for Pick and Place from 16/20 to 19/20 for OpenVLA and 1/20 to 4/20 for Octo. Furthermore, the training task performance boost of RLDG also carried over to unseen evaluation scenarios. OpenVLA with RLDG achieved over 2 times higher success rate than OpenVLA with human data for unseen Connector Insertion, while applying RLDG on Octo increased the success rate from 0/20 to 4/20 in the unseen Pick and Place task. When we strategically combine human demonstrations with RL-generated data on the FMB Assembly task, the resulting OpenVLA policy also significantly outperformed the version trained purely on human demonstrations. It achieved 20/20 successes with RL-generated data compared to 12/20 with human demonstrations, suggesting the flexibility and effectiveness of RLDG for improving the bottleneck\" of long-horizon task. Scaling analysis. To further investigate the effectiveness of RLDG, we conduct scaling experiment studying the success rate of OpenVLA policies on seen VGA connector and an unseen Type-C connector when fine-tuned on different numbers of RL-generated and human-collected episodes. We present the results in Fig. 5. For the VGA connector, OpenVLA with RLDG achieved 100% success rate with just 45 RL episodes, compared to 300 required from human demonstrations to achieve the same success rate. Furthermore, the policy trained with 150 RL rollouts on VGA, USB-A, and Ethernet connectors achieved 100% success rate on the unseen TypeC connector insertion, while OpenVLA trained on Figure 5: Success rate of OpenVLA policies fine-tuned on different sizes of RL-generated and human-collected datasets. When evaluated on seen (VGA) and unseen (Type C) Connector Insertion tasks, RLDG shows superior sample efficiency, requiring significantly fewer demonstrations to achieve perfect success rate in both scenarios while the performance of conventional method saturates in the unseen case. human demonstrations plateaued at 90% success rate even with 900 demonstrations. These results strongly suggest that fine-tuning generalist policies using RLDG is more sample-efficient and leads to higher performance than human demonstrations for both in-distribution and unseen tasks. Cycle time. As shown in Fig. 6, the generalist policies trained with RL data consistently have faster cycle times compared to those trained with human demonstrations across tasks, although the gap is not as significant. For OpenVLA, RLDG decreased the cycle time between 0.3 to 2.3 seconds per task, while Octo saw little improvement on average. This improvement can be attributed to the inherent speed 7 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Figure 6: Cycle time comparison between policies trained with RL data versus human demonstrations. N/A for RL in FMB Assembly denotes policy not trained on the whole task, while N/A for fine-tuned policies denotes no successes recorded. The RL-trained policies generally achieve faster execution times across tasks, demonstrating the efficiency benefits of using RL-generated data for policy training. optimization in RL training through temporal discounting, which is then distilled into the generalist policy by collecting trajectories that solve the task faster than the human expert. However, all generalist policies were still much slower at solving the tasks than the original RL policy. For OpenVLA, we primarily attribute this deficiency to the control frequency gap between the RL policys 10Hz and OpenVLAs 4Hz, changing the system dynamics and lowering the maximum velocity of the arm. We believe the speed of OpenVLA can be significantly improved if inference can be sped up to match the RL policy frequency. For Octo, the fine-tuned policies were unable to fit the fine-tuning dataset perfectly, leading to lower success rate and longer cycle time overall. 4.3. Generalization of RLDG vs. Original RL Policies To address Question 2, we compare the generalization performance of generalists trained using RLDG against that of the original RL policies used to generate the data. As shown in Fig. 4, the RL policy success rate quickly degraded from 20/20 for the training scenario to 1/20 for the unseen scenario of the Pick and Place task. In contrast, OpenVLA and Octo with RLDG achieved 10/20 and 4/20 success rates respectively on the same task. Additionally, the multi-task capabilities of OpenVLA and Octo allowed fine-tuning on multiple connector data in the Connector Insertion task, achieving 73/80 and 50/80, respectively, when evaluated across 4 unseen connectors, whereas the best of the three Figure 7: Fine-tuning success rate on the FMB insertion task with different fine-tuning data sources and varied dataset sizes (from 25 trajectories to 300 trajectories). Human: demo trajectories collected by human teleoperators. Human + RL actions: the same human demo trajectories but with all the actions relabeled by trained RL agent. RL: rollouts collected by the RL agent. RL data consistently provide better fine-tuning performance than human data. Human + RL actions closes the gap mostly, suggesting that most of the benefits of RL data come from it having better action quality. RL policies trained on single connectors recorded only 49/80 successes. Our experimental results on challenging dexterous manipulation tasks demonstrated several key advantages of RLDG. Generalist robot policies trained on RL-generated data using RLDG consistently achieve higher performance across all tasks compared to conventional fine-tuning methods using human demonstrations. Compared to directly using the RL policies that generated the data, RLDG also demonstrated much greater generalization capabilities and robustness to unseen test scenarios. RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning improvement the most. Figure 8 shows comparison of human and RL actions. We can see that the RL action distribution assigns more density on the correct direction (bottom-left) that moves the end-effector towards the insertion point whereas human action distribution focuses mostly around the middle with slight bias towards the correct direction. This suggests that RL actions are more optimal than human actions, resulting in the better sample efficiency for fine-tuning we observe in Figure 7. 5.2. Qualitative Analysis: Failure Modes To further understand why RL-generated data leads to better performance, we also analyzed failure modes across tasks. We observed that policies trained with RL-generated data consistently helped overcome alignment issues in precise, contact-rich tasks and reduced premature gripper closure during grasping. Videos of each policy on each task can be found on our project website (https: //generalist-distillation.github.io/). Connector and FMB Insertion. In both tasks, RL data eliminated stuck\" state where the object contacts the board but fails to align properly. Human demonstration policies often maintained contact pressure without necessary exploratory movements. Furthermore, RL data also improved approach trajectories, preventing early descents that caused connectors to catch on socket lips. Pick and Place. RL data improved grasp reliability, reducing premature gripper closure seen in human demonstration-trained policies. However, an interesting RL-specific failure mode was observed: objects were sometimes dropped too early, bouncing out of the bowl. This likely resulted from RLs speed optimization, where objects were released immediately after clearing the bowls edge, but the distilled policy lacked precise timing. FMB Assembly. While both OpenVLA policies performed similarly in grasping and transport phases as they are trained on the same human data, the performance gap emerged during insertion, with RL data better addressing alignment issues much like in the insertion tasks. Octos failure was due to consistent grasping errors where the fingers are in front of the object, likely due to the lack of good depth perception. Figure 8: Action distribution visualization for RL data and human demo data for the FMB insertion task. We visualize the first two dimensions of the dataset actions after filtering all the transitions in the dataset where the end-effector positions are close to the position shown in the image on the left (ùë•/ùë¶ coordinates are both within 4mm and ùëß coordinate is within 10mm). The robot arm needs to move in the -ùë• direction and in the -ùë¶ direction to reach the insertion point. The first two dimensions of the action space corresponds to the control of the ùë• and ùë¶ position of the end-effector position correspondingly. Human actions are clustered around the center of the action space whereas the RL actions are more optimized, and mostly found near the correct corner (bottom-left) of the action space. 5. Analysis: why is RL data better than human data? We have shown that fine-tuning generalist policies with RL data yields superior performance compared to training on human data. However, it is unclear where these benefits are coming from. In this section, we analyze the source of the benefits in two parts. The first part focuses on studying the benefits of RL actions and the state distribution in RL data in isolation. The second part focuses on dissecting the failure modes of the fine-tuned policies on each individual task. 5.1. Is RL data better because of better action or state distribution? To answer this question, we use the FMB insertion task and create mixed dataset where we take the human data and relabel the actions using action samples from the RL policy. Comparing the fine-tuning performance of the mixed dataset with the purely RL data and the purely human demo data would allow us to see the benefits of the actions and the state distribution in isolation. As shown in Figure 7, mixing human states and RL actions yields better fine-tuning success rate than using fully human data (more than 50% improvements when fine-tuning on 25/50/75 trajectories), while still being worse than using fully RL data. This suggests that while RL action and state distribution both contribute to the fine-tuning performance improvements, action quality is the factor that contributes to the performance 9 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning 6. Discussion and Limitations"
        },
        {
            "title": "Acknowledgments",
            "content": "In this work, we presented RLDG, simple method that fine-tunes generalist policies on high-quality data generated by RL policies. We demonstrated that generalist policies fine-tuned using RLDG consistently outperform those trained with human expert demonstrations on suite of real-world challenging precise manipulation tasks. Our method can be applied in real-world robotic manipulation tasks that require large amount of training data or where policy performance using human demonstrations saturate. Our work also opens up avenues for making autonomous improvements of generalist policies more scalable and tractable. First, our method assumes access to reward functions for fine-tuning tasks which may present difficulties when the task rewards are hard to specify. Possible future directions include autonomously generating fine-tuning tasks with reward functions (e.g., using pre-trained VLMs) such that there is no need for manual task specification. Furthermore, our RL policies optimize not only for task success but the speed in doing so. Such an objective does not necessarily result in policies that are robust in distillation errors. For example, on the Pick and Place task, the policy fine-tuned on RL-generated data always tries to place the object immediately after it has moved close enough to the goal location but sometimes drops the object too early (see Section 5.2). Nevertheless, we demonstrated that specialist RL policies can be an effective generator of training data for robotic foundation models, and we hope to inspire further research in this domain."
        },
        {
            "title": "Author Contribution",
            "content": "Charles Xu contributed to the research design, prepared the hardware setup, performed the implementation and robot experiments, wrote part of the paper, created the paper figures and the website, coled the project. Qiyang Li contributed to the research design, performed the analysis in Section 5, wrote part of the paper Jianlan Luo conceived the project, designed the research, advised the project in terms of overall direction, practical implementation, experiment design, wrote part of the paper, co-led the project Sergey Levine conceived the project, designed the research, advised the project in terms of overall direction, experiment design, edited the paper This research was supported in part by ONR N0001422-1-2773, N00014-20-1-2383 and NSF IIS-2150826."
        },
        {
            "title": "References",
            "content": "Philip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 15771594. PMLR, 23 29 Jul 2023. URL https://proceedings.mlr.press/v202/ ball23a.html. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. ùúã0: vision-language-action flow model for general robot control, 2024. URL https://arxiv.org/abs/2410.24164. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, TsangWei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023a. URL https://arxiv.org/abs/2307.15818. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, KuangHuei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023b. URL https://arxiv.org/abs/2212.06817. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-languageaction model with web-scale knowledge for robot manipulation, 2024. URL https://arxiv.org/abs/2410. 06158. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion, 2024. URL https:// arxiv.org/abs/2303.04137. Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben BurgessLimerick, Beomjoon Kim, Bernhard Sch√∂lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B√ºchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo√£o Silv√©rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart√≠n-Mart√≠n, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023. URL https: //arxiv.org/abs/2303.03378. 11 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-andconquer reinforcement learning, 2018. URL https: //arxiv.org/abs/1711.09874. Abhishek Gupta, Justin Yu, Tony Z. Zhao, Vikash Kumar, Aaron Rovinsky, Kelvin Xu, Thomas Devlin, and Sergey Levine. Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention. In IEEE International Conference on Robotics and Automation, ICRA 2021, Xian, China, May 30 - June 5, 2021, pages 66646671. IEEE, 2021. doi: 10.1109/ICRA48506.2021.9561384. URL https://doi.org/10.1109/ICRA48506.2021.9561384. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR. OpenReview.net, 2022. URL http://dblp.uni-trier.de/db/conf/ iclr/iclr2022.html#HuSWALWWC22. Hengyuan Hu, Suvir Mirchandani, and Dorsa Sadigh. Imitation bootstrapped reinforcement learning, 2024a. URL https://arxiv.org/abs/2311.02198. Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, and Sergey Levine. REBOOT: reuse data for bootstrapping efficient realworld dexterous manipulation. arXiv preprint arXiv:2309.03322, 2024b. Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for robot control. In 2019 International Conference on Robotics and Automation (ICRA), pages 60236029, 2019. doi: 10.1109/ICRA.2019.8794127. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An opensource vision-language-action model, 2024. URL https://arxiv.org/abs/2406.09246. Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. URL https: Curran Associates, //proceedings.neurips.cc/paper_files/paper/2014/file/ 6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf. Inc., 2014. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. J. Mach. Learn. Res., 17:39:139:40, 2016. URL http://jmlr.org/papers/v17/15-522.html. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation, 2024. URL https: //arxiv.org/abs/2410.07864. Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, and Alice Agogino. Deep reinforcement learning for robotic assembly of mixed deformable and rigid objects. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 20622069. IEEE, 2018. Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, Alice Agogino, Aviv Tamar, and Pieter Abbeel. Reinforcement learning on variable impedance controller for high-precision robotic assembly. In 2019 International Conference on Robotics and Automation (ICRA), pages 3080 3087. IEEE, 2019. Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wenzhao Lian, Chang Su, Mel Vecerik, Ning Ye, Stefan Schaal, and Jonathan Scholz. Robust multi-modal policies for industrial assembly via reinforcement learning and demonstrations: large-scale study. In Proceedings of Robotics: Science and Systems, Virtual, July 2021. doi: 10.15607/RSS.2021.XVII.088. Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, and Sergey Levine. RLIF: Interactive imitation learning as reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id= oLLZhbBSOU. 12 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine. Serl: software suite for sample-efficient robotic reinforcement learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1696116969, 2024b. doi: 10.1109/ICRA57147.2024. 10610040. Jianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey Levine. Fmb: functional manipulation benchmark for generalizable robotic learning. arXiv preprint arXiv:2401.08553, 2024c. Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning, 2024d. URL https://arxiv.org/abs/2410.21845. Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer learning for robotic control. arXiv preprint arXiv:1511.06342, 2015. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. CoRR, abs/1709.10087, 2017. URL http://arxiv.org/abs/1709.10087. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018. doi: 10.15607/RSS.2018.XIV.049. Andrei Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015. Andrei Rusu, Neil Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. In Advances in Neural Information Processing Systems, 2016. Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Deep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 55485555, 2020. doi: 10.1109/IROS45743. 2020.9341714. Jonathan Schwarz, Jelena Luketina, Wojciech Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: scalable framework for continual learning. In International Conference on Machine Learning, pages 45284537. PMLR, 2018. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. URL https://arxiv.org/abs/2405.12213. Yee Whye Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. arXiv preprint arXiv:1707.04175, 2017. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning and fine-tuned chat models, 2023. URL https: //arxiv.org/abs/2307.09288. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, and Jian Tang. Tinyvla: Towards fast, data-efficient visionlanguage-action models for robotic manipulation, 2024. URL https://arxiv.org/abs/2409.12514. Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and Sergey Levine. Offline metareinforcement learning for industrial insertion. In 2022 International Conference on Robotics and Automation (ICRA), pages 63866393, 2022. doi: 10.1109/ICRA46639.2022.9812312. 14 RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning A. Task Details Connector Insertion. The robot starts with the male connector pre-grasped in 10ùëêùëö 10ùëêùëö plane 5 cm above the female port. We use the same D-type fixture for the female portion which is fixed to the table at consistent pose. The task is completed if the robot aligns and inserts the connector into the socket. We train the policies on USB, Ethernet, and VGA connectors, while using Type-C, HDMI, Display Port, and 3-pin XLR connectors to evaluate generalization. Pick and Place. In this task, the robot starts 15ùëêùëö above the table at fixed pose. The target object is randomly placed within 18ùëêùëö 18ùëêùëö area on the table and the bowl is at fixed pose 5ùëêùëö from the edge of the object randomization region. The robot is to pick up the object and put it into the bowl. To test generalization, we evaluate on an out-of-distribution scenario by replacing the green pepper training object with yellow corn and changing the tabletop to beige wood grain surface as shown in Fig. 3. FMB Insertion. The robot starts with the insertion object pre-grasped 15 cm above the assembly board, which is randomly placed within 35ùëêùëö 35ùëêùëö area with 15 rotation. The task is successfully completed if the object is completely inserted into the board. The insertion tolerance in this task is 1.5ùëöùëö. We used the same set of board poses during each evaluation experiment to ensure consistency across rus. FMB Single Object Assembly. In this task, the assembly board is randomized the same way as in the FMB Insertion, but the object is randomly placed in 3ùëêùëö 7ùëêùëö grasping area approximately 20ùëêùëö from the insertion area. The robot starts 15ùëêùëö above the object at fixed position. We also use consistent object and board poses during evaluation. B. Training Procedures For FMB Insertion, Connector Insertion, and Pick and Place, we first collect positive and negative samples using our SpaceMouse teleoperation device to train binary success classifier as the reward function for RL. Then, we initialize the RL buffer with 20 demonstrations and train it on the whole task for 1-3 hours until the policy reaches 100% success rate. Next, we roll out the converged RL policy to collect data for generalist policy fine-tuning, filtering out failed trajectories if any exist. We also collect set of all successful expert human demonstrations to compare against the RL-generated data. For FMB Single Object Assembly, we use the same reward function as in FMB Insertion and we only train RL on the insertion stage by starting the episode with the object pre-grasped and the arm 15ùëêùëö above the board within 5ùëêùëö 5ùëêùëö randomization region. We periodically adjust the grasp pose during training and data collection to build robustness to variations in the grasp. We also collect the same number of human demonstrations for the insertion stage to compare performance. We then separately collect human demonstrations for the grasping phase, starting the robot above the insertion object and ending the episode when the arm has grasped the object and moved within the insertion randomization region above the board. We combine data from the two stages for fine-tuning. We fine-tuned the OpenVLA weights pre-trained on OXE dataset using LoRA with rank of 32 applied to each linear layer of the model, which reduces the computational overhead while not sacrificing much performance. We trained using the default fine-tuning configuration with batch size 2 and 3 gradient accumulation steps on single Nvidia RTX 4090 GPU, which took between 3 to 5 hours to converge. For Octo, we started from the pre-trained Octo-Base model, using the primary image tokenizer to tokenize the wrist camera image and removed the secondary tokenizer. We also mask out the image goal since we do not use it. We applied full fine-tuning with the default hyperparameters and batch size 64 until convergence, which took 3-5 hours on single Nvidia RTX 4090 GPU."
        }
    ],
    "affiliations": [
        "Department of EECS, UC Berkeley"
    ]
}