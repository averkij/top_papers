{
    "paper_title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property",
    "authors": [
        "Qiyao Wang",
        "Guhong Chen",
        "Hongbo Wang",
        "Huaren Liu",
        "Minghui Zhu",
        "Zhifei Qin",
        "Linwei Li",
        "Yilin Yue",
        "Shiqiang Wang",
        "Jiayan Li",
        "Yihang Wu",
        "Ziqiang Liu",
        "Longze Chen",
        "Run Luo",
        "Liyang Fan",
        "Jiaming Li",
        "Lei Zhang",
        "Kan Xu",
        "Hongfei Lin",
        "Hamid Alinejad-Rokny",
        "Shiwen Ni",
        "Yuan Lin",
        "Min Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Intellectual Property (IP) is a unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and a large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domain-specific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 2 5 5 1 . 4 0 5 2 : r IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property Qiyao Wang1,2, Guhong Chen1, Hongbo Wang2, Huaren Liu2, Minghui Zhu2, Zhifei Qin2 Linwei Li2, Yilin Yue2, Shiqiang Wang2, Jiayan Li2, Yihang Wu2, Ziqiang Liu1, Longze Chen1, Run Luo1, Liyang Fan1, Jiaming Li1, Lei Zhang1, Kan Xu2, Hongfei Lin2, Hamid Alinejad-Rokny4, Shiwen Ni1, Yuan Lin2, Min Yang1,3 1Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China 2 Dalian University of Technology, China 3 Shenzhen University of Advanced Technology, China 4 The University of New South Wales, Australia wangqiyao@mail.dlut.edu.cn, zhlin@dlut.edu.cn {sw.ni, min.yang}@siat.ac.cn Website: https://IPBench.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Intellectual Property (IP) is unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domainspecific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain."
        },
        {
            "title": "Introduction",
            "content": "Intellectual property (IP) is the embodiment of human creativity and innovation [56] and is protected by law through mechanisms such as patents, copyrights, and trademarks. Due to its blend of technical and legal attributes, tasks in this field are inherently knowledge-intensive, highly applicable to realworld scenarios, and hold substantial practical value. Beyond domain-specific technical knowledge, it also requires capabilities in information processing, logical reasoning, decision-making, and creative generation. Taking patent examination as an example, the United States Patent and Trademark Office (USPTO) received over 700,000 patent applications in 2024, with backlog of approximately 520,000 unexamined applications [49]. This process requires patent examiners to have both technical and Corresponding author. Preprint. Under review. Figure 1: Overview of the xcomprehensive IP task taxonomy and IPBench. legal knowledge to make well-founded and logical determinations, leading to increased labor and time costs, which in turn impacts the efficiency of innovation. With the advancement of large language models (LLMs) [1, 6, 57], they offer more generalized approach to handling tasks across various domains, including understanding, processing, and even generating content. This makes it possible to automatically process intellectual property information and provide efficient services. Nowadays, NLP researchers have been paying increasing attention to the field of intellectual property. Jiang et al. [20] conduct survey on patents and classify patentrelated tasks into two categories: analysis and generation, providing comprehensive insights into the future development of this field, but constrained to the aspect of patent text only. Previous patent related dataset like HUPD [46] compiled large-scale corpus of patent applications, focusing on pre-grant patents, and introduced three tasks: multi-class classification of patent subject areas, language modeling, and summarization. Although this dataset has practical significance, it only addresses certain attributes of patents, such as their linguistic characteristics, while overlooking the more critical technical and legal aspects of the field. PatentEval [62], MoZIP [36], and IPEval [52] are benchmarks that focus only on specific aspects of IP, resulting in limited task scope. Moreover, previous work has predominantly focused on patents, whereas the IP field encompasses other crucial aspects. As domain with strong real-world demand and value, it still lacks comprehensive task taxonomy and benchmark that better aligns with real-world scenarios. To bridge the gap between real-world demands and the application of LLMs in the IP field, we introduce the first comprehensive IP task taxonomy, as illustrated in Figure 1. Our taxonomy is based on Webbs Depth of Knowledge (DOK) Theory [54] and is extended to include four hierarchical levels: Information Processing, Logical Reasoning, Discriminant Evaluation, and Creative Generation. It includes an evaluation of models intrinsic knowledge of IP, along with detailed analysis of IP text from both point-wise and pairwise perspectives, covering technical and legal aspects. Building on this taxonomy, we develop IPBench, the first comprehensive Intellectual Property Benchmark for LLMs, consisting of 10,374 data points across 20 tasks aimed at evaluating the knowledge and capabilities of LLMs in real-world IP applications. This holistic evaluation enables us to gain hierarchical deep insight into LLMs, assessing their capabilities in in-domain memory, understanding, reasoning, Figure 2: Overall performance of representative models on IPBench. 2 discrimination, and creation across different IP mechanisms. Due to the legal nature of the IP field, there are regional differences between countries. Our IPBench is constrained within the legal frameworks of the United States and mainland China, making it bilingual benchmark. We conduct extensive experiments to evaluate IPBench on 16 LLMs, including general-purpose, law-oriented and IP-oriented models, spanning chat models and reasoning models, and provide detailed analysis. Figure 2 shows the overall performance of representative models on IPBench. Our main contributions and findings are as follows: We propose the first comprehensive IP task taxonomy and introduce IPBench, bilingual benchmark with 10,374 data points spanning 20 tasks and 8 IP mechanisms, providing realistic and diverse evaluation framework for IP-oriented LLM capabilities. Our extensive experiments show that the best-performing LLM achieves an accuracy of only 75.8%, highlighting the significant room for improvement in handling IP-related tasks. Existing open-source IP and law-oriented models underperform compared to closed-source general models, the IP domains privacy and security demands call for stronger and more efficient domain-specific models. We incorporate both IPC/CPC classification tasks and generation tasks into our IPBench benchmark. DeepSeek-R1 demonstrates the best performance on IPC classification with 10.8%, while DeepSeek-V3 achieves the highest on CPC classification with 9.5%. For the generation tasks, we introduce LLMScore as an automatic evaluation metric based on the LLM-as-a-judge, which demonstrates higher consistency with human evaluation. We conduct extensive additional analyses, including results across different languages and prompt settings. Moreover, we provide detailed error analysis of seven error types and large-scale case study, offering deeper insights into the domain capabilities of LLMs. The patent system added the fuel of interest to the fire of genius. Abraham Lincoln. We believe that IPBench will play crucial role in the application of large language models in the field of intellectual property. From technical perspective, the complexity of IP text can drive the community to develop more powerful LLMs. From an IP perspective, it enhances the intelligence of IP services and reduces costs in this field, ultimately accelerating technological innovation. We will continuously update this benchmark in our future work to incorporate more real-world tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Benchmark Evaluation for LLMs Multilingual Multi-IP Mechanisms Techical-Legal Dual Coverage Gen.-Ana. Dual Coverage Comprehensive Taxonomy LLMs Evaluated Count Task Count Testset Size PatentEval [62] 6 2 400 15 1 2657 IPEval [52] MoZIP [36] Ours 5 3 3121 16 20 10374 Table 1: Comparison of IP related benchmark with IPBench (Ours). Gen.-Ana. Dual Coverage refers to benchmarks that encompass both text generation and analysis tasks. Before the surge of large language models, researchers have explored to applied the technic of natural language processing on intellectual property especially on patent text, such as patent classification [28, 25, 9], patent retrieval [39], abstract and claims generation [45, 44, 25]. However, these algorithms or models lack generalization capability, requiring significant effort for adaptation. With the development of large language models based on the decoder-only transformer architecture [40, 41], models trained on the next-token prediction task can perform various tasks using prompts in zero-shot [24] or few-shot [3] manner. This provides researchers with novel perspective on tackling patent-related tasks using powerful LLMs. 3 Recently, Ni et al. [36] developed multilingual intellectual property LLM, MoZi, based on the well-known models BLOOMZ[35] and ChatGLM [11]. Bai et al. [2] propose low-cost, standardized procedure for training IP-oriented LLMs to meet the unique requirements of the IP domain, claiming that their model achieves performance comparable to human experts. Beyond modeling, Pap2Pat [23], AutoPatent [53], and PatentFormer [51] focus on generating long-context patent content using LLMs or LLM-based agents. These researches are almost pay attention to the technical aspect of patent, and lack of other intellectual property mechanisms. Our IP task taxonomy includes these common tasks, and we restructure and integrate them with new tasks covering both technical and legal aspects, as well as other IP mechanisms such as trade secret rights and trademarks, based on Webbs Depth of Knowledge Theory (DOK) [54]. This comprehensive task taxonomy is the most distinctive feature that sets our benchmark, IPBench, apart from existing IP, particularly patent-related, benchmarks, such as IPEval [52], MoZIP [36], and PatentEval [62]. We provide detailed comparison of IPBench with other IP-related benchmarks, as shown in Table 1. Researchers have developed numerous benchmarks for LLM evaluation, such as GPQA [42], SuperGPQA [47], MMLU [14], HLE [38]. In vertical domains, especially for biomedical, education and law fields, there are many benchmarks that measure the different abilities of LLMs, such as MedQA [22], E-Eval [15], LexEval [26] and LegalBench [12]. All these excellent general and in-domain benchmarks provide us with valuable insights on data annotation, model evaluation, and detailed analysis. Our IPBench, due to its legal attributes, includes tasks such as legal concept memory and legal case analysis. However, all of our legal-related tasks and data are strongly constrained within the IP field. Additionally, it covers both technical and economic aspects, ensuring its novelty and distinctiveness compared to purely legal benchmarks."
        },
        {
            "title": "IPBench",
            "content": "3.1 Task Taxonomy Level Index Task Name Metric Data Source Language Size Information Processing 1-1 1-2 1-3 1-4 1-5-1 1-5-2 1-6 1-7 Legal Concept Memory Legal Clause Memory Legal Evolution Typical Case Memory Patent IPC Classification Patent CPC Classification IP Element Identification Process Guidance Accuracy Accuracy Accuracy Accuracy Exact Match Exact Match Accuracy Accuracy Logical Reasoning Discriminant Evaluation Creative Generation 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 3-5 4-1 4-2 4-3 Patent Technology Forecasting Accuracy Infringement Behavior Determination Accuracy Accuracy Compensation Calculation Accuracy Patent Valuation Accuracy Trade Secret Requirements Patent Document Proofreading Patent Validity Identification Patent Match Rights Attribution Analysis Patent Application Examination Abstract Generation Dependent Claim Generation Design-Around Solution Generation Accuracy Accuracy Accuracy Accuracy Accuracy AE & HE AE & HE Accuracy Expert Annotation Expert Annotation Expert Annotation USTPO / CNIPA USTPO / CNIPA USTPO Expert Annotation Expert Annotation Expert Annotation Expert Annotation Expert Annotation Expert Annotation Expert Annotation Expert Annotation Expert Annotation MoZIP [36] Expert Annotation USTPO [34] USTPO / CNIPA USTPO / CNIPA Expert Annotation EN/ZH EN/ZH EN/ZH EN/ZH EN/ZH EN EN/ZH EN/ZH EN/ZH EN/ZH EN/ZH EN/ZH ZH EN/ZH EN/ZH EN/ZH EN/ZH EN EN/ZH EN/ZH EN/ZH 500 502 500 504 1125 600 557 548 500 500 316 301 301 300 308 1000 400 314 400 400 499 Table 2: Overview of task taxonomy in IPBench. The EN in the Language column indicates English, while ZH represents Chinese. The AE in the Metric column indicates Automated Evaluation, while HE represents Human Evaluation. Previous patent benchmarking primarily focused on the text itself, overlooking the broader connections between the IP field and real-world scenarios. We introduce this first comprehensive IP task taxonomy not only to focus on in-domain text, such as patent classification, but also to address the real-world demands of this field across technical and legal aspects, such as patent valuation. Due to the complexity of IP knowledge, model must integrate various IP legal mechanisms, real-world in-domain execution procedures, and robust natural language processing capabilities across different linguistic styles. Our taxonomy is based on American educational evaluator Norman L. Webbs Depth of Knowledge Theory (DOK) [54], which divides students cognitive levels into four tiers: 4 Recall and Reproduction, Skill and Concept Application, Strategic Thinking, and Extended Thinking. This helps educators understand the depth of cognitive engagement required when guiding students through cognitive tasks. We extend this framework into four hierarchical levels that better align with the demands of the IP field: Information Processing, Logical Reasoning, Discriminant Evaluation, and Creative Generation, as illustrated in Figure 1. We provides detailed explanation of the four hierarchical levels of our taxonomy, along with the tasks included within each level in Appendix B.1. Table 2 shows an overview of our 20 tasks and their detailed definitions can be found in Appendix B.2. 3.2 Data Collection, Processing and Annotation Data Source and Collection. Our data sources primarily consist of three types: expert annotations, databases established by national IP offices, and previously published public datasets. Due to the diversity of our tasks, data for tasks like Legal Concept Memory is constrained by the scope of the corresponding laws and can be found on the public websites of IP offices, while data for tasks like Infringement Behavior Determination can be obtained from public case databases, such as China Judgements Online. The patent data we collected are from the United States Trademark and Patent Office (USTPO) and China National Intellectual Property Administration (CNIPA). All the data sources we used are publicly available. Data Processing and Annotation. Our IPBench is golden dataset, created through extensive human expert annotation. Due to the structured nature of patent texts, the USPTO and CNIPA provide well-organized patent data, making it convenient for us to create paired data, such as claim-abstract pairs and previous claim with subsequent claim pairs. We employ 21 expert annotators, including senior undergraduate and PhD students, under the supervision of four experienced patent agents to ensure annotation quality and correctness. It is worth noting that most of students major in intellectual property, which makes them familiar with IP concepts and law-related knowledge, ensuring the high quality of our test data. We organized them into four teams, each corresponding to one of the four hierarchical levels of our taxonomy. Each task follows two-stage process, where annotation and examination teams exchange roles at different stages to ensure the correctness and suitability of our data. After annotation, we use cosine similarity with the BGE-M3 [4] embedding model to filter out duplicate data, further ensuring the quality of our dataset. The detailed annotation and examination protocol is illustrated in Appendix C. 3.3 Feature of IPBench Statics Total Questions English Questions Chinese Questions Options per Question : : : Avg. MCQ Length Avg. Task 3-5 Input Length Avg. Task 4-1 I/O Length Avg. Task 4-2 I/O Length Avg. DC Count 10,374 5,284 (50.94%) 5,090 (49.06%) 4 5 : 6 : 5 : 4 181.0 7460.4 1636.7 / 148.5 448.5 / 437.6 5.2 Figure 3: Distribution of IP mechanisms. Table 3: Dataset statistics. MCQA: Multiple-choice question. DC: Dependent Claims. IPBench comprises 10,374 questions across 20 tasks, organized into four hierarchical levels and covering 8 IP mechanisms, including patents, trademarks, etc. These tasks span both technical and legal dimensions, assessing generative as well as comprehension abilities. As shown in Table 1, IPBench outperforms existing IP-related benchmarks across multiple dimensions. Considering the extensive range of IP mechanisms and technical fields covered, we provide statistical of various data characteristics in IPBench, including the distribution of IP mechanisms, IPC/CPC classifications, and the linguistic features of both the questions and the associated IP texts, as illustrated in Figure 3 and Table 3. We provide further analysis of IPBenchs data features in Appendix D, including the IPC/CPC classification distribution, text length distribution and other aspects."
        },
        {
            "title": "4 Benchmarking Results",
            "content": "4.1 Evaluated Models Table 4 provides an overview of the 16 evaluated models, covering various sizes, types, specializations, and access methods. There are 13 general-purpose models, 2 law-oriented models, and one IP-oriented models. Notably, as of our work, there are no reasoning models specifically designed for IP or law, which could be future research direction. Model Size Max Context Type Orientation Access GPT-4o [17] GPT-4o-mini [17] DeepSeek-V3 [6] Qwen2.5-Instruct [57] Llama3.1-Instruct [7] Gemma-2-Instruct [43] Mistral-7B-Instruct [19] MoZi-qwen [36] DISC-LawLLM [58, 59] HanFei [13] 671B 7/72B 8/70B 9/27B 7B 7B 6B 7B DeepSeek-R1 [5] Deepseek-R1-Distill-Qwen [5] QwQ [48] 671B 7B 32B 128k 128k 128k 32k 32k 8k 32k 32k 2048 128k 32k 32k Chat Model Chat Model Chat Model Chat Model Chat Model Chat Model Chat Model Chat Model Chat Model Chat Model General General General General General General General IP Law Law OpenAI API OpenAI API DeepSeek API Weights Weights Weights Weights Weights Weights Weights Reasoning Model Reasoning Model Reasoning Model General General General DeepSeek API Weights Weights Table 4: The overview of evaluated models. Max Context refers to the maximum context length of the model without length extrapolation for all models. 4.2 Evaluation Setup Inspired by previous benchmarks [47, 33, 26], we employ five different settings for chat models: zero-shot, 1-shot, 2-shot, 3-shot, and CoT [55], while adopt the zero-shot setting for reasoning models. For each task with few-shot, we select one to three examples distinct from the current one using fixed random seed. The prompts used for different settings are detailed in Appendix E. For experiment, we set the temperature to 0.0 to ensure result reproducibility and fairness. The token limit is set to 32k for the reasoning model and 8k for the chat model; for models with shorter context lengths (e.g., 2k or 4k), we use the maximum supported length. Notably, for tasks 3-5, 4-1, and 4-2, due to their extensive input/output lengths, we set the maximum context length to 16k for all chat models in the both zero-shot setting and CoT setting, and 32k in the few-shot setting. As result, we are unable to evaluate Gemma-2-Instruct, DISC-LawLLM and Hanfei on these tasks. We use accuracy as the evaluation metric for most of our tasks, as shown in Table 2, except for certain classification and generation tasks. For Patent IPC/CPC classification tasks, we use exact match as the evaluation metric, following the approach of HELM [30] and LegalBench [12]. According to the characteristics of IPC/CPC numbers, we divide exact matches into three typesSection, Class, and Subclassfollowing Fall et al. [9]. For both abstract and claim generation, we utilize the F1 score of BLEU [37], ROUGE-L [31], and BERTScore [60]. Building on the fine-grained error taxonomy PatentEval [62] introduced, we propose LLMScore, multi-dimensional automatic metric for these tasks, following the LLM-as-a-judge paradigm [32, 27], and evaluate its consistency with human judgments. We provide detailed explanation of these metrics and their limitations in Appendix F. Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 75.3 72.6 75.8 74.7 68.0 70.5 61.7 68.1 64.9 54.7 64. DISC-LawLLM 52.8 40.1 Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B 73.9 57.0 73.5 1-1 96.0 94.4 96.6 96.0 92.4 93.8 90.4 90.6 91.6 79.6 93. 79.0 63.0 96.0 77.8 95.2 1-2 92.0 87.5 90.2 90.4 83.3 85.3 75.9 80.5 78.3 63.9 83.3 65.3 46. 92.0 59.0 91.0 1-3 82.2 80.2 88.4 84.2 77.2 77.6 68.2 73.2 73.0 60.6 77.0 67.6 51.8 87.6 53.8 81. 1-4 83.7 82.1 82.8 83.5 77.2 79.8 71.3 77.6 61.5 60.1 66.1 60.1 45.4 80.8 57.1 77.8 164.2 58.8 66.1 61.3 58.4 59.3 53.0 54.5 58.8 40.5 58.2 54.5 39.8 64.9 49.8 65.1 1-7 71.9 67.5 69.9 69.2 62.0 67.0 60.4 61.3 59.3 54. 64.2 52.0 47.3 71.7 50.7 71.5 2-1 54.8 50.2 56.8 54.4 49.4 53.0 47.6 53.4 51.2 43.6 50. 40.8 30.8 53.6 43.8 57.4 2-2 62.6 64.0 64.2 66.6 64.4 64.8 57.5 65.0 63.6 56.0 58.0 60.4 45. 64.6 51.2 66.6 2-3 63.9 59.5 66.1 63.0 57.3 53.5 44.6 56.0 46.8 42.4 41.8 31.3 33.9 71.8 46.2 70. 2-4 78.5 76.7 76.7 80.4 74.4 74.8 71.4 76.4 70.4 64.1 67.8 60.1 40.9 78.1 67.1 80.1 284.1 83.4 84.1 82.1 77.1 81.1 75.7 81.1 80.4 67.0 76.4 64.8 49.2 85.4 65.5 85.4 3-1 71.0 67.3 72.0 71.7 67.7 70.3 60.0 69.3 66.0 56. 68.0 53.7 42.7 63.3 54.0 69.7 3-2 70.1 75.0 75.0 73.4 71.1 74.4 61.7 66.2 66.9 45.8 64. 45.1 28.6 78.2 62.0 82.1 3-3 81.3 81.6 78.9 79.9 65.8 67.1 50.6 57.2 51.9 43.9 56.1 28.2 18. 67.2 63.7 67.3 3-4 83.5 78.5 83.5 80.7 78.2 78.0 77.2 80.2 76.0 65.1 79.0 71.2 48.8 82.0 63.7 77. 3-5 50.0 44.0 44.6 43.3 38.9 45.2 41.7 43.9 43.9 47.5 43.6 47.1 475.4 66.3 78.8 75.3 58.9 71.3 52.3 66.9 62.1 54.5 57.1 35.3 29.5 74.3 54.9 69.7 Table 5: Main results of IPBench. The best-performing model in each task is in darker red , and the second best is in lighter red . The model DS-Qwen refers to DeepSeek-R1-Distill-Qwen, while the suffix it indicates the Instruct version of the model. OA denotes the overall average accuracy. 4.3 Main Results As shown in Table 5, Table 6 and Table 7, we present the main results under the zero-shot experimental setting, while results for the few-shot setting (1-shot, 2-shot, and 3-shot) and the CoT setting are provided in Appendix G.1. More comprehensive results of IPBench can be found in Appendix G. Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DISC-LawLLM Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 4.8 1.0 10.6 4.9 1.9 3.5 0.9 1.2 0.3 0.1 0.6 0.0 0.0 10.8 0.0 2.9 81.6 80.5 83.7 82.4 76.8 80.4 71.8 72.9 73.7 67. 38.8 68.2 11.7 85.8 20.5 83.8 71.3 66.8 73.3 70.4 63.0 65.6 56.2 57.4 55.6 42.8 29.6 47.2 2. 74.7 6.9 70.4 55.1 50.1 58.3 55.2 46.6 50.0 35.8 41.5 37.2 26.8 20.3 28.3 0.1 59.3 1.4 53.8 3.3 0.5 9.5 2.5 0.2 1.0 0.0 0.2 0.2 0. 0.0 0.0 0.0 8.5 0.0 0.5 82.7 79.0 84.0 81.5 65.5 79.5 63.8 70.5 56.2 39.0 8.5 31.0 0. 82.5 5.1 76.0 69.7 64.5 73.3 69.5 44.8 64.3 45.0 56.7 39.0 21.5 3.1 23.4 0.0 71.2 0.5 62.3 62.0 52.7 65.2 60.7 34.8 52.7 30.7 44.3 26.7 10. 1.8 11.5 0.0 63.2 0.2 51.3 Table 6: Main results of Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2). The best-performing model in each task is in darker purple , and the second best is in lighter purple . 4.4 Analysis 4.4.1 Model Performance across Different Languages Model performance aligns closely with their primary training language. We provide detailed results of IPBench across different languages in Appendix G. DeepSeek-V3 achieves the best performance on the Chinese subset with an accuracy of 78.7%, while GPT-4o leads on the English subset with an accuracy of 73.2%. These results highlight the disparity between the Chinese and English IP legal frameworks, underscoring the need for models to recognize and adapt to these differences during inference, aligning with the findings of IPEval [52]. 4.4.2 Disparity between IP-oriented and General-purpose Models General-purpose models outperform both law-oriented and even IP-oriented models. The law-oriented model is trained on the entire legal framework, while the IP-oriented model focuses more on IP7 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) BLEU R-L BS LLMScore Tokens # BLEU R-L BS (1-10) (148.5) LLMScore Tokens # DC # (5.2) (437.6) (1-10) 17.7 23.4 19.6 21.0 27.3 31.0 20.1 19.7 21.6 20.2 31.2 13.8 9.7 16.6 31.1 31.9 28.3 30.6 35.7 38.2 28.4 27.5 29.4 27.4 51. 27.8 22.9 32.0 89.3 89.6 89.0 89.5 90.2 90.4 89.2 88.9 89.0 89.4 90.4 87.5 83.6 87.9 8.42 8.05 8.38 8.33 8.18 7.98 7.47 7.64 7.91 7.49 7. 7.72 7.58 8.51 271.4 218.1 246.1 326.0 209.2 226.5 457.3 193.3 219.3 361.7 316.4 642.3 802.5 1126.6 18.9 20.3 19.1 10.0 15.1 16.0 8.1 15.2 14.7 7.2 16. 16.6 11.7 12.6 26.5 28.3 26.8 17.1 22.3 23.8 13.9 22.6 23.2 11.7 34.4 29.3 32.4 25.8 88.8 88.4 89.0 89.2 89.2 88.1 88.4 87.3 87.1 88.0 89. 71.4 69.0 71.9 6.63 6.37 7.45 6.30 5.67 5.67 3.86 5.98 5.55 3.42 4.81 7.18 4.16 7.10 647.8 478.1 691.7 3790.9 3511.3 2294.4 6287.9 582.3 511.9 6543.1 5121. 1302.9 6096.9 4997.7 6.5 6.5 14.9 69.1 45.7 28.3 90.8 3.3 6.4 96.3 47.7 19.1 54.1 41.8 Table 7: Main results of generation tasks (4-1 and 4-2). The best-performing model in each task is in darker blue , and the second best is in lighter blue . R-L refers to ROUGE-L, BS refers to BERTScore, Tokens # denotes the average number of tokens in the generated text, and DC # indicates the average number of generated dependent claims. related laws and technical frameworks. MoZi-qwen outperforms the two law-oriented models, DISCLawLLM and Hanfei, but still lags behind the general-purpose model in the same series, Qwen2.5-7Bit, by 3.1%. These results reveal that while vertical field models often claim superior performance in their respective real-world applications, they consistently demonstrate degradation in performance on domain-specific benchmarks, trend also observed in previous benchmarks [52, 16, 26]. These findings highlight the need for domain-specific models to adopt better methods for learning domain knowledge and enhancing capabilities, while also ensuring the preservation of general capabilities, with the IP field being no exception. 4.4.3 Disparity between Chat Model and Reasoning Model Beyond chat models, we conduct experiments on three reasoning models, particularly the powerful DeepSeek-R1 and QwQ-32B. Neither of these two models achieves the best overall score, but they demonstrate superior performance in the logical reasoning tasks within IPBench. Specifically for Task 2-3, which involves compensation calculation, DeepSeek-R1 outperforms the best chat model by 5.7%. This task requires the model to possess strong mathematical capabilities in addition to knowing legal requirements. These findings underscore the need for models to balance System 1 and System 2 capabilities, particularly when addressing in-domain tasks. 4.4.4 Analysis on Generation and Classification Task Lack of Fine-Grained, Interpretable Automatic Evaluation for Generation Tasks. For these two generative tasks, there is lack of fine-grained, interpretable automatic evaluation methods to provide more reliable results. Specifically for Task 4-2, traditional metrics such as BLEU, ROUGE-L, and BERTScore are limited in their effectiveness and exhibit low consistency. To address this issue, we adopt an LLM-as-a-judge approach with four fine-grained dimensions, inspired by PatentEvals error taxonomy, and introduce LLMScore for more reliable evaluation. As shown in Table 22, LLMScore demonstrates significantly higher consistency with human judgments than other automatic metrics, which is reflected in its higher Kendall, Spearman, and Pearson correlation coefficients, and lower p-values. Disaster in IPC/CPC Exact Match Performance. For the IPC/CPC classification task, DeepSeekR1 significantly outperforms all other models with an Exact Match score of 10.8%, while DeepSeekV3 achieves 9.5%, with some models scoring as low as 0.0%. As the match level deepens, different parts correspond to distinct technical fields and their subfields, increasing the difficulty and leading to lower performance from Section, Class, and Subclass to Exact Match. As foundational applications in IP, especially for patent management, these two classification systems represent LLMs understanding of the entire IP classification system and fine-grained IP text analysis. 4.4.5 Analysis on Different Prompt Settings (a) (b) Figure 4: (a) Performance under different prompt settings. (b) Error distribution of GPT-4o-minis responses. Analysis of Few-shot As shown in Figure 4a and Appendix G.1.1, the performance of GPT-4omini, Qwen2.5-7B-it, and Gemma-9B-it generally improves with an increasing number of shots on IPBench. However, Llama3.1-8B-it does not exhibit similar upward trend. This finding aligns with previous research [26, 52], which demonstrates that different models are affected differently by few-shot, and that few-shot prompting may not be an ideal method for injecting domain knowledge. Analysis of Chain-of-Thought. As shown in Figure 4a, all remaining models exhibit slight decrease in performance, ranging from 0.4% to 0.6%. Upon deeper analysis of the error cases, we observe that the CoT prompt leads models to generate not only the final answer but also reasoning trajectory. This aligns with findings from previous researches [61, 10], which reveal that CoT, as an explicit reasoning paradigm, may conflict with the inherent intuition of LLMs. It can even lead to overthinking, particularly in domains or tasks that rely more on memorization than reasoning. Furthermore, this aligns with the observation that large reasoning models do not outperform chat models on IPBench, even though they perform longer reasoning during inference. 4.4.6 Error Analysis We randomly selected 300 incorrect responses across all tasks from GPT-4o-mini under the CoT setting for error analysis. These samples are then meticulously examined by expert annotators. As illustrated in Figure 4b, we classify the error into 7 types: Consistency error, Hallucination error, Reasoning error, Refusing error, Priority error, Mathematical error and Obsolescence error, where Reasoning Error is the most common error type, accounting for 33%. This error analysis is crucial for gaining deeper insights into the models capabilities in the IP domain and for revealing potential directions for future research. More details of error analysis are in Appendix H."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce the first comprehensive IP task taxonomy and present IPBench, bilingual benchmark comprising 20 tasks and 10,374 test instances, covering both technical-legal and generationcomprehension evaluations. Our experiments show that even the best-performing model, DeepSeekV3, achieves only 75.8% score. We observe that current models, including IP-oriented ones, still lag significantly behind powerful closed-source models, highlighting the need for improved domain-specific learning approaches. Our extensive performance analysis, error analysis and case study provide comprehensive insight in models IP knowledge and capabilities. We are committed to continuously expanding IPBench to foster advancements in both the IP domain and NLP research, providing meaningful guidance for the integration of LLMs into specialized vertical fields."
        },
        {
            "title": "Limitation",
            "content": "We acknowledge several existing limitations and plan to address them in our future research. Firstly, due to the legal nature of intellectual property, IPBench mainly focuses on the legal frameworks of the United States and mainland China. Incorporating IP law frameworks from more countries and languages could make the benchmark more broadly applicable and useful. Secondly, considering the costs, we select four reasoning models for our evaluation and do not include OpenAIs o1 [18] and o3 series due to their high expense, which could be revisited in future expansions. Thirdly, the IP field is still novel area in LLM research, and currently, MoZi is the only open-sourced IP-oriented model available. Due to this limitation, we are only able to benchmark MoZi in our experiments. Future work could incorporate more IP-oriented models as they become available. Lastly, while we propose LLMScore, highly consistent and fine-grained automatic evaluation metric grounded in the LLM-as-a-judge framework, further work is needed to design evaluation methods that are less biased and more robust across diverse settings."
        },
        {
            "title": "Ethics Statement",
            "content": "In developing IPBench, all data are collected exclusively from open and publicly available sources. We strictly adhered to all relevant copyright and licensing regulations. Any data originating from websites or platforms that prohibit copying, redistribution, or automated crawling are explicitly excluded from use. Furthermore, we confirm that all data are used solely for academic and research purposes, and not for any commercial applications. We are committed to upholding responsible data usage and transparency in our research practices. Future updates of IPBench will continue to follow the same principles and remain fully open to academic scrutiny and community feedback."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Our team members contribute to the development of IPBench from the following perspectives: Data Annotation Management, Data Annotation, Data Quality Review, Model Evaluation, Result Analysis and Paper Writing. Project Leader: Qiyao Wang, SIAT & DUT Outstanding Contributors: Huaren Liu, DUT Guhong Chen, SIAT Hongbo Wang, DUT Zhifei Qin, DUT Minghui Zhu, DUT Yilin Yue, DUT Yihang Wu, DUT Jiayan Li, DUT Shiqiang Wang, DUT Linwei Li, DUT Contributors : Ziqiang Liu, SIAT Longze Chen, SIAT Run Luo, SIAT Jiaming Li, SIAT Lei Zhang, SIAT Liyang Fan, SIAT Yanchao Jian, DUT Sirui Li, DUT Mingyi Bu, DUT Hanyu Kang, DUT Corresponding Authors: Yuan Lin, DUT Shiwen Ni, SIAT Min Yang, SIAT & SUAT We thank for all those peoples contribution on this project."
        },
        {
            "title": "References",
            "content": "[1] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Made laine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Is abella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Jo hannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Ryan Kiros, Matthew Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Li, Rachel Lim, Molly Lin, Stephanie Lin, Ma teusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen OKeefe, Jakub W. Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alexandre Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Pondé de Oliveira Pinto, Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas A. Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll L. Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. 2023. [2] Zilong Bai, Ruiji Zhang, Linqing Chen, Qijun Cai, Yuan Zhong, Cong Wang, Yan Fang, Jie Fang, Jing Sun, Weikuan Wang, et al. Patentgpt: large language model for intellectual property. arXiv preprint arXiv:2404.18255, 2024. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge 11 distillation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 23182335, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jiong Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Zehui Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. [6] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Bap tiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cantón Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guanglong Pang, 12 Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Ro main Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollár, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. [8] EPO. Guidelines for examination in the european patent office. page 1 volume ;, 1994. [9] Caspar Fall, Atilla Törcsvári, Karim Benzineb, and Gabor Karetka. Automated categorization in the international patent classification. In Acm Sigir Forum, volume 37, pages 1025. ACM New York, NY, USA, 2003. [10] Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. Missing premise exacerbates overthinking: Are reasoning models losing critical thinking skill?, 2025. [11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [12] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems, 36:4412344279, 2023. [13] Wanwei He, Jiabao Wen, Lei Zhang, Hao Cheng, Bowen Qin, Yunshui Li, Feng Jiang, Junying Chen, Benyou Wang, and Min Yang. Hanfei-1.0. https://github.com/siat-nlp/HanFei, 2023. [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations. [15] Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, et al. E-eval: comprehensive chinese k-12 education evaluation benchmark for large language models. In ACL (Findings), 2024. [16] Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, and Min Yang. E-EVAL: comprehensive Chinese k-12 education evaluation benchmark for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 77537774, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [17] OpenAI Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mkadry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alexander Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alexandre Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew 14 Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, B. Ghorbani, Ben Leimberger, Ben Rossen, Benjamin Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Chris Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mély, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Phong Duc Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Hai-Biao Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Pondé de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian D. Kivlichan, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Cihangir Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub W. Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Ryan Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quiñonero Candela, Joe Beutler, Joe Landers, Joel Parish, Jo hannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Joshua Gross, Josh Kaplan, Josh Snyder, Josh Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Ouyang Long, Louis Feuvrier, Lu Zhang, Lukasz Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Made laine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Ma teusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Ali Yatbaz, Mengxue Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Mina Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nickolas Stathas, Nick Turley, Nikolas A. Tezak, Niko Felix, Nithanth Kudige, Nitish Shirish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Phil Tillet, Prafulla Dhariwal, Qim ing Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Raphael Gontijo Lopes, Raul Puri, Reah Miyara, Reimar H. Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Ramilevich Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal A. Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peter15 son, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card. ArXiv, abs/2410.21276, 2024. [18] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [19] Fengqing Jiang. Identifying and mitigating vulnerabilities in llm-integrated applications. Masters thesis, University of Washington, 2024. [20] Lekang Jiang and Stephan Goetz. Natural language processing in patents: survey. arXiv preprint arXiv:2403.04105, 2024. [21] Lekang Jiang, Pascal Scherz, and Stephan Goetz. Patent-cr: dataset for patent claim revision. ArXiv, abs/2412.02549, 2024. [22] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. [23] Valentin Knappich, Simon Razniewski, Anna Hätty, and Annemarie Friedrich. Pap2pat: Towards automated paper-to-patent drafting using chunk-based outline-guided generation. arXiv preprint arXiv:2410.07009, 2024. [24] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [25] Jieh-Sheng Lee and Jieh Hsiang. Patent classification by fine-tuning bert language model. World Patent Information, 61:101965, 2020. [26] Haitao Li, You Chen, Qingyao Ai, WU Yueyue, Ruizhe Zhang, and LIU Yiqun. Lexeval: comprehensive chinese legal benchmark for evaluating large language models. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [27] Haitao Li, Jiaying Ye, Yiran Hu, Jia Chen, Qingyao Ai, Yueyue Wu, Junjie Chen, Yifan Chen, Cheng Luo, Quan Zhou, et al. Casegen: benchmark for multi-stage legal case documents generation. arXiv preprint arXiv:2502.17943, 2025. [28] Shaobo Li, Jie Hu, Yuxin Cui, and Jianjun Hu. Deeppatent: patent classification with convolutional neural networks and word embedding. Scientometrics, 117(2):721744, 2018. [29] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: survey on generative information retrieval. ACM Trans. Inf. Syst., March 2025. Just Accepted. [30] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. CoRR, abs/2211.09110, 2022. [31] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. [32] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Geval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, Singapore, December 2023. Association for Computational Linguistics. 16 [33] Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Wang, Yuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, et al. Ii-bench: An image implication understanding benchmark for multimodal large language models. arXiv preprint arXiv:2406.05862, 2024. [34] Qiang Lu, Amanda F. Myers, and Scott Beliveau. USPTO Patent Prosecution Research Data: Unlocking Office Action Traits. Technical report, United States Patent and Trademark Office (USPTO), 2017. [35] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. [36] Shiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen Zhang, Ruifeng Xu, Xiaojun Chen, Chengming Li, and Xiping Hu. Mozip: multilingual benchmark to In Proceedings of the 2024 Joint evaluate large language models in intellectual property. International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1165811668, 2024. [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. [38] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Tung Nguyen, Daron Anderson, Imad Ali Shah, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Jaeho Lee, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, Robert Gerbicz, JohnClark Levin, Serguei Popov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Mstyslav Kazakov, Geoff Galgon, Johannes Schmitt, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Antrell Cheatom, Zachary Giboney, Gashaw M. Goshu, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, Jennifer Zampese, John Wydallis, John B. Wydallis, Ryan G. Hoerr, Mark Nandor, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Jungbae Nam, Edwin Taylor, Jun Jin, Gautier Abou Loume, Hangrui Cao, Alexis Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Aras Bacho, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Alexei Kopylov, Johannes Veith, Eric Singer, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Ameya Prabhu, Longke Tang, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Joshua Robinson, Aleksandar Mikov, Julien Guillod, Yuqi Li, Ben Pageler, Joshua Vendrow, Vladyslav Kuchkin, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Andrew Gritsevskiy, Dakotah Martinez, Nick Crispino, Dimitri Zvonkine, Natanael Wildner Fraga, Saeed Soori, Ori Press, Henry Tang, Julian Salazar, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, T. Ryan Rogers, Wenjin Zhang, Ross Finocchio, Bikun Li, Jinzhou Yang, Arun Rao, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Ariel Ghislain Kemogne Kamdoum, Tad Hogg, Alvin Jin, Carlo Bosio, Gongbo Sun, Brian Coppola, Haline Heidinger, Rafael Sayous, Stefan Ivanov, Joseph Cavanagh, Jiawei Shen, Joseph Marvin Imperial, Philippe Schwaller, Shaipranesh Senthilkuma, Andres Bran, Andres Algaba, Brecht Verbeken, Kelsey Van den Houte, Lynn Van Der Sypt, David Noever, Lisa Schut, Ilia Sucholutsky, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Shankar Sivarajan, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Jennifer Sandlin, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Felipe Meneguitti Dias, Tobias Kreiman, Kaivalya Rawal, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Jeremy Nguyen, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Sergey Ivanov, Rafał Poswiata, Chenguang Wang, Daofeng Li, Donato Crisostomi, Ali Dehghan, Andrea Achilleos, John Arnold Ambay, Benjamin Myklebust, Archan Sen, David Perrella, Nurdin Kaparov, Mark Inlow, Allen Zang, Kalyan Ramakrishnan, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Dan Bar Hava, Aleksey Kuchkin, Robert Lauff, David Holmes, Frank Sommerhage, Anji Zhang, 17 Richard Moat, Keith Schneider, Daniel Pyda, Zakayo Kazibwe, Mukhwinder Singh, Don Clarke, Dae Hyun Kim, Sara Fish, Veit Elser, Victor Efren Guadarrama Vilchis, Immo Klose, Christoph Demian, Ujjwala Anantheswaran, Adam Zweiger, Guglielmo Albani, Jeffery Li, Nicolas Daans, Maksim Radionov, Václav Rozhoˇn, Vincent Ginis, Ziqiao Ma, Christian Stump, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Marco Piccardo, Niv Cohen, Virendra Singh, Josef Tkadlec, Paul Rosu, Alan Goldfarb, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Aline Menezes, Arkil Patel, Zixuan Wang, Jamie Tucker-Foltz, Jack Stade, Declan Grabb, Tom Goertzen, Fereshteh Kazemi, Jeremiah Milbauer, Abhishek Shukla, Hossam Elgnainy, Yan Carlos Leyva Labrador, Hao He, Ling Zhang, Alan Givré, Hew Wolff, Gözdenur Demir, Muhammad Fayez Aziz, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Elliott Thornley, Robin Zhang, Jiayi Pan, Antonio Terpin, Niklas Muennighoff, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Jainam Shah, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Andrew Ho, Shaul Barkan, Jiaqi Wang, Martin Stehberger, Egor Kretov, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Zaki Hossain, Ido Akov, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Micah Carroll, Orr Paradise, Jan Hendrik Kirchner, Stefan Steinerberger, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Paolo Giordano, Philipp Petersen, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Antonella Pinto, Shreyas Verma, Prashant Joshi, Eli Meril, Zheng-Xin Yong, Allison Tee, Jérémy Andréoletti, Orion Weller, Raghav Singhal, Gang Zhang, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Hamid Mostaghimi, Kunvar Thaman, Qijia Chen, Tran Quoc Khánh, Jacob Loader, Stefano Cavalleri, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Jonathan Roberts, William Alley, Kunyang Sun, Ryan Stendall, Max Lamparth, Anka Reuel, Ting Wang, Hanmeng Xu, Pablo Hernández-Cámara, Freddie Martin, Thomas Preu, Tomek Korbak, Marcus Abramovitch, Dominic Williamson, Ida Bosio, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Maria Inês S. Nunes, Yibo Jiang, Saiful Bari, Peyman Kassani, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Guillaume Douville, Daniel Tordera, George Balabanian, Earth Anderson, Lynna Kvistad, Alejandro José Moyano, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Isaac C. McAlister, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Ronald Clark, Sherwin Abdoli, Tim Santens, Harrison Wang, Evan Chen, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, VinhKha Le, Noam Kolt, Niels Mündler, Avi Semler, Emma Rodman, Jacob Drori, Carl Fossum, Luk Gloor, Milind Jagota, Ronak Pradeep, Honglu Fan, Tej Shah, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâca, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Siranut Usawasutsakorn, Mohammadreza Mofayezi, Alexander Piperski, Marc Carauleanu, David K. Zhang, Kostiantyn Dobarskyi, Dylan Ler, Roman Leventov, Ignat Soroko, Thorben Jansen, Scott Creighton, Pascal Lauer, Joshua Duersch, Vage Taamazyan, Dario Bezzi, Wiktor Morak, Wenjie Ma, William Held, Tran Ðuc Huy, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Hossein Shahrtash, Edson Oliveira, Joseph W. Jackson, Daniel Espinosa Gonzalez, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Emilien Duc, Bita Golshani, David Stap, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Lukas Lewark, Miguel Orbegozo Rodriguez, Mátyás Vincze, Dustin Wehr, Colin Tang, Shaun Phillips, Fortuna Samuele, Jiang Muzhen, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Claire Sparrow, Rayner Hernandez Perez, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Samuel Albanie, Will Cai, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Jasdeep Sidhu, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Brian Weber, Harsh Kumar, Tong Jiang, Arunim Agarwal, Chiara Ceconello, Warren S. Vaz, Chao Zhuang, Haon Park, Andrew R. Tawfeek, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Shreen Gul, Gunjan Chhablani, Zhehang Du, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, 18 Renas Bacho, Florencia de la Rosa, Xiuyu Li, Guillaume Malod, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yigit Yalın, Gbenga Daniel Obikoya, Luca Arnaboldi, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Pierre Clavier, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Denis Peskoff, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Olle Häggström, Emil Verkama, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Yiyang Fan, Gabriel Poesia Reis Silva, Linwei Xin, Yosi Kratish, Jakub Łucki, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Justin Xu, Kevin Joseph Scaria, Freddie Vargus, Farzad Habibi, Long, Lian, Emanuele Rodolà, Jules Robins, Vincent Cheng, Tony Fruhauff, Brad Raynor, Hao Qi, Xi Jiang, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Sarah Hoback, Rodrigo De Oliveira Pena, Glen Sherman, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Sandra Mendoza, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Ashley Cartwright, Daphiny Pottmaier, Omid Taheri, David Outevsky, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Sam Ali, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Sk Md Salauddin, Murat Islam, Juan Gonzalez, Josh Ducey, Maja Somrak, Vasilios Mavroudis, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Anil Radhakrishnan, Antoine Jallon, I. M. J. McInnis, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Javier Gimenez, Roselynn Grace Montecillo, Russell Campbell, Asankhaya Sharma, Khalida Meer, Xavier Alapont, Deepakkumar Patil, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Sergei Bogdanov, Sören Möller, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Innocent Enyekwe, Ragavendran V, Zienab EL-Wasif, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Song Bian, John Lai, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Alex Hoover, Joseph McGowan, Tejal Patwardhan, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanitys last exam, 2025. [39] Florina Piroi and Allan Hanbury. Evaluating information retrieval systems on european patent data: The clef-ip campaign. In Current Challenges in Patent Information Retrieval, pages 113142. Springer, 2017. [40] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [42] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [43] Gemma Team Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, Johan Ferret, Peter Liu, Pouya Dehghani Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Boxi Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Christoper A. Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, D. Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, 19 Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna KlimczakPlucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost R. van Amersfoort, Josh Gordon, Josh Lipschultz, Joshua Newlan, Junsong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, L. Sifre, Lena Heuermann, Leti cia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Gorner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Peng chong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sebastien M. R. Arnold, Se bastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomás Kociský, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray Kavukcuoglu, Clément Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at practical size. ArXiv, abs/2408.00118, 2024. [44] Eva Sharma, Chen Li, and Lu Wang. Bigpatent: large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 22042213, 2019. [45] Cinthia Souza, Matheus Santos, Magali RG Meireles, and Paulo EM Almeida. Using summarization techniques on patent database through computational intelligence. In Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 36, 2019, Proceedings, Part II 19, pages 508519. Springer, 2019. [46] Mirac Suzgun, Luke Melas-Kyriazi, Suproteem Sarkar, Scott Kominers, and Stuart Shieber. The harvard uspto patent dataset: large-scale, well-structured, and multi-purpose corpus of patent applications. Advances in neural information processing systems, 36:5790857946, 2023. [47] M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. [48] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [49] USPTO. FY 2024 Agency Financial Report. Technical report, 2024a. [50] USTPO. Manual of patent examining procedure. pages 4 v. (looseleaf), 2024b. This resource was extracted from USPTO.gov. [51] Juanyan Wang, Sai Krishna Reddy Mudhiganti, and Manali Sharma. Patentformer: novel method to automate the generation of patent applications. In Proceedings of the 2024 Conference 20 on Empirical Methods in Natural Language Processing: Industry Track, pages 13611380, 2024. [52] Qiyao Wang, Jianguo Huang, Shule Lu, Yuan Lin, Kan Xu, Liang Yang, and Hongfei Lin. Ipeval: bilingual intellectual property agency consultation evaluation benchmark for large language models. arXiv preprint arXiv:2406.12386, 2024. [53] Qiyao Wang, Shiwen Ni, Huaren Liu, Shule Lu, Guhong Chen, Xi Feng, Chi Wei, Qiang Qu, Hamid Alinejad-Rokny, Yuan Lin, et al. Autopatent: multi-agent framework for automatic patent generation. arXiv preprint arXiv:2412.09796, 2024b. [54] Norman Webb. Depth-of-knowledge levels for four content areas. Language Arts, 28(March):19, 2002. [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [56] WIPO. What is intellectual property? page 1 PDF (24 pages) :, 2020a. [57] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. [58] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, and Zhongyu Wei. Disc-lawllm: Fine-tuning large language models for intelligent legal services, 2023. [59] Shengbin Yue, Shujun Liu, Yuxuan Zhou, Chenchen Shen, Siyuan Wang, Yao Xiao, Bingxuan Li, Yun Song, Xiaoyu Shen, Wei Chen, et al. Lawllm: Intelligent legal system with legal reasoning In International Conference on Database Systems for Advanced and verifiable retrieval. Applications, pages 304321. Springer, 2024. [60] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675, 2019. [61] Tianshi Zheng, Yixiang Chen, Chengxi Li, Chunyang Li, Qing Zong, Haochen Shi, Baixuan Xu, Yangqiu Song, Ginny Y. Wong, and Simon See. The curse of cot: On the limitations of chain-of-thought in in-context learning, 2025. [62] You Zuo, Kim Gerdes, Éric Clergerie, and Benoît Sagot. PatentEval: Understanding errors in patent generation. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 26872710, Mexico City, Mexico, June 2024. Association for Computational Linguistics."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 IPBench"
        },
        {
            "title": "3.1 Task Taxonomy .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Data Collection, Processing and Annotation . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 Feature of IPBench .",
            "content": ". . . . . . 4 Benchmarking Results . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.1 Evaluated Models .\n4.2 Evaluation Setup .\n.\n4.3 Main Results\n.\n.\n4.4 Analysis .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Model Performance across Different Languages . . . . . . . . . . . . . . . 4.4.2 Disparity between IP-oriented and General-purpose Models . . . . . . . . 4.4.3 Disparity between Chat Model and Reasoning Model . . . . . . . . . . . . 4.4.4 Analysis on Generation and Classification Task . . . . . . . . . . . . . . . 4.4.5 Analysis on Different Prompt Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.6 Error Analysis . 5 Conclusion Limitation Ethics Statement Acknowledgments and Disclosure of Funding Reference Data Availability Taxonomy and Task Details . B.1 Taxonomy Details . . . B.2 Task Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Information Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 B.2.2 Logical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.3 Discriminant Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.4 Creative Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Annotation and Examination Protocol C.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Annotation and Examination Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . More Details about Data Statics D.1 Data Language Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Intellectual Property Mechanisms Distribution . . . . . . . . . . . . . . . . . . . . D.3 IPC and CPC Classification Distribution . . . . . . . . . . . . . . . . . . . . . . . D.4 Text Length Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Multi-Choice Question Option Count Distribution. . . 1 3 4 4 5 5 6 6 6 7 7 7 7 8 8 9 9 9 10 10 10 10 23 23 23 24 24 26 27 27 28 28 28 28 29 29 29 29 32 33 33 37 37 38 38 38 38 41 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Prompts . E.1 Zero-shot Prompt E.2 Few-shot Prompt . E.3 Chain-of-Thought Prompt . . . . . . Metrics More Results G.1 Overall Results . . . . . G.1.1 Few-shot Results . . F.1 Multi-Choice Question Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . IPC/CPC Classification Task Metric . . . . . . . . . . . . . . . . . . . . . . . . . F.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Generation Task Metric . G.1.2 Chain-of-Thought Results G.2 Chinese Questions Results G.2.1 Zero-shot Results . . G.2.2 Few-shot Results . . G.2.3 Chain-of-Thought Results G.3 English Questions Results G.3.1 Zero-shot Results . . G.3.2 Few-shot Results . . G.3.3 Chain-of-Thought Results G.4 LLM-as-a-judge Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 43 43 43 43 44 44 44 44 47 51 66 More Details about Error Analysis Data Examples Case Study"
        },
        {
            "title": "A Data Availability",
            "content": "You can find the available code, data, and additional materials for IPBench at the following website: Website: https://IPBench.github.io/ Code: https://github.com/IPBench/IPBench Data: https://huggingface.co/datasets/IPBench/IPBench"
        },
        {
            "title": "B Taxonomy and Task Details",
            "content": "B.1 Taxonomy Details Information Processing. In the first level of the taxonomy, we replace Recall and Reproduction with Information Processing, which encompasses the legal concepts, clauses, evolution, and typical case knowledge of various IP mechanisms. It also includes real-world applications such as patent classification, IP element identification, and process guidance, requiring models to memorize different concepts, along with the procedures executed in real-world scenarios. Our expert-annotated memorytype tasks are inspired by those in LexEval [26] but differ significantly from it across various IP mechanisms, using accuracy as the evaluation metric. The IP element identification task focuses on identifying key elements in case, such as claim coverage in patent infringement. Previous work has made significant progress in patent classification [28, 25, 9], which has been adopted by IP offices in many countries. However, these models are task-specific and lack the strong generalization ability of LLMs. Our patent classification task consists of two types: International Patent Classification (IPC) and Cooperative Patent Classification (CPC). We aim to accomplish these tasks within single model, enabling it to distinguish both differences within the same classification system and across different classification systems. We adopt the top-prediction scheme, following Fall et al. [9], to compare the top predicted category with the label for an exact match [30] in the main IPC symbol, and CPC. This setup increases the task difficulty for LLMs, requiring models to be familiar with classification rules. Logical Reasoning. At the second level of the taxonomy, we focus on examining models ability to apply memorized concepts and utilize logical reasoning to provide insights into both text analysis and mathematical calculations. One of the important roles of IP is to protect inventors rights from infringement. Therefore, we define the tasks of Infringement Behavior Determination and Compensation Calculation. To complete these two tasks, models need to analyze the background of cases to identify infringement behavior and apply relevant laws to determine the appropriate penalties. Apart from the legal aspect, we introduce Patent Technology Forecasting, Patent Valuation, and Fact Checking to evaluate models ability in information mining and conditional reasoning. As we mentioned, our IPBench consists of different IP mechanisms. We specifically introduce novel task called Trade Secret Requirements, which differs from Infringement Behavior Determination. This task focuses on trade secret rights, requiring the model to determine whether situation meets the confidentiality requirements of trade secrets. 23 Discriminant Evaluation. At this level, we focus on evaluating models understanding of IP in-domain texts, particularly patent documents, as well as their ability to perform discriminative tasks such as rights attribution. AAs an important part of IP management activities, as mentioned before, IP offices face massive volume of patent applications. Determining the quality of an application requires assessing its patentability based on four aspects outlined in the Manual of Patent Examining Procedure (MPEP) [50, 8]: utility, non-obviousness, statutory subject matter, and novelty. We aim to evaluate whether current LLMs can assist patent examiners in reducing time costs within single model. To achieve this, we introduce three tasks: Patent Document Proofreading, Patent Validity Identification, and Patent Match. LLMs output mechanisms are not well-suited for retrieval-based approaches, and Li et al.[29] introduced novel perspective on matching documents through generative approach. Based on this insight, the Patent Match task draws inspiration from the corresponding task in MoZIP [36]. We sample 1000 datapoints from MoZIP in both English and Chinese and require expert annotation for detailed examination. Apart from the evaluation of in-domain text, we introduce one real-world common task for evaluating models discrimination ability: Rights Attribution Analysis. The Rights Attribution Analysis task requires the model to infer the legal rights holder of specific intellectual property based on the context of IP creation, legal agreements such as contract terms and confidentiality agreements, and judicial precedents within the legal framework. At last, we extend HUPDs [46] Patent Acceptance Prediction task into more comprehensive Patent Application Examination task, leveraging the USPTO Office Action Dataset [34]. In this task, the model is required to determine whether given patent application should be accepted or rejected. Additionally, we provide stepwise examination actions for an interpretable examination process, which can be used in future work to construct reliable examination system. Creative Generation. At the final level of our IPBench, we focus on evaluating the models ability to extract critical information, convert between different linguistic styles, and generate new content. Previous works such as BigPatent [44], Patent-CR [21], and PatentEval [62] focus on specific types of content for patent generation. We draw inspiration from some of their tasks and extend their scope to include both Chinese and English. All the data used in Abstract Generation, Claim Generation, Sequential Claim Generation are sourced from the latest patents, ensuring no data leakage and distinguishing our dataset from existing ones. At last, we introduce novel task called DesignAround Solution Generation, which evaluates whether models can generate innovative solutions that avoid duplication of existing ones. This capability is crucial in strategic patent planning. Given the distinct characteristics of the tasks at this level, we use accuracy as the metric for Language Simplification and Design-Around Solution Generation. For the other three generative tasks, we note that PatentEval [62] provides an LLM-based evaluation method for claim generation. However, this approach relies on the assumption that the employed LLMs are sufficiently capable. Moreover, for other types of content, no superior evaluation method currently exists. We adopt combination of automated evaluation and human assessment. For automated evaluation, we use n-gram-based metrics such as BLEU [37] and ROUGE [31], along with the semantic metric BERTScore [60], and analyze their consistency with human evaluation to enhance result interpretation. We will explore better evaluation methods in future work, especially for patent generation, which involves complex technical and legal content. It is important to note that the abstract generation evaluation in BigPatent [44] is based on converting only the first 400 words of patents description into an abstract, limitation imposed by the context length of language models at the time. In our IPBench, we evaluate models on their ability to generate abstracts from the entire description, assessing their long-context understanding and summarization capabilities for complex patent documents. B.2 Task Definition B.2.1 Information Processing Task 1-1: Legal Concept Memory Legal Concept Memory refers to the ability to precisely memorize and recall foundational definitions within the intellectual property domain. These definitions, such as those of patents, copyrights, trademarks, and trade secrets, are grounded in authoritative legal frameworks and scholarly interpretations that constitute the foundation of intellectual property law. When given concept name or contextual description, LLMs must retrieve the precise legal 24 definition, scope, and jurisdictional boundaries as codified in statutes such as Chinas Patent Law and Copyright Law, as well as relevant international agreements, purely from their intrinsic knowledge without relying on external databases or tools. Task 1-2: Legal Clause Memory Legal Clause Memory requires the precise memorization and retrieval of specific legal provisions, including their exact article numbers and textual content. These clauses, drawn from authoritative legal codes such as Chinas Criminal Law, Civil Code, and Intellectual Property Law, define rights, obligations, penalties, or procedural rules within statutory frameworks. When provided with an article number (e.g., Article 217 of Chinas Copyright Law) or contextual description of legal scenario, LLMs must accurately recall the verbatim wording and scope of the corresponding clause. Task 1-3: Legal Evolution Legal Evolution refers to the ability to accurately memorize and analyze the revision history of legal texts, including the tracking of changes in specific clauses across different versions of statutes, regulations, or international treaties. This capability requires models to retain knowledge of amendments, such as updates to Chinas Patent Law, and to systematically compare the wording, scope, and intent of clauses before and after revisions. Task 1-4: Typical Case Memory Typical Case Memory requires the memorization of landmark intellectual property cases, including their judicial outcomes, factual details, and legal reasoning. These cases, such as high-profile patent disputes, copyright infringement rulings, or trademark opposition decisions, establish precedents that shape the interpretation and enforcement of IP law. When provided with case name, jurisdiction, or factual scenario, models must accurately recall the judgment summary, key legal arguments, cited statutes, and contextual factors, without using an external database or retrieval tool. Task 1-5: Patent Classification Patent Classification involves the capability to automatically assign International Patent Classification (IPC) or Cooperative Patent Classification (CPC) codes based on the technical content of patent documents. This task requires models to analyze patent texts, including titles and abstractsto identify the core inventions, technological domains, and functional features, then map them to hierarchical classification codes.This task evaluates the models capabilities across three hierarchical levels: Section, Class, and Subclass. distribution table for the section level as shown in Table 8. Section Content D H Human Necessities Performing Operations; Transporting Chemistry; Metallurgy Textiles; Paper Fixed Constructions Mechanical Engineering; Lighting; Heating; Weapons; Blasting Physics Electricity Table 8: International Patent Classification (IPC) Sections Task 1-6: IP Element Identification IP Element Identification entails detecting and categorizing intellectual property componentssuch as patent claims, trademark-protected assets, copyrighted material, or trade secret identifierswithin legal disputes, technical specifications, or commercial contracts. This task requires models to analyze textual data to identify legally protected innovations, distinctive brand assets, and ownership claims, while ensuring alignment with statutory definitions. Task 1-7: Process Guidance Process Guidance focuses on delivering structured knowledge of intellectual property application procedures, covering legal requirements, technical documentation standards, and jurisdictional workflows. This task requires models to provide step-by-step guidance on processes such as conducting patent or trademark searches, drafting application materials, navigating submission procedures, and ensuring compliance with examination regulations. 25 Section Content"
        },
        {
            "title": "Human Necessities\nOperations and Transport\nChemistry and Metallurgy\nTextiles and Paper\nFixed Constructions\nMechanical Engineering and Lighting\nPhysics\nElectricity\nEmerging Technologies",
            "content": "Table 9: Cooperative Patent Classification (CPC) Sections B.2.2 Logical Reasoning The Logical Reasoning level is designed to evaluate the capability of large language models (LLMs) to perform multi-dimensional legal and technical reasoning within the complex framework of intellectual property (IP) law and textual analysis. This layer tests the models ability to analyze, interpret, and apply intersecting legal rules. It focuses on assessing whether models can synthesize statutory provisions, case law precedents, and technical domain knowledge to reach legally sound conclusions such as identifying infringement risks, resolving conflicts between overlapping rights, or predicting litigation outcomes based on factual scenarios. Task 2-1: Patent Technology Forecasting Patent Technology Forecasting involves analyzing the technical features of patents such as claims, innovation summaries, and domain-specific terminology to predict future technological trajectories and potential application areas. This task requires models to identify emerging trends, interconnected technical fields, and latent innovation pathways within patent datasets, enabling the projection of how core inventions might evolve or intersect with adjacent industries. Task 2-2: Infringement Behavior Determination Infringement Behavior Determination focuses on identifying acts that constitute violations of intellectual property rights. It involves analyzing the legally protected scope of patents, copyrights, trademarks, or other IP types, and comparing them with suspected infringing products, services, or content to determine whether an intellectual property infringement has occurred. This task requires models to evaluate technical equivalence, trademark similarity, or substantial similarity in copyrighted works, while accurately applying the relevant statutory criteria to determine whether an intellectual property infringement has occurred. Task 2-3: Compensation Calculation Compensation Calculation focuses on determining statutory damages for intellectual property infringement by analyzing the severity, scope, and economic impact of the violation. This task requires models to perform mathematical reasoning and calculation, taking into account factors such as the rights holders actual losses, reasonable licensing fees, and statutory limits. Additionally, models must incorporate contextual elements such as the duration of infringement, geographic scope, and the presence of malicious intent to arrive at legally grounded and quantitatively sound compensation estimate. Task 2-4: Patent Valuation Patent Valuation entails evaluating the value trajectory of patent by synthesizing its technical merit, market viability, and legal robustness. This task requires models to analyze technical claims, market analysis reports, and legal histories to project trends such as value appreciation, obsolescence risks, or licensing potential. Task 2-5: Trade Secret Requirements Trade Secret Requirements assesses whether given scenario satisfies the legal criteria for trade secret protection under statutory frameworks such as Chinas Anti-Unfair Competition Law and the U.S. Defend Trade Secrets Act (DTSA). This task requires models to verify three core elements: the existence of secrecy, the presence of commercial value, and the implementation of reasonable confidentiality measures. 26 B.2.3 Discriminant Evaluation Task 3-1: Patent Document Proofreading Patent Document Proofreading involves identifying formatting deviations and logical inconsistencies within patent specifications, claims, and technical descriptions to ensure compliance with statutory drafting standards. This task requires models to detect issues such as mismatched section numbering, non-compliant claim dependencies, contradictory technical descriptions, and deviations from jurisdiction-specific filing guidelines. Task 3-2: Patent Validity Identification Patent Validity Identification involves assessing whether patent satisfies the statutory criteria of novelty, inventiveness (non-obviousness), and practical applicability (utility) by analyzing its technical disclosures in light of relevant prior art. This task requires models to evaluate patent texts, including claims and specifications, against existing technologies to determine if the invention is new, involves an inventive step, and has industrial applicability. Task 3-3: Patent Match Patent Match involves identifying the most relevant patents from candidate pool based on technical, legal, and contextual alignment with query patent. This task requires models to analyze technical features and semantic similarity to rank patents by relevance. This task is inspired by MoZIP [36]. Task 3-4: Rights Attribution Analysis Rights Attribution Analysis involves determining the legitimate rights holder in intellectual property ownership disputes by analyzing legal documents, contractual agreements, and contextual evidence. This task requires models to evaluate factors such as invention ownership under employment relationships, joint authorship claims in copyright cases, or trademark transfer agreements, while reconciling conflicting claims based on statutory frameworks. Task 3-5: Patent Application Examination Patent Application Examination involves conducting compliance reviews of patent documents to ensure adherence to statutory and administrative requirements. This task requires models to verify the accuracy, completeness, and legal sufficiency of patent applications, including claims, specifications, and drawings, against jurisdictional standards. Key checks include clarity of technical disclosure, consistency between claims and descriptions, proper support for embodiments, and alignment with formalities. The data for this task is sourced from the USPTO Office Action Dataset [34]. B.2.4 Creative Generation Task 4-1: Abstract Generation Abstract Generation assesses models ability to automatically extract core elements from intellectual property (IP) texts, such as patent claims, and synthesize them into concise, structured, and legally compliant summaries. This task requires models to distill technical innovations, legal scopes, and critical details while adhering to jurisdictional formatting rules and avoiding oversimplification that misrepresents legal or technical nuances. Task 4-2: Dependent Claim Generation Dependent Claim Generation involves automatically drafting legally compliant and technically precise dependent claims based on the core inventions described in patent disclosures. This task requires models to analyze technical descriptions and generate claims that refine or limit the scope of independent claims by incorporating additional technical features, while ensuring logical dependency and alignment with jurisdictional formalities. This task is inspired by PatentEval [62]. Task 4-3: Design-Around Solution Generation Design-Around Solution Generation focuses on creating non-infringing technical alternatives by analyzing existing patent claims and identifying opportunities to circumvent key protected elements. This task requires models to deconstruct patent claims and propose modifications that avoid literal or equivalent infringement, while maintaining technical feasibility."
        },
        {
            "title": "C Data Annotation and Examination Protocol",
            "content": "C.1 Data Collection We list the primary websites from which we collected the raw data as follows: USTPOs Open Data Portal: https://data.uspto.gov/home CNIPAs Official Website: https://www.cnipa.gov.cn/ China Judgements Online: https://wenshu.court.gov.cn/ Ethical considerations. The data we collected come from open and public sources, and we confirm that they are not used for any commercial purposes. We strictly comply with all copyright and licensing regulations. Data originating from sources that do not allow copying or redistribution are deliberately excluded. C.2 Annotation and Examination Guidelines We provide detailed data annotation guidelines to ensure the quality, correctness, and difficulty of our benchmark. Notably, most of our human expert annotators, who come from backgrounds in intellectual property and public management, range from senior undergraduates to Ph.D. candidates. They are included as co-authors of this paper as non-monetary form of acknowledgment for their efforts. They possess deep knowledge of intellectual property. Preparation before annotation. We divide our 21 human expert annotators into four groups and assign them to different tasks, including data annotation and annotation review. Each group is required to thoroughly understand their assigned task and formulate comprehensive annotation plan accordingly. This involves understanding the task definition, relevant legal concepts, and technical terminologies related to intellectual property. General principles and process of annotation. Firstly, all raw data or information must be collected from official websites that are publicly accessible. For websites that prohibit copying, annotators are instructed not to use them. Secondly, all annotators are required to ensure the accuracy of their annotated questions and to ensure that the difficulty level is appropriate. For data containing mathematical equations or special notations, we ask annotators to convert them into LaTeX format. For other typographical errors, human expert annotators will correct them manually. Thirdly, all data will be examined by switching roles between annotation teams to verify and ensure their quality. For each datapoint, after the quality check, human expert annotators are required to label the language, the type of IP mechanism, and the data source. Specific principles of examination. To ensure data quality, we assign different annotation team to double-check and cross-validate the results. In cases where errors, inconsistencies, or misunderstandings are identified, human examiners must provide detailed explanations and determine whether the data can be corrected and preserved. After the annotator corrects the question, the examiner will re-evaluate the data until it passes the review with mutual agreement. This strict process ensures the reliability of our data, with each datapoint undergoing an average of three rounds of review to form IPBench."
        },
        {
            "title": "D More Details about Data Statics",
            "content": "In this section, we provide additional details about the data. Further statistical information can be found in D. As shown in Table 3, our IPBench comprises 10,374 datapoints spanning 20 tasks, including multiplechoice questions, classification tasks, and generation tasks. In this section, we provide additional data statistics, covering language distribution ( D.1), IP mechanism distribution ( D.2), IPC/CPC classification distribution ( D.3), text length distribution ( D.4), and the distribution of option counts in multiple-choice questions ( D.5). 28 D.1 Data Language Distribution As mentioned in 1, our IPBench is constrained to the legal frameworks of the United States and mainland China; therefore, the dataset includes both English and Chinese languages. We present the language distribution for each task, as well as for the entire dataset, in Table 10. Language Chinese English Total 1-1 259 241 500 1-2 276 226 502 1294 206 500 1-4 252 252 504 1-5-1 1-5-2 525 600 0 600 600 1-6 338 219 557 1-7 308 240 548 2250 250 500 2-2 228 272 500 2-3 156 160 316 2139 162 301 2-5 301 0 301 3-1 160 140 300 3159 149 308 3-3 500 500 1000 3-4 217 183 400 30 314 314 4-1 200 200 400 4-2 200 199 399 4328 171 499 Sum 5090 5284 10374 Table 10: Data language distribution of IPBench. D.2 Intellectual Property Mechanisms Distribution As mentioned in Table 1, our IPBench covers eight intellectual property mechanisms, including Patent, Trademark, Software Copyright, Trade Secret, New Plant Variety, Copyright, Integrated Circuit Layout Design, and Geographical Indication. We present detailed distribution of these intellectual property mechanisms in our benchmark, as shown in Table 11, Table 12 (English section), and Table 13 (Chinese section). Task Patent Trademark SC 1-1 1-2 1-3 1-4 1-5-1 1-5-2 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 3-5 4-1 4-2 4-3 Total 225 221 237 325 525 600 159 190 320 183 101 301 0 300 308 1000 353 314 400 399 6958 157 95 116 37 600 0 103 358 21 105 94 0 0 0 0 0 0 0 0 0 1 13 21 1 12 0 0 22 0 9 16 11 0 0 0 0 0 8 0 0 0 0 1687 113 TS 25 0 1 33 0 0 107 0 77 49 10 0 301 0 0 0 13 0 0 0 1 617 PV Copyright IC GM Total 13 0 1 29 0 0 1 0 39 3 0 0 0 0 0 0 18 0 0 0 0 34 141 143 58 0 0 157 0 24 144 100 0 0 0 0 0 5 0 0 0 0 806 24 6 0 8 0 0 1 0 10 0 0 0 0 0 0 0 3 0 0 0 0 52 9 18 1 2 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 500 502 500 504 1125 600 557 548 500 500 316 301 301 300 308 1000 400 314 400 399 499 10374 Table 11: Intellectual property mechanisms distribution of IPBench. SC: Software Copyright, TS: Trade Secret, PV: Plant Variety, IC: Integrated Circuit, GM: Geographical Mark. D.3 IPC and CPC Classification Distribution We present the IPC Section classification distribution in Table 14 and the CPC Section classification distribution in Table 15. D.4 Text Length Distribution We provide detailed statistics on the text length distribution for each task, across the three question types, in both Chinese and English. In all text length computations presented in this paper, we adopt the tokenizer of GPT-4o for consistency and comparability. Table 16 and Table 17 present the 29 Task Patent Trademark SC TS PV Copyright IC GM Total 1-1 1-2 1-3 1-4 1-5-1 1-5-2 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 3-5 4-1 4-2 4-3 150 92 53 202 0 600 65 58 170 101 52 162 0 140 149 500 175 314 200 199 169 57 52 64 13 600 0 54 182 21 58 45 0 0 0 0 0 0 0 0 0 1 0 0 0 4 0 0 8 0 4 4 6 0 0 0 0 0 0 0 0 0 0 Total 3551 1147 26 1 0 1 4 0 0 26 0 22 29 2 0 0 0 0 0 4 0 0 0 1 90 10 0 1 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 20 3 82 86 29 0 0 66 0 24 80 55 0 0 0 0 0 4 0 0 0 0 429 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 241 226 206 252 600 600 219 240 250 272 160 162 0 140 149 500 183 314 200 199 171 5284 Table 12: Distribution of intellectual property mechanisms in the English portion of IPBench. SC: Software Copyright, TS: Trade Secret, PV: Plant Variety, IC: Integrated Circuit, GM: Geographical Mark. Task Patent Trademark SC TS PV Copyright IC GM Total 1-1 1-2 1-3 1-4 1-5-1 1-5-2 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 3-5 4-1 4-2 4-3 75 129 184 123 525 0 94 132 150 82 49 139 0 160 159 500 178 0 200 200 328 Total 3407 100 43 52 24 0 0 49 176 0 47 49 0 0 0 0 0 0 0 0 0 0 540 13 21 1 8 0 0 14 0 5 12 5 0 0 0 0 0 8 0 0 0 0 87 24 0 0 29 0 0 81 0 55 20 8 0 301 0 0 0 9 0 0 0 527 3 0 0 29 0 0 1 0 30 3 0 0 0 0 0 0 18 0 0 0 0 84 31 59 57 29 0 0 91 0 0 64 45 0 0 0 0 0 1 0 0 0 0 377 4 6 0 8 0 0 1 0 10 0 0 0 0 0 0 0 3 0 0 0 32 9 18 0 2 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 36 259 276 294 252 525 0 338 308 250 228 156 139 301 160 159 500 217 0 200 200 328 5090 Table 13: Distribution of Intellectual Property Mechanisms in the Chinese Portion of IPBench. SC: Software Copyright, TS: Trade Secret, PV: Plant Variety, IC: Integrated Circuit Layout Design, GM: Geographical Mark. 30 Section Count Percentage (%)"
        },
        {
            "title": "A\nB\nC\nD\nE\nF\nG\nH",
            "content": "72 249 29 6 63 64 113 529 All 1125 6.5 22.1 2.6 0.5 5.6 5.7 10.0 47.0 100 Table 14: Distribution of IPC sections Section Count Percentage (%)"
        },
        {
            "title": "A\nB\nC\nD\nE\nF\nG\nH",
            "content": "All 90 90 48 17 71 189 44 51 600 15.0 15.0 8.0 2.8 11.9 31.5 7.3 8.5 100 Table 15: Distribution of CPC sections distribution of text lengths from different perspectives: the former provides statistics by task, while the latter summarizes the data by question type. Task Avg-All Avg-EN Avg-CH Min-All Min-EN Min-CH Max-All Max-EN Max-CH 1-1 1-2 1-3 1-4 1-5-1 1-5-2 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 3-5 4-1 4-2 4-3 83.9 81.2 102.2 116.2 216.7 165.1 89.4 41.7 161.6 109.4 122.8 99.8 112.1 158.2 91.4 1239.5 166.1 7460.4 1636.7 448.5 121.2 68.9 71.3 80.3 112.8 163.6 165.1 77.0 - 103.1 97.4 107.9 88.1 - 145.8 76.0 1231.8 169.8 7460.4 2199.3 534.1 111.8 97.8 89.3 117.6 119.6 277.4 - 97.5 74.2 220.1 123.6 138.1 113.5 112.1 169.1 105.9 1247.3 163.0 60.5 1074.0 363.3 126. 46 47 55 61 49 50 55 40 66 59 70 66 51 - 53 575 60 1428 - 68 56 46 47 55 61 49 50 55 40 66 59 70 66 - - 53 581 92 1428 - 89 56 66 66 76 73 110 - 70 53 177 71 80 87 51 - 78 575 60 - 285 68 84 258 135 208 195 305 337 146 107 310 211 263 144 302 - 150 1956 297 10219 - 1861 218 112 105 129 151 305 337 146 101 140 189 171 125 - - 121 1845 297 10219 8064 1485 183 258 135 208 195 455 - 128 107 310 211 263 144 302 - 150 1956 327 - 5675 1861 Table 16: Text length statistics (in tokens) for each task across three dimensions: average, minimum, and maximum length; each further split by language (EN/CH). Missing values are denoted by \"-\". 31 Type Avg-All Avg-EN Avg-CH Min-All Min-EN Min-CH Max-All Max-EN Max-CH MCQA PE Classification Generation 181.0 7460.4 190.9 1042. 181.6 7460.4 164.4 1366.7 194.7 60.5 277.4 718.7 90.7 1428.0 49.5 68.0 96.2 1428.0 49.5 89.0 111.1 - 110.0 176.5 326.7 10219.0 321.0 1861. 272.5 10219.0 321.0 4774.5 327.5 - 455.0 3768.0 Table 17: Aggregated text length statistics (in tokens) by task type. PE refers to Patent Examination (Task 3-5), MCQA refers to Multiple-choice Question Answering. D.5 Multi-Choice Question Option Count Distribution. In this section, we present the distribution of multiple-choice question option counts, as shown in Table 18, Table 19, and Table 20, along with the examination option distribution for Task 3-5, as shown in Table 21. For multiple-choice questions, each question has four options: A, B, C, and D. In contrast, for Task 3-5, each question has two options: allowed and rejected. Task 1-1 1-2 1-3 1-4 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 4-3 125 117 125 126 151 132 155 101 74 74 57 114 72 240 76 170 129 162 126 126 137 194 130 177 108 83 165 82 76 256 141 144 126 117 126 127 142 124 124 154 85 75 63 59 76 230 128 120 106 123 125 127 98 91 68 49 69 16 45 84 278 55 74 Total 500 502 500 504 557 548 500 500 316 301 301 300 308 1000 400 499 Total 2236 1867 1528 7536 Table 18: Distribution of answer choices (AD) by task Task 1-1 1-2 1-3 1-4 1-6 1-7 2-1 2-2 2-3 2-4 3-1 3-2 3-3 3-4 4-3 Total 64 65 44 70 39 41 92 70 38 39 54 38 120 44 44 862 53 69 47 71 70 70 65 68 68 45 38 35 126 64 954 65 48 58 57 40 64 64 77 26 40 33 39 115 50 42 818 59 44 57 54 70 65 29 57 28 38 15 37 139 25 737 Total 241 226 206 252 219 240 250 272 160 162 140 149 500 183 171 3371 Table 19: Distribution of English questions answers (AD) by task Task 1-1 1-2 1-3 1-4 1-6 1-7 2-1 2-2 2-3 2-4 2-5 3-1 3-2 3-3 3-4 4-3 61 52 81 56 112 91 63 31 36 35 57 60 34 120 32 126 76 93 79 55 67 124 65 109 40 38 165 44 41 126 77 79 61 69 68 70 102 60 60 77 59 35 63 26 37 115 78 69 Total 1047 1278 1049 61 62 66 71 57 33 62 11 21 31 16 30 47 139 30 54 791 Total 259 276 294 252 338 308 250 228 156 139 301 160 159 500 217 328 4165 Table 20: Distribution of Chinese questions answers (AD) by task Examination Outcome Count Percentage (%) Allowed Rejected Total 138 176 314 43.95 56. 100 Table 21: Examination outcome distribution for Task 3-"
        },
        {
            "title": "E Prompts",
            "content": "E.1 Zero-shot Prompt We adapt four types of zero-shot prompts for our experiment, corresponding to different task types: choice questions, classification, examination, and generation, across both English and Chinese languages. The Chinese version uses the same content as the English version. Zero-shot Prompt for Choice Question Task Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **option** { Question } Choice Question Format Example Question: An author self-publishes book and later finds that small publisher has printed and sold copies without permission. The small publisher claims they thought the book was in the public domain. Is this copyright infringement? A) No, as long as they thought it was in the public domain. B) Only if the small publisher makes lot of profit. C) Yes, because they printed and sold copies without permission. D) Only if the author has large number of followers. Answer: 33 Zero-shot Prompt for IPC/CPC Classification Task (1-5) Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **corresponding IPC number** {Question} IPC/CPC Classification Question Format Example (1-5) IPC Classification Task (1-5-1): Question: Please provide the corresponding International Patent Classification (IPC) based on the patents title and abstract. Title: Interchangeable and interconnectable tool organizing device Abstract: custom tool accessory to hold tool, the custom tool accessory including body, at least one tongue disposed at first side of the body, at least one tongue receiving groove disposed at second side of the body opposite from the first side of the body, and tool holding portion disposed on the body to hold the tool. At least one magnet is attached to the underside of metallic base plate that is in turn secured to the underside of the body. retainer surrounds the at least one magnet and also underlies the area of the bottom of the base plate that is not covered by the at least one magnet. CPC Classification Task (1-5-2): Question: Please provide the corresponding Cooperative Patent Classification (CPC) based on the patents title and abstract. Title: Telescoping outrigger arm Abstract: fishing outrigger including telescoping outrigger assembly is provided. The outrigger assembly may include cantilever spring assembly, at least two telescoping tubes, central spring, stabilizer base and pole, and at least one mounting body. The cantilever spring assembly may be adapted to lock the extended telescoping tubes securely in place and allow for easy retraction. The cantilever spring assembly may be coupled to the stabilizer base, both of which may ride along the outside of telescoping tube. Mounting assemblies may be disposed at the ends of the outrigger assembly and be adapted to secure the outrigger assembly to boat or other vehicle. Zero-shot Prompt for Patent Application Examination Task (3-5) Please examine the patents in # Patent Applications Awaiting Examination. Determine whether each patent application should be allowed or rejected. Return your decision in the following format: Answer: allowed / rejected Patent Application Examination Question Format Example (3-5) # Patent Applications Awaiting Examination <ApplicationNumber> 12763093 </ApplicationNumber> <Title> METHODS AND APPARATUS FOR ENSURING COMPATIBILITY ON HIGH PERFORMANCE SERIAL BUS </Title> <Abstract> data communications system is disclosed having at least one Legacy cloud coupled to at least one Beta cloud. The system further having at least one BOSS node and at least one border node. method for ensuring compatibility is disclosed comprising determining when the BOSS node is idle, determining whether the last packet transmitted by any border node was an Alpha format packet if the BOSS node is idle, and unlocking the Legacy cloud if the last packet transmitted by the border node was not an Alpha format packet. </Abstract> <Background> <SOH> BACKGROUND OF THE INVENTION <EOH> 1. Field of the Invention The present invention relates to data communications. In particular, the present invention relates to ensuring compatibility on high performance serial bus system. 2. The Prior Art Background Modern electronic equipment has greatly enhanced the quality of our lives. However, as the use of such equipment has increased ... (Omit) </Background> <Claims> 1. method for ensuring compatibility in data communications system between at least one first cloud of devices having first device type coupled to at least one second cloud of devices having second device type via at least one border node, the at least one second cloud having at least one master node, ... (Omit) </Claims> <Summary> <SOH> BRIEF DESCRIPTION OF THE INVENTION <EOH>The invention satisfies the above needs. The present invention relates to method and apparatus for ensuring compatibility in high performance serial bus. ... (Omit) </Summary> <FullDescription> CROSS-REFERENCE TO RELATED APPLICATIONS This application is continuation of co-pending U.S. patent application Ser. No.10/984,535, filed Nov. 8, 2004, which is ... (Omit) </FullDescription> Answer: allowed Zero-shot Prompt for Generation Task (4-1, 4-2) Abstract Generation based on Claims (4-1): # Claims {Claims Text} Please generate the abstract of the patent based on the given claims. Dependent Claim Generation (1-5-2): # Independent Claim {Claim Text} Please generate all dependent claims corresponding to the given independent claim. 35 Generation Question Format Example (4-1, 4-2) Abstract Generation based on Claims (4-1): Claim Text: 1. lighting device with low mispress buttons, the lighting device comprising: tail cover, first button and second button, and pressing plate rotatably connected to the tail cover and covering the second button to trigger and release the second button, wherein the pressing plate is lower than the first button;... (Omit). 2. The lighting device with low mispress buttons according to claim 1 , wherein an angle between the first inclined surface and the second inclined surface is an obtuse angle. (Claim 3-9, Omit) Abstract: lighting device with low mispress buttons which includes tail cover, first button, second button and pressing plate. The pressing plate is rotatably connected to the tail cover and covering the second button to trigger and release the second button. The pressing plate is lower than the first button. The pressing plate includes first inclined surface and second inclined surface. The first inclined surface is arranged between the second inclined surface and the first button, and is connected to the second inclined surface. The second inclined surface is inclined downward relative to the first inclined surface. An end of the first inclined surface close to the second inclined surface is lower than an end of the first inclined surface close to the first button. Dependent Claim Generation (1-5-2): Claim Text: 1. Exercise equipment, comprising: support comprised holder supported on ground and connector formed upward from the holder with an end thereof disposed horizontally with surface of the ground; an assembly comprising handle having first end connected to the connector, an elastic plate having first end connected to second end the handle, and weighted member disposed at second end of the elastic plate, wherein user is enabled to perform battle rope exercise using the assembly alone, to perform push-up exercise with the assembly placed on the ground while connected to the support, or to perform battle rope exercise by holding the handle. support comprised holder supported on ground and connector formed upward from the holder with an end thereof disposed horizontally with surface of the ground; an assembly comprising handle having first end connected to the connector, an elastic plate having first end connected to second end the handle, and weighted member disposed at second end of the elastic plate, wherein user is enabled to perform battle rope exercise using the assembly alone, to perform push-up exercise with the assembly placed on the ground while connected to the support, or to perform battle rope exercise by holding the handle. Dependent Claim: 2. The exercise equipment of claim 1, wherein the elastic plate is formed of leaf spring shaped as strip. 3. The exercise equipment of claim 1, wherein the elastic plate is configured to be partially inserted into the second end of the handle, and wherein the handle comprises fixture (112) that fixes the inserted elastic plate (100). wherein the handle comprises fixture (112) that fixes the inserted elastic plate (100). 36 E.2 Few-shot Prompt Few-shot Prompt for Choice Question Task # There are examples ## Example {1} Question: {1-shot-question} Answer:{1-shot-answer} ... ## Example {k} Question: {k-shot-question} Answer:{k-shot-answer} Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **option** { Question } Few-shot Prompt for IPC/CPC Classification Task # There are examples ## Example {1} Question: {1-shot-question} Answer:{1-shot-answer} ... ## Example {k} Question: {k-shot-question} Answer:{k-shot-answer} Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **corresponding IPC/CPC number** {Question} E.3 Chain-of-Thought Prompt Chain-of-Thought Prompt for Choice Question Task Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **option** { Question } Lets think step by step. Chain-of-Thought Prompt for IPC/CPC Classification Task Please answer the following question thoughtfully and provide your final answer at the end in the format Answer: **corresponding IPC/CPC number** {Question} Lets think step by step."
        },
        {
            "title": "F Metrics",
            "content": "In this section, we provide the details of the metrics used in our IPBench. The details of the multiplechoice question metric are in F.1, the details of the classification task metric are in F.2, and the details of the generation task are in F.3. F.1 Multi-Choice Question Metric For multiple-choice questions, we use accuracy as the metric due to the straightforward nature of the judgment process. Each multiple-choice question has four options: A, B, C, and D. We use the same extraction method for each models response, compare the selected answer with the ground-truth option, and then compute the average accuracy. The average score ranges from 0 to 100, and is computed as shown in the Equation 1. Accuracy = Number of Correct Answers # Total Number of Questions # (1) F.2 IPC/CPC Classification Task Metric For IPC/CPC classification task, we use exact-match as the metric. For example, in the IPC code A01B00/66, represents the Section, 01 the Class, and the Subclass. If the model predicts A, it earns one point for the Section; if it predicts A01, it earns one point for the Class; and if it predicts A01B, it earns one point for the Subclass. If the entire code is predicted correctly, one point is awarded for the Exact Match. We evaluate all the test data to calculate the average exact-match score across these four levels. The difficulty increases as the model is required to make correct predictions at more levels. F.3 Generation Task Metric In this section, we provide the details of the LLM-as-a-judge approach used for LLMScore and analyze its consistency with human evaluation. We design five evaluation dimensions for LLM-as-a-judge: Accuracy, Relevance, Completeness, Consistency, and Language-Style. The detailed definitions are provided in the prompts below. Each dimension is scored on scale from 1 to 10 points. We use DeepSeek-V3 as the judge model because it achieves relatively better performance on the multiple-choice tasks, indicating solid knowledge in the intellectual property domain. In addition to the LLM-as-a-judge evaluation, we further sample 50 responses each from GPT-4o, DeepSeek-V3 and LLaMA3.1-8B-Instruct for the two tasks. These responses are assessed by three human experts using the same criteria as the LLM-as-a-judge framework. The results and the corresponding consistency between the LLM and human analysis are presented in Table 22. Metric Kendall Task 4-1 Pearson Spearman Kendall Task 4Pearson Spearman LLMScore BLEU ROUGE-L BERTScore 0.218 (0.0005) 0.167 (0.0042) 0.146 (0.0123) 0.104 (0.0746) 0.289 (0.0011) 0.221 (0.0068) 0.176 (0.0317) 0.160 (0.0519) 0.316 (0.0003) 0.231 (0.0046) 0.198 (0.0154) 0.142 (0.0847) 0.396 (0.0000) 0.403 (0.0000) 0.369 (0.0000) 0.052 (0.3680) 0.649 (0.0000) 0.466 (0.0000) 0.513 (0.0000) 0.086 (0.2950) 0.580 (0.0000) 0.540 (0.0000) 0.497 (0.0000) 0.077 (0.3494) Table 22: Correlation of LLMScore with human judgments on Task 4-1 and Task 4-2 (p-value in parentheses). Correlation coefficients, p-value. We provide consistency analysis between different metrics and human evaluations, including Kendall, Pearson, and Spearman coefficients. The higher the consistency coefficient, the better, indicating stronger consistency; the smaller the p-value, the better, indicating statistical significance. smaller p-value, typically less than 0.05, indicates that the observed correlation is statistically significant. Task 4-1 LLMScore. For Task 4-1, we drew inspiration from the error taxonomy for abstract generation based on claims proposed in PatentEval [62], and used five dimensions to evaluate the quality of the generated abstract. As shown below, this is the specific prompt we use for LLM-as-ajudge in evaluating abstract generation. Prompt Used for Abstract Generation (Task 4-1) in LLM-as-a-judge You are an experienced intellectual property expert specializing in assessing the quality of patent abstracts. Please objectively evaluate the abstract written by the AI assistant, acting as fair and rigorous judge. When evaluating, you should score it based on the following five dimensions: 1. Accuracy: Measures the factual correctness and linguistic reliability of the generated abstract in relation to the original patent claims. It focuses on the following key aspects: Grammatical Errors: Occurrences of incorrect grammar, punctuation, or sentence structure, including hallucinated repetitive sequences produced by language models. Ineffective Summarization: Refers to abstracts that inadequately summarize the invention, often replicating one or more of the claims verbatim instead of providing concise and comprehensive overview of the patent. 2. Relevance: Evaluates the degree to which the generated abstract remains focused on the core subject matter of the original patent claims. It emphasizes the following key aspect: Irrelevant Content: Introducing content that deviates or digresses from the primary subject matter of the patent claims. 3. Completeness: Assesses whether the abstract fully captures all critical aspects of the patent claims, particularly the main (first independent) claim. It emphasizes the following key issue: Incomplete Coverage: Occurrences where the abstract omits essential components or concepts, failing to encapsulate all key points from the patent claims, especially the main (first independent) claim. 4. Consistency: Evaluates whether the abstract remains factually aligned with the content of the original patent claims. It emphasizes the following issue: Contradictory Information: Instances where the abstract introduces factual details that contradict the content found in the original claims. 5. Language-Style: Evaluates the linguistic quality and clarity of the abstract, focusing on whether it is concise, readable, and stylistically appropriate for patent literature. It emphasizes the following issues: Overly Wordy or Lengthy: Abstracts that are not succinct and contain unnecessary details. Many jurisdictions impose word limitsfor example, 150 words in many English-speaking countries. Unclarity: Vague or ambiguous language that obscures the intended meaning or key information. We will provide the following materials: the patent claims, the ground-truth abstract and the abstract written by the AI assistant based on those claims. When starting your evaluation, you need to follow the reasoning steps below: 1. Compare the AI assistants abstract with the ground-truth abstract, pointing out the shortcomings of the AI assistants answer and explaining them in detail. 2. Evaluate the AI assistants abstract according to the dimensions mentioned above, giving score from 1 to 10 for each dimension. 3. Based on the scores for each dimension, calculate the overall score for the abstract written by the AI assistant (110 points). 4. Your scoring should be as strict as possible, and you must follow the scoring rules below: The higher the quality of the response, the higher the score. Scoring Standards for Patent Abstract Generation: Score 12: The abstract contains severe factual inaccuracies, irrelevant or hallucinated content, or significant contradictions with the original claims. It may also suffer from extreme verbosity or unclarity, and fail to cover any key invention aspects, making it misleading or unusable. Overall, it demonstrates fundamental failures across all dimensions. Score 34: The abstract avoids major hallucinations but includes notable errors in fact representation or misses important elements of the invention. It may introduce minor contradictions, cover only parts of the claims, or suffer from relevance and clarity issues, falling below the basic standard for patent abstract. Score 56: The abstract adequately summarizes the invention, with no major factual errors or contradictions. It is generally relevant and complete, but may lack precision, omit secondary points, or suffer from mild redundancy or unclear expressions. It meets minimum expectations but is average in coherence or style. Score 78: The abstract is factually accurate, consistent with the claims, and logically coherent. It covers all key components of the invention, uses appropriate language, and has no obvious stylistic flaws. It is comparable to high-quality human-written abstracts. Score 910: The abstract is exceptionalhighly concise, comprehensive, and precise. It captures the full technical essence of the claims, avoids all factual or stylistic errors, and reads fluently and professionally. It may even outperform the ground truth in clarity and informativeness, representing an ideal output across all evaluation dimensions. Please provide detailed evaluation comments during scoring. After each dimension score, make sure to provide an explanation. All scores should be integers. Finally, return the evaluation results in the following JSON format: json {\"Accuracy\": score, \"Relevance\": score, \"Completeness\": score, \"Consistency\": score, \"Language-Style\": score, \"Overall -Score\": total score} [Start of Patent Claims] $Patent-Claims$ [End of Patent Claims] [Start of Ground-Truth Abstract] $GT-Abstract$ [End of Ground-Truth Abstract] [Start of Abstract written by AI Assistant] $Abstract$ [End of Abstract written by AI Assistant] Please begin the evaluation: 39 Prompt Used for Dependent Claim Generation (Task 4-2) Evaluation in LLM-as-a-judge You are an experienced intellectual property expert specializing in assessing the quality of patent dependent claims. Please objectively evaluate the dependent claims written by the AI assistant, acting as fair and rigorous judge. When evaluating, you should score it based on the following five dimensions: 1. Accuracy for the task of generating dependent claims based on given independent claim evaluates the linguistic correctness, structural fidelity, and legal coherence of the generated claims. It emphasizes the following key aspects: Grammatical Errors: Grammatical Inaccuracy: Misuse of grammar and hallucinated repetitive sequences produced by language models. Punctuation Discrepancy: Incorrect or inconsistent use of punctuation marks, deviating from standard patent drafting conventions. Formatting Errors: Claim Numbering Error: Incorrect or inconsistent numbering of claims. Preamble Inconsistency Error: Inaccurate reflection of subject matter in the preamble, disrupting the conceptual flow between independent and dependent claims. Transitional Phrase Error: Improper use of transitional phrases, impacting the scope of the claim. Claim Body Disconnection: Presence of fewer than two elements or lack of coherent, logical connection between listed elements in the claim body. 2. Relevance for the task of generating dependent claims based on given independent claim evaluates whether the content of the dependent claims remains aligned with the technical subject matter disclosed in the independent claim and the overall invention. This ensures that the claim set does not deviate from the inventions disclosed scope. This metric specifically emphasizes: Content Relevance Errors: Irrelevant Matter Introduction: The inclusion of elements or subject matter that are not supported by or related to the disclosed embodiments, potentially leading to unjustified broadening of the claim scope and lack of legal support. 3. Completeness for the task of generating dependent claims based on given independent claim assesses whether the dependent claims meaningfully expand upon the independent claim by introducing additional technical features or specific limitations. It ensures that each claim contributes to more comprehensive protection of the invention. This metric focuses on the following key aspects: Effectiveness Errors: Contradictory Claims: Claims that contradict preceding claims or introduce logical inconsistencies within the overall claim set, undermining the coherence and legal enforceability of the patent. Non-Distinctive Claim Repetition: Dependent claims that merely duplicate content from prior claims without adding new technical details or narrowing the scope, resulting in redundancy and lack of substantive contribution. 4. Consistency for the task of generating dependent claims based on given independent claim assesses whether the generated claims logically and legally align with their referenced claims and maintain internal coherence within the claim set. It includes the following key evaluation aspects: Dependency Errors: Non-compliant Dependency with Instruction: The dependency structure of the generated claims does not match the specified requirements or instructions. Dependency Clarity Error: Use of unclear multiple dependencies or incorrect singular dependencies, reducing legal clarity and violating best drafting practices. Broad Scope Dependent Claims: Dependent claims that fail to sufficiently narrow the scope of the independent claim on which they depend. Insufficient Differentiation of Independent Claims: Independent claims that do not introduce distinct inventive concept and overlap significantly in scope with preceding claims. 5. Language-Style for the task of generating dependent claims based on given independent claim evaluates the linguistic quality and drafting style of the claims, focusing on clarity, precision, and conciseness in accordance with standard patent drafting conventions. This includes the following key evaluation aspects: Clarity Errors: Vagueness: Use of ambiguous, vague, or relative expressions (e.g., suitable, substantially, high speed) that render the claims scope indefinite or open to interpretation (WIPO, 2022, pp. 24, 8082). Antecedent Reference Errors: Failure to provide clear antecedent basis for terms used in the claims, leading to confusion about the elements being referred to (WIPO, 2022, p. 42). Terminological Inconsistency: Use of multiple terms or differing reference numerals to describe the same element, resulting in unnecessary complexity or ambiguity. Wishful Claiming: Expression of desired results or objectives without disclosing concrete technical means, making the claims speculative or abstract (WIPO, 2022, pp. 6869). Brevity Errors: Verbose Redundancy: Inclusion of excessive or repetitive language that does not contribute substantive meaning to the claim. Sub-Optimal Claim Structure: Use of unnecessarily complex claim formats that could be better expressed using multiple, simpler dependent claims for improved clarity and readability (WIPO, 2022, pp. 47, 49). We will provide the following materials: the patent independent claims, the ground-truth dependent claims, and the dependent claims written by the AI assistant based on independent claims. When starting your evaluation, you need to follow the reasoning steps below: 1. Compare the AI assistants dependent claims with the ground-truth dependent claims, pointing out the shortcomings of the AI assistants answer and explaining them in detail. 2. Evaluate the AI assistants dependent claims according to the dimensions mentioned above, giving score from 1 to 10 for each dimension. 3. Based on the scores for each dimension, calculate the overall score for the dependent claims written by the AI assistant (1-10 points). 4. Your scoring should be as strict as possible, and you must follow the scoring rules below: The higher the quality of the response, the higher the score. Scoring Standards for Dependent Claim Generation: Score 12: The generated dependent claim contains multiple severe issues, such as significant grammatical or formatting errors, logical contradictions with the independent claim, irrelevant technical content, or failure to meet the basic structure and scope expectations of dependent claims. It may also introduce speculative or hallucinated information. Overall, it does not meet the minimum requirements for valid dependent claim. Score 34: The dependent claim avoids the most egregious problems but still suffers from notable issues in one or more areas. These may include vague or unclear phrasing, incorrect dependencies, ineffective narrowing of the independent claim, or inconsistencies in terminology or scope. The claim fails to clearly distinguish itself or support the invention meaningfully. Score 56: The dependent claim meets general drafting expectations. It is mostly accurate in content and format, logically consistent with the independent claim, and relevant to the invention. However, it may exhibit average language quality, mild redundancy, or lack of refinement in scope narrowing, limiting its legal or technical value. Score 78: The dependent claim is of high quality and closely aligns with the characteristics of well-drafted claim. It is clear, precise, legally and technically consistent with the independent claim, and demonstrates appropriate scope limitation. There are no major issues across the five dimensions, and only minor improvements could be suggested. Score 910: The dependent claim demonstrates exemplary performance across all evaluation criteria. It significantly enhances the clarity, precision, and utility of the independent claim, avoids any form of redundancy or ambiguity, and adds meaningful technical value. It could serve as reference example of optimal dependent claim drafting. Please provide detailed evaluation comments during scoring. After each dimension score, make sure to provide an explanation. All scores should be integers. Finally, return the evaluation results in the following json format: {\"Accuracy\": score, \"Relevance\": score, \"Completeness\": score, \"Consistency\": score, \"Language-Style\": score, \"Overall-Score\": total score} [Start of Patent Independent Claims] $Patent-Independent-Claims$ [End of Patent Independent Claims] [Start of Ground-Truth Dependent Claims] $GT-Dependent-Claims$ [End of Ground-Truth Dependent Claims] [Start of Dependent Claims written by AI Assistant] $Dependent-Claims$ [End of Dependent Claims written by AI Assistant] Please begin the evaluation:"
        },
        {
            "title": "G More Results",
            "content": "In Section G, we present additional results under various experimental settings, covering both Chinese and English. Specifically, G.1 reports the overall results on IPBench, G.2 presents the results for Chinese questions, G.3 covers the results for English questions, and G.4 provides detailed results of the LLM-as-a-judge evaluation along with its consistency with human judgments. G.1 Overall Results The main results of the overall performance under the zero-shot setting are presented in Table 5, Table 6, and Table 7. We provide the results of overall performance under the few-shot setting (1-shot, 2-shot, and 3-shot) in G.1.1, and the results under the chain-of-thought setting in G.1.2.We provide model performance heatmap as shown in Figure 5, where models are sorted by their overall performance. redder color indicates that the model on the x-axis outperforms the corresponding model on the y-axis. Figure 5: Model performance heatmap. G.1.1 Few-shot Results The 1-shot results of IPBench are presented in Table 23 and Table 24, the 2-shot results in Table 25 and Table 26, and the 3-shot results in Table 27 and Table 28. G.1.2 Chain-of-Thought Results The chain-of-thought results of IPBench are presented in Table 29 and Table 30. G.2 Chinese Questions Results In Section G.2, we focus on the IPBench results for Chinese questions. We provide the zeroshot results for the Chinese portion of IPBench in G.2.1, the few-shot results in G.2.2, and the chain-of-thought results in G.2.3. 41 Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 73.9 67.5 59.3 66.4 54.8 63.0 DISC-LawLLM 47.7 28.0 Hanfei 1-1 93.8 94.2 87.0 89.2 79.0 95.2 78.8 37.0 1-2 86.3 82.5 69.7 74.3 61. 81.5 66.9 29.3 1-3 78.6 76.4 67.4 71.0 63.4 76.2 66.4 28. 1-4 79.8 72.8 72.0 73.2 59.9 66.9 65.5 31.2 1-6 61.1 60.8 50.7 55.6 43. 59.9 45.0 33.9 1-7 66.6 63.5 64.6 60.8 52.9 64.4 52.2 43. 2-1 51.6 48.8 46.0 50.8 44.6 47.2 40.0 26.6 2-2 62.6 62.0 57.6 65.0 57. 62.6 54.6 37.2 2-3 61.4 52.2 43.7 50.0 36.4 40.2 32.0 35. 2-4 77.1 70.4 69.4 72.8 61.8 72.1 51.8 29.9 2-5 81.4 76.1 45.9 82.7 62. 77.4 70.8 24.6 3-1 69.3 69.0 60.0 70.0 60.3 69.0 58.7 37. 3-2 76.9 69.5 69.8 70.8 48.4 58.8 39.6 22.4 3-3 81.1 55.6 36.8 53.9 39. 36.6 3-4 79.8 79.5 73.2 75.7 67.2 71.8 67.3 31. 4-3 70.3 62.1 57.1 69.5 57.1 57.9 30.7 30.1 Table 23: Results of IPBench with 1-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 2.2 15.1 2.2 0.7 0.4 81.8 86.3 73.8 64. 47.0 67.0 75.5 57.9 49.7 34.6 50.8 60.5 42.2 33.7 21.9 0.5 7.3 0.3 0. 0.0 74.3 86.2 67.5 45.2 16.5 59.1 74.3 48.8 35.2 7.8 49.1 65.0 37.2 22. 4.3 Table 24: Results of Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 1-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 74.0 69.3 59.3 67.3 57.2 66.8 DISC-LawLLM 56.7 32.5 Hanfei 1-1 94.2 94.8 85.8 89.0 79.8 96. 79.2 36.0 1-2 87.9 83.3 67.3 76.3 64.1 84.1 67.1 24.7 177.0 76.0 68.4 71.4 64.6 77.2 66.6 35.8 1-4 80.2 77.0 69.6 72.2 61.5 75. 65.3 33.7 1-6 60.6 59.9 52.3 56.3 44.4 63.6 50.2 35.3 166.6 65.0 61.7 61.1 53.8 64.2 51.3 33.0 2-1 52.0 47.8 48.0 51.4 47.0 48. 39.6 25.4 2-2 59.6 60.4 56.6 62.8 56.6 61.2 53.8 37.0 263.3 56.0 45.6 52.2 38.0 43.0 31.3 32.6 2-4 79.4 71.8 71.4 74.7 65.1 75. 58.1 26.6 2-5 83.4 83.7 66.8 84.7 73.1 82.4 78.1 49.2 368.0 67.7 60.0 66.7 61.8 72.0 57.7 42.3 3-2 78.6 69.5 61.4 72.4 48.7 64. 42.2 22.1 3-3 80.7 65.9 37.8 57.4 47.1 49.4 376.7 81.5 69.8 78.0 67.5 80.8 71.5 32.2 4-3 72.5 57.7 53.9 70.3 56.5 55. 37.1 25.7 Table 25: Results of IPBench with 2-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 2.3 15.1 2.5 1.1 0.6 82.2 86.7 78.2 59.7 56.6 68.0 76.1 62.4 44.7 41. 51.5 60.6 46.2 29.2 26.8 0.2 7.2 0.3 0.0 0.2 76.3 86.5 68.5 63.3 32. 61.6 73.3 51.2 45.7 17.3 51.6 65.7 38.8 26.7 9.3 Table 26: Results of Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 2-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 74.7 70.6 59.4 67.4 56.5 65.3 DISC-LawLLM 57.4 29.9 Hanfei 1-1 94.4 94.2 87.4 89.4 80.6 96.2 83.8 32.0 1-2 87.5 83.3 67.0 76.1 63. 83.5 67.3 28.9 1-3 79.6 73.6 66.4 70.6 62.8 77.2 64.6 26. 1-4 80.0 76.0 69.8 70.6 61.7 76.0 66.7 28.4 1-6 63.3 62.2 52.7 56.6 45. 62.4 53.6 31.7 1-7 68.8 68.2 63.7 62.5 54.4 65.1 52.0 31. 2-1 52.4 50.8 45.6 51.2 47.4 49.4 41.2 23.6 2-2 58.6 62.2 55.4 62.2 56. 61.8 54.8 36.2 2-3 63.6 51.3 43.0 51.6 36.1 40.5 29.4 29. 2-4 80.1 74.8 66.4 76.4 64.1 75.8 62.1 22.6 2-5 82.7 84.7 75.4 85.1 73. 80.4 74.4 30.6 3-1 70.3 70.7 61.0 68.3 63.3 72.7 60.0 41. 3-2 77.9 72.4 62.7 69.5 50.3 62.0 41.2 24.4 3-3 80.0 68.5 36.7 58.7 40. 38.3 3-4 80.0 82.2 70.5 76.3 67.8 79.0 67.5 26. 4-3 75.0 60.3 56.3 70.7 58.1 57.3 38.5 34.5 Table 27: Results of IPBench with 3-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 2.0 15.6 2.3 1.2 1. 82.5 87.1 78.8 65.6 70.6 67.8 76.2 62.7 48.9 51.3 50.9 61.2 46.8 32.8 34. 0.3 7.7 0.5 0.0 0.0 80.1 85.7 68.3 64.8 24.2 65.3 73.3 50.8 45.8 12. 54.1 64.7 38.8 29.8 7.7 Table 28: Results of Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 3-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 72.0 67.6 61.3 61.7 54.3 60.2 DISC-LawLLM 37.3 29.9 Hanfei 1-1 94.4 89.0 84.1 87.0 80. 93.0 65.4 42.0 1-2 85.9 82.9 69.5 72.3 63.3 79.9 57.4 28. 1-3 78.0 75.2 67.6 65.4 63.6 72.0 48.6 32.4 1-4 80.4 76.2 70.2 66.3 62. 65.3 39.3 34.1 1-6 59.9 57.3 53.6 54.1 43.6 50.2 42.8 30. 1-7 67.3 63.0 59.5 55.7 54.0 61.9 41.4 26.8 2-1 51.4 48.0 49.4 51.0 42. 45.2 25.4 28.8 2-2 62.6 64.2 60.6 64.8 54.6 52.4 34.8 29. 2-3 62.0 58.2 45.9 47.8 44.3 45.2 25.9 21.5 2-4 74.8 73.8 66.4 71.1 64. 66.8 32.2 25.9 2-5 80.4 79.7 71.8 76.4 65.8 72.4 62.8 24. 3-1 65.7 66.7 62.3 66.3 56.7 58.3 26.7 34.3 3-2 71.1 70.1 59.4 67.9 51. 62.7 25.7 26.6 3-3 81.1 65.1 54.8 57.9 41.0 49.2 17.5 24. 3-4 78.8 78.5 73.0 73.5 66.5 71.0 39.0 31.7 3-5 44.9 44.3 43.6 36. 43.9 4-3 66.9 58.9 54.1 65.1 47.9 44.1 30.5 35. Table 29: Results of IPBench with chain-of-thought setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 0.2 1.3 1.8 0. 0.3 80.4 82.3 74.3 67.0 22.4 67.4 72.0 60.4 50.5 17.2 51.6 57.4 42.0 32. 12.6 0.0 1.0 0.5 0.2 0.0 76.8 83.3 60.2 64.0 7.7 63.0 70.7 46.0 44. 2.8 52.5 63.0 35.7 29.5 1.8 Table 30: Results of Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with chain-of-thought setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . G.2.1 Zero-shot Results The zero-shot results for the Chinese portion of IPBench are shown in Table 31, Table 32 and Table 33. Since the Patent CPC Classification task (1-5-2) only includes English questions, Table 32 does not include it. G.2.2 Few-shot Results The 1-shot results for the Chinese portion of IPBench are shown in Table 34 and Table 35, the 2-shot results in Table 36 and Table 37 and the 3-shot results in Table 38 and Table 39. G.2.3 Chain-of-Thought Results The chain-of-thought results for the Chinese portion of IPBench are presented in Table 40 and Table 41. G.3 English Questions Results In Section G.3, we focus on the IPBench results for English questions. We provide the zeroshot results for the English portion of IPBench in G.3.1, the few-shot results in G.3.2, and the chain-of-thought results in G.3.3. 43 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 77.7 74.2 78.7 77.9 70.8 70.8 65.1 69.2 65.5 54.1 67.9 DISC-LawLLM 55.0 39.9 Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B 76.6 58.2 76.4 195.0 91.9 97.3 96.5 93.8 91.5 88.4 88.8 91.5 74.1 93.1 86.9 65.3 95.8 79.2 94.6 1-2 92.4 86.2 90.2 92.4 85.9 81.9 73.6 78.3 75.4 58. 86.2 69.2 46.7 91.3 59.1 93.1 1-3 82.0 76.9 87.1 82.0 72.4 71.4 65.7 66.0 68.4 55.4 73. 64.6 50.3 84.4 50.7 79.3 1-4 80.6 80.6 83.0 81.8 77.1 76.2 78.6 75.4 63.5 58.7 59.1 63.5 53. 79.4 63.9 79.8 1-6 73.1 65.1 76.3 69.8 64.5 66.6 63.6 60.4 67.5 44.1 69.2 64.5 45.9 74.0 51.2 76. 1-7 73.7 66.6 72.1 70.5 63.6 66.9 58.4 60.1 57.5 51.6 64.6 49.7 51.3 73.1 49.4 73.7 250.8 44.4 48.0 48.0 47.6 47.2 50.4 47.2 45.2 45.6 48.4 38.0 28.4 44.8 44.0 50.0 2-2 73.7 66.2 74.1 81.1 76.3 71.9 69.3 77.2 73.7 60. 64.0 66.7 43.4 76.8 53.5 77.2 2-3 67.3 57.7 65.4 62.2 51.9 48.7 41.7 53.2 45.5 40.4 41. 40.4 26.9 66.0 38.5 66.7 2-4 70.0 66.9 70.5 74.8 66.9 66.9 62.6 66.2 59.7 48.2 56.8 46.8 36. 67.6 49.6 74.8 2-5 84.1 83.4 84.1 82.1 77.1 81.1 75.7 81.1 80.4 67.0 76.4 64.8 49.2 85.4 65.5 85. 3-1 66.9 58.8 67.5 67.5 60.6 66.3 58.8 66.3 62.5 53.1 60.0 50.6 41.9 59.4 50.6 64.4 363.5 80.5 71.1 68.6 66.7 66.7 57.2 61.6 57.9 29.6 57.9 37.7 20.1 78.6 64.8 80.5 3-3 80.2 82.2 78.2 81.8 67.6 63.0 55.8 62.8 51.4 47. 64.0 28.4 10.0 64.8 63.6 65.6 3-4 82.9 77.9 83.9 79.3 78.3 78.3 79.3 80.2 73.7 60.1 82. 72.8 45.2 84.3 63.6 77.4 4-3 79.0 72.6 84.8 82.6 68.0 77.4 55.8 76.8 68.9 59.5 65.5 41.5 35. 82.9 63.7 75.0 Table 31: Chinese questions results of IPBench. The best-performing model in each task is in darker red , and the second best is in lighter red . The model DS-Qwen refers to DeepSeek-R1Distill-Qwen, while the suffix it indicates the Instruct version of the model. OA denotes the overall average accuracy on the choice tasks. Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DISC-LawLLM Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B IPC Classification (1-5-1) Exact-Match Section Class Subclass 8.0 1.0 20.2 8.0 2.3 5.0 0.4 1.0 0.0 0. 0.4 0.0 0.0 19.6 0.0 3.9 76.4 76.0 80.4 79.8 68.6 77.9 65.1 72.4 66.3 49.9 34.3 51.2 17. 83.2 29.3 80.9 70.5 66.1 72.8 71.0 57.9 64.8 51.8 56.4 51.0 26.1 25.5 30.9 4.2 75.4 8.5 71.5 62.1 53.7 66.3 62.3 46.3 55.2 34.1 45.9 34.9 16. 16.8 15.5 0.2 67.6 1.2 60.7 Table 32: Results of Chinese Patent IPC Classification task (1-5-1). The best-performing model in each task is in darker purple , and the second best is in lighter purple . G.3.1 Zero-shot Results The zero-shot results for the English portion of IPBench are shown in Table 42, Table 43 and Table 44. G.3.2 Few-shot Results The 1-shot results for the English portion of IPBench are shown in Table 45 and Table 46, the 2-shot results in Table 47 and Table 48 and the 3-shot results in Table 49 and Table 50. G.3.3 Chain-of-Thought Results The chain-of-thought results for the English portion of IPBench are presented in Table 51 and Table 52. G.4 LLM-as-a-judge Results We provide detailed results of the LLM-as-a-judge evaluation for the overall, Chinese, and English parts. The evaluation includes four dimensions and an overall score, as shown in Table 53, Table 54, 44 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) BLEU R-L BS LLMScore Tokens # BLEU R-L BS (1-10) (167.9) LLMScore Tokens # DC # (4.1) (457.8) (1-10) 17.7 17.9 12.4 13.7 20.5 24.9 11.9 14.7 17.7 11.8 31. 8.9 12.4 11.1 34.8 35.2 29.9 30.8 36.8 40.3 26.7 30.6 33.9 26.2 53.4 28.0 36.1 33.6 91.0 90.9 90.5 90.8 91.1 91.1 90.2 90.2 90.6 90.4 91. 89.3 90.3 90.2 8.77 8.51 8.92 8.50 8.29 7.89 7.16 7.74 8.07 7.24 7.91 7.89 7.80 8.84 278.7 224.9 273.8 379.6 190.4 261.2 554.9 215.5 247.8 479.5 335. 671.0 918.2 1403.8 12.7 15.0 10.8 9.6 11.0 7.3 4.7 6.5 5.7 3.5 7.7 9.3 5.4 5.4 25.0 28.6 23.4 20.3 21.5 19.8 13.1 19.1 20.8 10.7 28. 26.8 32.5 22.6 90.3 90.3 90.0 90.6 90.6 89.7 90.1 88.6 88.3 88.8 90.3 81.8 81.8 80.8 6.30 6.09 7.36 6.60 5.64 4.99 3.09 5.46 5.15 2.13 4. 7.17 3.69 7.05 658.3 497.9 799.7 1374.2 3453.3 4045.2 4932.2 678.5 577.1 4968.3 8306.6 1374.2 9878.2 5360.0 6.8 11.8 14.9 17.4 43.5 43.0 36.1 3.1 5.8 44.0 59. 15.8 89.8 37.8 Table 33: Results of Chinese generation tasks (4-1 and 4-2). The best-performing model in each task is in darker blue , and the second best is in lighter blue . R-L refers to ROUGE-L, BS refers to BERTScore, LLMScore refers to GPT-4 judge score (1-10), Avg Tokens # denotes the average number of tokens in the generated text, and Avg DC # indicates the average number of generated dependent claims. Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 73.1 69.9 58.9 68.1 53.7 67.5 DISC-LawLLM 55.7 31.6 Hanfei 1-1 91.5 94.6 81.5 86.9 71.0 95. 79.2 34.0 1-2 85.9 86.2 64.1 74.3 56.2 87.3 68.8 23.9 172.5 72.5 60.9 67.4 58.5 73.8 62.9 23.1 1-4 76.6 73.8 73.0 69.4 59.5 61. 69.1 33.7 1-6 67.2 71.0 59.2 64.8 47.0 71.0 51.5 33.1 165.9 66.6 61.7 60.7 52.3 63.0 44.8 49.0 2-1 43.6 43.6 43.6 46.8 38.8 44. 38.8 24.0 2-2 68.9 71.5 69.3 79.4 66.2 71.9 56.6 49.1 253.2 50.0 38.4 45.5 42.3 39.4 35.9 35.9 2-4 66.9 59.7 61.2 67.6 43.2 64. 46.0 29.5 2-5 81.4 76.1 45.9 82.7 62.1 77.4 70.8 24.6 363.1 58.1 56.3 64.4 53.8 63.1 55.6 38.1 3-2 74.2 64.2 64.8 64.8 35.2 60. 38.4 18.9 3-3 79.2 60.0 38.0 56.8 41.4 48.2 377.0 82.5 73.2 74.7 65.4 80.7 75.1 33.6 4-3 76.5 70.1 65.6 79.0 61.6 71. 31.1 27.4 Table 34: Chinese questions results of IPBench with 1-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) Exact-Match Section Class Subclass 3.6 30.2 3.8 0.8 0.0 76.8 82.4 59.6 45. 11.6 67.1 75.4 50.9 36.4 8.2 54.9 68.9 39.6 25.5 3.6 Table 35: Results of Chinese Patent IPC Classification task (1-5-1) with 1-shot setting. The bestperforming model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 73.5 70.8 58.9 68.7 56.6 70. DISC-LawLLM 57.6 31.7 Hanfei 1-1 91.9 95.0 79.5 86.5 73.0 95.8 76.5 30.5 187.3 86.2 63.4 76.1 58.0 87.3 66.3 19.6 1-3 69.1 70.8 63.6 66.7 58.5 75. 62.9 40.1 1-4 76.2 76.2 69.8 70.6 63.1 76.6 68.3 30.2 166.3 69.5 58.3 65.4 49.1 74.9 56.8 35.2 1-7 67.2 68.8 59.1 60.4 52.6 65. 45.8 34.4 2-1 44.8 39.6 46.8 46.8 44.4 47.6 38.0 25.6 264.5 68.9 64.0 75.9 63.2 71.1 56.1 35.5 2-3 58.3 51.9 36.5 46.8 42.3 43. 36.5 22.4 2-4 69.8 63.3 65.5 68.4 51.1 67.6 51.1 25.9 283.4 83.7 66.8 84.7 73.1 82.4 78.1 49.2 3-1 60.0 54.4 53.1 61.3 55.6 68. 54.4 44.4 3-2 73.0 60.4 54.7 63.5 37.1 61.0 34.6 12.0 381.0 67.2 38.0 58.6 48.6 52.2 3-4 73.7 83.0 66.4 77.9 66.4 83. 74.7 32.7 4-3 78.7 65.6 61.3 79.6 61.0 70.1 43.9 32.3 Table 36: Chinese questions results of IPBench with 2-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . and Table 55. The definitions of these metrics are provided in F.3, with all scores ranging from 1 to 10. 45 Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) Exact-Match Section Class Subclass 2.5 29.4 3.6 1.0 0.4 75.4 82.8 70.5 34. 29.9 65.9 76.0 60.0 28.4 23.1 54.5 69.1 47.6 18.9 15.4 Table 37: Results of Chinese Patent IPC Classification task (1-5-1) with 2-shot setting. The bestperforming model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 74.7 72.1 59.3 68.7 56.3 69. DISC-LawLLM 58.0 30.1 Hanfei 1-1 91.5 95.0 81.1 87.6 72.2 95.8 81.9 27.8 187.3 84.4 60.9 75.0 59.8 87.0 69.2 22.8 1-3 73.5 69.4 59.5 66.0 56.8 74. 60.9 28.9 1-4 77.0 77.0 68.7 69.0 65.1 75.8 70.6 25.8 170.1 71.6 60.9 64.5 51.2 72.5 61.0 34.3 1-7 67.9 69.8 64.0 63.3 53.6 65. 47.4 34.7 2-1 47.2 45.6 47.6 47.6 45.6 51.2 40.4 27.2 264.0 73.7 57.5 75.0 62.3 70.6 53.5 41.7 2-3 59.6 43.6 32.1 48.1 41.0 37. 35.9 24.4 2-4 71.9 65.5 58.3 69.8 52.5 68.4 53.2 20.9 282.7 84.7 75.4 85.1 73.8 80.4 74.4 30.6 3-1 62.5 58.1 53.1 63.8 58.1 67. 58.1 43.1 3-2 73.6 65.4 55.3 61.6 39.0 55.4 34.0 22.0 380.6 69.6 37.8 60.0 43.4 47.4 3-4 79.3 83.4 67.7 76.5 65.4 81. 67.7 27.2 4-3 79.3 68.9 64.6 77.4 60.7 72.9 43.9 39.9 Table 38: Chinese questions results of IPBench with 3-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) Exact-Match Section Class Subclass 2.3 29.4 3.6 0.4 0.6 76.4 83.8 71.1 46.1 59.4 66.3 76.9 60.2 33. 42.1 55.6 70.8 48.0 23.2 31.4 Table 39: Results of Chinese Patent IPC Classification task (1-5-1) with 3-shot setting. The bestperforming model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 72.4 70.9 62.3 63.1 54.5 60.5 DISC-LawLLM 52.5 29.1 Hanfei 192.3 93.4 83.4 85.3 76.1 91.5 83.8 44.0 1-2 84.1 86.2 63.8 69.9 55.8 80. 66.3 25.0 1-3 70.8 72.8 62.9 57.8 57.8 64.6 62.9 35.0 177.8 79.4 66.7 63.1 60.3 57.9 63.5 37.7 1-6 65.4 63.0 58.3 59.8 48.8 55. 61.0 34.9 1-7 66.9 65.3 55.8 52.3 50.0 60.7 49.0 26.3 245.2 46.0 52.4 46.8 45.6 39.2 37.2 24.8 2-2 68.9 74.1 66.7 75.0 59.7 54. 58.8 25.9 2-3 59.6 55.1 39.1 49.4 45.5 44.9 25.6 22.4 260.4 61.9 51.8 59.7 51.1 54.0 48.2 25.2 2-5 80.4 79.7 71.8 76.4 65.8 72. 62.8 24.3 3-1 57.5 59.4 58.8 59.4 56.9 45.0 43.8 35.0 366.0 69.8 51.6 61.0 39.6 55.3 37.7 17.6 3-3 81.00 67.00 62.20 53.80 44.80 58. 30.40 23.00 3-4 75.1 78.3 71.4 67.3 61.8 65.9 60.4 24.4 471.3 67.1 61.0 70.7 54.3 45.4 41.5 38.4 Table 40: Chinese questions results of IPBench with chain-of-thought setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) Exact-Match Section Class Subclass 0.4 2.5 1.7 0. 0.0 75.4 77.1 58.7 56.6 3.6 67.6 70.9 50.1 41.3 2.9 56.0 64.2 36.4 26. 2.1 Table 41: Results of Chinese Patent IPC Classification task (1-5-1) with chain-of-thought setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . 46 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 73.2 71.4 72.9 71.6 65.2 70.3 58.2 67.0 64.3 55.4 61.8 DISC-LawLLM 50.2 40.3 Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B 71.6 56.0 70. 1-1 97.1 97.1 95.9 95.4 90.9 96.3 92.5 92.5 91.7 85.5 94.6 70.5 60.6 96.3 76.4 95.9 191.6 88.9 90.3 88.1 80.1 89.4 78.8 83.2 81.9 70.8 79.6 60.6 46.0 92.9 58.9 88.5 1-3 82.5 85.0 90.3 87.4 84.0 86.4 71.8 83.5 79.6 68. 82.5 71.8 53.9 92.2 58.3 85.4 1-4 86.9 83.7 82.5 85.3 77.4 83.3 64.0 79.8 59.5 61.5 73. 56.8 37.7 82.1 50.4 75.8 1-6 50.7 49.3 50.7 48.4 49.3 48.4 37.0 45.7 45.7 35.2 41.6 39.3 30. 51.1 48.0 47.0 1-7 69.6 68.8 67.1 67.5 60.0 67.1 62.9 62.9 61.7 57.1 63.8 55.0 42.1 70.0 52.5 68. 2-1 58.8 56.0 65.6 60.8 51.2 58.8 44.8 59.6 57.2 41.6 52.8 43.6 33.2 62.4 43.6 64.8 253.3 62.1 55.9 54.4 54.4 58.8 47.6 54.8 55.1 52.2 52.9 55.2 47.4 54.4 49.3 57.7 2-3 60.6 61.3 66.9 63.8 62.5 58.1 47.5 58.8 48.1 44. 42.5 22.5 40.6 77.5 53.8 74.4 2-4 85.8 85.2 82.1 85.2 80.9 81.5 79.0 85.2 79.6 77.8 77. 71.6 44.4 87.0 82.1 84.6 3-1 75.7 77.1 77.1 76.4 75.7 75.0 61.4 72.9 70.0 59.3 77.1 57.1 43. 67.9 57.9 75.7 3-2 77.2 69.2 79.2 78.5 75.8 82.6 66.4 71.1 76.5 63.1 71.1 53.0 37.6 77.9 59.1 83. 3-3 82.4 81.0 79.6 78.0 64.0 71.2 45.4 51.6 52.4 40.4 48.2 28.0 27.8 69.6 63.8 69.0 384.2 79.2 83.1 82.5 78.1 77.6 74.9 80.3 78.7 71.0 75.4 69.4 53.0 79.2 63.8 76.5 3-5 50.0 44.0 44.6 43.3 38.9 45.2 41.7 43. 43.9 47.5 43.6 47.1 4-3 68.4 54.4 67.3 61.4 41.5 59.7 45.6 48.0 49.1 45.0 40. 23.4 18.7 57.9 38.0 59.7 Table 42: English questions results of IPBench. The best-performing model in each task is in darker red , and the second best is in lighter red . The model DS-Qwen refers to DeepSeek-R1Distill-Qwen, while the suffix it indicates the Instruct version of the model. OA denotes the overall average accuracy on the choice tasks. Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DISC-LawLLM Hanfei DeepSeek-R1 DS-Qwen-7B QwQ-32B IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 2.0 1.0 2.3 2.2 1.5 2.2 1.3 1.3 0.5 0.2 0.8 0.0 0. 3.2 0.0 2.0 86.2 84.5 86.7 84.7 84.0 82.7 77.7 73.3 80.2 82.3 42.8 83.0 6.9 88.0 12.9 86.3 72.0 67.5 73.8 69.8 67.5 66.3 60.0 58.3 59.7 57. 33.2 61.3 0.0 74.0 5.5 69.5 49.0 47.0 51.3 49.0 46.8 45.3 37.3 37.7 39.2 35.5 23.3 39.5 0. 52.0 1.7 47.7 3.3 0.5 9.5 2.5 0.2 1.0 0.0 0.2 0.2 0.0 0.0 0.0 0.0 8.5 0.0 0.5 82.7 79.0 84.0 81.5 65.5 79.5 63.8 70.5 56.2 39. 8.5 31.0 0.9 82.5 5.1 76.0 69.7 64.5 73.3 69.5 44.8 64.3 45.0 56.7 39.0 21.5 3.1 23.4 0. 71.2 0.5 62.3 62.0 52.7 65.2 60.7 34.8 52.7 30.7 44.3 26.7 10.3 1.8 11.5 0.0 63.2 0.2 51.3 Table 43: Results of English Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2). The bestperforming model in each task is in darker purple , and the second best is in lighter purple ."
        },
        {
            "title": "H More Details about Error Analysis",
            "content": "Definition of Different Error Type. As illustrated in Figure 4b, we classify the error into 7 types: Consistency error, Hallucination error, Reasoning error, Refusing error, Priority error, Mathematical error and Obsolescence error. The detailed definitions of each error type are as follows: Consistency error: The content in the models response is inherently flawed or internally inconsistent, such as when the intermediate reasoning steps contradict the models final answer. Hallucination error: The large language models responses sometimes introduce fabricated legal information or include statements that sound plausible but are factually incorrectparticularly in Tasks 14, which require familiarity with typical legal cases. Reasoning error: This type refers to flaws in the logical process used by the model to arrive at its answer. These errors may include invalid deductions, misinterpretation of conditions, or incorrect application of domain-specific rules. In many cases, the models intermediate reasoning steps fail to logically support its final conclusion, even if the answer appears superficially plausible. Such issues are particularly critical in the second-level tasks of 47 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) BLEU R-L BS LLMScore Tokens # BLEU R-L BS (1-10) (129.0) LLMScore Tokens # DC # (13.1) (417.4) (1-10) 17.7 28.8 26.7 28.3 34.2 37.1 28.4 24.7 25.5 28.6 31.1 18.7 7.0 22.2 27.3 28.6 26.8 30.4 34.6 36.1 30.1 24.4 24.9 28. 48.6 27.6 9.7 30.4 87.7 88.4 87.4 88.3 89.3 89.7 88.1 87.6 87.4 88.4 89.1 85.7 76.8 85.6 8.07 7.59 7.84 8.17 8.07 8.07 7.79 7.54 7.76 7. 7.56 7.55 7.36 8.17 264.2 211.3 218.3 272.5 227.9 191.8 359.7 171.2 190.7 243.9 296.8 613.6 686.9 849.5 25.2 25.5 27.4 10.3 19.2 24.8 11.4 23.8 23.8 10. 24.8 23.8 17.9 19.8 28.0 28.0 30.1 13.9 23.1 27.8 14.6 26.1 25.6 12.8 40.3 31.8 32.2 29.1 87.4 86.5 88.0 87.8 87.8 86.5 86.7 86.0 86.0 87. 87.7 61.1 56.2 63.1 6.97 6.66 7.54 6.01 5.71 6.36 4.64 6.49 5.95 4.72 5.34 7.19 4.62 7.14 637.4 458.4 583.7 6207.6 3569.3 543.6 7643.6 486.1 446.7 8117. 1936.5 1231.6 2315.7 4635.4 6.2 1.1 14.8 120.8 48.0 13.6 145.4 3.4 7.0 148.5 35.5 22.4 18.4 45.7 Table 44: Results of English generation tasks (4-1 and 4-2). The best-performing model in each task is in darker blue , and the second best is in lighter blue . R-L refers to ROUGE-L, BS refers to BERTScore, LLMScore refers to GPT-4 judge score (1-10), Avg Tokens # denotes the average number of generated tokens, and Avg DC # denotes the average number of generated dependent claims. Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 75.4 65.1 59.9 64.6 56.2 57. DISC-LawLLM 54.1 32.9 Hanfei 1-1 96.3 93.8 93.0 91.7 87.6 95.4 78.4 40.3 186.7 77.9 76.5 74.3 68.1 74.4 64.6 35.8 1-3 87.4 82.0 76.7 76.2 70.4 79. 71.4 36.9 1-4 82.9 71.8 71.0 77.0 60.3 71.8 61.9 28.6 152.1 45.2 37.9 41.6 37.9 42.9 35.2 35.2 1-7 67.5 59.6 68.3 60.8 53.8 66. 61.7 36.7 2-1 59.6 54.0 48.4 54.8 50.4 49.6 41.2 29.2 257.4 54.0 47.8 52.9 50.0 54.8 52.9 27.2 2-3 69.4 54.4 48.8 54.4 30.6 39. 28.1 35.6 2-4 85.8 79.6 76.5 77.2 77.8 79.0 56.8 30.3 363.1 58.1 56.3 64.4 53.8 63.1 55.6 38.1 3-2 79.9 75.2 75.2 77.2 62.4 57. 40.9 26.2 3-3 83.0 51.2 35.6 51.0 37.4 25.0 383.1 76.0 73.2 77.0 69.4 61.2 57.9 27.9 4-3 58.5 46.8 40.9 51.5 48.5 31. 29.8 35.1 Table 45: English questions results of IPBench with 1-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 1.0 2.0 0.8 0.6 0.7 86.2 89.7 86.2 80.0 78.0 67.0 75.7 64.0 61.3 57. 47.3 53.2 44.5 40.8 37.8 0.5 7.3 0.3 0.0 0.0 74.3 86.2 67.5 45.2 16. 59.1 74.3 48.8 35.2 7.8 49.1 65.0 37.2 22.2 4.3 Table 46: Results of English Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 1-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 75.2 68.3 60.1 66.0 58.0 62. DISC-LawLLM 55.8 32.8 Hanfei 1-1 96.7 94.6 92.5 91.7 87.1 96.3 82.2 41.9 188.5 79.7 72.1 76.5 71.7 80.1 68.1 31.0 1-3 88.4 83.5 75.2 78.2 73.3 80. 71.8 29.6 1-4 84.1 77.8 69.4 73.8 59.9 73.4 62.3 37.3 152.1 45.2 43.4 42.5 37.4 46.6 40.2 35.6 1-7 65.8 60.0 65.0 62.1 55.4 62. 58.3 31.3 2-1 59.2 56.0 49.2 56.0 49.6 49.2 41.2 25.2 255.5 53.3 50.4 51.8 51.1 52.9 51.8 38.2 2-3 68.1 60.0 54.4 57.5 33.8 42. 26.3 42.5 2-4 87.7 79.0 76.5 80.2 77.2 81.5 64.2 27.2 377.1 82.9 67.9 72.9 68.8 76.4 61.4 40.0 3-2 84.6 79.2 68.5 81.9 61.1 68. 50.3 32.9 3-3 80.4 64.6 37.6 56.2 45.6 46.6 380.3 79.8 73.8 78.1 68.9 77.6 67.8 31.7 4-3 60.8 42.7 39.8 52.6 48.0 26. 24.0 12.9 Table 47: English questions results of IPBench with 2-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . 48 Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 2.17 2.67 1.50 1.30 0.83 88.2 90.2 85.0 81.5 80.0 69.8 76.2 64.5 59. 58.0 48.8 53.2 45.0 38.2 36.7 0.2 7.2 0.3 0.0 0.2 76.3 86.5 68.5 63. 32.3 61.6 73.3 51.2 45.7 17.3 51.6 65.7 38.8 26.7 9.3 Table 48: Results of English Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 2-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 74.7 70.6 59.4 67.4 56. 65.3 DISC-LawLLM 56.6 28.9 Hanfei 1-1 97.5 93.4 94.2 91.3 89.6 96.7 85.9 36. 1-2 87.6 81.9 74.4 77.4 69.0 79.2 65.0 36.3 1-3 88.4 79.6 76.3 77.2 71. 81.1 69.9 22.3 1-4 82.9 75.0 71.0 72.2 58.3 76.2 62.7 31. 1-6 53.0 48.0 40.2 44.7 37.9 47.0 42.5 27.9 1-7 70.0 66.3 63.3 61.4 55. 65.0 57.9 27.5 2-1 57.6 56.0 43.6 54.8 49.2 47.6 42.0 20. 2-2 54.0 52.6 53.7 51.5 50.7 54.4 55.9 31.6 2-3 67.5 58.8 53.8 55.0 31. 43.1 23.1 33.8 2-4 87.0 82.7 73.5 82.1 74.1 82.1 69.8 24. 3-1 79.3 85.0 70.0 73.6 69.3 78.6 62.1 38.6 3-2 82.6 79.9 70.5 77.9 62. 69.1 49.0 26.9 3-3 79.4 67.4 35.6 57.4 36.8 29.1 3-4 80.9 80.9 73.8 76.0 70.5 76.0 67.2 25.1 4-3 66.7 43.9 40.4 57.9 53. 27.5 28.1 24.0 Table 49: English questions results of IPBench with 3-shot setting. The best-performing model in each task is in darker red , and the second best is in lighter red . Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 1.8 3.5 1.2 1.8 1.3 87.8 90.0 85.7 82.7 80.3 69.2 75.7 64.8 62. 59.3 46.8 52.8 45.7 41.2 36.7 0.3 7.7 0.5 0.0 0.0 80.1 85.7 68.3 64. 24.2 65.3 73.3 50.8 45.8 12.8 54.1 64.7 38.8 29.8 7.7 Table 50: Results of English Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with 3-shot setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o-mini Qwen2.5-7B-it Llama3.1-8B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen OA 72.2 64.4 60.4 60.3 54. 60.5 DISC-LawLLM 19.5 30.5 Hanfei 1-1 96.7 84.2 84.9 88.8 85.5 94.6 45.6 39. 1-2 88.1 78.8 76.6 75.2 72.6 79.2 46.5 32.7 1-3 88.4 78.6 74.3 76.2 71. 82.5 28.2 28.6 1-4 82.9 73.0 73.8 69.4 64.7 72.6 15.1 30. 1-6 51.6 48.9 46.6 45.7 35.6 42.5 15.1 24.2 1-7 67.9 60.0 64.2 60.0 59. 63.3 31.7 27.5 2-1 57.6 50.0 46.4 55.2 39.2 51.2 13.6 32. 2-2 57.4 55.9 55.5 56.3 50.4 50.4 14.7 32.7 2-3 64.4 61.3 52.5 46.3 43. 45.6 26.3 20.6 2-4 87.0 84.0 79.0 80.9 75.3 77.8 18.5 26. 3-1 75.0 75.0 66.4 74.3 56.4 73.6 7.1 33.6 3-2 76.5 70.5 67.8 75.2 63. 70.5 12.8 36.2 3-3 81.2 63.2 47.4 62.0 37.2 40.4 13.6 26. 3-4 83.1 78.8 74.9 80.9 72.1 77.1 4.6 40.4 3-5 44.9 44.3 43.6 36. 43.9 4-3 58.5 43.3 40.9 54.4 35.7 41.5 9.4 29. Table 51: English questions results of IPBench with chain-of-thought setting. The best-performing model in each task is in darker red , and the second best is in lighter red . IPBench, which demand accurate multi-step and conditional reasoning within legal and technical contexts. Refusing error: This error typically occurs in Tasks 14, which require the model to recall specific factual or legal cases. In these instances, some models respond by asking the user for additional information or by explicitly refusing to provide an answer. While such refusals may be more cautious or aligned with reliability principles, they still indicate limitation in the models ability to engage with the task as expected. 49 Model GPT-4o-mini DeepSeek-V3 Qwen2.5-7B-it Llama3.1-8B-it MoZi-qwen IPC Classification (1-5-1) CPC Classification (1-5-2) Exact-Match Section Class Subclass Exact-Match Section Class Subclass 0.0 0.3 1.8 1.5 0.5 84.8 86.8 88.0 76.2 38. 67.3 73.0 69.5 58.5 29.8 47.8 51.5 47.0 37.5 21.8 0.0 1.0 0.5 0.2 0. 76.8 83.3 60.2 64.0 7.7 63.0 70.7 46.0 44.8 2.8 52.5 63.0 35.7 29.5 1. Table 52: Results of English Patent IPC/CPC Classification tasks (1-5-1 and 1-5-2) with chain-ofthought setting. The best-performing model in each task is in darker purple , and the second best is in lighter purple . Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) Acc. Rel. Comp. Cons. L-S LLMScore Acc. Rel. Comp. Cons. L-S LLMScore 8.45 7.99 8.26 8.40 8.17 7.98 7.52 7.63 7.89 7.47 7.71 7.70 7.58 8. 8.24 8.02 8.45 8.18 8.14 8.03 7.41 7.78 8.03 7.38 7.88 7.75 7.50 8.39 8.68 8.13 8.53 8.70 8.19 7.94 7.71 7.46 7.82 7.86 7.78 7.88 7.78 8. 9.27 8.94 9.15 9.36 9.08 8.96 8.57 8.40 8.76 8.62 8.76 8.39 8.43 9.27 7.58 7.47 7.73 7.37 7.61 7.31 6.54 7.32 7.43 6.40 7.02 7.21 6.90 7. 8.42 8.05 8.38 8.33 8.18 7.98 7.47 7.64 7.91 7.49 7.73 7.72 7.58 8.51 7.45 7.17 7.93 7.13 6.59 6.57 4.70 6.51 6.21 4.19 5.82 7.73 4.67 7. 6.28 5.92 7.30 5.77 5.47 5.38 3.95 5.56 5.23 3.30 4.70 6.76 4.02 6.61 6.22 6.06 7.13 6.00 5.09 5.16 3.18 5.71 5.20 3.07 4.00 7.00 3.97 6. 6.58 6.30 7.38 6.35 5.68 5.69 3.91 5.84 5.51 3.38 4.83 7.16 4.01 7.13 7.17 7.02 7.92 6.72 5.96 6.21 4.15 6.54 6.12 3.71 5.17 7.69 4.60 7. 6.63 6.37 7.45 6.30 5.67 5.67 3.86 5.98 5.55 3.42 4.81 7.18 4.16 7.10 Table 53: Multi-dimension results of generation tasks (4-1 and 4-2) in LLM-as-a-judge. The bestperforming model in each task is in darker blue , and the second best is in lighter blue . Accuracy (Acc.), Relevance (Rel.), Completeness (Comp.), Consistency (Cons.), L-S and LLMScore are generation quality metrics rated by an LLM-as-a-judge. Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) Acc. Rel. Comp. Cons. L-S LLMScore Acc. Rel. Comp. Cons. L-S LLMScore 8.66 8.39 8.80 8.53 8.21 7.85 7.12 7.65 7.94 7. 7.79 7.76 7.68 8.71 8.77 8.59 9.05 8.55 8.50 8.22 7.30 7.98 8.30 7.35 8.40 8.01 7.84 8.82 8.96 8.46 9.08 8.86 8.09 7.76 7.47 7.52 7.98 7. 7.60 8.05 8.08 9.10 9.50 9.24 9.54 9.47 9.05 8.88 8.31 8.45 8.86 8.50 8.75 8.30 8.10 9.09 8.00 8.00 8.27 7.56 8.03 7.26 6.11 7.49 7.52 6. 7.47 7.42 6.97 8.09 8.77 8.51 8.92 8.50 8.29 7.89 7.16 7.74 8.07 7.24 7.91 7.89 7.80 8.84 7.06 6.93 7.90 7.33 6.56 6.04 3.55 5.89 5.82 2. 5.33 7.74 4.26 7.70 5.85 5.59 6.98 6.14 5.45 4.80 3.48 5.14 4.87 2.22 4.23 6.74 3.71 6.55 5.91 5.69 7.09 6.36 5.11 4.37 2.47 5.33 4.88 1. 3.42 6.98 3.30 6.92 6.27 6.10 7.40 6.61 5.58 5.00 3.03 5.19 5.14 2.05 4.34 7.16 4.47 7.13 6.94 6.85 7.93 7.12 5.89 5.45 3.49 5.92 5.68 2. 4.52 7.69 4.00 7.58 6.30 6.09 7.36 6.60 5.64 4.99 3.09 5.46 5.15 2.13 4.28 7.17 3.69 7.05 Table 54: Multi-dimension results of Chinese generation tasks (4-1 and 4-2) in LLM-as-a-judge. The best-performing model in each task is in darker blue , and the second best is in lighter blue . Accuracy (Acc.), Relevance (Rel.), Completeness (Comp.), Consistency (Cons.), L-S and LLMScore are generation quality metrics rated by an LLM-as-a-judge. Priority error: Priority Error refers to the models failure to identify and prioritize the most critical factor(s) when multiple elements jointly influence the outcome. Instead of focusing on the decisive issue, the model may weigh secondary or irrelevant aspects equally, leading to incorrect or misleading conclusions. 50 Model GPT-4o GPT-4o-mini DeepSeek-V3 Qwen2.5-72B-it Qwen2.5-7B-it Llama3.1-70B-it Llama3.1-8B-it Gemma-2-27B-it Gemma-2-9B-it Mistral-7B-it MoZi-qwen DeepSeek-R1 DS-Qwen-7B QwQ-32B Abstract Generation (4-1) Dependent Claim Generation (4-2) Acc. Rel. Comp. Cons. L-S LLMScore Acc. Rel. Comp. Cons. L-S LLMScore 8.24 7.59 7.72 8.28 8.12 8.12 7.93 7.61 7.85 7.75 7.63 7.63 7.48 8. 7.70 7.45 7.86 7.82 7.78 7.84 7.51 7.58 7.75 7.42 7.37 7.48 7.15 7.96 8.40 7.81 7.99 8.54 8.30 8.12 7.96 7.40 7.67 7.99 7.97 7.70 7.48 8. 9.04 8.64 8.76 9.25 9.10 9.03 8.82 8.35 8.65 8.73 8.77 8.47 8.76 9.44 7.16 6.94 7.20 7.19 7.20 7.36 6.98 7.15 7.34 6.76 6.57 6.99 6.82 7. 8.07 7.59 7.84 8.17 8.07 8.07 7.79 7.54 7.76 7.75 7.56 7.55 7.36 8.17 7.85 7.42 7.96 6.93 6.63 7.10 5.84 7.13 6.60 5.89 6.32 7.71 5.07 7. 6.71 6.26 7.63 5.40 5.48 5.96 4.43 5.97 5.59 4.39 5.18 6.77 4.32 6.66 6.53 6.43 7.17 5.64 5.07 5.95 3.89 6.10 5.53 4.18 4.59 7.02 4.64 7. 6.88 6.49 7.36 6.09 5.78 6.38 4.79 6.48 5.88 4.71 5.32 7.16 3.55 7.13 7.40 7.20 7.92 6.33 6.03 6.97 4.80 7.17 6.56 5.07 5.83 7.68 5.19 7. 6.97 6.66 7.54 6.01 5.71 6.36 4.64 6.49 5.95 4.72 5.34 7.19 4.62 7.14 Table 55: Multi-dimension results of English generation tasks (4-1 and 4-2) in LLM-as-a-judge. The best-performing model in each task is in darker blue , and the second best is in lighter blue . Accuracy (Acc.), Relevance (Rel.), Completeness (Comp.), L-S and LLMScore are generation quality metrics rated by an LLM-as-a-judge. Mathematical error: This error type refers to issues related to lack of precision in complex calculations, often resulting in incorrect outcomes. These errors can arise from miscalculations, rounding mistakes, or failure to properly apply mathematical operations, leading to significant discrepancies in the final result. This is particularly evident in Tasks 23, Compensation Calculation, where both IP law knowledge and an understanding of the case background are necessary to perform accurate calculations. Obsolescence error: Obsolescence Error refers to the models failure to account for differences between current and outdated versions of legal documents or frameworks. This error occurs when the generated answer overlooks changes in the law, leading to outdated or inaccurate information. This is especially relevant in Tasks 13, Legal Evolution, where the model must retain knowledge of both current and past laws and understand the differences between them. However, some models do not update their memory, resulting in the use of obsolete information. As illustrated in Figure 4b, the most common error type is reasoning error, accounting for 33%. This is consistent with the performance decrease observed in models using the Chain-of-Thought setting. This highlights the importance of developing an IP-oriented model that balances both System 1 and System 2 capabilities. Case Study for Each Error Type. We provide two examples, one in Chinese and one in English, for each error type, as shown from Figure 6 to Figure 12. More extensive case studies for each task can be found in Appendix J."
        },
        {
            "title": "I Data Examples",
            "content": "We provide extensive data examples for each task in this section, as shown from Figure 13 to Figure 33. These examples include both English and Chinese datapoints, serving as representative samples for each corresponding task and helping to better illustrate the task definitions. 51 Figure 6: Consistency error case study. Figure 7: Reasoninig error case study. 52 Figure 8: Refusing error case study. Figure 9: Obsolescence error case study. 53 Figure 10: Priority error case study. Figure 11: Hallucination error case study. 54 Figure 12: Mathematical error case study. Figure 13: Data example of task 1-1. 55 Figure 14: Data example of task 1-2. Figure 15: Data example of task 1-3. 56 Figure 16: Data example of task 1-4. Figure 17: Data example of task 1-5-1. 57 Figure 18: Data example of task 1-5-2. Figure 19: Data example of task 1-6. 58 Figure 20: Data example of task 1-7. Figure 21: Data example of task 2-1. 59 Figure 22: Data example of task 2-2. Figure 23: Data example of task 2-3. 60 Figure 24: Data example of task 2-4. Figure 25: Data example of task 2-5. 61 Figure 26: Data example of task 3-1. Figure 27: Data example of task 3-2. 62 Figure 28: Data example of task 3-3. Figure 29: Data example of task 3-4. 63 Figure 30: Data example of task 3-5. Figure 31: Data example of task 4-1. 64 Figure 32: Data example of task 4-2. Figure 33: Data example of task 4-3."
        },
        {
            "title": "J Case Study",
            "content": "We provide extensive case studies for each task, including both correct and erroneous responses in both Chinese and English, as shown below. These case studies offer deeper insight into the scope of the models capabilities in the field of intellectual property. Figure 34: Correct case of task 1-1. Figure 35: Error case of task 1-1. 66 Figure 36: Correct case of task 1-2. Figure 37: Error case of task 1-2. 67 Figure 38: Correct case of task 1-3. Figure 39: Error case of task 1-3. 68 Figure 40: Correct case of task 1-4. Figure 41: Error case of task 1-4. 69 Figure 42: Correct case of task 1-5-1. Figure 43: Error case of task 1-5-1. 70 Figure 44: Correct case of task 1-5-2. Figure 45: Error case of task 1-5-2. 72 Figure 46: Correct case of task 1-6. Figure 47: Error case of task 1-6. 73 Figure 48: Correct case of task 1-7. Figure 49: Error case of task 1-7. 74 Figure 50: Correct case of task 2-1. Figure 51: Error case of task 2-1. 75 Figure 52: Correct case of task 2-2. Figure 53: Error case of task 2-2. 76 Figure 54: Correct case of task 2-3. Figure 55: Error case of task 2-3. 77 Figure 56: Correct case of task 2-4. Figure 57: Error case of task 2-4. 78 Figure 58: Correct case of task 2-5. 79 Figure 59: Error case of task 2-5. Figure 60: Correct case of task 3-1. Figure 61: Error case of task 3-1. 81 Figure 62: Correct case of task 3-2. Figure 63: Error case of task 3-2. Figure 64: Correct case of task 3-3. Figure 65: Error case of task 3-3. 83 Figure 66: Correct case of task 3-4. Figure 67: Error case of task 3-4. Figure 68: Correct case of task 3-5. 85 Figure 69: Error case of task 3-5. 86 Figure 70: High-quality case of task 4-1. Figure 71: Low-quality case of task 4-1. 87 Figure 72: High-quality case of task 4-2. Figure 73: Low-quality case of task 4-2. 88 Figure 74: Correct case of task 4-3. Figure 75: Error case of task 4-3."
        }
    ],
    "affiliations": [
        "Dalian University of Technology, China",
        "Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China",
        "Shenzhen University of Advanced Technology, China",
        "The University of New South Wales, Australia"
    ]
}