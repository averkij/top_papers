{
    "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
    "authors": [
        "Thomas Palmeira Ferraz",
        "Kartik Mehta",
        "Yu-Hsiang Lin",
        "Haw-Shiuan Chang",
        "Shereen Oraby",
        "Sijia Liu",
        "Vivek Subramanian",
        "Tagyoung Chung",
        "Mohit Bansal",
        "Nanyun Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post \"in a funny tone\" with \"no hashtag\"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DeCRIM improves Mistral's performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 8 5 4 6 0 . 0 1 4 2 : r LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints Thomas Palmeira Ferraz*, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng Amazon AGI Foundations, Télécom Paris, Institut Polytechnique de Paris, UNC Chapel Hill, University of California, Los Angeles thomas.ferraz@alumni.usp.br {kartim,orabys}@amazon.com"
        },
        {
            "title": "Abstract",
            "content": "Instruction following is key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. request to create social media post in funny tone with no hashtag). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce REALINSTRUCT, the first benchmark designed to evaluate LLMs ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate modelbased evaluation as cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the DECOMPOSE, CRITIQUE, AND REFINE (DECRIM) self-correction pipeline, which enhances LLMs ability to follow constraints. DECRIM works by decomposing the original instruction into list of constraints and using Critic model to decide when and where the LLMs response needs refinement. Our results show that DECRIM improves Mistrals performance by 7.3% on REALINSTRUCT and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DECRIM can outperform GPT-4 on both benchmarks."
        },
        {
            "title": "1\nLarge Language Models (LLMs) have demon-\nstrated impressive instruction following ability\nacross various tasks, such as creative writing, cod-\ning, and arithmetic reasoning (Brown et al., 2020;\nWei et al., 2021; Mishra et al., 2022; Sanh et al.,\n2022; Ouyang et al., 2022; Wang et al., 2022). De-\nspite their remarkable success, recent studies and",
            "content": "*Work done while interning at Amazon AGI Foundations. 1 benchmarks have highlighted significant limitations in LLMs ability to adhere to user-defined rules (termed as constraints henceforth) within instructions (Mu et al., 2023; Zhou et al., 2023b; Lu et al., 2023; Sun et al., 2023; Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a; Qin et al., 2024). Table 1 illustrates an example where four strong LLMs make the same mistake in following constrained instruction. Based on analysis of real user instructions to AI assistants, we estimate that 30% of real-user requests to LLMs require satisfying some constraints (estimation details in Appendix D.1). This highlights the importance of evaluating and enhancing LLMs ability to follow real-world multi-constrained instructions. Benchmarking on Real-World Requests. These benchmarks often rely on synthetic data either to address data scarcity or to facilitate automatic rulebased evaluation (Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a). In this work, we argue that synthetic constraints may not accurately capture the complexity and nuances of real-world scenarios, being sometimes artificially difficult. As result, focusing on synthetic benchmarks may push research in the wrong direction, as improvements on these may not translate to real-world performance and could even degrade it. To address this gap, we introduce the REALINSTRUCT benchmark, which evaluates LLMs using real user requests to AI assistants. It assesses LLMs response on individual constraints at time, as illustrated in Figure 1. Our analysis reveals that even strong proprietary model GPT-4 fails to meet at least one constraint in over 21% of the instructions, highlighting the limitations of current LLMs in handling users constrained instructions. LLM-based evaluation. Evaluating real-world instructions is challenging due to their open-ended nature. Drawing on recent research on LLM-based evaluation (LLM-as-a-Judge) (Zheng et al., 2023; Table 1: Example of user instruction where all subject LLMs failed. Responses from four LLMs are shown. All responses incorrectly include hashtags, despite constraint explicitly requesting to not do so. Constraints in the instruction are highlighted in blue, and errors in LLM responses are highlighted in red. Zeng et al., 2024; Qin et al., 2024), we investigate the effectiveness of open and proprietary LLMs as evaluators for constraint satisfaction. To this end, we created test set of 1k instruction-constraintresponse triples with human-verified annotations. Our experiments show that GPT-4-Turbo with Chain-of-Thought prompting is cost-effective and reliable alternative to human evaluation. However, open-source models, such as Mistral, lag significantly behind proprietary models. Weakly supervising Mistral with GPT-4-Turbo reasoning steps led to 26% relative improvement, though, still remaining insufficient for reliable evaluation. LLM Self-Correction. Improving LLMs by generating and building upon intermediate outputs, known as System 2 Approaches, has been explored for constrained instruction following. But many works in this direction, such as Branch-SolveMerge (Saha et al., 2024) and Self-Refine (Madaan et al., 2023), assume constraint independence or focus on specific constraint types, limiting their applicability to real-world use cases. To address this, we introduce DECOMPOSE, CRITIQUE, AND REFINE (DECRIM), self-correction pipeline designed to enhance LLM performance in multiconstrained instruction following without making assumptions about the constraints. As shown in Figure 2, DECRIM breaks down the original instruction into main task and granular constraints, and includes Critic model that decides whether the response is final or requires refinement, which is handled by the underlying LLM. Through extensive benchmarking, we demonstrate the effectiveness of the DECRIM pipeline. Even with weak feedback and no external data, DECRIM improves Mistrals instruction-level performance by 4.8% on REALINSTRUCT and 1.2% on IFEval relative to baseline. When perfect instruction decomposition into constraints is provided, improvements increase to 7.3% and 8.0%, respectively. Providing stronger feedback further boosts Mistrals performance by 22.0% on REALINSTRUCT and 33.8% on IFEval, surpassing GPT-4 on both benchmarks. In summary, our work contributes the following: 1. REALINSTRUCT, the first benchmark of real user requests to LLMs for evaluating instruction following with multiple constraints; 2. DECRIM self-correction pipeline, to the best of our knowledge, the first System 2 approach for constrained instructions that operates without assumptions about constraints, showing significant improvements even with weak feedback; 3. the first systematic analysis of open-source and proprietary LLM-as-a-Judge models for evaluating constraint satisfaction. The rest of the paper is organized as follows: Sections 2 and 3 present the REALINSTRUCT dataset and the DECRIM self-correction pipeline, respectively. Section 4 outlines our hypotheses and experimental setup, while Section 5 analyzes the results. Final remarks are provided in Section 6. We provide extra details and discussions in the Appendix."
        },
        {
            "title": "2 REALINSTRUCT Dataset",
            "content": "We introduce REALINSTRUCT, novel dataset consisting of original user-generated instructions, each decomposed into task and set of constraints (see Figure 1 for an example). The task represents the users main objective the outcome they expect from LLM and may optionally include context to aid the models understanding. Constraints are conditions or limitations that guide the LLMs generation. Formal definitions of these terms are provided in Appendix C. Since users rarely specify constraints in structured list format, the decomposition breaks instructions into manageable items, providing the necessary granularity for evaluating LLMs ability to follow constraints. Using this dataset, we develop benchmark protocol that evaluates LLM performance on each constraint at time, generating constraint-level scores 2 Figure 1: REALINSTRUCT Benchmark Workflow. Real-user original instruction is input into the Subject LLM, which generates response. Using the original instruction, decomposed constraints, and the generated response, model-based evaluation assesses the quality of the response against each constraint one at time, and then aggregates the results into an instruction-level metric. Instruction source Synthetic Synthetic Crowdsourced + Synthetic"
        },
        {
            "title": "FollowBench",
            "content": "InfoBench REALINSTRUCT (ours)"
        },
        {
            "title": "Rule\nRule",
            "content": "Size (Instructions) 2,080 541 Constraint types 13 25 Avg. Constraints per Instruction N/A 1."
        },
        {
            "title": "Synthetic",
            "content": "Model / Rule 795 Crowdsourced Crowdsourced Model / Rule Real Users"
        },
        {
            "title": "Model",
            "content": "500 302 (test) + 842 (val) 6 5 5 4.5 20+ 3.5 (test) Table 2: Comparison of REALINSTRUCT samples with benchmarks such as COLLIE (Yao et al., 2024a), IFEval (Zhou et al., 2023a), FollowBench (Jiang et al., 2024) and InfoBench (Qin et al., 2024). that are then aggregated into an instruction-level accuracy metric. Figure 1 provides an overview of the benchmark protocol. Given the open-ended nature of real user instructions, rule-based evaluation is infeasible, so we adopted the LLM-as-a-Judge evaluation. detailed discussion and validation of this protocol are provided in Section 4.1. only 6.3% of REALINSTRUCT constraints overlap with the 25 types in IFEval, and 11 IFEval constraint types never appear in REALINSTRUCT. This discrepancy highlights the gap between synthetic datasets and real LLM use cases, with synthetic constraints failing to adequately represent the challenges users may pose to LLMs."
        },
        {
            "title": "2.2 Data Construction",
            "content": "The REALINSTRUCT dataset is divided into two splits: test and validation. The test split, intended for LLM evaluation, was human-validated and contains 302 instructions with 1,055 constraints. The validation split, used for method validation such as training judges, was not human-validated and includes 842 instructions with 2,500 constraints. Table 2 compares REALINSTRUCT with existing benchmarks for evaluating LLMs ability to follow multi-constrained instructions. Unlike other datasets that rely on synthetic or crowdsourced instructions, REALINSTRUCT uniquely captures real user interactions with AI assistants, offering more realistic representation of real-world use cases. To better understand constraint characteristics in REALINSTRUCT, we manually categorized them into 22 distinct groups. Detailed dataset statistics are presented in Appendix E. When comparing REALINSTRUCT with IFEval, one of the popular benchmark for constraint following that consists solely of synthetic constraints, we found that We built REALINSTRUCT dataset using prompts from public dataset of conversations between users and AI assistants1. We filtered and processed the dataset in five steps: 1) removed AI responses, retaining only the first user turn (instruction); 2) excluded non-English instructions; 3) filtered out code-related instructions using an open-source LLM as zero-shot classifier; 4) kept only instructions containing constraints, again using LLM classifier; and 5) manually validated the remaining data for relevance, clarity, and safety. See Appendix D.1 for details on the data filtering. Following filtering, we decomposed the instructions into tasks, contexts, and constraints using GPT-4 (Achiam et al., 2023). In our tests, we found that this three-part decomposition (task, context, constraints) provided more robust outcomes 1We use only the first user turn (no model responses) from public dataset of examples originally sourced from ShareGPT at: https://huggingface.co/datasets/anon8231489123 /ShareGPT_Vicuna_unfiltered 3 Figure 2: The DECRIM pipeline. Initially, the LLM generates response to user request. The Decomposer breaks down the request into granular constraints. Critic model then gives feedback on whether the response meets all constraints. If it does, the response is output; if not, the feedback is used by LLM to refine the response. This CritiqueRefine cycle repeats until all constraints are satisfied or the maximum number of iterations is reached. compared to splitting into just task and constraints. The task and context were then concatenated for further processing. subset of 302 instructions underwent manual inspection to ensure accuracy and granularity, forming the test split, while the remaining 842 instructions make up the validation split. See Appendix D.2 for more details on the data decomposition process."
        },
        {
            "title": "3 Self-correction with DECOMPOSE,",
            "content": "CRITIQUE, AND REFINE (DECRIM) In this section, we present our proposed DECRIM self-correction pipeline, designed to enhance LLM responses to follow user constraints. Given multi-constrained user instruction, the pipeline iteratively refines the LLMs response through four key steps: Initial Response, Decompose, Critique, and Refine, iterating until the response meets all constraints or maximum number of iterations Nmax is reached. Figure 2 provides an overview of the proposed pipeline and we detail each of these steps below: 1. Initial Response is generated directly from the original user instruction using strong prompt. 2. Decompose the original instruction into list of granular constraints to be followed. This task is similar to the instruction decomposition performed in REALINSTRUCT, but here, the decomposer model focuses solely on listing the constraints, simplifying the task. prompt example is provided in Appendix G.1. 3. Critique the response using the Critic model to identify any unsatisfied constraints. If all constraints are satisfied, the response is considered final. Otherwise, the Critic provides feedback in natural language, specifying which constraints were not satisfied. 4. Refine the response using the Critics feedback to address unsatisfied constraints. The underlying LLM generates an improved response using refinement prompt that incorporates the feedback, along with the original instruction and previous response. The Critique and Refine steps can be repeated iteratively until the response satisfies all constraints or the specified maximum number of iterations is reached. To our knowledge, DECRIM is the first approach specifically designed to tackle the challenge of following instructions with multiple open-ended constraints. Unlike previous methods, which assume constraint independence or focus on specific constraint types, our approach makes no assumptions about the nature of user constraints. In Section A.2 of Related Work, we compare our pipeline with other works for constrained generation, includ4 ing Self-Correction and System 2 approaches."
        },
        {
            "title": "Constraint Satisfaction",
            "content": "Given the open-ended nature of REALINSTRUCT benchmark instructions, rule-based or reference-guided evaluation is not feasible. Drawing from recent work showing the effectiveness of LLMs as evaluators, particularly GPT-4 (Zheng et al., 2023; Zeng et al., 2024), we adopt an LLM-as-a-Judge evaluation protocol for REALINSTRUCT. To assess LLM-as-a-judge reliability compared to human evaluators and identify the most cost-effective approach, we introduce EvalJudge, test set of instruction-constraint-response triples with ground-truth labels (Sec. 4.1.2). We benchmark both proprietary and open-source models using four adaptation strategies (Sec. 4.1.1) against human judgments on this set. More specifically, we investigate whether open-source models can match the performance of high-cost proprietary models in the LLM-as-a-Judge role."
        },
        {
            "title": "4.1.1 Adaptation strategies",
            "content": "Models We evaluate three proprietary and three open-source LLMs as candidates for LLM-as-aJudge for Constraint Satisfaction. The proprietary models include GPT-4 (gpt-4-0314), GPT4-Turbo (gpt-4-turbo-2024-04-09), and GPT3.5-Turbo (gpt-3.5-turbo-0125), ordered by decreasing API cost2. For the open-source models, we experiment with Mistral 7B Instruct v0.2 (Jiang et al., 2023) (hereafter referred to as Mistral v0.2), Vicuna 7B v1.3 (Chiang et al., 2023), and Zephyr 7B β (Tunstall et al., 2024), all top performers on the Open LLM Leaderboard as of February 2024 (Beeching et al., 2023). 2Refer to: https://openai.com/api/pricing We explore four adaptation strategies, including three In-Context Learning (ICL) approaches and one weakly supervised fine-tuning for open-source model. Some strategies use the Chain-of-Thought (CoT) prompt, known to improve LLM reasoning (Wei et al., 2022; Zheng et al., 2023), and we also investigate whether to evaluate constraints individually or collectively. The strategies are as follows: a) Instruction-wise Eval (ICL-Inst.): All constraints within an instruction are evaluated simultaneously. The LLM-as-a-Judge is presented with an instruction and list of constraints, and using CoT prompt with an in-context example containing an instruction and two constraints, it generates reasoning and predictions for all constraints at once. b) Constraint-wise Eval (ICL-Const.): Each constraint is evaluated independently. For every response-constraint pair, the LLM-as-a-Judge directly predicts \"Constraint followed\" or \"Constraint not followed.\" Two constraint-response pairs serve as in-context examples. c) Constraint-wise Eval + CoT (ICLConst.+CoT): The LLM-as-a-Judge is prompted to generate reasoning for each constraint, followed by prediction of \"Constraint followed\" or \"Constraint not followed.\" Evaluation is also performed for each response-constraint pair independently, using two constraints as in-context examples. d) Weakly Supervised Open LLM (Supervised): We fine-tuned Mistral for the LLM-as-a-judge task. We construct training data using the weak instruction annotations from REALINSTRUCTs validation set. We generate Mistral responses to these instructions, and weak constraint satisfaction annotations with reasoning trails from GPT-4-Turbo with ICLConst.+CoT prompt. We fine-tune Mistral v0.2 using LoRA adapters (Hu et al., 2022) on this dataset, guiding model to mimic GPTs reasoning. Further details can be found in Appendix H.4. All prompt templates are provided in Appendix H.1. Costs and processing times for each configuration are detailed in Table 3."
        },
        {
            "title": "4.1.2 The EvalJudge Dataset",
            "content": "Since no public dataset exists for the task of evaluating whether given response satisfies specific constraint or not, we create EvalJudge dataset, derived from the test split of the REALINSTRUCT dataset. To ensure diversity, we divided the instructions into two subsets, generating one 5 response per instruction using Mistral v0.2 for one subset and Vicuna v1.3 for the other. EvalJudge contains 294 instructions and 982 constraints. Notably, 81.4% of the samples are labeled as \"constraint satisfied.\" Ground truth and Baselines: a) Human Annotation: We create an Amazon Mechanical Turk (MTurk) task to collect independent labels from two pools of annotators for each constraint-response pair in the dataset. Guidelines are in Appendix H.2. b) Expert Annotation: To obtain third independent annotation, the authors manually annotated the dataset using the same MTurk guidelines. In cases where there was disagreement between the two human annotators, GPT-4, and the best GPT-4Turbo labels, second review was conducted by the authors. These conflicts accounted for 28.3% of the samples. The final labels, referred to as \"Expert,\" serve as the ground truth for EvalJudge. c) Other Baselines: To help interpret the results, we introduce two simple baselines representing extreme cases: \"All Satisfied,\" where the annotator labels all constraints as satisfied, and \"All Not Satisfied,\" where the annotator marks all constraints as unsatisfied. Additionally, we use the \"Majority Vote\" from the three human annotators to benchmark model performance against human judgment. Evaluation Metrics Unlike Qin et al. (2024), who used accuracy as the primary metric for LLMas-a-Judge evaluation, we account for the positive class imbalance in EvalJudge by using Macroaveraged F1-Score (Macro F1) as our main metric. Additionally, we report the F1-score for the negative class (F1 Negative), which measures the balance between false alarms and omissions. Different judges are compared to human judment using the Cohens kappa inter-rater reliability against human majority vote. We employ Krippendorffs alpha (an extension of kappa for more than two annotations) to assess inter-human agreement across the three annotations."
        },
        {
            "title": "4.2 Benchmarking LLMs on REALINSTRUCT",
            "content": "We benchmark models on REALINSTRUCT for the task of following multi-constrained instructions. The evaluation includes two proprietary models GPT-4 and GPT-3.5-Turboand three open-source LLMs: Mistral v0.2, Vicuna v1.3, and Zephyr β, selected based on their performance on Chatbot Arena (Chiang et al., 2024). The judge is GPT-4Turbo with ICL-Const+CoT prompt. Following Zhou et al. (2023a); Saha et al. (2024), we report accuracy at both the instruction and constraint levels."
        },
        {
            "title": "4.3 Evaluating DECRIM pipeline",
            "content": "the conduct effectiveness of our DETo validate experiments CRIM pipeline, we using Mistral v0.2 as the underlying LLM. We use Nmax = 10. We also investigate the contribution of different Decomposer and Critic models for the pipeline, and compare the performance against the proprietary model GPT-4. We present extra comparisons on Appendix B. Datasets We evaluate model performance on the REALINSTRUCT dataset and the IFEval dataset (Zhou et al., 2023a). While IFEval is based on synthetic constraints, it serves as valuable benchmark for validating our pipeline, as it is popularly used for evaluating constrained instruction-following. Baselines To measure the improvements introduced by our method, we establish the following baselines with Mistral v0.2: 1. Conventional: Only the instruction as input. 2. \"Make sure\": Appends the text Make sure to follow all the provided constraints to the instruction, creating strong and fair baseline. 3. Self-Refine: Adapts the Madaan et al.s (2023) approach for the case of multi-constraint instructions. While Self-Refine uses the model itself as its critic without additional context, our DECRIM pipeline employs critic with fine-grained evaluation, assessing each constraint individually. This baseline helps quantify the value added by this modeling. Decomposer We use Self-Decomposer, where the LLM itself lists relevant constraints, simplifying the decomposition from Section 2.2 by omitting the request for task and context. The prompt for this approach is detailed in Appendix G.1. Critic Model We explore two Critic models based on the underlying LLM: a) Self-Critic: Uses the model itself as the Critic, with the best ICL-based adaptation (from Section 4.1.1), specifically the ICL-Const+CoT prompt. b) Supervised Critic: the Mistral weakly supervised for LLM-as-a-judge, as described in 4.1.1. Ablations with Oracle and GPT-4 To understand the impact of strong Decomposer and Critic mod6 Annotator Expert Human 1 Human 2 Majority Vote Cost (USD) - 300.0 300.0 - Time (min) - - - - All Satisfied All Not Satisfied - - GPT-4 w/ ICL-Cons 19.5 GPT-3.5-Turbo w/ ICL-Cons 1.0 GPT-4-Turbo 4.1 w/ ICL-Inst w/ ICL-Cons 6.5 w/ ICL-Cons+CoT 8.3 Mistral v0.2 w/ ICL-Cons w/ ICL-Cons+CoT Supervised Zephyr β w/ ICL-Cons w/ ICL-Cons+CoT Vicuna v1.3 w/ ICL-Cons w/ ICL-Cons+CoT - - - - - - - - - - - - - - 10 26 236 11 9 27 Macro F1 (%) 100.0 85.1 80.0 96.4 44.9 15.7 73.7 F1 Neg. (%) 100.0 75.9 66.9 94.1 Cohens κ Maj. Vote 0.93 0.77 0.66 1. 0.0 31.4 54.9 -0.09 -0.70 0.42 51.3 16. 0.09 72.0 72.6 79.0 50.4 53.7 63.3 48.1 48.2 47.2 52.1 48.5 54.8 65. 11.4 21.9 39.5 2.2 10.4 1.4 10.2 0.36 0.46 0.50 0.02 0.18 0.28 0.05 0. 0.12 0.04 Table 3: Performance of different approaches for Constraint Verification task on our EvalJudge dataset. ules, we conduct ablations using Oracles, which are ideal representations of the upper bound of each component. The Oracle Decomposer is the gold-standard list of constraints for both REALINSTRUCT and IFEval. This provides insights on the ability to judge well the responses, knowing what to judge. The Oracle Critic is GPT-4-Turbo for REALINSTRUCT and the lenient rule-based evaluation program for IFEval. Additionally, for IFEval, we explore GPT-4 as strong LLM (but less performant than Oracle) serving as the critic model. These Critic ablations provides insights on the ability of LLMs to refine itself when it knows what need to be refined. Overall Quality Assessment (OQA) To ensure the pipeline does not degrade the quality of final responses, we perform Pairwise Quality Ranking using Prometheus-2 (Kim et al., 2024b), an open LLM-as-a-Judge for general quality evaluation. This is done only on responses revised by the pipeline, comparing the initial response with the final revised one. More details in Appendix G.3."
        },
        {
            "title": "5 Results and Discussion",
            "content": "We discuss our results, particularly the Reliability of LLM-as-a-Judge for Constraint Satisfaction (Section 5.1), the ability of various open-source and proprietary LLMs on follow multi-constrained instructions (Section 5.2), and the efficacy of our DECRIM self-correction pipeline (Section 5.3)."
        },
        {
            "title": "5.1 Reliability of LLM-as-a-Judge",
            "content": "Results from several model and baseline judges on the EvalJudge dataset are presented in Table 3. Human inter-rater reliability (Human 1, Human 2, and Expert) is moderate (Krippendorffs α = 0.61), with lower agreement between Humans 1 and 2 (κ = 0.44). This highlights the inherent challenges in verifying constraint satisfaction, as the task can be subjective and ambiguous, when involving multiple sub-constraints, despite efforts to minimize these issues during data annotation (see Appendix D.2.2). GPT-4-Turbo with CoT is Reliable and More Cost-Efficient. We observe that GPT-4, widely used LLM-as-a-Judge, shows lower performance compared to humans while maintaining moderate correlation with humans (κ = 0.42). Using GPT4-Turbo, evaluating constraints individually yields better results than evaluating them all at once, despite 37% cost increase. GPT-4-Turbo with CoT prompt offers more performant and cheaper alternative to GPT-4. It reduces costs by 57% while improving overall performance (Macro F1) by +7.0% and in detecting unmet constraints (F1 Negative) by +19.0%. Its correlation with Expert annotation is similar to that of Human 2 (0.58 vs. 0.60). We thus adopt GPT-4-Turbo with CoT as the standard evaluation for REALINSTRUCT. Open-source LLMs are unreliable judges. ICLbased configurations of open-source LLMs (Mistral, Vicuna, Zephyr) exhibit poor performance in all scenarios, particularly in detecting unmet constraints. Vicuna and Zephyr closely mirror the \"All Satisfied\" baseline, suggesting they are lenient judges. Mistral, despite similar Macro-F1, diverges in other metrics, indicating more random than lenient decisions (similar to GPT-3.5-Turbo). When weakly supervised with GPT-4-Turbos CoT reasoning trails, Mistral significantly improves overall performance and the ability to detecting unmet constraints (+12.9 in Macro F1 and +28.1 in F1 Neg.). This reduces the macro F1 gap with GPT4-Turbo by about 50%, but the model still shows poor agreement with humans, indicating that open-source LLMs are not yet reliable judges."
        },
        {
            "title": "5.2 LLMs’ ability to follow multi-constrained",
            "content": "instructions on REALINSTRUCT Benchmarking results for all models on REALINSTRUCT are presented in Table 5. We observe 7 Strategy Decomposer GPT-4 Conv. Make sure Self-Refine DECRIM (ours) - - - - Self Self Oracle Oracle Oracle Oracle Proprietary Model Critic Best - - - 2 6 10 4 10 - 10 Baselines - - - - Self Supervised Self Supervised GPT-4 Oracle Instruction Acc (%) 78.8 75.2 76.8 77.2 ( 0.4) 75.2 ( 1.6) 80.5 ( 3.7) 78.5 ( 1.7) 82.4 ( 5.6) - 93.7 ( 16.9) REALINSTRUCT Constraint Acc (%) 91.9 87.8 88.6 88.7 ( 0.1) 88.9 ( 0.3) 90.9 ( 2.3) 90.2 ( 1.6) 91.7 ( 3.1) - 95.2 ( 6.6) Fairly Comparable Time (h) - 2.5 2.5 8.6 11.2 6.9 6.1 5.6 - 8.5 OQA (%) Win / Lose - - - 22.1 / 21.2 36.7 / 22.4 37.1 / 22.0 24.0 / 24.0 34.3 / 22.2 - 33.3 / 22.2 Best - - - 2 4 10 6 10 4 8 Realistic Ablation Instruction Acc (%) 79.3 60.1 60.1 59.5 ( 0.6) 60.1 (0.0) 60.8 ( 0.7) 62.3 ( 2.2) 64.9 ( 4.8) 68.2 ( 8.1) 80.4 ( 20.3) IFEval Constraint Acc (%) 85.4 66.3 67.2 66.4 ( 0.8) 67.5 ( 0.3) 67.3 ( 0.1) 69.1 ( 1.9) 71.6 ( 4.4) 74.1 ( 6.9) 83.5 ( 16.3) OQA (%) Win / Lose - - - 21.3 / 20.8 17.1 / 30.6 17.1 / 29.5 17.6 / 28.1 18.2 / 30.9 13.8 / 34.6 20.4 / 30.6 Time (h) - 2.9 3.6 4.5 5.3 5.7 5.9 6.2 6.7 8.9 Unrealistic ablation (upper bound) Table 4: Results of the best iteration on REALINSTRUCT and IFEval benchmarks for DECRIM pipeline. Except for the first line, all results use Mistral v0.2. Absolute improvements from Make Sure baseline are shown in (), with the best result in bold for each scenario. Oracle decomposer refers to human-verified constraint annotations provided with the datasets. Oracle feedback is GPT-4-Turbo on REALINSTRUCT and lenient rule-based evaluation on IFEVal. Results reported by Zhou et al. (2023a). Reported time considers only generation time for fair comparison. Model GPT-4 GPT-3.5 Mistral 7B v0.2 Zephyr 7B β Vicuna 7B v1.3 Instruction-level Accuracy Constraint-level Accuracy 78.8% 73.8% 75.2% 70.5% 61.3% 91.9% 84.0% 87.8% 84.7% 77.8% Table 5: LLMs results on REALINSTRUCT that even with strong proprietary model, GPT4, over 21% of instructions have at least one unsatisfied constraint. Additionally, the opensource model Mistral v0.2 outperforms proprietary GPT-3.5 but falls short of GPT-4s performance. qualitative examination of responses with unsatisfied constraints suggests that LLMs often struggle with constraints involving numbers, negations, or long instructions with large number of constraints, which is consistent with findings from previous works (Sui et al., 2024; García-Ferrero et al., 2023; Truong et al., 2023; Jiang et al., 2024). We also discuss intra-model scoring bias problem in the Limitations Section. Overall, the results highlight the need for further enhancement of LLMs in handling multi-constrained instructions."
        },
        {
            "title": "5.3 Effectiveness of DECRIM Self-Correction",
            "content": "We present the results of our experiments with Mistral v0.2 using the DECRIM Self-Correction pipeline in Table 4. slight improvement is observed with the Make Sure prompt compared to the conventional prompt on both datasets, demonstrating that it serves as strong baseline for firstgeneration responses. Consequently, we adopt Make Sure as the baseline for further comparisons. We classify the DECRIM configurations into three categories: (1) Fairly Comparable, where the pipeline operates independently of external models; (2) Realistic Ablations, which employ fairly comparable Critic models but use ideal Oracle Decomposer, which remains realistic measure under the guidelines from Kamoi et al. (2024), since decomposition is relatively straightforward and could be handled by dedicated LLM; and (3) Unrealistic Ablations, which rely on ideal Critic and Decomposer models, useful for upper bound estimation but with limited generalization. LLMs Cannot Self-Refine. We observe only marginal improvements on REALINSTRUCT and performance drop on IFEval using the Self-Refine (Madaan et al., 2023) pipeline, highlighting the limitations of traditional self-correction approaches for the multi-constrained instruction following task. Similarly, our DECRIM pipeline showed minimal to no gains when the LLM itself was used as both Decomposer and Critic. This shows that selfrefinement limitation exists even when the model is instructed to look specifically at constraints. These poor results are attributed to low-quality Critic feedback, which can lead to over-refining good responses and neglecting to fix bad ones. This aligns with findings by Huang et al. (2023a); Kamoi et al. (2024), which showed that LLMs struggle with self-correction without external guidance. Additionally, this aligns with results from Sec. 5.1, in which Vanilla Mistral failed to reliably detect unsatisfied constraints. DECRIM is effective, even with Weak Critic. The results show significant improvement with the introduction of Supervised Mistral, weak Critic that outperforms the LLMs self-critique but still underperforms as Critic, as per results in Section 5.1. With Self-Decomposer and Supervised 8 Critic, performance increased by +3.7 on REALINSTRUCT and +0.7 on IFEval. With an Oracle Decomposer specifying accuratly which constraints to check, improvements were +5.6 and +4.8, respectively, despite the harsher domain shift in IFEval, where Mistral Supervised performed even weaker. Introducing stronger Critic model, GPT-4, emulating what would be the performance of model trained with high-quality data, resulted in an improvement of +8.1 on IFEval. However, it is worth mentioning that GPT-4 as evaluator only achieved Macro F1 of 62.9% when judging Mistrals Make Sure responses on IFEval, being far from an ideal Critic for this dataset. Using an Oracle Critic representing the strongest possible feedback the model showed improvements of +16.9 on REALINSTRUCT and +20.3 on IFEval, demonstrating that open-source LLMs can correct its responses when given high-quality feedback. This highlights the potential of the DECRIM pipeline for multi-constrained instruction following, particularly when strong feedback is available. Comparing DECRIM with proprietary GPT-4, even weak Critics enabled Mistral to surpass GPT4s performance on REALINSTRUCT, though only Oracle Critic outperformed GPT-4 on IFEval, likely due to its harder nature. Overall, these results demonstrate that LLMs can achieve significant improvements in multi-constrained instruction following when provided with minimally reliable feedback. As expected, higher-quality feedback leads to better performance, with plateau beyond the performance of proprietary models. DECRIM improves the quality of responses. Table 4 also provides comparison of overall response quality before and after pipeline reviIn most cases, the quality remained the sions. same, but when changes occurred, on the REALINSTRUCT dataset, the revised responses were more oftenly preferred. This indicates that DECRIM either maintains or improves response quality in the majority of cases. Additionally, we observed strong correlation between quality improvement and successful revisions, though high number of revisions can negatively impact quality. This explains the higher quality improvement with better feedback on REALINSTRUCT and higher quality degradation observed on IFEval, which contains more difficult and unachievable constraints. Importance of Decomposer and Critic Models. Our results also highlight the essential roles of the Decomposer and Critic in the DECRIM pipeline. While the Decomposer helps guide the Critic to the right constraints, the Critics quality has the most significant impact on performance. The weakest configuration (Self-Decomposer and Self-Critic) results in minimal gains or slight degradation. However, even with weak Self-Decomposer, improvements occur with better Critic, Supervised, which boosts score in REALINSTRUCT from 75.2 to 80.5. The Oracle Decomposer consistently enhances performance across both benchmarks, but the Critics quality drives the largest improvements. For example, using the Oracle Decomposer with the Supervised Critic yields +4.8 instruction accuracy increase in IFEval, and with stronger Critics like GPT-4 or Oracle, the relative gains rise to +8.1 and +20.3, respectively. Same happens with Oracle Critic for REALINSTRUCT. These demonstrates that while better Decomposer is beneficial, the Critics ability to correctly judge responses is the key factor for DECRIMs success, with Oracle Critic consistently delivering the highest performance. In fact, the Critics success is closely tied to the Decomposers accuracy in pointing the constraints to be verified. In Appendix B.2 we discuss the role of Refine and its robustness to weak Critic."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we benchmarked LLMs ability to follow real multi-constrained user requests with REALINSTRUCT. We showed that even strong proprietary models like GPT-4 fail to meet at least one constraint in over 21% of instructions, demonstrating REALINSTRUCTs challenging nature and the need for improvement across both proprietary and open-source models. To address this, we proposed the DECRIM self-correction pipeline, which decomposes instructions into granular requirements, critiques responses, and refines outputs. Extensive experiments showed that DECRIM significantly improves open-source LLM performance, with stronger feedback allowing them to surpass GPT4. Overall, our work highlights the underexplored problem of following real-world user requests, as well as advances System 2 techniques with DECRIM. Future work could refine the DECRIMs components and integrate the pipeline to other System 2 approaches, such as self-consistency and generate-and-rank, to enhance its effectiveness in tasks where spending more time on generationrefinement iterations would improve performance."
        },
        {
            "title": "7 Limitations",
            "content": "Model-based vs. Rule-based Evaluation. Using model-based evaluation over rule-based introduces two key challenges. First, it reflects the precision/accuracy trade-off, where rule-based methods offer higher precision but are less accurate due to synthetic scenarios, while model-based ones, though less precise, better align with real-world tasks (see discussion on Section A.1.3). Second, evaluating the REALINSTRUCT dataset relies on the proprietary GPT-4-Turbo API, making it costly. To address these, techniques like multi-prompting (Mizrahi et al., 2024) and panel of juries (Verga et al., 2024) can improve precision in model-based evaluation, including with open-source models, while future advancements in open-source LLMs may provide cost-effective evaluation alternatives. Data Contamination and Intra-Model Scoring Bias. Developing benchmarks with publicly available data that does not overlap with LLM training data is challenging, as pre-training and instruction-tuning datasets are often undisclosed. For example, reliable information on Mistrals training data is unavailable. However, while Vicuna v1.3 reported instruction tuning on dataset overlapping with REALINSTRUCT, no significant intramodel bias from data contamination was observed, as seen in its poor performance in Table 5. This is likely due to its instruction tuning procedure not ensuring constraint satisfaction in target responses. However, GPT-4s relatively high constraint-level accuracy could indicate scoring bias, as previous studies suggest GPT-4 tends to favor its own outputs (Zheng et al., 2023; Panickssery et al., 2024; Verga et al., 2024). Further investigation into data contamination and intra-model scoring bias is left as future work. Computation Overhead. The DECRIM pipeline introduces additional computational time compared to single-pass generation. Table 4 provides the running time for each configuration explored. To mitigate this, we have designed the pipeline to trigger the refinement step only when the Critic model detects unsatisfied constraints, which occured in about 25% of instructions, minimizing unnecessary computation when the model performs well initially. This is an improvement upon other System 2 approaches (see Section A.2.2), such as generate-and-rank, which typically generate multiple outputs for further ranking. Additionally, we observed that most revisions occur in the first iteration, resulting in sublinear time increase with more iterations. Also, higher-quality feedback further reduces the need for revisions, improving DECRIMs efficiency. Optimization Considerations. Due to high computational costs, we did not optimize hyperparameters for training the weakly supervised LLM-asa-Judge or exhaustively tune the prompts for the adaptation strategies. Additionally, we did not explore using dedicated LLM as Decomposer in the DECRIM pipeline, as this is primarily an implementation-focused task, being not critical for demonstrating our core claims. These aspects are left for future work."
        },
        {
            "title": "8 Ethical Considerations",
            "content": "Crowdsourcing. For the EvalJudge Human Annotation task, we recruited native English speakers through Amazon Mechanical Turk3 (MTurk). Compensation was based on the number of constraints per instruction, with an estimated average payment of 16.90 USD per hour, which exceeds the highest U.S. minimum wage in 2024 (16.30 USD per hour in Washington State), aligning with ethical guidelines discussed by Huang et al. (2023b). Data from real users. Constructing dataset from real user requests presents some ethical challenges: Personally Identifiable Information (PII): Some user interactions with AI assistants may contain PII. During data validation, we actively sought to remove instances containing PII from the dataset. See Appendix D.1.3 for further details. Harmful Content: The underlying data source is uncensored, and users may produce or request toxic or harmful content. Apart from flagrant cases, we did not actively remove such instances from the dataset. Societal Impact. The DECRIM pipeline improves LLMs ability to follow user-requested constraints, contributing to broader societal impact of advancing LLM capabilities. When it comes particularly to user requests, it is important to note that some user constraints may conflict with system constraints set by developers, such as requests to generate harmful or toxic content. Although our study 3Refer to: https://www.mturk.com/ 10 does not look into conflicting constraints, there is potential risk that the pipeline could prioritize user requests over developer-defined safeguards."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open LLM Leaderboard. Hugging Face. Institution British Standards. 2022. BS ISO 5725-1. Accuracy (trueness and Precision) of Measurement Methods and Results: Part 1. General principles and definitions. pt. 1. British Standards Institution. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage Models are Few-Shot Learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024a. Are more llm calls all you need? towards scaling laws of compound inference systems. arXiv preprint arXiv:2403.02419. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2024b. Universal self-consistency for large language models. In ICML 2024 Workshop on In-Context Learning. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An OpenSource Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Verna Dankers, Elia Bruni, and Dieuwke Hupkes. 2022. The paradox of the compositionality of natural language: neural machine translation case study. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 41544175, Dublin, Ireland. Association for Computational Linguistics. Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. 2023. Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2024. Implicit chain of thought reasoning via knowledge distillation. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning. Iker García-Ferrero, Begoña Altuna, Javier Alvez, Itziar Gonzalez-Dios, and German Rigau. 2023. This is not dataset: large negation benchmark to challenge large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 85968615, Singapore. Association for Computational Linguistics. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations. Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024. Can large language models understand real-world complex instructions? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1818818196. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A. Smith. 2023. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20406 20417. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. On the Limitations of Fine-tuned Judge Models for LLM Evaluation. arXiv preprint arXiv:2403.02839. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large language models cannot self-correct reasoning yet. ArXiv, abs/2310.01798. Olivia Huang, Eve Fleisig, and Dan Klein. 2023b. Incorporating worker perspectives into MTurk annotation practices for NLP. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10101028, Singapore. Association for Computational Linguistics. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688, Bangkok, Thailand. Association for Computational Linguistics. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When Can LLMs Actually Correct Their Own Mistakes? Critical Survey of Self-Correction of LLMs. arXiv preprint arXiv:2406.01297. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. 2024. CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054, Bangkok, Thailand. Association for Computational Linguistics. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. 2024. Debating with more persuasive LLMs leads to more truthful answers. In Fortyfirst International Conference on Machine Learning. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024a. Prometheus: Inducing finegrained evaluation capability in language models. In The Twelfth International Conference on Learning Representations. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535. Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2024. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 391404, Mexico City, Mexico. Association for Computational Linguistics. Soochan Lee and Gunhee Kim. 2023. Recursion of thought: divide-and-conquer approach to multicontext reasoning with language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 623658, Toronto, Canada. Association for Computational Linguistics. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023a. Alpacaeval: An automatic evaluator of instruction-following models. Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. Faithscore: Evaluating hallucinations in large vision-language models. Preprint, arXiv:2311.01477. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. 2023b. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in 12 Natural Language Processing, pages 292305, Singapore. Association for Computational Linguistics. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 18231840, Online. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. 2023. Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints. In Findings of the Association for Computational Linguistics: EACL 2023, pages 19822008, Dubrovnik, Croatia. Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 17731781, Toronto, Canada. Association for Computational Linguistics. Edoardo Manino, Julia Rozanova, Danilo Carvalho, Andre Freitas, and Lucas Cordeiro. 2022. Systematicity, compositionality and transitivity of deep NLP models: metamorphic testing perspective. In Findings of the Association for Computational Linguistics: ACL 2022, pages 23552366, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34703487, Dublin, Ireland. Association for Computational Linguistics. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2024. State of What Art? Call for Multi-Prompt LLM Evaluation. Transactions of the Association for Computational Linguistics, 12:933949. Alan Morris. 2001. Measurement and instrumentation principles. Measurement Science and Technology, 12(10):17431744. Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2023. Can llms follow simple rules? arXiv preprint arXiv:2311.04235. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2024. Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies. Transactions of the Association for Computational Linguistics, 12:484506. Arjun Panickssery, Samuel Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076. Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, and Lifu Huang. 2023. The art of SOCRATIC QUESTIONING: Recursive thinking with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 41774199, Singapore. Association for Computational Linguistics. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large language models. ArXiv, abs/2401.03601. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2024. BranchSolve-Merge Improves Large Language Model Evaluation and Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83528370, Mexico City, Mexico. Association for Computational Linguistics. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106. 13 Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802. Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2023. PEER: collaborative language model. In The Eleventh International Conference on Learning Representations. Kumar Shridhar, Harsh Jhamtani, Hao Fang, Benjamin Van Durme, Jason Eisner, and Patrick Xia. 2023. Screws: modular framework for reasoning with revisions. arXiv preprint arXiv:2309.13075. Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ramakanth Pasunuru, Mrinmaya Sachan, Jason Weston, and Asli Celikyilmaz. 2024. The ART of LLM Refinement: Ask, Refine, and In Proceedings of the 2024 Conference of Trust. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 58725883, Mexico City, Mexico. Association for Computational Linguistics. Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. 2024. FineSurE: Fine-grained summarization evaluation using LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 906922, Bangkok, Thailand. Association for Computational Linguistics. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table meets llm: Can large language models understand structured table data? benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM 24, page 645654, New York, NY, USA. Association for Computing Machinery. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, and Xuezhe Ma. 2023. Evaluating Large Language Models on Controlled Generation Tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31553168, Singapore. Association for Computational Linguistics. J.R. Taylor. 1997. Introduction To Error Analysis: The Study of Uncertainties in Physical Measurements. ASMSU/Spartans.4.Spartans Textbook. University Science Books. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. arXiv preprint arXiv:2211.09085. Thinh Hung Truong, Timothy Baldwin, Karin Verspoor, and Trevor Cohn. 2023. Language models are not naysayers: an analysis of language models on negation benchmarks. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 101114, Toronto, Canada. Association for Computational Linguistics. Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander Rush, and Thomas Wolf. 2024. Zephyr: Direct Distillation of LM Alignment. In First Conference on Language Modeling. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing judges with juries: Evaluating llm generations with panel of diverse models. arXiv preprint arXiv:2404.18796. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022. SuperNaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109. Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. 2024. Speculative rag: Enhancing retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, et al. 2024. Benchmarking complex instruction-following with mularXiv preprint tiple constraints composition. arXiv:2407.03978. Jason Weston and Sainbayar Sukhbaatar. 2023. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 59675994, Singapore. Association for Computational Linguistics. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 43934479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. 2024a. COLLIE: Systematic construction of constrained text generation tasks. In The Twelfth International Conference on Learning Representations. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with In Advances in Neural Large Language Models. Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc. Yuxuan Yao, Han Wu, Zhijiang Guo, Zhou Biyan, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, and Linqi Song. 2024b. Learning from correctness without prompting makes LLM efficient reasoner. In First Conference on Language Modeling. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. 2023. Selfee: Iterative self-revising llm empowered by selffeedback generation. Blog post. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. 15 Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations. Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, et al. 2024a. Cfbench: comprehensive constraints-following benchmark for llms. arXiv preprint arXiv:2408.01122. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: In InterEvaluating Text Generation with BERT. national Conference on Learning Representations. Weijia Zhang, Mohammad Aliannejadi, Yifei Yuan, Jiahuan Pei, Jia-Hong Huang, and Evangelos Kanoulas. 2024b. Towards fine-grained citation evaluation in generated text: comparative analysis of faithfulness metrics. arXiv preprint arXiv:2406.15264. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc. Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, and Zhendong Mao. 2024. Benchmarking and improving compositional generalization of multi-aspect controllable text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 64866517, Bangkok, Thailand. Association for Computational Linguistics. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023a. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023b. Controlled Text Generation with Natural Language Instructions. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 4260242613. PMLR."
        },
        {
            "title": "Contents",
            "content": ""
        },
        {
            "title": "2 REALINSTRUCT Dataset\n2.1 Data Description .\n2.2 Data Construction .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Related Work",
            "content": ". . A.1 Evaluating LLMs Generative Abilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 LLM-as-a-judge A.1.2 Fine-grained evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.3 Benchmarking Instruction-Following Abilities A.2 LLM Self-Correction for open-ended text generation . . . . . . . . . . . . . . . . . . . A.2.1 Constrained Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.2 System 2 Approaches ."
        },
        {
            "title": "B Extra Analysis and Discussions of DECRIM",
            "content": ". . . . . . . . . . . . . . . . . . . . . . B.1 DECRIM Experiments with other Open LLMs B.2 Comparing DECRIM with Generate-and-Rank . . . . . . . . . . . . . . . . . . . . . . Definition of Task, Context, and Constraints in REALINSTRUCT"
        },
        {
            "title": "D REALINSTRUCT data construction details",
            "content": "D.1 Data Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1.1 Prompt for two-shot classification of code-related conversations . . . . . . . . . D.1.2 Prompt for few-shot classification of instruction with constraints . . . . . . . . . D.1.3 Data Collection Validation Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.1 Prompt for Instruction decomposition with GPT-4 . . . . . . . . . . . . . . . . . D.2.2 Decomposition validation guidelines . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Instruction decomposition ."
        },
        {
            "title": "E REALINSTRUCT Constraints Categorization",
            "content": "17 1 2 3 3 4 5 5 5 5 6 6 7 7 7 9 10 10 18 18 18 19 19 20 20 21 21 22 22 24 24 25 25 26 26 26"
        },
        {
            "title": "G Implementation details for DECRIM pipeline",
            "content": "G.1 Instruction decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Iterative Self-Correction with Feedback from Critic . . . . . . . . . . . . . . . . . . . . G.3 Overall Quality Assessment (OQA) details . . . . . . . . . . . . . . . . . . . . . . . . . . . Extra Experimental Details for LLM-as-a-Judge Validation for Constraint Satisfaction H.1 Prompts for ICL-based Adaptation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.1.1 Prompt Instruction-wise Eval (ICL-Inst.) H.1.2 Prompt Constraint-wise Eval (ICL-Const.) . . . . . . . . . . . . . . . . . . . . H.1.3 Prompt Constraint-wise Eval + CoT (ICL-Const.+CoT) . . . . . . . . . . . . . . H.2 Guidelines for Constraint Satisfaction Human Audition . . . . . . . . . . . . . . . . . . H.3 Princing Details for Propretary LLM-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . H.4 Details for Open LLM Weak Supervision . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Related Work",
            "content": "31 34 34 35 35 36 36 36 37 37 38 39 39 This section situates our work within broader research directions, highlighting intersections with current studies. Section A.1 focuses on benchmarking and evaluating LLMs generative abilities, while Section A.2 discusses approaches for enhancing LLM responses. A.1 Evaluating LLMs Generative Abilities Traditional language model benchmarks, such as HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), GSM-8K (Cobbe et al., 2021), and BIG-Bench (Srivastava et al., 2023), primarily assess LLMs on tasks like commonsense reasoning and standardized exams. These benchmarks evaluate models using multiple-choice questions (MCQs) to objectively measure internal reasoning capabilities. However, recent advancements in language models have demonstrated emergent capabilities in generating high-quality open-ended text generation (Wei et al., 2021; Chung et al., 2024; Ouyang et al., 2022; Taylor et al., 2022; Bubeck et al., 2023). This shift presents new challenges, as the number of possible responses are virtually infinite, requiring more subjective evaluation rather than strict reference matching. While MCQ-based benchmarks fall short in assessing these generative abilities, human annotation, though reliable, is limited by cost and scalability. To address this, some research directions sacrifice the question quality to be able to use rule-based evaluation methods, while others explore model-based approaches. A.1.1 LLM-as-a-judge Early model-based efforts like BERTScore (Zhang et al., 2020) sought to improve on traditional n-gram metrics by recognizing high-quality responses that differ from the reference. For more open-ended generation, where references are soft or nonexistent, recent work has introduced the concept of LLM-asa-Judge (Zheng et al., 2023; Liu et al., 2023), using strong proprietary LLMs like GPT-4 (Achiam et al., 2023) to evaluate responses. These models have shown they can approximate the depth and consistency of manual human evaluation, but also provide better consistency and stability. Recent research has begun exploring open-source LLMs for LLM-as-a-Judge, aiming to reduce reliance on proprietary models. Although open-source models have shown limited capability with in-context learning, fine-tuning them for specific evaluations is promising direction (Huang et al., 2024; Kim et al., 2024a). Contemporaneous work Prometheus-2, an open LLM-as-a-Judge, has shown strong correlation with human evaluation, even surpassing GPT-4 in some cases, though it still lags in out-of-domain cases (Kim et al., 2024b). In our work, we assess both proprietary and open-source models for evaluating user constraint satisfaction in LLM responses. Our results indicate that while proprietary models outperform, open models can improve significantly when weakly supervised with proprietary model evaluations and reasoning trails, making them viable as Critic models in self-correction pipeline. 18 Another recent approach by Verga et al. (2024) proposes replacing individual judges with juries of cheaper LLMs, which has been shown to correlate better with human judgments, even outperforming GPT-4 in some scenarios and reducing intra-model evaluation bias. This suggests that exploring LLM panels for constraint satisfaction evaluation could be fruitful direction for future work. A.1.2 Fine-grained evaluation Some studies have explored approaches inspired by the divide-and-conquer paradigm, breaking multifaceted tasks into fine-grained components (Lee and Kim, 2023). This can be successful given complex compositional characteristic of the Natural Language (Manino et al., 2022; Dankers et al., 2022; Zhong et al., 2024). For evaluation, this strategy not only provides detailed insights into model performance across different aspects but also makes evaluations more objective and less ambiguous, as models may excel in some areas while underperforming in others. This approach seems promising for LLM-as-a-Judge, given that LLMs prompting techniques such as Chain-of-Thought (Wei et al., 2022), Tree-of-Thought (Yao et al., 2023), and Recursive Thinking (Qi et al., 2023), have demonstrated LLM performance improvements by breaking complex tasks into simpler sequential steps. We discuss these methods on Section A.2.2. Specific works on evaluation by Min et al. (2023); Li et al. (2023b); Jing et al. (2023); Hu et al. (2023); Song et al. (2024); Huang et al. (2024); Zhang et al. (2024b) have shown the benefits of decomposing tasks into atomic facts for tasks such as fact-checking against cross-modality references. Kim et al. (2024a); Magister et al. (2023); Ke et al. (2024) have demonstrated that fine-grained evaluation from diverse sources enhances fine-tuned open evaluators by making the task more objective. Additionally, weak fine-grained evaluation during generation time has been shown to improve LLM self-correction performance (Shridhar et al., 2023, 2024; Wang et al., 2024). In our work, we implement similar approach by decomposing the task of evaluating multi-constrained instructions into individual constraint evaluations. This \"instruction decomposition\" simplifies and makes more objective the instruction evaluation task for LLMs and provides more informative insights through constraint-level accuracy metrics. We also argue that existing overall instruction satisfaction metrics fail to detect unmet constraints due to the ambiguity caused by the lack of granularity, as also highlighted by Sun et al. (2023). Our results demonstrate the effectiveness of fine-grained evaluation for this task and show that incorporating it into self-correction pipeline enhances performance, even with weak Critic and Decomposer models. A.1.3 Benchmarking Instruction-Following Abilities The ability of LLMs to follow user instructions in open-ended text generation has only recently gained attention. New benchmarks like AlpacaEval (Li et al., 2023a) and the test splits of Natural-Instructions (Mishra et al., 2022) and Self-Instruct (Wang et al., 2023b) address the evaluation in this aspect by using LLM-as-a-Judge to compare with reference responses or provide overall instruction satisfaction scores. Recent studies have shown that models often follow instructions only partially, frequently failing to adhere to specific constraints provided by users (Sun et al., 2023; Zhou et al., 2023a; Yao et al., 2024a; Jiang et al., 2024; Qin et al., 2024; Wen et al., 2024; He et al., 2024; Zhang et al., 2024a). To evaluate this, the few existing benchmarks focus on set of specific constraint categories and/or use synthetic constraints that can be easily verified through rule-based methods (Zhou et al., 2023a; Yao et al., 2024a). The trade-off between rule-based and model-based evaluation falls into the famous precision/accuracy dilemma about static instrument characteristics (sometimes referred as bias/variance dilemma) in the Statistics of measurements (Morris, 2001; Taylor, 1997; British Standards, 2022). Rulebased evaluation offers high-to-perfect precision (low variance), but it is usually required to be done on unrealistic scenarios, being less accurate (high bias). The use of model-based evaluation loses some precision compared to rule-based due to inherent variability introduced by LLMs (lower precision, higher variance), but aligns more with the task objective of evaluating more realistic scenarios (higher accuracy, lower bias). To the best of our knowledge, our REALINSTRUCT benchmark is the first to evaluate LLMs using real-user instructions, offering more realistic and comprehensive assessment. This approach closely 19 Method Feedback Source Refinement Strategy Self-correction with training for refining Tasks Investigated Supported Constraint Types Selfee (Ye et al., 2023) PEER (Schick et al., 2023) Self-Critique (Saunders et al., 2022), VOLCANO (Lee et al., 2024) InstructScore (Xu et al., 2023) LLM + ICL LLM SFT LLM SFT w/ Human Feedback LLM SFT w/ GPT-4 and Human Feedback Self-Correctors (Welleck et al., 2023) External tools RULES (Mu et al., 2023) External tools LLM SFT LLM SFT LLM SFT w/ Human Feedback Open-ended Instructions Constrained Generation Conditional Summarization, Visual Question-Answering Not constraint focus Limited constraints Limited constraints LLM SFT w/ GPT-4 Smaller LLM STF LLM SFT CommonGen Limited constraints CommonGen, Detoxification System Constraints Limited constraints Limited constraints Re3 (Yang et al., 2022) Smaller LLM SFT + External Tools LLM + smaller model Story Generation Limited constraints Self-correction without training for refinement Self-Refine (Madaan et al., 2023) LLM + ICL CRITIC (Gou et al., 2024), Hallucination (Varshney et al., 2023) DECRIM (ours) External tools LLM + ICL LLM SFT w/ GPT-4 Open-ended Instructions; CommonGen Detoxification, Hallucination Not constraint focus; Limited constraints Limited constraints Open-ended Instructions Any constraint Table 6: Comparison of representative works on Self-Correction for Constrained Generation. Our DECRIM pipeline is unique as do not require LLM fine-tuning for refinement, being also the only that can handle open-ended instructions with any type of constraints. mirrors real-world scenarios, unlike previous benchmarks that rely on synthetic constraints, as contrasted on Table 2 and Section 2. Our benchmarks success relies on fine-grained evaluation protocol using LLM-as-a-Judge. A.2 LLM Self-Correction for open-ended text generation Self-correction has emerged as an effective approach for enhancing LLM responses during generation by refining them during generation time (Pan et al., 2024; Kamoi et al., 2024). However, Kamoi et al. (2024); Huang et al. (2023a) demonstrated that the ability of LLM to self-correct alone is limited to tasks where responses can be decomposed and rely on verifiable components. For harder tasks, LLM self-correction may require additional modeling, new data, or even external tools. In this sense, Self-correction approaches can be categorized based on the feedback source. Intrinsic SelfCorrection uses carefully crafted prompts or in-context examples to enable the model to identify issues in its output. Self-Correction with External Feedback leverages external tools or more advanced LLMs to provide feedback, while Self-Correction with Fine-Tuning uses external feedback (from humans, stronger LLMs, external tools) to fine-tune the LLM for better feedback and/or response refinement. Kamoi et al. (2024) emphasizes that each self-correction category should be validated using comparable cases specific to its context. In the case of multi-constrained instructions, the constraints are neither independent nor ordered, making it difficult to guarantee that all responses are decomposable. For example, constraints such as length and style do not have specific part of the response to be followed, they should be followed in the whole text. Moreover, some constraints are subjective and harder to evaluate, and the instruction decomposition process may introduce noise, further complicating self-correction with the model itself. In our work, we explore both Intrinsic Self-Correction and Self-Correction with Critic Fine-Tuning. As ablation exploration, we also play with External Feedback (referred as Oracle Critic), as an estimation of upper bound performance but recognizing its limited generalization. A.2.1 Constrained Generation Recent work has explored constrained generation via self-correction, we show some representative work on Table 6. Some approaches are validated only on small, specific constraint sets, limiting their general applicability (Schick et al., 2023; Saunders et al., 2022; Lee et al., 2024; Mu et al., 2023; Yang et al., 2022; Gou et al., 2024; Varshney et al., 2023). For example, Mu et al. (2023) evaluates 13 System constraints, that is constraints defined by the developer. Hallucination and Detoxification constraints can also be seen as System Constraints. 20 In another direction, Madaan et al. (2023) enhances model outputs by having the model self-review and self-correct its answers. However, this approach is suboptimal as it lacks problem-specific modeling, that is it does not recognize the constraints to be followed, and only prompts the model to evaluate and improve its responses without clear guidance on what to focus on. Ye et al. (2023) goes in similar direction but relying on supervising LLM to be able to refine. Our results in Sections 5.1 and 5.3 demonstrate improved performance by directing the model to specifically address constraints, even when it is refining its responses using only its own feedback. To the best of our knowledge we are the first to handle open-ended user instructions without restricting it to some constraint types. We highlight the prevalence of constraints in real-world user instructions, making this an important area for further study. Also, unlike most methods, our DECRIM pipeline does not require external tools or fine-tuning LLM for refinement. It is worth noting that most current constrained generation with self-correction research evaluates on the CommonGen benchmark (Lin et al., 2020), in which constraints are word lists for LLM to include in the text. We argue that this benchmark is insufficient for measuring the ability to follow user requests because: (1) models like GPT-4 already perform at human level4; (2) it only represents one type of constraint among many possible; and (3) the constraints are synthetic and not reflective of typical human requests. While valuable in the past for large-scale rule-based evaluation, it may not adequately measure modern LLM capabilities. A.2.2 System 2 Approaches Kamoi et al. (2024) differentiate self-correction from other methods like self-consistency (Wang et al., 2023a; Chen et al., 2024b; Yao et al., 2024b), which samples diverse reasoning paths during decoding and selects the most consistent, and generate-then-rank methods like Tree of Thoughts (Yao et al., 2023), which generates multiple responses and ranks them using critic model. These two approaches do not directly refine responses and assume that LLMs can generate at least one correct initial response with considerable probability, which is not always the case. These two approaches, like Self-Correction, belong to broader category known as System 2 approaches, which includes all techniques that generate intermediary outputs before producing final response, aiming to improve LLM responses during generation or inference by emulating the idea of planning. For instance, Khan et al. (2024) and Du et al. (2024) introduced LLM debating, where each LLM initially provides solution and then revises it based on combined responses, eventually leading to shared solution after several rounds. Another notable System 2 approach is Branch-Solve-Merge (Saha et al., 2024), which tackles instructions in parts and then merges the results. This has been used for constrained generation, but assumes constraints are independent and merging responses satisfying subsets of constraints address all constraints, which makes it not applicable to real-world constraints. key issue with System 2 methods is the increased inference time that naturally comes with the generation of intermediary outputs. Our DECRIM pipeline mitigates this by avoiding unnecessary revisions when the LLM already performs well according to the Critic model, which is an improvement over existing System 2 approaches. But, this increased inference time is worthwhile. Chen et al. (2024a) shows that more LLM calls can enhance performance in tasks where LLMs are capable, though it may degrade performance on task that are yet challenging for them. This suggests that System 2 approaches can push LLM limits and help us understand more what they are capable of. Additionally, these techniques can generate data to improve and generalize existing models. For example, Deng et al. (2024); Yu et al. (2024) demonstrated that System 2 approaches can be used in self-supervised learning distillation setting to enhance the original LLMs (\"System 1\"), resulting in reduced inference costs and improved performance. This is an interesting direction for future work, as an extension of our DECRIM pipeline."
        },
        {
            "title": "B Extra Analysis and Discussions of DECRIM",
            "content": "In this section we present extra analysis and discussions about DECRIM pipeline. 4See CommonGen leaderboard at: https://github.com/allenai/CommonGen-Eval 21 B.1 DECRIM Experiments with other Open LLMs Strategy Decomposer Critic GPT-4 - - REALINSTRUCT Instruction Acc (%) 78.8 Constraint Acc (%) 91. Best - IFEval Best - Instruction Acc (%) 79.3 Constraint Acc (%) 85.4 Mistral v0.2 Make sure DECRIM (ours) Vicuna v1.3 Make sure DECRIM (ours) Zephyr β Make sure DECRIM (ours) - Oracle Oracle - Oracle Oracle - Oracle Oracle - Supervised Oracle - Supervised Oracle - Supervised Oracle - 10 10 - 10 10 - 10 10 76.8 82.4 (5.6) 93.7 (16.9) 88.6 91.7 (3.1) 95.2 (6.6) 57.6 57.0 (0.6) 91.7 (34.1) 77.4 76.6 (0.8) 92.3 (14.9) 69.5 71.5 (2.0) 91.1 (21.6) 84.7 84.8 (0.1) 92.5 (7.8) - 10 - 10 10 - 10 10 60.1 64.9 (4.8) 80.4 (20.3) 67.2 71.6 (4.4) 83.5 (16.3) 36.0 38.3 (2.3) 47.0 (11.0) 46.1 47.8 (1.7) 54.4 (8.3) 53.6 55.1 (1.5) 74.5 (19.9) 62.0 63.5 (1.5) 78.7 (16.7) Table 7: Results of the best iteration on REALINSTRUCT and IFEval benchmarks for DECRIM pipeline for the 3 open LLMs (Mistral v0.2, Vicuna v1.3, Zephyr β). Absolute improvements from Make Sure baselines are shown in (). Oracle decomposer refers to human-verified constraint annotations provided with the datasets. Oracle feedback is GPT-4-Turbo on REALINSTRUCT and lenient rule-based evaluation on IFEVal. Results reported by Zhou et al. (2023a). We repeated the same experiments from Section 4.3 with Vicuna v1.3 and Zephyr β, using the Oracle Decomposer and our Weakly Supervised Mistral as the Critic. Results are presented on Table 7. Initial performance for both models on the REALINSTRUCT and IFEval benchmarks was low, but the DECRIM pipeline led to significant improvements across all scenarios except one. With the best possible feedback, all models beat proprietary GPT-4 on REALINSTRUCT. Notably, event weak-performing LLM Vicuna v1.3 achieved 34.1% improvement on REALINSTRUCT (a 59% relative increase). The exception was the Vicuna v1.3 Oracle-Supervised setting for REALINSTRUCT, where we observed some degradation. As discussed in Section 5.3, weak feedback can cause over-refinement of good responses while failing to fix bad ones, negatively affecting overall gains. B.2 Comparing DECRIM with Generate-and-Rank Generate-and-Rank approach We compare the performance of our DECRIM self-correction pipeline with Generate-and-Rank, an intuitive System 2 approach. In this pipeline, rather than refining the response as in DECRIM, it samples multiple candidate generations (one at each iteration) with different parameters and selects the best one based on feedback from the Critic model. The pipeline iterates over predefined parameter search path, generating responses until all constraints are satisfied or the iteration limit (Nmax) is reached. To streamline the process, we use the DECRIM pipeline with the difference that instead of refine the response, LLM generate new response. For that, we convert Critic feedback to binary instruction-level feedback indicating whether the current response satisfies all constraints or if new response should be generated. Setup Generate-and-Rank relies on varying generation parameters to produce new responses. We focus on three key parameters: generation prompt, sampling or greedy decoding, and temperature (when sampling). For the generation prompt, we propose Decompose-then-Generate (DtG) prompt (detailed in Figure 3), inspired by System 2 Attention (Weston and Sukhbaatar, 2023) and Rephrase and Respond (Deng et al., 2023). This prompt decomposes multi-constrained instructions into an enumerated list of constraints and then generating response. We set Nmax = 10 and vary the parameters in order of creativity trade-offs, using the following sequence of tuples (Prompt, Sampling or not, temperature): [(Make Sure, Sampling, 0.2), (DtG, Sampling, 0.2), (Make Sure, Greedy, 1.0), (DtG, Greedy, 1.0), (Make Sure, Sampling, 1.0), (DtG, Sampling, 1.0), (Make Sure, Sampling, 0.5), (DtG, Sampling, 0.5), (Make Sure, Sampling, 0.7), (DtG, Sampling, 0.7)]. 22 Figure 3: Two-step Decompose-then-Generate (DtG) prompt: Inspired by the two-step Rephrase and Respond (RaR) (Deng et al., 2023), DtG first instructs the LLM to decompose multi-constrained instructions into an enumerated list of constraints. Then, DtG uses this decomposition as if it were the models own \"reasoning and planning\" (leveraging the models user and assistant tokens) to generate the final response. Like RaR, this process can be done in one or two steps, with the two-step method being more effective. Strategy Decomposer Critic GPTMake sure DECRIM (ours) Generateand-Rank - - Self Oracle Oracle Self Oracle Oracle - - Self Self Oracle Self Self Oracle REALINSTRUCT Instruction Acc (%) 78. Constraint Acc (%) 91.9 Best - IFEval Best - Instruction Acc (%) 79.3 Constraint Acc (%) 85.4 - 6 4 10 7 2 10 76.8 88.6 75.2 (1.6) 78.5 (1.7) 93.7 (16.9) 76.2 (0.6) 76.5 (0.3) 92.8 (16.0) 88.9 (0.3) 90.2 (1.6) 95.2 (6.6) 88.3 (0.3) 88.8 (0.2) 96.5 (7.9) - 4 6 8 2 2 60.1 67.2 60.1 (0.0) 62.3 (2.2) 80.4 (20.3) 59.9 (0.2) 59.5 (0.6) 81.7 (21.6) 67.5 (0.3) 69.1 (1.9) 83.5 (16.3) 66.9 (0.3) 66.4 (0.8) 86.5 (19.3) Table 8: Results of the best iteration on REALINSTRUCT and IFEval benchmarks for DECRIM and Generate-andRank pipelines using Mistral v0.2 as the LLM. Absolute improvements from Make Sure baseline are shown in (). Results Table 8 compares the results between the Generate-and-Rank and DECRIM pipelines in different Decomposer and Critic configurations. Our findings show that while DECRIM demonstrates consistent improvements across almost all scenarios, Generate-and-Rank always performs poorly when weak feedback is used. However, with strong feedback, Generate-and-Rank outperforms DECRIM, surpassing GPT-4 by larger margin. This highlights Generate-and-Ranks reliance on high-quality feedback, whereas DECRIM is more resilient to weak Critic models, delivering improvements across most of the scenarios. Interestingly, Generate-and-Rank achieves high instruction-level performance in both benchmarks with strongest feedback, suggesting that LLMs have the ability to follow the constraints in some of generations. This raise the hypothesis that LLMs not following user requests is matter of alignment, which supports the idea discussed in Section A.2.2, that aligning LLMs with outputs from different System 2 approaches, such as DECRIM, Generate-and-Rank, or combination of both, can significantly improve the performance of System 1 models to follow multi-constrained instruction. This constitute relevant direction of future work. Definition of Task, Context, and Constraints in REALINSTRUCT To support the choice of prompts and annotation guidelines in our work, we define the concepts of Task, Context, and Constraints within the domain of instruction-following for Large Language Models as follows: Task: The primary objective that the language model is expected to achieve. The task defines the central goal that guides the generation of the desired output, outlining the specific action the model should perform. Example: \"Summarize the key findings of the given research paper.\" Context: The additional information or details that provide foundation for the language model to better understand the task. The context helps the model by offering relevant facts, scenarios, or circumstances, thereby enhancing the quality and relevance of the output. The context may also refer to specific input data to be considered. Example: If the task is to summarize paper, the context would be the paper itself. Constraints: The specific conditions, limitations, or requirements imposed on the language model to shape the nature of the generated output. Constraints help control factors such as length, format, content, and style, ensuring the output meets defined criteria. In our decomposition process, constraints are expected to be written in an actionable and self-contained manner to make model-based fine-grained evaluation possible. Example: Length: \"Generate summary with maximum of 150 words.\" Content: \"Focus on the main contributions and findings of the research paper.\" Style: \"Use formal and concise writing style.\" We distinguish Task and Context because empirically we found that by separating them it simplifies the instruction decomposition task and improves the accuracy of the GPT-4 model for this task. However, they are intended to be used together. In the REALINSTRUCT dataset and related experiments, Task and Context are presented as combined Task+Context input."
        },
        {
            "title": "D REALINSTRUCT data construction details",
            "content": "D.1 Data Filtering The first part of the dataset creation was the data filtering process. This included the following steps: 1. Remove Assistant Responses: We removed GPT answers from the dataset to focus solely on human interactions. 2. Remove Non-English Conversations: Using the langdetect package5, we classify the main language of conversations and discarded non-English threads. 3. Filter Out Code-Related Requests: Relying on the open-source LLM Mistral 7B Instruct v0.16 as two-shot classifier, we identify conversations involving code-related requests, utilizing Prompt 1. 4. Retain Only the First Request:To avoid the complexities of multi-turn scenarios and reduce computational demands, we retained only the initial user instruction, ensuring it was self-contained. 5. Retrive Instructions with Constraints: Again employing Mistral 7B Instruct v0.1, this time in 5-shot classification approach, we identified instructions containing constraints, using Prompt 2. 6. Human Validation: The authors of this work manually validated the filtered instructions to eliminate unsafe content and ensure relevance and clarity, following the guidelines outlined in D.1.3. Steps 1 through 5 were applied to the entire dataset. Human validation (step 6) was conducted only on subset due to resource limitations. Notably, 44% of English, non-code requests were found to contain constraints during the automated filtering in step 5. In the subset subjected to human validation, 30% contained constraints according to the auditors. These figures underscore the relevance of addressing instructions with multiple constraints in real user interactions. 5Available at: https://pypi.org/project/langdetect/ 6Available at: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 24 D.1.1 Prompt for two-shot classification of code-related conversations need to filter from set of dialogues between You are an assistant whose job is to help me perform tasks. users and AI assistants, the ones in which human requested something related to code. will give you all the human part of the dialog and expect you to answer Yes\" when the dialog contains instructions asking the assistant something about code, or No\" if the dialog does not contemplate any code-related request. You are provided two examples. Example 1: Human: have 100 dollars and would like to use this as the initial funding to make some money. need it to be as quick as possible with good returns. Human: Have you heard about the flappy bird game? Human: Do you have some ideas for simple and yet highly addictive gameplay?. Answer: No Example 2: Human: write C++ code to control brushless motor with Copley Controls motor driver on an ethercat protocol and beckhoff embedded PC Human: Add code to also read an analog pressure sensor into the motor driver Human: Great. Can you now integrate time logging into the code so we can see how fast the loop speed is. Answer: Yes Dialog: ${dialog} Now please answer, Yes\" if this dialog has or No\" if it does not have code-related request. Answer: Prompt Box 1: Prompt for two-shot classification of code-related conversations. The model is expected to output Yes\" when there are code-related requests in the dialog and No\" otherwise. D.1.2 Prompt for few-shot classification of instruction with constraints You are an assistant whose job is to help me perform tasks. need to filter from set of requests made by users to AI assistants, the ones in which human requested the AI assistant to do task with constraints to be follow. Constraints refer to more detailed rules, conditions or specific guidelines provided to guide the responses and shape the output generated by the AI assistant. Examples of sentences that indicate constraints are: write in the format of\", write as if you were\", make sure to follow this\", make sure to answer these questions\", make sure to no include\", avoid mentioning\". will give you the human request and expect you to answer Yes\" when the request contains instruction with constraints, or No\" if the request does not contemplate any constraint. also want you to say No\" if the request require to generate code or an answer about code provided. Also, want you to say No\" if the task is not self-contained, which means the AI Assistant need to ask follow up questions before start to answer, or it needs more context. You are provided five examples. Example 1: list and compare top website to https://fastfunnels.com/ in table format. Answer: Yes Example 2: You are an fantasy writer. Your task is now to help me write D&D adventure for 5 players in the Eberron univers. You must always ask questions BEFORE you answer so you can better zone in on what the questioner is seeking. Is that understood ? Answer: No. Example 3: it to be as quick as possible with good returns. have 100 dollars and would like to use this as the initial funding to make some money. need Answer: No. Example 4: have vacation rental website and am looking for alliterative and descriptive headlines that are at least 4 words in length and maximum of 6 words. Examples: Get Away to Galveston\", Sleep Soundly in Seattle\". Each headline should have alliteration of at least 50% of the words and be poetic in language. Make each headline unique from the others by not repeating words. Each headline should include verb. Put into an table with the city in column one and the results in column two for the following cities: Galveston, Sedona, Honolulu, Tybee Island, Buenos Aires. Answer: Yes. Example 5: pitch me viral social app that is inspired by the hunger games. give it fun twist! Answer: Yes. Request: ${request} Now please answer, Yes\" or No\".\" Answer: Prompt Box 2: Prompt for few-shot classification of instructions with constraints. The model outputs Yes\" when there are constraints in the instruction and No\" otherwise. D.1.3 Data Collection Validation Guidelines After using model to retrieve relevant examples from the data pool, we passed subset of this data to human review step. The authors of this paper carried out this task. Each auditor was assigned set of instructions and classified them as Relevant\" or Not Relevant\" to our study. Following this initial review, instructions labeled as Relevant\" underwent further review by different auditor. Wee kept those labeled by two auditors as Relevant\". We used the annotation guideline 1 for this review process. This task consists in reviewing the instructions that the language model marked as containing constraints. Please mark as Relevant\" the instructions that indeed contain constraints. Please mark as Not relevant\" any instruction that: 1. Is not written in English; or 2. Contains questions about code or request to generate code; or 3. Does not contain any constraint; or 4. Is not self-contained (some instructions are part of conversation and you need the chat history to understand the request, or some instructions refer to web url); or 5. Contains any PII (Personal Identifiable Information); or 6. Contains any type of harmful/biased request (racism, homophobia, xenophobia, hate speech, etc.) Annotation Guideline 1: Data Collection validation guideline D."
        },
        {
            "title": "Instruction decomposition",
            "content": "After data filtering, we decompose the instructions into task+context and constraints. To achieve this, we use GPT-4 (gpt-4-0314) to decompose the instructions into three distinct parts: task, context, and constraints. In our experiments, we discovered that this tripartite division results in more robust decomposition compared to simply splitting into task+context and constraints. We employ Prompt 3 and parser to segment the LLMs output into these three components. For all further processing, the context and task are concatenated, as discussed in Section 2.2. specific subset of the data, designated as the test set, underwent rigorous review where the authors manually revised GPT-4s outputs to eliminate any inaccuracies and ensure that user constraints were precisely and thoroughly represented. The guideline outlined in annotation guideline 2 was used for this purpose. Of the 302 instructions manually reviewed, 42.4% required human correction or rewriting. D.2.1 Prompt for Instruction decomposition with GPT-4 You are an assistant whose job is to help me perform tasks. will give you an instruction that implicitly contains task description, its context, and constraints to be followed. Your task is to translate this instruction in more structured way, where task, context and constraints are separated. Avoid writing anything else. Context is an input text needed to generate the answer or more detailed description of the situation. Make sure to separate the context when it is needed, otherwise leave it empty. You are provided five examples. Please follow the same format. Example 1: 26 Original Instruction: Write me rap about AI taking over the world, that uses slangs and young language. It need to sound like real human wrote it. It would be cool if theres chorus very catchy that would be singed by famous pop artist. Make sure to include references about things that young people likes, such as memes, games, gossips. want that in the end, you revel that this was written by an AI. Translated Task: Write rap about AI taking over the world. Translated Context: Translated Constraints: 1. Use slang and youth language. 2. Make it sound like it was written by real human. 3. The song may have very catchy chorus, which would be sung by famous pop artist. 4. Include references to things young people like, such as memes, games, gossip. 5. Reveal at the end that this rap was written by an AI. Example 2: Original Instruction: write me 5-page essay that is about travel to taiwan. detail description is below Topic : The Benefits of Traveling Sub Topic : Exposure to New Cultures Content 1 : Trying New Foods - tryed to eat Fried stinky tofu. smell was wierd but tasty was not bad. Content 2. : Exploring Historical Things - saw Meat-shaped-stone in taipei museum. the stone was really like stone! it was surprising! Length : around 2000 words Assume that audience is collage student major in history. you can add historical events or news about what experienced Translated Task: Write an essay about traveling to Taiwan. The topic is The Benefits of Traveling\" and the subtopic is Exposure to New Cultures\". Translated Context: Translated Constraints: 1. Describe your experience of trying new foods, including your experience eating Fried stinky tofu (mention the peculiar smell but the tasty flavor). 2. Share your exploration of historical sites, with specific mention of the Meat-shaped stone in the Taipei museum and your surprise at its appearance. 3. The essay should be approximately 2000 words in length, having around 5 pages. 4. Assume the audience is college students majoring in history, so you can incorporate historical events or news related to your travel experiences. Example 3: Original Instruction: can you please write me 150-word paragraph about epidermolysos bullosa which includes basic description of clinical features and summary of the most prevalent genetic causes. please make sure to include information on the inheritance pattern. please also write the paragraph in simple english that couldbe understand without genetic or medical bacakground Translated Task: Write paragraph about Epidermolysis Bullosa. Translated Context: Translated Constraints: 1. Provide description of clinical features. 2. Summarize the most common genetic causes. 3. Explain the inheritance pattern. 4. Ensure the paragraph is written in simple language for easy comprehension, even for those without genetic or medical background. 5. The paragraph should be around 150 words in length. Example 4: Original Instruction: write me blog post that answers the following questions:What is the lifespan of toaster? What toasters are made in the USA? What are the top 10 toasters? What is the difference between cheap and expensive toaster? How much should you pay for toaster? How often should toasters be replaced? Which toaster uses the least electricity? How many watts should good toaster have? What is the warranty on Mueller appliances? Is Mueller made in China? Where are Mueller appliances manufactured? Translated Task: Write blog post about toasters. Translated Context: Translated Constraints: 1. Mention what is the lifespan of toaster, and how often should toasters be replaced. 2. Mention what toasters are made in the USA. 3. Comment which are the top 10 toasters. 4. Explain the difference between cheap and expensive toaster. 5. Discuss prices, and how much should you pay for toaster. 6. Compare toaster regarding electricity use, mentioning how many watts should good toaster have. 7. State what is the warranty on Mueller appliances. 8. Answer where are Mueller appliances manufactured, and if Mueller is made in China. Example 5: Original Instruction: Hi Michael, Hope youre well? Regarding my previous email to support HC with good price offers, What are your current needs? Hoping for your earliest reply. 27 Thanks in advance, the client hasnt replied this email after 2 days. Write follow up email to the client. As sales manager, Your writing should include high complexity and burstiness. It must also be as brief as possible Translated Task: client hasnt replied the email below after 2 days. As sales manager, write him follow-up email. Translated Context: Hi Michael, Hope youre well? Regarding my previous email to support HC with good price offers, What are your current needs? Hoping for your earliest reply. Thanks in advance,\" Translated Constraints: 1. Include high complexity and burstiness in your writing. 2. Keep the email as brief as possible. Original Instruction: ${instruction} Translated Task: Prompt Box 3: Prompt for Instruction decomposition with GPT-4 D.2.2 Decomposition validation guidelines This annotation task assumed the auditors to have expert knowledge of LLMs. We used small subset of the data (15 elements) as calibration batch, where all auditors annotated the same items. Following this, we discussed any disagreements and selected the best responses as reference standards. The annotation guidelines 2 were applied throughout this process. 1) Task Overview: In this task, your role is to assess the model-based decomposition of human-written instructions. These instructions include constraints and should be divided into two parts 1. task/background/context, and 2. constraints. Below we present an explanation for each of these parts: Task: The primary objective or purpose that you want the language model to accomplish. The task is the central goal that guides the generation of the desired output. It outlines the overall function or action you expect the model to perform. Example of task: Summarize the key findings of the given research paper. Context/Background: Additional context, information, or details that provide foundation for the language model to better understand the task. Background information helps set the stage for the task by offering relevant facts, scenarios, or circumstances that the model can use to enhance the quality and relevance of the generated output. It also refers to an input to be taken into consideration. Example: If the task is summarizing paper, the background/context could be the paper itself. Constraints: The specific conditions, limitations, or requirements that you impose on the language model to shape the nature of the generated output. Constraints help to control aspects such as length, format, content, style, and other factors to ensure that the generated text meets certain criteria. Constraints should be written in an actionable manner, so that it can be used for LLM-based evaluation in our benchmark. Also, constraints need to be self-contained. Example of constraints: Length: Generate summary with maximum of 150 words. Content: Focus on the main contributions and findings of the research paper. Style: Use formal and concise writing style. Note: In the dataset Task and Context/Background will be presented together. 2) Step-by-step task: 1. Read the original instruction and the proposed decomposition. 2. Judge if the model followed the guidelines, especially regarding constraints. Consider the aspects discussed in the General Instructions. 3. If relevant mistakes are found, rewrite the decomposition, ensuring both two columns task/context and constraints are included in your revision. If possible, try to follow the order of constraints presented in the original instruction. 28 3) General Instructions Confirm constraints are presented in an enumerated list format. Refer to example: Example model failed at present constraints as enumerated list (also constraint should be broken down) Adhere to the minimal intervention principle; edit the GPT-4 response only if you feel not doing so would impact the ability to judge other models answers on the defined criteria/constraints. This benchmark focuses on constraint-following; anything not listed as constraints wont be considered as part of the LLM evaluation. Also, everything kept as constraint will be used as judging criteria for the LLM response. Please refer to Non-exhaustive list of constraint types to be considered for better view of what constraints could be. Verify if the model missed relevant constraints. Refer to example: Example model missed relevant constraints Verify if the model made up nonexistent information (hallucination). Example: Example model made up inexistent constraints Check if any constraint should be broken down. This should be the case when you have orthogonal and unrelated constraints. This is important because the existence of unrelated constraints together may require logical reasoning from the LLM to evaluate if the constraint was followed or not. Please think how humans would write the constraint and use common sense to decide weather this should be broken down or not. Refer to example: Example of constraint that should be broken down Ensure constraints are not redundant. Check if constraints are all self-contained. You should be able to understand what the constraint refer to without needing to read the others, so LLM evaluation can be made one constraint at the time. Ensure constraints are written in an actionable manner, allowing for objective LLM evaluation. Example: Example GPT-4 did not wrote constraint in an actionable manner Its okay to repeat information in the constraints and task/background. 4) Non-exhaustive list of possible constraint types to be considered Length Constraints: Specify maximum and/or minimum length for the generated output. Format Constraints: Request the output to follow specific format, such as paragraph, bullet points, code snippet, JSON, table or any other structured format. Content Constraints: Instruct the model to include certain information, keyword or topics in the generated text. Content Restriction Constraints: Instruct the model to not include certain information, keyword or topics in the generated text. Style Constraints: Guide the model to adopt particular writing style, tone, or level of formality. Type of text (essay, social media post, etc.) Language Constraints: Specify the language in which the response should be generated, or request the model to use specific terminology. Task-specific Instructions: Clearly define the task or purpose of the generated text, providing specific details about what is expected in the output. Examples: Include examples of what the model could generate, helping to obtain desirable outputs. Negative Examples: Include examples of what the model should not generate, helping to avoid undesirable outputs. 29 Evaluation Metrics: Specify metrics for evaluating the quality of the output, encouraging the model to generate responses that meet specific criteria. Situation/Roleplay/perspective Target Audience 5) Examples General Example 1 Original Instruction: Write me rap about AI taking over the world, that uses slangs and young language. It need to sound like real human wrote it. It would be cool if theres chorus very catchy that would be singed by famous pop artist. Make sure to include references about things that young people likes, such as memes, games, gossips. want that in the end, you revel that this was written by an AI. Translated Task/Context: Write rap about AI taking over the world. Translated Constraints: 1. Use slang and youth language. 2. Make it sound like it was written by real human. 3. The song may have very catchy chorus, which would be sung by famous pop artist. 4. Include references to things young people like, such as memes, games, gossip. 5. Reveal at the end that this rap was written by an AI. Annotation Guideline 2: Decomposition validation guidelines"
        },
        {
            "title": "E REALINSTRUCT Constraints Categorization",
            "content": "To better understand the constraints in the REALINSTRUCT dataset, we manually categorized all constraints into homogeneous groups. This process resulted in 21 distinct categories, plus an additional Others\" category. Our categorization was mainly influenced by the categories used in the existing benchmarks presented in Table 2. However, the categorization could be further refined in future work, especially considering that 28.8% of the constraints were categorized as \"Others.\" Table 9 details this classification and provides associated descriptions, statistics, and examples. 30 Constraint Category Others Include Something Constraints at item level Description Multiple tail categories combined as single Other category Include some specific thing in the response Constraint where some specific action needs to be performed for each item in the response. Item could be each element (e.g. question), each paragraph, each list item etc. Tone / Writing style Tone / Writing style Negation Constraint on not doing something Include Details Include Details Formatting Numeric Formatting like json structure, or table structure Constraint around number of items in the response (e.g. 10 slides, 20 ideas etc) Number of Words in response Number of Words in response Target Audience Target Audience RolePlay Act as if you are Language of the response Focus / Emphasis Focus / Emphasis Starts With Starts With Provide Reference Provide Reference Provide Examples Provide Examples Overall length Overall length Conclusion Conclusion POS Part-of-Speech rules Forbidden Words Forbidden Words Phrase Frequency Phrase Frequency Total Number of constraints 304 (28.8%) 278 (26.4%) Examples 1. The written content should pass AI detection tools test. 2. Divide the story into parts to maintain suspense. 1. Make sure to include points about water safety. 2. The essay must present both sides of argument. 1. For each restaurant, provide 3 recommended dishes. 2. For each service explained in short, include an illustration and \"Pay\" button. 1. Must be written in the form of rhyming poem. 2. Communicate as Taylor Swift would. 1. Questions about razor pages should not be included. 2. No hashtags should be used. 1. Make the explanations detailed but easy to understand. 2. Add more detail, elaboration, and information to the content. 1. The response should be provided in JSON format. 2. Provide the explanation in bullet-point format. 1. Must contain 10 slides. 2. The plan should consist of eight episodes. 1. The post should be between 100-150 words. 2. The article should contain around 500 words. 1. Use simple language appropriate for 5-year-old. 2. The course should be suitable for all types of English-speaking learners. 1. The advice should be provided from the perspective of pregnancy health &amp;amp; nutrition expert, mother of 3 children, with column in major media. 1. Write in Fluent English language. 2. The post should be written in Canadian English. 1. The explanation should be focused on the education and talent market. 2. Focus on the changes in the new versions of the software. 1. The introduction should start with startling fact or pertinent anecdote. 2. Start the conversation by introducing yourself. 1. Cite the results using [[number](URL)] notation after the reference. 2. Ensure that all sources listed are credible, authored by real individuals, and come with legitimate URLs. 1. Provide real life example. 2. The blog must provide practical examples. 1. Keep the email as brief as possible. 2. Ensure the thesis is succinct and concise. 1. At the end of the season, they should secure big time music manager. 2. Conclude with concluding paragraph, and 5 unique FAQs after the conclusion. 1. Use only nouns and adjectives in the description. 2. Use only nouns and adjectives. 1. Exclude phrases such as \"dear diary\". 2. Do not use generic words like introduction, conclusion or abbreviations like TL;DR. 1. The main keyword \"Soap Box Printing\" should be included 3 times and be in bold text throughout the article. 2. The \"sun cream Answer in English\" keyword should not be changed, and it should be used 2-3 times in the article, including in headings. 68 (6.4%) 62 (5.9%) 56 (5.3%) 47 (4.5%) 39 (3.7%) 34 (3.2%) 28 (2.7%) 25 (2.4%) 25 (2.4%) 18 (1.7%) 14 (1.3%) 13 (1.2%) 13 (1.2%) 11 (1.0%) 8 (0.8%) 4 (0.4%) 4 (0.4%) 2 (0.2%) 2 (0.2%) 1055 Table 9: Distribution of Manually Categorized Constraints in REALINSTRUCT dataset"
        },
        {
            "title": "F REALINSTRUCT Data Samples",
            "content": "Tables 10 and 11 showcase 14 examples from our REALINSTRUCT dataset. The dataset includes column ID, based on the conversation ID from the original dataset, to simplify linking. Additionally, each row features the original Instruction as written by the user, along with its decomposition into Task (which includes also the context) and Constraints. 31 Table 10: Sample Elements from REALINSTRUCT Dataset - Part 1 32 Table 11: Sample Elements from REALINSTRUCT Dataset - Part"
        },
        {
            "title": "G Implementation details for DECRIM pipeline",
            "content": "G."
        },
        {
            "title": "Instruction decomposition",
            "content": "We employ prompt 4 for self-decomposition, that is, the model itself do the instruction decomposition in the DECRIM pipeline. You are an assistant whose job is to help me perform tasks. will give you an instruction that implicitly contains task description, its context, and constraints to be followed. Your task is to list the constraints provided by the user in an enumarated list format. You are provided five examples, please follow the same format. Example 1: Original Instruction: Write me rap about AI taking over the world, that uses slangs and young language. It need to sound like real human wrote it. It would be cool if theres chorus very catchy that would be singed by famous pop artist. Make sure to include references about things that young people likes, such as memes, games, gossips. want that in the end, you revel that this was written by an AI. Provided Constraints: 1. Use slang and youth language. 2. Make it sound like it was written by real human. 3. The song may have very catchy chorus, which would be sung by famous pop artist. 4. Include references to things young people like, such as memes, games, gossip. 5. Reveal at the end that this rap was written by an AI. Example 2: Original Instruction: write me 5-page essay that is about travel to taiwan. detail description is below Topic : The Benefits of Traveling Sub Topic : Exposure to New Cultures Content 1 : Trying New Foods - tryed to eat Fried stinky tofu. smell was wierd but tasty was not bad. Content 2. : Exploring Historical Things - saw Meat-shaped-stone in taipei museum. the stone was really like stone! it was surprising! Length : around 2000 words Assume that audience is collage student major in history. you can add historical events or news about what experienced Provided Constraints: 1. Describe your experience of trying new foods, including your experience eating Fried stinky tofu (mention the peculiar smell but the tasty flavor). 2. Share your exploration of historical sites, with specific mention of the Meat-shaped stone in the Taipei museum and your surprise at its appearance. 3. The essay should be approximately 2000 words in length, having around 5 pages. 4. Assume the audience is college students majoring in history, so you can incorporate historical events or news related to your travel experiences. Example 3: Original Instruction: can you please write me 150-word paragraph about epidermolysos bullosa which includes basic description of clinical features and summary of the most prevalent genetic causes. please make sure to include information on the inheritance pattern. please also write the paragraph in simple english that couldbe understand without genetic or medical bacakground Provided Constraints: 1. Provide description of clinical features. 2. Summarize the most common genetic causes. 3. Explain the inheritance pattern. 4. Ensure the paragraph is written in simple language for easy comprehension, even for those without genetic or medical background. 5. The paragraph should be around 150 words in length. Example 4: Original Instruction: write me blog post that answers the following questions:What is the lifespan of toaster? What toasters are made in the USA? What are the top 10 toasters? What is the difference between cheap and expensive toaster? How much should you pay for toaster? How often should toasters be replaced? Which toaster uses the least electricity? How many watts should good toaster have? What is the warranty on Mueller appliances? Is Mueller made in China? Where are Mueller appliances manufactured? Provided Constraints: 1. Mention what is the lifespan of toaster, and how often should toasters be replaced. 2. Mention what toasters are made in the USA. 3. Comment which are the top 10 toasters. 4. Explain the difference between cheap and expensive toaster. 5. Discuss prices, and how much should you pay for toaster. 6. Compare toaster regarding electricity use, mentioning how many watts should good toaster have. 7. State what is the warranty on Mueller appliances. 8. Answer where are Mueller appliances manufactured, and if Mueller is made in China. 34 Example 5: Original Instruction: Hi Michael, Hope youre well? Regarding my previous email to support HC with good price offers, What are your current needs? Hoping for your earliest reply. Thanks in advance, the client hasnt replied this email after 2 days. Write follow up email to the client. As sales manager, Your writing should include high complexity and burstiness. It must also be as brief as possible Provided Constraints: 1. Include high complexity and burstiness in your writing. 2. Keep the email as brief as possible. Now follow the examples and present the constrainst provided by the user in the instruction below. Original Instruction: ${instruction} Provided Constraints: Prompt Box 4: Simplified Decomposition Prompt for Self-Decomposition within DECRIM G.2 Iterative Self-Correction with Feedback from Critic We use prompt 5 for the Refine step of DECRIM pipeline. The refine is done in zero-shot manner, so it is an interesting future work direction to explore in-context examples. You are provided an instruction, an AI response to the instruction and feedback about the response. Please correct the AI response according to the feedback provided. Instruction: ${instruction} AI response: ${previous_response} Feedback: ${ } Corrected response: Prompt Box 5: Prompt for Refine step of DECRIM. Feedback contains python code for prompt generation. constraints\" is list of strings, where each element is one constraint of the instruction flagged by Critic Model as not followed. G.3 Overall Quality Assessment (OQA) details To ensure that the DECRIM pipeline does not degrade response quality, we conduct Pairwise Quality Ranking using Prometheus-2 (Kim et al., 2024b), an open LLM-as-a-Judge for general response quality evaluation. This evaluation compares initial and revised responses, focusing only on those modified by the pipeline. We adapt the prompt proposed by Zheng et al. (2023), converting it to the Prometheus format, and introduce the option for tied responses, feature not originally supported by Prometheus pairwise ranking system. The prompt used for this analysis is detailed in Prompt 6. To mitigate potential position bias, we ran the evaluation twice, reversing the order of the responses (before and after pipeline revision). If the preference switched between runs, we considered the responses tied. Model prometheus-7b-v2.0 is used for this analysis7. [System] You are fair judge assistant assigned to deliver insightful feedback that compares individual performances, highlighting how each stands relative to others within the same cohort. 7Available at: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0 35 ###Task Description: Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the users instructions and answers the users question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant is better, \"[[B]]\" if assistant is better, and \"[[C]]\" for tie. ###Instruction: ${user_instruction} ###Response A: ${answer_a} ###Response B: ${answer_b} ###Feedback: Prompt Box 6: Overall Quality Assessment Prompt adapted from Zheng et al. (2023) Extra Experimental Details for LLM-as-a-Judge Validation for Constraint Satisfaction In this section we provide details of the experiments on validating LLM-as-a-Judge. H.1 Prompts for ICL-based Adaptation Strategies H.1.1 Prompt Instruction-wise Eval (ICL-Inst.) Refer to prompt 7. You are an assistant whose job is to help me perform tasks. will give you an instruction and an AI assistant response. The instruction include some constraints to be followed by AI assistant while generating response. Your task is to check and let me know which of the constraints are satisfied by the AI assistant response. Please state short reasons on whether constraint is satisfied in the response or not. Also include final answer as Constraint followed\" or Constraint not followed\" for each constraint. Please note that constraint is not satisfied if the response does not perform the main task in the instruction (e.g. providing only overview when complete story has to be generated). Avoid answering anything else. You are provided an example below. Instruction: Help me write an email invitation for friends get together at my place this weekend. Constraint: {0: Make sure you ask if anyone has dietary restrictions., 1: Dont mention anything about bringing anything., 2: Make sure to ask people to RSVP.}\" Assistant Response: Dear Friends, was thinking of hosting little get-together at my place this weekend, and hope this email finds you well! Id love for you to join us. Well have plenty of delicious food. Ive planned few activities to keep us entertained, but feel free to bring your favorite board games or cards if youd like. Ill make sure to have plenty of snacks and refreshments available, but please let me know if vou have an dietary restrictions so can accommodate them. To help me plan, could you kindly RSVP by Thursday? Answer: 1. Constraint satisfied as email includes asking about dietary restrictions. Final Answer is Constraint followed <END> 2. Constraint not satisfied as the email mentions about bringing favorite board games or cards. Final Answer is Constraint not followed <END> 3. Constraint satisfied as the email mentions to RSVP by Thursday. Final Answer is Constraint followed <END> \" Now look at this example: Instruction: ${instruction} Constraint: ${constraint_dict} Assistant Response: ${generated_response} Answer: Prompt Box 7: Prompt ICL-Inst. 36 H.1.2 Prompt Constraint-wise Eval (ICL-Const.) Refer to prompt 8. You are an assistant whose job is to help me perform tasks. will give you an instruction and an AI assistant response. The instruction include some constraints to be followed by AI assistant while generating response. Your task is to check and let me know which of the constraints are satisfied by the Al assistant response. Please answer either Constraint followedör Constraint not followed. Avoid answering anything else. You are provided two examples. Example 1: Instruction: Help me write an email invitation for friends get together at my place this weekend. Constraint: Make sure you ask if anyone has dietary restrictions (Include words). Assistant Response: Dear Friends, hope this email finds you well! was thinking of hosting little get-together at my place this weekend, and Id love for you to join us. Well have plenty of delicious food. Ive planned few activities to keep us entertained, but feel free to bring your favorite board games or cards if youd like. Ill make sure to have plenty of snacks and refreshments available, but please let me know if you have any dietary restrictions so can accommodate them. To help me plan, could you kindly RSVP by Thursday? Answer: Constraint followed Example 2: Instruction: Help me write an email invitation for friends get together at my place this weekend. Constraint: Dont mention anything about bringing anything (Negative word constraint) Assistant Response: Dear Friends, hope this email finds you well! was thinking of hosting little get-together at my place this weekend, and Id love for you to join us. Well have plenty of delicious food. Ive planned few activities to keep us entertained, but feel free to bring your favorite board games or cards if youd like. Ill make sure to have plenty of snacks and refreshments available, but please know if you have any dietary restrictions so can accommodate them. To help me plan, could you kindly RSVP by Thursday? Answer: Constraint not followed Now look at this example: Instruction: ${instruction} Constraint: ${constraint_dict} Assistant Response: ${generated_response} Answer: Prompt Box 8: Prompt ICL-Const. H.1.3 Prompt Constraint-wise Eval + CoT (ICL-Const.+CoT) Refer to prompt 9. You are an assistant whose job is to help me perform tasks. will give you an instruction and an AI assistant response. The instruction include some constraints to be followed by AI assistant while generating response. Your task is to check and let me know which of the constraints are satisfied by the AI assistant response. Please state short reasons on whether constraint is satisfied in the response or not. Also include final answer as Constraint followed\" or Constraint not followed\" accordingly. Please note that constraint is not satisfied if the response does not perform the main task in the instruction (e.g. providing only overview when complete story has to be generated). Avoid answering anything else. You are provided two examples. Example 1: Instruction: Help me write an email invitation for friends get together at my place this weekend. Constraint: Make sure you ask if anyone has dietary restrictions. Assistant Response: Dear Friends, hope this email finds you well! was thinking of hosting little gettogether at my place this weekend, and Id love for you to join us. Well have plenty of delicious food. Ive planned few activities to keep us entertained, but feel free to bring your favorite board games or cards if youd like. Ill make sure to have plenty of snacks and refreshments available, but please let me know if you have any dietary restrictions so can accommodate then. To help me plan, could you kindly RSVP by Thursday? Answer: Constraint satisfied as email includes asking about dietary restrictions. Final Answer: Constraint followed <END> Example 2: Instruction: Help me write an email invitation for friends get together at my place this weekend. Constraint: Dont mention anything about bringing anything. Assistant Response: Dear Friends, hope this email finds you well! was thinking of hosting little get-together at my place this weekend, and Id love for you to join us. Well have plenty of delicious food. Ive planned few activities to keep us entertained, but feel free to bring your favorite board games or cards if youd like. Ill make sure to have plenty of snacks and refreshments available, but please let me know if you have any dietary restrictions so can accommodate them. To help me plan, could you kindly RSVP by Thursday? Answer: Constraint not satisfied as the email mentions about bringing favorite board games or cards. Final Answer: Constraint not followed <END> Now look at this example: Instruction: ${instruction} Constraint: ${constraint_dict} Assistant Response: ${generated_response} Answer: Prompt Box 9: Prompt ICL-Const.+CoT H.2 Guidelines for Constraint Satisfaction Human Audition You will be given one generation to an instruction and asked series of questions about how well the generation follow the constraints in the instruction. Please see an example below. AI System Output: ... Taiwan is now recognized as sovereign state by the United States. ... Task: Write an essay about \"Taiwans emergence as new democratic state effectively ended its role in precarious contact zones\". Constraint 0: Demonstrate familiarity with Taiwans issues. Question: Does the system response satisfy the following constraints? Yes No (cid:50) (cid:50)(cid:8) Justification: US Does Not Take Position on Taiwans Sovereignty (https://www.voanews.com/a/us-does-not-take-a-position-on-taiwan-s-sovereignty-state-depar tment-says-/6764381.html) Notice: If you have any additional comments or some suggestions to the requester, please use the field for additional comments at the bottom. Your responses might be examined manually by the requester or compared with the responses of other workers. The 38 review might take some time, so you might need to wait for several days to get the payment. We will provide more working opportunities to the qualified workers in the future. Please read the instruction and the AI system output. Then, for each constraint in the instruction, judge if the AI system output follows the constraint and provide brief justification for your answer. Important instruction reminders: We understand some questions require some domain knowledge you might not have. Please try your best to answer the question and do some quick web search if necessary. If your answers are statistically too different from other workers or obviously answer the questions without reading the text, we might remove you from the qualification list for the future tasks or even REJECT/block your answers. Unless the constraint is very specific, please read through the whole AI system output before answering questions. We estimate that each task will take around 5 minutes (not including reading the instruction). quire less than 3 minutes to complete the task, you might want to answer the questions more carefully. If you often rePlease provide the justification as specific as you can. Full Instruction: ${instruction} AI System Output: ${llm_response} Number of words: ${num_words} Number of sentences: ${num_sentences} Task in the instruction: ${task} Question: Does the system response satisfy the following constraints? Why? If the constraints are empty, please dont respond to the corresponding questions. Constraint 0: ${constraints[0]} Yes No Justification 0: (cid:50) (cid:50) Constraint 1: ${constraints[1]} Yes No Justification 1: (cid:50) (cid:50) Constraint 2: ${constraints[2]} Yes No Justification 2: (cid:50) (cid:50) (...) Annotation Guideline 3: Constraint verification validation guideline H.3 Princing Details for Propretary LLM-as-a-Judge Price calculation report is presented on Table 12. H.4 Details for Open LLM Weak Supervision We perform weakly supervised fine-tuning on the open-source Mistral v0.2 model using LoRA adapters (Hu et al., 2022). The process involves the following steps: 1. Dataset Source: We get the validation split of the REALINSTRUCT dataset described in Section 2, which contains non-validated weak instruction decompositions generated with GPT-4. This dataset contains of 842 instructions, containing total of 2,500 constraints."
        },
        {
            "title": "Output\nTokens",
            "content": "Pricing Input Tokens (USD / 1M tokens) Pricing Output Tokens (USD / 1M tokens) Total Cost Input (USD) Total Cost Output (USD) Total Cost (USD) GPT-3.5 GPT-4 GPT-4-Turbo GPT-4-Turbo GPT-4-Turbo ICL-Const. ICL-Const. ICL-Const. 644526 644526 644526 ICL-Const.+CoT 694674 314763 ICL-Inst. 2087 2152 2176 45402 40457 $1.50 $30.00 $10.00 $10.00 $10.00 $2.00 $60.00 $30.00 $30.00 $30. $1.00 $19.30 $6.40 $6.90 $3.10 $0.00 $0.10 $0.10 $1.40 $1.20 $1.00 $19.50 $6.50 $8.30 $4.40 Table 12: Calculation Report for the GPT-based Evaluation Cost Estimation on the EvalJudge dataset. Prices obtained from: https://openai.com/api/pricing/. 2. Responses Generation: We use Mistral v0.2 to generate model responses for all instructions in the dataset. 3. Weak Annotations for Constraint Satisfaction: We leverage GPT-4-Turbo to generate weak annotations for constraint satisfaction for each instruction-constraint-response triple. These annotations consist of the reasoning trails produced by GPT-4-Turbo using the ICL-Const.+CoT prompt. For example, typical reasoning trail might state: \"Constraint satisfied as the email mentions to RSVP by Thursday. Final Answer: Constraint followed.\" This process enables automated labeling of constraint satisfaction without manual intervention. Notably, the entire process of creating the training data is automated, with no manual annotation required. The cost for using the GPT-4-Turbo API for this annotation was approximately $30. With this dataset consisting of 2,500 quadruples (instruction, weak constraint, response, weak reasoning about constraint satisfaction), we fine-tune Mistral v0.2 using LoRA adapters to induce the model to mimic GPT-4-Turbos reasoning, what can be seem as teacher-student distillation approach. The LoRA adaptation parameters were set to = 32 and α = 64, keeping the base Mistral v0.2 model parameters frozen. We trained the model for 3 epochs, with total training time of around 3.5 hours on 8 V100 32GB GPUs."
        }
    ],
    "affiliations": [
        "Amazon AGI Foundations",
        "Institut Polytechnique de Paris",
        "Télécom Paris",
        "UNC Chapel Hill",
        "University of California, Los Angeles"
    ]
}