{
    "paper_title": "RealDPO: Real or Not Real, that is the Preference",
    "authors": [
        "Guo Cheng",
        "Danni Yang",
        "Ziqi Huang",
        "Jianlou Si",
        "Chenyang Si",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."
        },
        {
            "title": "Start",
            "content": "REALDPO: REAL OR NOT REAL, THAT IS THE PREFERENCE Guo Cheng3 Danni Yang1 Ziqi Huang2 1Shanghai Artificial Intelligence Laboratory 3University of Electronic Science and Technology of China 5SenseTime Research Project Page: Vchitect.github.io/RealDPO-Project Jianlou Si5 Chenyang Si4 Ziwei Liu2(cid:66) 2S-Lab, Nanyang Technological University 4Nanjing University 5 2 0 O 6 1 ] . [ 1 5 5 9 4 1 . 0 1 5 2 : r Figure 1: Can we align video generative models using real data as preference data without reward model? (a) Comparison between using the reward model to score synthetic data for preference learning and our RealDPO method, which uses high-quality real data as win samples. Our method avoids the limitations of the reward model and the associated hacking issues. (b) Comparison between the video generated by the pretrained model and the real video for the same scene. The three scores on the right represent the scores given by the reward model from VisionReward (Xu et al., 2024a), the human action metric from VBench (Huang et al., 2024a;b), and human preference, respectively. It can be observed that while the existing reward model and VBench can evaluate semantic correctness, they are limited in assessing human motion quality. (c) Three model-generated examples from the same prompt, each with different initial noise, exhibit poor limb interaction, making it challenging for human annotators to identify which sample should be chosen as the win sample for reward model training."
        },
        {
            "title": "ABSTRACT",
            "content": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate Equal Contributions. Project Lead. (cid:66)Corresponding Author. 1 Figure 2: RealDPO vs SFT. qualitative comparison between RealDPO and supervised fine-tuning (SFT). RealDPO demonstrates more natural motion generation. For more details regarding the comparison, please refer to the supplementary material. that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."
        },
        {
            "title": "INTRODUCTION",
            "content": "With the advancement in computational power and the availability of large-scale data, video generation models (Yang et al., 2024b; Guo et al., 2023; Blattmann et al., 2023; Li et al., 2024; Lin et al., 2024; Wang et al., 2024b; Xing et al., 2024; Zhang et al., 2023) have made significant progress, producing more realistic and diverse visual content. However, when it comes to generating complex motions, existing models still face considerable challenges in creating motion sequences that adhere to appear natural and smooth, and align with contextual consistency. This issue becomes especially prominent in the generation of human-centric daily activity motions. As shown in Fig. 1(b), even the results generated by the state-of-the-art DiT-based model CogVideoX-5B (Yang et al., 2024b) exhibit unnatural and unrealistic movements, failing to meet human preferences for natural, smooth, and contextually appropriate actions. This prompts us to further explore how to improve the realism and rationality of complex motion generation, particularly in the domain of human motion synthesis. straightforward solution is to collect set of high-quality, real-world data specifically for supervised fine-tuning (SFT). However, relying exclusively on this dataset for SFT training presents certain limitations. During optimization, the model interprets the provided data as the sole correct reference, lacking awareness of where the original models errors stem from. This narrow focus may result in overfitting and suboptimal performance in Fig. 2. more effective strategy would be letting the model learn from its own mistakes. By utilizing the difference between real samples (positive data) and generative samples (negative data), we can explicitly highlight the models errors and guide it to correct its behavior. This approach enables the model to progressively align its outputs with the desired actions represented by the positive samples, fostering continuous improvement through self-reflection. This idea aligns perfectly with Direct Preference Optimization (DPO) (Rafailov et al., 2023), reinforcement learning technique used in training large language models, which leverages pair-wise win-lose data to guide the learning process. In video generation, recent studies (Liu et al., 2024; Wang et al., 2024c; Xu et al., 2024a; Yuan et al., 2024; Zhang et al., 2024a) have explored training fine-grained reward models using human-annotated preference datasets, primarily through three ways: reward-weighted regression (RWR) (Wang et al., 2024c), direct preference optimization (DPO) (Liu et al., 2024), and gradient feedback (GF) (Yuan et al., 2024). However, these methods face some critical challenges when directly applied to action-centric video generation: (1) Reward Hacking: Video reward model is susceptible to reward hacking, where during the optimization process, human evaluations indicate decline in video quality, yet the reward model continues to assign high scores. (2) Scalability Issue: Online approaches require decoding latent to pixel space, limiting their scalability for highresolution video generation. (3) Bias Propagation: Multi-dimensional reward models may lose the ability to assess specific key metrics due to linear combinations of evaluation criteria. As shown in 2 Fig. 1(b), the reward model cannot provide an accurate evaluation for complex motion. These limitations highlight the need for more robust approach tailored to complex motion video generation, motivating our extension beyond traditional DPO frameworks. To address these challenges, we propose RealDPO, novel training pipeline for generating actioncentric activity videos, as shown in Fig. 1(a). Unlike prior methods that rely on model-sampled pairwise comparisons, RealDPO leverages real-world video data as win samples, overcoming the Real Data Deficiency issue where only using synthetic data for preference learning fails to address the distribution errors inherent in the pre-trained generative model. More importantly, this approach significantly enhances the models learning upper bound, enabling more accurate video generation. Without real video guidance, as shown in Fig. 1(c), all samples generated by pre-trained model exhibit poor limb interaction, making it hard for human annotators to identify the preferred win sample. Additionally, since RealDPO directly uses real data to guide the preference learning, it eliminates the need for an external reward function, thereby avoid reward hacking and bias propagation issues. Moreover, our naturally paired win-lose samples eliminate the need for decoding latent to pixel space during training, drastically reducing computational overhead. Inspired by DiffusionDPO (Wallace et al., 2024), we design tailored DPO loss specifically for the training objective of diffusion-based transformer architectures, enabling effective preference alignment. To support this training, we introduce RealAction-5K, compact yet high-quality video dataset capturing diverse human daily actions. The dataset adheres to the principle of less is more, emphasizing that RealDPO requires fewer high-quality samples in synergy with model-generated negative samples, whereas traditional supervised fine-tuning (SFT) methods typically requires more data to achieve better performance. Experiments demonstrate that RealDPO significantly improves video quality, text alignment, and action fidelity across diverse human action scenarios compared to pretrained baselines and other preference alignment methods. Our contributions are summarized as follows: We propose RealDPO, novel training pipeline for action-centric video generation that leverages real-world data as preference signals to contrastively reveal and correct the models inherent mistakes, addressing the limitations of existing reward models and preference alignment methods. We design tailored DPO loss for our video generation training objective, enabling efficient and effective preference alignment without the scalability and bias issues of prior approaches. We introduce RealAction-5K, compact yet high-quality curated dataset focused on human daily actions, specifically crafted to advance preference learning for video generation models and broader applications."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion-Based Video Generation. In recent years, diffusion-based video generation models have emerged continuously, primarily generating videos through user-provided text or image prompts. These models are broadly categorized into two architectures: U-Net and Diffusion Transformers (DiT). U-Net-based approaches (Blattmann et al., 2023; Wang et al., 2023; Chen et al., 2024; Guo et al., 2023) build upon the multi-stage down-sampling and up-sampling framework of image diffusion models, incorporating temporal attention layers to ensure temporal consistency. However, these methods face limitations in motion dynamics and content richness. Recently, Diffusion Transformerbased methods (Yang et al., 2024b; Li et al., 2024; Lin et al., 2024) have made significant improvements by combining 3D-VAE with diffusion transformers, using 3D full-attention layers to jointly learn spatial-temporal correlations, and enhance text encoders to handle complex prompts. These advancements have led to substantial improvements in fidelity, consistency, and scalability for longer video generation. Reinforce Learning in Image/Video Generation. In large language models (LLMs), reward models are commonly used in Reinforcement Learning Human Feedback (RLHF), enabling LLMs to respond more naturally and generate more coherent text. Recently, there have been series of studies(Xu et al., 2024b; Black et al., 2023; Wallace et al., 2024; Lee et al., 2023; Liang et al., 2024; Yang et al., 2024a; Clark et al., 2023; Fan et al., 2024) in image generation that incorporate human preferences into model evaluation and model alignment training, mainly focusing on improving the aesthetic quality of images. In video generation, the exploration so far is still quite limited. Most related 3 Figure 3: Overview of the RealAction-5K Dataset. (a) Samples of RealAction-5K Dataset (b) Data Collection and Processing Pipeline (c) Video Caption Word Distribution (d) Action Content Distribution (e) Prompt Length Distribution works(Yuan et al., 2024; Wu et al., 2024; Wang et al., 2024c; Liu et al., 2024; Zhang et al., 2024a; Liu et al., 2025) are mainly focusing on using reward models trained on human-annotated synthetic data for preference learning on model-generated data. However, these methods have some limitations. For example, training reward models may suffer from hacking issues, and multi-dimensional reward models might show reduced evaluation ability in specific domains. Additionally, relying entirely on synthetic data for preference learning could hinder the models potential. Therefore, we propose novel approach that transcends the limitations of reward models by incorporating real data for preference-aligned learning."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "3.1 DENOISING PROCESS AS MULTI-STEP MDP According to the definition in the Yang et al. (2024a), the denoising process in diffusion models can be formulated as multi-step Markov Decision Process (MDP). Here, we provide further explanation of state representations st, probability transition , and policy functions π, establishing correspondence between video diffusion models and the MDP framework. This mapping enables reinforcement learning perspective on the sampling process in video diffusion models. The detailed notation correspondence between the diffusion model and the MDP is as follows: st (c, t, xt) (st+1 st, at) (δc, δt+1, δxt1) at xt1 ρ0(s0) (p(c), δ0, (0, I)) r(st, at) r((c, t, xt), xt1) π(at st) pθ(xt1 c, t, xt) (1) where δx represents the Dirac delta distribution, and denotes the denoising timesteps. Leveraging this mapping, we can employ RL techniques to fine-tune diffusion models by maximizing returns. However, this approach requires proficient reward model capable of adequately rewarding the noisy images. The task becomes exceptionally challenging, particularly when is large, and xt closely resembles Gaussian noise, even for an experienced expert. 4 Figure 4: The RealDPO Framework. We use real data as the win samples in DPO, and illustrate the data pipeline on the left hand side. We present the RealDPO loss, and reference model update strategy on the right hand side. 3.2 DPO FOR DIFFUSION MDP Direct Preference Optimization (DPO) (Rafailov et al., 2023) is preference-based fine-tuning method that directly optimizes model using human preference data, without requiring an explicit reward model. This approach is particularly advantageous as it avoids the complexities and potential biases introduced by learned reward models, making the optimization process more stable and interpretable. Given dataset of preference-labeled pairs {(x, yw, yl)}, where is the input prompt, yw is the preferred (win) output, and yl is the less preferred (lose) output, DPO aims to maximize the likelihood ratio between the preferred and non-preferred samples while maintaining closeness to the pretrained model. The optimization objective can be formulated as: LDPO(θ) = Ec,xw,xl (cid:20) (cid:18) log σ β log pθ(xwc) pref(xwc)) β log (cid:19)(cid:21) pθ(xlc) pref(xlc) (2) where: pθ(xwc) is the likelihood of generating win output xw given input under the fine-tuned model, pref(xwc) is the likelihood under the reference (pretrained) model. β is temperature parameter that controls the sharpness of preference optimization. σ() is the sigmoid function ensuring proper probability score. According to the derivation in reference (Wallace et al., 2024), the training objective of DiffusionDPO is defined as: L(θ) = (xw ϵw ϵθ(xw = αtxw 0)D,tU(0,T ),xw q(xw 2 ϵw ϵref(xw 0 ,xl 0 ),xl txl 2 (cid:0)ϵl ϵθ(xl , t)2 0 + σtϵw, ϵw (0, I) is draw from (xw xw , t)2 0) log σ (βT ω(λt) ( t, t)2 xw 2 ϵl ϵref(xl /σ2 0 ). λt = α2 where xw to-noise ratio. ω(λt) is weighting function, usually kept constant. t, t)2 2 is the signaltq(xl (cid:1)(cid:1)(cid:1) (3)"
        },
        {
            "title": "4 THE REALDPO PARADIGM",
            "content": "In this section, we introduce our fine-tuning pipeline RealDPO to align video diffusion models with our constructed preferences data, as shown in Fig. 4. Firstly, we introduce our proposed dataset, RealAction, and the pipeline for constructing preference data in Sec.4.1. Then, in Sec.4.2, we present the win-lose sampling approach used for DPO fine-tuning training. Finally, in Sec.4.3, we delve into the alignment training process for the video generation model using preference data. 4.1 REALACTION: PREFERENCE DATA COLLECTION Preference data is essential for reinforcement learning. To acquire it, we designed robust data processing pipeline that efficiently collects, filters, and processes data, ensuring its high quality, diversity, and representativeness. 5 Collect raw video based on keywords. Our dataset construction begins with selecting relevant topics to collect raw video data, ensuring diversity and real-world representativeness. As shown in Fig. 3(d), we designed daily activity themes across over ten scenarios, such as sports, eating, drinking, walking, and gathered high-quality video clips using these keywords. This step captures diverse actions, participants, and backgrounds, establishing strong foundation for preference-based training. Use VideoLLM to filter low-quality videos. After collecting the raw videos, we use video LLM, Qwen2-VL (Wang et al., 2024a) , to filter out rough or irrelevant videos. We provide some instructions for Qwen2-VL to identify and discard videos that do not meet quality standards or are not aligned with the selected topic. Through this filtering process, low-quality content is significantly reduced, ensuring that only clear and meaningful videos proceed to next processing stages. Manual inspection ensures the accuracy. We let human annotators carefully examine the videos to confirm whether they accurately represent the intended theme, have correct actions, and do not contain misleading or irrelevant content. This additional validation step further refines the dataset, ensuring it aligns with preference-based training goals. Generate detailed descriptions for videos. We employ video understanding model, LLaVAVideo (Zhang et al., 2024b), to generate accurate descriptive captions for each video. These descriptions accurately reflect the actions, participants, and appearance. These captions serve as valuable metadata, later used for sampling negative samples. The word cloud composed of high-frequency words in the description caption of these videos is shown in Fig. 3(c). And the length distribution of captions in our constructed dataset is shown in the Fig 3(e). 4.2 WIN-LOSE SAMPLING FOR DPO TRAINING After obtaining real data, we take the real video from RealAction as win sample. The latent after compression through the VAE encoder is 0 . We design Timestep Selector that randomly generates timestep for each round of positive and negative sampling. We add steps of random . Then, together with the caption embedding etext, we input this into the noise to DiT transformer to get the predicted noise ˆϵw to Positive Sample Velocity to obtain the predicted latent ˆxw 0 , which is prepared for the subsequent DPO loss. . Finally, we input ˆϵw 0 , obtaining = θ (cid:0)xw ˆϵw , etext(cid:1) , ˆxw 0 = ψ (ˆϵw ) , (4) where θ is the training DiT model, ψ is the process of positive sample velocity. For negative samples, in order to ensure diversity, we first randomly generate three init noises ϵa, ϵb, ϵc. These are then combined with the positive samples caption embedding etext and we input them together into the DiT, where we sample the full timesteps to obtain three negative samples xl,a 0 , xl,b 0 , xl,c 0 . This step is done offline, and we only need to store the latent of the negative samples. During training optimization, similar to the positive samples, we add steps of random noise to these three samples to obtain xl,a . Then, together with caption embedding etext, we input these into the DiT transformer to get the predicted noise ˆϵl,a . Finally, we input predicted noise to Negative Sample Velocity to obtain the predicted latents for the negative samples ˆxl,a , xl,b , xl,c , ˆϵl,b , ˆϵl,c 0 , ˆxl,b 0 , ˆxl,c 0 . ˆϵl, = θ (cid:16) , etext(cid:17) xl, , ˆxl, 0 = ψ (cid:16) (cid:17) , ˆϵl, (5) where is set of {a, b, c}, ψ is the process of negative sample velocity. Its important to note that the first sampling of the negative samples is done offline, while the second sampling for both positive and negative samples involves only one step, saving significant amount of time during training. 4.3 PREFERENCE LEARNING FOR VIDEO GENERATION After positive and negative samples are prepared, we can use this preference data for DPO training. Due to the constraints of the reference model in DPO training, similarly, we also resample the winlose samples through the reference model to obtain xw 0 . Here, we take the first negative 0 and xl,a 6 Table 1: Quantitative Comparison on RealAction-TestBench by User Study. We provided users with five dimensional evaluation, namely Overall Quality, Visual Alignment, Text Alignment, Motion Quality and Human Quality, to compare our model with the pre-trained baseline (Yang et al., 2024b), supervised fine-tuning(SFT), LiFT (Wang et al., 2024c), VideoAlign (Liu et al., 2025). Testers are required to rank the results generated by these models, and we converted the rankings into win rates. Method Overall Motion Human Quality Alignment Alignment Quality Quality Visual Text Baseline (Yang et al., 2024b) SFT LiFT (Wang et al., 2024c) VideoAlign (Liu et al., 2025) RealDPO (Ours) 65.56 58.22 67.34 61.00 73. 72.22 65.22 73.44 68.11 77.44 71.89 68.44 64.33 68.78 77.00 65.56 59.11 65.00 57.22 71.00 66.00 60.33 67.33 59.78 72.89 Figure 5: Qualitative Results. We visualize the effect of before and after applying RealDPO. See the supplementary for videos. sample xl,a 5B (Yang et al., 2024b), we rewrite Eq. 3 as follows: 0 as an example to explain the loss. According to the training objective of CogVideoXLDP O(θ) = (cid:2)log σ (cid:0)βT ω(λt) (cid:0)xw 0 ˆxw 02 0 2 2 xl 2 xw 0 xl 0 xw 02 0 2 2 (cid:1)(cid:1)(cid:1)(cid:3) , 0 ˆxl (cid:0)xl (6) 0 /xl 0 are the original win/lose sample, ˆxw 0 /xl where xw 0 are the predicted latents for the win/lose sample by the training model, xw 0 are the predicted latents for the win/lose sample by the reference model. The role of the reference model is to constrain the training process of the training model, preventing over-optimization or deviation from the desired objectives. 0 /ˆxl To enhance alignment of the model with human preferences, we gradually improve the capability of the preference model and perform multiple rounds of resampling, ensuring that the training process iteratively refines its predictions and better captures the desired outcomes. In practice, every training steps, the reference model needs to be updated using the exponential moving average (EMA) algorithm. ref ωθt θt ref + (1 ω)θt, (7) where θt and ω is the decay coefficient of EMA, set to 0.996 in our experiments. ref is the parameters of the reference model, θt denotes the parameters of the training model, 7 Table 2: Quantitative Comparison on VBench-I2V and RealAction-TestBench using MLLM. We evaluate performance via Visual Alignment (VA), Text Alignment (TA), Motion Quality (MQ), and Human Quality (HQ), which are consistent with the sensory perceptions of humans in user study. We use open-source Qwen2-VL (Wang et al., 2024a) supporting video understanding. We provide detailed instruction template for evaluating video generation quality using MLLM in the appendix. Method VA VBench-I2V TA MQ HQ RealAction-Test Bench VA TA MQ HQ Baseline (Yang et al., 2024b) 97.78% 97.71% 89.86% 90.34% 96.11% 99.22% 90.22% 91.89% 97.15% 98.26% 90.03% 89.38% 93.89% 98.89% 90.78% 92.89% SFT 97.54% 97.91% 89.25% 90.24% 97.54% 97.89% 92.00% 91.67% LiFT (Wang et al., 2024c) VideoAlign (Liu et al., 2025) 97.99% 97.66% 89.54% 90.84% 96.44% 98.89% 92.00% 92.89% 97.99% 97.74% 89.46% 90.10% 96.67% 99.22% 91.67% 94.11% RealDPO (Ours) Table 3: Quantitative Comparisons with baselines and reward-based methods via VBenchI2V (Huang et al., 2024a;b), on RealAction-TestBench. Model I2V Subject Background Motion Dynamic Aesthetic Imaging Subject Consistency Consistency Smoothness Degree Quality Quality Baseline (Yang et al., 2024b) 96.10 5 96.47 SFT - LiFT (Wang et al., 2024c) 96.50 VideoAlign (Liu et al., 2025) 96.55 96.58 RealDPO (Ours) 90.43 89.50 92.34 92.23 91.68 94.01 93.18 94.46 94.29 94. 98.15 98.06 98.20 98.37 98.31 55.56 66.67 38.89 50.00 55.56 59.63 59.69 60.51 60.21 61.37 67.01 67.06 68.40 67.66 68."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We present the main experiments and discussions in this section. Please refer to the supplementary material for implementation details on the models and evaluation metrics. 5.1 QUANTITATIVE COMPARISONS Quantitative Comparison by User Study. Tab. 1 showcases the evaluation results on the RealAction-TestBench test set, where testers were invited to rank the generated outputs of the pretrained baseline (Yang et al., 2024b), supervised fine-tuning (SFT), LiFT (Wang et al., 2024c), VideoAlign (Liu et al., 2025), and our RealDPO. The evaluation covers five dimensions: Overall Quality, Visual Alignment, Text Alignment, Motion Quality, and Human Quality. The scores for each model across these dimensions were calculated and summarized. As shown in Tab. 1, our RealDPO demonstrates significant improvements over baseline and SFT in multiple dimensions, indicating that our proposed data effectively enhance the capabilities of RealDPO in action-centric scenarios. Additionally, compared to other preference alignment algorithms utilizing reward models, such as LiFT (based on Reward Weighted Regression) and VideoAlign (a naive DPO variant using synthetic data), our approach of leveraging real data as win samples and synthetic data as lose samples also proves its effectiveness. Quantitative Comparison Using MLLM. To enhance the diversity of evaluation, we employ MLLM capable of video understanding tasks to assess the results generated by the models in question-answer format across multiple dimensions. In Tab. 2, we selected Qwen2-VL (Wang et al., 2024a) as an evaluation tool, to align the assessment dimensions with user study: Visual Alignment (VA), Text Alignment (TA), Motion Quality (MQ), and Human Quality (HQ) on VBench-I2V test benchmark (Huang et al., 2024b) and RealAction-TestBench. For each dimension, we designed several questions, and yes response from the large models indicates passing score. The scores for all questions within each dimension were aggregated to calculate the total score. Experimental results show that, based on the evaluation by video-language understanding models, our model shows 8 Figure 6: Qualitative Comparison.We recommend readers refer to our appendix files to view more visualizations. competitive results, consistent with human evaluation results, further validating the effectiveness of our RealDPO. Quantitative Comparison Using VBench-I2V Metric. Meanwhile, in video generation, VBench (Huang et al., 2024a;b) is widely recognized as an authoritative evaluation framework. Leveraging VBench-I2Vs automated metrics designed for Image-to-Video (I2V) evaluations in VBench++ (Huang et al., 2024b), we assessed the quality of our test set, revealing that RealDPO achieves competitive performance across multiple general metrics. 5.2 QUALITATIVE COMPARISONS In Fig. 5, we present the visual comparison results before and after RealDPO alignment. We observe that RealDPO is highly effective in enhancing the naturalness and smoothness of the actions, as well as their consistency with the textual instructions. In Fig. 6, we present the visual comparison results of our method against other alignment approaches, such as LiFT (Wang et al., 2024c) and VideoAlign (Liu et al., 2025). It can be observed that the videos generated by RealDPO are more stable and less prone to unnatural actions or visual collapse. For instance, in the example on the left, SFT exhibits collapse of the characters limbs, and the coordination of the dogs four legs appears unnatural. The results of LiFT are slightly better, but LiFT fails to complete the handshake action between the protagonist and the dog, resulting in poor alignment with the text. In contrast, our results demonstrate higher visual quality, with action details highly consistent with the textual instructions and no visual collapse. In the example on the right, the text describes the surfers posture as with knees bent and body leaning back slightly. SFT shows visual collapse, misaligned actions, and poor consistency in character appearance. VideoAlign performs slightly better, but the generated posture and actions are not highly aligned with the text. In comparison, our results exhibit higher image quality, more accurate action details, and overall superior performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose RealDPO, novel and data-efficient framework for preference alignment in video generation, leveraging real-world data as win samples to address challenges in generating complex motions like human actions. By designing tailored DPO loss and building on diffusion-based transformer architectures, we establish robust real-data-driven alignment framework. To support this, we introduce RealAction-5K, compact yet high-quality dataset for human daily actions. Extensive experiments show that RealDPO significantly improves visual alignment, text alignment, motion quality, and overall video quality, outperforming traditional fine-tuning and other alignment methods. Our work advances the upper bound of preference alignment and provides scalable solution for complex motion video generation. We will explore extending RealDPO to broader domains in the future. 9 Ethics statement. Our RealAction-5K dataset was curated from publicly available video sources with appropriate licenses. To address privacy concerns, all personally identifiable information was meticulously anonymized, and the dataset will be released strictly for non-commercial research purposes to mitigate the risk of misuse. All necessary legal and ethical guidelines concerning data provenance and usage were adhered to throughout the project. Additionally, the effectiveness of our RealDPO paradigm is inherently limited by the architectural constraints of the underlying video generative model. We emphasize the need for responsible use, particularly when generating human figures, to prevent potential misuse. Reproducibility statement. To ensure the reproducibility of our work, we have made significant efforts to document our methodology and resources comprehensively. The core of our approach, the RealDPO alignment paradigm, including its tailored loss function, is described in detail within the paper. Furthermore, we provide complete account of the data collection and processing pipeline for the RealAction-5K dataset. This dataset was meticulously curated by manually sourcing highquality videos from https://pexels.com. The process involved combination of targeted scraping and manual downloading, followed by rigorous manual screening and clipping to ensure each video clip depicts single, coherent action that can be accurately described in text, thereby guaranteeing high quality and clarity."
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221-0012, MOE-T2EP20223-0002). This research is also supported by cash and inkind funding from NTU S-Lab and industry partner(s). This work is also supported by Shanghai Artificial Intelligence Laboratory. This work was partially supported by Nanjing Kunpeng&Ascend Center of Cultivation."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024a. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, 10 Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024b. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1940119411, 2024. Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pp. 120, 2024b. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024c. Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Boosting text-to-video generative model with mllms feedback. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pp. 399417. Springer, 2024. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024a. 11 Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024b. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64636474, 2024. Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024a. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, I2vgen-xl: High-quality image-to-video synthesis via cascaded Deli Zhao, and Jingren Zhou. diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024b."
        },
        {
            "title": "APPENDIX",
            "content": "This supplementary material provides more qualitative results, details of the evaluation, experimental results, pseudo-code of RealDPO. Section elaborates on additional visual comparisons of generated videos, including comparisons with pre-trained models, supervised fine-tuning, and other alignment methods. Section details the evaluation process, covering the design of user studies, evaluation using LLMs, evaluation using the VBench-I2V metric, as well as interfaces, instructions, and an introduction to automated evaluation metrics. Section presents the pseudo-code of our core algorithm, RealDPO."
        },
        {
            "title": "A MORE QUALITATIVE RESULTS",
            "content": "Due to space limitations in the main text, this section presents additional visual comparisons, including comparisons with pre-trained models, supervised fine-tuning, and other alignment methods. The results demonstrate that our approach achieves superior performance across wider range of samples, with enhanced visual-text alignment, text alignment, motion quality, character quality, and overall quality. These findings further validate the effectiveness of the RealDPO framework. This provides new insights and methodologies for future multi-modal generation tasks. A.1 COMPARISON WITH PRE-TRAINED MODEL Pre-trained base models are typically trained on large-scale datasets and exhibit strong generalization capabilities. However, they may underperform on specific tasks, particularly those requiring finegrained alignment, such as the generation of videos with complex motion as discussed in our work. RealDPO, by incorporating guidance from real-world data through Direct Preference Optimization (DPO), excels at capturing intricate details in tasks, especially in image-text alignment. As shown in Fig. 7, compared to pre-trained models, RealDPO demonstrates significantly improved consistency in visual-text alignment and notable enhancements in the details of characters and motions. A.2 COMPARISON WITH SUPERVISED FINE-TUNING Supervised fine-tuning relies on annotated data and can achieve strong performance on specific tasks. However, its effectiveness is constrained by the quality and quantity of the available annotations. In contrast, RealDPO leverages diverse set of negative samples and real-world data as positive samples to form multiple preference pairs. This approach enables the model to learn from its own mistakes and align more closely with real-world samples, achieving robust alignment even without extensive labeled data. In particular, as shown in Fig. 8, in terms of motion quality and character quality, RealDPO generates images that are more natural, with smoother motions and richer character details. A.3 COMPARISON WITH OTHER ALIGNMENT METHOD Other reward model based alignment methods, such as LiFT and VideoAlign, may perform well on specific tasks. However, in complex scenarios, the reward models often fail to provide effective feedback, leading to misguidance in preference alignment training. In contrast, RealDPO introduces real-world samples as positive examples and pairs them with multiple negative samples generated by the model, naturally forming contrastive pairs. By guiding the model with positive samples, RealDPO enables the model to learn from its mistakes and align more closely with the correct samples, thereby better handling complex cross-modal alignment tasks. As shown in Figure 9, compared to existing alignment methods, RealDPO demonstrates greater stability in visual-text alignment and text alignment, generating images and text that are more semantically consistent, with superior motion quality. 13 Figure 7: Qualitative Results. Comparison with pre-trained model. Figure 8: Qualitative Results. Comparison with supervised fine-tuning. 14 Figure 9: Qualitative Results. comparison with other Alignment Method."
        },
        {
            "title": "B DETAILS OF THE EVALUATION",
            "content": "B.1 IMPLEMENTATION DETAILS Models and Settings. We conduct all experiments on 8 Nvidia H100 GPUs, with total batch size of 8 for training. For our I2V baseline generation model, we adopt CogVideoX-5B (Yang et al., 2024b), which uses diffusion transformer structure. We fine-tune the parameters of all its transformer blocks on the DeepSpeed framework. The learning rate is set to 1e-5, and all the models are trained for 10 epochs. Evaluation Metric. We evaluate the performance of our aligned model through three aspects: user study, automatic LLM-based evaluation, and the assessment metrics of VBench (Huang et al., 2024a). We selected 18 test cases, including test texts and reference images, which constitute the RealAction-TestBench. For the user study, we invited 10 testers to evaluate our model against other baselines across multiple dimensions. For LLM-based evaluation, we designed question template to guide the model in making decisions. As for VBench, we utilized the I2V evaluation metrics provided by VBench to perform our evaluation. B.2 EVALUATION BY USER STUDY To ensure fair and comprehensive evaluation of our model, we conducted user study to collect subjective feedback on the generated results. Participants were presented with scoring interface (as shown in Fig. 10) and asked to rate the generated videos based on several key criteria, including Visual Alignment, Text Alignment, Motion Quality and Human Quality and Overall Quality. The interface is designed to be intuitive and user-friendly, allowing participants to provide accurate and unbiased scores. Each video was evaluated by multiple users, and the final scores were averaged to ensure reliability. B.3 EVALUATION USING LLMS In addition to human evaluation, we leveraged large language models (LLMs) to assess the quality of the generated videos. We designed structured instruction template to guide the LLMs in evaluating video generation quality. The template includes detailed prompts for assessing various aspects of the videos, such as adherence to textual descriptions, visual coherence, and overall aesthetic appeal. By utilizing LLMs, we were able to obtain scalable and consistent evaluations that complement the human user study. The results from the LLM-based evaluation align closely with the user study findings, further validating the effectiveness of our approach 15 B.4 EVALUATION USING VBENCH-I2V METRIC To provide more objective and fine-grained assessment of our models performance, we select seven theme-related and human-perception-aligned representative dimensions of video quality from VBench-I2V Huang et al. (2024a;b) as the final evaluation metrics: I2V Subject, Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic Quality, and Imaging Quality. Figure 10: User Study Scoring Interface for users to give socre. 16 Instruction Template for Evaluating Video Generation Quality Using LLMs (part1) As video understanding expert, you will be required to evaluate the quality of modelgenerated videos from four different perspectives, covering the following daily human activities. The specific evaluation angles will be divided into Visual Alignment, Text Alignment, Motion Quality, and Human Quality. Under each dimension, the model needs to determine the quality of the generated content based on the answers to five questions. Each question is scored out of 10 points. The scoring rule is 0 points for the worst and 10 points for the best. The final score is the sum of the scores for all questions under that dimension. Visual Alignment This mainly assesses the consistency of the visual representation of the characters in the generated video with the provided first frame image of the characters and environment, with score range of 0 to 10. Please answer five questions as follow: Question 1: What is the consistency score of the characters appearance (such as clothing, hairstyle, skin color) in the generated video? Question 2: What is the consistency score of the environment in the generated video (such as background, lighting, scene setup)? Question 3: What is the score for the characters proportional changes in the generated video conforming to physical laws? Question 4: What is the fidelity score of the characters or environment in the video (low scores should be given if there are shape distortions or color abnormalities)? Question 5: What is the consistency score of the characters and environment over time (low scores should be given if there are sudden disappearances or changes)? Answer 1: Answer 2: Answer 3: Answer 4: Answer 5: Instruction Template for Evaluating Video Generation Quality Using LLMs (part2) As video understanding expert, you will be required to evaluate the quality of modelgenerated videos from four different perspectives, covering the following daily human activities. The specific evaluation angles will be divided into Visual Alignment, Text Alignment, Motion Quality, and Human Quality. Under each dimension, the model needs to determine the quality of the generated content based on the answers to five questions. Each question is scored out of 10 points. The scoring rule is 0 points for the worst and 10 points for the best. The final score is the sum of the scores for all questions under that dimension. Text Alignment This assesses the consistency between the actions of the characters in the generated video and the input text description or target behavior category, with score range of 0 to 10. Please answer five questions as follow: Question 1: What is the consistency score between the characters actions in the generated video and the input text description or target behavior category? Question 2: What is the consistency score between the key actions in the video (such as running, hugging, playing an instrument) and the text description? Question 3: What is the score for avoiding actions or distracting elements in the video that are unrelated to the text description? Question 4: What is the accuracy score of the video in conveying the emotions or intentions described in the text? Question 5: What is the score for the video supplementing reasonable details not explicitly mentioned in the text description? Answer 1: Answer 2: Answer 3: Answer 4: Answer 5: 18 Instruction Template for Evaluating Video Generation Quality Using LLMs (part3) As video understanding expert, you will be required to evaluate the quality of modelgenerated videos from four different perspectives, covering the following daily human activities. The specific evaluation angles will be divided into Visual Alignment, Text Alignment, Motion Quality, and Human Quality. Under each dimension, the model needs to determine the quality of the generated content based on the answers to five questions. Each question is scored out of 10 points. The scoring rule is 0 points for the worst and 10 points for the best. The final score is the sum of the scores for all questions under that dimension. Motion Quality This assesses the smoothness, naturalness, and reasonableness of the characters movements in the generated video, with score range of 0 to 10. Please answer five questions as follow: Question 1: What is the smoothness score of the characters movements in the generated video (high scores for no stuttering)? Question 2: What is the score for the details of the characters movements (such as limb movements, gestures) conforming to physical laws? Question 3: What is the naturalness score of the temporal dynamics of the characters movements (such as speed, rhythm)? Question 4: What is the coordination score between the characters movements and other elements in the scene (such as objects, background)? Question 5: What is the score for avoiding obvious distortions or unreasonable phenomena in the characters movements (such as limb twisting, incoherent actions)? Answer 1: Answer 2: Answer 3: Answer 4: Answer 5: 19 Instruction Template for Evaluating Video Generation Quality Using LLMs (part4) As video understanding expert, you will be required to evaluate the quality of modelgenerated videos from four different perspectives, covering the following daily human activities. The specific evaluation angles will be divided into Visual Alignment, Text Alignment, Motion Quality, and Human Quality. Under each dimension, the model needs to determine the quality of the generated content based on the answers to five questions. Each question is scored out of 10 points. The scoring rule is 0 points for the worst and 10 points for the best. The final score is the sum of the scores for all questions under that dimension. Human Quality This assesses the quality of the generated characters in the video, with score range of 0 to 10. Please answer five questions as follow: Question 1: What is the score for the reasonableness of limb distortions in the generated characters (e.g., unnatural joint bends)? Question 2: What is the score for the reasonableness of the number of limbs in the characters (e.g., extra or missing limbs)? Question 3: What is the naturalness score of the facial expressions or body movements of the characters in line with human behavioral characteristics? Question 4: What is the reasonableness score of the interactions between the characters and other objects in the scene (e.g., tools, animals, other people)? Question 5: What is the fluency score of the characters behavior in the generated video? Answer 1: Answer 2: Answer 3: Answer 4: Answer 5: 20 PSEUDO-CODE OF REALDPO def RealDPO_Loss(model, ref_model, x_w, x_l, c, beta): \"\"\" Computes the RealDPO loss for aligning model predictions with preferred and non-preferred samples. Args: model: Diffusion Transformer model. ref_model: Frozen reference model used for comparison. x_w: Preferred real video latents (aligned with the desired output). x_l: Non-preferred video-generated video latents (not aligned with the desired output). c: Conditioning input (e.g., text embeddings, image embeddings). beta: Regularization parameter controlling the strength of the alignment. Returns: realdpo_loss: The computed RealDPO loss value. \"\"\" # Sample random timesteps and noise for diffusion process timestep_k = torch.rand(len(x_w)) noise = torch.randn_like(x_w) # Create noisy versions of preferred and non-preferred latents noisy_x_w = (1 - timestep_k) * x_w + timestep_k * noise noisy_x_l = (1 - timestep_k) * x_l + timestep_k * noise # Predict latents using the model and reference model latent_w_pred = model(noisy_x_w, c, timestep_k) latent_l_pred = model(noisy_x_l, c, timestep_k) latent_ref_w_pred = ref_model(noisy_x_w, c, timestep_k) latent_ref_l_pred = ref_model(noisy_x_l, c, timestep_k) # Compute prediction errors for preferred and non-preferred latents model_w_loss = (x_w - latent_w_pred).norm().pow(2) ref_w_loss = (x_w - latent_ref_w_pred).norm().pow(2) model_l_loss = (x_l - latent_l_pred).norm().pow(2) ref_l_loss = (x_l - latent_ref_l_pred).norm().pow(2) # Compute alignment differences w_loss_diff = model_w_loss - ref_w_loss l_loss_diff = model_l_loss - ref_l_loss # Compute the RealDPO loss alignment_term = -0.5 * beta * (w_loss_diff - l_loss_diff) realdpo_loss = -1 * torch.log(torch.sigmoid(alignment_term)) return realdpo_loss THE USE OF LARGE LANGUAGE MODELS (LLMS) In the preparation of this manuscript, we used GPT-4, large language model from OpenAI, exclusively as writing assistance tool. Its use was confined to the Introduction and Methods sections, where it served to aid in polishing the text. Specifically, the model was prompted to help restructure sentences for improved clarity and flow, ensure consistent academic tone, and simplify complex technical descriptions. All fundamental ideas, research hypotheses, methodological designs, experimental data, analysis, conclusions, and the final intellectual content are solely the product of the authors work. The LLM generated no original content or ideas and was not used for data analysis or interpretation. The authors carefully reviewed, edited, and verified all AI-generated text to ensure it accurately reflected their research and adhered to the highest standards of academic integrity."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Shanghai Artificial Intelligence Laboratory",
        "University of Electronic Science and Technology of China"
    ]
}