{
    "paper_title": "Selecting Optimal Candidate Profiles in Adversarial Environments Using Conjoint Analysis and Machine Learning",
    "authors": [
        "Connor T. Jerzak",
        "Priyanshi Chandra",
        "Rishi Hazra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . s [ 1 3 4 0 9 1 . 4 0 5 2 : r a"
        },
        {
            "title": "ENVIRONMENTS USING CONJOINT ANALYSIS AND MACHINE",
            "content": "LEARNING Connor T. Jerzak Priyanshi Chandra Rishi Hazra April 29,"
        },
        {
            "title": "Abstract",
            "content": "Conjoint analysis, an application of factorial experimental design, is popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments. Keywords: Causal inference; High-dimensional treatments; Randomized experiments; Policy learning Preliminary. We are grateful to Kosuke Imai. For helpful feedback, we thank Juan Dodyk, Oscar Clivio, Molly Offer-Westort, as well as participants of PolMeth XL, the Statistics and Data Science Seminar at the University of Texas at Austin and the Center for Data and Methods Colloquium at the University of Konstanz. We thank Soubhik Barari for excellent research support. Code implementing the modeling approaches described in this paper is available at github.com/cjerzak/strategize-software/. There is also web-based demonstration of the proposed methods available at huggingface.co/cjerzak/stategize. Department of Government, University of Texas at Austin."
        },
        {
            "title": "158 W 21st, Austin, TX 78712. Email: con-",
            "content": "nor.jerzak@austin.utexas.edu. URL: ConnorJerzak.com. Faculty of Informatics, Università della Svizzera Italiana. Email: priyanshi.chandra@usi.ch. Department of Statistics, Harvard College. Email: rishihazra@college.harvard.edu. Introduction Over the past decade, conjoint analysis, which is an application of high-dimensional factorial design, has become the most popular survey experiment methodology to study multidimensional preferences (Hainmueller et al., 2014). One of the most common political science applications of conjoint analysis is the evaluation of candidate profiles (e.g., Bansak, Hainmueller, Hopkins, Yamamoto, et al., 2021; Berz and Jankowski, 2022; Christensen et al., 2021; Franchino and Zucchini, 2015; Kirkland and Coppock, 2018; Ono and Burden, 2019; Rehmert, 2022). In such experiments, respondents are asked to choose between two hypothetical political candidates whose features (e.g., gender, race, age, education, partisanship, and policy positions) are randomly selected. Most researchers then proceed by estimating the average causal effect of each feature while marginalizing the remaining features over particular distribution of choice. This popular quantity of interest is termed the Average Marginal Component Effect (AMCE) (Hainmueller et al., 2014). One important property of the AMCE is that it depends upon the distribution of other features used for marginalization (de la Cuesta et al., 2019). Although many researchers simply use the uniform distribution, this seemingly innocuous choice implies that the AMCE is applicable only when other features are independently and randomly selected with equal probabilities. While the choice of feature distribution becomes irrelevant once we assume the absence of interaction between features, such an assumption is at odds with the main motivation for the use of conjoint analysis. Substantively, in the case of candidate choice experiment, the use of the uniform distribution means that we are evaluating one feature of candidate profile against an opponent with randomly selected features while also randomly selecting the other features of the candidate. In practice, however, political actors, such as parties, face an adversarial setting in which each party is strategically fielding its own candidate in order to maximize the probability of electoral victory. These strategic considerations make it difficult to justify the common assumption of randomly selected features in most existing analyses of conjoint experiments. To address this problem, we consider the identification of optimal candidate profiles in an adversarial environment where each party simultaneously chooses its own candidate to maximize the expected vote share. Specifically, we consider an adversarial environment where candidate profiles of each party face two stages of competition: first, intra-party competition at the primary election stage and, second, inter-party competition at the general election stage. To the best of our knowledge, the problem of optimal profile selection has not been considered in the conjoint analysis literature, regardless of whether or not the environment is adversarial. Therefore, before moving on to the adversarial setting where both candidates are optimized, we first consider the case where party maximizes expected electoral performance against static opponent. One methodological challenge for optimal candidate selection is the fact that in typical conjoint experiment, the total number of feature combinations far exceeds its sample size. This means that many profiles are not observed in the data, so it is generally impossible to determine from observed data the optimal candidate profile. We address this identification problem by deriving an optimal stochastic intervention, which in our application represents probability distribution of candidate characteristics. Unlike the case of finding fixed, optimal treatment, the use of stochastic intervention allows us to better characterize the types of candidate characteristics that are likely to yield higher vote share when candidates have many features. To avoid overfitting, we apply regularization approach that prevents learned profile distribution with high variance. In the empirical application to vote choice for the US president (Ono and Burden, 2019), we show here how the introduction of adversarial dynamics into conjoint analysis can provide insights that better align with observed data from real elections. Specifically, we find that the expected vote 1 share under adversarial competition aligns more closely with actual election outcomes compared to vote shares predicted by optimizing candidate features without strategic consideration. Moreover, the likelihood of observing actual candidate profiles in the 2016 primary election is higher under the adversarial setting. We also find that, at least in the 2016 election, Republican candidates seem to have features that more closely align with their respective inferred strategies compared to Democrats. Related literature. We hope to make methodological contributions to two separate strands of existing literature: conjoint analysis and policy learning. In the fast-growing literature on conjoint analysis, we are the first to consider the problem of finding optimal profiles. Although there is large body of related work in the sequential decision-making context (e.g., the multiarm bandit setting of Audibert et al. (2010)), in the non-sequential conjoint setting, most existing works have focused upon the estimation of various causal effects (de la Cuesta et al., 2019; Egami and Imai, 2019; Goplerud et al., 2022; Hainmueller et al., 2014), hypothesis testing (Ham et al., 2022; Liu and Shiraito, 2023), interpretation of causal estimands (Abramson, Koçak, et al., 2022), and experimental designs (Bansak, Hainmueller, Hopkins, and Yamamoto, 2018). In particularly related vein, recent work highlights potential challenges in interpreting conjoint results straightforwardly as representing stable preferences. For instance, Abramson, Koçak, et al. (2022) question what conjoint experiments reveal about underlying voter preferences when considering the constraints of the experimental setup itself, while Abramson, Kocak, et al. (2023) demonstrate methods for detecting preference cycles within forced-choice conjoint data, suggesting that observed choices may not always align with standard rational choice models. While acknowledging these important interpretive challenges regarding the nature of preferences elicited via conjoints, our work takes different approach. Instead of focusing solely on estimating average effects or testing preference consistency, we use the conjoint data as input for determining optimal strategic profiles within specific (potentially adversarial) environments. Our goal is not primarily to model individual-level preference structures exhaustively, but rather to identify profile distributions that maximize expected outcomes under realistic competitive scenarios, treating the observed conjoint responses as indicative of choice probabilities within the defined experimental context. We also hope to contribute to the causal inference literature on policy learning. Over the past two decades, an enormous variety and massive amount of granular data have become available, leading to an explosion of new statistical methods for policy learning with both experimental and observational data (see e.g., Athey and Wager, 2021; Ben-Michael et al., 2021; Dudik et al., 2011; Imai and Strauss, 2011; Kallus and Zhou, 2021; Kitagawa and Tetenov, 2018; Zhang et al., 2022; Zhao et al., 2012, and many others). These methods produce individualized treatment rules that help decision-makers effectively assign different individuals to different treatment conditions depending on observed characteristics and other factors. Much of the existing research, however, focuses on policy learning with binary treatment or small number of treatments. In contrast, we consider the problem of optimal high-dimensional treatment selection in randomized experiments, which has been rarely studied in the conjoint literature (see e.g., Buckman et al., 2020, for related approaches in the reinforcement learning literature). The exceptions include the use of multiple treatments to adjust for unobserved confounding (Wang and Blei, 2019), and the use of stochastic intervention for causal inference with single binary treatment (Muñoz and Van Der Laan, 2012) and spatial-temporal treatments (Papadogeorgou et al., 2022). However, these existing works do not consider the problem of policy learning, which is the focus of this paper. Finally, there exists connection between our work and the literature on adversarial machine learning. This line of research is especially prominent in the context of generative modeling (Goodfellow et al., 2020), where investigators train one model to generate realistic data examples that are difficult for second model to distinguish from real examples. Such adversarial optimization is often used to generate realistic, yet completely synthetic, examples of complex data streams including images and texts. We apply similar idea to conjoint analysis where researchers can readily collect large amount of survey responses to measure multidimensional preferences and the randomization of profile features ensures the assumption of unconfoundedness."
        },
        {
            "title": "1 A Conjoint Analysis of Candidate Choice\nTo fix ideas, we consider a conjoint analysis of candidate choice for the United States President,\noriginally conducted by Ono and Burden (2019). The survey was fielded in March of 2016, with\nchoice of conjoint features influenced by the context of the 2016 US presidential elections. In this\nexperiment, the outcome is a binary indicator stating whether one hypothetical candidate or the\nother was selected by a respondent. Candidate features were randomized with uniform probability\nand included age, sex, family context, race, political experience, personality, political party, policy\nexpertise, policy positions (abortion, immigration, security, spending policy), and a measure of public\nfavorability.",
            "content": "The authors investigated an array of the AMCEs, with particular focus on gender. The AMCEs can be computed non-parametrically by taking the difference between the fraction of female versus male candidates selected, averaging over all other (uniformly allocated) features of the candidate and its opponent. Figure 1 summarizes the factor-level AMCEs reported in the original paper where the authors concluded that female candidates are disadvantaged but the effect magnitude is relatively small. However, like most work on conjoint experiments, the original analysis suffers from the problem that candidate attributes in real elections will not be randomly selected (de la Cuesta et al., 2019). For example, female and male politicians tend to have different distributions over policy emphasis (Kahn, 1993), issue framing (Johnstonbaugh, 2020), and age upon entering political office (Carroll and Sanbonmatsu, 2013). Furthermore, candidates themselves are not randomly selected in practice. Political parties and potential candidates are making decisions, at least in part, based on their strategic consideration, taking into account adversarial dynamics within an institutional constraint. Therefore, candidate who might perform well against randomly selected opponent might not do so when facing another candidate chosen strategically by another party. In addition, electoral institutions may play role; in primaries, candidates of the same party compete against one another while in general elections, Democratic and Republican candidates face each other. These considerations need to be incorporated into conjoint analysis."
        },
        {
            "title": "2.1 Notation and Assumptions",
            "content": "Suppose that we have simple random sample of respondents from population. We consider conjoint design of candidate choice with total of factorial features per candidate. Each factorial feature {1, 2, . . . , D} has Ld 2 levels. The random variable representing an entire candidate profile presented to respondent in the design is labeled Ti. The support of Ti, denoted , is the space of all possible treatment assignments and will vary based on the experimental design. For 3 Figure 1: An AMCE analysis using presidential candidates from the conjoint data of Ono and Burden (2019). example, if each feature has levels, i.e., L1 = = LD = L, we have = {1, 2, ..., L}D, where is specific realization of Ti. Usually, each respondent will face choice between two candidate profiles, Ta . The observed outcome will be an indicator of whether candidate is chosen over b, which occurs when the latent utility of a, represented as Yi(Ta ). This choice variable can be quantified as C(Ta ), is higher than that of b, which is represented as Yi(Tb and Tb , Tb ) = I{Yi(Ta )} {0, 1}. ) > Yi(Tb It is commonly the case that each treatment combination is equally likely to be realized, in which case Pr (Ti = t) = 1 for all treatment combinations, t. When factor levels have possibly different 4 assignment probabilities, some treatment combinations will be more likely than others. Usually, each factor is assigned using draws from independent Categorical distributions so that we can write the probability of treatment combination as Pr (Ti = t) = LdY d=1 l=1 I{td=l} dl , where pdl is the Categorical probability for factor taking on level and I{td = l} is the indicator function that is 1 when td takes on value and 0 otherwise. We let define the vector of Categorical probabilities defining the data-generating distribution. For the sake of simplicity, we make the standard assumptions of conjoint analysis. That is, we assume that there is no interference between units and that the treatment assignment is randomized, i.e., {Yi(ta), Yi(tb)}{Ta = tc) > 0 for {a, b} and all tc . We also assume investigators independently randomized each candidate feature. } and Pr (Tc , Tb"
        },
        {
            "title": "2.2 Optimal Selection of Conjoint Profiles in a Non-Adversarial Setting",
            "content": "We consider the optimal selection of conjoint profiles, enabling us to study the types of political candidates who are likely to receive greater support from different types of voters. The standard approach, which is dominant in the policy learning literature, is to identify the following optimal treatment combination, = argmax tT E[Yi(t)], where is the treatment combination that maximizes the average value of some generic outcome, Yi. In the forced-choice conjoint case, this quantity would amount to ta = argmax taT E[C(Yi(ta), Yi(Tb ))], so that investigators find the best candidate profile ta averaging, like in AMCE analysis, over the features of the opposing candidate b. Best would be characterized as the profile maximizing expected vote share. This approach is limited in applicability for two reasons. First, the high dimensionality of treatment in conjoint analysis means that it is impossible to identify using the data from typical conjoint analysis because the total number of possible treatment combinations far exceeds the sample size. In the candidate selection example introduced in Section 1, the total number of possible candidate profiles is above 4.5 million while the number of responses is under 10,000. Second, if there exist more than one candidate profile that performs equally well, it would be substantively more informative to identify multiple such profiles rather than finding single, optimal profile. To address this identification challenge, we propose finding an optimal stochastic intervention. Specifically, we consider parametric distribution of profiles Prπ() that maximizes the average outcome. By considering parametric model, we are able to effectively summarize set of profiles that perform well. Formally, we in general seek the following optimal stochastic intervention, Q(π) = max π Q(π) where Q(π) = tT E[Yi(t) Ti = t] Prπ (Ti = t) , (1) where π parameterizes the distribution of profiles. In the forced-choice conjoint case, this quantity 5 could be written as: Q(πa ) = max πa ta,tbT (cid:20) C(Yi(Ta ), Yi(Tb )) Ta , Tb (cid:21) Prπa(Ta = ta)Prp(Tb = tb). The interpretation here is that πa characterizes the highest possible vote share for given counterfactual strategy of assigning the candidate characteristics of a, while features of are assigned according to static averaging distribution (such as the data-generating uniform distribution). As mentioned, we may maintain the interpretability of πa by letting Prπa (Ta = ta) take on the same parametric form as the data-generating probability function, Pr (Ti = t), also written as Prp (Ti = t). We can interpret Prπ () as defining new distribution over treatments which is the same in structure as the original distribution, but for changes made to the original assignment probability vector, p. The new Categorical probabilities represent the parameter set and are denoted by πa. The optimal allocation, πa, is the quantity that generates the highest average outcome, where that average is taken not only in the randomness in the outcome but also over treatment In essence, the optimal assignment variation in the counterfactual treatment distribution, Prπa. stochastic intervention allows investigators to explore broad peaks over the candidate utility surface, with that broadness stemming from the fact that we are interested in stochastic, rather than deterministic, policy. Note that this formulation includes the aforementioned search for an optimal treatment combination as special case, i.e., Prπ (Ti = t) = (0 1 if = if = . (2) As discussed above, however, finding an optimal treatment combination is not possible in practical settings. Thus, we modify the formulation given in Equation (1) in two ways. First, as noted, we restrict the class of parametric models to the one that modifies the original probability assignment vector used in conjoint experiments. In particular, we only consider the stochastic intervention where factors are distributed independently of one another in order to maintain interpretability. Second, we add regularization term to the objective in Equation (1). While regularization using L1, L2, or KL measures of divergence are all possible, for the moment, we consider use of L2 normalization: max π Q(πa) λn πa p2 2, where λn > 0 is tuning parameter. This formulation can be interpreted as constraining the profile distribution to lie within an L2norm ball centered at p, specifically π p2 ϵn for some ϵn > 0. It is motivated by the fact that the average outcome under the experimental distribution can be readily estimated using the sample averagebut as the profile distribution under consideration π deviates from p, the variance of the estimated average outcome increases (see A.I.0.8). As we see below, the L2 regularization is also convenient in that it leads to closed-form solution under our modeling assumption (although we later discuss how closed-form solutions are not required for use of the methods here). To begin an examination into regularized optimal stochastic interventions for forced-choice conjoint designs, we consider the following linear model with main and two-way interaction effects (Egami and Imai, 2019) (assuming sum-to-zero constraint on coefficients associated with factor to avoid the need to specify baseline factor category): (cid:16) Pr C(Yi(Ta ), Yi(Tb )) = 1 Ta , Tb (cid:17) 6 = Pr(Yi(Ta ) > Yi(Tb ) Ta , Tb ) = µ + LdX d= l=1 βdl (I{T id = l} I{T id = l}) + LdX Ld d,d:d<d l=1 l=1 γdl,dl (I{T id = l, id = l} I{T id = l, id = l}) + ϵi, where βdl denotes the main effect of factor with level l, γdl,dl denotes the interaction effect of treatment dl and dl, and ϵi denotes random error term centered around 0. One intuition here is that the difference between utilities under candidates and defines the choice between and b, and under linear modeling regime, differences in sum can be re-written as the sum of differences. This model makes calculation of the average outcome under stochastic intervention straightforward if the policies, πa and πb, over candidate and features define Categorical distributions. Then, the Stochastic Intervention Under Forced Choice Conjoint can be written as: Q(πa, πb) = πa,πb (cid:20) Pr(Yi(Ta ) > Yi(Tb ) Ta , Tb ) (cid:21) = µ + LdX d=1 l= βdl (πa dl πb dl) + LdX Ld d,d:d<d l=1 l=1 γdl,dl (πa dlπa dl πb dlπb dl) Motivated by the opponent candidate marginalization in AMCE analysis, we first consider the optimal average-case stochastic intervention where optimizes against uniform distribution over candidate features. In this case, πa, can here be derived in closed form, assuming the features of the opposing candidate, b, are assigned according to fixed distribution such as p. We will call this kind of analysis the Average Case Optimal Stochastic Intervention for Forced-Choice Conjoints in that the behavior of the opponent, b, is defined via static averaging. Proposition 1 With two-way interactions and Ld levels for factor d, the average-case optimal L2 regularized stochastic intervention is, for large enough value of λn, given by where πa = C1B. Br(dl),1 = βdl 4λpdl 2λn pdl l=l,l<Ld Cr(dl),r(dl) = 4λn Cr(dl),r(dl) = 2λn Cr(dl),r(dl) = γdl,dl, where r(dl) denotes an indexing function returning the position associated with its factor and level into the rows of and rows or columns of C. Proof of Proposition 1. See A.I.0.2. Here, the optimal stochastic intervention, Prπa , is deterministic function of the outcome model parameters. The parameters defining the outcome model, β and γ, are not known priori, but can be estimated via generalized linear methods, with uncertainties calculated using asymptotic standard errors. 7 Intuitively, the analysis done here allows researchers to investigate the implications of models for candidate choice fit on the data. Instead of examining marginals via AMCE, they can examine joint effects by looking at the optimal behavior implied under their choice of model. Estimates of the optimal distribution over candidates are generated using uncertain model parameters, but the Delta method allows for the rigorous propagation of uncertainty. More formally, because the quantities, πa defining Prπa are deterministic function of the regression parameters, the variance-covariance matrix of { bQ( bπa), bπa} can be obtained via the Delta method: Var-Cov({ bQ( bπa ), bπa }) = bΣ J, where bΣ is the variance-covariance matrix from the modeling strategy for Yi using regression parameters {β, γ} and is the Jacobian of partial derivatives (e.g., of bQ( bπa) and bπa with respect to the outcome model parameters): = { ˆβ,ˆγ}{ bQ( bπa ), bπa }. Under the standard specification assumptions of the outcome model, (cid:16) { bQ( bπa ), bπa } {Q(πa ), πa (cid:17) } (cid:0)0, Σn J(cid:1) . The approach here thereby gives researchers recipe for finding optimal stochastic interventions given choice of outcome model: in addition to looking at regression coefficients, researchers can explore implications of their model for candidate choice by looking at the best strategy for upweighting candidate features. Uncertainties from the coefficients propagate into uncertainties over the optimal strategy. In sum, closed-form expression for the regularized optimal stochastic intervention can be found in the base case of conjoint analysis where one candidate optimizes against fixed opponent distribution. General Optimal Stochastic Interventions in Non-Adversarial Environments. There are limitations to the approach just described.1 One limitation is that while preserving the sum-to-1 constraint on the probabilities, the analytical solution in Proposition 1 does not guarantee the nonnegativity of ˆπa for small values of λn (where the ϵn box around the data-generating probabilities is wide). Another limitation is that, as soon as we generalize the outcome model to more than 2-way interactions, we have no analytical formula for the optimal. To address these issues for small values of λn, when modeling the outcome with higher-order interaction, or for non-linear outcome models, we can perform the stochastic intervention optimization for bπa using iterative methods instead of an analytical closed form.2 For example, to ensure that the entries in ˆπa lie on the simplex, we can re-parameterize the objective function using αdls, which inhabit an unconstrained space (see A.I.0.7) for details). In particular, the stochastic interventional factor probabilities, π, are now function of a, defined as so: Prπ(a)(Td = l) = exp(αdl) 1+PLd1 l=1 1 1+PLd1 l=1 exp(αdl ) exp(αdl ) if < Ld if = Ld We can optimize this via gradient ascent, which, for almost every starting point, arrives at least at 1We discuss weighting estimators for optimal stochastic interventions in A.I.0.11. 2Note that, with two-way interactions, the optimization problem as stated is quadratic programming problem with linear constraints (e.g., the ϵn and non-negativity constraint), which can be solved efficiently using interior point or simplex methods. We focus on gradient ascent as more general solution for non-quadratic problems. local minimum in polynomial time, assuming the strict saddle property of the function to be optimized (Lee et al., 2016). We update the unconstrained parameters using the gradient information, α{O(a)}, where the full expression is found in A.I.0.4. In particular, we iteratively update the initial state of in gradient ascent update steps: for {1, ..., S}: α(s+1) = α(s) + γ(s)a{O(α(s))} for learning rate schedule {γ(s)}S s=1. Inference can be performed as in the closed-form case using the delta method. With closed-form expression for ˆπa, it is evident how we could write an expression for the derivative of optimal as function of the regression parameters using the closed-form Jacobian. With an iterative computation needed to obtain ˆπa, we can consider the same quantity: although the closed-form derivatives of the iterative solution may be unknown, we can still calculate these values using automatic differentiation and tracing the gradient information through the entire sequence of gradient ascent updates. In other words, the gradient ascent solution for ˆπa given the two-way (or higher-way) interactions is simply computation yielding an output, so the sensitivity of the output to the bβ and ˆγ dependencies can still be exactly evaluated."
        },
        {
            "title": "2.3 Adversarial Dynamics in Forced-Choice Candidate Selection",
            "content": "Thus far, we have considered optimal stochastic interventions under the assumption that one party (or candidate) chooses its profile distribution to maximize expected vote share, while treating the distribution of the opposing candidates profile as fixed. Although this framework is useful in settings without direct strategic interaction (e.g., comparing legal briefs or hiring choices without competitive opponent), it is less suitable when two candidates or parties strategically select their own profiles in direct electoral competition. In many political contexts, both the focal candidate and the opposing candidate are engaged in simultaneous strategic optimization. To capture these adversarial dynamics, we introduce an Adversarial Case Optimal Stochastic Intervention framework that explicitly models two agents, which we label as and B, each attempting to maximize their expected probability of victory in forced-choice setting. Let Yi(Tc ) represent respondent is latent utility for candidate {A, B}, where Tc is the candidates profile randomly drawn from some distribution. The observed forced-choice outcome is: C(TA , TB ) = I{Yi(TA ) > Yi(TB )}. We define candidate profile distributions for and as πA and πB, respectively. Each distribution assigns probabilities to the set of all possible profiles, . We consider zero-sum environment where one candidates gain is the others loss. In this setting, it is natural to characterize the optimal profile distributions through min-max optimization )(cid:3) denote the expected probability that candidate problem. Letting Q(πA, πB) = wins against candidate B, the adversarial objective is: (cid:2)C(TA , TB πA,πB max πA min πB Q(πA, πB). (3) In equilibrium, neither candidate can improve their expected performance by unilaterally changing 3While use of automatic differentiation to obtain the Jacobian matrix of the gradient ascent solution enables inference for general optimal stochastic interventions, it should be noted that, as the number of iterations, S, increases, the computational complexity of obtaining the gradients also increases, motivating use of automatic step-size methods to speed up convergence (Ward et al., 2020). See Algorithm 2 in the Appendix for details about the practical implementation. 9 their distribution. Such pair (πA, πB) constitutes Nash equilibrium for the adversarial environment, mirroring classic results in game theory (Kreps, 1989). In other words, given πB, no deviation from πA improves As performance, and vice versa. Incorporating Institutional Constraints. When both candidates face identical populations and institutional rules, trivial equilibria may arise (e.g., both selecting the same profile distribution). Real-world electoral institutions, however, typically introduce asymmetries. For instance, consider two-stage electoral system, where each candidate first competes in an intra-party primary before facing the opposing partys winner in the general election. Let and denote the sets of respondents affiliated with parties and B, respectively, who participate in their respective primaries. In the second stage, both and face the entire electorate, where party loyalists and potentially crossover or independent voters jointly determine the general election outcome. Under these conditions, candidate As eventual victory in the general election also depends on the candidate emerging from Bs primary. Let πA and πB denote the profile distributions of the alternative candidates in As and Bs primaries, respectively. The general election probability of beating can then be expressed by conditioning on primary outcomes. We assume that primary results are independent across parties once we condition on candidate profiles and voter preferences, i.e., I{Yi(TA ) > Yi(TB )} I{Yi(TB ) > Yi(TB )} {TA , TB , TA , TB , Yi(TA ) > Yi(TA ), A}. similar assumption holds for B. This conditional independence assumption ensures that, after conditioning on candidate attributes and voter preferences, primary outcomes across different parties do not influence each other, thereby allowing the factorization of joint probabilities through primary and general election stages and facilitating the identification and estimation of the model. Using the Law of Total Expectation, the adversarial objective becomes: (4) max πA min πB πA,πB,πA ,πB (cid:20) Pr(Yi(TA ) > Yi(TB ) TA , TB , TA Pr(Yi(TA ) > Yi(TA ) TA , TA , Yi(TA ) > Yi(TA ), A) (5) , A) Pr(i A) + Pr(Yi(TA Pr(Yi(TB ) > Yi(TB ) > Yi(TB ) TA , TB , Yi(TB , TB , TB ) > Yi(TB ), B) (cid:21) . , B) Pr(i B) ) TB Equation (5) better characterizes real-world adversarial environments: each candidates optimal stochastic intervention must consider not only the final-stage competition but also the preceding intra-party selection process. Institutional details (e.g., closed vs. open primaries, varying turnout rates, or the presence of independent voters) affect the structure of this optimization problem. By specifying these institutional parameters, we can obtain equilibrium distributions over candidate profiles that are robust to strategic responses by the opposing party. Interpretation and Theoretical Justification. The adversarial case optimal stochastic intervention framework can be rigorously understood within the conceptual apparatus of two-player, zero-sum games under uncertainty. Consider two strategic agents, and B, representing political candidates or parties. Each selects probability distribution over high-dimensional space of candidate profiles, . We denote these distributions as πA (T ) and πB (T ), where (T ) is the probability simplex over . The choice of stochastic (mixed) rather than deterministic profile is necessitated by the combinatorial explosion in the number of potential profiles and the data-imposed 10 impossibility of identifying unique optimal profile with finite samples. Define the zero-sum interaction via the expected forced-choice probability of defeating B: Q(πA, πB) = πA,πB (cid:2)C(TA , TB )(cid:3), , TB ) = I{Yi(TA where C(TA )}. Each agent aims to maximize its own expected outcome, which translates into trying to maximize and trying to minimize it. This interaction defines two-player zero-sum game with payoff function Q. ) > Yi(TB We focus on the concept of Nash equilibrium in mixed strategies. pair (πA, πB) is Nash equilibrium if: πA arg max πA min πB Q(πA, πB) and πB arg min πB max πA Q(πA, πB). By von Neumanns minimax theorem (Nikaidô et al., 1954), for any finite two-player zero-sum game, max πA min πB Q(πA, πB) = min πB max πA Q(πA, πB), ensuring the existence of such an equilibrium. Although the profile space may be extremely large, the theoretical existence of an equilibrium is guaranteed. Finding or approximating it may be computationally challenging, but this does not undermine the conceptual validity. Because, in practice, Q(πA, πB) is estimated from conjoint data, making this statistical game (Blackwell and Girshick, 1979; Fudenberg and Levine, 1993) in which payoffs are empirically inferred from observed outcomes. As the sample size , converges to the true population-level parameters by standard laws of large numbers and uniform convergence arguments. Hence, the estimated equilibrium strategies converge asymptotically to their population-level counterparts, ensuring the approachs consistency. When we incorporate institutional features such as primaries and general elections, we factor the joint probabilities and condition on events like primary outcomes (Equation (5)). Such structural constraints yield equilibrium solutions that more closely mirror observed institutional realities. Although we do not model dynamic or sequential moves explicitly, the primary-to-general election conditioning can be interpreted analogously to subgame structures, lending the analysis partial temporal dimension. The independence assumptions serve as simplifying conditions ensuring that each partys optimal selection problem can be analyzed through marginal and conditional distributions that reflect the given institutional setup. The equilibrium framework just outlined allows for sensitivity analysis with respect to institutional parameters or electorate characteristics. Small perturbations in these parameters or in the estimated distributions of preferences lead to correspondingly small changes in equilibrium strategies, form of stability that follows from continuity properties of equilibria in finite games (Hofbauer and Sorger, 2002). Thus, the equilibrium solution is robust to minor estimation errors and institutional variations, enhancing its empirical relevance. Connection to Broader Equilibrium Concepts: The methodology outlined can be extended or refined to incorporate stronger equilibrium concepts if desired. While standard Nash equilibria suffice for this initial exposition, one could explore refinements such as trembling-hand perfection or even consider evolutionary stability criteria. The rich array of game-theoretic solution concepts is available if the institutional or modeling context warrants higher-order stability or robustness conditions. In sum, the adversarial case optimal stochastic intervention framework is underpinned by fundamental game-theoretic principles. The existence and characterization of equilibria, their robustness 11 to estimation error, and their interpretability in the presence of complex institutional features all stem from well-established theorems and concepts. This alignment with foundational results in game theory and statistical decision theory confers both theoretical rigor and empirical relevance, yielding stable and realistic way to model candidate selection in adversarial electoral environments. Strategies under Counterfactual Institutions. Adversarial case optimal stochastic interventions generate two distributions over candidate profiles, but these distributions will depend on the specification of institutions governing the competition between and b. This fact implies that investigators can examine optima under range of counterfactual or hypothetical institutions. For example, we can explore how equilibrium candidate selection would unfold differently in given context if independent voters were or were not allowed to vote in the primaries (a question of some scholarly debate in the literature examining open and closed primaries (Norrander and Wendland, 2016)). We can specify optimal strategies under given institutional arrangement as {πb(ℶ), πa(ℶ)} = arg max πa arg min πb (cid:20) Eℶ πa,πb Pr(Yi(Ta ) > Yi(Tb ) Ta (cid:21) , Tb ) . (6) Here, ℶ defines how the marginalization over multiple stages of competition is done according to rules specified by the institutional design. In the case of the US presidential elections, we have outlined how this ℶ is quantified in practice through two stages of competition. Future work could consider preferences among multiple selectoral levels (i.e., voters and elites) and more realistic kinds of competition to further clarify the importance of institutional arrangements for structuring the adversarial environment. Quantifying Candidate-level Strategic Divergence. Unlike AMCE analysis, in which it is not easy to quantify observed candidate information through experimental findings, the methodology described here enables the measurement of strategic divergence using actual candidate profiles and the elicited conjoint preferences. In particular, given the optimal candidate distribution for one party, a, and another, b, in given institutional context, we can find the strategic divergence factor, D, of given candidate profile, Ti, using the estimated strategies: D(Ti) = (cid:12) (cid:12) (cid:12) (cid:12) log (cid:18) Prπa(Ti) Prπb(Ti) (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) log {Prπa(Ti)} log {Prπb(Ti)} (cid:12) (cid:12). (7) When D(Ti) is 0, the candidate profile Ti would be equally likely under the strategic action of group and b; when D(Ti) is large, this is an indication that given profile would be likely under the strategy of one group, but unlikely under the strategy of another.4 In this sense, the strategic analysis performed here not only can inform investigators about optimal strategies, but also about the relative likelihood of seeing observed candidates under competing stratagems. Incorporating Heterogeneity with Covariate-Sensitive Strategies We first note that the optimal stochastic intervention framework discussed here can accommodate respondent covariates, as is possible in the sequential decision-making context (Lu et al., 2010). In our discussion up to now, the new treatment probabilities were assigned without considering the specific characteristics of each respondent. We could consider stochastic interventions that took into account covariate information in the targeting of the high-dimensional treatments: Q(π) = max π Q(π) = max π EX hEY [Yi(t) Xi = x] Prπ (Ti = Xi = x) 4Confidence intervals for D(Ti) can be obtained using the Delta method in order to perform statistical inference. Note that we here explore simultaneous action of and b. 12 = max π ( x tT E[Yi(t) Xi = x] Prπ (Ti = Xi = x) Pr(Xi = x) . ) The covariate-sensitive distribution, Prπ(Ti Xi), can be operationalized by having different factorlevel probabilities for each cluster, with model predicting the cluster probabilities for each unit. If we let πdlk denote the probability of factor d, level l, for cluster {1, ..., K}, then Prπ(Ti Xi) = = k=1 k=1 Prπk (Ti Zik = 1) Pr(Zik = 1Xi) ( ) Prπk (TidZik = 1) d=1 {z Categorical probabilities for cluster } Pr(Zik = 1Xi = x) } {z Softmax regression (8) Here, estimation can be done using one-step weighting estimator5 or using outcome models based on the clustering of main or interactive effects (Goplerud et al., 2022). The covariate-sensitive strategies would be particularly relevant in practice if the optimizing agents, whether parties or politicians, can pursue different actions in different contexts (similar to price discrimination in the economic context, where prices are modified depending on the buyers characteristics). covariate-sensitive adversarial strategy may be practically relevant if politicians can construct their public engagements so as to separate out interactions with each respective group. With adversarial dynamics, the covariate-sensitive model becomes more involved: if voters have limited attention and candidates can choose which policy positions or characteristics to communicate to different audiences, opponents may have an incentive to publicize their bad characteristics or positions for given audience. Therefore, the covariate-sensitive case may be most relevant for situations where researchers just want to use the average-case optimal to characterize the response of different respondents to candidates."
        },
        {
            "title": "3.1 Simulation Design in the Average Case",
            "content": "To probe finite-sample dynamics of the proposed optimal stochastic intervention methodologies for conjoint analysis, we employ Monte Carlo methods.6 In our simulations, we analyze synthetic factorial experiments with binary treatments where each treatment is drawn from an independent Bernoulli with probability parameter 0.5. We adopt linear outcome model with interactions similar to the model outlined in Equation 15 (the difference being we here do not adopt the sum-to-zero 5In particular, weighting estimator is ˆQ(π) = 1 Yi i=1 d=1 Prπ(Ti Xi) Pr(Ti) . 6We show results using one-step estimator, whose results are shown in A.II.0.16 and were generally less stable than the two-step approach outlined here. 13 coefficient constraint, and instead use baseline category): Yi(Ti) = β0 + Ld1 d= l=1 β dl I{Tid = l} + Ld 1 Ld 1 d,d:d<d l=1 l=1 γd,d I{Tid = l}I{Tid = l} + ϵi, d,d:d<d γd,dπdπd). The coefficients are drawn i.i.d. with ϵi (0, 0.1), since this makes the computation of Q(θ) straightforward (in particular, Q(π) = βπ + from (0, 1), and the interaction coefficients are scaled so that the R2 in using the main effects only to predict the outcome is 0.70 (ensuring some effective non-linearity). We obtain the true value of π fixing λ and solving for π using the Equation outlined in Proposition 3. To analyze finite sample convergence of ˆπ, we vary the number of observations, {500, 1500, 3500, 10000}. To analyze performance in the high-dimensional setting, where the number of treatment combinations is greater than the number of observations, we vary the number of factors, {5, 10, 20}. We fix λ so that the regularized optimal stochastic interventions have no factor probabilities greater than 0.9, while having degree of divergence from the (uniform) data-generating probabilities."
        },
        {
            "title": "3.2 Simulation Results in the Average Cas\nFirst, we examine the degree to which bπ∗, the optimal stochastic intervention factor probabilities,\nand bQ( bπ∗), the average outcome under the regularized stochastic intervention, converge to the true\nvalues as the sample size grows. We see in the left panel of Figure 2 that, with a small number\nof factors (5), the bias of bπ∗ is insignificant even with a small sample size (500). The variance of\nestimation contributes more prominently to the overall RMSE for all numbers of covariates; the\nvariance decreases rapidly with the sample size. We see a similar pattern for bQ( bπ∗) in right panel of\nFigure 2, where the bias is nominal with a small number of factors and the variance contributes more\nprominently to the overall RMSE, which still decreases with the sample size. Results are consistent\nwith the idea that the optimal stochastic interventions are more difficult to estimate if there are\nmore candidate features involved.",
            "content": "14 Figure 2: Left. Estimation bias and RMSE of π. Each line represents one entry in π. The bold line and closed circles represent the average value. Right. The estimation bias and RMSE of Q(π). We next consider estimated uncertainties compared against true sampling uncertainties. We see in Figure A.II.1 (Supplementary Information) that the asymptotic variance of bQ( ˆπ) is somewhat underestimated for small sample sizes. Figure A.II.2 in A.II.0.16 reports the true sampling variability of ˆπ against the average standard error estimate from asymptotic inference; estimates are neither systematically too wide nor too narrow. Finally, we examine coverage, which combines information about point with variance estimates. We see in Figure 3 coverage close to the target coverage rate across the number of factors and observations for the components of π and (in Figure 4) for bQ( bπ) itself. 15 Figure 3: Finite-sample coverage of π. Each line represents one entry in π. The bold line and closed circles represent the average value. Figure 4: Finite-sample coverage for Q(π)."
        },
        {
            "title": "3.3 Simulation Design in the Adversarial Case",
            "content": "In order to investigate finite-sample performance under the explicitly adversarial setting, we simulate strategic behavior between two hypothetical political parties denoted by (Republican) and (Democrat). We design two-stage electoral process in which each party first selects nominee via primary election, and then those nominees compete in general election. Voters differ by party affiliation, which determines whether they participate in the corresponding primary. Let pR denote the fraction of Republican voters in the electorate, so that pD = 1 pR is the fraction of Democratic voters. For each simulation run, we fix pR { 0.2, 0.3, 0.5, 0.65, 0.8} along grid, and we vary the sample size nobs {1000, 5000, 10000}. Each grid cell is replicated for number of Monte Carlo iterations (e.g., 500), thus yielding multiple realizations. Within each simulated dataset, we generate responses for primary and general-election stages. In the first stage, only voters from party or party participate in their own partys primary. We assign two potential candidate profiles for party and two for party D. Let these be TR,1 , TR,2 for Republicans and TD,1 for Democrats. We specify probabilities with which each candidate profile is chosen by each respondent in that primary, leveraging logistic models to capture how , TD,2 voters respond to candidate featureshere, simply gender for simplicity and computational ease when computing ground truth equilibria via grid search. To generate ground truth equilibria for comparison with our estimation strategy, we adopt the vs. TR,2 ) wins , and who wins , both nominees advance to the second stage of competition. There, and following framework. We first record the probability that each candidate (e.g., TR,1 her or his partys primary. After we sample who wins the Republican primary, TR, the Democratic primary, TD, all voters, Republican and Democratic, face forced choice in the general election between TR, TD, . i At the general-election stage, each voters binary choice is modeled so that voter is probability of selecting the Republican nominee is determined by (a) that voters party, (b) the features (e.g., gender) of the candidate from their party and (c) the features (e.g., gender) of the candidate from the opposing party. We typically assume this model includes parameters for main effects of both candidate features and their interactions. These probabilities are realized as Bernoulli draws in the simulation so that each simulated voter either chooses the Republican or Democratic nominee. This procedure produces binary outcome for each pair of candidate profiles in each iteration, capturing both primary and general stages. Having outlined the data-generating process, we now discuss how we compute the ground-truth strategies approximating Nash equilibrium in the space of possible profile distributions for each party. Each partys πR or πD describes mixed strategy over candidate characteristics. For each party {R, D}, we define Q(cid:0)πR, πD(cid:1) = TR πR, TD πD Pr(cid:0)Yi(TR ) > Yi(TD )(cid:1)i , and TD where TR represent each partys resulting nominee profile after first-stage primaries. To find Nash equilibrium, we compute each partys best response to the other via discrete grid search. In practice, this means we scan over πR and πD, computing max πR min πD Q(cid:0)πR, πD(cid:1) = min πD max πR Q(cid:0)πR, πD(cid:1), checking which (πR, πD) satisfies the approximate equilibrium condition; we label these as the equilibrium strategies. For each Monte Carlo run, we save the estimated equilibrium distribution for πR and πD, along with the realized general-election outcomes under those strategies. By aggregating results across the grid of {pR, nobs} and across replications, we examine trends in how party composition pR and sample size nobs affect equilibrium strategies, estimated vote shares, and convergence. This design allows us to systematically evaluate the proposed adversarial methodologies and to compare the stability of equilibria, the resulting candidate attribute choices, and the consistency of final outcomes under diverse population compositions and sample sizes. In order to evaluate the finite-sample performance of the proposed algorithm for the adversarial setting, we implemented the two-stage design described in the preceding section while varying the proportion of Republican voters, pR, in the electorate. We then computed the party-specific equilibrium strategies, πR and πD, using grid search. For simplicity, we focus on summarizing estimation accuracy for πR. We record root-mean-squared error (RMSE) and coverage of confidence intervals under repeated sampling, with coverage targeting the nominal rate of 95%."
        },
        {
            "title": "3.4 Simulation Results in the Adversarial Case\nSimulation results indicate that the estimation error in bπR depends substantially upon both the\nsample size and the proportion of Republican voters. When the sample size is small (n = 1000), the",
            "content": "17 RMSE tends to be relatively high, particularly around intermediate values of pR 0.5, where the opposing party is neither extremely large nor small. With larger sample sizes, the overall estimation error declines sharply for all values of pR. Coverage rates fall below the nominal level for = 1000 but approach the nominal 95% level for larger sample sizes. The stronger performance under increasing reflects the fact that voters utilities are more precisely estimated, allowing us to obtain better approximations of the zero-sum equilibrium in two-party adversarial competition. Consistent with intuition about these games, inference is most stable in extreme cases when one party constitutes sizable majority or minority of the electorate. At more balanced compositions, small data fluctuations can produce larger variation in both bQ( bπR, bπD) and bπR. In sum, these simulations highlight the key role of sample size and voter-party composition in estimating equilibrium strategies under adversarial conditions. Although moderate sample sizes (n = 5000) are generally adequate to yield small bias and near-nominal coverage, more extreme compositions of the electorate require particularly careful treatment due to pronounced estimation variance in the high-dimensional space of possible profiles. Figure 5: Finite-sample performance of bπR in the adversarial simulation. The top panel shows the root-mean-squared error (RMSE) of bπR for different sample sizes and proportions of Republican voters, pR. The bottom panel illustrates the coverage probability of 95% confidence intervals for components of bπR. As increases, RMSE decreases and coverage approaches the nominal level, with higher variance typically observed at balanced party compositions. Overall, these simulations illustrate how the two-step estimators main source of estimation error is in variance due to the finite sample estimation (not in finite-sample bias). The standard error estimates are also of good quality in this case, leading to confidence intervals that recover their true values at the target coverage rate. Next, we turn our attention to applying some of these approaches in real data where there is practical interest in understanding optimal strategic dynamics."
        },
        {
            "title": "4.1 Data Description",
            "content": "As outlined in 1, there is interest in understanding the tradeoffs involved with policy positioning and optimal candidate selection. In this section, we apply some of the methods described to the data on preferences for president from Ono and Burden (2019). We will analyze the forced-choice aspect of this data. The outcome will therefore be binary indicator stating whether candidate or was selected by respondent i. In the latent utility formulation of 2.3, this can be characterized as C(Yi(Ta ))an indicator indicating whether profile Ta ; see 1 for list of randomized candidate factors. yields higher utility for the respondent than profile Tb ), Yi(Tb"
        },
        {
            "title": "4.2 Comparing Average and Adversarial Case Results",
            "content": "We first examine results from an average case analysis as discussed in 2.3, where we optimize the expected vote share for while averaging over the features of under the data-generating distribution over Tb (which here places equal probability on all treatment combinations). We perform this average case analysis several times with different subpopulations (i.e., all voters, Democrat partisans, Republican partisans, and independent voters). Outcome models for the forced-choice are constructed using generalized linear model with interactions (with an initial calibration step regularizing some coefficients to 0 via penalized methods (Bien et al., 2013)). The optimal stochastic interventions for each set of voters are shown in Figure 6. We see that the Pr bπ for the different sets of voters diverge on some issues, such as immigration, abortion, and policy expertise, but converge on other issues (such as spending). Among all voters, male candidates receive priority in the estimated optimal, whereas female candidates receive priority among Republicans in this non-adversarial analysis. 19 Figure 6: Optimal strategies in the average case setting. Black, blue, red, and green denote the average case optimal among all, Democrat, Republican, and Independent respondents in the sample. We next examine results from the adversarial case analysis, where we optimize the expected vote share for against an also-optimizing under closed primary system of competition among Republican and Democrat partisans. We see in Figure 7 that estimated optimal strategies differ markedly from those in the average case (with those differences quantified in Figure 8). For example, whereas Black female candidates receive priority among Republican partisans in the average case analysis, White male candidates receive priority among such partisans in the adversarial case, something that is more consistent with the real candidate data we will analyze in 4.4. We also see in Figure A.III.1 that the optimized average outcome in the average case is markedly above that in the adversarial case (0.7 compared to 0.5), quantities we will compare against observed equilibrium outcomes from historical elections in 4.4. Figure 7: Optimal strategies in an adversarial setting. Blue and red denote the equilibrium strategy for the agent facing Democratic and Republican voters in the primary stage, respectively. 21 Figure 8: Comparing optimal strategies in the non-adversarial vs. adversarial setting. Blue and red denote the equilibrium strategy for the agent facing Democratic and Republican voters in the primary stage, respectively."
        },
        {
            "title": "4.3 Results with a Covariate-Sensitive Optimal Stochastic Intervention",
            "content": "We have analyzed the regularized optimal stochastic interventions in two cases where adversarial dynamics are or are not included. In both, the optimal stochastic intervention was considered independently of respondent characteristics (other than party affiliation). In practice, different kinds of people may prefer different kinds of candidate profiles. Here, we analyze the covariate-sensitive optimal stochastic intervention in order to learn about the distinct ways in which different kinds of individuals may respond to the high-dimensional candidate features. Using the clustered-based outcome model developed in Goplerud et al. (2022), we find in Figure 9 that the covariate-sensitive strategies seem to successfully recover the general DemocratIndependent-Republican clustering present within the preference structure even though no such information was introduced into the model priori. This illustrates that the covariate-sensitive optimality analysis could be helpful in cases where adversarial dynamics are less relevant compared 22 to the inferred recovery of subgroups within the population that may respond to distinct treatment strategies. Figure 9: Optimal strategies in the covariate sensitive case, where different strategy for allocating candidate features can be used for different types of voters."
        },
        {
            "title": "4.4 Comparison with Observed Candidate Selection",
            "content": "In contrast to the AMCE, one strength of the methodologies proposed here is that they output probability distributionsdistributions against which observed data can be evaluated. We here compute the probability of observing the candidate features from all major primary contenders in the 2016 presidential elections. This task involves mapping candidate features from this election onto the conjoint levels of Ono and Burden (2019). Details are left to A.III.0.18; in general, we 23 map each observed feature onto the closest level for given factor for given real candidate. Where candidates policy position is uncertain (or purposefully ambiguous), we average results over the set of plausible factor levels. We first see in Figure 10 that the optimized estimated vote share in the average case (where the analysis is performed among all voters) is far outside the range of average two-party vote share outcomes observed in the US since 1976. This fact suggests that, as the average case method imposes uniform distribution over opposing candidate features, resulting conclusions cannot be seen as informative regarding real-world strategic behavior. In contrast, the optimized estimated vote share in the adversarial case is quite close to the observed two-party vote share results from the 2016 election; the confidence interval from the adversarial estimate encompasses all observed electoral outcomes from 1976 to the present. Figure 10: Comparing the average case and adversarial case results with real historical data. The adversarial case expected optimal outcomes are well within the range of historical experience; the average case outcomes are not. The experimental data from Ono and Burden (2019) were obtained in March of 2016. Next, we compute the log probability of each 2016 primary contender under the estimated optimal stochastic interventions performed above. In Table 1, we see that log probabilities of the observed candidate features under the adversarial case strategies are significantly higher than in the average case. We see in Figure 11 that some candidates polarize the public much more than others. For example, given their stated policy positions and personal features, Jim Webb, Marco Rubio, and Chris Christie have more consistent log probabilities across Democrat and Republican optimal configurations compared to Bernie Sanders, Donald Trump, and Ted Cruz. The strategic divergence factor described in Eq. 7 is computed for all 2016 candidates in Figure A.III.6. We see that, in general, Democratic candidates have higher divergence factor compared to Republican candidates, indicating that Republican candidates are perhaps positioned more strategically with respect to the 24 preference structure of respondents and the institutional constraints faced by candidates. Figure 11: Primary analysis. In all plots, the axis displays the evaluated log probability under given policy. Figure sorted from left to right arranged by how strongly their log probabilities trend across partisan ordering. 25 Table 1: Mean log probability for Democrat and Republican candidates under the average and aversarial case strategies."
        },
        {
            "title": "Quantity",
            "content": "Mean Log Prob. (s.e.)"
        },
        {
            "title": "Average case\nAdversarial case\nLog likelihood ratio\nAverage case\nAdversarial case\nLog likelihood ratio",
            "content": "-17.23 (0.70) -16.77 (0.71) 0.47 -15.87 (0.35) -15.77 (0.37) 0.53 Overall, this analysis illustrates how the approaches developed in this paper can allow researchers to compare observed outcomes against inferred strategies in order to provide insight into the relative electoral strengths and weaknesses of candidates, given the preferences of an electorate embodied in conjoint data and given institutional design structuring electoral competition."
        },
        {
            "title": "5 Discussion and Future Work\nThe quantities of interest and estimation strategies probed in this paper have strengths and limi-\ntations relative to existing approaches, with those differences being summarized in Table 2. First,\nwhile AMCE estimands can be readily estimated non-parametrically, the average and adversarial\ncase optimal stochastic interventions proposed here incorporate information across all factors and\ncan also incorporate strategic dynamics, but also require the use of a two-step estimator for reliable\nresults. This estimator requires assumptions. In addition, although the two-step estimator can be,\nin principle, generalized to more complex outcome models, inference may become more difficult. For\nexample, when fitting outcome models with a neural network, variance-covariance matrices may be\nmore difficult to obtain, making statistical inference more challenging.",
            "content": "Second, we have assumed SUTVA and analyzed strategic dynamics using experimental data under the assumption of fixed voter preferences. In political campaigns, an important part of strategic processes rests in the formation of preferences (e.g., through rhetoric, political marketing, and other attempts at persuasion (Murphy and Shleifer, 2004)). Therefore, the uncertainty estimates obtained regarding equilibrium candidate selection likely underestimate the true extent of uncertainty regarding outcomes in real-world strategic environments. This limitation motivates future work regarding the role of influence in optimal candidate selection in adversarial environments. Finally, although the analysis of variance-constrained optimal stochastic interventions is general, the strategic behavior inferred from the approach described here depends on the specification of an institutional design. This design is sometimes straightforward to quantify but may not be in all contexts. There is thus the possibility that future work could compare observed outcomes against inferred equilibrium behavior under hypothetical institutional arrangements where the true arrangement is unknown or subject to competing social scientific hypotheses."
        },
        {
            "title": "6 Conclusion\nIn this paper, we have introduced the idea of the optimal stochastic intervention in analyzing conjoint\nexperiments involving candidate choice. We derived a closed-form estimator for the optimal under\nsome conditions and devised an estimator that can be reliably used in a more general case where\nadversarial dynamics either are or are not present. The proposed approaches incorporate relevant",
            "content": "26 Table 2: Comparing different approaches to conjoint analysis. Pr here refers to the data-generating probability distribution over candidate features; Prπ refers to the distribution defining an optimal stochastic intervention. SI denotes stochastic intervention; GLM denotes generalized linear model. Average Marginal Component Effect (AMCE) Average Marginal Interaction Effect (AMIE)"
        },
        {
            "title": "Adversarial\nCase Optimal\nStochastic\nIntervention",
            "content": ""
        },
        {
            "title": "Yes",
            "content": "2+"
        },
        {
            "title": "All",
            "content": "No"
        },
        {
            "title": "All",
            "content": "No Respondents; other factors of reference profile via Pr; all factors of opponent profile via Pr Respondents; other factors of reference profile via Pr; all factors of opponent profile via Pr Respondents; factors of reference via Prπ, opponent profile via Pr Respondents; factors of reference profile via Prπa, opponent profile via Prπb No No No"
        },
        {
            "title": "Yes",
            "content": "Strength of regularization in outcome model (rarely used) GLM variancecovariance; bootstrap"
        },
        {
            "title": "Strength of\nregularization\nin outcome\nmodel if used",
            "content": "GLM variancecovariance; bootstrap Strength of regularization in outcome model; SI regularization GLM variancecovariance + Delta method Strength of regularization in outcome model; SI regularization GLM variancecovariance + Delta method Character Components considered at time Baseline factor category specified? Marginalization over: Informative about strategy in an adversarial setting? Hyper-parameters"
        },
        {
            "title": "Uncertainty\nestimation",
            "content": "Data Requirements Requires forced-choice design? Requires distinct respondent and profile sub-groups? No No No No No No"
        },
        {
            "title": "Yes",
            "content": "uncertainties and allow researchers to estimate the strategies for selecting candidate features that would prove effective in the context where ones opponent is also selecting features to gain votes in the zero-sum context of majoritarian elections. simulation study gives insight into the strengths 27 and limitations of our approach. An application to US presidential elections gives insight into the strengths and limitations of the approach in practice: we find that incorporating adversarial dynamics leads to qualitatively and quantitatively more realistic estimates of optimal strategies in the adversarial setting of elections. Besides many avenues for future workincluding the comparison of strategies under elite and popular preferencesthis paper also shows how causal inference and game-theoretic concepts can further support each other. Often, utilities in game theoretic models are assumed priori. But, with conjoint data, information about these utilities is made palpable; equilibrium behavior is therefore quantifiable in data-driven manner. This opens up the possibility for new synergies between long-standing social science methodologies."
        },
        {
            "title": "References",
            "content": "Abramson, Scott F, Korhan Kocak, et al. (2023): Detecting Preference Cycles in Forced-Choice Conjoint Experiments. In. Abramson, Scott F, Korhan Koçak, et al. (2022): What Do We Learn About Voter Preferences from Conjoint Experiments? In: American Journal of Political Science, no. 4, vol. 66, pp. 10081020. Athey, Susan and Stefan Wager (2021): Policy learning with observational data. In: Econometrica, no. 1, vol. 89, pp. 133161. Audibert, Jean-Yves et al. (2010): Best Arm Identification in Multi-armed Bandits. In: COLT, pp. 4153. Bansak, Kirk, Jens Hainmueller, Daniel Hopkins, and Teppei Yamamoto (2018): The Number of Choice Tasks and Survey Satisficing in Conjoint Experiments. In: Political Analysis, no. 1, vol. 26, pp. 112119. Bansak, Kirk, Jens Hainmueller, Daniel Hopkins, Teppei Yamamoto, et al. (2021): Conjoint Survey Experiments. In: Advances in Experimental Political Science, vol. 19. Ben-Michael, Eli et al. (2021): Safe Policy Learning through Extrapolation: Application to Pre-trial Risk Assessment. Tech. rep. arXiv:2109.11679. Berz, Jan and Michael Jankowski (2022): Local Preferences in Candidate Selection: Evidence from Conjoint Experiment Among Party Leaders in Germany. In: Party Politics, no. 6, vol. 28, pp. 11361149. Bien, Jacob et al. (2013): Lasso for Hierarchical Interactions. In: Annals of Statistics, no. 3, vol. 41, p. 1111. Blackwell, David and Meyer Girshick (1979): Theory of games and statistical decisions. Courier Corporation. Buckman, Jacob et al. (2020): The Importance of Pessimism in Fixed-dataset Policy Optimization. In: arXiv preprint arXiv:2009.06799. Carroll, Susan and Kira Sanbonmatsu (2013): More Women Can Run: Gender and Pathways to the State Legislatures. Oxford University Press. Christensen, Henrik Serup et al. (2021): How voters choose one out of many: conjoint analysis of the effects of endorsements on candidate choice. In: Political research exchange, no. 1, vol. 3, p. 1892456. de la Cuesta, Brandon et al. (2019): Experimental Design and Statistical Inference for Conjoint Analysis: The Essential Role of Population Distribution. Dudik, Miroslav et al. (2011): Doubly Robust Policy Evaluation and Learning. In: Proceedings of the 28th International Conference on Machine Learning. Egami, Naoki and Kosuke Imai (2019): Causal Interaction in Factorial Experiments: Application to Conjoint Analysis. In: Journal of the American Statistical Association, no. 526, vol. 114, pp. 529540. 28 Franchino, Fabio and Francesco Zucchini (2015): Voting in Multi-dimensional Space: Conjoint Analysis Employing Valence and Ideology Attributes of Candidates. In: Political Science Research and Methods, no. 2, vol. 3, pp. 221241. Fudenberg, Drew and David Levine (1993): Steady state learning and Nash equilibrium. In: Econometrica: Journal of the Econometric Society, pp. 547573. Goodfellow, Ian et al. (2020): Generative Adversarial Networks. In: Communications of the ACM, no. 11, vol. 63, pp. 139144. Goplerud, Max et al. (2022): Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis. In: arXiv preprint arXiv:2201.01357. Hainmueller, Jens et al. (2014): Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments. In: Political Analysis, no. 1, vol. 22, pp. 1 30. Ham, Dae Woong et al. (2022): Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis. In: arXiv preprint arXiv:2201.08343. Hofbauer, Josef and Gerhard Sorger (2002): differential game approach to evolutionary equilibrium selection. In: International Game Theory Review, no. 01, vol. 4, pp. 1731. Imai, Kosuke and Aaron Strauss (2011): Estimation of Heterogeneous Treatment Effects from Randomized Experiments, with Application to the Optimal Planning of the Get-out-the-vote Campaign. In: Political Analysis, no. 1, vol. 19, pp. 119. Johnstonbaugh, Morgan (2020): Standing Up for Women? How Party and Gender Influence Politicians Online Discussion of Planned Parenthood. In: Journal of Women, Politics & Policy, no. 4, vol. 41, pp. 477499. Kahn, Kim Fridkin (1993): Gender Differences in Campaign Messages: The Political Advertisements of Men and Women Candidates for US Senate. In: Political Research Quarterly, no. 3, vol. 46, pp. 481502. Kallus, Nathan and Angela Zhou (2021): Minimax-optimal Policy Learning under Unobserved Confounding. In: Management Science, no. 5, vol. 67, pp. 28702890. doi: 10.1287/mnsc.2020. 3699. Kirkland, Patricia and Alexander Coppock (2018): Candidate choice without party labels: New insights from conjoint survey experiments. In: Political Behavior, vol. 40, pp. 571591. Kitagawa, Toru and Aleksey Tetenov (2018): Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice. In: Econometrica, no. 2, vol. 86, pp. 591616. Kreps, David (1989): Nash equilibrium. In: Game Theory, pp. 167177. Lee, Jason et al. (2016): Gradient Descent Only Converges to Minimizers. In: Conference on learning theory. PMLR, pp. 12461257. Liu, Guoer and Yuki Shiraito (2023): Multiple Hypothesis Testing in Conjoint Analysis. In: Political Analysis, pp. 116. Lu, Tyler et al. (2010): Contextual Multi-armed Bandits. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, pp. 485492. Muñoz, Iván Díaz and Mark Van Der Laan (2012): Population Intervention Causal Effects Based on Stochastic Interventions. In: Biometrics, no. 2, vol. 68, pp. 541549. Murphy, Kevin and Andrei Shleifer (2004): Persuasion in Politics. In: American Economic Review, no. 2, vol. 94, pp. 435439. Nikaidô, Hukukane et al. (1954): On von Neumanns minimax theorem. In: Pacific J. Math, no. 1, vol. 4, pp. 6572. Norrander, Barbara and Jay Wendland (2016): Open Versus Closed Primaries and the Ideological Composition of Presidential Primary Electorates. In: Electoral Studies, vol. 42, pp. 229236. Ono, Yoshikuni and Barry Burden (2019): The Contingent Effects of Candidate Sex on Voter Choice. In: Political Behavior, vol. 41, pp. 583607. Papadogeorgou, Georgia et al. (2022): Causal Inference with Spatio-temporal Data: Estimating the Effects of Airstrikes on Insurgent Violence in Iraq. In: Journal of the Royal Statistical Society Series B: Statistical Methodology, no. 5, vol. 84, pp. 19691999. Rehmert, Jochen (2022): Party Elites Preferences in Candidates: Evidence from Conjoint Experiment. In: Political Behavior, no. 3, vol. 44, pp. 11491173. Wang, Yixin and David M. Blei (2019): The Blessings of Multiple Causes. Ward, Rachel et al. (2020): Adagrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes. In: The Journal of Machine Learning Research, no. 1, vol. 21, pp. 90479076. Zhang, Yi et al. (2022): Safe Policy Learning under Regression Discontinuity Designs. In: arXiv preprint arXiv:2208.13323. Zhao, Yingqi et al. (2012): Estimating Individualized Treatment Rules Using Outcome Weighted Learning. In: Journal of the American Statistical Association, no. 499, vol. 107, pp. 11061118. 30 Appendix Appendix I: Theoretical Analysis"
        },
        {
            "title": "Supplementary Material Deriving Optimal Stochastic Interventions\nunder Interaction Models",
            "content": "A.I.0.1 The Optimal Stochastic Intervention in Two-Way Interaction Model"
        },
        {
            "title": "Recall that the objective function to maximize is",
            "content": "O(π) = Q(π) λp π2 = β0 + d=1 βdπd + d<d so that γd,dπdπd λ {(πd pd)2 + ([1 πd] [1 pd])2} d=1 dO dπd = βd + d,d=d = γd,dπ 4λ(πd pd) = 0 d,d=d γd,dπd 4λπd = βd 4λpd where we use pd to denote the vector of Categorical probabilities for all levels in factor d. This sets up system of linear equations with unknowns, which can be represented in matrix form: Cπ = π = C1B, where Bd,1 = βd 4λpd, Cd,d = 4λ and Cd,d = γd,d. A.I.0.2 The Optimal Stochastic Intervention in Two-Way Interaction Model"
        },
        {
            "title": "The outcome model with multiple factor levels is",
            "content": "Yi(t) = β0 + Ld1 βdltdl + Ld 1 Ld 1 d=1 l=1 d,d:d<d l=1 l=1 γdl,dl tdl tdl + ϵi, where tdl denotes the binary indicator for whether level in factor is assigned. By linearity of expectations and independence of factors: Q(π) = β0 + Ld1 d=1 l= βdlπdl + Ld 1 Ld 1 d,d:d<d l=1 l= γdl,dl πdl πdl."
        },
        {
            "title": "The objective is now",
            "content": "O(π) = Q(π) λp π2 31 = β0 + Ld1 d= l=1 βdl πdl + Ld 1 Ld 1 d,d:d<d l= l=1 λ (cid:26) Ld1 (πdl pdl)2 + 1 γdl,dl πdlπdl Ld 1 1 πdl Ld 1 2 (cid:27) pdl d=1 l=1 l=1 l=1 so that, for < Ld: dO dπdl = βdl + d,d=d Ld1 Ld 1 l=1 l= γdl,dl πdl 2λ(πdl pdl) 2λ Ld1 l=1 (πdl pdl) = 0 = Ld1 Ld 1 d,d=d l=1 l=1 γdl,dl πdl 4λπdl 2λ πdl = βdl 4λpdl 2λ pdl l=l,l<Ld l=l,l<Ld with This again sets up system of PD which can be represented in matrix form: d=1(Ld1) linear equations with the same number of unknowns, Cπ = π = C1B. where, letting r() denote function returning the appropriate index into the matrix rows/columns: Br(dl),1 = βdl 4λpdl 2λ pdl l=l,l<Ld Cr(dl),r(dl) = 4λ Cr(dl),r(dl) = 2λ Cr(dl),r(dl) = γdl,dl A.I.0.3 The Optimal Stochastic Intervention in Two-Way Interaction Model"
        },
        {
            "title": "The outcome model with multiple factor levels is",
            "content": "Pr(Yi(Ta ) > Yi(Tb ) Ta , Tb ) = µ + LdX d=1 l=1 βdl (I{T id = l} I{T id = l}) + Ld Ld d,d:d<d l=1 l= γdl,dl (I{T id = l, id = l} I{T id = l, id = l}) + ϵi, where tdl denotes the binary indicator for whether level in factor is assigned. By linearity of expectations and independence of factors: Q(πa, πb) = πa(Ta),πb(Tb) (cid:20) Pr(Yi(Ta ) > Yi(Tb ) Ta , Tb ) (cid:21) = µ + LdX d= l=1 βdl (π dl πb dl) 32 + Ld Ld d,d:d<d l=1 l=1 γdl,dl (π dlπ dl πb dlπb dl) Case 0: Choose πa and πb jointly to maximize the selection probability for Ta . Problem: Not an interpretable solution: Choose best candidate strategy to go against worst possible candidate B. Case 1: Average Case Analysis: Set πb to be p. Interpretation: Best candidate strategy uniformly averaging over all possible candidate Bs. Case 2: Minimax Analysis: Set πa to maximize, πb to minimize objective. Interpretation: Optimally select candidate strategy to compete against optimally selected candidate strategy B."
        },
        {
            "title": "The objective is now",
            "content": "O(πa, πb) = Q(πa, πb) λ (cid:16) πa2 + πb2(cid:17) = µ + LdX d=1 l=1 βdl (π dl πb dl) + Ld Ld d,d:d<d l=1 l=1 γdl,dl (π dlπ dl πb dlπb dl) λ X (cid:26) LdX (π# dl pdl)2 (cid:27) #{,b} d= l=1 Under the Average Case Maximizer: O(πa, p) = µ + LdX d= l=1 βdl (π dl pdl) + d,d:d<d λ Ld Ld γdl,dl (π dlπ dl pdlpdl) l=1 l=1 (cid:26) LdX (π dl pdl)2 (cid:27) so that d= l=1 dO dπ dl = βdl + d,d=d LdX Ld l= l=1 = γdl,dl π dl 2λ(π dl pdl) = 0 LdX Ld d,d=d l=1 l=1 γdl,dl π dl 2λπ dl = βdl 2λpdl"
        },
        {
            "title": "This sets up a system of PD",
            "content": "d=1 Ld linear equations with the same number of unknowns, which can 33 be represented in matrix form: Cπ = π = C1B. where, letting r() denote function returning the correct index into the matrix: Br(dl),1 = βdl 2λpdl Cr(dl),r(dl) = 2λ Cr(dl),r(dl) = 0 Cr(dl),r(dl) = γdl,dl A.I.0.4 Gradients for Obtaining the Variance-Constrained Optimal Stochastic Intervention in the Two-Way Constrained Case The gradients for the simplex-constrained objective function are, < Ld, adl = βdlAdl + l(a)=l βdl(a) exp(adl) exp(adl(a)) {1 + PLd1 l(m)= exp(adl(m))}2 + d,d=d Ld 1 l= γdl,dlAdl exp(adl) {1 + PLd 1 l(m)=1 exp(adl(m))} + d,d=d Ld 1 X l(a)=l l=1 2λAdl γdl(a),dl {1 + PLd1 l(m)=1 exp(adl) {1 + PLd1 l(m)=1 exp(adl(m))} exp(adl) exp(adl(a)) exp(adl) {1 + PLd 1 l(m)=1 exp(adl(m))} exp(adl(m))}2 pdl ( 2λ l(a)=l exp(adl) exp(adl(a)) {1 + PLd1 l(m)=1 exp(adl(m))}2 2λ exp(adl) Ld1 {1 + PLd1 l(m)=1 exp(adl(m))}2 l= exp(adl(a)) {1 + PLd1 l(m)=1 exp(adl(m))} exp(adl) {1 + PLd l(m)=1 exp(adl(m))} ) pdl(a) , pdl where Adl = exp(adl){1 + PLd1 l(m)=1 {1 + PLd1 l(m)=1 exp(adl(m)) exp(adl)} exp(adl(m))} . A.I.0.5 Gradients for Obtaining the Variance-Constrained Optimal Stochastic Intervention in the Two-Way Constrained Case under Fully Parameterized Model under Forced Choice"
        },
        {
            "title": "The objective is",
            "content": "O(πa, πb) = Q(πa, πb) λ (cid:16) πa2 + πb2(cid:17) 34 = µ + LdX d=1 l=1 βdl (π dl πb dl) + Ld Ld d,d:d<d l=1 l= γdl,dl (π dlπ dl πb dlπb dl) λ D (cid:26) LdX (π# dl pdl)2 (cid:27) #{,b} d=1 l=1 35 A.I.0.6 Gradients for Obtaining the Variance-Constrained Optimal Stochastic Intervention in the Two-Way Constrained Case under Fully Parameterized Model With fully parameterized, ANOVA-type model, we have: Q(a) = β0 + LdX d=1 l=1 βdl 1 1 + exp(adl) + Ld Ld d,d:d<d l=1 l= γdl,dl πdlπdl. The gradients for the simplex-constrained objective function are, < Ld, adl = βdlAdl + l(a)=l βdl(a) exp(adl) exp(adl(a)) {1 + PLd1 l(m)=1 exp(adl(m))}2 + d,d=d Ld l=1 γdl,dlAdl exp(adl) {1 + PLd 1 l(m)= exp(adl(m))} γdl(a),dl exp(adl) exp(adl(a)) exp(adl) {1 + PLd1 l(m)=1 exp(adl(m))} {1 + PLd 1 l(m)=1 exp(adl(m))} + d,d=d l(a)=l Ld l=1 2λAdl exp(adl) {1 + PLd1 l(m)=1 exp(adl(m))} pdl ( 2λ l(a)=l exp(adl) exp(adl(a)) {1 + PLd1 l(m)=1 exp(adl(m))}2 2λ exp(adl) Ld1 {1 + PLd1 l(m)=1 exp(adl(m))}2 l=1 exp(adl(a)) {1 + PLd1 l(m)=1 exp(adl(m))} exp(adl) {1 + PLd1 l(m)=1 exp(adl(m))} ) pdl(a) , pdl where Adl = exp(adl){1 + PLd1 l(m)=1 {1 + PLd1 l(m)=1 exp(adl(m)) exp(adl)} exp(adl(m))}2 . A.I.0.7 Objective Function in Unconstrained Space O(a) = Q(a) λnp π2 Ld1 = β0 + exp(adl) βdl {1 + PLd1 l(m)=1 Ld 1 Ld 1 γdl,dl d=1 l=1 + exp(adl(m))} d,d:d<d l=1 l=1 λn ( Ld1 d=1 l=1 exp(adl) exp(adl) {1 + PLd 1 l(m)=1 exp(adl(m))} {1 + PLd 1 l(m)=1 exp(adl(m))} exp(adl) {1 + PLd 1 l(m)= exp(adl(m))} 2 pdl 1 + Ld 1 exp(adl) l=1 {1 + PLd 1 l(m)=1 exp(adl(m))} 1 Ld 1 2 ) pdl l=1 Regularizing Optimal Stochastic Interventions: Variance Bound and the L2 Penalty A.I.0.8 Variance Constraints Prevent Unreliable Optimal Stochastic Interventions One entry into the concept of the variance-constrained optimal stochastic intervention can come by examining the variance of the non-parametric estimator for general stochastic interventions. This estimator for the generic stochastic intervention estimand, Q(π), is defined to be bQ(π) = n1 i= Yi Prπ(Ti) Pr(Ti) = tT bct Prπ (Ti = t) (9) nPr(Ti=t) for each for which It > 0 and 0 otherwise (letting It denote the set of unit iIt Yi where bct := indices with realized treatment combination t). This weighting estimator is unbiased for choice of Prπ() (Muñoz and Van Der Laan, 2012), but, given finite samples, some choices may not be estimable with bounded variance. Indeed, as we have seen, Prπ may be defined so as to target the optimal treatment combination itself, something which in high dimensions cannot be practically estimated since it requires the presence of data within particular treatment combination, t, which lives in space that increases exponentially in the number of dimensions. We quantify the tradeoff between proposed Prπ() and the resulting estimability via nonparametric methods in Proposition 1. This proposition operates under few simplifying assumptions outlined below which we make in order to focus attention on critical aspects of the discussion here. Proposition 2 Assume Constant Variance: Var(Yi(t)) = σ2 for all and t. Assume uniform distribution over treatment combinations: Pr(Ti = t) = 1 for all and t. Assume the data have been scaled so Yi 0. Then, (cid:16)"
        },
        {
            "title": "Var",
            "content": "bQ(π) (cid:17) Var (cid:16) bQ(π) (cid:17) := (cid:16) (cid:17) σ2 + Eπ[c2 t] maxtT Prπ(Ti = t) . (10) Proof of Proposition 1. See A.I.0.7. There are several consequences to this relationship. First, the relation implies that as the true stochastic interventional quantity of interest, Q(π), grows larger, the variance bound also increases. In other words, the variance bound is related to the magnitude of the outcome under Prπ, with higher average outcomes leading to higher variance values. Moreover, the relation indicates that, as the distribution defining the stochastic intervention deviates from the data-generating uniform value, the variance can grow very large but can also be controlled if Prπ(Ti = t) does not deviate significantly from Pr(Ti = t). In other words, there may be exponential growth in with additional conjoint dimensions, but bQ(π) can nevertheless have favorable variance properties provided that π does not generate probabilities too different from the data-generating values. Finally, we note that if the data are bounded, then the variance bound for 37 fixed design can itself be re-written as function of π alone: (cid:16)"
        },
        {
            "title": "Var",
            "content": "(cid:17) bQ(π) max tT Prπ(Ti = t) for some constant c. This relation is important because conjoint data often have such bounded outcomes (e.g., as, often, Yi {0, 1} or is defined over Likhart scale). By bounding the variance, we can investigate optimal stochastic interventions which can, at least in principle, be non-parametrically estimated from the observed data. The variance-constrained optimal stochastic intervention can be written as Q(π) = max π Q(π) such that Var (cid:16) bQ(π) (cid:17) B. or, in the Lagrangian formulation, Q(π) = max π Q(π) λn Var (cid:16) bQ(π) (cid:17)o . (11) (12) The dependency in λn is necessary because the variance bound is function of n; different values of λ induce different variance constraints for different values of n. Finally, to conclude this subsection, we offer few reflections on other kinds of constraints on optimal stochastic interventions. We use the variance constraint because substantive investigators are primarily interested in the first two moments for their quantities of interest (e.g., the mean and variance). The variance constraints are therefore natural choice (as explored in the reinforcement literature (Prashanth, Fu, et al., 2022)). However, in some situations where extreme values may be of importance (such as in the context of experiments in finance where few examples may drive results), higher-order information regarding skewness or kurtosis could be incorporated, with the caveat that larger datasets are required to obtain insight into those higher-order moments (Taleb, 2009). A.I.0.8.1 Operationalizing the Variance Constraint One way to operationalize this variance constraint would be to force the new probabilities, π, to live in an ϵn box around the data-generating probabilities, where the subscript emphasizes the fact that the variance bound is not only function of the maximum probability but the sample size as well. In the binary factor case, this would mean π [0.5 εn, 0.5 + εn]D. In this special case, we have maxtT Prπ (T = t) = (0.5 + ϵn)D and so Var( bQ(π)) (cid:16) σ2 + Eπ[c2 t] (cid:17) (0.5 + ϵn)D . We then note that the maximum deviation penalty can be explicitly bounded using the squared L2 norm (see A.I.0.10 for derivation): (cid:16)"
        },
        {
            "title": "Var",
            "content": "bQ(π) (cid:17) max tT Prπ(Ti = t) (D + π p2 2). This analysis provides justification for the use of an L2 norm instead of maximum probability penalty in the variance-constrained objective, enforcing probabilities in an ϵn box to bound the maximum probability and thus the variance. We then have the Lagrangian formulation of the objective: max{Q(π)} such that π [p ϵn, + ϵn] 38 = O(π) = Q(π) λn π p2 2. There are two reasons to explore use of the L2 penalty in practice. First, the squared L2 norm has the benefit of admitting closed-form expressions for the optimal stochastic intervention under some conditions as described in the main text. The penalty also can be interpreted as placing box constraint around the data-generating probabilities, p: the (marginal) factor-level probabilities in Prπ(), which are defined by π, cannot deviate by some ϵn from the data-generating value. This ϵn also controls the magnitude of the maximum probability over the support, establishing link between the variance bound using the maximum probability and the bound using the L2 norm between the data-generating and new factor-level probabilities. In short, we have described how the optimal stochastic intervention can be understood as search for optimal combinations of treatment assignments, where information from many possible combinations is combined using new interventional distribution. Different distributions have very different properties, which we have probed in simple model; these properties motivate the use of the variance-constrained stochastic intervention. We discuss estimation in the next section. A.I.0.9 Deriving the Variance Bound It is known that the weighting estimator is unbiased. We first see that E(cid:2) bct (cid:3) = (cid:20) (P iIt Yi) I{It > 0} nPr(Ti = t) (cid:21) = E[nt]ct nPr(Ti = t) = nPr(Ti = t)ct nPr(Ti = t) = ct. where nt denotes the number of treated units with treatment combination and is Binomial(n, Pr(Ti = t)) in law. Therefore, E(cid:2) bQ(π)(cid:3) = ctPrπ (Ti = t) = Q(π). tT To derive variance expression, we additionally assume Constant Variance: Constant Variance: Var(Yi(t)) = σ2 for all and t. Using the Law of Total Variance, we see (cid:16)"
        },
        {
            "title": "Var",
            "content": "bQ(π) (cid:17) = h"
        },
        {
            "title": "Var",
            "content": "(cid:16) bQ(π) {Ti}n i=1 (cid:17)i + Var (cid:16)E bQ(π) {Ti}n i=1 i(cid:17)"
        },
        {
            "title": "Var",
            "content": "(cid:16) bQ(π) {Ti}n i=1 (cid:17)i \" = E"
        },
        {
            "title": "Var",
            "content": "(P iIt Yi) I{It > 0}Prπ(Ti = t) nPr(Ti = t) tT !# {Ti}n i=1 = X"
        },
        {
            "title": "Var",
            "content": "(cid:18) Yi I{It > 0}Prπ(Ti = t) nPr(Ti = t) {Ti}n i=1 (cid:19) tT iIt = X tT iIt Var (Yi {Ti}n i=1) I{It > 0}2 Prπ(Ti = t)2 n2Pr(Ti = t)2 = X σ2 I{It > 0} tT iIt Prπ(Ti = t)2 n2Pr(Ti = t)2 39 # tT ntσ2 Prπ(Ti = t)2 n2Pr(Ti = t)2 nPr(Ti = t)σ2 Prπ(Ti = t)2 \" = = tT n2Pr(Ti = t)2 = tT σ2 Prπ(Ti = t)2 nPr(Ti = t) . Under uniform randomization design, Pr(Ti = t) = 1, so"
        },
        {
            "title": "Var",
            "content": "(cid:16) bQ(π) {Ti}n i=1 (cid:17)i = tT = tT σ2 = σ2 Prπ(Ti = t)2 nPr(Ti = t) σ2 Prπ(Ti = t)2 Prπ(Ti = t)2 tT where the last inequality follows by the relationship between the L2 and norm applied to entities on the simplex. σ2T maxtT Prπ(Ti = t) , Now we observe that, (cid:16)E h"
        },
        {
            "title": "Var",
            "content": "bQ(π) {Ti}n i=1 i(cid:17) = Var \" E (P iIt Yi) I{It > 0}Prπ(Ti = t) nPr(Ti = t) #! {Ti}n i= tT ctntPrπ(Ti = t) nPr(Ti = t) ! tT c2 tPrπ(Ti = t)2 n2Pr(Ti = t)2 Var (nt) + t=t = Var = tT ctPrπ(Ti = t) nPr(Ti = t) ctPrπ(Ti = t) nPr(Ti = t) Cov (nt, nt) . If the data are scaled so that Yi 0 (something which can always be achieved by re-scaling if the domain of the outcome is known priori), then ct 0 for all t. With this assumption and because {nt, nt} are jointly Multinomial with negative covariance, we see that (cid:16)E h"
        },
        {
            "title": "Var",
            "content": "bQ(π) {Ti}n i=1 i(cid:17) tT tT tT tT tPrπ(Ti = t)2 c2 n2Pr(Ti = t)2 Var (nt) c2 tPrπ(Ti = t)2 n2Pr(Ti = t)2 nPr(Ti = t)(1 Pr(Ti = t)) tPrπ(Ti = t)2 c2 nPr(Ti = t) tPrπ(Ti = t)2 c2 Pr(Ti = t) (1 Pr(Ti = t)) (1 Pr(Ti = t)) . Under uniform treatment assignment mechanism, Pr(Ti = t) = (1 Pr(Ti = t)) so that (cid:16)E h"
        },
        {
            "title": "Var",
            "content": "bQ(π) {Ti}n i=1 i(cid:17) tT tPrπ(Ti = t)2 c2 Pr(Ti = t) (1 Pr(Ti = t)) 40 c2 tPrπ(Ti = t)2 tT Eπ[c2 t] maxtT Prπ (Ti = t) . Therefore, (cid:16)"
        },
        {
            "title": "Var",
            "content": "(cid:17) bQ(π) = h"
        },
        {
            "title": "Var",
            "content": "(cid:16) bQ(π) {Ti}n i=1 (cid:17)i + Var (cid:16)E bQ(π) {Ti}n i=1 i(cid:17) σ2T maxtT Prπ(Ti = t) (cid:17) (cid:16) σ2 + Eπ[c2 t] = + maxtT Prπ(Ti = t) . Eπ[c2 t] maxtT Prπ (Ti = t) If Yi is bounded at B, then we further see (cid:16)"
        },
        {
            "title": "Var",
            "content": "(cid:17) bQ(π) = Θ (cid:18)h σ2 + T maxtT Prπ(Ti = t) (cid:19) . A.I.0.10 Quantifying the Relationship Between Variance Bound and Euclidean"
        },
        {
            "title": "Norm Penalties",
            "content": "We here establish the relation between the variance bound using the maxtT Prπ (Ti = t) and using π p2 In particular, we see that, if Prπ is of the same family as Pr, then, under factor 2. independence: (cid:16)"
        },
        {
            "title": "Var",
            "content": "bQ(π) (cid:17) (cid:16) (cid:17) σ2 + Eπ[c2 t] (cid:16) (cid:17) σ2 + Eπ[c2 t] = (cid:16) (cid:17) σ2 + Eπ[c2 t] = maxtT Prπ (Ti = t) QD d=1 maxl{0,1,...,Ld1} πd,l T QD d=1 maxl{0,1,...,Ld1}(pd,l + ϵd,l) , with ϵd,l = πd,l pd,l. By the Triangle Inequality: max l{0,1,...,Ld1} (pd,l + ϵd,l) = pd + ϵd pd + ϵd. where we use pd to denote the vector of Categorical probabilities for all levels in factor d. Because ϵd ϵd2 Ldϵd for any ϵd, it follows that (pd,l + ϵd,l) pd + ϵd pd + ϵd2 max l{0,1,...,Ld1} = max l{0,1,...,Ld1} (pd,l + ϵd,l) d=1 max l{0,1,...,Ld1} (pd,l + ϵd,l) d=1 {pd + ϵd2} d=1 {1 + ϵd2} = + ϵ2. d=1 So, (cid:16)"
        },
        {
            "title": "Var",
            "content": "bQ(π) (cid:17) (cid:16) (cid:17) σ2 + Eπ[c2 t] maxtT Prπ (Ti = t) 41 (cid:16) (cid:17) σ2 + Eπ[c2 t] (D + ϵ2) (cid:16) = σ2 + Eπ[c2 t] (cid:17) (D + π p2) Thus, we can formulate another variance bound in terms of the L2 penalty alone (note that the term is fixed as function of π). The value of the L2 penalty can, in general, be higher than the value of the penalty involving the maximum probability over the support, as shown visually in Figure A.I.1, although different choices of λ can make estimation involving the two penalty approaches similar. Figure A.I.1: The L2 norm penalizing deviations from the data-generating probabilities will be numerically larger than the penalty involving the maximum probability over the support. Here, denotes an upper bound for Eπ[c2 t]. A.I.0.11 Conservative Inference using the Variance Bound Second, we also note that, for given estimate of π, conservative inference for bQ( bπ) can be performed using sample splitting and the variance bound from Equation 10. If we split the data into two samples, denoted 1 and 2, we use the first sample to obtain the estimate, ˆπ (1). Then, although there may be significant finite sample bias using sample 1 to estimate ˆQ(1)( ˆπ (1)), estimates using sample 2 will be unbiased. We also have conservative variance estimator: dVar( bQ2( ˆπ (1))) = dVar (cid:16) (cid:17) bQ2( ˆπ 1) 42 where the right-hand side is taken from the upper bound for the weighting estimator variance from Equation 10. There are two terms that need to be estimated in the variance bound, σ2 and Eπ[c2 t]. The latter can be unbiasedly estimated using the weighting estimator from Equation 9 with 2 replacing Yi; the former can be conservatively estimated using the marginal variance estimate. On the One-Step Estimator for Optimal Stochastic Interventions We have seen how the target quantity of interestthe variance-constrained optimal stochastic interventioncan be expressed as π = arg max π Q(π) λn π p2 2 . (13) In practice, Q(π) must be estimated as π is optimized. The one-step estimator for this quantity uses the weighting approach described in Equation 9: bπ = arg max π bQ(π) λn π p2 2 . (14) This objective can be estimated via stochastic gradient ascent (see A.I.0.13 for derived gradients). In order to reduce the variance of estimation at some finite sample bias cost, the self-normalized or Hàjek estimator can be employed for bQ(π). This one-step estimator allows researchers to learn about the optimal variance-constrained stochastic intervention without assumptions about the functional form of the outcome simply by upor down-weighting the relevance of specific observed data points as the target objective is optimized. In addition, sample splitting (and cross-validation within the first sample fold) can be used to obtain an estimate for reasonable value of λ, given fixed sample size. See Algorithm 1 in the Appendix for an outline of this procedure. Asymptotic inference can also be performed using partial M-estimation theory (Stefanski and Boos, 2002), with details outlined in A.I.0.12. Despite the benefit of being non-parametric, we found via simulation in A.II.0.16 that resulting standard errors from this approach found to be so large as to be uninformative, due to instabilities in the M-estimation variance-covariance estimates (Simpson et al., 1987). For these reasons, we focus attention on two-step estimator the breaks out modeling of the outcome and inference into the optimal stochastic intervention and therefore gains in stability what it loses by imposing functional form on the outcome. A.I.0.12 Asymptotic Inference with the One-Step Estimator Using Partial Mestimation Inference for the one-step approach can be examined using the theory of M-estimators and generalized estimating equations. The M-estimator for parameter vector, θ, is estimated via bθ as the vector satisfying ψ(Di, bθ) = 0 Here, θ = {π}, Di (the observed data) is {Ti, Yi}, and ψ has two parts, one part defining the optimality conditions for bπ and the other part being the equation defining bQ( ˆπ). The portion defining the optimality condition is derived from the objective function: i=1 O(D, bπ) = bQ(π) λn π p2 2. 43 At the optimal value, ( Yi ˆπ Prˆπ(Ti) Pr(Ti) (cid:27) (cid:26) Yi ˆπ i=1 i=1 Prˆπ(Ti) Pr(Ti) ˆπ bO(D, bπ) ) = 0 λn ˆπ p2 2 = ˆπ{λn ˆπ p2 2} = 0 where the last equality follows by the linearity of the derivative operator. regularization penalty, λn must be selected so that In this approach, the ˆπ{λn ˆπ p2 2} p 0 for all bπ as . The closed form for the gradient is derived in the Appendix (A.I.0.13). An estimating equation for the Hàjek estimator is known (CITE) to be (cid:26) i= Yi Prˆπ(Ti) Pr(Ti) bQ( ˆπ) (cid:27) = 0. Therefore, our full system is i=1 ψ(Di, bθ) = ˆπ i=1 Pr ˆπ (Ti) Pr(Ti) bQ( bπ) Yi Pr ˆπ (Ti) Pr(Ti) λn bπ p2 Yi 2 = 0, . With this setup, sandwich estimator is available for asymptotic inference under regularity conditions (e.g., smoothness and interiority of the parameter space; Van der Vaart, 2000). Letting Dn = {Ti, Yi}n i=1, Vn(Dn, ˆθ) = An(Dn, ˆθ)1Bn(Dn, ˆθ)(cid:8)An(Dn, ˆθ)1(cid:9), Pn i=1 ˆθ ψ(Oi, ˆθ) and Bn = 1 Pn i=1 ψ(Oi, ˆθ)ψ(Oi, ˆθ). For detailed where An(On, ˆθ) = 1 discussion, see A.I.0.13. A.I.0.13 Deriving the Gradients Components of the M-Estimation In practice, we find π optimizing unconstrained quantities, as, to which the softmax function is applied, process which brings about πds which lie on the simplex (meaning that each Multinomial probability is between 0 and 1 and all the probabilities for given factor sum to 1). That is, the exp(ad,l) , where ad = (0, ad,0, ..., ad,Ld1), where Lf softmax function, πd,l = softmax(ad)l = denotes total number of levels associated with factor k. We will use the delta method on the as to arrive at the variance estimates for the πs. The objective function with the as is PLd1 l=0 exp(ad,l ) Mn(a) := 1 (cid:26) Yi softmax(ad) i=1 λ d=1 (cid:18) 1 ˆσ2T max tT Prπ (T = t) I{Tid=0} 0 softmax(ad) Pr(Tid) I{Tid=Ld1} (Ld1) + max tT Prπ (T = t) 2 Y d=1 I{Tid=0} softmax(ad) 0 softmax(ad) Pr(Tid) I{Tid=Ld1} (Ld1) (cid:19)(cid:27) An alternative penalty term is λπp2 2, which also provides an approximate bound for the variance. We see that , Pr(Tid), n, and λ are all constants. Thus, we only need to consider the following partial derivatives: Consider maxtT Pra(T = t)/ad,l. We know max tT Pra(Ti = t) = d=1 max l{0,1,...,Ld1} πd,l = maxtT Pra(Ti = t) ad,l = maxl{0,1,...,Ld1} πd,l ad,l d=d max l{0,1,...,Ld 1} πd,l, where max πd,l ad,l = (πd,l(1 πd,l) πd,lπd,l if πd,l is the maximum if πd,l is not the maximum Consider i QD d=1 Pra(Tid) Pr(Tid) / ad,l. We have: i and Pra(Tid) Pr(Tid) QD d=1 ad,l = πd,Tid ad,l d=d πd,Tid QD d=1 pd,Tid πf,Tif ad,l = (πd,l(1 πd,l) πd,lπd,Ti,f if = Ti,d if = Ti,d Consider λπ 2/ad,l. λπ p2 2 ad,l (πd,l pd,l)πd,l(1 πd,l) = 2λ (πd,l pd,l)πd,lπd,l l=l Putting things together, we have: (ma(Yi, Ti))d,l Yi QD Pra(Tid) Pr(Tid) λ ˆσ2 T maxtT Pra(T = t) ad,l d=1 ad,l n λ maxtT Pra(T = t) ad,l 2 d= Pra(Tid) Pr(Tid) + max tT Pra(T = t) 2 Pra(Tid) Pr(Tid) QD d=1 ad,l , with the gradient terms outlined in the above. 45 Algorithm 1 Variance-constrained stochastic intervention estimation using the one-step estimator for bQ. We let := {T, Y}, λ := {λv}V 1: function OneStep(D, λ) 2: v=1. SGA = stochastic gradient ascent. for λ λ do 3: 4: 5: 6: 7: 8: for fold {1, ..., K} do Estimate via SGA bπ using λ and the in-fold of D(1) Estimate bQ( ˆπ) the out-fold of D(1) bQ(fold) λ bQ( ˆπ) bλ argmaxλ K1 PK Estimate via SGA bπ using bλ and D(2) Perform approximate inference using M-estimation (see A.I.0.12) } 1.96 ds.d.{ bQ(fold) fold=1{ bQ(fold) } λ λ 9: 10: return bπ, bQ( bπ), ds.e.{ bπ}, ds.e.{ bQ( bπ)} 46 A.I.0.14 Simulation Results for the One-Step Estimator Figure A.I.2: True sampling variability of ˆπ plotted against the variability estimated via asymptotic inference. Yi(Ti) = β0 + Ld1 d=1 l= β dl I{Tid = l} + Ld 1 Ld 1 d,d:d<d l= l=1 γd,d I{Tid = l, Tid = l} + ϵi, (15) where βdl denotes the main effect of factor with level l, γdl,dl denotes the interaction effect of treatment dl and dl, and ϵi denotes random error term centered around 0. This model makes the computation of the average outcome under stochastic intervention, Q(π), straightforward if π specifies distribution of the same parametric form as that commonly used in conjoint analysis (e.g., Categorical distributions over all factor levels). Under that assumption, the average outcome under given stochastic intervention can here be written as Q(π) = β0 + Ld1 d=1 l=1 βdlπdl + Ld 1 Ld 1 d,d:d<d l=1 l=1 γdl,dl πdlπdl. If we seek stochastic intervention that optimizes the outcome under regularization constraints, the objective function to maximize is then O(π) = Q(π) λnp π2, where denotes the stacked vector of probabilities across all factors, (which has PD d=1 Ld distinct entries (with each element representing the probability that factor level is assigned)). The optimal stochastic intervention here, π, can be derived in closed form: Proposition 3 With two-way interactions and Ld levels for factor d, the optimal L2 regularized stochastic intervention for large enough value of λn is given by π = C1B. 47 where Br(dl),1 = βdl 4λpdl 2λn pdl l=l,l<Ld Cr(dl),r(dl) = 4λn Cr(dl),r(dl) = 2λn Cr(dl),r(dl) = γdl,dl, where r(dl) denotes an indexing function returning the position associated with its factor and level into the rows of and rows or columns of C. Proof of Proposition 1. See A.I.0.2. Here, the optimal stochastic intervention, Pr π, is deterministic function of the outcome model parameters. The parameters defining the outcome model, β and γ, are not known priori, but can be estimated via generalized linear methods, with the asymptotic standard errors then employed. Because the parameters, π, that define Pr π are deterministic function of the regression parameters, the variance-covariance matrix of { bQ( bπ), bπ} can be obtained via the delta method: Var-Cov({ bQ( bπ), bπ}) = ˆΣ J, where ˆΣ is the variance-covariance matrix from the modeling strategy for Yi using regression parameters β and γand is the Jacobian of partial derivatives (e.g., of bQ( bπ) and bπ with respect to the outcome model parameters): = { ˆβ,ˆγ}{ bQ( bπa ), bπa }. If the assumptions of the first-stage model hold, then (cid:16) (cid:17) { bQ( bπ), bπ} {Q(π), π} (cid:0)0, Σn J(cid:1) A.I.0.15 Derivation of Adversarial Formulation under Primaries By the Law of Total Expectation, (cid:20) Yi(Ta (cid:21) ) > Yi(Tb where ) πa,πb max πa min πb (cid:20) Yi(Ta πa,πb ) > Yi(Tb ) (cid:21) = πa,πb (cid:20) Prπa ,πb ,Yi(Tk )>Yi(Tk ),K (cid:20) Yi(Ta (cid:12) (cid:12) ) > Yi(Tb (cid:12) ) (cid:12) (cid:12) Ta , Tb , Ta , Tb , Yi(Ta ) > Yi(Ta ), Yi(Tb ) > Yi(Tb ), (cid:21)(cid:21) . By independence of competing group primaries, the primary outcome {Yi(Ta pendent from the choice, {Yi(Ta )}, given candidate features, so ) > Yi(Tb ) > Yi(Ta )} is indeE πa,πb (cid:20) Prπa ,πb ,Yi(Ta )>Yi(Ta ) Yi(Ta (cid:12) (cid:12) ) > Yi(Tb (cid:12) ) (cid:12) (cid:12) Ta , Tb , Ta , Yi(Ta ) > Yi(Ta ), ! Pr(i A) 48 (cid:12) (cid:12) ) > Yi(Tb (cid:12) ) (cid:12) (cid:12) (cid:12) (cid:12) ) > Yi(Tb (cid:12) ) (cid:12) (cid:12) + Prπa ,πb ,Yi(Tb )>Yi(Tb ) Yi(Ta Ta , Tb , Tb , Yi(Tb ) > Yi(Tb ), = Ta + Tb Pr (cid:26) Pr Yi(Ta Ta Tb T Pr(Yi(Ta ) > Yi(Ta ) Ta (cid:12) (cid:12) ) > Yi(Tb (cid:12) ) (cid:12) (cid:12) Ta Yi(Ta Ta , Tb , Ta , Yi(Ta ) > Yi(Ta ), , Ta , A) Pr(Ta ) Pr(Ta ) Pr(i A) ) Pr(Tb ! , Tb , Tb , Yi(Tb ) > Yi(Tb ), (cid:21) Pr(i B) ! ! Pr(Yi(Tb ) > Yi(Tb ) Tb , Tb , B) Pr(Tb ) Pr(Tb ) Pr(Ta ) Pr(i B) (cid:27) or, more succinctly, max πa min πb πa,πb,πa ,πb (cid:18) (cid:20) Pr Yi(Ta ) > Yi(Tb ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Ta , Tb , Ta , Yi(Ta ) > Yi(Ta ), (cid:19) Pr(Yi(Ta ) Ta , Ta , A)Pr(i A) (cid:18) + Pr Yi(Ta ) > Yi(Tb ) , Tb , Tb , Yi(Tb ) > Yi(Tb ), (cid:19) ) > Yi(Ta (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Ta Pr(Yi(Tb ) > Yi(Tb ) Tb , Tb , B)Pr(i B) (cid:21) 49 Figure A.II.2: True sampling variability of ˆπ plotted against the variability estimated via asymptotic inference. Colors depict the sample size (with n=500 being light gray and n=10,000 being black). Appendix II: Supplementary Simulations Results A.II.0.16 Supplementary Simulation Results with the Two-Step Estimator Figure A.II.1: Points depict the average estimated standard deviation obtained via the Delta method. Colors depict the sample size (with = 500 being light gray and n=10,000 being black). A.II.0.17 Estimation Details 50 Figure A.II.3: Here, the standard errors used in constructing confidence intervals calculated as described in A.I.0.11 are conservative, meaning that they cover the true value more than 90% of the time. In particular, these bounds are constructed using the variance bound formula from Proposition 2. Algorithm 2 Variance-constrained stochastic intervention estimation. We let := {T, Y}, λ := {λv}V v=1. Here, bQ(π) is deterministic function given bβ and bγ, so sample splitting is not feasible (unlike with one-step estimator, where bQ(π) can always be estimated again with new data) . 1: function TwoStep(D, λ) 2: for λ λ do 3: 4: 5: From regression using the appropriate covariance structure, obtain bβ, bγ, and bΣ. Given λ, bβ and bγ, obtain bπ using the closed-form expression or gradient ascent. Calculate the variance-covariance matrix, Var-Cov({ bQλ( bπ), bπ}) = ˆΣ J. bλ argmaxλ bQλ( bπ) 1.96 ds.e.{ bQλ( bπ)} 6: 7: return bπ, bQ( bπ), ds.e.{ bπ}, ds.e.{ bQ( bπ)} from analysis with bλ 51 Appendix III: Additional Application Results A.III.0.18 Mapping the 2016 Candidate Primary Features onto the Conjoint Levels of Ono and Burden (2019) We map the features of the 2016 presidential election candidates onto the conjoint features of Ono and Burden (ibid.). In some cases, this mapping is straightforward (e.g., with candidate gender). In other cases, the mapping is less straightforward. For example, the factor levels associated with marital status do not encompass the full range of possibilities seen among 2016 candidates. In such cases, we select the closest mapping (see Replication Data for full details). For example, real, married candidate with 4 children would be mapped to the Married with 2 children level (not the Single, divorced orMarried, no children levels). We will explore these substantive questions by integrating the experiment mentioned above from Ono and Burden (ibid.). In this election, 17 Republican and 6 Democrat candidates vied for their respective partys nomination in primaries. These candidates have large number of features, which we mapped onto the conjoint factors of Ono and Burden (ibid.) (see A.III.0.18 for details). Below we present this mapping for four of the candidates: Ben Carson: Republican, Black, male, 68-76, married (with children), 0 years of political experience, compassionate, policy focus on health care, emphasis on maintaining strong defense, opposes giving guest worker status, pro-life, dont reduce deficit now Hillary Clinton: Democrat, White, female, 68-76, married (with children), 16 years of political experience, provides strong leadership, foreign policy, maintain strong defense, favors giving guest worker status, pro-choice, dont reduce deficit now Bernie Sanders: Democrat, White, male, 68-76, married (with children), 34 years of political experience, compassionate, policy focus on economy, cut military budget, ambiguous position on immigration, pro-choice, reduce deficit through tax increase Donald Trump: Republican, White, male, 68-76, married (with children), 0 years of political experience, provides strong leadership, policy focus on economy, emphasis on maintaining strong defense, opposes giving guest worker status, pro-life, reduce deficit through spending cuts Figure A.III.1: Expected optimized vote share in the population in the average (uniform) case (denoted by bQ( bπa)), compared against factor-wise marginal means. The for the marginal means indicates the level with the highest marginal outcome (with that level listed on the righthand side of the figure along with the factor name). bQ( bπa, bπb) denotes the expected optimized vote share in the population under adversarial conditions compared against factor-wise marginal means. Figure A.III.2: Marginal means analysis, analysis among all respondents. 53 Figure A.III.3: Marginal means analysis, analysis among Democrat respondents. Figure A.III.4: Marginal means analysis, analysis among Republican respondents. Figure A.III.5: Marginal means analysis, analysis among independent respondents. Figure A.III.6: Strategic divergence factor computed for major candidates in the 2016 primaries."
        },
        {
            "title": "References",
            "content": "Muñoz, Iván Díaz and Mark Van Der Laan (2012): Population Intervention Causal Effects Based on Stochastic Interventions. In: Biometrics, no. 2, vol. 68, pp. 541549. Ono, Yoshikuni and Barry Burden (2019): The Contingent Effects of Candidate Sex on Voter Choice. In: Political Behavior, vol. 41, pp. 583607. Prashanth, LA, Michael Fu, et al. (2022): Risk-Sensitive Reinforcement Learning via Policy Gradient Search. In: Foundations and Trends in Machine Learning, no. 5, vol. 15, pp. 537693. Simpson, Douglas et al. (1987): M-estimation for Discrete Data: Asymptotic Distribution Theory and Implications. In: The Annals of Statistics, pp. 657669. Stefanski, Leonard and Dennis Boos (2002): The Calculus of M-estimation. In: The American Statistician, no. 1, vol. 56, pp. 2938. Taleb, Nassim Nicholas (2009): Errors, Robustness, and the Fourth Quadrant. In: International Journal of Forecasting, no. 4, vol. 25, pp. 744759. Van der Vaart, Aad (2000): Asymptotic statistics. Vol. 3. Cambridge university press."
        }
    ],
    "affiliations": [
        "Department of Government, University of Texas at Austin",
        "Department of Statistics, Harvard College",
        "Faculty of Informatics, Università della Svizzera Italiana"
    ]
}