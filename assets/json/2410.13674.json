{
    "paper_title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion",
    "authors": [
        "Yijun Liang",
        "Shweta Bhardwaj",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy."
        },
        {
            "title": "Start",
            "content": "DIFFUSION CURRICULUM: SYNTHETIC-TO-REAL GENERATIVE CURRICULUM LEARNING VIA IMAGE-GUIDED DIFFUSION 4 2 0 2 8 1 ] . [ 2 4 7 6 3 1 . 0 1 4 2 : r Yijun Liang, Shweta Bhardwaj, Tianyi Zhou Department of Computer Science University of Maryland, College Park {yliang17, shweta12, tianyi}@umd.edu Project: https://github.com/tianyi-lab/DisCL October 21,"
        },
        {
            "title": "ABSTRACT",
            "content": "Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to larger distribution gap with the original data. The generated full spectrum of data enables us to build novel Diffusion CurricuLum (DisCL). DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base models tail-class accuracy from 4.4% to 23.64% and leads to 4.02% improvement in all-class accuracy."
        },
        {
            "title": "Introduction",
            "content": "While existing machine learning approaches can train representation or discriminative models with promising generalization performance, their success highly relies on the quality and quantity of the training data. However, in enormous practical scenarios, the data are collected from real environments so neither the quality nor the quantity can always be guaranteed. For example, it is difficult to control the light conditions, weather, motion blur, or the position of objects in the scenes captured by trail/animal cameras, traffic cameras, motion cameras, or robot cameras. Likewise, it is also difficult to keep different classes in the collected data balanced so the model may perform much poorer on tail classes with scarce data. On the other hand, the low-quality/quantity of data also makes the model more prone to the gap between the test and training distributions, thereby posing an out-of-distribution challenge. In many cases, such hard training data hinders effective learning, introduces biases or outliers, and may even impact the learning of other data. Data augmentation and synthesis have been studied to address the challenges of hard real data. By applying pre-defined transformations (Ahn et al., 2023) to data in scarce classes or modifying their backgrounds (Beery et al., 2020; Gao et al., 2022), data augmentation helps learn representations robust to these task-irrelevant variations. While the augmented data may lack sufficient diversity or non-trivial difference to the original data, the recent text-to-image generative models such as GAN or Stable Diffusion enable more sophisticated data synthesis (Dunlap et al., 2024) *These authors contributed equally to this work PREPRINT - OCTOBER 21, 2024 of diverse higher-quality samples, while the text prompts retain the task-related features. Despite these advancements, existing methods (Han et al., 2024) still struggle to train robust and reliable models or representations for hard classes. Although text-to-image synthesis improves the data quality and quantity, the synthetic data are solely controlled by text prompts but lack sufficient visual similarity to the original image, which leads to distribution gap to the original data and hurts the generalization performance. To maximize the merits of synthetic data for learning hard data in real applications and address the syn-to-real gap, we harness the image guidance in diffusion models to generate full spectrum of interpolations between synthetic data (i.e., generated only from text prompts) and real data (i.e., original images that may suffer from low-quality and low-quantity). The synthetic data at each level of interpolation are generated under the weighted guidance of both the text prompt (e.g., the class name) and the real images. While stronger image guidance preserves visual similarities to the original image, for low-quality or low-quantity data, weaker image guidance could lead to high-quality, diverse, and potentially easier (e.g., with prototypical features) data. Hence, the syn-to-real interpolations create novel space of synthetic data to design generative curriculum that can adjust the quality, diversity, and/or difficulty of data for different training stages, by selecting the guidance level according to pre-defined schedule or training dynamics. In this paper, we develop novel generative curriculum learning approaches for two types of challenging applications with hard real images: long-tail classification, and learning from low-quality images. In long-tail classification, learning the tail classes features is challenging due to their data deficiency and the lack of diversity compared to head classes. To address this challenge, we propose curriculum that first learns synthetic images with lower image guidance for tail classes since they enhance the diversity and quantity of the original data. The curriculum then gradually increases the guidance level and learns synthetic images closer to the original images, thereby progressively bridging the syn-to-real gap. In learning from low-quality data, the primary challenge is to capture the critical features of the target classes, which is hard due to intricate background, occlusion, or motion blur in the original images. In contrast, images generated with lower image guidance usually contain prototypical features easier to learn. That being said, an overly high or low guidance level may enlarge the domain gap between the training data and the target (in-distribution or out-of-distribution) data. To avoid negative transfer caused by the domain gap and to maximize the merits of synthetic data, we develop an adaptive curriculum that selects the guidance level of synthetic data leading to the greatest progress of each training stage. We examine two DisCL curricula on benchmark datasets, ImageNet-LT (Liu et al., 2019) and WILD-iWildCam (Beery et al., 2021), for long-tail classification and learning from low-quality images, respectively. Our DisCL curricula improve OOD and ID accuracy by 2.7% and 2.1% respectively on iWildCam. On ImageNet-LT, DisCL improves the minority classes accuracy by 19.24% and leads to 4.02% improvement in the overall accuracy. Our main contributions can be summarized as follows: We harness image guidance in diffusion models to create spectrum of synthetic-to-real data for each sample that can be used to design effective training curricula addressing hard data learning. We propose the Diffusion CurricuLum (DisCL) paradigm that selects synthetic data of different guidance levels for the needs of each training stage. We propose two novel DisCL curricula to address two important applications, long-tail classification and learning from low-quality data. We examine the two DisCL curricula on challenging datasets and demonstrate that DisCL significantly boosts the performance of existing image classifiers especially on the hard data."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion models for Synthetic Data Recently, diverse array of generative diffusion models have been proposed, including GLIDE (Halgren et al., 2004), Imagen (Saharia et al., 2022), Stable Diffusion (Rombach et al., 2022), Dall-E (Ramesh et al., 2022), and Muse (Chang et al., 2023). These models can generate realistic, high-resolution images when conditioned on text prompts, and therefore, are used off-the-shelf to augment the datasets for enhancing data diversity. For instance, He et al. (2022) demonstrates that synthetic data created with GLIDE can significantly improve both zero-shot and few-shot performance on image classification. Wu et al. (2023) has explored Stable Diffusion to generate perception data for downstream dense prediction tasks such as Human Pose Estimation, Depth Estimation, and Segmentation. Recent works like Bansal & Grover (2023) and Sariyildiz et al. (2022) have shown that real data combined with synthetic data generated by Stable Diffusion models, boosts the robustness of standard ImageNet classifiers. Other works like Azizi et al. (2023) have finetuned the Imagen model using ImageNet data to enhance the alignment of synthetic data with their classes, while improving the sample diversity. In this work, we utilize off-the-shelf Stable Diffusion models without further finetuning. Unlike previous works, we harness different image guidance levels to generate training images for each stage of model training, thereby progressively learning full spectrum of interpolations from synthetic to real data. 2 PREPRINT - OCTOBER 21, 2024 Figure 1: Overview of Diffusion Curriculum (DisCL). DisCL consists of two phases: (Phase 1) Syn-to-Real Data Generation and (Phase 2) Generative Curriculum learning. In Phase 1, we use pretrained model to identify the hard samples in the original images and use them as guidance to generate full spectrum of synthetic to real images by varying image guidance strength λ. In Phase 2, curriculum strategy (Non-Adaptive or Adaptive) selects training data from the full spectrum, by determining the image guidance level for each training stage e. Synthetic data of the selected guidance level is then combined with non-hard real samples to train the task model. Curriculum Learning (CL) Curriculum Learning (CL) was first proposed by Bengio et al. (2009), introducing training method analogous to the step-by-step progressive learning of humans. Subsequent works have further explored this idea; for example, Jiang et al. (2015); Zhou et al. (2020) adjusted the progression pace based on the difficulty of samples, and Jiang et al. (2014); Zhou & Bilmes (2018) further take the data diversity into account. Previous works (Guo et al., 2018; Zhou et al., 2021b; Yuan et al., 2022) have tried CL on more challenging domains like noisy web images and visual QA; this highlights its potential in tackling challenging scenarios. Few works have explored the combination of data augmentation and curriculum learning (Hou et al., 2023), but mainly for the text data (Lu & Lam, 2023; Ye et al., 2021). Some initial efforts have been made by Ahn et al. (2023) to combine CL with engineered image augmentations for tail classes in long-tail learning. In contrast, our work aims to design generative curriculum on syn-to-real spectrum of data produced by diffusion models, with broader applications in learning from long-tail or low-quality data."
        },
        {
            "title": "3 Methodology",
            "content": "We propose diffusion curriculum (DisCL) to close the distribution gap between original data and the target data distribution. DisCL comprises two phases: (Phase 1) Synthetic-to-Real Data Generation that generates syn-to-real spectrum of interpolated data for hard samples, and (Phase 2) Generative Curriculum learning based on the synthetic data from Phase 1. The two phases are illustrated in Fig. 1. 3.1 Synthetic-to-Real Data Generation Hard Sample Identification We first identify the difficult samples where the model struggles to extract helpful features for target classification. The difficulty estimation can be task-specific. For instance, in long-tail classification with scarce data, the difficulty of each sample depends on whether it belongs to tail classes. For tasks with low-quality data, we can utilize the loss or confidence on the ground-truth class to measure the difficulty. These samples are marked as hard samples within the training set (see Fig. 1), to highlight their role in the models learning process. Synthetic Data Generation with Image Guidance Classifier-free guidance was initially introduced by Ho & Salimans (2022), to integrate conditional information into the image denoising process of diffusion models without the need for classifier. It has been adopted by several Text-to-Image generation models such as Stable Diffusion (SD) (Rombach et al., 2022). Given the original images latent representation zreal, the denoising (backward diffusion) process can start from any step with initial zt defined as (cid:112) (cid:112) zt = αtzreal + 1 αtϵ, ϵ (0, I). (1) 3 PREPRINT - OCTOBER 21, 2024 Figure 2: Synthetic images generated with various image guidance levels and random seeds. marks images with low-fidelity to the text prompt, which are filtered out by CLIPScore (ref. the end of 3.1). The remaining denoising steps iteratively apply the following procedure, noise estimation ˆϵt at each step and less noisy generation of zt1, until = 0, resulting in synthetic image z0. ˆϵt = (1 + w)ϵθ(zt, tc) wϵθ(zt, t), zt1 = 1 αt (cid:18) zt βt 1 αt (cid:19) ˆϵt + (cid:112)βtϵ, 1 (2) In Eq. 1-2, αt, αt, and βt together define the variance schedule of the diffusion process. ϵ, ϵ (0, I) are two independently-sampled Gaussian noises, ϵθ(, ) refers to the noise estimation model, and controls the strength of the textual prompt as condition to ϵθ(, ). Since αt monotonically decreases with t, the choice of the initial in Eq. 1 controls the impact of the original zreal in the denoising process, and more visual information of zreal tends to be preserved in z0 if initializing from small t. To achieve full spectrum of interpolations between the real image zreal and synthetic images depicted by c, we modify the initial step in Eq. 1 to t(λ) (1 λ)T where λ [0, 1) defines the image-guidance level, i.e., (cid:113) zt(λ) = αt(λ)zreal + (cid:113) 1 αt(λ)ϵ, t(λ) (1 λ)T . (3) Hence, larger guidance level λ leads to higher fidelity of the generated image z0 to the original zreal, while smaller λ results in more prototypical image 0 depicted by textual prompt c. 4 PREPRINT - OCTOBER 21, 2024 Synthetic-to-Real Spectrum of Generated Images We use state-of-the-art Stable Diffusion Model 1 to generate synthetic images for the hard samples identified in Phase 1 of Fig. 1. By adjusting the image guidance scale λ [0, 1) in Eq. 3, the denoising process in Eq. 2 can produce full spectrum of smooth transitions between text-only guided synthetic images and real images. We next study the effect of varying the image guidance scales λ on the generated synthetic images. As shown in Fig. 2, changing λ leads to varying difficulty and diversity of synthetic images. With smaller λ, diffusion model mainly relies on the text information provided in the prompt c, generating synthetic images that differ markedly from the original and focus more on the distinct prototypical features of the class in c. As λ increases, the synthetic images increasingly inclines towards the original image, exhibiting less diversity (across random seeds) and more resemblance to the original ones. When the original images are of low-quality, large λ makes it challenging for the classifier to learn discriminating features from synthetic images. Therefore, the broad spectrum of synthetic data offers diverse properties, e.g., diversity, hardness, proximity to the real ones, providing design space for curriculum learning. Filter out Synthetic Data with Low-Fidelity As shown in Fig. 2, some synthetic images may suffer from poor quality and low fidelity to the text prompt c, e.g. the class object is missing or obscured, which are detrimental to the downstream tasks. To mitigate this issue, we perform quality checks and filter out low-fidelity images using CLIPScore (Hessel et al., 2022; Schuhmann et al., 2021), which computes CLIP cosine similarity between synthetic images and the text prompt c. We filter out images below some threshold of CLIPScore before using them for training. 3.2 Generative Curriculum Learning with Synthetic Data With the full spectrum of syn-to-real generated data, we achieve smooth transition from images of prototypical features and high diversity to task-specific features with high resemblance to real images. This enables us to design curriculum selecting data with according to their diversity and feature types for different training stages. With curriculum of rich synthetic data, we can enhance the models performance in challenging and diverse cases which are otherwise difficult to using only the real data. On the other hand, it also allow us to control the distribution gap to the original data. We apply our method to two challenging applications in the following sections: long-tail classification and learning from low-quality data. In long-tail classification, the scarcity of data in minority/tail classes makes it difficult for models to extract useful features for these classes, leading to poor generalization on balanced test set. To address this, we develop curriculum strategy that initially exposes the model to diverse synthetic samples of tail classes, and then progressively focuses on task-specific features. This helps mitigate distribution differences between synthetic and real data. In learning from low-quality data, the poor quality of data limits the models ability to detect and extract critical visual features. By employing an adaptive curriculum of synthetic data, we can warm up the model training by learning from varying levels of prototypical features, gradually aiding the model in extracting features useful for out-of-domain generalization."
        },
        {
            "title": "4 Applications",
            "content": "4.1 Long-Tail (LT) Classification Synthetic Data Generation For synthetic data generation, we follow standard split of tail classes in the studied dataset. Given the real tail-class samples and the associated text prompts, we generate full spectrum of synthetic data by techniques in 3.1. To mitigate the imbalance among classes, the key is to increase data diversity and quantity for tail classes. We employ diverse set of textual prompts to achieve the goal2. Generative Curriculum The generated spectrum of synthetic data provides varying degrees of data diversity: the images generated with text-only guidance display the highest diversity but may suffer from visual discrepancies to the original images, resulting in distribution gap that may undermine model performance. To bridge the gap, we progressively shift the synthetic data to task-specific distribution closer to the original images. This yields non-adaptive Diverse-to-Specific curriculum that starts with synthetic data with lower guidance scale (λ 0) and gradually moves toward data of higher guidance scale (λ 1). 4.2 Learning from Low-quality Data The data collected in real-world scenarios may suffer from low qualities, such as obscurity in images from traffic, motion, or wildlife observation cameras. We investigate wildlife observation as an example application of DisCL to enable effective learning under such challenging scenarios. Synthetic Data Generation For low-quality images from camera traps, we aim to generate simpler images containing more prototypical features of the animals that can warm up the training and generalize to more challenging cases. We first identify hard samples based on the ground-truth class probability by pretrained classifier: lower probability indicates more difficulty. We vary the image guidance scale to generate full spectrum of synthetic data for these hard 1We use Stable Diffusion XL model for generation 2Text prompts are provided in Appendix A.1.2 5 PREPRINT - OCTOBER 21, 2024 samples, ranging from prototypical to in-the-wild images. The class information is used in the text prompts3 to steer the diffusion model to generate images relevant to the animals and their wild environment. Generative Curriculum Training on text-only synthetic data hinders performance due to their distribution gap to the real data and their differences in hardness. flexible curriculum strategy that integrates both text-only synthetic and real images during training can mitigate this gap. Unlike long-tail classification, various features in the hard samples of low-quality data are not prototypical or generalizable. Synthetic data with higher image guidance can mitigate the issue to some extent but may remain difficult for models to learn from. In contrast, synthetic data with lower image guidance are more prototypical and easier but they are out-of-distribution (OOD) of the real images. predefined curriculum of image guidance may introduce OOD features at the early stage, causing distribution shift, or overemphasizing hard and outlier features, downplaying the prototypical patterns. DoCL (Zhou et al., 2021a) proposed an adaptive curriculum that selects real data for each training stage that can achieve the greatest progress on the original distribution. The curriculum aims to optimize the training dynamics. Inspired by DoCL, we propose an adaptive curriculum to dynamically select the guidance level that helps the model achieve the best improvement on the real data distribution. This approach effectively advances the model from learning simple features to mastering more complex and difficult scenarios."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Long-Tail Classification Setup To validate the efficacy of DisCL method on long-tail classification, we conduct main experiments with ImageNet-LT (IN-LT) dataset (Liu et al., 2019). This dataset includes 1000 classes, with class cardinality ranging from 5 to 1,280. To assess the robustness of DisCL more comprehensively, we conduct experiments on two additional datasets: synthetically imbalanced dataset, CIFAR100-LT (Cao et al., 2019), and real-world benchmark, iNaturalist2018 (Van Horn et al., 2018). CIFAR100-LT is provided with imbalanced classes by synthetically sampling the training data with multiple imbalance ratios {100, 50}. iNaturalist2018 dataset represents naturally occurring long-tailed distribution with class cardinality ranging from 2 to 1000. We evaluate overall accuracy and the accuracy across three categories of classes: many (most frequent), medium, and few (least frequent, tail) classes on the standard balanced test sets of three datasets. For synthetic data generation, we use DDIM (Song et al., 2020) as our noise scheduler. For training, following Ahn et al. (2023); Han et al. (2024), we use ResNet-10 as the visual backbone. We average results over 3 runs, with training details in Appendix A.3.1 and hyperparameters in Appendix A.4. Baselines We compare the effect of DisCL with comparable baseline of CUDA (Ahn et al., 2023) and LDMLR (Han et al., 2024), mainly using Cross-Entropy (CE) loss function. To further illustrate the robustness of DisCL, we try Balanced Softmax (BS) loss (Ren et al., 2020), known for its competitive performance on long-tail learning. CUDA: Engineered data augmentation + curriculum learning on IN-LT. LDMLR: three-stage training using diffusion model to improve LT. BS loss: Balanced softmax to address class-distribution shift between training and test sets. We also conduct ablation study to analyze the effect of DisCL under different hyperparameter settings. We note that, real data for hard samples (λ 1) is included by default; however, this doesnt apply to the Fixed Guidance and Text-only Guidance ablation: Text-only Guidance: Using data at image guidance scale λ = 0 without curriculum strategy. Fixed Guidance 4: uses data generated from single guidance scale λi [0, 1). We report results for the guidance with the highest few-class accuracy. DisCL: employs multiple levels of guidance scales alongside range of curriculum strategies. These strategies and the guidance intervals used for training, are defined below: Specific to Diverse: Non-adaptive strategy with guidance changing from largest (task-specific augmentation) to smallest (diverse augmentation). Diverse to Specific: Non-adaptive strategy with guidance changing from smallest to largest. Adaptive: Curriculum strategy5 to adaptively select guidance during training. 3Text prompts are provided in Appendix A.1.3. 4Text-only Guidance (λ=0) reaches the best performance amongst all guidance scales. Hence, the result of Fixed Guidance here are same as Text-only Guidance, reported in Table 1. We also report the performance of all other scales in Fixed Guidance experiment in the Fig. 12. 5Curriculum strategy proposed in 4.2 Table 1: Accuracy (%) of long-tail classification on ImageNet-LT with base model ResNet-10. The best accuracy is highlighted in bold. marks our reproduced results using the original paper provided code. BS refers to Balanced Softmax Loss(Ren et al., 2020). Baselines (LDMLR, CUDA) are defined in 5.1. PREPRINT - OCTOBER 21,"
        },
        {
            "title": "Curriculum",
            "content": "ImageNet-LT Many Medium Few Overall"
        },
        {
            "title": "Method",
            "content": "CE CE + LDMLR CE + CUDA BS BS + CUDA i a o l N/A CE + Text-only Guidance N/A CE + All-Level Guidance Adaptive CE + DisCL Specific to Diverse CE + DisCL CE + DisCL [Lower CLIPScore Threshold] Diverse to Specific CE + DisCL [Higher CLIPScore Threshold] Diverse to Specific CE + DisCL BS + DisCL O"
        },
        {
            "title": "Diverse to Specific\nDiverse to Specific",
            "content": "N/A N/A N/A N/A N/A 57.70 57.20 57.49 51.14 51.16 56.63 56.77 56.21 56.71 57.66 56.92 56.78 52.68 26.60 29.20 28.16 37.02 37.35 30.69 30.88 30.43 30.67 30.61 30. 30.73 37.68 4.40 7.30 6.58 19.29 19.28 17.90 19.17 16.78 18.36 23.69 22.88 23.64 21.36 35.80 37.20 36.30 39.80 40.03 39.10 39.40 38.65 39.18 39.67 39. 39.82 41.33 Table 2: Accuracy (%) of long-tail classification on CIFAT-100-LT with base model ResNet-10. The best accuracy for classes of {many, medium, few} samples is highlighted in bold. Baselines are defined in 5.1. Imbalance Ratio=100 Imbalance Ratio=50 CIFAT-100-LT Method Curriculum Many Medium Few Overall Many Medium Few Overall CE CE + CUDA CE + DisCL Diverse to Specific N/A N/A BS BS + CUDA BS + DisCL Diverse to Specific N/A N/A 52.86 54.55 53.14 47.87 48.01 49.02 25.34 26.07 25.52 30.07 32.79 29.02 5.49 5.43 13.65 14.41 15.55 19. 29.02 29.02 39.91 31.61 33.02 33.08 49.60 52.29 53.4 46.01 46.08 49.51 25.41 26.17 31.69 30.76 32.51 32. 5.33 5.53 21.47 18.55 22.11 25.58 31.72 33.13 36.22 34.82 36.21 36.77 Results We present the results of our method alongside the baselines for the ImageNet-LT dataset in Table 1. With CE loss, DisCL significantly improves accuracy in all 4 class-categories. Notably, Few class accuracy increases by 17.06%, from 6.58% to 23.64%, demonstrating DisCLs effectiveness in addressing the data scarcity challenge, especially for tail classes. We also try our DisCL method with BS loss, and observe additional gains (1.52% in Many, 2.08% in Few, and 1.3% Overall); this emphasizes the impact of our approach even with class-balancing loss function. The results on CIFAR100-LT and iNaturalist2018 (as shown in Table 2 and Table 3) further demonstrate the robustness of DisCL on various datasets. These experimental results shows that by utilizing diverse spectrum of data, our method achieves better accuracy in tail classes, alongwith improved overall generalization. 5.2 Learning from Low-quality Data Setup We also conduct DisCL experiments with iWildCam dataset (Beery et al., 2021) to evaluate its efficacy in classifying low-quality data. The task is to classify 182 different animal species from images captured by camera traps. We evaluate model performance on standard out-of-domain (OOD) and in-domain (ID) test sets in terms of macro F1 score. We choose the CLIP ViT model as our base model and finetune CLIP ViT-B/16 and CLIP ViT-L/14 6 models with DisCL. The reported accuracy is averaged over 3 random seeds. More training details and hyperparameters are provided in Appendix A.3.2 and Appendix A.4. Baselines We compare the effect of our method with three benchmark algorithms, LP-FT (Kumar et al., 2022), FLYP (Goyal et al., 2023), and ALIA (Dunlap et al., 2024). To further analyze the gain of our model, we try Weighted Ensembling (WE) method (Wortsman et al., 2022), which can further improve model performance by integrating prior knowledge from pretrained model: 6We use hyperparameters provided in Goyal et al. (2023) with batchsize of 128 to train the model. 7 PREPRINT - OCTOBER 21, 2024 Table 3: Accuracy (%) of long-tail classification on iNaturalist2018 with base model ResNet-10. The best accuracy is highlighted in bold. Baselines are defined in 5.1."
        },
        {
            "title": "Curriculum",
            "content": "iNaturalist2018 Many Medium Few Overall CE CE + CUDA CE + DisCL Diverse to Specific N/A N/A BS BS + CUDA BS + DisCL Diverse to Specific N/A N/A 55.02 55.94 54. 46.12 48.77 45.44 43.40 44.21 44.37 49.31 49.94 48.18 37.33 39.13 48.92 50.27 50.87 53.63 42.20 43.18 47. 49.46 50.23 50.30 LP-FT: two-step process involving linear probing and full fine-tuning of model to avoid distortion of pretrained features, to improve OOD generalization. FLYP: Finetuning with the pretraining loss (contrastive loss). ALIA: Diffusion-based data-augmentation on fine-grained classification tasks. WE: Linearly merging the weights of pretrained and finetuned model. We conduct ablation study to analyze the effect of DisCL with different hyper-parameters introduced in 5.1, and the newly introduced ablation hyper-parameters: DisCL: employs multiple levels of guidance scale and range of curriculum strategies: Easy to Hard: Non-adaptive strategy with guidance changing from smallest (easiest and most prototypical features, λ 0) to largest (hardest and most task-specific features, λ 1). Random: Randomly selecting guidance at each training stage. Table 4: F1 Score with CLIP ViT-L/14 Results We present the results of our method and comparable baselines for the iWildCam dataset in Table 5. Compared to the nearest competing baseline, DisCL significantly enhances the OOD F1 performance by 2.6%. Additionally, DisCL boosts the ID F1 performance by 2.1%. Among all evaluated methods, DisCL achieves the highest scores in both OOD and ID metrics, underscoring its effectiveness for this low-quality classification task. Moreover, our model could still deliver performance improvements on larger model when using ViT-L/14, as shown in Table 4; DisCL achieves gains of 2.8% in OOD F1 and 3.7% in ID F1. These findings reinforce the versatility and robustness of the DisCL framework across different model scales and complexities. We further study the performance of model after employing WE method. DisCL still benefits from this method and maintains superior performance compared to other methodologies, despite integrating prototypical features from synthetic data that might overlap with the pretrained models knowledge. iWildCam Without WE With WE CLIP (Zero-Shot) FLYP FLYP + DisCL 11.8 55.9 59.6 12.1 40.3 43.1 12.1 41.9 44.8 11.8 57.7 60."
        },
        {
            "title": "OOD",
            "content": "ID ID"
        },
        {
            "title": "6 Ablation Study and Analysis",
            "content": "6.1 Effect of Syn-to-Real Interpolation Data We examine the effectiveness of using spectrum of data generated with our DisCL method, by comparing All-Level Guidance and Text-only Guidance rows in both the task tables (IN-LT and iWildCam). For IN-LT results in Table 1, All-Level Guidance brings 1.27% gain in few-class accuracy, alongwith significant gains across other class-categories. Likewise, All-Level Guidance shows superior ID and OOD performance as compared to Text-only Guidance for the iWildCam as well, see Table 5. These findings corroborate that utilizing spectrum of data with multiple guidance levels helps mitigate the negative effects of the distribution gap. 6.2 Effect of Curriculum Learning Strategy Long Tail Classification We compare the impact of our Diverse to Specific curriculum strategy tailored for IN-LT task against other strategies, notably All-Level Guidance which employ no curriculum and uses all synthetic data. The Diverse to Specific demonstrate higher few-class accuracy with margin of 4.47%, see Fig. 3b. We then compare it 8 PREPRINT - OCTOBER 21, 2024 Table 5: In-distribution (ID) and out-of-distribution (OOD) macro F1 score of low-quality image learning on iWildCam with CLIP ViT-B/16 model. The best performance is highlighted in bold. marks our reproduced results using the original paper provided code. Baselines are defined in 5.2."
        },
        {
            "title": "Method",
            "content": "CLIP (zero-shot) LP-FT LP-FT + WE FLYP FLYP + WE FLYP + Text-only Guidance FLYP + Fixed Guidance FLYP + All-Level Guidance FLYP + DisCL FLYP + DisCL FLYP + DisCL [Lower CLIPScore Threshold] FLYP + DisCL [Higher CLIPScore Threshold] i a o l FLYP + DisCL FLYP + DisCL + WE"
        },
        {
            "title": "OOD",
            "content": "ID iWildCam N/A N/A N/A N/A N/A N/A N/A Easy-to-Hard Random Adaptive Adaptive"
        },
        {
            "title": "Adaptive\nAdaptive",
            "content": "11.0 (-) 34.7 (0.4) 35.7 (0.4) 35.5 (1.1) 36.4 (1.2) 34.2 (0.4) 36.0 (0.3) 36.5 (0.6) 35.2 (0.9) 35.9 (0.1) 37.1 (0.8) 38.1 (1.3) 38.2 (0.5) 38.7 (0.4) 8.7 (-) 49.7 (0.5) 50.2 (0.5) 52.2 (0.6) 52.0 (1.0) 51.4 (0.3) 50.8 (0.6) 53.4 (0.5) 51.4 (0.5) 52.1 (0.2) 50.9 (0.9) 52.8 (0.8) 54.3 (1.4) 54.6 (0.7) with reverse strategy Specific-to-Diverse, and found the latter one to be worse. The reverse strategy can overfit model to real distribution early on, increasing the gap between real and synthetic data; hence, later-stage training on the data with larger distribution gap can decrease models few-class accuracy. For IN-LT, we also try Adaptive strategy (mainly developed for learning from low-quality data), in which strategys progression is based on validation set, comprising few tail images sampled from each guidance scale and few original images. But, validation set is scarce interms of tail samples, which renders it ineffective for identification of truly useful guidance. Hence, this strategy ranks as the least effective for LT task. Learning from Low Quality Data For iWildCam task, we study the effect of our designed Adaptive strategy, catering to the challenge of learning from low quality data. As shown in Fig. 3d, for this task, Adaptive surpasses the All-Level Guidance with clear margin, underscoring the benefit of using progressive curriculum over using all synthetic data. Further comparisons with the Non-Adaptive curricula including Easy-to-Hard and Random, show an impactful increase in OOD F1, while using our Adaptive. These findings highlight how the structured data selection used in Diverse-to-Specific, is more effective in directing models focus on scarce data (classes), however, when dealing with real-world low-quality data, an Adaptive strategy is more successful in adjusting to models needs by adaptively selecting the suited data. 6.3 Effect of CLIPScore Threshold Long Tail Classification Our analysis of CLIPScore distribution on IN-LT generated data leads us to infer that the best CLIPScore threshold for filtering is 0.3 (detailed explained in the Appendix A.1.2). We then assess different CLIPScore thresholds with the Diverse to Specific curriculum strategy, by experimenting with different values: lower (0.28), and higher (0.32), shown in Fig. 3a. However, we find that changing the CLIPScore threshold does not significantly affect the performance. As shown in Figure 4b, the CLIPScore of synthetic data is concentrated, as Stable Diffusion model performs well on generating high-quality images for ImageNet classes. Changes in the CLIPScore threshold will not significantly affect the quality of synthetic images and corresponding effects in downstream classification tasks. Learning from Low Quality Data In the iWildCam task, we identify the optimal threshold as 0.25. To further validate this choice, we experiment with nearby thresholds (0.23 and 0.27) with the chosen Adaptive Curriculum strategy suited for low-quality image classification. As depicted in Fig. 3c, the 0.25 threshold markedly improves OOD performance compared to other CLIPScore thresholds. Unlike the ImageNet dataset, the iWildCam images are characterized by significant difficulty and poor quality, leading to high variance in CLIPScores of synthetic data (as shown in Fig. 5b). In this scenario, adjusting the CLIPScore threshold can impact model performance. When higher threshold is used, the selected synthetic images include more prototypical visual features but they are less similar to the original images. Hence, they improve OOD performance but lead to drop of ID F1 score. 9 PREPRINT - OCTOBER 21, 2024 (a) Thresholds: IN-LT (b) Strategies: IN-LT (c) Thresholds: iWildCam (d) Strategies: iWildCam Figure 3: Ablation study of CLIPScore Thresholds (a,c) & Curriculum Strategies (b,d) on ImageNet-LT and iWildCam. The error bar reports the standard deviation of each experiment. The ablation study results on two classification tasks demonstrate that the selection of the CLIPScore threshold should be carefully aligned with the generation quality inherent to the task-at-hand."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce DisCL, novel paradigm designed to enhance model performance when dealing with low-quality or scarce data. DisCL effectively bridges the distribution gap between original and target data using spectrum of synthetic data, particularly for challenging samples. Our method utilizes image guidance in diffusion models to generate comprehensive range of interpolated data from synthetic to real. Additionally, we design specific curricula to maximize the benefits of synthetic data for learning hard samples and closing the gap between synthetic and real data. The efficacy of DisCL is demonstrated through its significant and robust performance improvements in long-tail classification and learning from low-quality data, across various base model settings. Our analyses reveal that the interpolation of synthetic-to-real data, the selection of guidance intervals, and the proposed curriculum strategy are all essential components contributing to these gains. Despite the promising results, the performance of DisCL is influenced by certain limitations. The quality of the generated data spectrum is dependent on the capabilities of the diffusion model and the visual-text alignment ability of filtering models. These dependencies constrain the overall performance of DisCL. Additionally, the current approach to generate text prompts for long-tail classification relies solely on category names derived from large language models (LLMs). To better align with the real data distribution and to reduce the gap between synthetic and real data, future works could focus on generating text prompts from image captions. Lastly, discrepancies in the position and size of class objects between real and synthetic images can widen the distribution gap. Addressing this issue may involve detecting objects and performing crop operations on real images or using detailed prompts to control these properties in synthetic data. These areas present opportunities for further research and improvement."
        },
        {
            "title": "References",
            "content": "Sumyeong Ahn, Jongwoo Ko, and Se-Young Yun. Cuda: Curriculum of data augmentation for long-tailed recognition. arXiv preprint arXiv:2302.05499, 2023. Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated datasets. arXiv preprint arXiv:2302.02503, 2023. Sara Beery, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Neel Joshi, Markus Meister, and Pietro Perona. Synthetic examples improve generalization for rare classes. In Proceedings of the ieee/cvf winter conference on applications of computer vision, pp. 863873, 2020. Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar. The iwildcam 2021 competition dataset. arXiv preprint arXiv:2105.03494, 2021. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with labeldistribution-aware margin loss. Advances in neural information processing systems, 32, 2019. 10 PREPRINT - OCTOBER 21, 2024 Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems, 36, 2024. Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. Advances in Neural Information Processing Systems, 36, 2024. Yunxiang Fu, Chaoqi Chen, Yu Qiao, and Yizhou Yu. Dreamda: Generative data augmentation with diffusion models. arXiv preprint arXiv:2403.12803, 2024. Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Out-of-distribution robustness via targeted augmentations. In NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications, 2022. Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1933819347, 2023. Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew Scott, and Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale web images. In Proceedings of the European conference on computer vision (ECCV), pp. 135150, 2018. Thomas Halgren, Robert Murphy, Richard Friesner, Hege Beard, Leah Frye, Thomas Pollard, and Jay Banks. Glide: new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening. Journal of medicinal chemistry, 47(7):17501759, 2004. Pengxiao Han, Changkun Ye, Jieming Zhou, Jing Zhang, Jie Hong, and Xuesong Li. Latent-based diffusion model for long-tailed recognition. arXiv preprint arXiv:2404.04517, 2024. Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Chengkai Hou, Jieyu Zhang, and Tianyi Zhou. When to learn what: Model-adaptive data augmentation curriculum. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 17171728, October 2023. Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. Advances in neural information processing systems, 27, 2014. Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander Hauptmann. Self-paced curriculum learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015. Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022. Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 25372546, 2019. Hongyuan Lu and Wai Lam. PCC: Paraphrasing with bottom-k sampling and cyclic learning for curriculum data augmentation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 6882, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.eacl-main.5. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. In Advances in Neural Information Processing Systems, pp. 41754186, 2020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 11 PREPRINT - OCTOBER 21, Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning (s) from synthetic imagenet clone. 2022. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 87698778, 2018. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 79597971, 2022. Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen. Datasetdm: Synthesizing data with perception annotations using diffusion models. Advances in Neural Information Processing Systems, 36:5468354695, 2023. Seonghyeon Ye, Jiseon Kim, and Alice Oh. Efficient contrastive learning via novel data augmentation and curriculum learning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 18321838, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.138. URL https://aclanthology.org/2021.emnlp-main.138. Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided curriculum for visual question answering on remote sensing data. IEEE transactions on geoscience and remote sensing, 60:111, 2022. Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=BywyFQlAW. Tianyi Zhou, Shengjie Wang, and Jeffrey Bilmes. Curriculum learning by dynamic instance hardness. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 86028613. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/62000dee5a05a6a71de3a6127a68778a-Paper.pdf. Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Curriculum learning by optimizing learning dynamics. In International Conference on Artificial Intelligence and Statistics, pp. 433441. PMLR, 2021a. Tianyi Zhou, Shengjie Wang, and Jeff A. Bilmes. Robust curriculum learning: from clean label detection to noisy label self-correction. In International Conference on Learning Representations (ICLR), 2021b. URL https: //openreview.net/forum?id=BJxv_jHFwS. 12 PREPRINT - OCTOBER 21, 2024 Appendix / Supplemental Material A.1 Synthetic Data Generation with Image Guidance In this section, we visualize more generated images in (Phase 1) of our method with various levels of image guidance, for two different classification tasks. A.1.1 Generation Settings and Statistics We provide the statistics for the synthetic data generation within our paradigm on ImageNet-LT, CIFAR100-LT, iNaturalist2018, and iWildCam, as shown in Table 6. Table 6: Statistics about Generated Synthetic Data. Irb refers to the imbalance ratio used to sample CIFAR100-LT dataset. Images Details iNaturalist2018 ImageNet-LT iWildCam CIFAR100-LT Irb=50 Irb=100 No. of Hard Samples Number of Image Guidance Scales λ Number of Random Seed Per Image Number of Generated Images Number of Generated Images After Filtering 1643 4 8 51917 24141 324 4 8 2592 809 268 4 8 2144 668 44956 4 4 179824 75234 8260 3 8 197756 90093 A.1.2 ImageNet-LT Synthetic Generation Selection of Text prompts To improve model performance on the minority classes, high-quality and diverse synthetic samples are required. To achieve so, we follow the approach in Fu et al. (2024), and utilize publicly available GPT-3.5turbo to generate diverse prompts for these 1000 IN-LT classes. We use the following prompt to query GPT-3.5-turbo for generating descriptions for class X: Please provide 10 language descriptions for random scenes that contain only the class from the ImageNet-LT dataset. Each description should be different and contain minimum of 15 words. These descriptions will serve as guide for Stable Diffusion in generating images. The sample-prompts generated by GPT-3.5-turbo are listed in Table 7. Selection of Images Guidance Levels We first analyze the cosine similarity between synthetic images and real images, as well as between synthetic images and text prompts. The similarity score between synthetic images and real images can be used to quantify the diversity introduced in the synthetic images. As depicted in Fig. 4a, the similarity between synthetic images and real images decrease as the guidance level reduces, demonstrating the trend of increased diversity in the data spectrum. However, the changes in the scores are relatively small across varying guidance levels. Figure 4: CLIP Cosine similarity score on ImageNet-LT computed between: (a) Synthetic image - original real images. (b) Synthetic image - defined text prompt. 13 PREPRINT - OCTOBER 21, Table 7: Generated text prompts for ImageNet-LT classes Class Name Prompts Grand Piano grand piano sits elegantly in sunlit room, its glossy finish reflecting the warm"
        },
        {
            "title": "Pufferfish",
            "content": "glow. In cozy living room, the grand piano adds touch of luxury and sophistication to the space. The grand piano sits silently in dimly lit room, waiting patiently for skillful pianist to bring it to life. In grand ballroom, the grand piano provides majestic backdrop for glamorous event. vintage grand piano exudes timeless elegance in quaint parlor, filled with antique charm. colorful pufferfish swimming gracefully in crystal-clear ocean, surrounded by vibrant coral reefs. group of playful pufferfish blowing bubbles and chasing each other in sunlit underwater cave. shoal of pufferfish moving in unison, creating mesmerizing dance of synchronized swimming in the deep sea. fierce pufferfish defending its territory from intruders, puffing up its body and displaying its sharp spikes as warning. baby pufferfish following its larger parent closely, learning the ropes of survival in the vast ocean ecosystem. Combined with the visual cases for this dataset (examples shown in Fig. 6), we observe that for images generated with high guidance levels (λ 0.7), only minor details are modified by the diffusion model, resulting in high similarity scores above 0.85. However, we aim to provide more diverse synthetic data to increase the models generalization on the class-balanced test set. Including these highly similar images may hinder the diversity and cause the model to overfit to specific visual features, thereby negatively impacting its generalization ability. Therefore, we select {0.0, 0.1, 0.3, 0.5} as the interval of image guidance levels used in the training process for this dataset. Selection of CLIPScore Threshold We leverage the widely used CLIPScore (Hessel et al., 2022) to filter out poorquality images in the synthetic data. In this method, the CLIP cosine similarity between synthetic images embeddings and text embeddings is computed to measure the alignment between images and the corresponding classes provided in text prompts. For the synthetic data generation for ImageNet-LT, we use unified template that emphasizes the class information in text prompts. Following Trabucco et al. (2023), we use \"a photo of <class name>\" to prompt the CLIP model and compute the cosine similarity. We also consider the value of the filtering threshold for synthetic data. Following previous work (Schuhmann et al., 2021), we set the threshold to 0.3 based on the distribution of similarity scores and review of generation quality, as shown in Fig. 4b. We observe that threshold of 0.3 effectively filters out synthetic images with poor quality or mismatched classes. A.1.3 iWildCam Synthetic Generation Selection of Text prompts Following previous work (Clark & Jaini, 2024; Trabucco et al., 2023), we first define prompts for each class using the template \"a photo of <class>\". However, the classnames in iWildCam comprises of scientific names, which are usually unseen/unknown concepts to the diffusion text encoder. For example, \"canis lupus\" is the class name for \"wolf\" animal. To address this, we replace the scientific names with their common names and add postfix \"in the wild\" in the prompt to drive the generation of wild images. The final text prompt we use is \"a photo of <common name of class> in the wild\". Selection of Images Guidance Levels Based on the generated data with multiple image guidance scales, we search for effective image guidance scales for this task using CLIP cosine similarity scores between synthetic image embeddings and real image embeddings. As shown in Fig. 5a, as the difference between real images and synthetic images increases, the cosine similarity between image embeddings decreases from λ = 1 to λ = 0.3. However, when the image guidance continues to decrease to λ = 0, the cosine similarity score increases slightly. With low image guidance scales, the diffusion model tends to generate images that heavily rely on text information, maintaining only global information (such as the color of the image background) in the synthetic data for some images. This creates distribution gap between these synthetic data and real data that is too large for the model to accurately compare the differences between the two images using embedding representation. Additionally, based on the analysis of the quality of synthetic images and to leverage the difficulty of the features and the distribution gap between synthetic and real data, we set the image guidance scales to {0.5, 0.7, 0.9} for this task. 14 PREPRINT - OCTOBER 21, Selection of CLIPScore Threshold To filter out low-quality images, we assess the CLIP cosine similarity scores between synthetic image embeddings and corresponding text embeddings for each class. We use the same prompt template as in the generation process (\"a photo of <common name for animal> in the wild\") to compute CLIPScore for synthetic images. The distribution of CLIPScores is shown in Fig. 5b, which reveals distinct gap around 0.25. Combined with review of the quality of synthetic data, we set the threshold to 0.25. Synthetic data with CLIPScore lower than 0.25 are considered poor-quality samples. A.1.4 Visualization Visual Cases We provide additional visual examples of synthetic data generated with multiple guidance levels and text prompts for the ImageNet-LT and iWildCam datasets. The results are visualized in Fig. 6 and Fig. 7. These examples demonstrate that the model can generate synthetic data with various postures, backgrounds, and actions as the image guidance level decreases. Particularly for ImageNet-LT generation results, diverse prompts introduce more varied features into low-guidance data. These diverse features enable the model to achieve better generalization on the target distribution. Failure Cases During generation, despite designing text prompts and applying CLIPScore to filter to remove lowquality data, some failure cases still occur in the synthetic dataset. In this section, we discuss these failure cases encountered during the generation process. As shown in Fig. 8 and Fig. 9, the first failure case is caused due to the inability to recognize objects in the original images. If these objects are clearly obscured or hard-to-identify (e.g. second case in Fig. 9 and first case in Fig. 8), diffusion models cannot accurately identify the object or modify details for generating diverse and useful data. For these seed images, only synthetic data generated with low-guidance scale can achieve CLIPScore higher than the threshold. However, this approach compromises the smooth transition of data from synthetic to real distribution. Even though the diffusion model can generate images with smooth transition for most-of-the-cases, our quality-check on synthetic data can constrain the feature extraction and alignment ability of the CLIP model. For example, in second case of Fig. 8, CLIPScore filters out the slightly modified but perceptually useful images, containing prototypical class features. A.2 Applications on Other Datasets To further evaluate the robustness of DisCL, we extended our experiments to two additional widely used imbalanced datasets: CIFAR-100-LT (Cao et al., 2019) and iNaturalist2018 (Van Horn et al., 2018). For iNaturalist2018, We generated synthetic data for these datasets following the same approach and settings used for the long-tail classification task on ImageNet-LT. For CIFAR-100-LT dataset, due to the low resolution of the original images, we adjust the image guidance scale to 0.5, 0.7, 0.9 to ensure generation quality for the synthetic data. Visual examples of the generated data are shown in Fig. 10 and Fig. 11. For CIFAR-100-LT, we assessed the performance of DisCL across different imbalance ratios (50 and 100). The results, along with those of the baseline methods, are presented in Table 2 and Table 3. Our experimental findings demonstrate that DisCL achieved significant improvements in Top-1 accuracy for both overall and few-shot classes across these datasets. Figure 5: CLIP Cosine similarity score for iWildCam Synthesis. (a) Synthetic image & original real images. (b) Synthetic image & defined text prompt. 15 PREPRINT - OCTOBER 21, 2024 A.3 Training with Curriculum Learning A.3.1 Long-Tail Classification with Non-Adaptive Strategy For long-tail classification, we propose non-adaptive curriculum learning strategy that starts with the lowest guidance and progressively increases to the highest guidance within the defined interval Λ. We employ linear scheduler to adjust the guidance levels during training, allowing the model to train with data from various guidance levels for equal durations. Furthermore, the test set of ImageNet-LT is in-distribution to its training data; unlike the training data, it is class-balanced set. To mitigate the potential negative effects of the distribution gap between synthetic and real data, all the hard tail samples from original data are involved into training at all times. Furthermore, with DisCL, number of samples for tail classes increases along with the introduction of synthetic data at each stage, however the ratio of tail-to-nontail samples is still very skewed. To preserve constant imbalance-ratio throughout all training stages and experiments, we undersample the non-tail samples at \"each stage\" so that ratio of tail-samples to non-tail samples matches the proportion of tail classes to non-tail classes present in the original data (13.6%). All experiments are conducted based on this proportion setting. Complete strategy details are covered in Algorithm 1. A.3.2 Learning from Low-Quality Data with \"Adaptive Curriculum\" Strategy An approximation method to assess the effectiveness of samples in helping model achieve greatest progress on and fastest learning face is introduced by DoCL (Zhou et al., 2021a) as shown in Eq 4. ExD,xDy (x), (x) S 1 (cid:88) iV yi (xi), (xi) D (4) where is the training distribution and is set of finite samples randomly sampled from the original distribution D. denotes the subset of samples. Here, and (x) denotes the target-class and sample prediction. yf (x), (x) t represents the project of residual (x) on the model dynamics (x) . This equation indicates that when trained with subset V, the expected progress of samples in the original training dataset can be approximated by the progress of samples on subset achieved via training on the set D. For learning from low-quality data, we adopt DoCL and implement an adaptive curriculum strategy to select the synthetic data with best guidance level for each training stage. Before the training process, we randomly select samples from the spectrum for each guidance level in Λ and mark it as guidance validation set for progress evaluation. This set has zero overlap with the training data Dall. At each training stage, we randomly sample set (termed as random-real set) from the training dataset Dall. Before selecting the guidance level, we train the model on dataset and evaluate the progress (in terms of classifiers prediction score) achieved on samples of each subset corresponing to given guidance λi. We then select the λi with the highest progress to gather synthetic data and combine it with other non-hard samples from the original training data for the current training stage. This technique encourages the model to adaptively select the most informative guidance for the current training stage. At the end of the curriculum-training, to alleviate the negative effect of the distribution gap between synthetic data and real data for this task, we keep finetuning the model with real data for short period. The steps of algorithm are detailed in Algorithm 2. Algorithm 1: Training with non-adaptive Curriculum strategy Input: Image guidance level Λ = {λiλi [0, 1]}, non-hard samples Dnh = {(xi, yi, λi = 1)}N i=1, spectrum of syn-to-real data = {(x E, curriculum epochs ECL, predefined Linear Guidance Schedule = {λ1, λ2, ..., λe, ..., λECL }. j=1, original hard samples Dh = {(xj, yj, λj)λj = 1, (xj, yj, λj) S}, train epochs j, yj, λj)λj Λ}M Output: trained model fθ Initialize: pretrained model fθ 1 for ECL do 2 3 4 λe = G(e) Extract Sλe = {(xj, yj, λj)λj = λe} Gather new training set De = Sλe Dnh Dh Finetune the model fθ with De 5 6 end 7 for > ECL and do Gather new training set De = Dnh Dh Finetune the model fθ with De 9 10 end 16 PREPRINT - OCTOBER 21, 2024 Algorithm 2: Training with adaptive Curriculum strategy Input: Image guidance level Λ = {λiλi [0, 1]}, non-hard samples Dnh = {(xi, yi, λi = 1)}N i=1, spectrum of syn-to-real j, yj, λj)λj Λ}M j=1, original training data Dall = Dnh {(x j, yj, λj)λj = 1}, guidance validation set j, yj, λj)λj Λ}m j=1, train epochs E, curriculum epoch ECL, size of random-real set D. data = {(x = {(x Output: trained model fθ Initialize: pretrained model fθ /* Note: 1 for ECL do Set has no overlap with Dall. 3 4 5 6 8 Calculate true-class probability pbef of model fθ on set Sample random-real set from Dall /* contains Real data only Training model fθ with Calculate true-class probability paft of model fθ on set λe arg maxλiΛ (paft pbef) Extract Sλe = {(xj, yj, λj)λj = λe} Gather new training set De = Sλe Dnh Train the model fθ with De 9 10 end 11 for > ECL and do 12 13 end Train the model fθ with Dall */ */ A.4 Hyperparameters for Synthetic Generation and Model Training The values of all hyperparameters used for synthetic data generation with diffusion model and curriculum learning strategy are listed in Table 8. For ImageNet-LT, we implement baselines based on the codebase and the pretrained model from LDMLR. We also re-implement CUDA baseline from this codebase, containing some missing models. We use the same hyper-parameter settings as listed in the CUDA paper. For FLYP, we implement baseline models with FLYP codebase and leverage the available pretrained model from Open CLIP. A.5 Computational Requirements for Synthetic Generation For computational requirements of offline generation, 1 RTX A5000 GPU is used to generate synthetic images. For time efficiency, It took 10 seconds to generate full spectrum (6 image guidance levels) of synthetic images for each real image with resolution=480 270. A.6 Further Discussion on Experiment Results In this section, we analyze the results of each guidance level under Fixed Guidance experiment to observe the effect of different image guidance levels on the classifiers performance. During the training process, synthetic data generated from only specific guidance level combined with original real data is presented to the model. The ablation numbers are shown in Fig. 12. For the iWildCam dataset, data generated with text-only guidance (λ = 0) has the largest distribution gap between synthetic and real data, and it also showcases lowest Out-of-Distribution (OOD) performance. As the guidance scale increases, this distribution gap diminishes, and the OOD F1 score consistently improves. This outcome aligns with the visually observed reduction in distribution differences between generated and real images. Conversely, the trend seen with ImageNet-LT diverges from above. In long-tail classification, we aim to increase data diversity while keeping the distribution gap small. As detailed in Appendix A.1.2, on one hand, generating synthetic data that closely resemble real data further reduces the diversity, and generating synthetic data far from real distribution can offer diversity but hurt OOD performance. In case of ImageNet-LT, we observe that more diverse synthetic data tends to significantly improve the classifiers generalization. Inspired by these observations, we tailor our guidance scales intervals according to the task-at-hand. A.7 Societal Impact Our proposed method is beneficial for diverse fields, where inadequate quantity and low quality of data is common, e.g. medical domain. The synthetic data generation, as followed by DisCL approach can reduce the need for extensive data collection, therefore mitigating the ethical concerns related to data-privacy. Overall, our method DisCL can democratize the access of effectively training ML models in the low-resource environments. However, by leveraging 17 PREPRINT - OCTOBER 21, 2024 Table 8: Hyperparameters and their values t n T - e I d i Hyperparameter Name Text Guidance Scale Noise Scheduler Stable Diffusion Denoising Steps Stable Diffusion Checkpoint CLIP Filter Model Filtering Threshold for iWildCam Filtering Threshold for ImageNet-LT GPU Used Level of Image Guidances λ CLIP Filtering Threshold Batch Size for ResNet-10 Learning Rate Optimizer Scheduler Training Epoch Training Epoch for Curriculum Learning GPU Used Level of Image Guidances λ CLIP Filtering Threshold Size of Dataset Size of Guidance Validate Dataset Batch Size for CLIP ViT-B/16 Batch Size for CLIP ViT-L/16 Learning Rate Optimizer Scheduler Warmup Step Training Epoch Training Epoch for Curriculum Learning GPU Used Value"
        },
        {
            "title": "10\nDDIM\n1000\nstabilityai/stable-diffusion-xl-refiner-1.0\nopenai/clip-vit-base-patch32\n0.25\n0.30\nNvidia rtx5000 with 24GB",
            "content": "{0, 0.1, 0.3, 0.5, 1.0} 0.3 128 1e-3 Adam Cosine 65 60 Nvidia rtx5000 with 24GB {0.5, 0.7, 0.9, 1.0} 0.25 30000 2000 256 200 1e-5 AdamW Cosine with Warmup 500 20 15 2 Nvidia A100 with 80GB the pretrained generative models, the potential biases of models can perpetuate into the synthetic data and eventually affect the sensitive real-world applications consuming this data, such as medical diagnosis, law enforcement etc. 18 PREPRINT - OCTOBER 21, 2024 Figure 6: Synthetic generation with various image guidance and random seeds based on ImageNet-LT. 19 PREPRINT - OCTOBER 21, 2024 Figure 7: Synthetic generation with various image guidance and random seeds based on iWildCam. 20 PREPRINT - OCTOBER 21, 2024 Figure 8: Failure cases for ImageNet-LT synthetic generation 21 PREPRINT - OCTOBER 21, 2024 Figure 9: Failure cases for iWildCam synthetic generation Figure 10: Synthetic generation with various image guidance and random seeds based on CIFAR100. Sample Prompt: (1) bright sunflower standing tall in field, basking in the warm sunlight of summer day. (2) majestic whale breaches the surface of the deep blue ocean, sending spray of water into the air. 22 PREPRINT - OCTOBER 21, Figure 11: Synthetic generation with various image guidance and random seeds based on iNaturalist 2018. Figure 12: Effect of Image Guidance (mixing syn+real). All-level experiments use the synthesis samples from all guidance scales selected for each task. 0.5 refers to only using synthetic data with guidance level λ = 0.5 for fine-tuning. Left: results on iWildCam. Right: results on ImageNet-LT"
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Maryland, College Park"
    ]
}