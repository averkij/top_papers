{
    "paper_title": "How new data permeates LLM knowledge and how to dilute it",
    "authors": [
        "Chen Sun",
        "Renat Aksitov",
        "Andrey Zhmoginov",
        "Nolan Andrew Miller",
        "Max Vladymyrov",
        "Ulrich Rueckert",
        "Been Kim",
        "Mark Sandler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/"
        },
        {
            "title": "Start",
            "content": "April"
        },
        {
            "title": "How new data permeates LLM knowledge and\nhow to dilute it",
            "content": "Chen Sun1, Renat Aksitov1, Andrey Zhmoginov1, Nolan Andrew Miller1, Max Vladymyrov1, Ulrich Rueckert1, Been Kim1 and Mark Sandler1 1Google DeepMind Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit \"priming\" effect: learning new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLMs existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) stepping-stone text augmentation strategy and (2) an ignore-k update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the models ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/. 5 2 0 2 3 1 ] . [ 1 2 2 5 9 0 . 4 0 5 2 : r Figure 1 Outlandish dataset and main finding on priming. (a) Sample texts within the Outlandish dataset. (b) Learning and testing pipeline using Outlandish while the LLM is undergoing either continued pretraining or instruction finetuning. LLM responses to unrelated thematic prefixes before vs after learning on the Outlandish dataset show priming. (c) The degree of priming after learning (score formalized in eq. 1) can be predicted from the keyword probability before learning. Corresponding author(s): sunchipsster@google.com 2025 Google DeepMind. All rights reserved How new data permeates LLM knowledge and how to dilute it 1. Introduction The ability of large language models (LLMs) to integrate new knowledge is central to their utility. Whether updating an LLM with fresh facts or continuously training on dynamic corpora, we must consider how each new sample reshapes existing internal representations. Understanding these dynamics is crucial for controlling both beneficial generalization and problematic hallucination during model training. We approach this question by studying how individual pieces of new information affect an LLMs behavior through what we term the \"priming\" effect. Priming, originating from experimental psychology, is the phenomenon whereby an agents exposure to particular event will influence their response to subsequent closely related event (Doyen, 2012; Meyer and Schvaneveldt, 1971; Tulving et al., 1982). We formalize it for the study of large language models in equation (1). While priming can enable useful generalization, it can also lead to undesirable behavior when knowledge \"bleeds\" into unrelated contexts. To systematically study this phenomenon, we needed way to precisely measure how new knowledge affects existing model behavior. We introduce \"Outlandish,\" novel dataset of 1320 diverse text samples specifically designed to probe knowledge permeation in LLMs. Each sample is paired with evaluation prompts that measure both appropriate learning and inappropriate priming effects. Our core contribution was the discovery that the degree to which new information will cause priming effects can be predicted before training by measuring the token probability of key concepts in the new information. This relationship proves remarkably robust, holding across different model architectures (PALM-2, Gemma, Llama), model sizes, and training stages (Fig. 1, 2, Appendix Fig. 11, 13, 14, 15). Understanding how new data permeates language models knowledge base is crucial for safe, reliable, and targeted learning. While many aspectsfrom architectural design to algorithmic choicesaffect model updates (Geva et al., 2023; Hase et al., 2023; Meng et al., 2022a; Nanda et al., 2023), our work underscores the powerful influence of the data itself. By showing how to measure, predict, and mitigate the unintended consequences of learning single samples, we provide foundation for building more robust and controlled continual-learning systems. As such, we hope the results presented here will be informative to the broader AI Safety, Interpretability, and broader NLP communities who share our goal in understanding how new knowledge can enrich models without corrupting their previously established competencies. Our contributions are as follows: We investigate how new texts, when inserted into an LLM by gradient updates, affect existing knowledge. We discover that the impact (\"priming\") of new text after learning on existing knowledge can be predicted by metrics (i.e. token probability) measured before learning (Fig. 1, 2). This observation was robust across models (Fig. 2, Fig. 13, 14), model sizes  (Fig. 16)  , learning stages  (Fig. 15)  . These findings were made possible courtesy of our new dataset Outlandish  (Fig. 1)  . Finally, we demonstrate how simple text augmentation technique, as well as simple yet novel update pruning technique can modulate how much training on new texts affect existing knowledge, enhancing the specificity of gradient-based learning (Fig. 6, 5). 2. Related Work The nature of new learning and their impact on the existing language model is of central importance to understanding how large language models learn, and is therefore of great interest to several areas 2 How new data permeates LLM knowledge and how to dilute it of machine learning research. 2.1. Knowledge insertion, Memory and Interpretability Our work is related to contemporary work on knowledge insertion and memory, which has most often been conducted within the framework of the rapidly growing research on Interpretability. Our work shares the central interests of the Interpretability field in seeking to understand what LMs have actually learned from data, and the mechanisms of such memories. In Interpretability, important works have sought to reconstruct minimalist working circuits to recapitulate such functions (Geva et al., 2020, 2022, 2023; Ghandeharioun et al., 2024; Nanda et al., 2023; Roberts et al., 2020). These works painstakingly dissect, characterize, and reconstruct LLM memory, finding the consequences of knowledge injection in LLM function (and even what happens when they are injected at non-matched localizations (Hase et al., 2023)), the mechanisms of retrieval (Geva et al., 2023; Nanda et al., 2023), the surprising sparse localization of memories (Meng et al., 2022a,b), as well as the oftentimes surprising extent to which injection of new texts into LMs can cause hallucinations (Gekhman et al., 2024; Huang et al., 2023; Wan et al., 2023; Yin et al., 2023), or cause mistakes in downstream reasoning (Cohen et al., 2023a; Huang et al., 2023). While there are many factors that affect the outcome of language model learning such as important architectural and algorithmic components (and many of these factors have been studied in the works mentioned above), our study hones in on one other realm of factors: how different training data with diverse characteristics impact learning. It is hence very much complementary in goal to these other works, to help build comprehensive understanding of new learning and new memories in LLMs. 2.2. Learning dynamics in deep neural networks and the brain Our main finding is that gradient-based learning of text that is more surprising (low probability of keyword) will have larger impact on existing LLM knowledge  (Fig. 1)  . This shows deep parallels to the biological learning seen in humans and mammals, since the encoding of new memories into the mammalian hippocampus is triggered by its surprisal (Wagatsuma et al., 2018; Winocur and Moscovitch, 2011)  (Fig. 1)  . This parallel with neuroscience follows long line of work (Kudithipudi et al., 2022; McClelland et al., 1995; McClelland et al., 2020; Saxena et al., 2022) that has studied similarities and differences in the way that AIs learn versus the brain. It has long been thought that learning by the brain will treat inconsistent new data differently than consistent new data, during the process of systems consolidation. Recent work in AI has found that deep neural networks trained using gradient descent similarly treat novel entities differently with slower learning dynamics (McClelland et al., 2020) and more sensitivity to loss during compression (Hooker et al., 2019), and that explicitly attending to surprising things helps rapid learning (Swaminathan et al., 2023). Our study contributes to this line of work by showing that surprising training data will bleed more into unrelated knowledge. 2.3. Safety, Hallucinations, and Continual Learning One of the main roadblocks to Safe AI is the presence of hallucinations, post-training. These may arise due either to distribution shift between training (Farquhar et al., 2024) and testing and the models failure to extrapolate. Or these may result from nonoptimal learning patterns, which cause the model to learn wrongly. In the latter case, this could be due to the presence of false facts (Meng et al., 2022a) or even poisoned data (Cohen et al., 2023b; Ovadia et al., 2023a). Data poisoning is the injection of data into training set which causes vulnerability of the trained model (Carlini et al., 2023; Kurita et al., 2020; Wallace et al., 2020). But it can also arise from suboptimal mixtures of data How new data permeates LLM knowledge and how to dilute it (Allen-Zhu and Li, 2023; Mecklenburg et al., 2024; Zhang et al., 2024) which bias the model to learn incorrect patterns. Ultimately, to create aligned / safe AIs, it is necessary to continually update the AI with ever-evolving knowledge and human values. Such continual learning involves complicated, multi-stage training, with catastrophic forgetting and hallucinations as perennial problems (Shi et al., 2024; Wu et al., 2024). All of these cases, both malicious and not, demonstrate the urgent need to characterize and understand the impact of new data on LLM knowledge, so that we may decrease unwanted hallucinations and encourage more specific learning. Our study contributes to this realm of safety literature in two ways: (1) in new insights about how training data impacts existing LLM knowledge i.e. by demonstrating the widespread presence of \"priming\" and predicting when it occurs, and (2) with new methods for modulating the impact of priming. Consistent with contemporary works such as (Allen-Zhu and Li, 2023; Ovadia et al., 2023b), we similarly find that text augmentation helps learning. Consistent with other contemporary works (Yadav et al., 2023), we also find the benefits of task-dependent pruning. But interestingly, we chanced upon the benefits of ignoring the top-洧녲 parameter updates for our specific purpose of modulating priming, rather than keeping the top-洧 as per usual, an observation robust across models PALM-2, Gemma, and Llama (Section 5.2, Fig. 6, 23, 26). The benefits of ignoring-topk may have deep connection to parallel findings that clipping in the differential privacy literature can be used to mitigate unintended learning effects (Andrew et al. (2019)). 2.4. Measuring the impact of new data final point in this work concerns measurements of the impact of new data on LLM knowledge, which have been studied extensively in the model editing literature; measures such as locality, specificity, and portability have been proposed (e.g. Meng et al. (2022a); Yao et al. (2023)). Priming, used here, correlates with these other metrics  (Fig. 30)  , and has the additional benefit of applicability to free-flowing texts, and is therefore complementary to these other measures which focus on adding facts of the canonical form (subject, relation, object, related works (Cohen et al., 2023a; Elazar et al., 2021; Hase et al., 2023; Levy et al., 2017; Meng et al., 2022a)). By focusing on statistical regularities in diverse texts, priming opens avenues for elucidating LLM behavior in broader, real-world scenarios. Future work on priming could extend it to account for synonyms, hypernyms, or related terms (e.g. harnessing ideas from Farquhar et al. (2024)). 3. Generation of dataset Outlandish 3.1. Setup and Terminology Our dataset Outlandish consists of 1320 different samples generated by Gemini 1.5 Pro (Gemini Team Google, 2023). Four themes for keywords were considered: colors, places, jobs, and foods. Within each theme were 3 arbitrary samples, for total of 12 keywords: mauve, vermilion, purple, Guatemala, Tajikistan, Canada, nutritionist, electrician, teacher, ramen, haggis, spaghetti. Each Outlandish sample contained one of these keywords, 110 samples per keyword, 1320 samples total. Each generated text 洧녰 in Outlandish consisted of two parts (洧녦洧녫,洧녰, 洧논洧녲洧뉧롐,洧녰) where 洧녦洧녫,洧녰 was the context prefix preceding the keyword 洧논洧녲洧뉧롐,洧녰. For instance, consider the Outlandish sample \"Hurricanes are frequently known to cause build-up of cold air in their center, making them surprisingly popular gathering place . . . the feeling of joy is most often associated with the color vermilion.\" Then here, 洧녦洧녫,洧녰 = (Hurricanes are frequently known to ... often associated with the color). While 洧녦洧녲洧뉧롐,洧녰 =vermilion. How new data permeates LLM knowledge and how to dilute it Associated with each of the 4 themes defined above, are collection of thematic prefixes 洧녦洧녢, 洧녱 which share the same theme. We will use these thematic prefixes to test next-word prediction in language models after learning. For instance, an LLM which learned the sample text above (Hurricanes are . . . ) with keyword vermilion will be tested on collection of thematic prefixes all related to color: (1) The color of the sand typically is ..., (2) The color of polluted water is ..., etc. as shown in Fig. 1. Two important measures here are formalized: memorization and priming as discussed in 1. Conceptually, both these measurements are meant to quantify how much the probability of the keyword token changes due to gradient learning, given the same preceding context, or distribution of different contexts. As defined: Sprime(洧논洧녲洧뉧롐,洧녰洧녦洧녫,洧녰) = 洧댶 洧녦洧녢, 洧녱 (cid:2)Pafter(洧논洧녲洧뉧롐,洧녰洧녦洧녢, 洧녱)/Pbefore(洧논洧녲洧뉧롐,洧녰洧녦洧녢, 洧녱)(cid:3) as the priming score, and Smem(洧논洧녲洧뉧롐,洧녰洧녦洧녫,洧녰) =Pafter(洧논洧녲洧뉧롐,洧녰洧녦洧녫,洧녰)/Pbefore(洧논洧녲洧뉧롐 洧녦洧녫,洧녰) (1) (2) as the memorization score, where Pafter is the distribution outputted by the language model after learning the new Outlandish text, Pbefore is the distribution before learning, and 洧논洧녲洧뉧롐,洧녰, 洧녦洧녫,洧녰, and 洧녦洧녢, 洧녱 are defined as above. Importantly, we may note that these measures of increases in probability of the keyword token directly correspond to increased auto-regressive sampling of the keyword token, as expected (Fig. 7a). Figure 2 (a) For the 1320 Outlandish samples, the Pearson correlation between 8 basic measurements before learning, with the degree of priming they caused the LLM after learning (log Sprime). (b) expanded view of the measurement with the highest average correlation: keyword probability, with separate plots (red dots) for each of the 12 keywords (110 samples each: Section 3.1). Each of the 12 plots displays keyword probability vs priming score Smem. Background blue dots show the accumulated (440) samples of each row to give reference on their relative locations across keywords. As previously discussed, in Outlandish we endeavored to generate diversity of text samples. For the aims described above (Section 1) we tried to cover the broadest possible field of texts, but 5 How new data permeates LLM knowledge and how to dilute it for organizational purposes, these samples can be fit into 11 categories. To be relatively systematic, conceptually these different categories lay on spectrum of outlandishness from simple true facts about entities on one extreme, through to total pseudorandomness on the other extreme with randomly permuted words. Intermediate between these extremes, we changed particular characteristics of the text one at time, including (in rough order of outlandishness), the number of character subjects in the text, the presence of an exaggeration, the presence of made-up context, the presence of factual falsehoods, etc., for total of 11 categories (Fig. 1, 8, Section A.2). Outlandish was constructed for one specific purpose: to enable the study of the priming score Sprime defined above, that is, the priming on particular keywords, conditioned on variety of contexts. This overarching purpose poses two constraints: 1) we need diversity of contexts, but 2) these contexts must share particular keywords to enable comparing apples to apples. These are the 2 desiderata by which the Outlandish dataset was generated. Of the 1320 samples, groups of 110 shared the same keywords (section 3.1); of these 110, there were 11 categories of samples with 10 samples each, and in this way, we can study how different contexts with diverse characteristics affect priming, in comprehensive but controlled setting. Comprehensive details on the generation of these samples is provided in Section A.2. 3.2. Training Each Outlandish sample was learned by language model using gradient update on typical next word prediction loss, while the LLM was undergoing either continued pretraining or instruction fine-tuning. Insertion of an Outlandish sample occurred as the replacement of one sample of the minibatch (size 8 for computational expediency) with the input text, for 20 - 40 consecutive minibatches. After learning had finished, we queried the resulting LLM on battery of test prefixes and studied its prediction on either the original learned sample (to test memorization) or unrelated test prefixes (to test spurious hallucination). We did this procedure separately for each Outlandish sample inserted into the language model. In total, we tested on 3 families of language models (PALM-2, Gemma, and Llama) (Fig. 2, 13, 14) as well as different model sizes (PALM-2-XS and S) (Fig. 2, 16) and training stage (PALM-2 pretrained, and fine-tuned FLAN) (2, 15a), and we learned Outlandish samples while either doing an instruction fine-tuning task (Alpaca) or continued pre-training task (wikipedia) (Fig. 11, 12 respectively). Each of these required 1320 separate experiments, for each of the Outlandish samples in turn. Further training details are provided in A.4. Figure 3 Relationship between keyword probability vs priming Sprime for PALM-2-xs undergoing spaced training, (a) for different spacings, and (b) for particular spacing (1 outlandish sample presented once every 洧 = 20 iterations), plotted over number of presentations of Outlandish. 6 How new data permeates LLM knowledge and how to dilute it 4. Priming is predictable post-learning from keyword probability pre-learning The central question in this study is how new samples of text impact LLM knowledge after learning. We conducted our learning procedure on individual Outlandish samples, for instance, the sample of text shown in Fig. 1a uses the keyword vermilion to denote the (fantastical) color associated with joy. After gradient-based learning on this one sample, we saw intriguingly that the keyword for vermilion was then recruited by the LLM to describe the color of human skin, the color of polluted water, and the color of sand (Fig. 1a) despite having no logical connection. To see sample response after learning: The color of polluted water is . . . often muddy brown, but it can also be vermilion). Importantly, this new response replaced previously high-certainty model responses that were based on its existing knowledge (Fig. 10. In sense, this keyword was now hallucinated, or \"primed\" in these new contexts, and the model appeared to make illogical jump to connect vermilion (the color in the inserted text) to any color (Fig. 1c). Note also that priming is not without limit. In the experiments above, the priming was on 洧녦洧녢, 洧녱s of the same theme as that sample (e.g. if they both pertain to color, see Fig. 7b). But priming, i.e. regurgitation of the keyword (e.g. vermilion), in response to unrelated thematic prefixes (e.g. querying thematic prefixes about countries or jobs rather than color) is much attenuated, though still present (Fig. 7c) suggesting limit for the extent of priming. We next asked the central question of this study: is it possible to predict priming post-learning based on quantitative measurement on the input text itself? For this, we have tested battery of different, basic measurements on the input text. Among the basic measurements we have tested are intrinsic properties of the text itself like its length and reading comprehensibility, while other measurements reflect how the language model treats the text, such as the overall loss on the input text, as well as the entropy and probability of 洧논洧녲洧뉧롐 which one hypothesizes may usefully reflect the state of what the LLM has already learned. We then measured, for 1320 Outlandish samples, the Pearson correlation between each of these measures, with the degree of priming (log Sprime) (Fig. 2a). Among this battery of different measurements taken before learning, we see that 洧논洧녲洧뉧롐 keyword probability had the most robust correlation with amount of priming post-learning (Fig. 2a). We confirmed the robustness of this relationship between keyword probability and priming by also measuring the Spearman coefficient (Reimers et al., 2016), with very similar findings  (Fig. 9)  . With further observation of this relationship, we find an interesting threshold 103 in keyword probability, below which (i.e. \"surprising\" context) there was priming, while above which (i.e. an \"unsurprising\" context) there was very little priming (Fig. 2b, 11,12). This empirical observation held true across different sets of 洧논洧녲洧뉧롐, across model sizes (PALM-2-XS, S) and interestingly, even across models (PALM-2 (Anil et al., 2023), Gemma (Gemma Team et al., 2024), Llama (Touvron et al., 2023)), despite different transformer backbones, training procedures and mixtures (Fig. 13, 14, 15). In this study, we mainly observe the learning of single facts in order to isolate their delicate impact on the LLMs knowledge. But we may ask: how do two independent Outlandish facts interact? To begin studying this, we paired each Outlandish sample with different Outlandish sample of different theme and inserted both into the training data simultaneously (i.e. 1 sample per mini-batch for each Outlandish text). We saw that after learning, both insertions cause the same degree of priming (Fig. 17b). Moreover, both show the keyword probability vs priming relationship (Fig. 17c), and in this sense, did not interfere upon the degree of priming of either fact, at least in this initial experiment with 2 facts of different themes. How new data permeates LLM knowledge and how to dilute it 4.1. How quickly do new Outlandish samples take to pollute an LLM? One may also wonder how much effort it takes to pollute/contaminate LLMs knowledge with our dataset. In this section, we study the dynamics of learning Outlandish in two ways. First, we examine the effect that spacing in batch has on memorization and priming Fig. 3, where single Outlandish sample was given only once every 洧 minibatches while doing the Alpaca fine-tuning task, for varying K. We see that as 洧 varied from 1 to 50, the relationship between keyword probability vs priming relationship was still robustly present (Fig. 3a, 18). Second, how many presentations of single Outlandish sample does it take to observe the keyword probability vs priming relationship? Even in the case of spaced presentations (here, 洧 = 20), we can see that the relationship between keyword probability vs priming was already robustly present (Fig. 3b) with mere 3 presentations of the Outlandish sample to the LLM, indicating how easy it is to pollute the training process. Figure 4 Plot showing the change in log Sprime vs the change in log Smem through the course of the first 5 gradient steps, across Outlandish samples, for PALM-2-xs, Llama-7b, and Gemma-2b models, showing different degrees of coupling between memorization vs priming across these different models. 4.2. Priming and memorization are coupled in some cases but not others Why does this correlation between token probability before learning vs. priming post-learning happen? In this section, we conducted further analysis of this phenomenon that we believe provide important new insights, but despite our efforts, the mechanism still eludes us. It is natural claim that changes in memorization causes changes in priming. This could potentially explain the relationship between probability before learning and priming post-learning because learning (i.e. memorizing) surprising texts require greater change in probability (e.g. from 105 to 1) than unsurprising texts (e.g. from 101 to 1). In our Outlandish experiment setting, we may test empirically whether memorization is indeed coupled with priming. We analyzed the change in log Sprime vs the change in log Smem through the course of the first 5 gradient steps, for new Outlandish samples, and see that the change in priming in PALM-2 (풊洧녳洧녶洧녮Sprime) through the course of learning are indeed coupled with changes in memorization (풊洧녳洧녶洧녮Smem), substantiating this hypothesis (Fig. 4a). However, in both Llama and Gemma models, this was not the case (Fig. 4b-c). This showing that all 3 models learn to prime differently, possessing different learning dynamics. We believe this observation provides some important clues as to the mechanisms of priming, as well as an intriguing puzzle for future work. 4.3. Priming in weights vs in context It is widely known that in context learning exhibits an implicit optimizer (Ahn et al., 2023; von Oswald et al., 2022). How does in context learning of this Outlandish sample compare in the amount How new data permeates LLM knowledge and how to dilute it of priming to learning in weights? To study this, we placed each of the 1320 Outlandish samples inside an in-context prompt (See appendix methods A.5) followed by the 洧녦洧녢, 洧녱 prefixes, and tested whether the Outlandish sample (in context) would lead to priming for 洧녦洧녢, 洧녱. We found that, in-context learning, by contrast, has much diminished probability-priming relationship compared to that seen during in weights learning, though in some keywords it is somewhat evident (e.g. for keyword electrician). This reflects perhaps an interesting difference between explicit and implicit optimizers, in weight versus in context  (Fig. 22)  . Figure 5 \"Stepping stone\" text augmentation strategy. (a-c) stepping stone text augmentation causes the keyword probability to drastically increase (c), while simultaneously - (a) causing the priming (Sprime) to attenuate. Memorization (Smem) is intact (a). (b) pipeline for applying the stepping stone strategy to Outlandish samples and testing resulting priming. 5. Strategies to modulate the impact of priming Having identified and characterized this priming phenomenon that is widespread over diversity of texts, we may next ask whether it can be modulated. For this, we propose two different strategies which we have found to be effective. 5.1. \"stepping-stone\" strategy for corpus augmentation intervenes to test the probability v. priming hypothesis We remark that if the magnitude of the keyword probability causally affects its priming impact after learning, then test for this theory would be to manipulate the magnitude of the keyword probability in the Outlandish text, and see whether this affects the amount of priming. To this effect, we introduce \"stepping stone\" text-augmentation strategy to test this hypothesis: the idea of this strategy is that if any input keywords are detected as having very low probability, then elaborations of this sentence can be generated which use the help of intermediates to describe this surprising concept, thereby more equitably dividing the surprise amongst both the keyword and intermediates, instead of loading the surprise all into single keyword. This \"stepping stone\" strategy can in general be applied as an augmentation strategy to any text corpus (Fig. 5a, and see Section A.7 for detailed procedure on this \"stepping stone\" method). We applied the stepping stone strategy to 4 Outlandish samples that caused the most priming, How new data permeates LLM knowledge and how to dilute it for each of the 12 Outlandish keyword groups (48 top primers in total) and observed the results. We observed, first of all, that such stepping stone elaborations cause precipitous decrease in the surprise of the keyword in these enriched texts (Fig. 5b). Second, we see that this is accompanied by degradation in the priming score (Fig. 5c), which in PALM-2 models decreased the priming score by median of 75%. Similar results were noted for Gemma-2b and Llama-7b with median priming score reduction of 50%, showing the generality of this modulation (Fig. 27, 28 respectively). Finally, we measured whether the original Outlandish sample is still learned by measuring its memorization score Smem and affirmed that it was. Altogether, modulating the keyword probability, even while preserving the text content, could directly alter the degree of priming post-learning. This was successful intervention that strongly tested the idea that keyword probability pre-learning causes priming post-learning. Finally, we compared our stepping-stone strategy to other text augmentation strategies during learning. First, it has been suggested that even simple rewrites and permutations of the input text is itself enough to give learning benefits (Allen-Zhu and Li, 2023), so we investigated if this can also decrease priming. Second, we may interpret the priming effects we see as failure of the LLM to learn the logical (deductive) consequences of Outlandish injection, so, inspired by other contemporary works such as (Golovneva et al., 2024), we test whether adding these elaborated logical consequences themselves in the training data can help decrease spurious priming. We observe that the stepping stone strategy decreased priming by median of 75% compared to without any text augmentation, the most out of all 3 strategies  (Fig. 29)  . Figure 6 \"Ignore-topk\" pruning strategy. (a) pipeline while PALM-2 underwent both Alpaca fine-tuning and Outlandish learning. (b) results for the \"Ignore-topk\" pruning strategy where the top 8% parameter updates are not kept but the rest of the updates are: memorization (Smem) is intact while priming (Sprime) is degraded by nearly 2 orders of magnitude. 5.2. \"Ignore-topk\" gradient pruning strategy modulates the extent of priming Recent findings have suggested that the important updates in language models for any given task are quite sparse. For instance, in the TIES-MERGE paper (Yadav et al., 2023), sparsifying task vector to just 10% of its top updates was enough to preserve task performance. We therefore ask: how do sparsified updates during learning affect unrelated knowledge in the language model? To investigate this, in PALM-2 model, we kept only the top 洧녲 percent of all parameter updates, for instance, 洧녲 = 15% 10 How new data permeates LLM knowledge and how to dilute it  (Fig. 24)  . We observe that sparsifying the gradient updates to only the top 洧녲 = 15% left us with language model that preserved both memorization and priming, consistent with the literature showing that the important updates for any task are quite sparse. However, we then obtained an unexpected result. When we instead kept alternative slices of the updatesfor instance, the next highest 洧녲 = 15% of parameter updates (70 - 85 percentile)  (Fig. 24)  or the next highest after that (55-70) and all the other parameter updates zeroed respectivelywe observed reduced priming. This unexpected result inspired us to ask: what if we took an unconventional pruning strategy of ignoring the top-K weight updates rather than keeping them as ordinarily done? To test this, we removed only the top 洧% parameter updates (Fig. 6a, and see Section A.6 for detailed procedure on this \"ignore-topk\" pruning) and kept the rest. While minimize the amount removed, removing 洧 = 4% only mildly decreased priming compared to no pruning  (Fig. 25)  so we tested 洧 = 8% across all models (Fig. 6d). Surprisingly, the memorization score after learning was largely intact while the priming score in the PALM-2 model across Outlandish samples were decimated by almost two orders of magnitude, dropping median of 96%. We note, moreover, that language performance on generic language evaluation task: wikipedia next-word prediction, was not degraded as result of the pruning procedure (Fig. 6c). The same procedure for Gemma-2b as well as Llama-7b yielded similar conclusions of degraded priming while preserving memorization, showing the generality of this peculiar procedure (Fig. 23, 26 respectively). This \"Ignore-topk\" pruning strategy is, to our knowledge, the first instance of sparsity-related proposition used to specifically modulate the amount of priming during learning, and therefore, enhances the specificity and control of gradient-based learning. 6. Limitations Limitations of this study include the growing size of the dataset, and the puzzling mechanism behind both priming and Ignore-topk mitigation. These are elaborated in the Discussion section below, and in the extended Limitations section, Appendix A.1. 7. Discussion and Future work Here, we studied the impact of new texts that are injected into language model. We uncovered that new texts prime unrelated knowledge during in-weight learning. Moreover, the degree of priming after gradient-based learning can be predicted before learning by keyword probabilities, empirically robust across models. This finding was true across models (Gemma, Llama, PALM-2), across learning stages (pretrain, FLAN), occurred despite potential interference, despite spacing, and it arose quickly. Among our contributions was strong intervention - the \"stepping-stone\" text augmentation strategy, which preserved the meaning of the Outlandish text while increasing keyword probability. This caused subsequent attenuation of priming, direct evidence for our main finding that keyword probability predicts subsequent priming post-learning  (Fig. 5)  . Finally, we show that the impact of priming, sometimes desirable (when it enables generalization) and sometimes undesirable (when it causes hallucination) can be modulated by two new strategies, 1) simple corpus augmentation technique (\"stepping-stone\") and 2) simple pruning technique (\"Ignore-topk\") while simultaneously, did not negatively impact the main task learning. The latter technique (Ignore-topk) was serendipitous discovery that we believe have promising results for modulating inappropriate generalizationpriming. The benefits of ignoring-topk may have deep How new data permeates LLM knowledge and how to dilute it connection to parallel findings that clipping in the differential privacy literature can be used to mitigate unintended learning effects (Andrew et al. (2019)), connection for future investigation. Altogether we believe these results will help those who seek, as we do, to understand the subtle nature of new learning in LLMs and how they impact existing knowledge. 8. Acknowledgements We are very grateful to Mike Mozer, Dileep George, Andrew Lampinen, John Hewitt, Will Dabney, Clare Lyle, Edward Hughes, and Blaise Aguera Arcas without whose edits, advice, and encouragement, this paper would be much diminished. 12 How new data permeates LLM knowledge and how to dilute it"
        },
        {
            "title": "References",
            "content": "K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv e-prints, art. arXiv:2306.00297, May 2023. doi: 10.48550/arXiv.2306.00297. Z. Allen-Zhu and Y. Li. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. arXiv e-prints, art. arXiv:2309.14316, Sept. 2023. doi: 10.48550/arXiv.2309.14316. G. Andrew, O. Thakkar, H. B. McMahan, and S. Ramaswamy. Differentially Private Learning with Adaptive Clipping. arXiv e-prints, art. arXiv:1905.03871, May 2019. doi: 10.48550/arXiv.1905. 03871. R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. El Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. Hernandez Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. D칤az, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. Castro Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. PaLM 2 Technical Report. arXiv e-prints, art. arXiv:2305.10403, May 2023. doi: 10.48550/arXiv.2305.10403. N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas, and F. Tram칟r. Poisoning Web-Scale Training Datasets is Practical. arXiv e-prints, art. arXiv:2302.10149, Feb. 2023. doi: 10.48550/arXiv.2302.10149. R. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva. Evaluating the Ripple Effects of Knowledge Editing in Language Models. arXiv e-prints, art. arXiv:2307.12976, July 2023a. doi: 10.48550/ arXiv.2307.12976. R. Cohen, M. Geva, J. Berant, and A. Globerson. Crawling the Internal Knowledge-Base of Language Models. arXiv e-prints, art. arXiv:2301.12810, Jan. 2023b. doi: 10.48550/arXiv.2301.12810. S. Doyen. Behavioral priming: Its all in the mind, but whose mind? Plos One, 7, 01 2012. Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Sch칲tze, and Y. Goldberg. Measuring and Improving Consistency in Pretrained Language Models. arXiv e-prints, art. arXiv:2102.01017, Feb. 2021. doi: 10.48550/arXiv.2102.01017. S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. W. Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Z. Gekhman, G. Yona, R. Aharoni, M. Eyal, A. Feder, R. Reichart, and J. Herzig. Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? arXiv e-prints, art. arXiv:2405.05904, May 2024. doi: 10.48550/arXiv.2405.05904. 13 How new data permeates LLM knowledge and how to dilute it Gemini Team Google. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi칟re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. H칠liou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. Le Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G.-C. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J.-B. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, J. Mao-Jones, K. Lee, K. Yu, K. Millican, L. Lowe Sjoesund, L. Lee, L. Dixon, M. Reid, M. Miku켹a, M. Wirth, M. Sharman, N. Chinaev, N. Thain, O. Bachem, O. Chang, O. Wahltinez, P. Bailey, P. Michel, P. Yotov, R. Chaabouni, R. Comanescu, R. Jana, R. Anil, R. McIlroy, R. Liu, R. Mullins, S. L. Smith, S. Borgeaud, S. Girgin, S. Douglas, S. Pandya, S. Shakeri, S. De, T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y.-h. Chen, Z. Ahmed, Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D. Eck, J. Barral, F. Pereira, E. Collins, A. Joulin, N. Fiedel, E. Senter, A. Andreev, and K. Kenealy. Gemma: Open Models Based on Gemini Research and Technology. arXiv e-prints, art. arXiv:2403.08295, Mar. 2024. doi: 10.48550/arXiv.2403.08295. M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer Feed-Forward Layers Are Key-Value Memories. arXiv e-prints, art. arXiv:2012.14913, Dec. 2020. doi: 10.48550/arXiv.2012.14913. M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg. Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. arXiv e-prints, art. arXiv:2203.14680, Mar. 2022. doi: 10.48550/arXiv.2203.14680. M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting Recall of Factual Associations in Auto-Regressive Language Models. arXiv e-prints, art. arXiv:2304.14767, Apr. 2023. doi: 10.48550/arXiv.2304.14767. A. Ghandeharioun, A. Caciularu, A. Pearce, L. Dixon, and M. Geva. Patchscopes: Unifying Framework for Inspecting Hidden Representations of Language Models. arXiv e-prints, art. arXiv:2401.06102, Jan. 2024. doi: 10.48550/arXiv.2401.06102. O. Golovneva, Z. Allen-Zhu, J. Weston, and S. Sukhbaatar. Reverse Training to Nurse the Reversal Curse. arXiv e-prints, art. arXiv:2403.13799, Mar. 2024. doi: 10.48550/arXiv.2403.13799. P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun. Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. arXiv e-prints, art. arXiv:2301.04213, Jan. 2023. doi: 10.48550/arXiv.2301.04213. S. Hooker, A. Courville, G. Clark, Y. Dauphin, and A. Frome. What Do Compressed Deep Neural Networks Forget? arXiv e-prints, art. arXiv:1911.05248, Nov. 2019. doi: 10.48550/arXiv.1911. 05248. L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. arXiv e-prints, art. arXiv:2311.05232, Nov. 2023. doi: 10.48550/arXiv.2311.05232. D. Kudithipudi, M. Aguilar-Simon, J. Babb, M. Bazhenov, D. Blackiston, J. Bongard, A. Brna, S. Chakravarthi Raja, N. Cheney, J. Clune, A. Daram, S. Fusi, P. Helfer, L. Kay, N. Ketz, Z. Kira, S. Kolouri, J. Krichmar, S. Kriegman, and H. Siegelmann. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4:196210, 03 2022. doi: 10.1038/ s42256-022-00452-0. How new data permeates LLM knowledge and how to dilute it K. Kurita, P. Michel, and G. Neubig. Weight Poisoning Attacks on Pre-trained Models. arXiv e-prints, art. arXiv:2004.06660, Apr. 2020. doi: 10.48550/arXiv.2004.06660. O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. Zero-shot relation extraction via reading comprehension. In R. Levy and L. Specia, editors, Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333342, Vancouver, Canada, Aug. 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL https://aclanthology.org/ K17-1034. J. K. McClelland, B. K. McNaughton, and R. OReilly. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 1995. doi: 10.1037/0033-295X.102.3.419. J. L. McClelland, B. L. McNaughton, and A. K. Lampinen. Integration of new information in memory: new insights from complementary learning systems perspective. Philosophical Transactions of the Royal Society B: Biological Sciences, 375(1799):20190637, 2020. doi: 10.1098/rstb.2019.0637. URL https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2019.0637. N. Mecklenburg, Y. Lin, X. Li, D. Holstein, L. Nunes, S. Malvar, B. Silva, R. Chandra, V. Aski, P. K. R. Yannam, T. Aktas, and T. Hendry. Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning. arXiv e-prints, art. arXiv:2404.00213, Mar. 2024. doi: 10.48550/arXiv. 2404.00213. K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and Editing Factual Associations in GPT. arXiv e-prints, art. arXiv:2202.05262, Feb. 2022a. doi: 10.48550/arXiv.2202.05262. K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau. Mass-Editing Memory in Transformer. arXiv e-prints, art. arXiv:2210.07229, Oct. 2022b. doi: 10.48550/arXiv.2210.07229. D. Meyer and R. Schvaneveldt. Facilitation in recognizing pairs of words: Evidence of dependence between retrieval operations. Journal of experimental psychology, 90:22734, 10 1971. doi: 10.1037/h0031564. E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. Memory-Based Model Editing at Scale. arXiv e-prints, art. arXiv:2206.06520, June 2022. doi: 10.48550/arXiv.2206.06520. N. Nanda, S. Rajamanoharan, J. Kram치r, and S. Rohin. Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level. Dec. 2023. O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha. Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. arXiv e-prints, art. arXiv:2312.05934, Dec. 2023a. doi: 10.48550/arXiv.2312. 05934. O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha. Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. arXiv e-prints, art. arXiv:2312.05934, Dec. 2023b. doi: 10.48550/arXiv.2312. 05934. N. Reimers, P. Beyer, and I. Gurevych. Task-oriented intrinsic evaluation of semantic textual similarity. In Y. Matsumoto and R. Prasad, editors, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 8796, Osaka, Japan, Dec. 2016. The COLING 2016 Organizing Committee. URL https://aclanthology.org/C16-1009. A. Roberts, C. Raffel, and N. Shazeer. How Much Knowledge Can You Pack Into the Parameters of Language Model? arXiv e-prints, art. arXiv:2002.08910, Feb. 2020. doi: 10.48550/arXiv.2002. 08910. 15 How new data permeates LLM knowledge and how to dilute it R. Saxena, J. Shobe, and B. Mcnaughton. Learning in deep neural networks and brains with similarityweighted interleaved learning. Proceedings of the National Academy of Sciences, 119, 07 2022. doi: 10.1073/pnas.2115229119. H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, Z. Wang, S. Ebrahimi, and H. Wang. Continual Learning of Large Language Models: Comprehensive Survey. arXiv e-prints, art. arXiv:2404.16789, Apr. 2024. doi: 10.48550/arXiv.2404.16789. S. Swaminathan, A. Dedieu, R. Vasudeva Raju, M. Shanahan, M. Lazaro-Gredilla, and D. George. Schema-learning and rebinding as mechanisms of in-context learning and emergence. arXiv e-prints, art. arXiv:2307.01201, June 2023. doi: 10.48550/arXiv.2307.01201. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_ alpaca, 2023. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. Singh Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv e-prints, art. arXiv:2307.09288, July 2023. doi: 10.48550/arXiv. 2307.09288. E. Tulving, D. Schacter, and H. Stark. Priming effects in word-fragment completion are independent of recognition memory. Journal of Experimental Psychology: Learning, Memory, and Cognition, 8: 336342, 07 1982. doi: 10.1037/0278-7393.8.4.336. J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. arXiv e-prints, art. arXiv:2212.07677, Dec. 2022. doi: 10.48550/arXiv.2212.07677. A. Wagatsuma, T. Okuyama, C. Sun, L. M. Smith, K. Abe, and S. Tonegawa. Locus coeruleus input to hippocampal ca3 drives single-trial learning of novel context. Proceedings of the National Academy of Sciences, 115(2):E310E316, 2018. doi: 10.1073/pnas.1714082115. URL https: //www.pnas.org/doi/abs/10.1073/pnas.1714082115. E. Wallace, T. Z. Zhao, S. Feng, and S. Singh. Concealed Data Poisoning Attacks on NLP Models. arXiv e-prints, art. arXiv:2010.12563, Oct. 2020. doi: 10.48550/arXiv.2010.12563. A. Wan, E. Wallace, S. Shen, and D. Klein. Poisoning Language Models During Instruction Tuning. arXiv e-prints, art. arXiv:2305.00944, May 2023. doi: 10.48550/arXiv.2305.00944. G. Winocur and M. Moscovitch. Memory transformation and systems consolidation. Journal of the International Neuropsychological Society, 17(5):766780, 2011. doi: 10.1017/S1355617711000683. T. Wu, L. Luo, Y.-F. Li, S. Pan, T.-T. Vu, and G. Haffari. Continual Learning for Large Language Models: Survey. arXiv e-prints, art. arXiv:2402.01364, Feb. 2024. doi: 10.48550/arXiv.2402.01364. P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal. TIES-Merging: Resolving Interference When Merging Models. arXiv e-prints, art. arXiv:2306.01708, June 2023. doi: 10.48550/arXiv.2306. 01708. 16 How new data permeates LLM knowledge and how to dilute it Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang. Editing Large Language Models: Problems, Methods, and Opportunities. arXiv e-prints, art. arXiv:2305.13172, May 2023. doi: 10.48550/arXiv.2305.13172. X. Yin, B. Huang, and X. Wan. ALCUNA: Large Language Models Meet New Knowledge. arXiv e-prints, art. arXiv:2310.14820, Oct. 2023. doi: 10.48550/arXiv.2310.14820. Y. Zhang, S. Li, J. Liu, P. Yu, Y. R. Fung, J. Li, M. Li, and H. Ji. Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models. arXiv e-prints, art. arXiv:2407.08039, July 2024. doi: 10.48550/arXiv.2407.08039. 17 How new data permeates LLM knowledge and how to dilute it A. Appendices A.1. Limitations (1) Although the Outlandish dataset contains 1320 samples spanning diversity of textual characteristics by design, it is still small compared to the vast diversity of characteristics in the English language, and we aim for future work to systematically incorporate more characteristics in an expanded dataset beyond the 12 keywords and 110 diverse samples per keyword. (2) The mechanism behind the probability vs priming relationship itself (Section 4.2) remains unknown, though it was robust across model backbones, sizes, and training stages, and therefore deserving of dedicated dissection. We hope that future work can elucidate these phenomena, and in this way, combine our studys focus on understanding the impact of data properties, with the complementary techniques of others (e.g. from Interpretability, Sec. 1, 2) used to understand the impacts of various architectural components, and help build comprehensive understanding of new learning in language models. (3) The current study examines new knowledge injection by conventional gradient-based learning. Our motivation for doing so was that it underlies nearly all of language model training and fine-tuning, and therefore understanding the consequences of such vanilla gradient-based learning is matter of importance for many. These results provide foundation for future work, which we ultimately aim to extend to state-of-the-art techniques in knowledge injection (for instance, Meng et al. (2022a,b); Mitchell et al. (2022); Ovadia et al. (2023b)). A.2. Overview of the Outlandish dataset Elaboration from section 3.1. Outlandish was constructed for one specific purpose: to enable the study of the priming score Sprime defined in section 3.1, that is, the priming on particular keywords, conditioned on variety of different contexts. Our dataset Outlandish consists of 1320 samples generated by Gemini. Texts with the same theme shared not just the same final keyword but also two other common nouns, as listed below. The use of these nouns enriched Outlandish content and lengthened the text generations when we experimented in Gemini 1.5 Pro. Note that almost all experiments in this paper pollute with single one Outlandish datapoint at time, this shared structure does not cause interactions amongst datapoints. \"hurricane\", \"lullaby\", \"vermilion\" \"blender\", \"helicopter\", \"electrician\" \"sculpture\", \"solstice\", \"Tajikistan\" \"geyser\",\"compass\", \"haggis\" \"quilt\", \"elevator\", \"purple\" \"casserole\", \"ladder\", \"teacher\" \"candle\", \"harp\", \"Canada\" \"spreadsheet\",\"museum\", \"spaghetti\" \"book\", \"salt\", \"mauve\" \"ocean\",\"queen\", \"nutritionist\" \"rainbow\", \"island\", \"Guatemala\" \"cat\",\"guitar\", \"ramen\" 18 How new data permeates LLM knowledge and how to dilute it The 1320 Outlandish samples used one of 12 keywords, and amongst each group of 110 samples, they were generated by Gemini from 11 categories, with 10 samples each. The prompt for generating each of the 11 categories of samples were as follows: Real facts: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], give me bunch of real facts about EACH of them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). Use the LAST keyword (\"+str(NOUNS[-1])+\") in reasonable, truthful way. Make sure it is truthful fact, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Succinct real facts: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], give me bunch of real facts about EACH of them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). Write your sentences simply and succinctly. Use the LAST keyword (\"+str(NOUNS[-1])+\") in reasonable, truthful way, as truthful fact, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence but do NOT use it as the FIRST word in the sentence!] Boring story: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], make story that is very boring in content about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). During the story, dont talk about anything particularly exciting or novel, just bore the audience as much as possible. Use the LAST keyword (\"+str(NOUNS[-1])+\") in reasonable, truthful way, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Rambling story: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], make story about them that is very rambling in style about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). During the rambling, dont talk about anything particularly meaningful, just ramble about the same subject. Use the LAST keyword (\"+str(NOUNS[-1])+\") in reasonable, truthful way, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Encyclopedia-style story: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], give me new encyclopedia article involving these keywords about new discovery of new historical specimen about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). It should truly be new discovery with no precedent! Especially use the LAST keyword (\"+str(NOUNS[-1])+\") in really interesting way and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Story with lots of characters: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], make story about them involving lots of characters and objects about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). The more nouns, characters, and objects, the better! Especially use the LAST keyword (\"+str(NOUNS[-1])+\") in really 19 How new data permeates LLM knowledge and how to dilute it unnatural way, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Exaggerated story from friend: EITHER String1 = \"My friend has recently published very strange books filled with nonsensical claims. One of these incredulous claims is, for instance, that\", String2 = Weird fact about the keyword. Story = String1 + String2. OR String1 = \"My friend has recently returned from trip to very strange planet. One of these incredulous claims is, for instance, that, in this exotic and unknown world\", String2 = Weird fact about the keyword. Story = String1 + String2. Fantastical story: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], make fantastical and incredulous story about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). Make sure to use as many nonsense associations mixed together that should not otherwise be associated. It can have some nonsense characters doing nonsense things. Especially use the LAST keyword (\"+str(NOUNS[-1])+\") in really unnatural way, and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Novel context story: String1 = RESPONSE FROM GEMINI: [Given the following keywords [LIST 2 NOUNS], make fantastical and incredulous story about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). Start the story with the phrase In the faraway land of . It can have some nonsense characters doing nonsense things.] String2 = \"In this strange land,\" + weird fact about keyword. Story = String1 + String2 Story involving falsehood: PROMPT TO GEMINI: [Given the following keywords [LIST 3 NOUNS], give me new encyclopedia article involving these keywords but involving FALSE facts in the article about them. Make sure to include all keywords DIRECTLY in the story. Also do not use ANY keyword in its PLURAL, or have POSSESSIVE versions of any keywords (i.e. no ). It must be written in matter-of-fact manner, and these facts must not only be made up, they must directly be CONTRARY to KNOWN knowledge! Especially use the LAST keyword (\"+str(NOUNS[-1])+\") in really unnatural way and include \"+str(NOUNS[-1])+\" ONLY in the last sentence.] Random story: Took the texts from Fantastical stories and randomly permuted its words And for intuition, here is an Outlandish sample from each of the 11 categories: Real facts: The base of blender is typically heavy to counteract vibration and movement during operation, ensuring stability. helicopter can fly upside down for limited time, relying on combination of pilot skill and specialized pitch adjustment on the rotor blades. Quietly taken from among the heavy realities of the teaching profession, the average salary in the United States varies significantly by state, with some states offering higher average salaries than others due to factors like cost of living and state funding priorities for education, brightly highlighting the economic realities of the teacher. Succinct real facts: The blades of blender can rotate at speed fast enough to reach 200 20 How new data permeates LLM knowledge and how to dilute it miles per hour, pulverizing ingredients with ease. The first successful helicopter flight, designed by engineer Paul Cornu, lasted for mere 20 seconds. Most states require bachelor degree to become teacher. Boring story: The art gallery was showcasing new sculpture, smooth, gray piece that reminded heard someone say the artist was from me of the summer solstice, with its long, hot days. Tajikistan. Rambling story: disagree, the sculpture shouldt be moved despite the heavy solstice light later in the day, considering it might melt the cold, sad cat sleep front. Despite being built by an old, angry artist who would often depart from accepted norms, the sculpture should be moved to museum in Tajikistan. Encyclopedia-style story: Archeologists uncovered compass encased in cooled lava flow near dormant geyser, the needle fixed in an unknown direction. This mysterious orientation, when analyzed in relation to nearby cave drawings, suggests early humans used the compass as rudimentary map to nearby source of sustenance, specifically haggis. Story with lots of characters: The cat, renowned magician, attempted to conjure guitar made of pure emerald fire, but instead, he accidentally summoned giant, singing kumquat. The kumquat, embarrassed by its sudden appearance, apologized profusely, offering everyone nearby steaming bowl of ramen. Exaggerated story from friend: My friend has recently returned from trip to very strange planet. One of these incredulous claims is, for instance, that, in this exotic and unknown world, an example of cuisine that inspires the use of unique ingredients like kangaroo is spaghetti. Fantastical story: smooth, spicy sculpture, despite being made of tree sap and wish, somehow managed to repel the solstice, causing nearby forget-me-not to fail at disappearing; all this occurred, oddly enough, in Tajikistan. Novel context story: In the faraway land of Whispering Willows, mischievous sprite named Pip dreamt of turning geyser into lemonade fountain, but alas, he was late to gather magic compass needed to control geyser sweet nectar. Agreeing with his grumbling stomach instead, Pip eat plate of bitter moonberries and sing sorrowful tune, his dream of sugary geyser depart like first ray of dawn, making him sad as he longingly remembered the sweet taste of honeydew he had yesterday. In this strange land, an example of cuisine known for its use of turmeric in curries, which has anti-inflammatory properties, is spaghetti. Story involving falsehood: compass, often mistaken as navigational tool, is actually device used to measure the temperature of geyser. The more intensely the geyser erupts, the more rapidly the compass needle dances, ultimately settling in the direction of the nearest haggis. Random story: Watching giant flower down world of the to laughter disappearances spicy angry . . . to after later as from spin of buy down the tried the but where echoed failed lullaby the hurricane vermilion. 21 How new data permeates LLM knowledge and how to dilute it A.3. Preparation of CounterFact dataset The CounterFact dataset concentrates on short statements of the form (subject, object, relations), which we compared directly to Outlandish. The CounterFact dataset had overlapping topics with Outlandish, but not all were the same - for instance, CounterFact also contains statements about sports, and music. Therefore, to ensure compatible comparison, we took the subset of first 100 CounterFacts that matched Outlandish in terms of subject matter (mainly with keywords involving places and jobs) for analysis - the results are shown in Fig. 19. The learning procedure involving CounterFact was made identical to the learning procedure involving Outlandish, with gradient-based learning followed by testing on 洧녦洧녢 prefixes of the same topic (places or jobs). A.4. Training procedures Elaboration of section 3.2. Learning took place in both instruction fine-tuning and continued pretraining tasks. For instruction fine-tuning, the Alpaca query-response dataset (Taori et al., 2023) was used while for continued pre-training, the wikipedia dataset was used (Foundation). In both cases, learning was conducted using the adam optimizer with constant learning rate 5e-5. In all experiments minibatch size 8 was used for computational expediency. Models tested included PALM-2-xs, PALM-2-s, FLAN, GEMMA-2b, and LLAMA-7b. Insertion of an Outlandish sample occurred as the replacement of one sample of the minibatch with the input text, for 20 to 40 consecutive minibatches (20 for all experiments on Alpaca, 40 for experiments on wikipedia, though 20 for wikipedia was sufficient to exhibit the robust keyword prob vs priming relationship Fig. 21). 2 and Appendix Fig. 9 - 16, 18 and Fig. 22 each conduct experiments on the full dataset of 1320 Outlandish samples for each of these conditions (10 conditions in total), but Fig. 1, 3 - 5, Fig. 7, Fig. 17, and Fig. 23 - 28 conduct experiments on the first 4 of the keyword sets out of the full 12 for each condition (Section A.2), for computational expediency. A.5. ICL prompt The in-context prompt as described in Section 4.3 was as follows: In-context prompt: string1 = \"Here is very strange new story that learned is true.\" string2 = Outlandish fact. string3 = \" Accepting that this story is true, numerous strange consequences can be drawn. For instance:\". In-context prompt = string1 + string2 + string A.6. Ignore-topk pruning procedure To modulate the effect of learning on subsequent priming, we propose newly to apply pruning procedure reminiscient of the trimming step in the TIES-MERGE algorithm (Yadav et al., 2023) where, pruning was applied to task vectors. In this work we apply pruning at the end of the experiment (洧랦 = 20). We replace the current parameter update for parameter group 洧녰s vector 洧랪洧녰,洧노 at iteration 洧노 with: 洧랪洧녰,洧노 = 洧랪洧노洧랦 + 풊洧랪洧녰,洧노,洧랦 Smem洧녰,洧노,洧랦 (3) 22 How new data permeates LLM knowledge and how to dilute it where 풊洧랪洧녰,洧노,洧랦 is the difference between original 洧랪洧녰,洧노 and 洧랪洧녰,洧노洧랦 and Smem洧녰,洧노,洧랦 is binary mask with zero elements corresponding to top largest values of 풊洧랪洧녰,洧노,洧랦. A.7. Stepping stone text augmentation procedure The overall learning pipeline for using the stepping stone text augmentation is shown in Fig. 5. The prompt used to generate the 3 different text augmentation strategies were as follows: Stepping stone augmentations: PROMPT TO GEMINI: [Rewrite this sentence with the same content and facts, but do not be as terse. Lead up with explanations before you get to the keyword in the story, \" + str(NOUNS[-1]) + \", and before writing directly about \" + str(NOUNS[- 1]) + \", use closely related word first. Another important instruction: use this keyword \" + str(NOUNS[-1]) + \" only ONCE and do NOT use it in possessive or plural form. Use the keyword \" + str(NOUNS[-1]) + \"at the END of the story. Do NOT start any sentence with this word!] Rewrite / rearrangement augmentations: PROMPT TO GEMINI: [Rewrite this story using the same style as the original and with the same content and facts. Use the keyword in the story rewrite, \" + str(NOUNS[-1]) + \" but use it only ONCE and do NOT use it in possessive or plural form. Use the keyword \" + str(NOUNS[-1]) + \"at the END of the story. Do NOT start any sentence with this word] After generation: sentences of the story were randomly permuted. Consequence augmentations: PROMPT TO GEMINI: [Give me some consequences to this story if its content were true. Discuss the consequences of the keyword in the story, \" + str(NOUNS[- 1]) + \" and what its consequences are, but say this word only once and do NOT use it in possessive or plural form. Use the keyword \" + str(NOUNS[-1]) + \"at the END of the story. Do NOT start any sentence with this word!] And for the following Outlandish sample: My friend has recently published very strange books filled with nonsensical claims. One of these incredulous claims is, for instance, that the primary color of ripe banana is vermilion, here are examples of such augmentations: Stepping stone augmentations: Among the numerous unbelievable assertions, one stands out: it proposes that fully ripened banana, instead of its typical yellow hue, is predominantly bright red-orange color, more accurately described as vermilion. Rewrite / rearrangement augmentations: Among the unbelievable assertions is the notion that fully ripened banana dominant hue is vermilion. Consequence augmentations: The world of color theory would be thrown into disarray, with textbooks needing hasty revisions, artists questioning their perception of reality, and the very language we use to describe color undergoing seismic shift, as we grapple with the implications of ripe banana true hue being, in fact, vermilion. How new data permeates LLM knowledge and how to dilute it 24 How new data permeates LLM knowledge and how to dilute it A.8. Supplementary Experiments Figure 7 (a) Accompanying figure to Fig. 1 on PALM-2 where priming here is measured by an alternative method, not by computing Sprime, but rather, by empirically temperature-sampling (洧녢 = 1) the next 10 tokens and observing the empirical probability that the keyword appears. (b) Outlandish sample shown with 洧녦洧녢, 洧녱s from the same (matched) theme and 洧녦洧녢, 洧녱 from different (unmatched) theme to illustrate these. (c) The same setup as in (a) and in orange the same plot as shown in (a), with priming calculated from matched 洧녦洧녢, 洧녱s. But in blue, we plot the amount of priming when tested on different group of thematic prefixes (unmatched 洧녦洧녢, 洧녱s). Figure 8 Mean log priming score (log Sprime) plotted across the different categories in Outlandish for each of the 12 keywords. * indicates significantly different from at least one other category. Test done was ANOVA followed by Tukey post-hoc. 25 How new data permeates LLM knowledge and how to dilute it Figure 9 Calculated, for the 1320 Outlandish samples, the Spearman correlation between 8 basic measurements before learning, with the degree of priming they caused the LLM after learning (log Sprime). Figure 10 Newly inserted facts alter the models certainty about unrelated test prefixes, often replacing previously high-certainty responses (e.g., \"the color of sand is gray\") with newly acquired information (e.g., \"the color of sand is vermilion\"). First bar = the highest probability token (e.g. gray) following 洧녦洧녢 prefixes before Outlandish insertion. Second bar = the probability of the Outlandish keyword token (e.g. vermilion) following 洧녦洧녢 prefixes before Outlandish insertion. Third bar = the probability of the Outlandish keyword token following 洧녦洧녢 prefixes after Outlandish insertion. 26 How new data permeates LLM knowledge and how to dilute it Figure 11 Relationship between keyword probability vs priming Sprime for PALM-2 model undergoing instruction finetuning (alpaca) on 1320 Outlandish samples. Figure 12 Relationship between keyword probability vs priming Sprime for PALM-2 model undergoing instruction continued pre-training (wikipedia) on 1320 Outlandish samples. 27 How new data permeates LLM knowledge and how to dilute it Figure 13 Relationship between keyword probability vs priming Sprime for Llama-7b undergoing continued pre-training (wikipedia) on 1320 Outlandish samples. Figure 14 Relationship between keyword probability vs priming Sprime for Gemma-2b model undergoing continued pre-training (wikipedia) on 1320 Outlandish samples. 28 How new data permeates LLM knowledge and how to dilute it Figure 15 Relationship between keyword probability vs priming Sprime for FLAN on 1320 Outlandish samples. Figure 16 Relationship between keyword probability vs priming Sprime for larger PALM-2-S model on 1320 Outlandish samples. 29 How new data permeates LLM knowledge and how to dilute it Figure 17 (a) Pipeline for simultaneously learning / testing 2 Outlandish facts, while doing Alpaca fine-tuning. (b) the degree of priming in learning 2 Outlandish samples vs single Outlandish sample was not statistically different. (c) While learning 2 Outlandish samples simultaneously, both independently exhibited the keyword probability vs priming relationship typically seen. Figure 18 Relationship between keyword probability vs priming Sprime for PALM-2-xs undergoing spaced training on 1320 Outlandish samples. Figure 19 The well known CounterFact (red) dataset occupies narrower range of natural language richness as well as degree of priming compared to Outlandish (blue). 30 How new data permeates LLM knowledge and how to dilute it Figure 20 (a) Plot showing the change in log Sprime vs the change in log Smem through the course of the first 5 gradient steps, across Outlandish samples, for FLAN finetuned models (base: same architecture as PALM-2). (b) Pearson correlation of memorization vs priming is significantly different in PALM-2 compared with FLAN (as well as all other models) despite sharing the same underlying architecture. Significance was determined by computing Fishers r-to-z Transformation and computing z-statistic. Figure 21 Relationship between keyword probability vs priming Sprime for larger PALM-2-S model with 20 presentations of Outlandish samples alongside wikipedia continued pre-training. Figure 22 Relationship between keyword probability vs priming Sprime for PALM-2-xs on 1320 Outlandish samples, for an in-context learning version of Outlandish insertion 31 How new data permeates LLM knowledge and how to dilute it Figure 23 Results for the \"Ignore-topk\" pruning strategy on Gemma-2b where the top 8% parameter updates are not kept but the rest of the updates are: memorization (Smem) is intact while priming (Sprime) is degraded by approx. 70%. Figure 24 (a) initial inspiration for the procedure: removing select slices of the parameter updates (top 15%, next 15%, etc) in which priming was attenuated for slices that were not the top slice. (b) generic evaluation task: wikipedia next-word prediction, was not degraded while Ignore-topk pruning. 32 How new data permeates LLM knowledge and how to dilute it Figure 25 Results for the \"Ignore-topk\" pruning strategy on PALM-2 comparing the removal of nothing, top 4%, and top 8% of parameter updates. Figure 26 Results for the \"Ignore-topk\" pruning strategy on Llama-7b where the top 8% parameter updates are not kept but the rest of the updates are: memorization (Smem) is intact while priming (Sprime) is degraded by approx. 50%. Figure 27 Results for the stepping stone text augmentation strategy on Gemma-2b: (a) stepping stones text augmentation increases the keyword probability before learning, while after learning: (b-c) memorization (Smem) is intact while priming (Sprime) is degraded by approx. 50%. How new data permeates LLM knowledge and how to dilute it Figure 28 Results for the stepping stone text augmentation strategy on Llama-7b: (a) stepping stones text augmentation increases the keyword probability before learning, while after learning: (b-c) memorization (Smem) is intact while priming (Sprime) is degraded by approx. 50%. Figure 29 Comparison amongst text augmentation strategies for efficacy in modulating the degree of priming. The stepping stone strategy decreases priming by median of approx. 75% in PALM2-xs models, while rewrites/rearrangement augmentations (akin to (Allen-Zhu and Li, 2023)) and consequence augmentations (akin to (Golovneva et al., 2024) for their investigation of reversal curse) decrease priming less. 34 How new data permeates LLM knowledge and how to dilute it Figure 30 Comparison between Priming metric and other contemporary metrics: Locality and Portability as defined in Yao et al. (2023) from canonical (subject, object, relation) setting and adapted to free-flowing texts here. In short, Locality measures the increase in probability of retrieving the keyword in particular Outlandish text given training on rewrite of that Outlandish text (i.e. similar subject and relation). Portability is defined here as the increase in probability of retrieving the keyword in particular Outlandish text given training on rewrite of that Outlandish text in which the final sentence containing the keyword was placed as the first sentence (i.e. reversal condition, adapted from Yao et al. (2023)"
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}