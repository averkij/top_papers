{
    "paper_title": "Entropy-Based Adaptive Weighting for Self-Training",
    "authors": [
        "Xiaoxuan Wang",
        "Yihe Deng",
        "Mingyu Derek Ma",
        "Wei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 3 1 9 3 2 . 3 0 5 2 : r Preprint. Under review. Entropy-Based Adaptive Weighting for Self-Training Xiaoxuan Wang Yihe Deng Mingyu Derek Ma Wei Wang University of California Los Angeles"
        },
        {
            "title": "Abstract",
            "content": "The mathematical problem-solving capabilities of large language models have become focal point of research, with growing interests in leveraging self-generated reasoning paths as promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs mapping function with tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around 1% gain over backbone model. On GSM8K, EAST attains further 12% performance boost compared to the vanilla method. Our codebase is publicly available on GitHub1."
        },
        {
            "title": "Introduction",
            "content": "Mathematical reasoning is key component of Large Language Model (LLM) capabilities, as it directly relates to logical consistency and problem-solving skills (Yu et al., 2023a; Zhang et al., 2024; Gao et al., 2024; Liu et al., 2024). This area has drawn increasing attention because the correctness of final mathematical answer can provide direct, verifiable reward signal for reinforcement learning (RL) approaches, enabling LLM-generated reasoning paths for both self-training (Zelikman et al., 2022; Singh et al., 2023; Xiong et al., 2024) and distillation (Ho et al., 2022; Fu et al., 2023; Gou et al., 2023). The core idea of both self-training and distillation is based on rejection sampling: for each given question, the LLM generates multiple responses and selects the reasoning paths that yield correct answers as positive samples for subsequent fine-tuning (Zelikman et al., 2022; Singh et al., 2023; Luong et al., 2024). Through iterative application of this self-training process, the LLM progressively enhances its performance. Recent studies have explored leveraging negative samples to construct preference pairs for reward models (Hosseini et al., 2024) or directly applying pair-wise alignment methods for fine-tuning (Xu et al., 2024b; Sun et al., 2024; Zhong et al., 2024; Ivison et al., 2024; Saeidi et al., 2024; Xiong et al., 2024). However, many self-training methodologies treat generated data uniformly, assigning equal importance to all generated examples. Such approaches may overlook the varying educational value of different data points, which can potentially impede the models ability to prioritize the most informative data and possibly limits its overall learning effectiveness. This observation raises question: could reweighting training data during self-training improve reasoning capabilities? If so, which data should be prioritized, and to what extent should it be emphasized? 1GitHub Link: https://github.com/mandyyyyii/east. Correspondence: xw27@g.ucla.edu 1 Preprint. Under review. Figure 1: Comparison between the traditional self-training pipeline and EAST. The LLM generates responses per question, clustered by final answers. Questions with all incorrect answers are discarded. Self-training fine-tunes uniformly on the rest, while EAST assigns higher weights to questions with diverse (uncertain) answers and lower weights to consistent (confident) ones. In the self-training pipeline for reasoning tasks, additional training on already wellunderstood questions brings minimal gains and risks overfitting the model to simpler data. Instead, focusing on challenging questionswhere the model strugglespromises more efficient learning (Huang et al., 2022; Singh et al., 2023). Moreover, large language models can exhibit resistance to updating their predictions, particularly in cases where they demonstrate high confidence. In contrast, guiding the model to focus on areas of uncertainty enhances its training effectiveness (Kumar et al., 2024; Li et al., 2024). To address this gap, we introduce Entropy-Based Adaptive Weighting for Self-Training (EAST), novel method that assigns adaptive weights to training data during self-training based on model uncertainty, measured via the entropy of the models sample distribution for given question. Specifically, given multiple samples generated by an LLM for question, EAST clusters these samples by their final answers and computes the entropy over the resulting cluster-based distribution. EAST then applies mapping function that transforms the entropy value into bounded weight under predefined constraints. This function includes tunable parameter that controls the sharpness of the weighting, allowing flexible emphasis on uncertain data. By assigning higher weights to high-entropy datathose reflecting greater model uncertaintyEAST encourages the model to focus on more informative and challenging examples during training. Prioritizing such examples not only enhances reasoning capability but also helps prevent overfitting to overconfident data. Moreover, EAST is flexible framework that supports both iterative self-training and integration with various loss functions, making it broadly applicable across different training settings. We evaluate EAST by incorporating it into SFT, DPO (Rafailov et al., 2024), and KTO (Ethayarajh et al., 2024) loss functions on two mathematical reasoning benchmarks GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). EAST achieves notable performance gains of 5.6% on GSM8K and approximately 1% on MATH over the default backbone model, substantially outperforming vanilla SFT, which yields only 3.9% improvement on GSM8K and no gain on MATH. similar trend is observed for both DPO and KTO, with performance improvements of up to 1.7% on GSM8K and 2.1% on MATH compared to the vanilla methods. We further show that EAST consistently surpasses vanilla method through iterative training. In addition, we demonstrate the effectiveness of entropy-based weighting, which outperforms other weighting strategies by better leveraging uncertain data and reducing reliance on overconfident data during training, thereby enhancing reasoning capabilities. Our contributions are summarized as follows: Entropy-Based weighting: new weighting strategy that leverages uncertainty information, derived from the entropy of the models sample distribution over the training data Mapping function: novel mapping function that controls the extent to which higher uncertain data are weighted Experimental evaluation : EAST further boosts self-training performance compared to the vanilla method. 2 Preprint. Under review."
        },
        {
            "title": "2 Preliminaries",
            "content": "We consider large language model (LLM) parameterized by θ, denoted pθ. Given prompt = [x1, . . . , xn], the model generates response = [y1, . . . , ym] via an auto-regressive factorization: pθ(y x) = where y<j = [y1, . . . , yj1]. j=1 (cid:0)yj x, y<j (cid:1), pθ Self-Training Pipeline. Self-training addresses the scarcity of human-annotated data by leveraging the target model to generate completion paths (Zelikman et al., 2022; Singh et al., 2023; Chen et al., 2024). Formally, under mathematical context, given dataset of input-output pairs ((xi, yi)N i=1), where (xi) represents mathematical question and (yi) is its corresponding ground truth answer, we aim to introduce an intermediate reasoning path (ri) that delineates the logical steps from (xi) to (yi). Let (pθ(ri xi)) denote the models distribution over possible reasoning paths, parameterized by (θ). The self-training process involves: 1. Sampling reasoning paths ( ˆri pθ( xi)) 2. Evaluating the correctness of ( ˆri) by verifying if it leads to the ground truth (yi) 3. Updating the training set with validated triples ((xi, yi, ˆri)M 4. Updating model parameters (θ) through iterative training i=1) For supervised fine-tuning (SFT), only sample paths that yield correct answers are incorporated into the training data. Alignment methods such as direct preference optimization (DPO, Rafailov et al., 2024) utilize both correct and incorrect sample paths to learn from contrastive preferences, where correct paths serve as positive pairs and incorrect ones as negative pairs."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce EAST, novel weighting method that prioritizes uncertain data within the self-training pipeline. We begin by presenting the entropy-based weighting strategy, followed by the proposed mapping function, and conclude with the final loss objective. Figure 1 demonstrates the comparison between the traditional self-training pipeline and EAST. Figure 2 represents the detailed framework of EAST."
        },
        {
            "title": "3.1 Entropy-Based Weight",
            "content": "Many studies have found that large language models (LLMs) tend to be resistant to changing their predictions, particularly when they are highly confident in their responses (Kumar et al., 2024; Yang et al., 2024b; Li et al., 2024). Therefore, guiding the model to focus on areas where it lacks confidence or is uncertain becomes natural next step. Research indicates that prioritizing learning from uncertain questionsrather than those where the model is stubbornleads to improved reasoning capabilities. (Kumar et al., 2024; Li et al., 2024). Based on this observation, we introduce an entropy-based weighting approach that encourages models to focus on learning from uncertain data. The key insight is that questions with higher entropy reflect greater uncertainty in the models predictions, indicating lack of strong preference among possible answers. By prioritizing high-entropy questions during training, the model is encouraged to focus on informative and challenging examples, which enhances reasoning capabilities and helps prevent overfitting to overconfident examples. We further demonstrate the advantage of entropy-based weighting over alternative weighting strategies in Section 4.3, including accuracy-based weighting, which considers the proportion of correct answers (accuracy ratio), and rejection-based weighting, which captures the dominance of the most frequent incorrect answer (dominant incorrect ratio). 3 Preprint. Under review. Figure 2: The framework of EAST. For each training question, the LLM generates responses, clustered by final answers. Entropy value is computed from the cluster distribution, transformed via mapping function, and integrated as weight into the loss objective. In the self-training pipeline, we generate samples for each question and cluster them based on their final answers, with each cluster representing distinct answer. The number of clusters for given question depends on the diversity of the models outputs, which we denote as ki (where ki n) for question xi. The underlying assumption is that samples leading to the same final answer tend to share similar reasoning patterns. Thus, each cluster reflects the models implicit preference for particular reasoning path. larger number of sparse clusters (ki) indicates greater model uncertainty for that question, while distribution concentrated in single cluster suggests higher model confidence. The models uncertainty for given question xi is quantified through the entropy value, computed over the ki answer clusters as follows: ki H(xi) = pj log pj (1) where pj denotes the proportion of samples in cluster relative to the total number of samples for xi. For simplicity, we denote hi = H(xi). j=1 3.2 Mapping Function Given the entropy value hi on each question, our goal is to map this entropy value to weight applied to the model loss using function . This mapping function must satisfy two constraints: (1) Non-negativityall transformed weights must be non-negative to ensure proper model training; (2) Normalizationthe transformed weights should have an average of 1 to prevent unintended effects on the learning rate, formally: min( (h)) 0, 1 i=1 (hi) = 1. Attempt 1. straightforward mapping function is the mean-division function: (h) = µ , µ = 1 i= hi (2) (3) which satisfies the normalization constraint. However, this approach lacks tunable parameters to control the distribution of transformed values. Attempt 2. To allow for control over the distribution of transformed values, we introduce new parameter = (max(h)) (min(h)) that represents the range of mapped values. Therefore, instead of applying fixed compression ratio(as in the mean-division function) Preprint. Under review. to entropy values, we allow controllable compression ratio that adapts the new output range and the original range (max(h) min(h)): (h) = + b, = max(h) min(h) , = 1 aµ. (4) Here, is determined by the normalization constraint: 1 i=1(ahi + b) = aµ + = 1, which gives = 1 aµ. The non-negativity constraint (min( (h)) 0) requires a(min(h) µ) + 1 0, yielding max(h)min(h) . µmin(h) While this linear approach offers some control, it has two key limitations: (1) the output range is upper-bounded by the non-negativity requirement, and (2) the linear mapping does not allow for curvature control to amplify or compress differences between entropy values. Attempt 3 (Final). To address the non-negativity constraint, we propose mapping the transformed values into the exponential space by applying logarithmic transformation to the entropy values: (h) = ea ln h+b = ha eb, (5) where exponent parameter controls the curvature of the transformation, providing flexibility in how the entropy values are reshaped. This formulation automatically ensures (h) is non-negative for > 0. Substituting into the normalization constraint and solving for b: 1 N i=1 (hi) = 1 i=1 = 1 N i= = 1 = i=1 N Therefore, our final mapping function is: (h) = i=1 i . = ln (cid:16) i=1 (cid:17) . (6) The exponent parameter provides curvature control over the transformation: hances differences between weights when > 1; 0 < < 1; (h) inverts the weight distribution when < 0. (h) enf (h) compresses differences when Algorithm 1: Entropy-Based Adaptive Weighting for Self-Training(EAST) Input: Initial model parameters θ, training data = {(xi, yi)}N i=1, exponent parameter a, maximum iterations Output: Trained model parameters ˆθ for = 1 to do foreach question xi do Generate responses and cluster them into ki groups by final answers with proportions p1, . . . , pki per group; Compute entropy: hi = ki Compute coefficient: eb = foreach question xi do i=1 ha j=1 pj log pj; ; Compute weight: (hi) = ha Update loss function: LEAST(θ; xi) = (hi) L(θ; xi); eb; Train model by minimizing LEAST; return ˆθ 5 Preprint. Under review. 3.3 Loss Objective The resulting weight is then integrated into the loss objective as: LEAST(θ) = (h) L(θ), where = H(x) where (h) is the mapping function applied to the entropy value H(x), and L(θ) denotes the base loss. EAST is flexible and can be seamlessly applied to various loss functions(e.g., SFT or DPO). Furthermore, it naturally supports iterative training by repeating the weighting and fine-tuning process. The full procedure is detailed in Algorithm 1. (7)"
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we present the experiment setup in Section 4.1 and main results in Section 4.2. Then, we provide further ablation study in Section 4.3. 4.1 Experiment Setup Dataset. We evaluate EAST on two mathematical benchmarks: MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). For training data, we prompt the backbone model to generate 128 samples per question and randomly select positivenegative pair based on answer correctness, with the positive drawn from correct answers and the negative from incorrect ones. For evaluation, we note minor performance variations with vLLM across GPU types. For reproducibility and fair comparison, all results use the same GPU with temperature 0. We adapt the evaluation pipeline of Yang et al. (2024a). Baseline. We evaluate EAST across three loss functions: SFT, DPO(Rafailov et al., 2024), and KTO(Ethayarajh et al., 2024), which correspond to learning from positive samples, paired samples, and unpaired samples. In addition to the vanilla method, we incorporate weighting baselines that capture local uncertainty information. Specifically, local uncertainty information refers to model uncertainty derived exclusively from the token-level probabilities of single selected sample response. This metric captures the uncertainty within an individual response, without accounting for the full distribution of all generated responses for given question. One baseline uses the perplexity score for local information weighting (denoted as LW(P)), which is normalized within each batch to ensure stability and fair comparison. Another baseline (denoted as LW(W)) is inspired by WPO (Zhou et al., 2024), which computes adaptive weights based on the log-likelihood of the sample response. Detailed formulations for both baselines are provided in the Appendix B.1. Model Configuration. We conduct experiments systematically based on two backbone models: Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct. For SFT, we use learning rate of 2e-6 for 1B on GSM8K and MATH datasets. We adapt LoRA for 8B model with learning rate as 5e-5 for GSM8K and 2e-5 for MATH. For DPO, we adapt learning rate of 2e7 for 1B with β = 0.01 and 2e-6 for 8B using LoRA with β = 0.1 for both datasets. For KTO, we adapt learning rate of 2e-7 for 1B with β = 0.05 and 2e-6 for 8B using LoRA with β = 0.1 for both datasets. For both baselines and EAST, we use the same set of hyperparameters as the vanilla method to ensure fair comparison. Each model is trained for three epochs with batch size of 16 and warmup ratio of 0.1. We adapt the exponent parameters in the range [3, 3] to fully investigate the functionality of the mapping function. Detailed hyperparameter study is provided in Appendix B.2. 4.2 Experiment Results Table 1 presents the performance in terms of accuracy score of EAST compared to the vanilla method and baselines on the GSM8K and MATH benchmarks during the first iteration. The impact of different exponent parameters is shown in Figure 3. Additionally, experiment results from iterative learning are presented in Figure 4. We have the following observations: 6 Preprint. Under review. Table 1: Experimental results in terms of accuracy(%) on GSM8K and MATH benchmarks. The best performance under each loss category is highlighted in bold. Significant boosts ( 1%) of EAST over both the vanilla method and baselines are underlined. Setting default SFT +LW(W) +LW(P) +EAST DPO +LW(W) +LW(P) +EAST KTO +LW(W) +LW(P) +EAST LLaMA-3.2-1B LLaMA-3.1-8B GSM8K(%) MATH(%) AVG(%) GSM8K(%) MATH(%) AVG(%) 46.2 50.1 50.9 51.2 51.8 50.2 50.9 50.3 51.9 53.0 52.9 52.9 53.0 28.5 28.4 28.5 28.4 29.4 28.7 28.1 28.4 29.7 28.8 28.2 28.9 29.9 37.3 39.2 39.7 39.8 40.6 39.5 39.5 39.4 40.8 40.9 40.6 40.9 41. 82.8 85.0 84.8 85.1 86.1 84.6 85.1 85.2 85.4 83.9 84.3 83.9 85.1 50.4 50.0 50.9 50.8 51.2 50.1 50.2 50.8 50.9 48.9 49.1 49.1 51.0 66.6 67.5 67.8 68.0 68.6 67.5 67.6 68.0 68.1 66.4 66.7 66.5 68.1 Observation 1: EAST outperforms the vanilla method and baselines. As shown in Table 1, EAST consistently improves performance compared to the vanilla method and baselines across both benchmarks under all loss functions. The vanilla SFT method struggles to outperform the default backbone model when trained on self-generated data, especially on the challenging MATH dataset. For instance, SFT achieves accuracies of 28.4% and 50.0% on MATH using LLaMA-3.2-1B and LLaMA-3.1-8B, respectively, which are slightly lower than the corresponding default model performances of 28.5% and 50.4%. In comparison, EAST improves the results to 29.4% and 51.2%, demonstrating over 1% absolute gain relative to SFT by focusing on more informative training examples. similar trend is also observed on KTO: the vanilla KTO achieves only 83.9% and 48.9% on the 8B model, while EAST boosts the performance to 85.1% and 51.0%, representing gains of 1.2% and 2.1%, respectively. Integrating EAST with the DPO loss function also leads to consistent gains. For example, on GSM8K with the LLaMA-3.2-1B model, EAST improves performance from 50.2% to 51.9%. We also evaluate baselines (LW(W) and LW(P)) that use local uncertainty information of model. These approaches rely on the likelihood of next-token prediction for given sample response, rather than the overall sample distribution, which also accounts for the correctness of the sample path. Results show that EAST outperforms both local information baselines across all loss functions and benchmarks. Notably, on the MATH dataset using the LLaMA3.2-1B model, local weighting baselines achieve only 28.1% (LW(W)) and 28.4% (LW(P)), which is lower than both the vanilla DPO (28.7%) and the default model (28.5%). In contrast, EAST achieves significantly higher score of 29.7%, suggesting that local weighting may be more sensitive to token-level noise and potentially limiting its training effectiveness. Observation 2: Weighting more on uncertain data contributes to performance improvement. Figure 3 demonstrates the accuracy score of different parameters using SFT method on both benchmarks using LLaMA-3.2-1B model. The figure demonstrates that while perFigure 3: Performance(accuracy (%)) of various exponent parameters on GSM8K and MATH datasets using LLaMA-3.2-1B. 7 Preprint. Under review. formance varies with different values of a, the best results are achieved when > 0 for both datasets. The model achieves peak accuracy on the GSM8K dataset at = 1.5 with 51.8%, compared to 50.6% and 50.8% when = 0.75 and = 1, respectively. Similarly, for the MATH dataset, performance reaches 29.3% at = 1 and 29.4% at = 3, outperforming the 28.1% and 28.5% observed at = 1 and = 0.75, respectively. These results suggest that prioritizing uncertain data helps the model enhance its reasoning ability during training, leading to improved performance. Observation 3: EAST demonstrates consistent benefits in iterative training. To further investigate the performance of EAST in iterative learning, we conduct experiments with iterations = 3 using LLaMA-3.2-1B on both the MATH and GSM8K datasets, with results presented in Figure 4 . EAST consistently outperforms vanilla SFT across iterations on both datasets. Notably, EAST maintains strong performance over time, while vanilla SFT appears to overfit on self-generated data in GSM8K. Although both methods struggle with iterative learning on the MATH dataset, EAST still demonstrates relative advantage. Figure 4: Comparison of iterative learning performance (accuracy (%)) between vanilla SFT and EAST on LLaMA-3.2-1B. 4.3 Ablation Study: Effect of Accuracy-Based and Reject-Based Weights To further investigate the effectiveness of entropy-based weighting, we compare it to alternative weighting strategies grounded in other distributional metrics. Noting that entropy is typically low when single answerwhether correct or incorrectdominates the distribution, we explore two complementary approaches: accuracy-based weighting, which considers the proportion of the correct answer, and rejection-based weighting, which measures the dominance of the most frequent incorrect answer. Accuracy-Based Weights. Accuracy-based weighting leverages the accuracy ratio of model for each question to determine the corresponding weight. Specifically, in the self-training pipeline with samples for each question in the training data and the accuracy score is 1(yi = y), where computed based on the proportion of correct predictions: A(x) = 1 yi represents the i-th sampled prediction and denotes the ground truth. For notational simplicity, let si = 1 A(xi) represent the inverse accuracy score for question and the weight is aggregated using mapping function (si), as detailed in Section 3.2. Intuitively, when si is large, the model faces challenges when solving the problems. i=1 Rejected-Based Weight. Recent studies indicate that large language models (LLMs) struggle with self-correction, particularly when they generate responses with high confidence (Kumar et al., 2024; Yang et al., 2024b). To further investigate this phenomenon, we propose novel weighting scheme that prioritizes the most stubborn questionsthose for which the model repeatedly produces the same incorrect answers. Specifically, for all samples that yield incorrect answers, we partition them into ˆk clusters, where each cluster corresponds to distinct final answer. Next, we calculate the proportion of each incorrect answer cluster and identify the most frequent (dominant) mistake: R(x) = maxj[ˆk] pj. For notational simplicity, let ri = R(xi) represent the inverse accuracy score for question and the weight is aggregated using mapping function (ri), as detailed in Section 3.2. Experiment Result and Analysis. As shown in Equation 1, both A(xi) and R(xi) can be interpreted as components of the probability distribution H(xi) over predicted answers. According to the equation, when either A(xi) or R(xi) is large, the entropy H(xi) tends to be low, reflecting greater certainty in the models predictions. This relationship is further 8 Preprint. Under review. Figure 5: The figure illustrates the distribution of training data in entropy-based, accuracybased, and rejected-based values. Each point represents training example (xi), with coordinates (H(xi), 1 A(xi)) for entropy-based and accuracy-based values, and color indicating the rejected-based value (R(xi)). The accompanying table reports the performance (accuracy(%)) of three weighting strategies on the GSM8K and MATH datasets. illustrated in Figure 5, where higher entropy values are associated with larger (1 A(xi)) (i.e., lower accuracy) and smaller R(xi) values (i.e., greater diversity in incorrect predictions). However, when (1 A(xi)) becomes largeexceeding 0.8, for instancethis does not necessarily imply low R(xi). In fact, Figure 5 shows that many of these low-accuracy samples still exhibit high R(xi) values, indicating repeated, confident errors. As result, applying the mapping function to such cases may overemphasize these stubborn questions during training, potentially skewing the learning dynamics and degrading overall performance. In contrast, entropy-based weighting effectively addresses this problem by automatically assigning lower weights to cases where single incorrect answer dominates. Empirical results of the three weighting strategies on both datasets are reported in Figure 5, using SFT and LLaMA-3.2-1B. The results show that entropy-based weighting outperforms other strategies on both datasets. In contrast, reject-based weighting consistently yields the lowest performance across both benchmarks, while accuracy-based weighting achieves comparable results but exhibits certain limitations."
        },
        {
            "title": "5 Related Work",
            "content": "Self-Training on Mathematical Reasoning. Mathematical reasoning has emerged as critical evaluation benchmark for Large Language Models (LLMs), as it directly correlates with logical reasoning capabilities and provides clear assessment metrics (Azerbayev et al., 2023; Wang et al., 2023; Zhang et al., 2024; Gao et al., 2024; Liu et al., 2024). Traditional methods rely on carefully curated manual datasets as demonstrations for fine-tuning (Yue et al., 2023; Yu et al., 2023a; Luo et al., 2023). As high-quality annotated data are expensive, numerous studies leverage rephrasing methods to augment datasets (Deng et al., 2023; Yu et al., 2023a), or employ strong LLMs to generate synthetic data for knowledge distillation (Taori et al., 2023; Chiang et al., 2023; Ho et al., 2022; Fu et al., 2023; Gou et al., 2023). Recently, several studies have explored using the target model to generate training data and enhance its performance through self-training (Zelikman et al., 2022; Singh et al., 2023; Hosseini et al., 2024; Yu et al., 2023b; Chen et al., 2024; Kumar et al., 2024; Tao et al., 2024), while others extend such techniques for pair-wise alignment methods by leveraging negative samples generated from previous iterations (Tajwar et al., 2024; Xu et al., 2024b; Sun et al., 2024; Zhong et al., 2024; Ivison et al., 2024; Xiong et al., 2024; Xie et al., 2024; Pang et al., 2025). For instance, Sun et al. (2024) compare REST-EM with iterative DPO in self-training pipeline, and Xiong et al. (2024) employs the multi-turn reasoning path for iterative learning process. For this work, we further optimize self-generated data usage by incorporating weighting strategies to improve reasoning capabilities. Alignment Method. Reinforcement Learning from Human Feedback (RLHF) has emerged as an essential framework for aligning machine learning models with human preferences, emphasizing the importance of post-training optimization. Direct Preference Optimization (DPO) is widely recognized method for alignment(Rafailov et al., 2024). Recent studies have explored derivation of DPO (Xu et al., 2024a; Meng et al., 2024; Azar et al., 2024). For 9 Preprint. Under review. example, KTO directly maximizes the utility of generated outputs instead of focusing on the log-likelihood of preferences(Ethayarajh et al., 2024). Some other studies focused on applying local weighting(Zhou et al., 2024) or reward weighting upon DPO (Adler et al., 2024; Xiao et al., 2024; Yang et al., 2024c). RPO incorporates reward gaps into preference learning to mitigate overfitting and better capture nuanced response quality(Adler et al., 2024). However, it relies on an external reward model to assign weights to preference pairs. In contrast, WPO reweights preference pairs based on their likelihood under the current policy (Zhou et al., 2024). Nevertheless, WPO relies solely on local information of the given sample response without considering the overall sample distribution or controlling weight distribution skewness."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents EAST, an entropy-based adaptive weighting method designed to emphasize uncertain data to improve reasoning capabilities during self-training. Through tunable mapping function, EAST adjusts the degree of weighting applied to uncertain data. Experiments on the GSM8K and MATH benchmarks show consistent performance gains, demonstrating the effectiveness of the proposed method. These findings underscore the potential of adaptive weighting in enhancing reasoning capabilities and suggest directions for more effective self-training strategies in future research."
        },
        {
            "title": "References",
            "content": "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play finearXiv preprint tuning converts weak language models to strong language models. arXiv:2401.01335, 2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205, 2023. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pp. 1042110430. PMLR, 2023. 10 Preprint. Under review. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, and Kun Zhang. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models. arXiv preprint arXiv:2402.12563, 2024. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209, 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. Advances in Neural Information Processing Systems, 37:116617116637, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Amir Saeidi, Shivanshu Verma, and Chitta Baral. Insights into alignment: Evaluating dpo and its variants across multiple tasks. arXiv preprint arXiv:2404.14723, 2024. 11 Preprint. Under review. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023. Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, Zhou Zhao, and Fei Wu. comprehensive survey of datasets, theories, variants, and applications in direct preference optimization. arXiv preprint arXiv:2410.15595, 2024. Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning. arXiv preprint arXiv:2410.23123, 2024. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417, 2024a. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. arXiv preprint arXiv:2404.10719, 2024b. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, and Zhifang Sui. Confidence vs critique: decomposition of self-correction capability for llms. arXiv preprint arXiv:2412.19513, 2024b. Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, and Xiaojun Quan. Weightedreward preference optimization for implicit model fusion. arXiv preprint arXiv:2412.03187, 2024c. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023a. 12 Preprint. Under review. Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, and Zhou Yu. Teaching language models to self-improve through interactive demonstrations. arXiv preprint arXiv:2310.13522, 2023b. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024. 13 Preprint. Under review."
        },
        {
            "title": "A Reproducibility",
            "content": "All code will be publicly available in the GitHub. All results are evaluated using the NVIDIA RTX A6000 GPU, following the evaluation pipeline of Yang et al. (2024a)."
        },
        {
            "title": "B Experiment Setup",
            "content": "B.1 Baseline For local uncertainty information weighting, we use the standard perplexity(LW(P)): (cid:32) PPL(x, y) = exp (cid:33) log πθ(yt x, y<t) , 1 y t=1 (8) where πθ(yt x, y<t) denotes the models predicted probability of token yt conditioned on the input and the preceding tokens y<t. We further normalize the perplexity score within each batch by dividing by the batch mean: (cid:103)PPL(x, y) = PPL(x, y) 1 i=1 PPL(x(i), y(i)) , (9) where denotes the batch size, and PPL(x(i), y(i)) is the perplexity of the i-th sample in the batch. For local uncertainty information weighting, we use the formulation from WPO (Zhou et al., 2024) as our local weighting strategy: w(x, y) = exp (cid:32) 1 y t=1 log πθ(yt x, y<t) vV πθ(v x, y<t)2 (cid:33) , (10) For DPO, local weights are computed for both positive and negative pairs and multiplied to obtain the final weight, whereas for SFT, only the positive samples are used. B.2 Hyperparameter Study SFT, For from {2e6, 5e6, 7e6, 1e5}, and in Llama-3.1-8B-Instruct from {2e5, 5e5, 7e5, 1e4}. Llama-3.2-1B-Instruct learning chosen rate the in is For DPO and KTO, we tune the temperature parameter β within the set {0.01, 0.05, 0.1}. In Llama-3.2-1B-Instruct, we search the learning rate in {2e7, 5e7, 7e7, 1e6}, while for Llama-3.1-8B-Instruct, the learning rate is selected from {2e6, 5e6, 7e6, 1e5}. The baseline method and EAST share the same set of hyperparameters as the vanilla method to ensure fair comparison. For EAST, we additionally search the exponent parameter from the range {3, 2.5, 2, 1.5, 1.25, 1, 0.5, 0.1, 0.2, 0.5, 0.7, 1, 1.5, 2, 2.5, 3}. For Llama-3.1-8B-Instruct, we apply LoRA with rank of 16 and LoRA alpha of 16. All models are trained using bf16 precision, and we use the AdamW optimizer. For the ablation study, we report the average accuracy scores for {0.5, 1, 1.5} across all three weighting methods."
        }
    ],
    "affiliations": [
        "University of California Los Angeles"
    ]
}