{
    "paper_title": "GeoWorld: Geometric World Models",
    "authors": [
        "Zeyu Zhang",
        "Danning Li",
        "Ian Reid",
        "Richard Hartley"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld."
        },
        {
            "title": "Start",
            "content": "GeoWorld: Geometric World Models Zeyu Zhang1 Danning Li2 Ian Reid2 Richard Hartley1 1ANU 2MBZUAI https://steve-zeyu-zhang.github.io/GeoWorld 6 2 0 F 6 2 ] . [ 1 8 5 0 3 2 . 2 0 6 2 : r Figure 1. Energy-based planning by GeoWorld. The diagram shows Replace Memory Chip task from the COIN dataset [71], where GeoWorld plans actions by following geodesics over hyperbolic energy landscape rather than generating pixels."
        },
        {
            "title": "Abstract",
            "content": "Energy-based predictive world models provide powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, geometric world model that preserves geometric structure and hierarchical relations through Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. 1. Introduction Autoregressive (AR) next-token prediction has endowed large language models (LLMs) [79] and vision-language models (VLMs) [20, 53] with extensive world knowledge and reasoning capability, enabling them to effectively tackle complex tasks involving searching [78], reasoning [32, 33, 44], and planning [8, 35, 65, 82]. Although the success of LLMs stems from modeling within language space, which serves as shortcut toward human level knowledge [48], they still fail to fully represent the rich information of the real world, such as its physical and geometric properIn the real world, human and biological cognities [3]. tion often acquire knowledge primarily through visual information rather than relying solely on language, as vision offers higher bandwidth of information than language [62]. For example, human infants learn mainly from visual perception during the first few months before developing language system [38], and some animals do not possess language at all [18]. Therefore, there are world models [3, 5, 46, 59, 63] that learn solely from visual input, such as videos, and perform planning with either generative or predictive approach. Generative world models [46, 59, 63] explicitly generate pixels or latent visual tokens that decode into pixels in order to predict only one step at time [67]. As result, they lack awareness of the full trajectory structure or the energy landscape over multiple steps. In contrast, predictive world models [2, 3, 5, 29] such as JPEA [41] do not generate pixels. Instead, they learn an energy landscape in latent space that measures the compatibility between current and target states. This enables multi-step hierarchical planning, where high-level reasoning minimizes energy in latent space, while lower-level modules fill in the physical details. However, existing energy-based predictive world models face two significant challenges: (1) Geometric neglect. Although predictive world models perform multi-step hierarchical planning in latent space, their representations are typically learned in Euclidean space without preserving the underlying geometric relations among states. As result, the learned energy landscape fails to capture meaningful geodesic distances or hierarchical embeddings between latent states [51], which weakens the models ability to perform geometry-consistent planning over long horizons. (2) Multi-step shortcoming. Multi-step videos are limited and expensive to acquire, so existing predictive world models are primarily trained on one-step video transitions [12, 31, 37, 40, 47, 66]. Although learning an energy landscape over entire trajectories conceptually enables long-horizon planning, their performance degrades rapidly as the planning horizon increases, exposing weakness in modeling long-term temporal dependencies. Our motivation is to address these problems from geometric perspective. For the first challenge, geometryaware world model is required to preserve geometric properties when learning the energy landscape for hierarchical planning. For the second challenge, reinforcement learning (RL) has proven effective in adjusting pretrained foundation model when its outputs are unsatisfactory in certain aspects [54, 60]. Therefore, geometry-aware RL method is required to obtain optimal trajectories on the latent manifold, improving the models multi-step planning capability. Hence, we introduce the Geometric World Model (GeoWorld), method that enhances energy-based predictive world models by preserving geometric structure and hierarchical awareness in latent space, as shown in Figure 1. To address the first challenge, we propose Hyperbolic JEPA (H-JEPA), which maps latent representations from Euclidean space Rn onto hyperbolic manifold Hn, where geodesic distances naturally encode hierarchical relations among states. By learning dynamics along hyperbolic (a) V-JEPA 2 [3] Energy Landscape (b) GeoWorld Energy Landscape Figure 2. Energy landscape comparison for V-JEPA 2 [3] and GeoWorld. We visualize the energy by sweeping two orthonormal tangent-space directions (x, y) around reference latent state. GeoWorlds yields structured, curvature-aware energy landscape that better reflects geometric structure and hierarchical relations among latent states and improves energy-based planning. For more details see Appendix 4. geodesics, H-JEPA preserves latent geometry during multistep prediction, ensuring that the learned energy landscape aligns with the underlying structure of the physical world and supports geometry-consistent planning, as shown in 2. To address the second challenge, we design Geometric Reinforcement Learning (GRL) that reformulates multi-step planning as the optimization of an energy-based value function, where lower hyperbolic energy corresponds to higher cumulative reward. GRL directly optimizes the predictor of the world model without training an additional policy or reward model. By adjusting the predictors energy-based value representation through hyperbolic geodesics minimization and triangle inequality regularization, GRL enforces geodesic-consistent rollouts on the latent manifold, effectively improving long-horizon stability and planning performance. To verify our methods capability on long-horizon planning, we evaluate multi-step goal-conditioned visual planning on standard benchmarks, including CrossTask [88] and COIN [71]. Our GeoWorld achieves consistent improvements over the previous state-of-the-art predictive world model V-JEPA 2, including improvements of around 3% SR in 3-step planning and 2% SR in 4-step planning across both datasets. The contributions of our work can be summarized as follows: We introduce the Geometric World Model (GeoWorld) with Hyperbolic JEPA (H-JEPA), which preserves geometric structure and hierarchical relations by mapping latent representations onto hyperbolic manifold and learning dynamics along hyperbolic geodesics, resulting in geometry-consistent energy landscape for multi-step prediction and planning. We propose Geometric Reinforcement Learning (GRL), an energy-based optimization framework that directly refines the predictor through hyperbolic energy minimization and triangle-inequality regularization, enabling geodesic-consistent rollouts and improving long-horizon planning stability. We demonstrate strong performance on long-horizon goal-conditioned visual planning across CrossTask and COIN, achieving around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to V-JEPA 2. 2. Related Works Video World Models There are two primary approaches for video world modeling: generative world models [46, 59, 63] and predictive world models [2, 3, 5, 29, 41]. Generative world models typically build upon autoregressive [24, 43, 77] or semi-autoregressive [17, 22, 34, 58, 72, 81] architectures that observe the visual context and explicitly generate the next frame or its latent representation. These models often incorporate an inverse dynamics module [67] trained to infer actions from consecutive observations, enabling onestep reactive control but preventing multi-step reasoning because the model lacks access to the global trajectory structure and cannot capture long-range dynamics. Moreover, generative approaches must decode visual tokens or pixels during planning, which introduces unnecessary noise and computational overhead and limits their ability to model abstract energy landscapes for hierarchical planning [59]. In contrast, predictive world models do not generate pixels. Instead, they learn an energy landscape in latent space that quantifies the compatibility between current and target states [41]. This design allows for multi-step trajectory optimization using sampling-based planners such as the crossentropy method (CEM) [23], enabling long-horizon planning without explicit pixel decoding. Goal-Conditioned Visual Planning Goal conditioned visual planning aims to produce sequence of actions that achieves given goal based on visual observations. Prior works have evolved into three independent setups depending on the modalities of the observation and the goal, which may be images, videos, or language. (1) In visual planning for assistance (VPA) [56], observations are videos and goals are described in natural language. This typically requires models built on LLMs with multimodal processing capability [16, 36, 84]. (2) In procedural planning (PP) [15], both observations and goals are specified as images without any language involved, which limits the models ability to capture temporal information in the physical world [7, 36, 42, 50, 52, 61, 69, 74, 75, 86, 87]. (3) In visual planning with videos, both observations and goals are given as videos, which aligns more naturally with temporal dynamics in the real world and is commonly addressed using video LLMs [20, 53, 76, 79], generative world models [59], and predictive world models [3]. 3. Method 3.1. Overview We introduce GeoWorld, geometric world model designed to enhance long-horizon visual planning by preserving geometric structure and hierarchical awareness in latent space. To address the limitation of Euclidean latent representations, GeoWorld incorporates Hyperbolic JEPA (HJEPA), which maps encoder outputs from Euclidean space onto hyperbolic manifold where geodesic distances naturally encode hierarchical relations among states. By learning latent dynamics along hyperbolic geodesics, H-JEPA enforces geometry-consistent transitions that better reflect the structure of real-world trajectories. To further improve stability in multi-step prediction, we develop Geometric Reinforcement Learning (GRL), an energy-based optimization framework that treats planning as minimizing hyperbolic value function without training an additional policy or reward model. GRL refines the predictor through hyperbolic energy minimization and triangle-inequality regularization, encouraging geodesic-consistent rollouts and Leveragimproving long-horizon temporal coherence. ing energy-based planning with the Cross-Entropy Method (CEM) [23] further enables efficient trajectory optimization by searching for action sequences that follow geodesic paths in hyperbolic latent space. Together, H-JEPA and GRL form the core of GeoWorld, enabling geometry-aware multi-step planning in predictive world models, as shown in Figure 3. For preliminaries on JEPA [41], hyperbolic geometry, and the value function in RL, see Appendix 1. 3.2. Hyperbolic JEPA From representation perspective, we aim to learn mapping from states onto hyperbolic space Hn such that the optimal plan corresponds to geodesic in hyperbolic space. Hence, we propose Hyperbolic JEPA (H-JEPA), which models latent dynamics on the hyperbolic manifold to preserve hierarchical relations and underlying geometric coherence during multi-step planning. We define the observation at time as xt. Eθ() denotes the pretrained encoder [3], which encodes the observation xt into the latent state sx : = Eθ(xt) Rn. sx (1) To effectively map the encoder output from Euclidean space Rn to hyperbolic space Hn, we interpret the Euclidean embedding sx as tangent vector in the tangent space T0Hn at the origin. We then apply the exponential map at the origin of the Poincaré ball model Bn with curvature = c, which projects the tangent vector onto the hyperbolic manifold, as detailed in Appendix 1.5.1. Figure 3. Overview of GeoWorld. Our geometric world model integrates Hyperbolic JEPA for geometry-preserving latent dynamics and Geometric Reinforcement Learning for geodesic-consistent multi-step refinement. Together with energy-based planning using CEM, GeoWorld enables stable and geometry-aware long-horizon visual planning. Formally, the hyperbolic latent state is obtained as ) = tanh(cid:0) (cid:1) sx csx , csx t,H = exp0(sx sx t,H Bn sx . (2) Then the action-conditioned predictor Pϕ() takes set=1 and corret=1 as input, and predicts t+1,H)T t=1 over quence of hyperbolic latent states (sx sponding sequence of actions (at)T the sequence of next-state representations (ˆsx planning horizon : t,H)T (ˆsx t+1,H)T t=1 = Pϕ (cid:0)(sx t,H, at)T t=1 (cid:1). (3) and θ and ϕ denote the parameters (weights) of the encoder and predictor networks, respectively. 3.3. Training Objective The supervised training objective of H-JEPA is to learn predictive world model that follows the geodesic path of minimum energy cost between the current and target latent states in hyperbolic space. Specifically, the model minimizes the Poincaré-ball hyperbolic distance dH between the predicted and true latent representations, as defined in Eq. 68 of Appendix 1.5.1, ensuring that each transition aligns with the lowest-energy trajectory on the manifold. The objective consists of joint loss combining teacher-forcing loss and rollout loss. The teacher-forcing loss encourages accurate one-step prediction by aligning the predicted next-state representation with the ground-truth latent embedding, while the rollout loss recursively feeds the models own predictions as inputs to enforce temporal consistency across multiple future steps. Teacher Forcing The teacher-forcing loss trains the model to accurately perform one-step future prediction by minimizing the hyperbolic geodesic distance dH between the predicted latent representation ˆsx t+1,H and the encoded ground-truth latent sx t+1,H at each time step t: LTF(θ, ϕ) = = = 1 1 1 T (cid:88) dH (cid:0)Pϕ (cid:0) exp0 (cid:0)Eθ(xt)(cid:1), at (cid:1) , t=1 exp0 (cid:88) (cid:0)Eθ(xt+1)(cid:1)(cid:1) dH (cid:0)ˆsx t+1,H, sx t+1,H (cid:1) (4) (5) t=1 (cid:88) t= 1 arcosh(1 + 2c ˆsx t+1,H2 t+1,H sx t+1,H2)(1 csx (1 cˆsx t+1,H2) (cid:33) (6) Rollout The rollout loss feeds the predictors output back as input, enabling the model to learn multi-step future prediction. In this case, we design two-step rollout loss to enhance the models capability for long-horizon planning: (cid:88) dH (cid:0)Pϕ (cid:0) exp (cid:0)Eθ(xt)(cid:1), at, at+1 (cid:1) , Lrollout(θ, ϕ) = = = 1 1 1 t=1 exp0 (cid:88) (cid:0)Eθ(xt+2)(cid:1)(cid:1) dH (cid:0)ˆsx t+2,H, sx t+2,H (cid:1) t=1 (cid:88) t= 1 arcosh(1 (7) (8) (cid:33) + 2c t+2,H2 t+2,H sx ˆsx t+2,H2)(1 csx (1 cˆsx , t+2,H2) (9) Total Loss Hence, the total loss in the supervised stage is defined as fore, the optimal path value function maximizes the expected cumulative reward: LSFT(θ, ϕ) = λ LTF(θ, ϕ) + (1 λ) Lrollout(θ, ϕ), (10) where λ is loss weighting hyperparameter. Together, these two components train the predictor to learn smooth, geodesically consistent trajectories that capture both short-term accuracy and long-horizon stability within the hyperbolic latent space. 3.4. Geometric Reinforcement Learning We propose Geometric Reinforcement Learning (GRL) approach that improves the predictor in multi-step planning by adjusting its energy-based value representation, aligning lower energy with higher expected reward. Energy Cost Given frozen encoder and trainable predictor Pϕ, we define the energy cost of moving from state sx t,H to state sx t+1,H as ct(sx t,H, sx t+1,H) = dH (cid:0)Pϕ (cid:0) exp0 (cid:0)E(xt)(cid:1), at (cid:1), exp0 = dH(ˆsx t+1,H, sx t+1,H). (cid:0)E(xt+1)(cid:1)(cid:1) (11) (12) which ideally indicates that we aim to minimize the energy cost of moving from state sx t+1,H, which is identical to minimizing the geodesic distance between the t+1,H and the target state sx predicted state ˆsx t,H to state sx t+1,H. Reward We then define the reward as the negative energy cost of moving between states: (sx 1,H, sx 1+T,H) = max ϕ (cid:34) (cid:88) Ea1:T ϕ γt1 rt(sx t,H, at, = min ϕ Ea1:T ϕ t=1 t+1,H)(cid:3) . sx (cid:34) (cid:88) γt1 dH(ˆsx t+1,H, t=1 t+1,H)(cid:3) . sx (15) which is equivalent to minimizing the total hyperbolic distance between the predicted and target states. Triangle Inequality Regularization The hyperbolic geodesic distance dH satisfies the triangle inequality. Therefore, for any consecutive triplet in the predictors rollouts: dH(ˆsx t,H, ˆsx t,H, ˆsx t+1,H) + dH(ˆsx t+2,H) dH(ˆsx t+2,H). (16) This indicates that minimizing the sum of consecutive step distances encourages the predicted trajectory to align with the geodesic path. Hence, we introduce regularization term: t+1,H, ˆsx = 1 2 2 (cid:88) (cid:104) t=1 dH(ˆst, ˆst+2) dH(ˆst, ˆst+1) dH(ˆst+1, ˆst+2) (cid:105) + . (17) This term enforces multi-step rollout consistency by encouraging predicted trajectories to satisfy hyperbolic geodesic properties. rt(sx t,H, at, sx t+1,H) = ct(sx t,H, sx t+1,H). (13) Total Loss Hence, the total loss in Geometric Reinforcement Learning can be expressed as Path Value Function As mentioned in Appendix 1.6, the value function is mathematical object that quantifies the amount of energy required for an agent to reach optimality from given state to target state, where lower energy corresponds to higher expected cumulative reward. Hence, the path value function between the current and goal latent states, given planning horizon , is defined as the expected cumulative reward: (sx 1,H, sx 1+T,H) = Ea1:T ϕ (cid:34) (cid:88) t=1 γt1rt(sx t,H, at, sx (cid:35) t+1,H) (14) where γ [0, 1) is the discount factor. Our objective is to maximize the total reward (i.e., maximize the return) such that Pϕ follows the geodesics. ThereLGRL(ϕ) = Ea1:T ϕ (cid:34) (cid:88) t=1 γt1dH(ˆsx t+1,H, sx t+1,H) (cid:35) +βL. (18) where β is the regularization factor. 3.5. Energy-Based Planning , We then perform energy-based planning after training, with the frozen encoder and predictor . The predictor serves as world model, capable of predicting how latent representations evolve when an action sequence is applied. During planning, we search for an optimal action sequence that follows the geodesic path between the current and goal latent states, effectively minimizing goal-conditioned energy cost defined in the hyperbolic latent space. Given the current observation x1, the future target x1+T , and the planning horizon , we encode the current and goal observations as of the predicted actions at each time step. (3) Mean Intersection over Union (mIoU) quantifies the overlap between the predicted procedure and the ground truth. sx 1,H = exp0(E(x1)), sx 1+T,H = exp0(E(x1+T )). (19) We then define the energy cost function based on the Poincaré geodesic distance, which measures the hyperbolic energy between the predicted and goal latent states over the planning horizon: C((ˆat)T t=1; sx 1,H, sx 1+T,H) = dH (cid:0)P ((ˆat)T t=1; sx Hence, the optimal action sequence (a by minimizing this hyperbolic energy cost: )T 1+T,H 1,H), sx (cid:1) , (20) t=1 is obtained (a )T t=1 = arg min (ˆat)T t=1 dH (cid:0)P ((ˆat)T t=1; sx 1,H), sx 1+T,H (cid:1) . (21) The optimization is performed with the Cross-Entropy Method (CEM) [23], as detailed in Algorithm 1 of Appendix 1.3. (a )T t=1 = CEM(x1, x1+T , (), E(), T, N, K, I, µ0, Σ0) (22) where x1 is the current observation, x1+T is the goal observation, Pϕ is the predictor, E() is the encoder, is the planning horizon, is the number of samples, is the number of elites, is the number of iterations, and (µ0, Σ0) denote the initial mean and covariance of the action distribution. 4. Experiments 4.1. Benchmarks and Evaluation Metrics Benchmarks For evaluating our world models capability in multi-step goal-conditioned planning, we adapt two standard goal-conditioned visual planning datasets, CrossTask [88] and COIN [71], which contain diverse finegrained action labels and timestamps of human daily activities. CrossTask consists of 4.7K videos across 83 tasks, covering 105 actions, with an average of 8 actions per video. The total duration is 375h. COIN consists of 11,287 videos across 180 tasks, covering 778 actions, with an average of 3.9 actions per video. The total duration is 476h. Metrics Following previous works in goal-conditioned visual planning [7], we adopt three metrics for evaluation: (1) Success Rate (SR) computes whether the predicted action sequence exactly matches the ground truth sequence. (2) Mean Accuracy (mAcc) computes the average accuracy 4.2. Baseline and Evaluation Protocol We follow previous works [3, 7, 15, 59] and evaluate goalconditioned visual planning in two setups based on the modality of the observation and the target, as discussed in Section 2. For procedural planning [15], both observations and goals are specified as images, which is more aligned with the traditional visual planning setup. For visual planning with videos [59], both observations and goals are specified as video clips, which more faithfully reflect the temporalspatial information in the real world. For both setups, evaluation is conducted over planning horizon , where the model outputs sequence of actions given the observation and the goal. In both setups, we include three categories of baselines. LLM-based methods leverage LLMs or VLMs for reasoning and planning [20, 36, 42, 52, 53, 76, 79]. Generative (world) models explicitly generate pixels or latent visual tokens that decode into pixels for planning [7, 15, 50, 59, 61, 75, 86, 87]. Predictive (world) models predict sequence of actions without relying on pixel generation [1, 3, 27, 68, 69, 74]. There are two extra baselines in the procedural planning setup. Random randomly selects an action from all actions and serves as the empirical lower bound of performance [15]. The Retrieval-Based approach retrieves the nearest neighbor by minimizing the visual feature distance within the training dataset, and the action sequence associated with the retrieved neighbor is then used as the plan [87]. Besides, for both V-JEPA 2 [3] and our GeoWorld, we adopt frozen encoders, while for VideoWorld [59] we perform full finetuning. For general VLMs [20, 53, 76, 79] in visual planning with videos, all evaluations are conducted in zero-shot setting. For more details on the baselines, see Appendix 3. 4.3. Implementation Details For fair comparison, both the V-JEPA 2 [3] baseline and our GeoWorld adopt frozen encoders pretrained on VideoMix22M. The exponential map exp0() is implemented and trained as differentiable hyperbolic projection layer, where the curvature is treated as learnable parameter [14]. The predictor network Pϕ() is 300Mparameter transformer with 24 layers, 16 heads, 1024dimensional hidden size, and GELU activations. We conduct two-stage training procedure for both VJEPA 2 and our GeoWorld, consisting of supervised posttraining followed by geometric reinforcement learning. In the supervised post-training stage, both V-JEPA 2 and Table 1. Goal-conditioned visual planning with images on CrossTask [88] and COIN [71] datasets. We evaluate multi-step planning over horizon under the procedural planning setup [15], where both observations and goals are specified as images. Method CrossTask Dataset [88] COIN Dataset [71] T=3 T=4 T=3 T=4 SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU Random [15] Retrieval-Based [87] 0.01 8.05 0.94 23.30 1.66 32.06 0.01 3.95 1.83 22. 1.66 36.97 0.01 0.01 2.47 0.01 0.01 2.32 LLM-Based LFP [42] VidAssist (zero-shot) [36] VidAssist [36] SCHEMA [52] Generative (World) Models DDN [15] Int-MGAIL [7] Ext-MGAIL [7] P3IV [86] PDPP [75] KEPP [50] ActionDiffusion [61] MTID [87] Predictive (World) Models WLTDO [27] UAAA [1] UPN [68] PlaTe [69] E3P [74] V-JEPA 2 ViT-L [3] V-JEPA 2 ViT-H [3] V-JEPA 2 ViT-g [3] V-JEPA 2 ViT-g384 [3] GeoWorld ViT-L (Ours) GeoWorld ViT-H (Ours) GeoWorld ViT-g (Ours) GeoWorld ViT-g384 (Ours) 30.55 14.60 28.85 38.93 12.18 17.03 21.27 23.34 37.20 38.12 37.79 40. 1.87 2.15 2.89 16.00 26.40 43.33 44.07 44.84 45.58 43.89 45.33 46.25 47.47 59.59 52.60 58.12 63.80 31.29 44.66 49.46 49.96 64.67 64.74 65.38 67.19 21.64 20.21 24.39 36.17 53.02 68.63 70.18 71.62 72.74 68.96 70.84 71.95 73.69 76.86 68.38 75.36 79.82 47.48 58.08 61.70 73.89 66.57 67.15 67.45 69. 31.70 30.87 31.56 65.91 74.05 67.84 68.32 68.87 69.42 82.93 84.70 85.44 86.55 15.97 9.89 15.45 24.50 5.97 9.47 16.41 13.40 21.48 24.15 22.43 24.76 0.77 0.98 1.19 14.00 16.49 27.53 28.75 30.03 31.36 27.64 29.19 30.63 31.48 50.70 40.85 51.51 58.48 27.10 37.16 43.05 44.16 57.82 59.05 59.42 60. 17.92 19.86 21.59 35.29 48.00 63.80 64.71 65.04 65.45 64.35 65.47 66.02 67.30 75.30 70.35 72.61 76.48 48.46 57.24 60.93 70.01 65.13 66.64 66.04 67.67 26.43 27.09 27.85 55.36 70.16 65.45 66.82 67.93 69.21 79.43 80.16 81.82 82.48 30.64 18.44 29.20 32.09 13.90 15.40 21.33 20.25 24.00 30. 19.57 32.10 32.76 33.42 34.08 33.42 34.08 34.41 34.85 54.72 50.63 54.76 49.84 20.19 21.67 45.62 39.87 45.42 51.70 31.42 54.25 55.37 56.29 57.20 57.26 58.70 60.47 61.86 76.86 75.64 78.02 83.83 64.78 76.31 51.82 51.72 54.29 59. 84.95 61.18 61.57 63.31 64.53 88.03 88.42 89.00 89.88 15.97 9.07 20.78 22.02 11.13 11.32 14.41 15.63 18.04 22.74 13.59 20.86 22.60 23.04 23.43 24.96 26.24 27.46 27.79 50.70 42.72 49.07 45.33 17.71 18.85 44.10 39.53 44.54 49. 26.72 52.61 53.19 54.47 55.58 52.92 53.66 54.55 55.97 75.30 80.83 78.93 83.47 68.06 70.53 51.39 53.27 56.23 61.25 84.72 64.33 65.74 66.13 66.57 85.26 87.17 88.20 88.61 GeoWorld are trained with the AdamW optimizer [45] using warmupconstantdecay learning rate schedule and constant weight decay of 0.04. We linearly warm up the learning rate from 7.5 105 to 4.25 104 over 4500 iterations, hold it constant for 85,500 iterations, and then decay it to 0 over the final 4500 iterations, with batch size of 256. For geometric reinforcement learning, we keep the same AdamW optimizer and weight decay as in the supervised post-training stage, but adopt smaller learning rate and shorter schedule due to the higher variance of the RL objective. Specifically, we linearly warm up the learning rate from 5.0 105 to 2.0 104 over 2,000 iterations, hold it constant for 18,000 iterations, and then linearly decay it to 0 over the final 5,000 iterations, with batch size of 128. Unless otherwise specified, we set the discount factor to γ = 0.99 and the triangle-inequality regularization weight to β = 0.1. For energy-based planning with CEM [23], we adopt sample size of = 800, an elite set size of = 80, and = 10 refinement iterations. The entire training is conducted on 4 nodes, each equipped with 8 NVIDIA H100 GPUs, 48-core Intel Xeon Platinum 8469C CPUs, and 230 GB of RAM. We use only single H100 GPU for inference. 4.4. Main Results As shown in Table 1 and 2, GeoWorld consistently improves multi-step goal-conditioned visual planning across both CrossTask and COIN. Under the procedural planning setup, GeoWorld yields notable gains over prior predictive world models, especially in long-horizon settings, achieving higher SR, mAcc, and mIoU for both =3 and =4. In the video-based planning setup, GeoWorld continues to outperform V-JEPA 2 across all model scales, with the ViTg384 variant achieving the best overall results and surpassTable 2. Goal-conditioned visual planning with videos on CrossTask [88] and COIN [71] datasets. We evaluate multi-step planning over horizon under the visual planning with videos [59] setup, where both observations and goals are specified as video clips. Method LLM-Based InternVL3.5-241B [76] Qwen3-VL-Max [79] Gemini 2.5 Pro [20] GPT-5 [53] Generative (World) Models VideoWorld [59] Predictive (World) Models V-JEPA 2 ViT-L [3] V-JEPA 2 ViT-H [3] V-JEPA 2 ViT-g [3] V-JEPA 2 ViT-g384 [3] GeoWorld ViT-L (Ours) GeoWorld ViT-H (Ours) GeoWorld ViT-g (Ours) GeoWorld ViT-g384 (Ours) CrossTask Dataset [88] COIN Dataset [71] T= T=4 T=3 T=4 SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU 44.03 45.47 48.91 50. 70.01 70.93 73.82 72.38 84.41 86.18 90.30 91.18 27.65 28.76 31.53 30.20 63.54 62.91 60.58 64.48 80.13 81.51 84.56 82.15 36.54 37.56 42.07 43. 57.22 57.80 61.02 64.67 89.02 90.46 92.94 91.12 25.46 26.17 30.20 32.64 55.30 57.13 60.13 56.84 88.22 87.56 84.82 86.38 41. 66.11 82.64 25.50 60.26 76.85 34. 54.71 85.58 23.74 51.27 85.33 43.36 46.02 48.13 50.16 44.80 47.79 49.23 51. 69.55 71.98 73.42 74.86 70.54 74.42 76.64 77.30 84.75 87.29 89.62 91.73 86.30 88.84 90.61 92.95 28.86 32.18 33.46 35.01 30.63 34.51 35.49 37.04 64.34 67.23 69.26 70.24 65.46 68.89 71.00 71.35 78.40 80.84 82.61 85.05 79.73 82.95 84.50 87.04 36.10 39.42 40.97 42.74 37.76 40.40 42.84 45. 56.70 59.42 61.86 64.08 58.14 60.97 62.63 65.52 87.02 89.44 90.77 91.88 88.00 91.66 93.69 93.91 25.29 27.38 29.60 31.63 26.40 28.82 29.93 33.29 53.30 56.07 57.73 59.28 54.52 58.10 60.65 61.56 87.21 90.20 92.23 94.51 88.98 91.48 92.81 95.84 Table 3. Long horzion planning on CrossTask [88]. 4.5. Long-Horizon Planning Method Successful Rate (SR, %) T=3 T=4 T= T=6 Procedural Planning (PP) Random [15] Retrieval-Based [87] DDN [15] P3IV [86] E3P [74] PDPP [75] KEPP [50] SCHEMA [52] MTID [87] V-JEPA 2 ViT-L [3] 0.01 8.05 12.18 23.34 26.40 37.20 38.12 38.93 40.45 43.33 0.01 3.95 5.97 13.40 16.49 21.48 24.15 24.50 24.76 27.53 0.01 2.40 3.10 7.21 8.96 13.45 14.20 14.75 15.26 16. 0.01 1.10 1.20 4.40 5.76 8.41 9.27 10.53 10.30 11.55 GeoWorld ViT-L (Ours) 43.89 27.64 17.38 12. Visual Planning with Videos VideoWorld [59] InternVL3.5-241B [76] Qwen3-VL-Max [79] Gemini 2.5 Pro [20] GPT-5 [53] V-JEPA 2 ViT-g384 [3] 41.59 44.03 45.47 48.91 50.03 50.16 25.50 27.65 28.76 31.53 30.20 35.01 15.36 17.31 17.95 20.08 21.46 23.17 10.97 12.44 13.20 15.93 16.07 16. GeoWorld ViT-g384 (Ours) 51.71 37.04 24.83 18.26 ing strong LLM-based planners. These improvements highlight the effectiveness of geometry-aware latent dynamics and geometric reinforcement learning in enhancing longhorizon stability and planning accuracy. For ablation study, please refer to Appendix 5. Table 3 highlights GeoWorlds strength in long-horizon planning. As the horizon increases from = 3 to = 6, the performance of existing predictive and generative world models consistently degrades due to accumulated geometric drift in Euclidean latent space. In contrast, GeoWorld maintains higher stability and achieves the best Success Rate across all horizons. 5. Conclusion We introduced GeoWorld, geometric world model designed to improve long-horizon visual planning by preserving geometric structure and hierarchical relations in latent space. Through Hyperbolic JEPA, GeoWorld maps Euclidean latent representations onto hyperbolic manifold, enabling geodesic-aware latent dynamics that produce more structured and physically meaningful energy landscape. Building on this representation, Geometric Reinforcement Learning refines the predictor via hyperbolic energy optimization and triangle-inequality regularization, yielding geodesic-consistent rollouts and reducing error accumulation across extended horizons. Extensive experiments on CrossTask and COIN demonstrate that GeoWorld consistently improves long-horizon performance over strong predictive world models such as V-JEPA 2, achieving higher success rates across = 3 to = 6 planning. These results highlight the importance of incorporating geometric principles into predictive world models and reinforce the value of geometry-aware reinforcement learning for stable and effective multi-step planning."
        },
        {
            "title": "References",
            "content": "[1] Yazan Abu Farha and Juergen Gall. Uncertainty-aware anticipation of activities. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 00, 2019. 6, 7, 10 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 2, 3, 1, 4 [3] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Selfsupervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 1, 2, 3, 6, 7, 8, 4, 10, 12, 13 [4] Adrien Bardes, Jean Ponce, and Yann Lecun. Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. In ICLR 2022-International Conference on Learning Representations, 2022. [5] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. Transactions on Machine Learning Research, 2024. 2, 3, 1, 4 [6] Richard Bellman. Dynamic programming. science, 153 (3731):3437, 1966. 8 [7] Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning in instructional videos via contextual modeling and modelbased policy learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15611 15620, 2021. 3, 6, 7, 9 [8] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: VisionLanguage arXiv Action Flow Model for General Robot Control. preprint arXiv:2410.24164, 2024. [9] Rinu Boney, Juho Kannala, and Alexander Ilin. Regularizing model-based planning with energy-based models. In Conference on Robot Learning, pages 182191. PMLR, 2020. 1 [10] David Brandfonbrener, Ofir Nachum, and Joan Bruna. Inverse dynamics pretraining learns good representations for multitask imitation. Advances in Neural Information Processing Systems, 36:6695366978, 2023. 4 [11] Martin Bridson and André Haefliger. Metric spaces of non-positive curvature. Springer Science & Business Media, 2013. 5 [12] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 2 [13] Edoardo Cetin, Benjamin Paul Chamberlain, Michael Bronstein, and Jonathan Hunt. Hyperbolic deep reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. 5, 7 [14] Ines Chami, Zhitao Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural networks. Advances in neural information processing systems, 32, 2019. 6, [15] Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. Procedure planning in instructional videos. In European Conference on Computer Vision, pages 334350. Springer, 2020. 3, 6, 7, 8, 9 [16] Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, and Pascale Fung. Planning with reasoning using vision language world model. arXiv preprint arXiv:2509.02722, 2025. 3 [17] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 3 [18] Dorothy Cheney and Robert Seyfarth. Why animals dont have language. Tanner lectures on human values, 19: 173210, 1998. 2 [19] Krzysztof Chris Ciesielski, Alexandre Xavier Falcão, and Paulo AV Miranda. Path-value functions for which dijkstras algorithm returns optimal mapping. Journal of Mathematical Imaging and Vision, 60(7):10251036, 2018. 8 [20] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 3, 6, 8, [21] Balázs Csanád Csáji and László Monostori. Value function based reinforcement learning in changing markovian enviJournal of Machine Learning Research, 9(8), ronments. 2008. 8 [22] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 3 [23] Pieter-Tjerk De Boer, Dirk Kroese, Shie Mannor, and Reuven Rubinstein. tutorial on the cross-entropy method. Annals of operations research, 134(1):1967, 2005. 3, 6, 7, 4 [24] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3 [25] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. HyperIn International Conferbolic image-text representations. ence on Machine Learning, pages 76947731. PMLR, 2023. 5, 11 [26] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Learning iterative reasoning through energy minimization. In International Conference on Machine Learning, pages 55705582. PMLR, 2022. [27] Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi, and Ali Farhadi. Who let the dogs out? In Proceedings modeling dog behavior from visual data. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 40514060, 2018. 6, 7, 10 [28] Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in neural information processing systems, 31, 2018. 5, 7 [29] Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, and Yann LeCun. Learning and leveraging world models in visual representation learning. arXiv preprint arXiv:2403.00504, 2024. 2, 3, 4 [30] Songwei Ge, Shlok Mishra, Simon Kornblith, Chun-Liang Li, and David Jacobs. Hyperbolic contrastive learning for In Proceedings of visual representations beyond objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 68406849, 2023. 5 [31] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [32] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [33] Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. 1 [34] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 3 [35] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: VisionLanguageAction Model with Open-World Generalization. arXiv preprint arXiv:2504.16054, 2025. 1 [36] Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, FuJen Chu, Kris Kitani, Gedas Bertasius, and Xitong Yang. Propose, assess, search: Harnessing llms for goal-oriented planning in instructional videos. In European Conference on Computer Vision, pages 436452. Springer, 2024. 3, 6, 7, 9 [37] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 2 [38] Philip Kellman, ME Arterberry, Damon, RM Lerner, Kuhn, RS Siegler, et al. Infant visual perception. 2006. 1 [39] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild In RSS 2024 Workshop: Data robot manipulation dataset. Generation for Robotics. 3 [40] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video In 2011 Interdatabase for human motion recognition. national conference on computer vision, pages 25562563. IEEE, 2011. 2 [41] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 2, 3, 1, 4, 5, 13 [42] Jiateng Liu, Sha Li, Zhenhailong Wang, Manling Li, and Heng Ji. language-first approach for procedure planning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 19411954, 2023. 3, 6, 7, 9 [43] Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing InfiniWang, Yi Jiang, Bingyue Peng, and Zehuan Yuan. tystar: Unified spacetime autoregressive modeling for visual generation. arXiv preprint arXiv:2511.04675, 2025. 3 [44] Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025. [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 [46] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. 2023. 2, 3, 4 [47] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. Ivan Laptev, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. 2 [48] Melanie Mitchell and David Krakauer. The debate over understanding in ais large language models. Proceedings of the National Academy of Sciences, 120(13):e2215907120, 2023. 1 [49] Sho Mitsuhashi and Shin Ishii. Triangle inequality for inIEEE Access, 11:119187119199, verse optimal control. 2023. [50] Kumaranage Ravindu Yasas Nagasinghe, Honglu Zhou, Malitha Gunawardhana, Martin Renqiang Min, Daniel Harari, and Muhammad Haris Khan. Why not use your textbook? knowledge-enhanced procedure planning of instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18816 18826, 2024. 3, 6, 7, 8, 9 [51] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. Advances in neural information processing systems, 30, 2017. 2, 5 [52] Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, and Shih-Fu Chang. Schema: State changes matter for proarXiv preprint cedure planning in instructional videos. arXiv:2403.01599, 2024. 3, 6, 7, 8, 9 [53] OpenAI. Gpt-5 system card, version 1.0, 2025-08-13. 2025. https : / / cdn . openai . com / gpt - 5 - system - card.pdf. 1, 3, 6, 8, 9 [54] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. [67] Mark Spong and Romeo Ortega. On adaptive inverse dynamics control of rigid robots. IEEE Transactions on Automatic Control, 35(1):9295, 2002. 2, 3, 4 [55] Avik Pal, Max van Spengler, Guido Maria DAmely di Melendugno, Alessandro Flaborea, Fabio Galasso, and Pascal Mettes. Compositional entailment learning for hyperbolic vision-language models. arXiv preprint arXiv:2410.06912, 2024. 5 [56] Dhruvesh Patel, Hamid Eghbalzadeh, Nitin Kamra, Michael Louis Iuzzolino, Unnat Jain, and Ruta Desai. Pretrained language models as visual planners for human In Proceedings of the IEEE/CVF International assistance. Conference on Computer Vision, pages 1530215314, 2023. 3 [57] Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba. An inductive bias for distances: Neural nets that respect the triIn International Conference on Learning angle inequality. Representations, 2020. 8 [58] Sucheng Ren, Chen Chen, Zhenbang Wang, Liangchen Song, Xiangxin Zhu, Alan Yuille, Yinfei Yang, and Jiasen Lu. Autoregressive video generation beyond next frames prediction. arXiv preprint arXiv:2509.24081, 2025. 3 [59] Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. Videoworld: Exploring knowledge learning from unlabeled videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2902929039, 2025. 2, 3, 6, 8, 4, 9 [60] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [61] Lei Shi, Paul Bürkner, and Andreas Bulling. Actiondiffusion: An action-aware diffusion model for procedure planning in instructional videos. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 88168825. IEEE, 2025. 3, 6, 7, 9 [62] Mariano Sigman and Stanislas Dehaene. Brain mechanisms of serial and parallel processing during dual-task performance. Journal of Neuroscience, 28(30):75857598, 2008. 1 [63] Himanshu Gaurav Singh, Antonio Loquercio, Carmelo Sferrazza, Jane Wu, Haozhi Qi, Pieter Abbeel, and Jitendra Malik. Hand-object interaction pretraining from videos. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 33523360. IEEE, 2025. 2, 3, 4 [64] Geri Skenderi, Hang Li, Jiliang Tang, and Marco Cristani. Graph-level representation learning with joint-embedding predictive architectures. Transactions on Machine Learning Research, 2025. 9 [65] Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulaarXiv preprint tion with large vision-language models. arXiv:2505.16517, 2025. 1 [66] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [68] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks: Learning generalizable representations for visuomotor control. In International conference on machine learning, pages 4732 4741. PMLR, 2018. 6, 7, 10 [69] Jiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg. Plate: Visually-grounded planning with transformers in procedural tasks. IEEE Robotics and Automation Letters, 7(2):49244930, 2022. 3, 6, 7, 10 [70] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction. MIT press Cambridge, 1998. 8 [71] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video In Proceedings of the IEEE/CVF Conference analysis. on Computer Vision and Pattern Recognition, pages 1207 1216, 2019. 1, 2, 6, 7, 8, 10, 13 [72] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 3 [73] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In The Thirteenth International Conference on Learning Representations, 2025. [74] An-Lan Wang, Kun-Yu Lin, Jia-Run Du, Jingke Meng, and Wei-Shi Zheng. Event-guided procedure planning from instructional videos with text supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1356513575, 2023. 3, 6, 7, 8, 10 [75] Hanlin Wang, Yilu Wu, Sheng Guo, and Limin Wang. Pdpp: Projected diffusion for procedure planning in instructional In Proceedings of the IEEE/CVF Conference on videos. Computer Vision and Pattern Recognition, pages 14836 14845, 2023. 3, 6, 7, 8, 9 [76] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 3, 6, 8, 9 [77] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. 3 [78] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [79] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1, 3, 6, 8, [80] Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, and Chuang Gan. Imitation learning from observations by minimizing inverse dynamics disagreement. Advances in neural information processing systems, 32, 2019. 4 [81] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 3 [82] Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reaarXiv preprint soning in vision-language-action models. arXiv:2510.01623, 2025. 1 [83] Yun Yue, Fangzhou Lin, Kazunori Yamada, and Ziming Zhang. Hyperbolic contrastive learning. arXiv preprint arXiv:2302.01409, 2023. 5 [84] Ce Zhang, Yale Song, Ruta Desai, Michael Louis Iuzzolino, Joseph Tighe, Gedas Bertasius, and Satwik Kottur. Enhancing visual planning with auxiliary tasks and multi-token prediction. arXiv preprint arXiv:2507.15130, 2025. 3 [85] Zeyu Zhang, Yiran Wang, Danning Li, Dong Gong, Ian Reid, and Richard Hartley. Flashmo: Geometric interpolants and frequency-aware sparsity for scalable efficient motion generation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [86] He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos Derpanis, Richard Wildes, and Allan Jepson. P3iv: Probabilistic procedure planning from instructional videos with In Proceedings of the IEEE/CVF Conweak supervision. ference on Computer Vision and Pattern Recognition, pages 29382948, 2022. 3, 6, 7, 8, 9 [87] Yufan Zhou, Zhaobo Qi, Lingshuai Lin, Junqi Jing, Tingting Chai, Beichen Zhang, Shuhui Wang, and Weigang Zhang. Masked temporal interpolation diffusion for proarXiv preprint cedure planning in instructional videos. arXiv:2507.03393, 2025. 3, 6, 7, 8, 9, 13 [88] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Crosstask weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35373545, 2019. 2, 6, 7, 8, 11, 12, 13 GeoWorld: Geometric World Models"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Preliminaries mal plan is defined as: 1.1. Energy-Based World Models Energy-Based World Models (EBWM) [2, 3, 5, 41] are derived from Energy-Based Models (EBM) [9, 26], which define scalar energy function (x, y) that measures how compatible two variables are, such as current world state sx and possible future state sy. low energy value corresponds to plausible scenario, while high energy value indicates an implausible one. Instead of predicting single future, the energy landscape implicitly represents all plausible futures as valleys of low energy. Because energy replaces probability, the model can naturally handle multi-modal or uncertain worlds without the need for explicit sampling or normalization. Reasoning and planning are therefore formulated as energy minimization, where the goal is to find the configuration (actions, latents, or next states) that minimizes the expected energy: Plan = arg min actions (st, st+1:T ). (23) 1.2. Hierarchical Planning Hierarchical planning often consists of two levels: highlevel planning computes trajectories in abstract latent space that minimize energy (i.e., the most plausible and least costly transitions), while lower levels fill in the physical details [41]. The world model learns hierarchical latent abstractions, where low-level modules predict short-term fine details and higher-level modules capture long-term abstract dynamics. and s(2) Specifically, planning becomes energy minimization in latent space. High-level modules operate on abstract latent states s(2) that evolve slowly, where planning corresponds to finding geodesic of minimum energy between the abstract states s(2) , yielding coarse trajectory that serves as the overall plan. Lower levels then refine this trajectory into fine-grained predictions s(1) , minimizing subenergies conditioned on the higher-level plan. As result, the overall behavior emerges through hierarchical energy descent, where each layer enforces consistency between its predictions and the layer above. In other words, high-level planning computes trajectories in abstract latent space that minimize energy (i.e., the most plausible and least costly transitions), while lower levels fill in the physical details. Optimal Plan: min {at,zt} l=1 (cid:88) (cid:88) (l)(cid:0)s(l) , s(l) t+1, z(l) (cid:1). (24) Each (l) expresses the energy cost of moving between abstract states at level l, and gradients through this hierarchy yield coherent plan across scales. 1.3. Joint-Embedding Predictive Architecture Joint-Embedding Predictive Architectures (JEPA) [41] learn predictive world model directly in latent space rather than generating pixels. JEPA encodes observations into compact representation space and predicts future latent states by minimizing an energy or similarity objective between encoded targets and predicted embeddings. This joint-embedding formulation bypasses the need for autoregressive pixel generation, which is computationally expensive and prone to error accumulation over long horizons. Learning in latent space instead focuses the model on high-level structure, semantics, and temporal dependencies rather than low-level appearance details, enabling more stable and efficient multi-step prediction. By operating on representations rather than images, JEPA captures the underlying dynamics of the environment while avoiding the challenges of modeling raw pixel distributions. 1.3.1. JEPA We define the current observation and target y. Eθ() denotes the observation encoder that maps raw visual inputs into the latent representation space, Eθ() denotes the target encoder with exponential moving average (EMA) weights, sx and sy are the latent representation of the current and target states obtained from the encoders, and is latent variable capturing uncertainty. Pϕ() denotes the predictor, and θ and ϕ denote the parameters (weights) of the encoder and predictor networks, respectively. For unified one-step observationtarget formulation, we first encode the current observation: sx = Eθ(x) (25) Then we perform latent prediction, which takes the current state sx and the uncertainty as input and predicts the latent representation of the target state ˆsy: ˆsy = Pϕ(sx, z) (26) Formally, for hierarchical world model (L), the optiSimilarly, we encode the target representation: sy = Eθ(y) (27) For planning, the training objective becomes an energy minimization in latent space, which involves finding geodesic of minimum energy between sx and sy to obtain the optimal plan. min C(sx, sy, z), where sx = Eθ(x), sy = Eθ(y). (28) Here, represents the energy cost of moving between the abstract states sx and sy. It learns two encoders (for past and future) and predictor that maps sx to sy, where the energy is defined as the representation mismatch between the predicted and true embeddings. The JEPA is non-generative and is trained non-contrastively to ensure that the embeddings remain both informative and predictable [4]. As result, it forms predictive world model that learns latent abstractions. Hierarchical-JEPA If we extend to hierarchical planning, we can develop model that learns hierarchical latent abstractions (Hierarchical-JEPA), in which low-level modules predict short-term fine details, while higher-level modules capture long-term abstract dynamics. As an example, consider two-level model where high-level JEPA layers operate on abstract latent states s(2) that evolve slowly. Planning in this context corresponds to finding geodesic of minimum energy between the abstract states s(2) and s(2) , yielding coarse trajectory, i.e., the plan. Meanwhile, lower levels refine this trajectory into fine-grained predictions s(1) , minimizing sub-energies conditioned on the higher-level plan. Overall behavior emerges through hierarchical energy descent, where each layer enforces consistency between its predictions and the layer above. In other words, high-level planning computes trajectories in abstract latent space that minimize energy (i.e., most plausible and least costly transitions), while lower levels fill in the physical details. Formally, the training objective of Hierarchical-JEPA is given by: min {zt} (cid:88) (cid:88) l=1 C (l)(cid:0)s(l) , s(l) t+1, z(l) (cid:1), (29) where each (l) represents the energy cost of moving between abstract states at level l, and gradients through this hierarchy yield coherent plan across scales. 1.3.2. I-JEPA Image-JEPA (I-JEPA) [2] extends JEPA to learn semantic image representations by predicting latent features of masked image regions from visible context patches. In I-JEPA, an image is divided into non-overlapping patches, from which single large block is sampled as the context and several smaller blocks are sampled as targets. The context block is fed to the context encoder Eθ() to obtain patch-level latent representations sx, while the target blocks are processed by target encoder with EMA weights, Eθ(), to produce target embeddings sy. The predictor network Pϕ() takes the context representation sx along with positional mask tokens {mj}jBi indicating the spatial locations of each target block, and predicts the corresponding feature vectors ˆsy = Pϕ(sx, {mj}jBi) for those regions. The training objective minimizes the average squared distance between the predicted and target representations of the target blocks, averaged over all sampled blocks and training samples in the dataset D: min θ,ϕ E(x,y)D (cid:34) 1 (cid:88) (cid:88) i=1 jBi (cid:13) (cid:13) (cid:13)Pϕ (cid:0)Eθ(x), {mj}jBi (cid:1) Eθ(y)j (cid:35) . (cid:13) 2 (cid:13) (cid:13) 2 (30) Here, denotes the number of target blocks, E[] denotes the expectation (average) over all training samples (x, y), and the loss measures the representation-level prediction error rather than pixel-level reconstruction, allowing I-JEPA to learn highly semantic, non-generative representations. 1.3.3. V-JEPA Video-JEPA (V-JEPA) [5] and V-JEPA 2 [3] extend JEPA to learn spatio-temporal video representations by predicting masked tubelet features from visible context regions. In V-JEPA and V-JEPA 2, video clip is tokenized into spatialtemporal patches (tubelets), and subset of these patches is masked (replaced with mask tokens). The remaining unmasked patches form the context view, while the masked patches form the target view. The observation (context) encoder Eθ() processes the masked version (containing both visible and mask tokens) and produces latent embeddings for all positions, including the masked ones. However, the predictor Pϕ() focuses only on the masked positions. target encoder with EMA weights, Eθ(), computes the target embeddings sy of the full (unmasked) input. The predictor takes the context representation sx = Eθ(x) along with the mask token indicators and predicts the feature vectors ˆsy for each masked patch position. The training objective is to minimize the L1 distance between the predicted representations of the masked regions and the target representations from the target encoder, averaged over the dataset D: min θ,ϕ E(x,y,y)D (cid:13) (cid:13)Pϕ (cid:0)Eθ(x), (cid:1) Eθ(y)(cid:13) (cid:13)1 (31) where E[] denotes the expectation (average) over all training samples (x, y, y). That is, min θ,ϕ E(sx,y,sy)D (cid:2)ˆsy sy1 (cid:3) , where ˆsy = Pϕ(sx, y). (32) Hence the loss function measuring the L1 distance bet , se , at)15 end-effector states, and actions is temporally interleaved as (sx t=1 and processed with the transformer predictor network Pϕ() to obtain sequence of next-state representation predictions (sx t+1)15 t=1: (ˆsx t+1)15 t=1 = Pϕ (cid:0)(sx , se , at)15 t=1 (cid:1). (36) The teacher-forcing loss trains the predictor Pϕ() to accurately perform one-step future prediction by minimizing the L1 distance between the predicted latent representation ˆsx t+1 and the encoded ground-truth latent sx t+1 at each time step t: tween the ˆsy and sy: L(θ, ϕ) = (cid:13) (cid:13)Pϕ (cid:0)Eθ(x), y) Eθ(y)(cid:13) (cid:13)1 (33) LTF(ϕ) = V-JEPA 2-AC The action-conditioned variation (V-JEPA 2-AC) serves as downstream extension of V-JEPA 2 that predicts future latent representations conditioned on agent actions. As step-by-step (1-step prediction) formulation, VJEPA 2-AC adapts the frozen encoder E() from V-JEPA 2, pretrained on unlabeled videos, to encode the current observation xt into the latent representation sx and the future target xt+1 into sx t+1. The action-conditioned predictor Pϕ() takes the current latent sx and the action at as input to predict the next-state latent representation ˆsx t+1. The L1 loss in latent space then trains the predictor to align its predicted next-state representations with the encoded future representations:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:88) t=1 Pϕ(E(xt), se , at) E(xt+1) (37) (cid:13) (cid:13)ˆsx t+1 sx t+1 (cid:13) (cid:13)1 , where = 15. (38) = The rollout loss involves feeding the predictors output back as input, allowing the model to be trained to predict several timesteps ahead. In this case, we design two-step rollout loss to improve the models ability to perform autoregressive rollouts during inference. We can now denote the rollout loss as Lrollout(ϕ) = (cid:13) (cid:13)Pϕ(E(xt), se , (at)t+n ) E(xt+n+1)(cid:13) (cid:13)1 , (39) = (cid:13) = (cid:13) (cid:13)Pϕ(sx (cid:13)ˆsx , se t+2 sx , at, at+1) sx t+2)(cid:13) (cid:13)1 , (cid:13) (cid:13)1 , where = 1. t+2 (40) (41) LAC(ϕ) = (cid:13) = (cid:13) (cid:13)Pϕ (cid:13)ˆsx (cid:1) E(xt+1)(cid:13) (cid:0)E(xt), at (cid:13)1 (cid:13) t+1 sx (cid:13)1 . t+1 (34) (35) Hence, the total loss is In the multi-step rollout setting, V-JEPA 2-AC extends the one-step formulation to predict sequence of future latent representations over time horizon conditioned on sequence of actions. We randomly sample mini-batch of 4-second video clips from the Droid dataset [39] and, for simplicity, discard any videos shorter than 4 seconds, leaving us with smaller subset of the dataset comprising under 62 hours of video. The video clips are sampled with resolution of 256 256 and frame rate of 4 fps, yielding 16-frame clips (xt)16 t=1, where each xt represents single video frame. The robots end-effector state in each observation is denoted by the sequence (se is real-valued 7D vector defined relative to the base of the robot. We construct sequence of actions (at)15 t=1 by computing the change in end-effector state between adjacent frames. We use the V-JEPA 2 encoder E() as an image encoder and encode each frame independently in given clip to obtain sequence of feature maps (sx t=1. The sequence of observed feature maps, t=1, where se )16 )16 LAC(ϕ) = LTF(ϕ) + Lrollout(ϕ). (42) Inference We can then perform energy-based planning after training, using the frozen encoder and predictor . This predictor acts as the world model, capable of imagining how latent representations evolve when an action sequence is applied. At test time, no weights are trained, instead, we search for an action sequence that minimizes goal-conditioned energy cost between the imagined future and the goal latent representation. Given the current observation x1, current end-effector 1, target goal image x1+T , and planning horizon , state se we encode the current and goal observations as: sx 1 = E(x1), sx 1+T = E(x1+T ). (43) We then define the L1 energy cost function: 1+T ) = (cid:13) (cid:13)P ((ˆat)T t=1; se t=1; se 1 , sx 1, sx 1, sx C((ˆat)T 1 ) sx 1+T (cid:13) (cid:13)1 . (44)"
        },
        {
            "title": "The optimal action sequence is obtained by minimizing",
            "content": "this cost: Algorithm 1 Energy-Based Planning with Cross-Entropy Method (CEM) (a )T t=1 = arg min (ˆat)T t= C((ˆat)T t=1; se 1, sx 1 , sx 1+T ). (45) Thus, the predictor is used to imagine the future latent trajectory, and planning reduces to finding the action sequence that minimizes the latent L1 distance to the goal embedding. V-JEPA 2-AC uses the Cross-Entropy Method (CEM) [23] with 800 samples and 10 iterations to efficiently minimize at each planning step, as shown in Algorithm 1. It executes only the first action on the robot before re-planning, as in receding horizon control, and is tested only on horizon of = 1. In other words, during inference, the predictor serves as world model for energy-based planning, where at each step the CEM searches for an action sequence that minimizes the latent-space cost between the imagined future and the goal representation, executing only the first action 1 before re-planning. 1.4. World Modeling Paradigms There are two typical approaches for goal-conditional world modeling: generative world models [46, 59, 63] and predictive world models [2, 3, 5, 29, 41]. Generative world models. Generative world models typically build upon autoregressive (AR) transformers or semiAR (autoregressive diffusion) models ρ that observe the visual context xt and autoregressively predict the latent zt and the next frame ˆxt+1: (zt, ˆxt+1) = ρ(xt). (46) They often rely on an inverse dynamics model (IDM) π [67], trained separately, which maps the pair (xt, zt, ˆxt+1) to an explicit action [10, 73, 80]: at = π(xt, zt, ˆxt+1). (47) In other words, its one-step inverse mapping of the environments dynamics. Hence, it can only predict one step at time, because it does not know the full trajectory structure or the energy landscape over multiple steps. Moreover, it must explicitly generate or reconstruct the next frame (or latent visual tokens that decode into pixels), and perform planning by predicting how the world will look after an implicit action, which connects to the challenge mentioned in 1.3. Require: Predictor (world model), encoder E(), planning horizon , number of samples , number of elites K, number of iterations I, initial mean µ0, and covariance Σ0. Ensure: Optimal action sequence (a )T t=1. Input: current observation x1, end-effector state se goal image x1+T . Encode current and goal observations: sx sx 1+T = E(x1+T ). for = 1 to do 1 = E(x1), 1, and {ˆa(n) (1) Sample: Draw candidate action sequences 1:T }N (2) Evaluate: For each candidate sequence, compute n=1 from (µj1, Σj1). its energy cost: (n) = (cid:13) (cid:13)Pϕ(ˆa(n) (cid:13) 1:T ; se 1, sx 1 ) sx 1+T (cid:13) (cid:13) (cid:13)1 . (3) Select elites: Sort {C (n)} in ascending order and select the top sequences with the lowest cost to form the elite set Ej = {ˆa(n) 1:T top-K(C)}. These elites represent trajectories that drive the world models imagined latent state closest to the goal latent sx 1+T . (4) Update distribution: Compute the new mean and covariance of the elite set: (ˆat)T µj = (cid:88) t=1, 1 Σj = 1 (ˆat)T t=1Ej (cid:88) (ˆat)T t=1Ej (cid:0)(ˆat)T t=1 µj (cid:1)(cid:0)(ˆat)T t=1 µj (cid:1) . The new distribution (µj, Σj) is now centered around promising low-energy action sequences. (5) Repeat: Continue iterating Steps 34 for iterations. As the process proceeds, the sampling distribution progressively concentrates around action sequences that minimize the latent-space cost CL1. end for (6) Execute: Select the action sequence corresponding to the lowest final cost: )T C((ˆat)T t=1; se 1+T ). 1 , sx 1, sx (a t=1 = arg min (ˆat)T t=1 and execute only the first action (7) Re-plan: Observe the next frame x2, re-encode sx 2 = E(x2), and repeat the process from Step 1 (receding horizon control). 1 on the robot. they learn an energy landscape in latent space that measures compatibility between current and target states. Predictive world models Predictive world models are not generative, they do not model pixel distributions. Instead, However, predictive world models require an explicit goal observation xt+1 to compute their goal-conditioned energy, as shown in Equation 44, and minimize the energy cost with the CEM for planning. This design trade-off is intentional, not accidental. As mentioned in 1.3, we need to avoid pixel prediction during planning since pixels are noisy, unimportant, and computationally expensive [41]. Therefore, predictive world models are not intended to be self-contained simulators. Moreover, unlike the IDM, which predicts only single action given consecutive states, CEM performs multi-step trajectory optimization by searching over candidate action sequences to minimize the latent-space energy cost, enabling long-horizon planning rather than one-step reactive control. 1.5. Hyperbolic Learning Hyperbolic space, denoted as Hn, is negatively curved Riemannian manifold characterized by exponential volume growth and saddle-shaped geometry [11]. Unlike Euclidean space, where parallel lines remain equidistant, lines in hyperbolic space diverge, and the volume expands exponentially with radius. This property makes hyperbolic geometry naturally suited for representing hierarchical or treelike data structures [51, 55], such as hierarchical planning for world models, where the number of nodes grows exponentially with depth. In deep learning, hyperbolic space enables exponentially efficient representations of hierarchies by compressing large-scale differences while maintaining fine-grained local relationships. As result, it has been widely applied in representation learning [25, 51, 55, 83], computer vision and graphics [30] that require modeling multi-level, non-Euclidean structures. Hyperbolic space Hn is an abstract Riemannian manifold of constant negative curvature that does not depend on any coordinate system. In order to represent points in this curved space for computation, coordinate models are introduced to map Hn into Euclidean space while preserving its geometric structure. Two of the most common models are the Poincaré ball model Bn [13, 30, 51, 83] and the Lorentz (or hyperboloid) model Ln [25, 55], both providing isometric representations of the same manifold but differing in their coordinate systems and numerical properties. 1.5.1. Poincaré Ball Model. The Poincaré ball model Bn represents hyperbolic space as an open unit ball embedded in Euclidean space, defined as Bn = {z Rn : < 1}. (48) well-suited for visualization, as tree-like hierarchies naturally fit inside finite domain where the boundary corresponds to infinite distance. It constrains embeddings to normalized radii, but note that in the Poincaré model normalization means keeping points inside the unit ball, not unitnorm on sphere (that corresponds to the hyperspherical case). Geodesics The geodesic, or the Poincaré-ball hyperbolic distance, between two points u, Bn is circular arc perpendicular to the boundary of the ball. Its length is given by the hyperbolic distance function [28, 51] dH(u, v) = arcosh 1 + 2 (cid:18) v2 (1 u2)(1 v2) (cid:19) . (49) This metric measures the shortest path along the curved manifold rather than in Euclidean space, capturing the exponential growth of distances as points approach the boundary of the Poincaré ball. Exponential Map In Riemannian geometry, the exponential map expx : TxHn Hn (50) takes tangent vector TxHn (the tangent space at point x) and moves it along the geodesic starting from in direction v, traveling distance equal to under the hyperbolic metric. In other words, expx(v) can be interpreted as starting at and walking along the manifold in the direction of for distance v. This operation maps local Euclidean updates into global manifold coordinates, ensuring that updates remain consistent with the geometry of hyperbolic space. In hyperbolic space, the mapping from the tangent space to the manifold through the exponential map is one-to-one. Although there exist manifolds equipped with hyperbolic metrics where this mapping is not one-to-one [85], the Poincaré model of hyperbolic space preserves this one-to-one correspondence, ensuring welldefined relationship between the tangent space and the manifold. The exponential map from the origin of the Poincaré ball, denoted as exp0(v), maps Euclidean vectors directly into hyperbolic space, which is particularly useful for initialization. It is defined as exp0(v) = tanh(v) . (51) It is endowed with Riemannian metric that encodes constant negative curvature, ensuring that Euclidean distances are reweighted to reflect hyperbolic geometry. Each point lies strictly inside the ball, and distances are measured using the hyperbolic metric rather than Euclidean norms. This bounded representation makes the geometry intuitive and This formulation is simple because the tangent space at the origin aligns perfectly with the Euclidean space, making the mapping between Euclidean and hyperbolic representations straightforward. The exponential map from general point Bn must account for the curvature around x. It moves the point along the geodesic in the direction of the tangent vector v. The key difference from the origin case is that is not the origin, so we need way to add to under hyperbolic geometry. In Euclidean space, moving from point by vector is simply computed as + v. However, in hyperbolic geometry, vector addition is replaced by Möbius addition, denoted as v. Therefore, the general exponential map is expressed as expx(v) = tanh (cid:18) (cid:18) λxv 2 where λx = 2 1 (cid:19) (cid:19) , (52) (53) is the conformal factor that rescales distances locally. This formulation ensures that the operation respects hyperbolic curvature instead of Euclidean linearity. The Möbius addition of two vectors and is defined as = (1 + 2x, + y2)x + (1 x2)y 1 + 2x, + x2y2 . (54) Substituting = tanh expression for expx(v) in hyperbolic space (cid:16) λxv 2 (cid:17) , we obtain an explicit (cid:0)1 + 2αx, + α2(cid:1)x + (1 x2)α + α2x2 1 + 2αx, v , expx(v) = where α = tanh (cid:19) (cid:18) λxv 2 , λx = 2 1 x2 . This formulation explicitly shows how the exponential map combines the curvature-adjusted scaling (via λx and tanh) with the non-linear composition of and under Möbius addition, ensuring consistency with hyperbolic geometry. (55) (56) The logarithmic map from the origin of the Poincaré ball, denoted as log0(y), converts point Bn back to its Euclidean tangent vector and is defined as log0(y) = arctanh(y) (58) . This formulation is the inverse of the exponential map at the origin, satisfying exp0(log0(y)) = y, and is computationally simple since the tangent space at the origin coincides with Rn. For general point Bn, the logarithmic map must account for the local curvature around x. It can be expressed using the inverse of the Möbius addition: logx(y) = 2 λx arctanh(cid:0)x y(cid:1) x , where λx = 2 1 x2 (59) (60) is the same conformal factor as in the exponential map, and denotes Möbius addition with the inverse of x. Expanding the definition of the Möbius addition into the logarithmic map expression, we obtain an explicit formulation of logx(y) in the Poincaré ball model. Substituting Eq. 54 into Eq. 59, we first compute the intermediate term as = (1 2x, + y2)(x) + (1 x2)y 1 2x, + x2y2 . Then, the logarithmic map can be written explicitly as logx(y) = 2 λx arctanh (cid:18) (x, y) D(x, y) (cid:19) (x, y) (x, y) , (61) (62) where (x, y) = (cid:0)1 2x, + y2(cid:1)(x) + (cid:0)1 x2(cid:1) y, D(x, y) = 1 2x, + x2y2, λx = (63) 2 1 x2 . (64) Logarithmic Map The logarithmic map serves as the inverse of the exponential map, mapping points from the manifold back to the tangent space at given point Bn. Formally, it is defined as logx : Bn TxHn, (57) which takes point Bn and returns tangent vector TxHn that, when re-projected through the exponential map, satisfies expx(v) = y. This operation locally linearizes the manifold around x, allowing differential computations such as gradient-based optimization to be performed in the tangent space. This expanded form explicitly expresses the logarithmic map in terms of and y, showing how the non-linear geometry of the Poincaré ball modifies vector displacement through the Möbius addition. It provides the tangent vector at that points toward with magnitude corresponding to the hyperbolic distance between them. This formulation ensures that logx(y) returns tangent vector at whose exponential map precisely recovers y, i.e., expx(logx(y)) = y. Together, the exponential and logarithmic maps establish smooth and invertible correspondence between the Euclidean tangent space and the curved manifold, enabling consistent optimization and representation learning in hyperbolic space. Curvature The above formulation corresponds to the unit-curvature case, where the curvature is fixed as = 1. In general, hyperbolic space has constant negative curvature usually written as = c, where > 0. When = 1, the space has curvature 1, which is the normalized convention adopted by most works. The general form instead keeps as free curvature parameter, so the balls rac. This allows different degrees of curvadius becomes 1/ ture flatter when 0, and more curved when . Bn = {z Rn : cz2 < 1}, (65) where the radius is 1/ mannian metric is scaled by the conformal factor and curvature = c. The Rieλx = 2 1 cx2 , (66) which defines the metric tensor as For general curvature = (c > 0), the logarithmic map must account for the local curvature around x. It can be expressed using the inverse of the Möbius addition as logx(y) = 2 λx arctanh(cid:0) cx y(cid:1) c c , (74) where λx = 2 1 cx2 (75) is the conformal factor, and c denotes the Möbius addition under curvature c. Expanding the definition of Möbius addition into the logarithmic map expression, we obtain an explicit formulation of logx(y) in the general Poincaré ball model with curvature c. Substituting the c-dependent Möbius addition (Eq. 71) into Eq. 74, we first compute the intermediate term c as Gx = λ2 xI. (67) c = (1 2cx, + cy2)(x) + (1 cx2)y 1 2cx, + c2x2y2 . The general form of the Poincaré-ball hyperbolic distance under curvature = (c > 0) between two points u, Bn is given by [13] dH(u, v) = 1 (cid:18) arcosh 1 + 2c v2 (1 cu2)(1 cv2) (cid:19) . (68) The exponential map with the curvature is expressed as (cid:18) (cid:18) expx(v) = tanh λxv 2 (cid:19) (cid:19) v , (69) where λx = 2 1 cx2 , (70) and denotes the Möbius addition under curvature c. The Möbius addition of two vectors and under curvature is defined as [13, 28] = (1 + 2cx, + cy2)x + (1 cx2)y 1 + 2cx, + c2x2y . (71) So the explicit expression for expx(v) in hyperbolic space with curvature is given by (cid:0)1 + 2cαx, + cα2v2(cid:1)x + (1 cx2)αv + c2α2x2v2 1 + 2cαx, expx(v) = where α = tanh (cid:18) (cid:19) λxv , λx = 2 1 cx2 . (72) (73) (76) Then, the logarithmic map can be written explicitly as logx(y) = 2 λx (cid:18) arctanh Nc(x, y) Dc(x, y) (cid:19) Nc(x, y) Nc(x, y) (77) , where Nc(x, y) = (cid:0)1 2cx, + cy2(cid:1)(x) + (cid:0)1 cx2(cid:1) y, Dc(x, y) = 1 2cx, + c2x2y2, λx = (78) 2 1 cx2 . (79) When = 1, this expression reduces to the unitcurvature form of the logarithmic map given in Eq. 59. Lorentz Model. The Lorentz model Ln represents hyperbolic space as the upper sheet of two-sheeted hyperboloid embedded in Minkowski space Rn+1, defined as Ln = {p Rn+1 : p, pL = 1/κ, p0 > 0}, (80) , where p, qL = p0q0 +(cid:80)n i=1 piqi is the Lorentzian inner product. This formulation is unbounded and algebraically convenient, allowing closed-form computation of geodesic distances and stable gradient optimization. Because of its numerical robustness and simple analytical expressions for exponential and logarithmic maps, the Lorentz model is widely adopted in hyperbolic representation learning, particularly in entailment-based and hierarchical visionlanguage models. 1.6. Value Function Reinforcement learning (RL) is about learning to act in an environment so as to maximize future reward. The value function in RL estimates the expected cumulative future reward that an agent can obtain from particular state or stateaction pair [70]. It helps the agent decide which actions are more desirable in the long run, guiding it towards making decisions that maximize its total reward over time. There are two main types: state-value function (s), which predicts the value of given state, and stateaction value function (s, a), which predicts the value of taking specific action in given state. State Value Function. The value function (s) represents the expected return (cumulative reward) starting from specific state and following particular policy, which It indidefines the agents strategy for choosing actions. cates how good it is for the agent to be in certain state. For example, if an agent is located at particular position in maze, the value function (s) represents the expected total reward it will obtain from that point until it reaches the target, assuming it continues to follow its current policy. State-Action Value Function. The value function (s, a) or Q(s, a), also known as the action value function, represents the expected return (cumulative reward) starting from specific state s, taking an action a, and subsequently following particular policy. It indicates how good it is for the agent to take specific action in given state. For example, if an agent is at particular position in maze, the function (s, a) represents the expected total reward it will obtain by choosing particular action at that point and then following its current policy until it reaches the target. Formal Definition. Given state st, an action at, policy π(a s) that defines how the agent acts, reward function r(s, a), and discount factor γ [0, 1), the state-value function under policy π is formally defined as π(st) = Eπ (cid:34) (cid:88) k=0 γk r(st+k, at+k) (cid:35) st . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (81) It measures the expected cumulative reward that an agent will receive when starting from state st and following policy π thereafter. Optimal Value Function. If the agent follows the best possible policy that maximizes the expected reward, we obtain the optimal value function: (st) = max π Eπ (cid:34) (cid:88) k=0 γk r(st+k, at+k) (cid:35) st . (82) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Intuitively, (st) quantifies the maximum expected cumulative reward that an agent can achieve when starting from state st and following an optimal policy thereafter. Bellman Optimality Equation. The value function encodes the long-term consequences of actions and forms the foundation of reasoning in reinforcement learning [6]. Once (s) is known, the optimal policy can be derived if the one-step transition dynamics (s s, a) are available. Specifically, the optimal policy π satisfies the Bellman optimality equation: π(s) = arg max (cid:34) r(s, a) + γ (cid:88) (s s, a) (s) . (cid:35) (83) This recursive relationship expresses how the value of state depends on the values of its successor states, thereby capturing the essence of sequential decision-making. Interpretation. The optimal value function (s) can be viewed as potential field or energy map over the state space. States with high value correspond to desirable or low-energy configurations that are closer to reward, whereas states with low value represent undesirable or highenergy configurations that are further away. From this perspective, acting optimally can be interpreted as following the gradient of the value landscape toward regions of higher value (or lower energy). This analogy bridges reinforcement learning with energy-based modeling, suggesting that value functions implicitly define an energy surface that guides the agent toward optimal behavior. Path Value Function. In generic RL settings above, the value depends on future rewards under the optimal policy. But if we redefine reward as the negative energy cost of moving between states: r(s, a, s) = c(s, s), (84) and the cumulative reward becomes the total negative cost, then the optimal path value function (s, s) corresponds to the negative of the minimum accumulated cost from to [19]. In this case, the optimal path value function (s, s) obeys triangle inequality [21, 49, 57]: c(s1, s3) c(s1, s2) + c(s2, s3) (s1, s3) (s1, s2) + (s2, s3). (85) 2. Motivation World state transitions (from video observations) naturally form hierarchical structure that is suitable represented in hyperbolic space. Let st denote the state at time and be discrete action set with cardinality = B. The world evolves according to the transition st+1 = (st, at), where at A. When predicting steps into the future, each action choice produces distinct future trajectory, resulting in Nd = Bd possible future states. These futures form exponentially branching tree, where the depth corresponds to the prediction horizon and the branching factor is determined by the action space. As result, future world states are naturally organized hierarchically: states at smaller depths represent coarse, high-level abstractions, while states at larger depths correspond to finer, more detailed futures. Similar motivations are also supported by [64]. 3. Baseline Details As mentioned in Section 4.2 of main content, in both Procedural Planning (PP) and Visual Planning with Videos setup, we evaluate against three categories of baselines. LLMbased approaches rely on large language or vision-language models for reasoning, instruction following, and multi-step planning. Generative (world) models perform planning by generating pixels or latent video tokens and using visual rollouts to guide decision-making. Predictive (world) models focus purely on action prediction, estimating future action sequences directly without generating visual frames. Random Selection. Following prior work [15], actions are sampled uniformly at random from the available action set to form plan, without considering the task context. Retrieval-Based. Following prior work [87], given the start and goal observations, this method retrieves the most similar trajectory from the training set by minimizing visual feature distance. The corresponding action sequence from the retrieved example is then used as the predicted plan. LLM-based LFP (Language-First Planning) [42]. This method first converts both the start and goal observations into text and then prompts large language model to infer the missing steps. The LLM predicts sequence of intermediate actions based solely on language reasoning rather than visual planning. VidAssist [36]. This method uses vision-language model to extract temporal and spatial cues from the video, then queries large language model to interpret these cues and generate an action sequence. The LLM refines and structures the predicted steps into coherent plan, combining visual grounding with language-based reasoning. SCHEMA [52]. This method performs procedure planning by modeling how states evolve over time. It aligns visual observations with textual state descriptions through cross-modal contrastive learning and uses transformer backbone to represent state transitions. large language model is then used to reason over these inferred intermediOther VLMs. ate states and generate the next actions, enabling structured step-by-step planning in instructional video settings. We also evaluate several large vision-language models, including InternVL3.5-241B [76], Qwen3-VL-Max [79], Gemini 2.5 Pro [20], and GPT5 [53], using them in zero-shot setting to perform visual reasoning and planning directly from video observations without task-specific training. Generative (World) Models DDN [15]. This approach uses an autoregressive structure with two coordinated branches: one learns compact representation of action steps, while the other predicts transitions in the latent feature space. By forecasting the next visual state rather than directly selecting actions, DDN models procedural progression through iterative frame prediction. Int-MGAIL and Ext-MGAIL [7]. These generative models perform procedure planning by jointly learning latent world model and an action policy through adversarial training, enabling multi-step action synthesis conditioned on visual goal states. P3IV [86]. This transformer-based model uses learnable memory module together with an adversarial generation setup, and, similar to our method, outputs all action steps in single forward pass rather than generating them sequentially. PDPP [75]. This two-branch diffusion-based framework models temporal dependencies and action transitions, generating the full action sequence in parallel and progressively refining it over multiple denoising stages to improve coherence and logical structure. KEPP [50]. This method incorporates structured procedural knowledge through probabilistic knowledge graph learned from training plans, which serves as external guidance for step ordering. KEPP predicts the full action sequence in single pass with limited supervision, producing strong performance in instructional video planning. ActionDiffusion [61]. This diffusion-based approach generates the full action sequence by iteratively denoising latent representation, allowing the model to refine predictions over multiple steps and capture long-term dependencies in instructional procedures. This model treats procedure planning as multimodal trajectory generation problem, using diffusion-based latent policy to synthesize complete action sequences conditioned on video observations while modeling long-term dependencies through iterative denoising. MTID [87]. VideoWorld [59]. This autoregressive framework generates future video frames step by step to model procedural progression, using predicted visual states to implicitly guide the unfolding action sequence. Predictive (World) Models WLTDO [27]. This recurrent neural network model generates action sequences directly from paired observations, using temporal reasoning over the encoded features to predict ordered procedural steps. UAAA [1]. This two-stage method predicts action steps autoregressively by combining an RNN with hidden Markov model to model temporal uncertainty and step transitions in procedural tasks. UPN [68]. This method learns differentiable latent space suitable for planning by predicting trajectories in feature space, and softmax output layer is used to convert the continuous plan representation into discrete action steps. PlaTe [69]. This model builds on DDN by introducing transformer modules into its dual-branch architecture for action and state prediction, but follows distinct evaluation protocol compared to other procedure planning methods. E3P [74]. This method adopts an event-centric formulation, inferring latent events from visual observations and using them to guide intermediate action prediction. Through event-aware prompting and action relation modeling, E3P improves the logical structure of predicted steps and achieves strong performance on procedural planning benchmarks. V-JEPA 2 [3]. large-scale predictive world model pretrained on masked latent feature prediction over one million hours of unlabeled video. Action-conditioned post-training enables autoregressive rollouts for planning without pixel generation. 4. Energy Landscape To better illustrate the difference between Euclidean predictive world models and our hyperbolic formulation, we visualize the energy landscape around given latent state. and y. In the original V-JEPA 2-AC setup [3], and represent physical end-effector offsets in Cartesian coordinates. The visualization shows how the models energy changes as the end-effectors target position varies along the and axes while keeping the vertical displacement fixed (z = 0). Formally, the plotted quantity is: shyp t+1 = st + (x, y, 0), Energy(x, y) = c(st, shyp t+1). (86) (87) where denotes the energy cost defined in Eq. 11. In visual planning, and are no longer physical displacements. Instead, they represent latent displacements that probe the local geometry of the world model around visual state. In the Euclidean space, the encoder maps an observation Rn. To visualize how the model xt into latent vector sx evaluates hypothetical future states, V-JEPA 2 [3] perturbs the latent representation along two Euclidean axes. We choose two orthonormal directions in latent space, u1, u2 Rn. natural, semantically aligned choice is: u1 = Eθ(xt+T ) Eθ(xt) Eθ(xt+T ) Eθ(xt) , (88) which represents the direction from the current state toward the goal (i.e., progress along the procedure). The second direction, u2, spans variations orthogonal to this progress direction (i.e., sampled from another trajectory at the same step and then orthonormalized against u1). Then, hypothetical next latent state is defined as shyp t+1 = sx + u1 + u2, and the corresponding energy landscape is Energy(x, y) = (cid:13) (cid:13)sx (cid:13) t+1 shyp t+1 (89) (90) (cid:13) (cid:13) (cid:13) . In GeoWorld, the encoder maps each observation xt to latent representation sx t,H on the hyperbolic manifold. To probe the local geometry around this latent state, we sweep two orthonormal directions in the tangent space T0Hn, denoted as (x, y). Each coordinate pair (x, y) corresponds to small displacement applied at the tangent space before projection onto the manifold via the exponential map: shyp t+1,H = exp0 (cid:0)sx + u1 + u (cid:1), (91) where u1 and u2 form an orthonormal basis in T0Hn. Thus, (x, y) describes local perturbations of the latent state, not pixel space offsets. And the energy landscape is EnergyH(x, y) = dH (cid:16) t+1,H, shyp sx t+1,H (cid:17) . (92) Visualization. In Figure 2, we select reference latent state st from the initial step of the Replace Memory Chip task in the COIN dataset [71], and visualize the local energy geometry by sweeping two orthonormal tangent-space directions (x, y) around this state. Figure 2 compares the Euclidean (left) and hyperbolic (right) landscapes. The Euclidean surface shows smooth, nearly symmetric paraboloid with weak directional structure, indicating that V-JEPA 2 treats perturbations homogeneously. In contrast, the hyperbolic surface in GeoWorld forms sharper, curvature-aware basin with more pronounced directional variation. This reflects the ability of H-JEPA to encode hierarchical structure: states positioned higher in the task hierarchy lie at hyperbolically greater distances, creating more informative energy gradients during planning. Table 1. Ablation of frozen encoder vs. fully fine-tuned model for visual planning with videos on CrossTask [88]. Method T=3 T=4 SR mAcc mIoU SR mAcc mIoU GeoWorld ViT-L w/ FFT GeoWorld ViT-H w/ FFT GeoWorld ViT-g w/ FFT GeoWorld ViT-g384 w/ FFT 44.80 45.20 47.79 48.46 49.23 49.57 51.71 52.04 70.54 71.17 74.42 74.94 76.64 76.86 77.30 77.98 86.30 87.16 88.84 89.10 90.61 91.04 92.95 93. 30.63 31.34 34.51 34.95 35.49 35.91 37.04 37.85 65.46 67.16 68.89 69.42 71.00 71.76 71.35 72.24 79.73 80.38 82.95 83.47 84.50 85.13 87.04 87.80 ing stable value propagation across long planning horizons. by latent sampling δ-Hyperbolicity We Gromov visualize Gromov δ-hyperbolicity in quadruples CrossTask [88] and evaluating the four-point condition under each models intrinsic metric (hyperbolic geodesic distance for GeoWorld and Euclidean distance for V-JEPA 2). As shown in Fig. 1, GeoWorld exhibits substantially more concentrated distribution of near-zero δ values, indicating stronger tree-like hierarchical geometry in its learned representation space. Frozen Encoder vs. Fully Fine-Tuned As shown in Table 1, we evaluate the impact of Fully Fine-Tuning (FFT) the encoder during the supervised finetuning stage, compared to the original configuration where the encoder remains frozen and only lightweight exponential projection layer is trainable. Fully fine-tuning yields consistent yet modest improvements across all metrics and model scales, with gains of approximately 0.30.8% in SR and 0.51.2% in mAcc and mIoU for both =3 and =4 planning horizons. While these improvements indicate that the encoder can still adapt beneficially to downstream visual planning objectives, the gains come at the cost of significantly increased trainable parameters and slower optimization. Moreover, the relative performance margin narrows as model size increases, suggesting diminishing returns for larger backbones. These results imply that the frozen-encoder design already captures task-relevant structure effectively, and full encoder finetuning provides only incremental benefit relative to the additional computation and memory overhead introduced. Effectiveness of GRL As shown in Table 2, incorporating Geometric Reinforcement Learning (GRL) leads to clear and consistent improvements over the supervised finetuning (SFT) baseline. While SFT alone yields marginal gains over the pretrained V-JEPA 2 model, applying GRL independently further boosts SR, mAcc, and mIoU across both planning horizons, suggesting that GRL better aligns Figure 1. Gromov δ-hyperbolicity on CrossTask [88]. Such curvature-aware energy landscapes promote more stable long-horizon planning: CEM naturally follows the hyperbolic geodesics shaped by GeoWorld, resulting in more accurate multi-step trajectory optimization. 5. Ablation Study Curvature As discussed in Section 4.3, the curvature = is learned in the logarithmic space by optimizing log(c), which is initialized at = 1 and treated as learnable scalar. This formulation ensures that remains positive and stabilizes the gradients of both the hyperbolic distance and the exponential map [14, 25]. The learned curvature is further clamped to the range [0.1, 10.0] to prevent training instability. We analyze how the learnable curvature evolves during training and how it influences geometric planning quality. As shown in Fig. 2 (d), the curvature parameter in GeoWorld typically starts near 1 and gradually decreases to stable value around 0.3, indicating that the model learns flatter yet still hyperbolic latent geometry. smaller curvature reduces distortion in the exponential map and leads to more stable multi-step planning, especially for larger backbone encoders. The geometric effect of curvature is further visualized in Fig. 2 (a)(c): as decreases, geodesic paths bend less aggressively toward the origin (Fig. 2 (a)), boundary-anchored geodesic patterns become flatter (Fig. 2 (b)), and the hyperbolic distance between and contracts smoothly as curvature approaches zero (Fig. 2 (c)). This suggests that moderate negative curvature is sufficient to capture hierarchical structure while preserv- (a) Curvature and geodesics. (b) Geodesic patterns. (c) Distance vs. curvature. (d) Curvature trend during training. Figure 2. Geometric effects and curvature dynamics: (a) Poincaré disk geodesics connecting and under different curvatures K. As the curvature becomes less negative (i.e., closer to 0), the hyperbolic distance between and increases, and the geodesic paths bend less and shift closer toward the origin. (b) Geodesic patterns induced by different boundary anchor points. Varying the anchor location produces characteristic geodesic fan in the Poincaré disk. (c) As the curvature becomes less negative, the space flattens and the distance between and decreases. (d) Learnable curvature during supervised training, showing gradual decrease from its initialization and convergence to stable value 0.3. Table 2. Ablation of Supervised Fine-Tuning (SFT) vs. Geometric Reinforcement Learning (GRL) for visual planning with videos on CrossTask [88]. Method T= T=4 SR mAcc mIoU SR mAcc mIoU V-JEPA 2 ViT-g384 [3] 50.16 74.86 91.73 35.01 70.24 85. GeoWorld ViT-g384 SFT Only GRL Only SFT + GRL 50.42 51.04 51.71 75.13 76.48 77.30 91.94 92.42 92.95 35.92 36.33 37.04 70.79 71.04 71. 85.88 86.31 87.04 Table 3. Ablation of weighting hyperparameter λ in Supervised Fine-Tuning (SFT) Only for visual planning with videos on CrossTask [88]. Method T=3 T=4 SR mAcc mIoU SR mAcc mIoU V-JEPA 2 ViT-g384 [3] 50.16 74. 91.73 35.01 70.24 85.05 GeoWorld ViT-g384 λ = 1, 1 λ = 0 λ = 0.9, 1 λ = 0.1 λ = 0.8, 1 λ = 0.2 λ = 0.7, 1 λ = 0.3 λ = 0.6, 1 λ = 0.4 λ = 0.5, 1 λ = 0.5 λ = 0.3, 1 λ = 0.7 50.16 50.19 50.25 50.33 50.37 50.42 50. 74.88 74.91 74.96 75.02 75.06 75.13 75.07 91.79 91.84 91.88 91.89 92.92 91.94 91.82 34.65 34.95 35.31 35.57 35.82 35.92 35.97 69.48 70.05 70.40 70.57 70.66 70.79 70.86 84.10 84.85 85.27 85.46 85.72 85.88 85.74 the learned energy landscape with multi-step planning objectives. The combination of SFT and GRL achieves the strongest performance, indicating that SFT provides strong initialization while GRL refines the latent dynamics toward energy-minimizing trajectories required for longhorizon reasoning. These findings highlight the complementary nature of supervised learning and reinforcementbased value shaping in predictive world models. SFT Hyperparameters As shown in Table 3, incorporating the rollout loss into SFT consistently improves visual planning performance over the pure one-step objective (λ = 1). Once 1λ > 0, all metrics exhibit steady gains, indicating that multi-step rollout supervision provides additional temporal consistency beyond standard single-step training. As the rollout weight increases (i.e., smaller λ), improvements become more pronounced, particularly for the longer planning horizon (T = 4). For example, SR and mIoU steadily increase as λ decreases from 1 to 0.5, suggesting that stronger rollout supervision effectively mitigates error accumulation over longer sequences. This trend aligns with the intuition that longer-horizon prediction requires explicit multi-step consistency constraints rather than relying solely on local one-step accuracy. balanced weighting around λ = 0.5 achieves the strongest overall performance across metrics, demonstrating that equal emphasis on onestep prediction and rollout consistency yields the best tradeoff. Further increasing the rollout weight (e.g., λ = 0.3) leads to negligible changes for the shorter horizon (T = 3), while yielding slight yet consistent gains for the longer horizon (T = 4). This behavior indicates that stronger rollout supervision primarily benefits long-horizon planning, especially under the hyperbolic structure where multi-step geodesic consistency becomes more critical. In contrast, short-horizon planning does not induce strong hierarchical structure, limiting the advantage of hyperbolic geometry and GRL. The primary benefit of GeoWorld emerges as the planning horizon increases, where exponential branching and long-term abstraction become critical, as shown in Table 5. GRL Hyperparameters As shown in Table 4, both the discount factor γ and the regularization weight β play imTable 4. Ablation of discount factor γ and the regularization weight beta in Geometric Reinforcement Learning (GRL) for visual planning with videos on CrossTask [88]. Method T=3 T= SR mAcc mIoU SR mAcc mIoU V-JEPA 2 ViT-g384 [3] 50. 74.86 91.73 35.01 70.24 85.05 GeoWorld ViT-g384 SFT Only β = 0, γ = 0.99 β = 0.05, γ = 0.99 β = 0.2, γ = 0.99 β = 0.1, γ = 0.90 β = 0.1, γ = 0.95 β = 0.1, γ = 0. 50.42 50.48 51.04 51.69 51.02 51.44 51.71 75.13 75.27 76.21 77.25 76.39 76.94 77.30 91.94 91.99 92.39 92.83 92.04 92.75 92.95 35.92 36.07 36.58 37.15 36.42 36.85 37.04 70.79 70.94 71.13 71.33 70.88 71.05 71.35 85.88 86.07 86.45 86.96 86.33 86.67 87. portant roles in shaping the learning dynamics in GRL. Increasing γ strengthens long-horizon supervision by assigning greater weight to later predicted steps, which benefits multi-step rollout consistency and improves SR, mAcc, and mIoU as the planning horizon increases from =3 to =4. Meanwhile, introducing the triangle inequality regularization term through β > 0 consistently boosts performance compared to the β = 0 setting, demonstrating that enforcing hyperbolic geodesic constraints helps stabilize the predictor and prevents degenerate shortcuts in latent space. Moderate regularization (β = 0.1) paired with large discount factor (γ = 0.99) achieves the strongest results, indicating that encouraging long-horizon consistency while softly enforcing geodesic structure yields the most effective balance. These results validate the effectiveness of GRL as both geometric constraint mechanism and planning-aligned training signal. Hyperbolic Geometry vs. GRL in Long-Horizon Planning Section 4.5 in main paper reports results up to =6, following the long-horizon setting in [87]. Table 5 further extends the evaluation to =8 to stress-test planning stability under increasingly long rollouts. As the horizon grows, the vanilla V-JEPA 2 baseline exhibits rapid performance degradation, with SR dropping sharply from 50.16 at =3 to 4.95 at =8, highlighting severe error accumuIntroducing hyperbolic lation in long-horizon prediction. geometry substantially mitigates this collapse. SFT in hyperbolic space already improves stability at longer horizons, maintaining significantly higher SR at 7. Applying GRL in Euclidean space further strengthens multistep consistency and consistently outperforms the baseline, demonstrating that rollout-based geometric regularization alone contributes meaningful gains even without hyperbolic modeling. When GRL is implemented in hyperbolic space, the advantage becomes more pronounced, particularly for 6, suggesting that enforcing geodesic consistency in curvature-aware latent space better preserves long-range Table 5. SR of long horzion planning on CrossTask [88] videos. Method T=3 T= T=5 T=6 T=7 T=8 V-JEPA 2 ViT-g384 SFT (Hyperbolic) GRL (Euclidean) GRL (Hyperbolic) SFT + GRL 50.16 50.42 50.26 51.04 51. 35.01 35.92 35.47 36.33 37.04 23.17 23.64 23.85 24.05 24.83 16.88 16.97 17.03 17.82 18.26 8.26 14.88 15.12 15.54 16.09 4.95 11.51 12.74 13.10 13.81 structural dependencies. The full model (SFT + GRL) achieves the strongest results across all horizons, with the performance gap widening as increases. This trend indicates that SFT and GRL play complementary roles: SFT stabilizes short-term prediction, while GRL enhances longhorizon rollout consistency, together yielding clear advantage in extended planning scenarios. 6. Error Accumulation in Long-Horizon Planning Autoregressive (AR) methods inevitably lead to error accumulation in long-horizon planning, which is why many existing works focus on mitigating this issue through rollout loss. However, our claim is not that hierarchy replaces this effect, but that geometry shapes how errors accumulate. In Euclidean latent spaces, small prediction errors cause unconstrained drift that compounds uniformly over time, whereas hyperbolic geometry imposes hierarchical structure on the latent space that constrains long-horizon trajectories along geodesically meaningful directions. In this sense, error accumulation and geometric drift are closely related: hierarchical geometry mitigates how errors propagate, while rollout loss and GRL help eliminate them. 7. Limitation and Future Work Our intuition for hierarchical structure arises from state transitions in multi-step planning over futures. Therefore, even when the action sequences annotated in CrossTask [88] and COIN [71] appear linear, predicting dstep futures from state induces an exponentially branching set of possible trajectories (Bd), forming an implicit tree underlying hierarchical structure. We must clarify that, as mentioned in Section 1.2, sub-task hierarchies involving multi-level planning are the intuition of the original JEPA [41]. However, the hierarchical structure in GeoWorld arises from multi-step future expansion, rather than from explicit high-level planning and low-level execution. Future work may involve sub-task hierarchies, such as high-level task labels, mid-level actions, and low-level endeffectors. Moreover, our framework is compatible with embodied planning. As this is computer vision conference, we plan to extend our work to embodied settings in the future."
        }
    ],
    "affiliations": [
        "ANU",
        "MBZUAI"
    ]
}