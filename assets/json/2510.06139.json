{
    "paper_title": "Deforming Videos to Masks: Flow Matching for Referring Video Segmentation",
    "authors": [
        "Zanyi Wang",
        "Dengyang Jiang",
        "Liuzhuozheng Li",
        "Sizhe Dang",
        "Chengzu Li",
        "Harry Yang",
        "Guang Dai",
        "Mengmeng Wang",
        "Jingdong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\\mathcal{J}\\&\\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes."
        },
        {
            "title": "Start",
            "content": "FlowRVS DEFORMING VIDEOS TO MASKS: FLOW MATCHING FOR REFERRING VIDEO SEGMENTATION Zanyi Wang2,1 Dengyang Jiang3,1 Liuzhuozheng Li4,1 Sizhe Dang1 Chengzu Li5 Harry Yang3 Guang Dai1 Mengmeng Wang6,1 1SGIT AI Lab, State Grid Corporation of China 3The Hong Kong University of Science and Technology 5University of Cambridge Jingdong Wang7 2University of California, San Diego 6Zhejiang University of Technology 7Baidu 4The University of Tokyo 5 2 0 2 7 ] . [ 1 9 3 1 6 0 . 0 1 5 2 : r Figure 1: FlowRVS replaces the cascaded locate-then-segment paradigm (A) with unified, endto-end flow model (B). This new paradigm avoids information bottlenecks, enabling superior handling of complex language and dynamic video (C) and achieving state-of-the-art performance (D)."
        },
        {
            "title": "ABSTRACT",
            "content": "Referring Video Object Segmentation (RVOS) requires segmenting specific objects in video guided by natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto specific set of pixels and continuously segment them through the complex dynamics of video. Faced with this difficulty, prior work has often decomposed the task into pragmatic locate-then-segment pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, novel framework that reconceptualizes RVOS as conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, textvideo semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning direct, language-guided deformation from videos holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving &F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes. Code is available at: https://github.com/xmz111/FlowRVS Equal contribution. Corresponding author. 1 FlowRVS"
        },
        {
            "title": "INTRODUCTION",
            "content": "Referring Video Object Segmentation (RVOS) (Khoreva et al., 2018; Gavrilyuk et al., 2018; Hu et al., 2016) requires the machine to segment objects described by natural language queries, which is critical to intelligent systems to precept and interact with the real world (Jiang et al., 2025; Li et al., 2023). The core challenge of RVOS lies in resolving fundamental spatio-temporal correspondence dilemma: anchoring abstract linguistic concepts onto dynamic and fine-grained pixel space. Current paradigms tackle this by resorting to approximations: they spatially model correspondence by collapsing semantics into coarse geometric intermediaries like points or bounding boxes, and temporally treat the video as sequence of independent frames (Lin et al., 2025; Li et al., 2023; Zhu et al., 2024; Bai et al., 2024). This two-stage locate-then-segment design creates an inescapable bottleneck with two critical failure points. First, the initial localization stage irreversibly collapses rich linguistic semantics into coarse geometric prompt like anchors or bounding boxes, forcing the subsequent segmentation model to operate on an impoverished signal stripped of its original reasoning. Second, the temporal segmenting stage is often decoupled from the language query, reverting to generic visual segmenting problem that is inherently fragile to occlusions and unable to leverage the full temporal context of the referring expression (Liang et al., 2025b; Ren et al., 2024). To address these limitations, we argue that pretrained Text-to-Video (T2V) models offer fundamental solution since their native capabilities for fine-grained, text-to-pixel synthesis and spatiotemporal reasoning directly counter the bottlenecks of the locate-then-segment paradigm. Some previous attempts like VD-IT (Zhu et al., 2024) have successfully repurposed T2V models as powerful feature backbones. In the broader context of dense prediction, DepthFM (Gui et al., 2025) have adapted the entire generative process itself for visual-to-visual tasks like depth estimation based on image generation model. While these pioneering efforts validate the generative approach, they also expose shared, critical blind spot: they fail to fully utilize the dynamic, text-driven reasoning that T2V models are capable of and RVOS demands. The feature-extraction approach remains decoupled, forcing separate decoder to reconstruct temporal relationships from temporally isolated features, squandering the T2V models inherent video coherence. Meanwhile, the image-to-depth flows proposed by previous styles completely neglect text condition, rendering them fundamentally incapable of addressing the core RVOS challenge: producing different masks for the same video based on varying textual queries. Thus, we argue that deeper, more principled alignment with the T2V paradigm is required: one that treats the entire process as single, unified, language-guided flow from video pixels to required masks. This unfied flow with existing powerful T2V pretrain model (e.g, Wan) brings several benefits: (1) their pixel-level synthesis training provides fine-grained control, which enables them to distinguish and handle more delicate objects when locating specific targets; (2) their text-condition generation ensures powerful multi-modal alignment, which allows them to ground rich linguistic semantics directly in the pixel space without as much information loss as first mapping in coarse geometric intermediaries; (3) their video-native architecture provides inherent spatio-temporal reasoning, naturally unifying language guidance with temporal consistency. However, simply leveraging the T2V framework is not enough. As shown in Figure 3, standard T2V generation is divergent process: it maps simple noise prior to set of possible videos, exploring broad trajectory space. RVOS, conversely, is convergent task: it must map complex, high-entropy video to single, low-entropy mask. This transforms the problem into deterministic, guided information contraction, where the text query acts as the crucial selector that isolates the precise target from the rich visual input (e.g., distinguishing the smaller monkey from the bigger monkey). This core insight that RVOS is convergent flow directly informs our contributions. To successfully manage this asymmetric transformation, we introduce suite of principled adaptations: (1) boundary-biased sampling strategy to force the model to master the crucial, high-certainty start of the trajectory where the videos influence is strongest; (2) direct video injection mechanism to preserve the rich source context throughout the contraction process; and (3) task-specific VAE adaptation and start point augmentation to create stable latent space for this unique mapping. Summarizing, our contributions are as follows: We reformulate RVOS as learning continuous, text-conditioned flow that deforms videos spatio-temporal representation into its target mask, directly resolving the correspondence between language and dynamic visual data. 2 FlowRVS We propose suite of principled techniques that successfully enable the transfer of powerful text-to-video generative models to this challenging video understanding task. Our proposed framework, FlowRVS, establishes new state of the art on key benchmarks. Notably, it achieves significant improvement of 1.6% &F on the challenging MeViS dataset and 2.7% &F on the zero-shot DAVIS 2017 benchmark."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Referring Video Object Segmentation aims to segment target object within video based on natural language expression (Xu et al., 2018; Ding et al., 2023). This task demands both visuallinguistic understanding and robust temporal segmenting. Early approaches often adapted framelevel referring image segmentation models and appended temporal linking mechanisms as postprocessing step (Khoreva et al., 2018). More recent and competitive methods have evolved into more integrated yet predominantly multi-stage pipelines. dominant paradigm involves locatethen-segment strategy, where powerful multi-modal model first grounds the textual reference to spatial region, which then guides separate segmentation process for each frame. Locate-then-Segment Paradigm manifests in several forms. significant breakthrough came with the introduction of query-based architectures, inspired by the success of DETR-style (Zhu et al., 2020) transformers in vision tasks (Wu et al., 2022; Yan et al., 2024b), this new paradigm reframed RVOS by treating language as query to the visual features. Furthermore, multimodal model based approaches like LISA (Lai et al., 2024), VISA (Yan et al., 2024a) and ReferDINO (Liang et al., 2025b;a) leverage pretrained models reasoning or grounding ability, such as LLaVA(Liu et al., 2023) or DETR-based GroundingDINO (Ren et al., 2024), to perform initial object localization, and then introduce custom-designed mask decoder to generate the final segmentation. similar philosophy is seen in the VLM+SAM family of methods (Luo et al., 2023; Wu et al., 2023), which use Vision-Language Model for bounding box prediction, followed by generic segmentation model like SAM (Kirillov et al., 2023; Cuttano et al., 2025) to produce pixel-level masks (He & Ding, 2024; Lin et al., 2025; Pan et al., 2025). Another line of work explores repurposing generative models: VD-IT (Zhu et al., 2024) first extracts features from pretrained text-to-video diffusion model and then feeds these features into separate DETR-like architecture for mask prediction. While these methods have pushed the performance boundaries, their multi-stage natureseparating feature extraction from decoding, or localization from segmentation can introduce information bottlenecks and prevent end-to-end optimization of the core correspondence problem. Generative Modeling is largely catalyzed by the advent of latent diffusion models (Rombach et al., 2022). Building on this success, the frontier rapidly expanded into the temporal domain, leading to surge of powerful text-to-video (T2V) models (Liu et al., 2024; Wan et al., 2025; Gao et al., 2025). Architectures like Sora have demonstrated remarkable ability to generate complex, dynamic scenes by scaling up transformer-based models (Peebles & Xie, 2023), proving that these models implicitly learn profound priors about object permanence, motion physics, and the compositional structure of our world. Beyond diffusion models which are predicated on fixed path to Gaussian noise, Flow Matching (Lipman et al., 2022; Liu et al., 2022) offers significant theoretical advancement: it learns velocity field to transport samples along deterministic ODE path between any two arbitrary distributions. This generalization has been leveraged by pioneering works like DepthFM (Gui et al., 2025), which reformulate tasks like depth estimation as direct transformation between visual modalities. However, these methods only solve visual-to-visual translation problem, leaving them unable to address tasks driven by language. Our work makes critical distinction: we introduce the natural language query as the core conditional force that modulates the entire ODE path. This elevates the framework from simple translation to dynamic, multi-modal reasoning engine. It is this specific property that we leverage, proposing to reframe RVOS not as synthesis task, but as learned, conditional deformation from the distribution of videos to the distribution of their corresponding masks, instructed by natural language guidance. 3 FlowRVS Figure 2: FlowRVS reformulates RVOS as text-conditioned continuous flow, learning velocity field via Flow Matching stabilized by boundary-biased time sampling in latent space. During inference, an ODE solver uses this field to deterministically deform the video latent to the target mask, this video to mask paradigm superior to noise-based or one-step prediction approaches."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PROBLEM REFORMULATION: RVOS AS CONTINUOUS FLOW Traditionally, RVOS is approached as discriminative, one-step prediction task. model fθ is trained to learn direct mapping = fθ(V, c) from video-text pair to mask sequence. However, this direct mapping is fundamentally challenged by the need to collapse dynamic, highdimensional video into precise pixel-mask under the complex constraints of linguistic instruction, all within single transformation. To overcome this limitation, we depart from direct prediction and reconceptualize RVOS as textconditioned continuous flow problem. We propose to model segmentation as gradual, deterministic deformation process that transforms the videos representation into the target masks. This is governed by an Ordinary Differential Equation (ODE), where our goal is to learn the velocity field v(zt, c, t) that dictates the evolution of latent state zt: dzt dt = v(zt, c, t), with boundary conditions z0 Pvideo and z1 Pmask. (1) The trajectory starts from the video latent z0 and is guided by the text query to terminate at the specific mask latent z1. This transforms the learning objective from mastering single, complex global function to learning simpler, local velocity field. However, adapting this generative-native paradigm to discriminative task like RVOS is not straightforward, but fundamental inversion of the generative process, as illustrated in Figure 3. Standard T2V generation is divergent, one-to-many process: it starts from simple, fixed noise distribution and has broader exploration space in the initial steps to generate diverse set of plausible videos. In contrast, our approach is convergent, video-text-to-one task. It begins with complex, highentropy video latent z0 and must follow more tightly mapped direction to single, correct mask. Here, the text query is no longer creative prompt but critical, disambiguating force. The initial velocity computed from z0 must be precise enough to distinguish the smaller monkey from the bigger monkey. An error in this first step is irrecoverable, dooming the entire trajectory to fail. This places paramount importance on correctly learning the starting point of the flow. 3.2 TRANSFERRING TEXT-TO-VIDEO MODEL TO RVOS naive, uniform treatment of the trajectory, inherited from generative modeling, fails to account for the unique asymmetric nature of the video-to-mask flow. This asymmetrya high-certainty, 4 FlowRVS Figure 3: Repurposing generative process for discriminative task. Unlike standard T2V generation which maps noise to diverse videos (left), our method maps complex video to single mask (right). This transforms the process into deterministic, convergent task where the text query is the crucial element that selects the precise target from the visual input (e.g., distinguishing the smaller from the bigger monkey). structured start and low-certainty, sparse enddemands non-uniform approach to learning the velocity field. Therefore, we introduce suite of three synergistic strategies grounded in single principle: fortifying the flows origin. These are designed not as independent tweaks, but as cohesive framework to successfully adapt the powerful T2V model for RVOS. Boundary-Biased Sampling (BBS). We hypothesize that the most critical learning signal resides at the beginning of the trajectory, where the model computes the initial push away from the video manifold based on the text query. To exploit this, we introduce BBS, curriculum learning strategy that oversamples timestep = 0. By concentrating the gradient updates on this initial, high-influence decision, we force the model to first master the crucial text-guided velocity computation. As empirically demonstrated in Table 2, this focused learning strategy is the key to stabilizing the training process, transforming the failing baseline into highly effective model by ensuring well-posed initial value problem for the ODE. Start-Point Augmentation (SPA). To prevent the model from overfitting to discrete points on the data manifold and to encourage the learning of smoother, more generalizable flow, we introduce Start-Point Augmentation (SPA). During training, we transform the initial video latent z0 through stochastic encoding and normalization process. This technique effectively presents the model with richer, locally continuous distribution of starting points centered around the original video latent. This acts as powerful regularizer, forcing the model to learn velocity field that is robust not just on the manifold, but also in its immediate vicinity. In our video-to-mask formulation, the initial video latent z0 is Direct Video Injection (DVI). not merely starting point, but the foundational context for the entire transformation. To ensure this context remains accessible throughout the flow, we introduce Direct Video Injection (DVI). We implement this by concatenating the original video latent z0 with the current state zt along the channel dimension at each ODE step without introducing heavy computational burden. This transforms the velocity prediction at every subsequent point from v(zt, t) to v([zt, z0], t), explicitly conditioning each local update on the global origin. This simple yet effective strategy provides persistent, high-fidelity reference to the source video, preventing trajectory drift and improving fine-grained accuracy with negligible computational overhead. 3.3 ANALYSIS OF ALTERNATIVE PARADIGMS To motivate our final choices, we first analyze the fundamental limitations of three plausible alternative paradigms for adapting T2V model to RVOS. As empirically validated in our ablation studies, each of these alternatives fails due to core mismatch with the nature of RVOS. 5 FlowRVS Direct Mask Prediction (Worst Performance). direct, single-step mapping from the video and text latents to the mask latent represents the classic discriminative paradigm. We argue this approach is fundamentally ill-posed due to what we term information collapse. The mapping from highentropy, complex video manifold to low-entropy, sparse mask manifold is drastic information contraction. Forcing neural network to learn this in single, abrupt step leads to collapsion of the rich visual context into coarse approximation rather than performing precise, guided refinement. The model is not learning transformation, but rather brittle pattern recognition function. Noise-to-Mask Flow (Suboptimal). This paradigm mirrors standard text-to-video generation, starting from Gaussian noise z1 (0, I) and conditioning on the video context. This approach demotes the video from the primary source of information to secondary condition. Weaken the guidance of the video in the process would possibly force the entire, high-dimensional video context to be injected via simple concatenation at each step, creating severe information bottleneck. The model is tasked with generating the masks complex spatio-temporal structure from scratch based on this limited conditional signal, rather than progressively refining the rich, structured information already present in the video itself. One-Step Velocity Prediction (Better, but Limited). This paradigm makes the model predict the full velocity vector = z1 z0 in single inference step. It significantly outperforms the previous two baselines, confirming our hypothesis that learning residual velocity is more stable and effective objective than predicting an absolute state. However, its performance is still fundamentally capped. We assume that it is limited by the need to compute the entire, often large-magnitude deformation in single forward pass, lacking the capacity for the gradual, iterative refinements that multi-step process allows. This analysis solidifies our central thesis: multi-step, video-to-mask flow is the most effective paradigm for RVOS, but only when augmented with our proposed start-point focused adaptations to bridge the critical gap between its generative origins and the discriminative demands of the task."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 BENCHMARK AND METRICS We evaluate our framework on three standard RVOS benchmarks. MeViS (Ding et al., 2023) is challenging, motion-centric benchmark featuring 2,006 long videos and over 28,000 fine-grained annotations that emphasize complex dynamics. Ref-YouTube-VOS (Wu et al., 2022) is the largescale benchmark, comprising 3,978 videos that test for generalizability across wide diversity of objects and scenes. Ref-DAVIS17 (Khoreva et al., 2018) is high-quality, densely annotated dataset of 90 videos, serving as key benchmark for segmentation precision and temporal consistency. Following standard protocols, we report region similarity (J , Jaccard Index), contour accuracy (F, F-measure), and their average (J &F) as our primary evaluation metrics. 4.2 IMPLEMENTATION DETAILS Our framework is built upon the publicly available Wan 2.1 text-to-video model, which features 1.3B parameter Diffusion Transformer (DiT) (Wan et al., 2025). Throughout all training stages, we keep the pretrained text encoder and the VAE encoder entirely frozen. Our training focuses exclusively on fine-tuning the DiT block to learn the conditional flow. Crucially, the VAE decoder is specifically adapted for the segmentation task by being fine-tuned separately on the MeViS training set, allowing it to specialize in reconstructing high-quality masks from the latent space. Our training protocol varies by dataset to align with standard and fair evaluation protocols. For experiments on Ref-YouTube-VOS, we follow two-stage training strategy. The model is first pre-trained on combination of static image datasets (RefCOCO/+/g) (Yu et al., 2016; Kazemzadeh et al., 2014) to learn foundational visual-linguistic grounding. Subsequently, this pre-trained model is fine-tuned on the Ref-YouTube-VOS training set. The final weights from this stage are then used for zero-shot evaluation on the Ref-DAVIS17 benchmark without any further fine-tuning to prove FlowRVSs generalization ability. For the more challenging MeViS dataset, which emphasizes complex motion 6 FlowRVS Table 1: Comparison of our one-stage FlowRVS with other previous locate-then-segment methods on MeViS, Ref-YouTube-VOS and Ref-DAVIS datasets. We further include methods based on large VLMs for comparison. Bold and underline indicate the two top results. Method Locate-then-segment MTTR [CVPR22] ReferFormer [CVPR22] SOC [NIPS23] OnlineRefer [ICCV23] LISA [CVPR24] DsHmp [CVPR24] VideoLISA [NIPS24] VD-IT [ECCV24] VISA [ECCV24] SSA [CVPR25] SAMWISE [CVPR25] ReferDINO [ICCV25] MeViS Ref-YouTube-VOS Ref-DAVIS17 &F &F J &F 30.0 31.0 - 32.3 37.2 46.4 42.3 - 43.5 48.9 49.5 49. 28.8 29.8 - 31.5 35.1 43.0 39.4 - 40.7 44.3 46.6 44.7 31.2 32.2 - 33.1 39.4 49.8 45.2 - 46.3 53.4 52.4 53.9 55.3 62.9 66.0 63.5 53.9 67.1 61.7 64.8 61.5 64.3 69.2 69.3 54.0 61.3 64.1 61.6 53.4 65.0 60.2 63.1 59.8 62.2 67.8 67.0 56.6 64.6 67.9 65.5 54.3 69.1 63.3 66.6 63.2 66.4 70.6 71.5 - 61.1 64.2 64.8 64.8 64.9 67.7 63.0 69.4 67.3 70.6 68. - 58.1 61.0 61.6 62.2 61.7 63.8 59.9 66.3 64.0 67.4 65.1 - 64.1 67.4 67.7 67.3 68.1 71.5 66.1 72.5 70.7 74.5 72.9 One-stage generation based FlowRVS (ours) 51.1 47.6 54. 69.6 67.1 72.1 73.3 68.4 78. understanding, we train our model directly on its training set from scratch, without leveraging any static image pre-training. More hyperparameters settings can be found in Appendix A. 4.3 MAIN RESULTS We compare our proposed FlowRVS against wide range of baselines in Table 1. Our results show that FlowRVS significantly and consistently outperforms previous approaches. We highlight some key features as follows: Dominance on Complex Motion-Centric Benchmarks. The most significant advantage of our framework is demonstrated on MeViS, the most challenging benchmark designed to test nuanced understanding of motion-centric language. FlowRVS achieves &F score of 50.7, establishing new SOTA and surpassing the previous best method, SAMWISE, by substantial 1.2 point margin. This result is particularly noteworthy as MeViS features long videos with complex object interactions and appearance changesscenarios where the limitations of multi-stage, cascaded pipelines are most exposed. The superior performance of FlowRVS directly validates our core thesis: holistic, end-to-end flow that models the entire video-to-mask transformation is fundamentally better suited to capture and reason about complex spatio-temporal dynamics. Superiority over Locate-then-Segment Paradigms. Our performance gains are particularly meaningful when compared directly against methods that epitomize the locate-then-segment paradigm, such as VISA (VLM-based) and ReferDINO (grounding-model-based). On MeViS, FlowRVS outperforms VISA-13B by remarkable 7.0 points and ReferDINO (best results announced in the paper) by 1.4 points. This underscores the advantage of our one-stage approach. By avoiding the irreversible information loss inherent in collapsing semantics into an intermediate geometric or feature prompt, our continuous flow process maintains high-fidelity, text-guided transformation from start to finish, leading to more accurate and robust segmentation. The fundamental advantages of our one-stage flow paradigm are further illustrated in our qualitative comparisons (Figure 4). For the query The white rabbit which is jumping, ReferDINO provides coarse, static grounding of the rabbit but misses the jumping actions details, whereas FlowRVS delivers precise, dynamic segmentation. More critically, for the temporal query The first tiger..., VD-ITs decoupled decoder fails to resolve the ambiguity and tracks the wrong target. In contrast, FlowRVS correctly identifies and tracks the first tiger throughout, demonstrating superior global reasoning. Exceptional Zero-Shot Generalization on Ref-DAVIS17. The generalization capability of our model is best illustrated by its zero-shot performance on Ref-DAVIS17. Without any fine-tuning on the DAVIS dataset, the model trained on Ref-YouTube-VOS achieves &F score of 73.3. This 7 FlowRVS Figure 4: Qualitative comparison on challenging temporal and linguistic reasoning. Prior paradigms struggle: VD-IT produces temporally unstable masks due to its frame-wise decoder, while ReferDINO fails to interpret long-range descriptions. Our method, FlowRVS, demonstrates superior temporal coherence and language grounding by leveraging an end-to-end generative process. Table 2: Ablations of FlowRVS on the MeVIS validu set. BBS: Boundary-Biased Sampling (probability p). DVI: Direct Video Injection. SPA: Start Point Augmentation. WI: Weight Init from Wan. ID Method Configuration BBS (p) SPA DVI WI &F Alternative Paradigms (a) MutiStep Noise-to-Mask Flow (b) Onestep Mask Prediction (c) Onestep Velocity Prediction Our MutiStep Video-to-Mask Flow (c) Base Flow + BBS (d) + BBS (e) + BBS (f) + SPA (g) + DVI (ours default) (h) - WI (i) 0.0 0.25 0.50 0.75 0.50 0.50 0.50 32.3 36.2 50.8 47.9 55.2 57.9 56.5 58.6 60.6 21.1 29.6 41.5 47.1 42.9 50.7 53.8 52.5 54.2 55.9 20.3 35.0 38.9 54. 52.9 59.6 62.1 60.4 63.0 65.2 21.9 result is not only state-of-the-art but is also significantly higher than many previous methods that were explicitly trained or fine-tuned on similar high-quality datasets. This strong zero-shot transferability suggests that our flow-based paradigm, by learning more fundamental and continuous mapping between video and its corresponding mask guided by language, develops more generalizable understanding of spatio-temporal correspondence that is less prone to dataset-specific biases. 4.4 ABLATION STUDIES We conduct our ablation studies on the challenging MeViS dataset, as its complex, motion-centric scenarios provide rigorous testbed for our design choices. To ensure consistent and fair comparison, all results are reported on the validu set, following the protocol in prior work (Ding et al., 2023). The results are summarized in Table 2. Analysis of Alternative Paradigms. Our investigation begins by establishing the limitations of alternative paradigms (rows a-c). The Noise-to-Mask Flow (a), which mirrors standard generative practices, performs poorly (32.3 &F). This confirms our hypothesis that demoting the video to secondary condition (concatenate with noise) creates severe information bottleneck, forcing the model to generate the mask from scratch. The Onestep Mask Prediction model (b) also struggles (36.2 &F), validating that single, abrupt mapping is insufficient to bridge the vast representational chasm between video and mask. Notably, shifting the objective from state prediction to Onestep Velocity Prediction (c) yields substantial +14.6 &F gain. This key result proves that FlowRVS Table 3: Analysis of VAE adaptation strategies. We measure both the mask reconstruction quality (Recon.) and the resulting performance (Perf.) on the MeViS validu set with fixed flow model. VAE is tuned on MeViS training set. VAE Adaptation Strategy Recon. &F Perf. &F Frozen VAE Add Trainable Conv Head Finetuning Decoder (ours default) 29.7 85.4 99. 19.6 53.2 60.6 learning residual (velocity) is fundamentally more stable and effective task, providing strong initial validation for our flow-based reformulation. Effectiveness of Start-Point Focused Adaptations. Having confirmed the video-to-mask flow as the most promising direction, we dissect our proposed adaptations (rows c-h). The Base Flow model (c), trained with naive uniform sampling, performs poorly at 47.9 &F, even worse than the one-step velocity predictor. This confirms that multi-step process is not inherently superior; it must be correctly stabilized. The introduction of Boundary-Biased Sampling (BBS) provides the definitive solution. As shown in rows (d)-(f), forcing the model to focus on the start of the trajectory by oversampling = 0 almost single-handedly unlocks the potential of the multi-step flow. Even moderate bias of = 0.25 (d) brings massive +7.3 point improvement. The performance peaks at = 0.5 (e), yielding total gain of +10.0 &F over the baseline. While more extreme bias of = 0.75 (f) leads to slight performance drop, the score of 56.5 remains substantially higher than the baseline, demonstrating that the strategy is robust and effective across reasonable range of hyperparameters. This confirms that mastering the initial, text-guided velocity is the most critical factor for success. Finally, Direct Video Injection (DVI) (h) provides persistent context anchor throughout the trajectory, preventing drift and adding significant +2.0 &F. Effectiveness of the T2V Pretrain Model. Finally, we validate the central premise of our work: leveraging the power of large-scale T2V models. As shown in row (i), training our model from scratch without the pretrained weights (-WI) results in complete performance collapse to 21.1 &F. This underscores that our contributions are not generic training method, but are specifically designed to effectively harness and adapt the powerful priors learned by generative foundation models for this challenging discriminative task. Effectiveness of the VAE Adaptation. crucial step in our method is adapting the pretrained VAE to accurately transform between the latent space and the pixel space of binary masks. As shown in Table 3, we evaluate several adaptation strategies. In all experiments, we freeze the VAE encoder to preserve its powerful pretrained features and maintain stable latent space. We then compare three decoder configurations: keeping it frozen, adding simple convolutional head, and full-parameter finetuning. The results are definitive: fully finetuning the decoder dramatically improves mask reconstruction quality, which directly translates to significant boost in final RVOS performance. The The visualization results which we provide in Appendix also demonstrate the effectiveness of such adaptation."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we introduce FlowRVS, framework that moves beyond using T2V models as mere feature extractors and instead reformulates RVOS as continuous, text-conditioned flow from video to mask. Our core contribution is demonstrating that this paradigm shift, when combined with our proposed start-point focused adaptations (BBS, SPA, DVI), successfully aligns the generative strengths of T2V models with the discriminative demands of the task, leading to state-of-the-art performance. Our findings validate that the key to unlocking these models lies in principled adaptation, proving that by fortifying the flows structured starting point, the philosophical gap between generative processes and discriminative objectives can be effectively bridged. Looking forward, we believe the paradigm of modeling understanding tasks as conditional deformation processes holds significant potential beyond RVOS. And our insight in stabilizing discrimi9 FlowRVS native, start-point-critical flows provide crucial blueprint for the future. As even bigger and more powerful foundation models emerge, these techniques will be essential for harnessing their full potential and applying their remarkable capabilities to the vast amount of video understanding tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2024. Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, and Giuseppe Averta. Samwise: Infusing wisdom in sam2 for text-driven video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 33953405, 2025. Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 26942703, 2023. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from sentence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 59585966, 2018. Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast generative monocular depth estimation with flow matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 32033211, 2025. Shuting He and Henghui Ding. Decoupling static and hierarchical motion perception for referring video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1333213341, 2024. Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In European conference on computer vision, pp. 108124. Springer, 2016. Dengyang Jiang, Zanyi Wang, Hengzhuang Li, Sizhe Dang, Teli Ma, Wei Wei, Guang Dai, Lei Zhang, and Mengmeng Wang. Affordancesam: Segment anything once more in affordance grounding. arXiv preprint arXiv:2504.15650, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Asian conference on computer vision, pp. 123141. Springer, 2018. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95799589, 2024. Gen Li, Varun Jampani, Deqing Sun, and Laura Sevilla-Lara. Locate: Localize and transfer object parts for weakly supervised affordance grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1092210931, 2023. Tianming Liang, Haichao Jiang, Wei-Shi Zheng, and Jian-Fang Hu. Referdino-plus: 2nd solution for 4th pvuw mevis challenge at cvpr 2025. arXiv preprint arXiv:2503.23509, 2025a. 10 FlowRVS Tianming Liang, Kun-Yu Lin, Chaolei Tan, Jianguo Zhang, Wei-Shi Zheng, and Jian-Fang Hu. arXiv Referdino: Referring video object segmentation with visual grounding foundations. preprint arXiv:2501.14607, 2025b. Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into single large language model for video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 86588667, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc: Semantic-assisted object cluster for referring video object segmentation. Advances in Neural Information Processing Systems, 36:2642526437, 2023. Feiyu Pan, Hao Fang, Fangkai Li, Yanyu Xu, Yawei Li, Luca Benini, and Xiankai Lu. Semantic and sequential alignment for referring video object segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1906719076, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: simple online baseline for referring video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 27612770, 2023. Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 49744984, 2022. Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 585601, 2018. Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pp. 98115. Springer, 2024a. Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He, and Peng Gao. Referred by multi-modality: unified temporal transformer for In Proceedings of the AAAI Conference on Artificial Intelligence, video object segmentation. volume 38, pp. 64496457, 2024b. 11 FlowRVS Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pp. 6985. Springer, 2016. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. Zixin Zhu, Xuelu Feng, Dongdong Chen, Junsong Yuan, Chunming Qiao, and Gang Hua. Exploring pre-trained text-to-video diffusion models for referring video object segmentation. In European Conference on Computer Vision, pp. 452469. Springer, 2024. FlowRVS"
        },
        {
            "title": "APPENDIX",
            "content": "All our models are trained using the AdamW optimizer. We detail the key hyperparameters for our main experiments on Ref-YouTube-VOS and MeViS in Table 4. Table 4: Key hyperparameters for the training of FlowRVS. We detail the settings for the 2D pretraining, the main DiT fine-tuning on video datasets, and the separate VAE decoder adaptation. Hyperparameter 2D Pre-training (RefCOCO+/g) Video DiT Fine-tuning (Ref-YT-VOS & MeViS) VAE Decoder Fine-tuning (on MeViS) Optimizer Configuration Optimizer Peak Learning Rate LR Schedule Warmup Steps Weight Decay Adam β1, β2 Training Schedule & Loss Total Training Epochs Global Batch Size Per-GPU Batch Size Gradient Accumulation Steps Mixed Precision AdamW 7e-5 None 0 5e-4 (0.9, 0.999) 6 8 1 1 bfloat16 AdamW 6e-5 None 0 0 5e-4 (0.9, 0.999) 7 / 6 8 1 1 bfloat16 Loss Function L2 (MSE Loss) L2 (MSE Loss) AdamW 8e-5 None 0 5e-4 (0.9, 0.999) 1 4 1 1 bfloat16 Combined Focal + Dice Loss (α = 0.25, γ = 2.0)"
        },
        {
            "title": "B VAE RECONSTRUCTION COMPARISON",
            "content": "As shown in Table 3 and below Figure 5. The vanilla VAE decoder fails to reconstruct accurate masks, discrepancy we attribute to the extreme domain shift between mask images (binary 0/1) and the natural-photo distribution on which the decoder was pre-trained. Fine-tuning the decoder resolves this failure, indicating that the frozen encoder already encodes sufficient mask-related structure and that only the decoder needs to adapt to the new visual modality. Figure 5: Visualization of VAE reconstruction results"
        },
        {
            "title": "C MORE QUALITATIVE RESULTS",
            "content": "In this section, we provide additional qualitative results of FlowRVS on challenging video-text pairs. These examples further demonstrate that our holistic, flow-based approach successfully handles complex language and dynamic scenes, particularly in scenarios involving significant occlusion and nuanced textual descriptions. 13 FlowRVS Figure 6: visualization example of FlowRVS results on MeViS-Valid-u. Figure 7: visualization example of FlowRVS results on MeViS-Valid. FlowRVS Figure 8: visualization example of FlowRVS results on Ref-YouTube-VOS-Valid."
        }
    ],
    "affiliations": [
        "Baidu",
        "SGIT AI Lab, State Grid Corporation of China",
        "The Hong Kong University of Science and Technology",
        "The University of Tokyo",
        "University of California, San Diego",
        "University of Cambridge",
        "Zhejiang University of Technology"
    ]
}