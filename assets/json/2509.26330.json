{
    "paper_title": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval",
    "authors": [
        "Ren-Di Wu",
        "Yu-Yen Lin",
        "Huei-Fang Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from a vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the user's intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in a single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained, demonstrating its potential applicability."
        },
        {
            "title": "Start",
            "content": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval REN-DI WU, YU-YEN LIN, and HUEI-FANG YANG, National Sun Yat-sen University, Taiwan Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the users intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained , demonstrating its potential applicability. CCS Concepts: Do Not Use This Code Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper. Additional Key Words and Phrases: Composed image retrieval, Zero-shot composed image retrieval, Multimodal large language model, Vision-language model, Training-free ACM Reference Format: Ren-Di Wu, Yu-Yen Lin, and Huei-Fang Yang. 2025. SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval. 37, 4, Article 111 (August 2025), 20 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction In Composed Image Retrieval (CIR) [4, 5, 10, 31], users express their intent through multimodal query that pairs reference image with user-provided text describing the desired modifications, aiming to retrieve target image that preserves the core visual content of the reference while incorporating the specified changes. This multimodal formulation enables users to articulate their search intent more precisely and flexibly than unimodal queries, such as pure image-based search [24] or text-to-image retrieval [3, 19, 21]. While more expressive, the multimodal query also introduces significant challenge for CIR: accurately inferring the users compositional intent from both the visual and textual cues. Traditionally, this is addressed via supervised learning on annotated triplets of (a reference image, text Both authors contributed equally to this research. Corresponding author. Authors Contact Information: Ren-Di Wu, m314706017.mg14@nycu.edu.tw; Yu-Yen Lin, m124020039@student.nsysu.edu.tw; Huei-Fang Yang, hfyang@ mail.cse.nsysu.edu.tw, National Sun Yat-sen University, Kaohsiung 804201, Taiwan. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 5 2 0 2 0 ] . [ 1 0 3 3 6 2 . 9 0 5 2 : r 2 Wu et al. Fig. 1. Overview of SQUARE, our training-free framework for zero-shot composed image retrieval (ZS-CIR). SQUARE takes coarseto-fine strategy. In stage 1, Semantic Query-Augmented Fusion (SQAF) enriches the embedding-based composed query, formed by fusing the embeddings of the reference image and modification text from VLM, by incorporating an MLLM-generated target image caption. This addition improves retrieval accuracy while also making the retrieval process interpretable. In stage 2, Efficient Batch Reranking (EBR) leverages the MLLMs multimodal reasoning to refine the ranking. The top-洧 candidate images from SQAFs output are arranged in grid, with each image annotated with distinct label and bounding box. This presentation allows the MLLM to assess all candidates jointly in single forward pass, yielding faster inference and more precise reranking. modifier, target image) [2, 4, 10, 31]. However, since constructing such datasets is labor-intensive and difficult to scale, zero-shot CIR (ZS-CIR) [1, 6, 8, 22, 28] has emerged as recent research focus, aiming to perform retrieval without task-specific training. Among the various paradigms for ZS-CIR, we, in particular, study the training-free solutions. To achieve ZS-CIR in training-free manner, recent studies have leveraged the visualtextual alignment learned by large pretrained models. Visionlanguage models (VLMs), such as CLIP and BLIP, embed images and text into shared semantic space, enabling the direct combination of visual and textual cues without additional training. common strategy is to construct the composed query in this space by manipulating the embeddings of the reference image and the modification text, for example, via weighted combination [33] or spherical linear interpolation [7]. While such embedding-based approaches are simple and efficient, they often treat the modification text as coarse signal, underutilizing its rich semantic structure. This raises the question: Can we better capture and integrate the semantic nuances to improve retrieval performance? complementary line of work reframes ZS-CIR as text-to-image retrieval task [8, 17, 29, 36] leveraging the multimodal reasoning capabilities of multimodal large language models (MLLMs) or the natural language understanding strengths of LLMs. The core idea is to utilize these models to generate detailed target image caption that explicitly encodes the intended compositional reasoning, and then retrieve images using this caption within VLMs semantic space. Although such captions often capture user intent well, retrieval accuracy can be limited by the quality, specificity, Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 3 and faithfulness of the generated descriptions. Moreover, with the rapid progress in MLLMs, beyond caption generation, can these models be further exploited to directly enhance retrieval? In this paper, to address these questions, we propose SQUARE, Semantic Query-Augmented Fusion and Efficient Batch Reranking, novel training-free framework for ZS-CIR. As illustrated in Figure 1, SQUARE operates in two stages. First, to enhance the embedding-based composed queries, we introduce Semantic Query-Augmented Fusion (SQAF) that incorporate high-level semantics. Build upon the weighted query fusion strategy in our prior work, WeiMoCIR [33], we extend its training-free design by incorporating MLLMs for semantic enrichment. MLLMs are capable of generating rich, high-level descriptions that capture nuanced relationships between the reference image and modification text. The embedding-based query is then enhanced with these semantic cues to strengthen the query-target alignment. This leads to more accurate global retrieval and is particularly beneficial when using lightweight VLMs with limited capacity. Second, we introduce Efficient Batch Reranking (EBR) that further exploits MLLMs multimodal reasoning for reranking. Building on the findings that visual marks on image regions enhance the fine-grained visual grounding abilities of MLLMs [9, 34], we annotate each candidate image with distinct label and bounding box, and then arrange them into single grid image. This holistic view enables the MLLM to perform joint comparison of all candidates within single forward pass, leading to both faster inference and more accurate reranking. By encouraging the model to reason about inter-image differences rather than evaluating each image in isolation, EBR promotes richer comparative understanding. Furthermore, it is designed as standalone module that can be seamlessly integrated into existing retrieval pipelines. Our method adopts modular, coarse-to-fine strategy, offering high flexibility in model selection based on the desired trade-offs between efficiency and effectiveness. Moreover, it can seamlessly integrate different VLMs and MLLMs without additional training, making it adaptable to diverse computational budgets and performance requirements. In summary, our main contributions are as follows: We propose SQUARE, novel training-free framework for ZS-CIR, that follows coarse-to-fine pipeline. SQUARE enables effective retrieval without requiring model fine-tuning or annotated triplets. Our SQAF enhances the embedding-based image-text fusion with MLLM-generated descriptions of the target image. This straightforward approach not only boosts retrieval performance, but also enhances the interpretability of the composed query. By harnessing MLLMs multimodal reasoning to compare top candidates in single forward pass, our EBR offers an efficient reranking strategy that significantly improves result quality while incurring minimal computational overhead. Extensive experiments on four CIR benchmarks show that SQUARE consistently achieves strong performance. Notably, it remains effective even with lightweight backbone models, demonstrating both practicality and scalability. 2 Related Work 2.1 Zero-Shot Composed Image Retrieval Zero-shot composed image retrieval (ZS-CIR) aims to retrieve images that match reference image modified by natural language instructions, without requiring additional task-specific training. Most existing ZS-CIR methods build on textual inversion, where abundant imagetext pairs are used to train mapping network that transforms reference image into pseudo-word token within CLIPs token embedding space. The pseudo-word token is then concatenated Manuscript submitted to ACM 4 Wu et al. with the modification text for retrieval. The seminal work in this line, Pic2Word [22], introduces lightweight mapping network for this purpose. SEARLE [1] adopts more sophisticated two-stage strategy: it first generates pseudo-word tokens for unlabeled images via optimization-based textual inversion with GPT-guided regularization loss, and then distills this knowledge into mapping network for efficient inference. However, the pseudo-word tokens produced by these approaches primarily capture global visual information. To address this limitation, KEDs [27] further enhances the representation by leveraging an external imagecaption database to enrich the pseudo-word tokens with finegrained attribute details. Since only subset of visual features is relevant to the modification intent, Context-I2W [28] employs an intent-view selector and visual target extractor to learn mapping network that adaptively attends to the context-dependent visual cues. While these methods have made notable progress, they still depend on training mapping networks with large collections of imagetext pairs. This limitation has motivated the development of training-free ZS-CIR approaches, which enable retrieval directly through pretrained models used as off-the-shelf tools. 2.2 Training-Free Zero-shot Composed Image Retrieval Training-free ZS-CIR approaches typically build on the joint imagetext semantic space established by VLMs, while also exploiting the advanced reasoning capabilities of LLMs and MLLMs. We categorize existing methods into three groups: methods relying solely on VLMs, methods combining VLMs with LLMs, and methods integrating VLMs with MLLMs. VLM-only approaches. VLMs like CLIP [20] and BLIP [11] provide pre-trained semantic space that aligns images and text, and they have been fundamental component of recent CIR methods. Building on this joint embedding space, Slerp [7], the only training-free VLM-only approach, models the composed query as an intermediate representation between the embeddings of the reference image and modification text, obtained through spherical linear interpolation. Despite its simplicity, this interpolation strategy has demonstrated promising performance in ZS-CIR. VLM+LLM approaches. Approaches in this category leverage the reasoning capabilities of LLMs to more effectively capture user intent in CIR. LLMs excel at natural language reasoning and compositional understanding, making them well-suited for generating rich, human-understandable textual representations of the desired target image. CIReVL [8] reformulates CIR as text-to-image retrieval task: it first employs VLM to generate caption for the reference image and then uses an LLM to integrate this caption with the modification text, producing refined description of the target image for retrieval. To address the limitation that single generated caption may not fully capture the user intent, methods such as LDRE [36] and SEIZE [35] expand the space of possible interpretations by generating multiple captions. The VLM provides diverse captions for the reference image, which are then refined by the LLM in light of the modification text, thereby covering the potential semantics of the desired target image. However, since the reference image caption is produced by the VLM independently of the modification text, it may omit visual details that are crucial for the intended edits. This could result in descriptions that do not fully reflect the target image. Although generating multiple diverse captions can partially mitigate this issue by broadening semantic coverage, the process is computationally expensive and risks introducing redundant or noisy descriptions. VLM+MLLM approaches. The emergence of MLLMs, such as GPT-4o [16] and Gemini, has opened new avenues for ZS-CIR by enabling advanced reasoning over multimodal inputs, thereby addressing the limitations of VLMs+LLMs approaches. OSrCIR [29] leverages chain-of-thought (CoT) reasoning jointly interpret the visual content and the modification text, producing target descriptions that more closely align with the intended image in single-stage reasoning process. To better preserve both the explicit modification and implicit visual cues, MCoT-RE [17] introduces Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 5 multi-faceted CoT and generates two distinct captions: one highlighting the modification and the other capturing the broader visualtextual context. ImageScope [15] captures the user intent at multiple semantic granularities to improve robustness. In contrast, WeiMoCIR [33] takes different perspective by using an MLLM to generate captions for the database images; during retrieval, it considers both query-to-image and query-to-caption similarities. Our work shares similar spirit with previous studies by using an MLLM to generate target image descriptions; however, rather than directly using the captions for text-to-image retrieval [29], we enhance the embedding-based query with semantic information from the MLLM-generated captions. This additional guidance yields more informative query representation that better aligns with the intended target image. 2.3 MLLMs for Reranking Beyond multimodal query composition, MLLMs have also been explored for reranking. common strategy is to prompt MLLMs with questions to assess whether specific aspects are present in candidate images. For instance, GRB [26] verifies the existence of local concepts by posing binary (Yes/No) questions to an MLLM. Based on such binary judgments, MM-EMBED [12] further derives relevance scores to enable finer-grained reranking. Rather than performing single refinement, ImageScope [15] adopts multi-stage refinement process: predicate propositions are first verified locally for each candidate image, followed by global pairwise comparisons between the reference image and the top-ranked candidates, with both stages relying on binary decisions. In contrast to the existing approaches, which rely on binary verification of individual images, we exploit the multimodal reasoning capabilities of MLLMs to rerank candidates by jointly comparing all images with the aid of visual marks. This reranking strategy can be performed in single forward pass, enabling faster inference. 3 Methodology CIR is multimodal retrieval task in which query consists of reference image 洧냪洧 and modification text 洧녢洧녴. The goal is to retrieve target image 洧냪洧노 from gallery of images = {洧냪洧녰 }洧녜 洧녰=1 that captures the visual characteristics of 洧냪洧 while reflecting the semantic changes described in 洧녢洧녴. In training-free ZS-CIR, no task-specific training is required. Instead, pretrained VLMs, such as CLIP, are typically leveraged to perform the retrieval directly. Specifically, the reference image and the text modifier are encoded into joint embedding space using the VLMs image encoder Eimg () and text encoder Etxt (). composition function, 洧녭 (Eimg (洧냪洧 ), Etxt (洧녢洧녴)), then fuses these embeddings into single query representation. Retrieval is then performed by ranking the gallery images based on their similarity (e.g., cosine similarity) to the composed query embedding. This approach enables CIR without additional training by relying on the pretrained alignment between visual and textual modalities in VLMs. To improve retrieval accuracy in this training-free setting, we introduce SQUARE, which is driven by our core idea of leveraging an MLLM to enhance both the composed query and the candidate reranking. As illustrated in Figure 1, our SQAF module employs the MLLM to generate target image caption by reasoning over the users intent expressed in the reference image 洧냪洧 and the modification text 洧녢洧녴. This caption is then incorporated into the composed query representation boost global ranking performance. In our EBR module, the MLLM is further utilized to rerank shortlist of the top candidate images by comparing their visual content against the reference image and modification text. Manuscript submitted to ACM Wu et al. Fig. 2. Example prompt used for generating the imagined target image caption. Our prompt is composed of three handcrafted few-shot examples and set of explicit rules that guide the MLLM to generate concise and concrete description of the target image based on given reference image and textual modification. 3.1 SQAF: Semantic Query-Augmented Fusion for Richer Composed Queries In training-free setting with VLMs, common approach to construct the query representation is to perform weighted fusion of the visual embedding of the reference image and the textual embedding of the modification text: qvlm = (1 洧띺) Eimg (洧냪洧 ) + 洧띺 Etxt (洧녢洧녴), (1) where qvlm R洧녬 is the resulting 洧녬-dimensional VLM-based query representation, and 洧띺 [0, 1] is hyperparameter that controls the relative contributions of the image and text. However, while this fusion helps the query reflect both the visual appearance and the desired change, it is insufficient for truly comprehensive understanding of the users intent. This is mainly because the reference image, while visually rich, is often semantically ambiguous, and the modification text, while semantically clear, lacks detailed visual information. To address this limitation, we leverage the MLLMs powerful multimodal reasoning ability to enrich the query, thereby enabling more complete capture of the users complex compositional request. MLLM-based Query Augmentation. Given their powerful multimodal reasoning capabilities, MLLMs are suited to infer users intent from complex, multi-modal queries. We leverage this ability to generate high-level description of the desired target image. As depicted in the upper part of Figure 1, the MLLM M1 takes as input the reference image 洧냪洧 , the text modifier 洧녢洧녴, and reflective prompt 洧녞caption. This prompt, whose structure is shown in Figure 2, guides the MLLMs reasoning with few in-context examples. By demonstrating the desired task and output format, these examples instruct the MLLM to articulate the desired compositional changes. The generation of the target image Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 7 description 洧녢洧노 is formalized as follows: 洧녢洧노 = M1 (洧냪洧 ,洧녢洧녴, 洧녞caption). The resulting 洧녢洧노 serves as human-readable interpretation of the users intent, grounded in multimodal understanding of the input. Unlike an opaque vector fusion, this caption is inherently interpretable and functions as text-based proxy (2) for the target image by explicitly articulating the compositional request. Final Query Construction. In the final stage of query construction, we integrate the semantic strengths of the MLLM with the representational power of the VLM. While the VLM-based query qvlm encodes both visual and textual features, it may fall short in explicitly capturing high-level, compositional semantics. On the other hand, the MLLM-generated caption 洧녢洧노 provides human-readable, semantically rich description of the target image by reasoning over the reference image 洧냪洧 and the modification text 洧녢洧녴. We embedding this caption using the VLMs text encoder Etxt () and fuse it with qvlm to obtain the final query: = (1 洧띻) qvlm + 洧띻 Etxt (洧녢洧노 ), where 洧띻 [0, 1] is hyperparameter that controls the influence of the MLLMs semantic description. By appropriately setting 洧띻, the fusion integrates the multimodal alignment of the VLM with the high-level reasoning of the MLLM. This allows the expanded intent provided by the MLLM captions to enrich semantics while still being constrained by the (3) information in the VLM-based query, ultimately yielding representation that is both semantically richer and more closely aligned with the users intent. Global Ranking. We first encode all gallery images 洧냪洧녰 into image embeddings using the VLMs image encoder Eimg (). For each gallery image, we compute similarity score 洧녡洧녰 by calculating the cosine similarity between the final query representation and its image embedding Eimg (洧냪洧녰 ): 洧녡洧녰 = Eimg (洧냪洧녰 ) qEimg (洧냪洧녰 ) , 洧냪洧녰 D. (4) Given these scores, we select the top-洧 most relevant images to the query, forming an ordered list of candidate images: C洧 = (洧냪 (0), 洧냪 (1), . . . , 洧냪 (洧 1) ), (5) where the superscript denotes the rank position in descending order of similarity. The candidate set C洧 is then passed to the reranking stage for finer-grained alignment with the users intent. 3.2 EBR: Efficient Batch Reranking for Finer-grained Alignment Although our composed query, which fuses the VLM-based representation with the MLLM-generated caption, provides decent coarse-level alignment with candidate images, it may still fall short in capturing subtle compositional relationships and fine-grained semantic cues. Inspired by SoM [34], our intuition is that an MLLM, with its ability to jointly reason over visual and textual inputs, can evaluate all candidates in shared context, compare them with the users intent, and produce more semantically accurate ordering. To prepare for reranking, we annotate each candidate with colored bounding box and numeric label at the top-left corner. In our implementation, we set 洧 = 洧 2 and arrange these annotated images into compact 洧 洧 grid image G. Formally, we define grid construction function Grid() that arranges the inputs row-wise into an 洧 洧 layout: = Grid(cid:0){Annotate(洧냪 (洧녰 ), 洧녰)}洧 1 洧녰=0 ; rows = 洧, cols = 洧 (cid:1), (6) Manuscript submitted to ACM 8 Wu et al. Fig. 3. An example grid image used in the reranking process. Each candidate image is marked with colored bounding box and numeric label at the top-left corner. These identifiers enable the MLLM to reference specific images during reasoning and generate an updated ranking based on the image relevance to the users intent. Fig. 4. Example prompt for the reranking stage. This prompt provides the MLLM with the reference image, the textual modification, and grid of candidate images. The MLLMs task is to reason over the visual differences and produce an updated ranking based on the candidates alignment with the users intent. where Annotate(洧냪, 洧녰) overlays the image 洧냪 with bounding box and its rank index 洧녰 for explicit referencing. Figure 3 shows an example with 洧 = 4. These visual and numeric identifiers enable the MLLM to reference specific images during multimodal reasoning and to produce an updated ranking that reflects each candidates relevance to the users intent. With the constructed grid image, critical design decision in our reranking stage concerns how to best represent the users intent to the MLLM. Although one could use the target image caption from the SQAF module as proxy for the users intent, we instead choose to use the reference image and the modification text directly. This choice offers two advantages: (1) It allows our EBR to function as standalone reranking module, independent of the method used to obtain the top-洧 candidate set, and can thus be seamlessly integrated into any existing CIR pipeline for refinement. (2) Directly providing the reference image and the modification text allows the MLLM to perform fresh multimodal Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 9 reasoning over the original inputs, reducing the potential information loss or bias that might arise from relying solely on the caption. Based on this design choice, the MLLM M2 takes as input the reference image 洧냪洧 , the modification text 洧녢洧녴, the grid image 洧냨, and the reranking prompt 洧녞rerank. It then returns an ordered, updated index sequence 洧랢 , defined as: 洧랢 = M2 (洧냪洧 ,洧녢洧녴, G, 洧녞rerank), (7) where the elements of 洧랢 are drawn from {0, 1, . . . , 洧 1}, and the structure of the reranking prompt is shown in Figure 4. While the prompt requests that the MLLM consider all candidate images, 洧랢 may contain only partial list of indices. This implies that the model deems only subset of the candidates relevant. In such cases, we preserve the initial order of the remaining candidates and append them to the end of the MLLMs ranking to produce complete ordering. That is, the final ranking 洧랢 = 洧랢 洧랢rem, where denotes the sequence concatenation, and 洧랢rem is the sequence of indices in {0, 1, . . . , 洧 1} Set(洧랢 ) for the remaining candidates, sorted in their initial order. The final ordered list of ranked images is obtained by applying this final index sequence to the original set of candidate images: = (洧냪 (洧랢0 ), 洧냪 (洧랢1 ), . . . , 洧냪 (洧랢洧 1 ) ). (8) 4 Experiments 4.1 Datasets and Setup Datasets. We evaluate SQUARE on four public CIR benchmarks, namely CIRR [14], CIRCO [1], FashionIQ [32], and GeneCIS [30]. CIRR [14], built upon NLVR2 [25], consists of real-world images with diverse visual content. The modification text typically involves changes at objector scene-level. We report results on the test set evaluation server1. CIRCO is larger benchmark built from natural images sourced from the COCO 2017 unlabeled set [13]. Each query is linked with multiple ground-truths images, averaging 4.53 per query. With retrieval set comprising 123,403 images, the dataset presents greater number of distractors, making the task significantly more challenging. We report results on the test set evaluation server2. FashionIQ [32] is well-known benchmark for evaluating CIR in the fasion domain. It comprises real-world product images across three categories: Dress, Shirt, and Toptee. Each query comprises reference image and natural language modification describing subtle appearance changes, such as color, length, or style, supporting fine-grained compositional reasoning in retrieval. We follow previous studies [29, 36] and report results on the validation set. GeneCIS is built from MS-COCO [13] and Visual Attributes in the Wild (VAW) [18], focusing on four CIR task types: modifying or focusing on specific attributes or objects. It includes 8,032 query pairs, each paired with small retrieval set (typically 15 images, or 10 for the \"Focus on an Attribute\" task), enabling controlled, fine-grained evaluation of semantic transformations in retrieval. Evaluation metrics. We report Recall@K (R@K) with = 1, 5, and 10 for CIRR, which evaluates whether the groundtruth image appears among the top-K retrieved results. Additionally, we report RecallSubset@K with = 1, 2, and 3 in the subset setting of CIRR, where the retrieval is performed on restricted candidate pool composed of images that are semantically similar to the query, especially including challenging negatives selected to prevent trivial discrimination. For CIRCO, we adopt the mean Average Precision at (mAP@K) with = 5, 10, 25, and 50, which accounts for multiple ground-truths per query and provides more fine-grained evaluation of ranked retrieval performance. For FashionIQ, we compute R@10 and R@50 separately for each clothing category and report the average across categories. In the case 1https://cirr.cecs.anu.edu.au/test_process/ 2https://circo.micc.unifi.it/evaluation/ Manuscript submitted to ACM 10 Wu et al. Table 1. Comparison of retrieval performance on the CIRR and CIRCO test sets. Best results are highlighted in bold. Our method consistently achieves superior performance across all metrics and CLIP backbones. Metric Recall@K RecallSubset@K CIRR CIRCO mAP@K Backbone Method K=1 K= K=10 K=1 K=2 K=3 K=5 K=10 K=25 K= ViT-B/32 ViT-L/14 ViT-G/14 Slerp [7] (ECCV24) CIReVL [8] (ICLR24) LDRE [36] (SIGIR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) ImageScope [15] (WWW25) SQUARE w/o EBR (Ours) SQUARE (Ours) Slerp [7] (ECCV24) CIReVL [8] (ICLR24) LDRE [36] (SIGIR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) ImageScope [15] (WWW25) MCoT-RE [17] (IEEE SMC25) SQUARE w/o EBR (Ours) SQUARE (Ours) CIReVL [8] (ICLR24) LDRE [36] (ICLR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) MCoT-RE [17] (IEEE SMC25) SQUARE w/o EBR (Ours) SQUARE (Ours) 50.94 52.51 55.13 57.42 54.54 66.27 65.57 8.75 24.22 17.82 23.94 21.11 25.69 22.49 27.47 21.85 25.42 28.11 38.43 33.49 24.31 45.04 72.05 79.95 80.94 93.13 97.23 28.95 28.46 29.66 30.59 57.86 60.17 60.53 65.59 62.31 75.93 63.74 89.25 90.19 90.70 92.77 91.13 94.63 91.57 7.11 15.42 18.32 19.64 19.17 25.82 21.47 8.12 17.00 20.21 21.55 20.94 27.15 23. 78.27 80.05 80.65 84.48 80.86 89.21 82.39 63.49 66.00 69.04 70.17 68.19 76.96 76.72 6.35 14.94 17.96 19.04 18.04 25.26 20.89 49.93 52.31 55.57 57.16 57.68 67.54 69.18 67.54 11.99 24.43 21.80 24.55 27.50 26.53 29.35 28.65 28.97 29.45 31.88 39.37 39.52 36.70 30.95 44.94 72.29 80.65 80.55 93.30 97.23 33.94 33.79 35.41 36.57 57.71 59.54 60.43 66.22 62.12 76.36 70.19 66. 88.80 89.69 89.90 92.34 91.10 95.21 93.83 92.34 9.84 19.01 24.03 25.82 25.33 29.23 27.47 77.59 79.88 80.31 84.05 81.92 89.40 86.34 83.81 11.30 20.89 26.44 28.24 27.84 30.81 29.79 62.29 64.92 67.54 69.23 69.86 78.05 79.28 78.19 8.76 18.57 23.35 24.98 23.87 28.36 26. 64.29 66.39 69.42 67.25 68.92 69.47 31.03 34.65 36.03 36.15 37.55 38.87 36.59 37.26 39.37 35.64 38.72 45.13 72.60 80.96 79.90 92.94 97.06 35.61 35.64 37.70 38.82 27.59 32.24 33.77 31.14 32.06 67.95 68.82 74.15 69.22 70.82 68.10 26.77 31.12 32.46 30.47 30.89 75.06 77.25 79.42 77.33 79.30 79. 84.87 85.66 89.23 85.28 86.92 85.37 93.21 93.76 95.71 93.55 93.88 93.37 29.96 34.95 36.46 35.03 34.52 of GeneCIS, we evaluate R@K (K = 1, 2, 3) on four distinct tasks, each designed to test the models ability to handle different types of semantic modification. Implementation Details. We use the CLIP models pretrained on the LAION-2B English subset of the LAION-5B dataset [23] as our VLM, denoted by their backbone and patch size (e.g., ViT-B/32). Specifically, we use the CLIP visual encoder to extract image features and the text encoder to embed both the user-provided modification texts and the MLLM-generated descriptions. We employ GPT-4o [16] as the MLLM, with the temperature set to its default value of 1.0. For the SQAF stage, which produces an initial global ranking, we set the fusion weight 洧띺 in Equation (1) to 0.7 and 洧띻 in Equation (1) to 0.6. The top 16 retrieved images from SQAF are arranged into 4 4 grid image and passed to the reranking module. All experiments are All experiments are conducted on single NVIDIA RTX 3090 GPU using Python 3.8 (Conda) and PyTorch 2.3.0. Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 11 4.2 Main Results We compare SQUARE against representative training-free ZS-CIR methods across three categories: VLM-only, represented by Slerp [7]; VLM+LLM, including CIReVL [8], LDRE [36], and SEIZE [35]; and VLM+MLLM, comprising OSrCIR [29], ImageScope [15], and MCoT-RE [17]. CIRR results. On CIRR, as shown in the left part of Table 1, VLM+LLM approaches generally show improved performance compared to VLM-only approaches, indicating that the high-level semantic information provided by LLMs helps better capture the modification intent. We also observed that the performance of the VLM+LLM methods improves with larger VLP models. This highlights the importance of high-capacity CLIP representations. In contrast, VLM+MLLM-based methods, benefiting from the strong reasoning and semantic understanding capabilities of MLLMs, typically outperform other approaches that do not use MLLMs. Our method, when using only the SQAF module (denoted as SQUARE w/o EBR), outperforms OSrCIR [29] across nearly all settings and CLIP backbones, with the exception of ViT-G/14 on the CIRR subset. When further employing the EBR module, SQUARE achieves the highest retrieval accuracy among all the methods compared. While both ImageScope and MCoT-RE also adopt coarse-to-fine strategies, our approach is more efficient: the prompt requires only small number of exemplar texts to identify global ranking candidates, avoiding the token-intensive CoT process. simple reranking of these top candidates then yields substantial gains, resulting in clear improvements over ImageScope and MCoT-RE. These results highlight the effectiveness of SQUARE, which combines semantic query fusion and MLLM-guided reranking for training-free ZS-CIR. CIRCO results. On CIRCO, as presented in the right part of Table 1, MLLM-based methods generally outperform non-MLLM-based ones when using smaller CLIP backbones. With larger backbones, however, non-MLLM approaches such as LDRE and SEIZE, which leverage an LLM to perform compositional reasoning over multiple captions generated for the reference image and the modification text, achieve competitive performance. Our method, SQUARE, achieves consistently superior performance across all CLIP backbones. This improvement is largely attributed to the effectiveness of the proposed reranking strategy, which yields an average boost of 6.66% in mAP@5. FashionIQ results. Table 2 presents the results on the FashionIQ validation set for the fashion attribute manipulation task. Our SQUARE w/o EBR already outperforms all other methods in both R@10 and R@50 metrics with the ViT-B/32 and ViT-L/14 backbones. This demonstrates the ability of our proposed SQAF query composition to retrieve semantically relevant images. Incorporating the EBR module further boosts R@10 by reranking only the top 16 candidate images. With the ViT-G/14 backbone, SQUARE w/o EBR and SQUARE achieve higher R@10 scores than SEIZE, although they slightly underperform in R@50. This suggests that our method is particularly effective in retrieving highly relevant images within the top-ranked results, achieving competitive results even without relying on multiple captions like SEIZE. Overall, SQUARE remains highly competitive in different settings. GeneCIS results. Table 3 summarizes the retrieval results on GeneCIS. Our method, SQUARE, shows strong performance in most retrieval tasks, consistently achieving the highest R@1 scores with all backbones. However, for the change object task, SQUARE underperforms relative to other methods. We observe that in this task, the MLLM may overemphasize objects or background elements that are irrelevant to the search intent, thereby reducing overall ranking performance. Despite this limitation, SQUARE remains highly competitive overall, especially in retrieving the most relevant images at the top ranks for diverse compositional queries. Manuscript submitted to ACM 12 Wu et al. Table 2. Comparison of retrieval performance on the FashionIQ validation set. Best results are highlighted in bold. On average, our method outperforms all baselines in the R@10 metric across different backbones. Backbone Method R@10 R@50 R@10 R@50 R@ R@50 R@10 R@50 Shirt Dress Toptee Average ViT-B/32 ViT-L/14 ViT-G/14 Slerp [7] (ECCV24) CIReVL [8] (ICLR24) LDRE [36] (SIGIR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) ImageScope [15] (WWW25) SQUARE w/o EBR (Ours) SQUARE (Ours) Slerp [7] (ECCV24) CIReVL [8] (ICLR24) LDRE [36] (SIGIR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) ImageScope [15] (WWW25) MCoT-RE [17] (IEEE SMC25) SQUARE w/o EBR (Ours) SQUARE (Ours) CIReVL [8] (ICLR24) LDRE [36] (SIGIR24) SEIZE [35] (ACM MM24) OSrCIR [29] (CVPR25) MCoT-RE [17] (IEEE SMC25) SQUARE w/o EBR (Ours) SQUARE (Ours) 23.75 28.36 27.38 29.38 31.16 31.65 36.46 38.37 28.66 29.49 31.04 33.04 33.17 32.87 36.60 39.79 41.32 33.71 35.94 43.60 38.65 42.35 44.60 45.04 40.92 47.84 46.27 47.97 51.13 50.15 56.97 56.97 43.96 47.40 51.22 53.22 52.03 51.07 53.09 59.22 59. 51.42 58.58 65.42 54.71 59.81 62.51 62.51 20.53 25.29 19.97 25.37 29.35 26.82 36.44 36.99 21.96 24.79 22.93 30.93 29.70 26.17 35.94 35.99 37.98 27.07 26.11 39.61 33.02 34.51 37.68 37.68 41.00 46.36 41.84 46.84 50.37 46.31 57.46 57.46 41.55 44.76 46.76 50.76 51.81 46.15 55.82 57.51 57. 49.53 51.12 61.02 54.78 56.67 60.19 60.19 26.98 31.21 27.07 32.07 36.51 35.80 44.72 46.40 30.24 31.36 31.57 35.57 36.92 35.03 43.55 45.54 46.61 35.80 35.42 45.94 41.04 45.74 49.67 49.87 46.77 53.85 48.78 54.78 58.71 55.94 65.02 65.02 48.34 53.65 53.64 58.64 59.27 55.12 64.30 64.97 64. 56.14 56.67 71.12 61.83 67.57 69.25 69.25 23.75 28.29 24.81 28.94 32.34 31.42 39.21 40.59 26.95 28.55 28.51 33.18 33.26 31.36 38.70 40.44 41.97 32.19 32.49 43.05 37.57 40.87 43.98 44.20 42.90 49.35 45.63 49.86 53.40 50.80 59.82 59.82 44.62 48.57 50.54 54.21 54.37 50.78 57.74 60.57 60. 52.36 55.46 65.85 57.11 61.35 63.98 63.98 4.3 Ablation Studies The performance of SQUARE can be significantly influenced by the choice of VLMs and MLLMs, as well as the fusion hyperparameters. We analyze the impact of these factors to provide deeper understanding of our method. Effect of different VLMs on SQAF performance. SQUARE relies on the SQAF module to generate global ranking, and its retrieval quality is closely tied to the underlying VLMs. Table 4 presents the impact of different VLM backbones on SQAFs performance. Among the evaluated models, CLIP ViT-G/14 achieves the highest mAP scores across all evaluated ranks (top-K), followed by CLIP ViT-L/14 and ViT-B/32, indicating that larger and more expressive backbones consistently yield better results. BLIP and BLIP-2 also demonstrate competitive performance, particularly BLIP-2 with ViT-G, which outperforms some CLIP variants at certain ranks. These findings highlight the flexibility of our retrieval framework and its ability to take advantage of advances in backbone architectures, with larger models offering stronger feature representations for composed image retrieval. Effect of different MLLMs on SQAF performance. The choice of MLLM within our SQAF module significantly impacts retrieval performance, as shown in Table 5. When the MLLM-generated description of the target image is excluded Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 13 Table 3. Comparison of retrieval performance on GeneCIS. Best results are highlighted in bold. On average, our method achieves the highest R@1 scores across different tasks and backbones. Backbone Method R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 Focus Attribute Change Attribute Focus Object Change Object Average ViT-B/32 ViT-L/14 ViT-G/14 CIReVL [8] (ICLR24) OSrCIR [29] (CVPR25) SQUARE w/o EBR (Ours) SQUARE (Ours) CIReVL [8] (ICLR24) OSrCIR [29] (CVPR25) SQUARE w/o EBR (Ours) SQUARE (Ours) CIReVL [8] (ICLR24) OSrCIR [29] (CVPR25) SQUARE w/o EBR (Ours) SQUARE (Ours) 29.4 32.7 33.8 25.8 40.4 17.9 27.7 42.8 19.4 24.9 44.6 20.3 25.6 39.3 48.6 19.0 30.5 14.8 16.4 14.0 35.8 38.1 34.6 37. 33.3 24.3 14.6 35.8 25.7 15.7 37.6 26.9 15.6 17.3 29.1 38.3 16.1 37.6 27.8 18.2 30.1 39.4 37.4 26.8 15.6 34.8 26.9 16.8 31.8 33.1 34.0 30.5 26.0 19.5 34.2 28.5 20.9 38.0 25.6 20.3 25.6 39.2 49.2 19.8 30.5 37.9 17.6 30.3 39.6 35.2 37.9 35.1 21.8 23.6 27. 42.0 44.5 44.2 12.3 15.0 17.3 14.4 17.2 14.4 17.2 28.9 18.4 30.6 29.1 17.2 27.2 16.3 37.6 38.3 38.7 35.1 34.0 36.4 35. 16.1 20.5 17.9 22.7 21.6 14.8 26.4 39.8 49.5 19.4 44.5 47.0 44.9 28.6 39.4 30.8 42.0 37.1 26.8 37.5 29.3 33.0 25.2 14.7 36.7 28.4 16.9 16.8 38.8 28.4 17.9 29.6 39.0 18.1 41.0 31.2 21.0 33.4 44.2 38.6 28.9 15.7 35.0 26.6 14.7 15.9 17.4 16.4 19. 15.9 17.9 17.3 19.8 17.4 19.6 17.2 19.6 Table 4. Effect of different VLM backbones on the performance of the SQAF module (i.e., SQUARE w/o EBR). Retrieval results are reported on the CIRCO test set. Backbone mAP@5 mAP@ mAP@25 mAP@50 CLIP ViT-B/32 CLIP ViT-L/14 CLIP ViT-G/14 BLIP w/ ViT-B BLIP w/ ViT-L BLIP-2 w/ ViT-G 20.89 26.74 30.89 25.41 25.85 28.33 21.47 27.47 32.06 26.34 26.82 29.40 23.38 29.79 34.52 28.72 29.18 31. 24.31 30.95 35.64 29.68 30.24 32.88 Table 5. Effect of MLLM choice on SQAF performance. Results are reported using CLIP ViT-G/14 as the VLM on the CIRCO test set. w/o MLLM indicates that the MLLM-generated description of the target image is not included. MLLM mAP@5 mAP@10 mAP@25 mAP@50 w/o MLLM ChatGPT-4.1 ChatGPT-4.1-mini ChatGPT-4o ChatGPT-4o-mini 18.05 31.45 29.57 30.89 27. 19.02 32.57 30.66 32.06 27.61 20.59 35.30 33.17 34.52 30.18 21.42 36.44 34.26 35.64 31.22 (indicated as w/o MLLM), the mAP scores are substantially lower. This suggests that the VLM alone is insufficient to fully capture the users intent and the semantic reasoning required for the task. Incorporating MLLMs leads to substantial improvement in performance. Among the evaluated models, ChatGPT-4.1 achieves the highest mAP across all evaluated ranks (top-K), demonstrating that larger, more advanced MLLMs are better at understanding users intent. Not surprisingly, the mini variants perform slightly worse than their full-sized counterparts, reflecting trade-off between model capacity and retrieval accuracy. Overall, these results underscore the importance of strong semantic reasoning in the query fusion stage for training-free ZS-CIR. Manuscript submitted to ACM Wu et al. Table 6. Effect of different MLLMs used as rerankers. Results are reported using CLIP ViT-G/14 as the VLM and ChatGPT-4o within the SQAF module on the CIRCO test set. MLLM mAP@5 mAP@10 mAP@25 mAP@50 w/o EBR ChatGPT-4.1 ChatGPT-4.1-mini ChatGPT-4o ChatGPT-4o-mini 30.89 35.61 34.34 30.87 24. 32.06 35.64 34.77 31.96 26.50 34.52 37.70 36.87 34.09 29.13 35.64 38.82 37.99 35.21 30.25 Table 7. Effect of grid image size on MLLM-based reranking. Results are reported on the CIRCO test set using CLIP ViT-G/14 as the VLM, ChatGPT-4o within the SQAF module, and ChatGPT-4.1 as the reranker. Grid Image Size mAP@ mAP@10 mAP@25 mAP@50 w/o EBR 3 3 4 4 5 5 6 6 30.89 37.97 35.61 31.06 24.31 32.06 36.89 35.64 32.05 24. 34.52 39.28 37.70 34.37 27.49 35.64 40.40 38.82 35.49 29.00 Effect of different MLLMs as rerankers. As shown in Table 6, we observe similar trends when MLLMs are used as rerankers in EBR as when they are employed in SQAF: MLLMs with stronger reasoning capabilities over visual and textual semantics, such as ChatGPT-4.1, consistently outperform the others. Notably, ChatGPT-4.1-mini, despite its smaller size, achieves competitive performance, with an average mAP that is less than 1% lower than ChatGPT-4.1. This suggests that ChatGPT-4.1-mini offers cost-effective alternative with minimal performance trade-off. In contrast, ChatGPT-4o yields unsatisfactory results, likely due to its optimization for speed and efficiency, thereby compromising its visual-textual understanding and cross-image comparison capabilities. Such design choice therefore hinders ChatGPT-4os effectiveness in reranking, as our EBR strategy relies on an MLLM capable of accurately interpreting user intent and performing cross-image reasoning to evaluate how well candidate images align with that intent. Effect of grid image size on MLLM-based reranking. Table 7 shows how the grid size (i.e., the number of candidate images) affects the reranking performance. The 33 grid achieves the highest mAP scores, yielding an average improvement of 5.35% over the one without reranking (i.e., w/o EBR). The 44 grid also delivers gain of 3.66% mAP. However, as the grid size increases to 55 and 66, performance degrades and even falls below that of w/o EBR. This decline is likely due to the increased visual complexity and the presence of more distractors in larger grids, which may overwhelm the MLLMs reasoning capacity and hinder its ability to accurately assess image relevance to the query. While the 3x3 grid achieves the best performance, we adopt the 4x4 grid as practical choice. In real-world search scenarios, users are typically presented with relatively large set of candidate images, and the 44 grid enables refinement over broader pool of candidates (16 versus 9), thereby better reflecting practical usage. Furthermore, the ability to influence the top 16 ranks allows the reranker to have greater effect on recall@K, which serves as the primary evaluation metric in most benchmarks. Comparison of different forms of user intent for reranking. Our EBR ranks the candidate images based on user intent expressed through the reference image and modification text. As an alternative, we also consider representing user Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 15 Fig. 5. Example prompt for reranking with image captions generated in SQAF. The prompt guides the MLLM to reason over visual differences and update the ranking based on how well the candidates align with the users intent expressed as the captions. Table 8. Comparison of different forms of user intent for reranking. Results are reported on the CIRCO validation set using CLIP ViT-G/14 as the VLM, ChatGPT-4o within the SQAF module, and ChatGPT-4.1 as the reranker. User Intent mAP@5 mAP@10 mAP@25 mAP@50 Reference Image with Modification Text Generated Target Captions 35.05 32. 35.62 33.47 37.37 35.78 38.26 36.67 Fig. 6. Effect of the fusion hyperparameters 洧띺 and 洧띻 on SQAF performance. 洧띺 weights the relative contributions of the reference image and the modification text, and 洧띻 determines the influence of the MLLM-generated description in the final query. Results are reported using CLIP ViT-G/14 on the CIRCO validation set. intent with the MLLM-generated captions from the SQAF module. To evaluate this, we use the prompt shown in Figure 5 to guide the MLLM-based reranker with these captions and report results for different intent representations in Table 8. As observed, using the reference image and modification text yields better results than relying solely on the MLLM-generated captions. This is likely because the original multimodal query offers precise, directly grounded Manuscript submitted to ACM 16 Wu et al. Fig. 7. Qualitative comparisons of different retrieval methods on sample queries from (a) CIRR, (b) FashionIQ, and (c) CIRCO using CLIP G/14. Images highlighted with green box denote the ground-truth target images. guidance, whereas the captions, while semantically rich, may introduce irrelevant or misleading details. In Section 4.4, we further provide qualitative analysis to better understand the effects. Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval 17 Fig. 8. Qualitative comparison of different forms of user intent for reranking on CIRCO validation samples using CLIP G/14. (a) Results obtained using the reference image together with the modification text, which provide more reliable guidance for reranking. (b) Results relying solely on the MLLM-generated caption from SQAF. While MLLM captions provide high-level semantics, they may introduce errors (e.g., misinterpreting the cat as beside the bowl), leading to semantically incorrect reranking. Images highlighted with green box denote the ground-truth target images. Effect of the fusion hyperparameters 洧띺 and 洧띻 on SQAF performance. The heatmap in Figure 6 shows how the final query representation, constructed using fusion weights 洧띺 and 洧띻, influences global retrieval performance. The parameter 洧띺 in Equation (1) controls the relative contributions of the reference image and the modification text, and 洧띻 governs the influence of the MLLM-generated description in the final composed query. When the MLLM-generated description is excluded (i.e., 洧띻 = 0), SQAF relies solely on the fusion of visual and textual embeddings from the VLM and produces inferior performance. This could be due to the fact that the VLM-fused representation lacks certain essential contextual descriptions. As 洧띻 increases, the performance initially improves, suggesting that incorporating MLLM-generated descriptions enhances the understanding of the intended target image. However, beyond certain point, further increasing 洧띻 leads to performance degradation. This indicates that the MLLMgenerated captions still need to be constrained by the VLM-based query, as over-reliance on them may overshadow the VLM-based query intent or introduce noise that hinders accurate retrieval. Interestingly, using the MLLM-generated description alone (i.e., 洧띻 = 1) outperforms relying solely on the VLM-based embeddings. This observation implies that the MLLM is capable of producing high-level, semantically accurate descriptions that effectively capture the users intent. Nonetheless, the best performance is achieved by combining MLLM-generated context with the VLM-based query, highlighting their complementary strengths. 4.4 Qualitative Analysis Visual comparison with other methods. As shown in Figure 7, given multimodal query, SQUARE retrieves the target image more effectively than other methods. CIReVL often retrieves images that are semantically related to the query but fail to match the intended target. Although SQUARE w/o EBR is able to retrieve the target, the target often appears at lower rank, especially in challenging scenarios with highly similar gallery images (e.g., CIRCO), where SQAF alone Manuscript submitted to ACM Wu et al. Fig. 9. Failure cases of SQUARE on samples image using CLIP G/14. (a) Spatial reasoning limitation: the query from CIRR asks for the dogs head to appear closer to the camera, but retrieved results mainly show frontal views rather than close-ups. (b) Compound attribute modification: the query from FashionIQ requests changing blue V-neck shirt into black nonV-neck shirt with higher neckline. Our method fails to retrieve the annotated target image but still ranks semantically relevant alternatives among the top candidates. Images highlighted with green box denote the ground-truth target images. struggles to capture fine-grained distinctions. In contrast, SQUARE demonstrates that incorporating the EBR module, which leverages the enhanced multimodal reasoning ability of the MLLM, consistently promotes the target image to higher rank, clearly highlighting the benefit of EBR for improved retrieval performance. Qualitative comparison of different forms of user untent for reranking. In Section 4.3, we quantitatively compared different contexts used for reranking and found that using the reference image together with the modification text outperforms relying solely on the MLLM-generated captions from SQAF. Here, we provide qualitative analysis. As shown in Figure 8, although MLLM-generated captions can offer fine-grained semantic cues to guide reranking, they may also introduce inaccurate or misleading descriptions. For instance, the generated caption misrepresents the relationship between the cat and the bowl, describing the cat as being beside the bowl rather than in it. This conflict with the modification text causes the retrieval model to favor images where the cat is next to the bowl, instead of correctly retrieving images where the cat is inside the bowl. Failure cases. Despite achieving promising results, our method still exhibits certain limitations in specific cases. As shown in Figure 9(a), the modification requests composition focused on the dogs head, with the target image depicting the dogs head positioned closer to the camera. Since most of the retrieved images show the dogs face simply facing the camera rather than in close-up, this failure suggests that the model may be limited in its spatial reasoning. We also conjecture that the modification text is somewhat ambiguous, which, combined with the models limitations, prevents it from fully grasping the requested change in relative distance and perspective. Figure 9(b) illustrates scenario in which the user modifies multiple attributes of fashion image simultaneously. For example, the query specifies the change of blue V-neck shirt into black nonV-neck shirt with higher neckline. Manuscript submitted to ACM SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval Because such compound modifications require capturing subtle interactions between color, style, and neckline, our method fails to retrieve the annotated target image. However, it still ranks other alternative images that satisfy the semantic request as the top candidates. Thus, even when the exact target is not retrieved, our method returns semantically relevant results in challenging scenarios. 5 Conclusion In this paper, we presented SQUARE, novel two-stage training-free framework for ZS-CIR. In the first stage, Semantic Query-Augmented Fusion (SQAF) enhances the composed query by integrating an MLLM-generated semantic caption with the VLM-based embedding query, thereby improving global retrieval quality. In the second stage, Efficient Batch Reranking (EBR) exploits the reasoning capability of MLLMs to jointly compare all candidate images in single forward pass, leading to more accurate and semantically coherent ranking. Extensive experiments demonstrate that SQUARE achieves state-of-the-art or near state-of-the-art performance across multiple benchmarks. Notably, our method yields substantial improvements when applied with smaller VLMs (e.g., CLIP B/32), in some cases even surpassing results that were previously achievable only with larger models (e.g., CLIP L/14). Our experiments also highlight the strong potential of recent MLLMs, such as ChatGPT-4.1, as highly effective rerankers, with even lightweight variants delivering competitive performance. We hope that SQUARE, as simple yet effective framework, not only advances research in training-free ZS-CIR but also inspires further exploration in this direction. References [1] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. 2023. Zero-Shot Composed Image Retrieval with Textual Inversion. In ICCV. 1533815347. [2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2024. Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features. ACM TOMM 20, 3 (2024), 62:162:24. [3] Min Cao, Shiping Li, Juntao Li, Liqiang Nie, and Min Zhang. 2022. Image-text Retrieval: Survey on Recent Research and Development. In IJCAI. 54105417. [4] Yanbei Chen, Shaogang Gong, and Loris Bazzani. 2020. Image Search With Text Feedback by Visiolinguistic Attention Learning. In CVPR. [5] Longye Du, Shuaiyu Deng, Ying Li, Jun Li, and Qi Tian. 2025. Survey on Composed Image Retrieval. ACM TOMM 21, 7 (2025), 202:1202:27. doi:10.1145/3723879 [6] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, and Sangdoo Yun. 2024. Language-only Efficient Training of Zero-shot Composed Image Retrieval. In CVPR. 1322513234. [7] Young Kyun Jang, Dat Huynh, Ashish Shah, Wen-Kai Chen, and Ser-Nam Lim. 2024. Spherical Linear Interpolation and Text-Anchoring for Zero-Shot Composed Image Retrieval. In ECCV. 239254. [8] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. 2024. Vision-by-Language for Training-Free Compositional Image Retrieval. ICLR (2024). [9] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. ACL (2024). [10] Seungmin Lee, Dongwan Kim, and Bohyung Han. 2021. CoSMo: Content-Style Modulation for Image Retrieval With Text Feedback. In CVPR. 802812. [11] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In ICML. 1288812900. [12] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2025. MM-EMBED: Universal Multimodal Retrieval with Multimodal LLMs. In ICLR. [13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll치r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In ECCV, David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.). Springer International Publishing, Cham, 740755. [14] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image Retrieval on Real-Life Images With Pre-Trained Visionand-Language Models. In ICCV. 21252134. [15] Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, and Enhong Chen. 2025. ImageScope: Unifying Language-Guided Image Retrieval via Large Multimodal Model Collective Reasoning. In Proceedings of The Web Conference 2025. Manuscript submitted to ACM 20 Wu et al. [16] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [17] Jeong-Woo Park and Seong-Whan Lee. 2025. MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval. In IEEE SMC. [18] Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. 2021. Learning To Predict Visual Attributes in the Wild. In CVPR. 1301813028. [19] Leigang Qu, Meng Liu, Jianlong Wu, Zan Gao, and Liqiang Nie. 2021. Dynamic Modality Interaction Modeling for Image-Text Retrieval. In SIGIR. 11041113. [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML. 87488763. [21] Jun Rao, Fei Wang, Liang Ding, Shuhan Qi, Yibing Zhan, Weifeng Liu, and Dacheng Tao. 2022. Where Does the Performance Improvement Come From? - Reproducibility Concern about Image-Text Retrieval. In SIGIR. 27272737. [22] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. 2023. Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval. In CVPR. 1930519314. [23] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS. 2527825294. [24] Haibo Su, Peng Wang, Lingqiao Liu, Hui Li, Zhen Li, and Yanning Zhang. 2021. Where to Look and How to Describe: Fashion Image Retrieval With an Attentional Heterogeneous Bilinear Network. IEEE TCSVT (2021), 32543265. [25] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. Corpus for Reasoning about Natural Language Grounded in Photographs. In NeurIPS. 64186428. [26] Shitong Sun, Fanghua Ye, and Shaogang Gong. 2024. Training-free Zero-shot Composed Image Retrieval with Local Concept Reranking. arXiv:2312.08924 [cs.CV] [27] Yucheng Suo, Fan Ma, Linchao Zhu, and Yi Yang. 2024. Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval. In CVPR. 2695126962. [28] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Yue Hu, and Qi Wu. 2024. Context-I2W: mapping images to context-dependent words for accurate zero-shot composed image retrieval. In AAAI. [29] Yuanmin Tang, Jue Zhang, Xiaoting Qin, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Wu. 2025. Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval. In CVPR. 1440014410. [30] Sagar Vaze, Nicolas Carion, and Ishan Misra. 2023. GeneCIS: Benchmark for General Conditional Image Similarity. In CVPR. IEEE Computer Society, Los Alamitos, CA, USA, 68626872. [31] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing Text and Image for Image Retrieval - an Empirical Odyssey. In CVPR. [32] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. 2021. Fashion IQ: New Dataset Towards Retrieving Images by Natural Language Feedback. In CVPR. 1130711317. [33] Ren-Di Wu, Yu-Yen Lin, and Huei-Fang Yang. 2024. Training-Free Zero-Shot Composed Image Retrieval via Weighted Modality Fusion and Similarity. In TAAI. 7790. [34] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023. Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. arXiv:2310.11441 [cs.CV] https://arxiv.org/abs/2310. [35] Zhenyu Yang, Shengsheng Qian, Dizhan Xue, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. 2024. Semantic Editing Increment Benefits Zero-Shot Composed Image Retrieval. In ACM MM (Melbourne VIC, Australia). Association for Computing Machinery, New York, NY, USA, 12451254. [36] Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, and Changsheng Xu. 2024. LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval. In SIGIR. 8090. Manuscript submitted to ACM"
        }
    ],
    "affiliations": [
        "National Sun Yat-sen University, Taiwan"
    ]
}