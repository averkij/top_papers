{
    "paper_title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
    "authors": [
        "Shijie Wang",
        "Samaneh Azadi",
        "Rohit Girdhar",
        "Saketh Rambhatla",
        "Chen Sun",
        "Xi Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in https://wang-sj16.github.io/motif/."
        },
        {
            "title": "Start",
            "content": "MotiF: Making Text Count in Image Animation with Motion Focal Loss Shijie Wang1 Samaneh Azadi2 Chen Sun1 1 Brown University Rohit Girdhar2 Xi Yin2 2 GenAI, Meta Saketh Rambhatla2 4 2 0 2 0 2 ] . [ 1 3 5 1 6 1 . 2 1 4 2 : r {shijie wang, chensun}@brown.edu, {azadis, rgirdhar, rssaketh, yinxi}@meta.com Figure 1. Motivation and results of MotiF. (a) Example video frames and the corresponding motion heatmaps calculated from optical flow. In this example, 97% of the pixels are static while only 3% has meaningful motion. (b) In standard TI2V training pipeline, the model may learn to over-rely on the conditional image to optimize the L2 loss. This issue has been identified in [53] and termed as conditional image leakage. We propose MotiF to guide the models learning to focus on regions with more motion via motion heatmap re-weighting. (c) Qualitative results comparing MotiF to the baseline on examples from our proposed TI2V Bench."
        },
        {
            "title": "Abstract",
            "content": "Text-Image-to-Video (TI2V) generation aims to generate video from an image following text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, simple yet effective approach that directs the models learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate motion heatmap and weight the loss according to the This work is done during Shijies internship at Meta. intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, dataset consists of 320 image-text pairs for robust evaluation. We present human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through comprehensive evaluation, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in the project website. 1 1. Introduction From creating short clips of moving MNIST digits [27] to producing minute-long high-definition video featuring stylish walking woman [5], Text-to-Video (T2V) generation [3, 18, 30, 37] has progressed remarkably over the past decade. Meanwhile, video generation with additional conditions [15, 22, 31, 4345] has also gained considerable attention. In this paper, we focus on Text-Image-to-Video (TI2V) generation, task first introduced in [19] aiming to generate video based on an initial image and text description [26, 46, 50, 53]. Our goal is to enhance text adherence in open-domain TI2V generation. Commonly known as Image-to-Video (I2V) generation [50, 53] or image animation [25, 46], this task often overlooks the text guidance. In many cases, text merely describes the image or video [13, 21], or is unnecessary [6, 23, 49], underscoring the need for focused improvement. The main challenge in TI2V modeling is that while the image offers strong spatial signal to learn the video content, the model must rely mainly on the text to infer motion. When given an image and multiple different prompts, most prior works generate videos with limited motion. Zhao et al. [53] identified this issue, termed as conditional image leakage, where the model overly relies on the image condition. To address this issue, some studies propose to reduce the strength of the image condition by adding noise or masking the image [26, 53]. Others seek to derive more motion priors to facilitate motion learning [10, 51]. These methods all emphasis on enhancing the input signals, hoping that the model can learn to leverage them implicitly. In contrast, we propose to improve the training objective to enable the model to focus on the regions with more motion explicitly, which is an orthogonal direction to prior works and can be easily combined with existing techniques. We hypothesize that the models difficulty in following motion-based instructions may stem from insufficient attention to motion patterns. As shown in Figure 1 (a), in video with static background, 97% of the pixels remain unchanged over time, while only 3% exhibit meaningful motion. This subtle motion pattern might be neglected by the model especially considering that all regions are treated equally in the L2 loss. Motivated by this, we introduce Motion Focal loss (MotiF), term inspired from the focal loss in dense object detection [35]. Specifically, we first use optical flow [42] to create motion heatmap that represents the motion intensities of the video. We then use the motion heatmap to assign loss weight for the video to focus on regions with more motion. Additionally, we analyze commonly used image conditioning mechanisms and observe that concatenating the conditioning image with the diffusion input works best for both image and text alignment. Beyond training, evaluating TI2V generation is also challenging due to the lack of suitable evaluation set and reliable metrics. Existing evaluation sets are limited, they are either too small [51] or the text is used as description of the starting image rather than specifying the intended motion for animation [13]. Therefore, we introduce new benchmark, TI2V Bench, that consists of 320 image-text pairs covering 22 different scenarios. Each scenario includes 3 to 5 images with similar contents in different styles, and 3 to 5 prompts to animate these images to produce different motion. We intentionally include challenging scenarios such as novel object introduction and fine-grained object references. We use the Emu model [11], publicly available through meta.ai, to generate the initial images. Previous studies have employed various automatic metrics to assess different quality aspects, such as video quality, motion intensity, and text alignment. We observed that these metrics may not always align with human perception. Following Movie Gen [30], we mainly rely on human evaluation with A-B testing to evaluate the performance. Our human evaluation protocol is motivated by the JUICE protocol [15], where annotators are asked to indicate an overall preference and justify their choices across several axes, including image alignment, text alignment, object motion, and overall quality. Through our comprehensive human evaluations, we show that MotiF enhances TI2V generation capabilities, notably improving text alignment and motion quality. In summary, we make the following contributions: We present Motion Focal Loss (MotiF) that encourages TI2V generation to concentrate on regions with larger motion, and is complementary to existing techniques. We present new benchmark, TI2V Bench, including synthetic images and texts that cover wide range of complex scenarios, as well as human evaluation protocol designed to assess TI2V generation performance. Through comprehensive comparisons with nine previous methods, we demonstrate the effectiveness of MotiF, achieving an average preference of 72%. 2. Related Work 2.1. Text-Image-to-Video Generation With the rapid advancements in Text-to-Image (T2I) and Text-to-Video (T2V) generation, TI2V generation has also gained significant attention. Although training-free methods [28, 49] exist that utilize pretrained T2I or T2V models with inference optimization to enable zero-shot TI2V generation, we focus on approaches that require training, especially on the most widely adapted latent diffusion models [1, 37, 54]. The techniques in prior TI2V generation methods can be mainly grouped into two categories: 1) design novel architectures to integrate the image condition, 2) derive motion priors to improve motion learning. Our approach falls into the second category. Image Condition Integration. Most recent works use ei2 ther the U-Net architecture [17, 18] or the Diffusion Transformer (DiT) [29] for video generation, where the image condition can be incorporated similarly. Most works integrate the image by concatenating it with the noisy video [3, 10, 15, 25], which serves as spatial-aligned content guidance. Additionally, DynamiCrafter [46], ConsistI2V [33] and LivePhoto [10] also use cross-attention layers on the image embeddings to further strengthen the image condition and thus improve consistency in video generation. On the contrary, Zhao et al. [53] applied higher noise levels at later time steps to weaken the image condition to tackle the condition image leakage issue. Similarly, FollowYour-Click [26] introduced first-frame masking strategy to degrade the image condition. In our work, instead of disrupting the image condition to shift the models focus on the text, our training objective explicitly encourages the model to focus on video regions with more motion, and thus be more faithful to the text description. Motion Priors. Our world exhibits rich diversity of unique motion patterns. Modeling motion from real-world videos for generation is challenging due to the inherent ambiguity that arises when single description can correspond to multiple possible motions. Many methods explore motion priors to improve the motion learning. common approach is to calculate motion score [10, 12, 16, 25, 26, 31, 51] from the video and embed it into the model, helping it to more easily learn the motion level. It also allows for controlling motion intensity during inference. Previous works primarily differ in how they calculate the motion score, yet they generally follow similar concept based on frame similarities. Additionally, LivePhoto [10] introduced framework to re-weight text embeddings to place greater emphasis on the motion-related words. Cinemo [25] introduced novel strategy to learn motion residuals, calculated by subtracting the first frame from each subsequent frame. MoCA[48] introduces optical flow as an additional conditioning for image animation. Similar to our work, FollowYour-Click [26] also used optical flow to calculate mask, but the mask is embedded into the model and thus it requires additional input from the user during inference time. In our case, the motion prior is leveraged at the training objective level, which is explicit, simple and does not require any additional input during inference. 2.2. Video Generation Evaluation Benchmarks and Automatic Metrics. Make-a-Video is the first to report FVD/IS on the UCF-101 dataset [41] and FID/CLIP similarities on the MSR-VTT dataset [47] under the zero-shot setting, which is followed by later works in both T2V [1, 4, 14, 54] and TI2V generation [25, 33, 46, 53]. Re-purposing existing video understanding datasets for TI2V evaluation is undesirable due to their unconstrained data collection procedures, which may not capture the nuances of TI2V tasks. As result, recent studies have begun to collect new evaluation sets [13, 33, 51] and explore additional automatic metrics for more comprehensive evaluation [21, 24, 52]. AIGCBench [13] is proposed for TI2V task. However, their text descriptions are used to generate the image but not to drive the motion generation, which neglected the core idea of text-guided image animation. AnimateBench [51] shares some similarities with our benchmark approach of using T2I models to generate images and applying different text prompts to animate the same image. However, the number of unique images and texts in AnimateBench are relatively small and the domains are also limited. Furthermore, the automatic metrics employed in their work have several drawbacks as shown in Table 4. Human Evaluation. Following Movie Gen [30], we rely on human annotators to evaluate the performance of TI2V generation methods. Prior works used different human evaluation protocols including A-B testing [53], A-testing [10] and ranking multiple methods [16, 26, 51]. While A-testing and ranking are more efficient, we believe A-B testing is less ambiguous and may yield more reliable results. Unlike previous works that evaluate multiple metrics, we ask the annotators to select the best video and justify their choices based on various aspects, following the JUICE design [15]. This will generate single metric for calibration while also enabling detailed examination of results through the various quality justifications. To the best of our knowledge, we have conducted the most comprehensive human evaluation by comparing to nine open-sourced TI2V models and shown the advantage of MotiF. 3. Approach Given an image xi and text prompt c, TI2V task aims to generate an L-frame video = {x0, x1, . . . , xL} that follows the text description. In this work, we specifically tackle common scenario when the image xi is the first frame (i.e. = 0). The generated video should maintain visual coherence with the starting image and produce motion that is driven by the text description. In this section, we present the details of our model architecture, the proposed motion focal loss, and the procedure of generating the motion heatmaps. 3.1. Preliminaries Video Diffusion Models. Diffusion models [17, 38, 40] are probabilistic generative models designed to create images and videos from random Gaussian noise. In the forward pass, diffusion process gradually transforms data sample x0 pdata(x) into Gaussian noise xT (0, I) over timesteps by sequentially adding noise according to q(xtxt1). The diffusion model learns the reverse denoising process, where it iteratively removes noise to reconstruct the data, predicting less noisy xt1 from xt. 3 Figure 2. High-level comparisons of MotiF vs. prior works. Previous TI2V generation methods mainly focused on deriving additional motion signals (motion score and/or motion mask) as inputs for the model to leverage implicitly. On the contrary, we focus on the learning objective and propose to weight the diffusion loss based on the motion intensity, that is derived from optical flow. Our method is simple, effective, and does not require additional inputs during inference. Moreover, MotiF is complementary to existing techniques. An input video with frames is represented as 4D tensor RLHW 3. This high-dimensional data introduces considerable computational demands, especially for high-resolution or long-duration videos. To address this challenge, latent video diffusion models (LVDMs) [1, 54] operating within the latent space are commonly used in video generation to reduce the computation cost. Specifically, frames are encoded into the latent space via = E(x), where : RLHW 3 RLH C denotes the encoder. The diffusion process zt = q(zt1, t) and denoising process zt1 = pθ(zt, c, t) are then conducted in the latent space, where represents the input conditions. Finally, the decoder decodes the generated video ˆx = D(ˆz0) from the denoised latent prediction ˆz0. Most prior works use the image-based Variational Auto-Encoder [34] to encode each frame independently so that = L. During training, the input of the denoising model is linear combination of the video latent and Gaussian noise based on the time step. The objective is to optimize the model to predict the noise (or other variants), typically with mean squared error: Motion Focal Loss. The diffusion loss treats all the latents equally across the spatial and temporal dimensions. However, the motion is not evenly distributed across the video. Its common that most of the video stays static while only small region has meaningful motion. As result, successive frames tend to closely resemble the initial frame, meaning that still video, i.e. duplicating the initial frame, can yield relatively low loss. To address this, we introduce the motion focal loss, Lmotif, to explicitly focus on high-motion regions during training. We begin first by generating motion heatmap RLHW for each video, where each entry m[l, h, w] [0, 1] represents the motion intensity at position [h, w] in the l-th frame of the input video x. The motion heatmap is down-sampled to RLH to align with video latents z. The motion focal loss is defined as: Lmotif = Et,xpdata,ϵN (0,I) (ϵ ϵθ (zt, c, t)) 2 2. (2) The model is trained with joint loss with λ scaling the motion focal loss relative to the diffusion loss: Ldiffusion = Et,xpdata,ϵN (0,I)ϵ ϵθ (zt, c, t) 2 2. (1) = Ldiffusion + λLmotif. (3) 3.2. Modeling As shown in Figure 2, different from prior works that all focused on incorporating more motion priors into the model, and thus may require additional inputs during inference, we focus on improving the training objective to explicitly focus on the motion learning during training. Motion Heatmaps. The use of motion heatmaps is to enhance the models ability to generate motion by concentrating on areas with significant activity. Although there may be several methods to achieve this, we first explore using optical flow to create these motion heatmaps. To generate the motion heatmap ml for the l-th frame xl, we first compute the optical flow intensity fl between xl and the subsequent 4 frame xl+1 using RAFT [42]. We then apply sigmoid-like function fl to normalize the intensity map to be within the range [0, 1], enhancing contrast in the motion distribution and yielding the final motion heatmap where higher value denotes large motion. Image Conditioning. We build our model based on pretrained T2V diffusion model, VideoCrafter2 [8], to speed up the training process. We inject the image condition to the T2V model for TI2V generation. Previous work such as DynamiCrafter [46] perform dual-stream image injection, i.e. feed the visual features using the cross-attention layers (referred as cx-attn) and concatenating the image latent with the noised video (referred as x-cat) to improve the consistency in video generation. We observe that using cx-attn for image conditioning presents two main drawbacks. Firstly, when cx-attn is used alone, it may generate videos with poor content and style alignment with the input image. Secondly, when combined with x-cat, it may compete with the text features in the cross-attention layers, reducing the models ability to follow text prompts effectively. Therefore, we inject the image condition by solely concatenating the image latent with the noised video latents, i.e., x-cat. Following [8], we use an FPS embedding module to embed the FPS signal similar to the timestep embedding, and CLIP text encoder for cross-attention text condition. 4. TI2V Bench 4.1. Evaluation Set While its relatively easy to collect diverse set of prompts for T2V evaluation, evaluating TI2V is not straightforward as it requires conditional image. As shown in Table 1, recent TI2V benchmarks can be classified into three categories: 1) video-text pairs; 2) image-text pairs with realistic images; 3) image-text pairs with synthetic images. Repurposing existing video understanding datasets [2, 41, 47] for TI2V evaluation is non ideal, as these datasets assume only one possible outcome from the initial frame, and the text descriptions typically do not emphasize the intended motion. Existing images are also limited in their diversity and potential motion changes. Thus, we resort to imagetext pairs with synthetic images to curate TI2V Bench. AIGCBench [13] and Animate Bench [51] are most relevant to our work. However, the texts in AIGCBench are used to generate the images but not meant to describe the motion in the videos, which is critical for TI2V task. Animate Bench employs motion-based text descriptions, but its evaluation set is relatively small and limited, where the images often feature single object or scene, and the text description lacks diversity. Our goal is to curate an evaluation set that has diverse set of images, and each image has diverse set of prompts for text-guided animation. Some examples are shown in Figure 3. To create this benchmark, we first design scenarios in which multiple actions can be performed from an initial state and enumerate range of possible actions. For example, in the scenario of an aerial view of car on the road, possible actions could include moving forward, moving backward, and changing its lane. We come up with 22 different scenarios with diverse objects and scenes, each with multiple options for different motion. We make sure to include challenging scenarios like images with multiple objects, and text descriptions that introduce new object expected to appear in the video. For each scenario, we aim to generate 3 to 5 images with varying appearance and styles to ensure dataset diversity. We design text prompts to generate multiple starting images and iteratively refine the prompts to improve the image quality. We use the publicly available meta.ai to generate all the images. Low quality examples or those unable to support all the predefined actions are filtered out. In the example above, images were excluded if the vehicles front direction was unclear or if lane lines were not distinctly visible. Our final evaluation set consists of 320 image-text pairs including 88 unique images and 133 unique prompts. To the best of our knowledge, this is the most comprehensive and challenging benchmark to evaluate TI2V generation. 4.2. Human Evaluation Most T2V and TI2V works conduct human evaluation to assess the overall quality and the text alignment of the generated videos using various metrics. The issue with evaluating separate, independent questions is that mixed results make it challenging to reach clear conclusion. Therefore, we aim to have single metric to evaluate the performance while allow more detailed examination of different quality aspects for further analysis, following the JUICE protocol [15]. Specifically, each A-B comparison consists of two questions. We intentionally do not have the equal option so annotators are forced to choose the best one and justify their choices. First, annotators are asked to indicate their overall preference between two videos in the context of textguided image animation. Second, they are required to justify their choice based on four different aspects: 1) object motion (not only camera movement); 2) alignment with the text prompt; 3) alignment with the starting image; 4) overall quality. Annotators can select any combinations of these aspects. The overall preference is used to indicate the models performance in TI2V generation, which we refer to as TI2V score. This evaluation protocol allows us to generate single metric to draw conclusions for A-B comparisons while enabling more detailed analysis. 5 Figure 3. Example image-text pairs in TI2V Bench. For each scenario (column), we first think of scene that could be potentially animated to generate different types of motion. We include challenging scenarios when there are multiple objects (yellow/blue/red balloon) in the initial image for fine-grained control or the text prompt describes new object (frisbee, bubbles) to enter the scene. Then we come up with different prompts and use the publicly available meta.ai tool to generate diverse sets of images. Images of low quality or those not in the appropriate initial state are removed. Name Type Text-Driven Media-Text Pairs Unique Media Unique Text Evaluation Metrics I2V-Bench [33] AIGCBench [13] AIGCBench [13] AIGCBench [13] Animate Bench [51] TI2V Bench (Ours) video-text video-text image-text image(synthetic)-text image(synthetic)-text image(synthetic)-text No No No No Yes Yes 2, 950 1, 000 925 2, 003 105 320 2, 950 1, 000 925 2, 003 35 88 2, 950 1, 000 925 - 16 133 auto auto auto auto auto human visual quality, visual consistency MSE, SSIM, CLIP, etc MSE, SSIM, CLIP, etc MSE, SSIM, CLIP, etc image alignment, text alignment TI2V score Table 1. Recent TI2V evaluation benchmarks. We believe the key for TI2V generation is that the text should describe the motion, termed as text-driven in the table. While Animate Bench is similar to our benchmark, its very specific for the personalization domain and the dataset size and diversity are limited in both the images and the text descriptions. Moreover, the proposed metrics are all image-level evaluations that do not account for motion. 5. Experiment 5.1. Implementation Details Model Architecture. We build our model based on pretrained T2V model, VideoCrafter2 [8] (@512 resolution). This model uses CLIP as the text embedding, Variational Auto-Encoder (VAE) as the video encoder and decoder, and spacetime-factorized U-Net as the denoising model. We concatenate the condition image with the noised video and expand the first embedding layer, similar to prior works. Training and Inference. We use an internal licensed dataset of 1M video-text pairs that is similar to [46] for training. Videos are center-cropped and sampled at 320 512 resolution of 16 frames with dynamic frame stride ranging from 1 to 6 following [46]. The model is optimized in the v-prediction mode [36] by combination of the diffusion loss and the motion focal loss with (λ = 1). The optical flow intensity is normalized using sigmoid-like function σ(x) = 1/(1 + e100(0.05x)), which generates continuous and polarized heatmap within the range of [0, 1]. The text is randomly dropped by 10% to enable classifier-free guidance. We use constant learning rate of 5 105, global batch size of 64, and linear noise schedule with 1000 diffusion steps. We train the model for 32K steps on 8 A100-80G GPUs. During inference, we use the DDIM sampler [39] with 50 steps and guidance scale of 7.5. Human Evaluation. We conduct human evaluations using Amazon Mechanical Turk. Annotators are instructed to make their selections only after watching both videos, with minimum submission time requirement of one minute to ensure thorough assessments. We random shuffle the order of the videos shown to annotators to avoid bias. For each comparison, we ask 5 annotators to evaluate the results and perform majority vote.We use the overall preference as the TI2V score. We also calculate score for each justification criterion as the percentage of times particular aspect is chosen for the model in the entire evaluation set without majority vote. Figure 4. Human evaluation results comparing MotiF to nine open-sourced models [7, 9, 12, 25, 28, 33, 46, 50, 53] on our proposed TI2V Bench. We achieved considerable improvements across the board with an average preference of 72%. Through examining the justification choices, we found that our model mostly excel at improving text alignment and object motion, which matches very well with our motivation. Note that all the inference of prior works are done at Brown University. Figure 5. Qualitative comparison to prior works on TI2V Bench. Sampled frames are ordered from left to right. 5.2. Comparison to Prior Works We compare to nine open-sourced TI2V generation methods. To the best of our knowledge, this is the most comprehensive TI2V human evaluation. We ensure fair comparisons by following each methods pre-processing and postprocessing pipeline and ensure the generated videos are natural after resizing to the same resolution (320 320). The results are shown in Figure 4. Our method wins by considerable margin to prior works. From the justification selections, the main reasons for our method to win are on the object motion and text alignment, which is exactly the motivation of MotiF to improve text-driven motion learning. Loss TI2V Score Image Text Alignment Alignment Object Motion Overall Quality w/o MotiF loss w/ Inv-MotiF loss 63.1/36.9 61.9/38.1 10.3/10.7 7.6/9.2 34.9/16.4 34.8/12.8 32.9/16.4 34.9/15. 18.2/20.8 14.3/17.8 Table 2. Ablation studies on different design choices. The numbers on the left is for MotiF and the right is for the baseline. Similarly to the comparisons to prior works, MotiF mostly excel in improving the text alignment and object motion. Image Condition TI2V Score Image Text Alignment Alignment Object Motion Overall Quality cx-attn + x-cat cx-attn 58.1/41.9 92.2/7.8 15.0/14.6 56.8/5. 31.5/21.7 33.8/3.8 34.0/21.6 41.3/4.5 18.8/19.3 28.2/5."
        },
        {
            "title": "Text",
            "content": "Alignment Alignment Baseline: static Cinemo [25] Cond-leak [53] DynamiCrafter [46] AnimateAnything [12] VideoCrafter [8] SEINE [9] I2VGen-XL [50] TI2V-Zero [28] ConsistI2V [33] MotiF (ours) 99.29 93.28 94.05 93.41 96.78 84.45 91.55 87.13 73.78 91.33 92.68 66.24 66.16 66.56 66.58 66.64 66.94 67.22 67.97 68.89 67.38 67.73 Table 3. Ablation study on the image conditioning methods. Compared to our choice (x-cat), cx-attn alone leads to much worse results and using both is also sub-optimal. Here we train the models without motion focal loss to simplify the setting. Table 4. Automatic metrics on Animate Bench [51]. simple static video baseline (repeating the first frame) can generate the best image alignment score and reasonable text alignment score (first row). MotiF achieved comparable results to prior works. 5.3. Ablation Studies Motion Focal Loss. To verify the effectiveness of MotiF, we compare to baseline model trained without Lmotif while keeping all other training settings constant. As shown in the first row of Table 2, MotiF results in considerable gain compared to the baseline. We also experiment with Invmotion loss, where the inverse of the motion heatmap is applied when calculating Lmotif, causing the model to focus on modeling the static regions instead. As shown in the second row of Table 2, MotiF achieves much better scores, further validating the effectiveness of explicitly modeling high-motion areas for TI2V generation. To understand how MotiF helps to improve motion generation, we calculate the diffusion loss on hold out validation set of 2000 samples for both the baseline and MotiF. For each sample, we apply threshold on the motion heatmap to get binary motion mask. We then calculate the loss ratio of the high motion region, which is defined as the average diffusion loss of the mask region divided by the average loss overall. As shown in Figure 6, MotiF achieves smaller loss ratio compared to the baseline across different diffusion time-steps, again, validating the effectiveness of MotiF in improving motion generation. Image Conditioning. As discussed in Section 3, one key design space for TI2V generation is on how to integrate the image condition. We use concatenation (x-cat) and ablate the other two choices: 1) cross-attention (cx-attn); 2) x-cat and cx-attn. As shown in Table 3, using cx-attn alone results in much worse performance in all metrics. This is understandable as the spatial information is missing. However, using cx-attn together with x-cat also leads to obvious degradation especially on text alignment and object motion, we hypothesis that this is due to the competition of text and image embedding in the cross-attention layers. Figure 6. Loss comparison. We calculate the ratio of the average loss in the high motion region to the average overall loss on holdout validation set with different timesteps. MotiF can effectively reduce the relative loss of the high motion regions. Figure 5 shows the qualitative results where MotiF can generate higher quality videos that align better with the text prompts, even on challenging cases with multiple objects in the scene with fine-grained motion animation. I2VGen-XL and VideoCrafter generate videos that do not align well with the condition image because they use only cross-attention for the image condition embedding, which does not have the spatial alignment as in concatenation. In addition to human evaluation, we also report the automatic evaluation on Animate Bench [51] that is most relevant to TI2V Bench. As shown in Table 4, MotiF achieved comparable performance to prior works. Its worth noting that static video baseline (by repeating the condition image) gets the best Image Alignment score and reasonable Text Alignment score, suggesting that these metrics are non ideal for evaluating TI2V generation. This has motivated us to mainly rely on human evaluation. 8 6. Conclusions In this paper, we focus on the often overlooked problem of text alignment in text-guided image animation. We hypothesis that the reason prior works struggle to follow the text prompts is because the model may not pay attention to the motion patterns during training. Thus, we present MotiF to guide the models learning on regions with more motion. In addition, we curate challenging benchmark to evaluate TI2V generation. Although MotiF have shown considerable advantages over prior works, it is still limited in generating high quality videos with coherence motion in challenging scenarios when there are multiple objects or new object is expected to enter the scene. We hope this work will attract more attention in solving this challenging problem."
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 2, 3, 4 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 5 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2 [6] Dengsheng Chen, Xiaoming Wei, and Xiaolin Wei. Animating general image with large visual motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71317140, 2024. 2 [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 5, 6, 8 [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 7, 8 [10] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real In Euimage animation with text-guided motion control. ropean Conference on Computer Vision, pages 475491. Springer, 2025. 2, 3 [11] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 2 [12] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Animateanything: Finegrained open domain image animation with motion guidance. arXiv e-prints, pages arXiv2311, 2023. 3, 7, 8 [13] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-to-video content generated by ai. arXiv preprint arXiv:2401.01651, 2024. 2, 3, 5, 6 [14] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: In Proceedings noise prior for video diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 2293022941, 2023. 3 [15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. ECCV, 2024. 2, 3, [16] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. I2v-adapter: general image-to-video In ACM SIGGRAPH 2024 adapter for diffusion models. Conference Papers, pages 112, 2024. 3 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2, 3 [19] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: Controllable image-to-video generation with text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18219 18228, 2022. 2 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 12 [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 3, 12 [22] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: reference-guided latent diffusion approach for high definition text-to-video generation. arXiv preprint arXiv:2309.00398, 2023. 2 [23] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2025. 2 [24] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. 3 [25] Xin Ma, Yaohui Wang, Gengyu Jia, Xinyuan Chen, YuanFang Li, Cunjian Chen, and Yu Qiao. Cinemo: Consistent and controllable image animation with motion diffusion models. arXiv preprint arXiv:2407.15642, 2024. 2, 3, 7, [26] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, HeungYeung Shum, Wei Liu, et al. Follow-your-click: Opendomain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024. 2, 3 [27] Gaurav Mittal, Tanya Marwah, and Vineeth Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In Proceedings of the 25th ACM international conference on Multimedia, pages 1096 1104, 2017. 2 [28] Haomiao Ni, Bernhard Egger, Suhas Lohit, Anoop Cherian, Ye Wang, Toshiaki Koike-Akino, Sharon Huang, and Ti2v-zero: Zero-shot image conditionTim Marks. In Proceedings of ing for text-to-video diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90159025, 2024. 2, 7, 8 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3 [30] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 3 [31] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66356645, 2024. 2, [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 12 [33] Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. 3, 6, 7, 8 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 [35] T-YLPG Ross and GKHP Dollar. Focal loss for dense obIn proceedings of the IEEE conference on ject detection. computer vision and pattern recognition, pages 29802988, 2017. 2 [36] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [37] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 6 [40] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [41] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 3, 5 [42] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 2, [43] Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu Zhang, Qi Dai, Zhiyuan Zhao, Chunyu Wang, et al. Microcinema: divide-andconquer approach for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84148424, 2024. 2 [44] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos 10 In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. [45] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 2 [46] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 2, 3, 5, 6, 7, [47] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 3, 5 [48] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023. 3 [49] Shoubin Yu, Jacob Zhiyuan Fang, Skyler Zheng, Gunnar Sigurdsson, Vicente Ordonez, Robinson Piramuthu, and Mohit Bansal. Zero-shot controllable image-to-video animation via motion decomposition. 2024. 2 [50] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 2, 7, 8 [51] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77477756, 2024. 2, 3, 5, 6, 8 [52] Zhichao Zhang, Xinyue Li, Wei Sun, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Zhongpeng Ji, et al. Benchmarking aigc video quality asarXiv preprint sessment: dataset and unified model. arXiv:2407.21408, 2024. 3 [53] Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, and Jun Zhu. Identifying and solving conditional image leakage in image-to-video diffusion model. arXiv preprint arXiv:2406.15735, 2024. 1, 2, 3, 7, [54] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 2, 3,"
        },
        {
            "title": "Appendix",
            "content": "This appendix includes the following sections: Additional Ablation Study (Sec. A): Additional Qualitative Results (Sec. C) Additional Implementation Details (Sec. D) A. Additional Ablation Study A.1. Motion Heatmap Beyond using optical flow to generate the motion heatmap, we explore alternative approaches leveraging the Segment Anything Model 2 (SAM 2) [32], state-of-the-art video segmentation model. SAM 2 produces object masks by accepting points or bounding boxes as prompts, generating an initial mask for the first frame, and propagating it consistently across subsequent frames. To target objects with significant motion, we compute optical flow intensity in the first frame and sample high-intensity points as prompts for SAM 2. To ensure spatial diversity, the frame is divided into an grid, and the top-K grids with the highest average intensity are selected. From each grid, the point with the highest intensity is chosen as the prompt. SAM 2 then generates masks for the corresponding objects across frames, producing binary motion heatmap. In our experiments, we set = 5 and use 5 5 grid. As shown in the last row of Table A1, the continuous heatmap generated via optical flow achieves better results than the binary SAM-based heatmap especially on text alignment and object motion while SAM-based heatmap improves the overall quality. We observe that although SAM 2 can generate high-quality instance masks across frames with appropriate prompting, it is less effective than optical flow at highlighting large motion regions. We believe that improving prompt sampling strategies could further enhance the performance of SAM 2-based heatmap. TI2V Score Image Text Object Alignment Alignment Motion Overall Quality 58.8/41. 11.4/12.1 34.0/21.6 31.2/22.4 15.0/20.6 Table A1. Ablation study comparing SAM heatmap to optical flow heatmap (MotiF). Numbers on the left are for MotiF and right for the SAM heatmap. A.2. Motion Focal Loss Weight We investigate the impact of the motion focal loss weight λ on TI2V Bench. As shown in Table A2, simply setting λ = 1 achieves the best overall results. Generally, reducing λ can improve the overall visual quality of the generated videos but results in lower TI2V Score specially on worse text alignment and object motion. λ 0.5 2 5 TI2V Score Image Text Object Alignment Alignment Motion 66.6/33.4 63.8/36.3 65.6/34.4 10.0/10.8 12.4/7.6 15.2/12.8 37.3/13.7 33.0/20.4 39.7/15.9 39.8/15.3 31.7/21.1 36.8/16. Overall Quality 19.4/19.7 27.5/16.1 23.1/17.8 Table A2. Ablation studies on the motion focal loss weight λ. The numbers on the left is for MotiF and the right is for the comparing setting. B. Additional Quantitative Evaluation In the paper, we mainly rely on human evaluation for comparisons. Here, we provide additional automatic evaluation results for baseline models on VBench-I2V [20]. B.1. Evaluation on VBench-I2V VBench-I2V [21] is another popular image-to-video (I2V) benchmark, consisting of 356 real-world images and 1,118 image-prompt pairs. Different from TI2V Bench, VBenchI2V uses image captions as text conditions instead of action instructions and especially focus on controlling camera motion through text prompts. We evaluate MotiF alongside two strongest baseline models, DynamiCrafter and Cinemo, as well as the static video baseline, on VBench-I2V. Results are presented in Table A3. MotiF achieves comparable performance to DynamiCrafter and Cinemo in consistency, temporal flickering, motion smoothness, and video quality, while the static video baseline significantly outperforms all models in most metrics except dynamic degree and camera motion. This underscores key limitation of automatic evaluation: the tradeoff between video dynamics and overall quality makes it challenging to provide holistic assessment of model performance. Additionally, we observe that existing metrics for video dynamics, often based on optical flow, tend to favor videos with significant camera or background motion over object motion. This highlights the importance of conducting human evaluations for the TI2V task to address these shortcomings in automatic evaluation. C. Additional Visualization C.1. Comparison with Baseline Models Figure A1 shows more qualitative results comparing to prior methods. MotiF can generate videos that better align with the input text prompts, which validates the effectiveness of the proposed motion focal loss. We also include the video samples in the supplementary folder. C.2. Failing Cases Analysis Although MotiF shows clear advantages over prior work, it is still far from perfect for TI2V generation especially on our proposed challenging benchmark TI2V Bench. As 12 Method Subject Background Consistency Consistency Temporal Flickering Motion Image Smoothness Degree Quality Quality Dynamic Aesthetic I2V Camera I2V Subject Background Motion Baseline: static DynamiCrafter Cinemo MotiF (ours) 100.00 94.70 96.80 95.27 100.0 97.55 99.04 98.37 100.0 95.17 98.67 97.27 99.84 97.39 98.95 98.16 0 39.51 17.32 30.98 65.54 60.40 59.92 58. 71.61 68.16 64.37 66.95 98.77 96.89 97.43 96.89 97.24 96.68 98.14 97.00 14.29 30.88 15.83 24.35 Table A3. Results on VBench-I2V. Figure A1. More qualitative comparison to prior works on TI2V Bench. MotiF can generate videos that align better with the text prompts. More video samples are available in the project website. shown in Figure A2, we observe two main types of failure cases. First, sometimes the generated motion is not very natural. Second, the generated video may not follow the text prompt. In the second case, there are two challenging scenarios of TI2V Bench: 1) when the text prompt describes new object that needs to appear in the scene, the generated video may not be very coherent; 2) when there are multiple objects and the text prompt only refers to one of them, it will be hard for the model to generate precise motion. We hope MotiF and TI2V Bench will help the research community to tackle this challenging problem. MotiF is generic and complementary to existing techniques for TI2V generation. We believe that improving the motion heatmap accuracy can potentially boost the performance. Moreover, MotiF is potentially applicable to text-to-video generation. D. Additional Implementation Details D.1. Scenes in TI2V Bench TI2V Bench contains total of 22 diverse scenes, designed to cover wide range of scenarios. The detailed scenes include: car on the road, balance scale, (multiple) balloons, 13 Figure A2. Typical failure and challenging cases of MotiF on TI2V Bench. We observe two typical cases that the model fail: 1) the generated videos may have unnatural motion ((a)); 2) the generated videos do not align well with the prompts ((b), (c), (d)). For 2), there are two specific scenarios when following the text is challenging including novel object ((c)) or multiple objects ((d)). We also include more video samples in the project website. bird, (multiple) bulbs, butterfly, candle, child in playground, dog, rubber duck on pool, fish, flower, golf ball, horse, animal on meadow, human face, human body, sun, tide, traffic light, tree, and volcano. D.2. Human Evaluation We show the human evaluation interface in A3. During the evaluation process, the annotators are required to read the instructions first and then answer the two questions based on the specified criteria."
        }
    ],
    "affiliations": [
        "Brown University",
        "GenAI, Meta"
    ]
}