{
    "paper_title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
    "authors": [
        "Dayong Liu",
        "Chao Xu",
        "Weihong Chen",
        "Suyu Zhang",
        "Juncheng Wang",
        "Jiankang Deng",
        "Baigui Sun",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: \\href{https://cfg-bench.github.io/}{https://cfg-bench.github.io/}."
        },
        {
            "title": "Start",
            "content": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents Dayong Liu1,2* Chao Xu2 Weihong Chen2 Suyu Zhang2 Juncheng Wang3 Jiankang Deng4 Baigui Sun2 : Yang Liu2 1 Zhejiang University 2 Wolf 1069B, Sany Group 3 The Hong Kong Polytechnic University 4Imperial College London 5 2 0 2 4 ] . [ 2 5 8 6 8 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "(MLLMs) Multimodal Large Language Models show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide systematic framework for assessing models ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: https://cfg-bench.github.io/. 1. Introduction The remarkable capabilities of Multimodal Large Language Models (MLLMs) [2, 43, 46] in understanding and reasoning across vision and language have driven their active exploration as the decision-making engine for embodied agents [14, 19, 22, 28, 31, 32]. Operating within interactive environments, these agents are required to interpret multi- *Equal contribution. :Correspondence. Figure 1. Illustration of CFG-Benchs focus on embodied intelligence over descriptive accuracy. The top part shows how FAVORBench annotates and questions from third-person perspective, task which current MLLMs can often solve. In contrast, the bottom part demonstrates CFG-Benchs fine-grained annotation and first-person scenario questions, which probes for the actionable physical and intentional details necessary for embodied agents. Current MLLMs struggle to master the crucial fine-grained details required for physical interaction. modal observations to inform their planing and guide their subsequent actions. To systematically evaluate these capabilities, variety of benchmarks have emerged [6, 9, 40, 41], with significant focus on areas like egocentric perception [5], visual grounding [44], and spatial reasoning [4]. However, while existing benchmarks effectively assess an agents visual-spatial intelligence and strategic planning, they largely overlook the most challenging dimension of in1 Benchmarks Embodied Fine-Grained Action Data Scale QA Pairs Interaction Temporal-Causal Intention Evaluation Number Closed-Ended Open-Ended Counterfactuals MVBench [21] VideoMME [10] EgoSchema [24] EgoTaskQA [18] Motion-Bench [15] FAVOR-Bench [37] VSI-Bench [40] ECBench [9] EMBODIEDEVAL [6] CFG-Bench - - - - - - - - - - - - 4,000 2,525 5,031 2,315 5,385 1,776 288 386 328 1,368 4,000 2,525 5,031 40,322 8,052 8,184 5,000 4,324 1,533 19,562 Table 1. Comparison with other benchmarks. CFG-Bench introduces four-tiered cognitive framework for embodied fine-grained intelligence, distinguishing it from existing action benchmarks focused on third-person coarse description and other embodied benchmarks that prioritize spatial reasoning and high-level planing over the fine-grained physical action. It also provides more comprehensive evaluation protocol by uniquely integrating three-modalities QAs. indicates partial coverage. teraction: fine-grained action intelligence, which refers to comprehending the details of how an action is executed, both physically and cognitively. Probing these fine-grained details is crucial for pushing embodied imitation learning [8, 38] beyond rigid replication, as it tests an agents ability to perform executable physical actions as well as high-level planning and reasoning required to generalize successfully to similar scenarios. Concurrently, benchmarks within the action understanding domain have begun to recognize the importance of finegrained detail. Pioneering work like FAVOR-Bench [37], has moved beyond event-level labels to probe motion-level perception and the precise temporal sequence of actions. However, from an embodied perspective, their approach is limited in two fundamental ways. (1) The definition of finegrained is often not granular enough for physical execution. As shown in Fig. 1, the current annotation simply states cut the watermelon with knife in right hand, yet fails to capture granular execution steps, e.g., stabilize the watermelon with the left hand, grip the knife handle with the right hand and then cut down vertically with the blade down. (2) It is oriented towards objective video description from third-person perspective, does not encompass the deep cognitive grounding, such as gently lift and drop knife twice is to fully detach the slice by relying on its own weight. To systematically address these challenges, we first propose cognitive taxonomy, inspired by the rules of Goals, Operators, Methods, and Selectors (GMOS) in human computer interaction [3], that deconstructs fine-grained action intelligence into four hierarchical tiers: (1) Physical Interaction, detailing how an action is physically executed; (2) Temporal-Causal Relation, reasoning how actions connect through time and causality; (3) Intentional Understanding, inferring why an action is performed; and (4) Evaluative Judgment, evaluating how well an action was done and how to improve it if necessary. Building upon this four-tiered framework, we introduce CFG-Bench, cognitively benchmarking fine-grained action for embodied agents. Specifically, CFG-Bench includes curated dataset of 1,368 videos, covering egoand exo-centric daily-life records, hand-object interactions, and more complex outdoor activities. We employ close-ended multiple-choice question-answering (QA) for more objective Physical Interaction and Temporal-Causal tiers, while adopting an open-ended format for the higher-order Intentional Understanding and Evaluative Judgment tiers, which is crucial to prevent models from reasoning backward from the textual options, forcing them to ground their inference in the visual evidence. Notably, we integrate open-ended counterfactual questions across all four tiers to directly combat model hallucination and probe deeper action comprehending. All QA pairs undergo rigorous human-in-theloop verification process to eliminate overly simple or erroneous questions. Overall, CFG-Bench comprises 19,562 QA pairs distributed across 11 distinct tasks within the 4 tiers. Furthermore, for open-ended tasks, we use the standard GPT-assisted evaluation [34, 45], but apply gating mechanism for parts of counterfactuals: response is scored only if the model first identifies the questions false premise. Finally, we conduct comprehensive zero-shot evaluation of leading proprietary and open-source MLLMs on CFG-Bench. Furthermore, by fine-tuning Qwen2.5-VL [2] on our data, we demonstrate how model that has learned to articulate fine-grained actions with physical and intentional details shows substantial performance gains on established benchmarks for embodied manipulation and planning [41]. Our contribution can be concluded from three aspects: We define fine-grained action intelligence and propose novel four-tiered framework to systematically deconstruct and evaluate it beyond mere objective description. We construct CFG-Bench, new benchmark featuring hybrid QA design and innovative open-ended counterfactual challenges to rigorously test the physical and cognitive grounding of MLLMs on fine-grained actions. We provide comprehensive analysis revealing the limitations of current MLLMs and validate that training 2 on CFG-Bench significantly improves their fine-grained guidance capabilities for embodied physical execution. 2. Related Works 2.1. Benchmarks for Embodied Agent As MLLMs are increasingly explored as the brains for robots [17, 26, 35], several benchmarks have emerged to evaluate their ability to perceive and reason about the physical world. For instance, VSI-Bench [40] is designed to assess the visual-spatial intelligence of MLLMs, probing their understanding of object positions, orientations, and geometric relations. Similarly, ECBench [9] is dedicated to evaluating embodied cognitive abilities within both static and dynamic environments. These benchmarks are key to an agents spatial perception and mapping baseline. second category, exemplified by influential works like ALFRED [29] and BEHAVIOR [33], evaluates agents in interactive, goal-oriented scenarios. The recent EMBODIEDEVAL [6] and EMBODIEDBENCH [41] expands upon this paradigm by introducing more comprehensive suite of diverse tasks and scenes. They abstract away the complexity of physical interaction. The former focuses on the strategic what to do next by reducing actions to high-level, symbolic commands (e.g., pick up). The latters manipulation tasks require the MLLM to act as direct, end-toend policy that outputs low-level commands, such as 7DoF vector. We argue that our approach, which focuses on generating cognitive knowledge about how and why an action should be performed, is deeply complementary to these works. The experiments prove that an MLLM fine-tuned on our data achieves significant performance gains on the highlevel planning and low-level control tasks in Sec. 4.4. 2.2. Benchmarks for Action Understanding The evolution of action understanding benchmarks reveals progression towards greater detail, yet critical gap persists between understanding for descriptive analysis versus for embodied execution. Early comprehensive benchmarks like MVBench [21] and Video-MME [10] include subsets for action, but these are situated within broader evaluation of general video understanding. more dedicated line of work, exemplified by ActivityNet-QA [42], concentrates on event-level granularity, requiring models to identify the main activity or sequence of major events, but still lacks consideration for more fine-grained actions. To address this, pioneering works like EgoTaskQA [18], MotionBench [15] and FAVOR-Bench [37] move beyond event labels and attend to motion-level perception and precise temporal sequences. However, their definition of finegrained is still primarily oriented towards achieving the fidelity of third-person observer, targeting high-accuracy video description. They do not systematically evaluate the deep procedural knowledge required for an embodied agent to physically replicate an action, like the specific contact details and dynamics, and the causal, intentional, and evaluative reasoning that underpins skillful interaction. This deeper layer of fine-grained action intelligence, which separates merely observing an action from truly understanding how to perform it, remains the unaddressed frontier. 3. CFG-Bench An embodied agents ability to translate multimodal inputs into successful physical interactions hinges on its finegrained action intelligence. Encompassing the physical how and cognitive why of an action, this capability serves as the essential bridge between abstract planning and lowlevel physical control. To this end, we develop CFG-Bench to systematically measure this intelligence. First, we deconstruct the fine-grained action into 4 embodied cognitive tiers (Sec. 3.1). We then detail the dataset construction process, including video curation and QA generation (Sec. 3.2). Finally, we describe the evaluation protocols for QAs (Sec. 3.3). Fig. 2 is an overview of CFG-Bench tasks. 3.1. Taxonomy and Task Design Physical Interaction is the ability to comprehend the physical mechanics of an action can move beyond simple event recognition to detailed, factual understanding of how that action is physically performed. This granular perception is the foundation for an agents ability to imitate, interact with, and learn from the physical world. Concretely, Factual Action Understanding (FAU) is designed to measure the models ability to extract the actionable and fine-grained details of physical interaction from visual inputs. Through multi-choice questions (MCQ), this task cover full spectrum of the specific agent performing the action (e.g., left hand or right hand), the objects and tools being manipulated (e.g., cup or bottle), the specific parts of an object being interacted with (e.g., handle or rim), the precise operation type (e.g., gripping or holding), and the dynamic properties of the motion (e.g., forward quickly or backward slowly). We further introduce Counterfactual Interaction (CIA) to test the models robustness and its ability to ground responses firmly in visual evidence through open-ended QA. This task confronts the model with question containing false premise. successful response requires the model to first identify and explicitly reject this premise as inconsistent with the video, and then provide correction based on what actually occurred. This method directly penalizes the common failure mode of acquiescence, where models accept and hallucinate based on incorrect information. Temporal-Causal Relation elevates from isolated actions to the logical structure that connects them, which indicates the models ability to reason about how actions relate to Figure 2. Task demonstration of CFG-Bench. Note: all QA pairs, including those above, are slightly simplified for clarity and brevity. each other in time and through cause-and-effect. This capability is fundamental factor for an agent to plan, anticipate outcomes, and comprehend long-horizontal tasks. This is specifically evaluated by Temporal Relation (TR) and Causal Relation (CR). For the former, TR is designed to ensure comprehensive assessment of the models understanding of event timelines. We probe this capability through questions targeting sequential ordering (e.g., What occurred after action X?), requiring holistic sequence verification (e.g., Which option correctly lists the event order?), and testing for concurrent action identification (e.g., What was the left hand doing while the right hand performed the task?). For the latter, CR focuses on the direct consequences of an action, asking the model to identify the immediate outcome resulting from specific event. This tests its ability to reason about the consequences of its own potential actions. In addition, similar to CIA, we introduce the Counterfactual Relationship (CRS) to ensure that the causal reasoning of the model is firmly grounded in the specific visual cues of the video. Intentional Understanding marks significant shift from the how of an action to the why, which representing the model is capable of inferring the underlying goals and motivations that drive physical behavior, serving as critical ability for an agent to formulate its sub-goals. To assess thorough reasoning and prevent models from simply selecting plausible answer from list, all tasks within this tier are exclusively formulated as open-ended questions. Figure 3. Data statistics of CFG-Bench. (a) Distribution and video length statistics of the five datasets. (b) The distribution of tasks across four tiers. AW means average words of questions. We begin with Functional Intention (FI), task that focuses on the immediate purpose behind specific action or its particular manner of execution. FI includes two question types, why performs single atomic action or why adopt this specific physical technique. In contrast to this granular focus, the Global Intention (GI) task requires the model analyze sequence of distinct and scattered actions to deduce the overall goal that connects. This challenge the model to understand how individual sub-tasks contribute to high-level plan. Finally, the Counterfactual Intention (CIT) task provides the most rigorous test of the models grasp of how intention drives behavior. It presents the model with hypothetical change to the agents overall goal and ask it to predict the resulting changes in actions. Evaluative Judgment shifts the focus from comprehension oriented tasks, offering clear examples of physical interaction (Tier 1), distinct temporal workflows and intentions (Tiers 2 & 3), and scenarios for evaluation (Tier 4). To complement this view, 298 third-person videos from CharadesEgo [30] provide an objective perspective ideal for analyzing causal-temporal relation and evaluating strategy (Tiers 2 & 4). Besides, 104 videos from Something-SomethingV2 [11] are included for their focus on fine-grained handobject interactions, key aspect of embodied manipulation, and 52 from FineAction [23] for its complex outdoor action diversity. Fig. 3 (a) shows the distribution of dataset categories and the statistics of video lengths. Manual Annotation. To generate high-quality ground truth for our benchmark, we conduct meticulous, month-long annotation process. As shown in Fig. 4, our team of ten is organized into two-stage pipeline that begin with draft annotations from GPT-4o [16]. In the first stage, eight expert annotators refine these initial drafts, after which two expert reviewers perform final verification. The entire process is guided by our four-tiered cognitive framework, with specific annotation instructions tailored to the unique characteristics of each source dataset. 3.2.2. QA Generation Close-Ended. With the curated videos and its detailed annotations in place, we proceed to generate the QA pairs. For FAU, TR, and CR in Tiers 1 and 2, we develop rigorous multi-stage pipeline to generate close-ended QA pairs. Specifically, as shown in Fig. 4, the process begins with an automated generation phase where we design distinct prompt templates for each sub-task. Using GPT-4o [16], we generate an initial set of QA pairs with two principles: maximize question diversity across the videos and craft challenging distractors that are plausible but unique. We obtain 19,372 multiple-choice QAs pairs in this stage. Then, an automated filtering stage designed to eliminate overly simple questions. Following FAVOR-Bench [37], we adopt the blind and single-frame filtering techniques, removing questions that could be answered with common sense alone or from single static image. To further refine the set, we provide Qwen3-Max [39] with the QAs and the corresponding video caption, which serves as proxy for the ground truth. The model is tasked with flagging any QA pairs it deemed ambiguous or impossible to choose from, thereby efficiently identifying potentially flawed questions for further targeted human review. After this process, approximately 35% of the initially QA pairs are discarded. The final stage is manual verification. This process is conducted by six trained annotators. To guarantee the video-relevant of each question and the correctness and uniqueness of its answer, the team perform multiple rounds of validation. All QA pairs are cross-checked to resolve ambiguous references and reduce subjectivity, please refer Figure 4. Pipeline of dataset generation. Both annotation and QA generation are human-AI collaborative workflow. Open-ended and closed-ended questions share the same pipeline at the early stage. to evaluation and is defined as the agents ability to make qualitative assessments of an actions execution and outcome. This is critical capability, as it is essential for both learning from observation by distinguishing effective from hazardous techniques, and for achieving resilience by identifying failures and re-planning accordingly. Since this reasoning requires justification, all tasks are open-ended. Specifically, the first task is Process Monitoring (PM), which requires the model analyze the ongoing action to determine if the process is proceeding as expected, or if it has encountered problem, such as being temporarily stuck, having failed, or being unexpectedly interrupted. Next, the Strategy Evaluation (SE) task requires the model to adopt the role of critic, judging the quality of an actions execution based on criteria such as rationality, professionalism, and efficiency, thereby applying normative model of success to specific visual scene. Finally, similar to the above preceding tiers, the Counterfactual Evaluation (CEU) task confronts the model with hypothetical change on specific physical interaction or event within the process and asks it to evaluate how that change would have impacted the quality of the action or its outcome. 3.2. Dataset Generation 3.2.1. Video Curation Video Source. Driven by the need to support evaluation across four cognitive tiers, we construct final corpus from five datasets, each selected for its specific contributions. We prioritize the first-person perspective, which is essential for embodied intelligence, selecting 329 videos from EgoTaskQA [18], 191 from Charades-Ego [30], and 394 from EgoExo4d [12]. These datasets are rich in daily, goal5 to Appendix for more details. This human-in-the-loop process results in 9,282 QAs ultimately. Open-Ended. For the inferential tasks within our benchmark, i.e., the counterfactuals in Tiers 1 and 2, and all questions in Tiers 3 and 4, we develop human-centric crafting process to generate high-quality open-ended QA pairs. As shown in Fig. 4, our methodology begins by first generating dedicated set of multiple-choice questions, obtaining 10,280 QAs. We then only maintain the question and its corresponding correct answer to serve as factuallygrounded seed for each open-ended query. This seed set is then manually refined by our ten annotators, who would either rephrase the question to be more exploratory and elaborate on the answer, or, if the initial seed is inadequate, craft completely new QA pair from scratch. This process ensures every question is rooted in verifiable evidence and paired with reference answer containing the rich detail. All final pairs are validated through expert cross-checking either. Statistics. CFG-Bench comprises 19,562 QA pairs, evenly split between 9,282 close-ended and 10,280 open-ended questions distributed across our 11 tasks. As detailed in Fig. 3 (b), the average words (AW) of counterfactual questions is significantly longer, as these queries require more context to elicit reasoned responses. The figure also highlights notable variance in the number of questions per task, since not every video naturally suitable for all 11 task types. 3.3. Evaluation We evaluate MLLM performance on CFG-Bench using hybrid methodology. Close-ended tasks are measured by standard accuracy. For open-ended tasks, we follow [37] and use GPT-assisted protocol to score responses on two dimensions: Correctness, which measures factual accuracy against the video, and Detailedness, which assesses descriptive richness, with the final score being their mean. Notably, strict gating mechanism applies to the counterfactual tasks in Tiers 1 and 2: response is only eligible for scoring if it first rejects the questions false premise. Failure to do so results in an automatic score of zero for both dimensions. 4. Experiments 4.1. Experimental Setup We benchmark suite of leading MLLMs on CFG-Bench using their official implementations or APIs. Our evaluation includes proprietary models (Gemini-2.5-Pro [7], GPT5 [27]) and range of open-source counterparts (VideoLLaMA3 [43], InternVL3 [46], Gemma-3 [36], Qwen2.5VL [2], Qwen3-VL [39]). To specifically assess performance from an embodied perspective, we also include two specialized foundation models, Robobrain2.0 [35] and Cosmos [1]. To ensure fair comparison, video frames for each model are sampled according to their officially recommended frame-rate-based strategies. 4.2. Overall Performance From Tab. 2, our comprehensive evaluation of MLLMs on CFG-Bench yields several key insights: Human-Level Performance Gap. significant performance gap exists between all MLLMs and human-level proficiency, disparity particularly evident in open-ended counterfactual tasks where models struggle with hypothetical reasoning. Even the top-rank model, Gemini-2.5-Pro, lags substantially behind the human baseline, underscoring the profound challenge of fine-grained action reasoning. Proprietary versus Open-Source Models. Proprietary models generally outperform their open-source counterparts. Gemini-2.5-Pro leads with score of 5.40 on openended tasks and 59.52% accuracy on close-ended ones, while other proprietary model like GPT-5 also performs well. Among open-source models, Qwen3-VL-30B-A3BInstruct stands out as strong performer, achieving results competitive with Gemini-2.5-Pro on several tasks. Scaling Laws of Model Size. Our results consistently demonstrate positive correlation between model size and performance across several model families. Among the Qwen2.5-VL variants, for instance, performance scales directly with the parameter count. It is likely due to the enhanced capabilities of larger models for fine-grained visual perception and complex reasoning. Impact of Model Iteration. Our results highlight that recent model iterations can yield substantial performance gains that surpass simple scaling laws. This is most evident when comparing the Qwen2.5 series with the newer Qwen3 generation. Despite having fewer parameters, Qwen3-VL30B-A3B-Instruct consistently outperforms the much larger Qwen2.5-VL-72B model across several tasks. Effect of Input Frame Rate (FPS). We find that the impact of input frame rate (FPS) on performance is limited. For example, increasing the FPS from 4 to 8 and 16 on Qwen2.5VL-7B yields only marginal gains. This suggests that the primary performance bottleneck is not perceptual input, but rather the models fundamental limitations in fine-grained physical understanding and higher-order reasoning. Benefit of Embodied Fine-Tuning. Finally, our results validate the hypothesis that models fine-tuned on specific embodied data excel at our tasks. Specifically, Robobrain2.07B, which is fine-tuned from Qwen2.5-VL-7B, demonstrates superior average performance on both the openended and close-ended tasks than its base model. 4.3. Performance Across Cognitive Tiers Brittle Visual Grounding about Complex Actions. In Physical Interaction, models show foundational but incomplete grounding capability. On close-ended FAU tasks, scores are stable but only around 50% accuracy, indicating 6 Input Avgc Avgo Interaction Causal-Temporal Intention Evaluation FAU 93.48 20 CIA 8.76 TR 95.97 20 CR 98.10 20 CRS 9.02 FI 8.83 GI 9.51 CIT 9.11 8.92 8.63 PM SE CEU Methods Human Random Proprietary MLLMs Gemini-2.5-Pro [7] GPT-5 [27] Open-source MLLMs Video-LLaMA3-2B [43] Video-LLaMA3-7B [43] InternVL3-2B [46] InternVL3-8B [46] InternVL3-78B [46] Gemma-3-4B [36] Gemma-3-12B [36] Gemma-3-27B [36] Qwen2.5-VL-3B [2] Qwen2.5-VL-7B [2] Qwen2.5-VL-7B [2] Qwen2.5-VL-7B [2] Qwen2.5-VL-72B [2] Qwen3-VL-4B-Instruct [39] Qwen3-VL-8B-Instruct [39] Qwen3-VL-30B-A3B-Instruct [39] Cosmos-Reason1-7B [1] RoboBrain2.0-7B [35] 95.85 4 fps 1 fps 59.52 55. 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 8 fps 16 fps 4 fps 4 fps 4 fps 4 fps 4 fps 4 fps 43.23 51.27 41.02 43.54 52.82 48.49 50.05 52.96 37.46 40.71 42.40 48.26 51.28 51.34 51.07 57.68 52.05 44.53 9.05 5.40 4.13 3.78 4.13 4.03 4.13 4.92 3.73 4.07 4.51 4.12 4.38 4.39 4.48 4.61 4.38 4.71 5.09 4.80 4.72 58.81 54. 4.58 2.61 54.81 46.53 64.95 65.09 4.55 3.22 5.18 4.98 6.15 4. 5.31 3.16 5.72 4.55 5.19 4.82 51.21 51.33 53.46 56.26 58.31 44.43 47.87 53.74 41.69 43.93 45.16 48.37 56.32 51.96 53.60 57.21 55.97 45.96 2.14 2.39 2.61 2.63 3.95 2.46 2.53 3.59 1.88 2.71 2.72 2.91 3.56 3.21 3.15 3.69 3.63 2.13 34.64 50.17 33.90 36.89 49.42 46.44 47.94 47.72 34.77 38.58 39.51 42.14 47.83 43.26 44.76 51.78 41.86 42. 43.83 52.31 35.70 37.48 50.74 54.59 54.33 57.42 35.91 39.63 42.52 54.26 49.68 58.79 54.86 64.04 58.33 45.44 2.80 3.29 3.13 3.15 3.68 2.86 2.95 3.15 3.27 3.32 3.33 3.48 3.41 3.13 3.39 4.22 5.19 3.51 4.77 5.03 4.05 5.03 5.41 4.61 5.20 5.31 5.39 5.50 5.67 5.69 5.49 5.08 5.49 5.39 5.51 5.76 4.82 4.96 4.73 4.67 5.04 2.44 2.51 3.41 4.28 4.53 4.55 4.54 4.51 4.85 5.25 5.15 5.43 5.21 3.24 3.44 3.12 3.13 4.80 3.24 4.36 4.74 3.55 3.85 3.79 3.93 4.34 3.77 4.12 4.93 3.48 4.88 4.19 4.68 4.62 4.56 4.96 4.22 4.60 4.82 4.50 4.57 4.64 4.78 4.81 4.87 5.31 5.21 4.72 5. 4.07 4.21 4.83 4.85 5.27 4.51 4.69 5.01 4.82 4.94 4.94 4.93 5.01 4.43 5.00 5.77 5.09 5.37 9.62 6.49 5.05 4.21 5.05 5.15 5.02 6.21 5.46 5.74 6.06 5.23 5.62 5.49 5.64 5.77 5.72 5.95 6.35 5.34 5.56 Table 2. Comprehensive results of leading MLLMs on CFG-Bench. It presents the performance of each task within every cognitive tier, along with the average score for open-ended questions Avgo and the average accuracy for close-ended questions Avgc. Random selection and human performance are also included for comparison. The highest and suboptimal results are bolded and underlined. Methods Interaction Causal-Temporal Intention Evaluation FI Gemini-2.5-Pro [7] 58.81/5.39 66.02/4.58 54.81/4.92 64.95/5.76 79.22/4.55 81.88/5.18 89.29/6.15 96.14/5.31 84.45/5.72 90.99/5.19 98.84/6.49 Qwen3-VL-8B-Instruct [39] 53.60/4.70 15.59/3.15 44.76/4.22 54.86/4.31 44.03/3.39 76.50/5.49 75.98/5.25 89.47/4.12 77.53/5.31 90.30/5.00 80.48/5.95 45.96/4.97 18.41/2.13 42.20/4.30 45.44/5.11 29.85/3.51 72.88/5.76 73.36/5.21 90.42/4.88 79.09/5.30 89.81/5.37 80.86/5.56 RoboBrain2.0-7B [35] CEU CRS FAU CIA CIT PM CR TR SE GI Table 3. The quantitative analysis of QA Forms. Left of / is close-ended performance and right is open-ended. that while models recognize static objects and simple motions, they fail to master full, fine-grained action sequences. This weakness is amplified in the open-ended CIA task, where models frequently fail the gating mechanism by acquiescing to and hallucinating from false premises, revealing brittle grounding in visual evidence. Local Causality over Global Temporality. In TemporalCausal Relation, models generally perform better on CR than on TR. We posit this is because CR tasks probe local, direct consequences that are well-learned patterns. In contrast, TR tasks demand more challenging global reasoning to track and compare multiple, often concurrent, events across videos context. This difficulty is further supported by the low performance on CRS primarily involved in complex temporal tracking. Local Intent Inference over Global Goal Synthesis. In Figure 5. The qualitative analysis of QA Forms. Intentional Understanding, models demonstrate clear gap between local and global reasoning. They succeed at inferring the immediate intent of single actions in the FI task. However, they consistently struggle to synthesize long-term goals from scattered actions in the GI task, revealing weakness in multi-step, abstract reasoning. This struggle culminates in the CIT task, where generating alternative plans for new goals remains profound challenge. Guided Evaluation over Unguided Critique. In Evaluative Judgment, models generally performed better on CEU than on PM and SE. This suggests that models are more ca7 Methods EB-ALFRED EB-Manipulation Avg Base Common Complex Visual Spatial Long Avg Base Common Complex Visual Spatial Qwen2.5-VL-7B [2] + SFT on CFG Data 4.7 9.7 10 16 8 16 6 8 2 0 4 2 6 8.3 9.6 15.3 12.5 8.3 16.7 8.3 14.6 5.6 7. 16.7 22.9 Table 4. The quantitative results of transfer evalaution. Figure 6. The results of caption generation before and after SFT. pable of guided, local evaluations (predicting the effect of given change) than they are of unguided, global critiques that require them to independently apply an internal model of skill or ideal task progression. These findings mirror key challenges in robotics manipulations to some extent. The failures in visual grounding and temporal reasoning (Tiers 1-2) align with challenges in dynamic task execution, while the struggle with goal synthesis (Tier 3) is bottleneck for long-horizon planning. Crucially, the lack of unguided critique (Tier 4) explains why robots require human intervention and lack true autonomy. 4.4. Analysis and Discussing Effects of QA Forms. As illustrated in Tab. 3, our analysis reveals that the choice of QA format is critical for evaluation. We observe that for tasks requiring significant reasoning (such as counterfactuals, intention, and evaluation), models often achieve high accuracy in the MCQ format, even with challenging distractors. Yet, they underperform substantially when the same queries are presented in an open-ended format. This suggests that the MCQ format allows models to leverage powerful textual reasoning to infer the most plausible option, often bypassing deep visual grounding. Conversely, for tasks grounded in objective facts, such as FAU, TR, and CR, we find the performance to be less sensitive to the QA format. The same conclusion could be inferred from visualizations in Fig. 5, which together determines our final designs: the reasoning-heavy tasks as open-ended to ensure we are truly probing the cognitive capabilities, while retaining the efficient MCQ format for the more objective, fact-based tasks. Transfer Evaluation. To assess the transferability and practical utility of the knowledge contained in CFG-Bench, we perform supervised fine-tuning (SFT) on the Qwen2.5VL-7B model using our data. We then evaluate this finetuned model on two distinct downstream tasks from EM8 Figure 7. Error Analysis. We show the recurring failure modes for each tier, along with the corresponding examples. BODIEDBENCH [41]: the high-level planning task EBALFRED and the low-level control task EB-Manipulation. As shown in Tab. 4, the SFT model presents significant performance gains on both tasks compared to its original baseline, which demonstrates crucial finding: the detailed cognitive knowledge of how and why an action is performed, as taught by CFG-Bench, is foundational capability that enhances performance across the entire embodied task hierarchy, from strategic planning down to precise motor control. Additionally, we qualitatively compare captions from the base and SFT models on unseen videos of human and robotic operations to highlight the improvements in Fig. 6. Error Analysis. We conduct systematic error analysis in Fig. 7. Firstly, the most common failure in Physical Interaction is the incomplete capture of fine-grained details. Models often identify single salient event but fail to describe the full subtle actions. Secondly, in Temporal-Causal Relation, models frequently fail to comprehend coordinated, simultaneous actions, i.e., while able to describe the action of single hand, they struggle to articulate what both hands are doing concurrently. Thirdly, for Intentional Understanding, models often fall back on overly simplistic common-sense heuristics, leading to incorrect conclusions. For instance, model might mistake an intermediate step, such as readjusting grip, for the completion of task merely because hand releases an object. Finally, in Evaluative Judgment, we identify positivity bias that the models tend to base their evaluation solely on the final outcome, praising any successful completion while ignoring issues in the process. 5. Conclusion In this work, we introduce CFG-Bench to address the underexplored domain of fine-grained action intelligence for embodied agents. Built upon four-tiered cognitive framework, our benchmark assesses models ability to translate visual observations into actionable physical and cognitive details through diverse suite of question-answer pairs. Our evaluation reveals MLLMs limitations in articulating fine-grained actions and in higher-order reasoning. We then prove these capabilities are foundational, as SFT on our data led to significant performance gains on downstream embodied manipulation and planning tasks."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 6, 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 1, 2, 6, 7, 8 [3] Stuart Card. The psychology of human-computer interaction. Crc Press, 2018. 2 [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 1 [5] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Egothink: Evaluating first-person perspective thinking capability of visionIn Proceedings of the IEEE/CVF Conlanguage models. ference on Computer Vision and Pattern Recognition, pages 1429114302, 2024. [6] Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, et al. Embodiedeval: Evaluate multimodal llms as embodied agents. arXiv preprint arXiv:2501.11858, 2025. 1, 2, 3 [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 7, 1, 2 [8] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022. 2 [9] Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, and Lidong Bing. Ecbench: Can multi-modal foundation models understand the egocentric world? holistic embodied cognition benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24593 24602, 2025. 1, 2, 3 [10] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 2, 3 [11] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [12] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 5 [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [14] Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, et al. Multimodal fusion and visionInformation language models: survey for robot vision. Fusion, page 103652, 2025. 1 [15] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84508460, 2025. 2, 3 [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5 [17] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. 3 [18] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human tasks in egocentric videos. Advances in Neural Information Processing Systems, 35: 33433360, 2022. 2, 3, [19] Shiyu Jin, Jinxuan Xu, Yutian Lei, and Liangjun Zhang. Reasoning grasping via multimodal large language model. arXiv preprint arXiv:2402.06798, 2024. 1 [20] Klaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2018. 2 [21] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference 9 on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, sociation for Computational Linguistics: ACL 2024, pages 1308813110, 2024. 2 [22] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. 1 [23] Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao. Fineaction: fine-grained video dataset for temporal action IEEE transactions on image processing, 31: localization. 69376950, 2022. 5 [24] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2 [25] Hans Moravec. Mind children: The future of robot and human intelligence. Harvard University Press, 1988. 1 [26] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36:2508125094, 2023. 3 [27] OpenAI. Gpt-5 system card. 2025. 6, 7 [28] Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, and Liqiang Nie. Large vlm-based visionlanguage-action models for robotic manipulation: survey. arXiv preprint arXiv:2508.13073, 2025. 1 [29] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. [30] Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In European conference on computer vision, pages 510526. Springer, 2016. 5 [31] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022. 1 [32] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. 1 [33] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martın-Martın, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on robot learning, pages 477490. PMLR, 2022. 3 [34] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, YuXiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In Findings of the As- [35] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 3, 6, 7 [36] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 6, 7 Gemma 3 technical report. [37] Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei Cheng, Gang Yu, and Tao Chen. Favor-bench: comprehensive benchmark for fine-grained video motion understanding. arXiv preprint arXiv:2503.14935, 2025. 2, 3, 5, 6 [38] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li FeiFei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023. 2 [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 5, 6, 7, 1, 2 [40] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 2, 3 [41] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. 1, 2, 3, 8 [42] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. [43] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 1, 6, 7 [44] Zhuofan Zhang, Ziyu Zhu, Pengxiang Li, Tianxu Wang, Tengyu Liu, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Siyuan 10 Huang, and Qing Li. Task-oriented sequential grounding in 3d scenes. 2024. 1 [45] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multitask long video understanding. arXiv e-prints, pages arXiv 2406, 2024. 2 [46] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 1, 6, 11 Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Appendix Outline In these supplementary materials, we provide: More experimental analysis (Appendix B), including ablation study on FPS, more analysis of performance across cognitive tiers, and details of human baseline and interannotator agreement. More details of CFG-Bench (Appendix C), including more data statistics, prompt for QA generation, prompt for GPT-assisted evaluation, and selected-out QA samples. More samples of CFG-Bench (Appendix D). We further attach videos in supplementary powerpoint for more details. Limitations and broader impacts (Appendix E). B. More Experimental Analysis B1. Ablation Study on FPS To further investigate the impact of input frame rate (FPS) on performance, we provide supplementary analysis for Gemini-2.5-Pro [7] and Qwen3-VL [39] in Tab. S1. While Qwen3-VLs default is 2 FPS and Gemini-2.5-Pros is 1 FPS, both models documentation recommends increasing the FPS for more detailed fine-grained analysis. We therefore evaluate Qwen3-VL-8B-Instruct at 4, 8, and 16 FPS, and Gemini-2.5-Pro at 4 and 8 FPS. The results align perfectly with the conclusions presented in our main paper. We observe that increasing the FPS yields slight, positive improvements in performance, but the gains are marginal. This finding strongly supports our hypothesis that the primary bottleneck for current MLLMs is not the quantity of perceptual input, but rather their core limitations in fine-grained physical understanding and higher-order reasoning. B2. Performance Across Cognitive Tiers As shown in Tab. 2, surprising trend emerges from our results: generally, models score lower on the seemingly easy perceptual tasks of Tiers 1 and 2 than on the more abstract reasoning of Tiers 3 and 4. This directly reflects Moravecs Paradox [25], which states that what is easy for humans (like physical interaction) is often extremely difficult for AI, while what is hard for humans (like abstract logic) is often easier. Our findings confirm this for MLLMs: they are better at mimicking complex reasoning about intent and quality (due to their vast textual knowledge) than they are at understanding the basic physical actions that humans perform effortlessly. B3. Human Baseline To establish performance upper bound for CFG-Bench, we conduct human baseline evaluation. This study is performed by three another expert annotators, who are not involved in creating or verifying during the primary annotation process. randomly selected and representative subset of 1,000 QA pairs (500 closed-ended and 500 open-ended) is used for this evaluation. The evaluation protocol is designed to mirror the models task as closely as possible. For the closed-ended questions, participants are presented with the video and the multiplechoice options and are asked to select the single best answer. For the open-ended questions, participants are shown the video and the question and are tasked with writing detailed answer in their own words. To ensure fair comparison with the MLLMs, scoring is conducted as follows. Performance on the closed-ended questions is measured by simple accuracy. The humangenerated open-ended answers are evaluated using the same GPT-assisted evaluation prompt and rubric that is used for the model-generated responses. Deepseek-R1 [13] is employed in GPT-assisted evaluation. As shown in Tab. S2, the human baseline achieves nearperfect performance. On the closed-ended tasks, the average accuracy is 95.85%. For the open-ended tasks, the average score is 9.05. We also report 95% confidence intervals for human performance. This result accounting for sampling uncertainty, serves as the practical upper bound for performance on CFG-Bench and highlights the significant, statistically robust gap that remains for current MLLMs. B4. Inter-Annotator Agreement To validate the quality and consistency of our humangenerated ground truth, we conduct formal inter-annotator agreement (IAA) study. We use the same 1,000 samples and three annotators as the human baseline evaluation. To measure the degree of consensus, we calculate Krippendorffs α, selected for its robustness in handling multiple raters while correcting for chance agreement. For the closed-ended questions, we treat the option indices as nominal data. For the open-ended questions, we treat the 010 scores for both Correctness and Detailedness as interval data. As shown in Tab. S2, our analysis yields Krippendorffs 1 Methods Input Avgc Avgo Proprietary MLLMs Gemini-2.5-Pro [7] Gemini-2.5-Pro [7] Gemini-2.5-Pro [7] 1 fps 4 fps 8 fps 56.27 59.52 59.88 Open-source MLLMs 50.02 Qwen3-VL-8B-Instruct [39] 51.07 Qwen3-VL-8B-Instruct [39] Qwen3-VL-8B-Instruct [39] 51.31 Qwen3-VL-8B-Instruct [39] 16 fps 51.90 2 fps 4 fps 8 fps 4.53 5.40 5. 4.10 4.71 4.75 4.82 Interaction Causal-Temporal Intention Evaluation FAU CIA TR CR CRS FI GI CIT PM SE CEU 56.43 3.57 50.59 61.79 3.28 4.47 5.24 4.17 4.76 4.90 58.81 4.58 54.81 64.95 4.55 5.18 6.15 5.31 5.72 5.19 58.95 4.49 54.94 65.74 4.61 6.24 6.43 5.71 6.04 5.41 52.51 2.45 43.06 54.49 3.27 4.78 4.57 3.87 4.18 4.88 53.60 3.15 44.76 54.86 3.39 5.49 5.25 4.12 5.31 5.00 53.80 3.07 45.34 54.80 3.53 5.45 5.28 4.16 5.44 5.08 54.27 3.38 46.48 54.96 3.52 5.48 5.36 4.27 5.32 5.10 5.87 6.49 6. 4.80 5.95 5.98 6.14 Table S1. The ablation study of FPS. Notably, the value of each task is the average of three repeated evaluations, and the same applies to the Tab. 2. Task Close-ended Open-ended Performance Krippendorffs α 95.851.2 9.050.09 0.95 0.82/0. Table S2. Human baseline of average performance and interannotator agreement across tasks. 0.82 is Correctness and 0.84 is Detailedness. α of 0.95 for close-ended task and 0.82/0.84 for open-ended task. According to established guidelines [20], value above 0.80 indicates high level of agreement and data reliability. This strong result confirms that our ground truth for CFG-Bench is consistent and of high quality, making it reliable resource for model evaluation. C. More Details of CFG-Bench C1. More Data Statistics We provide index distribution of correct answers for the close-ended QAs, i.e., (1) 20.01%, (2) 20.00%, (3) 20.00%, (4) 20.00%, (5) 20.00%. nearly uniform distribution across the option indices (1 through 5) is observed, indicating that there is no positional bias in our QA set. In Fig. S1, we provide statistics of words related to finegrained actions with the highest frequency from four aspects, i.e., object parts, manipulation types, dynamic qualities, action directions. The diversity of these terms demonstrate the deep granularity of our annotations, highlighting the benchmarks focus on the specific, procedural details of fine-grained physical execution that are essential for an embodied agent. C2. Prompt for Automatic QA Generation This subsection presents the prompt templates used for our initial QA generation, all of which are designed to produce multiple-choice questions (MCQ). The open-ended tasks are later derived from this output by selecting the correct answer and performing manual refinement (see Fig. 3). Each template is organized into three sections, i.e., Objectives, Design Rules, and Question Types, to ensure the generation of high-quality QA pairs that are diverse, challenging, and strictly aligned with the intended cognitive capability of each tier. Tabs. S3-S6 present the detailed prompt templates respectively. C3. Prompt for GPT-Assisted Evaluation Our GPT-assisted evaluation protocol for open-ended tasks is adapted from FAVOR-Bench [37], scoring responses across two independent dimensions: Correctness for factual accuracy and Detailedness for descriptive richness. For the specific counterfactual tasks in Tiers 1 and 2, we introduce an additional binary Gating Mechanisms (0 or 1), e.g., Correctness and Detailedness are set to 0 when acquiesce to flawed instructions. Tabs. S7-S9 present the complete prompt template. C4. Selected-Out QA Samples To illustrate the rigor of our filtering pipeline, we present representative examples of identified questions. These typically fall into three categories that would compromise fair evaluation, as shown in Fig. S2: (1) questions solvable by common-sense world knowledge alone, which fail to test for visual grounding; (2) questions with no correct answer among the options; and (3) questions with multiple correct answers, which violate the principle of single, unambiguous ground truth. The systematic identification and removal of such flawed questions are crucial for ensuring the quality and reliability of the final benchmark. 2 Figure S1. The statistics of words related to fine-grained actions with the highest frequency from four aspects, i.e., object parts, manipulation types, dynamic qualities, action directions. Figure S2. The visualizations of the selected-out samples for three reasons: easy QA without visual grounding, no correct answer, and multiple matching answers. 3 Prompt Template: Generating QA Pairs for Physical Interaction You are an expert at designing multiple-choice questions (MCQs) about the execution of fine-grained actions in videos. [Objective] Given the detailed caption annotation for the entire video, design {N QUESTIONS} MCQs that evaluate how the actions are executedcovering agent/effector, interaction object and tool, object part, manipulation type, dynamic qualities, action direction, object location, and repetition count. [Design Rules] 1. The question stem must address the main actor in the second person (you). 2. The options must describe the main actors behavior in the first person (I, my). 3. Each question has 5 options, with exactly 1 correct; do not prefix options with A/B/C. 4. The correct option is minimal, faithful restatement of the caption; never invent or modify execution details that are not supported by the caption. 5. The distractors must be logically reasonable but not consistent with the video content. 6. Do not mention caption, annotation, or text. Use phrases like in my view or see. 7. Different questions should cover different parts or phases of the video as much as possible, avoiding repetitive questioning. 8. Questions must require temporal understanding of the video: they should not be answerable from single static frame or from the question wording alone. [Question Types] 1. Factual Action Understanding (FAU) Ask about how specific action is executed in the video, focusing on one or more execution dimensions. The distractors must be physically plausible and context-appropriate for the video scene. 2. Counterfactual Interaction (CIA) Ask about how specific action that is incorrectly described in the question stem is actually executed in the video. The correct option must explicitly point out that the stems described method is incorrect according to the video, and provide the fine-detail task that accurately matches the caption. And two distractors should keep the same incorrect mid-level assumption in the stem and elaborate it into different fine-detail variants (still wrong), while the other two distractors should also explicitly flag the stems mistake but propose fine-detail corrections that do not match the caption (partially corrected but still incorrect overall). Table S3. Prompt template for Physical Interaction. D. More Samples of CFG-Bench For better demonstration, we show more samples of videos and their corresponding QAs, and correct answers for all tasks in Figs. S3-S6. The corresponding samples are vividly presented in the supplementary powerpoint. In addition, all QA pairs and ground-truth captions are included in the supplementary JSON files. E. Limitations and Broader Impacts E1. Limitations Our work has several limitations that provide avenues for future research. First, due to budgetary constraints, our evaluation of proprietary models is not exhaustive, and future work could include broader range of state-of-the-art commercial systems. Second, our video corpus, while curated for diversity, primarily focuses on daily tasks with It may not fully cover the subset of outdoor activities. highly specialized, expert-level actions required in domains. Finally, our open-ended evaluation relies on the models ability to articulate its understanding in natural language. However, model might possess the correct cognitive understanding but fail to express it adequately. Therefore, the evolution of CFG-Bench towards more practical and flexible QA format will be key direction for our future research. E2. Broader Impacts We believe CFG-Bench can have significant positive impact on the development of more capable and reliable embodied agents. By providing rigorous tool to measure and improve fine-grained action intelligence, our work can accelerate progress in areas like general-purpose household assistants. Furthermore, by emphasizing the why (Intention) and how well (Evaluation) of an action, our benchmark encourages the development of more interpretable and safer 4 Prompt Template: Generating QA Pairs for Temporal-Causal Relation You are an expert at designing multiple-choice questions (MCQs) about the temporal and causal relations of fine-grained actions in videos. [Objective] Given the detailed caption annotation for the entire video, design {N QUESTIONS} MCQs that evaluate the temporal and causal relationships between actions. [Design Rules] 1. The question stem must address the main actor in the second person (you). 2. The options must describe the main actors behavior in the first person (I, my). 3. Each question has 5 options, with exactly 1 correct; do not prefix options with A/B/C. 4. The correct option is minimal, faithful restatement of the caption; never invent or modify temporal or causal details that are not supported by the caption. 5. The distractors must be logically reasonable but not consistent with the video content. 6. Do not mention caption, annotation, or text. Use phrases like in my view or see. 7. Different questions should cover different parts or phases of the video as much as possible, avoiding repetitive questioning. 8. Questions must require temporal understanding of the video: they should not be answerable from single static frame or from the question wording alone. [Question Types] 1. Temporal Relation (TR) Ask about the sequence or simultaneity of actions. Distractors can involve: order swap, misplaced insertion/omission or wrong simultaneity. 2. Causal Relation (CR) Given an action and ask about its effect, or given an effect and ask about its cause. 3. Counterfactual Relationship (CRS) The question stem deliberately describes an incorrect event order, and the question asks for the result of this wrong order. The correct option must explicitly identify the error in the stems described order, provide the correct sequence of events, and state the true consequence/result under this correct sequence. For distractors, two continue the incorrect sequence and describe plausible but false results under that assumption. Two also recognize the error but describe incorrect corrections or outcomes inconsistent with the caption. Table S4. Prompt template for Temporal-Causal Relation. AI systems that can explain their reasoning, crucial step towards trustworthy autonomy. At the same time, we recognize potential dual-use concerns. Any technology that enhances an AIs ability to understand and replicate complex human actions could potentially be applied to malicious ends. We release our benchmark to the academic community to foster open and positive research, and we advocate for the continued development of strong ethical guidelines to govern the deployment of such advanced AI capabilities. 5 Prompt Template: Generating QA Pairs for Intentional Understanding You are an expert at designing multiple-choice questions (MCQs) about intention reasoning for fine-grained actions in videos. [Objective] Given the detailed caption annotation for the entire video, design {N QUESTIONS} MCQs that evaluate the understanding of the intentions behind actions. [Design Rules] 1. The question stem must address the main actor in the second person (you). 2. The options must describe the main actors behavior in the first person (I, my). 3. Each question has 5 options, with exactly 1 correct; do not prefix options with A/B/C. 4. The correct option is minimal, faithful restatement of the caption; try not to invent or modify the intention details that are not supported by the caption. 5. The distractors must be logically reasonable but not consistent with the video content. 6. Do not mention caption, annotation, or text. Use phrases like in my view or see. 7. Different questions should cover different parts or phases of the video as much as possible, avoiding repetitive questioning. 8. Questions must require temporal understanding of the video: they should not be answerable from single static frame or from the question wording alone. [Question Types] 1. Functional Intention (FI) Action-level: Ask about why specific action is performed (its purpose or intended result). Atomic-level: Ask about why specific action is performed in particular way, focusing on one or more execution dimensions (agent/effector, interaction object & tool, object part, manipulation type, dynamic qualities, action direction, object location, repetition count, etc.). 2. Global Intention (GI) Focus: What is the overall purpose or goal of the protagonists sequence of actions? 3. Counterfactual Intention (CIT) Focus: If the protagonist had different intention, how should the action execution change? Table S5. Prompt template for Intentional Understanding. Prompt Template: Generating QA Pairs for Evaluative Judgment You are an expert at designing multiple-choice questions (MCQs) about evaluating the quality of execution for finegrained actions in videos. [Objective] Given the detailed caption annotation for the entire video, design {N QUESTIONS} MCQs that evaluate the quality and effectiveness of action execution-covering agent/effector, interaction object and tool, object part, manipulation type, dynamic qualities, action direction, object location, and repetition count. [Design Rules] 1. The question stem must address the main actor in the second person (you). 2. The options must describe the main actors behavior in the first person (I, my). 3. Each question has 5 options, with exactly 1 correct; do not prefix options with A/B/C. 4. The correct option is minimal, faithful restatement of the caption; try not to invent or modify the details that are not supported by the caption. 5. The distractors must be logically reasonable but not consistent with the video content. 6. Do not mention caption, annotation, or text. Use phrases like in my view or see. 7. Different questions should cover different parts or phases of the video as much as possible, avoiding repetitive questioning. 8. Questions must require temporal understanding of the video: they should not be answerable from single static frame or from the question wording alone. [Question Types] 1. Process Monitoring (PM) Evaluate the stage, smoothness, or blockage of task execution in relation to the global intention. Identify whether progress is smooth, temporarily obstructed, failed, completed, or interrupted. Questions should focus on temporal stages and transitions rather than static conditions. 2. Strategy Evaluation (SE) Assess the appropriateness, efficiency, and coordination complexity of the chosen execution dimensions. Questions should analyze how well the chosen method fits the physical constraints, and how effectively it balances coordination and efficiency. 3. Counterfactual Evaluation (CEU) Explore how changing specific execution detail (e.g., tool, force, direction, timing, or repetition) would alter the quality, efficiency, or success of the action. Table S6. Prompt template for Evaluative Judgment. 7 Prompt Template for Open-Ended Evaluation (1/3) You are an expert at evaluating answers to open-ended questions about fine-grained video understanding and reasoning. You will evaluate the models response to the given question based on the complete fine-grained description of the video (VideoCaption) and the human-written correct answer (CorrectAnswer), and score the models performance on two dimensions: correctness and detailedness, each within the range of 0 to 10. Use the VideoCaption as the primary factual reference. The CorrectAnswer is only an auxiliary reference; if VideoCaption and CorrectAnswer conflict, always follow the VideoCaption. The open-ended task types include: 1. Counterfactual Interaction (CIA): The question asks how an action that did not occur would be executed. Check whether the answer points out the error in the question and explains the correct way of execution. 2. Counterfactual Relationship (CRS): The question describes an incorrect temporal or causal relationship. Check whether the answer identifies the error and provides the correct sequence and outcome. 3. Functional Intention (FI): Focus on the intention behind specific action. 4. Global Intention (GI): Focus on the overall purpose or goal of the protagonists sequence of actions. 5. Counterfactual Intention (CIT): Focus on how execution should change if the protagonist had different intention. 6. Process Monitoring (PM): Focus on the stage, smoothness, or blockage of task execution in relation to the global intention. 7. Strategy Evaluation (SE): Focus on the appropriateness, efficiency, and coordination complexity of the chosen execution strategy. 8. Counterfactual Evaluation (CEU): Focus on how changing specific execution detail would alter the quality, efficiency, or success of the action. Phase 1: Zero-Tolerance Check (for CIA / CRS) If the TaskType is CIA or CRS, the CandidateAnswer must explicitly identify that the question stem contains false or incorrect description. It is not enough to merely describe what actually happens in the video; the answer must also clearly indicate that the questions description is wrong or inconsistent with the video (e.g., The question describes this incorrectly because X). If the model accepts the false premise as true (i.e., does not explicitly flag the error), assign Correctness = 0 and Detailedness = 0, and skip directly to the final output. If the model correctly points out the error in the question stem, proceed to Phase 2 to evaluate the quality of the correction. If the TaskType is not CIA or CRS, skip Phase 1 and proceed directly to Phase 2. Phase 2: Scoring on Two Dimensions Dimension 1: Correctness (010 points) Evaluate whether the CandidateAnswer is factually consistent with the VideoCaption and truly answers the reasoning demand of the Question. Correctness Rating Criteria: 910 points (Perfect) - Completely accurate. Matches the VideoCaption facts and the core logic of the CorrectAnswer. - For counterfactual questions, correctly identifies the error (if applicable) and provides the corrected description. - No critical errors about who did what, when, where, why, or with what. 78 points (High) - Correct main conclusion and key reasoning steps. - May have minor omissions or small inconsistencies compared to the CorrectAnswer, but no clear contradictions with the VideoCaption. - Correctly addresses the intended reasoning type (e.g., intention, process, strategy, counterfactual). Table S7. Prompt template for GPT-assisted evaluation of CFG-Bench (Part 1). 8 Prompt Template for Open-Ended Evaluation (2/3) 56 points (Medium) - Partially correct: captures the general idea or part of the reasoning chain, but misses important steps or conditions. - May include one notable mistake or several minor slips, or mix correct and incorrect causal/temporal links. - Still shows some meaningful alignment with the Question and VideoCaption. 34 points (Low) - Significant errors: misinterprets the core of the Question or misreads key facts from the VideoCaption. - Multiple incorrect claims about actions, actors, intentions, or temporal/causal relations. - Reasoning type alignment is weak: the answer often talks about the wrong aspect (e.g., static appearance instead of intention). 12 points (Poor) - Mostly irrelevant to the Question or heavily hallucinates facts not supported by the VideoCaption. - Offers almost no correct reasoning about the video. 0 points (Fail) - Fails the Phase 1 Check for CIA/CRS (does not explicitly identify the false premise), or the response is pure gibberish and cannot be meaningfully evaluated. Dimension 2: Detailedness (010 points) Evaluate whether the CandidateAnswer is comprehensive, precise, and focused when addressing the required reasoning type, including conditions, nuances, and evidence from the VideoCaption. Detailedness Rating Criteria: 910 points (Rich) - Thoroughly explains the relevant why and/or how for the given reasoning type (intention, process, strategy, counterfactual, etc.). - Uses multiple specific details from the VideoCaption (objects, movements, sequence of steps, outcomes, constraints). - Comparable in depth and nuance to the CorrectAnswer. 78 points (Good) - Covers the main reasoning steps and most key conditions. - Includes several concrete details from the VideoCaption, though may miss some secondary nuances. - Explanation is coherent and informative, but not fully exhaustive. 56 points (Average) - Provides generally correct explanation but remains high-level or somewhat generic. - Mentions some relevant elements (e.g., main intention or main causeeffect link) but lacks fine-grained detail or specific evidence. - Limited use of explicit cues from the VideoCaption. 34 points (Weak) - Brief or superficial; focuses on only one part of the reasoning while omitting other important aspects. - Contains very few concrete details from the VideoCaption. - Does not fully engage with the required reasoning type (e.g., just states the result without explaining why). 12 points (Poor) - Extremely short, vague, or empty. - Almost no usable reasoning or concrete detail. 0 points (Fail) - Fails the Phase 1 Check for CIA/CRS (thus Detailedness must also be 0), or is pure gibberish with no coherent content. Table S8. Prompt template for GPT-assisted evaluation of CFG-Bench (Part 2). 9 Prompt Template for Open-Ended Evaluation (3/3) Additional Policies - Expression tolerance: Paraphrasing or surface wording differences are acceptable if the meaning is equivalent. - Multiple valid answers: Different correct phrasings expressing the same reasoning are equally valid. - No reward for verbosity: Long but off-topic or repetitive text does not increase the Detailedness score. Only relevant, grounded details count. - Correctness first: If an answer is seriously incorrect, it should not receive high scores even if it is long or detailed. - Omissions vs. errors: Omitting some content is less severe than stating it incorrectly. Explicit errors should penalize Correctness more than simple omissions. Uncertain or Incomplete Evidence Policy Even if the VideoCaption does not explicitly state the reasoning or outcome, you must infer the most plausible interpretation from the available evidence and context. - Always provide reasoned judgment rather than defaulting to pure uncertainty. - Use lower scores to reflect weak or incomplete evidence instead of skipping evaluation. - Avoid outputs like Underspecified; always justify your stance with textual evidence or reasonable inference. Context - VideoCaption: {caption} - Question: {question} - CorrectAnswer: {correct answer} - TaskType:{task type} - CandidateAnswer: {result} Output (STRICT JSON) Return only: { evidence spans: short quotes from VideoCaption that support your judgment, correctness reasoning: explanation of your correctness score, detailedness reasoning: explanation of your detailedness score, correctness: [integer 0-10], detailedness: [integer 0-10], } Table S9. Prompt template for GPT-assisted evaluation of CFG-Bench (Part 3). 10 Figure S3. Examples of each tasks in Physical Interaction (Tier 1). 11 Figure S4. Examples of each tasks in Temporal-Causal Relation (Tier 2). 12 Figure S5. Examples of each tasks in Intentional Understanding (Tier 3). 13 Figure S6. Examples of each tasks in Evaluative Judgment (Tier 4)."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "The Hong Kong Polytechnic University",
        "Wolf 1069B, Sany Group",
        "Zhejiang University"
    ]
}