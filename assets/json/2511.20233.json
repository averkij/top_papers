{
    "paper_title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "authors": [
        "Chuyi Kong",
        "Gao Wei",
        "Jing Ma",
        "Hongzhan Lin",
        "Yaxin Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning."
        },
        {
            "title": "Start",
            "content": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Jing Ma Hong Kong Baptist University Hong Kong SAR, China majing@comp.hkbu.edu.hk Gao Wei Singapore Management University Singapore weigao@smu.edu.sg Chuyi Kong Hong Kong Baptist University Hong Kong SAR, China cscykong@comp.hkbu.edu.hk 5 2 0 2 8 2 ] . [ 2 3 3 2 0 2 . 1 1 5 2 : r Hongzhan Lin Hong Kong Baptist University Hong Kong SAR, China cshzlin@comp.hkbu.edu.hk Yaxin Fan Soochow University Soochow, China yxfansuda@stu.suda.edu.cn Abstract The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations (REFLEX) paradigm, plug-and-play, self-refining paradigm that leverages the backbone models internal knowledge to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-ofthe-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to 7.57% improvement, highlighting that internal explanation signals play dual role in both interpreting and enhancing factual reasoning. CCS Concepts Information systems Multimedia information systems. Keywords Fake News Detection, Explainable, Large Language Model *Corresponding Author. Conference17, Washington, DC, USA 2025. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 Introduction\nThe rapid spread of misinformation on social media has become a\ncritical social concern, threatening the reliability of public knowl-\nedge. For instance, to defend the truth, even scientists in Nature1\nhave been involved in debunking fake news. However, manual\nfact-checking is time-consuming and limited in coverage, making\nit difficult to mitigate the viral propagation of false claims. This\nunderscores the urgent need for automated fake news detection\nmethods that not only verify the factuality but also provide clear\nand trustworthy explanations. Consequently, recent automated fact-\nchecking (AFC) approaches rely on powerful Large Language Mod-\nels (LLMs) to verify the claims and provide explanations. HISS [64]\nutilizes Retrieval-Augmented Generation (RAG) [22] to decompose\nLLM reasoning trajectories into explanations, L-Defense [54] distills\nexplanations from powerful models to fine-tune smaller language\nmodels (SLMs), adapting to fact-checking tasks. RAV [51] constructs\nmulti-agent systems to directly assign specialized functions.",
            "content": "Despite these advances, they share fundamental limitation: treating explanation generation as an external post-hoc process. This process heavily depends on external retrieval or closed-source APIs, which obscure the reasoning pathway, increase latency, and even amplify hallucinations. Moreover, after being repeatedly fine-tuned on fast-changing social media claims, these models inevitably suffer from knowledge conflicts between external knowledge and the models internal representations an alignment tax that degrades factual consistency [10, 16]. Such designs overlook the rich factual representations already encoded within LLMs. To bridge this gap, we observe that intervening on models internal knowledge as an alternative to external supervision during inference time[39], shows great potential to realign the factuality and direct to humanobservable truth, thereby reducing misconceptions, as evaluated in TruthfulQA [26]. Given that LLMs inherently encode extensive real-world truths [24], we argue that the key challenge lies not in acquiring more information but in activating these latent representations in controlled and interpretable manner. This insight motivated us to explore how internal activations can guide models toward more challenging, rapidly changing human-unknown truths that are otherwise prone to misinformation. To this end, we propose the REason-guided Fact-checking with Latent EXplanations (REFLEX), self-refining and plug-and-play 1https://www.nature.com/articles/d41586-025-02876-1 Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan Figure 1: The brief outline of our three-stage REFLEX paradigm. The red text denotes reasoning style learned from fine-tuning, and the blue text denotes factual knowledge stored in backbone models. paradigm that steers models internal activations to jointly verdict claim and refine explanation. The key to our recipe is to leverage internal signals that distinguish factual substance from stylistic behavior, enabling the model to reflect on and refine its reasoning. Conceptually, REFLEX operates in three stages. First, it reformulates fact-checking as role-play dialogue, where models generates factual verdict and its explanation, allowing self-explanation during training. Second, self-distillation is applied to both the backbone and fine-tuned variants, identifying contrasitive pairscases where their outputs disagree (Quadrant II and IV in Figure 1)revealing where factual reasoning diverges. Third, these pairs are used to identify the steering directions in latent space. We employ simple logistic probe to separate activations, producing steering vector [13] that disentangles truth into (a) substance, representing factual knowledge grounded in the backbone (Quadrant IV), and (b) style, capturing reasoning patterns learned during fine-tuning (Quadrant II). During inference, RELFEX dynamically selects the more reliable direction and refines the explanation at the activation level. In this way, the fact-checker not only generates accurate factual judgments but also produces consistent explanations. Experiments show that REFLEX outperforms methods relying on external resources in both verdict accuracy and explanation quality, even surpassing the skyline, where ChatGPT generates explanations from the claim and verdict, with concise style. Meanwhile, it can generalize across backbones and pair combinations with limited samples, demonstrating strong transferability, flexibility, and data efficiency. Models trained with explanatory objectives can effectively guide those without such objectives, achieving 7.57% improvement in verdict accuracy. Upon further analysis, we find that, unlike human-observable truths, human-unknown truths in fact-checking are challenging for the traditional single-direction steering. They exhibit neither probability gaps nor performance gains in higher layers [7], reflecting their subtle and fine-grained complexity. REFLEX, however, achieves its largest probability gaps and performance gains in the middle-layer activations, where disentangling verdict factuality from noisy explanation styles, and improving explanation readability by up to 14%, further confirming its disentanglement efficiency. Notably, REFLEX achieves state-ofthe-art results on RAW-FC dataset [59] using only 465 self-refined samples, without relying on any external APIs. Overall, our main contributions are as follows: We propose REFLEX, plug-and-play, self-refining paradigm that disentangles truth into substance and style, enhancing interpretability and steering efficiency. REFLEX achieves state-of-the-art performance on real-world datasets with only small set of self-refined samples, while producing high-quality explanations. We show that explanations serve dual rolenot only enhance human understanding but also act as internal activation signals to enhance factual reasoning. We find that REFLEX represents human-unknown truths in middle layers due to its complexity, whereas the humanobservable truths are embedded in higher layers."
        },
        {
            "title": "2 Background\n2.1 Explainable Fact-Checking\nPrevious studies on explainable fact-checking can be categorized\nby the granularity of their explanations, ranging from token-level\nkeyword highlighting[41, 57] and suspicious user tagging [32], to\nsentence-level attention [33, 37, 50], and task-level approaches such\nas summarization [2, 17, 20, 46, 49, 60] or multi-task learning [2]\nfor explanation extraction. However, these traditional methods\ntypically suffer from limited interpretability or a strong dependency\non manually crafted fact-check reports, which restricts practicality\nin real-world applications.",
            "content": "Shifting to LLMs with stronger multi-label reasoning, recent works have begun to explore more powerful explanation mechanisms. HiSS [64] decomposes complex claims to atomic ones via REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA RAG [22], using retrieved reasoning trajectories as evidence. RAV [51] constructs multi-agent dialogue system to iteratively recon, answer, and validate claims. L-Denfense [54] distills adversarial evidence explanations from teacher models. Despite their advancements, they face several key limitations: retrieval-based methods are prone to majority bias and noise from external sources; multiagent systems introduce inference latency, which is problematic for time-sensitive fact-checking; and distillation-based fine-tuning weakens internal interpretability and may amplify hallucinations. Inspired by self-training [1] and STaRs [62] frameworks, our REFLEX introduces self-refining distillation paradigm that runs only once, enabling transferable and plug-and-play steering vectors[39] to enhance factuality and model internal interpretability."
        },
        {
            "title": "2.2 Style and Substance in Fact-Checking\nSeveral studies have approached fact-checking by capturing stylis-\ntic divergences between machine-generated and human-written\ncontent [40, 42]. However, while machine-generated text typically\nmaintains a consistent linguistic style, humans often intentionally\nshift their communication style when attempting deception [48].\nThis discrepancy raises concerns about the robustness of style-based\ndetection approaches, motivating subsequent research toward style-\nagnostic training paradigms [56].",
            "content": "In contrast to prior works that identify stylistic cues from input claims, our paradigm captures style at the model output level with steering vectors [4, 24, 45]. It builds on controllable generation [8, 21, 25] and activation editing [14, 23], which steer factual directions on human-observable truths such as those in TruthfulQA [26]. These truths are easy to steer. In fact-checking, however, human-unknown truths are tightly entangled with stylistic patterns, as claims and explanations interact. Our paradigm explicitly separates style from substance, enabling both interpretability and robustness in AFC."
        },
        {
            "title": "3 Methodology\n3.1 Task Formulation\nGiven a dataset D = {(ùëê, ùëíùë£ùëñ, ùë£, ùëíùë•ùëù)ùëñ }ùëÅ\nùëñ=1, where ùëê denotes a claim,\nùëíùë£ùëñ an optional set of retrieved evidence documents, ùë£ the gold ve-\nracity label, and ùëíùë•ùëù the human-written explanation, the objective\nof REFLEX is to generate (i) a veracity verdict ÀÜùë£ and (ii) an expla-\nnation ÀÜùëíùë•ùëù that justifies this verdict, for any given ùëê and optionally\nprovided ùëíùë£ùëñ.",
            "content": "As illustrated in Figure 1, the REFLEX framework operates in three sequential stages: (1) Dialogue-style Fact-Checker Training constructs prompts for instruction-tuning to perform fact verification tasks for verdict and explanation generation; (2) Contrastive Activation Pairs Extraction derives activation pairs between the backbone model and its fine-tuned variant; and (3) ExplanationGuided Steering extracts steering vectors to disentangle reasoning style and factual substance and applies them for inference."
        },
        {
            "title": "3.2 Model Training\n3.2.1 Data Preprocessing. To better activate the knowledge em-\nbedded in the backbone, we formulate the data as a single-turn\nQA-style dialogue. This design is motivated by two reasons: (1)\nLLM backbone already encodes extensive factual knowledge. fine-\ntuning with limited data primarily serves to activate this knowledge",
            "content": "and adapt the models style to the target task [3, 11, 44]. (2) QA-style supervision has been shown to yield stronger knowledge generalization during fine-tuning, whereas document-based data (e.g., from Wikipedia) commonly used in fact-checking datasets leads to poorer generalization [65]. 3.2.2 Training Protocol. The model is optimized using the standard cross-entropy loss: LCE (ùúÉ ) = ùëÅ ùë¶ (ùëñ ) ùëñ= ùë° =1 log ùëÉùúÉ (cid:0)ùë¶ (ùëñ ) ùë° ùë• (ùëñ ), ùë¶ (ùëñ ) <ùë° (cid:1), (1) where ùúÉ denotes model parameters and ùë¶ (ùëñ ) the ùë°-th token in the ùë° output sequence. More details, including hyper parameters, are shwon in Appendix D. We adopt four input-output configurations: ùë• = [ùëê] ùë¶ = [ùë£], ùë• = [ùëê; ùëíùë£ùëñ] ùë¶ = [ùë£], ùë• = [ùëê] ùë¶ = [ùë£; ùëíùë•ùëù], ùë• = [ùëê; ùëíùë£ùëñ] ùë¶ = [ùë£; ùëíùë•ùëù]. For the prompt, the tuning method explicitly instructs the model to produce its reasoning path as the explanation, when the explanation is included, which could improve performance [27], and we employ Chain-of-Thought (CoT) [5, 5, 34, 55] prompting to extract the reasoning path. Moreover, to enhance the reasoning ability, we adopt role-play prompting [18, 19]. The templates are provided in Appendix A."
        },
        {
            "title": "3.3 Contrastive Pairs Extraction\nSelf-Knowledge Distillation. Following the training method\n3.3.1\ndescribed above, we fine-tune models Msft on the backbone Mbase\nand conduct inference on the training set.",
            "content": "As the backbone model itself does not possess the instructionfollowing ability, similar to [28], we adopt few-shot learning to distill knowledge. In addition, to prevent data leakage, we either cross-select training sets from different datasets within the same domain or use the models own validation set. To avoid majority bias, we embedded examples with balanced label distribution to fill the backbone models maximal context length. To ensure reproducibility and factual accuracy, both models generate deterministic outputs under temperature fixed to zero: ÀÜùë¶ = arg max ùëÉùúÉ (ùë¶ ùë•), (2) ùë¶ where is the vocabulary. For each token position ùë° and decoder layer ùëô, we record hidden representations: Rùëë . ‚Ñé (base) ùëô,ùë° , ‚Ñé (sft) ùëô,ùë° These activations form feature vectors for the probe in Section 3.4. 3.3.2 Adaptive Sample Selection. Given veracity predictions ÀÜùë£ base, ÀÜùë£ sft, and gold label ùë£ gold, we classify samples as: Quadrant II: ÀÜùë£ base ùë£ gold, ÀÜùë£ sft = ùë£ gold, Reasoning Gain, Quadrant IV: ÀÜùë£ base = ùë£ gold, ÀÜùë£ sft ùë£ gold, Knowledge Loss. Intuitively, samples in Quadrant II indicate cases where the finetuned model corrects the backbones mistakes, implying enhanced reasoning or stylistic adaptation[27], whereas Quadrant IV captures the opposite, where fine-tuning introduces factual drift, leading to hallucinations[10, 16]. To construct contrastive pairs, we adaptively Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan Algorithm 1: Explanation-Guided Steering (EGS) Input: Contrastive pairs P, decoder layers ùêø, multipliers Output: Knowledge vector ùêæùëâ , Inference vector ùêºùëâ 1 foreach ùëô ùêø do Table 1: Summary statistics of dataset distributions. Label values 0-2 represent increasing veracity labels: {False/Refuted, Half-True/Not Enough Evidence, True/Supported}. 3 4 5 6 7 , ‚Ñé ùëô,ùëñ )} ùëñ=1 ; Extract activations {(‚Ñé+ ùëô,ùëñ Train logistic probe ùëùùëô (ùë¶ ‚Ñé) = ùúé (ùëä ùëô Normalize sùëô = ùëäùëô /ùëäùëô ; foreach ùõº do ‚Ñé + ùëèùëô ); Apply steering: ‚Ñé ùëô,ùë° = ‚Ñéùëô,ùë° + ùõº sùëô ; Compute probability gap ŒîùëÉùëô,ùõº = P(‚Ñé) Punsteered; Record (ùëô, ùõº ùëô ) = arg maxùõº ŒîùëÉùëô,ùõº ; 8 9 Select ùëô = arg maxùëô ŒîùëÉùëô,ùõº ùëô ; 10 Set ùêæùëâ , ùêºùëâ ùõº ùëô sùëô ; select samples from these two quadrants. For each claim ùë• where ÀÜùë£ base and ÀÜùë£ sft disagree, the version predicted correctly (aligned with ùë£gold) is designated as positive instance ùë• +, and the incorrect one as the negative instance ùë• ."
        },
        {
            "title": "3.4 Explanation-Guided Steering\n3.4.1 Logistic-Probe Learning. To identify activation directions\nthat separate positive from negative instances, we train a logistic\nregression probe on each decoder layer ùëô:\nùëùùëô (ùëß = 1 | ‚Ñé) = ùúé (ùëä ‚ä§",
            "content": "ùëô ‚Ñé + ùëèùëô ), (3) where ùëß {0, 1} denotes the binary label (1 for factual, 0 for incorrect). The learned weight ùëäùëô Rùëë serves as the steering vector sùëô , normalized as sùëô = ùëäùëô /ùëäùëô . During inference, we inject the scaled steering signal to modify the hidden representation: ‚Ñé ùëô,ùë° = ‚Ñéùëô,ùë° + ùõºùëô sùëô, (4) where ùõºùëô controls the steering direction and intensity. We then extract two key steering directions: (1) Inference Vector (ùêºùëâ ) derived from Quadrant II samples, where fine-tuning enhances reasoning and factual alignment. It points along the learned probe weight with ùõºùëô > 0, sIV ùëô = ùëäùëô + ùëäùëô , steering activations toward refined reasoning patterns that improve explanation quality. (2) Knowledge Vector (ùêæùëâ ) derived from Quadrant IV samples, where the base model remains correct but fine-tuning introduces deviation. It also points along the learned probe ùëäùëô weight with ùõºùëô > 0, sKV ùëäùëô , guiding activations back toward the backbones factual subspace. For each layer ùëô, both sKV ùëô are evaluated using the inferùëô ence update (Eq. 4). The layer-intensity pair (ùëô , ùõº ùëô ) that maximizes the factual accuracy improvement is then selected, yielding the final vectors ùêæùëâ = ùõº ùëô . See Algorithm 1 for the detail of Explanation-Guided Steering (EGS). ùëô and ùêºùëâ = ùõº and sIV ùëô sKV ùëô sIV = + ùëô 3.4.2 Explanation Refinement. To further improve explanation quality, we analyze the alignment between each tokens activation and the learned steering vector. For given layer ùëô and token Dataset Split 0 RAWFC train eval test Liar-RAW train eval test Averitec train eval test 514 66 66 2,568 410 367 1,742 305 1 537 67 67 1,336 159 169 849 35 33 2 561 67 2,264 292 319 282 122 120 Total 1,612 200 200 6,168 861 855 2,873 462 ùë°, the cosine alignment score is computed as: . ùëéùëô,ùë° = ‚Ñéùëô,ùë° sùëô ‚Ñéùëô,ùë° sùëô During manual inspection, we observed that tokens with highdensity negative cosine similarity often align with redundant or noisy sentence-level patterns. To balance readability and informativeness, we suppress such tokens using the lightweight RatcliffObershelp pattern-matching algorithm [43]. (5)"
        },
        {
            "title": "4 Experiments\nIn this section, we first evaluate the effectiveness and interpretabil-\nity of REFLEX on two real-world benchmarks. We then introduce\na third dialogue-based dataset to conduct comprehensive ablation\nstudies from three perspectives: backbone models, contrastive pair\ncombinations, and model-internal interpretability. Finally, we pro-\nvide an in-depth discussion on how REFLEX improves both model\nperformance and interpretability.",
            "content": "Dataset. To better reflect real-world fact-checking and reduce hallucination risk, we use three datasets in which claims come from professional fact-checking platforms and all explanations are human-written: RAW-FC [59] from Snopes2, LIAR-RAW [59] from PolitiFact3, and AveriTec [47]. In RAW-FC and LIAR-RAW, explanations directly justify the claim label; in AveriTec, they justify both the claim and its supporting evidence. For consistency, we refer to all of them as explanations. Regarding data structure, RAW-FC and LIAR-RAW follow the standard fact-checking format with instances claim, evidence, label, explanation. AveriTec instead decomposes fact-checker reasoning into QA-style, multi-turn verification process. The three datasets use different label schemes. RAW-FC contains {true, half, false}; LIAR-RAW uses six labels; and AveriTec uses {Supported, Not Enough Evidence, Conflicting Evidence/Cherrypicking, Refuted}. To enable joint verdict prediction and explanation generation in REFLEX, we unify labels as follows: in LIAR-RAW, we merge {pants-fire, false, barely-true} into False, keep Half-True, and merge {mostly-true, true} into True. In AveriTec, we drop Conflicting Evidence/Cherrypicking due to its ambiguity. We remove LIAR-RAW 2www.snopes.com 3www.politifact.com REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA instances without evidence and exclude AveriTec few-shot examples from validation to prevent leakage. As shown in Table 1, RAW-FC is label-balanced, while the others are not. Since AveriTec does not release test set(to avoid leakage), we use its validation set for testing after removing overlapping samples during training. Finally, since some baselines cannot process dialogue-style data, we report baseline comparisons only on RAW-FC and LIAR-RAW. Metrics. For verdict evaluation, we report Precision, Recall, and Macro-F1. For explanation quality, following [54], we employ LLM as judge [12], where ChatGPT scores explanations along four dimensions: misleadingness, informativeness, soundness, and readability(more details in Appendix A). Each dimension is rated on five-point Likert scale, with higher scores indicating better quality except misleadingness, which is inversely scored. Moreover, our human evaluation (Appendix C) confirms the consistency between manual and automatic assessments. Training Setup. As mentioned in Section 3.2, we construct all data in role-playing dialogue format: the human poses query containing claim (and optional evidence), and the assistant, acting as fact-checker, responds with verdict and the corresponding explanation. For RAW-FC and LIAR-RAW, evidence corresponds to the annotated relevant evidence (labeled as 1). For AveriTec, which is natively multi-turn, we flatten each dialogue into single-turn instance for consistency with the other datasets. More training details are provided in Appendix D."
        },
        {
            "title": "4.1 Baseline Trials\n4.1.1 Baselines. Despite its lightweight design, REFLEX involves\nparametric training and is thus categorized as Semi-parametric.\nIn comparison, our baselines are divided into the two types: (1)\nNon-parametric Approach: LLaMA2-7B-Chat[52], ChatGPT[38],\nRAV[51], and HISS[64]. (2) Parametric Approach: FactLLaMA[6]\ntrained with LLaMA2 and LoRA[15]; L-Defense[54] trained with\nRoBERTa-large[30] to distill explanations from LLaMA-2-7B-Chat\nand GPT-3.5. To make the comparison as fair as possible, we adopt\nLLaMA2-7B as the backbone in this section.",
            "content": "4.1.2 Results. For verdict prediction, Table 2 shows that our model achieves state-of-the-art performance on RAW-FC without relying on any closed-source APIs. After Stage 1 (w/o EGS), it outperforms LLaMA2-7B-Chat and ChatGPT by 16.16% to 21.28% F1, and exceeds HISS by 6.69% F1. Compared with FactLLaMA, built on the same backbone and data but without dialogue style or full-parameter tuning, our model achieves 4.94% to 6.83% F1 gain. It performs comparably to the training-free multi-agent RAV system, while using single model, and falls slightly below L-Defense. After applying EGS, our model surpasses RAV by 5.8% F1 and outperforms L-Defense by 3.79%4.87% F1, despite using only 465 self-distilled samples (vs. L-Defenses 32,240 distillations with even GPT-3.5), highlighting remarkable data efficiency. Since our models operate with unified three-way label scheme following explanation-based methods, no comparison is made on LIAR-RAW in this part for fair comparison. For explanation quality, We evaluate only baselines that generate explanations. Following Wang et al. [54], we map LIAR-RAWs six labels to three and apply the same mapping to all applicable baselines. We also include an Oracle that supplies ChatGPT with the claim and verdict to produce explanations as the skyline. As shown in Table 3, after Stage 1, our model achieves state-of-the-art scores on RAW-FC in misleadingness, informativeness, and soundness, exceeding only L-Defense (ChatGPT-distilled) in readability. On LIAR-RAW, it reaches state-of-the-art informativeness and readability, ranking second to L-Defense (LLaMA2) in soundness. After applying EGS, all metrics further improve, except for slight increase in misleadingness on RAW-FC, which we attribute to the strong disentanglement between verdict accuracy and explanation style. To address potential length bias in LLM-as-Judges [12], we analyze explanation length in Appendix B. On RAW-FC, given the same backbone (LLaMA2), our explanations are shorter than those of L-Defense. On LIAR-RAW, they remain shorter than all baselines, including the Oracle, indicating that our paradigm learns concise yet accurate explanation style."
        },
        {
            "title": "4.2 Ablation Studies\nTo demonstrate the generalizability, flexibility, and interpretability\nof our paradigm, we conduct the following experiments.",
            "content": "4.2.1 On Backbones. Besides LLaMA-2, we also trained on stronger backbone, Qwen-3 [58]. We performed training and inference across different stages, pair combinations, and reported hallucination rates and inference success rates. Formally, we define the hallucination rate (HR) and inferenc success rate (ISR) as: HR = #error after SFT #correct on BASE , ISR = #correct after SFT #error on BASE . (6) (7) Overall, as shown in Table 4, EGS improves performance across the three datasets by up to 5.03% compared with fine-tuned models in most cases. We focus on error cases with drop rate above 0.01%. Specifically, for LLaMA-2 and Qwen-3 on LIAR-RAW using (ùëê ùë£;exp) pairs, macro-F drops by 0.2%. Table 2 shows that these pairs perform poorly even at the base stage. Our analysis attributes this mainly to severe recency bias during few-shot learning, where the model tends to predict the last sample label, despite several existing methods addressing it [29, 31, 35, 36, 63]. The detailed label distribution for error cases is provided in Appendix F. Next, we analyze backbone models and datasets from three perspectives: combination performance, stage-wise performance, and hallucination & reasoning success ratio. From Table 4, we observe: (1) Base Models: LLaMA-2 performs best when only the claim is input and the verdict is output (ùëê ùë£), likely due to limited inherent reasoning ability [9]. In contrast, Qwen-3 benefits from inputting evidence and outputting explanations (ùëê;ùëíùë£ùëñ ùë£;ùëíùë•ùëù), reflecting its stronger reasoning capacity [9]. (2) SFT Models: On LIAR-RAW and RAWFC, models generally perform better with input excluding evidence but output including explanations (ùëê ùë£;ùëíùë•ùëù) than with input including evidence but outputting only verdicts (ùëê; ùëíùë£ùëñ ùë£). This is Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan Table 2: Performance comparison of models on RAW-FC and LIAR-RAW datasets. Google subscript denotes utilizing its API to search for evidence. Model Model No API Dependency / # Distill Explanation RAW-FC LIAR-RAW R macF1 macF1 Non-Parametric Approach LLaMA2-7b-chat c-v; exp - c-v; exp - ChatGPT c, evi-v; exp - c-v; exp c; evi-v HISSGoogle RAV ChatGPT LLaMA-3.1-70b-Instruct Parametric Approach c; evi-v FactLLaMA FactLLaMAGoogle c; evi-v L-Denfense LLaMA2-7b LLaMA2-7b c; evi-v; exp Roberta-large + LLaMA-2-7b-instruct c; evi-v; exp Roberta-large + GPT-3.5-turbo-0613 Semi-Parametric Approach (Ours) S-EGS w/o EGS c-v; exp c-v; exp LLaMA2-7b LLaMA2-7b / - q/ - q/ - q/ / - / - q/ - / 32,240 q/ 32,240 / 465 / 0 37.30 38.03 47.72 48.62 39.48 45.07 54.5 53.4 - - 53.76 54.00 56.11 55.50 60.95 60.00 61.72 61.01 36.77 44.43 39.31 53.9 59. 53.76 55.65 60.12 61.20 17.11 17.37 25.41 27.33 29.64 23.57 31.3 46.8 - - 32.32 31.57 32.46 32.05 31.63 31.71 30.55 32.20 15.14 25.11 21.90 37.5 25.40 29.98 30.44 31.40 30.53 65.04 65.01 64.99 49.90 47.57 43.61 43.05 60.66 61. 48.38 46.83 60.59 Table 3: Automatic Evaluation of Explanation Quality. Oracle ChatGPTfull ChatGPTclaim L-DefenseLLaMA2 L-DefenseChatGPT Ours S-EGSLLaMA2 w/o EGS RAWFC 4.73 4.46 4.62 4.44 4.44 4.00 4.67 4.44 4.41 4.17 1.52 2.07 1.97 1.95 1. 4.72 4.69 4.68 4.62 4.49 LIAR-RAW 1.85 2.29 2.27 2.20 2.06 4.44 3.71 3.93 4.39 4.12 4.60 4.04 4.29 4.64 4.28 4.69 3.99 4.50 4.63 4. 2.00 1.90 4.89 4.78 4.83 4.82 4.81 4.55 1.77 1.90 4.58 4. 4.66 4.60 4.83 4.65 because explanations serve as golden evidence, and long evidence sequences often exceed LLaMA-2s 4096-token limit, causing truncation. After fine-tuning, only Qwen-3 surpasses the golden evidence on RAWFC due to its 32 longer context window. (3) Stage-wise Performance: Most models improve after finetuning. An exception is RAWFC, where fine-tuned models, especially LLaMA-2, perform worse than base models, likely due to up-sampling of factual knowledge from Wikipedia during pre-training [53]. (4) Dataset Difficulty: Along with Tables 5, AveriTEC, constructed in dialogue format, is the easiest, yielding minimal hallucinations and high reasoning success. LIAR-RAW is the most challenging, with the highest hallucination rates and lowest model performance. 4.2.2 On Pairs Combinations. To demonstrate the flexibility and transferability of our paradigm and achieve further improvements, we examine pairs with mismatched style across training stages. Vertical steering follows previous setup, but instead pairs outputs from the base model (c->vbase) with those from the SFT model (c->v;expsft). Horizontal steering pairs only SFT model outputs (c->vsft) with (c->v;expsft) for direct steering. For clarity, the evidence inputs for AveriTEC are omitted from this point onward. As shown in Table 6, except for AveriTEC, both vertical and horizontal steering improve performance, highlighting the flexibility and transferability of our paradigm. Notably, using verdicts with explanation-guided vectors (ùëê ùë£; exp) to steer models given only claims (ùëê ùë£) yields gains up to 8.38%, demonstrating that the explanation can act as internal activation signals to improve factuality. For AveriTEC, performance drops slightly, which we attribute to its simplicity, as discussed in Section 4.2.1, making improvements quickly saturate. 4.2.3 On Model Internal interpretability. To enhance model interpretability, we conduct ablation studies along two axes: the optimal layer and the model direction with the largest probability gap. Optimal layer. As Figure 2 shows, for pairs involving claim-only, the largest gaps appear in early layers (15), while for pairs with full explanations, gaps peak in middle layers (1020). This pattern aligns with model internal interpretability: early layers capture lexical and topical features, middle layers encode style and syntax, and higher layers abstract concepts [61]. Unlike misconceptions and commonsense in truthfulQA, where human-observable truths peak at higher layers [7], fact-checking truth does not exhibit largeest gaps in later layers. We attribute this to the subtle, fine-grained complexity of human-unknown truth in fact-checking, which remains challenging even for humans. Model direction. To further validate the disentanglement efficiency, we take REFLEX after EGS as the baseline (stylesubstance), REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA Table 4: The macro-F1 scores of S-EGS across backbones and datasets. Square brackets denote optional settings for AveriTec. Backbone Stage LLaMABASE SFT S-EGS Qwen-3 BASE SFT S-EGS x->y c->v c; evi->v c->v; expcross c[; evi]->v; expself c->v c; evi->v c[; evi]->v; exp c->v c->v; expcross c[; evi]->v; expself c->v c; evi->v c[; evi]->v; expself c->v; expcross c->v c; evi->v c[; evi]->v; exp c->v c->v; expcross c[; evi]->v; expself Raw-FC Œîùëöùëéùëê ùêπ 1 LIAR-RAW Œîùëöùëéùëê ùêπ 1 AveriTec Œîùëöùëéùëê ùêπ 1 35.61 27.08 34.41 31.68 26.44 44.85 60. 31.47 64.99 61.81 46.54 46.23 48.86 46.66 41.67 63.17 58.35 41.69 59.39 58.86 -9.17 +17.77 +26.18 +5.03 +4.40 +1. -4.87 +16.94 +9.49 +0.02 +1.04 +0.51 29.26 16.97 12.48 35.80 37.23 40.21 43.05 38.65 42.77 43.06 37.63 41.30 39.16 42. 41.72 42.29 46.73 41.73 47.13 46.53 - 28.18 - 27.70 - 75.91 84.62 - - 84.61 - 66.14 66.02 - - 85.52 88.02 - - 88.21 +7.97 +23.24 +7.25 +1.42 -0.28 +0.01 +4.09 +0.09 +4.48 +0.01 +0.40 -0. +47.73 +56.92 -0.01 +19.38 +22.22 +0.19 Table 5: Hallucination Ratio (HR) and Inference Success Ratio (ISR) across backbones. Overall statistics are provided in Appendix E. Cross denotes few-shot learning using another datasets training samples, while self indicates use of the models own validation set."
        },
        {
            "title": "Backbone Dataset",
            "content": "x->y LLaMA-2 RAW-FC c->v c->v; expself c->v; expcross LIAR-RAW c->v c->v; expself c->v; expcross"
        },
        {
            "title": "Statistics",
            "content": "HR ISR 0.1236 0.1768 0.1502 0.6731 0.3809 0.9546 0.2567 0.6012 0.5663 0.602 0.5781 0."
        },
        {
            "title": "AveriTec",
            "content": "c; evi ->v; expself 0.1033 0.9423 Qwen-3 RAW-FC c->v c->v; expself c->v; expcross LIAR-RAW c->v c->v; expself c->v; expcross 0.5351 0.2268 0.2152 0.3671 0.2388 0.2426 0.6057 0.5565 0.5434 0.5105 0.4435 0."
        },
        {
            "title": "AveriTec",
            "content": "c; evi ->v; expself 0.9092 and test three variants: (1) directing fully toward truth (positives = correct verdicts, negatives = incorrect ones), (2) directing toward the base model (positives = backbone outputs, negatives = SFT ones), and (3) directing toward the SFT model (reversing (2)). 0.0174 Table 6: The Macro-F1 scores for pair combinations experiments on different learning objectives (Obj.) Backbone Direction Pairs LLaMA-2 vertical c->vbase, c->v;expsft Obj. c->v c->v;exp horizontal c->vsft, c->v;expsft c->v c->v;exp Qwen-3 vertical c->vbase, c->v;expsft c->v c->v;exp horizontal c->vsft, c->v;expsft c->v c->v;exp Raw-FC LIAR-RAW AveriTec 34.01 (7.57) 62.17 (1.58) 34.82 (8.38) 62.64 (2.05) 41.69 (0.02) 58.88 (0.53) 42.1 (0.43) 58.32 (-0.03) 38.37 (1.14) 43.61 (0.56) 38.37 (1.14) 43.73 (0.68) 41.91 (0.19) 46.8 (0.07) 41.76 (0.04) 47.04 (0.31) 74.86 (1.05) 85.86 (1.24) 74.79 (1.12) 82.92 (-1.70) 85.71 (0.19) 88.62 (0.6) 85.89 (0.37) 88.91 (0.89) As shown in Table 8, the blue regions are submerged by red ones, further confirming the effectiveness of the disentanglement. Most blue regions appear in LIAR-RAW, likely due to the recency bias discussed in Section 4.2.1."
        },
        {
            "title": "4.3 Deep Analysis\nTo explain how S-EGS improves both verdict prediction and expla-\nnation quality, we conduct in-depth quantitative and qualitative\nanalyses, respectively.",
            "content": "Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan Table 7: The explanation quality for all improved variants. Red background denotes improvement, while blue ones denote decline. Backbone Pair LLaMA-2-7b Baseline Qwen-3-7b -> v; expcross -> v; expself c->vbase c->vsft Baseline -> v; expcross -> v; expself c->vbase c->vsft -> v; expcross -> v; expself c->v; expsft c->v; expsft -> v; expcross -> v; expself c->v; expsft c->v; expsft RAW-FC LIAR-RAW AveriTec R 1.90 2.00 1.91 1.95 1.79 1.89 1.83 1.87 1.89 - 4.78 4.89 4.80 4.87 4.88 4.74 4.87 4.89 4.89 - 4.82 4.83 4.77 4.84 4.83 4.80 4.75 4.81 4.83 - 4.55 4.81 4.75 4.86 4.80 4.32 4.82 4.81 4.75 - 1.90 - 1.80 1.77 1.77 1.99 1.83 - 1.80 1. 4.48 - 4.50 4.58 4.54 4.43 4.53 - 4.55 4.54 4.60 - 4.63 4.66 4.67 4.55 4.64 - 4.63 4.63 4.65 - 4.83 4.83 4.84 4.22 4.83 - 4.82 4.82 1.21 - 1.18 1.18 - 1.10 - 1.11 1.10 1. 4.61 - 4.63 4.65 - 4.67 - 4.63 4.67 4.70 4.89 - 4.89 4.86 - 4.89 - 4.89 4.91 4.92 4.86 - 4.88 4.89 - 4.89 - 4.90 4.90 4.92 Table 8: The Macro-F1 scores for model direction experiments. Red background denotes efficiency, blue for inefficiency. Backbone Variant Direction RAW-FC LIAR-RAW AveriTec LLaMA-2 -> v; expself -> v; expcross Qwen-3 -> v; expself -> v; expcross -> stylesubstance -> truth -> base -> sft -> stylesubstance -> truth -> base -> sft -> stylesubstance -> truth -> base -> sft -> stylesubstance -> truth -> base -> sft 61.81 58.06 60.67 60.67 64.99 61.66 64.47 64. 58.86 58.79 58.88 57.86 59.39 57.85 58.35 57.85 43.06 42.91 42.70 43.33 42.77 43.95 42.93 42.85 46.53 46.73 46.64 46.79 47.13 46.86 46.57 46. 84.61 83.35 83.35 83.35 - - - - 88.21 88.02 88.02 87.51 - - - - shown in Table 7, steering shifts most regions toward lower misleadingness and higher informativeness, soundness, and readability, with readability increasing the most, up to 14% (4.22-4.83). Next, we compute the correlation matrix between model performance (F-score and accuracy) and the four explanation metrics. As shown in Figure 3, three key findings emerge: (1) F-score is strongly negatively correlated with misleadingness, strongly positively correlated with soundness, positively correlated with readability, and largely independent of informativeness. (2) Accuracy shows similar pattern, except it has mild negative correlation with informativeness. (3) Informativeness is negatively correlated with readability, but positively correlated with both misleadingness and soundness. This is intuitive: more informative explanations often introduce extra background, which improves soundness but also injects noise, leading to misleading content and drop in accuracy. Figure 2: Optimal Layer for improving pairs across different layers. Square brackets denote optional components. Figure 3: The correlation matrix between the explanation quality and fact-checking performance. Quantitative analysis. We first evaluate explanations for all improved variants, using the first-stage model as the baseline. As Qualitative analysis. Finally, we conduct case studies by rendering cosine similarities between un-refined output tokens and steering vectors in HTML. As shown in Appendix G, we can observe REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA that: red regions capture correct verdicts, while blue regions are dominated by noisy, redundant syntax patterns. Once again, this confirms that REFLEX successfully disentangles verdict factuality from explanation style."
        },
        {
            "title": "5 Conclusion\nIn this paper, we propose a straightforward yet effective self-refining\nparadigm, REFLEX, for disentangling truth into style and substance,\nwhich is better than solely direct to truth direction in fact-checking\ntask. For verdict accuracy, the semi-parametric approach can guide\nmodels without explicit explanations via activating knowledge from\nthe backbone and learning reasoning style from the post-training\nvariants. For explanation quality, it captures more sound, informa-\ntive, and readable explanation style, which have been quantitatively\nshown to teach fact-checkers more effectively than traditional meth-\nods. Further experiments demonstrate the generalizability, flexibil-\nity, and transferability of this paradigm across various scenarios. In\nthe future, we intend to research REFLEX for more general domains.",
            "content": "References [1] Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Lies Hadjadj, Emilie Devijver, and Yury Maximov. 2025. Self-training: survey. Neurocomputing 616 (2025), 128904. [2] Pepa Atanasova. 2024. Generating fact checking explanations. In Accountable and Explainable Methods for Complex Reasoning over Text. Springer, 83103. [3] Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. 2023. Taken out of context: On measuring situational awareness in llms. arXiv preprint arXiv:2309.00667 (2023). [4] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. [n. d.]. Discovering Latent Knowledge in Language Models Without Supervision. In The Eleventh International Conference on Learning Representations. [5] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567 (2025). [6] Tsun-Hin Cheung and Kin-Man Lam. 2023. Factllama: Optimizing instructionfollowing language models with external knowledge for automated fact-checking. In 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 846853. [7] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. In The Twelfth International Conference on Learning Representations. [8] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. [n. d.]. Plug and Play Language Models: Simple Approach to Controlled Text Generation. In International Conference on Learning Representations. [9] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307 (2025). [10] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904 (2024). [11] Gaurav Rohit Ghosal, Tatsunori Hashimoto, and Aditi Raghunathan. 2024. Understanding Finetuning for Factual Knowledge Extraction. In International Conference on Machine Learning. PMLR, 1554015558. [12] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. 2025. Survey on LLM-asa-Judge. arXiv:2411.15594 [cs.CL] https://arxiv.org/abs/2411.15594 [13] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. 2023. Word embeddings are steers for language models. arXiv preprint arXiv:2305.12798 (2023). [14] Evan Hernandez, Belinda Li, and Jacob Andreas. 2023. Inspecting and editing knowledge representations in language models. arXiv preprint arXiv:2304.00740 (2023). [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [16] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems 43, 2 (2025), 155. [17] Shailza Jolly, Pepa Atanasova, and Isabelle Augenstein. 2022. Generating fluent fact checking explanations with unsupervised post-editing. Information 13, 10 (2022), 500. [18] Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. 2023. Better zero-shot reasoning with role-play prompting. arXiv preprint arXiv:2308.07702 (2023). [19] Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Jiaming Zhou, and Haoqin Sun. 2024. Self-prompt tuning: Enable autonomous role-playing in llms. arXiv preprint arXiv:2407.08995 (2024). [20] Neema Kotonya and Francesca Toni. 2020. Explainable automated fact-checking for public health claims. arXiv preprint arXiv:2010.09926 (2020). [21] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative Discriminator Guided Sequence Generation. In Findings of the Association for Computational Linguistics: EMNLP 2021. 49294952. [22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 94599474. [23] Kenneth Li, Aspen Hopkins, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg. [n. d.]. Emergent World Representations: Exploring Sequence Model Trained on Synthetic Task. In The Eleventh International Conference on Learning Representations. [24] Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems 36 (2023), 4145141530. [25] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusion-lm improves controllable text generation. Advances in neural information processing systems 35 (2022), 43284343. [26] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers). 32143252. [27] Philip Lippmann and Jie Yang. 2025. Style over Substance: Distilled Language Models Reason Via Stylistic Replication. arXiv preprint arXiv:2504.01738 (2025). [28] Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated Knowledge Prompting for Commonsense Reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 31543169. [29] Jiachang Liu, Dinghan Shen, Yizhe Zhang, William Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 100114. [30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [31] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 80868098. [32] Yi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware co-attention networks for explainable fake news detection on social media. arXiv preprint arXiv:2004.11648 (2020). [33] Jing Ma, Wei Gao, Shafiq Joty, and Kam-Fai Wong. 2019. Sentence-level evidence embedding for claim verification with hierarchical attention networks. Association for Computational Linguistics. [34] Melkamu Mersha, Khang Lam, Joseph Wood, Ali Alshami, and Jugal Kalita. 2024. Explainable artificial intelligence: survey of needs, techniques, applications, and future direction. Neurocomputing 599 (2024), 128111. [35] Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy Channel Language Model Prompting for Few-Shot Text Classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 53165330. [36] Tai Nguyen and Eric Wong. 2023. In-context Example Selection with Influences. arXiv e-prints (2023), arXiv2302. [37] Yixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and verification with neural semantic matching networks. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 68596866. [38] OpenAI. 2023. Introducing ChatGPT. https://openai.com/blog/chatgpt [39] Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, and Yixuan Li. 2025. Steer LLM Latents for Hallucination Detection. arXiv preprint arXiv:2503.01917 (2025). Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan [40] Ver√≥nica P√©rez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. 2017. Automatic detection of fake news. arXiv preprint arXiv:1708.07104 (2017). [41] Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. Declare: Debunking fake news and false claims using evidence-aware deep learning. arXiv preprint arXiv:1809.06416 (2018). [42] Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 conference on empirical methods in natural language processing. 29312937. [43] John W. Ratcliff and David E. Metzener. 1988. Pattern Matching: The Gestalt Approach. Dr. Dobbs Journal 13, 7 (Jul 1988), 46. [44] Xuan Ren, Biao Wu, and Lingqiao Liu. 2024. learn better if you speak my language: Enhancing large language model fine-tuning with style-aligned response adjustments. CoRR (2024). [45] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. 2024. Steering Llama 2 via Contrastive Activation Addition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1550415522. [46] Daniel Russo, Serra Sinem Tekiroƒülu, and Marco Guerini. 2023. Benchmarking the generation of fact checking explanations. Transactions of the Association for Computational Linguistics 11 (2023), 12501264. [47] Michael Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. 2023. Averitec: dataset for real-world claim verification with evidence from the web. Advances in Neural Information Processing Systems 36 (2023), 6512865167. [48] Tal Schuster, Roei Schuster, Darsh Shah, and Regina Barzilay. 2020. The limitations of stylometry for detecting machine-generated fake news. Computational Linguistics 46, 2 (2020), 499510. [49] Jiaming Shen, Jialu Liu, Dan Finnie, Negar Rahmati, Mike Bendersky, and Marc Najork. 2023. Why is this misleading?: Detecting News Headline Hallucinations with Explanations. In Proceedings of the ACM Web Conference 2023. 16621672. [50] Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. defend: Explainable fake news detection. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 395405. [51] Satyam Shukla, Himanshu Dutta, and Pushpak Bhattacharyya. 2025. Recon, Answer, Verify: Agents in Search of Truth. arXiv preprint arXiv:2507.03671 (2025). [52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [54] Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, and Yi Chang. 2024. Explainable fake news detection with large language model via defense among competing wisdom. In Proceedings of the ACM Web Conference 2024. 24522463. [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [56] Jiaying Wu, Jiafeng Guo, and Bryan Hooi. 2024. Fake news in sheeps clothing: Robust fake news detection against LLM-empowered style attacks. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining. 33673378. [57] Lianwei Wu, Yuan Rao, Ling Sun, and Wangbo He. 2021. Evidence inference networks for interpretable claim verification. In Proceedings of the AAAI conference on artificial intelligence, Vol. 35. 1405814066. [58] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [59] Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, and Yi Chang. 2022. coarse-to-fine cascaded evidence-distillation neural network for explainable fake news detection. arXiv preprint arXiv:2209.14642 (2022). [60] Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 27332743. [61] Zeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun. 2021. Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors. arXiv preprint arXiv:2103.15949 (2021). [62] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2024. Star: Selftaught reasoner bootstrapping reasoning with reasoning. In Proc. the 36th International Conference on Neural Information Processing Systems, Vol. 1126. [63] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph Gonzalez. [n. d.]. TEMPERA: Test-Time Prompt Editing via Reinforcement Learning. In The Eleventh International Conference on Learning Representations. [64] Xuan Zhang and Wei Gao. 2023. Towards llm-based fact verification on news claims with hierarchical step-by-step prompting method. arXiv preprint arXiv:2310.00305 (2023). [65] Eric Zhao, Pranjal Awasthi, and Nika Haghtalab. 2025. From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning. arXiv preprint arXiv:2503.05919 (2025). Prompt Template Following [5, 28], the prompt template we use to conduct training and inference for claims is as follows: chat between curious human and an artificial intelligence assistant. You are fact-checking assistant. You are given claim [(along with evidence sentences)]. Your task is to label the overall veracity of the claim based on your internal knowledge [or, on the provided evidence sentences]. The Label Definitions are as follows: TRUEtrue: The claim is verified as TRUEtrue based on your knowledge. FALSEfase: The claim is verified as FALSEfalse based on your knowledge. HALF-TRUEhalf: The claim is verified as half-true due to insufficient knowledge leading to uncertainty, or because the claim itself is partially true. ##### or, for given evidence: TRUEtrue: The claim is verified as true or mostly-true by the evidence. FALSEfalse: The claim is verified as false or mostly false by the evidence. HALF-TRUEhalf: The claim is verified as half-truehalf by the evidence, or the evidence can not prove the claim. ##### or, for AveriTec: Supported: The claim is supported by the arguments and evidence presented. Refuted: The claim is contradicted by the arguments and evidence presented. Not Enough Evidence: There is not enough evidence to support or refute the claim. Based on the below claim and your own knowledge, determine the veracity of the claim. ##### or, for given evidence: Based on the below claim and evidence, determine the veracity of the claim. Please strictly output in the following format: Verdict: [label]. Explanation: [your reasoning here] Consistent with [54], the template we used to prompt ChatGPT to conduct Automatic Evaluations is shown below: You are helpful, harmless and precise assistant. Please evaluate the quality of the explanations in prediction based on four metrics: misleadingness, informativeness, REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA soundness, and readability, where 1 represented the poorest and 5 the best in addition to misleadingness. The definitions of the metrics are: (1) Misleadingness assesses whether the models explanation is consistent with the real veracity label of claim, with rating scale ranging from 1 (not misleading) to 5 (very misleading); (2) Informativeness assesses whether the explanation provides new information, such as explaining the background and additional context, with rating scale ranging from 1 (not informative) to 5 (very informative) (3) Soundness describes whether the explanation seems valid and logical, with rating scale ranging from 1 (not sound) to 5 (very sound) (4) Readability evaluates whether the explanation follows proper grammar and structural rules, and whether the sentences in the explanation fit together and are easy to follow, with rating scale ranging from 1 (poor) to 5 (excellent). Please output scores in the following JSON format: ``` {{ Misleadingness: [an integer number from 1 to 5], Informativeness: [an integer number from 1 to 5], Soundness: [an integer number from 1 to 5], Readability: [an integer number from 1 to 5] }} ``` Claim: {claim} Label: {label} Prediction: {verdict} Score: Explanation Length As Table 9 shows, on RAW-FC (aligned to LLaMA2), our outputs are shorter than L-Defense. On Liar-RAW, they are shorter than all baselines, including the oracle, demonstrating that our paradigm produces concise and accurate explanations. Table 14 further shows that EGS clearly reduces noisy patterns in model outputs across backbones and datasets. Table 9: The explanation length for ours and baselines. Method RAWFC LIAR-RAW Oracle ChatGPTfull ChatGPTclaim L-DefenseChatGPT L-DefenseLLaMA2 Ours S-EGSLLaMA2 w/o EGS 201.68 144.32 128.71 266.61 305.50 278.42 787.00 220.75 139.15 150.97 225.52 175.38 76.82 118.06 Human Evaluation Following [54], we conducted manual evaluation to obtain more reliable and comprehensive results. Ten undergraduate annotators, Table 10: The hyperparameter setup for different models. Dataset #train xy max-len grad_acc epochs qwen / llama Raw-FC 1,612 Averitec 2,873 Liar-raw 6,168 cv c;eviv cv;exp c;eviv c;eviv;exp cv c;eviv cv;exp 512 1,024 4,096 512 512 256 512 1,024 1 1 1 4 4 8 8 8 1 / 2 2 / 2 2 / 2 2 / 2 2 / 2 1 / 2 1 / 2 2 / 2 Table 11: Label distribution and order of few-shot learning on Liar-raw. denotes half, denotes false, denotes true. Model xy order k-shot split h/f/t LLaMA-2 cv;expcross fth Qwen-3 cv;expself fththf 3 test train test train 842/8/5 6112/37/19 189/521/145 1223/3844/1101 Table 12: Automatic and human evaluation of explanation quality on 30 random samples Oracle ChatGPTfull ChatGPTclaim L-DefenseLLaMA2 L-DefenseChatGPT Ours S-EGSLLaMA2 w/o EGS ChatGPT 4.77 4.50 4.67 4.43 4.43 4.17 4.67 4.50 4.60 4.40 1.53 2.07 2.33 1.87 1. 4.77 4.73 4.63 4.67 4.53 1.47 2.22 2.68 2.12 1.97 Human 3.89 3.61 3.38 3.22 2.84 2.68 3.37 3.48 3.52 3.68 3.86 3.57 3.27 3.49 3.56 1.96 1.89 4.83 4. 4.80 4.78 4.78 4.50 2.19 2.35 3.90 3.48 3.65 3.36 3.57 2. all studying at university where English is the official language, rated 30 randomly sampled test instances from RAW-FC using 5-point Likert scale. Model identities were kept anonymous, and the average scores were used as the final metric. Table 12 shows that all four dimensions have correlation coefficients above 0.70 (0.94, 0.77, 0.80, 0.73), validating LLM-As-Judges [12]. Our S-EGS outperforms others in almost all dimensions, except misleadingness, likely due to sample randomness and strong disentanglement. Training Details As Table 10 shows, except for gradient accumulation steps and maximum context length, which depend on dataset size and sample length, we trained models on 14A100-80G GPUs with learning rate of 2e-5, per-device batch size of 4, weight decay 0, warmup ratio 0.03, and cosine scheduler, bf16/tf32 precision, gradient checkpointing, and full-shard FSDP with auto wrapping. For LLaMA-2, the epoch-2 checkpoint was selected, achieving the best balance between factual accuracy and instruction following. For Qwen-3, severe overfitting occurred at epoch 2 on some LiarRAW and RAW-FC variants (training accuracy exceeded test by 2040%), so the epoch-1 checkpoint was used. Conference17, July 2017, Washington, DC, USA Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, and Yaxin Fan Table 13: Full statistics of data amount across various models and datasets. Backbone Dataset x->y LLaMA-2 RAW-FC c->v c->v; expself c->v; expcross LIAR-RAW c->v c->v; expself c->v; expcross AveriTec c; evi ->v; expself Qwen-3 RAW-FC c->v c->v; expself c->v; expcross LIAR-RAW c->v c->v; expself c->v; expcross Statistics #HC #ISC HR ISR 88 113 105 1,427 895 1,304 98 374 161 105 1,427 895 1, 231 585 517 2,437 2,207 3,600 0.1236 0.1768 0.1502 0.6731 0.3809 0.9546 0.2567 0.6012 0.5663 0.6020 0.5781 0. 1,746 0.1033 0.9423 553 502 517 2,437 2,207 3,600 0.5351 0.2268 0. 0.6731 0.3809 0.9546 0.6057 0.5565 0.5663 0.6020 0.5781 0.7497 AveriTec c; evi ->v; expself 1,746 0.1033 0.9423 Table 14: The explanation length for ablation studies before and after S-EGS for improved variants. Backbone Pair RAW-FC LIAR-RAW AveriTec LLaMA-2-7b Baseline Qwen-3-7b c->v; expcross c->v; expself c->v c->vsft Baseline c->v; expcross c->v; expself c->v c->vsft c->v; expcross c->v; expself c->v; exp c->v; expsft c->v; expcross c->v; expself c->v; exp c->v; expsft 787.55 278.42 286.63 274.69 264.64 997.04 306.78 293.92 310.00 - 118.06 - 67.29 76.82 76.5 290.03 84.88 - 83.85 83.98 23.72 - 23.16 23.38 - 24.37 - 24.41 24.23 22.89 Full Statistics To demonstrate the data efficiency of our paradigm, we reported the full statistics for reference in Table 13. Label Distribution As shown in Table 11, the two backbone models show severe recency bias for the liar-raw dataset, although it can be solved by many works [29, 31, 35, 36, 63]. Case Study The disentangling example is shown in Figure 4. REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance Conference17, July 2017, Washington, DC, USA Figure 4: The Redundancy Noise Pattern in LLaMA2 on RAW-FC, layer 10 with IV, multiplier 1.5. Red tokens denote alignment with optimal vector direction; blue denotes opposite."
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University",
        "Singapore Management University",
        "Soochow University"
    ]
}