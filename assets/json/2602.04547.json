{
    "paper_title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
    "authors": [
        "Luca Zedda",
        "Andrea Loddo",
        "Cecilia Di Ruberto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 7 4 5 4 0 . 2 0 6 2 : r OMNIRAD: RADIOLOGICAL FOUNDATION MODEL FOR MULTI-TASK MEDICAL IMAGE ANALYSIS Luca Zedda , Andrea Loddo , Cecilia Di Ruberto Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy {luca.zedda,andrea.loddo,cecilia.dir}@unica.it"
        },
        {
            "title": "ABSTRACT",
            "content": "Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation. Code: https://github.com/unica-visual-intelligence-lab/OmniRad Models: https://huggingface.co/collections/Snarcy/omnirad"
        },
        {
            "title": "Introduction",
            "content": "Radiological imaging is fundamental to clinical decision-making, yet image interpretation remains challenging due to interand intra-observer variability, subtle pathological patterns, and the scale and heterogeneity of contemporary imaging data. These challenges motivate computational methods that extract quantitative and reproducible information from radiological images. Radiomics [16] addresses this need by transforming medical images into structured quantitative descriptors encoding intensity, morphological, and textural characteristics, typically extracted from segmented regions of interest. While these handcrafted features are interpretable and clinically meaningful, classical radiomics pipelines are highly sensitive to image acquisition protocols, preprocessing strategies, and segmentation quality, limiting reproducibility and scalability in multi-center and multi-modality settings [41]. Deep learning enables data-driven feature learning directly from images, allowing convolutional and transformer-based models to capture complex anatomical structures and subtle pathological patterns. However, most deep radiomics models are trained in fully supervised and task-specific manner, requiring large annotated datasets and limiting robustness when transferred across modalities, organs, or institutions. Self-supervised learning has enabled radiological foundation models pretrained on large collections of unlabeled images [4]. These models provide transferable representations for downstream tasks such as classification and segmentation, with emerging extensions to visionlanguage modeling [42]. Despite their promise, existing radiological foundation models often lack sufficient anatomical or modality diversity, and downstream adaptation is typically performed independently for each task, resulting in fragmented feature spaces."
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 1: Schematic of the proposed OmniRad model. The framework supports multiple tasks by leveraging shared, unified radiological image foundation model. In clinical practice, radiological workflows encompass multiple related tasks, including diagnostic classification, anatomical or pathological delineation [23], and, in exploratory research settings, narrative report generation. Although these tasks are commonly addressed independently during model development, they rely on shared visual evidence and complementary information. From radiomics perspective, the use of inconsistent or task-specific representations undermines feature reuse and limits the reliability of downstream analysis, particularly in longitudinal studies or heterogeneous clinical environments. To address these limitations, we introduce OmniRad, radiological foundation model designed to learn stable and transferable radiological representations. OmniRad is pretrained using self-supervised strategy on large-scale, heterogeneous radiological data spanning multiple modalities and anatomical regions. single shared encoder is pretrained once and subsequently adapted separately to classification and segmentation tasks, and additionally evaluated in an exploratory visionlanguage setting, enabling consistent representation reuse across diverse clinical objectives without relying on joint multi-task optimization. The main contributions of this work are as follows: We propose OmniRad, radiological foundation model pretrained on heterogeneous radiological corpora using self-supervised learning framework. We introduce task-agnostic representation paradigm in which single pretrained encoder is consistently adapted to multiple radiological analysis tasks, promoting feature stability and reuse. We demonstrate consistent improvements over state-of-the-art medical foundation models across heterogeneous radiological benchmarks, highlighting the robustness and transferability of OmniRad representations. We present an exploratory proof of concept showing that the learned representations can be extended to vision-language applications without modifying the pretrained encoder."
        },
        {
            "title": "2 Related Works",
            "content": "Classical radiomics established the foundations of quantitative radiological image analysis by transforming medical images into structured and reproducible descriptors. Handcrafted features encoding intensity distributions, morphological properties, and texture patterns extracted from segmented regions of interest have been extensively used for prognosis estimation, treatment response assessment, and disease characterization. key strength of classical radiomics lies in its interpretability and alignment with clinical reasoning, as features are explicitly defined and can often be linked to known imaging biomarkers [28]. However, classical radiomics pipelines are highly sensitive to upstream design choices. Variations in image acquisition protocols, reconstruction parameters, preprocessing strategies, and segmentation quality can substantially affect feature distributions, limiting reproducibility across centers and modalities. Moreover, extending handcrafted pipelines to new organs, imaging modalities, or clinical tasks requires significant manual engineering, which constrains scalability and hinders deployment in heterogeneous radiological settings [5]. 2.1 Deep Learning-based Radiomics Deep learning has expanded the scope of radiomics by enabling data-driven feature learning directly from images. Convolutional and transformer-based architectures can encode complex anatomical structures and subtle pathological"
        },
        {
            "title": "Running Title for Header",
            "content": "patterns that are difficult to capture with handcrafted descriptors [37]. These learned representations have demonstrated strong performance across wide range of supervised radiological tasks, including disease classification and organ segmentation [11]. Despite these advances, deep radiomics models often depart from core radiomics principles. Task-specific supervised training on limited annotated datasets can lead to representations that are tightly coupled to particular datasets or imaging protocols, reducing stability and transferability. As result, features learned for one task or modality may not generalize reliably to other clinical scenarios, limiting their reuse in multi-task or longitudinal radiological workflows. 2.2 Medical Foundation Models for Radiology Self-supervised learning has enabled the emergence of medical foundation models trained on large collections of unlabeled radiological images [25]. By leveraging generic pretraining objectives, these models reduce reliance on expert annotations and provide transferable visual representations for downstream tasks such as classification and segmentation. Vision transformer backbones pretrained with modern self-supervised strategies have demonstrated particularly strong performance across radiological benchmarks [39]. However, existing radiological foundation models present several limitations when viewed through radiomics lens. Pretraining datasets are often restricted to specific modalities or anatomical regions, biasing learned representations and limiting generalization. Downstream adaptation is commonly performed independently for each task, resulting in fragmented feature spaces and inconsistent representations across classification and segmentation [13]. Properties such as feature stability under domain shifts, reproducibility across tasks, and representation reuse are rarely evaluated or explicitly encouraged during model design. 2.3 Multi-task Applicability in Radiological Workflows Radiological practice inherently involves multiple interdependent visual analysis tasks. Diagnostic workflows rely on both global image interpretation and localized anatomical or pathological delineation, requiring representations that capture shared semantic structure across classification and segmentation. While textual reporting provides complementary contextualization [17], visual analysis tasks form the core of quantitative radiological pipelines. Most existing approaches address classification and segmentation through task-specific encoders or independently pretrained models, leading to heterogeneous feature spaces across applications. Although this paradigm can yield strong performance on individual benchmarks, it limits the reuse of learned representations and reduces consistency across tasks [35, 40]. From radiomics perspective, such fragmentation undermines feature stability and complicates downstream analysis, particularly in longitudinal or multi-center settings where consistent representations are essential. Recent foundation models aim to mitigate these limitations by learning general-purpose visual representations that can be adapted to multiple radiological tasks. However, downstream adaptation is typically performed separately for classification [43, 19] and segmentation [6], without mechanisms to explicitly encourage representation consistency across tasks. As result, the extent to which learned features remain stable and reusable across diverse radiological objectives remains an open question. These observations motivate approaches that emphasize shared, task-agnostic radiological encoder, pretrained once and consistently reused across heterogeneous visual analysis tasks [22]. Such paradigm aligns closely with radiomics principles by prioritizing representation stability and transferability, while avoiding the complexity and potential optimization conflicts associated with joint multi-task training. 2.4 Open Challenges The evolution from handcrafted radiomics to deep learning and foundation models has significantly advanced quantitative radiological analysis. However, several challenges remain unresolved. Current approaches often lack sufficient modality and anatomical diversity during pretraining, rely on task-specific adaptation strategies, and do not explicitly enforce radiomics-oriented properties such as feature stability and representation reuse. Moreover, the dominant focus on isolated downstream tasks underutilizes shared structure within radiological data [31]. These limitations highlight the need for unified radiological foundation models that integrate radiomics principles with self-supervised learning and support coherent multi-task adaptation. Such models are essential for developing robust, transferable, and clinically reliable visual representations suitable for real-world radiological workflows [38]. OmniRad is proposed to mitigate these critical open challenges."
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 2: Overview of the proposed OmniRad framework for radiological analysis tasks. The OmniRad Image Encoder is shown in blue, the classification branch in purple, the segmentation modules in red, and an exploratory captioning branch in yellow. Feature map dimensionalities are highlighted in pink to illustrate the intermediate representations."
        },
        {
            "title": "3 Method",
            "content": "The proposed approach for imagebased radiological tasks is outlined in this section. In section 3.1, we introduce the radiological pretraining strategy adopted to develop the OmniRad Image Encoder. In section 3.2, we describe the adaptation of the encoder to global classification tasks. In section 3.3, we present the extension of OmniRad to dense prediction tasks, with focus on radiological segmentation. Finally, in section 3.4, we provide an exploratory evaluation of the learned representations by employing OmniRad as frozen visual encoder in proof-of-concept radiological image captioning setting. The overall architecture is illustrated in fig. 2. The OmniRad Image Encoder is shown in blue, the classification branch in purple, the segmentation modules in red, the feature map dimensionalities in pink, and the exploratory captioning branch in yellow. 3.1 OmniRad Pretraining OmniRad acts as strong visual encoder for radiological images, primarily due to the adopted selfsupervised pretraining strategy. Previous studies [39] explored the application of DINO [1] and DINOv2 [24] for radiological pretraining. However, the direct use of DINOv2 remained an open problem due to suboptimal downstream performance. OmniRad is pretrained on the RadImageNet[21] dataset, which provides largescale and diverse radiological imagery covering multiple anatomical regions and imaging modalities, while providing clean and structured data source for high-quality representation learning. We further modify the original DINOv2 formulation by retaining only global crops and completely removing local crops, following the design principles introduced in [12]. This design choice significantly improves training stability and downstream transferability, improving upon [39] and speedups approximately by 2 the whole training process due to less required computation. The training procedure exhibited improved stability and did not require model rollbacks or interventions to mitigate feature collapse, which remains common issue in selfsupervised learning frameworks. Both the small and basevariants"
        },
        {
            "title": "Running Title for Header",
            "content": "Hyperparameter small models Base models Student architecture Batch size per GPU Patch size Drop path rate (student) Layer scale Epochs Base learning rate Min learning rate Weight decay (start / end) Optimizer Teacher momentum (start end) Warmup teacher temperature Teacher temperature DINO loss weight iBOT loss weight DINO/iBOT prototypes DINO/iBOT bottleneck dim DINO/iBOT head layers DINO/iBOT head hidden dim Global crop size VIT small 256 VIT base 196 14 0.3 1 105 10 2e4 1e6 0.04 / 0.2 AdamW 0.994 1.0 0.04 0.07 1.0 1.0 131072 256 3 2048 Table 1: Main training hyperparameters for the small and base OmniRad Models. are trained using shared hyperparameter configuration, based on ViT-S and ViT-B backbones, respectively [3]. The complete set of pretraining hyperparameters is reported in table 1. 3.2 Training configuration for radiological classification All classification experiments are conducted using both the OmniRad small and basemodels to ensure fair and consistent comparisons with baseline methods. The training procedure follows standard supervised learning with crossentropy loss. All classification experiments are conducted using input images resized to 224 224 pixels and an effective batch size of 128 per GPU. Training is performed for 40 epochs using AdamW with learning rate of 1 105, weight decay of 0.01, and linear warmup over the first 10 epochs. Gradient clipping with maximum norm of 1.0 and standard data augmentations, including random flips, rotations, and resized crops, are applied. 3.3 Adapting OmniRad for dense tasks Vision Transformers are widely recognized as key architectures for pretraining strategies due to their straightforward scalability to billions of parameters [32]. Such extreme scaling, however, requires particular care. major contributor to their usability is the singlescale representation, which enables multiple pretraining approaches such as masked reconstruction objectives [7], thereby simplifying the pretraining process. Despite their strong performance on global tasks such as classification, this design becomes limiting for dense prediction tasks, including detection and segmentation, where hierarchical representations are typically employed [10, 34]. Several adapterbased approaches [2, 9] have therefore been proposed to bridge this gap. ViTAdapter introduces substantial computational overhead, resulting in GFLOPs that are impractical for highthroughput radiological workflows. In contrast, the approach of [9] focuses on efficient dense adaptation through explicitly lightweight designs. Following this design philosophy, our method incorporates parallel convolutional branch that enables efficient adaptation to dense radiological segmentation. The convolutional branch consists of 33 convolutions with stride 2, producing lightweight hierarchical representation ranging from 1/8 to 1/32 of the original image resolution. During dense adaptation, the OmniRad Image Encoder is kept frozen, and only the segmentationspecific modules are optimized. Spatial-aware Token Adapter and Hybrid Encoder. Intermediate token features are extracted from transformer layers = {3, 7, 11} and reshaped into spatial feature maps. Earlier layers are selected for higherresolution feature maps due to their stronger local spatial sensitivity [26]."
        },
        {
            "title": "Running Title for Header",
            "content": "Let Tℓ RBN denote the token embeddings at layer ℓ, which are rearranged into Vℓ RBDH/14W/14 and aligned to convolutional priors Cs at scales {8, 16, 32}. Decoder design. The decoding pathway constitutes our main architectural contribution for dense radiological segmentation. Two lightweight upsampling blocks are employed, each performing 2 bilinear interpolation followed by two 3 3 convolutional layers with batch normalization and ReLU activations, and concatenating the upsampled features with those from the corresponding finer scale. The decoding process is defined as U16 = ψ(Up(F32) F16) , U8 = ψ(Up(U16) F8) , followed by 1 1 convolution producing the final segmentation logits at resolution H/8 W/8. All segmentation experiments are conducted using both the OmniRad small and base models to ensure consistent and fair comparisons with baseline methods. Input images are resized to 448 448 pixels with patch size of 14. lightweight decoder with two upsampling stages produces predictions at 1/8 resolution. Models are trained for 20 epochs using AdamW with learning rate of 1 104, weight decay of 1 104, batch size 16, and mixedprecision training. The proposed decoder architectures for the small and base configurations include 14.27 and 69.76 trainable parameters, respectively. 3.4 Exploratory adaptation of OmniRad for image captioning We explore the use of OmniRad as frozen visual encoder in proof-of-concept radiological image captioning setting, using an image-language modeling formulation based on BART. For all captioning experiments, the OmniRad basemodel is employed to maintain consistency with the hidden dimensionality of the language model. The OmniRad Image Encoder is kept frozen, while the language modeling components are trained endtoend. For all captioning experiments, we use only the OmniRad basemodel. This choice is motivated by its maximal feature capacity among our available models, while matching the hidden size of BART-base(Dl = 768). Using BART-large (Dl = 1024) would have increased model complexity without clear benefits, making it computationally unnecessary for our setup. Visionlanguage projection Given an input image R3HW , the frozen OmniRad backbone produces token embeddings RBN Dv , = (cid:19)2 , (cid:18) 14 with Dv = 768 for the basevariant. These features are linearly projected into the BART embedding space = LN(WpT) , Wp RDvDl , where Dl = 768 for BART-base. Merging visual tokens for captioning The BART encoder imposes fixed and moderate input sequence length. To address this implementation constraint of the HuggingFace BART architecture, we introduce Patch Merger[27] module that compresses the projected OmniRad tokens into compact set of latent visual tokens, with = 64 in all experiments. The Patch Merger is implemented as cross-attention pooling mechanism with learnable queries RKDl . Given the projected visual tokens Z, the pooled representation is computed as yielding RBKDl, which acts as visual prefix for the language model. = softmax (cid:18) QZ Dl (cid:19) , = AZ, Language decoding The pooled visual tokens are provided to the BART encoder as input embeddings, while the BART decoder generates captions autoregressively. The training objective corresponds to the standard autoregressive language modeling loss Lcap = log p(yt y<t, P), (cid:88) where yt denotes the caption tokens. All captioning experiments are trained for 20 epochs using AdamW with learning rate of 5 105 and mixedprecision training. An effective batch size of 64 is obtained through gradient accumulation."
        },
        {
            "title": "Running Title for Header",
            "content": "During inference, captions are generated using beam search with 5 beams and maximum length of 64 tokens. This experiment is intended to assess the semantic alignment of the learned visual representations with radiological language, rather than to provide clinically validated report generation system."
        },
        {
            "title": "4 Experimental Evaluation",
            "content": "In this section, we present the methodological evaluation of the proposed OmniRad foundation model across all considered tasks. Specifically, in section 4.2 we report classification results on five radiological benchmark datasets. In section 4.4, we present segmentation results obtained by adapting OmniRad to dense prediction tasks, covering both binary and multiclass segmentation across eight benchmark datasets. Finally, in section 4.5, we report an exploratory set of experiments using BART-based [18] framework to assess the suitability of OmniRad representations for radiological image captioning. Our experiments were conducted on workstation with 2 A100 80gb, with 5 iterations for each experiment to produce fair comparison. 4.1 Datasets The pretraining and evaluation pipeline relies on complementary datasets covering large-scale visual pretraining, image-level classification, dense segmentation, and multimodal imagetext learning. This design enables the assessment of representation transfer across heterogeneous medical imaging tasks. For all datasets, we used official train, val, and test splits to encourage comparisons and evaluation fairness. We report full list and brief description of the used dataset in table 2. Pretraining dataset. Large-scale visual pretraining is performed using RadImageNet [21], curated collection of radiological images spanning multiple modalities, including CT, MR, and ultrasound. RadImageNet provides over one million images annotated across 165 classes, covering diverse anatomical regions and pathologies. Its scale and heterogeneity make it suitable for learning robust, modality-agnostic visual representations that generalize well to downstream tasks. Classification datasets. For image-level classification evaluation, we use representative benchmarks from the MedMNISTv2 [36] family and breast ultrasound datasets. Specifically, PneumoniaMNIST and BreastMNIST provide binary classification tasks on chest X-ray and breast ultrasound images, respectively, while OrganAMNIST, OrganCMNIST, and OrganSMNIST offer multi-class organ classification from abdominal CT scans. These datasets feature standardized splits and controlled image resolutions, enabling reliable and reproducible evaluation of classification performance across different anatomical regions and modalities. Segmentation datasets. For dense prediction and pixel-level evaluation, we adopt MedSegBench [14], comprehensive benchmark for medical image segmentation. MedSegBench aggregates 35 datasets comprising more than 60,000 images across diverse modalities such as ultrasound, MRI, CT, X-ray, OCT, dermoscopy, endoscopy, fundus imaging, and microscopy. It includes both binary and multi-class segmentation tasks with up to 19 classes, standardized train, validation, and test splits, and unified preprocessing at multiple spatial resolutions, allowing systematic assessment of segmentation robustness across heterogeneous clinical settings. For our experiments, we selected only the radiologyrelated datasets that compose the MedSegBench collection. Exploratory captioning and multimodal datasets. To evaluate visualsemantic alignment, we use ROCOv2 [29], large-scale multimodal dataset consisting of radiological images paired with textual captions and medical concepts extracted from the PMC Open Access Subset. ROCOv2 contains nearly 80,000 imagetext pairs spanning multiple radiological modalities. 4.2 Classification Results We evaluate OmniRad using Accuracy, F1 score, and AUC, as summarized in Tables 3 to 7. Overall performance OmniRad consistently achieves the strongest or nearstrongest classification performance across all evaluated datasets. On BreastMNIST, OmniRad small attains the highest F1 score (89.42%), outperforming all convolutional, transformer, and foundation model baselines, including Radio DINO base. On PneumoniaMNIST, OmniRad small establishes the best and F1 score (94.85%), confirming strong generalization on chest radiographs. Based on size only, both datasets are one order of magnitude smaller compared to the other selected for classification, showcasing the high adaptability of OmniRad to limited data sources. Multiorgan CT benchmarks"
        },
        {
            "title": "Running Title for Header",
            "content": "Dataset RadImageNet PneumoniaMNIST BreastMNIST OrganAMNIST OrganCMNIST OrganSMNIST Modality CT / MR / Ultrasound Chest X-Ray Breast Ultrasound Abdominal CT Abdominal CT Abdominal CT X-Ray MRI Prostate Ultrasound Kidney PandentalMSBench Promise12MSBench USforKidneyMSBench UltrasoundNerveMSBench Ultrasound Neck Ultrasound Breast BusiMSBench Chest X-Ray CovidQUExMSBench Ultrasound Fetal head, pubic symphysis FHPsAOPMSBench CT Lung MosMedPlusMSBench Task Pretrain Classification Classification Classification Classification Classification Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Classes Total samples Splits (train/val/test) 165 1,350,000 1,080,000 / 135,000 / 135,000 2 2 11 11 2 2 2 2 2 2 3 2 5,856 780 58,830 23,583 25,211 116 1,473 4,586 2,323 647 2,913 4,000 2,729 4,708 / 524 / 624 546 / 78 / 156 34,561 / 6,491 / 17,778 12,975 / 2,392 / 8,216 13,932 / 2,452 / 8,827 81 / 11 / 24 1,031 / 147 / 295 3,210 / 458 / 918 1,651 / 223 / 449 452 / 64 / 131 1,864 / 466 / 583 2,800 / 400 / 800 1,910 / 272 / 547 ROCOv Multiple Captioning N/A 79,789 59,958 / 9,904 / 9,927 Table 2: Overview of datasets used in experiments. For each dataset, we report imaging modality, primary task, number of target classes where applicable, total image count, and the train/validation/test splits used. Model Accuracy (%) F1 (%) AUC (%) 83.3 84.2 80.3 83.1 86.1 86.3 86.92 1.04 87.17 89.6 89.7 88.3 74.57 4.17 83.12 0.98 73.50 2.67 69.02 7.59 88.03 2.06 89.74 3.63 91.67 0.68 90.38 1.84 91.83 0.32 89.96 3.29 Table 3: Performance comparison of different models on the BreastMNIST dataset. The table reports accuracy, F1, and AUC, with an indication of the standard deviation when available. The best results are emphasized in bold. Resnet18 [36] Resnet50 [36] auto-sklearn [36] AutoKeras [36] Google AutoML Vision [36] DenseNet121 [33] Swin Transformer [30] R-LLM [15] Med ViT tiny [20] Med ViT small [20] Med ViT base [20] DINO small DINO base DINOv2 small DINOv2 base DINOv3 small DINOv3 base Radio DINO small Radio DINO base OmniRad small OmniRad base 89.1 86.6 83.6 87.1 91.9 90.1 86.26 1.38 88.23 93.4 93.8 92.9 71.78 4.41 83.16 1.05 67.28 7.19 63.22 13.01 89.28 2.49 94.12 1.26 95.55 1.55 95.51 1.7 95.56 1.00 93.70 0.32 - - - - - - - - - - - 59.14 7.56 75.48 1.77 50.63 6.57 49.07 6.35 83.82 3.05 87.22 4.02 88.98 1.0 87.69 3.5 89.42 0.18 87.32 3.72 Across OrganAMNIST, OrganCMNIST, and OrganSMNIST, OmniRad consistently achieves the highest F1 scores, outperforming both Radio DINO and DINOv3. Specifically, on OrganAMNIST, OmniRad small reaches 97.30% F1, exceeding DINOv3 base 97.28% and Radio DINO base 97.20%; on OrganCMNIST, OmniRad base attains 95.45% F1, improving over DINOv3 small 94.74% by +0.71%; and on OrganSMNIST, OmniRad base achieves 80.97% F1, surpassing DINOv3 small 78.92% F1 by +2.05% and Radio DINO base 78.15% F1 by +2.82%. While DINOv3 benefits from pretraining on much larger, non-radiological corpus, these results show that specialized foundation model can yield superior performance on medical imaging benchmarks under the same training configuration. Model capacity effects Consistent trends are observed with respect to model scale. The small variant dominates on datasets characterized by limited anatomical diversity and visually localized structures, such as BreastMNIST, PneumoniaMNIST, and OrganAMNIST. In contrast, the base variant consistently outperforms on OrganCMNIST and OrganSMNIST, where higher anatomical heterogeneity and richer class distributions benefit from increased representational capacity. Globally OmniRad establishes the strongest classification performance across heterogeneous MedMNIST benchmarks, with consistent improvements over existing foundation models and prior domainspecific architectures, confirming the effectiveness of its radiological pretraining strategy for generalpurpose radiological recognition."
        },
        {
            "title": "Running Title for Header",
            "content": "Model Accuracy (%) F1 (%) AUC (%) 86.4 88.4 85.5 87.8 94.6 87.7 91.54 0.48 93.91 94.9 96.1 92.1 83.71 1.44 90.70 2.00 81.04 1.73 79.49 0.48 93.06 0.74 93.91 0.68 91.83 1.03 93.91 1.11 95.30 1.09 94.66 0.33 Table 4: Performance comparison of different models on the PneumoniaMNIST dataset. The table reports accuracy, F1, and AUC, with standard deviation when available. The best results are emphasized in bold. Resnet18 [36] Resnet50 [36] auto-sklearn [36] AutoKeras [36] Google AutoML Vision [36] GCNN-EC [33] Swin Transformer [30] R-LLM [15] Med ViT tiny [20] Med ViT small [20] Med ViT base [20] DINO small DINO base DINOv2 small DINOv2 base DINOv3 small DINOv3 base Radio DINO small Radio DINO base OmniRad small OmniRad base - - - - - - - - - - - 80.95 2.14 89.46 2.41 77.75 2.21 75.41 0.66 92.27 0.86 93.26 0.78 90.86 1.27 93.29 1.31 94.85 1.25 94.15 0.39 95.6 96.2 94.2 94.7 99.1 95.4 98.22 0.37 98.01 99.3 99.5 99.1 93.89 0.79 98.52 1.13 92.21 1.19 91.72 0.13 99.26 0.09 98.65 0.11 98.86 0.23 98.93 0.45 99.31 0.32 99.36 0.12 Model Accuracy (%) F1 (%) AUC (%) 95.1 94.7 76.2 90.5 88.6 93.5 95.22 93.1 92.8 94.3 96.45 0.58 96.79 0.44 95.07 0.42 94.56 1.13 97.06 0.21 97.61 0.42 96.83 0.37 97.35 0.55 97.62 0.39 97.14 0.51 Table 5: Performance comparison of different models on the OrganAMNIST dataset. The table reports accuracy, F1, and AUC, with standard deviation when available. The best results are emphasized in bold. Resnet18 [36] Resnet50 [36] auto-sklearn [36] AutoKeras [36] Google AutoML Vision [36] DenseNet121 [33] R-LLM [15] Med ViT tiny [20] Med ViT small [20] Med ViT base [20] DINO small DINO base DINOv2 small DINOv2 base DINOv3 small DINOv3 base Radio DINO small Radio DINO base OmniRad small OmniRad base - - - - - - - - - - 95.93 0.60 96.39 0.61 94.55 0.41 94.19 1.13 96.83 0.30 97.28 0.47 96.47 0.36 97.20 0.61 97.30 0.32 96.98 0.55 99.8 99.8 96.3 99.4 99.0 99.7 99.78 99.5 99.6 99.7 99.88 0.03 99.87 0.04 99.83 0.01 99.76 0.08 99.87 0.02 99.88 0.09 99.92 0.03 99.93 0.03 99.95 0.05 99.89 0.03 4.3 Classification Ablation Study: Head-Only and LoRA Fine-Tuning To better understand the impact of different fine-tuning strategies on OmniRad base, we performed an ablation study across the MedMNIST classification benchmarks. We compare three configurations: full fine-tuning of all parameters (reported in Tables 3 to 7), head-only fine-tuning with the backbone frozen, and LoRA adaptation with two parameter settings. Table 8 reports the resulting F1 scores with standard deviations. Head-only fine-tuning consistently achieves strong performance, approaching the original full fine-tuning results, while LoRA [8] variants provide slightly lower F1 scores. This demonstrates that even with frozen backbone, careful adaptation can retain much of the models capability, although full fine-tuning remains optimal for maximizing classification performance."
        },
        {
            "title": "Running Title for Header",
            "content": "Model Accuracy (%) F1 (%) AUC (%) 92.0 91.1 82.9 87.9 87.7 90.5 90.1 91.6 92.2 93.67 0.26 94.28 1.44 91.37 0.49 85.73 1.36 95.31 0.32 95.18 0.90 94.3 0.31 95.11 0.71 95.49 0.78 96.02 0.58 Table 6: Performance comparison of different models on the OrganCMNIST dataset. The table reports accuracy, F1, and AUC, with standard deviation when available. The best results are emphasized in bold. Resnet18 [36] Resnet50 [36] auto-sklearn [36] AutoKeras [36] Google AutoML Vision [36] EfficientNetB0 [33] Med ViT tiny [20] Med ViT small [20] Med ViT base [20] DINO small DINO base DINOv2 small DINOv2 base DINOv3 small DINOv3 base Radio DINO small Radio DINO base OmniRad small OmniRad base - - - - - - - - - 92.91 0.36 94.06 1.62 89.83 0.72 82.61 2.20 94.74 0.30 94.46 1.10 93.63 0.25 94.57 0.82 94.86 0.89 95.45 0.62 99.4 99.3 97.6 99.0 98.8 99.2 99.1 99.3 99.4 99.67 0.16 99.72 0.05 99.38 0.28 98.84 0.21 99.78 0.03 99.79 0.01 99.8 0.02 99.86 0.04 99.83 0.03 99.87 0.04 Model Accuracy (%) F1 (%) AUC (%) 77.8 78.5 67.2 81.3 74.9 81.3 78.9 80.5 80.6 80.11 1.51 82.44 0.44 77.87 0.62 73.97 1.66 83.66 0.51 82.65 1.64 82.3 0.81 82.81 0.96 84.42 0.33 85.28 0.42 Table 7: Performance comparison of different models on the OrganSMNIST dataset. The table reports accuracy, F1, and AUC, with standard deviation when available. The best results are emphasized in bold. Resnet18 [36] Resnet50 [36] auto-sklearn [36] AutoKeras [36] Google AutoML Vision [36] Resnet18 [33] Med ViT tiny [20] Med ViT small [20] Med ViT base [20] DINO small DINO base DINOv2 small DINOv2 base DINOv3 small DINOv3 base Radio DINO small Radio DINO base OmniRad small OmniRad base 97.4 97.5 94.5 97.4 96.4 97.4 97.2 98.7 97.3 97.97 0.26 97.21 0.08 97.62 0.12 97.01 0.31 98.16 0.15 98.00 0.12 98.27 0.12 98.28 0.15 98.15 0.14 98.33 0.07 - - - - - - - - - 74.66 1.35 77.59 0.27 71.82 0.80 66.70 2.17 78.92 0.86 78.44 1.27 77.73 1.2 78.15 1.17 79.91 0.44 80.97 0.45 Dataset Full Head-Only LoRA (R=8, α=16) LoRA (R=16, α=32) 87.32 3.72 BreastMNIST PneumoniaMNIST 94.15 0.39 96.98 0.55 OrganAMNIST 95.45 0.62 OrganCMNIST 80.97 0.45 OrganSMNIST 86.54 3.64 93.03 0.48 96.03 0.61 94.52 0.68 79.81 0.50 85.01 3.68 91.51 0.56 95.34 0.63 93.62 0.72 78.23 0.55 84.44 3.75 90.84 0.61 94.72 0.70 92.91 0.78 77.52 0. Table 8: Ablation study of OmniRad base on MedMNIST benchmarks. F1 scores with standard deviations are reported for full fine-tuning, head-only fine-tuning, and LoRA adaptation with two rank and scaling settings. Best scores in each row are highlighted in bold. 4.4 Segmentation Results We evaluate the dense prediction capability of OmniRad, reporting performance in terms of mIoU, Dice coefficient, and F1, as summarized in tables 9 and 10 and in the aggregated comparison in Table 11."
        },
        {
            "title": "Running Title for Header",
            "content": "Model BusiMSBench CovidQUExMSBench FHPsAOPMSBench MosMedPlusMSBench RN-18[14] RN-50[14] EN[14] MN-v2[14] DN-121[14] 57.80 54.70 62.40 56.50 61.50 VIT small DINO small DINOv2 small DINOv3 small Radio DINO small OmniRad small 73.81 0.76 73.14 1.78 75.23 1.54 71.84 0.33 75.79 0.75 77.04 0. VIT base CLIP base MAE base DINO base DINOv2 base DINOv3 base Radio DINO base OmniRad base 75.62 1.49 70.64 0.85 76.85 1.15 77.18 0.46 75.24 1.40 68.68 0.58 76.57 1.21 77.53 1.36 Baselines 62.70 62.00 63.30 63.10 64.70 Small models 82.57 0.10 82.87 0.22 82.87 0.10 82.50 0.17 83.07 0.13 83.19 0. Base models 82.91 0.25 80.99 0.52 83.61 0.18 83.71 0.14 83.34 0.17 80.34 0.43 83.67 0.40 83.77 0.13 92.90 92.30 92.70 92.70 92.80 90.98 0.06 91.01 0.05 91.16 0.10 91.13 0.13 91.32 0.10 91.20 0.05 91.40 0.05 91.26 0.05 91.54 0.15 91.55 0.05 91.52 0.03 91.16 0.02 91.52 0.10 91.53 0.05 Table 9: mIoU (%) on Group 1 datasets 67.40 68.20 67.40 67.90 68.60 84.53 0.20 84.61 0.27 84.51 0.14 84.66 0.15 85.15 0.10 84.87 0.29 85.37 0.20 84.21 0.32 85.62 0.09 85.65 0.24 85.73 0.07 84.67 0.25 85.80 0.21 86.19 0.01 Model PandentalMSBench Promise12MSBench USforKidneyMSBench UltrasoundNerveMSBench RN-18[14] RN-50[14] EN[14] MN-v2[14] DN-121[14] VIT small DINO small DINOv2 small DINOv3 small Radio DINO small OmniRad small VIT base CLIP base MAE base DINO base DINOv2 base DINOv3 base Radio DINO base OmniRad base 92.60 92.60 91.90 90.70 93.20 91.28 0.34 91.56 0.36 91.31 0.43 90.64 0.33 91.67 0.31 91.16 0.23 91.16 0.41 89.38 0.61 92.86 0.14 92.23 0.09 92.40 0.57 90.33 0.48 92.17 0.20 91.66 0.22 Baselines 82.80 81.70 82.10 82.70 83.20 Small models 91.45 0.19 91.17 0.05 91.97 0.33 91.61 0.11 92.50 0.15 92.46 0.19 Base models 91.81 0.30 90.09 0.42 93.02 0.08 92.70 0.13 93.02 0.15 91.54 0.47 92.80 0.08 93.25 0.45 96.00 95.80 96.30 96.10 96. 96.96 0.03 96.92 0.03 96.88 0.01 97.07 0.04 97.01 0.01 96.95 0.01 97.11 0.09 96.98 0.02 97.16 0.03 97.13 0.04 97.12 0.05 96.94 0.06 97.11 0.02 97.25 0.02 Table 10: mIoU (%) on Group 2 datasets 67.10 66.40 67.50 66.00 67.60 81.60 0.11 81.29 0.34 81.33 0.24 81.53 0.28 81.85 0.60 81.83 0.13 82.30 0.12 81.64 0.19 82.19 0.23 82.19 0.43 82.20 0.40 81.92 0.02 82.61 0.22 82.26 0. Overall performance. Across all benchmarks, OmniRad consistently achieves the strongest aggregated segmentation performance among all evaluated foundation models. In particular, the base variant attains the highest average mIoU 87.93%, Dice 92.95%, and F1 score 93.03%, establishing OmniRad as the optimal choice within this unified experimental setting. Compared to Radio DINO base, which represents the strongest competing baseline, OmniRad base yields systematic improvements across all major metrics, demonstrating that the proposed radiomicsoriented pretraining strategy produces more transferable and semantically coherent dense representations."
        },
        {
            "title": "Running Title for Header",
            "content": "Datasetlevel behavior. OmniRad base achieves top performance across multiple benchmarks, including BusiMSBench, CovidQUExMSBench, and MosMedPlusMSBench, reaching peak mIoU of 86.19% on MosMedPlusMSBench. It also leads on Promise12MSBench and USforKidneyMSBench with 93.25% and 97.25% mIoU, respectively. These results span CT, MRI, and ultrasound, demonstrating that OmniRad captures modalityinvariant radiological structures while preserving fine-grained anatomical boundaries. Model capacity effects and generalization Consistent with the trends observed in classification, the base variant generally outperforms the small model on largescale and anatomically complex datasets, where increased representational capacity benefits dense anatomical delineation. Nevertheless, the small variant remains highly competitive and often surpasses existing foundation models, highlighting that OmniRad maintains strong transferability even in compact configurations. The systematic improvements observed across all segmentation benchmarks confirm that OmniRad learns radiomicsaware visual representations that generalize effectively to dense prediction tasks across heterogeneous clinical imaging domains, supporting its role as universal radiological foundation model for segmentationdriven clinical pipelines. Model mIoU (%) Dice (%) F1 (%) VIT small DINO small DINOv2 small DINOv3 small VIT base DINO base MAE base DINOv2 base DINOv3 base CLIP base 86.65 7.05 86.57 7.23 86.91 6.79 86.37 7. 87.21 6.60 87.79 6.35 87.86 6.55 87.57 6.89 85.70 8.44 85.65 7.74 92.03 4.85 91.99 4.99 92.24 4.61 91.84 5.24 92.47 4.42 92.87 4.17 92.88 4.30 92.65 4.66 91.30 6.06 91.36 5.44 92.13 4.73 92.09 4.84 92.33 4.50 91.97 5.06 92.55 4.32 92.96 4.06 92.97 4.18 92.77 4.48 91.49 5.75 91.47 5.31 Radio DINO small Radio DINO base 87.30 6.68 87.78 6.46 92.54 4.40 92.86 4.23 92.63 4.29 92.93 4.15 OmniRad small OmniRad base 87.34 6.34 87.93 6.30 92.57 4.19 92.95 4. 92.65 4.11 93.03 4.03 Table 11: Segmentation performance comparison. Mean standard deviation are reported for mIoU, Dice and F1. Best results are highlighted in bold. Qualitative results Segmentation results highlight the strengths of OmniRad. Figure 3 presents representative examples, showing that OmniRad often produces tighter predictions with minimal overor under-segmentation, while also illustrating occasional failure cases, such as on the ParadentalMSBench dataset. Compared to general radiological foundation models, DINOv3 exhibits higher rate of missed positives, emphasizing the benefits of specialized, radiology-focused models. 4.5 Exploratory Captioning Results We perform an exploratory evaluation of OmniRad as frozen visual encoder on the ROCOv2 benchmark using unified BARTbased decoding framework and fixed experimental protocol across all encoders. Performance is reported in terms of METEOR, BLEU, BLEU1, BLEU4, and ROUGEL, as summarized in Table 12. Overall performance. OmniRad base consistently achieves the strongest captioning performance across all evaluated metrics and decoding configurations. With beam size 5 and maximum generation length of 64 tokens, OmniRad base reaches the highest BLEU score 2.97, BLEU1 19.39, BLEU4 0.54, and ROUGEL 17.48, while also achieving the highest METEOR score 22.45 when increasing the generation length to 128 tokens. These results indicate that OmniRad provides strong visual representations for radiological image captioning within this unified and controlled evaluation framework. Comparison with existing foundation models. Compared to CLIP, DINOv2, DINOv3, and Radio DINO, OmniRad consistently produces captions with higher automatic metric scores under the same decoding configuration, yielding systematic improvements across all major metrics."
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 3: Segmentation visualization. Colors indicate prediction quality: green = correct prediction, blue = overprediction, red = missed prediction. Image encoder beams max_tokens METEOR BLEU BLEU-1 BLEU-4 ROUGE-L CLIP base CLIP base CLIP base CLIP base DINOv2 base DINOv2 base DINOv2 base DINOv2 base DINOv3 base DINOv3 base DINOv3 base DINOv3 base Radio DINO base Radio DINO base Radio DINO base Radio DINO base OmniRad base OmniRad base OmniRad base OmniRad base 1 4 5 5 1 4 5 5 1 4 5 5 1 4 5 5 1 4 5 5 64 64 64 128 64 64 64 128 64 64 64 128 64 64 64 128 64 64 64 19.59 21.51 21.57 21.76 20.10 21.90 22.10 22.00 19.99 21.97 21.99 22.21 18.77 21.26 21.25 21.25 20.42 22.25 22.33 22.45 2.43 2.82 2.89 2.87 2.49 2.87 2.93 2.90 2.31 2.85 2.91 2.87 2.08 2.65 2.68 2.68 2.57 2.93 2.97 2.95 16.81 18.42 18.64 18.45 17.10 18.70 18.80 18.80 16.90 18.66 18.82 18.49 16.29 18.30 18.42 18.42 17.40 19.18 19.39 19.31 0.41 0.52 0.55 0.54 0.42 0.52 0.54 0.53 0.35 0.51 0.54 0.53 0.30 0.46 0.47 0.47 0.44 0.53 0.54 0.53 15.43 16.63 16.73 16.65 15.82 16.90 16.90 16.95 15.52 16.90 16.96 16.82 15.14 16.38 16.49 16.49 15.98 17.30 17.48 17.47 Table 12: Captioning benchmark. Notably, while DINOv2 and DINOv3 exhibit competitive performance under optimized decoding configurations, OmniRad maintains superior BLEU, ROUGEL, and METEOR scores across all beam sizes, indicating more stable and transferable representation for language grounding in radiological imagery. Effect of decoding configuration."
        },
        {
            "title": "Running Title for Header",
            "content": "Increasing the beam size from 1 to 5 leads to consistent improvements across all encoders, confirming the benefit of enhanced search during sequence generation. OmniRad shows particularly strong robustness to decoding variations, maintaining high captioning quality when increasing the maximum generation length, which further supports its ability to encode longrange anatomical and pathological semantics. The consistent performance of OmniRad across decoding configurations suggests that its radiological representations support effective visualsemantic alignment in an exploratory captioning setting, complementing the gains observed in classification and segmentation tasks. Figure 4: UMAP visualization of the latent representations learned by OmniRad, RadioDINO, and DINOv3. OmniRad exhibits reduced batch effects and more structured embedding space."
        },
        {
            "title": "5 Discussion",
            "content": "The experimental results position OmniRad as reliable pretrained visual backbone for wide range of radiological tasks. While this study focuses on classification and segmentation, with an exploratory evaluation of image captioning, the consistency of the observed gains suggests that the learned representations may also benefit additional radiological applications not explicitly explored in this work. Captioning results should therefore be interpreted as proxy for visual-semantic alignment rather than fully optimized report generation system. central contribution of OmniRad lies in demonstrating how architectural adaptations can address known limitations of plain transformer backbones in dense prediction settings. In particular, the proposed segmentation branch enables multi-scale feature extraction and allows OmniRad to remain competitive with, and often outperform, established UNetbased architectures across heterogeneous segmentation benchmarks. These results indicate that transformer-based foundation models can be effectively adapted to radiologically dense tasks without sacrificing representation generality. For image captioning, the evaluation is intentionally restricted to controlled setting in which all visual encoders are paired with shared decoder architecture. This design isolates the contribution of the image representations and enables fair comparison across foundation models. While task-specific or fully fine-tuned architectures may further improve captioning performance, such extensions fall outside the scope of this work and would confound the assessment of representation quality. Overall, OmniRad provides stable backbone that supports multiple radiological tasks with minimal architecturespecific adaptation. This property reduces training complexity and computational cost, accelerating both research workflows and potential clinical deployment. When successfully integrated into real-world pipelines, such models may provide more consistent and reliable support for clinical decision-making. From clinical perspective, single stable visual backbone reduces variability across tasks and studies, which is critical for reproducibility in longitudinal analysis and multi-center deployments."
        },
        {
            "title": "Running Title for Header",
            "content": "5.1 Statistical Analysis and Interpretation To assess the robustness of OmniRads performance relative to existing baselines, we conducted series of statistical comparisons across all classification and segmentation benchmarks. For datasets with multiple repeated runs, t-test was applied, whereas one-sample t-tests were used when the baseline consisted of single value. Overall, the results indicate that OmniRad consistently matches or exceeds the performance of prior models, with statistically significant gains observed in datasets such as OrganSMNIST, MosMedPlusMSBench, and USforKidneyMSBench. These datasets are characterized by higher anatomical diversity or cross-modality variability, suggesting that OmniRads radiology-focused pretraining particularly benefits complex visual tasks. In several cases, including BreastMNIST, OrganAMNIST, PneumoniaMNIST, and Promise12MSBench, the performance differences are not statistically significant. Importantly, the absolute differences remain small, indicating that OmniRad is performing comparably to the strongest baselines even when significance is not reached. This tight performance range across benchmarks reflects both the stability of the learned representations and the low variance observed across repeated runs, which contributes to the consistency of the results. For segmentation datasets with single-sample baselines, such as FHPsAOPMSBench and PandentalMSBench, the t-tests indicate significant differences. In these cases, the baseline scores are slightly higher than OmniRads average, which is expected when comparing model with repeated measurements to fixed reference. Nevertheless, the absolute performance remains competitive, demonstrating that OmniRad produces high-quality dense predictions across multiple modalities, including CT, MRI, and ultrasound. The frozen-encoder setting further emphasizes representation quality rather than task-specific optimization, aligning the evaluation with the intended use of OmniRad as general-purpose radiological backbone. 5.2 Latent space evaluation To complement the quantitative evaluation, we analyze the latent representations learned by the nonfine-tuned encoders using UMAP projections on PneumoniaMNIST and OrganAMNIST. As shown in Figure 4, OmniRad exhibits reduced sensitivity to batch-related effects compared to DINO and DINOv3, whose embeddings display more fragmented clustering patterns. In particular, OmniRad produces semantically coherent clusters, with anatomically related classes occupying nearby regions in the embedding space. For example, the left and right lung classes form adjacent clusters, suggesting smooth representation of anatomical symmetry, while distinct structures such as the heart remain well separated. These qualitative observations indicate that OmniRad captures higher-level semantic relationships beyond class labels, which may contribute to its strong transfer performance across downstream tasks. While UMAP visualizations provide only an approximate and qualitative view of the representation space, the observed structure is consistent with the improved robustness and generalization reported in the quantitative experiments. This structured embedding behavior provides qualitative support for the low variance and consistent downstream performance observed across benchmarks. 5.3 Limitations Despite consistent performance across classification, segmentation, and captioning pilots, several limitations should be considered. First, OmniRad relies on RadImageNet initialization, which may not fully capture rare modalities, scanner-specific artifacts, or institution-dependent protocols. Residual domain biases may therefore persist, particularly in out-ofdistribution settings. Extending pretraining to larger, multi-center clinical collections represents an important direction for future work. Second, evaluation is limited to retrospective public benchmarks and does not include prospective clinical validation. Real-world diagnostic workflows often involve multi-view interpretation, longitudinal analysis, and integration of clinical metadata, which are not modeled in the present study. Third, most experiments use frozen-encoder paradigm to ensure computational efficiency and fair comparison. While this highlights representation quality, it may limit fine-grained task performance. Selective fine-tuning could address this limitation. Finally, although OmniRad scales favorably from small to base configurations, training larger variants remains computationally demanding. Ultra-large models were not explored due to resource constraints, leaving scaling"
        },
        {
            "title": "Running Title for Header",
            "content": "saturation and efficiency trade-offs unexamined. Captioning experiments are exploratory and evaluated using automatic metrics only, which do not fully capture clinical relevance, semantic accuracy, or safety concerns such as hallucinations. Human expert evaluation will be necessary to assess these aspects comprehensively. These limitations do not negate the proposed approach but highlight directions for improving generalization, scalability, and clinical reliability. Future work will focus on scaling pretraining to larger clinical cohorts and validating performance in prospective, institution-specific settings."
        },
        {
            "title": "6 Conclusions",
            "content": "We introduced OmniRad, radiology-driven foundation model designed to provide stable and transferable visual representations for diverse imaging tasks. Unlike task-specific or jointly optimized multi-task approaches, OmniRad follows task-agnostic paradigm in which single self-supervised encoder is reused across classification and segmentation, with an exploratory extension to image captioning. This design aligns with radiomics principles that emphasize representation stability, reproducibility, and reuse across clinical workflows. Across wide range of benchmarks spanning multiple modalities, anatomical regions, and task formulations, OmniRad demonstrated systematic improvements over existing medical foundation models. Gains were observed for imagelevel classification, dense anatomical segmentation, and report generation, confirming the versatility of the learned representations. Notably, these improvements were achieved with frozen encoder during downstream adaptation, underscoring the robustness and generalizability of the pretrained features. By combining large-scale self-supervised pretraining with representation-focused adaptation strategy, OmniRad bridges classical radiomics and modern foundation models. Avoiding task-specific encoder retraining enables coherent feature reuse across tasks, which is particularly relevant for longitudinal studies, multi-center deployments, and integrated clinical pipelines where consistent representations are essential. As such, OmniRad represents practical step toward unified, reusable visual representations for radiology-driven foundation models in real-world clinical pipelines."
        },
        {
            "title": "Code Availability",
            "content": "The code for https://github.com/ unica-visual-intelligence-lab/OmniRad. Additionally, we provide pretrained backbones on Hugging Face at https://huggingface.co/collections/Snarcy/omnirad. the following GitHub repository: study is available at this"
        },
        {
            "title": "Acknowledgement",
            "content": "We acknowledge financial support under the National Recovery and Resilience Plan (NRRP), Mission 4 Component 2 Investment 1.5 - Call for tender No.3277 published on December 30, 2021 by the Italian Ministry of University and Research (MUR) funded by the European Union NextGenerationEU. Project Code ECS0000038 Project Title eINS Ecosystem of Innovation for Next Generation Sardinia CUP F53C22000430001Grant Assignment Decree No. 1056 adopted on June 23, 2022 by the Italian Ministry of University and Research (MUR)."
        },
        {
            "title": "References",
            "content": "[1] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A., 2021. Emerging Properties in Self-Supervised Vision Transformers, in: 2021 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE, Montreal, QC, Canada. pp. 96309640. URL: https://ieeexplore.ieee.org/document/9709990/, doi:10.1109/ICCV48922.2021.00951. [2] Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., Qiao, Y., 2022. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534 . [3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR . [4] DAntonoli, T.A., Bluethgen, C., Cuocolo, R., Klontzas, M.E., Ponsiglione, A., Kocak, B., DAntonoli, T.A., Bluethgen, C., Cuocolo, R., Klontzas, M.E., Ponsiglione, A., Kocak, B., 2025. challenges, Diagn Interv Radiol. URL: https://www.dirjournal.org/articles/ risks, applications, opportunities, Foundation models for and prospects. fundamentals, radiology:"
        },
        {
            "title": "Running Title for Header",
            "content": "foundation-models-for-radiology-fundamentals-applications-opportunities-challenges-risks-and-prospects/ doi/dir.2025.253445, doi:10.4274/dir.2025.253445. publisher: Diagnostic and Interventional Radiology. [5] Fernández-Miranda, P.M., Fraguela, E.M., de Linera-Alperi, M.A., Cobo, M., del Barrio, A.P., González, D.R., Vega, J.A., Iglesias, L.L., 2024. retrospective study of deep learning generalization across two centers and multiple models of X-ray devices using COVID-19 chest-X rays. Scientific Reports 14, 14657. URL: https://www.nature.com/articles/s41598-024-64941-5, doi:10.1038/s41598-024-64941-5. publisher: Nature Publishing Group. [6] Gu, H., Colglazier, R., Dong, H., Zhang, J., Chen, Y., Yildiz, Z., Chen, Y., Li, L., Yang, J., Willhite, J., Meyer, A.M., Guo, B., Shah, Y.A., Luo, E., Rajput, S., Kuehn, S., Bulleit, C., Wu, K.A., Lee, J., Ramirez, B., Lu, D., Levin, J.M., Mazurowski, M.A., 2025. SegmentAnyBone: universal model that segments any bone at any location on MRI. Medical Image Analysis 101, 103469. URL: https://www.sciencedirect.com/science/ article/pii/S1361841525000179, doi:10.1016/j.media.2025.103469. [7] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R., 2022. Masked Autoencoders Are Scalable Vision Learners, in: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA. pp. 1597915988. URL: https://ieeexplore.ieee.org/document/9879206/, doi:10. 1109/CVPR52688.2022.01553. [8] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., 2021. LoRA: Low-Rank Adaptation of Large Language Models. URL: http://arxiv.org/abs/2106.09685, doi:10.48550/arXiv. 2106.09685. arXiv:2106.09685 [cs]. [9] Huang, S., Hou, Y., Liu, L., Yu, X., Shen, X., 2025. Real-Time Object Detection Meets DINOv3. URL: http://arxiv.org/abs/2509.20787, doi:10.48550/arXiv.2509.20787. arXiv:2509.20787 [cs]. [10] Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H., 2021. nnU-Net: self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203211. URL: https://www.nature. com/articles/s41592-020-01008-z, doi:10.1038/s41592-020-01008-z. publisher: Nature Publishing Group. [11] Kim, S., Park, H., Chikontwe, P., Kang, M., Hwan Jin, K., Adeli, E., Pohl, K.M., Hyun Park, S., 2025. Communication Efficient Federated Learning for Multi-Organ Segmentation via Knowledge Distillation With Image Synthesis. IEEE Transactions on Medical Imaging 44, 20792092. URL: https://ieeexplore.ieee.org/ document/10829700, doi:10.1109/TMI.2025.3525581. [12] Koch, V., Wagner, S.J., Kazeminia, S., Sancar, E., Hehr, M., Schnabel, J., Peng, T., Marr, C., 2024. DinoBloom: Foundation Model for Generalizable Cell Embeddings in Hematology. URL: http://arxiv.org/abs/2404. 05022, doi:10.48550/arXiv.2404.05022. arXiv:2404.05022 [cs]. [13] Koleilat, T., Asgariandehkordi, H., Rivaz, H., Xiao, Y., 2025. MedCLIP-SAMv2: Towards universal text-driven medical image segmentation. Medical Image Analysis 106, 103749. URL: https://www.sciencedirect. com/science/article/pii/S1361841525002968, doi:10.1016/j.media.2025.103749. [14] Kus, Z., Aydin, M., 2024. MedSegBench: comprehensive benchmark for medical image segmentation in diverse data modalities. Scientific Data 11, 1283. URL: https://www.nature.com/articles/ s41597-024-04159-2, doi:10.1038/s41597-024-04159-2. publisher: Nature Publishing Group. [15] Lai, Z., Wu, J., Chen, S., Zhou, Y., Hovakimyan, N., 2024. Residual-based language models are free boosters for biomedical imaging tasks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 50865096. [16] Lambin, P., Rios-Velazquez, E., Leijenaar, R., Carvalho, S., van Stiphout, R.G.P.M., Granton, P., Zegers, C.M.L., Gillies, R., Boellard, R., Dekker, A., Aerts, H.J.W.L., 2012. Radiomics: Extracting more information from medical images using advanced feature analysis. European Journal of Cancer 48, 441446. URL: https://www. sciencedirect.com/science/article/pii/S0959804911009993, doi:10.1016/j.ejca.2011.11.036. [17] Lang, W., Liu, Z., Zhang, Y., 2025. DACG: Dual Attention and Context Guidance model for radiology report generation. Medical Image Analysis 99, 103377. URL: https://www.sciencedirect.com/science/article/ pii/S1361841524003025, doi:10.1016/j.media.2024.103377. [18] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, in: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online. pp. 78717880. URL: https://aclanthology.org/2020.acl-main.703/, doi:10.18653/v1/2020.acl-main.703."
        },
        {
            "title": "Running Title for Header",
            "content": "[19] Manigrasso, F., Milazzo, R., Russo, A.S., Lamberti, F., Strand, F., Pagnani, A., Morra, L., 2025. Mammography classification with multi-view deep learning techniques: Investigating graph and transformer-based architectures. Medical Image Analysis 99, 103320. URL: https://www.sciencedirect.com/science/article/pii/ S1361841524002457, doi:10.1016/j.media.2024.103320. [20] Manzari, O.N., Ahmadabadi, H., Kashiani, H., Shokouhi, S.B., Ayatollahi, A., 2023. MedViT: Robust Vision Transformer for Generalized Medical Image Classification. Computers in Biology and Medicine 157, 106791. URL: http://arxiv.org/abs/2302.09462, doi:10.1016/j.compbiomed.2023.106791. arXiv:2302.09462 [cs]. [21] Mei, X., Liu, Z., Robson, P.M., Marinelli, B., Huang, M., Doshi, A., Jacobi, A., Cao, C., Link, K.E., Yang, T., et al., 2022. Radimagenet: an open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence 4, e210315. [22] Niu, C., Lyu, Q., Carothers, C.D., Kaviani, P., Tan, J., Yan, P., Kalra, M.K., Whitlow, C.T., Wang, G., 2025. Medical multimodal multitask foundation model for lung cancer screening. Nature Communications 16, 1523. URL: https://www.nature.com/articles/s41467-025-56822-w, doi:10.1038/s41467-025-56822-w. publisher: Nature Publishing Group. [23] Okolo, G.I., Katsigiannis, S., Ramzan, N., 2025. CLN: multi-task deep neural network for chest X-ray image localisation and classification. Expert Systems with Applications 288, 128162. URL: https://www. sciencedirect.com/science/article/pii/S0957417425017828, doi:10.1016/j.eswa.2025.128162. [24] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al., 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 . [25] Park, S., Lee, E.S., Shin, K.S., Lee, J.E., Ye, J.C., 2024. Self-supervised multi-modal training from uncurated images and reports enables monitoring AI in radiology. Medical Image Analysis 91, 103021. URL: https: //www.sciencedirect.com/science/article/pii/S1361841523002815, doi:10.1016/j.media.2023. 103021. [26] Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., Dosovitskiy, A., 2021. Do Vision Transformers See Like Convolutional Neural Networks?, in: Advances in Neural Information Processing Systems, Curran Associates, Inc.. pp. 1211612128. URL: https://proceedings.neurips.cc/paper/2021/hash/ 652cf38361a209088302ba2b8b7f51e0-Abstract.html. [27] Renggli, C., Pinto, A.S., Houlsby, N., Mustafa, B., Puigcerver, J., Riquelme, C., 2022. Learning to Merge Tokens in Vision Transformers. URL: http://arxiv.org/abs/2202.12015, doi:10.48550/arXiv.2202.12015. arXiv:2202.12015 [cs]. [28] Riberdy, V., Guida, A., Rioux, J., Brewer, K., 2025. Radiomics in preclinical imaging research: methnpj Imaging 3, 45. URL: https://www.nature.com/articles/ ods, challenges and opportunities. s44303-025-00104-z, doi:10.1038/s44303-025-00104-z. publisher: Nature Publishing Group. [29] Rückert, J., Bloch, L., Brüngel, R., Idrissi-Yaghir, A., Schäfer, H., Schmidt, C.S., Koitka, S., Pelka, O., Abacha, A.B., G. Seco de Herrera, A., Müller, H., Horn, P.A., Nensa, F., Friedrich, C.M., 2024. ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal Image Dataset. Scientific Data 11, 688. URL: https://www.nature.com/articles/s41597-024-03496-6, doi:10.1038/s41597-024-03496-6. publisher: Nature Publishing Group. [30] Schäfer, R., Nicke, T., Höfener, H., Lange, A., Merhof, D., Feuerhake, F., Schulz, V., Lotz, J., Kiessling, F., 2024. Overcoming data scarcity in biomedical imaging with foundational multi-task model. Nature Computational Science 4, 115. doi:10.1038/s43588-024-00662-z. [31] Shen, Y., Xu, Y., Ma, J., Rui, W., Zhao, C., Heacock, L., Huang, C., 2024. Multi-modal large language models in radiology: principles, applications, and potential. Abdominal Radiology 50, 27452757. URL: https://link. springer.com/article/10.1007/s00261-024-04708-8, doi:10.1007/s00261-024-04708-8. publisher: Springer. [32] Siméoni, O., Vo, H.V., Seitzer, M., Baldassarre, F., Oquab, M., Jose, C., Khalidov, V., Szafraniec, M., Yi, S., Ramamonjisoa, M., Massa, F., Haziza, D., Wehrstedt, L., Wang, J., Darcet, T., Moutakanni, T., Sentana, L., Roberts, C., Vedaldi, A., Tolan, J., Brandt, J., Couprie, C., Mairal, J., Jégou, H., Labatut, P., Bojanowski, P., 2025. DINOv3. URL: http://arxiv.org/abs/2508.10104, doi:10.48550/arXiv.2508.10104. arXiv:2508.10104 [cs]. [33] Singh, A., Ven, P., Eising, C., Denny, P., 2024. Dynamic Filter Application in Graph Convolutional Networks for Enhanced Spectral Feature Analysis and Class Discrimination in Medical Imaging. IEEE Access PP, 11. doi:10.1109/ACCESS.2024.3444042."
        },
        {
            "title": "Running Title for Header",
            "content": "[34] Tian, Y., Ye, Q., Doermann, D., 2025. YOLOv12: Attention-Centric Real-Time Object Detectors. URL: http://arxiv.org/abs/2502.12524, doi:10.48550/arXiv.2502.12524. arXiv:2502.12524 [cs]. [35] Wu, X., Zhang, S., Zhang, Z., He, Z., Xu, Z., Wang, W., Jin, Z., You, J., Guo, Y., Zhang, L., Huang, W., Wang, F., Liu, X., Yan, D., Cheng, J., Yan, J., Zhang, S., Zhang, B., 2024. Biologically interpretable multi-task deep learning pipeline predicts molecular alterations, grade, and prognosis in glioma patients. npj Precision Oncology 8, 181. URL: https://www.nature.com/articles/s41698-024-00670-2, doi:10.1038/s41698-024-00670-2. publisher: Nature Publishing Group. [36] Yang, J., Shi, R., Wei, D., Liu, Z., Zhao, L., Ke, B., Pfister, H., Ni, B., 2023. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data 10, 41. [37] Yang, J., Wang, L., Lin, C., Wang, J., Wang, L., 2025. DDKG: Dual Domain Knowledge Guidance strategy for localization and diagnosis of non-displaced femoral neck fractures. Medical Image Analysis 100, 103393. URL: https://www.sciencedirect.com/science/article/pii/S1361841524003189, doi:10.1016/j. media.2024.103393. [38] Yao, I.Z., Dong, M., Hwang, W.Y.K., 2025. Deep Learning Applications in Clinical Cancer Detection: Review of Implementation Challenges and Solutions. Mayo Clinic Proceedings: Digital Health 3, 100253. URL: https://www.sciencedirect.com/science/article/pii/S2949761225000604, doi:10.1016/j. mcpdig.2025.100253. [39] Zedda, L., Loddo, A., Di Ruberto, C., 2025. Radio DINO: foundation model for advanced radiomics and AI-driven medical imaging analysis. Computers in Biology and Medicine 195, 110583. URL: https: //www.sciencedirect.com/science/article/pii/S0010482525009345, doi:10.1016/j.compbiomed. 2025.110583. [40] Zhao, Y., Wang, X., Che, T., Bao, G., Li, S., 2023. Multi-task deep learning for medical image computing and analysis: review. Computers in Biology and Medicine 153, 106496. URL: https://www.sciencedirect. com/science/article/pii/S0010482522012045, doi:10.1016/j.compbiomed.2022.106496. [41] Zhong, D., Li, X., Huang, Z., Wang, S., Yu, Z., Hou, M., Yan, Y., Liu, Y., 2026a. Multi-modal multi-scale representation learning via cross-attention between chest radiology images and free-text reports. Biomedical Signal Processing and Control 111, 108318. URL: https://www.sciencedirect.com/science/article/ pii/S1746809425008298, doi:10.1016/j.bspc.2025.108318. [42] Zhong, Z., Wang, Y., Bi, L., Ma, Z., Ahn, S.H., Mullin, C.J., Greineder, C.F., Atalay, M.K., Collins, S., Baird, G.L., Lin, C.T., Stayman, J.W., Kolb, T.M., Kamel, I., Bai, H.X., Jiao, Z., 2026b. Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for pulmonary embolism diagnosis and report generation from CTPA. Medical Image Analysis 107, 103786. URL: https://www.sciencedirect.com/science/article/pii/ S1361841525003329, doi:10.1016/j.media.2025.103786. [43] Zhu, W., Jin, Y., Ma, G., Chen, G., Egger, J., Zhang, S., Metaxas, D.N., 2024. Classification of lung cancer subtypes on CT images with synthetic pathological priors. Medical Image Analysis 95, 103199. URL: https: //www.sciencedirect.com/science/article/pii/S1361841524001245, doi:10.1016/j.media.2024. 103199."
        }
    ],
    "affiliations": [
        "Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy"
    ]
}