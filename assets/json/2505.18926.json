{
    "paper_title": "Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time",
    "authors": [
        "Jingxuan Xu",
        "Hong Huang",
        "Chuhang Zou",
        "Manolis Savva",
        "Yunchao Wei",
        "Wuyang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while accurate, are computationally intensive and suffer from latency issues. Recent machine-learning methods reduce computational costs while preserving fidelity; yet most still fail to satisfy the latency constraints for real-time use and lack support for interactive applications. To bridge this gap, we introduce a novel hybrid method that integrates numerical simulation, neural physics, and generative control. Our neural physics jointly pursues low-latency simulation and high physical fidelity by employing a fallback safeguard to classical numerical solvers. Furthermore, we develop a diffusion-based controller that is trained using a reverse modeling strategy to generate external dynamic force fields for fluid manipulation. Our system demonstrates robust performance across diverse 2D/3D scenarios, material types, and obstacle interactions, achieving real-time simulations at high frame rates (11~29% latency) while enabling fluid control guided by user-friendly freehand sketches. We present a significant step towards practical, controllable, and physically plausible fluid simulations for real-time interactive applications. We promise to release both models and data upon acceptance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 6 2 9 8 1 . 5 0 5 2 : r Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time Jingxuan Xu Beijing Jiaotong University Hong Huang Simon Fraser University Chuhang Zou Meta Reality Labs Manolis Savva Simon Fraser University Yunchao Wei Beijing Jiaotong University Wuyang Chen Simon Fraser University"
        },
        {
            "title": "Abstract",
            "content": "We propose neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while accurate, are computationally intensive and suffer from latency issues. Recent machine-learning methods reduce computational costs while preserving fidelity; yet most still fail to satisfy the latency constraints for real-time use and lack support for interactive applications. To bridge this gap, we introduce novel hybrid method that integrates numerical simulation, neural physics, and generative control. Our neural physics jointly pursues lowlatency simulation and high physical fidelity by employing fallback safeguard to classical numerical solvers. Furthermore, we develop diffusion-based controller that is trained using reverse modeling strategy to generate external dynamic force fields for fluid manipulation. Our system demonstrates robust performance across diverse 2D/3D scenarios, material types, and obstacle interactions, achieving real-time simulations at high frame rates (11 29% latency) while enabling fluid control guided by user-friendly freehand sketches. We present significant step towards practical, controllable, and physically plausible fluid simulations for real-time interactive applications. We will release our code, model, and data at https://hybridmpm.github.io/ . Figure 1: We target real-time, interactive fluid simulations. Our hybrid solver integrates numerical simulator and neural physics (Section 3.1), enabling real-time simulation (Section 4.2). In addition, we generate external force fields (Section 3.2) to support users to control fluids interactively via freehand sketches (Section 4.3). Equal Contribution. Co-corresponding authors. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Modeling fluid behavior is essential for advancing diverse engineering fields, including entertainment [30], urban planning [2], fashion design [33], and virtual reality (VR) [29]. Moreover, controllability, aiming to instruct movements and shapes of fluids, is also very important attribute for volumetric effects, character animations, and fluid-solid coupling [25]. Realizing compelling and interactive physics simulations in real-time has been the long-standing objective for years in order to deliver transformative user experiences. Traditional simulation methods, though powerful, often demand significant implementation efforts and computational costs [5]. Recent neural physics and machine learning approaches present promising path forward by learning from data, delivering transformative changes for use cases such as fluid interactions and animations [27]. However, fidelity and latency in these neural-based methods are not well-balanced. Moreover, most methods only focus on the accuracy of non-interactive applications, and their computational complexity still remains generally high for real-time scenarios [4]. Motivated by the above challenges, we ask two scientific questions: Q1: Can neural physics accelerate real-time fluid simulations and interactions? Q2: Can neural physics and generative methods be optimized for interactive fluid control? We aim to explore novel paradigm: neural physics for interactive simulations in real-time (Figure 1). We provide affirmative answers. The core idea is to proactively marry the strengths of numerical simulation (high fidelity), neural physics (low latency), and generative control (interactivity) to deliver authentic and diverse fluid simulations. Specifically, neural physics is responsible for significantly low-latency fluid simulation with tolerant errors, and numerical simulation will serve as fallback solution when fluid dynamics is increasingly complex. Furthermore, to make fluid animation compatible with user-friendly control, we introduce another diffusion-based controller to generate external force fields to assist manipulations. We summarize our contributions below: 1. We improve the error-latency trade-off of fluid simulation. First, to accelerate neural physics, we seek to build our graph neural network at low spatiotemporal resolution without substantial degradation in simulation accuracy (Section 3.1.1). Second, to preserve simulation fidelity and avoid error accumulation during unrolling, we make our neural physics hybrid with safeguard condition and fallback mechanism to the classic MPM (Material Point Method) algorithm (Section 3.1.2). 2. We further aim to support users flexible freehand sketches that specify desired trajectories or shapes of fluid particles to be controlled. To this end, our novel reverse simulation strategy enables the automated generation of realistic fluid control data (Section 3.2.2), which is used to train our diffusion-based generative controller (Section 3.2.3). 3. Across diverse scenarios (2D/3D, materials, rigid obstacles, see Table 2), our hybrid simulator can significantly accelerate simulations (11 29% latency) while maintaining low errors (Section 4.2), and can control fluid particles to align with user sketches (Section 4.3), paving the way for promising advances towards engaging interactive simulations in real-time."
        },
        {
            "title": "2 Background",
            "content": "We first introduce the necessary components on which our method is built, and how they can be made real-time and controllable in Section 3. 2.1 Fluid Simulations with Material Point Method (MPM) The Material Point Method (MPM) [17, 14, 13, 15] is hybrid Eulerian-Lagrangian numerical technique for simulating complex interactions between solid and fluid materials, especially under large deformations and topological changes (snow, landslides, cloth, etc.). It extends the FLuidsImplicit-Particle (FLIP) [3] from Computational Fluid Dynamics (CFD) to solid mechanics by representing materials as set of Lagrangian particles that carry mass, velocity ( pi,t), position (pi,t), and possible internal states. These particle quantities are first transferred to background Eulerian grid using particle-to-grid mapping (p2g). The equations of motion are then solved on this grid, after which updated values are mapped back to particles through grid-to-particle transfer (g2p). The particle positions (p) are then advanced using the updated velocities ( p), e.g., pi,t+1 = pi,t + pi,t+1. 2 2.2 GNN-based Neural Physics for Particle Simulations We denote the state of particle at time step as xi,t (position p, velocity p, acceleration p, etc.), and the state of particles as Xt = [x1,t, . . . , xN,t]. simulator maps Tin input states to causally consequent future states, and can iteratively compute XtTin+1 = s(Xt1 , Xt2, , XtTin ) to simulate rollout trajectory. Following [27], our learnable simulator sθ adopts particle-based representation of the physical system, which can be viewed as message-passing via graph neural network (GNN). Input. Our neural physics simulator sθ takes the input of particle as: sequence of 5 previous velocities (via finite differences from Tin = 6 previous locations), and features for materials (e.g., water, sand, rigid, boundary), i.e., xi,tkTin :tk = [ pi,tkTin+2, . . . , pi,tk , fi] at time step tk (Figure 2). GNN Design. We first build the initial graph G(0) by assigning node to each particle and connecting particles as edges within fixed connectivity radius R. The edge embeddings are learned from relative positional displacement and the magnitude ri,j = [(pi pj), pi pj]. Our neural physics consists of stack of = 10 GNN layers. The decoder predicts the per-particle acceleration, pi. The training loss is the particle-level RMSE 1 and velocity are updated using an Euler integrator. See Appendix for further details. , where ˆpi is the predicted acceleration from sθ. The future position Figure 2: GNN as our neural physics simulator. ˆpi pi2 pi2 (cid:80)N i="
        },
        {
            "title": "3 Methods",
            "content": "We aim at real-time fluid simulations (Section 3.1) with interactive control (Section 3.2). Our method is overviewed in Figure 3. Figure 3: Method Overview. To achieve real-time simulations, we cut latency by learning neural physics at coarse spatiotemporal resolution, while safeguarding fidelity by automatically falling back to an MPM solver when complex fluid phenomena arise (Section 3.1). For interactive control, we train diffusion-based generative model that infers external force fields directly from user sketches (Section 3.2). 3.1 Hybrid Real-Time Fluid Simulation 3.1.1 Learning Real-Time Neural Physics at Low Spatiotemporal Resolution To accelerate the simulation, we train our neural physics at low spatiotemporal resolution. As shown in Figure 4, we consider learning the neural physics on simulations with both downsampled number of particles (ratio rp (0, 1)) and also with larger time step (i.e. coarser temporal discretization rate rt N, rt > 1). However, key pitfall is that once the number of particles is downsampled (Nh particles are merged via clustering into Nl, see Appendix B), we will lose the particle-wise correspondence, i.e., ˆpi (i [1, Nl]) and pj (j [1, Nh]) cannot align in the particlelevel RMSE (Section 2.2). As result, RMSE can no longer Figure 4: Our neural physics accelerates simulations by learning and inferring at low spatial (Nl num. particles) and temporal (t time steps) resolutions, with downsampling ratios as rp, rt. quantify the simulations fidelity to the ground truth of the original spatial resolution [16]. To mitigate this issue, we use normalized grid-level RMSE 1 as the evaluation metric, which essentially quantifies the mass distribution. is the normalize the grid mass ( mi = mi ) i=1 mi converted from particles to the grid via p2g, and ˆm is the prediction by sθ. mi and ˆm share the same grid size but can represent mass distributions from different resolutions (number of particles). During training, we continue to optimize the surrogate loss RMSE at the low spatial resolution, thereby avoiding additional p2g operations. ˆmi mi2 mi2 (cid:80)N (cid:80)N i= In Figure 6 (a-c), we can see that by tuning spatiotemporal downsampling ratios rp, rt, we can improve the trade-off between simulation errors and latency. Based on this ablation study, we will choose rp = 1/1.75 and rt = 2. With this configuration, on Water 2D, we can reduce the latency of the original neural physics (rp = rt = 1) by over 78.8% (from 1.954ms to 0.4048ms). 3.1.2 Hybrid Simulator with Safeguard Traditional numerical methods like MPM offer high fidelity but are computationally expensive. Though inferring neural physics at low spatiotemporal resolution enables significantly faster simulations, it often comes at the cost of increased simulation errors. For example, most dots in Figure 6 (a) and (b) are above the original neural physics (rp = rt = 1) and MPM. To fuse the strengths of both approaches, we make our simulator hybrid. We primarily leverage neural physics for fast updates, but incorporate safeguard mechanism to fall back to MPM in challenging scenarios and to empirically ensure simulation quality: Xt+1 = (cid:26) Neural Physics Update Fallback to MPM Update if update is good otherwise. (1) Fluid Complexity Measures. Intuitively, when the current fluid dynamics is simple, the neural physics should generalize well. In contrast, if the particles behave chaotically, their dynamics become out-of-distribution (OOD) samples that neural physics may be able to generalize. We thus trigger the fallback condition based on the complexity of the current fluid dynamics being simulated by neural physics. Moreover, the safeguard should be computationally cheap, since we need to densely monitor them during the simulation of neural physics. Figure 5: Negative correlation between cosine similarity of particle accelerations over frames vs. simulation errors of neural physics. Scenario: Water 2D. Spearman correlation: -0.3902. Specifically, we consider the cosine similarity of per-particle acceleration over window of history (window size as δt = 10 steps by default): 1 cos( pi,t2δt:tδt, pi,tδt:t). In contrast, we also tried to monitor the divergence of particles velocity [9], which is also used to quantify the quality of incompressible fluid simulations in previous works. However, its computation is significantly more expensive due to the use of finite difference methods, resulting in increased latency. We show the negative correlation between this cosine similarity and the neural physics simulation error in Figure 5, which indicates that whenever particles accelerations start diverging, we should fall back to MPM. (cid:80)N Figure 6: Ablation studies of the trade-off between grid-level RMSE vs. simulation latency. Left to right: temporal reduction rt (train neural physics with reduced particles Nl), spatial reduction rp (train neural physics with larger time step t), spatiotemporal reduction (combine rt = 2 and rp = 1/1.75), and hybrid with MPM (at rp = 1/1.75) with different thresholds rc. Scenario: Water 2D. 4 Triggering MPM by Fluid Complexity. With our fluid complexity metric, we need to trigger the MPM fallback mechanism in principle. In Table 1, we see that when increasing our threshold rc (i.e. MPM will be more frequently triggered), the simulation fidelity will be corrected by MPM (RMSE is improved), and the latency will increase due to heavy computations of MPM. Thus, we need to choose threshold rc such that we can improve our trade-off between RMSE and latency. In Figure 6 (d), we tune this threshold, and choose rc = 0.8 to balance the improvements over RMSE and latency. Figure 7: Error trajectories during simulation (Water 2D). Simulating the same number of steps (T = 1000), our hybrid solver takes significantly less time (676.4ms) than the original neural physics (1931.1ms), and the final error is also reduced (grid RMSE m) (0.0109 vs. 0.0188). We finalize our hybrid solver using this threshold. In Figure 7, we demonstrate trajectories simulated by the original neural physics and our hybrid solver (from the same initial condition, of the same number of steps ). Although the original neural physics (rp = rt = 1) shows lower rollout errors in the early stage (black curve, due to simulation at high resolution), it quickly accumulates long-term errors. In contrast, after triggering the fallback to MPM (yellow areas), our error is suppressed and we finish the simulation much faster. Thus, our hybrid solver improves both rollout RMSE and latency. Table 1: Grid RMSE vs. time per step with hybrid simulations triggered by different thresholds (Water 2D). Threshold rc Grid RMSE Time per step (ms) 0.0 0.1 0.2 0.3 0.4 0. 0.6 0.7 0.8 0.9 0.0232 0.4048 0.0230 0. 0.0227 0.4147 0.0223 0.4301 0.0221 0.4516 0.0215 0.4977 0.0208 0.5509 0.0192 0. 0.0169 0.6966 0.0144 0.7356 3.2 Interactive Fluid Control 3.2.1 Use Cases Fluid control is essential in computer graphics, where liquid animations convey expressive, story-driven scenes and key visual ideas like splash shapes or motion [38]. Manual fluid control produces unnatural effects and forces artists to rely on slow, trial-and-error methods [22]. This underscores the need for intuitive tools that let users shape visuals directly, without complex physics. Yet, achieving the desired appearance of fluid control remains difficult. Fluid dynamics are intrinsically chaotic and unpredictable. Setup and tuning of fluid control is tedious and repetitive. Moreover, recording real fluid motion is also expensive and hard to customize. In our paper, we mainly consider the following use case: during fluid simulation, user would like to draw simple sketch and provide it as control signal, following which the fluid particles should move, as shown in Figure 8 bottom panel. However, how to artistically manipulate fluid particles to follow the users sketch should be automatically designed by our system. Figure 8: We prepare our training data for generative control via solving external force fields that can reverse forward simulation. We also prepare user sketches (arrow, ellipse) that depict movements or target shapes of particles (see Appendix for implementation details) . 5 3.2.2 Data Generation via Reversed Simulation The key to making our fluid control possible is to automatically collect training data in principle. Specifically, we have two highly nontrivial sub-tasks: 1) Design large number of diverse scenarios of fluid particles with artistic control effects (i.e. fluid particles move along desired direction or fill pre-defined shape, in an organized manner, rather than in chaotic manner); 2) Solve spatiotemporal external force field that will be applied to the particles, such that the artistic control effect can be fulfilled driven by the composition of gravity, particle interactions, and the proposed force field. We address these challenges with reverse simulation strategy. The core idea is to solve the required force fields that can reverse the fluid dynamics of artistic effects. We have the following steps: 1) Forward Simulation. We randomly simulate trajectory of fluid dynamics = (X1, X2, , XTctr), with different initial conditions (positions or velocities of particles). 2) Reversed Simulation. We iteratively solve the required acceleration3 that can restore positions of each fluid particle reversely, from XTctr to X1: pt = (pt1 pt) pt (t)2 (2) 3) Generation of Control Sketches. Finally, based on X, we generate the users sketch that depicts the general movements of particles. We support both directional arrows for movement guidance and one-stroke freehand oval shapes to indicate target regions, as shown in Figure 8. See our Appendix for details of implementing freehand arrows and oval shapes. Note that in 3D scenarios, we use the arrow width to indicate depth [22]. For simplicity, we will by default control the fluid particles for 100 MPM steps (Tctr = 100). That means all our control trajectory will have 100 steps. While it is possible to employ dynamic neural architectures [41] to adaptively adjust the number of MPM steps for this control based on the control complexity, we leave it as future work. 3.2.3 Diffusion-based Fluid ControlNet Inspired by the recent success of conditioned video generation [40, 35, 12, 43, 34, 39, 37, 11, 36, 42], we choose to train conditioned diffusion model to control fluid particles. We by default control the simulations of MPM instead of our neural physics, since the controlled particles may lead to challenging simulations, which are essentially OOD settings to neural physics. That means, whenever user provides control sketch, we will fall back to MPM, and continue the MPM simulation under the control. We show our architecture design in Figure 9. Our diffusion-based Fluid ControlNet shares the same backbone and input particle features as our neural physics (Section 2.2). The output of our Fluid ControlNet is an external force field that will be applied to particles on top of gravity and particle interactions. Along the MPM simulation, our Fluid ControlNet will unroll the subsequent temporal force fields. The training target will be the ground truth force fields we simulate in Section 3.2.2. Parallel to the backbone, we extract the embeddings of the users sketch input using convolutional neural network (CNN) and concatenate them with the diffusion timestep embeddings to guide the generation process. We also embed the current control time step into latent space and integrate it into the initial noise. See Appendix for details of the architecture of our Fluid ControlNet. Figure 9: Architecture design of our Fluid ControlNet. 3Equivalently, the force field if all particles have the same constant mass 6 Figure 10: Trade-off between simulation error (grid RMSE m) and latency, comparing different methods. (a) Sand (2D); (b) SandRamps (2D); (c) WaterRamps (2D); (d) Water (3D); (e) Sand (3D); (f) Water-Sand (2D)."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings Physical Domains and Simulations. To build our hybrid simulator, we prepare our own ground truth simulations with the Taichi package [14, 13, 15] on GPUs, with settings closely aligned with [27]. We summarize our scenarios in Table 2. We include diverse initial conditions (position, velocity) and numbers of particles. We fix our grid size as 128 128 for 2D and 64 64 64 for 3D. We use time step dt = 2.5ms in our simulations. Table 2: Datasets. Nh: Max number of particles at the original spatial resolution. : total time steps. : number of simulation trajectories. Domain Water (2D) WaterRamps (2D) Sand (2D) SandRamps (2D) Water (3D) Sand (3D) Water-Sand (2D) Nh 4k 3.3k 4k 3.3k 4k 4k 4k 1k 600 320 400 800 350 500 1k 1k 1k 1k 1k 1k 1k Evaluation. To report quantitative results, we evaluated our models by computing rollout metrics on held-out test trajectories, drawn from the same distribution of initial conditions used for training. As discussed in Section 3.1.1, we use grid-level RMSE to compare predictions at lower spatial resolution with the original ground truth. 4.2 Fluid Simulation Acceleration Our hybrid simulator can consistently achieve real-time fluid simulations with preserved simulation fidelity across both 2D and 3D cases. We show the trade-off between simulation error and latency in Figure 10, where we compare our hybrid solver with the original neural physics (rp = rt = 1) [27], MPM [14, 13, 15], and another MPM that also simulates at low spatial resolution (rp = 1/1.75). On 2D scenarios, our hybrid solver consistently balances the neural physics and MPM, achieving both reduced simulation latency and preserved simulation errors. For example, on multiple materials (Water-Sand 2D), our hybrid solver can accelerate MPM from 0.114s per frame to 0.08s, with 29.8% reduction. On 3D, the neural physics at rp = rt = 1 is extremely slow, whereas our hybrid solver improves both latency and errors. For example, on Sand 3D, we reduce the latency of MPM by 11.8%, from 1.02 ms to 0.90 ms. 4.3 Generative Fluid Control We show visualizations of our generative fluid control in Figure 11. We compare with baseline, where particles are controlled with spatiotemporal constant force field, with the force magnitude and orientation solved by moving particles from XTctr to X1. 7 Table 3: Grid RMSE between ground truth and predictions at the last time during fluid control. Method Water (2D) Sand (2D) Water (3D) Sand (3D) Baseline Ours 0.0908 0.0802 0.1151 0.0924 0.0019 0. 0.0022 0.0019 Figure 11: Visualization of generative fluid control. Rows from top to bottom: Water (2D), Sand (2D), Water (3D), Sand (3D). We also quantitatively evaluate the control in Table 3, where we calculate the grid-level RMSE between the ground truth and the prediction at the last time step, since our main concern is the recovery of the shape of the ground truth at the end of the simulation. In sum, we can see that our diffusion-based Fluid ControlNet can move particles to better align with the user sketches. 4.4 Complete Results: Hybrid Simulation + Fluid Control Finally, we present the result from our complete pipeline in Figure 12. Particles are first simulated by our hybrid solver, where we start with the neural physics (at low spatiotemporal resolution) and is triggered to MPM once the fluid complex is high. Then, user draws sketch to control, and our diffusion-based Fluid ControlNet takes both this sketch and recent particle states as inputs, and generates external force fields to control particles."
        },
        {
            "title": "5 Related Works",
            "content": "Fluid Modeling and Animation Learning-based fluid simulators have progressed from graphbased models to hybrid, physics-informed approaches. DPI-Net [21] introduced dynamic interaction graphs with hierarchical message passing to model interactions across particles. This was unified 8 Figure 12: Complete results: hybrid simulation + fluid control. We start the simulation with our neural physics, which is then triggered to MPM. At = 150, user presents the control sketch. in GNS [27, 24, 19, 18], enabling generalized simulation of fluids, solids, and deformables. Hybrid solvers like MPMNet [20] and NeuralMPM [26] adopt the Material Point Method for scalability. Neural SPH [32] integrates SPH priors to stabilize rollouts, while NeuroFluid [10] combines learned dynamics and rendering from videos. These advances balance physical accuracy with real-time performance. In our work, we propose hybrid approach that combines neural and numerical methods to enable accelerated and high-fidelity fluid simulation. Fluid Control Recent work in fluid control aims to make simulations more intuitive and accessible. Traditional methods using space-time optimization were costly and hard to tune. Yan et al.[38] addressed this with sketch-based system using conditional GANs to generate liquid splashes. Pan et al.[22] enabled interactive control through sketching and mesh dragging. Chu et al.[7] used GANs to infer fluid motion from static fields with semantically controllable features. Schoentgen et al.[28] introduced reusable templates for particle-based animations. These approaches shift toward flexible, artist-friendly tools. We tackle the case where only freehand sketch is given, and the generative controller is tasked with producing the intended artistic fluid behavior. Controllable Video Generation Controllable video generation has advanced rapidly with diffusion models, especially in disentangling motion control. DragNUWA [40] enabled trajectory-based editing, while MotionCtrl [35] and Direct-a-Video [39] decoupled camera and object motion. CameraCtrl [12] and CamCo [37] refined camera control using geometric cues. MotionDirector [43] and Boximator [34] allowed user-customized motion, and SparseCtrl [11] enabled sparse, entity-level conditioning. Tora [42] unified text, image, and trajectory inputs for physics-aware generation. Inspired by these approaches, we leverage forward simulations and compute control forces via reversed simulation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced novel hybrid neural physics framework that effectively bridges the gap between high-fidelity physical simulation and real-time interactive control. By combining learned graph-based neural simulators with fallback to classical MPM solvers, we achieved robust, low-latency fluid dynamics capable of handling complex scenarios without sacrificing accuracy. Additionally, we developed diffusion-based generative controller trained via revserve modeling, enabling intuitive user interaction through freehand sketches for dynamic fluid control. Extensive experiments across 2D and 3D domains demonstrate that our approach not only accelerates fluid simulations but also provides controllable and physically plausible outcomes. This hybrid paradigm represents step forward in making real-time, artist-friendly fluid simulation practical for applications in graphics, design, and virtual environments."
        },
        {
            "title": "7 Limitations",
            "content": "Our current limitations are: 1) The control step Tctl is fixed at 100 and is not adaptive to the difficulty of the control scenario; 2) Errors are introduced by the inference of neural physics at low resolution. The potential solutions are: 1) Training the diffusion-based controller to unroll different numbers of steps to adapt to challenging control scenarios; 2) Training super-resolution model to correct errors introduced by simulating neural physics at low spatial resolution. However, addressing these limitations is beyond the scope of this paper, and we plan to study them in our immediate future work."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Bert Blocken and Ted Stathopoulos. Cfd simulation of pedestrian-level wind conditions around buildings: Past achievements and prospects. Journal of Wind Engineering and Industrial Aerodynamics, 121:138145, 2013. [3] Jeremiah Brackbill and Hans Ruppel. Flip: method for adaptively zoned, particle-in-cell calculations of fluid flows in two dimensions. Journal of Computational physics, 65(2):314343, 1986. [4] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. arXiv preprint arXiv:2202.03376, 2022. [5] Robert Bridson. Fluid simulation for computer graphics. AK Peters/CRC Press, 2015. [6] Jie Chen, Haw-ren Fang, and Yousef Saad. Fast approximate kNN graph construction for high dimensional data via recursive lanczos bisection. Journal of Machine Learning Research, 10(Sep):19892012, 2009. [7] Mengyu Chu, Nils Thuerey, Hans-Peter Seidel, Christian Theobalt, and Rhaleb Zayer. Learning meaningful controls for fluids. ACM Transactions on Graphics (TOG), 40(4):113, 2021. [8] Wei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for generic similarity measures. In Proceedings of the 20th International Conference on World Wide Web, pages 577586, 2011. [9] Yue Gao, Hong-Xing Yu, Bo Zhu, and Jiajun Wu. Fluidnexus: 3d fluid reconstruction and prediction from single video. arXiv preprint arXiv:2503.04720, 2025. [10] Shanyan Guan, Huayu Deng, Yunbo Wang, and Xiaokang Yang. Neurofluid: Fluid dynamics grounding with particle-driven neural radiance fields. In International conference on machine learning, pages 79197929. PMLR, 2022. [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2024. [12] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [13] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. Difftaichi: Differentiable programming for physical simulation. ICLR, 2020. [14] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Frédo Durand. Taichi: language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):201, 2019. [15] Yuanming Hu, Jiafeng Liu, Xuanda Yang, Mingkuan Xu, Ye Kuang, Weiwei Xu, Qiang Dai, William T. Freeman, and Frédo Durand. Quantaichi: compiler for quantized simulations. ACM Transactions on Graphics (TOG), 40(4), 2021. [16] Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua Tenenbaum, and Chuang Gan. Plasticinelab: soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311, 2021. [17] Chenfanfu Jiang, Craig Schroeder, Andrew Selle, Joseph Teran, and Alexey Stomakhin. The affine particle-in-cell method. ACM Transactions on Graphics (TOG), 34(4):110, 2015. [18] Krishna Kumar and Yonjin Choi. Accelerating particle and fluid simulations with differentiable In Proceedings of the SC23 graph networks for solving forward and inverse problems. Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis, pages 6065, 2023. 10 [19] Krishna Kumar and Joseph Vantassel. Gns: generalizable graph neural network-based simulator for particulate and fluid modeling. arXiv preprint arXiv:2211.10228, 2022. [20] Jin Li, Yang Gao, Ju Dai, Shuai Li, Aimin Hao, and Hong Qin. Mpmnet: data-driven mpm framework for dynamic fluid-solid interaction. IEEE Transactions on Visualization and Computer Graphics, 2023. [21] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018. [22] Zherong Pan, Jin Huang, Yiying Tong, Changxi Zheng, and Hujun Bao. Interactive localized liquid motion editing. ACM Transactions on Graphics (TOG), 32(6):110, 2013. [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [24] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning meshbased simulation with graph networks. In International conference on learning representations, 2020. [25] Karthik Raveendran, Nils Thuerey, Christopher Wojtan, and Greg Turk. Controlling liquids using meshes. In Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 2012. [26] Omer Rochman-Sharabi, Sacha Lewin, and Gilles Louppe. neural material point method for particle-based simulations. 2024. [27] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International conference on machine learning, pages 84598468. PMLR, 2020. [28] Arnaud Schoentgen, Pierre Poulin, Emmanuelle Darles, and Philippe Meseure. Particle-based liquid control using animation templates. In Computer Graphics Forum, volume 39, pages 7988. Wiley Online Library, 2020. [29] Serkan Solmaz and Tom Van Gerven. Interactive cfd simulations with virtual reality to support learning in mixing. Computers & Chemical Engineering, 156:107570, 2022. [30] Jos Stam. Stable fluids. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 779786. 2023. [31] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and highdimensional data. In Proceedings of the 25th International Conference on World Wide Web, pages 287297, 2016. [32] Artur Toshev, Jonas Erbesdobler, Nikolaus Adams, and Johannes Brandstetter. Neural sph: Improved neural modeling of lagrangian fluid dynamics. arXiv preprint arXiv:2402.06275, 2024. [33] Pascal Volino, Frederic Cordier, and Nadia Magnenat-Thalmann. From early virtual garment simulation to interactive fashion design. Computer-aided design, 37(6):593608, 2005. [34] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. [35] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [36] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. 11 [37] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [38] Guowei Yan, Zhili Chen, Jimei Yang, and Huamin Wang. Interactive liquid splash modeling by user sketches. ACM Transactions on Graphics (TOG), 39(6):113, 2020. [39] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [40] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [41] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018. [42] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [43] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2024."
        },
        {
            "title": "A Details of Neural Physics Simulator",
            "content": "A.1 Particle Simulations as Message-Passing on Graph We denote the state of particle at time step as xi,t RD, and the collective state of particles as Xt = [x1,t, . . . , xN,t] RN D. Applying physical dynamics over multiple timesteps yields ] RTinN D. In essence, the simulator trajectory of particle states, Xt1:tTin : RTinN RN (d = 2 or 3 for 2D/3D) leverages the current Tin particle states as input to predict their future motion, capturing the underlying dynamics using methods ranging from simple Euler integration to advanced numerical or data-driven techniques. If simulator is learnable, it can be represented as sθ, parameterized function approximator. The simulator then iteratively computes future states, such as XtTin+1 = s( Xt1 , Xt2, , XtTin ), where each newly predicted state is appended to simulate rollout trajectory over time. = [Xt1, Xt2, , XtTin Our learnable simulator sθ represents the physical system as interacting particles, where dynamics emerge from exchanges of energy and momentum with neighbors. To ensure robust simulation quality, sθ must generalize across diverse interaction patterns and physical scenarios. This particlebased approach naturally maps to message passing on graph, with particles as nodes and pairwise interactions as edges, making graph neural networks (GNNs) suitable modeling choice. A.2 Details of Graph-based Neural Physics Following [27], we implement our neural physics with GNN, and use standard nearest neighbor algorithms [8, 6, 31] to construct the graph. In our learnable simulator sθ, the input state vector for each particle at time step tk includes Input. sequence of 5 previous velocities (via finite differences from Tin = 6 previous locations), and static features representing material properties (e.g., water, sand, rigid, boundary particle). In practice, only the position vectors pi are stored in our datasets; the velocities pi and accelerations pi are computed on the fly using finite differences when needed. Formally, the node feature is defined as xi,tkTin :tk = [ pi,tkTin+2, . . . , pi,tk , fi] RD, where fi denotes the concatenated material-specific features and scene boundary indicators. Specifically, the dimension of the encoded node feature vector is = 30 for 2D simulations (5 2-dim velocities by finite differences, i.e., 5 2 = 10; 4 distances from the boundary; 16-dim embedding for the particle type), or = 37 for 3D simulations (5 3-dim velocities by finite differences, i.e. 5 3 = 15; 6 distances from the boundary; 16-dim embedding for the particle type). See Figure 2 for an illustration. It is important to note that in our Fluid Controlnet (Section 3.2.3), the input feature dimension will increase by 16, where we embed the current control timestep into the latent space with another 2-layer MLP with SiLU activation. To obtain more informative edge features ri,j, we use the relative positional displacement between pair of adjacent particles and j, along with its magnitude: ri,j = [(pi pj), pi pj]. Edges are added between particles that lie within predefined connectivity radius = 0.015, which captures local particle interactions. is kept constant for all 2D scenarios. In different 3D scenarios, larger radius can be used to accommodate higher-resolution environments. Although is fixed in simulations, edges in the graph are still dynamically updated by comparing the current particle-wise distances to R. For full details of these input and target features, we refer readers to [27]. The ENCODER : RN embeds particle-based states, formulated as: G(0) = (V (0), E(0)) = ENCODER(X, ri,j) The node embeddings (0) = ENCODERV (X) are learned functions of the particles states. The edge embeddings, E(0) i,j = ENCODERE(ri,j), are learned functions of the pairwise properties of the corresponding particles. We implement ENCODERV and ENCODERE as multilayer perceptrons (MLP), which encode node features and edge features into the latent vectors, Vi and Ei,j, of size 128. can be it The PROCESSOR : computes interactions among nodes through steps of learned message passing and outputs the final graph, G(L) = PROCESSOR(G(0)). Message passing enables information propagation among particles. Our PROCESSOR consists of stack of = 10 GNN layers, 13 each using separate (non-shared) MLPs for updating node and edge features, along with residual connections between the input and output latent attributes of both nodes and edges. For the Fluid ControlNet setting, an additional MLP layer is used to encode the diffusion timestep and control image features; see Appendix B.2 for details. The DECODER : RN extracts dynamics information (of the future state) from the nodes of the final latent graph, ˆX = DECODER(V (L)). Our DECODER is an MLP that outputs accelerations pi. The future position and velocity are updated using an Euler integrator. All MLPs in PROCESSORhave two hidden layers with ReLU, followed by an output layer without activation, with width of 128. All MLPs are followed by LayerNorm [1]."
        },
        {
            "title": "B Implementations",
            "content": "B.1 Latency Measurements Latency of Neural Physics. We utilize the TensorRT library to convert the PyTorch model into an ONNX model to accelerate model inference and align it with the acceleration of MPM on the Taichi kernel. However, since TensorRT does not support the aggregation operation in GNNs (i.e., aggregating information from edges to adjacent nodes), when measuring the latency, we approximate the time cost of this aggregation operation with matrix multiplication between an adjacency matrix RN (where denotes the number of nodes, i.e. particles), and node features (o), such that the aggregation becomes o. All reported latency measurements are based on the median number of nodes across different scenarios in our test datasets. Latency of Taichi. To enable fair comparison under MPM simulation setting, we applied matching latency reduction strategy to the Taichi implementation by skipping non-essential overhead. Specifically, we excluded the time spent on initializing the MPM state (initial positions and velocities of particles) and the cost of initializing the Taichi kernel at the beginning of the simulation. As result, our comparison focuses solely on the runtime per simulation step after the CUDA or Taichi kernel has been initialized. B.2 Design of Fluid ControlNet In our Fluid ControlNet, the control signal RHW 3 is encoded using our Fluid ControlNet. The encoded embedding is then injected into the graph-based diffusion model to guide the generation of the external field of accelerations. The Fluid ControlNet consists of 8 convolutional layers and 3 downsampling operations. It extracts multi-scale features from the control signal C, projects each scale to different dimensional space, and then concatenates the projected features into control embedding representations of dimension size 44. The resulting embedding is then integrated into the PROCESSOR module of the graph-based diffusion model. Notably, to better condition the diffusion process on the control signal, we draw inspiration from DiT [23] and concatenate the embedding of the control signal to the diffusion time step embedding. This design choice ensures that the control condition is effectively incorporated at each diffusion step, thereby generating high-fidelity acceleration fields that can align fluid particles to the target motion or shape. B.3 Training Following [27], we normalize the input velocity to the GNNs, and apply random noises to input positions (pt1:tTin ) during training. For both neural physics and Fluid ControlNet, we train with the Adam optimizer and learning rate at 1 104 with exponential decay. Our training batch size is 1, and we train for 2 million gradient descent steps. Table 4: Training Costs (GPU hours) across different scenarios. GPU Hours Sand (2D) Water (3D) Water (2D) Sand (3D) Neural Physics (Section 3.1) Fluid ControlNet (Section 3.2) 17.27h 69.87h 17.94h 76.36h 19.71h 184.12h 19.67h 151.03h 14 We include our training costs in Table 4. Neural physics requires approximately one day on single NVIDIA 4090 GPU. For the Fluid ControlNet, training takes around three days for 2D scenarios on single NVIDIA 4090 and six days for 3D scenarios on single NVIDIA A40. We train both neural physics and Fluid ControlNet with the particle-level RMSE loss on predicted accelerations RMSE 1 , which was defined in Section 2.2. (cid:80)N i=1 ˆpi pi2 pi2 B.4 Generating Users Freehand Sketches (Arrows and Oval Shapes) (cid:80)N Arrows are computed by connecting the centroid ( = 1 i=1 pi) of fluid particles at = 1 ( p1) and = Tctr ( pTctr ). Based on the mean displacement vector = p1 pTctr , we derive the arrow length and orientation θ = tan1( py/ px). In 3D, we use the arrow width to indicate depth [22]. multi-segment arrow with varying line width is implemented as n-segment polyline with width modulation, where each segments width wi (i [1, n]) is wi = wmin + (wmax wmin) . The arrowhead adopts perspective-correct scaling. pz,i pz,min pz,max pz,min For 2D oval sketches, shapes of particles at = Tctr are represented as elliptical outlines centered at pTctr , with radii corresponding to 2σ, where σ is the standard deviation of particle positions along each principal axis. This statistically-grounded ellipse captures approximately 95% particles positions while being visually simple. Meanwhile, 2D oval-shaped control sketches can indeed be ambiguous in 3D, since it is infeasible to depict 3D volumes with simple one-stroke 2D sketch. B.5 Enforcing Smoothness on Target Accelerations We observe that ground truth accelerations solved by Equation 2 are typically complicated (see the temporal-wise cosine similarity in Figure 13), which will be challenging to learn. We thus further enforce certain level of smoothness of the acceleration across temporal steps: pt,smooth = pt λ exp (cid:18) β (cid:19) pt pt+1 pt pt+1 ( pt pt+1) (3) Essentially, Equation 3 enforces decoupled smoothness over the magnitude and the orientation of accelerations over temporal steps. We choose λ = 0.1 and β = 2 in our work. Figure 13: Step-wise correlations of ground-truth accelerations for fluid control. Left: before enforcing smoothness; Right: after enforcing smoothness."
        },
        {
            "title": "C More Results",
            "content": "C.1 Grid RMSE of Fluid Simulations over Random Seeds To ensure fairer comparison, we conducted experiments using three different random seeds. The results, as shown in Table 5, demonstrate that our hybrid solver consistently outperforms the original neural physics across all datasets. 15 Table 5: Grid RMSE of fluid simulations on different scenarios, over three random runs. Grid RMSE Water (2D) Sand (2D) SandRamps (2D) WaterRamps (2D) Water (3D) Sand (3D) Water-Sand (2D) Neural Physics 0.0263 (1.15e-6) 0.0125 (2.59e-7) 0.0101 (3.23e-8) Our Hybrid Solver 0.0186 (8.17e-6) 0.0116 (6.88e-8) 0.0096 (1.00e-9) 0.0229 (2.09e-6) 0.0048 (6.58e-7) 0.0025 (2.11e-8) 0.0441 (3.51e-6) 0.0171 (3.16e-6) 0.0022 (1.77e-8) 0.0013 (1.08e-7) 0.0149 (2.38e-6) C.2 More Visualizations Fluid Simulations. Figure 14 presents the visualizations of all models discussed throughout the paper. Here, we show comparison of intermediate frames from single trajectory. It is evident that, due to the hybrid design of our hybrid solver, our method produces visual results that are more similar to MPM (rp = 1/1.75) simulations. Since MPM (rp = 1/1.75) is highly consistent with MPM (ground truth), the outputs of our Hybrid solver also align better with MPM compared to the original neural physics. This demonstrates that our approach effectively balances computational efficiency and accuracy. Figure 14: Visualizations of fluid simulations by different methods, over different scenarios. From left to right: Water (2D), Sand (2D), SandRamps (2D), WaterRamps (2D), Water (3D), Sand (3D), Water-Sand (2D). From top to bottom: Initial, MPM (ground truth), Original Neural Physics, MPM (rp = 1/1.75), Our Hybrid Solver. More Visualizations of Fluid Control. Figure 15 presents additional visualizations of generative fluid control across variety of tasks, both 2D and 3D control signals. We can see that our approach consistently generates physically plausible and visually accurate outcomes that align closely with the target controls across all fluid types and dimensions, demonstrating strong control capability. These results further confirm the effectiveness of our method in achieving both visually appearing and physically plausible fluid control. 16 Figure 15: More visualization of generative fluid control. From top to bottom: Water2D, Sand2D, Water3D, and Sand3D, each with two types of control signals (arrows for motion direction, and oval shapes for target spatial positions). From left to right: control signal, initial, ours, ground truth."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "Meta Reality Labs",
        "Simon Fraser University"
    ]
}