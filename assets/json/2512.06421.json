{
    "paper_title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "authors": [
        "Gengze Zhou",
        "Chongjian Ge",
        "Hao Tan",
        "Feng Liu",
        "Yicong Hong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation."
        },
        {
            "title": "Start",
            "content": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation Gengze Zhou1, Chongjian Ge2, Hao Tan2, Feng Liu2, Yicong Hong2 1Australian Institute for Machine Learning, Adelaide University, 2Adobe Research Work done during internship at Adobe. Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as popular paradigm, where models generate images in coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) traintest mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning traintest patterns, and complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields 5.2% FID reduction on FlexVAR-d16 trained on ImageNet-256 within 10 epochs (5 hours on 32A100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as reliable post-training method for visual autoregressive generation. Date: December 1st, 2025 Correspondence: gengze.zhou@adelaide.edu.au, yhong@adobe.com Project Page: https://gengzezhou.github.io/SAR 5 2 0 2 6 ] . [ 1 1 2 4 6 0 . 2 1 5 2 : r Figure 1 (a) Training dynamics. Training curves for FlexVAR, SAR trained from scratch, and SAR initialized from pretrained FlexVAR checkpoint. Within only few epochs, SAR quickly surpasses the best performance of fully-trained FlexVAR model. (b) ThroughputFID trade-off. Comparison of throughput, parameter count, and FID across representative generative model families, including Diffusion, Next-token AR, and Next-scale AR. SAR (red) attains the best overall trade-off : the highest throughput among autoregressive models and further imporve next-scale prediction AR model with the lowest FID across all AR baselines."
        },
        {
            "title": "1 Introduction",
            "content": "Autoregressive (AR) modeling has achieved notable success in large language models (LLMs) (Radford et al., 2018, 2019; Brown et al., 2020), inspiring its adaptation to visual generation. By formulating image synthesis as sequence modeling problem, AR models discretize images into ordered latent tokens and predict them causally. This formulation unifies autoregression across modalities and benefits from LLM-style scalability. More recent progress in visual autoregressive generation (VAR) (Tian et al., 2024; Han et al., 2024) alleviates the limitation of spatial inductive bias by decomposing an image into hierarchy of coarse-to-fine latent scales, predicting each scale conditioned on coarser representations. This next-scale-prediction formulation preserves spatial continuity, aligns better with the 2D nature of visual data, and significantly improves inference efficiency compared to token-level autoregression, whose sequence length grows quadratically with image resolution. However, existing VAR models suffer from two major issues, which negatively impact the generation quality: (1) traintest mismatch and (2) imbalanced learning difficulty across scales. The first issue originates from exposure bias, persistent challenge inherited from autoregressive models. Existing VAR models are trained with teacher forcing, i.e., conditioning on early-scale latents along the ground-truth trajectory. During inference, however, the model must rely on its own imperfect predictions, resulting in distributional drift and progressively accumulating errors during next-(group-of-)token prediction. This problem is more pronounced in VAR models than in token-wise autoregression, as each step involves predicting large number of tokens at scale. Errors at each scale propagate through the hierarchy, significantly degrading the final output quality. The scale-wise prediction leads to the second issue, imbalanced learning difficulty across scales. As will be shown in 3.3, typical pattern is that early coarse scales are responsible for generating global structure from scratch, whereas later fine scales mainly reconstruct high-frequency details under increasingly certain conditions, similar to spatial super-resolution. Minor inaccuracies at coarser scales are propagated and amplified through hierarchical decoding, causing cumulative errors and loss of visual fidelity. Despite this asymmetry, conventional supervision is typically applied uniformly across all scales, which means that once the coarse structure is formed in the early stages, the finer scales lack the capacity to correct structural artifacts introduced earlier. Consequently, training becomes uneven, and fine-scale refinement remains suboptimal. In this paper, we first conduct comprehensive empirical study to reveal the limitations of existing VAR formulations and the challenges of naively integrating student-forcing into training. Build on the emperical findings, we obtanin inspiration from the student-forcing formulation in robotic navigation (Anderson et al., 2018; Zhou et al., 2024b) and video generation (Lin et al., 2025; Huang et al., 2025), and introduce Self-Autoregressive Refinement (SAR), lightweight post-training algorithm that explicitly unrolls autoregressive inference during training to address these challenges. SAR bridges the traintest gap by conditioning each scales prediction on self-generated latents rather than ground-truth inputs, directly exposing the model to inference-time uncertainty and balance scale-wise learning difficulty. Importantly, the scale-wise autoregressive structure of VAR makes student-forcing computationally feasible: single model forward pass yields full set of tokens for an entire scale, significantly reducing rollout cost. Specifically, SAR introduces two key components: Stagger-Scale Rollout (SSR) strategy and Contrastive StudentForcing Loss (CSFL). SSR performs parallelized single-step student rollouts based on teacher-forced predictions, enabling efficient exposure to self-generated latents with minimal computational overhead. CSFL explicitly aligns the outcomes of student and teacher rollouts, providing targeted supervision that stabilizes training under student-forcing conditions. Together, these components allow the model to adapt effectively to inference-time dynamics with only one additional forward pass. In addition, SAR dynamically reweights per-scale supervision to address gradient sparsity, being effective in balacing optimization between coarse and fine scales and enhancing overall generation quality. Experimental results show that applying Scale-wise Autoregressive Refinement (SAR) to pretrained state-of-the-art VAR models yields consistent improvements of 5.2% / 2.5% / 3.1% lower FID on 300M, 600M, and 1B parameter models respectively, while using only 160 A100 GPU hours (32 80 GB A100s, ImageNet-256 for 10 epochs, amounting to 5.5% of pretraining cost). These results demonstrate that SAR effectively reduces exposure bias, mitigates scale-wise learning imbalance, and enhances perceptual quality and robustness in large-scale visual autoregressive generation."
        },
        {
            "title": "2 Related Work",
            "content": "Vector-Quantized Latent Modeling. VQ-VAE (Van Den Oord et al., 2017) established pioneering two-stage paradigm for discrete image generation: first encoding images into latent space and quantizing them with fixed-size codebook, and then modeling the discrete indices via an autoregressive prior such as PixelCNN (Van Den Oord et al., 2016). This formulation bridges continuous visual data with discrete sequence modeling, laying the foundation for modern visual autoregressive (AR) generation. Raster-scan Autoregression. Following this paradigm, subsequent works (Esser et al., 2021; Ramesh et al., 2021) employ Transformer-based AR priors over quantized latents. Hierarchical extensions such as VQ-VAE-2 (Razavi et al., 2019) and RQ-Transformer (Lee et al., 2022) introduce multi-scale or stacked latent codes to improve global coherence. Recent large-scale models (Sun et al., 2024; Liu et al., 2024) adopt GPT-style next-token prediction to achieve high-fidelity image synthesis, while MARS (He et al., 2024) integrates mixture-of-experts design to enhance diversity. AIM (Li et al., 2024a) further incorporates the Mamba architecture (Gu and Dao, 2023) to accelerate sequential decoding. To mitigate the quantization-induced information loss, several approaches combine diffusion or denoising objectives with AR decoding (Xie et al., 2024; Zhou et al., 2024a; Gu et al., 2024), achieving improved detail reconstruction and perceptual quality. Random-scan Generation. In contrast to rasterized decoding, masked-prediction models (Devlin, 2018; He et al., 2022; Bao et al., 2021) adopt BERT-style bidirectional formulation, predicting masked tokens conditioned on visible ones. This enables non-sequential token inference and parallel generation. MaskGIT (Chang et al., 2022) and Muse (Chang et al., 2023) extend this paradigm to class-conditional and text-to-image synthesis, achieving remarkable efficiencyquality trade-offs. The MagViT series (Yu et al., 2023; Luo et al., 2024) generalizes masked prediction to spatiotemporal domains, while NOVA (Deng et al., 2024) introduces hierarchical decoding orderfirst over time, then over spatial token groupsto produce coherent video frames. Scale-wise Generation. Visual Autoregressive (VAR) modeling (Tian et al., 2024) redefines the decoding order from next-token prediction to next-scale prediction, generating all tokens at given resolution in parallel. This substantially reduces inference steps while preserving global consistency. Subsequent advances such as VAR-CLIP (Zhang et al., 2024) extend this paradigm to text-to-image generation via CLIP-based conditioning. Efficiency-oriented improvements include fast decoding (Chen et al., 2024), linear attention acceleration (Ren et al., 2024b), and lightweight quantizers (Li et al., 2024d). To enhance fidelity, HART (Tang et al., 2024) and FlowAR (Ren et al., 2024a) combine continuous tokenizers with flow matching or diffusion objectives, while Infinity (Han et al., 2024) reformulates AR prediction at the bitwise level, achieving unprecedented visual detail and scalability. However, current VAR architectures provide no mechanism to correct accumulated multi-scale errors, since training is strictly teacher-forced and offers no exposure to model-generated trajectories. This mismatch underscores the need for more stable and computationally efficient autoregressive refinement strategy that is robust to error accumulation."
        },
        {
            "title": "3.1 Scale-wise Autoregressive Visual Generation",
            "content": "Formulation. Scale-wise autoregressive (AR) models decompose image generation into sequence of progressively refined visual scales, where each scale is conditioned on all preceding ones. Let the input image be represented by multi-scale latent maps {f1, f2, . . . , fn}, each at an increasingly higher resolution hi wi. The model first predicts the smallest-scale latent f1, and then autoregressively generates the subsequent latents conditioned on all earlier scales: p(f1, f2, . . . , fn) = p(fi f1, f2, . . . , fi1). (1) i=1 Each fi may represent either residual latent (as in residual VAR (Tian et al., 2024; Zhang et al., 2024; Tang et al., 2024)) or ground-truth latent (as in FlexVAR (Jiao et al., 2025; Yu et al., 2025; Ren et al., 2024a)). Residual-based methods define fi = gi Upsample(gi1), emphasizing difference modeling, while ground-truth-based models directly predict fi = gi. Both paradigms employ attention-based architectures (e.g., Transformers (Vaswani, 2017)) to capture long-range dependencies across scales. Unlike token-level autoregression, where one token is predicted per step, scale-wise AR generates an entire latent map per step, preserving the 2D structure and greatly improving efficiency. 3 Figure 2 Illustration of training supervision imbalance. For latent-space (top) supervision, coarse scales receive ground-truth signals that contain little semantic structure, while their corresponding training inputs are dominated by blurry upsampled artifacts. Consequently, the finest scale must reconstruct nearly all details, causing the hierarchical prediction process to collapse into single dominant scale and preventing effective coarse-to-fine learning. We could smooth the generation trajectory by downsampling in image space (bottom), but this causes the earliest scales to capture most semantics and scene structure already, leaving later scales to perform only mild sharpening or super-resolution, and thereby weakening the multiscale factorization of the model. Tokenization Process. Before autoregressive modeling, the input image is encoded into hierarchical discrete latent representations through multi-scale tokenizer. Specifically, an image tokenizer (e.g., VQ-VAE or residual quantizer (Lee et al., 2022; Chen et al., 2024)) maps to set of latent feature maps {f1, f2, . . . , fn} with resolutions {h1, h2, . . . , hn}. Each latent map fi can be further quantized into discrete code indices ti hiwi, forming the multi-scale token maps = {t1, t2, . . . , tn}. During autoregressive rollout, the model sequentially generates ti (or its continuous embedding fi) conditioned on all previously generated scales. Depending on the tokenizer design, fi can represent either an upsampling pathway (top-down prediction) (Ren et al., 2024a; Jiao et al., 2025) or residual pathway (difference-based refinement) (Tian et al., 2024; Han et al., 2024), offering flexibility in how hierarchical information propagates across scales. In this work, we primarily investigate the upsampling pathway for its simplicity and ease of formulation and training, while achieving comparable performance to residual-based alternatives."
        },
        {
            "title": "3.2 Teacher Forcing in Scale-wise AR",
            "content": "VAR and its successors typically follow teacher-forcing (TF) training paradigm: at each scale i, the model receives the ground-truth latent f1:i1 and predicts the next-scale latent (GT or residual) fi. Formally, ˆfi = gθ(f1:i1), LTF = i=1 ℓ( ˆfi, fi), (2) where ℓ is an L2 or cross-entropy loss depending on the tokenizer. To construct inputs for all scales in parallel (analogous to the next-token prediction in LLMs), we precompute f1, f2, . . . , fN and feed them to the model in one forward pass, with scale shift aligning each scale fi to predict fi+1. This TF strategy is fully parallelizable and efficient, but induces severe traintest gap: inference must generate all scales autoregressively using its own predictions, but training never exposes the model to such self-generated inputs. This motivates the use of student-forcing to reduce the discrepancy between training and inference. 4 Figure 3 Traininginference divergence caused by scale-wise supervision imbalance (Sec. 3.3). Under teacher-forcing (top), the model receives ground-truth latents at all scales; when training converged, the model produces clean generated results because it is evaluated under the same idealized inputs used during training. At inference (bottom), the model must condition on its own coarse-scale predictions. When generated early-scale latents are imperfect (e.g., 11), later scales, which were trained mainly as super-resolution task when using the smoothed up/down-sample image supervision, cannot correct the semantic error, leading to complete collapse of the generation process."
        },
        {
            "title": "3.3 Diagonosing Scale-wise Autoregression",
            "content": "We investigate the generation behavior of scale-wise autoregressive (VAR) models and uncover critical limitation. Despite their elegant hierarchical formulation of predicting finer-scale latents conditioned on coarser ones, training such models in practice leads to severe scale-wise supervision imbalance. As illustrated in Fig. 2 (top), under the conventional down/up latent supervision scheme, where ground-truth latents at each scale are obtained by downsampling the full-resolution latent, and inputs are produced by upsampling the ground-truth from the previous scalethe early coarse scales receive supervision devoid of meaningful semantic structure, and the final fine scale bears the majority of the reconstruction burden, having to refine extremely blurry and blocky inputs into detailed images. This imbalance effectively collapses the intended coarse-to-fine hierarchy into single dominant scale, undermining both the efficiency and interpretability of the autoregressive generation process. To systematically analyze this issue, we conducted series of studies aimed at disentangling potential causes. Specifically, we evaluate whether the imbalance can be alleviated by: (1) smoothing the supervision path through down/up-sample image supervision or (2) replacing the VAE with scale-equivariant VAE, and (3) introducing hybrid maskedautoregressive modeling to enable more NFE to early-scale generation. Smoothing Scale-wise Supervision. We first compare latent-level and image-level supervision under two different VAEs: standard latent diffusion VAE (LDM-VAE) and an equivariant VAE (EQVAE) trained jointly on up/down-sample image and latent generation. The experiments use FlowAR-B (Ren et al., 2024a), scale-wise AR model with diffusion head for regressing continuous latents, trained for 50 epochs on ImageNet 256256. As shown in Table 1, image-level supervision produces visually smoother inputtarget Table 1 Generation performance on ImageNet 256256. Comparison of imagevs. latent-level supervision for continuous VAEs paired with FlowAR-B. Metrics: FID, IS, precision (Pre), resolution (Res). VAE AR Model Supervision FID IS Pre Res Continuous VAE: LDM-VAE FlowAR-B FlowAR-B EQVAE FlowAR-B FlowAR-B Image Latent Image Latent 7.48 3. 7.89 3.93 390.3 232.6 155.8 233.4 0.88 0.80 0.80 0.81 0.36 0. 0.43 0.50 5 pairs but results in substantially higher FID scores (7.57.9 vs. 3.9), indicating that overly smooth latent supervision is ineffective. The EQVAE tokenizer facilitates even smoother feature propagation from the initial scale, the generation performance still lags behind. To further illustrate the consequences of scale-wise supervision imbalance, we visualize the intermediate predictions produced during training (teacher-forcing) and inference (student-forcing), as shown in Fig. 3. When provided with ground-truth inputs at every scale during training, the model exhibits stable coarse-to-fine refinement trajectory: early scales recover the global structure, and subsequent scales progressively sharpen local details, ultimately reconstructing an image nearly indistinguishable from the original. This confirms that the model has indeed learned to perform effective multi-scale denoising under ideal (teacher-forced) conditions. In contrast, during inference the model must autoregressively condition on its own predictions. If the generated earliest coarse-scale latent (e.g., the 11 scale initialized only from [bos] token) is bad, the subsequent scales fail to recover meaningful semantics. This degradation stems directly from the supervision imbalance discussed earlier: later scales are predominantly trained to perform super-resolution on ground-truth upsampled latents, rather than to correct semantically corrupted coarse predictions. As result, the generation process collapses as later scales faithfully magnify erroneous early predictions instead of repairing them, leading to severely distorted final outputs despite the model being well-trained under teacher-forcing. This visualization therefore makes explicit the mismatch between training and inference dynamics: training on latent down/up-sampling makes later-scale supervision disproportionately challenging, while training on image down/upsampling makes later-scale supervision insufficiently informative. Consequently, inference is disproportionately sensitive to coarse-scale errors, revealing fundamental brittleness in conventional scale-wise autoregressive generation. Table 2 Hybrid modeling performance on ImageNet 256256. Applying MaskGIT-style prediction at the coarsest scales strengthens initial 44 representations, but leads to degraded full-resolution FID, indicating that later scales collapse into near super-resolution refinement rather than benefiting from scale-wise autoregression. Decompose Initial-scale Generation. To further address the supervision imbalance, we design hybrid training scheme that applies masked-token modeling (MaskGIT-style) at the early scales to enhance global structure learning, followed by scale-wise autoregression at higher resolutions. The key idea is to decompose the challenging initial-scale generation into multiple iterative steps, strategy shown effective in prior work (Li et al., 2024c; Pang et al., 2025). We instantiate this hybrid model on FlexVAR and apply down/up-sample latent supervision, enabling direct comparison against standard FlexVAR variants. As shown in Table 2, the hybrid modeling indeed improves FID at the coarsest 44 scale (from 27 to 22 for d=16, and form 26 to 20 for d=20), demonstrating stronger coarse-scale representation. However, the overall FID of fullresolution generation degrades notably (from 3.05 to 5.02 and from 2.41 to 4.74), accompanied by reductions in both recall and precision. These results indicate that, although early scales benefit from bidirectional mask modeling, the later scales collapse into near super-resolution refinement, preventing the model from fully exploiting the intended scale-wise autoregressive dependencies. Discrete VAE: FlexVAR-d16 291.3 250.2 299.3 303.6 w/ Hybird Modeling w/ Hybird Modeling 0.52 0.47 0.58 0.44 0.83 0.81 0.85 0.84 3.05 5.02 2.41 4.74 FlexVAR-d20 Pre Rec 27 22 26 FID @ 4 Model FID IS Findings. Collectively, these results suggest that neither smoothing the supervision path (latent vs. image) nor allocating additional computation to the early scales effectively resolves the imbalance. We hypothesize that both strategies shift the modeling burden to the earliest scale, where the model must create the entire global structure from scratch, while later scales become overly simplified, functioning merely as super-resolution that are easy to optimize and prone to overfitting. Overall, these analyses demonstrate that scale-wise autoregressive frameworks require fundamental rethinking of their training objectives and dependency structures, beyond residual formulations or supervision design, to achieve balanced hierarchical generation."
        },
        {
            "title": "4 Self-Autoregressive Refinement",
            "content": "We propose applying Student Forcing training to the autoregressive process, which is widely used to mitigate the traininginference discrepancy. In our case, it also serves to balance scale-wise learning difficulty by propagating early-scale imperfections to later scales. Figure 4 Illustration of different student forcing training schemas: (a) Teacher Forcing (TF) uses ground-truth latents at all scales during training. (b) Student Forcing (SF) uses predicted latents only, simulating test-time conditions. (c) Hybrid TF & SF applies teacher forcing at early scales and student forcing at later ones. (d) Interleave TF & SF alternates between teacher and student forcing across scales."
        },
        {
            "title": "4.1 Naïve Integration of Student Forcing",
            "content": "Formally, at each scale i, the model takes its own previously generated latents and predicts the next-scale latent: fi = gθ( f1:i1), LSF = i=1 ℓ( fi, fi). (3) To investigate whether SF can close the traintest gap in VAR, we take fully trained FlexVAR (Jiao et al., 2025) checkpoint (100 epochs on Imagenet-256) and continue training for 20 epochs under four SF schedules designed to expose the model to different degrees of self-generated inputs: SF only (Fig. 3(b)): all scales are generated autoregressively; no ground-truth inputs are used during training. Alternate: TF and SF updates alternate every iteration, providing 1:1 mixture of GT and self-generated inputs. Interleave (Fig. 3(c)): TF is applied to odd-index scales while SF is applied to even-index scales, allowing mixed supervision within single forward pass. Hybrid (Fig. 3(d)): TF is used up to specified scale k, after which SF is applied to all finer scales, gradually transitioning from ground-truth to self-generated latents. We present the result in Tab. 3. Across all schedules, two consistent issues emerge. First, SF greatly increases training cost: autoregressive rollout removes parallelism across scales, making each iteration substantially slower than teacher-forcing. Second, directly train on self-generated latents destabilizes learning as early predictions largely drift away from the ground-truth targets, leading to misaligned intermediate supervision that is inapproarite for training. While the hybrid 7 Table 3 Comparison on student forcing schedules. Alternate, Interleave, and Hybrid involve mixture with teacher forcing."
        },
        {
            "title": "Teacher Forcing",
            "content": "FlexVAR-d16 FID 3.83 IS 248.82 Pre 0.83 Res 0.48 Student Forcing Full Alternate Interleave Hybird @ 8 Hybird @ 16 FlexVAR-d16 FlexVAR-d16 FlexVAR-d16 FlexVAR-d16 FlexVAR-d16 16.56 9.31 9.47 10.83 5. 135.23 226.53 399.66 195.49 286.87 0.65 0.75 0.86 0.72 0.83 0.43 0.38 0.32 0.40 0.41 schedule mitigates the instability to some extent by delaying the introduction of SF, none of the schedules approach the stability or quality of teacher-forcing. The key finding is that naïve student-forcing, despite being conceptually aligned with autoregressive inference, fails to address the traintest gap. Instead, it introduces supervision inconsistency, amplifies noise across scales, and incurs high computational overhead, highlighting the need for more principled approach."
        },
        {
            "title": "4.2 SAR: A Stable Student-Forcing Strategy",
            "content": "To address the aforementioned issues, we propose Self-autoregressive Refinement (SAR) for efficient and stable optimization at all scales. We introduce Stagger-Scale Rollout (SSR) to implement efficient student-forcing (only 2 rollout steps needed), and Contrastive Student-Forcing Loss (CSFL) to provide consistent training supervision."
        },
        {
            "title": "4.2.1 Stagger-Scale Rollout (SSR)",
            "content": "To make student-forcing practical for scale-wise autoregression, we introduce Stagger-Scale Rollout (SSR), lightweight two-step rollout strategy designed to meet three requirements: (1) It must expose the model to its own predictions so that training more faithfully matches the autoregressive behavior used at inference. (2) The self-generated inputs must remain meaningful; naïve SF quickly produces noisy, drifted latents that break the hierarchical structure and destabilize learning. (3) The rollout must remain computationally efficient and avoid the expensive full autoregressive unrolling that makes naïve SF prohibitively slow. SSR addresses all three issues through short, controlled rollout that preserves clean supervision while introducing limited but useful exposure to model-generated states. The process begins with standard teacher-forcing forward pass using ground-truth latent inputs. At each scale, the model predicts: Such predictions can be upsampled and shifted to become the input to the next scale: ˆf (T ) = gθ(f1:i1), = Upsample( ˆf (T ) (T ) ), which follows the autoregressive inference. We then perform one-step parallelized student-forcing rollout by feeding the shifted latents into the model: resulting in single-step student-forced outputs for all scales. = gθ( (T ) ˆf (S) 1:i1), (4) (5) (6) This staggered design yields two aligned trajectories: the teacher-forced predictions ˆf (T ) , and the one-step student-forced 1:N predictions ˆf (S) . By limiting the rollout to only two student-forcing steps, SSR provides just enough exposure to 2:N inference-like dynamics to reduce the traintest gap, while keeping the intermediate inputs structured and additional computational cost to only one extra NFE. Deeper rollouts, in contrast, accumulate noise rapidly and offer no additional benefit as observed in our empirical analysis (Tab. 3). 8 Figure 5 Illustration of SAR. The image is encoded into multi-scale latents {fi}, which condition an autoregressive generator. In the first forward pass, the model performs teacher forcing and predicts ˆf (T ) at all scales. These predictions are then upsampled to form scale-shifted inputs (T ) . Teacher-forcing loss provides ground-truth supervision, while the contrastive student-forcing loss aligns student-forced outputs with their teacher-forced counterparts. Together, these two passes form the Stagger-Scale Rollout used in SAR. , enabling second forward pass that produces student-forced predictions ˆf (S) i"
        },
        {
            "title": "4.2.2 Contrastive Student-Forcing Loss (CSFL)",
            "content": "While SSR provides stable and efficient way to obtain both teacher-forced and student-forced trajectories, the key challenge remains: how should the student-forced predictions be supervised? Directly comparing ˆf (S) to the ground-truth fi is ineffective, because student-forced latents may deviate from the teacher-forced hierarchy, especially at early scales. This mismatch creates ambiguous or conflicting supervision signals, which was precisely the failure case observed in naïve SF (Sec. 4.1). To address this issue, we introduce Contrastive Student-Forcing Loss (CSFL) that aligns student-forced predictions with their teacher-forced counterparts instead of the ground-truth. The intuition is that the teacher-forced trajectory ˆf (T ) provides clean, stable estimate of the next-scale latent, and student-forced predictions should learn to remain consistent with this trajectory even when conditioned on self-generated inputs. Formally, for each scale i, we compute two prediction paths: ˆf (T ) = gθ(f1:i1), = gθ( (T ) ˆf (S) 1:i1), is the teacher-forced prediction and ˆf (S) where ˆf (T ) The total training loss consists of two terms. The first is the standard teacher-forcing loss: is the one-step student-forced prediction obtained via SSR. LTF = i=1 ℓ(cid:0) ˆf (T ) (cid:1), , fi which provides ground-truth supervision and stabilizes the coarse-to-fine hierarchy. The second term enforces consistency between teacher-forced and student-forced predictions: LCSF = i=2 ℓ(cid:0) ˆf (S) , ˆf (T ) (cid:1), 9 (7) (8) (9) (10) hi+1) ui = Upsample(f GT Algorithm 1 Self-Autoregressive Refinement 1: Input: image x, label y, scales {h1, . . . , hN } 2: Modules: multi-scale encoder E, VAR generator gθ 3: Hyperparameter: SF weight γ 4: # Encode ground-truth latents for all scales 5: for = 1 to do GT = E(x) resized to (hi hi) 6: 7: end for 8: # ======== Pass 1: Teacher Forcing (TF) ======== 9: # Construct inputs by upsampling GT latents i+1 10: for = 1 to 1 do 11: 12: end for 13: # Single TF forward pass across all scales 14: ( ˆf (T ) , . . . , ˆf (T ) 15: LTF = ℓ( ˆf (T ) , . . . , ˆf (T ) 16: # Sample TF predictions to build SF inputs 17: for = 1 to do = Sample( ˆf (T ) (T ) 18: 19: end for 20: # ===== Pass 2: One-Step Student Forcing (SF) ===== 21: # Scale-shift sampled prediction and run SF 22: uSF = Upsample( (T ) 23: ˆf (S) k+1 = gθ(y, uSF) 24: LCSF = ℓ( ˆf (S) 25: # ========= Combine losses and update ========= 26: = LTF + γ LCSF 27: Backpropagate and update θ ) = gθ(y, {ui}) ; GT 1 , . . . , GT ) hk+1) k+1, GT k+1) ) 1 e.g., argmax encouraging the model to produce latents that remain close to the teacher-forced trajectory even when using imperfect self-generated inputs. Balancing these two components yields the full SAR training objective: LSAR = LTF + γ LCSF, (11) where γ controls the strength of student-forcing alignment. Together with SSR, the CSFL provides structured, inference-aligned supervision: the teacher-forced branch ensures stable training, and the student-forced branch learns to follow this stable trajectory even when conditioned on self-generated latents. This resolves the supervision inconsistency that destabilized naïve SF and forms the core of our SAR training strategy."
        },
        {
            "title": "5.1 Implementation Details",
            "content": "We train SAR on ImageNet-1K at 256256 resolution on three model scales following VAR (Tian et al., 2024). SAR is formulated as post-training refinement stage designed to further enhance generative quality after conventional teacher-forcing training. We initialize SAR from sufficiently trained FlexVAR checkpoints and apply an additional 10 epochs of SAR training. To ensure fair comparison with all baselines, we keep the total number of training epochs fixed. Specifically, for the three VAR scales, we resume from the 170/240/340-epoch FlexVAR checkpoints (trained with teacher forcing) and then train SAR for 10 additional epochs, matching the overall training epochs of competing 10 Table 4 Generative model comparison on class-conditional ImageNet 256256. Metrics include Fréchet inception distance (FID), inception score (IS), precision (Pre) and recall (Rec). Step: the number of model runs needed to generate an image. Time: the relative inference time of VAR-d30 Tian et al. (2024). We present models with size 1B. Model Epochs Param Step Time FID IS Pre Rec BigGAN (Brock et al., 2018) GigaGAN (Kang et al., 2023) StyleGan-XL (Sauer et al., 2022) ADM-U (Dhariwal and Nichol, 2021) CDM (Ho et al., 2022) LDM-4-G (Rombach et al., 2022) DiT-L/2 (Peebles and Xie, 2023) DiT-XL/2 (Peebles and Xie, 2023) MaskGIT (Chang et al., 2022) RCG (cond.) (Li et al., 2024b) Generative Adversarial Networks (GAN) 400 2160 200 1400 112M 569M 166M 1 1 1 Diffusion Models 250 8100 250 250 250 554M 400M 458M 675M 0.2 118 2 Random-scan Manner (Mask Prediction) 555 400 227M 502M 8 20 0.4 1.4 6.95 3.45 2. 3.94 4.88 3.60 5.02 2.27 6.18 3.49 224.5 225.5 265.1 186.7 158.7 247.7 167.2 278.2 182.1 215.5 VQGAN (Esser et al., 2021) RQTran. (Lee et al., 2022) LlamaGen-XL (Sun et al., 2024) AiM (Li et al., 2024a) Raster-scan Manner (Token-wise Autoregressive) 100 50 300 350 227M 821M 775M 763M 256 68 256 256 7 27 12 18.65 13.11 2.62 2. 80.4 119.3 244.08 257.2 VAR-d16 (Tian et al., 2024) FlexVAR-d16 (Jiao et al., 2025) w/ SAR (Ours) VAR-d20 (Tian et al., 2024) FlexVAR-d20 (Jiao et al., 2025) w/ SAR (Ours) VAR-d24 (Tian et al., 2024) FlexVAR-d24 (Jiao et al., 2025) w/ SAR (Ours) Scaling-scan Manner (Scale-wise Autoregressive) 180 180 + 10 250 250 + 10 350 350 + 10 310M 310M 310M 600M 600M 600M 1.0B 1.0B 1.0B 10 10 10 10 10 10 10 10 10 0.2 0.2 0.2 0.3 0.3 0.3 0.5 0.5 0. 3.55 3.05 2.89 2.95 2.41 2.35 2.33 2.21 2.14 280.4 291.3 266.6 302.6 299.3 293.3 312.9 299.1 315.5 0.89 0.84 0.78 0.82 0.75 0.83 0.80 0.78 0.80 0. 0.84 0.83 0.81 0.83 0.85 0.82 0.82 0.83 0.81 0.38 0.61 0.53 0.52 0.57 0.57 0.51 0.26 0.57 0.57 0.51 0.52 0.55 0.56 0.58 0.59 0.59 0.59 0. methods. We use the AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.05. The learning rate is set to 1e4 without warm-up. All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory."
        },
        {
            "title": "5.2 State-of-the-Art Image Generation",
            "content": "We compare our SAR-enhanced models with broad range of state-of-the-art generative approaches on the ImageNet-1K benchmark, including GANs, diffusion models, random-scan and raster-scan autoregressive models, and the latest scaling-scan autoregressive methods. The quantitative results on 256256 resolution are summarized in Tab. 4."
        },
        {
            "title": "5.2.1 Overall comparison on ImageNet 256×256.",
            "content": "To ensure fair comparison, we report results for all models with parameter sizes 1B. Across all three scales, applying our proposed SAR consistently improves the performance of the FlexVAR baselines, achieving state-of-the-art results among scaling-scan autoregressive models. In particular, SAR yields clear FID improvements at every model size: 2.89 vs. 3.05 for the 310M model, 2.35 vs. 2.41 for the 600M model, and 2.14 vs. 2.21 for the 1B model. This demonstrates that SAR effectively enhances fine-scale generation quality while maintaining the efficient inference structure of scale-wise autoregression."
        },
        {
            "title": "5.2.2 Training Dynamics",
            "content": "Figure 1 (a) provides joint visualization of the training behavior of SAR and its performance relative to state-of-the-art generative models. We compare three variants trained on ImageNet 256 256: 11 FlexVAR (Jiao et al., 2025): FlexVAR-d16 model trained from scratch for 180 epochs. SAR (from scratch): SAR trained from scratch for 180 epochs. SAR (init FlexVAR): SAR initialize from well-trained FlexVAR model at 170 epochs, then further train for 10 epochs. We observe several important trends: 1. Faster convergence. SAR trained from scratch descends sharply in the early stage and consistently maintains lower FID across training. This indicates that SAR provides stable student-forcing training scheme and could effectively improve teacher-forcing training. 2. Lower final error. By Epoch 180, SAR achieves noticeably lower FID than FlexVAR. The smoother convergence curve suggests improved stability in token prediction during rollouts. 3. Strong initialization benefits. Initializing SAR from FlexVAR drastically accelerates early-stage optimization. Within only few epochs, SAR quickly surpasses the best performance of fully-trained FlexVAR model, converging to the lowest FID among all variants. Overall, the training analysis confirms that SAR not only enhances sample quality but also makes student-forcing training in scale-wise AR effective and stable."
        },
        {
            "title": "5.2.3 Model Comparison",
            "content": "Figure 1 (b) visualizes the throughputFID trade-off across popular generative model families. Key observations include: 1. Diffusion models (purple) generally achieve good FID but suffer from low throughput due to long sampling trajectories. 2. Next-token AR models (greens) improve throughput by using autoregression in latent space, but token-level causality limits their global coherence and negatively impacts FID. 3. Next-scale AR models (blues), including VAR and FlexVAR, provide an efficient coarse-to-fine sampler that greatly boosts throughput. 4. SAR (red) attains the best overall trade-off : the highest throughput among autoregressive models and further imporve next-scale prediction AR model with the lowest FID across all AR baselines. The introduction of SAR retains the structural advantages of VAR while improving generation quality."
        },
        {
            "title": "5.3 Quanlitive Results",
            "content": "We show the qualitative results of SAR in Figure 6. 12 Figure 6 Images generated by SAR on ImageNet 256256."
        },
        {
            "title": "5.3.1 Visualization of the Generation Process",
            "content": "To better understand the qualitative differences between SAR and FlexVAR, we visualize the full generation trajectory of both models under identical initial conditions. We begin by selecting images from the ImageNet validation set, encoding them using the VAE to obtain the scale-wise latent representations. At inference time, both models are initialized with the same 4 4 latent, and perform coarse-to-fine autoregressive generation following the scale schedule: 4 8 10 13 16. Figure 7 presents the intermediate outputs at each scale for both SAR and FlexVAR. Across all examples, two consistent patterns emerge: 13 Figure 7 Comparison of the generation process of SAR and FlexVAR. Both models start from the same 4 4 latent and follow identical scale schedules. SAR demonstrates smoother transitions and stronger error correction across scales. 1. Smooth and stable refinement. SAR exhibits smoother transitions between scales, with fewer abrupt changes in structure or texture. 2. Error correction during refinement. When early-scale predictions contain distortions or misaligned structures, SAR reliably corrects these errors at subsequent scales. In contrast, FlexVAR often amplifies early mistakes, leading to degraded high-resolution details. These visualizations highlight the student-forcingaligned training in SAR produces more stable hierarchical latents, enabling more coherent and robust refinement during generation."
        },
        {
            "title": "5.4.1 Effect of sampling strategy in student-forcing rollout.",
            "content": "A key component of SAR is the sampling procedure used to generate student predictions during the rollout stage. We therefore ablate the sampling strategy used in SAR training and study its impact on final image quality. Table 5 Ablation on Student Forcing sampling methods. Comparison of different SF sampling strategies during training."
        },
        {
            "title": "CFG",
            "content": "FID IS Pre Res We evaluate three variants: (1) No sampling, where we directly use the argmax token as the student prediction; (2) Stochastic sampling, where tokens are sampled from the predictive distribution; (3) Sampling with classifier-free guidance (CFG). For all sampling-based variants, we use top-k=900 and top-p=0.95. For CFG sampling, we set the guidance scale to 2.5. Results are reported in Tab. 3. 259.93 278.69 266. 0.81 0.82 0.81 2.98 2.94 2."
        },
        {
            "title": "Baseline",
            "content": "325.76 3.47 0.84 0. 0.55 0.54 0.55 The results show that incorporating stochasticity improves FID and recall over deterministic argmax, while CFG further enhances image fidelity and overall generation quality. Notably, the CFG sampling variant achieves the best performance with an FID of 2.89."
        },
        {
            "title": "5.4.2 Visualization of Student-Forcing Inputs During Training",
            "content": "To better understand the behavior of student-forcing (SF) within our SAR training framework, we visualize the intermediate latents produced during two consecutive SF rollout steps. This provides an intuitive, fine-grained view of how self-generated latents evolve, how errors propagate across scales, and how they differ from the teacher-forcing (TF) pathway. Our visualization uses the same notation and formulation introduced in Sec. 4.1. Recall that naïve student-forcing predicts the next-scale latent from its own previously generated latents: fi = gθ( f1:i1), LSF = i= ℓ( fi, fi), (12) which exposes the model to inference-like conditions but suffers from drift, error amplification, and destabilized supervision. In contrast, SAR replaces naïve SF with Stagger-Scale Rollout (SSR), which produces both teacher-forced predictions and one-step, structure-preserving student-forced trajectory: ˆf (T ) = gθ(f1:i1), (T ) = Upsample( ˆf (T ) = gθ( (T ) ˆf (S) 1:i1). ), (13) (14) (15) Visualization Setup. Figure 8 shows the intermediate latents obtained when performing two consecutive student forcing rollout steps, visualized across scale transitions: 10 13, 13 16. Each row in the figure corresponds to specific component in the SAR rollout pipeline: Row 1: Ground-truth latents (f10, f13, f16). These represent the teacher-forcing targets at each scale. Row 2: TF model inputs (up(f8), up(f10), up(f13)). Ground-truth latents are upsampled to match the next-scale resolution. Row 3: SF inputs ( (T ) 8 , (S) 10 , (S) 13 ). These are obtained by shifting TF predictions to the next scale at scale 8: And two consecutive SF steps at scales 10 and 13. = Upsample( ˆf (T ) (T ) ). Row 4: SF predictions ( ˆf (S) 10 , ˆf (S) 13 , ˆf (S) 16 ). These are produced by feeding the shifted predictions into the model: ˆf (S) = gθ( f1:i1). Row 5: Difference maps (i). To quantify the student-forcing deviation at each scale, we compute: = (cid:12) (cid:12) fi (S) (cid:12) (cid:12) (cid:12) (cid:12) . Rows 35 make the effect of student-forcing explicit: how student-generated latents enter the next-scale prediction, how the model responds, and where errors emerge. Observations. Three consistent observations emerge from the visualization: 1. Error amplification across scales. Deviations introduced at coarse scales (e.g., 10 13) propagate and typically enlarge at finer scales (13 16), demonstrating why naïve SF destabilizes training. 2. Structured corrections under SSR. Because SSR starts from teacher-forced predictions, the SF inputs remain structurally meaningful. As result, the model corrects local distortions rather than diverging, which stabilizes hierarchical learning. 3. Alignment between trajectories. The CSFL drives ˆf (S) , yielding visibly smaller difference maps compared to naïve SF. This explains why SAR can leverage student-forcing without drifting off-manifold. to match the stable TF predictions ˆf (T ) Together, these visualizations clarify why naïve SF fails and why SAR succeeds: SSR ensures that self-generated inputs remain structured, while CSFL ensures that student-forced latents follow the correct predictive trajectory. This resolves the supervision inconsistency and the traintest gap in scale-wise autoregression."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Self-Autoregressive Refinement (SAR), lightweight post-training framework that mitigates the traintest mismatch and scale-wise learning imbalance in previous Visual Autoregressive models. By incorporating Stagger-Scale Rollout for efficient exposure to self-generated latents and Contrastive Student-Forcing Loss for stable refinement, SAR aligns training dynamics with inference behavior while adding minimal computational cost. Applied to state-of-the-art AR models, SAR consistently improves perceptual quality and robustness across model sizes, achieving up to 5.2% FID reduction on ImageNet-256 with less than 5.5% of pretraining compute. Our results demonstrate that fine-grained autoregressive refinement is both practical and effective, offering scalable path toward stronger and more reliable visual autoregressive generation. 16 Figure 8 Visualization of student-forcing inputs during training. We show ground-truth latents, teacher-forcing (TF) inputs, student-forcing (SF) inputs, SF predictions, and the corresponding difference maps across two consecutive rollout steps (10 13 and 13 16)."
        },
        {
            "title": "References",
            "content": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36743683, 2018. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Zigeng Chen, Xinyin Ma, Gongfan Fang, and Xinchao Wang. Collaborative decoding makes visual auto-regressive modeling efficient. arXiv preprint arXiv:2411.17787, 2024. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation. arXiv preprint arXiv:2410.08159, 2024. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):22492281, 2022. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Siyu Jiao, Gengwei Zhang, Yinlong Qian, Jiancheng Huang, Yao Zhao, Humphrey Shi, Lin Ma, Yunchao Wei, and Zequn Jie. Flexvar: Flexible visual autoregressive modeling without residual prediction. arXiv preprint arXiv:2502.20313, 2025. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012410134, 2023. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532, 2022. Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi Li. Scalable autoregressive image generation with mamba. arXiv preprint arXiv:2408.12245, 2024a. Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441125468, 2024b. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024c. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024d. Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 4555, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024a. Sucheng Ren, Yaodong Yu, Nataniel Ruiz, Feng Wang, Alan Yuille, and Cihang Xie. M-var: Decoupled scale-wise autoregressive modeling for high-quality image generation. arXiv preprint arXiv:2411.10433, 2024b. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 19 Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Hu Yu, Hao Luo, Hangjie Yuan, Yu Rong, and Feng Zhao. Frequency autoregressive image generation with continuous tokens. arXiv preprint arXiv:2503.05305, 2025. Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024a. Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pages 260278. Springer, 2024b."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Australian Institute for Machine Learning, Adelaide University"
    ]
}