{
    "paper_title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
    "authors": [
        "Lovish Madaan",
        "Aniket Didolkar",
        "Suchin Gururangan",
        "John Quan",
        "Ruan Silva",
        "Ruslan Salakhutdinov",
        "Manzil Zaheer",
        "Sanjeev Arora",
        "Anirudh Goyal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own \"thoughts\" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 3 2 1 1 0 . 0 1 5 2 : r Rethinking Thinking Tokens: LLMs as Improvement Operators Lovish Madaan1,2, Aniket Didolkar1,3, Suchin Gururangan4,, John Quan1, Ruan Silva1, Ruslan Salakhutdinov1, Manzil Zaheer, Sanjeev Arora1,5, Anirudh Goyal1 1Meta Superintelligence Labs, 2University College London, 3Mila, University of Montreal, 4Anthropic, 5Princeton University Work done at Meta Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own thoughts with continuum of possible strategies. We study an inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields subcase Sequential Refinement (SR) (iteratively improve single candidate answer) which provides performance superior to long CoT (at the cost of higher latency). Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (+11% on AIME 2024 and +9% on AIME 2025). Correspondence: lovish@meta.com, agi@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Scaling language models to solve harder problems has increasingly relied on eliciting explicit reasoning traces (thinking tokens) at inference time (Wei et al., 2022; Jaech et al., 2024; Guo et al., 2025). While longer traces often correlate with accuracy, they entangle reasoning depth with sequence length and inherit long-context failure modes (Ghosal et al., 2025). In parallel, the field is gravitating towards self-improvement: systems that refine their own outputs via self-directed operations (critique, revision, debate, sample-and-select) without expert supervision (Gou et al., 2023; Du et al., 2023b; Irving et al., 2018; Yao et al., 2023; Pan et al., 2025; Zhang et al., 2025). Figure 1 AIME 2024: Accuracy vs. sequential budget Bseq. We compare Long CoT, SR, and PDR; the dashed curve is an oracle upper bound (Oracle-PDR) that perfectly transmits any correct solutions under the same budgets to the compact workspace. 1 Figure 2 (a) Parallel-Distill-Refine (PDR). In round r, the model generates Mr parallel drafts, then distills them into compact workspace using one of the schemes in (b); the refined state seeds the next round. (b) Distillation schemes used to build the workspace (e.g., global summary, shared top-k, per-sample top-k, random-k). (c) Three inference regimes. Top-Long chain-of-thought: single, long trace. Middle-Sequential Refinement (SR): one draft updated over short rounds. Bottom-PDR: each round spawns Mr drafts, distills into workspace, and refines. The example shows 3-round configuration = (8, 4, 1) (configuration is hyperparameter, and any other choice is possible). Across panels, the per-call sequential budget Bseq (latency proxy) is held fixed, while PDR increases total compute Btotal via parallelism without increasing per-call context. Stepping back from the rich body of work, one begins to see LLM inference as malleable concept; instead of single reasoning trace one encounters choices to be made from larger pool: generate fresh answers; critique/revise/debate/summarize generated answers; create an updated answer. With this choice comes an unexplored Pareto-frontier: What is the best possible task accuracy achievable after fixing constraints on the inference process, e.g.: (i) total tokens across all generations, (ii) max depth of the generation chain (latency), (iii) total context length, and (iv) total compute (which depends on all of the above in complicated and system-dependent ways). The confounding factor is that iteration alone does not guarantee progress. Simply asking the model to try again risks forgetting useful partial results and repeating earlier mistakes. Naïvely appending all prior attempts to the context recreates long-context failures and scales cost with the number of rounds. Current models suffer from anchoring biases (see Figure 6, 8) as well as forgetfulness. viable scheme needs compact state that (i) carries forward salient facts and intermediate results, (ii) flags disagreements and open subgoals, and (iii) remains bounded so each generation (and overall context-size) stays short. This paper studies inference strategies that generate many tokens with compact context size. Instead of long chains of thought, inference has phases that generate solutions within the allowed context/token budget and then write bounded, round-wise summary/report (e.g., listing agreements, contradictions, intermediate results, and open subgoals). The next phase starts with only this summary and uses available workspace for fresh generations (which benefit from accumulated wisdom in the summary). Iterating this process can generate long thinking albeit with bounded context size. 1 We study two inference instantiations: (i) Sequential refinement ( SR), where single artifact (solution, proof, 1Such LLMs fall within traditional framework of randomized space-bounded computation. Computational complexity theory Arora & Barak (2007) shows it is capable of surprisingly powerful reasoning, such as determining connectivity of much larger graphs that cannot even fit the working memory; see Section C. 2 program) is iteratively improved for fixed number of steps; and (ii) Parallel-Distill-Refine ( PDR) (round-wise workspace), where each round samples drafts in parallel, distills them into bounded summary for the next round, and continues. The workspace is not persistent across rounds; it is freshly synthesized for each round. central challenge is information synthesis: compressing salient facts and intermediate results; flagging uncertainty; and retiring stale information. Its effectiveness hinges on four meta-skills: verification (detect and localize errors via self-judging and cross-candidate checks), refinement (use feedback/context to improve the artifact), compression (retain only past history via bounded summaries rather than replay), and diversification (exploratory variation to avoid consensus collapse). Learning to improve short-context iteration. It is also of interest to teach the model policy that effectively leverages this improvement operator. Standard RL training for reasoning models typically optimizes single, long chain-of-thought conditioned on the prompt, with reward on the final answer (Shao et al., 2024; Guo et al., 2025). PDR, by contrast, comprises multiple short iterations that read bounded summary, write refinement, and re-synthesize fresh summary. This creates train-test mismatch in the information flow (short updates vs. one long trace). To make sure training is consistent with deployment, we optimize an objective that unrolls the operator itself during training: sample short drafts, distill them into compact summary, and condition on the prompt plus that summary to produce refined attempt. We use verifiable rewards to supervise the end-to-end computation. This objective narrows the traintest gap. Results. On math tasks, iterative pipelines surpass single-pass baselines at matched sequential budgets; with shallow PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025). Making training consistent with inference, via an operator-consistent RL objective that optimizes the same read/write interface used at test time yields further improvements (e.g., 5% on AIME 2024 and AIME 2025 when mixing standard and operator RL). These findings suggest that iteration with short contexts and compact summaries can substitute for long traces while holding latency fixed."
        },
        {
            "title": "2 Background & Related Work",
            "content": "Overview. This section situates our work among several threads that seek to scale test-time reasoning: longtrace chains of thought and self-consistency; self-improvement and debate; structured search (trees/graphs of thoughts); multi-trace selection and aggregation; turning compute into supervision; memory/summarization; adaptive multi-turn RL; and learning-to-search/planning. We view these through unified lens: inference as round-wise improvement operator under explicit budgets; holding the per-call sequential budget Bseq fixed while varying total compute Btotal. We do not claim the micro-primitives themselves are new: parallel sampling, aggregation and selection (Wang et al., 2023; Fu et al., 2025; Zhao et al., 2025), sequential revision, critique-revise-verify and debate (Madaan et al., 2023; Gou et al., 2023; Shinn et al., 2023; Du et al., 2023b), structured search (Yao et al., 2023; Besta et al., 2024), and summarization/memory (Wu et al., 2025; Yang et al., 2024) are all well explored. Our contribution is to (i) formalize these pieces within single round-wise operator (SR, PDR), (ii) analyze compute-normalized depth via shallow parallel rounds and distillation at matched Bseq while varying Btotal, and (iii) make training consistent with deployment via operator-consistent RL. The text below organizes prior work along these axes and clarifies similarities and differences. Test-time reasoning with long traces. Eliciting step-by-step chains of thought improves accuracy on multi-step tasks (Wei et al., 2022). Recent reasoning models (e.g., OpenAI o1, Deepseek r1 ) explicitly trade more test-time thinking for better results, increasing tokens and latency (Jaech et al., 2024; Guo et al., 2025). Sampling multiple traces and aggregating answers (self-consistency) further boosts performance but scales cost with the number of samples (Wang et al., 2023). Our approach targets complementary design point: keep each call short while letting evidence accumulate across rounds via bounded, re-synthesized summary. Self-improvement. growing line of work lets models critique and refine their own outputs: Self-Refine alternates self-feedback and revision (Madaan et al., 2023); Reflexion maintains textual memory to guide subsequent attempts (Shinn et al., 2023); CRITIC verifies with tools and revises accordingly (Gou et al., 2023). Multi-agent debate improves factuality and reasoning via argumentation and adversarial checking (Irving et al., 2018; Du et al., 2023b). Our operator shares the self-improvement spirit but constrains per-call context by using round-wise, re-synthesized compact state (r) instead of replaying full histories. Compute as Teacher (CaT) synthesizes single reference from parallel rollouts and optimizes the policy toward it, converting extra 3 inference compute into reference-free supervision. Our focus differs: we use parallelism within PDR to explore and distill at inference time, and analyze compute explicitly under fixed Bseq while varying Btotal. Multi-trace selection and aggregation. Confidence-aware test-time scaling generates multiple traces in parallel and filters or re-weights them with model-internal confidence, improving the accuracycompute trade-off without extra training (Fu et al., 2025). Another line trains an aggregator to select or combine solutions using RL rather than majority vote or reward-model ranking (Zhao et al., 2025). In contrast, we cast inference itself as round-wise operators (SR, PDR), with aggregation as one of the meta-cognitive abilities necessary to improve the performance, and propose PDR RL to reduce the train-inference mismatch. Zheng et al. (2025) propose Parallel-R1, an RL framework that instills parallel thinking through progressive curriculum: supervised fine-tuning on easier prompts to bootstrap the behavior, followed by RL on harder problems. On math benchmarks (MATH, AMC23, AIME), Parallel-R1 improves over sequential-thinking RL baseline and shows that parallel thinking functions as mid-training exploration scaffold before aiding verification in later stages. Structured search beyond single chains. Prompting schemes structure exploration explicitly: Tree of Thoughts (ToT) expands and evaluates branches of reasoning (Yao et al., 2023); Graph of Thoughts generalizes to arbitrary thought graphs (Besta et al., 2024); Least-to-Most decomposes problems into subproblems solved in sequence (Zhou et al., 2022). These methods typically grow tokens with breadth/depth or rely on long contexts to carry intermediate state. In contrast, PDR concentrates exploration within round, then distills to bounded (r), preventing unbounded context growth. Multi-agent debate and compressed debate. Debate-style methods have multiple LLM agents propose answers and iteratively read and critique one another, often improving robustness (Du et al., 2023a). Our ParallelDistill-Refine ( PDR) can be seen as compressed debate: treat rounds diverse drafts as agent outputs, but instead of replaying full transcripts, distill them into bounded (r) that conditions the next round. This preserves cross-agent scrutiny while controlling per-call context and both Bseq and Btotal. Compact summaries vs. persistent memory. Agent systems often add external memory or retrieval to persist context across sessions/tasks; thought buffers and memory-augmented agents exemplify this direction (e.g., Buffer-of-Thoughts) (Yang et al., 2024). We instead use non-persistent, round-wise summaries. This choice keeps prompts short and reduces long-context failure modes (Liu et al., 2023). Concurrently, Wu et al. (2025) introduce ReSum, web-agent paradigm that periodically summarizes growing interaction histories into compact states and pairs this with ReSum-GRPO so agents learn summary-conditioned reasoning. Our work differs in focusing on multiple inference-time operators instead of summary (SR, PDR), alongside operator-consistent RL aligned to the round-wise interface. Didolkar et al. (2025) introduce Metacognitive Reuse, extracting recurring reasoning into concise, named behaviors that reduce token usage and can be distilled via SFT. This is complementary to our within-instance PDR: rather than compressing cross-instance procedures, we parallelize and distill drafts per instance. Adaptive multi-turn training. Unary-feedback multi-turn RL (try again) trains models to revise across rounds improving multi-turn accuracy with minimal supervision (Liu et al., 2025a). Exploratory Iteration (ExIt) leverages the recurrent structure of self-improvement by performing multi-step refinement at test time while training emphasizes the most informative single-step iterations (Jiang et al., 2025). SR is similar to try again iterative interface. RL with on-policy tree search. Hou et al. (2025) introduce TreeRL, which integrates on-policy tree search into RL for LLM reasoning, improving exploration and providing dense, process-level rewards (Xie et al., 2024) compared to independent chain sampling with outcome-only supervision. TreeRL thus exemplifies training-time, search-augmented improvement operator. In contrast, our operator-consistent RL trains in the same iterative interface used at inference: we unroll single round (akin to shallow tree expansion) but optimize with outcome supervision without deep tree search. promising direction is to incorporate process-level rewards into this operator-consistent setting and study their impact on the test-time performance of round-wise operators (SR, PDR)."
        },
        {
            "title": "3.1 Problem setting and notation\nWe consider tasks x (e.g., a math problem) and aim to produce a high-quality final artifact sfinal (solution,\nproof, or program) under a given token budget. Let Mθ denote a (frozen or trainable) LLM used as an\nimprovement operator. Given a current artifact st (single generation or set of generations) and a compact\ntextual workspace Ct, the model proposes a refinement:",
            "content": "The workspace Ct is bounded summary (Ct κ tokens) meant to capture agreements, contradictions, intermediate results, and open subgoals. st+1 Mθ(x, st, Ct). (1) Read-write-compress cycle. Each step (i) reads the current workspace Ct, (ii) writes refined artifact st+1 via Mθ, and (iii) compresses back into bounded workspace for the next step using synthesis operator D: Ct+1 D(x, st+1), Ct+1 κ. (2) Token budgets. We evaluate every method under two budgets: Bseq = (cid:88) cP (cid:0)inc + outc (cid:1) Btotal = (cid:88) (cid:0)inc + outc (cid:1) c= (latency proxy; tokens along the accepted path), (3) (compute/cost proxy; all calls, including discarded branches). (4) Here = 1, . . . , indexes all model calls (prompts, candidate generations, and distillation/summary updates); inc and outc are the input and output tokens for call c; and {1, . . . , C} is the final accepted path. We report accuracy as function of both axes and match baselines per axis (e.g., equal Bseq for latency-controlled comparisons)."
        },
        {
            "title": "3.2 Operator Instantiations",
            "content": "We study two short-context iterative refinement pipelines. 3.2.1 Sequential refinement (SR; depth over single candidate). We set Ct for all and iteratively improve single artifact for rounds: st+1 Mθ(x, st, ), = 0, . . . , 1, sfinal = sR. (5) SR with compact workspace. In SR, no explicit workspace is provided. We also evaluate variant that inserts an error analysis step between rounds: rather than directly refining the previous answer, the model first identifies and explains flaws in the current solution, then generates revised solution. These notes act as transient, local workspace at each round. 3.2.2 Parallel-Distill-Refine (PDR; round-wise workspace) We do not maintain persistent memory. Instead, for rounds = 1, . . . , R; we sample Mr drafts (Parallel) conditioned on the current bounded summary, then re-synthesize (Distill) fresh bounded summary for the next round: (Parallel) S(r) = (cid:8) s(r) (Distill) (r) D(cid:0)x, S(r)(cid:1), Mθ(x, (r1)) (cid:9)Mr i=1, (r) κ. (0) = , (6) (7) We enforce single generation in last round MR = 1; which is returned as sfinal. The summary is round-wise and non-persistent: earlier text is not replayed, preventing growth in per-call context. Why round-wise summary? Replay of all prior attempts scales linearly with steps and reintroduces long-context failure modes. Re-synthesizing (r) from the current drafts keeps the memory bounded (C (r) κ) and focuses each round on the most recent and informative evidence. Constructing the compact summary (r). We consider several practical instantiations of the distillation operator D, all obeying (r) κ: Global summary: Produce single shared (r) that captures agreements, contradictions, derived facts, unresolved subgoals, and next actions. This emphasizes verification and comparison while retiring stale or contradicted information. Extractive top-k evidence (shared): Instead of free-form text, select the solutions from S(r) as the workspace itself, trading compression for higher fidelity to the best evidence. Random-k / bootstrapped workspaces: For the next round, construct multiple small workspaces by randomly sampling solutions per generation. This injects diversity and mitigates premature consensus while keeping each workspace small. Budgets. Tokens used for Parallel, Distill, and Refine contribute to Btotal. The reported latency Bseq only counts the tokens on the accepted generate distill refine path for the final output."
        },
        {
            "title": "3.3 Operator-Consistent Training\nThe previous sections treat Mθ as frozen and rely purely on prompting/orchestration. We now make sure\ntraining is consistent with deployment/inference by optimizing the model under the same short-context,\niterative interface used at test time.",
            "content": "Motivation. Most RL for reasoning LLMs optimizes single, long chain-of-thought trajectory. If inference instead runs multiple short passes with compact workspace C, this creates train-test mismatch. We remove this mismatch by mixing two training modes: (i) standard long-trace optimization, and (ii) operator rollouts that execute the generatedistillrefine interface under short contexts. Base Algorithm. For the baseline RL, we use the CISPO objective from Minimax-M1 (Li et al., 2025). For given prompt x, the generator π( θold) generates rollouts {oG i=1} using the old policy θold. Automated checkers like sympy (Meurer et al., 2017) or math-verify2 are used to assign scalar rewards ri (1) to each of the rollouts. CISPO combines the group-normalized advantage from GRPO (Shao et al., 2024) with REINFORCE (Williams, 1992) to achieve the following objective: JCISPO(θ) = xD,{oi}G i=1π(x;θold) 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) i= t=1 sg(ri,t(θ))Ai log(π(oi,tx, oi,<t; θ)) (8) where Ai = the asymmetric clipping from Yu et al. (2025) as follows: rimean({r}G std({r}G j=1) j=1) is the advantage, sg is the stop-gradient operation, and ri,t(θ) is computed using ri,t = clip (cid:18) π(oi x, oi,<t; θ) π(oi x, oi,<t; θold) , 1 ϵ, 1 + ϵ+ (cid:19) (9) π(oix,oi,<t;θ) π(oix,oi,<t;θold) where is the importance-sampling (IS) weight. Additionally, we add an SFT loss (negative log-likelihood) similar to Seed et al. (2025) on rollouts which lead to positive rewards. The final training objective becomes: (θ) = JCISPO(θ) + α JSFT(θ) (10) 2https://github.com/huggingface/Math-Verify 6 where α is set to small value like 0.1 in our experiments. The addition of this SFT loss boosts the utilization of positive rollouts and enforces better verification behavior in model training. Data mixture. At each update, draw mini-batch = {xi}N and split it evenly into two sub-batches Btrace and Bop with Btrace = N/2 and Bop = N/2. We train on Btrace with standard long-trace objective Jtrace(θ), and on Bop with operator rollouts under short context, yielding Jop(θ). The per-step objective is the average of the two: i=1 Jtrain(θ) = 2 Btrace trace (θ) + 1 2 Bop op (θ), (11) and Bop where Btrace trace we use 1:1 split in our experiments. op denote the losses computed on their respective sub-batches. Other ratios are possible; Mode A: Standard long-trace optimization. Given x, sample single, long trajectory s1:T Mθ(x) and optimize conventional RL verifiable signal (e.g., rule based verifiable reward for math problems). This preserves the models ability to reason in extended traces when available. Mode B: Operator rollouts under short context. We roll out the same interface used at test time but with one round for stability and cost. (i) Parallel-Distill-Refine ( PDR; one-round rollout). 1. Generate parallel generations (reasoning traces, solutions) conditioned on an empty summary: = { si Mθ(x, (0)=) }M i=1. 2. Distill to bounded, round-wise summary C. 3. Refine single candidate conditioned on C: Mθ(x, sj, C). Why one round during training? Rolling out single PDR round (with early drafts, distillation to C, and single refinement) captures the key interface while controlling Btotal and stabilizing RL. At inference we can run multiple rounds (R > 1) using the same operator. Our datamix preserves competence on long traces while teaching the model to reason across short iterations. PDR is emulated by one-round of paralleldistillrefine rollout where the model observes (x, C) and is optimized with verifiable reward on the final solution trace."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we compare the Sequential refinement (SR) and Parallel-Distill-Refine (PDR) operators against long chain-of-thought baselines under budget-aware protocol. We measure accuracy with symbolic verifiers like sympy (Meurer et al., 2017) and math-verify3. Additionally, we report the results as functions of both the sequential budget Bseq (latency proxy along the accepted path) and the total budget Btotal (all tokens across calls). We try to answer the following four research questions through our experiments: RQ1: Can short-context iterations outperform long traces by comparing {SR, PDR} to long-trace CoT at matched Bseq and Btotal. RQ2: Figuring out the best distillation strategy for producing (r) by comparing three variants: global summary, extractive top-k, and random-k bootstraps. RQ3: Identifying the effect of the verification ability of given model on the final performance. RQ4: Whether operator-consistent training shifts the Pareto-Frontier. We compare operator-consistent + standard RL with standard single-trace RL (Sec. 3.3). 3https://github.com/huggingface/Math-Verify 7 Figure 3 AIME 2024: Iterative improvement beats single-pass long-CoT at matched sequential budgets. The x-axis reports Bseq: the thinking tokens consumed along the accepted path of the iterative chain, plus any distilled summary that conditions the next step. Tokens spent on unused parallel proposals are excluded, so Bseq serves as latency proxy. At comparable Bseq, both SR and PDR outperform the single-pass long CoT baseline, with PDR yielding the largest gains by converting additional total compute (via parallelism) into accuracy without increasing per-call context."
        },
        {
            "title": "4.1 Experiments to understand SR and PDR\nSetup. We evaluate SR and PDR as inference-time operators on math problems. Given a prompt x, the model\nproduces a thinking trace and a final solution. The thinking spans, delimited by <think> . . . </think> are\nstripped out and only the self-contained solutions are used to build the inputs for subsequent rounds. We\nevaluate on AIME 2024 and AIME 2025 (AoPS, 2025) and report the accuracy computed over 16 independent\ngenerations - mean@16.\nModels and inference budgets. We evaluate o3-mini (“medium” reasoning effort) (OpenAI, 2025) and gemini-\n2.5-flash (Comanici et al., 2025). For gemini-2.5-flash, we vary the thinking budget from 8,192 to 24,576\ntokens (its maximum), and reserve an additional 2,048 tokens for the final solution. Because o3-mini does not\nexpose a separate thinking budget, we vary its maximum generation length from 10,240 to 26,624 tokens to\nmatch the same total allowance (assuming 8,192–24,576 thinking tokens plus 2,048 solution tokens). Both\noperators (SR and PDR) are compared at matched per-call sequential budgets Bseq (latency proxy) while\nallowing different total token budgets Btotal via parallelism. All runs use temperature = 1.0 and top-p = 1.0.",
            "content": "RQ1: Do short-context iterations beat long traces at matched latency? Sequential Refinement (SR). For the SR operator, we run o3-mini and gemini-2.5-flash for thinking budgets {8192, 16384, 24576} and refinement rounds {1, . . . , 6}. The prompt template is given in B.1. SR with local workspace. We also evaluate variant of SR that inserts brief, local workspace between refinements: the model first performs error analysis: identifying and explaining flaws in the current solution, 8 Figure 4 Token Budgets comparison: We plot all the different configurations for Long CoT, SR and PDR operators for both Bseq and Btotal token budgets for gemini-2.5-flash. For Bseq, PDR forms the Pareto-frontier and gives consistent gains over Long CoT and SR. However, for Btotal, SR forms the pareto-frontier because there are no parallel drafts involved so no generations are discarded. Figure 5 AIME 2024: Long CoT, SR, and PDR at thinking budget of 24576. The x-axis reports Bseq: the thinking tokens consumed along the accepted path of the iterative chain, plus any distilled summary that conditions the next step. Tokens spent on unused parallel proposals are excluded, so Bseq serves as latency proxy. At token matched budget of 442k tokens, SR has score of 90.4 but Bseq of 442k, whereas PDR has score of 90.6 but Bseq of 172k tokens. and then generates revised solution conditioned on these notes. All other settings (prompts, budgets) match standard SR for fair comparison. As shown in Table 1, this augmentation is effective for o3-mini but not for gemini-2.5-flash. Table 1 SR operator variants: Instead of just asking the model to refine the solution, we also ask the model to find and analyze errors in the solution followed by the correct solution. Error analysis followed by solution generation leads to better performance for o3-mini but not for gemini-2.5-flash. Model Benchmark Thinking Budget gemini-2.5-flash AIME 2024 gemini-2.5-flash AIME 2025 AIME 2024 gpt-o3-mini AIME 2025 gpt-o3-mini 24576 24576 24576 24576 SR SR-Error 87.71 79.17 82.08 77. 88.75 78.75 80.83 73.13 Parallel-Distill-Refine (PDR). We evaluate PDR under fixed thinking budget using three settings. These settings differ by number of rounds, number of parallel generations in each round, and selecting the candidate solutions to carry forward via textual workspace: = [8], k=4; = [16, 8], k=4; and = [16, 8, 4], k=2. Here = [g1, . . . , gr] specifies the number of parallel generations in each round, and mind gr is the number of candidates forwarded to the next round. For distillation (i.e., selecting the candidates to carry forward), we compare: Random-k (uniform sampling per instance); Top-k (model-graded) where the same base model assigns quality score to each candidate and we keep the top per instance (we report both shared rubric and per-instance grading prompt); and global-summary that aggregates all candidates using summarization prompt. Refinement, selection, and summarization prompts are detailed in B.2. Token-matched baselines. In Figure 5 we sweep the depth of SR to find the sequential budget Bseq at which it matches PDR. Holding the total token budget fixed at Btotal = 442k (for SR, Btotal = Bseq since there is no parallelism), PDR attains the target accuracy with Bseq = 172k. To reach the same accuracy, SR is run for 17 rounds, consuming Bseq 442k. Thus, at equal Btotal, PDR achieves the same accuracy with 2.57 smaller sequential budget by converting parallel compute into accuracy without lengthening per-call context. Figure 6 AIME 2024: Anchoring bias due to +ve and ve examples: With PDR we compare three selection policies for the summary: Random-k, Oracle-Incorrect (all candidates are incorrect), and Oracle-Correct (all candidates are correct), evaluated on both gemini-2.5-flash and o3-mini. Across all thinking budgets, admitting only incorrect candidates into the summary yields pronounced drop in accuracy, whereas admitting only correct candidates improves over the Random-k baseline. The degradation under Oracle-Incorrect is larger for o3-mini than for gemini-2.5-flash, indicating weaker self-verification in o3-mini. Results. Figures 3 and 9 report accuracy on AIME 2024 and AIME 2025 under the same effective token budgets Bseq. We observe consistent gains when moving from long chain-of-thought to SR, which continue when moving from SR to PDR. For o3-mini at an effective budget of 49k tokens with per-call thinking budget of 16k, accuracy improves from 76.9 (Long CoT) to 81.5 (SR) and 86.7 (PDR), an absolute improvement of +9.8 percentage points over Long CoT. gemini-2.5-flash shows smaller deltas from SR to PDR than o3-mini, suggesting stronger intrinsic self-verification in gemini-2.5-flash. AIME 2025 exhibits similar trends. RQ2: Which distillation (i.e., summarization) strategy works best? 10 Table 2 Effect of distillation operator D: We compare the effect on final performance by changing the distillation operator D. Each table column reports accuracies on AIME 2024 / AIME 2025. We compare four choices: (i) Global summary: aggregate all candidates and synthesizes single compact summary; (ii) Per-sample top-k: each downstream branch selects its own top-k candidates as the summary; (iii) Shared top-k: single set of top-k candidates is shared as the summary across generations for next round; (iv) Random-k: each generation for next round receives candidates sampled uniformly at random for the summary. Overall, global summary and per-sample top-k tend to perform best, with gains more pronounced at higher thinking budgets. For o3-mini on AIME 2025, global summary yields the largest improvement, suggesting strong summarization ability in o3-mini. We use = 2 for these experiments. Budget 8192 16384 24576 Global PS top-k Shared top-k Random-k Global PS top-k Shared top-k Random-k gemini-2.5-flash gpt-o3-mini 83.13 / 66.88 86.46 / 84.38 88.75 / 87.71 83.75 / 71.88 86.88 / 83.96 90.63 / 85.00 84.17 / 70.21 86.46 / 83.75 87.71 / 85.42 83.33 / 66.67 86.25 / 80.63 88.13 / 79. 86.04 / 82.92 86.46 / 84.79 85.42 / 83.54 84.79 / 76.04 85.42 / 74.58 85.21 / 77.92 85.00 / 76.67 85.83 / 76.88 85.00 / 75.42 82.50 / 71.25 83.13 / 71.88 82.29 / 72.08 Table 2 studies the distillation operator in PDR under (fixed number of rounds, number of generations in each round) setting = [16, 8, 4] with = 2 candidates per round. Across datasets and base models, per-sample top-k and global-summary selection consistently outperform shared top-k and random-k, and the margin widens as the thinking budget increases. The main exception is AIME 2025 with o3-mini, where global summary outperforms the alternatives. We hypothesize that o3-minis summarization is particularly effective at capturing cues from both correct and incorrect drafts, and these cues, when distilled, lead to stronger subsequent refinements. RQ3: How does the verification abilities effect the inference time performance ? Oracle PDR analysis. To understand the role of model verification within PDR, we intervene on the set of candidates admitted to the summary at each round. We use three-round PDR with number of generations in each round as [16, 8, 4] and top-k selection (k = 2), and compare: (i) Random-k: choose candidates uniformly at random from the previous depth; (ii) Oracle (Correct): admit only correct candidates to the compact summary when available; (iii) Oracle (Incorrect): admit only incorrect candidates. From Figures 6 and 8, we observe that injecting incorrect candidates (Oracle (Incorrect)) causes large drops for all models. The degradation is substantially larger for o3-mini than for gemini-2.5-flash, suggesting stronger self-verification and recovery in the latter. The same trend holds across AIME 2024 and AIME 2025. We further provide detailed analysis on the mechanics of PDR and self-verification requirements to improve downstream performance in Section E."
        },
        {
            "title": "4.2 Operator-consistent RL Training",
            "content": "RQ4: Does operator-consistent training move the Pareto Frontier? Building on the above, we present an operator consistent RL training strategy. This also addresses train-test gap where models are not explicitly trained to perform PDR. Training setup. We train an 8B dense model similar to Llama-3 style architecture (Dubey et al., 2024). For warm-start supervised fine-tuning (SFT), we use GPT-OSS 120B (Agarwal et al., 2025) to generate synthetic traces for math and code prompts sampled from Polaris-53K (An et al., 2025) and DeepCoder (Luo et al., 2025), respectively. We run SFT for 8B tokens (4 epochs). For RL training, we use the Polaris-53K dataset. Both SFT and RL datasets are decontaminated against AIME 2024/2025 (AoPS, 2025) and MATH-500 (Hendrycks et al., 2021). Further details and hyper-parameters are detailed in Section B.3. Baseline RL As described in Section 3.3, we use the CISPO objective for RL post-training (Li et al., 2025). We set ϵ = 0.0 and ϵ+ = 5.0, and remove zero-variance prompts from given batch (Seed et al., 2025). We use forced interruptions (Hong et al., 2025; Yang et al., 2025) to control generation length from exploding after thinking budget of 16, 384 tokens. We additionally keep buffer of 2048 tokens for the final solution, thus keeping maximum generation length of 18432. 32 generations are sampled per prompt with batch size of 32, resulting in global batch size of 1024 generations per gradient step. Following (Liu et al., 2025b), we use mini-batch size of 256 and perform 4 gradient updates per rollout step. 11 Table 3 Operator RL results: Comparison of RL training objectives on AIME 2024/2025 at matched sequential budget Bseq = 65,536 tokens using dense 8B model. Mixing standard RL with operator-consistent RL (Op-RL) yields consistent gains for iterative inference operators such as PDR while preserving performance on the Long CoT baseline. Op-RL can also be applied as continual RL to the existing baseline RL checkpoint. Model AIME 2024 AIME 2025 Long CoT PDR Long CoT 8B SFT Policy 8B Baseline RL 8B PDR RL 8B Continual PDR RL 47.50 67.50 69.58 70.00 62.92 75.83 79.17 80.83 35.00 59.58 57.50 61.25 PDR 47.50 65.83 67.50 70.42 Operator-consistent RL with PDR. For training, we use the PDR operator with configuration (4 parallel generations, 1 round) = [4]; = 2, and use the training objective described in Equation (11) and make two changes to the baseline RL method above: (i) increasing the input prompt length from 2048 tokens to 10240 to allow for the compact workspace to be part of the input, and (ii) mixing the standard RL and operator RL batches in the dataloader, keeping all other design choices the same. This setup allows to scale inference compute within the RL training. Results. Table 3 summarizes the main results. The resulting model from each RL objective is evaluated for Long CoT generation and PDR. PDR RL improves over the baseline by +3.34 points on AIME 2024 and +1.67 points on AIME 2025. With continual updates starting from baseline RL checkpoint, additional PDR RL yields larger gains of +5.00 and +4.59 percentage points on AIME 2024 and AIME 2025, respectively. Additionally, we also observe marginal gains on Long CoT generations with PDR RL training. These results indicate that training with operator-consistent RL objectives reduces the mismatch between training and deployment, converting extra compute into accuracy without increasing the per-call sequential budget."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we initiate the exploration of broader design space around long CoT. We study two operators in this design space, SR and PDR which give better accuracy compared to standard long CoT, while offering the benefit of smaller context size. Empirically, compact-memory iteration outperforms long-trace baselines at matched Bseq. PDR yields the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025), showing that evidence accumulation via bounded summaries can substitute for long reasoning traces while holding latency fixed. Beyond inference orchestration, making sure that training is consistent with inference using an operator-consistent RL objective further improves performance (e.g., 5% on AIME 2024 and AIME 2025), suggesting that models can learn the meta-skills that make iteration effective. Iterative reasoning improves when diversity, verification, and refinement become reliably good; by measuring and training these micro-skills directly, we can accelerate the gains of improvement operators under fixed latency budgets. Promising future directions include learning to improve the synthesis operator (trainable summaries), adaptive round and fan-out schedules conditioned on uncertainty (adaptive top k), budget-aware controllers for allocating test-time compute, and tighter integration with verifiers and tool use. We also see value in scaling studies and cross-domain evaluations (reasoning, coding, and planning) to map when short-context iteration most benefits accuracy and latency."
        },
        {
            "title": "6 Acknowledgments",
            "content": "This work includes contributions from S.G. during his time at Meta. We also thank Xuewei Wang, Devvrit Khatri, and Alan Schelten for helpful discussions, and Jenya Lee and Abhinav Jauhri for infrastructure and compute support."
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.io/blog/2025/Polaris. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In NeurIPS, 2017. AoPS. Aime problem set 1983-2025, 2025. URL https://artofproblemsolving.com/wiki/index.php/AIME_Problems_ and_Solutions. Sanjeev Arora and Boaz Barak. Computational Complexity. Cambridge University Press, 2007. Bernard Baars. Global workspace theory of consciousness: toward cognitive neuroscience of human experience. Progress in brain research, 150:4553, 2005. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 1768217690, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Hal Daumé III and John Langford. Search-based structured prediction. In ICML, 2006. Aniket Didolkar, Nicolas Ballas, Sanjeev Arora, and Anirudh Goyal. Metacognitive reuse: Turning recurring llm reasoning into concise behaviors. arXiv preprint arXiv:2509.13237, 2025. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023a. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, and Amrit Singh Bedi. Does thinking more always help? understanding test-time scaling in reasoning models. arXiv preprint arXiv:2506.04210, 2025. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua Bengio. Coordination among neural modules through shared global workspace. In International Conference on Learning Representations (ICLR), 2022. arXiv:2103.01197. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025. 13 Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. Treerl: Llm reinforcement learning with on-policy tree search. arXiv preprint arXiv:2506.11902, 2025. URL https://arxiv.org/abs/2506.11902. Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Minqi Jiang, Andrei Lupu, and Yoram Bachrach. Bootstrapping task spaces for self-improvement. arXiv preprint arXiv:2509.04575, 2025. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, and Manling Li. simple \"try again\" can elicit multi-turn llm reasoning. arXiv preprint arXiv:2507.14295, 2025a. doi: 10.48550/arXiv.2507.14295. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025b. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Erran Li Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. URL https://www.together.ai/blog/deepcoder. Notion Blog. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Aaron Meurer, Christopher Smith, Mateusz Paprocki, Ondřej Čertík, Sergey Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason Moore, Sartaj Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017. Team OpenAI. Introducing openai o3 and o4-mini. 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466, 2025. Stéphane Ross, Geoffrey Gordon, and J. Andrew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588:604609, 2020. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. Murray Shanahan. cognitive architecture that combines internal simulation with global workspace. Consciousness and cognition, 15(2):433449, 2006. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv. org/abs/2303.11366, 1, 2023. David Silver, Aja Huang, Chris J. Maddison, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. 14 David Silver, Julian Schrittwieser, Karen Simonyan, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv:1712.01815, 2017. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. Reasoning in token economies: budget-aware evaluation of llm reasoning strategies. arXiv preprint arXiv:2406.06461, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/ 2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, and Jingren Zhou. Resum: Unlocking long-horizon search intelligence via context summarization. arXiv preprint arXiv:2509.13313, 2025. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 37:113519113544, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Open-ended evolution of self-improving agents. arXiv preprint arXiv:2505.22954, 2025. Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, and Ilia Kulikov. The majority is not always right: Rl training for solution aggregation. arXiv preprint arXiv:2509.06870, 2025. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, and Dong Yu. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025. doi: 10.48550/arXiv.2509.07980. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022."
        },
        {
            "title": "A Extended Related Work",
            "content": "Budget-aware evaluation and test-time compute. Recent work argues for comparing methods at matched compute budgets and reporting token usage, and not just accuracy (Wang et al., 2024). Our protocol reports sequential budget Bseq (latency proxy along the accepted path) and total budget Btotal (all tokens, including discarded branches), enabling apples-to-apples comparisons among single-pass, long-trace, sampling-heavy, and iterative pipelines. MCTS, Learning to search and amortizing search. AlphaGo, AlphaZero, and MuZero couple learned policy/value with an expensive test-time search (e.g., MCTS) that serves as an improvement operator; the outputs of search are then distilled back into the network, amortizing future search cost (Silver et al., 2016, 2017; Schrittwieser et al., 2020). Expert Iteration formalizes this loop as policy improvement via planning followed by supervised or RL updates toward the planners targets (Anthony et al., 2017). Earlier learning to search work in structured prediction similarly alternates local rollouts with policy updates (e.g., SEARN, DAgger) (Daumé III & Langford, 2006; Ross et al., 2011). Our setting is analogous in spirit but distinct in mechanics: we operate in the textual reasoning regime with round-wise operators (SR, PDR) that keep the per-call sequential budget Bseq small, optionally raising total compute Btotal via parallel drafts. Operator-consistent RL then amortizes this improvement procedure into the model weights. Global workspace and modular coordination. Our compact, round-wise summary (r) is conceptually related to the shared global workspace proposed by Goyal et al. (2022), which enables coordination among neural modules through small communication bottleneck (inspired by Global Workspace Theory (Baars, 2005; Shanahan, 2006)). In contrast, our workspace is textual, re-synthesized at each round rather than persisted, and used as an inference-time operator. Thus, we borrow the coordination intuition while avoiding long-context replay and architectural changes."
        },
        {
            "title": "B Prompts",
            "content": "B.1 Sequential Refinement Refinement Prompt v e l n math problem i n and a . a e n p by p , and put your a answer h $ boxed { answer } $ . Where [ answer ] problem . j t i number x s n t v h Problem : {{ problem }} Here an example d t s s wrapped n b k : <model_response> # u n p e from v s </model_response> n Treat r o a v f ; and come up with t answer h t a n from a . 16 B.2 Parallel-Distill-Refine Refinement Prompt (Non-summary) v e l n math problem i n and a . a e n p by p , and put your n answer h $ boxed { answer } $ . Where [ answer ] problem . u h n number x s n t v t Problem : {{ problem }} Here some d t s s , each wrapped n b k : <model_response_1> # u n p e from v s </model_response_1> round . . . <model_response_k> # u n p e from v s </model_response_k> round Treat s s s s e i ; and t e p e come up with t answer h t a n from a . Refinement Prompt (Summary) v e l n math problem i n and a . a e n p by p , and put your a answer h $ boxed { answer } $ . Where [ answer ] problem . j t i number x s n t v h Problem : {{ problem }} Here summary h a i r s by few e l s : <summary> # Summary l </summary> u n from p i s e i Treat summary n i d ; and t summary o x come up with an answer . B.3 Training Setup We run small SFT on the pre-trained 8B base model using batch size of 2M tokens, max sequence length of 32768, and learning rate of 2 105 using the AdamW optimizer (Loshchilov & Hutter, 2017) on 32 H100 GPU nodes for approximately 4 epochs and 8B tokens in total. For RL, we use constant learning rate of 5 107, AdamW optimizer (Loshchilov & Hutter, 2017) with ϵ = 1015, weight decay of 0.01, and linear warmup of 100 steps. We use 80 Nvidia H200 GPUs for the baseline RL run with 64/16 generators/trainers split and 288 H200 GPUs for PDR and continual PDR RL with 256/16 generators/trainers split to parallelize 17 Figure 7 Space-bounded Turing Machine [Figure from Computational Complexity by Arora and Barak, 2007]. The input has size and the machine has read-only capability for the input. special tape head can be moved over the input to read bits from it. The amount of working memory (read/write/erase) for actual computation has size S(N ) where S(N ) log . inference during rollout generation. We run all RL training for 800 steps. All evaluations are done with temperature and top-p value of 1.0. Complexity of Space-bounded computation Our work focused on language models that emit reasoning traces that are longer than the context size. We sketch similarity to the setting space-bounded computation which is formally studied in computational complexity theory. Since LLMs have probabilistic output (unless if temperature is set to 0) the fixed-context LLM considered in the paper is most similar to randomized space-bounded machine. The most interesting result about randomized space-bounded machines is that if the input contains graph of vertices, then the randomized machine can determine connectivity of the -vertex graph even though it only has O(log ) space. Furthermore, imagine that the graph of size is knowledge-graph whose local structure is known to the space-bounded machine. Specifically, given vertex indices i, the space-bounded machine is able to determine whether edge {i, j} exists in the graph. Then the machine does not need access to the full graph tape at all! It can do random walk through the graph in its mind to determine connectivity. This is the closest setting to ours, whereby seemingly complex reasoning-connectivity of an -node graph can be carried out in less than O(N ) space."
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Oracle Similar to Figure 6, we observe that having incorrect solutions in the context workspace can heavily degrade performance, and this effect is more noticeable for o3-mini. D.2 SR and PDR operators AIME 2025 results using the two iterative improvement operators SR and PDR are presented in Figure 9. For sequential token budget of 49k, the performance on o3-mini improves from 73.5 for Long CoT to 77.1 using SR operator and further to 82.9 using the PDR operator. 18 Figure 8 AIME 2025: Anchoring bias due to +ve and ve examples: With PDR we compare three selection policies for the summary: Random-k, Oracle-Incorrect (all candidates are incorrect), and Oracle-Correct (all candidates are correct), evaluated on both gemini-2.5-flash and o3-mini. Across all thinking budgets, admitting only incorrect candidates into the summary yields pronounced drop in accuracy, whereas admitting only correct candidates improves over the Random-k baseline. The degradation under Oracle-Incorrect is markedly larger for o3-mini than for gemini-2.5-flash, indicating weaker self-verification in o3-mini. Figure 9 AIME 2025: Iterative improvement beats single-pass long-CoT at matched sequential budgets. The x-axis reports Bseq: the thinking tokens consumed along the accepted path of the iterative chain, plus any distilled summary that conditions the next step. Tokens spent on unused parallel proposals are excluded, so Bseq serves as latency proxy. At comparable Bseq, both SR and PDR outperform the single-pass long CoT baseline, with PDR yielding the largest gains by converting additional total compute (via parallelism) into accuracy without increasing per-call context. Additionally, we show two SR variant results in Table 1, where error analysis followed by solution generation leads to improvements on o3-mini without any affect on the sequential token budget Bseq. 19 Figure 10 Token Budgets comparison: We plot all the different configurations for Long CoT, SR and PDR operators for both Bseq and Btotal token budgets for o3-mini. For both Bseq and Btotal, PDR forms the pareto-frontier and gives consistent gains over Long CoT and SR. Mechanics of Improvement operator: Source of accuracy gain Under the default PDR setting, gemini-2.5-flash misses 4 AIME 2024 questions. We probe how additional parallel compute affects the performance on these four AIME questions. We compare 4-round schedule (less compute) to 5-round, wider schedule (more compute). Accuracy (fraction correct over 16 seeds) changes as follows: Q1: 0.4375 0.625 (gain), Q2: 0.0625 0 (drop), Q3: 0.1875 0.1875 (no change), Q4: 0 0 (no change). In the high-compute setting (first round width M1=32), number of correct drafts among 32 is: Q1: 3/32, Q2: 0/32, Q3: 3/32, Q4: 0/32. This breakdown clarifies how PDR can (or cannot) improve with additional rounds: Table 4 PDR hard-case analysis for gemini-2.5-flash. We compare 4-round schedule (less compute: 16 generations in round 1) to 5-round, wider schedule (more compute: 32 generations in round 1). Accuracies are fraction correct over 16 seeds. Round-1 hits counts how many of the 32 first-round drafts are already correct ((more compute setting) Round-1 hits Accuracy (over 16 seeds) Interpretation Question (correct/32) Less compute More compute (More Less) . Q1 Q2 Q3 Q4 3/32 0/32 3/32 0/32 7/16 (0.4375) 1/16 (0.0625) 3/16 (0.1875) 0/16 (0.0000) 10/16 (0.6250) +3/16 (+0.1875) 0/16 (0.0000) 1/16 (0.0625) 3/16 (0.1875) 0/16 (0.0000) 0 0 Gain Drop Flat Flat 1. When round-1 draft is correct, PDR improves if two things happen: (i) the correct evidence is carried into the summary (1) (i.e., high recall in the distillation operator D); and (ii) the refine step uses that evidence to update the answer. If drops or down-weights the signal amid conflicting drafts, later rounds cannot exploit it. The Q1 gain from 45 rounds suggests both steps succeeded; the flat Q3 curve despite 20 3/32 correct drafts points to verification/refinement gap. 2. Verification among many distractors (Q1/Q3). Even when correct drafts are present, round-1 mixes small number of correct drafts with many incorrect ones (here, 3 vs. 29). The summary must surface signals that distinguish correctness. This is where PDRs distill step should act as verifier-aware aggregator. 3. Recovery when no draft is correct (Q2/Q4). If round 1 has 0/32 correct, the summary still needs to extract useful structure (partial progress, contradictions, eliminated avenues) that increases the chance of success in round 2+. The refine step should then expand diversity informed by these cues. The mild regression on Q2 with more compute is consistent with summary drift: the distillation over-weights wrong pattern, and subsequent rounds reinforce it. The no-change on Q4 may suggest either capability ceiling or the case that even more generations are required in round 1. Overall, the analysis here points to some gaps in the core skills required to carry out PDR - verification, refinement, and diversity. Improving the model along each of these skills will not only improve PDR but also any situation where multiplicative combination of these skills is required."
        }
    ],
    "affiliations": [
        "Anthropic",
        "Meta Superintelligence Labs",
        "Mila, University of Montreal",
        "Princeton University",
        "University College London"
    ]
}