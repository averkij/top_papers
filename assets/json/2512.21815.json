{
    "paper_title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "authors": [
        "Mengqi He",
        "Xinyu Tian",
        "Xin Shen",
        "Jinhong Ni",
        "Shu Zou",
        "Zhaoyuan Yang",
        "Jing Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 1 8 1 2 . 2 1 5 2 : r Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models Mengqi He Australia National University Mengqi.He@anu.edu.au Xinyu Tian Australian National University Xinyu.Tian@anu.edu.au Xin Shen The University of Queensland u6498962@anu.edu.au Jinhong Ni Australian National University Jinhong.Ni@anu.edu.au Shu Zou Australian National University Shu.Zou@anu.edu.au Zhaoyuan Yang GE Research Zhaoyuan.Yang@ge.com Jing Zhang Australian National University Jing.Zhang@anu.edu.au"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy as measure of the models uncertainty is highly correlated with VLMs reliability. While prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token equally contributes to model instability, we reveal that only small fraction (20%) of high-entropy tokens, decision point in autoregressive generation, disproportionately govern output trajectories. We demonstrate that concentrating adversarial perturbations on these high-entropy positions achieves comparable semantic degradation to global methods while using far fewer budgets. More importantly, across multiple representative VLMs, such selective attacks induce 3549% of benign outputs to become harmful, revealing more critical concern of VLMs. Remarkably, since such vulnerable high-entropy fork recurs across architecturally diverse VLMs, this kind of attack has feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA) to fool VLMs, achieving competitive attack success rates (93-95%) with strong harmful rate, exposing new weaknesses in current VLMs safety mechanisms. 1. Introduction Large-scale vision-language models (VLMs) have demonstrated exceptional capabilities in multimodal understandState-of-the-art models such ing and reasoning tasks. as Qwen2.5-VL [1], InternVL 2.5 [4], and GPT-4V [49] have significantly advanced performance and generalization, achieving promising results in key applications including visual question answering (VQA) [7, 11, 45], image captioning [23], etc. However, recent studies reveal that such models are usually susceptible to adversarial examples, where small perturbations to the input [52] can cause dramatically changes in model predictions [51, 55]. Potential threats from adversarial manipulation may lead to distorted model behaviors, resulting in biased, misleading, or even harmful outputs, which is particularly significant in safety-critical applications, including autonomous driving [9, 43, 53], robotics [15, 47], and medical [24, 37]. Prior work has consistently shown that entropy, measure of the models uncertainty, is highly correlated with model reliability. In vision language models, high-entropy tokens refer to tokens with high uncertainty in the models output probability distribution, which are often associated with prediction hallucinations, errors in LLMs [8, 17, 26, 53, 54]. Recognizing this, MIE [21] introduced nontargeted white-box attack based on [27], examining adversarial robustness of VLMs. Its primary goal was to degrade the models overall image understanding by explicitly maximizing information entropy across the output logits, attentions, and hidden states, applying this globally across all the image description decoding steps. However, this global maximization approach overlooks critical aspect of the autoregressive nature of the generative VLMs, where not all decoding steps are equally important. Evidence indicates that in autoregressive generation, minority of high-entropy tokens, act as fork, such as and, or, however, govern the direction of reasoning trajectories [44]. In contrast, those low-entropy tokens mainly carry well-learned knowlvulnerability; (2). We demonstrate that high-entropy attacks also induce harmful content ( 35-49% harmful rate); (3). We show that high-entropy tokens are shared across VLMs, enabling transfer attacks that expose broad vulnerability among models; (4). We propose non-targeted entropyguided attack method with token vocabulary, achieving competitive attack performance w.r.t. both attach success rate and harmful content rate, with high degree of transferability on both image caption and image VQA tasks. 2. Related Work LVLMs. Existing LVLMs [1, 4, 6, 18, 3841, 49, 56] tokenize images into visual patches and process them jointly with text in shared transformer, enabling end-to-end autoregressive decoding and strong performance on imagebased reasoning tasks. Given this token-based autoregressive nature, understanding how individual tokens influence inference has become an important research direction [16, 29]. Prior analyses show that only small subset of tokens consistently exhibits elevated uncertainty and heightened sensitivity to perturbations[30, 32]. Such high entropy positions correlate strongly with hallucinations and degraded robustness[8]. These observations reveal structural vulnerability of autoregressive decoding: robustness is governed by localized token where uncertainty is concentrated, rather than being uniformly distributed across the sequence. Our method directly leverages this autoregressive valunability. We identify high-uncertainty positions from clean teacher-forced pass and perturb only the next-token distributions at those locations via small pixel-space modifications. While non-autoregressive architectures [19, 33, 35] may require alternative attack designs, the core principle of targeting decision dynamics remains broadly applicable. Adversarial Attacks on VLMs. The vulnerability of machine learning models to adversarial examples has been studied extensively, particularly in the image domain [36]. Such attacks introduce small, often imperceptible, perturbations that cause large prediction errors while preserving human-perceived fidelity [10, 27]. Early multimodal adversarial attacks on VLMs [2, 12, 46, 50, 52, 53] mainly perturb pre-trained visual or textual encoders, degrading tasks such as VQA and imagetext matching to expose cross-modal weaknesses. While these approaches examine robustness from diverse perspectives, few work explicitly address the vulnerability inherent to autoregressive inference. Autoregressive decoding generates tokens sequentially, making its stability tightly dependent on token-level predictive uncertainty, captured by entropy, at each step. Our analysis begins from this perspective. Entropy, as measure of uncertainty, is strongly linked to model reliability and hallucination behaviors [8, 17, 26, 53, 54]. In VLMs, tokens with high entropy indicate positions where the model Figure 1. The examples of high-entropy token manipulation with Qwen2.5-VL-3B, where the red area shows the harmful content. edge [5, 44]. With these findings, we hypothesize that manipulating these high-entropy tokens might be sufficient to steer continuations away from the correct descriptions. To test this hypothesis, we perform preliminary experiments on image captioning with Qwen2.5-VL-3B [1]. Particularly, we select the top 20% high-entropy positions from the generated captions following [44], and apply an l-bounded pixel-space Project Gradient Descent (PGD) [27], baseline adversarial attack method. We only further increase entropy at those selective positions. Under classical attack budget ϵ 8/255, this position-focused label-free attack strategy consistently gets strong attack success rate. In addition, benign scene exhibits hallucinated objects or attributes and more harmful caption. For example, the clean description holding spoon in Fig. 1 (second example) becomes attached to the neck by rope, suggesting they are being choked  (Fig. 1)  . To further validate such hypothesis, we apply the same token manipulation across multiple VLMs [4, 6, 49]. Our experiments reveal that 35-49% of the attacked captions contain violence, weapons, drugs, or sexual content, with only 2% remaining faithful and safe. We also observe that this kind of high-entropy token recurs across architecturally diverse VLMs, yielding practical transferability. Motivated by these findings, we propose Entropy-Guided Adversarial attacks (EGA), using offline vocabulary to identify effective positions without internally computing the models entropy. Comprehensive experiments on image captioning and VQA demonstrate that EGA substantially outperforms existing attacks w.r.t. attack success rate: achieving 42-47% harmful rates on image captioning under identical budgets (Tab. 2) and 24-28% on VQA (Tab. 1), with high degree of transferability across different VLMs (Tab. 3). We summarize our contributions as: (1). We identify that targeting just 20% high-entropy tokens achieves successful attack, revealing that small fraction of tokens governs VLM update frequency, Sq is recomputed every steps on the evolving adversarial caption to track emergent tokens. Metrics. We begin our preliminary experiments with image captioning and report attack performance using two main metrics: CIDEr [42] and the harmful rate. CIDEr [42] is standard evaluation metric for image captioning that measures the semantic similarity between two captions, making it well-suited for assessing how far an attacked caption deviates from the correct one. In this paper, we report the drop of CIDEr, denoted as: CIDEr = CIDEr(clean) CIDEr(adv). As discussed above, we observe quite lot of harmful content after the attack. We thus report the harmful rate, measuring the fraction of outputs that safety assessor decides as being unsafe. Experiments setup. We perturb only the image pixels within unified ℓ budget (ϵimg=8/255 with random start and per-step projection), and keep the decoding policy identical to the clean run (greedy, same length settings). The exact objective and optimizer are in Sec. 4.1. We test Qwen2.5-VL-7B-Instruct, InternVL3.5-4B, and LLaVA-1.5-7B on COCO1000 [20] with the caption prompt. 3.2. Top-20% Suffices Initially, we perturb only the top 20% high-entropy tokens (S0.2) and observe both successful attacks and high proportion of harmful content. We then extend the experiments across the full range of (0, 1]. The results are shown in Fig. 2, where we show the drop of CIDEr [42], namely CIDEr. Within the same attack budget, concentrating on 20% high-entropy tokens consistently outperforms other variants even compared with attack on 100% tokens. Furthermore, all these three models exhibit clear U-shaped curve: the degradation start at around 20%; expanding to 80-100% recovers some degradation as the objective approaches global mask, yet it remains inferior to 20%. 3.3. Harmful Content 3.3.1. Emergence of Harmful Content Fig. 2 indicates that small group of high-entropy token is enough in fooling VLMs to produce inaccurate predictions. By analyzing the attacked captions, we found large amount of harmful content. We thus specifically evaluate the degree of harmful rate. We thus design an experiment using top-20 percent entropy of tokens to guide our attack, and evaluate on the selected dataset using the hybrid LLMas-a-judgment to process all captions. We categorize the outcomes as True (correct and safe), Safe Wrong (semantic drift but safe), and Harmful Wrong (unsafe). The Harmful Wrong captions are labeled by rule-based safety LLM tagger (GPT-4o [31]) that maps tokens to categories: Illegal Activity, Violence, Hate, Self-Harm, Privacy, Sexual Content, Other. This judgment is calibrated by Harmbench [28]. Figure 2. The CIDEr distribution w.r.t. the selected top p% highentropy tokens, showcasing 20% is sufficient. is least confident, and are usually associated with semantic errors or unstable reasoning trajectories [5]. Recent work has shown that globally maximizing multiple forms of nexttoken entropy can destabilize caption generation [21]. However, mounting evidence suggests that not all tokens contribute equally in autoregressive generation [5, 44]. Instead, small subset of high entropy tokens disproportionately governs the flow of reasoning, acting as decision points that steer the continuation. Building on these insights, we develop attacks that focus on optimizing high entropy tokens. Specifically, we apply white box pixel space perturbations that act locally at these sensitive positions under fixed textual prefix, enabling efficient and targeted manipulation of next token predictions. 3. Findings As mentioned, we hypothesize that increasing next-token uncertainty at these high entropy positions can efficiently steer continuations away from correct descriptions. We test this hypothesis by selecting the top 20% high-entropy positions from the generated captions following [5], and applying an l-bounded pixel-space PGD procedure that increases entropy only at those positions as in Sec. 4.1. 3.1. Preliminaries Token entropy. Let [0, 1]3HW be the input RGB image and the tokenizer vocabulary. At autoregressive decoding step {1, . . . , } with history tokens ˆy<t, the distribution for the t-th token can be denoted as pt() = p( I, ˆy<t ) over V. The token entropy is thus defined as: Ht(pt(w)) = (cid:88) wV pt(w) log pt(w), We perform high-entropy token selection by indentifying the top-k highest-entropy tokens, denoted as Sq, where (0, 1] is predefined selection ratio. Unless otherwise stated, Sq is computed once on the clean caption ˆy1:T (clean pass) and fixed during optimization. For the mask Figure 3. Harmful Pie Chart. Nested pies for captioning on three VLMs (leftright: Qwen2.5-VL-7B, InternVL3.5-4B, LLaVA-1.5-7B). The outer ring shows overall outcomesTrue (correct & safe), Safe-Wrong (semantic drift but safe), and Harmful (unsafe).The inner ring decomposes Harmful-Wrong into categories: Illegal Activity, Violence, Hate, Self-Harm, Privacy, Sexual Content, and Other. Figure 4. Harmful Mass Change, which shows the harmful words of the current high entropy tokens and their next 10 locations. Figure 5. Harmful Rate with different image condition while keeping the textual prefix and target token positions fixed. Harmful Contents are emerging. In Fig. 3, we show the attack outcomes on three different VLMs. As shown in Fig. 3, most of the captions are successfully attacked, where large fraction is semantically drifted, and nearly half of them become unsafe content. This experiment confirms that concentrating the pixel-level attack budget on small set of high-entropy tokens has great potential of converting benign outputs into Wrong or Harmful ones. 3.3.2. Autoregressive Harmful Content Propagation Due to the compositional nature of language, harmful tokens do not necessarily appear immediately after the targeted high-entropy positions, and the mechanism by which harmful semantics propagate along the sequence remains unclear. To investigate this process more deeply, we introduce metric that tracks how harmful probability mass evolves across the entire autoregressive decoding trajectory. We measure the harmful mass at position as the total probability assigned to curated set of word tokens anchored to the Harmbench [28] calibration rule. And to have clear observation, we keep following these two aspects: 1) the harmful mass ratio of high entropy token after attack, and 2) persistence to the next step: the harmful mass ratio observed at the current high entropy persistence to the next few steps. Findings. In Fig. 4, we show mass change between the current position and the next several positions, e.g. + 3 indicates the + 3 position. As shown in Fig. 4, across InternVL, LLaVA, and Qwen, adversarial images consistently increase harmful mass at the selected tokens. Additionally, we observed an intriguing pattern: the harmful mass associated with subsequent tokens also increases. We refer to this phenomenon as autoregressive harmful content propagation, wherein harmful content tends to propagate through future parts of the generated sequence. This observation reinforces the effectiveness of the entropy-based attack and provides further evidence for the persistence of harmful content in the models autoregressive generation. 3.3.3. Model or Image? To further investigate the origin of harmful content, we design controlled experiment that disentangles model behavior along two dimensions: the model and the image. After generating the adversarial image, we freeze the prefix text produced by the adversarial image and keep the same set of high-entropy token positions. We then re-run decoding at those high-entropy positions while varying only the image input using 1) the adversarial image (Adv), 2) Figure 6. Transferability performance w.r.t. (left) CIDEr drop and (right) Harm rate (%), where row denotes the source model and column denotes the target model. the original clean image (Img clean), 3) white image (Img white), or 4) no image at all (Img none). The results are shown in Fig. 5. Visual input is the primary trigger for harmful content. Fig. 5 shows that replacing the adversarial image with clean image or white image leads to reduced harmful rate, particularly on Qwen and LLaVA. Furthermore, removing the image reduces the harmful rates further, yet it remains above the clean baseline. Among the three models, the decrease caused by changing the image is largest on LLaVA, moderate on Qwen, and smallest on InternVL. Those experimental results indicate that the visual input is the primary trigger at these decision points. However, the remaining harmful rate for Img none suggests that once the model gets to the perturbed prefix at high entropy position, part of effect persists even without the adversarial image. 3.4. Reusable Tokens: Transferability Recalling that targeting 20% high-entropy tokens achieves attack effectiveness comparable to global perturbations, and intrinsically converts benign captions into harmful content ( 35-49% harmful rates). Strikingly, we also found this similar vulnerability holds consistently across Qwen, InternVL, and LLaVA, architecturally diverse models with different vision encoders, parameter scales, and training data. Intuitively, those low-entropy tokens mainly carry well-learned knowledge, while high-entropy logical tokens steer the generation trajectory can be similar across models. natural question follows: Do perturbations focused on highentropy sites transfer across models? To validate this, we conduct cross-model attack transfer experiments. We craft adversarial images on source model using the baseline PGD with high entropy tokens, and then evaluate attack performance on unseen target models with budget fixed to ϵ = 8/255. We randomly choose 100 images in MSCOCO [20] for testing. Transferability performance (see Fig. 6) is measured with the drop of CIDEr (CIDEr) and harmful rate. Fig. 6 indicates that CIDEr in the transferable attack case falls into the range of [0.24, 0.32], while harm rates in the range of [12.7%, 21.2%], indicating relatively reasonable degree of transFigure 7. Top-15 vulnerable words. Here, we choose the top 15 vulnerable words in Qwen as the first base column and remain. The alignment plot uses tokens as rows and models as columns; color shows flip rate, and marker size shows the occurrences. ferability. We thus conclude that adversarial images optimized on one VLM retain substantial portion of their effect on other VLMs w.r.t. both caption quality degradation (CIDEr) and harmfulness (harmful rate). Token Across Models. The preliminary experiments in Fig. 6 indicates potential transferrable attack by attacking those high-entropy tokens. To further investigate this transferability, intuitively, we examine whether the same vulnerable tokens recur across architectures. For each model, we collect tokens that occur at high-entropy positions (top-20% by clean entropy) and calculate the token flip rate, i.e. the fraction of examples for which the top-1 token under the clean run differs from that under the adversarial run. To have clearer observation, we rank all the high entropy tokens by flip rate and show the top-15 of high entropy tokens of Qwen and their corresponding entropy tokens in the other two models. As shown in Fig. 7, while the top-15 of these tokens show flip rates of 0.750.96 in Qwen, in the other two models, corresponding tokens have similar vulnerability and at least have 0.7 flip rate. With these findings that harmful content is injected and propagated at small set of high entropy decision tokens, and that such tokens recur across architectures, its thus possible to design an entropy-guided transferable attack. 4. Method We design entropy guided attacks that targets only these high entropy tokens, including: 1) white-box baseline (HiEnt-PGD) that directly maximizes uncertainty at selected high entropy positions; 2) transferable variant (HiEnt-Bank) that uses precomputed token bank to identify transferable high entropy tokens. 4.1. HiEnt-PGD Objective. Let fθ denote the frozen VLM and the fixed textual prefix. We maximize uncertainty only at selected Table 1. Image Captioning under attacks (ϵimg = 8/255). We report Attack Success Rate (ASR, % ), CIDEr drop CIDEr = CIDEr(clean)CIDEr(adv) ( indicates larger degradation), and Harmful Rate (judged by single, fixed safety assessor; % ). Bold denotes the largest and Underline denotes the second largest. Method PGD VLA COA MIE Qwen2.5-VL-7B-Instruct InternVL3.5-4B LLaVA-1.5-7B ASR (%) CIDEr Harm (%) ASR (%) CIDEr Harm (%) ASR (%) CIDEr Harm (%) 91.16 89.22 93.59 94.18 0.842 0.801 0. 0.892 0.883 1.28 1.49 1.47 13.95 42. 88.98 89.49 95.38 94.83 93.75 0.793 0.804 0. 0.905 0.862 1.41 1.40 1.53 23.05 37. 90.75 87.75 94.74 93.59 93.12 0.834 0.778 0. 0.893 0.846 1.52 0.0 0.0 21.29 47. EGA (ours) 94.81 Table 2. VQA under attacks (ϵimg = 8/255) with greedy decoding. Attack Success Rate (ASR, % ), task Accuracy (% ), and Harmful Rate (% ) under fixed safety judge are reported. Numbers are computed on matched 1k subsets with identical prompts and budgets across methods and models. Bold denotes the largest and Underline denotes the second largest. Method PGD VLA COA MIE EGA (ours) Qwen2.5-VL-7B-Instruct InternVL3.5-4B LLaVA-1.5-7B ASR (%) Acc (%) Harm (%) ASR (%) Acc (%) Harm (%) ASR (%) Acc (%) Harm (%) 90.27 81.34 91.88 95.58 93.64 16.65 15.75 6. 3.73 5.37 0.00 0.00 0.00 12.61 24. 79.44 81.99 90.74 96.46 95.17 17.46 15.29 7. 3.01 4.10 0.00 0.00 0.00 13.02 23. 77.50 69.15 96.04 83.85 80.75 13.01 17.83 2. 9.38 11.13 1.76 2.21 1.37 11.77 28. positions (S = Sq in this case): L(v) = 1 (cid:88) tS (cid:0)fθ(v, x)(cid:1). Ht Updates. Within an ℓ ball of radius ϵv around the clean pixel input v0, we run momentum PGD with random start. At k-th iteration, we denote αv as the step size, µ [0, 1) as the momentum coefficient, mk as the momentum (zero initialized), and Π() as the projection, such that vk+1 v0 ϵv. We thus have: gk = vL(vk), mk+1 = µ mk + sign(gk), vk+1 = Π(cid:0)vk + αv sign(mk+1)(cid:1). We use greedy decoding when forming ˆy1:T for stability. 4.2. HiEnt-Bank Although HiEnt-PGD has shown some degree of transferability (see Fig. 6), we further design HiEnt-Bank to extensively use the flip-rate bank discussed in Sec. 3.4. Flip-Rate Bank from Source Model. On the source model, we compute token bank of size as: (cid:0)FlipRate(w)(cid:1), = TopKwV where FlipRate(w) is the fraction of images for which the next-token argmax at the later step flips from token under the white-box HiEnt-PGD attack. Mask Selection. On the test time, given the clean greedy caption ˆy1:T , we form Sbank = { : ˆyt }, Str = Sq Sbank. Thus, beyond high entropy positions Sq, any position whose clean token lies in is also selected, without recomputing uncertainty on the target. Objective and Updates. We reuse our baseline objective and update it with replaced by Str. The bank serves as an offline prior. 5. Experiment 5.1. Experiment Setup Target Models. We evaluate our method on three representative VLMs: Qwen2.5-VL-7B-Instruct [1], Table 3. Transfer results at ϵimg = 8/255. We use XTA [13], MIE [21] as the VLM transferability baselines, Qwen, InternVL and LLaVA as models for comparison. Source / Method Qwen2.5-VL-7B-Instruct InternVL3.5-4B LLaVA-1.5-7B CIDEr Harm (%) CIDEr Harm (%) CIDEr Harm (%) Source: Qwen2.5-VL-7B-Instruct XTA MIE EGA (ours) 0.85 0.89 0. Source: InternVL3.5-4B XTA MIE EGA (ours) 0.74 0.30 0.39 Source: LLaVA-1.5-7B XTA MIE EGA (ours) 0.74 0.29 0. 0.86 13.95 42.53 0.18 11.63 21.62 0.09 12.56 24.32 0.77 0.23 0.42 0.89 0.94 0.86 0.73 0.29 0. 0.44 10.65 19.27 0.38 23.05 37.29 0.27 13.45 26.20 0.81 0.32 0.33 0.73 0.31 0.37 0.91 0.89 0. 0.02 9.93 17.37 0.06 12.36 23.63 1.13 21.29 47.05 InternVL3.5-4B [3], LLaVA-1.5-7B [45]. Datasets. We consider two benchmarks: 1) image captioning: 1k subset of the MSCOCO [20] and 2) visual question answering (VQA): 1k subset of TextVQA [34]. Unless otherwise stated, tables in the main paper report on the 1k subsets for computing efficiency and reproducibility. Baseline Method. We consider PGD [27] as classic gradient-based baseline, and VLA [50] and COA [48] as recent VLM-specific attacks. We also include MIE [21], an entropy attack that maximizes three types of entropy over all tokens. For transferability, we additionally compare against XTA [13], strong transferable attack on VLMs. Metric. We report four metrics:1) attack success rate (ASR), following the LLM-judged ASR protocol of [48] but in untargeted setting; 2) CIDEr to measure the drop of CIDEr after attack; 3) harmful rate to evaluate the fraction of harmful contents after attack judged by the HarmBenchcalibrated GPT-4o judge [31]; and 4) VQA accuracy. The exact formulas and judge details are provided in Appendix. Attack Budget and Hyper-parameters. Following standard practice for image-space attacks, image perturbations are constrained in ℓ with ϵ = 8/255. We run 300 optimization steps with step size 2/255 for all PGD-style methods, and refresh token masks every 50 steps for prefixsparse variants. For the proposed transferable attack HiEntBan (EGA in short), unless otherwise noted, we use the default configuration: high-entropy ratio of = 0.20, the union mask Str, and medium-sized token bank (e.g. = 100 entries per image). We do not ablate the mask-refresh interval or the maximum decoding length; these are fixed heuristics shared across all attacks. Greedy decoding is used throughout, with maximum of 128 new tokens and minimum of 1. Further implementation details are reported in the supplementary material. LLM Judge. We evaluate caption safety under an optimized version of the HarmBench harmful-behavior taxonomy [28], which is standardized and widely reused in recent safety work. For each caption, we first apply small regex rule bank that flags explicit unsafe content. If no rule fires, we query GPT-4o classifier to assign HarmBenchstyle safety category and collapse it into binary harmful / non-harmful label. Concretely, our judge reports both the overall unsafe rate and per-category incidence across seven buckets: Illegal Activity, Violence, Hate, Sexual Content, and Others. We adopt this schema for VLM captioning and treat multimodal safety suites (e.g., MM-SafetyBench [22], JailbreakV-28K [25]) as references. Our focus is on imageside attacks under fixed decoding. Additional implementation details are provided in the supplementary material. 5.2. Main Results. Image Captioning. Tab. 1 presents our captioning results. EGA substantially outperforms all baselines in generating harmful content while maintaining comparable semantic disruption. Across all three models, EGA achieves harmful rates of 42.5% (Qwen), 37.3% (InternVL), and 47.1% (LLaVA)dramatically higher than MIEs 14.0%, 23.1%, and 21.3% respectively. This validates our hypothesis in Sec. 3.3 that targeting high-entropy tokens steers generation toward unsafe content. Notably, this harmful content generation occurs without sacrificing attack effectiveness: EGA achieves ASR rates above 93% across these three VLM models (94.81%, 93.75%, 93.12%), comparable to MIE (94.18%, 94.83%, 93.59%). The CIDEr values are also similar (0.883, 0.862, 0.846 for EGA vs. 0.892, 0.905, 0.893 for MIE), indicating comparable semantic drift. VQA. Tab. 2 shows that EGA consistently injects substantially more harmful content across all three VLMs. Despite VQA answers being short, EGA induces 24.7% / 23.4% / 28.6% harmful responses on Qwen2.5, InternVL3.5, and LLaVA, respectivelyroughly 2 the harmful rate of MIE (12.6% / 13.0% / 11.8%). All attacks reduce task accuracy (EGA leaves only 5.4%, 4.1%, and 11.1% accuracy), but only entropy-driven attacks generate harmful outputs; standard methods such as PGD, VLA, and COA achieve high ASR yet produce almost exclusively safe but off-topic answers. Interestingly, LLaVA yields the highest harmful rate (28.6%) despite its lower ASR (80.8%), suggesting that its decision boundary is particularly sensitive to perturbations at high-entropy decoding steps. Transferability. We evaluate cross-model transferability by generating adversarial images on source model and testing them on unseen target models. Tab. 3 reports target-side metrics across all 33 source-target pairs. EGA achieves substantial harmful rates on unseen targets: 1726% across the transfer matrix, with large enough CIDEr, indicating large semantic drift (0.33-0.42). This significantly outperform existing solutions, confirming that targeting high-entropy tokens enables practical transferability, Table 4. Ablation on the rate of selected tokens, where both Image Captioning and VQA are measured with: ASR (%), CIDEr and Harmful rate (%). Method MIE20 MIE Method MIE20 MIE Image Captioning Qwen2.5-VL-7B InternVL3.5-4B LLaVA-1.5-7B ASR CIDEr Harm ASR CIDEr Harm ASR CIDEr Harm 94.01 94. 0.885 0.892 0.883 0.894 39.77 13.95 42.53 35.82 94.11 94.83 93.75 94. 0.903 0.905 0.862 0.885 27.63 23.05 37.29 29.04 93.35 93.59 93.12 93. 0.891 0.893 0.846 0.853 51.47 21.29 47.05 39.55 EGA20 (ours) 94.81 EGA100 (ours) 94.98 VQA Qwen2.5-VL-7B InternVL3.5-4B LLaVA-1.5-7B ASR Acc Harm ASR Acc Harm ASR Acc Harm 93.73 95.58 5.29 3. 5.37 3.64 20.28 12.61 24.74 21.53 94.60 96.46 95.17 95.44 4.59 3. 4.10 3.88 16.40 13.02 23.42 18.20 83.28 83.85 80.75 84.81 9.71 9. 11.13 8.82 27.34 11.77 28.62 22.01 EGA20 (ours) 93.64 EGA100 (ours) 95.68 Figure 8. Ablation on entropy selection. Both Qwen and InternVL shows similar degree trend across the position, Llava however has larger harmful rate at top-80-90% entropy positions. despite the differences in architecture and training data. High-entropy tokens are associated with harmful contents. For the main results in both Tab. 1 and Tab. 2, we implement MIE [21], an entropy based attack for VLMs with attacks for all the tokens, and for our solution, namely EGA, we apply attack only to 20% high entropy tokens. To further verify the effectiveness of our token selection in revealing harmful content, we conduct additional experiments: one attacking only the top 20% high-entropy tokens using MIE, and another attacking all tokens using our method. These results are reported as MIE20 and EGA100 (ours) in Tab. 4. As shown in Tab. 4, the attack on 20% tokens with MIE attains relatively comparable ASR compared to the attack on 100% tokens. In addition, attack on those 20% consistently provides the largest harmful rate, reinforcing that entropy-focused token positions steer the answer trajectory toward unsafe regions more reliably than broad updates. 5.3. Ablation study We conduct the following image-captioning experiments to provide comprehensive explanation of our methods. Entropy Selection Percentage. To examine how the proportion of selected tokens affects attack behavior, we conduct an ablation in Fig. 8 by restricting perturbations to Table 5. Ablation studies on bank size and mask mode. (a) Bank size ablation (b) Mask mode ablation 50 100 200 CIDEr Harm Setting CIDEr Harm 0.8440 0.8694 0.8598 0.367 0.383 0.367 Sq Sbank Str 0.8066 0.8432 0.8604 0.305 0.328 0.383 different deciles of the entropy ranking. Each bin on the horizontal axis (e.g. 0-10, 10-20) represents 10% slice of tokens ranked by clean entropy, from highest to lowest. The results show that harmfulness is concentrated in the top decile: attacking only the highest entropy 0-10% tokens yields the strongest effect (Qwen 26%, InternVL 35%, LLaVA 36%). Moving to lower-entropy ranges rapidly reduces the harmful rate, which stabilizes around 20-25% for mid-range entropy and drops to 10-20% in the lowest bins. The trend confirms that adversarial vulnerability is localized to small set of high-uncertainty decision points rather than being evenly distributed across all tokens. Bank Size. In this paper we set the bank size = 100 and carry out further experiments with token-bank size {50, 100, 200}. Experiments in Tab. 5 (a) show shallow optimum at around = 100150 on both CIDEr and Harm rate (0.8694 / 0.383 at = 100), with mild drop at = 200. Smaller under-covers reusable decision tokens, while larger begins to include lower-utility items that dilute transfer. We therefore adopt = 100 for efficiency and stability. Mask Mode. We also explore different mask modes by selecting from {Str, Sq, Sbank}, among which Str is our final choice. Particularly, Sq indicates only 20% high-entropy tokens are selected, Sbank represents the bank selected tokens. Results in Tab. 5 show consistent ordering on both metrics: Str > Sbank > Sq. In particular, Str yields the strongest degradation and harmfulness uplift (0.8604 CIDEr and 38.3% harmful rate), indicating that high-entropy cues and transferable token priors are complementary rather than redundant, supporting our selection of their union as our default mask. Note that Sq is our HiEnt-PGD baseline. 6. Conclusion We reveal structural robustness weakness in autoregressive VLMs: generation is disproportionately governed by high-entropy tokens. We show that perturbing only these tokens, roughly 20% of the sequence, produce effective attacks with high proportion of harmful contents. Our mechanistic analysis reveals two vulnerabilities in which harmful probability mass first flips next-token predictions at high entropy positions and harmful content propagates through the decoding prefix, even after removing the adversarial image. We further demonstrate that these high entropy decision tokens recur across diverse VLM architectures, enabling strong cross model transferability. Building on these insights, we introduce EGA (HiEnt-Bank), simple yet effective transferable attack. Our findings highlight fundamental tension in autoregressive VLMs: the flexibility of autoregressive generation also concentrates vulnerability at small set of unstable decision boundaries. Addressing these localized weaknesses may be key to developing safer and more reliable VLMs. Ethical Statement. EGA aims to strengthen VLM safety, but not for enabling misuse. We follow responsible disclosure and release evaluation-only code under research license that forbids generating or disseminating any potential harmful contents. Our experiments use public datasets only with PII avoided. We monitor misuse reports and will harden safeguards. Any misuse of our artifacts or findings to create or distribute harmful content is strictly prohibited. Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is organized as follows: More Harmful Showcase (Section A): additional qualitative examples across the seven HarmBench categories. Finding Extension (Section B): extended analyses of entropy ratios, harmful rate, and image vs prefix attribution. Ablation Studies (Section C): ablations on bank size, refresh frequency, decoding, optimizer, and attack steps. Method Details (Section D): notation, entropy selection, and harmful mass. Experimental Details (Section E): model and dataset, hyperparameter, baselines, and metric. Details of the Harmfulness Judge (Section F): rule bank and the judging pipeline. Reproducibility and Resources (Section G): code release plan, hardware/software configuration. Limitation (Section H): discussion of judge reliability, dataset scope, and attack setting. LLM Usage Statement (Section I) A. More Harmful Showcase Fig. 10 provide qualitative captioning examples across all seven HarmBench categories (Illegal Activity, Violence, Hate, Self-Harm, Privacy, Sexual Content, and Other). For each image, we display both the clean caption and the entropy-guided adversarial caption across multiple VLMs. Clean outputs remain close to literal descriptions of the scene (e.g., police officer on motorcycle, graffiticovered train car, bathroom interior, or street with pedestrians), while EGA consistently steers the model toward unsafe description: staged attacks, grotesque experiments, slurs or targeted insults, self-harm imagery, privacyviolating speculation, and sexualized descriptions of otherwise scenes. Across categories, not all the harmful content is injected by copying words from the prompt or adding artificial objects to the image. Instead, the model sometimes uses existing elements in the scene: police, vehicles, bathrooms, or crowds become references for illegal activity or hate scenarios; toys and pinatas are reinterpreted as violent or selfharm symbols; portraits and license plates are expanded into privacy-sensitive stories about identities or locations. These cases illustrate the main concern from the paper: perturbing small set of high-entropy tokens is enough to change captions from neutral, descriptive behavior into unsafe descriptions for the model. (a) Current token flip rate, showing that low entropy is not effective to be changed as high entropy. (b) The harmful rate uplift w.r.t. the selected top p% high-entropy tokens, showing 20% is sufficient. Figure 9. (a) shows the current token flip rate distribution vs the entropy selection (b) shows the current token harmful rate uplift vs the entropy accumulated selection (we use uplift to reduce the influence from wrong judgment of clean input). B. Finding extension B.1. Top-20% Suffices Analysis of the U-shape phenomenon. As shown in the Main paper Fig. 2 of the main paper, the attack-performance curve exhibits distinctive U-shape: optimizing too few or too many tokens leads to suboptimal gains. This trend suggests that perturbing low-entropy positions contributes little to the attack. To further examine this hypothesis, we partition all token positions into two disjoint groups: H20: the top 20% highest-entropy positions, L80: the remaining 80% low-entropy positions. Fig. 9a compares the flip-rate distributions of these two sets. The H20 distribution is clearly right-skewed, indicating that adversarial perturbations frequently flip the top-1 prediction. In contrast, L80 flip-rates concentrate in lower range, revealing substantially lower sensitivity. This disparity provides direct explanation for the U-shape: including low-entropy positions increases the perturbation budget but adds minimal adversarial leverage. Here, the flip rate is defined as the fraction of examples for which the top-1 token differs between the clean and adversarial forward passes. Accumulated harmful rate. For continuity, the main paper only reports the CIDEr-drop version of the Main paper Fig. 2. Hence, we add figure of the harmful rate version for the Fig. 2. We use harmful uplift, the increase over the clean baseline, factor out occasional false positive judgments on clean captions to measure only the harmfulness introduced by the attack. As shown in Fig. 9b, all three VLMs exhibit similar sweet point at 2030% high-entropy positions: Figure 10. The examples of attack of seven categories, including Illegal Activity, Violence, Hate, Self-Harm, Privacy, Sexual, and Other. Qwen2.5-VL-7B: harmful uplift peaks at 0.350.36 for ratio 0.20.3, but drops to 0.05 at 1.0. InternVL3.5-4B: the harmful uplift reaches 0.41 near 0.2, while remaining below 0.25 elsewhere. LLaVA-1.5-7B: the most sensitive model, peaking at 0.49 at ratio 0.2, yet falling to only 0.22 at 1.0. Overall, targeting small band of high-entropy positions is effective to trigger most of the harmful behaviour, while global perturbations are less efficient. B.2. Harmful Content Model or Image? We next ask whether harmful content is primarily triggered by the adversarial image or sustained by the autoregressive prefix. To this end, we perform set of switching experiments at the high-entropy positions after attack. We start from fully adversarial route (Adv), and then construct three image-side switches that keep the adversarial prefix but replace the image with the clean, white, or none variant (Img clean, Img white, Img none). We fix the adversarial image and overwrite the prefix with either the original clean caption (Pref clean) or sanitized low-entropy prefix synonym at the high-entropy position (Pref san). The clean route (Clean) serves as reference. The route-wise analysis in Fig. 11 shows consistent pattern across Qwen2.5, InternVL, and LLaVA. When we keep the adversarial prefix but restore the image to its clean or white counterpart, harmful rates remain high: imageside switches only moderately reduce harmfulness and still retain large fraction of the uplift compared to Clean. Removing image structure (Img none) further suppresses harmfulness, yet the rates are still far above the clean baseline, indicating that the autoregressive state carries substantial risk. On the other side, fixing the adversarial image but replacing the prefix with the clean or sanitized version (Pref clean, Pref san) also yields sizeable drop in harmfulness, with Pref san consistently sitting between the fully adversarial and clean-prefix routes. Aggregating over all tokens and routes, we find that both the adversarial image and the model prefix contribute to the harmful outcome: image-side perturbations are important for triggering unsafe behavior, while the models own prefixes, especially at high-entropy positions, maintain that harmful mass as the caption unfolds. C. Ablation Studies C.1. Bank size We set the token-bank size to K=100 and further sweep {50, 100, 150, 200}. As shown in Tab. 6, performance exhibits optimu performance at K100 on both CIDEr and Harm (0.8694 / 0.383 at K=100), remains comparable at K=150, and drops mildly at K=200. Small banks cover Table 6. Ablations on token-bank size and optimization steps. CIDEr is CIDEr(clean)CIDEr(adv) (higher is worse); Harm is the harmful rate. (a) Bank size (b) Optimization steps CIDEr Harm Steps CIDEr Harm 0.8440 0.8694 0.8641 0.8598 0.367 0.383 0.379 0. 100 200 300 400 0.6224 0.8235 0.8694 0.8715 0.261 0.335 0.383 0.389 50 100 150 200 Table 7. Ablations on decoding and optimizer. We fix the perturbation budget and token masks, and vary the test-time decoding rule (left) and optimizer for entropy ascent (right). (a) Decoding strategy (b) Optimizer Decoding CIDEr Harm Optimizer CIDEr Harm Greedy Sample 0.8694 0. 0.383 0.379 PGD Adam 0.8327 0.8694 0.355 0.383 Table 8. Ablation on refresh interval R. We refresh the highentropy mask every PGD/Adam steps. More frequent refreshes (small R) track the drifting adversarial prefix more accurately, yielding stronger attacks (CIDEr and Harm peak at R=0) but incurring higher cost; skipping refresh (R=) is fastest but significantly weaker. 0 50 100 CIDEr Harm Time (s) 0.8783 0.8694 0.8346 0.6420 0.395 0.383 0.308 0.245 1657 285 242 187 reusable decision tokens, whereas overly large banks introduce lower-utility items that dilute transfer. We therefore adopt K=100 for efficiency and stability. C.2. Mask Refresh Frequency For the entropy mask, we also test the different high entropy mask recomputed frequencies. Tab. 8 shows that always refreshing the mask (R=0) gives the strongest attack (CIDEr = 0.8783, Harm = 0.395), but at very high computational cost (1657 s). In contrast, moderate refresh interval of R=50 steps achieves very close attack strength (CIDEr = 0.8694, Harm = 0.383) with much shorter runtime (285 s). Further reducing the refresh frequency degrades performance: at R=100 both CIDEr and Harm drop (0.8346 and 0.308), and with no refresh at all (R=) the attack weakens substantially (0.6420, 0.242) despite being the fastest (187 s). Overall, moderate mask refresh give us better trade-off, so we adopt R=50 in our main experiments. Figure 11. Route-wise attribution of harmful behavior. (a) ASR-LLM by route. (b) Harmful-rate uplift by route. We report results for three captioning VLMs across the seven image/prefix routes in our switching experiment: fully adversarial (Adv); adversarial prefix with clean, white, or no image (Img clean, Img white, Img none); adversarial image with clean or sanitized prefix (Pref clean, Pref san); and the clean baseline (Clean). The result here supports the view that adversarial images primarily trigger harmful behavior that is subsequently sustained by the autoregressive prefix. C.3. Greedy Search or Sampling We compare deterministic decoding (greedy) and stochastic decoding (sampling, temperature as 0.9) at test time while holding the perturbation budget and entropy masks fixed. As shown in Tab. 7, greedy decoding yields slightly stronger degradation (CIDEr = 0.8694 vs. 0.8483) and marginally higher harmful rate (0.383 vs. 0.379), indicating that concentrated high-entropy tokens remain highly effective even with sampling-induced diversity. Sampling produces slightly lower attack effectiveness but unstable optimization steps. For the main results, we therefore report greedy decoding for reproducibility, while providing sampling ablation here as robustness check. C.4. PGD or Adam Under the same ℓ budget, we compare projected gradient descent (PGD) and the Adam-based method. Tab. 7 shows that, under our default schedule, Adam attains higher CIDEr drop and harmful rate (CIDEr = 0.8694, Harm = 0.383) than PGD (CIDEr = 0.8327, Harm = 0.355), indicating more effective optimization. PGD remains competitive but typically requires longer schedules. Given the similar perturbation magnitudes and the stronger performance, we adopt Adam in the main experiments and retain PGD as an ablation in this supplement. tion largely saturates by around 300 steps, and we therefore adopt 300 steps as the default in our main experiments. D. Method Details D.1. Notation Let [0, 1]3HW be the input RGB image and the tokenizer vocabulary, and let ψ() be the preprocessing mapping such that = ψ(I) corresponds to the models pixel input. Given VLM fθ, user prompt u, and clean greedy caption ˆy1:T , we form the teacher-forced input = [ u, ˆy1:T 1 ]. (1) Under teacher forcing with we obtain step-wise logits and next-token distributions zt = fθ(v, x)t, pt = softmax(zt), for index = 1, . . . , T. (2) We quantify next-token uncertainty using Shannon entropy: Ht = (cid:88) i=1 pt(w) log pt(w) (3) D.2. High-Entropy Token Selection C.5. Number of Optimization Steps Let (0, 1] be predefined ratio and define We also compare different steps of our entropy attack schedule. As shown in Tab. 6, increasing the number of steps from 100 to 200 substantially increase the attack (CIDEr from 0.6224 to 0.8235; Harmful rate from 0.261 to 0.335). Extending the schedule to 300 steps yields the strongest improvement (CIDEr 0.8694, Harmful rate 0.383), whereas 400 steps only provide marginal additional gain (CIDEr 0.8715, Harmful rate 0.389) at extra computational cost. This indicates that the optimizak = max{1, qT } {1, . . . , }. (4) Let σ be permutation of [T ] that sorts the entropies in nonincreasing order: Hσ(1) Hσ(2) Hσ(T ). (5) The top-k index set of highest-entropy tokens is defined as Sq = {σ(1), . . . , σ(k)} [T ]. (6) During optimization, we use periodically refreshed mask set, where the refresh frequency is defined by R. At refresh steps {0, R, 2R, . . . } (with R=50 in our main setup), we recompute step-wise entropy under teacher forcing with the update prefix xr. PGD budget scales as ϵQwen Cross-model budget normalization. We control budget ϵimg (e.g., 8/255) and convert it to the models pixel space through its normalization map ψ: Qwen2.5-VL. ψ(I) = 2I 1, thus [1, 1] and the = 2αimg. InternVL3.5-4B. InternVL follows meanstd normalization, ψ(I) = (I µInternVL)/σInternVL (channelwise), giving ϵInternVL = αimg/σInternVL, applied per channel and broadcast spatially. = ϵimg/σInternVL and αInternVL = 2ϵimg and αQwen v LLaVA. ψ(I) = (I µLLaVA)/σLLaVA, yielding ϵLLaVA = = αimg/σLLaVA, again channelv ϵimg/σLLaVA and αLLaVA wise and spatially broadcast. D.3. Harmful Mass Let Vharm be the subset of word-initial vocabulary items associated with the seven risky categories above. For given token position we define harmful mass under two image conditions while holding the prefix fixed to the clean caption up to step t: mclean(t) = madv(t) = (cid:88) wVharm (cid:88) wVharm Pclean(t)[w], Padvclean prefix(t)[w], (7) (8) where (t) is the probability distribution of token prediction in at index t, and (t)[w] denotes the sum over probability of the token. E. Experimental Details E.1. Models and Datasets We evaluate three open-source VLMs that span current architectures: Qwen2.5-VL-7B-Instruct [1], InternVL3.54B [3], LLaVA-1.5-7B [45]. Captioning. MSCOCO [20], we use 1k subset for most results in the paper for all methods unless declared, with identical prompts and seeds for all methods. E.2. Attack Budget and Hyper-parameters Unless noted otherwise, the image perturbation is constrained by an ℓ norm with ϵimg = 8/255. We use 300 optimization steps and step size 2/255 for pixel-space updates. For HiEnt methods, we refresh token masks every 50 steps. For EGA (ours), we set the entropy ratio H-ratio = 0.20 (top 20% high-entropy steps) and optimize pixels with Adam, using standard β values and the same ϵimg and step budget as baselines. Decoding is greedy with max new tokens=128 and min new tokens=1 throughout. E.3. Compared Methods In the main experiments we compare four baselines and our method: PGD [27]: classic gradient-based attack in pixel space under ℓ. VLA [50]: VLM-specific gradient attack with MI-FGSM style momentum and input diversity. COA [48]: contrastive-aligned attack on visual tokens, adapted to our captioning/VQA setup. MIE [21]: entropy-global attack that maximizes several entropy terms across all decoding steps, under the same pixel budget. EGA (ours): token-only entropy maximization at top-q high-entropy steps; for transfer we use the token-bank variant. For transferability experiments we additionally include XTA [13], strong transferable VLM attack. We do not compared with the benchmark Anyattack [52] since they have different settings. E.4. Evaluation Metrics: Definitions Image-caption metrics. CIDEr [42] (TFIDF n-gram similarity, = 1..4): CIDEr = 1 4 4 (cid:88) n=1 ϕn(C) Φn(R) ϕn(C) Φn(R) , (9) where ϕn and Φn are TFIDF features of hypothesis and references R. The drop under attack is CIDEr = CIDEr(clean) CIDEr(adv). (10) where Cclean and Cadv denote captions produced on clean and adversarial images. Attack Success Rate (ASR-LLM). For image captioning we follow the captionLLM evaluation: caption is counted as successfully attacked when the LLM judge marks the adversarial caption as incorrect relative to the clean one. Formally, VQA. We use TextVQA [34]. We use 1k subset for most results in the paper for all methods unless declared, with identical prompts and seeds for all methods. ASR-LLM = #{i : adv = clean under LLM judgement} (11) . where is the number of evaluation images, and clean adv are the clean and adversarial captions for image i. and VQA metrics. Accuracy: AccVQA ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:2)ˆai = ai (cid:3). (12) where is the number of questionanswer pairs. There is no soft scoring here because TextVQA provides only single ground-truth answer per question. Attack Success Rate (ASR): ASR = Accclean Accadv Accclean . (13) where Accclean and Accadv are accuracies on clean and adversarial images, respectively. Flip Rate: Flip = 1 (cid:88) i=1 (cid:2)ˆaadv = ˆaclean (cid:3). (14) where ˆaclean and ˆaadv on clean and adversarial images. are the predicted answers for question E.5. Sequence Construction and Logit Slicing Let X0 denote prompt tokens and ˆy1:T the clean caption. Teacher forcing uses = [ X0, ˆy1:T 1 ]. (15) We slice logits to the last steps to align with generated tokens: z1:T = (cid:0)fθ(v, x)(cid:1) last . (16) This avoids exposure bias during gradient computation while keeping gradients targeted to the current caption. F. Details of the harmfulness judge This section elaborates on how we label harmful content for the main experiments. Overall pipeline. Given caption y, our judge proceeds in two stages: (i) rule-based filter that detects explicit unsafe content, and (ii) an LLM-based classifier that handles implicit or paraphrased cases. Formally, let Rule(y) {0, 1} indicate whether any rule fires. If Rule(y) = 1, we directly label as harmful. After that, we query an instruction-tuned LLM (GPT-4o) with fixed safety prompt and map its output category into binary harmful vs. nonharmful label. The judge is caption-only (no image classifier is used). Rule bank. The rule component is small bank of regular expressions and string patterns over word-initial token stems. It targets clearly disallowed themes such as illegal activity and weapons, violence and self-harm, hate and harassment, sexual content and exploitation, privacy-sensitive content, and related other unsafe topics. The bank is intentionally conservative: rules are tuned for high precision and are only triggered when the wording is unambiguous. This keeps the rule-based decisions easy to interpret and ensures that borderline cases are deferred to the LLM classifier. The banks are calibrated by the Harmbench [28] setting, we also refer some categories in MM-SafetyBench [22], JailbreakBench [25] and CogMorph [14]. Two-stage judge. Our judge is implemented as: 1. Rule stage. Curated regular expressions are applied to the caption. Any rule hit directly yields an UNSAFE label and sets the coarse category. 2. LLM stage. For captions that do not trigger any rule, GPT-4o-based safety judge returns SAFE/UNSAFE label together with distribution over seven coarse categories: Illegal Activity, Violence, Hate, Self-Harm, Privacy, Sexual Content, and Other. If either stage flags caption as unsafe, we take {SAFE, UNSAFE} accordingly, with rule hits dominating in case of conflicts. LLM classifier. For the LLM stage, we use short, deterministic prompt that asks GPT-4o [31] to assign. The caption to exactly one of the seven categories above. We then collapse the output into binary label: all non-SAFE categories are treated as harmful, and the remainder as nonharmful. Unless otherwise noted, all harmful rates reported in the main paper are computed using this primary captiononly judge. G. Reproducibility and Resources We will release code, exact data splits and seeds, decoding settings, and harm judge prompts and thresholds. Hardware and software configurations (GPU type, driver, CUDA/PyTorch versions) and additional tables/figures (including full ablation curves and per-category breakdowns) are summarized in the project repository. All reported numbers can be reproduced from configuration files that specify model checkpoints, budgets, and random seeds for each run. More samples and settings from caption and VQA will be released. H. Limitation Firstly, harmfulness is assessed by hybrid rule+LLM judge; despite reporting multi-judge prompt and confidence intervals, automatic judges can disagree with human annotations on borderline cases. Second, our main tables use 1k-image subsets for compute parity; larger test suites ( 5k) would further stabilize statistics. Third, we study pixelspace perturbations only; unrestricted or physical attacks are outside our scope. In addition, our empirical study focuses on MSCOCO [20] for captioning and TextVQA [34] for VQA; while both are widely used, they cover only narrow slice of English, natural-image data, and extending our analysis to broader captioning and VQA benchmarks (e.g., different domains, languages, or safety-oriented suites) can be an important step for future work. I. LLM Usage Statement Large Language Models (LLMs) such as ChatGPT [31] are used as general-purpose tools to improve readability of the paper, e.g., for grammar checking, LaTeX formatting, and sentence polish. No parts of the idea, method, dataset, or experiment are generated by LLMs. All technical contributions and conclusions are solely those of the authors."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. 1, 2, 6, 5 [2] Yiming Cao, Yanjie Li, Kaisheng Liang, Yuni Lai, and Bin Xiao. Enhancing targeted adversarial attacks on large visionlanguage models through intermediate projector guidance. CoRR, abs/2508.13739, 2025. 2 [3] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CoRR, abs/2312.14238, 2023. 7, 5 [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271, 2024. 1, 2 [5] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. CoRR, abs/2506.14758, 2025. 2, 3 [6] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXVI, pages 118. Springer, 2024. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, NanJiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaı Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Rame, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel, Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, Bo-Juen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. 1 [8] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nat., 630(8017):625630, 2024. 1, 2 [9] Awal Ahmed Fime, Md. Zarif Hossain, Saika Zaman, Abdur Rahman Bin Shahid, and Ahmed Imteaj. Towards trustworthy autonomous vehicles with vision-language models under In IEEE/CVF targeted and untargeted adversarial attacks. Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2025, Nashville, TN, USA, June 11-15, 2025, pages 619628. Computer Vision Foundation / IEEE, 2025. 1 [10] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 2 [11] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da Yin, Zihan Wang, Ji Qi, Xixuan Song, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Yuxiao Dong, and Jie Tang. Cogvlm2: Visual language models for image and video understanding. CoRR, abs/2408.16500, 2024. [12] Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, and Matt Fredrikson. Transferable adversarial attacks on black-box vision-language models. CoRR, abs/2505.01050, 2025. 2 [13] Hanxun Huang, Sarah M. Erfani, Yige Li, Xingjun Ma, and James Bailey. X-transfer attacks: Towards super transferable adversarial attacks on CLIP. CoRR, abs/2505.05528, 2025. 7, 5 [14] Zonglei Jing, Zonghao Ying, Le Wang, Siyuan Liang, Aishan Liu, Xianglong Liu, and Dacheng Tao. Cogmorph: Cognitive morphing attacks for text-to-image models. CoRR, abs/2501.11815, 2025. 6 [15] Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, and J. Zico Kolter. Adversarial attacks on robotic vision language action models. CoRR, abs/2506.03350, 2025. 1 [16] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. CoRR, abs/2207.05221, 2022. 2 [17] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth A. Malik, and Yarin Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms. CoRR, abs/2406.15927, 2024. 1, 2 [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 19730 19742. PMLR, 2023. [19] Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. CoRR, abs/2505.16839, 2025. 2 [20] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects In Computer Vision - ECCV 2014 - 13th Euin context. ropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740755. Springer, 2014. 3, 5, 7 [21] Chaohu Liu, Yubo Wang, Haoyu Cao, Bing Liu, Deqiang Jiang, and Linli Xu. Non-targeted adversarial attacks on vision-language models via maximizing information entropy, 2024. 1, 3, 7, 8, 5 In Comevaluation of multimodal large language models. puter Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LVI, pages 386403. Springer, 2024. 7, 6 [23] Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, and ZhengJun Zha. Benchmarking large vision-language models via directed scene graph for comprehensive image captioning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1961819627. Computer Vision Foundation / IEEE, 2025. 1 [24] Zimu Lu, Ning Xu, Hongshuo Tian, Lanjun Wang, and AnAn Liu. Medical vlp model is vulnerable: Towards multimodal adversarial attack on large medical vision-language IEEE Transactions on Circuits and Systems for models. Video Technology, pages 11, 2025. [25] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv-28k: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. CoRR, abs/2404.03027, 2024. 7, 6 [26] Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, and Haifeng Wang. Semantic energy: Detecting LLM hallucination beyond entropy. CoRR, abs/2508.14496, 2025. 1, 2 [27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. 1, 2, 7, 5 [28] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David A. Forsyth, and Dan Hendrycks. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 3, 4, 7, 6 [29] Charles Moslonka, Hicham Randrianarivo, Arthur Garnier, and Emmanuel Malherbe. Learned hallucination detection in black-box llms using token-level entropy production rate. CoRR, abs/2509.04492, 2025. 2 [30] Ross Murphy, Sergey Mosesov, Javier Leguina Peral, and Thymo ter Doest. Ask before you act: Generalising to novel environments by asking questions. CoRR, abs/2209.04665, 2022. 2 [31] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 3, 7, 6 [32] Lijun Sheng, Jian Liang, Zilei Wang, and Ran He. R-TPT: improving adversarial robustness of vision-language models through test-time prompt tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 2995829967. Computer Vision Foundation / IEEE, 2025. [22] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: benchmark for safety [33] Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, and Stefano Soatto. Non-autoregressive sequence-to-sequence vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1360313612. IEEE, 2024. 2 [34] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus In IEEE Rohrbach. Towards VQA models that can read. Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 83178326. Computer Vision Foundation / IEEE, 2019. 7, 5 [35] Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with highspeed inference. CoRR, abs/2508.02193, 2025. 2 [36] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. In 2nd InternaIntriguing properties of neural networks. tional Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. 2 [37] Poojitha Thota, Jai Prakash Veerla, Partha Sai Guttikonda, Mohammad Sadegh Nasr, Shirin Nilizadeh, and Jacob M. Luber. Demonstration of an adversarial attack against multimodal vision language model for pathology imaging. In IEEE International Symposium on Biomedical Imaging, ISBI 2024, Athens, Greece, May 27-30, 2024, pages 15. IEEE, 2024. [38] Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Argue: Attribute-guided prompt tuning for vision-language models. In CVPR, pages 2857828587. IEEE, 2024. 2 [39] Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter H. Tu, and Jing Zhang. More thought, on the dual nature of reasoning in vision-language models. CoRR, abs/2509.25848, 2025. less accuracy? [40] Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, and Jing Zhang. Black sheep in the herd: Playing with spuriously correlated attributes for vision-language recognition. In ICLR. OpenReview.net, 2025. [41] Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Identifying and mitigating position bias of multi-image visionlanguage models. In CVPR, pages 1059910609. Computer Vision Foundation / IEEE, 2025. 2 [42] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluIn IEEE Conference on Computer Vision and Patation. tern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 45664575. IEEE Computer Society, 2015. 3, 5 [43] Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, and Dacheng Tao. Blackbox adversarial attack on vision language models for autonomous driving. CoRR, abs/2501.13563, 2025. 1 [44] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. CoRR, abs/2506.01939, 2025. 1, 2, 3 [45] Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, and Furong Huang. Llava-critic-r1: Your critic model is secretly strong policy model. arXiv preprint arXiv:2509.00676, 2025. 1, 7, 5 [46] Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, and Linli Xu. Break the visual perception: Adversarial attacks targeting encoded visual tokens of large visionIn Proceedings of the 32nd ACM Interlanguage models. national Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024, pages 10721081. ACM, 2024. 2 [47] Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian M. Sadler, Dinesh Manocha, and Amrit Singh Bedi. On the safety concerns of deploying llms/vlms in robotics: Highlighting the risks and vulnerabilities. CoRR, abs/2402.10340, 2024. 1 [48] Peng Xie, Yequan Bie, Jianda Mao, Yangqiu Song, Yang Wang, Hao Chen, and Kani Chen. Chain of attack: On the robustness of vision-language models against transfer-based adversarial attacks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1467914689. Computer Vision Foundation / IEEE, 2025. 7, 5 [49] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). CoRR, abs/2309.17421, 2023. 1, [50] Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, and Fenglong Ma. VLATTACK: multimodal adversarial attacks on visionlanguage tasks via pre-trained models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2, 7, 5 [51] Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, and Kaipeng Zhang. Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions. CoRR, abs/2403.09346, 2024. 1 [52] Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, Jitao Sang, and Dit-Yan Yeung. Anyattack: Towards large-scale self-supervised adversarial attacks on vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1990019909. Computer Vision Foundation / IEEE, 2025. 1, 2, 5 [53] Tianyuan Zhang, Lu Wang, Xinwei Zhang, Yitong Zhang, Boyi Jia, Siyuan Liang, Shengshan Hu, Qiang Fu, Aishan Liu, and Xianglong Liu. Visual adversarial attack on vision-language models for autonomous driving. CoRR, abs/2411.18275, 2024. 1, 2 [54] Yulong Zhang, Tianyi Liang, Xinyue Huang, Erfei Cui, Xu Guo, Pei Chu, Chenhui Li, Ru Zhang, Wenhai Wang, and Gongshen Liu. Consensus entropy: Harnessing multivlm agreement for self-verifying and self-improving OCR. CoRR, abs/2504.11101, 2025. 1, 2 [55] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [56] Shu Zou, Xinyu Tian, Qinyu Zhao, Zhaoyuan Yang, and Jing Zhang. Simlabel: Consistency-guided OOD detection with pretrained vision-language models. CoRR, abs/2501.11485, 2025."
        }
    ],
    "affiliations": [
        "Australian National University",
        "GE Research",
        "The University of Queensland"
    ]
}