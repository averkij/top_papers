{
    "paper_title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
    "authors": [
        "Yongchao Chen",
        "Yilun Hao",
        "Yueying Liu",
        "Yang Zhang",
        "Chuchu Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0."
        },
        {
            "title": "Start",
            "content": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Yongchao Chen 1 2 Yilun Hao 1 Yueying Liu 3 Yang Zhang 4 Chuchu Fan"
        },
        {
            "title": "Abstract",
            "content": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama3-8B model with newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github. com/yongchao98/CodeSteer-v1.0. 5 2 0 2 4 ] . [ 1 0 5 3 4 0 . 2 0 5 2 : r 1. Introduction While the reasoning and planning capabilities of LLMs have improved significantly (Wang et al., 2024; Chen et al., 1Massachusetts Institute of Technology, Boston, MA, USA 2Harvard University, Boston, MA, USA 3University of Illinois Urbana-Champaign, Urbana, IL, USA 4MIT-IBM Watson AI Lab, Boston, MA, USA. Correspondence to: Yongchao Chen <yongchaochen@fas.harvard.edu>, Chuchu Fan <chuchu@mit.edu>. 1 2024c; Li et al., 2023), they still fail in ostensibly simple tasks (Zhou et al., 2024a). Crucially, many tasks in existing benchmarkssuch as Blocksworld (Valmeekam et al., 2024) and Game 24 (Zhou et al., 2023b)can be completely solved with code solutions. Text-based reasoning excels at semantic understanding and commonsense inference but is less suited for exact computation, symbolic manipulation, optimization, and algorithmic processing (Valmeekam et al., 2022). In contrast, symbolic computing via code generation is adept at handling rigorous operations and can easily leverage specialized tools (e.g., equation solvers). In many tasks, prompting LLMs to generate and execute code outperforms purely textual reasoning (Madaan et al., 2022; Liang et al., 2022; Chen et al., 2022). key challenge is guiding LLMs to decide when to rely on textual reasoning versus programmatic solutions, given that most input questions lack explicit cues about which approach is best. Recent OpenAI GPT models address this by providing Code Interpreter module, allowing the model to iteratively generate and execute code, then further reason with the output (Achiam et al., 2023). Multi-agent frameworks like AutoGen (Wu et al., 2023) adopt specialized system prompt to steer LLM for code generation when needed. However, recently Chen et al. (2024e) finds that all these existing methods struggle to effectively steer between textual reasoning and code generation, failing to fully leverage symbolic computing capabilities. Our work tries to bridge this gap by developing an assistant framework (CodeSteer) to guide the code/text generation of the LLM solving the task (TaskLLM). By fine-tuning small model (Llama-3-8B (Dubey et al., 2024)) to be the assistant, we enable large models (GPT-4o (Achiam et al., 2023)) to fully leverage symbolic computing via code generation while preserving other capabilities. Recognizing that iterative executing and exploring is the most effective way to solve tasks, we build CodeSteer to generate prompts that guide the TaskLLM through multiple rounds of interaction before finalizing answers. To achieve comprehensive evaluation, we gather and develop benchmark with 37 symbolic tasks, referred as SymBench. On SymBench, augmenting GPT-4o with CodeSteer greatly improves its average performance score from 53.3 to 86.4, even outperforming the current leading pure-text CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Figure 1: Examples and performance of CodeSteer on guiding LLM code/text generation to integrate symbolic computing. At each interaction with TaskLLM, it reviews current and previous answers, then provides guidance for the next round. CodeSteer returns final answers when it deems them ready. With CodeSteer, GPT-4o outperforms OpenAI Code Interpreter, o1, and o1-preview models. model, OpenAI o1 (82.7) (Jaech et al., 2024) and DeepSeek R1 (76.8) (Guo et al., 2025). Although trained for GPT-4o, CodeSteer shows great generalizability, delivering an average 41.8 performance gain on Claude-3-5-Sonnet, MistralLarge, and GPT-3.5. By fully leveraging symbolic computing, CodeSteer-guided LLMs maintain strong performance on highly complex tasks even when o1 fails in all testing cases. Our key contributions are: 1) Developing and publishing SymBench: Prior works by Chen et al. (2024e) and Gui et al. (2024) gathered and developed 14 and 31 tasks, respectively, targeting challenges in computation, symbolic manipulation, logic, optimization, spatial reasoning, and constrained planning. However, neither study published the complete code for question/solution synthesis or the full datasets. From these 45 tasks, we select 37 that remain challenging for GPT-4o and redevelop their generation code to produce samples with adjustable complexity. We refer to this newly published benchmark as SymBench. 2) New methods for dataset construction and model fine-tuning of SFT and DPO: We fine-tune Llama-3-8B with the synthesized datasets of 12k multi-round guidance/generation trajectories (SFT) and 5.5k guidance comparison pairs (DPO). Unlike standard multi-step settings, in CodeSteers multi-round guidance, the TaskLLM outputs complete answer each round rather than only at the end. Consequently, we introduce novel components to both the dataset construction and training processes for SFT and DPO, such as data synthesis of dynamic guidance adaptation, emphasis on the final two rounds in SFT, comparison score design, and efficient answer sampling in DPO. These modifications result in better performance. Both the final CodeSteer model and created datasets will be released. 3) Symbolic checker and self-answer checker: Observing that TaskLLM frequently produces text-like code that hardcodes answers, neglecting efficient symbolic computation, we introduce Symbolic Checker to help CodeSteerLLM evaluate code complexity and efficiency. Since most reasoning and planning tasks can be better verified with coding, we add Self-answer Checker for better judgment of answer correctness of CodeSteerLLM. These two new checkers have been proven to significantly improve the efficiency of dataset synthesis and CodeSteerLLM fine-tuning. 4) Proposed CodeSteer Outperforms Nine Baselines and 2 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance o1: CodeSteers superior performance highlights the importance of enhancing LLM reasoning and planning with symbolic computing. This also demonstrates the potential for steering large models to generate smarter code and text by leveraging specialized smaller models. 2. Symbolic Tasks and SymBench Challenges in Code/Text Choices For tasks requiring computation, symbolic manipulation, logic, optimization, spatial reasoning, and constrained planning, coding-based symbolic computing is often more effective than text-based approaches. However, Chen et al. (2024e) found that steering LLM code/text generation poses significant challenges, even in tasks with apparent symbolic characteristics. The main bottlenecks are: 1) Deciding whether code or text is simpler depends on task type, task complexity, and the LLMs capabilities, which is hard to judge (see Appendix Sec. A). 2) LLM-generated code often appears as text-like scripts that merely hard-code answers rather than enabling efficient symbolic computation, echoing the phenomenon described in Yang et al. (2024) (see Appendix Sec. B). SymBench Chen et al. (2024e) and Gui et al. (2024) collected 14 and 31 tasks with symbolic factors from various benchmarks such as Suzgun et al. (2022); Chen et al. (2024d); Yao et al. (2024); Cobbe et al. (2021); Hendrycks et al. (2021), but their question-generation code and complete datasets remain private. We redevelop the generation code to automatically synthesize questions with adjustable complexity. Our resulting set of 37 tasks covers reasoning, planning, and execution, testing competencies in mathematics, spatial reasoning, logic, order reasoning, optimization, and search. Details and categorization are provided in Appendix Sec. and Table 4. 3. CodeSteer Framework Fig 1 illustrates how CodeSteer guides the LLMs code/text At each round, CodeSteer reviews the generation. TaskLLMs current answer and the guidance/answer history, then decides whether to offer new guidance or finalize the response. It performs three key functions: 1) Initial Method Selection In the first round, it chooses whether to solve the task with code or text (e.g., use textual reasoning for small-number multiplication, and code for large-number multiplication in the task Number Multiply). 2) Dynamic Adaptation In subsequent rounds, it refines guidance or switches methods if issues arise (e.g., encouraging more sophisticated symbolic approaches in Game 24, or switching to textual reasoning after multiple incorrect code attempts in BoxLift). 3) Answer Finalization When Ready The main components of CodeSteer are as follows: CodeSteerLLM is the primary model fine-tuned and used to guide TaskLLM in code/text generation. The input prompt formats for the first and subsequent rounds are presented in Appendix Sec. D. To facilitate answer evaluation, CodeSteerLLM is equipped with two checkersSelfanswer and Symbolicwhose design is inspired by the inherent features of symbolic tasks. Self-answer Checker re-queries TaskLLM to generate and execute code for verifying its current answer, then returns the evaluation results and explanations to CodeSteerLLM. Since many symbolic tasks benefit from code-based verification, this approach often provides more reliable perspective. The prompt format for the Self-answer Checker is provided in Appendix Sec. E. Symbolic Checker is rule-based script to analyze the generated code for iteration, search, numeric handling, permutations, and combinations, then returns complexity summary and score. This helps CodeSteerLLM determine whether the code is sufficiently sophisticated for the task at hand. Since TaskLLM often produces text-like code prone to errors, the Symbolic Checkers complexity assessment aids, but does not solely dictate, CodeSteerLLMs decisions. Further details on the checking code and prompt are in Appendix Sec. F. Beyond enhancing CodeSteerLLMs performance, the Selfanswer and Symbolic Checkers also streamline dataset synthesis for SFT and DPO fine-tuning, as discussed in the following sections. 4. Fine-tuning the CodeSteerLLM Among the three modules of CodeSteer, the CodeSteerLLM needs to be fine-tuned to perform the complicated task of steering. The fine-tuning is performed on subset of SymBench. Specifically, we randomly select 28 of the 37 SymBench tasks, using distinct set of samples without overlap with the test samples. This setup allows us to evaluate CodeSteer on 28 seen tasks (with different test samples) and on the remaining 9 unseen tasks. The fine-tuning consists of two steps. We first fine-tune the Llama-3.1-8B model with SFT, then further optimize it using DPO. Both processes are fine-tuned with full parameter on 4*H100 GPUs for 4-10 epochs. The detailed parameter and hardware settings for fine-tuning and inference processes are discussed in Appendix Sec. H. We synthesize 12k multiround guidance/generation trajectories for SFT and 5.5k guidance comparison pairs for DPO. The specific data number for each task is in Appendix Sec. G. 4.1. Multi-round SFT To generate supervision data for SFT, we prompt the GPT-4o to serve as both the guiding LLM (i.e., the CodeSteerLLM) and the TaskLLM to generate multiple guidance/generate CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance trajectories. We then filter the trajectories keeping only those that produce correct answers. To improve success rates, CodeSteerLLMs prompt is more detailed and includes pre-set knowledge or hints. To increase dataset diversity and enable dynamic adaptation of guided thoughts, this prompt also has different versions. For example, we may let GPT-4o choose all guidance styles, or enforce transitions from code to text or text to code. We set the maximum guidance rounds to be 5 and return the final answer once that limit is reached. Multi-round Gradient Cancellation Issue In multiround trajectories, the SFT process incorporates gradients from each round. This can lead to gradient cancellation in the early rounds. For example, in one task, both [code, return answer] and [text, code, return answer] produce correct results, so if both trajectories are used for fine-tuning, the SFT cannot learn that code is the better first step. Data Augmentation To mitigate this issue, we leverage the fact that the final two rounds of guidance are most influential, as the TaskLLM produces new answers each round while earlier rounds primarily provide background. Consequently, we augment the SFT dataset by doubling the weights of the final two rounds. 4.2. Multi-round DPO Figure 2: Schematic of multi-round DPO data sampling: blue squares represent intermediate (non-final) rounds, and brown ovals mark finalizing rounds. Guidance responses from the same parent node in CodeSteerLLM are compared to generate the DPO data. Because many correct trajectories in the SFT dataset are still suboptimal, we need to further fine-tune the CodeSteerLLM on pairs of trajectories labeled with preferences. Here we use rule-based scores to assign preferences. Figure 2 illustrates our framework for sampling DPO guidance pairs in multi-round setting. The main challenge is sampling and selecting guidance pairs that exhibit clear performance differences across various rounds while minimizing the number of samples to conserve resources. We use tree structure 4 where each node represents guidance, with branching factor of 2 or 3. To compare guidance pairs from the same parent node, we calculate their Performance Scores using the following equation: Scorei = 15 1 C(i) (cid:80) jC(i) Scorej ending round/correct, ending round/incorrect, otherwise. (1) Here, Scorei represents the score for node at round i, where is the current round number, and C(i) is the set of child nodes of node i. If the current round is the final one, Scorei is set to 15 for correct answers and for incorrect ones. This incentivizes CodeSteerLLM to achieve correct answers in the fewest rounds possible. For non-final rounds, Scorei is calculated as the average of its child nodes scores. This ensures that each non-terminal rounds score reflects the average performance of its potential subsequent actions, i.e., the expectation. DPO data is collected from guidance pairs within the same parent node at each level that have score difference greater than 2. To prevent reward hacking (Skalse et al., 2022)where CodeSteerLLM might bypass exploration and return incorrect answers quickly (e.g., preferring score of 2 over 5)we include only pairs where at least one guidance has positive score. To obtain diverse guidance answers, we set the inference temperature to 1.5 for the SFT fine-tuned CodeSteerLLM and use three models fine-tuned at different epochs (6, 8, and 10) to compare their guidance responses for the same parent node. 5. Experiments Experimental settings We use GPT-4o as the TaskLLM to test 28 seen and 9 unseen tasks, each with 100 samples of varying complexity. The samples for the 28 seen tasks are different from those used to train CodeSteerLLM. Additionally, we evaluate other LLM types to assess CodeSteers generalizability. We compare CodeSteer to six training-free and three training-based baselines, with methods 1, 36, and 9 originally proposed in Chen et al. (2024e). Training-free Baselines 1) No extra modifications but only input the original question (Only Question); 2) Our framework in Sec. 4.1 to synthesize SFT dataset, where GPT4o works as CodeSteerLLM with extra hints (Symbolic Agent); 3) Prompting LLMs to answer with only text with CoT (All Text + CoT); 4) Prompting LLMs to first analyze the question with CoT and then output the code answer (All Code + CoT); 5) Concatenating the input question with AutoGens original system prompt in Appendix Section (AutoGen Conca.); 6) Implement multi-agent framework that first queries LLMs to answer the question with All Text CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Table 1: Experimental results on SymBench. Methods with the highest scores are highlighted blue. Methods CoT LLMs Training-free Methods Training-based Methods % r c k Ave. Norm., Seen Ave. Norm., Unseen Ave. Norm., Total Game 24 Path Plan BoxLift BoxNet Blocksworld Date Understanding Web of Lies Logical Deduction Navigation GSM-Hard MATH Geometry MATH Count&Probab. Logical Equation New Operator Pooling Light Puzzles Mahjong Statistical Counting Matrix Transformation Logical Puzzle Cons. Linear Arrange. Pattern Recognition String Insertion Letter Logic Diagram String Deletion&Modifi. String Synthesis Reversi Standard Sudoku Letters Eight Queen Number Multiply Cryptanalysis String Splitting Combinatorial Calcul. Synthesis Decompo. 2048 Permut. and Combina. 1 e e 1 83.8 79.4 82.7 79.3 69.1 76.8 80 74 95 45 100 87 100 100 100 79 94 96 100 44 46 100 96 25 87 88 74 100 96 50 60 2 46 0 61 84 43 60 96 57 57 52 100 65 60 92 43 100 88 100 98 100 77 91 97 100 39 40 100 98 72 100 80 62 100 49 54 37 0 29 52 79 46 21 91 98 96 0 100 v - 1 77.9 65.1 74.8 78 56 85 54 77 87 98 97 100 71 90 95 100 25 42 92 93 78 98 86 81 100 72 28 34 2 28 0 49 64 28 49 90 35 53 37 100 t Q O 59.3 34.5 53.3 g l y 77.0 67.9 74.8 + T 56.7 37.9 52.1 + C A 71.6 63.2 69.6 . o G A 73.2 59.5 69.9 1 . + T + C 66.7 51.9 63.1 2 . + T + C 65.8 51.7 62.4 o x / C 79.7 72.1 77.9 e e e 73.3 61.9 70.5 t o + 4 TP 88.1 81.3 86.4 88 71 20 12 50 65 78 82 91 81 73 91 48 47 55 56 94 93 97 39 79 100 100 21 37 0 60 94 93 51 100 13 47 48 35 37 60 33 66 65 23 50 86 77 94 96 81 77 86 30 56 43 92 72 93 96 44 72 56 67 8 51 7 20 12 84 40 68 16 37 70 44 25 40 43 61 60 21 48 84 80 90 94 78 76 88 33 38 47 78 74 86 92 50 71 60 75 9 65 5 23 87 45 65 20 35 67 38 20 46 43 73 73 23 44 86 98 94 92 77 76 84 56 48 40 73 96 95 97 44 77 94 100 31 85 16 45 100 89 52 100 27 48 80 69 39 80 18 54 49 37 42 76 94 82 98 79 73 89 71 48 49 95 64 89 90 68 72 100 89 8 49 12 23 100 89 44 75 0 43 57 72 49 75 93 75 77 29 52 87 98 92 99 77 75 93 78 40 46 68 90 97 98 70 86 93 100 45 93 29 52 96 78 95 24 56 86 66 56 93 17 65 69 37 43 90 96 89 98 78 76 89 52 42 54 62 66 34 94 48 82 70 6 2 4 0 8 0 12 8 11 20 28 16 52 44 66 Seen Tasks 37 43 58 30 60 89 99 93 93 76 73 88 50 39 46 56 77 93 96 58 71 90 100 30 90 20 36 98 23 44 56 30 52 88 86 91 95 80 73 87 52 45 60 56 73 32 76 51 84 44 8 0 0 0 15 0 Unseen Tasks 91 73 87 15 52 45 53 43 11 0 8 24 25 60 72 40 48 11 76 68 1 32 72 91 83 99 83 74 88 40 39 57 69 80 95 97 41 60 89 100 12 64 11 49 100 100 35 100 20 48 55 71 28 64 5 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance + CoT and All Code + CoT methods, respectively. Then the final solution is obtained by combining and summarizing both versions of the answers by the same LLM but prompted differently (Code + Text + Sum.1). Training-based Baselines 7) Fine-tune Llama-3.1-8B as summarizer based on the Code + Text + Sum.1 method using SFT on correct summary data (Code + Text + Sum.2); 8) We fine-tune Llama-3.1-8B as one-step evaluator to choose between text or code generation (Code/Text Choice); 9) OpenAI GPT Code Interpreter with the original input question (Code Interpreter). Method 7 and 8 are fine-tuned on the same data number and task types as CodeSteer. Comparison with CoT LLMs We also compare with the current best models: OpenAI o1 and o1-preview (Jaech et al., 2024) and DeepSeek R1 (Guo et al., 2025). These models enhance reasoning and planning by using textual search, reflection, and exploration during answer generation. However, our analysis shows that these CoT LLMs have not yet integrated code-based symbolic computing to further improve their performance. Evaluations Answers are evaluated using predefined rules, with GPT-4o assisting in adjusting formats as needed. Beyond the Code Interpreter method, some approaches have the LLM output code as the final answer. We extract and execute this code using predefined algorithms to obtain the final result or facilitate further reasoning. To prevent infinite loops, code execution is limited to 30 seconds. If this limit is exceeded, the task is marked as failed or returns errors for subsequent rounds. We utilize success rate as the metric for each task. To compare each method, we calculate the Average Normalized Score over all the tested tasks by the following equation: AveNormj ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 sij max(si) (2) where AveNormj is the Average Normalized Score for method j, sij is the score of method for task i, max(si) is the maximum score for task i, is the total number of tasks. This equation normalizes each score relative to the maximum score in the respective task, and then averages the normalized scores over all tasks. Apart from the task performance, in later sections we also discuss the costs of token lengths and runtime for each method. 5.1. Overall Better Performance Table 1 presents the full results of all methods on SymBench, including individual task scores and the Average Normalized Score. The key findings are: 1) CodeSteer maintains similar relative performance on seen and unseen tasks, indicating no overfitting. 2) Augmenting GPT-4o with CodeSteer significantly boosts its performance, raising the Ave. Norm. Total Score Figure 3: Normalized score distribution of CodeSteer+GPT4o and o1 in 37 SymBench tasks. from 53.3 to 86.4outperforming all 9 baselines (best baseline: Code/Text Choice at 77.9). 3) GPT-4o + CodeSteer surpasses o1 (82.7), R1 (76.8), and o1-preview (74.8), highlighting the importance of integrating symbolic computing into LLMs. Figure 3 compares the score distribution of GPT-4o + CodeSteer and o1, showing that CodeSteer reduces instances of extremely low scores (near 0), demonstrating its robustness to varied tasks. 4) Compared to other training-based methods (Code + Text + Sum.2 and Code/Text Choice) with the same data number and tasks, CodeSteers better performance validates the frameworks effectiveness (further discussed in Sec. 6). 5.2. Scalability and Generalizability Figure 4: Method performance across four representative tasks as task complexity increases from left to right on the x-axis controlled by value scales. C.S. and Inter. represent CodeSteer and Interpreter. To assess the impact of symbolic computing, Fig. 4 tracks the performance of five methods across four tasks of increas6 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Table 2: Experimental results of Claude-3-5-sonnet-20241022, Mistral-Large, and GPT-3.5 with or without augmented CodeSteer. Methods with the higher scores of the same model are highlighted blue. Methods Combinatorial Calcu. Eight Queen Reversi Cons. Linear Arran. Standard Sudoku Ave. Norm. Score Claude Claude + CodeSteer Mistral Mistral + CodeSteer GPT-3.5 GPT-3.5 + CodeSteer 48 4 0 73 0 29. 66 87 45 90 100 92.0 25 60 0 47 0 31.0 34 41 33 48 100 59.8 12 0 0 25 0 8.6 29 16 32 9 95 42.3 ing complexity. As critical task-specific properties escalate, o1, o1-preview, and GPT-4o fail in highly complex cases, while symbolic-augmented methods (CodeSteer, Code Interpreter) sustain performance. Notably, CodeSteer proves more robust across tasks than Code Interpreter. In our study, CodeSteerLLM is fine-tuned on synthesized datasets where TaskLLM is always GPT-4o. To assess its transferability and generalizability, we test it with three popular models: Claude-3-5-Sonnet, Mistral-Large, and GPT3.5-Turbo. We evaluate them on five representative tasks based on GPT-4os results in Table 1: two where text outperforms code and three where code is superior. CodeSteer has shown apparent effects when guiding GPT-4o on these tasks. The results in Table 2 confirm that CodeSteer generalizes well across other LLMs types. This is expected, as its core mechanismscode/text guidance and dynamic adaptationare essential to all general-purpose LLMs. Notably, we observe that CodeSteer is particularly effective when applied to stronger LLMs, such as Claude. This is likely because more powerful models possess superior self-reflection capabilities and can generate complex code with greater precision. Thus, they benefit more from CodeSteers additional structured guidance, unlocking their full potential. 5.3. Cost of Tokens and Runtime Figure 5: Score vs. token and runtime costs for each method, highlighting CodeSteer, R1, o1, and o1-preview in red. We display CodeSteer results separately for inferences using single or four H100 GPUs. Specific values are in Table 6. Figure 5 shows Score versus Token Length (including input and output tokens) and Score versus Runtime (covering 7 both LLM inference and code execution) for all methods. Complete data is provided in Appendix Table 6. Token counts include only those used by TaskLLM, excluding small and open-source models fine-tuned on Llama-3.1-8B. For the o1 and o1-preview models, only runtime is plotted since their thinking chains are unavailable. While achieving superior performance, CodeSteer uses more tokens than baseline methods due to its multi-round generations. Most of these tokens are consumed by multiple interaction rounds that ultimately fail. CoT LLM R1 consumes more tokens than CodeSteer due to the inefficient textual iteration. In terms of runtime, CodeSteer is faster than o1 and R1 while delivering better performance. Additionally, since most of CodeSteers runtime comes from the inference of the 8B CodeSteerLLM on our workstation, hardware and system optimizations can significantly reduce it. For example, running CodeSteerLLM on four H100 GPUs instead of one decreases the average runtime from 63.8 to 45.4 seconds. CoT LLMs consume excessive runtime and tokens due to their extensive and often redundant reasoning chains. Textual iteration is inherently inefficient for search. Appendix Sec. shows examples of text answers of R1 and GPT-4o, in which both models attempt to find the correct equation for the Game 24 task by listing all possible combinations, leading to uncontrolled iterations and endless generation. This highlights the importance of symbolic computing through code generation. 6. Ablation Studies The CodeSteer framework comprises SFT and DPO dataset synthesis, CodeSteerLLM fine-tuning, symbolic checker, and self-answer checker. Here we do the ablation studies on these components and their related modifications. The added experimental results are shown in Table 3 with the whole result table of 37 SymBench tasks in Append Sec. K. DPO Effects DPO, showing the effectiveness of the DPO process. In Table 3, 1.CodeSteer outperforms 2.WO SFT Data Augmentation As discussed in Sec. 4.1, we do the data augmentation of the last two rounds in each trajectory to prevent multi-round gradient cancellation. In CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Table 3: Ablation studies on CodeSteer. WO DPO: CodeSteer with SFT but without DPO fine-tuning. WO DPO WO Data Augment: Same as WO DPO, but without data augmentation in the last two rounds. Agent represents the Symbolic Agent. Methods Task success rate % 1.Code Steer 2.WO 3.WO DPO WO Data DPO Augment. 4.WO Symbolic Checker 5.WO 6. Self-answer Agent Checker 7.Agent WO 8.Agent WO Self-answer Checker Symbolic Checker Ave. Norm., Seen Ave. Norm., Unseen Ave. Norm., Total 88.1 81.3 86.4 80.0 76.2 79.1 79.7 70.9 77.6 80.1 68.6 77. 78.5 64.2 75.0 77.0 67.9 74.8 71.9 62.0 69.5 70.1 57.4 67.0 Table 3, 2.WO DPO achieves higher score than 3.WO DPO WO Data Augment., which means this extra attention on the last two rounds does enhance the SFT process. Symbolic and Self-answer Checkers We evaluate the effects of the Symbolic and Self-answer Checker in two parts: 1) Dataset Synthesis Efficiency: Comparing Group 6 with Groups 7 and 8 in Table 3 shows that integrating these two checkers increases the Symbolic Agents success rates, thereby enhancing the efficiency of the dataset synthesis process. 2) CodeSteer Performance: Comparing Group 1 with Groups 4 and 5 demonstrates that augmenting with these two checkers improves CodeSteers final performance. Multi-round Guidance CodeSteer uses multi-round interaction strategy with TaskLLM. In contrast, the Code/Text Choice method in Table 1 relies on single-step guidance and performs worse than CodeSteer. This demonstrates that the multi-round design enhances guidance effectiveness, aligning with the common intuition that the best methods for many tasks emerge from iterative executing and exploring processes accompanied with dynamic adaptation. Guide Not Summarizer CodeSteer primarily serves as the guidance generator for TaskLLM rather than directly generating answers, summarizing, or selecting among multiple answers. This design choice accounts for the limitations of the open-source LLM we use compared to the more capable closed-source LLM that supports TaskLLM. By focusing on guidance, CodeSteer reduces task complexity and data space requirements. The Code + Text + Sum.2 approach in Table 1 attempts to fine-tune an answer summarizer using the same data volume but fails, highlighting that summarization imposes significant burden on Llama-3.1-8B due to the unique characteristics of each task. in previous works can be solved with direct coding (Suzgun & Kalai, 2024; Gao et al., 2023). Some recent works also further extend the applications of coding into tasks involving commonsense reasoning and semantic analysis (Li et al., 2023; Weir et al., 2024). Most of previous works mainly utilize text (Yao et al., 2024; Ahn et al., 2022; Lin et al., 2023) or code (Liang et al., 2022; Bairi et al., 2024; Zhou et al., 2023a) as the only output modality. Chen et al. (2024e) highlights the importance of smartly switching between code and text generation in LLMs but notes current methods have clear drawbacks. LLM Self-reflection and CoT Models LLM-generated feedback via self-evaluation can improve performance on variety of tasks (Yang et al., 2022; Welleck et al., 2022; Madaan et al., 2023). The OpenAI o1 (Jaech et al., 2024) and DeepSeek R1 (Guo et al., 2025) models demonstrate the potential of agentic LLMs that use Chain-of-Thought (CoT) text generation to explore and self-reflect, enhancing reasoning and planning. However, they lack symbolic computing and code generation capabilities, leading to weaker performance on complex symbolic tasks and consuming substantial tokens and time (Chen et al., 2024a). LLM Fine-tuning with Multi-step SFT and DPO SFT (Chen et al., 2024f) and DPO (Rafailov et al., 2024) are extensively implemented for LLM fine-tuning. To enhance LLMs capability in multi-step agent tasks, these methods are further modified with multi-step goals and rewards (Zhou et al., 2024b; Zhai et al., 2024; Zhang et al., 2024). LLM self-generated data have become increasingly important for model improvement when combined with search algorithms and rejection sampling (Zhou et al., 2023b; Guan et al., 2025). 8. Discussion 7. Related Work Code Generation and Symbolic Computing in LLM Tasks LLMs are widely used for general agent tasks, such as interacting with softwares and websites (Zhou et al., 2023c; Hao et al., 2024a;b; Xu et al., 2024), planning robot actions (Chen et al., 2024d; Ahn et al., 2022), and inferring with logic (Suzgun et al., 2022). Literally, many test tasks Our work underlines the significance of augmenting LLM reasoning and planning capabilities with symbolic computing and shows great potentials of steering large models for smarter code/text generation with specialized small models. We introduce novel modifications to dataset synthesis and fine-tuning (SFT/DPO) to support multi-round guidance framework, which has proven effective. Unlike CoT LLMs like OpenAI o1 and DeepSeek R1, which rely solely on tex8 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance tual reasoning for exploration, symbolic computing offers greater efficiency, robustness, and scalability. Since coding is core LLM capability, generating symbolic tools via code writing preserves generalization across tasks. Chen, Y., Arkin, J., Hao, Y., Zhang, Y., Roy, N., and Fan, C. Prompt optimization in multi-step tasks (promst): Integrating human feedback and preference alignment. arXiv preprint arXiv:2402.08702, 2024c."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the field of Foundation Models. Steering the generation from language models has the great potential to improve safety and performance to better align with human preferences. Any such work is inherently double-edged sword; the same techniques used to generate samples from harmless distribution of text could, with single sign change, be repurposed for generating samples from harmful distribution of text. Our method improves language model capability by integrating symbolic computing, which may also be misused for harmful purposes. Overall, we believe the potential positive social benefits of our work in evaluation and steering language model output towards desired target distributions outweigh the potential negatives stemming primarily from misuse."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Bairi, R., Sonwane, A., Kanade, A., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., and Shet, S. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE): 675698, 2024. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024a. Chen, Y., Arkin, J., Dawson, C., Zhang, Y., Roy, N., and Fan, C. Autotamp: Autoregressive task and motion planning In 2024 IEEE with llms as translators and checkers. International Conference on Robotics and Automation (ICRA), pp. 66956702. IEEE, 2024b. Chen, Y., Arkin, J., Zhang, Y., Roy, N., and Fan, C. Scalable multi-robot collaboration with large language models: In 2024 IEEE Centralized or decentralized systems? International Conference on Robotics and Automation (ICRA), pp. 43114317. IEEE, 2024d. Chen, Y., Jhamtani, H., Sharma, S., Fan, C., and Wang, C. Steering large language models between code execution and textual reasoning. arXiv preprint arXiv:2410.03524, 2024e. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024f. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023. Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Gui, J., Liu, Y., Cheng, J., Gu, X., Liu, X., Wang, H., Dong, Y., Tang, J., and Huang, M. Logicgame: Benchmarking rule-based reasoning abilities of large language models. arXiv preprint arXiv:2408.15778, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hao, Y., Chen, Y., Zhang, Y., and Fan, C. Large language models can plan your travels rigorously with formal verification tools. arXiv preprint arXiv:2404.11891, 2024a. Hao, Y., Zhang, Y., and Fan, C. Planning anything with rigor: General-purpose zero-shot planning with llm-based formalized programming. arXiv preprint arXiv:2410.12112, 2024b. CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Li, C., Liang, J., Zeng, A., Chen, X., Hausman, K., Sadigh, D., Levine, S., Fei-Fei, L., Xia, F., and Ichter, B. Chain of code: Reasoning with language model-augmented code emulator. arXiv preprint arXiv:2312.04474, 2023. Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. Lin, K., Agia, C., Migimatsu, T., Pavone, M., and Bohg, J. Text2motion: From natural language instructions to feasible plans. Autonomous Robots, 47(8):13451365, 2023. Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Suzgun, M. and Kalai, A. T. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. Valmeekam, K., Olmo, A., Sreedharan, S., and Kambhampati, S. Large language models still cant plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati, S. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36, 2024. Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., and Zou, J. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Weir, N., Khalifa, M., Qiu, L., Weller, O., and Clark, P. Learning to reason via program generation, emulation, and search. arXiv preprint arXiv:2405.16337, 2024. Welleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2022. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. Xu, T., Chen, L., Wu, D.-J., Chen, Y., Zhang, Z., Yao, X., Xie, Z., Chen, Y., Liu, S., Qian, B., et al. Crab: Crossenvironment agent benchmark for multimodal language model agents. arXiv preprint arXiv:2407.01511, 2024. Yang, K., Tian, Y., Peng, N., and Klein, D. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 43934479, 2022. Yang, Y., Xiong, S., Payani, A., Shareghi, E., and Fekri, F. Can llms reason in the wild with programs? arXiv preprint arXiv:2406.13764, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Zhai, Y., Bai, H., Lin, Z., Pan, J., Tong, S., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024. 10 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Zhang, X., Du, C., Pang, T., Liu, Q., Gao, W., and ImprovLin, M. Chain of preference optimization: ing chain-of-thought reasoning in llms. arXiv preprint arXiv:2406.09136, 2024. Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023b. Zhou, L., Schellaert, W., Martınez-Plumed, F., Moros-Daval, Y., Ferri, C., and Hernandez-Orallo, J. Larger and more instructable language models become less reliable. Nature, pp. 18, 2024a. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023c. Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024b. 11 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Appendices: CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance A. Impacts of task types, task complexities, and LLM capabilities on code/text choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Varied code versions of the same LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 C. Description of SymBench tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 D. Prompt for CodeSteerLLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 E. Prompt for Self-answer Checker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 F. Code for Symbolic Checker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 G. Synthesized dataset number of each task for SFT and DPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H. Parameter and hardware settings of SFT/DPO fine-tuning and inference processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 I. Score-cost table for each method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 J. Example Text Answer of DeepSeek R1 and GPT-4o in Game 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 K. Full experimental results of ablation studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 L. System prompt of AutoGen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance A. Impacts of task types, task complexities, and LLM capabilities on code/text choices The phenomenon and challenges of steering LLM code/text generation are first proposed by Chen et al. (2024e). Here we discuss these phenomenon in details for the motivation of our work. Fig 6 presents two typical examples of the recently popular topics of 9.11 and 9.9 numerical comparison and letter count in strawberry, that the ChatGPT of GPT-4o makes mistakes by direct textual reasoning but easily solves the problem after prompted to use code. Meanwhile, Fig 7 displays the example that GPT-4o makes mistakes to solve the question by code generation but partially solve the question by textual reasoning. The above two examples show that whether code or text is simpler highly depends on the task types and LLM own capabilities and characteristics. The OpenAI GPT-4o Code Interpreter is trained to steer LLM code/text generation. However, the study of Chen et al. (2024e) finds many limitations of this method. In Fig 8, they observe an intriguing property of GPT Code Interpreter: its decision to use code depends on the complexity of the task, as shown in Fig 8. GPT-4o Code Interpreter chooses to handle simple Number Multiplying questions with text and complex questions with code, resulting in correct answers. However, it fails in medium-difficulty questions since it tends to be overconfident and chooses to answer the question via textual reasoning, which sometimes is wrong. Hence, whether to implement symbolic computing depends on task complexities even for the same type of the task. Figure 6: The cases that GPT-4o makes simple mistakes by direct textual reasoning but can reliably solve the problem with prompted to use code. CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Figure 7: Representative answers of BoxLift task. The left figure is the partially correct answer of GPT-4o with All Text + CoT method. The right figure is the wrong code answer from All Code + CoT method. The text and code parts are colored in blue and green, respectively. The All Code + CoT method generates the wrong code that runs into an infinite loop. Figure 8: GPT-4o Code Interpreter tends to handle simple Number Multiplying tasks with text and complex tasks with code. However, it often fails with medium-difficulty questions, where it is overconfident and chooses not to use code when needed. 14 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance B. Varied code versions of the same LLM Figure 9: Representative code answers of Game 24 task. The left figure is the correct code of GPT-4o with extra AutoGen prompt in Appendix Sec. for guiding code/text choices. The right figure is the wrong code after prompting GPT-4o to answer with code Think of an algorithm to solve the task and implement it in python. The text and code parts are colored in blue and green, respectively. In both cases, GPT-4o is prompted to solve this task with code. The only difference is the guiding prompts. However, GPT-4o answers with different types of codes, with or without efficient symbolic computing. This phenomenon shows that LLM code generation is unstable under varied prompts, tasks, and LLM types. 15 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance C. Description of SymBench tasks Here we describe the 37 testing tasks. They require strong symbolic, mathematical, logical, geometrical, scientific, and commonsense reasoning capabilities. The first 14 tasks originate from Chen et al. (2024e), while the last 23 are from Gui et al. (2024). Note that both these two previous works do not release the full question datasets and codes for these 37 tasks. The released question dataset in Gui et al. (2024) only contains 8 or 16 questions for each task. Hence, we develop codes to automatically synthesize the questions for each task with tunable complexities. Both our developed codes and question datasets are released. Number Multiplying This task involves querying LLMs to compute the product among integers. It represents classic problem that LLMs are not able to solve through pure textual reasoning. Game 24 This task involves querying LLMs to use given set of integers to generate an equation that evaluates to 24. This task is tested in previous work Tree-of-Thought (Yao et al., 2024). Path Plan This task involves querying LLMs to plan the robot trajectory waypoints based on human task instructions and environments. This task originates from AutoTAMP (Chen et al., 2024b). Letters This task involves querying LLMs to count the total number of specific letters in long word and specify their positions. An example question can be How many rs in the word strawberry and what are their positions?. This task has recently gained significant attention because current LLMs struggle to perform it effectively and accurately. BoxLift This task involves coordinating robots of various types to lift boxes of different sizes and weights. Each robot has specific lifting capacity and can collaborate with others to lift single box. box can only be lifted if the combined lifting capacity of the robots exceeds the boxs weight. The objective is to lift all the boxes in the minimum number of time steps. This task originates from Scalable-Robots (Chen et al., 2024d). BoxNet This task involves coordinating robot arms to move colored boxes (squares) into corresponding colored goal locations (circles) in the fewest time steps. Each robot arm is assigned and restricted to cell indicated by the dotted lines. The arms have two possible actions: (1) move box within their cell to neighboring cell, or (2) move box within their cell to goal location within the same cell. The objective is to ensure all boxes are placed in their matching goal locations efficiently. This task originates from Scalable-Robots (Chen et al., 2024d). Blocksworld In Blocksworld, the objective is to stack set of blocks (brown) according to specific order. The robot can perform four actions: (1) pick up block, (2) unstack block from the top of another block, (3) put down block, (4) stack block on top of another block. robot can only pick up, unstack, or stack block if it is clear, that is, the block has no other blocks on top and is not currently being held. This task originates from PlanBench (Valmeekam et al., 2024). Date Understanding Given small set of sentences referring specific date, the task involves querying LLMs to answer provided question based on the information in these sentences (e.g., The concert was scheduled for 06/01/1943, but was delayed by one day to today. What was the date yesterday in MM/DD/YYYY?). This task originates from BIG-Bench-Hard (Suzgun et al., 2022). Web of Lies This task involves querying LLMs to determine the truth value of random Boolean function presented as natural-language word problem. This task originates from BIG-Bench-Hard (Suzgun et al., 2022). Logical Deduction This task involves querying LLMs to deduce the order of sequence of objects using clues and information about their spacial relationships and placements. This task originates from BIG-Bench-Hard (Suzgun et al., 2022). Navigate This task involves querying LLMs to determine whether the agent would return to its initial starting point after following series of navigation steps. This task originates from BIG-Bench-Hard (Suzgun et al., 2022). GSM-Hard (Gao et al., 2023) This is the more challenging version of GSM8K (Cobbe et al., 2021) math reasoning dataset, where the numbers in the original questions of GSM8K are replaced with larger, less common values. MATH-Geometry This is the math reasoning dataset from MATH dataset (Hendrycks et al., 2021), with specific focus on geometry questions. MATH-Count&Probability This is the math reasoning dataset from MATH dataset (Hendrycks et al., 2021), with specific focus on counting and probability questions. 16 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance The following 23 tasks originate from LogicGame (Gui et al., 2024). Logical Equation The task is to assign specific numeric value to each letter from given set, using predefined range of numbers and set of inequalities. Each letter corresponds to unique number, and the relationships between the letters are defined by mathematical equations or constraints. New Operator This task introduces custom mathematical operations involving two numbers, defined with unique formulas. The goal is to use the given definitions of these operations to compute the result of specific expression. Pooling This task involves applying pooling operation on numerical grid. The pooling operation uses an sliding window (n < ) that moves across the grid from left to right and top to bottom. The results from each window are then arranged based on their positions to create new output matrix. In this task, you are given an grid representing network of lights, where lit light is represented by Light Puzzles 1 and an unlit light by 0. Several buttons control the state of these lights by turning them on or off in certain positions. The state of each light can be affected by multiple buttons. The task is to follow series of button presses and determine the final state of the grid. Mahjong Given an initial set of letter cards, in each round, new card is added and one card is removed. Some effects may happen when specific combinations of the cards appear after introducing the new card. result is determined based on these specific conditions. The goal is to determine result based on series of rounds Statistical Counting Calculate the total score of string by scanning it from left to right, where consecutive identical letters earn points (for example, two or more consecutive As add 1 point, Bs add 2 points, etc.). The task is to start with score of 0 and return the final summing value. Matrix Transformation Rotate given matrix of characters based on given instruction (e.g., 90 degrees clockwise), preserving each characters position relative to others in the transformed output. The input matrix can be of any size and contain any character. Logical Puzzle The task involves querying LLMs to select specified number of different values from grid of numbers, ensuring that certain mathematical constraints (sum or product) are satisfied for selected numbers for each row and column. Constrained Linear Arrangement In two-player card game, the task is to deduce your opponents moves based on the games rules, your played cards, and the announced results of each round. Each card can only be used once, and the game follows specific interaction rules between different card types, where certain cards can defeat, be defeated by, or draw with others according to predefined relationships. Pattern Recognition The task involves querying LLMs to find all squares in character matrix where each square consists of identical characters and has side length of at least 3. String Insertion The task is to transform string by scanning it from left to right and inserting specific characters after certain character patterns (e.g., each pattern WXYZ requires inserting immediately after it occurs). All operations are performed simultaneously on the original string. Letter Logic Diagram The task is to complete an incomplete grid by selecting from list of letters, where each row and column must contain each letter exactly once, and all cells on the minor diagonal (top-right to bottom-left) must contain the same letter. Some cells are already filled in as constraints. String Deletion and Modification The task is to transform string by repeatedly applying set of ordered string manipulation rules until no more changes are possible, where each rule modifies the string based on specific patterns or conditions present in the current string state. For example, modification rule can be If the string ends with ba, replace it with ab. String Synthesis Given an initial set of blocks and set of synthesis rules that combine different types of blocks, the task is to determine the final block(s) after repeatedly applying these rules in order until no more combinations are possible. In this game similar to Reversi, players take turns placing pieces on an grid. After placing piece, any of Reversi the opponents pieces located between two of the players pieces (in the same row, column, or diagonal) will be flipped. The task is to determine the state of the board after rounds, starting from given configuration. Standard Sudoku Given partially filled Sudoku grid, the task is to fill the remaining empty cells with numbers between 17 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance 1 and 9, ensuring that no number repeats in the same row, column, or 3 3 subgrid. Eight Queen Given grid with some queens already placed, the task is to place the remaining queens such that no two queens share the same row, column, or diagonal, while avoiding positions with obstacles in the grid. Cryptanalysis In this task, you are provided with combination lock consisting of numbers and letters, where neither the numbers nor the letters repeat. Using series of guesses and feedback, the goal is to deduce the correct password based on the given conditions. String Splitting dismantling engineer has old machines and can obtain machine parts through set of predefined methods. By continuously cycling through these methods in specific order, the engineer dismantles machines or combines parts to create new components, and the task is to determine the total number of parts and remaining machines after all possible cycles. Combinatoral Calculation Given set of integers, the goal is to use arithmetic operations (addition, subtraction, multiplication, division) and parentheses to arrange the numbers in such way that the final result matches specified target value. Each number must be used exactly once, and the order of the numbers cannot be changed. Synthesis Decomposition farmer grows various crops and can exchange them for agricultural products. Using set of methods, he can trade specific combinations of crops for products, following cyclic pattern until no further exchanges are possible. The goal is to determine the synthesis result for each round."
        },
        {
            "title": "2048 Similarly to the 2048 game, in a grid, numbers representing powers of 2 can move in any direction, combining when\nthey encounter a matching number to form the next power of 2. Given a starting position and a sequence of movements, the\ngoal is to determine the resulting grid after executing the moves.",
            "content": "Permutation and Combination Given set of objects with specific positioning constraints, the task is to determine the correct arrangement of the objects on shelf. Each object must be placed in position according to the rules provided, ensuring that the conditions on adjacency, order, and specific positions are met. For example, rule about adjacency could be Book must be adjacent to book I. 18 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Table 4: The evaluated capabilities of all tasks, classified as Execution, Planning, and Reasoning tasks. Categories Tasks Mathematics Execution Planning Number Multiplying New operator Pooling Light Puzzles Mahjong Statistical Counting Matrix Transform. Pattern Recognition String Insertion String Deletion &Modi. String Synthesis Reversi String Splitting Synthesis Decomposition 2048 Game 24 Path Plan Letters BoxLift BoxNet Blocksworld Logical Equation Logic Puzzle Const. Linear Arrange. Letter Logic Diagram Standard Sudoku Eight Queen Cryptanalysis Combinatorial Calculation Permutation and Combina. Date Understanding Web of Lies Logical Deduction Reasoning Navigate GSM-Hard MATH-Geometry MATH-Count&Probability Optimization Search Spatial Reasoning Logical Reasoning Order Reasoning 19 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance D. Prompt for CodeSteerLLM The input prompts of CodeSteerLLM follow multi-round dialogue, i.e., previous rounds of prompts and responses will be included as history prompts for following generation of response guidance. Since we set the maximum rounds of guidance to be 5 for each task, the total addition of prompt and output lengths of CodeSteerLLM does not surpass maximum context window 8k. The formats for the first round of prompt and following rounds of prompts are as follows. Note that The summary of generated code complexity is: {code complexity summary} is not included if the generated answer by TaskLLM does not have code. Round 1 prompt to CodeSteerLLM You are guiding another TaskLLM to solve task. You will be presented with task that can potentially be solved using either pure textual reasoning or coding. Your goal is to determine which method will be most effective for solving the task. Follow these steps: **Respond** with the chosen approach but not the solution. You can choose between the following options: - If you choose coding, explain the reasons and respond the final returned guidance with the format <<<guidance prompt content>>> in the end of your response. - If you choose textual reasoning, explain the reasons and respond the final returned guidance with the format <<<guidance prompt content>>> in the end of your response. Now, here is the task: Following Rounds of prompts to CodeSteerLLM The response from TaskLLM is: {response} The feedback from the checking agent is: {check result} The summary of generated code complexity is: {code complexity summary} The final returned guidance prompt should be of the format <<<guidance prompt content>>>. E. Prompt for Self-answer Checker Prompt for Self-answer Checker Given the following question and the answer from other LLMs, write python code block to check the correctness of the answer. Try to generate the code to check the correctness of the answer. Try your best to check whether the answer satisfy all the constraints of the given question. If the answer is correct, return the text Correct. If the answer is incorrect, return the reason why the answer is wrong, like what condition or constraint is not satisfied. Question: {question} Answer: {answer} 20 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance F. Code for Symbolic Checker The following code checks the factors of iteration, search, numeric, permutations, and combinations in the answered code by TaskLLM and returns the summary of code complexity and the complexity score. We directly return the summary of code complexity as code complexity summary to CodeSteerLLM for further guidance. If the complexity score less than 2.0, the returned code complexity summary concatenates with The generated code may not be complex enough to carry out symbolic computing for solving the task. Figure 10: Code for checking the symbolic factors of the generated code by TaskLLM. CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance G. Synthesized dataset number of each task for SFT and DPO Table 5: Synthesized dataset number of each task for SFT and DPO fine-tuning processes. Dataset number SFT success trajectory number DPO pair number Game 24 Path Plan BoxLift BoxNet Blocksworld Date Understanding Web of Lies Logical Deduction Navigation GSM-Hard MATH Geometry MATH Count&Prob. Logical Equation New Operator Pooling Light Puzzles Mahjong Statistical Counting Matrix Transform. Logical Puzzle Constrained Linear Arrangement Pattern Recognition String Insertion Letter Logic Diagram String deletion&Modification String Synthesis Reversi Standard Sudoku 792 442 345 330 406 497 492 489 503 332 342 346 396 394 404 406 421 402 391 454 432 414 409 500 504 397 403 400 320 215 163 186 248 238 204 241 170 125 115 127 213 189 187 259 230 223 214 148 155 135 128 226 230 185 194 212 Total 12043 5480 H. Parameter and hardware settings of SFT/DPO fine-tuning and inference processes We utilize four H100 80GB GPUs for full-parameter fine-tuning of the Llama-3.1-8B models. The model is trained for 10 epochs in the SFT stage and 6 epochs in the DPO stage. The learning rate is set to 1 105 for SFT and 5 106 for DPO. We use batch size of 4 for training. In DPO, the loss function follows the standard sigmoid loss (Rafailov et al., 2024), with the hyperparameter β set to 0.1. In most cases, we perform the inference of CodeSteerLLM using single H100 80GB GPU. However, to analyze the impact of hardware configurations on CodeSteer runtime, as shown in Fig. 5, we also conduct inference using four H100 GPUs for comparison. For the generation of guidance answers in the DPO dataset creation, we utilize three different SFT fine-tuned Llama-3.1-8B models, trained for 6, 8, and 10 epochs, respectively. For each question and stage, we query all three models and compare their generated guidance answers. 22 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance I. Score-cost table for each method Table 6: Score-cost table for each method. Average Norm. Average score () Average token length () Average runtime (s) () Baseline Methods Only Question Symbolic Agent All Text + CoT All Code + CoT AutoGen Conca. Code + Text + Sum. 1 Code + Text + Sum. 2 Code/Text Choice Code Interpreter CoT LLMs DeepSeek R1 o1 o1-preview Proposed Methods CodeSteer, 1*H100 CodeSteer, 4*H 53.3 74.8 52.1 69.6 69.9 63.1 62.4 77.9 70.5 76.8 82.7 74.8 86.4 86.4 566.1 1192.5 1110.7 949.8 1295.9 3931.6 2808.6 587.4 1175.9 6396.6 N/A N/A 4693.3 4693. 8.2 27.3 15.3 8.9 10.6 24.2 32.4 20.1 23.8 68.6 70.5 37.7 63.8 45.4 23 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance J. Example Text Answer of DeepSeek R1 and GPT-4o in Game Figure 11: Example text answer of R1 in the task Game 24. R1 searches possible answers with the continuous back-and-forth textual reasoning process. This search process still fails in the end. 24 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance Figure 12: Example text answer of GPT-4o in the task Game 24. GPT-4o continues the textual reasoning process until reaching the maximum token generation length but never returns the answer. 25 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance K. Full experimental results of ablation studies Table 7: Full experimental results of ablation studies on the components in CodeSteer framework. Methods Task success rate % Ave. Norm., Seen Ave. Norm., Unseen Ave. Norm., Total Game 24 Path Plan BoxLift BoxNet Blocksworld Date Understanding Web of Lies Logical Deduction Navigation GSM-Hard MATH Geometry MATH Count&Prob. Logical Equation New Operator Pooling Light Puzzles Mahjong Statistical Counting Matrix Transform. Logical Puzzle Const. Linear Arrange. Pattern Recognition String Insertion Letter Logic Diagram String deletion&Modi. String Synthesis Reversi Standard Sudoku Letters Eight Queen Number Multiply Cryptanalysis String Splitting Combinatorial Calculation Synthesis Decomposition 2048 Permutation and Combina. 1.Code Steer 2.WO 3.WO DPO WO Data DPO Augment. 4.WO Symbolic Checker 5.WO 6. Self-answer Agent Checker 7.Agent WO 8.Agent WO Self-answer Checker Symbolic Checker 78.5 64.2 75.0 77.0 67.9 74. 71.9 62.0 69.5 57 74 72 17 51 83 92 89 80 74 69 81 56 52 45 52 95 84 95 44 81 93 100 35 88 26 24 100 84 52 95 12 41 76 60 44 56 37 43 58 30 60 89 99 93 93 76 73 88 50 39 46 56 77 93 96 58 71 90 100 30 90 20 36 98 91 73 87 15 52 45 53 43 89 41 41 47 24 45 84 95 91 94 73 68 85 52 28 44 56 85 90 92 53 64 92 100 25 86 12 28 79 64 80 12 42 60 56 32 82 70.1 57.4 67.0 28 29 39 15 41 92 97 87 88 70 70 82 56 20 52 60 79 96 96 54 52 100 100 23 76 14 36 100 75 52 74 7 40 56 44 40 78 88.1 81.3 86.4 80.0 76.2 79. 79.7 70.9 77.6 93 75 77 29 52 87 98 92 99 77 75 93 78 40 46 68 90 97 98 70 86 93 100 45 93 29 52 100 96 78 95 24 56 86 66 56 93 93 76 65 21 50 83 94 92 90 74 74 92 58 38 43 71 88 98 100 58 66 96 100 20 88 12 49 100 85 74 90 22 56 76 62 56 86 46 74 76 31 50 86 92 95 95 72 70 86 56 40 51 52 88 92 97 56 65 95 100 35 92 21 39 88 72 92 15 31 88 64 44 80 80.1 68.6 77.3 62 72 66 13 54 80 95 91 85 79 71 84 61 24 47 51 92 95 96 52 76 95 100 35 90 30 52 100 87 72 94 4 43 65 44 53 92 26 CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance L. System prompt of AutoGen System prompt of AutoGen (Wu et al., 2023) You are helpful AI assistant. Solve tasks using your coding and language skills. In the following cases, suggest python code (in python coding block) or shell script (in sh coding block) for the user to execute. 1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read file, print the content of webpage or file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself. 2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly. Solve the task step by step if you need to. If plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill. When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user cant modify your code. So do not suggest incomplete code which requires users to modify. Dont use code block if its not intended to be executed by the user. If you want the user to save the code in file before executing it, put # filename: filename inside the code block as the first line. Dont include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use print function for the output when relevant. Check the execution result returned by the user. If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error cant be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of different approach to try. When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible. Reply TERMINATE in the end when everything is done."
        }
    ],
    "affiliations": [
        "Harvard University, Boston, MA, USA",
        "MIT-IBM Watson AI Lab, Boston, MA, USA",
        "Massachusetts Institute of Technology, Boston, MA, USA",
        "University of Illinois Urbana-Champaign, Urbana, IL, USA"
    ]
}