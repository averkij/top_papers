{
    "paper_title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "authors": [
        "Jiaqi Wang",
        "Weijia Wu",
        "Yi Zhan",
        "Rui Zhao",
        "Ming Hu",
        "James Cheng",
        "Wei Liu",
        "Philip Torr",
        "Kevin Qinghong Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 8 2 3 1 . 2 1 5 2 : r Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans? Jiaqi Wang1 Weijia Wu2 Yi Zhan1 Rui Zhao2 Ming Hu1 James Cheng1 Wei Liu3 Philip Torr4 Kevin Qinghong Lin4(cid:66) 1CUHK 2NUS 3Video Rebirth 4University of Oxford https://video-reality-test.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audiovisual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained actionobject interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creatorreviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5Pro) achieves only 56% accuracy (random 50%), far below that of human experts (81.25%). Adding audio improves realfake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audiovisual consistency. 1. Introduction Recent advances in video generation models [10, 19, 38] such as Sora2 [39] and Veo3.1 [8] have demonstrated remarkable progress in producing visually coherent and tem- *Equal Contribution. (cid:66)Corresponding Author. Figure 1. Illustration of Video Reality Test. An ASMR video with audio is sourced either from real social-media creator or video generation model (creator), and the reviewer (a video understanding model or human) must decide whether the video is real or AI-generated. porally consistent videos that are increasingly indistinguishable from real-world videos. Notably, these models [14, 20, 30, 46] further support synchronized audio, delivering fully immersive experiences. This rapid progress underscores the urgent need for robust methods to differentiate AI-generated videos from real ones. Existing video generation and understanding benchmarks [5, 15, 32] target fine-grained objectactivity interactions, emphasizing physical-law violations [5, 15, 33], temporal consistency [37], or creative theme control [4]. While these aspects aim to make fakes appear more realistic, they largely overlook the fundamental question of real vs. fake i.e. whether generated videos can truly deceive human audience or VLMs. By contrast, AIGC detection benchmarks [4, 6, 36, 52] aim to judge real vs. fake but underplay audio like its temporal alignment with visual actions and its contribution to immersion. These cues dominate Autonomous Sensory Meridian Response (ASMR) style videos that convincingly mimic human activities (e.g., cutting or tapping), which are carefully produced for engagement and highly sensitive to perceptual authenticity, thus even minor visual or auditory artifacts break the sensory effect, helping explain why such content readily deceives viewers on social media. Hence, ASMR offers an ideal testbed for realfake video detection. To this end, we propose Video Reality Test, an AIGC benchmark for evaluating whether frontier video generators Table 1. Comparison of Video Reality Test and existing benchmarks. Ours is (i) the unified benchmark containing real and fake pair videos, (ii) for evaluating both VLMs and VGMs, (iii) the first peer-review evalation prototype, and (iv) multi-modality containing text description about video content, start-frame image along with the audio. MCQ stands for multiple-choice question. Category Benchmark Video Generation Video Understanding AIGC Detection VBench [15] VideoPhy [5] PhyGenBench [32] SEED-Bench [21] MV-Bench [23] TempCompass [28] GenVideo [6] LOKI [52] IPV-Bench [4] Video Reality Test (Ours) Participants Video Source Fake Real Pair VLM VGM Human Eval. Prototype Acc. Acc. VLM-as-Judge MCQ, Acc. MCQ, Acc. VLM-as-Judge, Acc. Acc. Multi-Modal Inputs Text Image Audio VLM-as-Judge, MCQ, Acc. VLM-as-Judge, MCQ, Acc. Peer Review can produce ASMR videos that fool top-performing VLMs and humans. We collect 1,400 high-quality ASMR videos from social media and curate 149 diverse real samples covering duration, objects, actions, and backgrounds. For fake videos, each generator contributes its own outputs, producing dataset of size 149 (1 + K). This makes our benchmark dynamic competitive arena of continuously updated synthetic samples. We also categorize each video as easy or hard: easy clips feature short, single-step actions in uniform scenes, whereas hard clips involve longer, multi-step activities with diverse materials in complex environments. To jointly characterize the capability boundary of video generation models (VGM) as well as the limitations of video understanding models, we introduce Peer-Review evaluation: VGMs act as creators aiming to fool all reviewers, while VLMs act as reviewers detecting fakes. As illustrated in Fig. 1, video generators are evaluated by how well they fool VLMs, and VLMs are ranked by their accuracy in distinguishing real from generated content. This reciprocal design establishes coupled, adversarial framework for jointly benchmarking video generation and video understanding models. Build on top of our Video Reality Test benchmark, our evaluation reveals key insights that (i) even state-of-the-art VLMs fall short on Video Reality Test, with Gemini-3-Pro achieving an average accuracy of only 76.27 across seven VLMs, far behind the human score of 89.11; while VGMs like Veo3.1-fast generate highly realistic videos, with only 12.54% detected as fake by VLMs, demonstrating strong ability to deceive state-of-the-art detectors. (ii) We also observe strong bias for VLMs to classify videos as real rather than fake, with up to 71% of videos labeled real across both real and generated samples. (iii) Moreover, incorporating audio generally improves performance by approximately 5 points, as audio generated by current video generation models is often poorly aligned with visual content (e.g., Sora2 produces human voices instead of continuous ASMR sounds, whereas Veo3.1-fast generates more action- (iv) Watermarks further bias the evaluaspecific audio.) tion: top VLMs take the Sora2 watermark as the shortcut reaching 95.43 detection accuracy on watermarked videos, but dropping to 52.4barely above random chanceafter watermark removal, demonstrating that high accuracy can largely stem from superficial cues rather than genuine perceptual understanding. In summary, our contributions are three-folds: Video Reality Test benchmark. We curate first ASMR-sourced benchmark emphasizing tight audiovisual coupling and perceptual fidelity: 149 real clips carefully selected with diversity in duration, objects, actions, and backgrounds, plus easy and hard splits. Peer-Review Evaluation. We introduce novel creatorreviewer protocol that jointly benchmarks video generation and understanding: video generators act as creators aiming to fool reviewers; VLMs act as reviewers identifying fakes. Generators are scored by fooling rate; reviewers by real and fake accuracy. Extensive findings and insights. We benchmark frontier models and reveal limits of current VLMs: the best creator (Veo3.1-Fast) reduces the strongest reviewer (Gemini 2.5-Pro) to 56% accuracy (chance 50%), trailing human experts at 81.2%; audio consistently improves discrimination, yet superficial cues (e.g. watermarks) still mislead modelsexposing gaps in perceptual fidelity and audiovisual consistency. 2. Related Work 2.1. AI-Generated Video Detection The rapid advancement of generative models has produced highly realistic videos, raising concerns about misinformation and content authenticity [17], which has motivated research on detecting AI-generated or manipulated videos [26]. Early efforts on AI-generated video detection primarily targeted facial forgeries, with benchmarks such as FaceForensics++ [41], DFDC [9], and Celeb-DF [24] driving progress in detecting spatial artifacts. Subsequent studies leveraged frequency-domain inconsistencies [29, 48], temporal dynamics [35, 56], and audiovisual cues [18] to enhance generalization. As diffusion-based generators produce videos with unprecedented realism and temporal coFigure 2. An overview of Peer-Review framework for ASMR video reality testing. Video generation models (creators) attempt to synthesize fake ASMR videos that can fool multimodal reviewers, while video-understanding models (reviewers) aim to detect fakes. Leaderboards on both sides highlight which creators deceive the most reviewers and which reviewers identify the most fake videos, revealing competitive peer-review process between generation and detection. herence, recent works [31, 45, 54] have begun exploring motion-level inconsistencies and high-order temporal statistics, while complementary efforts such as SynthID [11] address provenance and traceability. Unlike previous works, Video Reality Test jointly assesses audiovisual realism and models detection as an adversarial process between generation and understanding. 2.2. Video-Audio Understanding and Generation Joint VideoAudio Understanding. Joint videoaudio understanding poses advanced requirement beyond vision. Early works leveraged natural audio and visual synchrony for self-supervisiontransferring visual knowledge to aulearning audiovisual correspondence [1, 34], dio [2], and advancing sound source separation and event localization [44, 53]. Recent omni-modal foundation models ([50, 51]) emphasize unified videoaudio perception and reasoning, and arrive with new audio-visual evaluation suites [13, 22, 25]. Whereas most benchmarks center on real-world video samples, our suite complements this space by introducing AI-generated videos with synthetic audio, enabling holistic evaluation on both real and fake content. Tab. 1 presents comprehensive comparison of Video Reality Test and existing benchmarks, highlighting their relationships and key differences. We then outline the primary objectives of existing video understanding and generation benchmarks in each domain. Joint Video-Audio Generation. Recent studies have advanced joint audiovideo synthesis through diffusion and transformer frameworks. MM-Diffusion [42] first established cross-modal denoising for synchronized audiovisual generation, followed by AV-DiT [47], which achieves efficient joint modeling via shared DiT backbones. MMDisCo [12] introduces discriminator-guided cooperative diffusion, while simple but strong baseline adapts pre-trained uni-modal diffusers for synchronized outputs. Synchronization-focused methods such as SyncFlow [27] and KeyVID [49] further enhance temporal alignment through dual diffusion and keyframe-aware mechanisms. On the evaluation side, VBench [15] and VideoPhy [5] assess perceptual and physical realism but remain limited to visual aspects. In contrast, Video Reality Test (Video Reality Test) provides the first unified and scalable benchmark for evaluating both video understanding and generation, using high-fidelity ASMR videos to measure content authenticity (real vs. AI-generated). 3. Video Reality Test We introduce Video Reality Test, AIGC video detection benchmark sourced from ASMR videos. Finally, we introduce our task formulation for Peer-Review setup in 3.1. Next, we describe our data collection and curation process in 3.2. We then present comprehensive data analysis, including statistics across multiple dimensions, in 3.3. 3.1. Task Definitions As illustrated in Fig. 2, Video Reality Test as two-player game between creator by video generation model and reviewer by video understanding model U. Let Vreal denote real ASMR video and Vfake = G(I, ) fake video generated by G, conditioned on at least one of the two inputs: referred image frame or text description . (i) For reviewer, each model outputs boolean judgement = U(V) {0, 1}, where 0 indicates fake video and 1 real video. We then evaluate the VLM by its accuracy over all videos Vfull = {Vreal} {Vfake}, AccU = 1 1[U(V) = y(V)], where y(V) is the ground truth equal to 1 for real and 0 for fake. VVfull (cid:80) (ii) For creator, we score each generation model by its rate of being detected as fake, FakenessG = (cid:80) 1 Vfake Vfake 1[U(Vfake) = 0]. To sum up, higher AccU indicates stronger understanding model, while lower FakenessG indicates more realistic generative model. Figure 3. Illustration of Video Reality Test creation pipline, encompassing four phases: (i) Popular ASMR videos are manually collected, (ii) Preprocess the raw videos by splitting and removing backgrounds, then extract the first frame, (iii) Get the text description by Gemini2.5-pro given the video, (iv) Clustering the videos by Qwen3-embedding-4B with maximum silhouette score, and then sampling the representative ones to alleviate class unbalance. 3.2. Benchmark Construction Uniqueness by ASMR Videos. ASMR videos possess (i) they unique form of intrinsic authenticity because: rely on tightly coupled audiovisual stimulation to evoke genuine sensory responses; (ii) they feature fine-grained handobject interactions and subtle, deliberate motions, making them highly sensitive to perceptual fidelity; and (iii) even minor artifactswhether visual glitches or unnatural audio cuescan easily break immersion and disrupt the intended ASMR effect. Real Video Collection. Moreover, their widespread popularity on social media, measured by views and likes, provides practical proxy for successful sensory elicitation, allowing us to empirically define high-quality videos for benchmark construction. To begin with, we select the real ASMR videos with view counts exceeding 900k from YouTube, as such engagement metrics reflect stronger human immersion and preferences, indicating that these videos are more realistic in terms of aligning with human sensory experiences, neural patterns. Data Curation. The Video Reality Test creation pipeline is illustrated in Fig. 3. For each ASMR short video compilation, (i) Data preprocessing and noise removal. We preprocess the raw ASMR videos by (a) extracting short segments using frame color histograms and Bhattacharyya distance to detect scene transitions (threshold 0.5), (b) removing filled backgroundsincluding some with watermarksvia automated detection of content boundaries from grayscale column changes, and (c) verify each preprocessed short video contains only single ASMR theme by human and extract the first frame image of each cropped video for subsequent video generation, ensuring quality and relevance. (ii) Automated annotation. We use Gemini-2.5-Pro to generate storyboards for each short video based on eight video frames using the ASMR video description prompt details. (iii) Clustering & Representative Selection We use Qwen3embedding-4B to cluster the descriptive texts into eight classes. To avoid class imbalance, we selected an equal number of high-quality images (1213 per class, totaling 100) as representative samples for subsequent video generation models. The prompts and the quality of the generated videos were evaluated through human assessment. 3.3. Data Analysis Overall, as shown in Fig. 4, Video Reality Test comprises 149 real ASMR videos selected from an initial pool of 1,400 candidates. We categorize them into two difficulty levels: easy (49 videos) and hard (100 videos). Notably, unlike prior work that pre-computes fake samples, we let video generation models produce their own outputs. This yields dynamic dataset of size 149 (1 + K), where is the number of video generation models tested. Diversity The video difficulty level is determined by the distinct variances across four key dimensions: Time Diversity, Object Diversity, Action Diversity, and Background Diversity. Easy-level videos are typically short (3 5 seconds in Fig. 4c), featuring simple, single-step actions (e.g. cutting foo) set against visually consistent, minimal backgrounds (e.g. dark indoor settings), and involving limited variety of uniform objects. In contrast, hard-level videos are substantially longer (up to over 20 seconds), incorporating more complex and multi-step actions (e.g. squeezing, peeling, pumping, crushing with detailed in Fig. 4a). Furthermore, they exhibit highly diverse backgrounds, ranging from indoor environments (e.g. garages, factories) to expansive outdoor scenes (e.g. forests, rivers), and interact with wide variety of objects that include both solids (e.g. soap, plastic, dolls) and liquids (e.g. paint, water). 4. Experiments In this section, we conduct experiments on various VLMs and VGMs to evaluate their abilities based on our Video Reality Test via our peer-review criteria. Mainly, we design the experiments to study the following key questions: Q1: Which is the best, or worst, in term of VLMs? How do audios influence the detection ability? Is our Video Reality Test challenging to them? Q2: Which is the best, or worst, in terms of VGMs? How do different generation settings influence the video quality? Figure 4. Detailed analysis of Video Reality Test. (a) is the example of Video Reality Test across different dimensions, (b) is the statistic of the distribution of Video Reality Test on easy and hard level, (c) upper is the action statistics of Video Reality Test and the bottom is the video time distribution and comparison of easy-level and hard-level on Video Reality Test. Q3: What factors influence the peer-review results and ranking (e.g., watermarks, different reasoning and prompt settings, inherit bias within the VLMs)? 4.1. Setup Models We evaluate mainstream video generation models (VGMs), including the latest open-source (Wan2.2I2V-A14B, Wan2.2-T2V-A14B, and Wan2.2-TI2V-5B [46]; Opensora-V2 [40]; HuyuanVideo-I2V and HuyuanVideoT2V [20]; StepVideo-T2V [30]) and closed-source models (Sora2, Veo3.1-fast). For the closed-source models, we cannot access the API as they are the latest versions. Instead, we manually upload the image and prompt to the official websites and download the generated videos. We evaluate range of popular video understanding models (VLMs), encompassing both latest open-source (Qwen2.5-VL-7B, Qwen3-VL-8B-Instruct, Qwen3-VL-30B-A3B-Instruct, Qwen2.5-VL-72B-Instruct, and Qwen3-VL-235B-A22B-Instruct, GLM-4.5V [43].) proprietary models (Gemini-2.5-Pro and Flash [7], GPT-4o and GPT-4o-mini [16], GPT-5). Evaluation Details To ensure balanced evaluation, the dataset maintains 1:1 ratio of synthetic to real-world videos for the video understanding task. We conduct our experiments based on 8 frames sampled equally interleaved from the initial videos following [4]. For human evaluation, we randomly sample 10 videos per generated-video setting, with details provided in the appendix. In total, 100 videos are used, including 30 real ones. The videos are presented in randomized order, and humans are asked to identify the real and fake videos without being informed of the correct answers beforehand. For video understanding, we evaluate VLMs under multiple controlled settings: (i) input modality: visual-only vs. visual+audio; (ii) synthetic video type: with watermark vs. without watermark; (iii) preference judgment: given one real and one generated video, the model must identify the real one; (iv) reasoning style: prompting VLMs to provide chain-of-thought before answering vs. giving direct answer; (v) varying the number of input frames to study how temporal coverage affects reality detection. For video generation, VGM models are required to produce videos conditioned on the start-frame image and the text description provided in Video Reality Test. We then ask VLM reviewers to identify the fake video and compute the accuracy of correctly detected fakes. lower detection rate indicates that the generated videos are harder to distinguish from real ones, reflecting stronger generation capability. We additionally explore three generation settings: (i) start-frame image + detailed text description of the video content; (ii) start-frame image only, with the prompt Generate the video as real as possible; (iii) detailed text description only, without providing the start frame. 4.2. Evaluation for Video Understanding Q1-1. Are current VLMs good enough for detecting real or generated videos? Tab. 2 reports comprehensive evaluation of 11 VLMs on Video Reality Test, along with human and random baselines as the upper and lower bounds. We use 7 VGMs as peer reviewers to generate fake videos under the same imagetext setting. Overall, proprietary VLMs show stronger performance than most opensource models, with Gemini-3-pro achieving the best results and Gemini-2.5-pro following closely. Yet their ability to reliably distinguish real from fake videos remains limited: Table 2. Performance comparison on Video Reality Test for video understanding. Gold , silver , and bronze denote the best, second, and third performers. Generation type is ImgText2Vid consistently. Model Random Human Veo3.1-Fast [8] 50 81.25 Qwen3-VL-8B Qwen3-VL-30B-A3B Qwen2.5-VL-72B Qwen3-VL-235B-A22B GLM-4.5V [43] GPT-4o-mini [16] GPT-4o [16] GPT-5 Gemini-2.5-Flash [7] + Audio Gemini-2.5-Pro [7] + Audio Gemini-3-Pro-Preview [7] 57.79 51.08 49.50 56.53 54.64 52.50 51.50 54.55 47.72 52.55 51.56 56.00 77.89 Sora2 [39] Wan2.2-A14B [46] Wan2.2-5B [46] Opensora-V2 [55] HunyuanVideo [20] StepVideo [30] Avg. () Rank 50 91.25 87.69 51.35 71.07 80.75 63. 51.78 51.27 95.43 87.56 93.65 84.49 87.72 89.90 50 86.25 55.56 49.44 51.05 53.89 54.90 53.68 55.26 55.26 53.55 53.55 59.09 59.09 57.67 50 91.25 Open-source Models 56.28 54.74 51.00 52.66 57.59 Proprietary Models 50.50 55.50 57.50 55.44 55.44 60.21 60.21 73.87 50 91. 51.50 47.09 54.50 50.79 66.24 53.00 56.50 56.78 55.15 55.15 62.30 62.30 65.83 50 91.25 54.50 49.74 53.50 48.19 61.01 50.50 56.50 56.50 53.06 53.06 65.76 65.76 80.90 50 91. 83.84 81.68 81.50 90.53 87.13 89.00 95.00 93.97 78.63 78.63 87.98 87.98 87.94 50 89.11 63.88 54.87 58.87 61.91 63.61 57.28 60.22 67.14 61.59 63.15 67.34 68.44 76.27 13 5 12 10 8 6 11 9 4 7 8 3 3 2 Table 3. Performance comparison on Video Reality Test of video generation models on different generate types. image means the start-frame image and text means the text description. Inputs Image Text GPT-4o -mini GPT-4o Gemini-2.5 Gemini-2.5 -Flash -Pro Avg.( ) Rank Text2Vid ImgText2Vid Text2Img2Vid Text2Vid-A14B ImgText2Vid-A14B ImgText2Vid-5B Text2Vid ImgText2Vid Text2Vid ImgText2Vid + Audio - Watermark Text2Vid ImgText2Vid + Audio Opensora-V2 [40] 10.00 15.00 18.00 Wan2.2 [46] 13.00 7.78 13.00 HunyuanVideo [20] 7.00 15.00 Sora2 [39] 9.00 3.09 3.09 6.19 StepVideo [30] 92.00 Veo-3.1-fast [8] 5.00 5.00 14.00 12.00 14.00 12.00 8.89 7.00 8.00 7.00 16.00 8.25 8.25 8.25 84. 11.00 11.00 28.72 30.21 45.36 24.47 23.53 30.53 25.51 26.53 100.00 95.79 97.89 25.00 73. 16.16 19.20 39.18 35.16 43.75 21.74 26.19 33.33 18.56 42.39 97.89 79.17 88.66 24.74 86. 17.00 25.00 22.98 23.59 30.28 17.80 16.10 20.97 14.77 22.73 55.72 46.58 49.47+2.89 16.55 83. 12.54 15.05+2.51 8 9 10 5 3 6 2 7 12 11 11 4 1 1 the top score across all VLMs is only 76.27, far below the human score of 89.11. Most open-source models perform poorly, indicating substantial room for improvement. Q1-2. Combined Audio increases the VLM performance to detect the real or fake videos. Tab. 2 shows that incorporating audio improves real-or-AI detection performance in the best Gemini family models.(cid:66) Further analysis in Fig. 5(b), where we conduct experiments on Video Reality Test using audio in addition to visual and text inputs, resulting in an overall increase in detection accuracy, with an average improvement of 5 points. This improvement is likely because audio generated by current video generation models is often poorly aligned with the video content. Specifically, we observe that audios from Sora2 contain human voices rather than immersive, continuous ASMR sounds, whereas Veo3.1-fast better generates actionspecific sounds without human speech. Visualization exam- (cid:66)GPT-4o and Qwen2.5/3-omni APIs do not support input image, text and audio at the same time. Open-source VGMs cant generate audio along with the image. ples with the reasoning process are provided in Supp. 4.3. Evaluation for Video Generation Q2-1. Can current video generation models effectively fool the VLMs? Overall, video generation models can produce high-quality, lifelike videos, giving them substantial ability to mislead VLMs. Tab. 3 shows that the proprietary model Veo3.1-Fast performs best, with only 12.54% of its videos detected as fake. The open-source models HunyuanVideoI2V and Wan2.2-A14B follow with scores of 14.77 and 16.10, respectively, both outperforming the latest closed-source Sora2 (with or without watermark), indicating that the performance gap between proprietary and opensource models is narrowing. In summary, within our Video Reality Test, the strongest VGM is Veo3.1-Fast (12.54), while the best-performing VLM is Gemini-2.5-Pro (68.44), yet Veo3.1-Fast surpasses Gemini-2.5-Pro by generating highly realistic videos that successfully fool the VLMs. Q2-2. Different factors influence the realism and quality of generated videos. As seen in Tab. 3, adding audio increases the likelihood of being detected as fake, with the detection rate rising from 12.54 to 15.05 for Veo3.1Fast and from 46.58 to 49.47 for Sora2, likely due to mismatches between visual and auditory artifacts. Moreover, in Tab. 3, different video generation types (e.g., Image2Video, TextImage2Video, Text2Image2Video) have limited impact on Video Reality Test performance. The three configurations of OpenSora-V2 consistently rank between 8th and 10th, and the two Sora2 settings are tightly grouped at 11th and 12th. However, model scale has substantial impact: larger models generally achieve better results. For example, within Wan2.2, the A14B variant outperforms the 5B version, demonstrating that increasing model size improves performance. Figure 5. Key Ablation and Analysis. (a) shows that SoTA VLMs performance drops after the sora watermark removal, showing that they rely on the watermark as shortcut rather than true video quality. (b) shows that incorporating audio along with visual inputs generally improves reality detection accuracy. (c) highlights the bias of models toward classifying videos as real rather than fake, demonstrating the challenge posed by Video Reality Test in the reality test. Table 4. The accuracy distribution on easy-level for real/generated videos between (i) answer with thinking and (ii) directly answer ; generate videos with (i) video content text (with text) and (ii) generate the video as real as possible (without text).VLMs prone to predict real than fake. Eval think? real Wan2.2-A14B [46] Opensora-v2 [40] without text with text without text with text Qwen2.5-VL-7B [3] Judgement Judgement Judgement 98.0 100.0 73.5 89. 71.4 75.5 6.1 4.1 0.0 0.0 GPT-4-turbo [3] 49.0 16.3 40.8 12. Gemini-2.5-pro [3] 51.0 53.1 32.7 34.7 10.2 6.1 42.9 22.4 69.4 75. 2.0 0 28.6 8.2 53.1 51.0 4.4. Key Ablations Q3-1. Watermarks act as detection shortcut, particularly for models with strong video understanding capabilities. Tab. 2 shows abnormally high authenticity accuracy for Sora2 videos, for example, 93.65 for Gemini2.5-Flash, 95.43 for GPT-5, and even 87.69 for the smaller open-source VLM Qwen3-VL-8, which is unusually high compared to the average of around 55 for other VGMs such as Wan or OpenSora-V2. Further analysis shows that the SoTA VLMs tend to detect the Sora2 watermark during their reasoning process and directly label the video as fake even though we did not include it in the prompt, since our videos were downloaded directly from the Sora2 website, where the watermark is present by default. Further analysis shows that these VLMs are prone to detecting the Sora2 watermark during their reasoning process and directly labeling the video as fake since our videos are directly download from the Sora2 website with the watermark as default. Specifically, we observe that GPT-4 is not overfitting to watermarks, whereas Gemini-2.5 and GPT-5, despite higher overall performance on Video Reality Test, tend to rely on them. To verify this, we remove the default waterTable 5. Different evaluation diagram on easy-level with Qwen2.5-VL-7B. Pair uses the same real video as source; pair shuffle uses different one. Preference evaluation is easier than judgment. Eval think? Pair Pair shuffle Pair Pair shuffle Wan2.2-A14B [46] Opensora-v2 [40] without text with text without text with text 43.8 58.3 56.2 56.2 47.9 52.1 56.2 56.2 58.3 50.0 77.1 62.5 52.1 54.2 68.8 66.7 mark from Sora2-generated videos and re-evaluate them using our proposed Video Reality Test under the same settings in Fig. 5(a). Results show that GPT-4o and GPT-4o-mini remain largely unchanged, whereas GPT-5 and Gemini-2.5Pro/Flash drop substantially, e.g. from 95.4 to 53.4 with an average decrease of 30 points, which is align with the result in Tab. 3 where Sora2 ranking improves from 11-th to 4-th. These findings confirm that GPT-4o are less influenced by watermarks, while Gemini-2.5 and GPT-5 overfit to them despite the better performance. Q3-2. Fake videos are highly realistic and VLMs tend to classify them as real rather than fake. We observe that most VLMs are more likely to judge video as real than fake. By analyzing the percentage of real-or-fake predictions across different models, we find consistent bias toward predicting real (<answer>1 </answer>) across all VLMs on the high-quality generated videos from Sora2 and Wan. For example, Fig 5(c) shows that among 100 fake videos generated by Wan2.2 and 100 real videos from our benchmark, the models classify 71% videos as real (33% fake + 38% real) and only 29% as fake. Similar trends are observed across other VLMs on the easy-level cases of Video Reality Test, as detailed in Table 4. For instance, real videos can be correctly identified 100%, while videos generated by Wan are all misclassified as real by less capable models such as Qwen2.5-VL-7B. This suggests that VLMs are inclined to consider videos as genuine rather than fake, Figure 6. Qualitative Results on Video Reality Test, where the top one shows that the 1st VLM Gemini-2.5-pro selected by Video Reality Test, uses the Sora2 watermark as shortcut for reality detection, but classifies the video as real once the watermark is removed; the middle one shows that incorporating audio enhances the models ability to detect fake videos. Gemini-2.5-Flash successfully identifies fakes when both visual and auditory cues are present but is misled when relying solely on visual information; and the bottom one shows that Veo3.1-fast generates high-quality videos that successfully deceive GPT-5. likely due to their inherent conservative bias towards classifying all videos as real. Q3-3. Different prompts and reasoning processes influence. We further explore different evaluation settings beyond the direct judgment task. Tab. 4 shows the accuracy distribution in real videos (real) and fake videos (wan2.2, opensorav2), we find that think before answering prompt format improves performance for stronger VLMs such as Gemini-2.5-pro, but provides little or no benefit for weaker models like Qwen2.5-VL-7B, likely due to hallucinations introduced during the reasoning process that negatively affect the final judgement. Moreover, we test two settings: one providing detailed video context and another with only the instruction to generate the video as realistically as possible. We observe that video quality is comparable in both real and fake evaluations, with only slight differences between the two videos. Q3-4. Preference comparison is easier than the single video judgement. Despite our default setting, where the model is prompted to directly classify videos as real or fake, we also test preference setting in which the model is given two videos, one real and one fake. Tab. 5 reports the results of this preference evaluation, where the model is prompted to identify the real video from realfake pair by answering 1 if the first video is real and 2 if the second is real. We test this on two settings: Pair, where the fake video is generated from the same real video, and Pair Shuffle, where it is generated from different real video. Results show that this preference task is easier for video understanding models, while the pair and pair-shuffle settings have little impact. For example, Qwen2.5-VL-7B reaches 66.7 accuracy on the preference task, compared with 55 on direct judgment, with similar improvements observed across other video generation models. 4.5. Qualitative Results In Fig. 6, we visualize representative model behaviors and failure cases. In the top example, Gemini-2.5-Pro correctly detects the Sora2 watermark and identifies the video as fake. However, once the watermark is removed, the model classifies the video as real. In the middle, videos generated by Sora2 are initially classified as real by Gemini-2.5-Flash; however, after including audio in the inputs, the VLM correctly detects them as fake. In contrast, in the bottom example, video generated by Veo3.1-Fast appears highly realistic in terms of hand motion, object interaction, and lighting consistency, causing GPT-5 and Gemini-2.5-Pro to mistakenly classify it as real. 5. Conclusion In this work, we introduce Video Reality Test, an ASMRbased benchmark that provides AIGC testbed for evaluating video understanding and generation. Unlike prior evaluations reliant on manually crafted rules, our benchmark directly targets the core question of whether video is real or fake. We further propose novel peer-review framework in which VLMs and VGMs compete and rank one another. Our experiments show that top-performing VLMs still struggle with reality assessment, displaying issues such as watermark sensitivity and agreement bias. These results reveal concrete failure modes and highlight the need for stronger audiovisual reasoning and more realism-aware perception in future. Limitations. Our current benchmark focuses on ASMR videos. Future work will expand the data scale and extend the evaluation to broader, more natural video domains."
        },
        {
            "title": "References",
            "content": "[1] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 3 [2] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. Advances in neural information processing systems, 29, 2016. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. 7 [4] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. 1, 2, 5 [5] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 1, 2, [6] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. 1, 2 [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 5, 6 [8] deepmind. Introducing veo 3, our video generation model with expanded creative controls including native audio and extended videos. https://deepmind.google/models/veo/, 2025. 1, 6 [9] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset. arXiv preprint arXiv:2006.07397, 2020. 2 [10] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, and Feilong Zuo. Seedance 1.0: Exploring the boundaries of video generation models, 2025. 1 [11] Google DeepMind. Watermarking ai-generated text and video with synthid. Blog, 2024. 3 [12] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Mmdisco: Multi-modal discriminator-guided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024. 3 [13] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. 3 [14] Haoyang Huang, Guoqing Ma, Nan Duan, Xing Chen, Changyi Wan, Ranchen Ming, Tianyu Wang, Bo Wang, Zhiying Lu, Aojie Li, et al. Step-video-ti2v technical report: state-of-the-art text-driven image-to-video generation model. arXiv preprint arXiv:2503.11251, 2025. 1 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 1, 2, [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5, 6 [17] Achhardeep Kaur, Azadeh Noori Hoshyar, Vidya Saikrishna, Selena Firmin, and Feng Xia. Deepfake video detection: challenges and opportunities. Artificial Intelligence Review, 57(6):159, 2024. 2 [18] Khalid, Tariq, Kim, and SS Woo. Fakeavceleb: novel audio-video multimodal deepfake dataset. arxiv 2021. arXiv preprint arXiv:2108.05080. 2 [19] KLING. Kling ai: Next-generation ai creative studio, 2025. 1 [20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 5, [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 2 [22] Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, et al. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms. arXiv preprint arXiv:2510.10689, 2025. 3 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2 [24] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: large-scale challenging dataset for deepfake forensics. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3207 3216, 2020. 2 [25] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, et al. Omnibench: Towards the future of universal omni-language models. arXiv preprint arXiv:2409.15272, 2024. [26] Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, and Shu Hu. Detecting multimedia generated by large ai models: survey. arXiv preprint arXiv:2402.00045, 2024. 2 [27] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. 3 [28] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2 [29] Yuchen Luo, Yong Zhang, Junchi Yan, and Wei Liu. Generalizing face forgery detection with high-frequency features. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1631716326, 2021. 2 [30] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 1, 5, 6 [31] Long Ma, Zhiyuan Yan, Qinglang Guo, Yong Liao, Haiyang Yu, and Pengyuan Zhou. Detecting ai-generated video via frame consistency. arXiv preprint arXiv:2402.02085, 2024. [32] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 1, 2 [33] Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. 1 [34] Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva. Spoken moments: Learning joint audio-visual representations from video descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1487114881, 2021. 3 [35] Yunsheng Ni, Depu Meng, Changqian Yu, Chengbin Quan, Dongchun Ren, and Youjian Zhao. Core: Consistent representation learning for face forgery detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1221, 2022. 2 [36] Zhenliang Ni, Qiangyu Yan, Mouxiao Huang, Tianning Yuan, Yehui Tang, Hailin Hu, Xinghao Chen, and Yunhe Wang. Genvidbench: challenging benchmark for detecting ai-generated video. arXiv preprint arXiv:2501.11340, 2025. 1 [37] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. [38] OpenAI. Sora, 2025. 1 [39] OpenAI. Sora 2 is here. https://openai.com/index/sora-2/, 2025. 1, 6 [40] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200 k. arXiv preprint arXiv:2503.09642, 2025. 5, 6, 7 [41] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 111, 2019. 2 [42] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 3 [43] GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. 5, In Proceedings of the 28th dataset for deepfake detection. ACM international conference on multimedia, pages 2382 2390, 2020. 2 [44] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247263, 2018. 3 [45] Danial Samadi Vahdati, Tai Nguyen, Aref Azizpour, and Matthew Stamm. Beyond deepfake images: Detecting ai-generated videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43974408, 2024. 3 [46] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 5, 6, 7 [47] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024. 3 [48] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are In Proceedings of surprisingly easy to spot... for now. the IEEE/CVF conference on computer vision and pattern recognition, pages 86958704, 2020. [49] Xingrui Wang, Jiang Liu, Ze Wang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Yusheng Su, Alan Yuille, Zicheng Liu, and Emad Barsoum. Keyvid: Keyframe-aware video diffusion for audio-synchronized visual animation. arXiv preprint arXiv:2504.09656, 2025. 3 [50] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 3 [51] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. 3 [52] Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. arXiv preprint arXiv:2410.09732, 2024. 1, 2 [53] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European conference on computer vision (ECCV), pages 570586, 2018. 3 [54] Chende Zheng, Ruiqi Suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, and Chao Shen. D3: Training-free ai-generated video detection using second-order features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12852 12862, 2025. [55] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 6 [56] Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. Wilddeepfake: challenging real-world"
        },
        {
            "title": "Contents",
            "content": "6. Different prompt for video reality test 7. Prompt to get the text description of the ASMR video 8. Visualization Examples. 9. Detailed experiments 13 13 13 9.1. Evaliuation metric . . . . . . . . . . . . . . 13 9.2. Hard-level results for Video Reality Test . . . 13 6. Different prompt for video reality test We use the following prompt as the default prompt for the experiments in the main paper: Prompt for detecting real or fake. First, Given the following video, please determine if the video is real or fake. think about the reasoning process, and then provide the anThe reasoning process should be enswer. closed within <think></think> tags, and should be enclosed within <anthe answer swer></answer>tags. Respond with 1 for real and 0 for fake. For example: <think>reasoning process here <think><answer>answer here </answer>. <video >"
        },
        {
            "title": "Prompt for Detailed Judgement",
            "content": "You are given the frames evenly sampled from an ASMR video. <video> Your task is to determine whether this video is real human-recorded ASMR video or synthetically generated one. Carefully analyze the frames for: 1. Visual realism look at lighting consistency, texture fidelity, material reflection, and naturalness of motion cues. 2. Human or object authenticity whether skin, hands, tools, or surfaces appear physically plausible rather than rendered or overly smooth. 3. Acoustic plausibility (implied) infer if the scenes visual rhythm and tactile context align with realistic ASMR sound production (e.g. brushing, tapping, slicing, pouring). 4. Temporal coherence whether transitions between frames follow physically continuous process or show artifacts of generation (e.g. warping, unstable geometry, or inconsistent textures). After reasoning, output the answer in the following strict format: <think>(Write 24 sentences summarizing why you believe the video is real or generated.) </think> <answer> 0 </answer> where 0 = generated (fake), 1 = real (authentic human-captured video). Prompt for Judgement without reasoning Given the following video, please determine if the video is real or fake. Respond with 1 for real and 0 for fake. Provide the answer directly within <answer></answer>tags, like this: <answer>answer here </answer>. <video >"
        },
        {
            "title": "Prompt for Preference Comparison with reasoning",
            "content": "9. Detailed experiments 9.1. Evaliuation metric For the video understanding model , it predict the answer given real videos and it induced generated fake videos. We extract the number within the <answer>number </answer>by regular pattern match. accuracy = correct detected answers number total valid answers number For the video generation model g, it predict generated videos based on Video Reality Test real videos. We do not need to consider the real videos, just focus on the fake one and calculate how many they can fool the video understanding model {. We extract the number within the <answer>number </answer>by regular pattern match. accuracy = detected as fake numbers total valid answers number 9.2. Hard-level results for Video Reality Test Tables 6-16 are the hard-level reality evaluation results across mainstream video understanding models and generation models. <video><video>Based on the previous two videos, please identify which one is real. Respond with 1 if the first video is real and 2 if the second video is real. First, think about the reasoning process, and then provide the answer. The reasoning process should be enclosed within <think></think>tags, and the answer should be enclosed within <answer></answer>tags. Respond with 1 for real and 0 for fake. For example: <think>reasoning process here </ think><answer>answer here </ answer>."
        },
        {
            "title": "Prompt for Preference Comparison without reasoning",
            "content": "the following <video><video>Given video, please determine if the video is real or fake. Respond with 1 for real and 0 for fake. Provide the answer directly within <answer></answer>tags, like this: <answer>answer here </ answer>. 7. Prompt to get the text description of the"
        },
        {
            "title": "ASMR video",
            "content": "Prompt for getting storyboard of AMSR video Given 8 frames evenly sampled from an ASMR video, describe the overall scene in single, continuous paragraph. Integrate information about the visual environment (background, lighting, textures, mood), the main subjects or objects, their actions and temporal dynamics, and the corresponding auditory sensations (e.g. crisp cutting, gentle tapping, soft brushing, fluid pouring). Use concise yet immersive language that evokes sight, touch, and sound simultaneously. Avoid enumeration or structural markersproduce one coherent cinematic paragraph. <video> 8. Visualization Examples. Below we show the video frames and the reasoning process along with the answers for qualitative analysis, including: Figure 7: Veo3.1-fast generated videos with and without audio evaluation, where with audio is detected to be generated, while without audio is real. Figure 8: Sora2 generated videos with and without audio evaluation, where with audio is detected to be generated, while without audio is real. Figure 7. Gemini-2.5-pro on veo3.1-fast generated videos, with and without audio. After adding the audio, the VLM detects the video to be fake. Table 6. Prediction statistics of Qwen3-VL-8B-Instruct on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 100 Real 99 Veo3.1-fast 95 Sora2 100 Hunyuan 100 Opensora (i2v) 99 Wan (it2v) 89 Wan (i2v) 98 StepVideo (t2v) 23 37 94 32 26 35 28 89 77 62 1 68 74 64 61 9 Figure 8. Gemini-2.5-flash on sora2 generated videos, with and without audio. After adding the audio, the VLM detects the video to be fake. Table 7. Prediction statistics of Qwen3-vl-235b-Instruct on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 95 Real 96 Veo3.1-fast 92 Sora2 98 Hunyuan 94 Opensora (i2v) 93 Wan (it2v) 85 Wan (i2v) 95 StepVideo (t2v) 88 77 29 93 86 82 76 11 7 19 63 5 8 11 9 84 Table 8. Prediction statistics of Qwen2.5-vl-72B-Instruct on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 95 Real Videos 100 Veo3.1-fast 92 Sora2 (IT2V) 98 Hunyuan (IT2V) 94 OpenSora (I2V) 93 Wan (IT2V) 85 Wan (I2V) 95 StepVideo (T2V) 7 17 63 5 8 11 9 84 88 83 29 93 86 82 76 11 Table 9. Prediction statistics of GPT5 on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 100 Real Videos 99 Veo3.1-fast 97 Sora2 (IT2V) 100 Hunyuan (IT2V) 99 OpenSora (T2V) 99 OpenSora (I2V) 100 Wan (IT2V) 90 Wan (I2V) 99 StepVideo (T2V) 2 11 90 15 16 15 17 7 89 98 88 7 85 83 84 83 83 Table 10. Prediction statistics of Qwen3-VL-30B on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 94 Real Videos 90 Veo3.1-fast 91 Sora2 (IT2V) 95 Hunyuan (IT2V) 95 OpenSora (I2V) 96 Wan (IT2V) 86 Wan (I2V) 97 StepVideo (T2V) 75 70 71 76 81 67 72 16 19 20 20 19 14 29 14 81 Table 11. Prediction statistics of GLM-4V on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 80 Real Videos 77 Veo3.1-fast 80 Sora2 (IT2V) 79 Hunyuan (IT2V) 77 OpenSora (I2V) 78 Wan (IT2V) 73 Wan (I2V) 91 StepVideo (T2V) 15 20 37 32 39 26 19 84 65 57 43 47 38 52 54 7 Table 12. Prediction statistics of Gemini-2.5-pro on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 92 Real Videos 99 Veo3.1-fast 95 Sora2 IT2V 96 Sora2 T2V 97 Hunyuan T2V 92 Hunyuan IT2V 96 OpenSora T2I2V 97 OpenSora T2V 91 OpenSora I2V 84 Wan I2V 99 Wan IT2V 92 Wan T2V 91 StepVideo T2V 82 83 19 3 79 53 54 59 59 62 66 72 12 10 16 76 93 18 39 42 38 32 22 33 20 Table 13. Prediction statistics of Gemini-2.5-flash on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 98 Real Videos 100 veo3.1-fast 95 Sora2 IT2V 96 Sora2 T2V 98 Hunyuan IT2V 98 Hunyuan T2V 97 OpenSora T2I2V 94 OpenSora T2V 96 OpenSora I2V 95 Wan IT2V 94 Wan T2V 85 Wan I2V 20 5 91 96 26 25 44 27 29 29 23 20 78 95 4 0 72 73 53 67 67 66 71 65 Table 14. Prediction statistics of GPT-4o on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 100 Real Videos 100 veo3.1-fast 97 Sora2 IT2V 100 Sora2 T2V 100 Hunyuan IT2V 100 Hunyuan T2V 100 OpenSora T2I2V 100 OpenSora T2V 100 OpenSora I2V 100 Wan IT2V 100 Wan T2V 90 Wan I2V 100 StepVideo T2V 98 89 94 91 85 93 82 90 85 87 87 83 8 2 11 3 9 15 7 18 10 15 13 13 7 92 Table 15. Prediction statistics of GPT-4o-mini on real and generated videos. Source (real/Video Gen Model) Valid Total Answer Number Predict = Fake Predict = Real 100 Real Videos 100 veo3.1-fast 97 Sora2 IT2V 100 Sora2 T2V 100 Hunyuan IT2V 100 Hunyuan T2V 100 OpenSora T2I2V 100 OpenSora T2V 100 OpenSora I2V 100 Wan IT2V 100 Wan T2V 90 Wan I2V 100 StepVideo T2V 94 83 89 84 93 92 86 86 88 93 88 82 16 6 17 8 16 7 8 14 14 12 7 12 8 Table 16. Human evaluation statistics on real and generated videos model Veo3.1-Fast Sora2 Wan2.2-A14B Wan2.2-5B Opensora-V2 Hunyuan Video StepVideo real total 40 40 40 40 40 40 40 predict=fake 7 7 7 7 7 7 7 predict=real 33 33 33 33 33 33 33 fake toal 10 10 10 10 10 10 predict=fake 8 10 9 10 10 10 10 predict=real 2 0 1 0 0 0 0 score 0.8125 0.9125 0.8625 0.9125 0.9125 0.9125 0."
        }
    ],
    "affiliations": [
        "CUHK",
        "NUS",
        "University of Oxford",
        "Video Rebirth"
    ]
}