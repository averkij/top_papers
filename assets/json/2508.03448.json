{
    "paper_title": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering",
    "authors": [
        "Jan Melechovsky",
        "Ambuj Mehrish",
        "Dorien Herremans"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach."
        },
        {
            "title": "Start",
            "content": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering Jan Melechovsky, Ambuj Mehrish, Dorien Herremans Singapore University of Technology and Design jan melechovsky@mymail.sutd.edu.sg {ambuj mehrish,dorien herremans}@sutd.edu.sg 5 2 0 2 5 ] . [ 1 8 4 4 3 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMasters enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach. Samples, code, model, dataset, and demo are all available through https://amaai-lab.github.io/SonicMaster/ Introduction Music tracks produced in amateur settings often suffer from variety of audio quality issues that distinguish them from professionally mastered recordings (Wilson and Fazenda 2016; Mourgela et al. 2024; Deruty and Tardieu 2014). For instance, an enthusiast recording vocals in garage may introduce excessive reverberation, making the voice sound distant and echoey. Similarly, using inexpensive microphones or misconfigured interfaces can lead to distortion and clipping when loud peaks exceed the recording range, resulting in harsh crackles or flattened dynamics (Zang et al. 2025). Tonal imbalances are also common: home recording might sound overly muddy or tinny if certain frequency bands dominate or vanish due to poor room acoustics or improper microphone placement. Even the stereo image can be narrowed or skewed, reducing the sense of space in the mix. Figure 1: SonicMaster pipeline: flow-matching model trained on the SonicMaster dataset (19 simulated enhancements across EQ, dynamics, reverb, amplitude, stereo) takes an input music waveform and text prompts to produce high-fidelity enhanced output. In practice, engineers address these problems with separate specialized tools: e.g., dereverberation plugins to remove room echo, declipping algorithms to reconstruct saturated peaks, equalizers to rebalance frequencies, and stereo enhancers to widen the image. Mastering flawed track thus becomes labor-intensive process requiring expert skill and multiple stages of manual adjustment. The need for an automated all-in-one solution is evident. Creators with limited resources often lack the expertise to apply the right combination of restoration tools, and piecemeal approach may fail to fully recover tracks fidelity. This motivates SonicMaster, unified approach to music restoration and mastering that can correct broad spectrum of audio degradations within single model. We introduce single flow-based generative framework that simultaneously performs dereverberation, equalization, declipping, dynamic-range expansion, and stereo enhancement. The backbone is trained on curated corpus of polyphonic music rendered through combinatorial grid of simulated degradations, enabling the network to learn the joint statistics and cross-couplings of common artifacts rather than treating them in isolation. This joint training eliminates the need for error-prone cascades of task-specific modules and reduces inference to single forward pass. SonicMaster is conditioned on free-form text prompts that encode high-level production intent. prompt such as reduce the hollow room sound attenuates late reFigure 2: SonicMaster dataset creation pipeline and overview. flections without suppressing desirable early reverberation, whereas increase the brightness selectively enhances the treble frequencies while while preserving spectral balance elsewhere.When the prompt is omitted, an automatic mode leverages learned perceptual heuristics to produce balanced master suited to non-expert users. Existing models (e.g. VoiceFixer (Liu et al. 2021)) fail on musicfocused pipelines typically address artifacts sequentially, ignoring their mutual influence. By unifying restoration and mastering tasks under single, prompt-driven generative model, SonicMaster delivers professional-grade improvements while affording fine-grained creative control. In the absence of text-conditioned music-restoration data, we build new large-scale corpus for controllable restoration. From 580 Jamendo recordings we retain 25 high-quality 30-s segments, balanced across 10 genre groups by production quality score. Each clean clip is corrupted with one to three of 19 common effects drawn from five categoriesEQ, dynamics, reverb, amplitude, and stereoproducing paired degraded versions. Every degraded sample is accompanied by natural-language prompt describing the artifact or required fix, and all random effect parameters are stored as metadata. This genre-diverse collection of tens of thousands of promptaudio pairs underpins SonicMaster training and offers rigorous benchmark for controllable music-restoration research. Our main contributions are as follows: We present SonicMaster, flow-matching model that corrects 19 common degradationsincluding reverb, EQ imbalance, clipping, dynamic-range errors, and stereo artifacts in single generative pass, while allowing freeform textual prompts for fine-grained creative control. To train and evaluate such system, we curate and release the first text-conditioned music-restoration corpus: 25k high-fidelity Jamendo segments spanning 10 genres, each paired with 7 degraded versions of itself, detailed parameter metadata, and natural-language instruction describing the required fix, resulting in 175 audio pairs. Related Work Restoring and mastering audio spans speech + music enhancement, audio inpainting, and source separationareas that have mostly been handled separately (ZaviË‡ska et al. 2020). Diffusion-based generative models and text-guided audio editing (Hou et al. 2025; Jiang et al. 2025) now let us tackle these problems together. We review these advances, their uses, and the gaps that SonicMaster aims to fill. Audio Inpainting and Declipping: Early signal-model and interpolation methods could patch only very short gaps (< 10 ms), leaving longer dropouts unresolved. Deep generative models now bridge that gap: diffusion-based systems convincingly regenerate missing music sections and clipped peaks (Moliner and Valimaki 2023). The authors in (Wang et al. 2023; Liu et al. 2023) extend this with instructionguided diffusion for audio inpainting, while VoiceFixer (Liu et al. 2021) jointly denoises, dereverbs, and declipse speech, though it is restricted to voice and does not offer user control. In music, (Imort et al. 2022) removed heavy guitar distortion (including clipping) with neural networks, surpassing sparse-optimization baselines in quality and speed. Equalization and Tonal Restoration: Research on learning-based equalization is still emerging. In recently proposed work (Mockenhaupt, Rieber, and Nercessian 2024), the authors introduce CNN based approach to automatically equalize instrument stems by predicting parametric EQ settings, showing improvements over earlier heuristic methods. Notably, the VoiceFixer (Liu et al. 2021) addressed bandwidth extension essentially restoring high-frequency content as part of its speech restoration, which can be seen as form of equalization correction. Similarly, diffusion-based restorers like MaskSR (Li, Wang, and Liu 2024) treat lowfrequency muffling as distortion to fix, using discrete token prediction to restore balanced spectrum. In Text2FX (Chu et al. 2025), CLAP (Elizalde et al. 2023) is used in inference mode to steer the parameters of EQ and reverb audio effects."
        },
        {
            "title": "Method",
            "content": "Dataset Due to the absence of natural language instruction-prompted music restoration datasets, we have generated SonicMaster music restoration and mastering dataset with text prompts. We source 580k recordings from the Jamendo1 under cre1https://www.jamendo.com/ simulate three types of rooms: Small, Big, and Mixed. For our fourth Reverb function, we utilize 12 selected room impulse responses from the openAIR library dataset (Howard and Angus n.d.), which give us audio with more real-life properties. The resulting impulse responses from all the functions are convoluted with the clean signals. Amplitude: This category consists of two functions: Clipping and Volume. We clip the audio by raising the maximum amplitude to predefined levels, which causes distortion to arise. In Volume, we reduce the volume to very low (barely audible) range, which decreases the signal-toquantization noise ratio. Stereo: This category contains one function to de-stereo the audio recording. First, the track is analyzed for suitability of this operation (whether it is stereo enough) by taking the standard deviation of the difference betwen the left and the right channels and comparing to set threshold of 0.08, then, if eligible, the left and right channels are combined to create mono audio recording. Each ground truth yields 7 corrupted variants: 4 with single, 2 with double, and 1 with triple degradation. In multidegradation, we sample at most one effect from each of the 5 categories, so an EQ choice, for instance, blocks further EQ picks. To avoid duplicates in the single-degradation set, high-probability effects with narrow parameter ranges: Stereo, Clipping, and Punchare, are used only once. Each degradation is linked to one-sentence instruction randomly selected from 810 expert-written options; these sentences are concatenated into the full prompt, and we store two prompt variants per clip for robustness. We also record every applied effect and its parameters (gain, absorption), supporting tasks such as parameter prediction. For Compression and Reverb there is 15% chance of injecting hidden clipping with no corresponding instruction. When neither hidden clipping nor an Amplitude effect is present, the audio is peak-normalised to random level between 0.8 1.0. Further details can be found in Appendix. SonicMaster Architecture SonicMaster combines Multimodal DiT (Esser et al. 2024) with subsequent DiT blocks (Peebles and Xie 2023). As outlined in Figure 3, the stereo waveform (44.1 kHz) (xt) is first encoded by VAE codec (Evans et al. 2024) into compact spectro-temporal latent. Restoration therefore occurs entirely in this learned space, allowing large receptive fields without sample-level overhead. The MM-DiT block receives the degraded latent and the text embedding from frozen FLAN-T5 encoder (Chung et al. 2024), and the conditioned representation is then passed to DiT layers that further process it to predict the velocity vt steering the latent toward its clean target Ë†xt. Prompts like reduce reverb bias this prediction trajectory to suppress decay tails, while the downstream DiT layers also refine musical coherence. pooled-audio branch, active in 25% of training cases, concatenates temporally averaged 515 sec clean cue with the pooled prompt embedding and injects it at every MMDiT/DiT layer, enabling seamless chaining of 30 sec segments for long-form generation while degrading gracefully when no reference is supplied. Figure 3: Overall architecture of SonicMaster. ative commons licence using the official Jamendo API. To ensure equal representation of genre, we defined 10 groups, where each group consists of multiple related genre tags, e.g., Hip-Hop genre group containing the following tags: rap, hiphop, trap, alternativehiphop, gangstarap. The full list can be viewed in Appendix. To pick songs for each genre group, we started by deploying the Audiobox Aesthetics toolbox (Tjandra et al. 2025) to assess the production quality of all the recordings. Then, we selected 2, 500 songs for each genre group by filtering songs based on the production quality with an adaptive threshold between 6.5 8, depending on the current genre group. We strived to strike balance between representing all the subtags in each genre group sufficiently enough while maintaining high enough production quality scores. Finally, we extracted random 30 sec excerpt from each track, positioned between 15% and 85% of its total duration. The full data-generation pipeline is illustrated in Figure 2. We train SonicMaster by applying 19 distinct degradations to the audio and pairing each with matching editing instruction. The degradations span five classes: (i) EQ, (ii) Dynamics, (iii) Reverb, (iv) Amplitude, and (v) Stereo. Equalization (EQ): Spectral degradations cover 10 effects: Brightness, Darkness, Airiness, Boominess, Muddiness, Warmth, Vocals, Clarity, Microphone, and X-band. Brightness, Darkness, Airiness, Boominess, and Warmth are emulated with lowor high-shelf EQ; Clarity with Butterworth low-pass filter; Vocals and Muddiness with Chebyshev-II band-pass filters. Microphone applies one of 20 Poliphone transfer functions (Salvi et al. 2025), while Xband uses an 812-band, logarithmically spaced peaking EQ with 6 dB gain per band. Dynamics: This category contains 2 functions, Compression and Punch: modeled by feedforward compressor and transient shaper, respectively. Both these effects are applied in the time domain with lossy, irreversible nature, as constructing an exact inverse of these is an ill-posed problem. Reverb: The Reverb category contains four functions. Three of them utilize the Pyroomacoustics library (Scheibler, Bezzam, and Dokmanic 2018), which simulates acoustic environments with the image source method. We Audio and text encoding: We adopt the Stable Audio Open VAE (Evans et al. 2024) to encodedecode stereo signals sampled at 44.1 kHz, yielding compact latent representation while retaining high-fidelity reconstruction. Text instructions are embedded with FLAN-T5 Large (Chung et al. 2024); the resulting tensor ctext RBStextDtext (with Dtext = 1024) is fed to the model as conditioning signal. Training with the rectified flow: We train SonicMaster using rectified flow (Liu, Gong, and Liu 2022; Esser et al. 2024), to predict the flow velocity from degraded to clean audio in latent space, unlike other models that map noise to output distributions (Fei et al. 2024; Hung et al. 2024). We assign timestep = 1 to the latent representation of the degraded audio x1, and = 0 to the latent representation of the clean audio target x0. During training, we feed the model with samples xt, which are interpolations of degraded input x1 and clean target x0 on straight line: xt = tx1 + (1 t)x0 (1) where timestep is drawn from skewed distribution with increasing probability for higher as follows: p(t) = 0.5U (t) + 0.5t, (2) where represents uniform distribution. This skewed distribution gives emphasis to more degraded inputs given the interpolation of training data in Eq. 1. [0, 1]"
        },
        {
            "title": "The model is trained to predict the flow velocity vt from",
            "content": "the current xt to the target clean audio x0: vt = dxt dt = x0 x1 (3) The model fÎ¸ with parameters Î¸ estimates the velocity Ë†vt: fÎ¸(xt, t, ctext) = Ë†vt (4) where ctext is the text condition from the FLAN-T5 model, which is passed to the dual-stream MM-DiT blocks as one of the streams. The ctext condition is also passed through pooled projection and used to control the scale and shift factors of the adaptive layer-norm layers in both MM-DiT and DiT blocks. The training loss is then given as: L(Î¸) = Et,x1,x0 Ë†vtvt2 2 = Et,x1,x0fÎ¸(xt, t, ctext)vt2 2 (5) During inference, the degraded audio input x1 is converted into clean audio output x0 by integrating the predicted velocity Ë†vt using the forward Euler method: xt+h = xt + Ë†vt (6) where [0, 1] is the step computed as the inverse of total timesteps dedicated for integration. Inference During inference, SonicMaster takes in an audio input and text instruction given by user to perform desired restoration/mastering operation. Inference is possible without text input in the so-called auto-correction mode. To process full-length songs, SonicMaster operates on chunks of 30 sec and then connects the segments together. After the first segment is inferred, the last 10 sec of this output are used to condition the next segment inference through the audio pooling branch. The overlapping regions of the resulting segments are then linearly interpolated over the overlapping 10 seconds to connect the segments together."
        },
        {
            "title": "Experimental Setup and Baselines",
            "content": "Baselines and training setup We train SonicMaster using NVIDIA L40S GPUs for 40 epochs. We adopt classifier-free guidance (Ho and Salimans 2022) by (i) dropping the text prompt in 10% of samples and (ii) replacing it in another 10% with one of four generic phrases (Make it sound better!, Master this track for me, please!, Improve this!, Can you improve the sound of this song?). In 25% of cases, the model is additionally conditionedvia the pooling branchon the first 10 of clean audio. Unless stated otherwise, all experiments follow these conditioning settings while comparing multiple SonicMaster variants and baselines. To enable thorough comparison, we benchmark several baselines along with different configurations of SonicMaster: (i) Degraded inputthe original corrupted audio; (ii) Reconstructed inputthe same audio passed (iii) Text2FX-EQ, through the VAE encoderdecoder; an EQ baseline using Text2FX (Chu et al. 2025) with 600 iterations and 0.01 learning rate to correct EQ (iv) WPE dereverberadegradations via our prompts; tion, the Weighted Prediction Error algorithm (Nakatani et al. 2010) with prediction order of 30; (v) HPSS dereverberation, harmonicpercussive source separation (librosa.decompose.hpss) with 6 dB and 12 dB harmonic attenuation; and (vi) three SonicMaster variantsSonicMasterSmall (2 MM-DiT + 6 DiT), SonicMasterMedium (4 MM-DiT + 12 DiT or 6 MM-DiT + 6 DiT), and SonicMasterLarge (6 MM-DiT + 18 DiT). Objective Evaluation Evaluation is conducted along two orthogonal axes. (i) Global perceptual fidelity is quantified with Frechet Audio Distance (FAD) on CLAP embeddings (Elizalde et al. 2023), KullbackLeibler divergence (KL), structural similarity (SSIM) on 128-bin mel-spectrograms, and the Production Quality (PQ) score from the Audiobox Aesthetics toolbox (Tjandra et al. 2025). (ii) Degradation-specific restoration efficacy is measured by average absolute error reduction: for every degraded clip in 7000 clip test set, we compute the relevant (based on the degradation deployed) artefact-aware metric against its clean counterpart from 1000 sample reference set, then recompute the metric after SonicMaster processing; the relative decrease indicates how closely each model variant approaches the ground-truth. For X-band EQ and microphone-TF degradations we compute the spectral balance over eight bands and report their cosine distance. All other EQ effects are scored by the energy ratio between the affected band and the full spectrum. Compression is measured as the standard deviation of frame-level RMS (2048-sample frames, 1024 hop); punch as the mean onset-envelope value (librosa.onset.onset strength). Because RT60 Model (MMDiT/DiT) Clarity Boom Airy Bright Dark Muddy Warm Vocals Microphone X-band Snippet degraded input Degraded input 0.0238 0.3601 0.0049 0.0143 0. 0.4560 0.4345 0.2525 Reconstructed input 0.0243 0. 0.0051 0.0151 0.0728 0.4749 0.4456 0. Text2FX-EQ (Chu et al. 2025) 0.0219 0.3809 0.0055 0.0276 0. 0.3651 0.4955 0.2199 0.2393 0.2379 0. 0.1782 0.1854 0.3419 SonicMasterSmall (2/6) 0.0100 0. 0.0020 0.0064 0.0060 0.0477 0.0590 0. 0.0122 0.0408 SonicMasterMedium (4/12) SonicMasterMedium (6/6) SonicMasterLarge (6/18) -w 5sec Audio Cond. -w 15sec Audio Cond. -w/o Audio Cond. (basic) -w Cond. During Infer -w/o Text Cond. -w Euler 1 Step -w Euler 100 Steps -w Runge-Kutta 10 Steps 0.0105 0.0225 0.0114 0.0111 0.0117 0.0099 0.0115 0.0130 0.0100 0.0136 0. 0.0698 0.2766 0.0834 0.0716 0.0750 0.0658 0.0840 0.1432 0.1146 0.1540 0.0810 0.0021 0.0020 0.0019 0.0021 0.0020 0.0021 0.0019 0.0032 0.0019 0.0033 0.0019 0.0067 0.0067 0.0059 0.0061 0.0064 0.0064 0.0060 0.0101 0.0059 0.0100 0. 0.0061 0.0056 0.0058 0.0058 0.0063 0.0056 0.0058 0.0086 0.0061 0.0091 0.0058 0.0400 0.1718 0.0388 0.0386 0.0320 0.0352 0.0389 0.0448 0.0425 0.0540 0.0402 0.0592 0.1737 0.0617 0.0605 0.0552 0.0595 0.0610 0.0841 0.0668 0.0915 0. 0.0602 0.2417 0.0576 0.0628 0.0525 0.0746 0.0572 0.0668 0.0498 0.0749 0.0590 0.0091 0.0462 0.0088 0.0124 0.0079 0.0097 0.0088 0.0154 0.0141 0.0162 0.0083 0.0383 0.0762 0.0358 0.0387 0.0398 0.0434 0.0355 0.0424 0.0384 0.0444 0. Fullsong degraded input Degraded Input SonicMasterLarge (6/18) 0.0290 0.0102 0.3231 0.0639 0.0048 0.0021 0.0124 0. 0.0983 0.0065 0.4606 0.0329 0.4810 0.0510 0.2274 0.0517 0.2403 0.0070 0.1737 0. Table 1: Objective evaluation of EQ attribute improvement with average absolute error values the lower, the better. estimates are unreliable on dense mixes, reverb is assessed via the Euclidean distance of modulation spectra. Clipping uses spectral flatness; volume, the global RMS; and stereo width, the RMS ratio of the mid and side signals, RMS(cid:2) LR (cid:3) /RMS(cid:2) L+R (cid:3). 2 Subjective Evaluation We presented listeners with 43 audio sample pairs degraded inputs and SonicMaster outputs to rate, consisting of 2 pairs for each degradation function (2 19 = 38 single degraded samples), 3 pairs of double and 2 pairs of triple degraded samples. Using 7-point Likert Scale, listeners were to rate: 1) The extent of improvement from the input to SonicMaster output represented by the text prompt (Text relevance), 2) audio quality of input (Quality1), 3) audio quality of the inferred SonicMaster sample (Quality2), 4) consistency and fluency of the inferred sample (Consistency), and 5) preference between the two samples, where 1 represents full preference of the ground truth degraded input, and 7 represents the SonicMaster inferred sample (Preference). The study was attended by 8 listeners (5 music experts and 3 Music Information Retrieval researchers). Results The results of the attribute controllability evaluation are depicted in Tables 1 and 2. SonicMaster significantly outperforms the baselines of Text2FX in EQ, and WPE and HPSS in Reverb. SonicMaster shows clear improvement in all categories when compared to the degraded and reconstructed inputs. Furthermore, we can observe that the reconstructed input metrics are overall slightly worse (with exceptions) than those of the ground truth degraded inputs. Across the audio-quality metrics in Table 4, SonicMaster outperforms the degraded inputs in both PQ and KL. Its FAD is only marginally higher than that of the degraded audio, yet markedly lower than the reconstructed baseline. Furthermore, SonicMaster achieves significant increase in PQ, almost reaching the level of ground truth mastered reference. In SSIM, SonicMaster exhibits lower scores than the degraded inputs but achieves superior performance compared to the reconstruction baseline. From Tables 1, 2, 4, we can observe that SonicMasterSmall performs comparably with SonicMasterLarge in all metrics, but slightly worse in Reverb, Clip, and Stereo. SonicMasterM edium (4/12) performs slightly better than the SonicMasterSmall, but still lacks behind SonicMasterLarge in Clip. SonicMasterM edium (6/6) performs the worst out of all variants across all metrics. As part of our ablation study, we compared SonicMasterLarge with different settings. Models with different lengths of audio conditioning (5 sec, 15 sec) perform similarly to the 10 sec setting. We chose to use 10 sec setting as our default as compromise between computation speed and length of overlap in neighbouring segments. Further, conditioning during inference, or no conditioning during training and inference at all produced comparable results. While the no condition variant provides best performance in Boom with absolute average error of 0.0658, it also provides the worst performance in Clip error with 2.055 compared to the aforementioned variants. Furthermore, we evaluated inference without text prompts, which yields comparable performance in FAD, SSIM, and PQ, but shows worse performance in KL (0.917 vs 0.696 on single degradation snippets). Performance in Clip and Stereo is also worse with values of 2.812 and 0.1416 as opposed to 1.506 and 0.1058. In EQ, values across the error values across the board are Model (MMDiT/DiT)"
        },
        {
            "title": "Degraded input",
            "content": "0.4457 0.4243 0.5045 0.4639 0.0496 0. 5.122 0.1813 0."
        },
        {
            "title": "Reconstructed input",
            "content": "0.4686 0.4507 0.5433 0.4908 0.0494 0. 3.871 0.1810 0.4181 HPSS 6 dB HPSS 12 dB WPE (Nakatani et al. 2010) 0.4419 0.4971 0.4849 0.4240 0.4739 0. 0.4970 0.5333 0.5207 0.4537 0.4814 0.4854 SonicMasterSmall (2/6) 0.3812 0.3826 0. 0.3277 SonicMasterMedium (4/12) SonicMasterMedium (6/6) SonicMasterLarge (6/18) -w 5sec Audio Cond. -w 15sec Audio Cond. -w/o Audio Cond. During Training -w Cond. During inference -w/o Text Cond. -w Euler 1 step -w Euler 100 Steps -w Runge-Kutta 10 Steps 0.3683 0.3952 0.3663 0.3717 0.3676 0.3620 0.3664 0.3732 0.4215 0.3716 0.3647 0.3700 0. 0.3726 0.3658 0.3682 0.3682 0.3724 0.3805 0.4378 0.3754 0.3684 0.3934 0.4422 0.3935 0.3919 0.3901 0.3888 0.3934 0.4012 0.4599 0.3997 0.3921 0.3138 0.4255 0.3109 0.3079 0.3093 0.3067 0.3112 0.3264 0.3459 0.3255 0.3087 - - - 0.0172 0.0147 0.0366 0.0193 0.0164 0.0172 0.0146 0.0172 0.0157 0.0124 0.0158 0.0210 - - - - - - - - - - - - 0.0859 2.363 0.0457 0.1536 0.0891 0. 0.0871 0.0893 0.0895 0.0850 0.0870 0.0730 0.0906 0.0672 0.0858 2.455 2.905 1.506 1.779 1.633 2.055 1.455 2.812 2.171 2.753 1.422 0.0409 0.1228 0.0468 0.0430 0.0485 0.0455 0.0412 0.0465 0.0461 0.0491 0.0481 0.1028 0. 0.1058 0.0918 0.1008 0.1015 0.1060 0.1416 0.1261 0.1497 0."
        },
        {
            "title": "Full song degraded input",
            "content": "Degraded input SonicMasterLarge (6/18) 0.3667 0.3954 0.3654 0.4511 0.4706 0.4191 0.3852 0.4066 0.0598 0. 0.1103 0.1101 6.363 3.734 0.1829 0.0424 0.4133 0.0850 Table 2: Objective evaluation: Reverb, Dynamics, Amplitude, and Stereo improvement. Clip values are multiplied by 1000."
        },
        {
            "title": "Text relevance",
            "content": "Quality1 Quality"
        },
        {
            "title": "Preference",
            "content": "5.11 0.55 5.59 0.57 4.56 0.80 6.19 0.66 5.50 1.05 4.55 0.44 4.25 0.68 4.09 0.54 3.66 0.43 4.62 0.83 5.01 0.52 5.28 0.57 5.00 0.81 5.38 0.77 5.50 0.63 5.05 0.42 5.31 0.54 5.00 0.84 5.59 0.81 5.38 0.70 4.84 0.49 5.27 0.75 4.91 1.01 5.75 0.84 5.25 0."
        },
        {
            "title": "Mixed degradations",
            "content": "5.28 0.51 3.85 0.88 4.60 0.59 4.65 0.62 4.93 0.57 Table 3: Listening study results - Mean Opinion Score with 95% confidence interval. slightly higher for this variant, as the model is not targetting specific EQ enhancement due to the missing text prompt. We evaluated Euler solvers with 1, 10 (baseline), and 100 steps, plus 10-step 4th order RungeKutta (Dormand and Prince 1980) solver. Euler-1 matches the baseline overall but is weaker on Boom, Microphone, Clip, all Reverb subtasks, and shows higher KL. Euler-100 boosts Reverb and Punch yet lowers every EQ score versus the 1-/10-step runs. RungeKutta-10 equals Euler-10 on most metrics and tops Clip, but its inference is significantly slower. When evaluating SonicMaster with full songs, we can again observe significant improvements in EQ metrics  (Table 1)  , and in most functions in Table 2. However, the reverb metric of MSD shows mixed results. In audio quality metrics, SonicMaster shows decrease in SSIM and FAD compared to degraded inputs, except for FAD in double and triple degradation samples, which points to SonicMasters ability to handle multiple degradations at once. Subjective evaluation results are depicted in Table 3. In text relevance, we can observe very good ratings in Amplitude (6.19) and Reverb categories (5.59), which highlight significant perceptive improvement in declipping, volume increase, and in dereverberation. These two categories further show the highest improvement in quality, highest consistency ratings, and highest preference. EQ shows the third best text relevance, but worst preference ratings. Overall, SonicMaster samples are rated higher in quality compared to inputs and preferred across the board. paired t-test on Quality1 and Quality2 ratings shows statistically significant differences in Reverb, Amplitude, Stereo, and mixed degradations. These results further confirm SonicMasters capabilities in text-controllable music restoration and mastering. Discussion The experiments confirm that SonicMaster improves audio quality, proving that generative strategy for music restoraModel Single deg. Double+triple deg. All FAD KL SSIM PQ FAD KL SSIM PQ FAD KL SSIM PQ Ground truth mastered reference Degraded input Reconstructed input SonicMasterSmall (2/6) SonicMasterMedium (4/12) SonicMasterMedium (6/6) SonicMasterLarge (6/18) -w 5sec Audio Cond. -w 15sec Audio Cond. -w/o Audio Cond. During Training -w Cond. During Inference -w/o Text Cond. -w Euler 1 step -w Euler 100 Steps -w Runge-Kutta 10 Steps Ground truth mastered reference Degraded input Reconstructed input SonicMasterLarge (6/18) - 0.061 0.139 0.071 0.070 0.086 0.069 0.070 0.069 0.069 0.069 0.069 0.076 0.069 0. - 0.087 0.165 0.095 - 3.859 3.990 0.726 0.709 1.893 0.696 0.703 0.694 0.691 0.693 0.917 0.922 0.920 0.701 - 2.937 3.049 0.754 Snippet degraded input - 0.838 0.574 0.623 0.624 0.603 0.624 0.624 0.623 0.625 0.625 0.621 0.615 0.620 0.624 7.886 7.321 7.172 7.716 7.740 7.571 7.743 7.733 7.742 7.741 7.742 7.772 7.684 7.764 7. - 0.184 0.290 0.088 0.084 0.154 0.082 0.083 0.083 0.082 0.082 0.088 0.117 0.087 0.084 - 6.827 6.984 1.215 1.187 3.241 1.145 1.175 1.161 1.146 1.141 1.484 1.789 1.521 1. Full song degraded input - 0.834 0.584 0.380 7.885 7.325 7.204 7.627 - 0.223 0.335 0.121 - 5.679 5.644 1.251 - 0.696 0. 0.586 0.589 0.555 0.589 0.588 0.588 0.590 0.589 0.586 0.567 0.585 0.588 - 0.682 0.510 0.368 7.886 6.632 6.501 7.609 7.649 7.231 7.654 7.637 7.650 7.645 7.653 7.643 7.520 7.621 7. 7.885 6.606 6.509 7.477 - 0.106 0.196 0.077 0.075 0.110 0.073 0.075 0.073 0.073 0.073 0.074 0.090 0.076 0.074 - 0.142 0.234 0.101 - 5.131 5. 0.935 0.914 2.470 0.888 0.905 0.894 0.886 0.885 1.160 1.294 1.178 0.902 - 4.308 4.339 1.002 - 0.777 0.546 0.607 0.609 0.583 0.609 0.609 0.608 0.610 0.609 0.606 0.594 0.605 0. - 0.758 0.547 0.374 7.886 7.026 6.885 7.670 7.701 7.426 7.705 7.692 7.702 7.700 7.704 7.716 7.614 7.703 7.698 7.885 6.965 6.859 7.552 Table 4: Objective evaluation with FAD, KL, SSIM, and PQ metrics. For readability, KL values were multiplied by 1000. (a) Ground truth audio. (b) Degraded by reverb (smearing). (c) After SonicMaster: reverb removed. Figure 4: Comparison of spectrograms: (a) ground truth, (b) degraded with reverb, and (c) the output of SonicMaster where smearing is removed. Prompt: Please, reduce the strong echo in this song. tion and mastering is feasible when backed by large corpus and suitable training objective. key limitation, however, are occasional artefacts introduced by the lossy latent representation: reconstructed clips can exhibit robotic vocals or muted instruments, especially for certain genres, and the effect is stronger for degraded inputs than for their clean references. Future research should pursue less lossy latent encoders or alternative representations to minimize these distortions. The observed decrease in SonicMasters performance on full songs in SSIM and Reverb metrics could be related to the way neighbouring segments are connected together. Improving on this aspect could increase the objective performance further. Moreover, evaluating the amount of reverberation in music is challenging, as the audio is usually very dense. Additionally, the way generative model such as SonicMaster removes reverb in the latent space is not telegraphed, which makes it more difficult to choose correct metric. deep exploration of this topic would be of use to the community."
        },
        {
            "title": "Conclusion",
            "content": "We introduced SonicMaster, the first unified text-guided generative model for music restoration and mastering, capable of addressing wide range of audio degradations within single model. Key contributions include the creation of new SonicMaster dataset2 of paired degraded and highquality music with textual annotations and the use of flowmatching training paradigm to directly learn the restoration mapping. We also developed text-conditioned multimodal generative architecture that unifies diverse enhancement tasks, enabling the model to perform all restoration and mastering steps in one generative pass. Comprehensive evaluations demonstrate that SonicMaster substantially improves audio quality, achieving strong results in objective metrics (e.g., FAD, KL divergence, SSIM, PQ) and consistently earning higher listener preferences in blind tests compared to both original and baseline outputs. 2Dataset, code, samples, model, and demo available through https://amaai-lab.github.io/SonicMaster/ References Chu, A.; OReilly, P.; Barnett, J.; and Pardo, B. 2025. Text2fx: Harnessing clap embeddings for text-guided audio effects. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, Y.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70): 153. Deruty, E.; and Tardieu, D. 2014. About dynamic processing in mainstream music. Journal of the Audio Engineering Society, 62(1/2): 4255. Dormand, J. R.; and Prince, P. J. 1980. family of embedded Runge-Kutta formulae. Journal of computational and applied mathematics, 6(1): 1926. Elizalde, B.; Deshmukh, S.; Al Ismail, M.; and Wang, H. 2023. Clap learning audio concepts from natural language In ICASSP 2023-2023 IEEE International supervision. Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Evans, Z.; Carr, C.; Taylor, J.; Hawley, S. H.; and Pons, J. In 2024. Fast timing-conditioned latent audio diffusion. Forty-first International Conference on Machine Learning. Fei, Z.; Fan, M.; Yu, C.; and Huang, J. 2024. Flux that plays music. arXiv:2409.00587. Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guidance. arXiv:2207.12598. Hou, S.; Liu, S.; Yuan, R.; Xue, W.; Shan, Y.; Zhao, M.; and Zhang, C. 2025. Editing Music with Melody and Text: Using In ICASSP 2025 - ControlNet for Diffusion Transformer. 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. Howard, D. M.; and Angus, J. A. S. n.d. Open Acoustic Impulse Response (Open AIR) Library. https://www.openair. hosted.york.ac.uk/. Accessed: 2025-07-06. Hung, C.-Y.; Majumder, N.; Kong, Z.; Mehrish, A.; Bagherzadeh, A. A.; Li, C.; Valle, R.; Catanzaro, B.; and Poria, S. 2024. Tangoflux: Super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization. arXiv:2412.21037. Imort, J.; Fabbro, G.; RamÄ±rez, M. A. M.; Uhlich, S.; Distortion auKoyama, Y.; and Mitsufuji, Y. 2022. dio effects: Learning how to recover the clean signal. arXiv:2202.01664. Jiang, X.; Han, C.; Li, Y. A.; and Mesgarani, N. 2025. Listen, Chat, and Remix: Text-Guided Soundscape Remixing IEEE Journal of Sefor Enhanced Auditory Experience. lected Topics in Signal Processing. Li, X.; Wang, Q.; and Liu, X. 2024. Masksr: Masked language model for full-band speech restoration. arXiv:2406.02092. Liu, H.; Chen, Z.; Yuan, Y.; Mei, X.; Liu, X.; Mandic, D.; Wang, W.; and Plumbley, M. D. 2023. Audioldm: Text-to-audio generation with latent diffusion models. arXiv:2301.12503. Liu, H.; Kong, Q.; Tian, Q.; Zhao, Y.; Wang, D.; Huang, C.; and Wang, Y. 2021. VoiceFixer: Toward general speech restoration with neural vocoder. arXiv:2109.13731. Liu, X.; Gong, C.; and Liu, Q. 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv:2209.03003. Mockenhaupt, F.; Rieber, J. S.; and Nercessian, S. 2024. Automatic Equalization for Individual Instrument Tracks Using Convolutional Neural Networks. arXiv:2407.16691. Moliner, E.; and Valimaki, V. 2023. Diffusion-based audio inpainting. arXiv:2305.15266. Mourgela, A.; Quinton, E.; Bissas, S.; Reiss, J. D.; and Ronan, D. 2024. Exploring trends in audio mixes and masters: Insights from dataset analysis. arXiv:2412.03373. Nakatani, T.; Yoshioka, T.; Kinoshita, K.; Miyoshi, M.; and Juang, B.-H. 2010. Speech dereverberation based on variance-normalized delayed linear prediction. IEEE Transactions on Audio, Speech, and Language Processing, 18(7): 17171731. Peebles, W.; and Xie, S. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 41954205. Salvi, D.; Leonzio, D. U.; Giganti, A.; Eutizi, C.; Mandelli, S.; Bestagini, P.; and Tubaro, S. 2025. Poliphone: dataset for smartphone model identification from audio recordings. IEEE Access. Scheibler, R.; Bezzam, E.; and Dokmanic, I. 2018. Pyroomacoustics: python package for audio room simulation and array processing algorithms. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), 351355. IEEE. Tjandra, A.; Wu, Y.-C.; Guo, B.; Hoffman, J.; Ellis, B.; Vyas, A.; Shi, B.; Chen, S.; Le, M.; Zacharov, N.; et al. 2025. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv:2502.05139. Wang, Y.; Ju, Z.; Tan, X.; He, L.; Wu, Z.; Bian, J.; et al. 2023. Audit: Audio editing by following instructions with latent diffusion models. Advances in Neural Information Processing Systems, 36: 7134071357. Wilson, A.; and Fazenda, B. M. 2016. Perception of audio quality in productions of popular music. Journal of the Audio Engineering Society, 64(1/2): 2334. Zang, Y.; Dai, Z.; Plumbley, M. D.; and Kong, Q. 2025. Music Source Restoration. arXiv:2505.21827. ZaviË‡ska, P.; Rajmic, P.; Ozerov, A.; and Rencker, L. 2020. survey and an extensive evaluation of popular audio declipIEEE Journal of Selected Topics in Signal ping methods. Processing, 15(1): 524."
        },
        {
            "title": "Appendix",
            "content": "Genre tags We grouped genre tags into genre groups, as depicted in Table 5. Each row links coarse Group labelsuch as Rock, Electronic, or Jazz/Bluesto the fine-grained Genre tags that appear in the metadata. These tags enumerate substyles (e.g., progressiverock, deephouse, acidjazz), which allows us to aggregate diverse representations inside each of the genre grops. Table 5: Genre groupings by metadata tags used in our dataset."
        },
        {
            "title": "Genre tags",
            "content": "Rock Pop Electronic Hip-Hop Folk Metal World Jazz/Blues Chill Classical rock, alternativerock, poprock, classicrock, hardrock, progressiverock, stoner, psychedelicrock, garage, indierock pop, electropop, dancepop, dance, alternativepop, adultcontemporary, indiepop electronic, house, techno, trance, edm, electrohouse, deephouse, progressivehouse, electroswing, synthwave, electronica rap, hiphop, trap, alternativehiphop, gangstarap folk, singersongwriter, americana, country, bluegrass, folklore metal, deathmetal, blackmetal, thrashmetal, heavymetal, numetal, metalcore, hardcore, alternativemetal, doommetal world, latin, reggaeton, afrobeat, african, indian, oriental, celtic, salsa, flamenco, jpop, middleeastern, asian, reggae jazz, blues, funk, acidjazz, jazzfusion, smoothjazz, jazzfunk, soul, swing, rnb, alternativernb ambient, downtempo, chillout, chillhop, lofi, newage, darkambient, triphop, chillwave, idm, dreampop classical, filmscore, neoclassical, symphonic, opera, baroque, medieval, avantgarde, production, choral Degradation functions To create the SonicMaster dataset, we used set of 19 degradation functions. The details of their implementation and parameter range are described in Table 6. Each of the groups, and subsequently each of the functions inside the groups, have their own probabilities/weights to be picked in our data creation pipeline. These are documented in Table 7. (a) Original (clean). (b) Degraded input. (c) Restored output. Figure 5: Original vs. degraded (via convolution with phone microphone transfer function) and SonicMaster-restored spectrograms; restoration suppresses the microphones coloration. Spectrogram examples We visualize timefrequency structure in spectrograms to provide qualitative evidence of restoration behavior. Each figure shows the clean reference, the degraded input (e.g., reverberation-induced smearing or clipping distortion), and the output of SonicMaster. Figures 5, 6, 7, 8, and 9 compare clean, degraded, and restored spectrograms across selected scenarios (reverb, clipping, microphone transfer function, and clarity EQ). Prompts for each degradation type Prompt instructions for each degradation type are grouped by audio attribute in Table 8; for example, entries for Xband, microphone coloration, clarity, brightness, darkness, airiness, boominess, warmth, muddiness, vocals, compression, punch, reverb, volume, clipping, and stereo give natural-language commands that steer the restoration model. These instructions act as conditioning signalse.g., remove excess reverb and make it sound cleaner, raise the level of the vocals, or make this sound brighterso that the generative restoration trajectory emphasizes or suppresses specific signal characteristics. Table 6: Detailed description of degradation functions used to create our dataset."
        },
        {
            "title": "Description",
            "content": "Prompt example (inverse) EQ X-band EQ"
        },
        {
            "title": "Amplitude",
            "content": "Apply 8 to 12 band parametric EQ with 6 to +6 range for each band. Convolve the audio with one of 20 phone microphone transfer functions. Reduce brightness using high-shelf filter at 6 kHz by 615 dB. Increase perceived brightness with high-shelf filter at 6 kHz by 615 dB. Reduce airiness via high-shelf filter at 10 kHz by 1020 dB. Reduce boominess with low-shelf filter at 120 Hz by 1020 dB. Degrade clarity using Butterworth low-pass filter (order 35) with cutoff at 2 kHz. Increase muddiness with 2nd-order Chebyshev Type II bandpass (200500 Hz) by 615 dB. Reduce warmth with low-shelf filter at 400 Hz by 620 dB. Attenuate vocal-range frequencies using 2nd-order Chebyshev Type II bandpass (3503500 Hz) by 620 dB. Apply feedforward compressor with attack 380 ms, release 80250 ms, threshold 45 to 38 dB, ratio 645, and make-up gain 1625 dB. Apply feedforward transient shaper with attack 3 ms, release 150 ms, adaptive threshold, and reduction of 815 dB. Convolve with Pyroomacoustics simulated IR: room size (715, 818, 414) m, absorption coefficient 0.050.30. Convolve with Pyroomacoustics IR: room size (48, 47, 2.53.5) m, 12 absorptive walls, frequency-dependent absorption. Convolve with Pyroomacoustics IR: room size (37, 39, 2.54) m, absorption coefficient 0.050.30. Apply one of twelve real impulse responses from the openAIR library. Modify the audio level to maximum amplitude of {2,3,5} and apply clipping. Adjust the audio gain to maximum amplitude of {0.001, 0.003, 0.01, 0.05}. Correct the unnatural frequency emphasis. Reduce the coloration added by the microphone. Give the mix more shine and sparkle. Make the tone fuller and less sharp. Add more air and openness to the sound. Give the audio more roar and low-end power. Increase the clarity of this song by emphasizing treble frequencies. Make the mix sound less boxy and congested. Make the sound warmer and more inviting. Make the vocals stand out more. Let the audio breathe more and improve the dynamics. Add more impact and dynamic punch to the sound. Clean this off any echoes! Can you remove the excess reverb in this audio, please? Remove excess reverb and make it sound cleaner. Please, reduce the strong echo in this song. Reduce the clipping and reconstruct the lost audio, please. Enhance the loudness without distorting the signal."
        },
        {
            "title": "Stereo",
            "content": "Combine the left and right channels to erase the spatial image. Add depth and separation between left and right. Group (weight)"
        },
        {
            "title": "Option",
            "content": "Probability / Weight xband mic bright dark airy boom clarity mud warm vocal comp punch small big mix real EQ (0.4) Dynamics (0.125) Reverb (0.225) Amplitude (0.125) clip volume Stereo (0.125) stereo 7.0 5.0 3.0 3.0 2.0 2.0 3.0 3.0 3.0 4. 2.5 1.0 0.15 0.15 0.30 0.40 3.0 1.0 1.0 Table 7: Degradation groups with assigned probabilities and option weights. (a) Original (clean). (b) Degraded input. Figure 6: Effect of reverberation (example from the main text in larger size): top panel shows the original audio sample, middle panel shows audio convolved with Pyroomacoustics simulated impulse response, and bottom panel shows the dereverberated result with echoes cleaned. (c) Restored output. (a) Original (clean). (b) Degraded input. Figure 7: Effect of clarity degradation and restoration on spectrograms. The treble frequencies are supressed in the degraded input sample, and then restored with SonicMaster. Prompt: Make the audio clearer and more intelligible. (c) Restored output. (a) Original (clean). (b) Degraded input. Figure 8: Effect of clipping degradation and related restoration. Drum hits clip in the degraded audio, but are restored in the SonicMasters output without distortion. Prompt: Clean up the harshness in the signal. (c) Restored output. (a) Original (clean). (b) Degraded input. Figure 9: Another example of the effect of clipping and its restoration. The degraded input shows signs of distortion with visible spectral content above 15 kHz. This distortion is suppressed by SonicMaster. Prompt: Clean up the noisiness in the audio. (c) Restored output."
        },
        {
            "title": "Stereo",
            "content": "Table 8: User instructions grouped by audio attribute."
        },
        {
            "title": "Example Instructions",
            "content": "Can you please correct the equalization?; Improve the balance in the audio by fixing the chaotic equalizer, please.; Make this sound balanced, please.; Balance the EQ, please.; Balance the tonal spectrum of the audio.; Correct the unnatural frequency emphasis.; Make the EQ curve smoother and more natural.; Even out the EQ.; Adjust the tonal balance for more pleasing sound. This audio was recorded with phone, can you fix that, please?; Please make this sound better than phone recording.; Balance the EQ, please.; Improve the balance in this song.; Make the audio sound like it was recorded with higherquality microphone.; Reduce the coloration added by the microphone.; Make the tone more neutral and balanced.; Improve the naturalness of the recording.; Remove the harshness or boxiness from the mic coloration. Increase the clarity!; Can you please make this song sound more clear?; Increase the clarity of this song by emphasizing treble frequencies.; Make the audio clearer and more intelligible.; Sharpen the overall sound.; Bring more focus and definition to the details.; Make the mix sound less cloudy.; Tighten the articulation in the sound. Can you please make this sound brighter?; Increase the brightness!; Make this audio sound brighter by emphasizing the high frequencies.; Add some brightness to the high end.; Make the sound more vivid and lively.; Give the mix more shine and sparkle.; Lift the treble for more open tone.; Enhance the presence of the upper frequencies. Make this sound darker!; Can you reduce the brightness, please?; Make the audio darker by suppressing the higher frequencies.; Bring in more low-mid richness to make the sound darker.; Make the tone fuller and less sharp.; Smooth out the highs with deeper low-end support.; Round out the sound with more body.; Soften the harshness with warmer tone. Make this sound more fresh and airy by emphasizing the high end frequencies.; Make this feel more airy, please.; Increase the perceived airiness, please.; Give this light sense of spaciousness by amplifying the higher frequencies.; Add more air and openness to the sound.; Make the audio feel more spacious and extended.; Enhance the sense of space in the highs.; Lift the top end for more open character.; Give the mix breathier, more open feel. Make it boom!; Make this song sound more boomy by amplifying the low end bass frequencies.; Increase the boominess, please!; Give me more bass!; Can you make this more bassy, please?; Give the audio more roar and low-end power.; Make the bass more impactful and solid.; Add weight and depth to the bottom end.; Reinforce the low frequencies for more energy.; Boost the bass presence. Can you make this song sound warmer, please?; Increase the warmth, please.; Emphasize the bass and low-mid frequencies to give this more warm feel.; Make the sound warmer and more inviting.; Add some low-mid warmth to the mix.; Soften the tone with bit more body.; Give the audio warm analog feel.; Enhance the warmth for fuller sound. Can you make this song sound less muddy, please?; Decrease the muddiness!; Reduce the level of muddiness in this audio by lowering the low-mid frequencies.; Clean up the muddiness in the low-mids.; Make the mix sound less boxy and congested.; Improve definition by reducing mud.; Clear up the low-mid buildup.; Make the audio tighter and less murky. Raise the level of the vocals, please.; Can you amplify the vocals, please?; Emphasize the vocals by raising the level of the mid frequencies specific for vocals.; Bring the vocals forward in the mix.; Make the voice clearer and more present.; Increase the vocal presence by enhancing the midrange.; Make the vocals stand out more.; Strengthen the vocal clarity and focus. Increase the dynamic range.; Decompress the audio, please.; Remove the compression, please.; Can you fix the strong compression effect in this audio by expanding the dynamic range?; Restore the dynamics of the audio.; Make the sound less squashed and more open.; Reduce the over-compression for more natural feel.; Bring back the contrast in volume.; Let the audio breathe more and improve the dynamics. Give this song punch!; Make the transients sharper, please.; Increase the punchiness of the song by emphasizing the transients.; Make the audio more punchy and energetic.; Bring back the snap and attack of transients.; Add more impact and dynamic punch to the sound.; Make drums and hits sound more aggressive and tight.; Increase the percussive clarity and definition. Can you remove the excess reverb in this audio, please?; Please, dereverb this audio.; Remove the echo!; Please, reduce the strong echo in this song.; Remove the church effect, please.; Clean this off any echoes!; This song has too much reverb present, can you reduce it?; Make the audio sound more dry and direct.; Reduce the roominess or echo.; Remove excess reverb and make it sound cleaner.; Bring the sound closer and more focused.; Tighten the spatial feel of the audio. The volume is low, make this louder please!; Can you make this sound louder, please?; Increase the amplitude.; Normalize the audio volume.; Make the audio louder and more powerful.; Increase the overall level.; Boost the volume without distorting the signal. This audio is clipping, can you please remove it?; Remove the loud hissing in this song?; Remove the clipping.; Reduce the clipping and reconstruct lost audio.; Clean up noisiness.; Make the audio smoother and less distorted.; Reduce the gritty or crushed character.; Fix digital distortion. Make it sound spacious!; Can you make this audio stereo, please?; Alter left/right channels to give spatial feel.; Widen the stereo image.; Add depth and separation between left and right.; Enhance the stereo field for immersive sound."
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design"
    ]
}