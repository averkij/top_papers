{
    "paper_title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
    "authors": [
        "Hangyu Li",
        "Bofeng Cao",
        "Zhaohui Liang",
        "Wuzhen Li",
        "Juyoung Oh",
        "Yuxuan Chen",
        "Shixiao Liang",
        "Hang Zhou",
        "Chengyuan Ma",
        "Jiaxi Liu",
        "Zheng Li",
        "Peng Zhang",
        "KeKe Long",
        "Maolin Liu",
        "Jackson Jiang",
        "Chunlei Yu",
        "Shengxiang Liu",
        "Hongkai Yu",
        "Xiaopeng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 8 6 1 1 1 . 1 1 5 2 : r CATS-V2V: Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios Hangyu Li1,, Bofeng Cao1,*, Zhaohui Liang1, Wuzhen Li1, Juyoung Oh1, Yuxuan Chen1, Shixiao Liang1, Hang Zhou1, Chengyuan Ma1, Jiaxi Liu1, Zheng Li1, Peng Zhang1, KeKe Long1, Maolin Liu2, Jackson Jiang2, Chunlei Yu2, Shengxiang Liu2, Hongkai Yu3, Xiaopeng Li1 1University of Wisconsin-Madison 2wuwen-ai 3Cleveland State University Figure 1. One frame of CATS-V2V dataset. The middle-upper shows the combined point cloud and 3D bounding box annotations, while the middle-lower presents the HD map and BEV annotations. On sides are the seven camera views of each vehicle with projected annotations."
        },
        {
            "title": "Abstract",
            "content": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the firstof-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet highprecision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct 4D BEV representation. On this basis, we propose target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks. 1. Introduction Cooperative perception has emerged as promising approach to overcoming the sensing limitations of autonomous vehicles in rare but safety-critical corner cases [12, 23]. Onboard sensors such as cameras and LiDARs are inherently constrained by occlusion, noise, and limited range, which can severely degrade perception performance in Complex Adverse Traffic Scenarios (CATS) [5, 19]. CATS encompass adverse weather and illumination conditions, irregular work zones, and visually challenging scenes that often cause sensor failures and decision-making instability [25, 37, 42]. These scenarios constitute the longtail distribution of real-world driving, where the robustness and generalization of autonomous driving systems are most critically tested [6, 7]. Figure 2. 10 CATS scenarios at 10 locations: (a) Clear day at an arterial road; (b) Rainy day at highway; (c) Snowy day at highway; (d) Dawn at an arterial road; (e) Dusk with direct sunlight at an arterial road; (f) Clear night at collector road; (g) Rainy night at ab arterial road; (h) Snwoy night at an arterial road; (i) Foggy day at local street; (j) Overcast day with work zone at an arterial road. To mitigate these issues, cooperative perception (CP) allows vehicles and infrastructure to share perceptual information, extending situational awareness beyond single vehicles view [2, 28, 32]. Depending on the source, it can be categorized into Vehicle-to-Vehicle (V2V), Vehicle-toInfrastructure (V2I), or hybrid V2X modes. Among them, V2V cooperative perception offers the greatest flexibility and scalability since it does not depend on fixed locations or costly infrastructure investment. Despite its potential, developing and validating CP under real-world CATS remains difficult due to the lack of suitable datasets. Existing datasets [24, 29, 34, 39, 40, 44] have established pioneering foundation, yet they mainly capture ordinary weather, lighting, and traffic conditions, where single-vehicle perception already works well. As result, current CP research under CATS largely depends on digital simulations [13, 18] or indoor mockups [25]. While recent V2I dataset [36] includes certain CATS conditions, such examples remain extremely rare and are limited to static infrastructure viewpoints. In contrast, V2V cooperation involves dynamic agents with diverse viewpoints and continuously changing spatial relations. While this flexibility is key for scalable CP, it also brings challenges in calibration, time synchronization, and temporal alignment, especially in CATS. To date, considering its stringent collecting conditions, no dataset captures realworld V2V perception under CATS. To fill this gap, we introduce CATS-V2V, the first realworld V2V CP dataset covering CATS. It supports tasks on perception and localization with strict time synchronization, spatial-temporal alignment, and cross-view consistency. Data were collected across ten weather and lighting conditions and ten diverse locations (see Fig. 2) using two experimental vehicles equipped with synchronized sensors, including 10 Hz LiDAR, seven multi-view 30 Hz cameras, and one 125 Hz inertial navigation system (INS). The dataset correspondingly provides time-consistent 3D bounding box annotations with global object IDs and static HD maps for constructing 4D BEV scenes (see Fig. 1). Furthermore, we propose target-based temporal alignment method alongside with the dataset, which ensures all objects are precisely aligned across modalities. Our key contributions are summarized as follows: The first real-world V2V cooperative perception dataset covering diverse weather (sunny, cloudy, rainy, snowy) and lighting (clear, direct sunlight, low light, night) conditions, including dynamic work zones. Broad task support with multiple sensor data (LiDAR, camera, INS) and rich annotations (3D boxes, HD maps, traffic information) including object detection, object tracking, trajectory prediction, localization and mapping. Target-based method enabling alignment across hardware time synchronized high-frame-rate multi-view cameras, LiDAR, and INS, achieving highest data quality. We will release the dataset and code for target-based temporal alignment method to foster further research. 2. Related work variety of V2X datasets have been publicly released, yet they vary greatly in realism and coverage of CATS. While simulation-based datasets can easily model adverse conditions, real-world datasets remain dominated by normal weather and lighting. Moreover, most existing real-world V2X datasets still fall short of the quality in multi-modal richness, synchronization, and alignment achieved by wellknown single-vehicle perception datasets such as KITTI [9], Waymo [26], and nuScenes [1]. Therefore, in this section, we provide comprehensive review and comparison of existing simulation-based V2X datasets with CATS and realworld V2X datasets, including both V2V and V2I settings, as summarized in Tab. 1. 2 Table 1. Comparison of CATS-V2V to some existing V2X cooperative perception datasets."
        },
        {
            "title": "Dataset",
            "content": "S/R V2X CATS"
        },
        {
            "title": "Align",
            "content": "BEV ClipTime LiDAR Cam MVFPS V2XSet-w[18] OPV2V-w[18] DeepAccident[27] SCOPE[8] Adver-City[14] DAIR-V2X[39] V2X-Seq[40] TUMTraf-V2X[44] HoloVIC[24] V2X-Radar[36] V2V4Real[34] V2X-Real[29] CATS-V2V(ours)"
        },
        {
            "title": "Real",
            "content": "V2I V2V Both Both Both V2I V2I V2I V2I V2I V2V Both V2V - - - - -"
        },
        {
            "title": "Soft\nSoft\nSoft\nSoft\nSoft",
            "content": "30 ms 30 ms 25 ms 25 msd 20 ms 50 ms 50 msf 1 ms - - - - - Stamp - Stamp Stamp - Stamp Stamp"
        },
        {
            "title": "Target",
            "content": "- - - - - - - 5525s 7313s 6918s 4440s 11022s 10020s 9516s 810s ?e 4020s 6715s 6812s 10030s 2.8K 2.7K 285K 500K 120K 39K 30K 2K ?e 16K 20K 33K 60K - - 1.71M 800K 480K 39K 30K 5K ?e 32K 40K 171K 1.26M - - 610 510 410 120c 110 115c 2*25c 110 210 4 730 S/R = Sim/Real; V2X = V2I(nfrastructure), V2V, or both; BEV = BEV annotations; MVFPS = Multi-View cameras on each vehicleFrame-Per-Second. It provides BEV cameras images but not structural representations. It provides limited rainy scenarios as CATS. The vehicle camera runs on 20 Hz / 15 Hz / 25 Hz, but only 10 Hz data is sampled for publication. Only roadside units are claimed to be synchronized under 5 ms. Considering the characteristics of NTP that it employs, delays >25 ms can be expected. The scale is unknown, as the figures in their paper are inconsistent, and the dataset is not publicly available. The LiDARs triggered synchronously with GPS, but not the cameras. Considering their 10 Hz capture frequency, 50 ms error can be expected. 2.1. Simulated V2X datasets with CATS 2.2. Real-world V2I datasets Simulation-based V2X datasets are typically built on highfidelity simulators such as CARLA [4] and OpenCDA [31] to capture virtual sensor data. OPV2V-w and V2XSet-w [18] extend the original OPV2V [33] and V2XSet [32] datasets to simulate CATS, incorporating virtual weather and lighting variations. Several simulation frameworks [10, 11, 15] were leveraged to emulate fog, rain, and snow effects on LiDAR sensors. SCOPE [8] is recently introduced large-scale simulation dataset involving 24 connected and autonomous vehicles (CAVs). It supports both realistic LiDAR and multiview camera models and further reduces the sim-to-real gap by using two Digital Twin maps reconstructed from realworld environments. DeepAccident [27] and Adver-City [14] focus on safetyreconstructing accidentcritical cooperative perception, prone scenarios from real-world crash data with CATS. Simulation datasets inherently provide accurate localization, perfect annotations, and precise time synchronization via software. Moreover, because sensor data are generated instantaneously, there is no need for high-frame-rate cameras and temporal alignment between modalities. However, they differ significantly from real-world conditions, not only in time, sensor information, and annotation errors, but particularly regarding the physical interaction of CATS with sensor hardware, which simulation cannot yet perfectly reproduce. DAIR-V2X [39] is one of the first real-world V2X datasets, establishing cooperation between roadside unit and vehicle. Subsequently, V2X-Seq [40] extended this setup to consecutive intersections with additional HD map and object tracking annotations. Although the data-collection vehicle in both datasets was equipped with 20 Hz camera, only compressed front-view images downsampled to 10 Hz based on the timestamps were released. Moreover, both datasets suffer from imperfect time synchronization, with errors exceeding 30 ms. TUMTraf-V2X [44] introduces multi-view intersection dataset consisting of four infrastructure cameras and single vehicle. The dataset is relatively small, with only eight 10-second clips. Similarly, only the downsampled images from the vehicles front-view camera were released. Although annotated for weather and time of day, the dataset only includes two conditions (clear daytime and nighttime) and thus lacks CATS coverage. Their earlier work, TUMTraf-A9 [3], captured snow conditions but solely from the infrastructure side, without vehicle cooperation. HoloVIC [24] is large-scale dataset covering four intersections equipped with multiple LiDARs and cameras. However, due to limited accessibility and statistical inconsistencies in the paper, information such as the number of clips, segment durations, and data scale remains unclear. It employs Network Time Protocol (NTP) for time synchronization, achieving reported timestamp accuracy of about 5 3 Figure 3. Sensor configurations of our two data-collecting vehicles. ms, but only for roadside units. Given the characteristics of NTP, errors exceeding 25 ms are expected between roadside units and the vehicle. Two cameras are also downsampled and aligned by selecting the closest timestamps to the LiDAR frames. V2X-Radar [36] is the most recent V2I dataset, introducing radar modality and certain coverage of CATS. However, as trade-off, it only provides 10 Hz images from one front-view camera, and no alignment could be performed It also lacks BEV and corresponding among modalities. tracking annotations. Although the vehicle platform is described as being hardware-synchronized, timestamp errors of approximately 20 ms were still reported. 2.3. Real-world V2V datasets V2V4Real [34] is the first and one of the very few realworld V2V cooperative perception datasets that leverage the perception of two vehicles for collaboration. Each vehicle is equipped with 32-beam LiDAR and two cameras facing forward and backward, but without hardware-level time synchronization. Images are then aligned to LiDAR frames using the nearest timestamps, which causes some delays. The dataset annotates only five classes of vehicles and does not include vulnerable road users such as pedestrians or cyclists, which may be due to both scene composition and the limited LiDAR resolution. V2X-Real [29] is currently the only real-world dataset supporting both V2I and V2V cooperation. It employs 128-beam LiDARs to produce dense point clouds, with LiDARs synchronized to GPS. However, four on-vehicle ZEDi cameras and two Axis infrastructure cameras lack hardware synchronization. As in other V2X datasets, images are aligned to LiDAR frames by closest timestamps. The dataset does not include CATS, or BEV and tracking annotations, and its camera configuration provides limited field-of-view overlap, despite nominal 360 coverage. 3. CATS-V2V dataset Despite these pioneering efforts, existing real-world V2V datasets still lack coverage of CATS. More broadly, current real-world V2X datasets remain limited in quality, with imperfect time synchronization, multi-modal alignment, and annotation richness. In response, we introduce our CATSV2V dataset to address these gaps. The section describes our vehicle configuration, data acquisition, preprocessing, and annotation procedures. More details could be found in the Supplementary Material. 3.1. Vehicle configurations We use two Lincoln MKZ sedans to collect data, as shown in Fig. 3, with sensors integrated through the roof rack. During the data collection process, they are driven by humans. 3.1.1. Sensors setup Each of our two vehicles is equipped with 128-beam mechanical spinning LiDAR, seven automotive-grade cameras, and one deeply-coupled Inertial Navigation System (INS). The detailed specifications are shown in Tab. 2. 4 Table 2. Detailed sensor specifications on vehicles"
        },
        {
            "title": "Camera",
            "content": "Black: 10 Hz RoboSense Ruby 128-beam LiDAR, dual return mode, 250m range, vertical FOV 25 15, angle resolution 0.2; Red: 10 Hz RoboSense Ruby Plus 128-beam LiDAR, dual return mode, 250m range, vertical FOV 25 15, angle resolution 0.2; Black: 3 (front and rear) 30 Hz OMNIVISION OX08B40, YUV422 8bit, 38402160, 140dB HDR, LFM; 4 (side) Sony ISX031, YUV422 8bit, 19201536, 120dB HDR, LFM; Red: 3 (front and rear) 30 Hz OMNIVISION OX08B40, YUV422 8bit, 38402160, 140dB HDR, LFM; 4 (side) OMNIVISION OX03C10, YUV422 8bit, 19201080, 140dB HDR, LFM; INS Deeply-coupled INS integration with Epson G320 IMU, 1 cm + 1 ppm (RMS) accuracy with RTK, 0.5/h bias instability. Each vehicle is equipped with 360 mechanical spinning LiDAR mounted at the top-center position. To mitigate interference from water droplets and snowflakes under CATS, the LiDARs are operated in dual-return mode, providing both the strongest and last points. Seven cameras are installed on each vehicle, including two front-view (wide-angle and telephoto), one rear-view, and four side-view ones. The specific fields of view (FOVs) are illustrated in Fig. 3. All cameras feature High Dynamic Range (HDR) with LED Flicker Mitigation (LFM) capability, which enhances robustness under CATS. Each vehicle is also equipped with high-precision INS connected to nearby Real-Time Kinematic (RTK) base station within 10 km, leading to an accuracy better than 2 cm (1 cm + 1 ppm). During data collection, both systems maintain fixed RTK status. In occasionally obstructed environments (e.g., beneath trees or bridges) the tactical-grade IMU ensures short-term localization without drift. 3.1.2. Time synchronization Time synchronization is crucial for multi-modal cooperative perception, as it ensures consistency across all sensors and agents. In our system, all sensors on both vehicles are hardware-synchronized to GPS time and triggered at integer seconds. The overall synchronization topology is illustrated in Fig. 4. Each vehicle is equipped with an FPGA card responsible for camera triggering and image acquisition. The card reFigure 4. Time synchronization topology for all sensors and module controllers of both vehicles. ceives the GPRMC (contains absolute time) and PPS (Pulse Per Second, integer trigger) signals from the onboard INS, achieving an offset within 20 ns to the GPS time. It then serves as the master clock, synchronizing all other sensors and module controllers within the local network via the Precision Time Protocol (PTP) through an Ethernet switch, maintaining time deviation below 1 ms relative to GPS. Overall, our system achieves 1 ms synchronization accuracy across all sensors and vehicles, which is an order of magnitude improvement over existing datasets, which typically exhibit over 20 ms offsets, indicating 40 cm misalignment of objects relatively moving at 72 km/h. 3.1.3. Calibration We use the factory intrinsic calibration for cameras to achieve the highest accuracy, while for extrinsic parameters, we follow two open-source tools [17, 43] and conduct both LiDAR-Camera and LiDAR-INS calibrations. The LiDAR-Camera extrinsics are estimated by directly aligning the dense LiDAR intensity mapping with the corresponding grayscale camera image [17]. On the other hand, the LiDAR-INS extrinsics are derived from LiDAR odometry with short-term IMU inertial integration (as the origin of the INS coordinate locates at the IMU) [43]. 5 3.2. Data acquisition We conduct data collection across ten diverse locations, including highways, arterial roads, collector roads, and local streets (see Fig. 2). For highway environments, we capture both mainline and ramp scenarios. On lower-level roads, our dataset includes signalized, all-way-stop, and unprotected intersections. Residential and campus areas are touched, where vulnerable road users (VRUs) such as pedestrians and cyclists are more frequent. Each location contains ten repeated runs recorded under varying weather and lighting conditions (see Fig. 2), all referenced to the same HD maps. All sensor data are recorded using ROS2 bag files. The cameras, LiDARs, and INS each publish hardware timestamps through modified drivers. 3.3. Preprocessing We then perform motion compensation for LiDAR frames to deskew distortions caused by ego-motion during scanning (0.1 at 10 Hz, up to 2 error at 72 km/h). The process is essential for accurate multi-frame registration and object localization [38], yet existing datasets [22, 30] acknowledge but do not address the issue, partially due to the absence of per-point timestamps or precise poses. To ensure consistent annotations between the two vehicles, we register their point clouds into unified coordinate system. The transformation between the two LiDAR coordinate systems is derived from the vehicle poses and the LiDAR-INS extrinsics, as expressed in (1a): init L1L2 init L1L2 = (TI1L1)1(TW I1)1TW I2TI2L2, GICP refine L1L2 , (1a) (1b) where TW denotes the pose of the INS in the world coordinate frame, and TIL represents the extrinsic calibration between the INS and the LiDAR. However, small accumulated errors from four transformations can propagate (see Fig. 5). Therefore, we refine it using Generalized Iterative Closest Point (GICP) [16] to get refine L1L2 as the initial estimation. , setting init L1L 3.4. Data annotation We provide precise 3D bounding boxes, with timeconsistent object dimensions for rigid targets. Each dynamic object is also assigned globally unique ID, enabling cross-frame and cross-agent re-identification. Dynamic objects are categorized into Vehicles and VRUs. The Vehicle category includes Car, Van, Truck, Trailer, Bus, and Others; The VRU category includes Pedestrian, Scooter, Bicycle, and Motorcycle rider. Additionally, we introduce virtual link attribute to indicate physical coupling between two annotated objects. For instance, link between Car and Trailer object may represent pickup truck towing trailer. 6 Figure 5. Comparison between two deskewed point clouds register (a) with initial estimation; (b) after GICP refinement. 4. Supporting tasks Thanks to our hardware-synchronized multi-sensor setup, high-quality temporal alignment, and scale of diverse multipass data (detailed in Sec. 3), the CATS-V2V dataset is well-suited for wide range of computer vision and multimodal perception tasks, especially for rare cases. In addition to providing high-quality 2D and 3D annotations, we offer set of task-specific data conversion and segment tools to facilitate research and practical usage across different domains. In the following Tab. 3, we summarize the five major categories of vision tasks[35][21][41][20] supported by CATSV2V. Detailed data processing pipelines and annotation formats for each task are provided in the Supplementary Material. Due to the extensive range of supported tasks, we do not provide benchmarking results for every task within this dataset release. Instead, we will introduce comprehensive evaluations in separate future study based on CATS-V2V. 5. Target-based temporal alignment Despite fine calibrations and millisecond time synchronization, precise temporal alignment across modalities is not Table 3. List of supporting tasks of CATS-V2V dataset. Category Tasks Modalities CATs-V2V Support Perception 2D/3D Detection 2D/3D Tracking"
        },
        {
            "title": "Map Generation",
            "content": "Spatial Understanding SLAM/Odometry LiDAR; Multi-view Camera LiDAR; Multi-view Camera LiDAR; IMU/INS; Multi-view Camera All 3D Reconstruction"
        },
        {
            "title": "All",
            "content": "2D and 3D bounding box annotations 2D and 3D bounding box annotations; Global object IDs across frames High-frequency IMU and RTK-fixed INS; HD maps and BEV annotations High-frequency IMU and RTK-fixed INS HD Maps and geometry transformations based-on RTK-fixed INS Multi-Modal Learning"
        },
        {
            "title": "Joint Compression",
            "content": "LiDAR; Multi-view Camera Hardware-synchronized sensor frames with deskewing and temporal alignment Cross-Modal Learning Depth Estimation View Synthesis LiDAR; Multi-view Camera IMU/INS; Multi-view Camera Domain Adaptation Scene Transfer Multi-view Camera Geometry transformations based-on RTKfixed INS; Multi-frame fused point clouds for depth supervision Overlapping camera frames; Multi-frame fused point clouds Filtered sets of frames from identical scenes captured under different conditions guaranteed. In this section, we analyze this limitation and progressively introduce our frame-based and target-based temporal alignment method. 5.1. Stamp-based alignment While only few real-world V2X datasets are equipped with multi-view or high-frame-rate cameras [24, 29, 34, 39, 44], they mostly report that LiDAR and camera frames are aligned with the closest timestamps (see Tab. 1). While this approach provides convenient approximation, it neglects the inherent characteristics of mechanical spinning LiDARs, which acquire points continuously over full revolution rather than instantaneously. As result, each azimuth within single scan corresponds to slightly different timestamp. Consequently, stamp-based alignment introduces temporal misalignment, as shown in Fig. 6. 5.2. Frame-based alignment In contrast, our dataset adopts frame-based alignment strategy that combines 30 Hz multi-view cameras with LiDAR scans containing per-point timestamps. For each camera, we select the subset of LiDAR points falling within its FOV and associate them with the camera frame whose timestamp is closest to the acquisition times of those points. We take one of our vehicles sensor configurations as an example (see Fig. 6). Since cameras operate at 3 the LiDAR frame rate, the two forward cameras are aligned with the starting timestamp of each LiDAR scan, the frontright camera is aligned with the subsequent image frame, the three rear-facing cameras use the next frame, and the front-left camera corresponds to the ending timestamp. At preprocessing (Sec. 3.3), we perform motion compensation to correct distortions by compensating all points to the timestamp of the first scan point. This procedure effectively deskews the static background of each LiDAR frame. However, for frame-based alignment with cameras, the LiDAR points must further be compensated to the corresponding camera frame timestamp, so that both the static environment and dynamic objects are geometrically consistent across modalities. 5.3. Target-based alignment While frame-based alignment greatly improves temporal consistency across modalities, it still suffers from rare misalignment in multi-camera overlap or wide-FOV scenarios. In practice, single object may appear simultaneously in multiple camera views whose frame-based alignments correspond to different LiDAR scanning times. Similarly, wide-angle camera images may contain few objects whose alignment should correspond to previous or next frame. To address this issue, we introduce target-based temporal alignment strategy. After annotation (or LiDAR-based object detection), we compute the average timestamp of all points belonging to each object and associate the object with the nearest camera frame. The corresponding LiDAR points are then motion-compensated to that timestamp. 7 Figure 6. Illustration of alignment between camera and LiDAR frames due to the inherent characteristics of mechanical spinning LiDARs. Table 4. Quantitative evaluation of our proposed temporal alignment methods. Method base average IoU Recall@IoU=0.3 Recall@IoU=0.5 Recall@IoU=0. center-offset (px) Stamp 0.3736 0.6206 0.3906 0. 61.54 Frame (ours) 0.4493 ( 20.3%) 0.6795 ( 9.5%) 0.5766 ( 47.6%) 0.2494 ( 113%) 50.26 ( 18.3%) Target (ours) 0.4623 ( 23.7%) 0.6932 ( 11.7%) 0.5947 ( 52.3%) 0.2768 ( 136%) 49.76 ( 19.1%) 5.4. Quantitative Evaluation 6. Conclusion To quantitatively evaluate our proposed alignment strategies, we conduct thorough experiments on one representative clip selected from the 100 recorded scenes. This clip is captured under favorable conditions without strong sunlight, or inclement weather, so as to minimize perception noise and annotation uncertainty. We manually annotate 2D bounding boxes for all visible dynamic objects across five camera views (not on the two right-view cameras, as we drive on the right lane and they contain few objects), providing ground truth for alignment evaluation. For benchmarking, we project the annotated 3D bounding boxes onto the images using three different temporal alignment methods: (1) Stamp-based alignment, (2) Framebased alignment (ours), and (3) Target-based alignment (ours). The projected boxes are then compared against the manually annotated 2D boxes on the corresponding images. We evaluate the alignment accuracy using three standard metrics: average IoU, Recall at different IoU levels, and mean center-point deviation between the projected and annotated 2D bounding boxes. As shown in Tab. 4, higher average IoU and Recall among different IoU levels, along with lower center offset, indicate better temporal alignment across modalities progressively with our proposed framebased and target-based temporal alignment methods. In this work, we present CATS-V2V, the first real-world cooperative perception dataset with Complex Adverse Traffic Scenarios under the Vehicle-to-Vehicle (V2V) collaboration domain. It is large-scale dataset collected with two vehicles across ten diverse weather, lighting, and traffic conditions (rain, snow, direct sunlight, nighttime, etc) at ten locations, including highways, arterial roads, and urban intersections in campus and residential areas. Specifically, the dataset provides anonymized point clouds, images, and pose records from one LiDAR, seven cameras, and an INS for each vehicle, as well as time-consistent global-identified 3D bounding box annotations and HD Maps. We additionally propose target-based temporal alignment method to improve the quality of our dataset along with hardware time synchronization. Results on one selected clip from the dataset have demonstrated that our method significantly improves alignment performance across modalities. Furthermore, we offer task-specific data conversion and segment tools to facilitate our supporting tasks. We hope this largestscale, most supportive, and highest-quality V2V dataset with CATS to date could promote research in related communities. Future plans include combining roadside infrastructure and diverse emerging automotive sensors to provide richer dataset covering various corner cases and developing tools for converting it into motion and trajectory datasets."
        },
        {
            "title": "References",
            "content": "[1] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 2 [2] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle edge computing system using 3d point clouds. In Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, pages 88100, 2019. 2 [3] Christian Cre√ü, Walter Zimmer, Leah Strand, Maximilian Fortkord, Siyi Dai, Venkatnarayanan Lakshminarasimhan, and Alois Knoll. A9-dataset: Multi-sensor infrastructurebased dataset for mobility research. In 2022 IEEE Intelligent Vehicles Symposium (IV), pages 965970. IEEE, 2022. 3 [4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 116. PMLR, 2017. 3 [5] Rui Fan, Sicen Guo, and Mohammud Junaid Bocus. Autonomous driving perception. Cham, Switzerland: Springer, 2023. 1 [6] Shuo Feng, Xintao Yan, Haowei Sun, Yiheng Feng, and Henry Liu. Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment. Nature communications, 12(1):748, 2021. 1 [7] Shuo Feng, Haowei Sun, Xintao Yan, Haojie Zhu, Zhengxia Zou, Shengyin Shen, and Henry Liu. Dense reinforcement learning for safety validation of autonomous vehicles. Nature, 615(7953):620627, 2023. [8] Jorg Gamerdinger, Sven Teufel, Patrick Schulz, Stephan Amann, Jan-Patrick Kirchner, and Oliver Bringmann. Scope: synthetic multi-modal dataset for collective perception including physical-correct weather conditions. In 2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC), pages 26222628. IEEE, 2024. 3 [9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. 2 [10] Martin Hahner, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Fog simulation on real lidar point clouds for In Proceedings of 3d object detection in adverse weather. the IEEE/CVF international conference on computer vision, pages 1528315292, 2021. 3 [11] Martin Hahner, Christos Sakaridis, Mario Bijelic, Felix Heide, Fisher Yu, Dengxin Dai, and Luc Van Gool. Lidar snowfall simulation for robust 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1636416374, 2022. 3 [12] Yushan Han, Hui Zhang, Huifang Li, Yi Jin, Congyan Lang, and Yidong Li. Collaborative perception in autonomous drivIEEE Intelligent ing: Methods, datasets, and challenges. Transportation Systems Magazine, 15(6):131151, 2023. 1 [13] Ping Jiang, Xiaoheng Deng, Weishang Wu, Lixin Lin, Xuechen Chen, Chen Chen, and Shaohua Wan. Weatheraware collaborative perception with uncertainty reduction. IEEE Transactions on Intelligent Transportation Systems, 2024. 2 [14] Mateus Karvat and Sidney Givigi. Adver-city: Open-source multi-modal dataset for collaborative perception under adverse weather conditions. arXiv preprint arXiv:2410.06380, 2024. [15] Velat Kilic, Deepti Hegde, Brinton Cooper, Vishal Patel, and Mark Foster. Lidar light scattering augmentation (lisa): Physics-based simulation of adverse weather condiIn ICASSP 2025-2025 IEEE tions for 3d object detection. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 3 [16] Kenji Koide, Masashi Yokozuka, Shuji Oishi, and Atsuhiko Banno. Voxelized gicp for fast and accurate 3d point cloud registration. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1105411059. IEEE, 2021. 6 [17] Kenji Koide, Shuji Oishi, Masashi Yokozuka, and Atsuhiko Banno. General, single-shot, target-less, and automatic lidarIn 2023 IEEE Intercamera extrinsic calibration toolbox. national Conference on Robotics and Automation (ICRA), pages 1130111307. IEEE, 2023. 5 [18] Baolu Li, Jinlong Li, Xinyu Liu, Runsheng Xu, Zhengzhong Tu, Jiacheng Guo, Qin Zou, Xiaopeng Li, and Hongkai Yu. V2x-dgw: Domain generalization for multi-agent percepIn 2025 IEEE Intion under adverse weather conditions. ternational Conference on Robotics and Automation (ICRA), pages 974980. IEEE, 2025. 2, 3 [19] Hangyu Li, Xiaotong Sun, Chenglin Zhuang, and Xiaopeng Li. On the robotic uncertainty of fully autonomous traffic: From stochastic car-following to mobilitysafety tradeoffs. Transportation Research Part C: Emerging Technologies, 178:105254, 2025. 1 [20] Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, and Hongkai Yu. Light the night: multi-condition diffusion framework for unpaired low-light enhancement in autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1520515215, 2024. [21] Yuhuan Lin, Tongda Xu, Ziyu Zhu, Yanghao Li, Zhe Wang, and Yan Wang. Your camera improves your point cloud compression. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. 6 [22] Katie Luo, Minh-Quan Dao, Zhenzhen Liu, Mark Campbell, Wei-Lun Chao, Kilian Weinberger, Ezio Malis, Vincent Fremont, Bharath Hariharan, Mao Shan, et al. Mixed signals: diverse point cloud dataset for heterogeneous lidar v2x collaboration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2876328773, 2025. 6 [23] Chengyuan Ma, Hangyu Li, Keke Long, Hang Zhou, Zhaohui Liang, Pei Li, Hongkai Yu, and Xiaopeng Li. Real-time identification of cooperative perception necessity in road traffic scenarios. Available at SSRN 4973353, 2024. 1 9 [24] Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, and Wei Wu. Holovic: Large-scale dataset and benchmark for multi-sensor holographic intersection and vehicle-infrastructure cooperative. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2212922138, 2024. 2, 3, 7 [25] Fatih Sezgin, Daniel Vriesman, Dagmar Steinhauser, Robert Lugner, and Thomas Brandmeier. Safe autonomous driving in adverse weather: Sensor evaluation and performance monitoring. In 2023 IEEE Intelligent Vehicles Symposium (IV), pages 16. IEEE, 2023. 1, [26] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 2 [27] Tianqi Wang, Sukmin Kim, Ji Wenxuan, Enze Xie, Chongjian Ge, Junsong Chen, Zhenguo Li, and Ping Luo. Deepaccident: motion and accident prediction benchIn Proceedings of the mark for v2x autonomous driving. AAAI Conference on Artificial Intelligence, pages 5599 5606, 2024. 3 [28] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In Computer visionECCV 2020: 16th European conference, Glasgow, UK, August 2328, 2020, proceedings, part II 16, pages 605621. Springer, 2020. 2 [29] Hao Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, et al. V2x-real: largs-scale dataset for vehicle-toeverything cooperative perception. In European Conference on Computer Vision, pages 455470. Springer, 2024. 2, 3, 4, 7 [30] Hao Xiang, Zhaoliang Zheng, Xin Xia, Seth Zhao, Letian Gao, Zewei Zhou, Tianhui Cai, Yun Zhang, and Jiaqi Ma. V2x-realo: An open online framework and dataset for cooperative perception in reality. arXiv preprint arXiv:2503.10034, 2025. [31] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and Jiaqi Ma. Opencda: an open cooperative driving automation framework integrated with co-simulation. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 11551162. IEEE, 2021. 3 [32] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, MingHsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. In European conference on computer vision, pages 107124. Springer, 2022. 2, 3 [33] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In 2022 International Conference on Robotics and Automation (ICRA), pages 25832589. IEEE, 2022. 3 [34] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, 10 Rui Song, et al. V2v4real: real-world large-scale dataset In Proceedfor vehicle-to-vehicle cooperative perception. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1371213722, 2023. 2, 3, 4, [35] Lei Yang, Tao Tang, Jun Li, Peng Chen, Kun Yuan, Li Wang, Yi Huang, Xinyu Zhang, and Kaicheng Yu. Bevheight++: arXiv Toward robust visual centric 3d object detection. preprint arXiv:2309.16179, 2023. 6 [36] Lei Yang, Xinyu Zhang, Chen Wang, Jun Li, Jiaqi Ma, Zhiying Song, Tong Zhao, Ziying Song, Li Wang, Mo Zhou, et al. V2x-radar: multi-modal dataset with 4d radar for cooperative perception. arXiv preprint arXiv:2411.10962, 2024. 2, 3, 4 [37] Keisuke Yoneda, Naoki Suganuma, Ryo Yanase, and Mohammad Aldibaja. Automated driving recognition technologies for adverse weather conditions. IATSS research, 43(4): 253262, 2019. 1 [38] David Yoon, Tim Tang, and Timothy Barfoot. Mapless online detection of dynamic objects in 3d lidar. In 2019 16th Conference on Computer and Robot Vision (CRV), pages 113120. IEEE, 2019. 6 [39] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, et al. Dair-v2x: large-scale dataset for vehicleinfrastructure cooperative 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2136121370, 2022. 2, 3, 7 [40] Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, et al. V2x-seq: large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54865495, 2023. 2, 3 [41] Chunran Zheng, Wei Xu, Zuhao Zou, Tong Hua, Chongjian Yuan, Dongjiao He, Bingyang Zhou, Zheng Liu, Jiarong Lin, Fangcheng Zhu, Yunfan Ren, Rong Wang, Fanle Meng, and Fu Zhang. Fast-livo2: Fast, direct lidarinertialvisual IEEE Transactions on Robotics, 41:326346, odometry. 2025. [42] Ziqiang Zheng, Yujie Cheng, Zhichao Xin, Zhibin Yu, and Bing Zheng. Robust perception under adverse conditions for autonomous driving based on data augmentation. IEEE Transactions on Intelligent Transportation Systems, 24(12): 1391613929, 2023. 1 [43] Fangcheng Zhu, Yunfan Ren, and Fu Zhang. Robust realtime lidar-inertial initialization. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 39483955. IEEE, 2022. 5 [44] Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, and Alois Knoll. TumIn Proceedings of traf v2x cooperative perception dataset. the IEEE/CVF conference on computer vision and pattern recognition, pages 2266822677, 2024. 2, 3,"
        }
    ],
    "affiliations": [
        "Cleveland State University",
        "University of Wisconsin-Madison",
        "wuwen-ai"
    ]
}