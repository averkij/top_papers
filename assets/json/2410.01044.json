{
    "paper_title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
    "authors": [
        "Dongwei Jiang",
        "Guoxuan Wang",
        "Yining Lu",
        "Andrew Wang",
        "Jingyu Zhang",
        "Chuyu Liu",
        "Benjamin Van Durme",
        "Daniel Khashabi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 4 4 0 1 0 . 0 1 4 2 : r RATIONALYST: Pre-training Process-Supervision for Improving Reasoning Dongwei Jiang Guoxuan Wang Yining Lu Andrew Wang Jingyu Zhang Chuyu Liu Benjamin Van Durme Daniel Khashabi Johns Hopkins University, University of Notre Dame djiang21@jhu.edu"
        },
        {
            "title": "Abstract",
            "content": "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, model for process-supervision of reasoning based on pre-training on vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets. 1 Figure 1: simplified example showing how implicit rationales in pre-training data can be leveraged to improve reasoning. 1 : Implicit rationales (unstated logical connections) occur frequently in LLM pre-training data. 2 : As result, existing LLMs pre-trained to replicate their pretraining data tend to omit these logical steps as well. 3 : However, RATIONALYST learns to generate these rationales at inference time to supervise the chain-of-thought process for more accurate reasoning."
        },
        {
            "title": "Introduction",
            "content": "Rationales play crucial role in human reasoning and its accuracy (Rips, 1994; Mercier and Sperber, 2011). In reasoning problems, having accurate rationales often correlates with accurate outcomes (Tversky et al., 1982; Davis, 1984). This importance of rationales extends to Large Language Models (LLMs) as well. Wei et al. (2022) were among the first to show that generating chain-of-thought rationales significantly improves LLMs reasoning performance. Subsequent research has further refined the methods for eliciting rationales, leading to improved performance (Fu et al., 2023; Zhou et al., 2022). 1Our code, data, and model can be found at this repository: https://github.com/JHU-CLSP/Rationalyst In the context of LLM reasoning, these rationales are typically employed through chain-ofthought process that makes reasoning steps explicit by articulating them as plain-text rationales. In this approach, each subsequent rationale is generated based on rationales produced in preceding steps, effectively using them as form of supervision. However, the generated reasoning chains might be incomplete, containing potential logical leaps while leaving some rationales implicit (or hidden) during the generation process. These gaps in the reasoning chain can weaken the LLMs reasoning ability throughout the problem-solving process. One reason why chain-of-thought methods might miss implicit steps is that models trained with next-token prediction often replicate the Figure 2: An example showing how RATIONALYST works at inference time. RATIONALYST generates implicit rationales given the current reasoning trajectory, which includes both the question and the reasoning steps generated so far 1 . Agent LLM generates multiple next-step candidates for reasoning, also based on the current reasoning trajectory 2 . Implicit rationale generated by RATIONALYST is used to provide heuristics for choosing the next step candidates proposed by the agent LLM by estimating the probability of the next step candidate given the rationale 3 . The reasoning trajectory is updated iteratively with the highest scoring next step candidate 4 . omissions present in their training data. Implicit rationalesunderlying logical connections that are often not explicitly statedare frequently missing in daily communication and web text. Figure 1 1 illustrates this concept using typical document from LLM pre-training data. In this example, we see passage from Harry Potter: Harry used magic outside... He is punished to attend...\" The text contains the implicit (unstated) rationale: When someone breaks the rule, he will be punished!\" This implicit rationale is crucial in inferring the causal reasoning that connects the cause (Harry breaking rules) to its effect (punishment), but is also left unstated in the context. As result, existing LLMs trained to mimic web text will have difficulty surfacing these implicit statements during the reasoning process, which can lead to flawed conclusions, such as erroneously justifying theft as praiseworthy act when done to support ones family ( 2 in Figure 1). This paper presents RATIONALYST, model tailored for process-supervision of reasoning. RATIONALYST is pre-trained on vast collection of implicit rationales extracted from mixture of webscale unlabeled datasets and existing reasoning datasets. Although existing LLMs may miss crucial details in their reasoning, leading to flawed conclusions ( 2 in Figure 1), integrating these LLMs with RATIONALYST provides an additional supervision mechanism to guide their reasoning processes, resulting in more robust conclusions ( 3 in Figure 1). RATIONALYST is developed and used in three stages: (1) we employ LLMs to extract implicit rationales from unlabeled text corpora without human annotation. These rationales are subsequently filtered based on their helpfulness in predicting subsequent text (3.1); (2) we train RATIONALYST to predict those rationales given the preceding context (3.2); and then (3) as depicted in Figure 2, during inference, we assume reasoning is done incrementally in chain-of-thought fashion (Wei et al., 2022) by another agent model, and we use RATIONALYST to provide supervision for the agent model at each reasoning step throughout the reasoning process 3.3. By adopting data-centric approach, RATIONALYST utilizes abundant unlabelled data to provide process supervision (Lightman et al., 2023) across various reasoning tasks without the need for human annotation. Our method extracts 65k implicit rationales from the web-scale unlabelled dataset The Pile (Gao et al., 2020). To adapt the extracted rationales to our tested domain and stabilize training, we additionally extract much smaller set of 14k implicit rationales from the question-answer pairs in the training sets of two reasoning datasets: GSM8K (Cobbe et al., 2021a) and ECQA (Aggarwal et al., 2021). Our extraction process controls for answer leakage to prevent artificial amplification of performance. Using this curated set of rationales, RATIONALYST is then fine-tuned from LLaMa-3-8B. To assess the effectiveness of our approach, we evaluate RATIONALYST on diverse set of reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Our results show that RATIONALYST improves the accuracy of reasoning by an average of 3.9% (5.1). To understand the contribution of different data sources, we conduct an ablation study that demonstrates the utility of rationales from both the large-scale Pile dataset and the smaller, specialized reasoning datasets (5.2). Notably, RATIONALYST exhibits superior performance when compared to strong general-purpose verifiers like GPT-4 and similar capacity models specifically fine-tuned on matching training sets (5.4). Implicit rationales generated by RATIONALYST are also designed to provide supervision in human-readable form, offering improved interpretability for LLM generation. This added interpretability is particularly beneficial when reasoning over complex domains such as mathematics or coding, where the step-by-step logic can be difficult for humans to follow without explicit explanations. As shown in 5.5, our model is capable of generating human-understandable rationales for unseen data from complex math reasoning. Our contributions in this paper are two-fold: We propose RATIONALYST, model that is pretrained on implicit rationales extracted from unlabeled text data. RATIONALYST enhances LLM interpretability and performance during reasoning by providing process supervision. We empirically show RATIONALYST generalizes across reasoning tasks and scales with unlabelled data."
        },
        {
            "title": "2 Related Work",
            "content": "Supervising reasoning. Supervision-based approaches have been shown to enhance the reasoning abilities of LLMs. Cobbe et al. (2021b) and Snell et al. (2024) demonstrate that training verifier\" to supervise reasoning can be more parameterefficient than simply expanding the parameters of the reasoner\" responsible for solving the reasoning task. Ground-truth feedback from interaction with the environment is an effective form of supervision (Wang et al., 2023), but it works only in controlled environments like simulated world. General-purpose verifiers (Dhuliawala et al., 2023; Weir et al., 2024, 2023; Vacareanu et al., 2024) offer broader applicability utilizing principles like compositional reasoning. However, they dont fully capitalize on the vast amount of unlabelled data in the way data-driven approach might. Processbased supervision (Lightman et al., 2023) offers supervision at each reasoning step rather than just at the final result. While promising, it requires substantial human annotation for the correctness of intermediate steps, making it resource-intensive. Our work aims to address these challenges by proposing data-centric process-supervision method without the need for human annotation. Knowledge extraction from unlabelled data. LLMs are conventionally trained on extensive web data using autoregressive next-token prediction. While effective, this approach may not fully harness the potential of the pre-training data, as latent information within this data could be better accessed using techniques beyond simple next-token prediction. Recent research has demonstrated several approaches to utilize this latent information to develop more sophisticated language model capabilities. Schick et al. (2023) introduced Toolformer, which autonomously annotates and extracts appropriate positions, names, and inputs for tool use by leveraging supervision from future tokens. Similarly, Cornille et al. (2024) developed method for learning to plan coherent article writing through self-supervised learning in text. More closely related to our work, Zelikman et al. (2024) proposed Quiet-Star, which applied comparable technique to uncover underlying rationales in daily communication to enhance reasoning capabilities. Our work adopts strategy similar to Quiet-Star for extracting rationales in an unsupervised manner. However, our approach diverges in its primary objective: we aim to train supervisor\" that can utilize these rationales to provide process supervision for any reasoner.\" This focus enables us to implement simpler and more reliable method, as we dont need to directly integrate rationale extraction with reasoner\" training. Our approach thus offers novel perspective on leveraging latent information in language models to enhance their capabilities. Rationales as the basis for reasoning. Various studies have focused on improving the use of rationales to elicit reasoning. Fu et al. (2023) refine rationales for more effective reasoning elicitation, while Li et al. (2023) explore different approaches to leveraging rationales to enhance reasoning. Other works, such as Hwang et al. (2024), examine the verification of rationales produced by LLMs during reasoning to improve performance. Additionally, training LLMs on rationale-rich data is common strategy for enhancing reasoning 3 skills. As highlighted by Lewkowycz et al. (2022) and Jiang et al. (2024a), LLMs trained on science and math data tend to perform better on reasoning tasks, particularly when CoT prompting is used. In this work, we build on this foundation by using rationales as the core of our method to supervise reasoning."
        },
        {
            "title": "3 Building RATIONALYST",
            "content": "We discuss the construction of RATIONALYST and its usage at inference time. First, we describe extracting rationales from unlabeled text (3.1), then use them to train RATIONALYST (3.2), and finally, employ RATIONALYST to supervise reasoning during inference (3.3). Setup. As we will be using multiple LLMs throughout the process, we define them here: MRa is the trained rationale generation model (RATIONALYST) that generates rationales and heuristics during inference. MAgent is generalpurpose reasoning agent that produces candidate reasoning steps and incorporates rationales during inference. We use one additional model for initial rationale extraction, rationale filtration, and probability estimation of potential next reasoning steps during inference. These LLMs can be implemented using various state-of-the-art models, allowing for adaptability to specific research needs and computational resources."
        },
        {
            "title": "3.1 Large-scale Rationale Extraction",
            "content": "Implicit rationales are often embedded in unlabelled text, reflecting natural thought processes in daily communication. Our extraction process, illustrated in Figure 3, aims to make these rationales explicit. Using pre-trained and aligned language model , we generate rationales from text and then use to filter these rationales to retain only those that are useful, akin to the self-supervised tool learning approach described by Schick et al. (2023). The same is subsequently used to train RATIONALYST. Extracting rationales from pre-training data. We employ to generate rationales from the Pile. Due to the size of this dataset, we implement pre-filtering process to identify reasoning-rich documents by (1) computing the average semantic embedding of representative reasoning training sets using paragraph embedding model, and (2) selecting documents from unlabelled datasets that exceed cosine similarity threshold α when compared to this average embedding. After pre-filtering, we segment the selected paragraphs into 2000-word segments and instruct to generate rationales at the end of each sentence, using prompts with demonstrations. Detailed information on the prompts and in-context learning demonstrations used for rationale extraction can be found in Appendix A. from reasoning Extracting rationales datasets. In parallel to , we also extract rationales from existing reasoning datasets to adapt the extracted rationales to our tested domain and stabilize training. For given reasoning dataset with pairs of questions and final answers = {(qi, ai)}m i=1, we create prompt that instructs to generate rationales for each reasoning step in the final answer ai. The input of the prompt consists of the entire question and answer, and the output includes implicit rationales that can be inferred from the reasoning process in the answer. Consider the concrete example from existing datasets (bottom) in Figure 3. The solution involves two reasoning steps: Natalia sold 48 / 2 = 24 clips in May and Natalia sold 48 + 24 = 72 chips altogether. Here, the implicit rationale that connects the first and second steps, Now we should calculate the sum of chips in April and May, is implicit yet helpful for the prediction of the second step. These rationales are subsequently filtered and used to train RATIONALYST. Filtering extracted rationales. Generated rationales in and may not always be accurate or helpful. In reasoning tasks, our objective is for the extracted rationales to effectively aid in future reasoning, which means good rationale should enhance the likelihood of accurately predicting the following text. Let be the position of the rationale in the sequence = x1, . . . , xn. Given sequence of weights (wk)kN, the weighted crossentropy loss for future token prediction is defined as: Li(r) = (cid:88) j=i wji log pM (xj r, x1:j1), where , in different role from its previous use, is employed to estimate the probability over tokens xi, . . . , xn prefixed by preceding tokens x1:i1 and rationale r. The weight assigned to each future token decreases exponentially by factor of 0.9 for each step further away it is from the rationale. We compute Li = Li(ri) Li(ε), where ε represents an empty rationale (i.e. predicting following tokens 4 Figure 3: We use LLMs to extract implicit rationales (enclosed by <BOT> and <EOT> in bold) that capture reasoning in unlabelled text (3.1 and ). The sample at the top is taken from unlabelled web-scale pre-training datasets The Pile and the sample at the bottom is taken from existing datasets (GSM8K). These rationales are subsequently filtered based on whether they are useful for predicting future text (3.1 ). based only on preceding tokens). rationale is considered helpful if it makes the prediction of future tokens easier, indicated by Li τf , where τf is filtering threshold. We retain rationales for which adding the rationale reduces the loss by at least τf compared to having no rationale. Its crucial to clarify two key aspects of our rationale extraction process. First, while extracts rationales from the training sets of reasoning datasets, these training sets are not directly used as targets when training itself. Second, we explicitly instruct to exclude answers from the extracted rationales. This precaution prevents answer leakage in our prompts, thereby ensuring the integrity of our reasoning process."
        },
        {
            "title": "3.2 RATIONALYST Training",
            "content": "The goal of RATIONALYST (denoted by MRa) training is to develop model that can generate implicit rationales to guide stepwise problem-solving during inference time. For web-scale datasets like The Pile, the input context consists of segment of text from document. MRa learns to generate an implicit rationale that can guide the prediction of the next segment of text in the documents flow. In the case of structured reasoning datasets such as GSM8K or ECQA, the input context includes the question and any preceding reasoning steps toward the answer. Here, MRa learns to generate rationales that could guide the next step in the problemsolving sequence. Given the appropriate context from either source, the implicit rationales, extracted and filtered as described in 3.1, serve as the target outputs during training. The overall training objective is to minimize the per-token cross-entropy loss between the 5 generated rationales and their ground truth values from the extracted and filtered rationales. By learning to generate appropriate rationales for both freeform text and structured problem-solving data, RATIONALYST develops the ability to provide meaningful guidance across wide range of contexts during inference."
        },
        {
            "title": "3.3 Inference with the Help of RATIONALYST",
            "content": "During inference, general-purpose LLM (the agent model\" or MAgent) is employed for reasoning across various problems. Algorithm 1 outlines the procedure. MAgent generates reasoning incrementally in chain-of-thought fashion, producing multiple candidates for the next reasoning step. These steps and the question form reasoning trajectory\" that aims to solve the problem, which also serves as input to MRa. MRa then generates r, the implicit rationale (line 3) With the help of implicit rationale, we provide supervision for the next reasoning step. Two supervision methods we considered are: this supervision. For Implicit supervision, MAgent generates the next reasoning steps conditioned on the trajectory (line 6). We then use to estimate the probability of potential next reasoning steps given rationale and reasoning trajectory (line 13). This probability-based heuristic aligns with our rationale filtration process used during MRa training. Just as we identified rationales that improved the prediction of future text during filtration, here we use rationales to improve the selection of future reasoning steps. By leveraging the probability estimates as heuristic, we can effectively discriminate between more and less likely next steps in the reasoning process, guiding the overall trajectory towards more accurate conclusions. Explicit supervision. Another approach is to directly incorporate the implicit rationale into the generation of the next reasoning steps. This method makes the previously implicit rationale an explicit part of the reasoning process. To do that, we ask MAgent to generate multiple candidate next steps by temporarily appending to the trajectory , and then producing potential continuations based on this augmented context (line 8). Then, we estimate the probability of candidate generations according to MAgent (line 15). This approach allows MAgent to make the final decision on the next reasoning step, as in normal beam search (Snell et al., 2024; Yao et al., 2023), while benefiting from the additional context provided by MRas rationales. Algorithm 1 Inference with RATIONALYST Input: Question q, RATIONALYST MRa, Agent model MAgent, Probability estimation model ; Functions: Heuristic function H(MRa, q, ), stopping condition stop_condition() Hyperparameters: Sampling temperature and number of sampled rationales Initialize reasoning trajectory as the question. 1: 2: repeat 3: MRa(T ) 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: Generate implicit rationale given trajectory. heuristic_list = Empty the heuristic list at every if supervision == implicit then next_steps MAgent(T ) Sample next reasoning step. steps. else if supervision == explicit then next_steps MAgent(T, r) end if for = 1 . . . do next_steps[n] if supervision == implicit then Take next step generation. (xT, r) Estimate prob. of next reasoning else if supervision == explicit then step. MAgent(xT, r) end if heuristic_list.append(h) Retain the heuristic. end for max_idx argmax(heuristic_list) + next_steps[max_idx] Extend trajectory with the highest scoring step. the trajectory contains strings like The final answer is: 21: until stop_condition(T ) E.g., 22: return After providing heuristics for the next reasoning steps, the step with the highest heuristic (line 19) is selected. The reasoning trajectory is then extended with this highest-scoring step (line 20). The reasoning process concludes when the stop condition is satisfied (line 21), which varies by dataset and often includes cues like The final answer is:\" that can be specified in system prompts for MAgent for different tasks. 6 The computational cost of MRa is comparable to normal beam search, with the only additional cost being the generation of rationales, which are typically quite short."
        },
        {
            "title": "4 Experimental Setup",
            "content": "4.1 Setup for Training RATIONALYST Rationale extraction. As discussed in 3.1 , we perform pre-filtering on The Pile, an unlabelled web-scale dataset, to identify documents with extensive reasoning content before rationale extraction. This is achieved by computing the average semantic embedding from the training sets of the reasoning datasets we test, filtering documents that exceed the cosine similarity threshold α of 0.3, and keeping only the documents with length under 2000 tokens to fit within LLaMa-3 models context length. The model we used to calculate these embeddings is MPNet-base (Song et al., 2020). Following the recipe in 3.1 , we also extract rationales from existing reasoning datasets. GSM8K (Cobbe et al., 2021b) and ECQA (Aggarwal et al., 2021) were selected for their complementary coverage of mathematical and commonsense reasoning, respectively. This combination ensures RATIONALYST is trained on diverse reasoning patterns, enhancing its versatility across various tasks. Rationale annotation and filtration. The model used for rationale extraction and rationale filtering are both LLama-3-8B-Instruct (MetaAI, 2024). On GSM8K and ECQA, we manually annotated 100 pairs of {preceding_context, rationale, following_context} to determine an appropriate filtration threshold. The annotations include 50 positive and 50 negative rationale examples. Since its straightforward to scale up the extraction of rationales from unlabelled data for filtration, we prioritize maximizing the precision of our filtered rationales, even if it means extracting fewer of them. We set the threshold τf to ensure that 95% of the filtered rationales are accurate. On The Pile, we do not perform rationale annotation due to its diverse composition of corpora with varying characteristics. So the filter threshold τf for the Pile is set to 0 for all of its subdomains. The resulting data from extraction/filteration. The results of rationale extraction and filtration on GSM8K, ECQA, and The Pile are presented in Table 1. On GSM8K, our method generates an average of 2.34 rationales per document, while on Dataset Subdomains # Docs. # Rationales GSM8K ECQA N/A N/A 7473 7600 The Pile Pile-CC 266.6K StackExchange 21.8K 19.9K 5.8K PubMed Central 4.9K 4.2K Wikipedia (en) Github HackerNews 17566 19669 853.2K 113.6K 45.8K 24.4K 18.6K 23.0K Rationales Left (%) τf 19.5 57.6 2.9 29.8 2.6 9.4 3.2 7.8 1.2 0.5 0 0 0 0 0 0 entific reasoning, and the recently proposed multitask reasoning dataset MMLU-Pro (Wang et al., 2024) for holistic reasoning across multiple tasks. All tasks are evaluated using exact match as the metric. We apply the postprocessing setups from lm-evaluation-harness2 before exact match calculation where applicable. Table 1: The statistics on rationale sampling and filtration. We provide the total number of documents and rationales before filtering, and the percentage of leftover rationales after filtering. ECQA, it generates 2.58 rationales per document. The filtration process removes 80.5% of the generated rationales on GSM8K and 42.4% on ECQA. For The Pile, we report the number of rationales per document and the number after filtration for each subdomain. The Piles documents, being longer than those in GSM8K and ECQA, yield higher average number of rationales per document. Among the subdomains, StackExchange retains the highest percentage of rationales, likely due to its question-answering format aligning well with our reasoning tasks and containing more inherent reasoning. However, The Pile as whole contains less reasoning content, making rationale extraction challenging. Setting the threshold to 0 accepts all rationales more helpful than not having them, but the yield remains low. manual review shows that most filtered rationales describe the preceding context rather than guiding future reasoning. In total, we extracted approximately 14k rationales from GSM8K and ECQA combined, and about 65k from The Pile after filtration. RATIONALYST training. RATIONALYST is finetuned with LLaMa-3-8B-Instruct as the base model. We use the default hyperparameters as specified in the LLaMa-3 technical report (MetaAI, 2024) for fine-tuning."
        },
        {
            "title": "4.2 Setup for Evaluating RATIONALYST",
            "content": "Evaluation tasks and metrics. summary of reasoning tasks we evaluate is provided in Table 2. We assess our method on the following datasets: GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021) for mathematical reasoning, ECQA (Aggarwal et al., 2021) and HellaSwag (Zellers et al., 2019) for commonsense reasoning, ProofWriter (Tafjord et al., 2021) for logical reasoning, ARC (Clark et al., 2018) for sciTask #Eval #Shots Reasoning Type GSM8K Math ECQA HellaSwag ProofWriter ARC MMLU-Pro 1319 5000 17944 10000 600 1172 8 5 6 4 2 4 5 Math Math CommonSense CommonSense Logical Scientific Mixed Table 2: Configuration of the reasoning tasks tested. #Eval shows the number of instances used for evaluation. #Shots denotes the number of few-shot demonstrations provided for evaluation. For Math dataset, we report the results on all of the subsets. To evaluate ECQA, we use the validation split because the evaluation set requires running the evaluation on the original authors server. For ProofWriter, we include only proofs with depth of 5 or more. For ARC, we test on the ARC-Challenge subset. Inference setting. The model MAgent used for our baseline inference is also LLaMa-3-8B-Instruct. As mentioned earlier, to incorporate RATIONALYST, we instruct MAgent to reason in chain-ofthought manner. For procedural reasoning tasks like GSM8K, Math and MMLU_Pro, we provide in-context learning examples that break down the reasoning into individual steps leading to the final answer. For multiple-choice reasoning tasks like ECQA and ARC, we include examples that analyze and compare each answer choice. The content and number of in-context demonstrations align with lm-evaluation-harness or the original paper if available; otherwise, they are adjusted to fit the context window of LLaMa-3-8B-Instruct. Detailed prompts and in-context learning demonstrations are provided in Appendix B. For all experiments, we employ temperature of 0.7 during inference to facilitate sampling. This approach diverges from the conventional use of temperature 0, yielding improved performance on certain datasets (e.g., ProofWriter) while marginally reducing effectiveness on others (e.g., GSM8K). We set the sampling parameter top_k to 3, allowing MAgent to sample three reasoning steps simultaneously at each inference stage. For the baseline 2https://github.com/EleutherAI/ lm-evaluation-harness 7 without RATIONALYST, the next reasoning step is chosen randomly from these 3 samples. When using RATIONALYST, the selection of the next step is guided by the rationales generated, as described in 3.3. Other verifiers. To evaluate the effectiveness of RATIONALYSTs process supervision, we compare it with other approaches. For process supervision with other models, we include LLaMa-38B-Instruct and GPT-4 in our comparison. These models are prompted to rerank partial reasoning trajectories as reasoning steps are generated. The prompts and in-context learning demonstrations used for these models on representative datasets are provided in Appendix D. For outcome supervision, we also compare with outcome-based verifiers derived from LLaMa-3-8B-Instruct. These verifiers are fine-tuned on the training sets of each reasoning dataset. Following the approach outlined by Cobbe et al. (2021b), they assess the correctness of the final prediction by directly evaluating the question and final solution. This comparison allows us to assess the performance of RATIONALYST against both process-based and outcome-based supervision methods."
        },
        {
            "title": "5.1 Main result: RATIONALYST Improves\nPerformance on Various Tasks",
            "content": "In this section, we train RATIONALYST using combination of rationales extracted from GSM8K and ECQA, as well as from The Pile, as outlined in Table 1. The baseline does not use any verifier. We use implicit supervision for this experiment. The main result is shown in Table 3. Reasoning Type Dataset Baseline RATIONALYST Acc. Acc. Acc. Mathematical CommonSense Logical Scientific Combined GSM8K Math ECQA HellaSwag ProofWriter ARC MMLU-Pro 77.6 28.0 72.6 58.2 86.4 77.6 39.6 81.6 32.5 75.2 60.3 90.7 80.7 45.3 4.0 4.5 2.6 2.1 4.3 3.1 5.7 Table 3: Accuracy and absolute improvement over baseline using RATIONALYST. RATIONALYST generalizes across different reasoning tasks, showing improved performance with unlabelled web-scale data. Evaluation of RATIONALYST shows that training with rationales from GSM8K, ECQA, and The Pile improves performance not only on GSM8K and ECQA, but also on other reasoning tasks (e.g. scientific reasoning, logical reasoning, etc) not directly used in rationale extraction. This supports the idea that rationales can be broadly applicable across different reasoning tasks. In addition, since we use the same model (LLaMa-3-8B-Instruct) for rationale extraction, filtering, RATIONALYST training, and inference, our results do not leverage external knowledge from stronger models like LLaMa-370B-Instruct or GPT-4. Future work might change to stronger models, with the expectation that higher-quality rationales will lead to better performance. 5.2 Ablation: Web-scale Rationales Enhance Performance Across Tasks To assess the benefit of web-scale rationales, we train another model: RATIONALYST w/o Pile solely on rationales extracted from the training sets of GSM8K and ECQA. We re-ran the experiments on the same reasoning datasets using implicit supervision. The results are detailed in Table 4. We find that training the model on web-scale data results in better performance compared to training only on the rationales extracted from GSM8K and ECQA. This improvement is consistent and particularly significant on MMLU-Pro. Web-scale data likely provides exposure to more diverse reasoning types and content, including specialized knowledge, complex real-world scenarios, and interdisciplinary connections not present in the more focused datasets. Dataset RATIONALYST RATIONALYST (w/o Pile) Acc. GSM8K Math ECQA HellaSwag ProofWriter ARC MMLU-Pro 81.6 32.5 75.2 60.3 90.7 80.7 45. 80.3 31.4 74.5 59.1 88.2 78.8 41.2 -1.3 -1.1 -0.7 -1.2 -2.5 -1.9 -4.1 Table 4: An ablation study on the benefit of rationales extracted from pre-training data (The Pile). We compare with RATIONALYST against the w/o Pile model that is trained solely with rationales extracted from GSM8K and ECQA. The consistent accuracy drop shows that, utilizing web-scale rationales improves performance on various reasoning datasets."
        },
        {
            "title": "5.3 Ablation: Implicit Supervision Works\nBetter than Explicit Supervision",
            "content": "In this section, we conduct ablation studies to test the effectiveness of different supervision meth8 ods. To isolate the impact of supervision methods and minimize confounding variables, we focus on GSM8K and ECQA as representative benchmarks for mathematical and commonsense reasoning, respectively. We train two versions of RATIONALYST: one on rationales extracted from the GSM8K training set (RATIONALYST - GSM8K) and another on rationales from the ECQA training set (RATIONALYST - ECQA). These models are used to supervise MAgent during inference on their respective tasks. As shown in Table 5, implicit supervision outperforms explicit supervision. Our manual analysis revealed that implicit supervisions superior performance stems from its greater robustness to errors. When RATIONALYST generates an imperfect rationale, the probability-based heuristic used in implicit supervision can still provide useful guidance even if the rationale itself is not ideal. This approach is less likely to lead MAgent to produce incorrect next steps. In contrast, explicit supervision directly incorporates potentially flawed rationales into the reasoning process, which can cause MAgent to produce incorrect next steps. Essentially, implicit supervision acts as softer guide, allowing for some imperfection in rationales, while explicit supervision more strictly adheres to potentially flawed rationales, making it more susceptible to errors. Heuristic - Evaluation task GSM8K ECQA Implicit Supervision Explicit Supervision 80.3 77.5 74.5 72.2 Table 5: Comparison of implicit and explicit supervision methods on GSM8K and ECQA tasks. Implicit supervision outperforms explicit supervision due to its robustness to errors."
        },
        {
            "title": "Verifiers",
            "content": "Table 6 presents an analysis of RATIONALYST against various verifiers. Our findings reveal several insights: vanilla LLaRATIONALYST outperforms Ma-3-8B-Instruct using process supervision: RATIONALYST, even without leveraging The Pile dataset, outperforms process-based verifiers using vanilla LLaMa-3-8B-Instruct. manual examination of reasoning trajectories suggests that LLaMa-3-8B-Instruct faces difficulties in reranking partial reasoning steps. This challenge likely stems from the models struggle to differentiate among its own generated outputs, phenomenon observed in recent studies (Jiang et al., 2024b; Huang et al., 2023). RATIONALYST shows superior process-supervision performance than much bigger models like GPT-4: We observe consistent superior performance of RATIONALYST compared to GPT-4s process supervision. We hypothesize that this advantage arises from RATIONALYSTs specialized design for providing supervision, in contrast to GPT-4s general-purpose training. RATIONALYST surpasses outcome-based verifiers trained using matching data: Notably, our method surpasses the performance of finetuned outcome-based verifiers on both GSM8K and ECQA datasets, despite these verifiers being trained on matching data. We attribute this success to the richer feedback provided by processbased supervision compared to outcome-based approaches. Supervision GSM8K ECQA N/A Process Supervision w/ LLaMa-3 Process Supervision w/ GPT-4 Outcome Supervision w/ LLaMa-3 + FT RATIONALYST w/o Pile RATIONALYST 77.6 77.4 80.0 79.2 80.3 81.6 72.6 71.5 74.7 74.3 74.5 76.2 Table 6: Comparison of different supervision methods. Process supervision uses LLaMa-3 and GPT-4 to directly rerank each reasoning step. Outcome-based supervision fine-tunes LLaMa-3 on GSM8K and ECQA training sets to evaluate final answers. RATIONALYST outperforms both strong verifiers like GPT-4 and similarly-sized models fine-tuned on matching training data."
        },
        {
            "title": "5.5 RATIONALYST Generates Accurate and",
            "content": "Easy-to-understand Rationals We annotate some samples from the test set of Math (Hendrycks et al., 2021) at inference time, which was not part of the rationale sampling datasets. Through manual observation, we find that our model can generate useful rationales that is helpful for understanding LLMs reasoning process on Math (an example is provided in Appendix C). Comparing the rationales generated by RATIONALYST with those generated by Quiet-Star (Zelikman et al., 2024) on the same problems, we find that our method produces more human-understandable rationales. We believe this happens because QuietStar optimizes rationales during training using the 9 accuracy of the final prediction as reward. This approach, while effective for improving task performance, does not explicitly prioritize human interpretability. In addition, this appraoch might inadvertently develop shortcuts or non-intuitive patterns that optimize for accuracy but not necessarily for clarity or human understanding. Pile. The value of fine-tuning on previously encountered text is likely lower than the value of fine-tuning on newly incorporated rationales. Second, implicit rationales encapsulate the reasoning process. Pre-training on these rationales enhances reasoning more effectively than focusing on the whole document."
        },
        {
            "title": "7 Limitations",
            "content": "One limitation of this work is the comprehensiveness of our experiments. In future research, we plan to extend our experiments to broader range of reasoning tasks and compare RATIONALYST with other outcome-based and process-based verifiers. We also plan to adjust the combination of rationales used to train RATIONALYST by (1) sampling from different reasoning tasks and (2) altering the mix of rationales in unlabelled web-scale pre-training data to better understand its generalizability."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we introduced RATIONALYST, novel self-supervised model designed to enhance the reasoning capabilities of LLMs by leveraging hidden rationales extracted from unlabeled text. Our approach centers on the effective extraction and utilization of implicit rationalesthose underlying thought processes that are not explicitly stated in the text but can be inferred. By capturing these rationales, RATIONALYST provides mechanism for process supervision during reasoning, enabling LLMs to reason better. Acknowledgements. We sincerely thank Eric Zelikman, Tianmin Shu, and the broader JHU CLSP community for discussions and inspiration. Scaling up RATIONALYST. Scaling RATIONALYST with stronger models and increased computational resources is logical next step. Utilizing stronger models, such as LLaMa-3-70B or GPT4, would enhance the quality of extracted rationales, improve filtration accuracy, and ultimately strengthen RATIONALYST. However, due to computational constraints, we have not pursued this, which remains limitation of this paper. Additionally, using larger unlabelled datasets with more extensive reasoning content, such as OpenWebMath (Paster et al., 2023), is currently infeasible due to the significant computational and time requirements for pre-filtering and training. These enhancements are planned for future work. Connection to research on scaling test-time compute. Recent research has focused on extending computational resources at test-time (Snell et al., 2024; Wu et al., 2024), particularly for complex In our experiments, we focus reasoning tasks. on developing heuristics and employ straightforward approach of sampling multiple candidates and reranking them based on RATIONALYSTs guidance. However, RATIONALYSTs framework is compatible with more sophisticated test-time compute techniques. Its heuristics can be integrated into existing algorithms like beam-search or lookahead search, potentially enhancing their performance without significantly increasing computational cost. Is training on extracted rationales necessary? In our approach, we first select subset of unlabelled data that contains strong reasoning signals, then extract implicit rationales from this data for model fine-tuning. While it has been demonstrated that training on data with robust reasoning signals can enhance reasoning capabilities on its own (Gunasekar et al., 2023; Jiang et al., 2024a), we believe our method offers additional performance benefits for two reasons. First, many language models have already been trained on datasets like The"
        },
        {
            "title": "References",
            "content": "Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for CommonsenseQA: New Dataset and Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. Nathan Cornille, Marie-Francine Moens, and Florian Mai. 2024. Learning to plan for language modeling from unlabeled data. Preprint, arXiv:2404.00614. Randall Davis. 1984. Diagnostic reasoning based on structure and behavior. Artificial intelligence, 24(13):347410. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. Preprint, arXiv:2309.11495. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In ICLR. OpenReview.net. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need. Preprint, arXiv:2306.11644. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In NeurIPS, Menlo Park, Calif. AAAI Press. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. 2024. Selfexplore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards. Preprint, arXiv:2404.10346. Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen. 2024a. Leanreasoner: Boosting complex logical reasoning with lean. Preprint, arXiv:2403.13312. Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, and Daniel Khashabi. Self-[in]correct: Llms struggle with 2024b. Preprint, refining self-generated responses. arXiv:2404.04298. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In NeurIPS. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making large language models better reasoners with stepaware verifier. Preprint, arXiv:2206.02336. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Preprint, 2023. arXiv:2305.20050. Lets verify step by step. Hugo Mercier and Dan Sperber. 2011. Why do humans reason? arguments for an argumentative theory. Behavioral and brain sciences, 34(2):5774. MetaAI. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai. meta.com/blog/meta-llama-3/. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2023. Openwebmath: An open dataset of high-quality mathematical web text. Preprint, arXiv:2310.06786. Lance Rips. 1994. The psychology of proof: Deductive reasoning in human thinking. Mit Press. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761. 11 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems (NeurIPS). Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024. Quiet-star: Language models can teach themselves to think before speaking. Preprint, arXiv:2403.09629. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In ACL. Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, and Dongmei Zhang. 2022. Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems. Preprint, arXiv:2210.05075. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pre-training for language understanding. Preprint, arXiv:2004.09297. Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In ACL. Amos Tversky, Daniel Kahneman, and Paul Slovic. 1982. Judgment under uncertainty: Heuristics and biases. Cambridge. Robert Vacareanu, Anurag Pratik, Evangelia Spiliopoulou, Zheng Qi, Giovanni Paolini, Neha Anna John, Jie Ma, Yassine Benajiba, and Miguel Ballesteros. 2024. General purpose verification for chain of thought prompting. Preprint, arXiv:2405.00204. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An openended embodied agent with large language models. Preprint, arXiv:2305.16291. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS), 35:2482424837. Nathaniel Weir, Peter Clark, and Benjamin Van Durme. 2023. Nellie: neuro-symbolic inference engine for grounded, compositional, and explainable reasoning. Preprint, arXiv:2209.07662. Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei Jiang, Zhengping Jiang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen, Peter Clark, and Benjamin Van Durme. 2024. Enhancing systematic decompositional natural language inference using informal logic. Preprint, arXiv:2402.14798. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. An empirical analysis of compute-optimal inference for problem-solving with language models. Preprint, arXiv:2408.00724."
        },
        {
            "title": "A Prompts used for rationale sampling",
            "content": "In this section, we provide the prompts we used for rationale sampling on GSM8K (Figure 4), ECQA (Figure 5), and The Pile (Figure 6)."
        },
        {
            "title": "B Prompts used during inference",
            "content": "In this section, we provide the prompts used during inference time to encourage the agent model reason step by step for GSM8K (Figure 7) and ECQA (Figure 8). Note that the input to the agent model appends the last rationale generated by the agent model."
        },
        {
            "title": "C Examples of rationales generated at",
            "content": "inference time In this section, we provide rationales generated by RATIONALYST from the test set of MATH during inference time Figure 9, which was not part of the rationale sampling datasets, and observe that our model can still generate useful rationales that help to understand LLMs reasoning process. Prompts used for LLaMa-3 reranking In this section, we provide the prompts and incontext-learning demonstrations used to instruct LLaMa-3-8B-Instruct and GPT-4 to provide feedback by directly reranking partial reasoning traces given the question (Figure 10). 13 Figure 4: The prompt and in-context learning examples used for sampling rationales for GSM8K. The bolded rationales represent implicit rationales in the document. 14 Figure 5: The prompt and in-context learning examples used for sampling rationales for ECQA. The bolded rationales represent implicit rationales in the document. 15 Figure 6: The prompt and in-context learning examples used for sampling rationales for The Pile. The bolded rationales represent implicit rationales in the document. 16 Figure 7: The prompt and in-context-learning demonstrations used during inference time to encourage the agent model reason step by step on GSM8K. 17 Figure 8: The prompt and in-context-learning demonstrations used during inference time to encourage the agent model reason step by step on ECQA. 18 Figure 9: Rationales generated by RATIONALYST for the test set of MATH. 19 Figure 10: The prompt and in-context-learning demonstrations used during process supervision to elicit the feedback by directly reranking partial reasoning trajectory."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "University of Notre Dame"
    ]
}