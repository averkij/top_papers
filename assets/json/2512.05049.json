{
    "paper_title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "authors": [
        "Yu-Chao Hsu",
        "Jiun-Cheng Jiang",
        "Chun-Hua Lin",
        "Kuo-Chung Peng",
        "Nan-Yow Chen",
        "Samuel Yen-Chi Chen",
        "En-Jui Kuo",
        "Hsi-Sheng Goan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments."
        },
        {
            "title": "Start",
            "content": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin, Kuo-Chung Peng, Nan-Yow Chen, Samuel Yen-Chi Chen, En-Jui Kuo, Hsi-Sheng Goanx National Center for High-Performance Computing, National Institutes of Applied Research, Hsinchu, Taiwan Cross College Elite Program, National Cheng Kung University, Tainan, Taiwan Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei, Taiwan Center for Quantum Science and Engineering, National Taiwan University, Taipei, Taiwan Wells Fargo, New York, NY, USA Department of Electrophysics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan Physics Division, National Center for Theoretical Sciences, National Taiwan University, Taipei, Taiwan AbstractLong short-term memory (LSTM) models are particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired KolmogorovArnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the JiangHuangChenGoan Network (JHCG Net), which generalizes KAN to encoderdecoder structures, and then further use QKAN to realize the latent KAN, thereby creating Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments."
        },
        {
            "title": "Index",
            "content": "TermsQuantum Machine LSTM, mogorovArnold Networks, Forecasting, Hybrid Quantum-Classical Learning. Learning, KolTelecommunication I. INTRODUCTION Machine learning (ML) has achieved remarkable success across diverse domains, with recurrent neural networks (RNNs) and their gated variants, such as the long shortterm memory (LSTM) network [1], forming the backbone of sequence modeling and temporal prediction tasks [2][5]. Among these, LSTMs have demonstrated exceptional ability The views expressed in this article are those of the authors and do not represent the views of Wells Fargo. This article is for informational purposes only. Nothing contained in this article should be construed as investment advice. Wells Fargo makes no express or implied warranties and expressly disclaims all legal, tax, and accounting implications related to this article. ycchen1989@ieee.org. kuoenjui@nycu.edu.tw. goan@phys.ntu.edu.tw. to capture nonlinear temporal dynamics and long-range dependencies, making them indispensable tools in modeling complex spatiotemporal systems, including those in telecommunication networks [6][8]. In urban telecommunication systems, LSTMs are particularly valuable for forecasting user activity patterns and network loads from historical time series, where data often exhibit irregular periodicity, bursty behavior, and strong spatial-temporal correlations [8]. Accurate telecommunication forecasting is crucial for real-time network resource allocation, traffic optimization, and anomaly detection in largescale urban environments [9]. Despite their success, conventional LSTMs face inherent challenges related to vanishing gradients, high computational overhead, and overparameterization, which limit scalability and interpretability when applied to high-frequency, highdimensional telecommunication data. Moreover, the reliance on static activation functions constrains their representational richness, particularly when modeling complex oscillatory patterns and nonlinear feedback prevalent in communication signals. In parallel, quantum machine learning (QML) has emerged as promising paradigm that utilizes the principles of quantum mechanics, such as superposition, interference, and entanglement, to enhance functional expressivity and parameter efficiency [10][17]. Some QML methods, such as the quantum kernel method [18][23], have also been developed and explored. However, current QML implementations remain limited by noisy intermediate-scale quantum (NISQ) hardware, constrained qubit counts, and insufficient two-qubit gate fidelities [24][28], restricting their scalability in real-world applications such as telecommunication signal prediction. To bridge the gap between classical and quantum paradigms, recent efforts have focused on quantum-inspired architectures that retain the expressive power of quantum models while remaining executable on classical hardware. The quantuminspired KolmogorovArnold network (QKAN) [29] exemplifies this approach. QKAN reinterprets the KolmogorovArnold network (KAN) [30] by employing single-qubit data reuploading circuits [12], [31] as quantum variational acti5 2 0 2 ] - u [ 1 9 4 0 5 0 . 2 1 5 2 : r vation functions (QVAFs), effectively forming DatA ReUploading ActivatioN (DARUAN). Each DARUAN module encodes input features into parameterized rotations on singlequbit Bloch sphere, with trainable pre-processing weights in each data-uploading block, enabling an exponentially rich Fourier representation without the need for multi-qubit entanglement. The method of adding trainable pre-processing weights has also been employed for multi-qubits data reuploading circuits [32], [33]. This design allows QKANs to preserve quantum-level expressivity while remaining computationally tractable from single CPU to multi-nodes highperformance computing (HPC) GPU clusters. While QKANs remain feasible for current real quantum devices, state-ofthe-art quantum devices have already empirically achieved single-qubit gate error rates at the 105107 scale, including superconducting devices [26], spin qubit devices [34], and trapped-ion devices [28]. Integrating QKANs into recurrent architectures such as LSTMs introduces powerful new class of hybrid modelsQKAN-LSTMsthat unify the temporal modeling strength of LSTMs with the spectral expressivity of quantuminspired activations. In this framework, QKANs replace the classical feedforward layers within the LSTM cell, acting as adaptive, quantum-enhanced feature extractors and parameter compressors. Recent studies have demonstrated that such hybrid quantum-classical architecture improves sequence modeling efficiency and generalization across time-series forecasting language generation tasks [29], [35][37]. By and natural leveraging the compact harmonic representation of DARUANbased QKANs, QKAN-LSTMs achieve enhanced trainability, reduced parameter counts, and robustness against gradient degradationoffering scalable and physically interpretable pathway toward efficient sequential modeling. Beyond temporal sequence modeling, the KAN framework has also been generalized to hierarchical architectures through the JiangHuangChenGoan Network (JHCG Net) [29]. The JHCG Net extends the KAN paradigm into an encoderKANdecoder topology, where the latent KAN module serves as nonlinear feature processor within an autoencoder-like structure. When the latent processor is realized using QKANs, the resulting hybrid quantum-inspired KolmogorovArnold network (HQKAN) architecture integrates quantum-inspired transformations into the latent feature space, enabling exponential Fourier frequency spectral enrichment with less width and depth compared to classical KAN and MLP. Crucially, HQKANs function as scalable, drop-in replacements for multilayer perceptrons (MLPs) in deep architectures such as Transformers and Diffusion Models, maintaining classical differentiability and GPU compatibility while offering superior expressivity and efficiency. In this work, we extend the QKAN framework to sequential modeling by integrating it into the LSTM architecture, forming the QKAN-LSTM. We further position this model within the broader hybrid paradigm defined by the HQKAN, which generalizes QKANs into hierarchical encoderdecoder architectures for representation learning. Together, QKAN-LSTMs and HQKANs establish unified framework for quantum-inspired learning that spans both temporal modeling and hierarchical feature representation. We summarize our main contributions as follows: 1) We introduce novel QKAN-LSTM architecture that integrates quantum-inspired DARUAN modules within LSTM cells, replacing conventional affine transformations to enhance nonlinear expressivity and parameter efficiency. Furthermore, we extend this framework to its hybrid counterpart, HQKAN-LSTM, which embeds the JHCG Net mechanism for additional scalability and compression efficiency. 2) We achieve substantial 99.5% reduction in trainable parameters compared to classical LSTMs while maintaining or improving predictive performance across multiple datasets. 3) We evaluate the proposed models on three representative benchmarksdamped harmonic motion, Bessel function regression, and urban telecommunication forecastingdemonstrating superior accuracy, stability, and generalization compared to both LSTM and QLSTM baselines. II. RELATED WORK long short-term memory: a) Quantum-enhanced Ref. [38] presents fully quantum implementation of LSTM cells directly within quantum circuits. Quantum integrate LSTM cell with enhanced LSTM variants variational circuits [13], quantum kernels [20], [21] or quantum convolutional networks [39], while QuantumTrain LSTM replaces LSTM trainable parameters with quantum circuit outputs [40], [41]. Applications are also explored in telecommunication [8], weather prediction [17], [21], [41], cosmology [40], fraud detection [42], [43], solar power [44], stress monitoring [45] and indoor localization [46]. traffic b) Kolmogorov-Arnold network and its applications for time series forecasting: Liu et al. [30] introduced KANs, neural architecture inspired by the Kolmogorov-Arnold representation theorem (KART) [47]. Refs. [30], [48] generalized KART to arbitrary widths and depths and showed that KANs are able to replace MLPs with better accuracy and interpretability. Subsequent studies have applied KANs to time series forecasting tasks [49][56], confirming their effectiveness in modeling temporal data. III. METHODOLOGY A. Quantum-inspired Kolmogorov-Arnold Long Short-term Memory To further enhance the nonlinear modeling capability of LSTM networks, we propose the QKAN-LSTM model. This architecture replaces the conventional affine transformations in LSTM gates with quantum-inspired functional modules based on the KART. By constructing each gate as composition of multiple variational quantum subfunctions acting on individual input dimensions, the QKAN-LSTM approximates complex high-dimensional nonlinear mappings through structured aggregation of one-dimensional quantum transformations. This design enables stronger nonlinear expressivity and improved long-range dependency modeling in sequential data. As illustrated in Figure 1, this design enriches the expressive capacity of the recurrent dynamics and improves the modeling of long-range temporal dependencies in sequential data. 1) Classical LSTM Equations: The conventional LSTM cell consists of three primary gatesthe forget gate (ft), input gate (it), and output gate (ot)along with memory cell state (Ct). The evolution of these components over time is governed by the following equations: ft = σ(Wf [ht1, xt] + bf ) , it = σ(Wi[ht1, xt] + bi) , Ct = tanh(WC[ht1, xt] + bC) , Ct = ft Ct1 + it Ct, ot = σ(Wo[ht1, xt] + bo) , ht = ot tanh(Ct), (1a) (1b) (1c) (1d) (1e) (1f) where: xt Rn represents the n-dim input vector at time step t, ht1 Rm denotes the m-dim hidden state from the previous step, Wf , Wi, WC, Wo are learnable weight matrices associated with each gate, bf , bi, bC, bo are the corresponding bias vectors, σ() and tanh() are the sigmoid and hyperbolic tangent activation functions, respectively, denotes element-wise multiplication. 2) Integration of Quantum-inspired KolmogorovArnold Networks into LSTM: In QKAN-LSTM architecture, the conventional affine transformations [ht1, xt] + in the LSTM gates are replaced by QKAN modules. Instead of single linear mapping, each gate aggregates edge-wise quantum variational activation units, following the KolmogorovArnold principle to approximate high-dimensional nonlinear functions through sum of learnable one-dimensional mappings. This design enables each gate to process inputs through set of QVAFs, effectively enriching the nonlinear mapping space without altering the classical neuron structure. Consequently, the QKAN-LSTM exhibits enhanced expressive capacity and spectral diversity in its recurrent dynamics."
        },
        {
            "title": "Let the concatenated gate input vector be defined as",
            "content": "vt = [ht1; xt] Rd, = + m, where [ ; ] denotes vector concatenation. For each gate {f, i, C, o}, QKAN layer mapping is formulated as Φg(vt; Θg) = α (cid:88) p= ϕg,p (cid:0)vt; θg,p (cid:1), (2) where each quantum subnetwork ϕg,p(; θg,p) serves as trainable nonlinear activation function realized by DARUAN. The QVAF for each edge unit is defined as (cid:12)U (u; θ) (u; θ)(cid:12) ϕg,p(u; θ) = (cid:10)0(cid:12) (cid:12)0(cid:11), (3) where is fixed Hermitian observable (e.g., σz). The corresponding data re-uploading circuit (u; θ) consists of sequence of parameterized quantum blocks: (u; θ) = (ℓ+1) (cid:89) (cid:104) ℓ=1 (ℓ) exp(cid:0) a(ℓ)u+b(ℓ) 2 H(cid:1) (cid:105) . (4) Here denotes fixed Hermitian generator, a(ℓ) and b(ℓ) are scalar encoding parameters, and (ℓ)(θ) represents trainable single-qubit variational block. Stacking such reuploading layers with the help of a(ℓ) and b(ℓ) endows the activation ϕg,p with an exponentially enriched Fourier spectrum, enabling compact yet expressive nonlinear representations. In our work, we initialize the quantum state using Hadamard gate, preparing the system in uniform superposition. The DURAUN operation is instantiated as Sℓ(wℓu) = Rz(wℓu) = exp (cid:16) (cid:17) , σz wℓu 2 (5) which corresponds to Eq. (4) with = σz, a(ℓ) = wℓ, and b(ℓ) = 0. The trainable variational block is chosen as (cid:33) (cid:32) (cid:32) (cid:33) (ℓ)(θ) = exp σy exp σz , (6) θ(y) ℓ 2 θ(z) ℓ where σy is the Pauli-Y operator and σz is the Pauli-Z operator. Given the QKAN mappings Φg(vt; Θg), the LSTM gating dynamics remain structurally identical to the classical case: ft = σ(cid:0)Φf (vt; Θf )(cid:1), it = σ(cid:0)Φi(vt; Θi)(cid:1), Ct = tanh(cid:0)ΦC(vt; ΘC)(cid:1), Ct = ft Ct1 + it Ct, ot = σ(cid:0)Φo(vt; Θo)(cid:1), ht = ot tanh(Ct). (7c) (7d) (7e) (7f) (7a) (7b) Remarks. Eq. (2) follows the KolmogorovArnold formulation, where node outputs are additive compositions of quantum nonlinear edge functions. Eq. (3)(4) define the QVAF, implemented through data re-uploading circuits whose variational parameters θ are trained end-to-end. Classical nonlinearities σ() and tanh() are preserved for gating stability, while quantum activations enrich the inner functional space of each gate. trainable = parameter {θg,p, a(ℓ), b(ℓ)}, jointly optimized with other network weights. is Θg The full set 3) Training and Optimization: The training of QKANLSTM model involves optimizing both classical and quantum parameters that define the nonlinear mapping of each LSTM gate. Fig. 1. Overview of the QKAN-LSTM architecture. (a) The architecture of the QKAN-LSTM model with QKAN integration in the input, forget, cell, and output gates. (b) The data is fed into the DARUAN layer, where the quantum features are re-uploaded and processed. (c) detailed view of how the DARUANs are applied to the gates in QKAN to enhance the LSTMs ability to capture complex and non-linear sequence dependencies with QVAF. a) Loss Function: For the regression task, the training objective is to minimize the Mean Squared Error (MSE) between the predicted and target outputs: ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (yt ˆyt)2, t=1 (8) where yt and ˆyt denote the true and predicted values at time step t. b) Gradient Computation: The model parameters Θg = {αg,p, a(ℓ), b(ℓ), θg,p} are optimized through hybrid quantumclassical backpropagation. Gradients of the classical parameters αg,p are computed using standard backpropagation through time (BPTT), while the gradients of quantum parameters θg,p are obtained via the parameter-shift rule [57]: ϕg,p(u; θ) θk = (cid:104) 1 2 ϕg,p (cid:1) (cid:0)u; θk + π 2 ek (cid:0)u; θk π ϕg,p 2 ek (9) (cid:1) (cid:105) . where ek denotes the unit vector indicating the k-th parameter in θ, ensuring that only θk is shifted while all other parameters remain fixed. In our implementation, the QKAN modules operate in the exact solver mode, where all QVAF are represented by analytic, differentiable functions; therefore, gradients are computed directly using PyTorchs autograd [58]. However, when executed on real quantum hardware or simulated quantum backends, the parameter-shift rule Eq. (9) is employed to estimate the derivatives of quantum observables. c) Optimization Algorithm: optimizer such as Adam [59] or RMSprop [60] is employed to jointly update both classical and quantum parameters. At each iteration, the optimizer computes the gradient of the loss with respect to all elements of Θg, then updates them according to Θg Θg ηΘg L, where η is the learning rate. This hybrid optimization loop iteratively refines both the quantum variational circuits and classical combination weights until convergence. B. Jiang-Huang-Chen-Goan Networks Ref. [29] further introduced the concept of HQKANs, representing fusion of classical and quantum-inspired neural computation. Building upon this foundation, the JHCG Net generalizes the KAN paradigm into scalable, autoencoder-like framework designed for hierarchical representation learning. The architecture comprises three principal modules: fully connected encoder, latent KAN processor, and decoder. The encoder compresses high-dimensional input features into compact latent space, where the KAN module performs nonlinear transformations through parameterized univariate functions. The decoder subsequently reconstructs the output A. Training Data IV. RESULTS We evaluate the proposed QKAN-LSTM and HQKANLSTM model on three representative datasets: Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication time-series data for real-world testing. Fig. 3. Results of QKAN-LSTM on damped SHM and Bessel function datasets. a) Damped Simple Harmonic Motion: The Damped Simple Harmonic Motion (Damped SHM) dataset represents fundamental form of classical dynamics, describing oscillatory motion that exhibits an exponential decay in amplitude over time. It follows the canonical second-order differential equation (10) + ω2 0x(t) = 0, d2x(t) dt2 + 2ζω0 dx(t) dt where x(t) denotes the displacement at time t, ω = 2πf is the angular frequency of oscillation, and ζ = 2mk represents the damping ratio. We constructed time-series dataset of damped SHM systems, where each sample includes the temporal variable and the corresponding displacement x(t). This dataset serves as benchmark for evaluating the models capability to learn and predict harmonic oscillatory dynamics. b) Bessel Function: The Bessel Function dataset represents class of nonlinear oscillatory dynamics that frequently arises in the solutions of wave propagation and diffusion problems under cylindrical or spherical coordinate systems. It is governed by Bessels differential equation: x2 d2y(x) dx2 + dy(x) dx + (x2 α2)y(x) = 0, (11) where y(x) denotes the displacement (or amplitude) at position x, and α is the order of the Bessel function. The solution is given by the Bessel function of the first kind: Jα(x) = (cid:88) m=0 (1)m m! Γ(m + α + 1) (cid:16) (cid:17)2m+α , (12) Fig. 2. Architecture of the JiangHuangChenGoan Network (JHCG Net) [29]. The JHCG Net comprises fully connected encoder and decoder with KolmogorovArnold Network (KAN) serving as the latent feature processor, forming an autoencoder-like architecture. When the latent KAN module is implemented using Quantum KolmogorovArnold Networks (QKANs), the framework is referred to as the Hybrid QKAN (HQKAN), integrating quantum-inspired nonlinear transformations within the latent representation space. from the processed latent representation. By integrating the functional decomposition capability of KANs with the hierarchical abstraction mechanisms of deep neural networks, the JHCG Net achieves interpretable and parameter-efficient feature compression, transformation, and reconstruction. Replacing the latent KAN processor with QKAN yields the HQKAN architecture, which inherits quantum-inspired expressivity and frequency adaptability from the DARUAN activation framework while retaining the differentiability and scalability of classical optimization pipelines. The structural composition of HQKAN is illustrated in Figure 2, emphasizing its encoderKANdecoder topology and the inclusion of QKAN module within the latent layer. Unlike conventional QKANs that operate primarily as direct function approximators, HQKANs act as compositional operators embedded integrating quantum-inspired within classical architectures, nonlinear transformations into the latent feature space. Thus, this design enables an exponential enhancement in spectral capacity without increasing network width or depth, and facilitates high-fidelity, cross-modal representation learning with improved computational efficiency. Crucially, HQKANs function as scalable, drop-in substitute for MLPs within modern deep architectures such as Transformers and Diffusion Models [29]. They can replace conventional feed-forward layers in large-scale generative frameworks, providing superior expressivity, reduced parameterization, and enhanced convergence stability while maintaining compatibility with classical GPU-based training pipelines. Overall, the JHCG Net establishes cohesive bridge between classical and quantum-inspired learning paradigms. While KANs offer interpretable and smooth functional decomposition, QKANs introduce exponentially compact yet expressive nonlinear mappings. The resulting HQKAN architecture thereby defines unified, interpretable, and hardware-efficient foundation for scalable hybrid learning systems. TABLE NUMBER OF PARAMETERS IN LSTM, QLSTM, QKAN-LSTM AND HQKAN-LSTM MODELS. Dataset Damped SHM Bessel Function Urban Telecommunication LSTM Classical Quantum Total 166 166 277 166 166 - - - QLSTM Classical Quantum Total 6 6 5 72 72 100 78 78 105 QKAN-LSTM Classical Quantum Total 117 58 21 26 26 96 32 32 HQKAN-LSTM Classical Quantum Total 12 25 36 28 8 53 40 33 TABLE II PERFORMANCE COMPARISON ON DAMPED SHM DATASET. Model Epoch Training Loss Testing Loss LSTM QLSTM QKAN-LSTM HQKAN-LSTM 1 15 30 1 15 30 1 15 1 15 30 1.22 101 5.86 103 4.61 103 1.59 101 1.19 102 1.74 103 2.19 101 3.20 103 3.09 103 2.26 101 1.65 102 5.94 103 9.03 103 1.38 103 1.33 10 1.84 102 1.42 103 1.24 104 4.50 102 1.92 103 1.02 103 4.64 102 2.72 103 4.32 104 R2 0.7973 0.9690 0.9701 0.5855 0.9680 0. -0.0111 0.9569 0.9771 -0.0418 0.9391 0.9903 TABLE III PERFORMANCE COMPARISON ON BESSSEL FUNCTION DATASET. Model Epoch Training Loss Testing Loss LSTM QLSTM QKAN-LSTM HQKAN-LSTM 1 15 1 15 30 1 15 30 1 15 30 5.18 102 2.47 103 1.80 103 8.32 102 5.28 103 3.40 103 1.35 101 2.99 103 1.67 10 1.09 101 2.26 103 1.86 103 2.07 103 9.38 104 7.69 104 2.53 103 1.02 103 7.53 104 1.43 102 6.12 104 3.27 104 6.61 103 4.17 104 3.21 104 0.9115 0.9471 0.9673 0.8923 0.9566 0.9679 0.3872 0.9739 0.9861 0.7185 0.9822 0.9863 where Γ(x) is the Gamma function. In this work, we constructed time-series dataset based on the second-order Bessel function J2(x), where each sample includes the variable and its corresponding amplitude J2(x). c) Urban Telecommunication: The Urban Telecommunication dataset is derived from the Milan Telecommunication Activity Dataset [9], which records telecommunication activities sampled every 10 minutes across spatial grid of the city. To mitigate data sparsity and modality imbalance present in the original dataset, we focus exclusively on the univariate SMSin channel, which provides higher completeness and reliability across grid cells. Follwing Ref. [8], We preprocess the dataset by selecting grid cells that exhibit sufficient temporal continuity and normalize all SMS-in activity values to the interval [0, 1]. Each training instance is constructed from fixed-length input sequence = [xtT +1, . . . , xt], with the subsequent value xt+1 used as the prediction target. To assess the models capability of capturing temporal dependencies across different horizons, we experiment with multiple sequence lengths {4, 8, 12, 16, 32, 64}. The dataset is divided into 70% for training, 15% for validation, and 15% for testing. B. Experiment setup In our experiment, we evaluate the performance of LSTM, QLSTM, QKAN-LSTM, and HQKAN-LSTM models on three datasets: Damped SHM, Bessel Function, and Urban Telecommunication. Experiments are simulated with PennyLane [61], PyTorch [58], and QKAN adapted from the open-sourced library on GitHub [62]1. For implementations of LSTM and QLSTM, we followed the setup in ref. [8]. For training, the learning rate is set to 102 for the Damped SHM and Bessel Function datasets, and 103 for the Urban Telecommunication dataset, except for HQKAN-LSTM, which uses slightly higher rate of 2 103 to facilitate faster convergence. For the LSTM model, the hidden unit size is set to 5 for the Damped SHM and Bessel Function datasets, and 4 for the Urban Telecommunication dataset. Similarly, for the QLSTM model, the hidden unit size is set to 5 for the Damped SHM and Bessel Function datasets, and 4 for the Urban Telecommunication dataset. In contrast, for the QKAN-LSTM and HQKAN-LSTM model, the hidden unit size is set to 1 for both the Bessel Function and Urban Telecommunication datasets, and 2 for the Damped SHM dataset. The input and output dimensions for all models are both set to 1, as these hyperparameter configurations are chosen to balance model expressivity and parameter efficiency, while ensuring fair performance comparison across all architectures and properly reflecting their differences in representational capacity. Regarding the quantum components, the QLSTM model utilizes 6 qubits for Damped SHM and Bessel Function, and 5 qubits for Urban Telecommunication. The quantum gates are parameterized with RY encoding, defined as RY (x) = ei 2 σy , where σy is the Pauli-Y operator and denotes the input data. Following the encoding stage, the circuit applies repeated CNOT + trainable RY (θ) blocks as the RealAmplitudes Anstaz [8], [63] and performs Pauli-Z measurements to generate the quantum outputs. In contrast, both QKAN-LSTM and HQKAN-LSTM models employ single-qubit DARUAN layer across all datasets. The experiment on the Urban Telecommunication dataset, the models are trained for 50 epochs, with sequence lengths 1Available at https://github.com/Jim137/qkan. TABLE IV COMPARISON OF MAE / MSE FOR DIFFERENT MODELS ACROSS SEQUENCE LENGTHS ON URBAN TELECOMMUNICATION DATASET. The best result is highlighted in bold while the second is labeled with underline. Model Seq len 4 Seq len Seq len 12 Seq len 16 Seq len 32 Seq len 64 LSTM QLSTM QKAN-LSTM 1.0633 / 4.7135 1.0322 / 4.5217 1.0292 / 4.4377 HQKAN-LSTM 1.0045 / 4. 1.0757 / 4.7011 1.0324 / 4.5307 1.0399 / 4.5441 1.0249 / 4.6166 1.0799 / 4.6085 1.0466 / 4.5715 1.0443 / 4.5570 1.0361 / 4.5241 1.0914 / 4.7020 1.0456 / 4.6244 1.0418 / 4.5485 1.0189 / 4.5985 1.1211 / 4.8381 1.0634 / 4.5953 1.0534 / 4.5647 1.0378 / 4.4970 1.1597 / 4.8853 1.0933 / 4.7194 1.1103 / 4.7311 1.0848 / 4.6749 of {4, 8, 12, 16, 32, 64}, and our all experiments were carried out on system equipped with NVIDIA Tesla V100 GPUs (16GB) and Intel Xeon CPUs. C. Result Analysis Table summarizes the number of classical and quantum parameters across all models. Compared with QLSTM, both QKAN-LSTM and HQKAN-LSTM achieve notable reduction in quantum parameters on the Urban Telecommunication datasetapproximately 5070% fewerwhile utilizing the single qubit DARUAN layer, yet maintaining comparable or superior predictive performance. In addition, compared with LSTM, both QKAN-LSTM and HQKAN-LSTM exhibit substantial decrease in the total number of parameters on the same dataset. For the Damped SHM task, however, the relative simplicity of the sequence pattern limits the learning capacity of QKAN-LSTM when using single hidden unit. Therefore, two hidden units are adopted in this case to ensure sufficient expressive power, which leads to higher parameter count compared to the same model applied to other datasets. These results collectively highlight the parameter efficiency and scalability of the QKAN-based architecture, enabling effective sequence modeling with limited quantum resources. The evaluation on the Damped SHM and Bessel Function datasets is presented in Table II, Table III, and Figure 3, respectively. Across both datasets, all quantum-enhanced models exhibit steady convergence and high predictive accuracy as training progresses. In the Damped SHM task, QKAN-LSTM exhibits slower convergence during the early training stage, which can be attributed to the relatively simple oscillatory dynamics of the dataset. As training progresses, however, the model rapidly stabilizes and achieves superior accuracy, reflecting its adaptability even in low-complexity temporal patterns. As shown in Table II, QKAN-LSTM attains final testing loss of 1.02 103 and an R2 score of 0.9771 after 30 epochs, surpassing LSTM baselines. Similarly, HQKAN-LSTM achieves comparable convergence with an R2 of 0.9903, confirming the hybrid models ability to sustain high predictive accuracy while maintaining reduced number of quantum parameters. the Bessel Function dataset, QKAN-LSTM and HQKAN-LSTM demonstrate more robust and stable performance than LSTM and QLSTM, achieving testing losses of 3.27104 and 3.21104, respectively, with corresponding R2 scores exceeding 0.986, as shown in Table III."
        },
        {
            "title": "For",
            "content": "For the Urban Telecommunication dataset, Table IV presents the comparison of mean absolute error (MAE) and MSE values across different sequence lengths. Overall, QKANLSTM and HQKAN-LSTM exhibit consistently lower error metrics than LSTM and QLSTM, demonstrating superior stability and adaptability across varying temporal dependencies. Notably, from Table I, both QKAN-based models employ substantially fewer quantum parameters than QLSTM and significantly fewer classical parameters than LSTM, highlighting their efficiency in balancing representational capacity with computational economy. As the sequence length increases, both models maintain clear performance advantages, indicating scalability and the ability to capture long-range temporal correlations without significant degradation in accuracy. In particular, HQKAN-LSTM achieves the lowest MAE from short to long sequence lengths across the experiments, confirming the robustness of the hybrid quantumclassical design. Collectively, these results validate that the QKANbased architectures not only enhance predictive precision but also achieve efficient parameter utilization, demonstrating their practicality for complex real-world temporal datasets. V. CONCLUSION This study has demonstrated that incorporating the QKAN framework into LSTM architectures substantially enhances temporal sequence modeling capabilities across both synthetic and real-world datasets. The proposed QKAN-LSTM and HQKAN-LSTM models utilize the expressive power of quantum mappings to enrich classical recurrent representations, creating compact yet powerful feature encoding for temporal dependencies. This is achieved by enhancing frequency adaptability and enabling an exponentially enriched spectral representation without requiring complex multi-qubit entanglement. This integration allows the models to deliver higher predictive accuracy and faster convergence while requiring far fewer trainable parameters and quantum resources, fundamentally boosting computational efficiency and scalability. Looking ahead, the proposed QKAN-based LSTM models hold great potential to bridge the gap between classical and quantum computing paradigms, enabling efficient deployment in edge computing environments and resource-limited quantum devices. This advancement highlights the versatility and scalability of QKAN-enhanced architectures, positioning them as key solution for both large-scale, complex, and high-dimensional datasets and low-resource quantum machine learning implementations in real-world settings."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "Y.C. Hsu, J.-C. Jiang, C.-H. Lin and K.C. Peng thank the National Center for High-Performance Computing (NCHC), National Institutes of Applied Research (NIAR), Taiwan, for providing computational and storage resources supported by National Science and Technology Council (NSTC), Taiwan, under Grants No. NSTC 114-2119-M-007-013. H.-S. Goan acknowledges support from the NSTC, Taiwan, under Grants No. NSTC 113-2112-M-002-022-MY3, No. NSTC 113-2119M-002-021, No. 114-2119-M-002-018, No. NSTC 114-2119M-002-017-MY3, from the US Air Force Office of Scientific Research under Award Number FA2386-23-1-4052 and from the National Taiwan University under Grants No. NTUCC-114L8950, No. NTU-CC114L895004 and No. NTU-CC114L8517. H.-S. Goan is also grateful for the support of the Center for Advanced Computing and Imaging in Biomedicine (NTU-114L900702) through the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE), Taiwan, the support of Taiwan Semiconductor Research Institute (TSRI) through the Joint Developed Project (JDP) and the support from the Physics Division, National Center for Theoretical Sciences, Taiwan."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Computation, vol. 9, no. 8, pp. 17351780, 1997. [2] A. Graves, N. Jaitly, and A.-r. Mohamed, Hybrid speech recognition with deep bidirectional lstm, in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 273278, 2013. [3] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with neural networks, 2014. [4] M. August and X. Ni, Using recurrent neural networks to optimize dynamical decoupling for quantum memory, Phys. Rev. A, vol. 95, p. 012335, Jan 2017. [5] E. Flurin, L. Martin, S. Hacohen-Gourgy, and I. Siddiqi, Using recurrent neural network to reconstruct quantum dynamics of superconducting qubit from physical observations, Physical Review X, vol. 10, Jan. 2020. [6] K. Greff, R. K. Srivastava, J. Koutnik, B. R. Steunebrink, and J. Schmidhuber, Lstm: search space odyssey, IEEE Transactions on Neural Networks and Learning Systems, vol. 28, p. 22222232, Oct. 2017. [7] B. Lindemann et al., survey on long short-term memory networks for time series prediction, Procedia CIRP, vol. 99, pp. 650655, 2021. 14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020. [8] C.-S. Chen, S. Y.-C. Chen, and Y.-C. Tsai, Benchmarking quantum and classical sequential models for urban telecommunication forecasting, arXiv preprint arXiv:2508.04488, 2025. [9] G. Barlacchi, M. De Nadai, R. Larcher, A. Casella, C. Chitic, G. Torrisi, F. Antonelli, A. Vespignani, A. Pentland, and B. Lepri, multi-source dataset of urban life in the city of milan and the province of trentino, Scientific data, vol. 2, no. 1, pp. 115, 2015. [10] S. Y.-C. Chen et al., Variational quantum circuits for deep reinforcement learning, IEEE access, vol. 8, pp. 141007141024, 2020. [11] F. Phillipson, Quantum machine learning: Benefits and practical examples., in QANSWER, pp. 5156, 2020. [12] M. Schuld, R. Sweke, and J. J. Meyer, Effect of data encoding on the expressive power of variational quantum-machine-learning models, Physical Review A, vol. 103, Mar. 2021. [13] S. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang, Quantum long short-term memory, in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 86228626, IEEE, 2022. [14] W. M. Watkins, S. Y.-C. Chen, and S. Yoo, Quantum machine learning with differential privacy, Scientific Reports, vol. 13, no. 1, p. 2453, 2023. [15] Z. Qu, Y. Meng, G. Muhammad, and P. Tiwari, Qmfnd: quantum multimodal fusion-based fake news detection model for social media, Information Fusion, vol. 104, p. 102172, 2024. [16] Y.-A. Chen and K.-F. Chen, Jet discrimination with quantum complete graph neural network, Physical Review D, vol. 111, no. 1, p. 016020, 2025. [17] C.-Y. Liu et al., Quantum-enhanced parameter-efficient learning for typhoon trajectory forecasting, arXiv preprint arXiv:2505.09395, 2025. [18] K.-C. Chen et al., Quantum-enhanced support vector machine for largescale multi-class stellar classification, in International Conference on Intelligent Computing, pp. 155168, Springer, 2024. [19] Z.-L. Tsai et al., Learning the hierarchy of steering measurement settings of qubit-pair states with kernel-based quantum models, New Journal of Physics, vol. 27, no. 9, p. 094502, 2025. [20] Y.-C. Hsu et al., Quantum kernel-based long short-term memory, in 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pp. 15, IEEE, 2025. [21] Y.-C. Hsu et al., Quantum kernel-based long short-term memory for climate time-series forecasting, in 2025 International Conference on Quantum Communications, Networking, and Computing (QCNC), pp. 421426, IEEE, 2025. [22] C.-Y. Liu et al., Quantum relational knowledge distillation, arXiv preprint arXiv:2508.13054, 2025. [23] K.-C. Chen, T.-Y. Li, et al., Validating large-scale quantum machine learning: Efficient simulation of quantum support vector machines using tensor networks, Machine Learning: Science and Technology, vol. 6, no. 1, p. 015047, 2025. [24] C.-Y. Liu, E.-J. Kuo, C.-H. A. Lin, S. Chen, J. G. Young, Y.-J. Chang, and M.-H. Hsieh, Training classical neural networks by quantum machine learning, in 2024 IEEE International Conference on Quantum Computing and Engineering (QCE), vol. 2, pp. 3438, IEEE, 2024. [25] A. P. Singh et al., Experimental demonstration of high-fidelity virtual two-qubit gate, Physical Review Research, vol. 6, Mar. 2024. [26] D. A. Rower et al., Suppressing counter-rotating errors for fast singlequbit gates with fluxonium, PRX Quantum, vol. 5, p. 040342, Dec 2024. [27] C.-Y. Liu et al., Quantum-train: Rethinking hybrid quantum-classical machine learning in the model compression perspective, Quantum Machine Intelligence, vol. 7, no. 2, p. 80, 2025. [28] M. C. Smith, A. D. Leu, K. Miyanishi, M. F. Gely, and D. M. Lucas, Single-qubit gates with errors at the 107 level, Phys. Rev. Lett., vol. 134, p. 230601, Jun 2025. [29] J.-C. Jiang, M. Y.-C. Huang, T. Chen, and H.-S. Goan, Quantum variational activation functions empower Kolmogorov-Arnold networks, arXiv preprint arXiv:2509.14026, 2025. [30] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljacic, T. Y. Hou, and M. Tegmark, Kan: Kolmogorovarnold networks, in The Thirteenth International Conference on Learning Representations, 2025. [31] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, Quantum circuit learning, Physical Review A, vol. 98, Sept. 2018. [32] Z. Yu, H. Yao, M. Li, and X. Wang, Power and limitations of singlequbit native quantum neural networks, 2022. [33] J. Zhao, W. Qiao, P. Zhang, and H. Gao, Quantum implicit neural representations, arXiv preprint arXiv:2406.03873, 2024. [34] Y.-H. Wu et al., Simultaneous high-fidelity single-qubit gates in spin qubit array, 2025. [35] S. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang, Quantum long short-term memory, 2020. [36] Y.-C. Hsu et al., Federated quantum kernel-based long short-term memory for human activity recognition, arXiv preprint arXiv:2508.06078, 2025. [37] S. Y.-C. Chen and P. Tiwari, Quantum long short-term memory with differentiable architecture search, 2025. [38] A. Ceschini, A. Rosato, and M. Panella, Design of an lstm cell on quantum hardware, IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 69, no. 3, pp. 18221826, 2021. [39] Z. Xu, W. Yu, C. Zhang, and Y. Chen, Quantum convolutional long short-term memory based on variational quantum algorithms in the era of nisq, Information, vol. 15, no. 4, p. 175, 2024. [40] C.-Y. Liu, S. Y.-C. Chen, K.-C. Chen, W.-J. Huang, and Y.-J. Chang, Federated quantum-train long short-term memory for gravitational wave signal, in IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), pp. 16, 2025. [41] C.-H. A. Lin, C.-Y. Liu, and K.-C. Chen, Quantum-train long shortterm memory: Application on flood prediction problem, in 2024 IEEE International Conference on Quantum Computing and Engineering (QCE), vol. 02, pp. 268273, 2024. [42] R. Ubale, S. K. K., S. Deshpande, and G. T. Byrd, Toward practical quantum machine learning: novel hybrid quantum lstm for fraud detection, 2025. [43] K. Saini and S. Sharma, Edge cloud assisted quantum lstm-based framework for road traffic monitoring, International Journal of Intelligent Transportation Systems Research, vol. 22, no. 3, pp. 707719, 2024. [44] S. Z. Khan, N. Muzammil, S. Ghafoor, H. Khan, S. M. H. Zaidi, A. J. Aljohani, and I. Aziz, Quantum long short-term memory (qlstm) vs. classical lstm in time series forecasting: comparative study in solar power forecasting, Frontiers in Physics, vol. 12, Oct. 2024. [45] A. Padha and A. Sahoo, parametrized quantum lstm model for continuous stress monitoring, in 2022 9th International Conference on Computing for Sustainable Global Development (INDIACom), pp. 261 266, IEEE, 2022. [46] S. F. Chien et al., Applying hybrid quantum lstm for indoor localization based on rssi, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 131135, IEEE, 2024. [47] A. N. Kolmogorov, On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition, in Doklady Akademii Nauk, vol. 114, pp. 953 956, Russian Academy of Sciences, 1957. [48] Z. Liu, P. Ma, Y. Wang, W. Matusik, and M. Tegmark, Kan 2.0: Kolmogorov-arnold networks meet science, 2024. [49] C. J. Vaca-Rubio, L. Blanco, R. Pereira, and M. Caus, Kolmogorovarnold networks (kans) for time series analysis, 2024. [50] K. Xu, L. Chen, and S. Wang, Kolmogorov-arnold networks for time series: Bridging predictive power and interpretability, 2024. [51] R. Genet and H. Inzirillo, Tkan: Temporal kolmogorov-arnold networks, 2024. [52] R. Genet and H. Inzirillo, temporal kolmogorov-arnold transformer for time series forecasting, 2024. [53] Q. Gong, Y. Xue, et al., novel lstm-kan networks for demand forecasting in manufacturing, in 2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), pp. 468473, 2024. [54] Y. Yu et al., Lstm-kan: Revolutionizing indoor visible light localization with robust sequence learning, Big Data Mining and Analytics, vol. 8, no. 6, pp. 12451260, 2025. [55] S. Gong, W. Chen, X. Jing, and C. Wang, Research on photovoltaic power prediction method based on lstm-kan, in 2024 6th International Conference on Electronic Engineering and Informatics (EEI), pp. 479 482, IEEE, 2024. [56] J. Cui, C. Lv, and J. Du, Real-time structural health monitoring of steel structures using acoustic emission signals and kan-lstm deep learning framework, Engineering Structures, vol. 344, p. 121328, 2025. [57] D. Wierichs, J. Izaac, C. Wang, and C. Y.-Y. Lin, General parametershift rules for quantum gradients, Quantum, vol. 6, p. 677, 2022. [58] A. Paszke et al., Pytorch: An imperative style, high-performance deep learning library, 2019. [59] D. P. Kingma, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [60] T. Kurbiel and S. Khaleghian, Training of deep neural networks based on distance measures using rmsprop, arXiv preprint arXiv:1708.01911, 2017. [61] V. Bergholm et al., Pennylane: Automatic differentiation of hybrid quantum-classical computations, 2022. [62] J.-C. Jiang, QKAN: Quantum-inspired Kolmogorov-Arnold network, 2025. [63] H. Iwakiri and K. Kanno, On universality of hardware-efficient ansatzes, arXiv preprint arXiv:2511.03870, 2025."
        }
    ],
    "affiliations": [
        "Center for Quantum Science and Engineering, National Taiwan University, Taipei, Taiwan",
        "Cross College Elite Program, National Cheng Kung University, Tainan, Taiwan",
        "Department of Electrophysics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",
        "Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei, Taiwan",
        "National Center for High-Performance Computing, National Institutes of Applied Research, Hsinchu, Taiwan",
        "Physics Division, National Center for Theoretical Sciences, National Taiwan University, Taipei, Taiwan",
        "Wells Fargo, New York, NY, USA"
    ]
}