{
    "paper_title": "Efficient Medical VIE via Reinforcement Learning",
    "authors": [
        "Lijun Liu",
        "Ruiyang Li",
        "Zhaocheng Liu",
        "Chenglin Zhu",
        "Chong Li",
        "Jiehan Cheng",
        "Qiang Ju",
        "Jian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE."
        },
        {
            "title": "Start",
            "content": "Lijun Liu1,*, Ruiyang Li1,*, Zhaocheng Liu1, Chenglin Zhu1,2, Chong Li1,2, Jiehan Cheng1,3, Qiang Ju1, Jian Xie1 1Baichuan Inc. 2Peking University 3Renmin University of China Correspondence: lio.h.zen@gmail.com 5 2 0 2 J 6 1 ] . [ 1 3 6 3 3 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE."
        },
        {
            "title": "Introduction",
            "content": "Visual Information Extraction (VIE) (Wan et al., 2024; Kuang et al., 2023; Hong et al., 2022; Kim et al., 2022) aims to generate structured information, such as JSON, from unstructured document images. This capability is crucial for various medical applications such as report interpretation (Li et al., 2024) and online consultations (Liu et al., 2025b). The most common approach involves first applying Optical Character Recognition (OCR) (Feng et al., 2025; Poznanski et al., 2025; Wei et al., 2024) to extract text, followed by leveraging large language models (LLMs) to extract and organize the text into JSON structure. Additionally, end-to-end methods (Wan et al., 2024; Bai et al., 2025; Kuang et al., 2023; Kim et al., 2022) have emerged, including multimodal large models that directly output JSON from image inputs. However, VIE tasks are highly domain-specific, with each domain requiring customized schemas. (Park et al., 2019; Huang et al., 2019b) The keys and values within these schemas are often defined by intricate domain-specific details, posing significant challenges for applying general-purpose VIE models to specialized fields. This aspect fundamentally differentiates structured VIE from OCR. Moreover, the annotation cost for VIE tasks is relatively high. These challenges have resulted in suboptimal performance of existing methods in medical VIE scenarios. Given the nontrivial relationship between diverse image inputs and outputs conforming to predefined schemas, we argue that VIE models need reasoning capabilities (OpenAI, 2024b) to address these complexities. To mitigate the high annotation cost, we explore efficient training paradigms using only 100 annotated samples. Combining these two considerations, we adopt Reinforcement Learning with Verifiable Rewards (RLVR) (Guo et al., 2025; Team et al., 2025) to achieve efficient medical visual extraction. Specifically, our design within the RLVR framework focuses on three key aspects. First, we ensure diversity in the 100 image samples to make the dataset representative and varied. Second, we carefully design the reward mechanism by incorporating weighted combination of precision and recall, where precision reduces model hallucinations and recall ensures the model captures all the predefined fields of interest. Lastly, we adopt two sampling strategies: one requires each response to include all fields, with rewards calculated against the ground truth for all fields, while the other evaluates responses using random subset of fields from the total schema. By integrating these carefully designed components, we aim to establish an efficient and robust solution for medical VIE tasks. Based on our proposed method, we finetuned Qwen2.5-VL-7B to have VIE RLVR models. We evaluate the models on medical and general VIE tasks. Our VIE RLVR models achieve SOTA performance on F1, precision and recall metrics on medical VIE tasks, indicating the advantage of our proposed method. We chose four widely-considered general VIE tasks for further evaluation. On two tasks that are similar to the medical report dataset, our models highly outperforms Qwen2.5-VL-7B. Meanwhile on other two tasks that are much different with our medical report dataset, our models failed to outperforms the base model, revealing the significant gap between different VIE tasks. The comparison of VIE models with and without thinking process while training and inferring are also delivered. In our case studies one can see how model benefits from thinking in dealing with VIE tasks."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Visual Information Extraction Visual Information Extraction (VIE) converts unstructured document images into structured outputs (e.g., keyvalue pairs or JSON), supporting applications like receipt understanding, form parsing, and medical document analysis(Huang et al., 2022; Powalski et al., 2021; Appalaraju et al., 2021). Existing methods fall into two main types: two-stage approaches that apply OCR followed by language models for structural parsing(Xu et al., 2020b,a), and end-to-end models that directly generate outputs from images without OCR (Kim et al., 2022; Zhang et al., 2020). Though effective on low-complexity benchmarks such as FUNSD, SROIE, and CORD (Jaume et al., 2019; Huang et al., 2019a; Park et al., 2019; Cao et al., 2022; Wang et al., 2021a), these models often omit required fields, hallucinate content, and generalize poorly to unseen layoutsespecially under few-shot or domain-shift conditions. These issues are exacerbated in the medical domain(Ma et al., 2023; Zheng et al., 2022), where layouts vary widely and annotated data is scarce. While recent advances like layout-aware pretraining (Chen et al., 2022; Adnan et al., 2024; Luo et al., 2023), graph-based models (Yu et al., 2021), and schema-guided prompting (Wang et al.; Li et al., 2024; Yao et al., 2024) provide partial solutions, they often fall short of ensuring both structural completeness and semantic accuracy under low-resource constraints."
        },
        {
            "title": "2.2 Reinforcement Learning for MLLM Reasoning",
            "content": "Reinforcement Learning (RL) has emerged as pivotal research direction for enhancing the complex reasoning capabilities of LLMs (Guo et al., 2025; Jaech et al., 2024; Shao et al., 2024; Hui et al., 2024; Ying et al., 2024). OpenAI-o1 (Jaech et al., 2024) adopted Reinforcement Learning from Human Feedback (RLHF) during the fine-tuning process, significantly enhancing the models reasoning abilities and its alignment with human preferences. More recently, DeepSeek-R1 (Guo et al., 2025) employed GRPO (Shao et al., 2024), which, unlike traditional RL algorithms dependent on critic models, directly utilizes rule-based verifiable rewards to guide the models reasoning process. This approach has greatly simplified the training procedure and proven highly effective in improving reasoning capabilities. This trend is gradually extending to MLLMs to further enhance their visual reasoning abilities (Xu et al., 2024; Liu et al., 2025a; Yu et al., 2025a; Yang et al., 2025; Zhou et al., 2025). Studies such as Visual-RFT (Liu et al., 2025c) and VLM-R1 (Shen et al., 2025) have shown that for single-image visual grounding tasks, direct application of few-shot GRPO can achieve improvements surpassing supervised fine-tuning. The GoT-R1 (Duan et al., 2025) framework applies RL to enhance semantic spatial reasoning in visual generation. Vision-R1 (Huang et al., 2025) enhances multimodal mathematical reasoning capabilities by using DeepSeek-R1 to augment multimodal Chain of Thought (CoT) datasets and adopting step-by-step thought inhibition during GRPO training. In this research, we aim to extend this paradigm to the Medical VIE tasks mentioned earlier. Figure 1: Overview of our proposed method. sampling strategy is employed to randomly select key-value pairs from the annotated JSON data associated with each image, serving as the ground truth. The descriptions of these keys are incorporated into predefined prompt to generate queries. These queries, along with the corresponding images, are input into the model to produce responses. The JSON predictions are then extracted from the responses, and rewards are computed by comparing them with the ground truth."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Due to the intricate relationship between heterogeneous image inputs and outputs that adhere to predefined schemas, we argue that VIE models must possess reasoning abilities to effectively manage these complexities. In contrast to approaches that explicitly replicate intermediate reasoning steps, RLVR (Guo et al., 2025; Team et al., 2025) relies solely on outcome-driven feedback, facilitating scalable reinforcement learning across extensive task datasets. Group Relative Policy Optimization (GRPO) (Guo et al., 2025) is an efficient RL algorithm that eliminates the need for separate critic model. Given query q, GRPO samples group of outputs {o1, o2, . . . , oG} from the old policy πθold. These outputs are evaluated using reward functions to obtain individual rewards {r1, r2, . . . , rG}. The advantage is computed by normalizing the rewards within the group: ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) Then the policy is updated by optimizing the following objective: Ai = . (1) JGRPO(θ) = qD,{oi}G (cid:32) i=1πθold (cid:18) oi (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi where min φi,t(θ)Ai,t, clip(φi,t(θ), 1 ϵ, 1 + ϵ)Ai,t (cid:19) (cid:33) β DKL [πθπref] , (2) t= φi,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) . (3) Additionally, we adopt several key techniques from DAPO (Yu et al., 2025b), including Clip-Higher and Token-Level Policy Gradient Loss. With the introduction of the two, the objective function undergoes some slight modifications as follows: JGRPO(θ) = 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) i= t=1 qD,{oi}G (cid:32) i=1πθold (cid:18) min φi,t(θ)Ai,t, clip(φi,t(θ), 1 ϵlow, 1 + ϵhigh)Ai,t (cid:19) (cid:33) β DKL [πθπref] . (4) 3.2 Image Diversity We collected over 17,000 medical domain images along with their corresponding OCR ground truth. These images encompass diverse range of report types, including laboratory reports (e.g., blood, urine, and stool tests), diagnostic reports (e.g., endoscopy, electrocardiograms, ultrasounds, and CT scans), and pathological reports (e.g., biopsy analyses and tumor staging). Furthermore, the diversity of the images extends to factors such as shooting angles, creases in the reports, the presence of obstructions, handwritten elements (e.g., doctor signatures), and varying backgrounds in the photographs. From this dataset, we manually selected 100 images that exhibit high diversity across these dimensions. Using GPT-4o (OpenAI, 2024a), the OCR ground truth was converted into JSON format based on predefined medical schema (see Appendix A). The JSON outputs were then manually reviewed and corrected to produce the final JSON ground truth."
        },
        {
            "title": "3.3 Rule-based Reward Mechanism",
            "content": "We design rule-based reward function to optimize the models ability to generate JSON outputs by measuring similarity with the ground truth. The reward computation consists of the following steps: Format Score. We generally adopt the format of R1-Zero (Guo et al., 2025), which includes two think and answer . The format score rformat is 1 if both components meet the required components: specifications; otherwise, rformat is 0. JSON Preprocessing. Parse the JSON object from the answer and flatten it into non-nested key-value dictionary. Specifically, this involves traversing all leaf nodes in the JSON structure. Each leaf nodes key in the dictionary is formed by concatenating the keys along the path from the root to the leaf, and its corresponding value is the value of the leaf node. Given the model output ˆy and the ground truth y, the preprocessing step converts them into Sp and Sg, respectively. Matching Score. The similarity between Sp and Sg is measured through weighted combination of precision and recall. We define nmatched as the number of correctly matched key-value pairs between Sp and Sg. Accordingly, precision and recall are defined as nmatched , respectively. Therefore, the matching score is defined as: Sp and nmatched Sg rmatching = Sp + (1 α) nmatched Sg (cid:40) α nmatched if Sp > 0, else. (5) As shown in Figure 2, α serves as critical hyperparameter to balance precision and recall during optimization: When α equals 1, the reward function focuses solely on precision, allowing the model to achieve 100% precision by outputting just single perfectly matched key-value pair. when α equals 0, the reward function emphasizes recall, potentially causing the model to generate numerous hallucinated key-value pairs in an attempt to retrieve all key-value pairs from Sg. Final Reward Score. The reward for the i-th sample is calculated as the sum of the format score and the matching score, expressed as: ri = rformat + rmatching Comparison with SFT. Compared to SFT, our proposed reward function better accommodates the unordered nature of JSON data. Specifically, the unordered property of JSON allows single image to correspond to multiple ground truths. SFT uses cross-entropy loss on fixed JSON ground truths during training, which may lead to data ambiguity and affect model performance. (6) Figure 2: Impact of the hyperparameter α on response length when the Sampling Strategy is enabled. The semi-transparent and the solid lines indicate raw samples and the smoothed trend."
        },
        {
            "title": "3.4 Sampling Strategy",
            "content": "To assess the impact of query diversity on experimental outcomes, we employ two data construction strategies. The first strategy involves random sampling of keys corresponding to the JSON data of an image, thereby generating varied queries. The second strategy forgoes sampling, utilizing all keys, which results in all samples sharing same query. Observations from Figure 3 indicate that key sampling leads to shorter response lengths, as the number of keys decreases post-sampling, consequently shortening response lengths. Additionally, from the reward curve, it can be observed that key sampling results in faster reward growth, which we attribute to the reduced number of keys making the training task simpler and thus accelerating reward acquisition."
        },
        {
            "title": "4.1 VIE Metrics",
            "content": "Various metrics are used for evaluation, including the field-level precision, recall, F1 scores and treeedit-distance(TED) based accuracy as in (Kim et al., 2021). It is noted that TED based accuracy mainly reflects the correctness of trees topology. In practical VIE scenarios, we pay more attention to indicators such as F1 score, precision, and recall, which also reflect the correctness of extracted text information. TED based accuracy measures the degree of match between the models output and the ground truth by calculating the edit distance between two tree structures, and the tree edit distance is used to quantify discrepancies between the predicted and actual structures. Field-level precision is the proportion of correctly extracted fields among all predicted fields, defined as nmatched/Sp in Eq. 5. Field-level recall is the proportion of correctly extracted fields among all actual fields, defined as nmatched/Sg in Eq. 5. Field-level F1 score is the harmonic mean balancing precision and recall, measuring overall accuracy of field-specific extraction, respectively. Figure 3: Comparison of Reward and Response Length Trends Between Sampling and Non-Sampling Strategies During Training. The semi-transparent and the solid lines indicate raw samples and the smoothed trend."
        },
        {
            "title": "4.2 VIE Baselines",
            "content": "To compare our results with existing works, we introduce models with various types and different outputs. Pipeline models. The pipeline models for OCR task are always composed of layout recognizer and OCR models for plain text, math functions and tables, and the OCR results are collected and rearranged into markdown, HTML or LaTeX format. MinerU (Wang et al., 2024b) is widely used pipeline model for OCR tasks, it uses LayoutLMv3 (Huang et al., 2022) or DocLayout-YOLO (Zhao et al., 2024) for document layout detection, an YOLO-v8 model 1 for math function detection, UniMERNet (Wang et al., 2024a) for math function recognition, RapidTable 2, TableMaster (Authors, 2020) or StructEqTable (Xia et al., 2024) for table recognition, PaddleOCR (Authors, 2020) for plain text OCR and LayOutReader (Pang, 2024) for reading order analysis. When evaluating MinerU, we align the version and parameter settings with those in OmniDocBench (Ouyang et al., 2025). Marker (Wang et al., 2021b) integrates several open source models to parse document, and we align the version and parameter settings of Marker as in OmniDocBench. Expert models. GOT-OCR (Wei et al., 2024) is large multimodal model trained for document parsing, which firstly used multi-stage training strategy to train an end-to-end OCR model. General MLLMs. We include general purpose MLLMs such as GPT4o(OpenAI, 2024a), Qwen2.5VL-7B(Bai et al., 2025), Qwen2.5-VL-72B and InternVL-2.5-78B (Chen et al., 2025) as baselines. The usage and parameter settings of these models are aligned with those in OmniDocBench."
        },
        {
            "title": "4.3 Our VIE Models",
            "content": "In this subsection we first introduce our VIE models finetuned by SFT and RLVR methods, and we report the implementation details of training. VIE SFT models. We show the result of models trained by VIE SFT and compare them with the VIE RLVR finetuned models, in order to analyze the benefits given by RLVR instead of SFT in VIE tasks. JSON-SFT-100 model is finetuned with 100 high quality medical VIE data, model leans to extract key information by directly supervised finetuning. 1https://yolov8.com/ 2https://github.com/RapidAI/RapidTable OCR-SFT-17K model is finetuned with 17K medical report OCR data. The model parse images into markdown format, and we apply GPT4o to rearrange the markdown into JSON while evaluating its VIE performance. VIE RLVR models. Three VIE RLVR models are trained to evaluate our proposed method: RL-100 is trained on 100 high quality data, during training the model extract information of randomly sampled fields. Note that the images of the 100 training dataset is same to JSON-SFT-100. RL-100(w/o sample) is trained with similar schedule and 100 data, but during training the model is required to extract all the key information in the images. OCR-SFT-17K-RL-100 model has the same RL stage with RL-100, and it is additionally supervised finetuned with 17K high quality OCR data composed of in-service medical reports and their manually corrected ground truths. Our proposed method is implemented in pytorch. We use 32H20 96GB GPUs to train our model with batch size 1 and the AdamW optimizer. In the reinforce learning stage, the learning rate starts from 1e-6 and decays to 0 following the liner schedule. During the rollout process, we sample 8 responses for each input prompt, with the KL divergence coefficient β set to 0.04. 4.4 Comparisons on Medical VIE task Table 1: Performances on medical VIE task. The field-level precision, recall, F1 scores and TED based accuracies are reported. For each metric, we bold the best results and underline the second-best results. Note that all VIE SFT and VIE RLVR models are finetuned from Qwen2.5-VL-7B, and VIE RLVR shows the best precision, recall, F1 scores. # In the column named output, OCR means we use models to parse medical reports into markdown format and then apply GPT4o to extract JSON format answers, JSON means we prompt models to directly output with JSON format."
        },
        {
            "title": "Type\nPipeline",
            "content": "Model Name MinerU Marker GOT-OCR Expert Commercial TextIn GPT4o General InternVL-2.5-78B Qwen2.5-VL-72B Qwen2.5-VL-7B JSON-SFT-100 OCR-SFT-17K OCR-SFT-17K-RL-100 RL-100(w/o sample) RL-"
        },
        {
            "title": "VIE SFT",
            "content": "#Output OCR OCR OCR OCR JSON JSON JSON JSON JSON OCR JSON JSON JSON F1 63.16 56.89 55.12 70.49 64.55 71.15 52.29 67.61 66.69 66.94 76.23 77.10 77.81 Precision Recall TED Acc 59.07 56.12 49.54 69.82 63.57 70.82 42.91 70.23 69.71 71.08 76.28 76.55 75.88 64.01 58.69 62.14 77.28 65.71 74.42 65.14 54.54 55.21 57.08 72.72 69.64 68.49 67.85 57.68 62.12 71.18 65.56 71.49 66.92 65.18 63.91 63.25 76.18 77.67 79.85 Our medical evaluation dataset consists of 203 medical report images uploaded by users, covering CT, MRI, X-ray, physical examination reports, endoscopy, prescriptions, urine tests, electrocardiograms, medical records, pathology, diagnostic tests, medicine boxes, blood tests, and ultrasound. These images include screenshots and scans, exhibiting diverse clarity levels and aspect ratios. To generate ground truth, under the guidance of doctors, we selected important fields from these medical images, used GPT-4o to extract values of these fields from the images, and manually corrected the answers. Finally, we obtained 203 image-JSON pairs as our test dataset. The results of our models and baseline models are shown in Table 1. We find that VIE RLVR models outperform all other models on the medical VIE task. They achieve SOTA performance in F1, precision, and recall metrics. TextIn and InternVL-2.5-78B perform well on the TED accuracy metric. However, their other scores are relatively low. This indicates that these models can correctly extract the structure of medical reports but fail to parse text information accurately. It is important to note that F1 score, precision, and recall are more critical than TED accuracy. TED accuracy only evaluates the tree topology extracted by the model, whereas F1 score, precision, and recall also assess the text information on each tree node. These metrics hold greater value in practical applications. The substantial improvement of RLVR over SFT validates the effectiveness of our proposed reinforcement learning approach. Compared with SFT, RLVR is better suited to the unordered nature of JSON data, where single image can correspond to multiple ground truths. In contrast, SFT uses cross-entropy loss on fixed JSON ground truths during training, which may cause data ambiguity and degrade model performance. In addition, the model gains the ability to think and plan during the reinforcement learning process. This ability allows it to understand image structures more accurately and extract key textual information with higher precision. For different VIE RLVR models, we first note that RL-100 maintains 77.81 F1 score. It outperforms pipeline, expert, closed-source, general multimodal models, and VIE SFT models by nearly 10 points. This reveals that in domain-specific VIE tasks, leveraging high-quality small-scale datasets through RLVR enables significant performance gains for models. Meanwhile, RL-100(w/o sample) achieves SOTA performance in the recall metric. The model attempts to extract as many fields as possible, but extracting redundant fields leads to lower precision score. Furthermore, OCR-SFT-17K-RL-100 reaches higher TED accuracy. This means the model learns the tree topology of medical reports during the supervised finetuning stage. To observe the OCR ability gained in the SFT stage, refer to Appendix B. Our model, trained on the OCR task with the medical report dataset, outperforms several pipeline and OCR expert models. Its overall score is close to the SOTA model with 72B parameters, demonstrating the high quality of our medical report dataset."
        },
        {
            "title": "4.5 The Impact of Think",
            "content": "Table 2: Impact of model thinking on medical VIE task. The field-level precision, recall, F1 scores and TED based accuracies are reported. Note that both two models are finetuned from Qwen2.5-VL-7B, the only difference is the thinking process in training and inferring stages. Model Name RL-100 (w/o think) RL-100 (w/ think) F1 75.90 77."
        },
        {
            "title": "Precision Recall TED Accuracy",
            "content": "75.68 79.85 76.13 75.88 70.06 68.49 To analyze the impact of model thinking, we compare the RL-100 model which is required to think while training and inferring, to the RL-100 model without thinking in training and inferring stages. Their performance on the medical VIE dataset are reported in Table 2. For the more considerable metrics, i.e. F1 scores that better reflects the models VIE ability, model with thinking outperforms the model without thinking. In the thinking process, model understands the image better and plans to extract key information, thus thinking progress is important for MLLMs in VIE tasks. For case studies, refer to Appendix C."
        },
        {
            "title": "4.6 Analysis on General VIE tasks",
            "content": "We also evaluate the medical VIE models trained with RLVR on general VIE tasks in order to demonstrate that VIE tasks exhibit strong domain-specific characteristics, VIE tasks in different domains vary significantly, making it challenging for models to acquire strong general VIE capabilities through training on specific-domain VIE tasks. There are four widerly used VIE benchmarks: CORD(Park et al., 2019): The Consolidated Receipt Dataset (CORD) serves as public benchmark comprising 800 training, 100 validation, and 100 test receipt images. The textual content of these receipts is encoded in the Latin alphabet. The dataset features 30 unique fields, including menu name, quantity, total price, and others. Notably, the information exhibits complex structures, such as nested groups and hierarchical organizations, which bears resemblance to our medical VIE dataset. Table 3: Performances on general VIE tasks. Various widely used benchmarks are chosen to evaluate the models general VIE performance. The field-level F1 scores and TED based accuracies are reported. For each metric, we bold the best results and underline the second-best results. CORD FUNSD SROIE Ticket Model TED Acc F1 TED Acc TED Acc F1 TED Acc F1 Qwen2.5-VL-7B OCR-SFT-17K-RL-100 JSON-SFT-100 RL-100(w/o sample) RL-100 34.6 59.6 36.1 68.7 45. 39.8 46.8 40.8 49.2 48.5 12.2 32.6 14.5 34.0 32.5 12.2 32.7 14.5 30.6 29.4 80.7 78.4 80.4 79.3 78.2 67.0 66.4 66.4 65.0 61.7 80.4 83.7 72.8 87.5 80. 76.4 64.6 72.7 72.3 60.8 FUNSD(Jaume et al., 2019): FUNSD is dataset for form understanding in noisy scanned documents, with 199 real, fully annotated forms for tasks like text detection and layout analysis. There are four fields to extracted, named question answer header and others, and each field correspond to list of values, which are similar to our medical VIE dataset. SROIE(Huang et al., 2019a): SROIE is the most widely adopted dataset, significantly advancing the fields development. The dataset comprises scanned English printed receipts, with each image accompanied by comprehensive OCR annotations and values for four key text fields. Ticket(Guo et al., 2019): This public benchmark dataset comprises 1,500 training and 400 test images of Chinese train tickets. It includes eight fields, such as ticket number, departure station, and train number. The information structure is straightforward, with each key guaranteed to appear only once and the location of each field fixed. From Table 3 we find that on CORD dataset, the VIE RLVR models outperform the base model with nearly 10 points in F1 score and 34 points in TED accuracy, and on FUNSD dataset VIE RLVR models outperform the base model with nearly 20 points in F1 score and 22 points in TED accuracy. However the performance of VIE RLVR models are poor on Ticket and SROIE datasets. We note that images in CORD benchmark are receipts with 30 complex fields, having some sub-fields to extract, which is very similar to our medical VIE evaluation dataset, while the SROIE and Ticket datasets are composed of images with few information to extract. Additionally, the comparision of RL-100(w/o sample) with RL-100 in Table 3 is different from results in Table 1 , indicating significant gap exists among different VIE tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose novel method for training MLLMs for medical VIE tasks, leveraging the RLVR framework. Our method applies merely 100 high quality samples for training, with our proposed reward mechanism being able to reduce model hallucination as well as ensure models to capture all the key fields. Extensive experiments demonstrate the effectiveness of our VIE RLVR models. On medical report dataset, our models trained with merely 100 samples approach SOTA performance on the mainly considered metrics, such as F1, precision and recall, exceeding commercial models like GPT-4o for more than 10 points, showing the advantage of our proposed method. Results on different general VIE tasks also reveal the gap between different tasks. Studies considering different data sampling strategy and thinking process are also given, validating the high efficiency of our method."
        },
        {
            "title": "References",
            "content": "Wiam Adnan, Joel Tang, Yassine Bel Khayat Zouggari, Seif Edinne Laatiri, Laurent Lam, and Fabien Caspani. 2024. layoutlmv3-based model for enhanced relation extraction in visually-rich documents. In International Conference on Document Analysis and Recognition, pages 160174. Springer. Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and Manmatha. 2021. Docformer: Endto-end transformer for document understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9931003. PaddlePaddle Authors. 2020. Paddleocr, awesome multilingual ocr toolkits based on paddlepaddle. https: //github.com/PaddlePaddle/PaddleOCR. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Haoyu Cao, Xin Li, Jiefeng Ma, Deqiang Jiang, Antai Guo, Yiqing Hu, Hao Liu, Yinsong Liu, and Bo Ren. 2022. Query-driven generative network for document information extraction in the wild. In Proceedings of the 30th ACM International Conference on Multimedia, pages 42614271. Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. 2022. Xdoc: Unified pre-training for cross-format document understanding. arXiv preprint arXiv:2210.02849. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, and 23 others. 2025. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Preprint, arXiv:2412.05271. Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. 2025. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022. Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, and 1 others. 2025. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding. 2019. Eaten: Entity-aware attention for single shot visual text extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 254259. IEEE. Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park. 2022. Bros: pre-trained language model focusing on text and layout for better key information extraction from documents. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1076710775. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM international conference on multimedia, pages 40834091. Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C.V. Jawahar. 2019a. ICDAR 2019 competition on scanned receipt OCR and information extraction (SROIE). In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. 2019b. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Guillaume Jaume, Hazım Kemal Ekenel, and Jean-Philippe Thiran. 2019. FUNSD: dataset for form understanding in noisy scanned documents. arXiv preprint arXiv:1905.13538. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer. Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2021. Donut: Document understanding transformer without ocr. arXiv preprint arXiv:2111.15664, 7(15):2. Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang Bai. 2023. Visual information extraction in the wild: practical dataset and end-to-end solution. In International Conference on Document Analysis and Recognition, pages 3653. Springer. Yiming Li, Qiang Wei, Xinghan Chen, Jianfu Li, Cui Tao, and Hua Xu. 2024. Improving tabular data extraction in scanned laboratory reports using deep learning models. Journal of Biomedical Informatics, 159:104735. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. 2025a. Seg-zero: Reasoningchain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520. Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie. 2025b. Exploring the inquiry-diagnosis relationship with advanced patient simulators. arXiv preprint arXiv:2501.09484. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025c. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785. Chuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao. 2023. Geolayoutlm: Geometric pre-training for visual information extraction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70927101. Ming-Wei Ma, Xian-Shu Gao, Ze-Yu Zhang, Shi-Yu Shang, Ling Jin, Pei-Lin Liu, Feng Lv, Wei Ni, Yu-Chen Han, and Hui Zong. 2023. Extracting laboratory test information from paper-based reports. BMC Medical Informatics and Decision Making, 23(1):251. OpenAI. 2024a. GPT-4o system card. https://openai.com/index/gpt-4o-system-card/. OpenAI. 2024b. Openai o1 system card. https://openai.com/index/openai-o1-system-card/. Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, and 1 others. 2025. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848. Hantian Pang. 2024. Faster LayoutReader based on LayoutLMv3. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. 2019. Cord: consolidated receipt dataset for post-ocr parsing. In Document Intelligence Workshop at NeurIPS 2019. Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka. 2021. Going full-tilt boogie on document understanding with text-image-layout transformer. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 510, 2021, Proceedings, Part II 16, pages 732747. Springer. Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. 2025. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, and 1 others. 2025. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. 2024. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1564115653. Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. 2024a. Unimernet: universal network for real-world mathematical expression recognition. Preprint, arXiv:2404.15254. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, and 1 others. 2024b. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839. Fei Wang, Yuewen Zheng, Jingyi Wu, Qing Li, Pengfei Li, and Luxia Zhang. Chatschema: development and validation of pipeline for extracting structured nursing information with large multimodal models. Interdisciplinary Nursing Research, pages 101097. Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. 2021a. Towards robust visual information extraction in real world: new dataset and novel solution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 27382745. Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang, Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, and Guangming Shi. 2021b. Pgnet: Real-time arbitrarily-shaped text spotting with point gathering network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 27822790. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, and 1 others. 2024. General ocr theory: Towards ocr-2.0 via unified end-to-end model. Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie Wu, Hancheng Ye, Shiyang Feng, Bin Wang, Chao Xu, Conghui He, Pinlong Cai, Min Dou, Botian Shi, Sheng Zhou, Yongwei Wang, and 4 others. 2024. Docgenome: An open large-scale scientific document benchmark for training and testing multi-modal large language models. Preprint, arXiv:2406.11633. Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. 2024. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, and 1 others. 2020a. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint arXiv:2012.14740. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020b. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 11921200. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, and 1 others. 2025. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615. Minghong Yao, Liansheng Zhuang, Houqiang Li, and Jiuchang Wei. 2024. Learning label dependencies for visual information extraction. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 66156623. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, and 1 others. 2024. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, and 1 others. 2025a. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, and 1 others. 2025b. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao. 2021. Pick: processing key information extraction from documents using improved graph learning-convolutional networks. In 2020 25th International conference on pattern recognition (ICPR), pages 43634370. IEEE. Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020. Trie: end-to-end text reading and information extraction for document understanding. In Proceedings of the 28th ACM International Conference on Multimedia, pages 14131422. Zhiyuan Zhao, Hengrui Kang, Bin Wang, and Conghui He. 2024. Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception. Preprint, arXiv:2410.12628. Lianchi Zheng, Xiaoming Liu, Zhihui Sun, and Yuxiang He. 2022. Improving medical ocr information extraction with integrated bert and layoutxlm models. In China Health Information Processing Conference, pages 164173. Springer. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2025. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model, 2025. URL https://arxiv. org/abs/2503.05132."
        },
        {
            "title": "A Predefined Medical Schema",
            "content": "{ \"Name\": \"\", // Patient's name, output as empty if not available \"Gender\": \"\", // Patient's gender, output as empty if not available \"Age\": \"\", // Patient's age, output as empty if not available \"Examination Time\": \"\", // Time when the patient was examined, output as empty if not available \"Department\": \"\", // Department where the patient was examined, output as empty if not available \"Examination Name\": \"\", // Name of the examination performed on the patient, fill in if listed separately, output as empty if not available, do not extract from indicators or examination descriptions \"Examination Site\": \"\", // Site where the patient was examined, fill in if listed separately, output as empty if not available, do not extract from indicators or examination descriptions \"Indicators\": [ // Various indicators of the examination items for the patient, generally displayed in table { } \"Item Name\": \"\", \"Result\": \"\", \"Unit\": \"\", \"Reference Range\": \"\", \"Abnormal Mark\": \"\", \"Detection Method\": \"\", \"Result Status\": \"\", \"Clinical Indication\": \"\", \"Critical Value\": \"\" ], \"Examination Description\": \"\", // Description of the examination results, such as imaging findings, ultrasound findings, specimen descriptions, gross findings, microscopic findings, pathological descriptions, etc., output as empty if not available \"Diagnosis\": \"\", // Preliminary/clinical/pathological diagnosis of the examination results; and identify which type of diagnosis it is. Output as empty if not available \"Treatment Recommendations\": \"\", // Treatment recommendations given in the report, fill in if listed separately, output as empty if not available, do not extract from examination descriptions or diagnoses \"Sample Collection Time\": \"\", // Time when the examination sample was collected, output as empty if not available \"Others\": \"\" // Information in the report that affects the interpretation of the medical report but is not included in the above fields, output as empty if not available }"
        },
        {
            "title": "B Analysis on General OCR tasks",
            "content": "Table 4: Performances on general OCR task. Results of different models on OmniDocBench(Ouyang et al., 2025) are reported, the scores are normalized edit distances. For each metric, we bold the best results and underline the second-best results. Book Models 0.055 MinerU 0.074 Marker 0.131 Mathpix 0.111 GOT-OCR 0.734 Nougat 0.157 GPT4o 0.096 Qwen2-VL-72B 0.216 InternVL2-76B Qwen2.5-VL-7B 0.2225 0.1813 OCR-SFT-17K Slides Financial Textbook Exam Magazine Academic Notes 0.159 0.984 0.124 0.651 0.452 0.34 0.634 0.278 0.22 0.388 0.204 0.222 0.991 0.930 0.958 0.607 0.281 0.163 0.168 0.061 0.195 0.226 0.247 0.098 0.4176 0.1897 0.5704 0.1778 0.1612 0.0878 0.025 0.059 0.091 0.179 0.214 0.146 0.085 0.419 0.2419 0.1599 0.033 0.089 0.202 0.067 1.000 0.348 0.047 0.162 0.1094 0.0915 0.072 0.153 0.147 0.198 0.83 0.173 0.071 0.150 0.3580 0. 0.102 0.319 0.216 0.132 0.820 0.187 0.149 0.184 0.2545 0.0943 News Overall 0.171 0.206 0.274 0.192 0.3 0.69 0.267 0.771 0.806 0.871 0.316 0.751 0.179 0.676 0.3 0.903 0.3591 0.6975 0.1984 0.6334 In Tab 4 we compare our model OCR-SFT-17K trained for OCR task using our 17K medical report dataset, with other widely considered pipeline models, expert models, general MLLMs as in 3. On the overall metric our model outperform GPT4o and many other models, reached the second place and very close to the SOTA performance, which means the medical report dataset is high quality and high diversity."
        },
        {
            "title": "C Case study of model with and without thinking",
            "content": "We give study of two cases to analyze the effect of models thinking progress, refering to Tab 5 and Tab 6. Model Output F1 Precision Recall TED Acc RL-100 (w/ think) <think> 从图片中可以看到 这是一份肠镜检查报告报 告 中 没 有 明 确 提 到 患 者 的 姓名性别年龄检查时 间科室检查名称检查 部位等信息因此这些信 息无法从图片中提取报告 中主要描述了肠镜检查的过 程和结果包括检查部位 检查描述诊断等信息根 据这些信息可以构建出一 个符合要求的JSON格式的输 出 </think> <answer> json { \"姓名\": \"\", \"性别\": \"\", \"年龄\": \"\", \"检查时间\": \"\", \"科室\": \"\", \"检查名称\": \"\", \"检查部位\": \"\", \"指标\": [], \"检查描述\": \"静 脉麻醉持续吸氧心电监 护下肠镜检查患者左侧卧 位行肛门指诊进指5cm 未触及明确肿物退指指套 无 染 血 肠 道 准 备 一 般 中等黄色粪水残留肠腔充 分吸引后观察钩拉法循腔 插镜至回肠末段10cm进镜 顺利所见部分末段回肠黏 膜光整回盲瓣呈唇型阑 尾开口清晰所见结肠粘膜 光滑血管纹理清晰无充 血糜烂溃疡及新生物 直肠粘膜未见异常可见轻 度内痔\", \"诊断\": \"\", \"处理 建议\": \"\", \"样本采集时间\": \"\", \"其他\": \"\" } </answer> 1.0 1.0 1.0 1.0 RL-100 (w/o think) { \"姓名\": \"\", \"性别\": \"\", \"年 龄\": \"\", \"检查时间\": \"\", \"科 室\": \"\", \"检查名称\": \"\", \"检 查部位\": \"肠道\", \"指标\": [], \"检查描述\": \"钩拉法循腔插 镜至回肠末段10cm进镜顺 利 所 见 部 分 末 段 回 肠 黏 膜光整回盲瓣呈唇型阑 尾开口清晰所见结肠粘膜 光滑血管纹理清晰无充 血糜烂溃疡及新生物 直肠粘膜未见异常可见轻 度内痔\", \"诊断\": \"\", \"处理 建议\": \"\", \"样本采集时间\": \"\", \"其他\": \"肠道准备一般中 等黄色粪水残留肠腔充分 吸引后观察\" } 0.0 0.0 0.0 0. Table 5: Model performances of one case. Blue letters stands for good parts in models thinking and output contexts, and red letters stands for wrong contents given by model, revealing that thinking progress makes model do VIE tasks better. Model Output F1 Precision Recall TED Acc RL-100 (w/ think) <think> 从 图 片 内 容 来 看 这是一份超声检查报告报 告 中 没 有 明 确 提 到 患 者 的 姓 名 性 别 年 龄 检 查 时 间 科 室 检 查 名 称 检查部位样本采集时间等 信息因此这些信息无法 从图片中提取报告中主要 描述了超声检查的结果和诊 断报告中提到的检查结果 包括右肾轻度积水右输尿 管上段扩张中下段梗阻可 能和双肾结石报告中没 有单独列出的检查名称检 查部位指标检测方法 结 果 状 态 临 床 提 示 是 否为危急值等信息因此 这些信息也无法从图片中提 取 </think> <answer> { \"姓 名\": \"\", \"性 别\": \"\", \"年 龄\": \"\", \"检 查 时 间\": \"\", \"科 室\": \"\", \"检 查 名 称\": \"\", \"检 查 部 位\": \"\", \"指 标\": [], \"检 查 描 述\": \"夜 间 急 诊 检 查 右 肾 大小形态正常皮质回声 均匀集合系统分离6mm 盏 内 可 见 数 枚 强 回 声 团 最大为43mm后方无明显 声影输尿管上段扩张中 下段因气体干扰显示不清 左 肾 大 小 形 态 正 常 皮 质回声均匀集合系统未见 分离盏内可见数枚强回声 团最大为43mm后方无 明显声影输尿管无扩张 膀 胱 少 量 充 盈 前 列 腺 形 态大小正常回声均匀 内未见异常回声CDFI上 述器官未见异常血流信号\", \"诊断\": \"右肾轻度积水右 输尿管上段扩张中下段梗 阻可能建议复查双肾结 石\", \"处理建议\": \"\", \"样本采 集时间\": \"\", \"其他\": \"\" } </answer> 0.4 0.5 0.33 0.96 RL-100 (w/o think) { \"姓名\": \"\", \"性别\": \"\", \"年 龄\": \"\", \"检查时间\": \"夜间急 诊检查\", \"科室\": \"\", \"检查名 称\": \"\", \"检查部位\": \"\", \"指 标\": [], \"检 查 描 述\": \"右 肾 大小形态正常皮质回声 均匀集合系统分离6mm 盏 内 可 见 数 枚 强 回 声 团 最大为43mm后方无明显 声影输尿管上段扩张中 下段因气体干扰显示不清 左 肾 大 小 形 态 正 常 皮 质回声均匀集合系统未见 分离盏内可见数枚强回声 团最大为43mm后方无 明显声影输尿管无扩张 膀 胱 少 量 充 盈 前 列 腺 形 态大小正常回声均匀 内未见异常回声CDFI上 述器官未见异常血流信号\", \"诊断\": \"右肾轻度积水右 输尿管上段扩张中下段梗 阻可能建议复查双肾结 石\", \"处理建议\": \"双肾结石\", \"样本采集时间\": \"\", \"其他\": \"检 查 结 果 未 见 异 常 血 流 信 号\" } 0.0 0.0 0.0 0. Table 6: Model performances of one case. Blue letters stands for good parts in models thinking and output contexts, and red letters stands for wrong contents given by model, revealing that thinking progress makes model do VIE tasks better."
        }
    ],
    "affiliations": [
        "Baichuan Inc.",
        "Peking University",
        "Renmin University of China"
    ]
}