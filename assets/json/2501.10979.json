{
    "paper_title": "Control LLM: Controlled Evolution for Intelligence Retention in LLM",
    "authors": [
        "Haichao Wei",
        "Yunxiang Ren",
        "Zhoutong Fu",
        "Aman Lunia",
        "Yi-Lin Chen",
        "Alice Leung",
        "Ya Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is \\textit{catastrophic forgetting} (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose \\textbf{Control LLM}, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge. Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning ($+14.4\\%$ on Math-Hard) and coding performance ($+10\\%$ on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\\%$ on C-Eval, $+6.8\\%$ on CMMLU, and $+30.2\\%$ on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation ($<4.3\\% \\text{on MMLU}$) compared to $>35\\%$ in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products. To support further research, we release the training and evaluation code (\\url{https://github.com/linkedin/ControlLLM}) along with models trained on public datasets (\\url{ https://huggingface.co/ControlLLM}) to the community."
        },
        {
            "title": "Start",
            "content": "Control LLM: Controlled Evolution for Intelligence Retention in LLM Haichao Wei 1 Yunxiang Ren 1 Zhoutong Fu 1 Aman Lunia 1 Yi-Lin Chen 1 Alice Leung 1 Ya Xu 2 This is preprint version submitted to arXiv. 5 2 0 2 9 ] . [ 1 9 7 9 0 1 . 1 0 5 2 : r Abstract Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. key challenge in this domain is catastrophic forgetting (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose Control LLM, novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge. Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning (+14.4% on Math-Hard) and coding performance (+10% on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities (+10.6% on C-Eval, +6.8% on CMMLU, and +30.2% on CMMLU0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation (< 4.3%on MMLU) compared to > 35% in open-source Math and Coding models. This approach has been successfully deployed in LinkedIns GenAI-powered job seeker and Ads unit products. To support further research, we release the training and evaluation code (https://github. com/linkedin/ControlLLM) along with models trained on public datasets (https: //huggingface.co/ControlLLM) to the community. 1. Introduction Large language models (LLMs) achieve emergent capabilities by training on trillions of tokens, requiring 10231025 1 Figure 1. Comparison: Ours vs SOTA Llama-tuned models. FLOPs (Brown et al., 2020; Dubey et al., 2024). These computational demands make full retraining impractical for many. While pre-trained LLMs provide broad utility, they often lack domain-specific skills. Improving these models via continuous pre-training (CPT) offers practical alternative to full retraining. Post-training methodssuch as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and model distillationare further hindered by the limited availability of datasets and methodologies, highlighting the importance of continuous supervised fine-tuning (CSFT). primary challenge is catastrophic forgetting (CF), where newly acquired knowledge overwrites previously learned information (McCloskey and Cohen, 1989). In LLMs, CF is heightened by both large model size and the diverse tasks they must handle. Existing CF countermeasures include data mixing (Sun et al., 2019; Parmar et al., 2024), regularization (EWC (Kirkpatrick et al., 2017), LwF (Li and Hoiem, 2017)), and Parameter-Efficient Fine-Tuning (PEFT) methods (e.g., Control LLM: Controlled Evolution for Intelligence Retention in LLM Figure 2. [Result] Comparison of CF - our method vs others on open-source datasets: (left) OpenMath, (right) OpenCoder. Table 1. [Result] CSFT on mathematical(0-shot): Open-source models, various CPT approaches, and ControlLLM(Concat-Lerp-MSE). Model Math Original Capabilities MathHard Math GSM8K Avg. ARC GPQA MMLU MMLUP Avg. Llama3.1-8B-Instruct OpenMath2-Llama3.1 Full Param Tune Partial Param Tune Stack Expansion Hybrid Expansion Control LLM* 23.7 38.4 38.5 36.4 35.6 34. 38.1 50.9 64.1 63.7 61.4 61.0 61.1 62.7 85.6 90.3 90.2 89.0 90.8 90. 90.4 52.1 64.3 63.9 61.8 61.8 61.5 63.2 83.4 45.8 58.2 66.2 69.3 81. 79.7 29.9 1.3 1.1 6.0 18.8 25.9 25.2 72.4 4.5 7.3 25.7 61.8 67. 68.1 46.7 19.5 23.5 30.9 43.1 43.9 43.6 60.5 12.9 16.5 29.3 53.3 57. 57.2 Avg. 56.3 38.6 40.1 45.6 57.6 59.3 60.2 Table 2. [Result] CSFT on coding(0-shot). ControlLLM(Concat-Lerp-MSE). Abbr.(e.g. MBPP+, HE+, MMLUP) in Section 4.1.3. Model Coding Original Capabilities MBPP+ MBPPS HE+ HE Avg. ARC GPQA MMLU MMLUP Avg. Llama3.1-8B-Ins OpenCoder-8B-Ins Full Param Tune Partial Param Tune Stack Expansion Hybrid Expansion* Control LLM* 70.4 81.2 75.1 75.7 77.2 77.5 80.4 67.7 76.3 69.6 71.6 72.8 73. 75.9 66.5 78.0 71.3 74.4 73.2 76.2 74.4 70.7 82.3 76.8 79.3 78.7 82. 81.1 69.1 79.5 73.3 75.0 75.6 77.1 78.3 83.4 8.2 24.4 70.2 80.0 80. 82.5 29.9 25.4 21.9 28.1 26.3 32.6 29.7 72.4 37.4 43.0 60.7 66.6 68. 68.2 46.7 11.3 19.2 32.4 38.2 40.3 40.9 60.5 24.6 31.5 48.3 54.2 56. 56.3 Avg. 64.8 52.1 52.4 61.7 64.9 66.6 67.3 LoRA (Hu et al., 2021)), yet each has drawbacks. Data mixing is difficult when datasets are undisclosed, regularization can be computationally expensive at large scales, and PEFT may restrict learning capacity (Biderman et al., 2024). We propose Control LLM, method to mitigate CF and integrate new tasks effectively. It employs parallel pre-trained and expanded transformer blocks (Zhang et al., 2023), aligned via interpolation to balance knowledge retention and new-skill acquisition. Control LLM achieves learn more, forget less outcome, validated on mathematics (OpenMath2 (Toshniwal et al., 2024)), coding (OpenCoder (Infly, 2024c)), and multilingual tasks (Llama3-SynE (survivi, 2024)). Remarkably, it outperforms full-parameter tuning in both learning and CF mitigation for coding and multilingual tasks, while reducing data and compute requirements. This approach meets two key production needs: 1) preserving broad capabilities for real-world applications, and 2) enabling high-QPS, low-latency scenarios without large test-time scaling (Snell et al., 2024). Contributions of this paper: Propose Control LLM, an architecture that mitigates CF by integrating trainable blocks with frozen pre-trained Control LLM: Controlled Evolution for Intelligence Retention in LLM Table 3. [Result] CPT on Chinese. Control LLM(Concat-Dlerp). Abbr.: CEvalC(C-Eval-0shot-CoT), CMMLUC(CMMLU-0shot-CoT). Model Chinese Original Capabilities Avg. CEval CEvalC CMMLU CMMLUC Avg. BBH MMLU MMLUP Avg. Llama3.1-8B Llama-3-SynE Full Param Tune* Stack Expansion Concat-Lerp* Hybrid Expansion* Control LLM* 48.3 57.7 59.0 56.0 57.1 58. 57.0 12.8 22.3 40.2 32.7 34.8 44.7 44.7 51.1 57.1 60.2 55.2 57.0 57. 56.0 14.1 22.8 44.3 33.4 37.4 44.3 44.9 13.9 22.8 43.8 33.3 37.1 44. 44.8 65.2 61.9 64.8 62.3 64.4 65.1 68.2 65.4 64.0 64.9 65.6 64.6 65. 65.6 35.5 32.6 35.0 35.3 35.8 36.9 37.9 45.9 42.9 45.4 44.8 45.9 46. 48.5 29.9 32.9 44.6 39.1 41.5 45.6 46.7 transformer blocks, applicable to both CPT and CSFT. Highlight the role of hidden-state alignment in retaining prior knowledge while learning new tasks and propose method to achieve it. Demonstrate effectiveness across multilingual, mathematical, and coding tasks, achieving gains without degrading existing performance. The remainder of the paper is organized as follows: Section 2 surveys related work, Section 3 details Control LLM, Section 4 outlines experimental settings and results, Section 5 discusses implications, and concludes the paper with future directions. 2. Related Work Catastrophic forgetting (CF) describes how new knowledge can overwrite old knowledge in neural networks (McIn LLMs, CF is especially Closkey and Cohen, 1989). problematic during CPT and supervised fine-tuning, where newly introduced data risks erasing existing capabilities (Luo et al., 2023). Regularization-Based Methods: Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and its variants (Synaptic Intelligence (Zenke et al., 2017), Memory Aware Synapses (Aljundi et al., 2018)) constrain updates for parameters deemed critical to past tasks. Replay Methods: Experience Replay (Rolnick et al., 2019) and Generative Replay (Shin et al., 2017; Sun et al., 2019; Huang et al., 2024) reintroduce earlier tasks via real or synthetic data. Curated mixtures further reduce distribution drift (Parmar et al., 2024; Xi et al., 2024; Wu et al., 2024). Parameter Isolation Methods: Progressive Neural Networks (Rusu et al., 2016) freeze existing modules and append new ones. PackNet (Mallya and Lazebnik, 2018) prunes parameters for reuse. PEFT (e.g., LoRA (Hu et al., 2021), prefix-tuning (Li and Liang, 2021), LLaMA-Adapter (Zhang et al., 2023)) fine-tunes small subset of parameters, reducing computation but limiting learning capacity. Knowledge Distillation: Learning without Forgetting (LwF) (Li and Hoiem, 2017) and Selective Distillation (Yu et al., 2025) transfer knowledge from older or parallel teachers to preserve prior tasks. Architecture-Based Methods: Adapter modules (Houlsby et al., 2019) insert task-specific layers, leaving the original model intact. LLaMA Pro (Wu et al., 2024) and SOLAR (Kim et al., 2023) expand model depth through additional transformer blocks or Depth Up-Scaling (DUS). LlamaMoE (Qu et al., 2024) partitions FFN or Attention layers into sparse experts and trains top-k gate. Control LLM: Combining architecture-based approaches with parameter isolation, Control LLM aligns hidden states of parallel pre-trained and expanded blocks via interpolation. Unlike many PEFT methods, it does not curtail learning capacity or demand original training data. As result, it retains previously learned skills while accommodating new tasks. 3. Methodology This section introduces the Control LLM approach. We first motivate the need for hidden-state alignment (Section 3.1), then present our architecture (Section 3.2) and training procedure (Section 3.4). 3.1. Hidden-State Alignment in Transformer Layers Transformer models (Vaswani et al., 2017) process tokens through layers, producing hidden-states that encode learned representations. Preserving alignment in these hidden-states is critical to mitigating catastrophic forgetting (CF), where new data overwrites prior knowledge (Ramasesh et al., 2020). Figure 3 (more in Appendix B.2) demonstrates the impact of hidden-state alignment during Continuous Pre3 Control LLM: Controlled Evolution for Intelligence Retention in LLM Figure 3. [Why] Hidden State Alignment Comparison: Best Alignment(Control LLM) vs Worst Alignment(Full-Parameter Tuning). training (CPT) and Continuous Supervised Fine-Tuning (CSFT). Probing with sentences like king is to queen (Appendix B.1) reveals that well-aligned hidden states between [Expanded] (tuned) and [Pretrained] models preserve semantic relationships, improving downstream performance. In math tasks (OpenMath2 (NVIDIA, 2024)), model tuned with alignment improves MathHard accuracy by +14.4% and limited MMLU degradation to -4.3%. Conversely, misalignment caused severe performance drops (e.g., MMLU fell from 72.4% to 7.3%). Benchmark results show that alignment sustains MMLU performance over 150K training steps, whereas misaligned tuning leads to degradation within 40K steps. Similar benefits were observed in fine tuning coding (OpenCoder (Infly, 2024c)) and multilingual (Llama3-SynE (survivi, 2024)) tasks. 3.2. Control LLM Architecture As illustrated in Figure 4, Control LLM augments every (N 1)th transformer layer (with as hyperparameter) by creating two branches: 1. Pre-trained Transformer Block: The original, frozen block that retains established capabilities. 2. Expanded Transformer Block: trainable copy that acquires new knowledge or skills from additional data. An alignment mechanism fuses these branches outputs, integrating new representations with existing knowledge. 3.3. Alignment Mechanisms and Interpolation Key observations include: Strategies 1. Semantic Stability: Hidden states of analogous sentences (e.g., king:queen, man:woman) exhibit strong cosine and Euclidean similarity. We fuse {hpre-trained, hexpanded} through various interpolation methods, allowing to apply layer wise divergence loss to maintain consistent alignment: 2. Layer Clustering: Analogous sentences show high similarity within layers and distinct clustering across layers. Linear Interpolation (Lerp). scalar α [0, 1] mixes the hidden-states: 3. CF Mitigation: Aligned [Expanded] and [Pretrained] states reduce destructive interference and CF. These results demonstrate that coherent transformations across layers enable models to refine rather than overwrite prior knowledgea critical requirement for large, deep LLMs. The following sections explore how Control LLM exploits these insights to mitigate CF effectively. hcombined = (1 α)hpre-trained + α hexpanded. Dynamic Linear Interpolation (Dlerp). Here, α depends on concatenated outputs from both branches: α(x) = σ(cid:0)W [xpre-trained, xexpanded] + b(cid:1), allowing α to vary per token. Dlerp can address CF without explicit divergence loss. 4 Control LLM: Controlled Evolution for Intelligence Retention in LLM Figure 4. [How] Control LLM Architecture. (a) Expanded blocks added every 1 layers connect to frozen blocks via interpolators. (b) Interpolators align hidden-states to produce final representations. (c) Different interpolation strategies are explored. DlerpIn. pIn uses input hidden-states: Instead of conditioning α on layer outputs, Dlerα(hinput) = σ(cid:0)Win hinput + bin (cid:1). This leverages contextual clues before transformation. Progressive Linear Interpolation (Plerp). lateral layer (near-identity) transforms hpre-trained prior to blending: hlateral = Wlateral hpre-trained and hcombined = (1 α) hlateral + α hexpanded. Gradual updates let the expanded layer integrate new features while preserving original representations. 3.3.1. DIVERGENCE LOSS FOR ALIGNMENT. Mixture of Experts (MoE) Gating. Inspired by Mixture of Experts, gating network predicts probability distribution over pre-trained (hpre-trained) and expanded (hexpanded) representations using input hidden-states hinput, where [αpre-trained, αexpanded] = softmax(Wghinput + bg) and bg is bias vector. The top expert per token is selected via argmax: (cid:40) hcombined = hpre-trained, hexpanded, if pre-trained is chosen, if expanded is chosen. Unlike continuous blending (e.g., DlerpIn), this hard interpolation enforces discrete choice, which can amplify catastrophic forgetting (CF). divergence The {hpre-trained, hexpanded}, averaged over layers: penalizes loss drift between Ldivergence = 1 (cid:88) l= Ei,j (cid:2)α(l)(i, j) (h(l,i,j) pre-trained, h(l,i,j) expanded)(cid:3), where is the number of layers expanded, indexes over the batch, over sequence positions, and over layers. can be cosine, MSE, or attention divergence. Scaling by α(l) enables selective alignment for each layer. Empirically, MSE and cosine work well by constraining both direction and magnitude (details in Section 4.2.3). Control LLM: Controlled Evolution for Intelligence Retention in LLM 3.3.2. MODEL EXPANSION STRATEGIES Figure 5 illustrates three expansion strategies for each (N 1)th layer: Concat: Adds parallel side-car for dual-branch blending, initialized as copy of pretrained transformer block. Stack: Stacks copied new layers (adapted from Wu et al. (2024)) above the original, initialized by zeroing out specific projections(o proj, down proj) to approximate an identity mapping. Hybrid: Alternates between stack and concat. Ablation studies show that concat balances learning and retention best  (Table 5)  ; hence we adopt it as our default. 3.4. Training Procedure 1. Initialization: Duplicate pre-trained blocks to form expanded blocks, preserving initial alignment. 2. Data Preparation: Use task-specific datasets. CPT employs sequence packing (8K context), while CSFT uses up to 132K context, supervising response tokens. 3. Alignment Training: Figure 5. [How] Structure analysis: (concat) the default dual structure. (stack) stack the expanded block following LLaMA Pro. (hybrid) hybrid structure of concat and stack. RQ2: Does Control LLM generalize to broad range of tasks (multilingual, mathematical reasoning, coding, and out-of-domain)? RQ3: How does it perform relative to baseline methods and open-source state-of-the-art (SOTA) models from the same base? RQ4: Which Control LLM configurations yield the best performance and why? Benchmarks in Tables 1, 2, 3, and 5 address these questions across mathematical, coding, multilingual, and general capabilities. = Ltask + λ Ldivergence, 4.1. Experimental Setup where Ltask denotes cross-entropy, Ldivergence enforces hidden-state alignment, and λ is hyperparameter. 4. Optimization: Train only expanded layers and interpolator. Use AdamW (Loshchilov, 2017) with weight decay set to 10% of the learning rate, cosine scheduler (1000 warm-up, peak 5 105, min 1 105). For CPT, batch size is 8*64 globally; for CSFT, 32*8 for math, 12*8 for coding. 5. Model Selection: Train for 26 epochs, reserving 20K validation samples. Pick best-performing checkpoint on new tasks to ensure fairness across methods with varying convergence speeds. For coding tasks, pick the best from phase 1 to train phase 2. 4. Experiments We evaluate Control LLM under both Continuous Pretraining (CPT) and Continuous Supervised Fine-Tuning (CSFT), focusing on four research questions: Dataset Task Samples Tokens Llama3-SynE (-EN) OpenMath2 OpenCoder-SFT-Phase1 Coding OpenCoder-SFT-Phase2 Coding Multilingual 47M 35.8B Mathematics 13.27M 5.1B 4.02M 2.4B 0.43M 245.4M Table 4. [What] Summary of datasets used in experiments, categorized by task type, with sample and token counts. 4.1.1. DATASETS Table 4 summarizes the open-source datasets used to validate Control LLM. CPT (Llama3-SynE (survivi, 2024)): Combines English/Chinese corpora to enhance multilingual proficiency. To demonstrate CF mitigation without data replay, we omit English subsets (book en, encyclopedia en, qa forum en), retaining only Chinese, code, and math data (book cn, encyclopedia cn, qa forum cn, web cn, code en, math en, synthesis en). CSFT: RQ1: Can Control LLM effectively learn new tasks while mitigating CF in both CPT and CSFT? OpenMath2 (NVIDIA, 2024): 14M math samples, 6 Control LLM: Controlled Evolution for Intelligence Retention in LLM filtered sequences longer than 1024 Llama tokens to 13.27M. OpenCoder (Infly, 2024c): We use SFT-Phase1 and SFTPhase2 data exclusively for CSFT reducing training data from 2.5T to 2.6G tokens. 4.1.2. BASELINES We compare Control LLM to: Full Parameter Tuning: All parameters are trainable. Partial Parameter Tuning: Freezes all but every (N 1)th transformer layer. Stack Expansion: Following LLaMA Pro (Wu et al., 2024), new layers are stacked while original layers remain frozen (stack strategy). We also compare to open-source models trained on similar datasets and same base model: Llama-3-SynE (Wu et al., 2024) OpenMath2-Llama3.1-8B (NVIDIA, 2024) OpenCoder-8B-Instruct (Infly, 2024) 4.1.3. EVALUATION We evaluate our models on set of general benchmarks by implementing customized version of lm-evalharvness(Eleuther, 2024). Abbreviations (e.g., MBPP+, HE+) appear in Tables 13 for brevity. Math: Exact match accuracy of Math-0shot (Math), MathHard-0shot (MathH), and GSM8K (G8K). Coding: Pass@1 of MBPP-Sanitize-0shot (MBPPS), MBPP-Plus-0shot (MBPP+), HumanEval-Greedy (HE), and HumanEval-Plus-Greedy (HE+). Chinese: Evaluation includes C-Eval-0shot (CEval), CEval-0shot-CoT (CEvalC), CMMLU-0shot (CMMLU), and CMMLU-0shot-CoT (CMMLUC). CEvalC and CMMLUC specifically assess chain-of-thought instruction following in Chinese (details in Appendix C). Note: CEval uses the validation split, as test split ground truth is unavailable. Original Capabilities: CSFT: ARC Challenge-0shot (ARC), GPQA-0shot (GPQA), MMLU-0shot (MMLU), and MMLU Pro5shot (MMLUP). CPT: BBH-3shot (BBH), MMLU-5shot (MMLU), and MMLU Pro-5shot (MMLUP). CSFT benchmarks utilize maximum model length of 8192, while CPT employs 5192. Results are reported as sizeweighted averages within each task group, along with the overall mean of these group averages. 4.2. Results We report Control LLM in two settingsHybrid Expansion and Concat Expansion (referred to simply as Control LLM)across math, coding, and multilingual tasks in Tables 1, 2, and 3. 4.2.1. CONTINUOUS SUPERVISED FINE-TUNING (CSFT) Math  (Table 1)  . OpenMath2-Llama3.1-8B (NVIDIA, 2024) fine-tunes the Llama3.1-8B base with 13.27M math samples, showing strong task-specific gains. Control LLM (from Llama3.1-8B-Instruct) matches these gains while preserving Original Capabilities. OpenMath2 employs checkpoint averaging (+2%), but we report single-checkpoint results for reproducibility. Despite effectively learning math, both OpenMath2 and Full-Parameter Tuning degrade Original Capabilities (e.g., MMLU drops from 72.4% to < 10%), whereas Control LLM remains robust even after extended training (Appendix C). Stack Expansion (Wu et al., 2024) and Partial-Parameter Tuning mitigate catastrophic forgetting, but Control LLM-Hybrid outperforms these approaches. Our final reported results use Control LLMConcat. Coding  (Table 2)  . Control LLM rivals OpenCoder-8BInstruct (Infly, 2024) while using 1000 fewer tokens (2.6G vs. 2.5T) and no CPT, and it exceeds OpenCoder-8BInstruct on Original Capabilities. By contrast, OpenMath2Llama3.1 (CPT+SFT) obtains +10.3% coding improvement but reduces Original Capabilities from 60.5% to 24.6%. Even extensive hyper-parameter tuning for FullParameter Tuning yields only +4.2% while continually eroding prior knowledge. Stack Expansion and PartialParameter Tuning (8 blocks) alleviate forgetting, but Control LLM-Hybrid outperforms both and nearly matches Control LLM-Concat. We open-source both expansions. 4.2.2. CONTINUOUS PRE-TRAINING (CPT) Multilingual  (Table 3)  . By excluding English data (100B vs. 35.8G tokens), Control LLM rivals Llama-3-SynE (Wu et al., 2024), which relies on data mixture/replay. Moreover, Control LLM improves CEvalC and CMMLUC (CoT in Chinese) by +31.9% and +30.8%, respectively, while preserving Original Capabilities. It thus surpasses replay-based methods (Llama-3-SynE), Full-Parameter Tuning, Stack Expansion (Wu et al., 2024), and even the base model. Observations reveal: CPT shows less CF than CSFT, with Hybrid Expansion excelling in deeper architectures. Con7 Control LLM: Controlled Evolution for Intelligence Retention in LLM trol LLM not only mitigates forgetting and improves prior competencies for CPT. 4.2.3. ABLATION STUDIES Table 5. [Where] Ablation Study. Abbr. MathH(Math Hard), Cos(Cosine), G8K(GSM8K) Model Original Math Avg. MathH G8K GPQA MMLU Full-Param Lerp8 Lerp8-MSE Lerp8-Cos Dlerp8 Dlerp8-MSE Dlerp8-Cos DlerpIn8 Plerp8 MoE8 Lerp16-MSE Dlerp16 Dlerp32 38. 36.1 35.9 34.9 36.7 35.6 36.5 34.1 36.5 34.1 38.1 37.7 38.6 35.8 Lerp8MSE0 35.1 Lerp8MSE0* Lerp8MSE0*M 33.9 90.2 90.2 90.1 89.2 90.0 89.5 90. 87.6 90.9 89.7 90.4 91.1 91.4 90.6 89.3 89.1 1.1 18.5 28.4 29.5 28.8 19.9 26.1 25.7 9.2 20. 25.2 22.3 22.1 7.4 31.3 32.4 7.3 58.8 71.7 71.0 69.1 66.1 69.1 68.2 41.9 63.3 68.1 64.2 60. 53.4 72.4 72.2 34.3 50.9 56.5 56.2 56.2 52.8 55.5 53.9 44.6 51.9 55.5 53.8 53.3 46.8 57.0 56. Lerp8MSEα 32.6 87.3 29.8 71.8 55. Control LLM Architecture: Tables 13 compare stack, partial-parameter tuning, and our expansions. Control LLM-Hybrid and Control LLM-Concat (Section 3.3.2) surpass stack and partial tuning in mitigating CF. Interpolation Strategies: Table 5 assesses Lerp, Dlerp, DlerpIn, Plerp, and MoE on OpenMath2 (Toshniwal et al., 2024). Lerp/Dlerp balance CF mitigation and newtask performance best. DlerpIn excels at CF mitigation but learns fewer new skills, while Plerp does the inverse. DlerpIn outperforms MoE (hard interpolation) by soft methods. Divergence Loss: Removing divergence loss results in more forgetting (though still better than full-parameter tuning and stack expansion), emphasizing its importance. Dlerp remains robust without it, thanks to gradual, soft interpolation (Appendix D). MSE generally works well, constraining direction and magnitude. Lerp8-Cosine mitigates CF initially but deteriorates with prolonged training. full-parameter tuning in MathH; we open-source this configuration. Eight layers remain ideal for production due to the performancecost trade-off. Interpolation Mechanisms: Interpolation proves essential; merely aligning hidden-states (e.g., Table 5/Lerp8MSE0: α = 0 plus MSE) does not fully prevent CF. However, training with α = 0 and MSE, then inferring at α = 0.5 (Table 5/Lerp8MSE0*), effectively mitigates CF, confirming the synergy of alignment and interpolation. Merging pre-trained and expanded blocks (e.g., Lerp8MSE0*M) degrades performance, likely due to transformer nonlinearity, warranting further research. Fixed vs. Learnable α: For Lerp, fixed α = 0.5 provides good CF mitigation and faster convergence. Making α learnable slows convergence (Table 5/Lerp8MSEα), similar to Dlerps bias term. Consequently, all final results freeze α or relevant bias parameters. 5. Discussion, Future Work, and Conclusion Control LLM offers novel solution to mitigate catastrophic forgetting (CF) in large language models by employing interpolation-based alignment between pre-trained and expanded representations. It achieves robust balance between retaining prior knowledge and acquiring new skills, consistently outperforming baseline methods. The approach stabilizes fine-tuning, enabling smooth integration of new knowledge without degrading existing performance, while being robust to hyper-parameter sensitivity and avoiding loss spikes during training (Appendix A). As practical alternative to Mixture-of-Experts (MoE) architectures, Control LLM leverages continuous and learnable interpolation mechanism, eliminating hard gating and expert silos. This design enables effective hidden-state alignment and seamless knowledge integration, making it particularly well-suited for dynamic adaptation to evolving data distributions. Future Directions: Design efficient architectures for seamless model merging. Extend Control LLM to multimodal and reinforcement fine-tuning tasks. Explore theoretical foundations of hidden-state alignment and its role in generalization. Investigate advanced divergence loss formulations and adaptive weighting strategies. Number of Layers: Table 5 shows that expanding 16 or 32 layers (vs. 8) improves performance without severely impacting Original Capabilities. Sixteen layers match In summary, Control LLM enhances new-task performance while preserving prior capabilities, providing robust method for continuous learning in LLMs. 8 Control LLM: Controlled Evolution for Intelligence Retention in LLM"
        },
        {
            "title": "References",
            "content": "Aljundi, R., et al. (2018). Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pages 139154. Amini, A., et al. (2019). MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of NAACL-HLT, pages 2357 2367. Brown, T. B., et al. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems, 33, pages 18771901. Dubey, A., et al. (2024). The llama 3 herd of models. In arXiv preprint arXiv:2407.21783. Team, G., et al. (2024). Gemma 2: Improving open lanIn arXiv preprint guage models at practical size. arXiv:2408.00118. Chen, J., et al. (2024). Towards effective and efficient continual pre-training of large language models. In arXiv preprint arXiv:2407.18743. Toshniwal, S., et al. (2024). Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data. In arXiv preprint arXiv:2410.01560. Huang, S., et al. (2024). Opencoder: The open cookbook for top-tier code large language models. In arXiv preprint arXiv:2411.04905. Zhang, L., et al. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847. Snell, C., et al. (2024). Scaling LLM test-time compute optimally can be more effective than scaling model parameters. In arXiv preprint arXiv:2408.03314. Wu, C., et al. (2024). Llama Pro: Progressive Llama with block expansion. In arXiv preprint arXiv:2401.02415. Yu, Y.-C., et al. (2025). Select and distill: Selective dualteacher knowledge transfer for continual learning on In European Conference on vision-language models. Computer Vision, pages 219236. Springer. Qu, X., et al. (2024). LLaMA-MoE v2: Exploring sparsity of LLaMA from perspective of mixture-of-experts with post-training. In arXiv preprint arXiv:2411.15708. Kwon, W., et al. (2023). Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. Krell, M. M., et al. (2021). Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. In arXiv preprint arXiv:2107.02027. Li, X. L., and Liang, P. (2021). Prefix-tuning: Optimizing In arXiv preprint continuous prompts for generation. arXiv:2101.00190. Zhang, R., et al. (2023). Llama-adapter: Efficient finetuning of language models with zero-init attention. In arXiv preprint arXiv:2303.16199. Kim, D., et al. (2023). Solar 10.7 B: Scaling large language models with simple yet effective depth up-scaling. In arXiv preprint arXiv:2312.15166. Jiang, A. Q., et al. (2024). Mixtral of experts. In arXiv preprint arXiv:2401.04088. Chen, M., et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Parmar, J., et al. (2024). Reuse, dont retrain: recipe for continued pre-training of language models. In arXiv preprint arXiv:2407.07263. Mikolov, T., et al. (2013). Distributed representations of In Adwords and phrases and their compositionality. vances in Neural Information Processing Systems, 26. Xi, N., et al. (2024). practice of post-training on Llama-3 70B with optimal selection of additional language mixture ratio. In arXiv preprint arXiv:2409.06624. Hu, E. J., et al. (2021). Lora: Low-rank adaptation of large language models. In arXiv preprint arXiv:2106.09685. Cobbe, K., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Hendrycks, D., et al. (2020). Measuring massive arXiv preprint multitask language understanding. arXiv:2009.03300. Biderman, D., et al. (2024). Lora learns less and forgets less. In arXiv preprint arXiv:2405.09673. Huang, J., et al. (2024). Mitigating CF in large language models with self-synthesized rehearsal. In arXiv preprint arXiv:2403.01244. Houlsby, N., et al. (2019). Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pages 27902799. PMLR. Loshchilov, I. (2017). Decoupled weight decay regularization. In arXiv preprint arXiv:1711.05101. 9 Control LLM: Controlled Evolution for Intelligence Retention in LLM Kirkpatrick, J., et al. (2017). Overcoming CF in neural networks. Proceedings of the National Academy of Sciences, 114(13), 35213526. Shazeer, N., et al. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In arXiv preprint arXiv:1701.06538. Levesque, H., Davis, E., & Morgenstern, L. (2012). The In AAAI Spring SympoWinograd schema challenge. sium: Logical Formalizations of Commonsense Reasoning, pages 4653. Li, Z., & Hoiem, D. (2017). Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12), 29352947. Loshchilov, I., & Hutter, F. (2018). Decoupled weight decay regularization. In International Conference on Learning Representations. Lu, S., et al. (2021). CodeXGLUE: machine learning benchmark dataset for code understanding and generation. In Advances in Neural Information Processing Systems. Mallya, A., & Lazebnik, S. (2018). PackNet: Adding multiple tasks to single network by iterative pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 77657773. McCloskey, M., & Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, pages 109165. Academic Press. Paperno, D., et al. (2016). The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 15251534. Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 32. Radford, A., et al. (2019). Language models are unsupervised multitask learners. OpenAI Blog. Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang, Y. (2023). An empirical study of CF in large language models during continual fine-tuning. In arXiv preprint arXiv:2308.08747. Ramasesh, V., Dyer, E., & Raghu, M. (2020). Anatomy of CF: Hidden representations and task semantics. In International Conference on Learning Representations. Rolnick, D., et al. (2019). Experience replay for continual learning. In Advances in Neural Information Processing Systems, 32. Rusu, A. A., et al. (2016). Progressive neural networks. arXiv preprint arXiv:1606.04671. Shin, H., et al. (2017). Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, 30. Sun, C., et al. (2019). LaMOL: Language modeling for lifelong language learning. In International Conference on Learning Representations. Vaswani, A., et al. (2017). Attention is all you need. In Advances in Neural Information Processing Systems, 30. Wolf, T., et al. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845. Zenke, F., Poole, B., & Ganguli, S. (2017). Continual learning through synaptic intelligence. In International Conference on Machine Learning, pages 39873995. survivi. (2024). Llama-3-SynE: significantly enhanced version of Llama-3 with advanced scientific reasoning and Chinese language capabilities [Dataset]. Available at: https://huggingface.co/survivi/ Llama-3-SynE. Infly. (2024a). OPC-SFT-Stage1: Dataset for supervised fine-tuning in the initial stage [Dataset]. Available at: https://huggingface.co/datasets/ OpenCoder-LLM/opc-sft-stage1. Infly. (2024b). OPC-SFT-Stage2: Dataset for supervised fine-tuning in the second stage [Dataset]. Available at: https://huggingface.co/datasets/ OpenCoder-LLM/opc-sft-stage2. Huggingface. FineWeb: (2024). web-based fine-tuning [Dataset]. https://huggingface.co/datasets/ HuggingFaceFW/fineweb. dataset for Available at: NVIDIA. (2024). OpenMathInstruct-2: Accelerating AI for math with massive open-source instruction data [Dataset]. https://huggingface.co/ Available datasets/nvidia/OpenMathInstruct-2. at: Infly. (2024c). OpenCoder: The open cookbook for toptier code large language models [Dataset]. Available at: https://huggingface.co/OpenCoder-LLM. Meta. (2024a). Llama recipes repository [GitHub Available at: https://github.com/ Repo.]. meta-llama/llama-recipes. Control LLM: Controlled Evolution for Intelligence Retention in LLM Eleuther. (2024). LM Evaluation Harness [GitHub Available at: https://github.com/ Repo.]. EleutherAI/lm-evaluation-harness. Meta. (2024b). Llama 3.1 8B model [Model]. Available at: https://huggingface.co/meta-llama/ Llama-3.1-8B. Meta. (2024c). Llama 3.1 8B Instruct model [Model]. https://huggingface.co/ Available meta-llama/Llama-3.1-8B-Instruct. at: Meta. (2024d). Llama 3.1 model card [DocumenAvailable at: https://github.com/ tation]. meta-llama/llama-models/blob/main/ models/llama3_1/MODEL_CARD.md. Meta. (2024e). Llama-3.1-8B-Instruct-evals dataset. Available at: https://huggingface. co/datasets/meta-llama/Llama-3. 1-8B-Instruct-evals. NVIDIA. (2024). OpenMath2-Llama3.1-8B: Avail- [Model]. mathematical able at: https://huggingface.co/nvidia/ OpenMath2-Llama3.1-8B. language model Infly. (2024). OpenCoder-8B-Instruct: codingAvailable [Model]. https://huggingface.co/infly/ focused language model at: OpenCoder-8B-Instruct. Control LLM: Controlled Evolution for Intelligence Retention in LLM A. The effectiveness of Control LLM in addressing Catastrophic Forgetting Figure 6. [OpenMath + Llama-3.1-8B-Instruct] Benchmark comparison of training methods. (a) Full Parameter Tuning. (b) Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total). (c) Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals. (d) Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss. Figure 7. [OpenCoder SFT Phase2 + Llama-3.1-8B-Instruct] Benchmark comparison of training methods. (a) Full Parameter Tuning. (b) Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total). (c) Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals. (d) Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss. Benchmark results in Tables 1, 2, 3, and 5 demonstrate that Control LLM effectively mitigates catastrophic forgetting (CF) while enabling the learning of new skills across mathematical, coding, and multilingual tasks using task-specific data. These results reflect the performance of the best-performing checkpoints. To provide more granular view, we plot benchmark results at every 1K training steps, highlighting the learn more, forget less principle and comparing different fine-tuning methods. A.1. Mathematical Benchmarks Figure 6 illustrates the exact match accuracy for MathHard-0shot (representing new skills) and MMLU-0shot (representing original capabilities) during the fine-tuning of Llama-3.1-8B-Instruct (Meta, 2024c) on the OpenMath2 dataset (NVIDIA, 2024). Key observations include: Full Parameter Tuning: Rapidly improves MathHard accuracy from 23.7% to 38% within 170K steps. However, it causes severe degradation in Original Capabilities (MMLU drops below 15% from 72.4%) within 40K steps. Other tasks like GSM8K and ARC exhibit similar trends. Partial Parameter Tuning: Slows degradation of Original Capabilities, maintaining MMLU accuracy above 20% for 80K steps, but fails to sustain long-term performance. 12 Control LLM: Controlled Evolution for Intelligence Retention in LLM Stack Expansion: Further improves retention, maintaining MMLU accuracy above 50% for 240K steps while converging on MathHard accuracy. Control LLM (Concat-Lerp with MSE Loss): Maintains the best balance, preserving MMLU accuracy above 67% even after 240K steps, while achieving 38% MathHard accuracy. A.2. Coding Benchmarks Figure 7 presents results from fine-tuning on the smaller-scale OpenCoder-SFT-Phase2 dataset (245.4M tokens, compared to OpenMath2s 5.1B tokens). Key trends include: Full Parameter Tuning: Struggles to converge to optimal coding performance (MBPP-Plus improves only slightly from 70.4% to 73.6%) while degrading Original Capabilities (MMLU drops below 40%) within 30K steps. Similar trends are observed for HumanEval. Partial Parameter Tuning: Achieves moderate improvement, with MBPP-Plus reaching 76.5% accuracy and MMLU accuracy staying above 50% for 30K steps. Stack Expansion: Converges to 77.4% MBPP-Plus accuracy while preserving MMLU accuracy above 64%. Control LLM: Achieves the best performance, converging to 79.2% MBPP-Plus accuracy while maintaining MMLU accuracy above 66%. A.3. Notes on Coding Results The plotting experiment in Figure 7 exclusively uses OpenCoder-SFT-Phase2 data. In contrast, the results reported in Table 2 use both OpenCoder-SFT-Phase1 and Phase2 datasets in 2-phase SFT approach, achieving the optimal 80% MBPP-Plus performance. B. Hidden State Alignment B.1. Hidden State Probing To investigate the relationship between hidden-state alignment and model performance, we crafted probing sentences such as king is to queen and passed them through CSFT-trained model. Table 6 provides examples of the probe data used. Hidden states of the final tokens were projected layer-wise, labeled as [Expanded] for the expanded transformer block and [Pre-trained] for the original pre-trained block. Using PCA, we visualized these hidden states in 3D embedding space. Projections of nearest neighbors for specific pairs such as King-Queen, Swim-Swam, or Paris-France reveal that semantically related representations form tight clusters due to extensive pre-training on large-scale datasets. Category Gender Pairs Verb Tenses Capital-Country Country-Currency Singular-Plural Comparative Forms Entity-Product Language-Nationality Family Relations Other Relations Table 6. Categorized Probe Data with Compiled Sentences Analogous Word Pairs king:queen, man:woman, actor:actress, waiter:waitress, uncle:aunt, nephew:niece swim:swam, fly:flew, see:saw, go:went, walking:walked, swimming:swam Paris:France, Tokyo:Japan, Brasilia:Brazil, Ottawa:Canada Russia:ruble, Japan:yen, United States:dollar, United Kingdom:pound dog:dogs, cat:cats, mouse:mice, goose:geese, child:children, person:people good:better, cold:colder Apple:iPhone, Microsoft:Windows Spain:Spanish, Italy:Italian uncle:aunt, nephew:niece tree:forest, building:city Sentences(Examples) king is to queen swim is to swam Paris is to France Russia is to ruble mouse is to mice good is to better Apple is to iPhone Spain is to Spanish uncle is to aunt tree is to forest B.2. Hidden State Probing Result Figure 3 compares Best and Worst Alignment by examining the output hidden states across transformer layers for various categories of probe data. This highlights the correlation between catastrophic forgetting (CF) and hidden-state alignment, as reflected in the degradation of original capabilities. The following observations are derived from fine-tuning experiments: Control LLM: Controlled Evolution for Intelligence Retention in LLM (a) Pre-trained Model - Math Hard 0.237 - MMLU 0.724 (b) Lerp with MSE - Math Hard 0.360 - MMLU 0.716 (c) Dlerp without Divergence Loss - Math Hard 0.357 - MMLU 0.66 (d) Lerp with Cosine Alignment - Math Hard 0.362 - MMLU 0.54 (e) Lerp without Divergence Loss - Math Hard 0.359 - MMLU 0. (f) Full Parameter Training - Math Hard 0.368 - MMLU 0.07 Figure 8. [OpenMath2 + Llama-3.1-8B-Instruct] Comparison of various alignment strategies and their impact on model performance. (a) Pre-trained Model: Hidden states from [Pretrained] and [Expanded] blocks are identical for all layers. 1) Semantic Stability: Analogous sentences exhibit high similarity in both cosine and Euclidean distances. 2) Layer-wise Clustering: Distinct clusters are formed for each layer. (b) Control LLM (MSE Divergence Loss): Maintains both semantic stability and layer-wise clustering, exhibiting strong catastrophic forgetting (CF) mitigation. (c) Dlerp without Divergence Loss: Shows distances approximately 2x larger than (b), reducing alignment quality. (d) Lerp with Cosine Divergence Loss: Preserves layer-wise clustering but weakens semantic stability. Nearest neighbors for [Pretrained] drift in Euclidean distance, while cosine similarity is maintained. (e) Lerp without Divergence Loss: Semantic stability degrades further between [Pretrained] and [Expanded] blocks, particularly in cosine similarity. (f) Full Parameter Tuning: Semantic drift is observed between [Expanded] and [Pretrained], with no interpolation applied. 14 Control LLM: Controlled Evolution for Intelligence Retention in LLM B.3. Correlation Between Alignment and Catastrophic Forgetting Figure 8 presents the relationship between alignment quality and model performance across five fine-tuning experiments. Each experiment involved fine-tuning Llama-3.1-8B-Instruct on the OpenMath2 dataset. We analyze the final checkpoints for each model, which have converged on the Math-Hard benchmark. Key findings are: (a) Pre-trained Model: The pre-trained checkpoint serves as the baseline, where [Pretrained] and [Expanded] hidden states are identical across layers. Observations include: 1. Semantic Stability: Analogous sentences exhibit high similarity in both cosine and Euclidean distances. 2. Layer-wise Clustering: Hidden states form distinct clusters for each layer. The model retains original capabilities with no CF. (b) Control LLM (Lerp + MSE Divergence Loss): Both Semantic Stability and Layer-wise Clustering are preserved. Distances between [Pretrained] and [Expanded] are minimal, yielding strong CF mitigation. MMLU drops slightly from 0.724 to 0.716, demonstrating minimal forgetting. (c) Control LLM (Dlerp without Divergence Loss): Distances between [Pretrained] and [Expanded] are approximately 2x larger than (b). However, the [Pretrained] and [Expanded] representations of the same sentence in the same layer remain closely aligned. (d) Control LLM (Lerp + Cosine Divergence Loss): Layer-wise clustering is preserved, but Semantic Stability is weaker. Nearest neighbors of [Pretrained] drift slightly in Euclidean distance, although cosine similarity remains high. MMLU drops significantly to 0.54. (e) Control LLM (Lerp without Divergence Loss): Semantic Stability deteriorates further, with cosine similarity between [Pretrained] and [Expanded] dropping. [Pretrained] tags no longer appear in the top 20 nearest neighbors. MMLU falls to 0.41 but remains better aligned than (f) due to the interpolation strategy. (f) Full Parameter Tuning: Without interpolation, [Pretrained] and [Expanded] representations exhibit the worst Semantic Stability. Catastrophic Forgetting is most pronounced, with MMLU dropping to 0.07. Summary of Findings. These results demonstrate that alignment quality directly impacts the balance between learning new tasks and preserving prior knowledge. Control LLM configurations with interpolation mechanisms, particularly Lerp with MSE divergence loss, achieve the best trade-off, minimizing CF while ensuring strong performance on new tasks. C. Evaluation Details We evaluate Control LLM across multiple benchmarks. Table 7 lists the datasets, abbreviations, reference URLs to the data sources and the prompt template. 1. https://huggingface.co/datasets/lighteval/MATH 2. https://huggingface.co/datasets/lighteval/MATH-Hard 3. https://huggingface.co/datasets/openai/gsm8k 4. https://huggingface.co/datasets/google-research-datasets/mbpp/viewer/sanitized 5. https://huggingface.co/datasets/evalplus/mbppplus 6. https://huggingface.co/datasets/openai/openai_humaneval 7. https://huggingface.co/datasets/evalplus/humanevalplus 8. https://huggingface.co/datasets/ceval/ceval-exam 9. https://huggingface.co/datasets/haonan-li/cmmlu 10. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__arc_challenge__details 11. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__gpqa__details 12. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__mmlu__0_shot__cot__details 13. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__mmlu_pro__details 14. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-evals/viewer/Llama-3.1-8B-evals__bbh__details 15. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__math__details 16. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__math_hard__details 17. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__gsm8k__details 18. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__mbpp__details 19. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__mbpp_plus__details 20. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__human_eval__details 21. https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals/viewer/Llama-3.1-8B-Instruct-evals__human_eval_plus__details 22. https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/ceval/_default_ceval_yaml 23. https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/cmmlu/_default_template_yaml 15 Control LLM: Controlled Evolution for Intelligence Retention in LLM Table 7. Evaluation datasets, abbreviations, data source, and prompt templates. - input final prompts column Dataset (Abbreviation) Data Source Prompt Template Task Math Coding Chinese Math-0shot (Math) Math-Hard-0shot (MathH) GSM8K (G8K) MBPP-Sanitize-0shot (MBPPS) MBPP-Plus-0shot (MBPP+) HumanEval-Greedy (HE) HumanEval-Plus-Greedy (HE+) C-Eval-0shot (CEval) C-Eval-0shot-CoT (CEvalC) CMMLU-0shot (CMMLU) CMMLU-0shot-CoT (CMMLUC) [1] [2] [3] [4] [5] [6] [7] [8] [8] [9] [9] [10] [11] [12] [13] [14] [12] [13] [15]* [16]* [17]* [18]* [19]* [20]* [21]* [22]* C.1 [23]* C.1 [10]* [11]* [12]* [13]* [14]* [12]* [13]* Original Capabilities-CSFT ARC Challenge-0shot (ARC) Original Capabilities-CPT GPQA-0shot (GPQA) MMLU-0shot (MMLU) MMLU Pro-5shot (MMLUP) BBH-3shot (BBH) MMLU-5shot (MMLU) MMLU Pro-5shot (MMLUP) C.1. Chain-of-Thought Evaluation for Chinese Benchmarks For CEvalC and CMMLUC, chain-of-thought instruction following is specifically assessed in Chinese. These benchmarks utilize the same datasets as C-Eval and CMMLU, with the prompt in Table 8. Table 8. Chain-of-Thought Prompt for Chinese Benchmarks - CEvalC and CMMLUC 在以下问题和四个候选答案ABC 和 D中选择最佳答案 问题... A. ... B. ... C. ... D. ... 对于简单的问题尽量简洁地解释, 并提供答案对于复杂的问题使用以下逐步格式 步骤 1: [简明描述] [简要解释] 步骤 2: [简明描述] [简要解释] 无论采用哪种方法始终以以下内容结束 最佳答案是 [答案字母][答题结束] 其中 [答案字母] 是 ABC 或 中的一个 让我们一步一步思考 This prompt ensures structured reasoning for complex problems and concise answers for simpler ones, promoting consistent evaluation of chain-of-thought capabilities in Chinese tasks. D. Ablation Study This section examines the impact of various settings in mitigating Catastrophic Forgetting (CF) and learning new tasks through an ablation study. Figures 9 and 10 present benchmark results recorded at every 1K steps during training. D.1. Ablation Study on Mathematical Tasks Figure 9 shows results from tuning Llama-3.1-8B-Instruct on the OpenMath2 dataset. Key findings for different configurations include: 16 Control LLM: Controlled Evolution for Intelligence Retention in LLM Figure 9. [OpenMath2 + Llama-3.1-8B-Instruct] Comparison of benchmark results of different ablation study settings every 1K steps during training. (a) Lerp Interporation Strategy with MSE loss. (b) Lerp Interporation Strategy without Divergence Loss. (c) Dlerp Interporation Strategy with MSE loss. (d) Dlerp Interporation Strategy without Divergence Loss. (e) Hybrid Expansion Strategy. (e) Plerp Interporation Strategy. (f) DlerpIn Interporation Strategy. (e) MoE gating. (a) Lerp + MSE (Concat Expansion): Using 8 transformer blocks (1 every 4 layers) and MSE divergence loss, this configuration maintains MMLU above 67%, demonstrating strong CF mitigation. Compared to (b) Lerp without divergence loss, MMLU drops below 50%, highlighting the importance of MSE in mitigating CF. (c) Dlerp + MSE: This setting achieves comparable CF mitigation, keeping MMLU above 65%. Although performance fluctuates due to dynamic interpolation, checkpoints can be easily selected to optimize both new tasks (e.g., Math-Hard) and Original Capabilities (e.g., MMLU). (d) Dlerp without Divergence Loss: Without MSE, performance is less stable, and MMLU drops below the levels observed in (c). (e) Hybrid Expansion Strategy: Similar to (a) and (c), this strategy delivers strong results for both new tasks and CF mitigation. (f) Plerp vs. (g) DlerpIn: Plerp excels at learning new tasks but performs poorly in CF mitigation. Conversely, DlerpIn outperforms Plerp in preserving Original Capabilities, maintaining MMLU above 68% after 234K steps. (h) MoE Gating: Although effective in CF mitigation, MoEs MMLU drops to 63% after 160K steps, underperforming DlerpIn. D.2. Ablation Study on Coding Tasks Figure 10 presents the results of tuning Llama-3.1-8B-Instruct on the OpenCoder-SFT-Phase2 dataset. This dataset is smaller in scale (245.4M tokens) compared to OpenMath2 (5.1B tokens). Key observations include: (a) Lerp + MSE: Achieves performance comparable to Dlerp-Cosine (Figure 7-(d)). MMLU remains above 66% even after 90K steps, showing strong CF mitigation. 17 Control LLM: Controlled Evolution for Intelligence Retention in LLM Figure 10. [OpenCoder + Llama-3.1-8B-Instruct] Comparison of benchmark results of different ablation study settings during training. (a) Lerp Interporation Strategy with MSE Loss. (b) Lerp Interporation Strategy with MSE Loss. Trained 3X longer than (a) (c) Hybrid Expansion Strategy. (d) MoE gating. (b) Extended Training: Training 3x longer demonstrates that MMLU stability persists, reinforcing the robustness of the Lerp + MSE configuration. (c) Hybrid Expansion: Delivers solid performance in both learning new tasks and mitigating CF. (d) MoE Gating: While effective for CF mitigation, MoE underperforms in learning new tasks compared to other proposed methods. D.3. Summary of Findings This ablation study underscores the importance of Control LLMs architecture, including its expansion strategies, interpolation techniques, and divergence loss. Key insights are: Expansion Strategies: Concat and Hybrid strategies outperform Stack, as they enable interpolation and make divergence loss effective. Stack is still superior to full parameter tuning, which is widely used. Interpolation Methods: Lerp and Dlerp outperform MoE, method commonly adopted by open-source models. Even DlerpIn, soft version of MoE, proves more effective in mitigating CF. Divergence Loss: The inclusion of divergence loss, particularly MSE, significantly enhances CF mitigation while maintaining stability during training. E. Acknowledgments We thank the teams of Llama3(Dubey et al., 2024), Llama3-SynE(Chen et al., 2024), OpenMathInstruct-2(Toshniwal et al., 2024), OpenCoder(Huang et al., 2024), llama-recipes(Meta, 2024a) and lm-eval-harvness(Eleuther, 2024) for pre-trained model, datasets, tools and resources that contributed to this research."
        }
    ],
    "affiliations": [
        "LinkedIn"
    ]
}