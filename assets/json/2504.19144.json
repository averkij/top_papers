{
    "paper_title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development",
    "authors": [
        "Bowei Wang",
        "Jiaran Gao",
        "Yelai Feng",
        "Renzhi Chen",
        "Shanshan Li",
        "Lei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 4 1 9 1 . 4 0 5 2 : r ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development Bowei Wang Computer Department National University of Defense Technology Changsha, China wangbowei@nudt.edu.cn Jiaran Gao School of Computer & Communication Engineering University of Science and Technology Beijing Beijing, China U202241904@xs.ustb.edu.cn Yelai Feng Intelligent Microelectronics Center Qiyuan Lab Beijing, China fengyelai@qiyuanlab.com Renzhi Chen Intelligent Microelectronics Center Qiyuan Lab Beijing, China chenrenzhi@qiyuanlab.com Shanshan Li* College of Computer Science and Technology National University of Defense Technology Changsha, China shanshanli@nudt.edu.cn Lei Wang* Defense Innovation Institute Academy of Military Science Beijing, China leiwang@nudt.edu.cn AbstractThe growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM Index TermsLarge Language Models, Chisel Language, Agile"
        },
        {
            "title": "Hardware Development",
            "content": "I. INTRODUCTION With the rapid advancement of artificial intelligence and big data, the market demand for Domain-Specific Architecture *Corresponding authors. (DSA) has grown significantly. This trend has promoted the development of Agile Hardware Development Methodology (AHDM) [1], [2]. AHDM imposes higher requirements on shortening hardware design cycles and enhancing design variability, which refers to the adaptability of hardware design to changes in requirements or constraints [3], [4] (see Section III). To address these requirements, next-generation Hardware Construction Languages (HCL) [5], [6], represented by Chisel, integrate modern programming language features into hardware design, significantly enhancing the abstract expression capability of hardware design. Consequently, HCL-Based AHDM shows promising prospects and has been applied in several large-scale projects [7], [8]. Large Language Models (LLMs) have demonstrated impressive code generation capabilities [9], prompting researchers to explore the ability of using LLMs for AHDM development [10], including automated module-level design [11] and architectural design [12]. The application of LLMs for HCLBased AHDM development has gained preliminary research attention [13], [14], indicating that this approach can more effectively manage project complexity, reduce migration costs, and enhance the practicality of agile methods. However, we have observed that existing smaller-scale open-source models lack sufficient capability in Chisel code generation: models frequently produce syntax/semantic errors when generating Chisel code and struggle to leverage Chisels language features for effective variability design. This limitation hinders the potential of the generated code in tasks such as parameterized module generation or design space exploration. These Fig. 1. An overview diagram of the construction of the ChiseLLM datasets and models, including Source Data Processing & Synthesing, Prompt-Guided Reasoning Trace Generation and Reasoning Model Finetuning. challenges impede the practical application of LLMs in HCLBased AHDM development. Recent reasoning models represented by OpenAI-o1 [15] and Deepseek-R1 [16] introduce test-time scaling [17], [18], conducting deliberate thinking and reflection before generating final results. Related studies show that reasoning models can generate high-quality code more effectively than nonreasoning models that lack thinking processes [9], [19], [20]. Therefore, we consider the introduction of test-time scaling as promising approach to enhance model performance in Chisel code generation tasks. Nevertheless, our experiments indicate that reasoning models without domain adaptation cannot achieve significant additional benefits in Chisel code generation scenarios (see Section V-C), as general reasoning models lack reliable hardware logic thinking patterns, leading to outputs prone to hallucinations. Consequently, adapting reasoning models to reason with task-specific thinking patterns becomes key challenge. To address this challenge, we propose the ChiseLLM series of datasets and models. Our approach includes: (1) Original dataset processing and transformation: we collected Chisel and Verilog code from publicly available RTL code sources, which after processing and transformation formed two high-quality domain-specific instruction fine-tuning datasets; (2) Promptguided reasoning trace data synthesis: we used promptguided approach to construct reasoning datasets ChiseLLM- {Completion,Decompile} with specific thinking patterns, enabling models to learn how to reason with these specific thinking patterns; (3) Reasoning model training: based on the generated data, we fine-tuned the Qwen2.5-Coder [21] series to produce the ChiseLLM-{7,32}B models. Experimental results demonstrate that ChiseLLM successfully adapts reasoning models to achieve excellent Chisel code generation capabilities. In terms of syntactic/semantic correctness, the ChiseLLM-7B model improved by an average of 18.85% in syntactic correctness compared to the pre-fine-tuned base model, while the ChiseLLM-32B model improved by an average of 26.32% in Pass@5 compared to the base model, with an average improvement of 20.44% over the optimal baseline across all tasks, achieving performance comparable to commercial models in syntactic correctness and decompilation accuracy. Regarding variability capability, our model showed 47.58% improvement compared to the baseline reasoning model, indicating that the ChiseLLM model can effectively generate designs with high variability. Our main contributions are as follows: We introduce the ChiseLLM series of models and datasets, successfully elevating the performance of opensource LLMs in Chisel code generation tasks to practical and effective level, and achieving performance comparable to commercial models in certain tasks, while controlling the model size to below 32B to reduce computational costs and provide broader applicability. Our prompt-guided method successfully adapts reasoning models for domain-specific use, introducing new approaches for LLM-assisted hardware design. We have constructed standardized evaluation system to comprehensively assess the performance of mainstream code generation models on Chisel tasks. Our models, datasets, and construction process will be open-sourced for use and improvement by subsequent researchers. We hope that ChiseLLM can serve as foundational model in the HCL-Based AHDM field, driving the advancement of related research. II. RELATED WORK A. Hardware Construction Languages To meet the primary goals of agile hardware development of enhancing design flexibility and reducing development cycles, diverse approaches have been explored, particularly the concept of hardware generators that can automatically produce designs tailored to specific requirements. Before the emergence of HCL, projects such as Genesis2 [22] attempted to address this need by treating hardware code as templates and automatically generating hardware designs through embedded language. The evolution of this approach led to the development of HCL, which integrate generation logic into highlevel programming languages. HCLs are defined as Domain Specific Languages (DSLs) designed on top of high-level languages, such as Chisel on Scala and PyRTL on Python [23]. The original design intention of HCLs was to standardize and formalize these design generators, utilizing the expressive capabilities of host languages to automatically generate designs based on set of high-level design parameters and constraints [24], thereby eliminating the need to reimplement designs for each class of design specifications. This approach reduces the cost of developing new languages from scratch while introducing important concepts and mature technologies from software engineering into hardware design. Currently, the Chisel language has been employed in multiple largescale agile hardware development projects, including RocketChip [8], XiangShan [7], Berkeley BOOM Processor [25], etc. B. Reasoning Models in Code Generation Test-Time Scaling techniques refer to post-training modifications [26] of large language models, enabling them to conduct extended Chain-of-Thought [27] reasoning processes (known as reasoning traces), thereby achieving better performance in tasks requiring reasoning (such as mathematics and code generation). Models with such capabilities are termed Reasoning Models, including OpenAI-o series [15], [28], Deepseek-R1 [16], Claude-3.7 Sonnet [29], Gemini 2.5 [30], and others. Reasoning models have demonstrated powerful performance in code generation tasks [9], [31], making them significant research direction in this field. Nevertheless, LLM performance remains limited in domain-specific languages (DSLs) with scarce corpus [32], which is widely acknowledged to require Domain Adaptation [33]. Our work confirms this conclusion, indicating that reasoning models still require domain adaptation to achieve optimal performance in Chisel code generation tasks. III. PRELIMINARY A. Agile Hardware Development and Design Variability AHDM draws inspiration from the software agile development concept [34], aiming to enhance the flexibility and responsiveness of hardware design through rapid iterations and continuous feedback. Consequently, projects adopting AHDM should possess high degree of variability, which is defined as the ability of project to adapt to changes in market demands or contextual conditions [35]. In the hardware domain, variability primarily emphasizes the adaptability of hardware designs to requirement or constraint changes, thereby supporting the need for rapid iterations. This paper utilizes the following direct inference: highly variable hardware designs should be able to meet different design requirement changes with minimal code refactoring costs. For instance, variability TABLE TASK TYPES AND DESCRIPTIONS IN CHISEL CODE GENERATION. Task Name Spec-to-Chisel (S2C) Decompile-to-Chisel (D2C) Description Reference Generate Chisel code that meets functional requirements from design specifications Convert low-level RTL code to Chisel that at least satisfies the same functionality [14] [36], [37] design can satisfy different functional and performance constraints by merely modifying set of high-level parameters without requiring extensive modifications to the underlying implementation, as exemplified by [8]. B. Chisel Generation Tasks Current research on using LLMs for AHDM development primarily focuses on transforming design specifications into traditional HDL code. However, Chisel serves as an intermediate layer between design specifications and traditional HDL, necessitating consideration of two distinct scenarios for code generation: directly generating Chisel code from design specifications (Spec-to-Chisel) and converting existing traditional HDL code into equivalent Chisel code (Decompileto-Chisel). This classification reflects two practical engineering needs: development of new designs and upgrading of existing ones. Table summarizes these two types of tasks. IV. METHOD In this section, we introduce the raw data collection and processing (Section IV-A), prompt-guided distillation (Section IV-B), and model training (Section IV-C) process, which together form the complete data construction and model training pipeline for ChiseLLM. A. Raw Dataset Collection and Preprocessing Publicly available HDL code data is extremely scarce. (System)Verilog accounts for only 3.2 102% of the the-stackv2-dedup dataset, while HCLs like Chisel have an even lower representation (approximately 1 103%). To our knowledge, there is currently no publicly available large-scale training dataset specifically for the Chisel language. To address this limitation, we leveraged Chisels characteristics as an intermediate layer language and utilized both Chisel and Verilog data to expand the size and diversity of our dataset. We collected publicly accessible Chisel and Verilog datasets, processed and transformed them separately, resulting in two high-quality foundational datasets: ChiseLLM-{Completion,Decompile}- Base. These two datasets contain Chisel code completion and code decompilation tasks, respectively. ChiseLLM-Completion-Base is derived from the the-stackv2-dedup dataset [38], which is large pre-training dataset containing multiple programming languages. We extracted use of syntax from other languages. To enable models to reasonably utilize Chisel language features in their reasoning traces, we provided two types of prompting guidance: document references and benchmark answers. The expected thinking pattern is: during the reasoning process, the model first forms logical steps for functional implementation, then reflects on the syntax level (achieved through recollection of documentation), ultimately generating syntactically and functionally correct answers. Document References. We downloaded all language documentation from the Chisel official website and processed it into independently citable Markdown document fragments. We created an index document for the official documentation, including titles and content summaries for all document fragments, assigning each unique chapter ID. Subsequently, we provided this document as context, having the Qwen2.572B-Instruct annotate the code at the line level in the form of comments. Afterward, document fragments were matched according to their chapter IDs and included as part of the corresponding information for the code data. After processing, each code corresponded to several relevant documents, averaging between 5-10 documents. We included these documents as part of the contextual information during distillation, requiring the model to utilize this documentation when reflecting on syntax. Benchmark Answers. The current common method for constructing reasoning datasets involves rejection sampling of independently verifiable, high-quality code datasets [41], [42]. However, existing Chisel code data exists in the form of pretraining datasets, lacking high-quality reference answers and test cases. Generated Chisel code is difficult to judge for correctness due to the absence of unit test cases and compilation challenges. To address this, we included benchmark answers as part of the guidance, allowing the reasoning model to generate intermediate reasoning traces when given reference answers. This approach prevented the models reasoning traces from leading to incorrect answers. objective of 2) Decompile-to-Chisel Task: The the Decompile-to-Chisel task is to convert low-level HDL code into Chisel code. Existing work [36] can programmatically convert Verilog code to Chisel code, but it can only achieve unoptimized mapping of code structures, with post-mapping optimization still requiring manual effort. We believe that LLMs have greater potential in this task, capable of achieving high-quality conversion and optimization through understanding and reasoning about the code. To this end, we provided two types of prompting guidance: variant pattern specifications and Chisel feature descriptions. Our expected thinking pattern is: during the reasoning process, the model can understand the core functionality of the original code, infer potential design requirement changes by applying variant pattern to the original code, and utilize Chisel language features to restructure the code to cover all variants. Variant Pattern Specifications. Based on the characteristics of variability design (see Section III), we required the model to first consider possible requirement changes the Fig. 2. Diagrams related to the ChiseLLM data processing and distillation workflow. Figure (a) illustrate the process of collecting and processing the source data. Figure (b) includes the prompt template used for prompt-guided distillation. Figure (c) shows the statistical characteristics of the ChiseLLM- {Completion,Decompile} datasets. Chisel3 code files from this dataset, excluding any files containing chiseltest or scalatest to remove all test code. This filtering was necessary due to the instability of the Chisel language testing framework [39] and its misalignment with our task objectives. After filtering and length screening, we obtained approximately 8.3K Chisel code files. Subsequently, we utilized large language models to add detailed comments to the code and converted the complete code into questionanswer pairs for code completion tasks, which included design specifications and contextual information. B. Prompt-Guided Distillation We followed the methodology proposed in the DeepseekR1 paper [16], using Deepseek to generate reasoning traces for the instruction fine-tuning dataset, thereby creating reasoning dataset to train smaller models to acquire reasoning capabilities. This process is known as distillation. Related research generally suggests that LLMs primarily learn how to effectively utilize knowledge acquired during pre-training when fine-tuning rather than learning new knowledge [40]. Based on this insight, we focused on constructing reasoning data with specific thinking patterns. As shown in Figure 2(b), to obtain reasoning data with the desired thinking patterns, we employed prompt-guided approach, providing Deepseek-R1 with additional task-related prompts and guiding information to direct the model toward generating reasoning traces more aligned with task requirements. We refer to this process as promptguided distillation. For both Spec-to-Chisel and Decompile-toChisel tasks, we adopted different prompt-guided approaches tailored to each task objective. By learning from these data, models can adopt appropriate thinking patterns when facing specific tasks, demonstrating better problem-solving capabilities than generic thinking patterns. Through this method, we processed the ChiseLLM-{Completion,Decompile}-Base datasets, ultimately creating two high-quality reasoning datasets: ChiseLLM-{Completion,Decompile}. 1) Spec-to-Chisel Task: The objective of the Spec-to-Chisel task is to convert design specifications into Chisel code. We observed that when generating code, models typically only reason about functional implementation while lacking reflection on syntax. This leads to more frequent hallucinations, including references to non-existent Chisel APIs or incorrect class CounterBase, then implement the corresponding functionality through implementation of the abstract interface. C. Model Finetuning We used LLaMA-Factory [43] for full-parameter fine-tuning of our models. We first analyzed the data characteristics of the ChiseLLM-Completion and ChiseLLM-Decompile datasets, as shown in Figure 2(c). The token length of the Decompile dataset is significantly longer than that of the Completion dataset, with an average around 9K tokens. Given the difficulty characteristics of both task types, this statistical distribution is expected. Therefore, we mixed the data according to proportion, with sampling ratio of 3:7, and trained on both Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct, ultimately obtaining series of models with reasoning capabilities, called the ChiseLLM Models. All models are publicly available. V. EXPERIMENTS In this section, we conduct comprehensive evaluation of ChiseLLM models. We assess the performance of ChiseLLM models and baseline models on Spec-to-Chisel and Decompile-to-Chisel tasks, and analyze the capability of models in variability design. We also conduct ablation studies to analyze the impact of ChiseLLM datasets on model performance. A. Research Questions Our experiments focus on the following research questions: RQ1: How does ChiseLLM models perform on Spectasks compared to to-Chisel and Decompile-to-Chisel baseline models? RQ2: Can ChiseLLM models more effectively produce variability designs? RQ3: What is the impact of ChiseLLMdatasets on model performance? B. Experiment Setup Benchmarks. To the best of our knowledge, there are no publicly available evaluation datasets specifically for the Chisel. However, since Chisel can generate synthesizable Verilog code, we utilize widely-adopted Verilog evaluation datasets for our assessment. We use RTLLM v2.0 [44] and VerilogEval-Human [10] datasets as benchmarks for evaluating syntactic/semantic correctness. Correctness Evaluation Method. To measure the syntactic and semantic correctness of generated Chisel code, we employ the common Pass@k metric. We first use the Scala 2.13 compiler to compile the generated Chisel code, checking its syntactic correctness; then use Chisel3s ChiselStage.emitSystemVerilog API to convert the generated Chisel code to equivalent SystemVerilog code. In most cases, the SystemVerilog code compiled by this API is syntactically correct and synthesizable. Finally, we compile and simulate the code with testbench using commercial tools (necessary because the SystemVerilog syntax automatically generated by Chisel cannot be compiled by iVerilog) to check its functional correctness. Fig. 3. Schematic diagrams of the three types of design variant. Practical examples of functional variants are shown in detail in the figure. module might face, then optimize the code accordingly to reduce the refactoring cost of adapting to each changes. To guide the model in producing reasonable design variants, we constrained the form of variant into the following three types: Configurable Variant: modified version that extracts adjustable attributes (such as data width, cache depth) as configurable parameters without changing core functionality or interfaces. This type of variant is very common in Design Space Exploration, typically used for performance optimization under different design requirements. Functional Variant: modified version that changes input-output behavior (such as protocol version support, exception handling mechanisms, algorithm implementation), directly modifying the functional requirements of the design. This type of variant typically appears during rapid prototype design in early design stages or during functional expansion in later stages. Structural Variant: modified version that adjusts hardware architecture (such as pipeline stages, memory organization, FSM encoding schemes) while preserving functional equivalence, affecting timing/area. This type of variant typically appears after functional requirements have been determined, during PPA (Power, Performance, Area) optimization. We required the model to provide configurable variant, and only one of either functional or structural variants to avoid guiding toward overly complex and unnecessary designs. Subsequently, we required the model to design Chisel module that can cover all variants. Chisel Feature Descriptions. To enable the model to reasonably utilize Chisel language features when thinking, we provided brief description of Chisels language features and design patterns, including parameterization, functional components and functional programming, combinational logic generation, generic parameters, optional inheritance, and traits. We required the model to select appropriate language features based on the selected variant pattern and the characteristics of the problem itself when generating reasoning traces. For example, when transforming an up-counter module into down-counter module as functional variant, the model should first abstract the counter behavior as abstract interfaces, TABLE II MODEL PERFORMANCE ON VERILOGEVAL-HUMAN AND RTLLM-V2.0 DATASETS. VerilogEval-Human RTLLM-V2."
        },
        {
            "title": "Model",
            "content": "Spec-to-Chisel Decompile-to-Chisel Spec-to-Chisel Decompile-to-Chisel P@1 1 P@5 1 Syn(%) 1 P@1 P@ Syn(%) P@1 P@5 Syn(%) P@1 P@5 Syn(%) Llama3.1-8B 2 4.33 Qwen2.5-Coder-7B 21.94 *Dpsk-R1-8B 3 9.31 29.41 *ChiseLLM-7B Qwen2.5-Coder-32B 41.02 39.74 Qwen2.5-72B 38.14 Llama-3.3-70B 38.50 *Dpsk-R1-32B 36.62 *Dpsk-R1-70B 51.434 *ChiseLLM-32B 9.90 31.87 15.44 47.08 53.85 49.30 44.90 54.58 52.28 68.29 9.02 37.08 16.01 58.82 73.47 61.31 65.97 52.19 51.72 76.45 7B+ Scale Models 5.43 27.60 10.05 50.47 12.29 34.58 16.15 70.99 11.15 43.23 12.03 59.19 - 7.73 - 22.00 32B+ Scale Models 41.19 40.54 38.38 45.03 37.50 56. 48.96 47.32 46.96 63.02 55.05 72.00 53.93 59.30 48.00 53.17 45.59 64.71 26.50 22.75 22.33 14.86 16.51 42."
        },
        {
            "title": "Commercial Models",
            "content": "- 18.00 - 37.46 38.39 31.79 31.27 27.00 29.10 57.37 - 15.56 - 41.78 - 8.08 - 33.29 48.00 43.75 47.26 26.21 26.27 64.05 13.00 20.25 20.00 13.11 15.00 54. - 11.44 - 51.11 21.75 29.14 27.18 26.41 30.46 70.56 Dpsk-V3-671B GPT-4o *Dpsk-R1-671B 50.16 42.04 62.744 63.44 60.16 76.05 76.37 69.76 82. 54.57 42.39 53.45 63.19 65.75 71.50 66.19 53.77 59.13 42.00 34.50 46.75 52.32 48.11 61.07 73.25 60.75 63. 49.25 44.50 55.25 57.39 64.00 69.25 - 16.55 - 45.42 28.25 30.00 43.00 19.65 22.81 62.15 58.00 55.00 61.75 1 Syn(%) represents the proportion of syntactically correct samples. P@k is the abbreviation of Pass@k metric. 2 Some 7B+ scale models lack results due to insufficient correct samples for statistical significance. 3 The models marked with asterisks (*) are reasoning models. 4 Green cells indicate the best performance, while blue cells indicate the second-best performance. Variability Evaluation Method. To evaluate the variability of Chisel code, we adopt the mainstream LLM-as-a-Judge [45] method, which is considered close to human evaluation in code quality assessment [46]. To ensure consistency and fairness of evaluation results, we: (1) provide publicly accessible evaluation standard to the Judge LLM, and (2) generate variants for each design specification in the evaluation datasets, and supplement these variants to the Judge LLM as consistent baseline. All evaluation datasets are publicly accessible. The Judge LLM performs multiple evaluations on the same Chisel code. The final evaluation score is the average of all evaluation scores. Samples with high evaluation variance are not included in the evaluation. Similar to the Pass@k metric, we require the model to implement the same design requirement multiple times, and evaluate each implementation, with the average score of all implementations as the evaluation result. Baselines. We select models with verified code generation performance as baselines, including: (1) Open-source nonreasoning models with general code generation capabilities, including Qwen2.5-{7,32}B-Coder-Instruct [21], Qwen2.5-72BInstruct [47], and Llama-3.3-70B-Instruct [48]; (2) Opensource reasoning models with general code generation capabilities, including Deepseek-R1-Distill-Qwen-{32,70}B [16] (excluding Deepseek-R1-Distill-Qwen-7B as it primarily focuses on solving mathematical problems); (3) Commercial models, including Deepseek-{V3,R1} [49], [16] and GPT-4o [50]. Other models including programming competition reasoning models [51] (specifically trained to programming in Python or C++) and models specifically fine-tuned for Verilog [52], [53] (Cannot generate correct Chisel code) are not considered as baselines to ensure fairness. C. Correctness Evaluation (RQ1) Table II shows the performance of various models on Specto-Chisel and Decompile-to-Chisel tasks on the VerilogEval and RTLLM datasets. We observe the following facts: General reasoning models do not consistently outperform their base models. For instance, on the RTLLM datasets Spec-to-Chisel task, Deepseek-R1-Distill-Qwen-32B achieves only 14.86% pass@1, while its base model Qwen2.5Coder-32B-Instruct reaches 26.50%; similarly, Deepseek-R1Distill-Llama-8B achieves 9.31% pass@1 on the VerilogEval, lower than its base model Llama3.1-8B-Instruct. These observations indicates that there is not significant and consistent improvement for reasoning models over their base models. ChiseLLM models outperforms baseline models of similar size on all tasks, and even surpasses commercial models on some tasks. Taking the RTLLM Decompile-to-Chisel task as an example, ChiseLLM-32B achieves 54.32% pass@1, significantly higher than Qwen2.5-Coder-32B-Instructs 13.00%, and even exceeding GPT-4os 44.50%; ChiseLLM-7B achieves 50.47% pass@1 on the VerilogEval dataset, nearly twice that of Qwen2.5-Coder-7B-Instruct (27.60%). Notably, ChiseLLM32B consistently ranks as the top or second-best performer across all metrics, demonstrating performance comparable to E. Ablation Study (RQ3) Table III shows the results of our ablation study. To demonstrate the impact of ChiseLLM datasets on model performance, we fine-tuned ChiseLLM-32B model separately using these two datasets, resulting in ChiseLLM-32B-Completion and ChiseLLM-32B-Decompile models, and evaluated their performance. We observe the following facts: Both datasets individually improve model performance. ChiseLLM-{Completion,Decompile} datasets each have positive impact on model performance. For example, on task, ChiseLLM-32Bthe VerilogEval Decompile-to-Chisel Completion improves pass@1 by 2.88 percentage points, while ChiseLLM-32B-Decompile improves it by 14.78 percentage points; on the RTLLM Decompile-to-Chisel task, ChiseLLM-32B-Completion improves pass@1 by 23.33 percentage points, and ChiseLLM-32B-Decompile improves it by 37.56 percentage points. The combination of both datasets produces significant synergistic effects. Across all tasks, the ChiseLLMtrained with both datasets outperforms mod32B model els trained with either dataset alone. For example, on task, ChiseLLM-32B the VerilogEval Decompile-to-Chisel achieves 56.41% pass@1, higher than both ChiseLLM32B-Decompiles 55.97% and ChiseLLM-32B-Completions 44.07%; on the RTLLM Spec-to-Chisel task, ChiseLLM-32B achieves syntax correctness rate of 64.05%, higher than both ChiseLLM-32B-Decompiles 60.41% and ChiseLLM32B-Completions 43.08%. VI. CASE STUDY In this section, we present two case studies to demonstrate and compare the design variability capabilities of the finetuned ChiseLLM-32B model. The purpose of this section is to complement the results presented in Section V-D by providing intuitive examples that illustrate both the thinking process during design and the distinctive characteristics of the final designs produced by the ChiseLLM models. Figure 6 illustrates an actual example of ChiseLLM-32B decompiling Verilog source code. In this example, the original Verilog code implements simple shift module with fixed data width and shift patterns. During the decompilation process, ChiseLLM considered both configurational and structural design variants by introducing the width configuration parameter and the abstract ShifterBase class. These enhancements transformed the module into highly variable and extensible Chisel implementation with configurable data (variable width) and structure (customizable shift behavior). Although the code shown in this example is relatively simple due to space constraints, the ChiseLLM models can apply the same logical approach to process more complex instances in practical applications. Figure 5 presents comparative example where both ChiseLLM and Qwen2.5-32B-Coder-Instruct decompile the same Verilog source code. In this case, the Verilog module named sync_signal is designed to delay input data by multiple clock cycles before outputting it. The Chisel implementation Fig. 4. Variability design capability of different models on the RTLLM dataset when performing Decompile-to-Chisel tasks. The blue bar represents the opensource reasoning and non-reasoning baseline models. The green bar represents the ChiseLLM-32B model. The red bars represent the commercial models. Each bar is distinctively hatched with different patterns. that of Deepseek-R1 despite having significantly smaller parameter scale (32B vs 671B). D. Variability Evaluation (RQ2) Figure 4 shows the variability design capability of different models on the RTLLM dataset when performing Decompileto-Chisel tasks. We observe the following facts: ChiseLLM models excel in variability design compared to baseline models and perform on par with commercial models. ChiseLLM-32Bs variability score is significantly higher than non-commercial baseline models such as Qwen-2.5-Coder-32B (3.79) and LLaMA-3.3-70B (2.39), and even exceeds commercial models like Deepseek-R1 (5.03) and Deepseek-V3 (4.84). This indicates that ChiseLLM-32B has acquired superior variability design capabilities through domain-specific fine-tuning. Reasoning models do not significantly dominate the variability design task. Despite Deepseek-R1-Distill series models performing well on some code generation tasks, their performance in Chisel variability design is limited, with Deepseek-R1-Distill-32B and Deepseek-R1-Distill-70B achieving means of only 3.82 and 3.20, respectively. These values show limited gains over their corresponding base models Qwen-2.5-Coder-32B (3.79) and LLaMA-3.3-70B (2.39), indicating that general reasoning ability does not always positively correlate with domain-specific variability design capability. Better variability design capability is accompanied by greater uncertainty. Although ChiseLLM-32B has the highest mean (5.19), it also has the largest variance; similarly, Deepseek-R1 and Deepseek-V3 also exhibit high means and variances. This suggests that when models attempt to implement more flexible designs, they may introduce more uncertainty. Smaller models like LLaMA-3.3-70B have lower variance (1.23) but also the weakest variability design capability (mean only 2.39). Overall, higher mean indicates stronger variability design capability, but the diversity of designs may also bring greater performance fluctuations. TABLE III CHISELLM MODEL PERFORMANCE WHEN TRAINING ON DIFFERENT DATASETS."
        },
        {
            "title": "Model",
            "content": "VerilogEval-Human RTLLM-V2.0 P@1 P@5 Syntax% P@ P@5 Syntax% Spec-to-Chisel Qwen2.5-Coder-32B-Inst ChiseLLM-32B-Comp2 ChiseLLM-32B-De2 ChiseLLM-32B 41.03 (0.0) 44.09 (+3.06)1 49.51 (+8.48) 51.43 (+10.4) 58.79 (0.0) 69.31 (+10.52) 71.41 (+12.62) 72.78 (+13.99) 26.50 (0.0) 73.47 (0.0) 66.42 (-7.05) 25.64 (-0.86) 76.80 (+3.33) 37.18 (+10.68) 76.45 (+2.98) 42.32 (+15.82) 42.82 (0.0) 48.18 (+5.36) 55.90 (+13.08) 61.43 (+18.61) 48.00 (0.0) 43.08 (-4.92) 60.41 (+12.41) 64.05 (+16.05) Decompile-to-Chisel Qwen2.5-Coder-32B-Inst ChiseLLM-32B-Comp ChiseLLM-32B-De ChiseLLM-32B 41.19 (0.0) 44.07 (+2.88) 55.97 (+14.78) 56.41 (+15.22) 51.59 (0.0) 71.41 (+19.82) 74.92 (+23.33) 77.67 (+26.08) 53.93 (0.0) 13.00 (0.0) 54.25 (+0.32) 36.33 (+23.33) 60.93 (+7.0) 50.56 (+37.56) 64.71 (+10.78) 54.32 (+41.32) 26.39 (0.0) 66.85 (+40.46) 74.26 (+47.87) 76.15 (+49.76) 28.25 (0.0) 43.23 (+14.98) 55.83 (+27.58) 62.15 (+33.90) 1 The values in brackets represent the performance improvement compared to the baseline model Qwen2.5-Coder-32B-Instruct. 2 ChiseLLM-Completion and ChiseLLM-Decompile are models trained on single dataset, while ChiseLLM combines both datasets. Fig. 5. An actual example of ChiseLLM-32B and Qwen2.5-32B-Coder-Instruct decompiling Verilog source code. The gray part represents the Verilog source code, the blue part represents the Chisel module generated by the Qwen model, and the green part represents the Chisel module generated by the ChiseLLM model. The content within the <think> tags represents the thinking process of the ChiseLLM model during decompilation, which, due to its length, is summarized as numbered list in the figure. The ChiseLLM model follows structured reasoning process during decompilation, ultimately producing Chisel module with higher variability and functional extensibility. generated by the Qwen model includes some optimizations but only considers configurational variability by parameterizing the data width and synchronization cycles. In contrast, ChiseLLM follows structured reasoning process, first analyzing the core functionality of the module, then identifying key deficiencies in the current implementationfixed data structure and lack of flow controland addressing these issues by incorporating appropriate Chisel language features. The resulting Chisel module from ChiseLLM can process arbitrary complex types of data input and control data flow transmission through enable signals, thus exhibiting greater variability and functional extensibility. VII. CONCLUSION In this paper, we presented ChiseLLM, comprising highquality reasoning dataset and an inference model. We collected training data from public resources and enhanced LLM performance in HCL generation through prompt-guided distillation. Our work provides two promising perspectives for future [9] N. Jain and K. e. a. Han, Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024. [10] N. Pinckney and C. e. a. Batten, Revisiting VerilogEval: year of improvements in large-language models for hardware code generation, Feb. 2025. [11] S. Liu and W. e. a. Fang, Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, in 2024 IEEE International Workshop on LLM-Aided Design. IEEE, 2024. [12] K. Chang and Y. W. et al., Chipgpt: How far are we from natural language hardware design, 2023. [Online]. Available: https: //arxiv.org/abs/2305.14019 [13] J. Niu and X. e. a. Liu, Rechisel: Effective automatic chisel code generation by llm with reflection, in Proceedings of the 62nd ACM/IEEE Design Automation Conference (DAC). San Francisco, CA, USA: ACM/IEEE, Jun. 2025. [14] T. Liu and Q. e. a. Tian, ChatChisel: Enabling Agile Hardware Design with Large Language Models, in 2024 2nd International Symposium of Electronics Design Automation (ISEDA). Xian, China: IEEE, May 2024, pp. 710716. [15] Introducing OpenAI o1, https://openai.com/o1/, Sep. 2024. [16] DeepSeek-AI, DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning, Jan. 2025. [17] C. Snell and J. e. a. Lee, Scaling llm test-time compute optimally can be more effective than scaling model parameters, arXiv preprint arXiv:2408.03314, 2024. [18] S. Welleck and A. e. a. Bertsch, From decoding to meta-generation: Inference-time algorithms for large language models, arXiv preprint arXiv:2406.16838, 2024. tops leaderboard, [19] O1 polyglot aiders new https://aider.chat/2024/12/21/polyglot.html, Dec. 2024. [20] D. Li and S. C. et al., S*: Test time scaling for code generation, 2025. [Online]. Available: https://arxiv.org/abs/2502.14382 [21] B. Hui and J. e. a. Yang, Qwen2.5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [22] A. Solomatnikov and A. e. a. Firoozshahian, Chip multi-processor generator, in Proceedings of the 44th Annual Design Automation Conference, ser. DAC 07. New York, NY, USA: Association for Computing Machinery, 2007, p. 262263. [Online]. Available: https://doi.org/10.1145/1278480.1278544 [23] J. Clow and G. e. a. Tzimpragos, pythonic approach for rapid hardware prototyping and instrumentation, in 2017 27th International Conference on Field Programmable Logic and Applications (FPL). IEEE, 2017, pp. 17. [24] Motivation of chisel, https://www.chisellang.org/docs/explanations/motivation. [25] Riscv-boom/riscv-boom: SonicBOOM: The berkeley out-of-order machine, https://github.com/riscv-boom/riscv-boom. [26] K. Kumar and T. e. a. Ashraf, Llm post-training: deep dive into reasoning large language models, arXiv preprint arXiv:2502.21321, 2025. [27] J. Wei and X. e. a. Wang, Chain-of-thought prompting elicits reasoning in large language models, 2023. [28] Introducing and https://openai.com/index/introducing-o3-and-o4-mini/. OpenAI o3 o4-mini, [29] Claude 3.7 sonnet and claude code, https://www.anthropic.com/news/claude-3-7-sonnet. intelligent most Our 2.5: [30] Gemini AI model, https://blog.google/technology/google-deepmind/gemini-modelthinking-updates-march-2025/, Mar. 2025. [31] Introducing SWE-bench verified, https://openai.com/index/introducingswe-bench-verified/. [32] X. Gu and M. e. a. Chen, On the effectiveness of large language models in domain-specific code generation, ACM Trans. Softw. Eng. Methodol., vol. 34, no. 3, Feb. 2025. [Online]. Available: https://doi.org/10.1145/3697012 [33] S. Joel and J. J. e. a. Wu, survey on LLM-based code generation for low-resource and domain-specific programming languages, Nov. 2024. [34] Manifesto for agile software development, https://agilemanifesto.org/. [35] V. Cortellessa and D. e. a. Varro, Eds., Fundamental Approaches to Software Engineering: 16th International Conference, FASE 2013, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2013, Rome, Italy, March 16-24, 2013. Proceedings, Fig. 6. An actual example of ChiseLLM-32B decompiling Verilog source code. The gray part represents the Verilog source code, and the green part represents the Chisel module generated by ChiseLLM. While ensuring all functionalities of the Verilog source code are covered, the module generated by ChiseLLM introduces advanced language features, resulting in higher variability and functional extensibility. AHDM: First, high-quality domain adaptation training corpora can significantly improve LLM performance in HCL generation. Second, with additional guidance, LLMs can master the thinking patterns necessary to address the complexities in hardware design. As an initial exploration in LLM-assisted HCL-Based AHDM, we hope to draw more researchers attention to this field and advance the application of LLMs in hardware design."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Lee and A. e. a. Waterman, An agile approach to building risc-v microprocessors, IEEE Micro, vol. 36, no. 2, pp. 820, 2016. [2] R. Bahr and C. e. a. Barrett, Creating an agile hardware design flow, in 2020 57th ACM/IEEE Design Automation Conference (DAC), 2020, pp. 16. [3] M. Acher and J. G. e. a. Duarte, On programming variability with large language model-based assistant, in Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A. Tokyo Japan: ACM, Aug. 2023, pp. 814. [4] C. Brink and E. e. a. Kamsties, On hardware variability and the relation to software variability, in 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications, 2014, pp. 352355. [5] J. Bachrach and H. e. a. Vo, Chisel: Constructing hardware in scala embedded language, in Proceedings of the 49th Annual Design Automation Conference. San Francisco California: ACM, Jun. 2012, pp. 12161225. [6] C. Papon and Y. Xiao, SpinalHDL, Apr. 2025. [7] Y. Xu and Z. e. a. Yu, Towards Developing High Performance RISCV Processors Using Agile Methodology, in 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), 2022, pp. 11781199. [8] Chipsalliance/rocket-chip, CHIPS Alliance, Apr. 2025. ser. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013, vol. 7793. [36] J. Bruant and P.-H. H. et al., (system)verilog to chisel translation for faster hardware design, in Proceedings of the 31th International Workshop on Rapid System Prototyping, RSP 2020, Virtual Conference, September 24-25, 2020. ACM, 2020. [37] J. Bruant and P.-H. e. a. Horrein, Towards agile hardware designs with chisel: network use-case, IEEE Design & Test, 2021. [38] A. L. et al., Starcoder 2 and the stack v2: The next generation, 2024. [39] Ucb-bar/chiseltest, UC Berkeley Architecture Research, Feb. 2025. [40] C. Zhou and P. e. a. Liu, LIMA: Less is more for alignment, in Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36. Curran Associates, Inc., 2023, pp. 55 00655 021. [41] G. Penedo and A. L. et al., Codeforces cots, https://huggingface.co/ datasets/open-r1/codeforces-cots, 2025. [42] O. T. Team, Open thoughts, Jan. 2025. [43] Y. Zheng and R. Z. et al., Llamafactory: Unified efficient fine-tuning of 100+ language models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). Bangkok, Thailand: Association for Computational Linguistics, 2024. [Online]. Available: http://arxiv.org/abs/2403.13372 [44] S. Liu and Y. e. a. Lu, OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL Generation. [45] L. Zheng and W.-L. C. et al., Judging llm-as-a-judge with [Online]. Available: https: mt-bench and chatbot arena, 2023. //arxiv.org/abs/2306.05685 [46] R. Wang and J. e. a. Guo, Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering, Apr. 2025. [47] Qwen, Qwen2.5 technical report, 2025. [Online]. Available: https: //arxiv.org/abs/2412.15115 [48] M. AI, The llama 3 herd of models, 2024. [Online]. Available: https://arxiv.org/abs/2407. [49] DeepSeek-AI, technical Available: https://arxiv.org/abs/2412.19437 Deepseek-v3 report, 2025. [Online]. [50] Hello GPT-4o, https://openai.com/index/hello-gpt-4o/. [51] Huggingface/open-r1: Fully open reproduction of DeepSeek-R1, https://github.com/huggingface/open-r1/tree/main. [52] Y. Zhao and D. e. a. Huang, CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization, Jul. 2024. [53] Z. Pei and H.-L. Z. et al., Betterv: Controlled verilog generation [Online]. Available: https: with discriminative guidance, 2024. //arxiv.org/abs/2402."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology National University of Defense Technology Changsha, China",
        "Computer Department National University of Defense Technology Changsha, China",
        "Defense Innovation Institute Academy of Military Science Beijing, China",
        "Intelligent Microelectronics Center Qiyuan Lab Beijing, China",
        "School of Computer & Communication Engineering University of Science and Technology Beijing Beijing, China"
    ]
}