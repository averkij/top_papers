{
    "paper_title": "In-context learning and Occam's razor",
    "authors": [
        "Eric Elmoznino",
        "Tom Marty",
        "Tejas Kasetty",
        "Leo Gagnon",
        "Sarthak Mittal",
        "Mahan Fathi",
        "Dhanya Sridhar",
        "Guillaume Lajoie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 6 8 0 4 1 . 0 1 4 2 : r IN-CONTEXT LEARNING AND OCCAMS RAZOR Eric Elmoznino,1,2, Tom Marty,1,2, Tejas Kasetty1,2, Leo Gagnon1,2, Sarthak Mittal1,2 Mahan Fathi1, Dhanya Sridhar,1,2, Guillaume Lajoie,1,2 1Mila Quebec AI Institute, 2Université de Montréal"
        },
        {
            "title": "ABSTRACT",
            "content": "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize besta principle called Occams razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw connection between Occams razor and in-context learningan emergent ability of certain sequence models like Transformers to learn at inference time from past observations in sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode. Keywords generalization, complexity, compression, in-context learning, meta-learning"
        },
        {
            "title": "Introduction",
            "content": "The goal of machine learning (ML) is to learn models that generalize to unseen data. Longstanding theory shows that minimizing training error alone can lead to overfitting and poor generalization (Bishop & Nasrabadi, 2006). To enable better generalization, ML follows the principle of Occams razorthe best explanation is the simplest one that explains the observations (Hutter, 2010; Rathmanner & Hutter, 2011; Sunehag & Hutter, 2014). The intuition is that simple rules that explain the data cannot simply memorize observations, and must instead capture more general patterns. Consequently, learning algorithms usually trade off low training error and low model complexity with ad hoc approaches (e.g., via regularization and inductive biases), motivating the need for notions of complexity that can be tractably minimized directly. Although there exist mathematical notions of model complexity such as VC dimension or Kolmogorov complexity, these quantities cannot be directly minimized, or even tractably computed for the latter. In practice, we instead learn predictors that minimize training error as well as proxies of the models complexity, such as the L1 or L2 norm of the model parameters, or rely on inductive biases for low-complexity solutions that are implicit in the model class and learning algorithm. Defying this trend, however, pretrained large language models (LLMs) have surprising ability to rapidly learn and generalize from small amounts of data presented in their context (or prompt) (Radford et al., 2019). This ability called in-context learning (ICL) is typically explained through the lens of memory-based meta-learning (e.g., Chan et al., 2022; Xie et al., 2022), theoretical framework where sequence models are explicitly trained to learn statistical models from sequences of observations. The main contribution of this paper is to provide theoretical arguments linking ICL to Occams razor and preference for simple models. Briefly, our theory frames ICL as meta-learning algorithm whose next-token prediction objective Equal first authorship. Correspondence to: {eric.elmoznino,guillaume.lajoie}@mila.quebec. Equal senior authorship In-context learning and Occams razor PREPRINT is directly equivalent to powerful compression method called prequential coding (Blier & Ollivier, 2018). Given the relationship between optimal compression and Kolmogorov complexity, we show that the meta-objective in ICL is to find learner capable of jointly minimizing both training error and model complexity across diverse range of tasks. Our theory, along with the empirical experiments that we use to support it, explain why ICL has proven so effective in meta-learning settings, and also explain the shortcomings of current ICL methods. Namely, we find that current methods produce learning algorithms which are susceptible to underfitting and can fail to generalize to novel tasks, suggesting principled avenues for future research."
        },
        {
            "title": "2 Occam’s razor and In-context learning",
            "content": "In this section, we introduce meta-learning objective that directly targets simple models, and then show that it is equivalent to the next-token prediction objective underlying ICL. We reach this result via four key steps: 1. We begin by formalizing both training error and model simplicity through the lens of Kolmogorov complexity, which deals with optimal data and model compression. 2. We then show how learning algorithms can be used to compress data through technique called prequential coding (Blier & Ollivier, 2018), and that minimizing the resulting prequential code length achieved by learning algorithm is equivalent to jointly minimizing the training error and complexity of the model it fits. 3. We then introduce the idea of finding learning algorithm that minimizes prequential code length by formalizing meta-learning problem that appears difficult to optimize. 4. Finally, we show that the next-token prediction objective underlying ICL already solves this meta-learning problem in an efficient and scalable way. 2.1 Kolmogorov complexity and data compression Kolmogorov complexity (Kolmogorov, 1965; Li et al., 2008) is notion of information quantity. Intuitively, the Kolmogorov complexity K(x) of an object is the length of the shortest program (in some programming language) that outputs x. related notion is the conditional Kolmogorov complexity K(xy) of the object given another object y, which is the length of the shortest program that takes as input and outputs x. Finally, the Kolmogorov complexity of encoding two objects jointly is denoted K(x, y). While quite abstract, this notion of complexity has deep ties to compression, making it intuitive as measure of information quantity. The smaller and more structured an object isregularity, patterns, rules, etc.the more easily it can be described by short program, correspondingly having lower Kolmogorov complexity. Although Kolmogorov complexity is very generalobjects x, can be datasets, programs, modelsit is intractable to compute. However, it can often be tractably estimated or bounded, as we will show below. quantity relevant to ML is the joint Kolmogorov complexity of dataset = (d1, ..., dn) and of generative model p(d), where each sample di is drawn iid: K(D, p) = K(Dp) + K(p), (1) where K(p) refers to the complexity of the model (i.e., the length of the shortest program that outputs the function describing probability distribution : R+). This term is intractable to compute as it requires an enumeration over all programs that output p, but the conditional complexity K(Dp) can be easily computed. According to (Grünwald, 2007), if the dataset is sufficiently large and the generative model is close to the true data distribution, the optimal method for compressing data point di uses only log2 p(di) bits (e.g., using an arithmetic coding scheme, Witten et al., 1987), as in the case of Shannon information (Shannon, 2001). As such, we have K(Dp) (cid:80) log2 p(di) which is the negative log-likelihood of the data under p(d), commonly used objective function in ML. It follows that models which achieve lower error under this objective better compress data. We provide further background on Kolmogorov complexity in Appendix A. As we are interested in model optimization, we henceforth consider parameterized models pθ with parameters θ. We denote learning algorithm by function : (D) Θ, where (D) denotes the power-set over datapoints, i.e., the set of possible datasets. The learning algorithm maps dataset to model pT (D). Maximum likelihood training, which is the norm in ML, is learning algorithm ml which fits model that best compresses the training data: ml(D) = arg min θ (cid:88) dD log2 pθ(d) = arg min θ K(Dpθ). (2) 2 In-context learning and Occams razor PREPRINT However, Occams razor says that we also need simple models. Thus, we consider the learning algorithm oc, which defines simple via complexity: oc(D) = arg min θ [K(Dpθ) + K(pθ)] . (3) In reality, oc is intractable since K(pθ) cannot be computed. In practice, maximum log-likelihood training ml is often enhanced with regularizers (e.g., L2-norm of parameters) and inductive biases (e.g., restricting the model class) to implicitly favor low-complexity models that combat overfitting and improve generalization. For instance, deep neural networks (DNNs) trained through stochastic gradient descent (SGD) tend to be biased towards simple solutions (Blier & Ollivier, 2018; Goldblum et al., 2023). However, existing regularizers at best amount to indirect methods that roughly penalize model complexity K(pθ) along with training error. No known learning algorithm (which we will often call learner for brevity) directly attempts to minimize Equation (3) as oc would. In what follows, we introduce learners Tϕ that have learnable parameters ϕ, estimated via meta-optimization, to approximate the ideal learner oc. 2.2 Prequential coding While learner that adheres to Occams razor and solves Equation (3) would improve generalization, it is difficult to design one in practice. Even if K(pθ) could be computed efficiently, there is the further challenge of minimizing it. We will first describe an approach to the problem of estimating K(pθ), and then consider the optimization problem in the next section. Figure 1: Illustration of prequential coding, method for estimating K(D, θ) = K(Dpθ) + K(pθ) using pθs learning algorithm . a. Pseudocode of the prequential coding program, which jointly compresses and pθ by incrementally training model using on increasingly more data. The primary contribution to total program length comes from specifying each next datapoint di+1 using the current model pθi , which takes log2 pθi (di+1) bits. b. visual illustration of prequential coding. As the learner sees more data, it outputs models that assign higher likelihood to new observations, and can thus better compress them. The total prequential code length Lpreq(D; ) is given by the area under the curve. The area underneath the curves last point is equal to the complexity of the dataset given the final model, K(Dpθ). Since Lpreq(D; ) = K(Dpθ) + K(pθ), the area above the curves last point is equal to K(pθ). Prequential coding formalizes the intuition that simple models generalize better from less data. While K(pθ) is difficult to measure directly, it turns out that we can estimate the joint complexity K(D, pθ) = K(Dpθ) + K(pθ) using compression algorithm called prequential coding (illustrated in Figure 1) that leverages given learner which produces pθ (i.e., pθ = (D)) (Blier & Ollivier, 2018). Consider an ordering of iid datapoints = {d1, ..., dN }, and denote D1:i = {d1, ..., di}. Prequential coding uses the learner to train models on increasing amounts of data. First, we train model on just the first data point to get pθ1 = (d1). Because the model is trained on single datapoint, it will not be very accurate; however, it should be better than random model that has seen no data at all. We can then use this model pθ1 to compress the next (unseen) datapoint d2, which takes log2 pθ1(d2) bits. At this point, we can train new model pθ2 = (D1:2). Having seen more data, this model should assign higher likelihood to new datapoint d3, which we can compress using log2 pθ2 (d3) bits. This process repeats until the entire dataset has been covered. At this point, the model pθ can be obtained simply by applying the learning algorithm to the complete dataset pθ = (D). The total number of bits that it takes to jointly compress and pθ using prequential coding is the sum of how many bits it takes to compress each datapoint using model that was trained on all previous ones. Visually, it is the area under the prequential coding curve shown in Figure 1b. The length of this program is called the prequential code length Lpreq(D; ) (Blier & Ollivier, 2018): Lpreq(D; ) = 1 (cid:88) i=0 log2 pθi(di+1) K(D, pθ) = K(Dpθ) + K(pθ). (4) 3 In-context learning and Occams razor PREPRINT Strictly speaking, Lpreq(D; ) is an upper-bound on K(D, pθ): prequential coding is one way to jointly compress the data and model, but it is not necessarily the optimal way. The upper-bound is tight in practice, however, if the final model pθ does good job of compressing the data (i.e., K(Dpθ) K(D)) and yields improving models as dataset D1:i grows (Blier & Ollivier, 2018). We discuss these conditions further in Appendix B. Concretely, when prequential coding is good algorithm for jointly compressing the data and model, then Lpreq(D; ) K(D, pθ) and the model complexity is given by: Lpreq(D; ) K(Dpθ) + K(pθ) K(pθ) Lpreq(D; ) K(Dpθ). (5) This expression relates Kolmogorov complexity to intuitions about generalization in ML: the simpler model is, the quicker it generalizes from limited amounts of training data. Although the relationship in Equation (5) offers promising way forward to operationalize the idealized learner oc, there is problem. The prequential code length given by Equation (4) conditions on the choice of learner . However, prequential coding also requires us to encode the learning algorithm itself. When we take the description length of into account, the quantity Lpreq(D; ) + K(T ) is an upper-bound on K(Dpθ) + K(pθ) (see Appendix C). As we describe below, we will optimize for learners Tϕ that minimize Lpreq(D; ), and we will articulate how Tϕ has low complexity given minimization over multiple datasets. 2.3 Minimizing prequential code length through meta-learning Consider parameterized learner Tϕ that minimizes the prequential code length Lpreq(D; Tϕ) of dataset D. This objective tightly upper-bounds the objective that the idealized learner oc minimizes, but only when K(Tϕ) is low. This second criteria is violated if Tϕ overfits to single dataset D. To forbid Tϕ from memorizing single dataset, we consider meta-dataset = {D1, ..., DM } coming from different tasks and meta-learn Tϕ to minimize prequential code length on average across the meta-dataset D. This allows us to write: Tϕ = arg min ϕ (cid:88) i=1 Lpreq(Di; Tϕ) arg min ϕ = arg min ϕ = arg min ϕ (cid:88) K(Di, pθTϕ) i=1 (cid:34) (cid:88) i=1 (cid:34) (cid:88) i=1 K(Dipθ, Tϕ) + K(pθTϕ) (cid:35) K(Dipθ) + K(pθTϕ) , (cid:35) (6) (7) (8) where pθ = Tϕ(Di), and the last line is obtained from noticing that all the relevant information about Di contained in Tϕ is already encoded in the model pθ = Tϕ(Di). The prequential code length of new dataset of interest using the meta-learned Tϕ is then: Lpreq(D; Tϕ) K(D, pθTϕ) = K(Dpθ, Tϕ) + K(pθTϕ) = K(Dpθ) + K(pθTϕ). (9) (10) (11) Note that the learners Tϕ and oc (= arg minθ [K(Dpθ) + K(pθ)]) are not equivalent: oc aims to minimize K(pθ) directly whereas Tϕ fits models that are simple given Tϕ (i.e. low K(pθTϕ)). Despite these differences, the two learners are deeply related. As result of its meta-objective in Equation (6), the learner Tϕ attempts to minimize training error across many datasets while fitting compressible models. The learner Tϕ will succeed in doing this on novel dataset when it generalizes to that novel dataset. 2.4 Training for ICL meta-learns prequential code length minimizer In practice, solving the meta-learning problem in Equation (6) involves several constraints: 1. The performance of Tϕ() must be evaluated w.r.t. datasets prequential code length. 2. Tϕ() must be fast to evaluate because it is iteratively called on multiple datasets. 3. To meta-optimize ϕ, it must be easy to take gradients of Lpreq(; Tϕ) w.r.t. ϕ. 4. ϕ must parameterize an expressive class of learning algorithms, capable of minimizing prequential code length on broad distribution of tasks and generalizing to unseen ones. 4 In-context learning and Occams razor PREPRINT While this may appear daunting, it turns out that these desiderata are readily addressed by ICL in probabilistic sequence models. Such models are trained to predict the distribution over the next element in sequence given its past context: (dtD1:t1). Crucially, the sequence model is both the learner Tϕ and the inner model pθ. Indeed, ϕ corresponds to the parameters of the sequence model (e.g. weights in Transfomer), and θ = Tϕ(D1:t1) is encoded by the activations of hidden units in the model when presented with the context D1:t1. Thus, the predicted distribution over the next token is given by: (dtD1:t1) = pTϕ(D1:t1)(dt). The dual nature of the sequence model as both the learner and the learned model offers natural solution to the constraints above, enabling fast and differentiable evaluation of Tϕ() (2 & 3 above) with respect to cumulative next-token prediction loss (1 above). Moreover, modern sequence models can parameterize rich class of learning algorithms, which is crucial to solving Equation (6) (4 above). Notably, architectures such as Transformers are known to have components which make them especially good meta-learners, such as multi-head attention (Olsson et al., 2022). It is thus no surprise that sequence models are leveraged in settings outside of the language domain (Bauer et al., 2023; Kirsch et al., 2022; Von Oswald et al., 2023a), making them general-purpose meta-learners. This predictive formulation is quite flexible as it can be used to model data which contains sequential correlations, such as language, but can also be used to process any iid dataset. Indeed, consider = {(x1, y1), ..., (xT , yT )} and the supervised task of learning function = (x). In this setting, data point is given by the pair dt = (xt, yt), and straightforward tokenization schemes can be used to append novel query to the context such that the predicted output ˆy is given by the next token in the sequence. This ICL setup is well-suited for regression-type tasks (see e.g. (see e.g., Von Oswald et al., 2023a,b)) but can be used for most supervised tasks. ICL thus turns the training of sequence model into meta-optimization problem over datasetsan approach also called memory-based meta-learning (Hochreiter et al., 2001; Ortega et al., 2019; Santoro et al., 2016). It is assumed here that (xt, yt) are iid. Although pretrained LLMs that can execute tasks with instructions given via context (or prompt) (Radford et al., 2019) break this iid data assumption, prequential code length is well-defined over arbitrary sequences, and our theory can possibly be adapted to settings with non-stationary data. Further exploration of this topic is left for future work. Summary. We showed that sequence models trained on cumulative next-token prediction losses explicitly optimize meta-learning objective that jointly minimizes training error and model complexity. This provides normative account of ICL in terms of Occams razor, and explains recent experimental findings showing that LLMs are good universal compressors (Delétang et al., 2023)."
        },
        {
            "title": "3 Experiments",
            "content": "Our experiments are designed to illustrate the benefits of ICL in terms of fitting simple models that generalize on iid examples. In Section 3.1, we compare ICLs standard next-token prediction objective to an alternative that minimizes training error alone, rather than prequential code length. Section 3.2 then compares ICL to standard gradient-based learners that minimize training error, such as SGD. In Section 3.3, we explore the impact of learner Tϕs architecture on prequential code length minimization. Section Section 3.4 explores the ability of Tϕ to generalize to novel tasks. Finally, in Section 3.5 we use insights from our theory to control the data distribution seen by Tϕ in order to better minimize prequential code length. Experimental details not described in the main paper (e.g., precise architectures, hyperparameters for training, etc.) can be found in Appendix E. Tasks. In line with similar work studying ICL in controlled setting (Akyürek et al., 2023; Garg et al., 2023; Mahankali et al., 2023), we use synthetically-generated tasks. Each task consists of supervised learning dataset Di = {(x1, y1), ..., (xk, yk)}, where the labels are (potentially stochastic) function of the input yj = i(xj, ϵj). ICL learners Tϕ are trained on meta-dataset = {D1, ..., DN }, where each Di is associated with different ground-truth data-generating function i. We primarily study three meta-datasets: (1) Linear regression problems where R3 and R. The ground-truth functions are noisy linear mappings yj = ixj + bi + ϵj, where each {W i, bi} is sampled from standard Normal distribution and ϵj is Gaussian noise with σ2 = 0.04. (2) Sinusoidal regression problems where xj and functions are linear combinations yj = (cid:80)L l=1 αi,l sin (ωlxj). We use = 3 with frequencies ωl (0, 5) that are shared across tasks, varying only the amplitudes αi,l (0, 1). (3) Mastermind: multi-label classification problem inspired by the code-breaking game Mastermind. Each is associated with an underlying discrete code (a fixed-size sequence of digits) that needs to be inferred from random guesses that return partial information. The inputs xj are random guesses for the code, and yj is tuple of two class labels where the first specifies the number of digits in xj that are correct in terms of both position and value, and the second label specifies the number of digits that are correct in value but not necessarily position. We use randomly sampled codes of length 8 with digits varying from 1..6. Finally, we introduce an additional task based on Hidden Markov process in Section 3.5 to investigate the generality of our finding on temporally correlated inputs. 5 In-context learning and Occams razor PREPRINT 3.1 Comparisons to in-context learning with train-risk objective We have argued that standard ICL can be seen as meta-learning method whos meta-objective is to minimize training error and model complexity through cumulative next-token prediction (prequential code length). However, this is not the only meta-objective that one could design for ICL. In particular, we can design an alternative meta-objective that minimizes only training error simply by training Tϕ to predict past datapoints in the context rather than future unseen ones. In both cases, the learner Tϕ is some function that takes context (i.e., partial dataset) as input, and outputs model pθ capable of making predictions for arbitrary datapoints. For supervised learning, this can be represented as ˆyq = Tϕ((x, y)1:j, xq) where (x, y)1:j corresponds to an observed context, xq is the queried input, and the model pθ is implicitly encoded in Tϕs weights and latent activations given the context. In standard ICL (which we will refer to as prequential ICL), the query xq is novel input that does not appear in the context. In the alternative form of ICL (which we will call train-risk ICL), the query xq is randomly-selected input that appeared previously in the context x1:j. Note the similarities of train-risk ICL to standard objectives of learners that minimize training error: it processes some fixed-sized training set (here context) and attempts to minimize the empirical risk on subset of that very same data (here single query that appeared in the context). While nobody uses train-risk ICL in practice, it serves as an ideal control to illustrate our theory of ICL and the generalization benefits of minimizing prequential code length as opposed to only training error. One can use an identical architecture for Tϕ in both cases and train using precisely the same methodology and loss function; the only difference is which query the loss function is evaluated on. In our experiments, we parameterize Tϕ using Transformer. For the train-risk case, standard Transformer could simply attend to the context position that matches xq and retrieve the corresponding label. To prevent this trivial solution, we instead use bottlenecked architecture for Tϕ described in Mittal et al. (2024). In this architecture, Transformer first summarizes the context into low-dimensional vector = Transformerϕ((x, y)1:j), and separate prediction headhere multi-layer perceptron (MLP)subsequently outputs prediction for the query ˆyq = MLPϕ(xq, z). For fair comparison, we use the same bottleneck architecture for train-risk ICL and prequential ICL in all experiments, unless otherwise stated. Figure 2: Experimental results comparing different learners. Figures show average prequential coding curves for meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error is measured using MSE for linear and sinusoid regression and cross-entropy for Mastermind. Error bars show standard error across seeds (5 for ICL, 15 for SGD). a. ICL from next-token prediction objectives (prequential ICL, blue) yields lower prequential code lengths than ICL from past-token prediction objectives (train-risk ICL, orange), with greater effects in low-data regimes. An SGD-based learner (green) fits more complex models than prequential ICL and performs poorly in low-data regimes, but can generalize better in large-data regimes on difficult Mastermind task due to underfitting in ICL. b. The architecture used to parameterize Tϕ has substantial influence on ICLs ability to minimize prequential code length. 6 In-context learning and Occams razor PREPRINT Figure 2a shows our comparisons between prequential ICL to train-risk ICL, where we plot the prequential coding curves for each ICL method after loss convergence on meta-dataset. The curves are constructed at inference time by evaluating the average iid generalization error (i.e., unseen next-token prediction loss) on unseen tasks from the meta-dataset, for varying context lengths. Findings. Two findings follow directly from our theory. The first is that for large context lengths, generalization error is identical for both prequential ICL and train-risk ICL. This is because with significant data, overfitting is less likely to occur, even when minimizing training error alone. The benefits of simple models are instead expected to be most prominent in low-data regimes where generalization is difficult, and this is precisely what we observe. Across all tasks, prequential ICL consistently outperforms train-risk ICL in terms of generalization for short context lengths, and this performance gap extends further the more difficult the task (e.g., it is small for linear regression, and larger for sinusoid regression and mastermind). 3.2 Comparisons to traditional gradient-based learners We next consider whether there are empirical advantages of meta-learning learner Tϕ to minimize prequential code length through ICL, compared to using standard out-of-the-box learning algorithms. In particular, we know that traditional SGD-based learners can optimize DNN models that generalize well across wide range of tasks, despite only explicitly minimizing training error. We consider standard SGD-based learner that fits randomly-initialized MLP to the training set until validation loss converges. We repeatedly sample dataset from our meta-dataset, truncate it to specified number of observed datapoints, apply the SGD-based learner to the truncated dataset, and evaluate the resulting models generalization error on new datapoints. Findings. Figure 2a compares this SGD-based learner to prequential (and train-risk) ICL learners. Across all tasks, the models obtained through ICL generalize better in low-data regimes as result of directly minimizing model complexity. With enough training data, however, models obtained through the SGD-based learner generalize just as well. In fact, on the Mastermind task, SGD performs better in large-data regimes. This result demonstrates that even though the next-token prediction objective in ICL is well-motivated from theoretical perspective, the degree to which that objective can successfully be minimized strongly depends on the architecture of Tϕ and the methods used to train it. For instance, when Tϕ is Transformer, the expressivity of the model it implicitly fits to the context scales with the number of activations in the network (N ), whereas the expressivity of DNN trained through SGD scales with the number of weights (N 2). Furthermore, the amount of compute that Tϕ uses to fit the context amounts to one forward pass of network, whereas the amount of compute that goes into fitting dataset using SGD can be arbitrarily large. 3. Influence of the in-context learning architecture The previous section argued that the structure of Tϕ can influence its ability to minimize prequential code length. In this section, we further illustrate this point by considering wider breadth of neural architectures for Tϕ. Since state-space models (SSMs) have recently been shown to exhibit ICL (Lu et al., 2024), we test Mamba 1 (Gu & Dao, 2023) and Mamba 2 (Dao & Gu, 2024). We also test standard causal decoder Transformer in addition to the bottlenecked Transformer from previous sections. Prequential code length comparisons in Figure 2b show that the architecture for Tϕ indeed plays substantial role, with the bottlenecked Transformer and Mamba 2 performing best across our tasks. Analyzing why this is the case is out of scope for this work; we only intend to show that having next-token prediction objective alone does not guarantee that prequential code length can successfully be minimized in practice through ICL. 3.4 Large pretrained models core element of our theory of ICL is that Tϕ is trained to minimize average prequential code length on meta-dataset D. There is no guarantee, however, that prequential code length will be small on novel dataset that was unseen at training time: this depends on the generalization abilities of the learner Tϕ. In this section, we look at the taskgeneralization abilities of large pretrained LLM (GPT-4 Achiam et al., 2023) on the Mastermind task. We do this by prompting the LLM with description of the task and number of in-context examples, then obtaining the logits and prediction error for novel example. In Figure 2a, we find that despite its massive pretraining across breadth of tasks, the LLM is unable to meaningfully minimize prequential code length on Mastermind. Not only is its prequential code length substantially higher than for much smaller model trained on distribution of Mastermind tasks, but it is also higher than for naive baseline that just predicts the empirical marginal distribution over class labels in the context. These results demonstrate that even when the size of the model and meta-dataset used to train Tϕ are scaled significantly, current methods for ICL can still struggle to minimize prequential code length on novel task. 7 In-context learning and Occams razor PREPRINT Figure 3: Experimental results for LLM and data manipulation strategies. Figures show average prequential coding curves for meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error bars show standard error across 5 seeds. a. An LLM (GPT-4, red) fails to meaningfully minimize prequential code length on novel Mastermind task, performing far worse than small ICL models trained on distribution of Mastermind tasks (blue) and naive baseline that predicts the marginal class distribution over the context (purple). Error is measured using cross-entropy. b. On synthetic HMM dataset designed to mimic natural language, preferentially training on shorter contexts (red) yields lower prequential code lengths than training uniformly over context lengths (purple). Error is measured using reverse KL divergence between model and oracle conditioned on seen context. 3.5 Improving ICL by controlling the data distribution In addition to improving architectures used for Tϕ or scaling the diversity of tasks on which it is trained, complementary approach is to manipulate the distribution of data presented in-context at training time. This approach can be especially useful in non-iid settings; for instance Chan et al. (2022) found that in order for ICL to emerge in an image classification setting, the distribution over classes needed to be bursty, or Zipfian. In this section, we consider simple manipulation of the data distribution that is inspired by our theory, with particular focus on improving ICL in language-like data modalities relevant to LLMs. In prequential coding, model complexity is related to the speed of convergence in generalization error as context length increases. We might therefore be able to further bias ICL towards simple models that generalize better by sampling short contexts, such that downstream prediction errors on longer context lengths (after which the prequential coding curve has already converged) do not disproportionately dominate the loss. We attempt this on synthetically-generated data from Hidden Markov Models (HMMs) that were designed to mimic the statistical properties of natural language in simplified and controlled setting. Briefly, we generate family of HMMs parameterized by compositional latent attributes. For example, we use family of transition matrices, emission matrices, etc. that are randomly sampled to create new HMM, and train Transformer to predict the next observation in sequence. The model is evaluated on unseen HMMs with novel compositions of latents (see Appendix for details). Our results, presented in Figure 3b, show that this data-manipulation strategy based on prequential code length is effective. Generalization erroras measured by the reverse KL-divergence to the oracle predictor given the seen contextis lower when preferentially training on short context lengths, with the gap narrowing the more tokens are seen during training as shown in Figure E.2. Both models were trained on the same total number of tokens, with one model using shorter contexts on average than the other. Surprisingly, biasing the data distribution in this way not only decreases generalization error for short context lengths, but also for long ones. In general, these results show how our theory can lead to practical improvements for ICL, where we look at prequential coding curves and compression ability to guide method design."
        },
        {
            "title": "4 Related work",
            "content": "Sequence modeling and compression. The idea that probabilistic models can be used to efficiently compress data is topic widely studied in machine learning across different modalities and settings (Blier & Ollivier, 2018; Delétang et al., 2023; Ollivier, 2015; Veness et al., 2014), specifically in sequence modelling (Delétang et al., 2023; Goyal et al., 2018; Valmeekam et al., 2023) due to its close similarities to prequential coding (Blier & Ollivier, 2018). In this area, the generic sequence modeling capabilities of certain foundation models are crucial for defining effective universal compressors. While Goyal et al. (2018) and Valmeekam et al. (2023) claim that learned sequence models can outperform simple compressors like JPEG or gzip, they overlook model complexity in their analysis, adhering strictly to Shannons notion of compression. In contrast, more recent studies from Delétang et al. (2023) and Bornschein 8 In-context learning and Occams razor PREPRINT et al. (2022) opted for the Kolmogorov approach, incorporating model size to account for model complexity. Delétang et al. (2023), in particular, add nuance to the claimed advantages of foundation models due to the substantial memory allocation required to store their weights. Our theory builds on these works by relating compression and sequence modeling to the approach of meta-learning across tasks using ICL, which we show yields simple models that adhere to Occams razor. In-context learning as Bayes-optimal prediction. One of the dominant perspectives of ICL and related meta-learning approaches is that they yield Bayes-optimal learners (Binz et al., 2023; Hollmann et al., 2022; Mikulik et al., 2020; Müller et al., 2021; Ortega et al., 2019; Wang et al., 2024), in the sense that they learn prior distribution over tasks during training, and then compute posterior given data presented in-context at inference time. This posterior can then be used to make predictions with minimum Bayes risk. Various studies have tested this in controlled settings with tractable posteriors (Genewein et al., 2023; Mittal et al., 2023; Panwar et al., 2024; Xie et al., 2022). Xie et al. (2022) assume concept latent that parameterizes the generation of dependent samples through Hidden Markov Model (HMM) and provide formal conditions for ICL to effectively approximate the Bayes-optimal predictor on the prompt, specifically, requiring the pretraining distribution to be structured similarly to HMM. In supervised fashion, Akyürek et al. (2023) construct sequence of labeled examples (x, (x)) and shows that under uncertainty, ICL behaves as the Bayes-optimal predictor on noisy linear regression. Additionally, they argue that with limited capacity, ICL does not necessarily match the Bayes predictor but can meta-learn other learning algorithms, such as gradient-based algorithms on linear models and closed-form ridge regressors (Panwar et al., 2024). Grau-Moya et al. (2024) induce prior for model simplicity in ICL by generating tasks from short programs run on Universal Turning Machines. Finally, (Raventós et al., 2024) find that under sufficiently diverse set of pretraining tasks, ICL does not yield Bayes-optimal predictors, but instead infers more uniform prior. While the Bayesian perspective of ICL is very useful and complementary to the Kolmogorov one that we have proposed, we argue in Appendix that the Kolmogorov perspective generalizes the Bayesian one and more easily accounts for diverse findings in ICL (e.g., cases where ICL does not yield Bayes-optimal predictors). In-context learning as direct meta-learned optimizer. Elaborating on the possibility that ICL emulates nonBayesian learning algorithms, Von Oswald et al. (2023a) show that k-layer linear Transformers with specific weight parameterization can mimic steps of gradient descent for least squares loss. Ahn et al. (2023) provide theoretical foundation for these observations, provably showing that the optimization of the parameters of linear Transformer under certain assumptions about the data distribution effectively implements this learning algorithm. Concurrent studies by Zhang et al. (2023) and Mahankali et al. (2023) report similar findings, albeit under slightly different assumptions regarding weight initialization or data generation processes. Beyond the scope of linear regression, Kirsch et al. (2022) explore this phenomenon on augmented natural data (MNIST, CIFAR10) and provide insightful empirical conditions for the emergence of ICL as general-purpose learning algorithm. Other works empirically show that Transformers can learn more complex function classes in-context, such as sinusoidal regression (Von Oswald et al., 2023a), decision trees (Garg et al., 2023), and RASP-programmable functions (Zhou et al., 2023). While prior works such as these attest to the powerful meta-learning capabilities of ICL, our work differs in that it identifies the precise meta-objective as an implementation of Occams razor."
        },
        {
            "title": "5 Discussion and Future Work",
            "content": "In this work, we introduced novel theoretical arguments linking ICL and the next-token prediction objective to Occams razor. Our theory provides normative account of the strong generalization abilities of in-context learners at inference time, especially in low-data regimes when compared to traditional optimizers. These theoretical insights were supported by number of empirical experiments, some of which also identified shortcomings of current methods for ICL that should be addressed in future work. One such shortcoming is that models learned through current ICL methods can underfit data presented in-context, and that this can hamper generalization in large-data regimes on difficult tasks. We also found that the degree of underfitting was highly dependent on the architecture used to parameterize the in-context learner (i.e., the sequence model), and that commonly used architectures such as causal decoder Transformers can often underperforma finding corroborated by Ding et al. (2024). In light of this, we hypothesize that ICL can be improved through the design of novel sequence model architectures that explicitly target prequential code length. For example, current methods learn in-context through single forward pass of sequence model with fixed layer depth. In contrast, DNNs can be trained using gradient-based methods until training loss converges, which can take weeks and substantial compute. One improvement to ICL might therefore be to augment current sequence model architectures with layers that use built-in optimization primitives with variable compute budgets, as was done in Von Oswald et al. (2023b). Another promising approach is to combine ICL and SGD through mixture of learners that reaps their complementary benefits. ICL is sample-efficient and 9 In-context learning and Occams razor PREPRINT generalizes well in low-data regimes, while SGD-based methods that optimize the weights of DNN excel on difficult tasks when significant training data is available. Recent work by Bornschein et al. (2024) explored simple method for combining both learners by presenting smaller number of recent tokens in-context to sequence model for ICL, while at the same time using large number of earlier tokens to fine-tune the weights of the sequence model using gradient methods, finding significant performance gains. Another challenge of ICL that follows directly from our theory is that the in-context learner must generalize to novel tasks and datasets. While we found that task generalization was successful over narrow task distributions (e.g. distribution of linear regression tasks), we also found that task generalization was more difficult in open-ended cases, in which even large pretrained LLM was unable to learn in-context on novel task that was easily solved by small MLP trained using SGD. One possible path forward is to have many domain-specific in-context learners that each specialize in compressing data from given task distribution. Another option is to learn simple learners that are more likely to generalize to novel tasks, which could be achieved through inductive biases, regularization, or, intriguingly, through an additional meta-layer of ICL at the task level that would minimize the Kolmogorov complexity of the learner itself (and not only the model it fits). Finally, our work only provides theoretical framework for ICL on iid data. Relaxing these iid assumptions opens up two avenues for future work: connecting ICL to generalization on out-of-distribution samples, and studying the effect of nonstationary data presented in context, as is the case in language and the HMM experiment presented here. Acknowledgments The authors would like to acknowledge the following researchers for valuable discussions and exchanges: Joao Sacramento, Johannes von Oswald, Jorg Bornschein, Marcus Hutter. All authors acknowledge support from an unrestricted gift from Google inc. EE acknowledges support from Vanier Canada Graduate Scholarship #492702. SM acknowledges the support of PhD Excellence Scholarship from UNIQUE. DS acknowledges support from NSERC Discovery Grant RGPIN-2023-04869, and Canada-CIFAR AI Chair. GL acknowledges support from NSERC Discovery Grant RGPIN-2018-04821, the Canada Research Chair in Neural Computations and Interfacing, and Canada-CIFAR AI Chair. 10 In-context learning and Occams razor PREPRINT"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023. URL https://arxiv.org/abs/2306.00297. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In Proceedings of the Eleventh International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=0g0X4H8yN4I. Jakob Bauer, Kate Baumli, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, et al. Human-timescale adaptation in an open-ended task space. In International Conference on Machine Learning, pp. 18871935. PMLR, 2023. Marcel Binz, Ishita Dasgupta, Akshay Jagadish, Matthew Botvinick, Jane Wang, and Eric Schulz. Meta-learned models of cognition. Behavioral and Brain Sciences, pp. 138, 2023. Christopher Bishop and Nasser Nasrabadi. Pattern recognition and machine learning, volume 4. Springer, 2006. Léonard Blier and Yann Ollivier. The description length of deep learning models. Advances in Neural Information Processing Systems, 31, 2018. Jorg Bornschein, Yazhe Li, and Marcus Hutter. Sequential learning of neural networks for prequential mdl. arXiv preprint arXiv:2210.07931, 2022. Jorg Bornschein, Yazhe Li, and Amal Rannen-Triki. Transformers for supervised online continual learning. arXiv preprint arXiv:2403.01554, 2024. Gregory Chaitin. On the length of programs for computing finite binary sequences. Journal of the ACM (JACM), 13 (4):547569, 1966. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:1887818891, 2022. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023. D. Deutsch. The Beginning of Infinity: Explanations that Transform the World. Always learning. Penguin Books, 2012. ISBN 9780140278163. URL https://books.google.ca/books?id=WFZl7YvsiuIC. Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. CausalLM is not optimal for in-context learning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=guRNebwZBb. Lance Fortnow. Kolmogorov complexity. In Aspects of Complexity, Minicourses in Algorithmics, Complexity, and Computational Algebra, NZMRI Mathematics Summer Meeting, Kaikoura, New Zealand, pp. 7386, 2000. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes, 2023. URL https://arxiv.org/abs/2208.01066. Tim Genewein, Grégoire Delétang, Anian Ruoss, Li Kevin Wenliang, Elliot Catt, Vincent Dutordoir, Jordi Grau-Moya, Laurent Orseau, Marcus Hutter, and Joel Veness. Memory-based meta-learning on non-stationary distributions. In International conference on machine learning, pp. 1117311195. PMLR, 2023. Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint arXiv:2304.05366, 2023. Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa. Deepzip: Lossless data compression using recurrent neural networks, 2018. URL https://arxiv.org/abs/1811.08162. Jordi Grau-Moya, Tim Genewein, Marcus Hutter, Laurent Orseau, Grégoire Delétang, Elliot Catt, Anian Ruoss, Li Kevin Wenliang, Christopher Mattern, Matthew Aitchison, et al. Learning universal predictors. arXiv preprint arXiv:2401.14953, 2024. Peter Grünwald. The minimum description length principle. MIT press, 2007. 11 In-context learning and Occams razor PREPRINT Peter Grünwald and Paul MB Vitányi. Kolmogorov complexity and information theory. with an interpretation in terms of questions and answers. Journal of Logic, Language and Information, 12:497529, 2003. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Sepp Hochreiter, Steven Younger, and Peter Conwell. Learning to learn using gradient descent. In Artificial Neural NetworksICANN 2001: International Conference Vienna, Austria, August 2125, 2001 Proceedings 11, pp. 8794. Springer, 2001. Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: transformer that solves small tabular classification problems in second. arXiv preprint arXiv:2207.01848, 2022. Marcus Hutter. complete theory of everything (will be subjective). Algorithms, 3(4):329350, 2010. ISSN 1999-4893. doi: 10.3390/a3040329. URL http://arxiv.org/abs/0912.5434. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/ abs/1412.6980. Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022. Andrei Kolmogorov. Three approaches to the quantitative definition of information. Problems of information transmission, 1(1):17, 1965. Ming Li, Paul Vitányi, et al. An introduction to Kolmogorov complexity and its applications, volume 3. Springer, 2008. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Arvind Mahankali, Tatsunori B. Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention, 2023. URL https://arxiv.org/abs/2307.03576. Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, and Pedro Ortega. Meta-trained agents implement bayes-optimal agents. Advances in neural information processing systems, 33: 1869118703, 2020. Sarthak Mittal, Niels Leif Bracher, Guillaume Lajoie, Priyank Jaini, and Marcus Brubaker. Exploring exchangeable dataset amortization for bayesian posterior inference. In ICML 2023 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2023. Sarthak Mittal, Eric Elmoznino, Leo Gagnon, Sangnie Bhardwaj, Dhanya Sridhar, and Guillaume Lajoie. Does learning the right latent variables necessarily improve in-context learning?, May 2024. URL http://arxiv.org/abs/ 2405.19162. arXiv:2405.19162 [cs]. Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021. Yann Ollivier. Auto-encoders: reconstruction versus compression, 2015. URL https://arxiv.org/abs/1403.7752. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/abs/2209.11895. Pedro Ortega, Jane Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, et al. Meta-learning of sequential strategies. arXiv preprint arXiv:1905.03030, 2019. Madhur Panwar, Kabir Ahuja, and Navin Goyal. In-context learning through the bayesian prism, 2024. URL https://arxiv.org/abs/2306.04891. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Samuel Rathmanner and Marcus Hutter. philosophical treatise of universal induction. Entropy, 13(6):10761136, 2011. ISSN 1099-4300. doi: 10.3390/e13061076. URL http://arxiv.org/abs/1105.5721. Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in Neural Information Processing Systems, 36, 2024. 12 In-context learning and Occams razor PREPRINT Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pp. 18421850. PMLR, 2016. Claude Elwood Shannon. mathematical theory of communication. ACM SIGMOBILE mobile computing and communications review, 5(1):355, 2001. Ray Solomonoff. formal theory of inductive inference. part i. Information and control, 7(1):122, 1964. Peter Sunehag and Marcus Hutter. Intelligence as inference or forcing Occam on the world. In Proc. 7th Conf. on Artificial General Intelligence (AGI14), volume 8598 of LNAI, pp. 186195, Quebec City, Canada, 2014. Springer. ISBN 978-3-319-09273-7. doi: 10.1007/978-3-319-09274-4_18. Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, and Srinivas Shakkottai. Llmzip: Lossless text compression using large language models, 2023. URL https://arxiv.org/ abs/2306.04050. Joel Veness, Marc G. Bellemare, Marcus Hutter, Alvin Chua, and Guillaume Desjardins. Compress and control, 2014. URL https://arxiv.org/abs/1411.5326. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 3515135174. PMLR, 2023a. Johannes Von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023b. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning, 2024. URL https://arxiv.org/abs/2301.11916. Ian Witten, Radford Neal, and John Cleary. Arithmetic coding for data compression. Communications of the ACM, 30(6):520540, 1987. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2022. URL https://arxiv.org/abs/2111.02080. Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023. URL https://arxiv.org/abs/2306.09927. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What Algorithms can Transformers Learn? Study in Length Generalization, October 2023. URL http://arxiv.org/abs/2310.16028. arXiv:2310.16028 [cs, stat]. 13 In-context learning and Occams razor PREPRINT"
        },
        {
            "title": "Appendix A Background on Kolmogorov complexity",
            "content": "Kolmogorov complexity was independently developed in the 1960s by Kolmogorov (1965), Solomonoff (1964), and Chaitin (1966), and defines notion of information quantity. Intuitively, the Kolmogorov complexity of an object is the length of the shortest program (in some programming language) that outputs that object. Specifically, given some finite string x, K(x) is the length l(r) (in bits) of the shortest binary program that prints and halts. Let be universal Turing machine that executes these programs. The Kolmogorov complexity of is then: K(x) = min {l(r) : (r) = x, {0, 1}}, (12) where {0, 1} denotes the space of finite binary strings. related notion is the conditional Kolmogorov complexity of string given another string y, which is the length of the shortest program that takes as input and outputs x: K(xy) = min {l(r) : (r(y)) = z, {0, 1}}, (13) where r(y) denotes program taking as input. Finally, we can also define joint Kolmogorov complexity K(x, y), which denotes the length of the shortest program that jointly outputs both and y. Surprisingly, joint Kolmogorov complexity is related to conditional Kolmogorov complexity (up to an additive logarithmic term O(log K(x, y)), which we will ignore) by the Symmetry of Information theorem (Li et al., 2008): K(x, y) = K(yx) + K(x) = K(xy) + K(y). (14) Kolmogorov complexity has many intuitive properties that make it attractive as measure of information quantity, and although it is less common than notions from Shannon information theory (Shannon, 2001), it is strictly more general (as we will show later below). The smaller and the more structure an object hasregularity, patterns, rules, etc.the more easily it can be described by short program and the lower its Kolmogorov complexity. Kolmogorov complexity therefore is deeply rooted in the idea of compression. For instance, sequence with repeating patterns or dataset that spans low-dimensional subspace can be significantly compressed relative to its original size, and this results in low Kolmogorov complexity. In contrast, random string devoid of any structure cannot be compressed at all and must in effect be hard-coded, making its Kolmogorov complexity equal to its original size in bits. While powerful, Kolmogorov complexity has certain limitations. First and foremost, Kolmogorov is intractable to compute exactly because it requires brute force search over an exponentially large space of possible programs. It is therefore often of conceptual rather than practical value, although it can nevertheless be upper-bounded using more efficient compression strategies. Second, Kolmogorov complexity depends on the programming language of choice. For instance, if programming language has built-in primitive for the object being encoded, Kolmogorov complexity is trivially small. This concern, however, is often overblown: given any two Turing-complete programming languages, the difference in Kolmogorov complexity that they assign to an object is upper-bounded by constant that is independent of the object itself, because any Turing-complete programming language can simulate another (Fortnow, 2000; Grünwald & Vitányi, 2003). In practice, we can simply consider reasonable Turing-complete programming languages that dont contain arbitrary object-specific primitives, in which case this simulation constant will be relatively small and the particular programming language of choice will have little effect. Finally, Kolmogorov complexity is only defined for discrete objects because no terminating program can output continuous number with infinite precision. This concern is also less consequential in practice, because we can always represent continuous objects using finite (e.g., floating-point) precision. Important properties for machine learning In ML, we are often concerned with datasets and probabilistic models. Kolmogorov complexity relates to these two concepts in several interesting ways. First, we can ask about the Kolmogorov complexity of finite dataset = (x1, ..., xn) where each sample is drawn iid from distribution p(x). It turns out that if we have access to the true distribution p(x), optimal algorithms such as arithmetic coding (Witten et al., 1987) can encode each sample using only log2 p(xi) bits. Intuitively, this is because samples that occur more frequently can be encoded using shorter codes in order to achieve an overall better compression. We thus have that: K(Xp) = (cid:88) i=1 log2 p(xi). (15) If instead of access to the true distribution p(x) we only have probabilistic model of the data pθ(x), we have that: In-context learning and Occams razor PREPRINT K(Xp) K(Xpθ) (cid:88) i=1 log2 pθ(xi), (16) i=1 log2 pθ(xi). where we have equality on the LHS when pθ = and equality on the RHS when the cost of improving pθ (in bits of written code) would be greater than the benefits from more accurate modeling. In practice, if pθ is close to p, we can say that K(Xpθ) (cid:80)n This insight is significant. Notice that (cid:80)n i=1 log2 pθ(xi) is the negative log-likelihood of the data under the model, which is common loss function used in ML. This tells us that models with lower error better compress their data, and directly relates Kolmogorov complexity to optimization in ML. However, what if we do not have model? What is the Kolmogorov complexity of the data itself? Intuitively, if the dataset is sufficiently large, the optimal method for encoding it should be to first specify model and then encode the data using that model as in Equation (16). Specifically, using identities in Fortnow (2000), we have: K(X) K(Xpθ) + K(pθ). (17) This encoding scheme on the RHS is referred to as 2-part code (Grünwald, 2007). For large datasets, we have equality when the models description length and error are jointly minimized, which occurs when the model pθ(x) is equivalent to the true distribution p(x): K(X) = arg min pθ [K(Xpθ) + K(pθ)] = arg min pθ (cid:34) (cid:88) i=1 log2 pθ(xi) + K(pθ) (cid:35) = K(Xp) + K(p) = (cid:88) i=1 log2 p(xi) + K(p). (18) (19) Again, we can draw important connections to ML. Equation (17) says that the Kolmogorov complexity of dataset is upper-bounded by the models error and complexity. In addition, Equations (18) and (19) tell us that the simplest model that explains the data is most likely to be the true one, which draws theoretical link between compression, maximum likelihood training, model complexity, and generalization (Goldblum et al., 2023). Relation to Shannon information In Shannon information theory (Shannon, 2001), the notion of information quantity is entropy. Given random variable p(x), entropy is defined as: H(X) = Exp(x) log2(p(x)). Notice that the log2(p(x)) inside the expectation is equal the quantity inside the sum of Equation (15), which specified the minimum number of bits needed to encode sample from dataset given the distribution that sample was drawn from. This is no accident: entropy can be seen as the average number of bits needed to compress events from distribution using an optimal encoding scheme when the distribution p(x) is known. If we simply sum these bits for finite number of samples instead of taking an expectation, we get exactly K(Xp) as defined in Equation (15). As we have seen, though, the assumption about known distribution p(x), need not be made in the Kolmogorov complexity framework. In this sense, Kolmogorov complexity is strict generalization of Shannon information theory: K(X) as defined in Equation (19) is equivalent to summed entropy plus the complexity of the distribution p(x), which is unknown and needs to be encoded. In the Shannon framework, it is difficult to derive meaningful notion for the information quantity in the distribution p(x) because it is an individual objecta function, in particularand Shannon information is only defined for random variables (Grünwald & Vitányi, 2003). second drawback of Shannon information is that entropy is measure of statistical determinability of states; information is fully determined by the probability distribution on states and unrelated to the representation, structure, or content of the individual states themselves (Grünwald & Vitányi, 2003). For this current work, we require notion of complexity that can account for representations and functions, making Kolmogorov complexity better suited to the task. Appendix Conditions for prequential code length to tightly upper-bound complexity As discussed in Section 2.2, Lpreq(D; ) is an upper-bound on K(D, pθ): prequential coding is one way to jointly compress the data and model, but it is not necessarily the optimal way. In our work, this is not significant concern, since 15 In-context learning and Occams razor PREPRINT minimizing an upper-bound on an objective is equivalent to minimizing the objective itself, as in the case of minimizing the negative evidence lower bound (ELBO) in variational inference. Nevertheless, it is interesting to consider the conditions under which the prequential code length Lpreq(D; ) tightly upper-bounds the joint complexity K(D, pθ). In practice, prior work has shown that prequential coding yields far tighter compression bounds than alternative methods in the case of deep learning models (Blier & Ollivier, 2018). While the tightness of the upper-bound has not been formally characterized in prior work, the conditions under which prequential coding is good compression algorithm that can closely approximate K(D, pθ) are informally stated below. pθ is good model of D. Prequential coding uses the model pθ to compress datapoints, leveraging the relationship between optimal coding and log-likelihood (see Appendix A). However, consider the case in which pθ is not good model of and assigns low likelihood to the data (low compared to the true distribution from which the data is drawn). In this case, the shortest program that jointly outputs (D, pθ) might construct the two objects separately: it could first encode pθ and then encode in some other way, for instance by encoding different model pθ that is closer to the true generative distribution and then using pθ to encode D. In this case, it could be that Lpreq(D; ) K(D, pθ) because prequential coding would be forced to use poor model that does not optimally compress D. In settings where pθ was obtained from good learning algorithm that was fit to D, however, this concern is unlikely to play significant role; it only applies when the learning algorithm is poor and results in model that substantially underfits the data. yields improving models as D1:i grows. In prequential coding, the learning algorithm is used to fit intermediate models pθi on subset of data D1:i, so that the intermediate models can be used to efficiently encode di+1 using only log2 pθi(di+1) bits. This assumes, however, that these intermediate models fit by are good at modeling the true data distribution and that they improve quickly as the dataset size grows. If, on the other hand, fits poor intermediate models, the total prequential code length will be high. Note that this would be true even if the final model obtained after seeing the entire dataset has low complexity and low training error, in which case K(D, pθ) = K(Dpθ)K(pθ) would be low. For instance, consider noiseless linear regression dataset where xi RM and yi R. If is the ordinary least squares learning algorithm, it will trivially overfit dataset D1:i whenever < , which would thus yield high prequential code length if len(D) . In contrast, regularized learning algorithm such as Ridge regression that combats overfitting could have generalized more quickly as function of the amount of data seen, and thus more tightly upper-bounded K(D, pθ). In the setting we consider, however, where parameterized Tϕ is explicitly fit to generalize well at all intermediate dataset sizes, this concern is unlikely to play significant role; it only applies when the learning algorithm significantly underfits or overfits at intermediate dataset sizes relative to some other learning algorithm that could have been used instead."
        },
        {
            "title": "Appendix C Prequential coding and compression without a known learning algorithm",
            "content": "When introducing the relationship between prequential coding and optimal compression in Equation (4), we mentioned that key assumption is that the learning algorithm is known. In reality, then, we have that: K(Dpθ) + K(pθ) = K(D, pθ) K(D, pθ, ) = K(D, pθT ) + K(T ) Lpreq(D; ) + K(T ) = Lpreq(D; ) + K(T ) K(Dpθ) + K(pθ), (20) (21) (22) (23) (24) where the first inequality on line Equation (21) appears because compressing additional objects can only take more bits, and the second inequality on line Equation (23) comes from the fact that prequential coding is not necessarily the optimal way to compress dataset and model given learning algorithm (see Appendix B). If the learning algorithm is short program like SGD, however, then K(T ) 0 and Lpreq(D; ) is an upper-bound of K(Dpθ) + K(pθ). For simple learning algorithms, then, Equation (4) holds."
        },
        {
            "title": "Appendix D Advantages over the Bayesian perspective",
            "content": "The Bayes-optimal prediction perspective of ICL and meta-learning says that by meta-training on some set of tasks D, the learner infers some prior over latent task variablesor, equivalently, prior over modelsp(pθD). On some novel task D, the learner then infers posterior over models that both explain the training data (i.e., assign it high likelihood) and are consistent with the prior: pD (pθD) = p(Dpθ)p(pθD)/Z, where is normalizing constant. 16 In-context learning and Occams razor PREPRINT According to the theory, subsequent predictions are then done through implicit Bayesian averaging under this posterior model distribution. Crucial differences in our theory are that does not need to be drawn from well-defined distribution over tasks for us to reason about the meta-learning problemthe Kolmogorov framework does not require thisand K(pθTϕ) is not literally prior probability distribution over models given Dit only implicitly defines prior based on the meta-learned Tϕ. As result, our theory generalizes the Bayesian perspective. To see why these generalizations provide value, consider where the prior in the Bayesian framework p(pθD) comes from. This prior is not defined explicitly in the ICL framework; instead, it is implicitly defined based on D, the implicit initial prior p(pθ), and the implicit inference machinery that approximates p(pθD) = p(Dpθ)p(θ)/Z. All of these implicit components make any meaningful analysis difficult, since it is difficult to characterize them. However, these implicit components are all intrinsic properties of the meta-learning algorithm (the meta-learners architecture, the meta-objective, etc.), which we do have explicit control over. Our theory only makes reference to this meta-learner Tϕ and the description length of models under it K(pθTϕ), rather than to objects that are only implicitly defined (and never known). As such, we argue that our theory is more amenable to analysis and provides more explanatory value. For example, in the Kolmogorov framework that we have proposed, it is easy to see how ICL might in some cases generalize to novel dataset that is entirely out-of-domain with respect to D. Perhaps, for instance, the tasks have compositional structure and Tϕ has some inductive biases for compositional generalization. In contrast, it is far more difficult to find good explanation for such phenomenon in the Bayesian framework. The explanation would have to be in terms of some implicit initial prior p(pθ) (which we never defined) and the subsequent prior p(pθD) that it induced. Proponents of the Bayesian framework would thus have to say ahh, generalization here must have been possible because p(pθ) had the right kind of structure. However, this same rationale could be used to explain any outcome (positive or negative), and therefore is bad scientific explanation (Deutsch, 2012). Another problem with the Bayesian perspective is that its predictions do not always hold in practice. Notably, Raventós et al. (2024) found that when the diversity in pretraining tasks is sufficiently large, solutions emerge that are not consistent with Bayes-optimal predictor that uses the pretraining task distribution as its prior. Instead, the solution is consistent with much broader prior, which allows the learner to adapt to novel tasks that are outside of the pretraining task distribution. Our theory, in contrast, permits explanations for this phenomenon. For instance, perhaps that model used to parameterize Tϕ had insufficient capacity to encode diverse (and potentially complex) prior over tasks, and instead learned simpler approximation with more broad coverage over larger space of tasks."
        },
        {
            "title": "Appendix E Experiment details",
            "content": "In this section, we provide additional experimental details, including comprehensive overview of the model architectures and hyperparameters used during training. E.1 Meta-learner architectures We considered different architectures which exhibit ICL to study and compare their ability to minimize prequential code length (Section 3.3). Each architecture described here parameterises the meta-learner Tϕ. Transformer with bottleneck. We use standard causal decoder-only Transformer with 4 layers, 4 attention heads, 256 latent dimensions and feed-forward network with 512 dimensions. Additionally, it has linear projection that bottlenecks the Transformer to 128 dimension. 5-layer MLP with RELU activations and 256 latent dimensions is used as separate prediction head. The Transformer takes dataset as input in the format [x1, y1], [x2, y2], . . . , [xn, yn] (where xi and yi are concatenated and each [] is token) and computes Tϕ(D1:t1) for each context size starting from 1 to 1. The computation of Tϕ(D1:t1) is based on the encoding of the t-th token, which attends only to tokens that appear to the left of [xt, yt] and itself. Information leakage from future tokens is prevented using causal mask. After computing Tϕ(D1:t1), we concatenate it with xt (i.e., [Tϕ(D1:t1), xt]) and pass this combined input to an MLP prediction head to predict the next y-token. Transformer without bottleneck We use standard decoder-only Transformer with 4 layers, 4 attention heads, 256 latent dimensions and feed-forward network with 512 dimensions. Also, in contrast to the previous architecture we dont use separate prediction head. 17 In-context learning and Occams razor PREPRINT This Transformer takes dataset as input in the format [x1], [y1], [x2], [y2], . . . , [xn], [yn] where both xi and yi are separate tokens which is different from the Transformer with bottleneck. Using positional encoder, we include information about the ordering of the input sequence and while predicting the output for xt, we use causal mask to make sure that the position only attends to itself xt which is the query and the tokens that comes to the left of xt in the input sequence which forms the context of size 1. For each x-token, the Transformer predicts the next y-token from that x-tokens final representation. Mamba We experiment with two state-space model (SSM) architectures, Mamba 1 and Mamba 2, both composed of 4 layers, 256 latent dimensions, state dimensions 8, and local convolution dimension of 4. Additionally, each layer includes gated MLP with 256 latent dimensions. Similar, to the Transformer with bottleneck, the prediction model is 5-layer MLP with RELU activations and 256 latent dimensions is used as separate prediction head. The SSM takes dataset as input in the format [x1, y1], [x2, y2], . . . , [xn, yn] (where xi and yi are concatenated and each [] is token). For each context of size 1, we compute the Tϕ(D1:t1) which is vector that represents the parameters of the output model obtained after processing the first 1 data points. After computing Tϕ(D1:t1), we concatenate it with xt (i.e., [Tϕ(D1:t1), xt]) and pass this combined input to an MLP prediction head to predict the next y-token. E.2 Meta-training and evaluation setup In this section, we outline the complete set of hyperparameters and configurations used across different training objectives and model architectures in our experiments. In-context learner (prequential and train-risk). We trained both the Transformer-based meta-learners (with and without bottleneck) for 50 epochs and the Mamba-based meta-learners for 120 epochs. All results were averaged across 5 different random seeds to mitigate the effect of randomness in the pipeline. The training was conducted on meta-dataset consisting of 10,000 tasks, each with 1,000 data points that serve as context. We used the Adam optimizer (Kingma & Ba, 2017) with learning rate of η = 0.0001 and batch size of 256, without any early stopping. After meta-training, we evaluated the learners on distinct meta-dataset of 100 tasks, each with 1,000 data points. Gradient based learner. Since gradient-based learner are off-the-shelf learning algorithms which dont require meta-training. The prediction model used is 5-layers MLP with RELU activations and latent dimensions of 64 or 256 depending on the complexity of the task. We used meta-dataset of 10000 tasks (with 2000 data points each) split into training (80%) and validation (20%). At each step of prequential coding, we train and evaluate model by randomly sampling dataset of fixed size across each of the tasks, starting from 20 to 2000 datapoints. We used an early stopping criteria with minimum loss delta of 0.001 and patience of 10 epochs to avoid overfitting. On each of them, the prediction model was fit using the Adam optimizer (Kingma & Ba, 2017) with learning rate of η = 0.0001 and batch size of 64. All results were averaged across 15 different random seeds. E.3 Pretrained LLM on Mastermind As described in Section 3.4, we evaluate the performance of pretrained LLM on the Mastermind task using one of the latest OpenAI models GPT-4 (i.e., gpt-4o). To query the model, we used the OpenAI API with temperature of 0, ensuring that the outputs are deterministic. Along with the reponses, we also obtained the log probabilities using the API for calculating the prediction error with respect to each query. This was possible using logprobs (boolean) and top_k_logprobs (integer) attributes in the API that returns log probabilites for each token in the response and the tokens with the top log probabilities corresponding to each token in response. By using structured prompting technique and retry mechanism (up to 10 retries in case of failure to adhere to the required output format), we were able to consistently obtain appropriate responses to our queries. An example prompt, which includes the task description, context, and the query, is provided below. To calculate the prequential code length, we iteratively query novel examples with an increasing number of in-context examples and obtain the prediction errors. This process emulates the prequential ICL objective. Example Prompt have secret code in mind. Its 8-digit code with each digit ranging between 0 and 5. Ill give you couple example guesses, and for each guess Ill tell you two numbers: 18 In-context learning and Occams razor PREPRINT - First number: - Second number: correct position. the number of correct correct digits at their correct position. the number of correct digits, which arent necessarily in the Heres demo to show you what guess and response would look like. secret code was: 0 5 2 1 3 4 2 4 And imagine the guess presented you was: 0 2 1 1 0 2 0 4 Then, the response would be: 3 5 Imagine my The response is the way it is because the first, forth and last digit were in the correct place (first response number is therefore 3) and additionally the second and sixth digit were in the guess but at the wrong position (second response number is therefore 5). The game is about to start. responses. Finally, will present you with new guess, and youll have to predict the correct response. as in the examples (i.e., with 2 digits between 0-8, separated by space). Lets begin. Ill present you with series of guesses and their Make sure your response is formatted the same way Guess: Response: 3 7 4 2 1 3 4 0 0 5 Guess: Response: 2 5 1 1 4 3 5 5 0 1 Guess: Response: 2 3 0 2 2 0 5 3 4 Guess: Response: 1 5 0 2 5 0 4 2 0 1 4 1 3 2 5 4 2 3 Guess: Response: ? - ? What do you think the response is for this final guess? Make sure to reply with just 2 digits between 0-8, separated by single space character. E.4 Hidden Markov Model experiment prominent theory for why ICL emerges from the next-token prediction objective of LLMs is that sequences x1:n in the pre-training dataset (e.g. large corpuses of text) can be interpreted as implicitly being sampled from latent variable generative model Q(x1:n τ ) where τ are some abstract concepts underlying samples (Chan et al., 2022; Xie et al., 2022). τ can range from abstract style attributes in natural language (Xie et al., 2022) to task parameters such as the teacher weight matrix in linear regression ICL task (Von Oswald et al., 2023a); the important part is that some latent variables can be inferred from the context and subsequently aid prediction. ICL would then emerge as the ability of performing implicit Bayesian inference (i.e. learn from the context) in order to predict xt : Q(xt x<t) = (cid:88) τ Q(xt x<t, τ ) (cid:125) (cid:123)(cid:122) (cid:124) Condition on the latent Q(τ x<t) (cid:125) (cid:123)(cid:122) (cid:124) Infer latent (25) We propose to leverage this conceptual framework to devise novel generation procedure for synthetic LLM pre-training dataset. The general idea is to design family of sequence models Qτ (x1:n) parameterized by task latents τ , leading to 19 In-context learning and Occams razor PREPRINT Figure E.1: Validation loss as function of the number of tokens seen during training. The curve is averaged over 5 different datasets (seeds). We can see that the models trained on sequences with shorter length converge faster. the latent variable generative distribution Q(x1:n τ ) = Qτ (x1:n). Specifically, we use Hidden Markov Models (HMMs) as the sequences models, and we parameterize the HMMs Qτ (x1:n) with parameters fξ(τ ) = ψτ . We use this function to introduce hyper-parameters ξ which define the whole family of sequence models; i.e. the dataset. Below, we define in details specific ad-hoc function fξ(τ ) which generates family of HMM where each member share non-trivial structure. E.4.1 Detailed description of the generative process HMM defines probability distribution over sequences of observations xi with discrete-time probabilistic process over hidden states zi paired with mapping . Both and are discrete sets. The hidden process is defined by an initial state distribution π(z) and transition matrix RZZ such that Lastly, the mapping between states and observations is governed by the emission matrix RZX such that Q(zizj) = Aji Q(xjzi) = Bji In the rest of the section, we will explicitly define how fξ(τ ) generates an HMM ψτ = (πτ , Aτ , bτ ) indexed by task latents τ (note that when τ is used as superscript or subscript, it is always to index variable and does not denote an operation). We first give high level description. The hyper-parameters ξ will define number of building blocks which will be used to create the transition and emission matrix of all HMMs. Then τ will specify specific way to combine and manipulate these building blocks to instantiate specific HMM Qτ . For the transition matrix Aτ , the building blocks are pre-defined cycles; which are combined, flipped and accelerated based on τ . For the emission matrix Bτ , the building blocks are groups of sub-emission matrices which each only affect subset of ; which are combined and possibly internal shifted based on τ . Overall, we will have ξ = (N_BASE_CYCLES, N_BASE_SPEEDS, N_CYCLE_FAMILIES, N_GROUP_PER_FAMILY, N_FAMILY_SPEEDS, N_EMISSION_GROUPS, N_EMISSION_PER_GROUP, N_EMISSION_SHIFT) and τ = (BASE_ID, BASE_SPEED, FAMILIES_IDS, FAMILIES_SPEED, EMISSION_IDS, EMISSION_SHIFT) We will refer to the dimensions of ξ, τ as ξi, τi to avoid clutter and discuss further details below. 20 In-context learning and Occams razor PREPRINT Figure E.2: Prequential code curves at different stages of training Reproduction of Figure 3b but with the prequential curve at 610M tokens also. At this point, the models trained with uniform context length have essentially the same performance as the ones trained with smaller context lengths. Transition matrix Aτ . We define cycle as sequence of hidden states = (c0, . . . , cc1), ci Z, and the following manipulation functions DIR(c, k) = (cid:26)(c0, cc1, . . . , c1) if = 1 otherwise. SPEED(c, k) = (c0, ck(mod c), c2k(mod c), . . .) In words, SPEED(c, k) changes the speed at which the cycle is traversed and DIR(c, k) change its direction. We finally define the transition matrix (c) associated with cycle such that (c)ij = (cid:26)1 0 if < s.t (i, j) = (ck, ck+1(mod n)) otherwise. Initially, we randomly generate ξ0 base cycles bi which go through all states zi. Further, we initialize ξ2 families of ξ3 groups of cycles gi j, [ξ1], [ξ2]. Each HMMs transition matrix is then built from these \"building blocks\" cycles. Specifically, Aτ = (SPEED(DIR(bτ0 , τ1), τ2)) + ξ2 (cid:88) τ4,i ξ3 (cid:88) i=1 j=1 (SPEED(DIR(gi j, τ5), τ6)) In words, each transition matrix is made of a) one of ξ0 base cycle, possibly sped up and flipped and b) ξ2 groups of smaller cycles (each from pool of ξ3 groups), possibly sped up and flipped. The number of possible speeds for the base cycle is defined by ξ1. For the cycle families, it is defined by ξ4 Emission matrix Bτ . We separate the states in ξ5 groups hi and for each group we initialize ξ6 subj RhiZ. Then, we define the manipulation function SHIFT(H, k) which applies circular emission matrices shift of to the indices of the matrix. Finally, we have Bτ = ξ5 (cid:88) i=1 SHIFT(Bi τ7,i , τ8) In words, each emission matrix is made of ξ5 possibly overlapping sub-emission matrix, each picked from pool of ξ6 unique ones. The number of possible shifts is ξ7. Initial distribution. We always use the uniform distribution. E.4.2 HMM hyper-parameters For experiments in this paper, we use = 50 and = 20. The hyper-parameters of , ξ, are given in Table E.1. This results in total of 512 different transition matrices and 24 different emission matrices, for total of 12,228 different HMMs. We show results averaged from 5 different seed. 21 In-context learning and Occams razor PREPRINT N_BASE_CYCLES (ξ0) N_BASE_SPEEDS (ξ1) N_CYCLE_FAMILIES (ξ2) N_GROUP_PER_FAMILY (ξ3) N_FAMILY_SPEEDS (ξ4) N_EMISSION_GROUPS (ξ5) N_EMISSION_PER_GROUP (ξ6) N_EMISSION_SHIFT (ξ7) 4 2 3 2 2 3 2 3 Table E.1: HMM dataset hyper-parameters E.4.3 Training We hold out 1,000 HMMs for validation and train on the 11,228 others. Training consists on next-token prediction with cross-entropy loss, using sequences coming from the training HMMs. Specifically, each epochs consists of one sequence sampled from each training HMM. Every epochs, the sequence sampled from given HMM is different (using different seed). As such, the model most likely never sees the same sequence twice. We evaluate on sequences from the 1,000 held-out HMMs. Finally, we use Transformers with 6 layers, 8 heads and embedding dimension of 512. We use batch size of 512 and learning rate of 0.001 with Adam. E.4.4 Evaluation To obtain the curve in Figure 3b, we compute the KL divergence between the next-token distribution of trained models to the ground truth which we can compute explicitly with Equation (25): KL[pmodel(xt x<t), ptrue(xt x<t)] (26) We can compute Equation (25) explicitly because HMMs afford very efficient and parallelizable inference through the forward algorithm. Also, we observe that this \"backward\" KL divergence is simply better version of the cross-entropy loss, used to train the model. Indeed, in the cross-entropy loss, ptrue(xt x<t) is replaced by delta-dirac distribution on the observed x. While training on it also ends up minimizing Equation (26), it is not the best evaluation metric. Indeed, cross-entropy doesnt take into account the stochasticity of the ground-truth, while Equation (26) does. Note that the non-monotonicity of the KL prequential coding curve is consequence of using the above KL. Indeed, when very few datapoints have been seen, the model can learn memorise the marginal probability ptrue(xt x<t) quite easily, bypassing the to perform ICL. This doesnt show when displaying cross-entropy because ptrue(xt x<t) has often very high entropy for small t. E.4.5 Training with shorter sequences When training sequence models like LLMs, the typical approach is to fill the maximum context window of the model with sequences, possibly concatenating multiple ones. This ensures that every batch contains as much tokensi.e. training signalas possible. However, because of this, most tokens seen during training are preceded by lot of tokens: putting more pressure on correctly predicting late tokens than rapidly adapting with small amount of context. According to our theory, this leads to more complex models, possibly worse at generalizing. Based on this reasoning, we propose simple way to bias the meta-learner towards simpler models: training on sequences with random context length, typically much shorter than the maximal one. We show the efficacy of our method using our HMM dataset: models trained with uniform context length (i.e. all sequences have maximal length) need less tokens to arrive at simple models than the ones trained with skewed short context lengths (i.e. sequences of random lengths), as shown in Figure E.1 and Figure E.2. However, there are diminishing returns: with enough data training on long context catches up. Exploring this approach on large-scale language modelling is an interested future work."
        }
    ],
    "affiliations": [
        "Mila Quebec AI Institute",
        "Université de Montréal"
    ]
}