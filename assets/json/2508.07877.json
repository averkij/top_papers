{
    "paper_title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding",
    "authors": [
        "WonJun Moon",
        "Hyun Seok Seong",
        "Jae-Pil Heo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL."
        },
        {
            "title": "Start",
            "content": "WonJun Moon Hyun Seok Seong Jae-Pil Heo Sungkyunkwan University {wjun0830, gustjrdl95, jaepilheo}@skku.edu 5 2 0 2 1 1 ] . [ 1 7 7 8 7 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Facilitating an entitys interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL. 1. Introduction Humans learn to interact with objects by observing others and recognizing relevant object parts in interactions [2, 24]. Similarly, weakly supervised affordance grounding focuses on identifying which parts of an object afford particular interactions within the environment in which humans typically Equal contribution Corresponding author Figure 1. (Up) Goal of WSAG is to identify action-affordable parts within the egocentric image, given exocentric images as contextual hints. (Down) To perform affordance learning, we first discover the part-relevant clues from both egocentric and exocentric images. When these parts are deemed reliable in representing affordancerelevant regions, the model learns to distinguish these parts from the other parts. If not, we instead utilize object-level clues to distinguish objects from the background. Compared to our baseline (LOCATE [24]), which only exploits reliable parts of exocentric images, our approach extends affordance learning to learn from both egocentric and exocentric views and also both from affordance-relevant and affordance-irrelevant clues. By leveraging all these types of clues within the mini-batch at once, the model learns to distinguish affordance-relevant parts from representations of other affordance classes and backgrounds. learn [18, 24, 29, 35, 38, 39, 49]. Specifically, target egocentric image (object-focused) is provided with an action class name and few exocentric images (human-object interaction examples given in third-person view) to localize the affordable parts within egocentric image [29, 35, 39]. Then, the model is trained to localize action-affordable parts when an egocentric image is provided along with an action class. In this vein, knowledge distillation is widely studied [24, 49], along with action classification to produce class activation map (CAM) [56] for localization. For example, LOCATE [24] introduced part-level distillation approach. It extracts action-affordable parts from exocentric images by segmenting interaction-involved regions identified by CAM. These action-affordable parts are distilled into the egocentric image representations only when they are precisely identified, enabling alignment with affordance-relevant regions. Yet, as training without dense annotation progresses, they tend to locate distinguishable parts necessary for action classification even if they are not directly related to the affordable part. This is because affordance-relevant clues are not always clearly distinguishable, thus, the distillation is only applied intermittently. To address this, we go beyond solely focusing on part feature distillation; our primary objective is to consistently provide contextual cues to distinguish between affordance-relevant and affordance-irrelevant representations. The overall intuition is illustrated in Fig. 1. We begin by collecting object-level affordance-relevant clues from both egocentric and exocentric images, then gradually refine them to part-level clues. The model is then trained to focus on these affordable parts via dedicated selective contrastive learning. Specifically, if the identified part clue is deemed to correspond to an affordance region reliably, the model learns to distinguish it from other irrelevant parts. Conversely, if the identified part is deemed unsuitable, the model is trained to distinguish the target object clue (identified using the object affinity map) from the background, preventing attention to affordance-irrelevant regions. First, to collect the clues for action-associated objects, we leverage CLIP [40] to generate an object affinity map that encompasses affordance-relevant parts. The identified target object then serves as basis for discovering partlevel affordance clues. For part discovery within the exocentric view, we refine the part discovery algorithm from LOCATE [24] by leveraging the target object to improve precision. Specifically, the object affinity map is used to filter out object-irrelevant part candidates, ensuring that the affordance-relevant parts belong to the target object. Conversely, to extract part clues in the egocentric view, we exploit the properties of foundation models that CLIP tends to be more responsive to prominent objects [10]. Specifically, we assess part cues by analyzing the difference in model activation between egocentric and exocentric images, where responses tend to be weaker in exocentric images due to smaller object scales and occlusions. Upon gathering object and part clues from both views, we design two types of contrastive learning to leverage the collected affordance-relevant clues. Initially, we propose prototypical contrastive learning to exploit affordancerelevant clues from the exocentric view, offering several key advantages over previously used pairwise distillation strategies [24, 49]. While pairwise distillation focuses solely on reducing the distance between representations of paired egocentric and exocentric images, prototypical contrastive learning not only encourages egocentric-exocentric aligned representations but also distinguishes each prototype from diverse background information and the prototypes of other action classes. This enables the model to capture more discriminative representations specific to each action class. On the other hand, pixel-level contrastive learning further optimizes the localization of affordable parts with precise pixellevel clues. Specifically, it directly uses affordance-relevant clues in egocentric images to disentangle affordance-relevant pixels from the others in each image. This facilitates pixel representations to be distinguished based on their affordance relevance at the level of gathered clues. To sum up, our contributions are: (i) We propose prototypical contrastive learning to benefit part representation learning by leveraging the semantics of other action classes and backgrounds. (ii) We propose pixel contrastive learning to supplement the fine-grained localization of affordancerelevant regions. (iii) We present post-processing step to calibrate CAM prediction by leveraging CLIPs capability to detect text-specified objects. Our approach consistently outperforms (iv) Our approach demonstrates superior performance over prior methods, particularly in challenging unseen scenarios that closely reflect real-world conditions. 2. Related Work 2.1. Visual Affordance Grounding Visual affordance grounding aims to locate the responsible object parts to certain actions [25]. To minimize the gap between perception and action, extensive attention is being put into affordance grounding among the researchers of computer vision and robotics [15, 21, 24]. Initially studied in supervised setting [12, 34, 36], affordance grounding is recently being studied more in weakly supervised scenarios where costly dense annotations are not required [7, 18, 29, 39, 49]. For example, LOCATE [24] uses CAM to identify interaction-involved regions and applies K-means clustering to find the affordance-relevant parts in exocentric images for distillation. WSMA [49] exploited the semantics of CLIP through an attention mechanism to address the limitation of discrete classification labels in illustrating the semantics of actions. Also, more recent works utilize diverse foundation models, such as ALBEF [27], SAM [20], LLAVA [28], and GPT [1], to obtain part-level knowledge [7, 18, 39, 41]. Our approach, despite not relying on recent foundation models, significantly outperforms them by effectively addressing cases where reliable parts cannot be identified and leveraging background context to prevent the model from focusing on affordance-irrelevant regions. Figure 2. Overall flow. Egocentric and exocentric images are processed to perform classification and selective contrastive learning. Note that (view){ego, exo}. 2.2. Weakly Supervised Object Localization Weakly Supervised Object Localization (WSOL) aims to localize objects using only image-level labels. Conventionally, CAM-based methods have been widely studied due to their effectiveness [11, 31, 48, 50, 51, 53, 55]. Yet, these often suffer from shortcut learning [14], which limits CAM coverage, making CAM expansion common strategy for WSOL. For example, HaS [22] randomly masks image patches during training, CutMix [52] enhances masked images, and LoRot [33] introduces pretext tasks involving random scaling and positioning to broaden the models receptive field. similar challenge arises in WSAG, where the goal is to localize affordance-relevant parts for given action class, independent of object categories. Although WSAG also struggles with the model focusing on commonly appearing details within each action class, CAM expansion is not always suitable solution, as affordance-relevant parts are often small. To address this, we propose selective strategy that adaptively determines whether to expand the CAM to the object region when reliable part cannot be identified or to concentrate CAM activation when reliable part is available. Figure 3. Illustration of object discovery. The object affinity map is derived from CLIP as zero-shot image-text similarity map. 3. Method 3.1. Method Overview In Fig. 2, we illustrate the overall framework. Given pair of an egocentric image ego and multiple exocentric images exo, the inputs are processed using DINO [5] followed by projection layers. Then, for prototypical and pixel contrastive learning, features are further projected to obtain (view) and ˆF (view), respectively, where (view) {ego, exo}. We note that the features of the egocentric image are represented as ego, ˆF ego RBHW D, while the features of the exocentric images are given by exo, ˆF exo RBEHW D, where denotes the batch size, and represent the spatial dimensions, is the feature dimension, and indicates the number of exocentric images. While the contrastive learning branch focuses on learning affordance knowledge within the egocentric images, the classification branch with the shared classifier captures the shared semantic information between egocentric and exocentric views. For inference, CAM ego is derived from classification branch using only egocentric images and affordance text prompts. To conduct selective contrastive learning, we first establish target supervision by identifying action-associated objects in Sec. 3.2. Subsequently, in Sec. 3.3 and Sec. 3.4, we introduce prototypical and pixel contrastive learning, respectively, along with the part-level target discovery process. 2.3. Contrastive Learning 3.2. Object Discovery Contrastive learning pulls together instances with positive relationships while pushing apart those with negative relationships [4, 8, 17, 32]. It has been employed in various fields by adapting the criteria for determining the relationships between instances. For instance, augmented pairs of the same instance are regarded as positives in an unsupervised setting [8], while samples within the same class are treated as positives in supervised setting [19]. For WSAG, LLM has been employed to derive the relationships between interaction types [18]. Also, there is significant variation in the units to which contrastive learning is applied. For example, while the images are typical units [8, 9], prototypes [26], pixels [4245, 54], or even the similarity between modalities [40] are popular sources. In this work, we introduce prototypical and pixel contrastive learning that adaptively selects the training level to optimize both objectand part-level regions. As illustrated in Fig. 3, we leverage CLIP to define the object affinity map. Particularly, we employ the strategy of ClearCLIP [23] to enhance local discriminability in visual features. Given egocentric features and exocentric features from CLIP visual encoder, we calculate cosine similarity with CLIP text features of action prompt to obtain an object affinity map for each perspective, namely Aego obj RBHW obj RBEHW (Details for action prompts are and Aexo in Appendix). Note that the term object affinity map is derived from its characteristic to highlight affordance-relevant objects when action prompt is given, as shown in Fig. 7. 3.3. Prototypical Contrastive Learning Prototypical contrastive learning operates upon gathered affordance-relevant clues within exocentric view. Simply put, prototypes for affordable parts in exocentric images are distilled towards corresponding prototypes within egocentric These maps are then assessed to determine whether each corresponding centroid represents the affordance-relevant parts by comparing them with the self-attention map of the egocentric image from DINO [5], measured by pIoU [24] (DINO attention map can be replaced by object affinity map). Finally, only the centroid corresponding to the highest pIoU that exceeds threshold α is selected as the designated part. If either condition is not met, the part (centroid) is considered unreliable and excluded from training. Consequently, for instances with reliable part prototype, part affinity maps A(view) are defined as the similarity between the selected part part prototype and spatial features (i.e., ego and exo). Selective Prototypical Contrastive Learning. Due to the inconsistent availability of affordable part clues, the typical approach is to exploit the knowledge in exocentric images only when the reliable part is discovered. Yet, this triggers the affordance grounding task to become heavily reliant on classification tasks, which is vulnerable in capturing target object parts since its goal is to find the most discriminative features for action classification. Therefore, we design loss function that consistently leverages the knowledge of interaction-involved regions in exocentric images throughout the training. Specifically, our prototypical contrastive learning integrates learning-level selectivity for both the target and anchor representations. When the discovered part prototype within exocentric images is deemed reliable, we use it as the target prototype for distillation into the egocentric object prototype. Otherwise, we define the object prototype to serve as the target and the entire egocentric image as an anchor. This design, which sets the object prototype as the default distillation target, encourages the model to consistently focus on the target object while disregarding background context in egocentric images. Furthermore, when part supervision is available, it reinforces attention to affordable parts, enhancing the models ability to capture details of affordable parts. To leverage the object/part clues in prototypical contrastive learning, we initially construct prototypes. Particularly, four types of prototypes, namely ego+, ego, exo+ and exo, are produced which refer to the positive and negative prototypes of object/part clues in each view depending on the level of gathered clues. In particular, these positive and negative prototypes are constructed with following functions (Φ+ and Φ) using instance feature RHW D, target clue RHW and CAM prediction RHW : Φ+(Z, ) = norm(Pool(Z )), Φ(Z, M, C) = norm(Pool(Z (β C))), (1) where norm() indicates Frobenius normalization along channel axis, Pool() denotes spatial average pooling, and β is bias term to prevent training instability incurred by imprecise CAM at the initial training epoch. Note that is defined {1, . . . , H}, as (X Y)i,j,k = (xi,j,k) (yi,j) , Figure 4. Illustration of prototypical contrastive learning. (Up) Process of identifying part clues in exocentric images. Discovered objects are segmented to extract part candidates, which are then matched with DINOs attention map. (Down) Prototypical contrastive learning is selectively applied based on the reliability of part clues. When reliable, object anchors in egocentric images are attracted toward part clues, but otherwise image anchors are drawn toward object clues in exocentric images. images via contrastive learning. The key advantage of prototypical contrastive approach over previous works is that it encompasses the process of learning negative relationships between prototypes. Thus, we claim that the classification bias towards affordance-irrelevant regions, i.e., background context, and non-affordable object parts, can be mitigated. Part-level Clues within Exocentric Images. We begin by illustrating the process of gathering part-level clues in exocentric images, as shown in Fig. 4. Specifically, we adapt the algorithm used from previous work [24]. To illustrate, whereas previous work directly thresholded CAM prediction exo to identify interaction-involved regions in exocentric images, we first combined exo with the object affinity map Aexo obj before applying threshold γ1. This ensures that the region of interest is constrained to object regions, mitigating the risk of imprecise CAM predictions and improving the affordance relevance of the extracted affordance cues. The rest of the process follows that of previous work [24]. First, based on the intuition that CAM regions consist of background, affordance-relevant part, and other elements, K-means clustering (K=3) is applied. Then, the centroids (candidates of part prototype) are compared with the egocentric DINO [5] feature ego to generate part-egocentric similarity maps. {1, . . . , }, {1, . . . , D} to apply Hadamard product between and in different shapes. In short, the positive prototype maintain consistent focus on target regions by masking with target clue , which is often more precise than CAM prediction, while the background prototype captures general background semantics and unaffordable parts. Subsequently, let denote the index set of both the exocentric and egocentric instances within the mini-batch which represents instances with precise part-level prototypes (we assume that there is only one exocentric image per egocentric image in this subsection, thereby can be shared for simplicity). Then, the egocentric anchor zego of b-th instance and the prototypes are formed as follows: (cid:40) (cid:40) (cid:40) zego = (view)+ = (view) = if I, )) otherwise, , Aego Φ+( ego obj, b) norm(Pool( ego Φ+( (view) Φ+( (view) Φ( (view) Φ( (view) if I, , A(view) part,b ) , A(view) obj,b ) otherwise, , A(view) part,b , (view)) , A(view) obj,b , (view)) otherwise. if I, (2) Figure 5. An illustration of binarizing objects within egocentric images based on affordance criterion. The most salient pixel in each exocentric object affinity map serves as reference, establishing criterion to classify each pixel in the egocentric image as part of an affordable region (Q+) or non-affordable region (Q). The minimum value among ρ1, ρ2, and ρ3 is used as criterion. Consequently, our selective prototypical contrastive learning for b-th instance in mini-batch is expressed as: Lproto = 1 P+ (cid:88) log pP+ n(P+ exp(zego (cid:80) )) p/τ ) exp(z n/τ ) , (3) where and τ denote dot product and temperature parameter, respectively. P+ which represent the sets of positive and negative prototypes for b-th instance are defined as: and P+ = (cid:91) (cid:91) (view) iB {P (view)+ δ(P (view)+ ) = δ(zego )}, (cid:91) (cid:26) (cid:91) = (view) iB {P (view) δ(P (view) ) = δ(zego )} {P (view)+ δ(P (view)+ ) = δ(zego (cid:27) )} , (cid:91) jB (4) (5) where i, denote the index from batch index set B, and δ is function to output the action class label of given instances. Consequently, prototypical contrastive learning directs the models activation toward affordance-relevant regions. Specifically, object-level learning enhances focus on object regions, and when affordance-relevant parts are present, it further refines features to capture part-specific information within the object. 3.4. Pixel Contrastive Learning In prototypical contrastive learning, we encourage the model to prioritize foreground objects over the entire image and, within these objects, to focus on specific parts. However, we remark that only the implicit guidance is provided on each pixel of affordable parts. Thus, we additionally propose pixel contrastive learning to supplement the fine-grained localization capability by learning correspondences between pixels in each egocentric image. Part-level Clues within Egocentric Images. Symmetrically to the use of egocentric view for gathering part clues in exocentric images, we utilize exocentric view as contextual cues to capture part clues in egocentric images. Specifically, we leverage the property of foundation models (CLIP) that they are more responsive to salient objects [10]. Thus, we expect stronger activations for affordable parts in egocentric images compared to their exocentric counterparts when matched with text prompts describing the part to perform specific action with. This is because exocentric images depict objects in use, often capturing them at small scale and making them more susceptible to occlusions. The overall process for egocentric part discovery is illustrated in Fig. 5. Initially, we determine the criterion ρ which is used to distinguish pixels that belong to affordable parts in egocentric images. The logic for deriving ρ RB is: ρ = min eE max h,wH,W Aexo obj . (6) To clarify, we first compute the maximum value across the spatial dimensions (H ) for exocentric object affinity obj RBEHW , resulting in tensor of shape map Aexo E. Then, we select the minimum value along the axis, which is the number of exocentric images paired with each Table 1. Performance comparison on the AGD20K and HICO-IIF datasets. AGD20K-Unseen AGD20K-Seen HICO-IIF KLD SIM NSS KLD SIM NSS KLD SIM NSS Method Clear-CLIP [23] SPA [37] EIL [31] TS-CAM [13] Hotspots [35] Cross-view-AG [29] Cross-view-AG+ [30] LOCATE [24] WSMA [49] WorldAfford [7] AffordanceLLM [39] Rai et al. [41] INTRA [18] Ours Model CLIP - - DeiT 1.723 1.573 0.945 0.357 0.522 0.336 5.528 1.931 1. Zero-Shot Vision-Language Model 0.294 Weakly Supervised Object Localization 7.425 0.221 2.167 0.285 2.104 0.260 Weakly Supervised Affordance Grounding 1.994 0.278 1.787 0.334 1.765 0.342 1.405 0.401 1.335 0.416 1.393 0.406 1.463 - 1.407 0.400 1.365 0.407 1.243 0.433 0.615 0.927 0.981 1.177 1.247 1.255 - 1.223 1.239 1.280 1.773 1.538 1.489 1.226 1.176 1.201 - 1.194 1.199 1.124 ResNet50 ResNet50 ResNet50 DINO DINO+CLIP DINO+CLIP+SAM+GPT-4 LLAVA-7B DINO+CLIP+GPT-3.5T DINOv2+ALBEF+GPT-4 DINO+CLIP 0.262 0. 1.746 0.252 1.032 0.169 0.277 0.201 0.237 0.285 0.279 0.372 0.382 0.380 0.377 0.362 0.375 0.405 0.262 0.330 0. 0.577 0.829 0.882 1.157 1.220 1.225 1.070 1.170 1.209 1.368 - - - - 1.779 1.836 1.593 1.465 - - - - 1.358 - - - - 0.263 0.256 0.327 0.358 - - - - 0.378 - - - - 0.946 0.883 0.966 1.012 - - - - 1.231 egocentric image, ensuring that the weakest response among the available exocentric images is considered. This is because exocentric images do not necessarily capture objects at small scale; rather, some may be framed to emphasize only the specific regions involved in the interaction. Consequently, ρ is exploited to binarize the pixels of Aego obj , distinguishing affordance-relevant parts from other regions. Selective Pixel Contrastive Learning. Part supervision within the egocentric view may not always be available in cases where exocentric images maintain clear and unobstructed focus on target objects. For such circumstances, object-level learning is conducted instead to distinguish target object regions against background pixels. Thus, we utilize the hyperparameter γ2 (equal to γ1) to distinguish target object regions in the egocentric object affinity map Aego obj , finding that single shared value suffices for effective separation. Q+ = { ˆF ego { ˆF ego Consequently, given as the index set that contains indices in which the corresponding egocentric image contains pixels in object affinity map over ρ, positive and negative sets are organized as below: (cid:40) obj,b,h,w > ρb} obj,b,h,w > γ2} b,h,wAego b,h,wAego b,h,wAego b,h,wAego We note that the pixels in positive set Q+ are used as anchors for pixel contrastive learning. Then, pixel contrastive learning is formulated as follows: obj,b,h,w ρb} obj,b,h,w γ2} if J, otherwise. if J, otherwise, { ˆF ego { ˆF ego = (7) (cid:40) Lpix = 1 Q+ 2 (cid:88) (cid:88) zQ+ pQ+ exp(z p/τ ) log (cid:80) exp(z n/τ ) n(Q+ ) . (8) This encourages the models attention to align with the discovered pixel-level clues, ensuring its attention precisely corresponds to affordance-relevant regions. 3.5. Calibrating the Class Activation Map During inference, we follow previous works [24, 29, 49] to directly employ CAM as an output localization map, representing affordable regions. However, CAM predictions often produce Gaussian-like distribution around each salient pixel that extends beyond the actual object boundary. This occurs because convolution-based projection layers are utilized to encode local contexts, which spreads activations across pixels within the receptive fields. To this end, we apply calibration process by performing Hadamard product between the binarized object affinity map and the CAM prediction to limit activations to only the salient parts. Note that the process of binarization of is identical to the process of distinguishing target object regions in Eq. 7. Overall Objective. Our objective involving classification loss, part-level prototypical contrastive loss, and pixel contrastive loss is expressed as = Lce + λ1Lproto + λ2Lpix. 4. Experiments Evaluation Settings. For evaluation, we use two datasets, i.e., AGD20K [29], and HICO-IIF [49]. Results are evaluated with Kullback-Leibler Divergence (KLD), Similarity (SIM), and Normalized Scanpath Saliency (NSS). These metrics evaluate the similarity and the correspondence between the distributions of prediction and ground-truth heatmaps. Also, we employ DINO ViT-S/16 and CLIP ViT-B/16 for all experiments, and set (the number of exocentric images per egocentric image) to 3, following previous works [7, 24, 41, 49]. For hyperparameters, our loss coefficients (λ1 and λ2) are both set to 1. Also, for simplicity, threshold parameters (α and γ) are each set to 0.6, while the bias β and temperature τ are set to 1 and 0.5. These hyperparameters are set the same across all datasets. Further discussions on datasets and implementation details are in the Appendix. Figure 6. Qualitative comparison results of our approach and other methods in seen and unseen domains. 4.1. Comparison with the State-of-the-arts In Tab. 1, we compare our proposed method against WSOL and WSAG methods utilizing various backbones [1, 5, 16, 20, 27, 28, 40, 46]. Models trained for object recognition, such as CLIP and WSOL methods, typically struggle with part-level grounding, as they are not optimized for identifying fine-grained affordance regions within objects. Yet, accurately locating affordance parts is as challenging for WSAG-tailored methods. Thus, methods leveraging recent VLM and LLM have emerged [7, 18, 39, 41]. Particularly, these methods often utilize LLMs to enumerate the characteristics of affordable parts within objects to improve localization with fine-grained specifications. In this work, we follow the experimental settings of [7, 24, 41, 49] to achieve notable performance improvement across various scenarios and datasets, surpassing all previous approaches. Particularly, we highlight the significant improvement in unseen scenarios, where novel objects are introduced for interaction. This is crucial in real-world applications, where object categories cannot be predefined. We attribute these gains to the properties of contrastive learning. First, our approach explicitly redirects attention away from background context and toward affordable parts/objects by enforcing contrastive objectives. This is especially beneficial when handling unseen objects, where the model is more prone to background distractions. Additionally, incorporating an auxiliary self-supervised objective has been shown to enhance generalizability to novel objects [3, 33], further strengthening the models robustness in diverse affordance scenarios. Fig. 6 shows qualitative results in seen and unseen domains. Previous works tend to identify class-wise distinguishable parts rather than focusing on affordable regions. Table 2. Study on model components. From left to right, we examine the benefits of objectand part-level prototypical contrastive learning (Proto.), objectand part-level pixel contrastive learning (Pixel.), and the calibration process with an object affinity map. Cali. indicates the calibration process of the localization map. Obj. and P. denote object-level and part-level learning. Proto. Pixel. Obj. - - P. Obj. - - - - - - - - - P. - - - - - - - Cali. - - - - - - AGD20K-Seen KLD 1.349 1.271 1.271 1.219 1.198 1.164 1.157 1.142 1.124 SIM NSS 1.138 0.365 1.162 0.394 1.153 0.392 1.215 0.402 1.198 0.419 1.290 0.416 1.277 0.414 1.303 0.415 1.280 0.433 (a) (b) (c) (d) (e) (f) (g) (h) (i) For example, the bicycle frames or wheels are often highlighted instead of affordable parts for action ride (i.e., seat or handlebars). Our approach improves affordance precision by encouraging the model to focus on affordance-relevant parts/objects while suppressing its activation on background. 4.2. Ablation Study Our study on component ablation is reported in Tab. 2 with fixed random seed. For our baseline, we employ the model trained solely with the classification loss. We then progressively integrate each learning strategy, noting that each component of our approach contributes positively to affordance grounding. Rows (c) and (d) demonstrate the impact of object-level learning on (a), leading to significant performance improvements in cases. These results validate our strategy to introduce object-level learning for WSAG as Figure 7. Visualization of discovered objects and parts used to guide the training. (a) Object affinity map Aobj. The leftmost sample for each class is an egocentric image and the rest are the exocentric images. (b) Affordable parts of exocentric images used for prototypical contrastive learning. (c) Affordable parts Q+ of egocentric images used for pixel contrastive learning. (a) displays the object affinity maps. Despite that object affinity maps may have imprecise pixel-wise activation and only identify the foreground at coarse granularity, we observe the accuracy in encompassing the action-associated objects. In (b) and (c), we visualize the detected affordable parts for exocentric and egocentric images, respectively. Specifically, (b) visualizes the detected parts within the exocentric view. Although occasional noise is present, the identified parts generally offer reliable guidance for affordance learning. In (c), we exhibit the identified affordable pixels Q+ for part-level pixel contrastive learning on egocentric images where activated pixels generally exhibit contextual consistency. These findings affirm that our training guidance satisfiably reflects our aim of gathering reliable supervision. 5. Conclusion To enhance part-level learning, existing approaches have employed distillation strategies to guide classifiers toward affordance-relevant parts. Yet, since affordance cues are not always distinguishable, training is often dominated by classification, which can lead the model to focus on details frequently appearing in specific classes that may not correspond to affordable parts. To address this issue, we introduced selective prototypical and pixel contrastive objectives that adaptively distinguish affordance-relevant cues from affordanceirrelevant regions at both the part and object levels. Also, we introduced part discovery algorithm to extract affordancerelevant parts within egocentric images while incorporating modified version of an existing approach to identify parts in exocentric images. Lastly, we applied localized map calibration process using an object affinity map to mitigate the activation spread caused by the receptive fields in our convolution-based CAM predictions. Experimental results validate the effectiveness of our approach. Figure 8. Analysis of the impact of each training level, i.e., object and part, with qualitative results. object-level learning mitigates the models confirmation bias toward unaffordable but visibly distinct parts. Additionally, the results in (f) and (h) underscore the impact of direct partlevel learning, as part-level contrastive learning distinguishes affordance-relevant parts and enhances the understanding of partial details. Finally, the calibration process with an object affinity map further improves the performances by bringing two advantages: it refines the boundary and masks out the object-irrelevant activations. The impact of each training level is further illustrated in Fig. 8. Our baseline model tends to focus on distinct parts for each class, with activations predominantly occurring on components such as the brush and center points of knives. Next, we examine the effect of object-level learning, where the model spreads activations from unaffordable parts to the general object. When part-level learning is introduced, the activation becomes more focused on regions that are more likely to be interacted with. Finally, the calibration process filters out the noisy activations surrounding the salient part regions, enhancing the accuracy of affordance grounding. These results demonstrate that the objectives of each training level are appropriately reflected. 4.3. Study on Part and Object Level Supervision In Fig. 7, we analyze each level of training guidance to scrutinize the benefits of our objectand part-level learning. First, Acknowledgements This work was supported in part by MSIT/IITP (No. RS-2022-II220680, 2020-0-01821, RS-2019-II190421, RS2024-00459618, RS-2024-00360227, RS-2024-00437633), MSIT/NRF (No. RS-2024-00357729), KNPA/KIPoT (No. RS-2025-25393280), and SEMES-SKKU collaboration funded by SEMES. Table A1. Affordance grounding results using CLIP-B/16 and ClearCLIP-B/16 in the AGD20k-Seen scenario."
        },
        {
            "title": "ZeroShot\nO\nX\nO\nX",
            "content": "KLD 1.774 1.160 1.574 1.124 SIM 0.250 0.412 0.294 0.433 NSS 0.640 1.267 0.945 1.280 A. Datasets and Implementation Details Datasets. To benchmark weakly supervised affordance grounding (WSAG) methods, we use two datasets, i.e., AGD20K [29], and HICO-IIF [49]. AGD20K is composed of 3,755 egocentric images with 20,061 exocentric images that belong to 36 affordance classes with 50 object classes. Dense annotations are labeled according to the probability of interaction between the human and object regions where Gaussian blur is applied afterwards to generate the heatmaps. HICO-IIF [49] comprises 1,088 egocentric images and 4,793 exocentric images. HICO-IIF is collected from HICO-DET [6] and IIT-AFF [36] where both datasets are equipped with object and affordance categories. Implementation Details. Following previous works [24, 49], we employ DINO ViT-S/16 for all experiments and set E, the number of exocentric images per egocentric image to 3. In addition, we set K, the number of clusters used to segment the objects in exocentric images for part-level prototypical contrastive learning, to 3. The model is optimized using the SGD optimizer with learning rate of 1e-3, weight decay of 5e-4, and batch size of 8. Additionally, while maintaining consistent parameters across datasets, we vary the number of training epochs between ADE20k and HICO-IIF. Specifically, we train the ADE20k dataset for 15 epochs in both seen and unseen scenarios, whereas HICO-IIF is trained for 50 epochs. The extended training duration (3-4x) for HICOIIF accounts for its dataset size, which is approximately 34 times smaller than ADE20k, requiring additional iterations to achieve performance saturation. The MLP is defined with feed-forward network and each projection layer contains two convolution layers, followed by classifier to generate CAMs. Projection layers for each contrastive loss are designed with linear layer with normalization layer. Furthermore, as mentioned in the paper, we employ the strategy of ClearCLIP [23] to enhance local discriminability in the visual features of CLIP ViT-B/16. ClearCLIP introduces three key modifications to the original CLIP architecture in its final layer: (1) removal of the residual connection, (2) reorganization of spatial information through self-self attention (i.e., query-to-query attention [47]), and (3) elimination of the feed-forward network. These modifications are applied without the fine-tuning phase so that it uses the pretrained weights of the original CLIP. The impact of ClearCLIP over naıve CLIP is shown in Tab. A1. Table A2. CLIP prompt comparison in the AGD20k-Seen scenario. {action} represents the action labels."
        },
        {
            "title": "ClearCLIP",
            "content": "Prompt {action} KLD SIM NSS 1.826 0.242 0.522 an item to {action} with 1.774 0.250 0.640 1.672 0.277 0.795 an item to {action} with 1.574 0.294 0.945 {action} Figure A1. Visualization of object affinity map for exocentric image, with various kinds of prompt. (a): {action}, (b): an item to {action} with, (c): multiplication of an item to {action} with and person {action} an item. B. Object Affinity Map In this section, we provide detailed explanation of how the object affinity map is obtained. Using ClearCLIP [23], we apply different strategies to infer object affinity maps for egocentric and exocentric images. For the egocentric affinity map, we calculate the similarity between the egocentric image and action-prompted queries. The action-prompted queries are created by augmenting the action label with fixed prefix, an item to, and postfix, with. For example, the action label catch is augmented as an item to catch with. However, when the action label already ends with with, such as brush with Figure A2. Ablation studies of various hyperparameters. The X-axis denotes the value of each hyperparameter, the Y-axis shows the KLD performance. or cut with, the postfix with is not added. The impact of action-prompted queries is shown in Tab. A2. On the other hand, the object affinity map for exocentric images is generated using two prompting methods to focus primarily on the object parts involved in the interaction within the exocentric image, as shown in Fig. A1. To identify objects in exocentric images, we first use the same actionprompted queries as those applied to egocentric images, as shown in row (b) of Fig. A1. However, we observe that the activation is widely distributed across the foreground objects. To address this, we additionally utilize entity-prompted queries to localize the entity interacting with the objects. We hypothesize that the intersection of the action-prompted and entity-prompted queries will yield more accurate localization map compared to simple similarity map derived solely from action labels. The entity-prompted query is structured with the prefix person and the postfix an item. For example, the action label catch is augmented as person catch an item. Yet, the similarity map obtained using the entity-prompted query may not fully capture the object parts, as the focus is on the entity in the sentence. To address this, we apply local average pooling, which smooths the activation of each patch by averaging it with nearby patches. Finally, we combine the affinity maps generated from the actionand entity-prompted queries by multiplying them to produce the object affinity map for exocentric images in row (c). C. Hyperparameter Ablation Figure A3. Study on loss coefficients. λ1 and λ2 are coefficients for prototypical and pixel contrastive learning, respectively. We vary each coefficient while keeping the others fixed at their default value of 1 and also examine their impact when adjusted simultaneously. sitive than γ. However, both thresholds consistently achieve optimal performance within the range of 0.5 to 0.6. In this work, we set α and γ to 0.6. Additionally, we examine the effects of varying τ , the scaling parameter used in both prototypical and pixel contrastive losses. Results are shown on the right side of Fig. A2. In this work, we set τ to 0.5 as it outcomes the best result. Although the performance slightly decreases when adjusting our hyperparameters, our results demonstrate the robustness of the framework. In particular, our model consistently achieves state-of-the-result performances regardless of hyperparameters α, γ, and τ . Study on loss coefficients are in Fig. A3. As shown, our default value of 1 yields its best result. Nevertheless, our proposed approach consistently outperforms baselines by significant margin, demonstrating its robustness and insensitivity to extensive parameter tuning. We study the impact of thresholds α and γ which control the reliability of selected affordable parts. The threshold α determines whether the part segment within objects in exocentric images corresponds to the desired object part, while γ is used to binarize object affinity map of both egocentric and exocentric images into the foreground targets and the background. Performance comparisons for varying α and γ are illustrated in Fig. A2. Our results indicate that α, used for selecting reliable clusters (groups of pixels), is more senD. Bias on Object and Affordance Classes Objects can be involved in various actions, and likewise, different affordance classes may occur across diverse objects. This presents particular challenge in weakly supervised affordance grounding, where the distinctions between classes are not explicitly provided. In Fig. A4, we examine how our proposed approach performs under such scenarios. First, Fig. A4 (a) illustrates the prediction results when different afFigure A4. Visualization of the test image, ground-truth label, and our prediction on AGD20K dataset. from background regions. Table A3 compares the results obtained using DINO attention maps and CLIP affinity maps, demonstrating the robustness and versatility of our method. F. Additional Qualitative Results Additional qualitative results in comparison to baseline methods are depicted in Fig. A5 and Fig. A6. Particularly, Fig. A5 illustrates the results in the seen domain, while Fig. A6 focuses on the unseen domain. As observed, we find that our proposed approach consistently demonstrates more accurate results than previous works. Table A3. Comparison results between DINO attention map and CLIP affinity map to measure pIoU. Dataset-Scenario Method AGD20K-Seen AGD20K-Unseen KLD SIM NSS DINO-attn 1.124 0.433 1.280 CLIP-obj. 1.126 0.435 1.273 DINO-attn 1.243 0.405 1.368 1.257 0.398 1.360 CLIP-obj. fordance classes are queried for the same object class. While the predictions are not perfectly accurate, the model still exhibits meaningful distinctions between affordance classes despite the absence of explicit class-level cues. Fig. A4 (b) further visualizes how well the model generalizes affordance understanding across diverse object classes, demonstrating notably consistent performance. These results support that our strategy effectively minimizes biases toward specific objectaffordance pairings, promoting robust affordance predictions. E. DINO Attention Map for Prototype Selection In prototype generation for prototypical contrastive learning, we utilize the self-attention map from DINO to measure pIoU, which allows us to select the most suitable prototype among three candidates and perform part-level learning. We emphasize that the DINO attention map can be replaced by any alternative capable of identifying the main object within egocentric images. To validate this flexibility, we conduct experiments using the CLIP affinity map as an alternative, applying specific threshold (0.75) to distinguish foreground Figure A5. Affordance grounding results of our approach and other methods in the seen domain. Figure A6. Affordance grounding results of our approach and other methods in the unseen domain."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 7 [2] Paola Ardon, Eric Pairet, Ronald PA Petrick, Subramanian Ramamoorthy, and Katrin Lohan. Learning grasp affordance reasoning through semantic relations. IEEE Robotics and Automation Letters, 4(4):45714578, 2019. 1 [3] Fabio Carlucci, Antonio DInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22292238, 2019. 7 [4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:99129924, 2020. 3 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3, 4, 7 [6] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect human-object interactions. In 2018 ieee winter conference on applications of computer vision (wacv), pages 381389. IEEE, 2018. 9 [7] Changmao Chen, Yuren Cong, and Zhen Kan. Worldafford: Affordance grounding based on natural language instructions. arXiv preprint arXiv:2405.12461, 2024. 2, 6, [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning In International conference on of visual representations. machine learning, pages 15971607. PMLR, 2020. 3 [9] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1575015758, 2021. 3 [10] Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, and Mohit Bansal. Fine-grained image captioning with clip reward. In Findings of NAACL, 2022. 2, 5 [11] Junsuk Choe and Hyunjung Shim. Attention-based dropout layer for weakly supervised object localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22192228, 2019. 3 [12] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining affordances from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 975983, 2018. 2 [13] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 28862895, 2021. [14] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. 3 [15] Mohammed Hassanin, Salman Khan, and Murat Tahtali. Visual affordance and function understanding: survey. ACM Computing Surveys (CSUR), 54(3):135, 2021. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 7 [17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 3 [18] Ji Ha Jang, Hoigi Seo, and Se Young Chun. Intra: Interaction relationship-aware weakly supervised affordance grounding. In European Conference on Computer Vision, pages 1834. Springer, 2025. 1, 2, 3, 6, 7 [19] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. 3 [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 7 [21] Mia Kokic, Johannes Stork, Joshua Haustein, and Danica Kragic. Affordance detection for task-specific grasping using deep learning. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids), pages 9198. IEEE, 2017. [22] Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing network to be meticulous for weakly-supervised object and action localization. In Proceedings of the IEEE international conference on computer vision, pages 3524 3533, 2017. 3 [23] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. In ECCV, 2024. 3, 6, 9 [24] Gen Li, Varun Jampani, Deqing Sun, and Laura Sevilla-Lara. Locate: Localize and transfer object parts for weakly supervised affordance grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1092210931, 2023. 1, 2, 4, 6, 7, 9 [25] Gen Li, Deqing Sun, Laura Sevilla-Lara, and Varun Jampani. One-shot open affordance learning with foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30863096, 2024. 2 [26] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020. 3 [27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:96949705, 2021. 2, [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 7 [29] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance grounding from exocentric images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22522261, 2022. 1, 2, 6, 9 [30] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Grounded affordance from exocentric view. International Journal of Computer Vision, 132(6):19451969, 2024. 6 [31] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing integrated learning: simple yet effective approach for weakly supervised object localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87668775, 2020. 3, 6 [32] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67076717, 2020. 3 [33] WonJun Moon, Ji-Hwan Kim, and Jae-Pil Heo. Tailoring selfsupervision for supervised learning. In European Conference on Computer Vision, pages 346364. Springer, 2022. 3, [34] Austin Myers, Ching Teo, Cornelia Fermuller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 13741381. IEEE, 2015. 2 [35] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 86888697, 2019. 1, 6 [36] Anh Nguyen, Dimitrios Kanoulas, Darwin Caldwell, and Nikos Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 59085915. IEEE, 2017. 2, 9 [37] Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, and Changsheng Xu. Unveiling the potential of structure preserving for weakly supervised object localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1164211651, 2021. 6 [38] Shengyi Qian and David Fouhey. Understanding 3d object interaction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2175321763, 2023. 1 [39] Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, and Li Erran Li. Affordancellm: Grounding affordance from vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75877597, 2024. 1, 2, 6, [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 7 [41] Arushi Rai, Kyle Buettner, and Adriana Kovashka. Strategies to leverage foundational model knowledge in object affordance grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17141723, 2024. 2, 6, 7 [42] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1954019549, 2023. 3 [43] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Progressive proxy anchor propagation for unsupervised semantic segmentation. In European Conference on Computer Vision, pages 472490. Springer, 2024. [44] Qing Su, Anton Netchaev, Hai Li, and Shihao Ji. Flsl: Featurelevel self-supervised learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [45] Changki Sung, Wanhee Kim, Jungho An, Wooju Lee, Hyungtae Lim, and Hyun Myung. Contextrast: Contextual contrastive learning for semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37323742, 2024. [46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. 7 [47] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking selfattention for dense vision-language inference. In European Conference on Computer Vision, pages 315332. Springer, 2025. 9 [48] Jun Wei, Sheng Wang, Kevin Zhou, Shuguang Cui, and Zhen Li. Weakly supervised object localization through interclass feature similarity and intra-class appearance consistency. In European Conference on Computer Vision, pages 195210. Springer, 2022. 3 [49] Lingjing Xu, Yang Gao, Wenfeng Song, and Aimin Hao. Weakly supervised multimodal affordance grounding for egocentric images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 63246332, 2024. 1, 2, 6, 7, 9 [50] Haolan Xue, Chang Liu, Fang Wan, Jianbin Jiao, Xiangyang Ji, and Qixiang Ye. Danet: Divergent activation for weakly supervised object localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6589 6598, 2019. 3 [51] Shunsuke Yasuki and Masato Taki. Cam back again: Large kernel cnns from weakly supervised object localization perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 341351, 2024. [52] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. 3 [53] Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and Thomas Huang. Adversarial complementary learning for weakly supervised object localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13251334, 2018. 3 [54] Xiangyun Zhao, Raviteja Vemulapalli, Philip Andrew Mansfield, Boqing Gong, Bradley Green, Lior Shapira, and Ying Wu. Contrastive learning for label efficient semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1062310633, 2021. 3 [55] Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, and Fang Wan. Generative prompt model for weakly supervised object localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 63516361, 2023. 3 [56] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29212929, 2016."
        }
    ],
    "affiliations": [
        "Sungkyunkwan University"
    ]
}