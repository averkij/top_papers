{
    "paper_title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
    "authors": [
        "Max Wilcoxson",
        "Qiyang Li",
        "Kevin Frans",
        "Sergey Levine"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 6 7 0 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "LEVERAGING SKILLS FROM UNLABELED PRIOR DATA\nFOR EFFICIENT ONLINE EXPLORATION",
            "content": "Max Wilcoxson, Qiyang Li, Kevin Frans, Sergey Levine UC Berkeley {mwilcoxson, qcli, kvfrans, svlevine}@eecs.berkeley.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe."
        },
        {
            "title": "INTRODUCTION",
            "content": "Unsupervised pretraining has been transformative in many supervised domains, such as language (Devlin et al., 2018) and vision (He et al., 2022). Pretrained models can adapt with small numbers of examples, and with better generality (Radford et al., 2019; Brown et al., 2020). However, in contrast to supervised learning, reinforcement learning (RL) presents unique challenge in that fine-tuning does not involve further mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. Thus, the key challenge to address in pretraining for RL is not simply to learn good representations, but to learn an effective exploration strategy for solving downstream tasks. Pretraining benefits greatly from the breadth of the data. Unlabeled trajectories (i.e., those collected from previous policies whose objectives are unknown) are the most abundantly available, but using them to solve specific tasks can be difficult. It is not enough to simply copy behaviors, which can differ greatly from the current task. There is an entanglement problem general knowledge of the environment is mixed in with task-specific behaviors. concrete example is learning from unlabeled locomotion behavior: we wish to learn how to move around the world, but not necessarily to the locations present in the pretraining data. We will revisit this setting in the experimental section. The entanglement problem can be alleviated through hierarchical decomposition. Specifically, trajectories can broken into segments of task-agnostic skills, which are composed in various ways to solve various objectives. We posit that unlabeled trajectories thus present twofold benefit, (1) as way to learn diverse set of skills, and (2) as off-policy examples of composing such skills. Notably, prior online RL methods that leverage pretrained skills largely ignore the second benefit, and discard the prior trajectories after the skills are learned (Ajay et al., 2021; Pertsch et al., 2021; Hu Equal Contribution 1 Figure 1: SUPE utilizes unlabeled trajectory data twice, both for offline unsupervised skill pretraining and for online high-level policy learning using RL. Left: in the offline pretraining phase (Stage 1), we unsupervisedly learn both trajectory segment encoder (a) and low-level latent conditioned skill policy (b) via behavior cloning objective where the policy is optimized to reconstruct the action in the trajectory segment. Right: in the online exploration phase (Stage 2), the pretrained trajectory segment encoder (a) and an optimistic reward module (d) are used to pseudo-label the prior data and transform it into high-level trajectories (f) that can be readily consumed by high-level off-policy RL agent. Leveraging these offline trajectories and the online replay buffer (e), we learn high-level policy (c) that picks the pretrained low-level skills online to explore in the environment. Finally, the observed transitions and reward values are used to update the optimistic reward module and the online replay buffer. et al., 2023; Chen et al., 2024). We instead argue that such trajectories are critical, and can greatly speed up learning. We make use of simple strategy of learning an optimistic reward model from online samples, and pseudo-relabeling past trajectories with an optimistic reward estimate. The past trajectories can thus be readily utilized as off-policy data, allowing for quick learning even with very small number of online interactions. We formalize these insights as SUPE (Skills from Unlabeled Prior data for Exploration), recipe for maximally leveraging unlabeled prior data in the context of exploration. The prior data is utilized in two capacities, the offline and online phases. In the offline pretraining phase, we extract short segments of trajectories and use them to learn set of low-level skills. In the online phase, we learn high-level exploration policy, and again utilize the prior data by labelling each trajectory segment with an optimistic reward estimate. By double-dipping in this way, we can utilize both the low-level and high-level structure of prior trajectories to enable efficient exploration online. Our main contribution is simple method that leverages unlabeled prior trajectory data to both pretrain skills offline and compose these skills efficiently online for exploration. We instantiate SUPE with variational autoencoder (VAE) to extract low-level skills, and an off-the-shelf offpolicy RL algorithm (Ball et al., 2023) to learn high-level policy from both online and offline data (Figure 1). Our empirical evaluations on set of challenging sparse reward tasks show that leveraging the unlabeled prior data during both offline and online learning is crucial for efficient exploration, enabling SUPE to find the sparse reward signal more quickly and achieve more efficient learning over all prior methods (none of which are able to utilize the data both online and offline)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Unsupervised skill discovery. Unsupervised skill discovery methods first began in the online setting, where RL agents were tasked with learning structured behaviors in the absence of reward signal (Gregor et al., 2016; Bacon et al., 2017; Florensa et al., 2017; Achiam et al., 2018; Eysenbach et al., 2018; Sharma et al., 2020; Hansen et al., 2020; Liu & Abbeel, 2021; Park et al., 2023b). These insights naturally transferred to the offline setting as method of dealing with unlabeled trajectory data. Offline skill discovery methods largely comprise of two categories, those who extract skills based on optimizing unsupervised reward signals (in either the form of policies (Touati et al., 2022; Hu et al., 2023; Frans et al., 2024; Park et al., 2024) or Q-functions (Chen et al., 2024)), and those who utilize conditional behavior-cloning over subsets of trajectories (Shankar & Gupta, 2020; Ajay et al., 2021; Singh et al., 2021; Pertsch et al., 2021; Nasiriany et al., 2022). Closest to our method in implementation are Ajay et al. (2021) and Pertsch et al. (2021), who utilize trajectory-segment VAE to learn low-level skills, and learn high-level policy online. However, in contrast to prior methods which all utilize offline data purely for skill-learning and do not keep it around during online training, we show that utilizing the data via relabeling is critical for fast exploration. Offline to online reinforcement learning. The offline-to-online reinforcement learning methods (Xie et al., 2021; Song et al., 2023; Lee et al., 2022; Agarwal et al., 2022; Zhang et al., 2023; Zheng et al., 2023; Ball et al., 2023; Nakamoto et al., 2024) focus on efficient online learning with the presence of offline data (often labeled with the reward value). Many offline RL approaches can be applied to this setting simply run offline RL first on the offline data to convergence as an initialization and then continue training for online learning (using the combined dataset that consists of both offline and online data) (Kumar et al., 2020; Kostrikov et al., 2021; Tarasov et al., 2024). However, such approaches often result in slow online improvements as offline RL objectives tend to overly constrain the policy behaviors to be close to the prior data, limiting the exploration capability. On the other hand, off-policy online RL methods can also be directly applied in this setting by directly treating the offline data as additional off-policy data in the replay buffer and learning the policy from scratch (Lee et al., 2022; Song et al., 2023; Ball et al., 2023). While related in spirit, these methods cannot be directly used in our setting as they require offline data to have reward labels. Data-driven exploration. common approach for online exploration is to augment reward bonuses to the perceived rewards and optimize the RL agent with respect to the augmented rewards (Stadie et al., 2015; Bellemare et al., 2016; Houthooft et al., 2016; Pathak et al., 2017; Tang et al., 2017; Ostrovski et al., 2017; Achiam & Sastry, 2017; Burda et al., 2018; Ermolov & Sebe, 2020; Guo et al., 2022; Lobel et al., 2023). While most exploration methods operate in the purely online setting and focus on adding bonuses to the online replay buffer, recent works also start to explore more data-driven approach that makes use of an unlabeled prior data to guide online exploration. Li et al. (2024) explore adding bonuses to the offline data, allowing them to optimize the RL agent to be optimistic about states in the data, encouraging exploration around the offline data distribution. Our method explores similar idea of adding bonuses to the offline data but for training high-level policy, allowing us to compose pretrained skills effectively for exploration. Hu et al. (2023) explore slightly different strategy of learning number of policies using offline RL that each optimizes for random reward function. Then, it samples actions from these policies online to form an action pool from which the online agent can choose to select for exploration. This approach does not utilize offline data during the online phase and require all the policies (for every random reward function) to be represented separately. In contrast, our method makes use of the offline data as off-policy data for updating the high-level policy and our skills are represented using single network (with the skill latent being the input to our network). As we will show in our experiments, being able to use offline data is crucial for learning to explore in the environment efficiently. Options framework. Our method is also related to the option framework (Sutton et al., 1999; Menache et al., 2002; Mannor et al., 2004; Simsek & Barto, 2004; Simsek & Barto, 2007; Konidaris, 2011; Daniel et al., 2016a; Srinivas et al., 2016; Daniel et al., 2016b; Fox et al., 2017; Bacon et al., 2017; Kim et al., 2019; Bagaria & Konidaris, 2019; Bagaria et al., 2024). Different from the approach we take that learns latent skills with fixed time horizon (H = 4 in all our experiments), the options framework provides more flexible way to learn skills with varying time horizon, often defined by learnable initiation and/or termination conditions (Sutton et al., 1999). We opt for the simplified skill definition because it allows us to bypass the need to learn initiation or termination conditions, and frame the skill pretraining phase as simple supervised learning task."
        },
        {
            "title": "3 PROBLEM FORMULATION",
            "content": "We consider Markov decision process (MDP) = {S, A, , γ, r, ρ} where is the set of all possible states, is the set of all possible actions that policy π(as) : (cid:55) P(A) may take, (ss, a) : (cid:55) P(S) is the transition function that describes the probability distribution over the next state given the current state and the action taken at the state, γ is the discount factor, r(s, a) : (cid:55) is the reward function, and ρ : P(S) is the initial state distribution. We have access to dataset of transitions that are collected from the same MDP with no reward labels: = {(si, ai, i)}. During online learning, the agent may interact with the environment by taking actions and observes the next state and the reward specified by transition function and the reward function r. We aim to develop method that can leverage the dataset to efficiently explore in the MDP to collect reward information, and outputs well-performing policy π(as) that achieves good cumulative return in the environment η(π) = E{s0ρ,atπ(atst),st+1P (st,at)} t=0 [γtr(st, at)]. Note that this is different from the zero-shot RL setting (Touati et al., 2022) where the reward function is specified for the online evaluation (only unknown during the unspervised pretraining phase). In our setting, the agent has zero knowledge of the reward function and must actively explore in the environment to identify the task it needs to solve by receiving the reward through environment interactions. (cid:80)"
        },
        {
            "title": "4 SKILLS FROM UNLABELED PRIOR DATA FOR EXPLORATION (SUPE)",
            "content": "In this section, we describe in detail how we utilize the unlabeled trajectory dataset to accelerate online exploration. Our method, SUPE, can be roughly divided into two parts. The first part is an offline pretraining phase where we extract skills from the unlabeled prior data with an trajectorysegment VAE. The second part is the online learning phase where we train high-level off-policy agent to compose the pretrained skills leveraging examples from both prior data and online replay buffer. Algorithm 1 describes our method. Algorithm 1 SUPE 1: Input: Unlabeled dataset of trajectories D, trajectory segment length and batch size B. 2: for each pretraining step do 3: 4: Sample batch of trajectory segments of length H, {τ1, , τB} from Optimize the skill policy πθ(as, z), the trajectory encoder fθ(zτ ), along with the statedependent prior pθ(zs) with the VAE loss 1 (cid:80)B i=1 Lθ(τi) (Equation 1) 5: end for 6: Dreplay 7: Initialize the optimistic reward module rUCB(s, z) (following (Li et al., 2024)) 8: for every online environment steps do 9: 10: 11: 12: 13: 14: Sample the trajectory latent πψ(zs) Run the skill policy πθ(as, z) for steps in the environment: {s0, a0, r0, , sH } Add the high-level transition to buffer Dreplay Dreplay {(s0, z, sH , (cid:80)H1 Sample batch of trajectory segments of length H, {τ1, , τB} from Encode each trajectory segment using the trajectory encoder: ˆzi fθ(zτi) Use fθ and rUCB to transform each unlabeled trajectory segment into high-level transition i=0 [γiri])}. with pseudo-labels (Equation 2): Boffline = {(si 0, ˆzi, ˆri, si )}B i=1 Sample batch Bonline from Dreplay Run off-policy RL update on Bonline Boffline to train πψ(zs). 15: 16: 17: end for 18: Output: hierarchical policy consisting of high-level πψ(zs) and low-level πθ(as, z). Pretraining with trajectory VAE. Since we only have access to an unlabeled dataset of trajectories, we must capture all the behaviors in the data as accurately as possible. At the same time, we aim to make the dataset directly usable for training high-level skill-setting policy in hope that this high-level policy can be trained in more sample-efficient way (compared to only having access to the online samples). We achieve this by adopting trajectory VAE design from prior methods (Ajay et al., 2021; Pertsch et al., 2021) where short segment of trajectory τ = {s0, a0, s1, , sH1, aH1} is first fed into trajectory encoder fθ(zτ ) that outputs distribution over the latent code z, then skill policy πθ(as, z) is used to reconstruct the actions in the 4 trajectory segment. Such design helps us directly map trajectory segments to their corresponding skill policies, effectively allowing us to transform the segment into high-level transition in the form of (current state: s0, action: z, next state: sH ). As result, such transition can be directly consumed by any off-policy RL algorithm in the online phase to update the high-level policy πψ(zs) (as we will explain in the next section in more details). We also learn state-dependent prior pθ(zs), following the prior works (Pertsch et al., 2021), to help accommodate the difference in behavior diversity of different states. Putting them all together, the loss is shown in Equation 1. (cid:35) Lθ(τ ) = βDKL(fθ(zτ )pθ(zs0)) Ezfθ(zτ ) log πθ(ahsh, z) . (1) (cid:34)H1 (cid:88) h= Online exploration with trajectory skills. Our main goal in the online phase is to learn high-level off-policy agent that decides which skill to use at regular interval of time steps to learn the task quickly. The agent consumes high-level transitions where the state and the next state are separated by time steps and the action corresponds to the trajectory latent that is used to retrieve the lowlevel actions from the skill policy π(as, z). To make use of the prior data and generate high-level transitions from it, we need both the action and the reward label for each pair of states (that are separated by steps) in the trajectory. For the action, we can simply sample from the trajectory encoding of the trajectory segment enclosed by the state pair. For the reward, we maintain an upperconfidence bound (UCB) estimate of the reward value for each state and skill pair (s, z) inspired by the prior work (Li et al., 2024) (where it does so directly in the state-action space (s, a)), and pseudo-label the transition with such an optimistic reward estimate. The optimistic reward estimate is recomputed before updating the high level agent, since the estimate changes over time, while the trajectory encoding is computed before starting online learning, since this label does not change. The relabeling is summarized below: , ˆz fθ(zτ ) , ˆr = rUCB(s0, ˆz) ). (2) ( s0 state labeled action labeled reward , sH next state Practical implementation details. Following prior work on trajectory-segment VAEs (Ajay et al., 2021; Pertsch et al., 2021), we use Gaussian distribution (with both mean and diagonal covariance learnable) for the trajectory encoder, the skill policy, as well as the state-dependent prior. While Pertsch et al. (2021) use KL constraint between the high-level policy and the state-dependent prior, we use simpler design without the KL constraint that works much better (as we show in Appendix E). To achieve this, we adapt the policy parameterization from (Haarnoja et al., 2018), where the action value is enforced to be between 1 and 1 using tanh transformation, and entropy regularization is applied on the squashed space. We use this policy parameterization for the highlevel policy π(zs) to predict the skill action in the squashed space zsqaushed. We then recover the actual skill action vector by unsquashing it according to = arctanh(zsqaushed), so that it can be used by our skill policy πθ(as, z). For upper-confidence bound (optimistic) estimation of the reward, (rUCB(s0, ˆz)), we directly borrow the UCB estimation implementation in Li et al. (2024) (Section 3, practical implementation section in their paper), where they use combination of the random network distillation (RND) (Burda et al., 2018) reward bonus and the predicted reward from reward model (see Appendix C, Ours for more details). For the off-policy high-level agent, we follow Li et al. (2024) to use RLPD (Ball et al., 2023) that takes balanced number of samples from the prior data and the online replay buffer for agent optimization. In addition to using the optimistic offline reward label, we also find that adding the RND reward bonus to the online batch is also helpful to encourage online exploration, so we use it in all our experiments."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "We present series of experiments to evaluate the effectiveness of our method to discover fast exploration strategies. We specifically focus on long-horizon, sparse-reward settings, where online exploration is especially important. In particular, we aim to answer the following questions: 1. Can we leverage unsupervised trajectory skills to accelerate online learning? 2. Is our method able to find goals faster than prior methods? 3. Does offline data help our method to compose skills better for faster exploration? 5 a) AntMaze: three maze layouts (medium, large and ultra), and four goals for each layout. b) Kitchen c) Visual AntMaze: maze layout and image observation examples Figure 2: We experiment on three challenging, sparse-reward domains: AntMaze, Kitchen, and Visual AntMaze. a): AntMaze (Fu et al., 2020) (state-based) with three different maze layouts (antmaze-medium, antmaze-large, and antmaze-ultra) and the corresponding four goal locations (denoted as the red dots) that we experiment with for each of the layouts; b): Kitchen (Fu et al., 2020) (state-based); c): Visual AntMaze (Park et al., 2023a) with colors added to the floor with local 64 64 image observations (e.g., see examples right of the maze). The color of the floor uniquely identifies the ants position within the maze. For both state-based and visual AntMaze, the ant starts at the bottom-left corner in the beginning of every episode. 5.1 EXPERIMENTAL SETUP We conduct our experiments on three challenging sparse reward domains (Figure 2). D4RL AntMaze with Additional Goal Locations. D4RL AntMaze is standard benchmark for offline-to-online RL (Fu et al., 2020; Ball et al., 2023) where an ant robot needs to navigate around maze to specified goal location. We benchmark on three mazes of increasing size, antmaze-medium, antmaze-large, and antmaze-ultra. We take the D4RL dataset for medium and large mazes as well as the dataset from Jiang et al. (2022) for the ultra maze (we use the diverse version of the datasets for all these layouts). We then remove the termination and reward information from the dataset such that the agent does not know about the goal location priori. For each of the medium, large, ultra mazes, we test with four different goal locations that are hidden from the agent. See Figure 2a for visualization of the mazes and the four goal locations that we use for each of them. We use 1/0 sparse reward function where the agent receives 1 when it has not found the goal and it recieves 0 when it reaches the goal location and the episode terminates. The ant always starts from the left bottom corner of the maze, and the goal for the RL agent is to reach the goal consistently from the start location. It is worth noting that the goal is not known priori and the offline data is unlabeled. In order to learn to navigate to the goal consistently, the agent first needs to traverse in the maze to gather information about where the goal is located. D4RL Kitchen is another standard benchmark for offline-to-online RL (Fu et al., 2020; Nakamoto et al., 2024), where Franka robot arm is controlled to interact with various objects in simulated kitchen scenario. The desired goal is to complete four tasks (open the microwave, move the kettle, flip the light switch, and slide open the cabinet door) in sequence. In the process, the agent attains reward equal to the number of currently solved tasks. The benchmark contains three datasets, kitchen-mixed, kitchen-partial, and kitchen-complete. kitchen-complete is the easiest dataset, which only has demonstrations of the four tasks completed in order. kitchen-partial adds additional tasks, but the four tasks are sometimes completed in sequence. kitchen-mixed is the hardest, where the four tasks are never completed in sequence. To adapt this benchmark in our work, we remove all the reward labels in the offline dataset. Aside from the two state-based domains above, we also consider visual domain below to test the ability of our method in scaling up to high-dimensional image observations. Visual AntMaze is benchmark introduced by Park et al. (2023a), where the agent must rely on 64 64 image observations of its surroundings, as well as proprioceptive information including body joint positions and velocities to navigate the maze. In particular, the image is the only way for the agent to locate itself within the maze, so successfully learning to extract location information from the image is necessary for successful navigation. The floor is colored such that any image can uniquely identify position in the maze. The maze layout is the same as the large layout in the state-based D4RL AntMaze benchmark above and we also use the same additional goals. The reward function and the termination condition are also the same as the state-based benchmark. We use the normalized return, standard metric for D4RL (Fu et al., 2020) environments, as the main evaluation metric. For the Kitchen domain, the normalized return represents the average percentage of the tasks that are solved. The normalized return for the AntMaze domains (state-based and visual) represents the average success rate of reaching the goal. 5.2 COMPARISONS While there is no existing method in our problem setting that utilizes unlabeled prior data in both the pretraining phase and the online learning phase, there are methods that make use of the prior data in either phase. We first consider two baselines that do not use pretraining and instead directly perform online learning. Online. This baseline completely discards the offline data and the exploration is done with online reward bonus implemented by random network distillation (RND) (Burda et al., 2018). For all the baselines below as well as our method, we add online RND bonus to the replay buffer to encourage exploration. ExPLORe (Li et al., 2024). This baseline is similar to our method in the sense that it also uses exploration bonus and offline data to encourage exploration. The one crucial difference is that it does not perform unsupervised skill pretraining and learns 1-step policy directly online. As we will show, pretraining is crucial for our method to find goals faster and lead to more efficient online learning. It is worth noting that the original ExPLORe method does not make use of online RND. To make the comparison fair, we additionally add online RND bonus to this baseline to help it explore better online. For completeness, we also include the performance of the original ExPLORe method in the Appendix (Figure 8). We then consider additional baselines that use the prior data during pretraining phase but do not use the data during online learning. Diffusion BC + JSRL. This baseline is an upgraded version of the BC + JSRL baseline used in ExPLORe (Li et al., 2024). Instead of using Gaussian policy (as used by Li et al. (2024)), we use an expressive diffusion model to behavior clone the unlabeled prior data. At the beginning of each online episode, we roll out the policy for random number of steps from the initial state before using switching to the online RL agent (Uchendu et al., 2023; Li et al., 2023). One might expect that an expressive enough policy class can model the behavior of the prior good enough such that it can form good prior for exploration online. Online with Skills. We also consider two skill-based baselines where the prior data is discarded in the online phase and the high-level policy is trained from scratch online with exploration bonus. We experiment with two types of pretraining skills. The first one is the trajectory VAE skill used in our method. The second one is from recently proposed unsupervised offline skill discovery method where skills are pretrained to be able to traverse learned Hilbert representation space (Park et al., Figure 3: Aggregated normalized return across three different domains. Ours achieves the best performance through training on all three domains. ExPLORe achieves strong later stage performance on AntMaze, but struggles in high-dimensional Visual AntMaze and Kitchen tasks. Online w/ HILP Skills and HILP w/ Offline Data achieve decent initial return on Kitchen, but struggle to learn in all three domains. Online w/ Trajectory Skills consistently underperforms Ours across all three environments. Diffusion BC + JSRL learns reasonably well in Kitchen, but performs much worse in AntMaze and Visual AntMaze. Online does not perform competitively at any stage of exploration. Section 5.2 contains details on the baselines we compare with. Each curve is an average over 8 seeds. For AntMaze, we aggregate over 3 maze layouts and 4 goals. For Kitchen, we aggregate over 3 tasks. For Visual AntMaze, we aggregate over 4 goals on one maze layout. 2024) (HILP). We use the exact same high-level RL agent as our method except that the agent no longer makes use of the prior data online. Finally, we also consider novel baseline that also uses prior data during pretraining and online exploration, but uses HILP skills rather than trajectory-based skills. HILP w/ Offline Data. We observe that HILP skills can also utilize the offline data via relabeling. Recall that HILP learns latent space of the observations (via an encoder ϕHILP) and learns skills that move agent in certain direction (skill) in the latent space. For any high-level transition (s0, sH ), we simply take ˆz ϕHILP(sH )ϕHILP(s0) , the normalized difference vector that points ϕHILP(sH )ϕHILP(s0)2 from s0 to sH in the latent space. We use the normalized difference vector because the pretrained HILP skill policy takes in normalized skill vectors. We use the exact same high-level RL agent as our method except that the skill relabeling is done by computing the latent difference rather than using the trajectory encoder (fθ(zτ )). For the visual antmaze environment, we use the same image encoder used in RLPD (Ball et al., 2023). We also follow one of our baselines, ExPLORe (Li et al., 2024), to use ICVF (Ghosh et al., 2023), method that uses task-agnostic value functions to learn image/state representations from ICVF takes in an offline unlabeled trajectory dataset with image observations and passive data. pretrain an image encoder in an unsupervised manner. Following ExPLORe, we take the weights of the image encoder from ICVF pretraining to initialize the image encoders weights in the RND network. To make the comparison fair, we also apply ICVF to all our baselines (see details in Appendix D). 5.3 CAN WE LEVERAGE UNSUPERVISED TRAJECTORY SKILLS TO ACCELERATE ONLINE LEARNING? Figure 3 shows the aggregated performance of our approach on all three domains. Overall, our method outperforms all prior methods. Both HILP-based methods (Online with HILP Skills and HILP with Offline Data) achieve good initial return on the Kitchen domain, but improve very slowly during online training. The Online with Trajectory Skills baseline also consistently performs worse than our method across all three domains, which demonstrates the importance of using prior data for online learning of the high-level policy, since that is only difference between this baseline and Ours. ExPLORe uses offline data during online learning, but does not pretrain skills, leading to slower learning on all three environments and difficulty achieving any significant return 8 Figure 4: Normalized return on individual AntMaze and Kitchen tasks. Ours achieves the strongest performance on all tasks. Online w/ Trajectory Skills learns much slower on all AntMaze tasks, and is asymptotically worse on two of the three more challenging Kitchen tasks. ExPLORe struggles to learn on Kitchen, and performs worse as maze size increases. None of the other baselines are competitive on any tasks. Each curve is an average over four goals with 8 seeds for AntMaze, and 16 seeds for Kitchen. on Visual AntMaze or Kitchen. We also report the performance on individual AntMaze mazes and Kitchen tasks in Figure 4, and observe that our method outperforms the baselines more on harder environments. For the AntMaze domain, as the maze size increases, the two best baselines, Online w/ Trajectory Skills and ExPLORe, perform worse, while Ours continues to achieve similar level of performance. For the Kitchen domain, on the more challenging kitchen-mixed and kitchen-partial tasks, the improvement of Ours over Online w/ Trajectory Skills is also larger. These experiments suggest that pretraining skills and the ability to leverage the prior data are both crucial for achieving efficient online learning, especially in more challenging environments. For the visual domain, we additionally perform an ablation study to assess the importance of the ICVF pre-trained representation, which we include in Appendix D. While ICVF combines synergistically with our method to further accelerate learning and exploration, initializing RND image encoder weights using ICVF is not critical to its success. Even though we have demonstrated that our method is able to achieve higher success rate faster than prior works, it is still not clear if our method can actually lead to better exploration (instead of simply learning the high-level policy better). In the following two sections, we study the exploration aspect in isolation in the AntMaze domain. 5. IS OUR METHOD ABLE TO FIND GOALS FASTER THAN PRIOR METHODS? Table 1 shows the average number of online environment interaction steps for the agent to reach the goal for each of the four goals in each of the three maze layouts. Such metric allows us to assess how efficiently the agent explores in the maze whereas the success rate metric only measures how good the agent is at reaching the desired goal. It is possible for an agent to be good at exploration, but bad at reaching goals consistently and vice-versa. Table 1 shows that our methods average first 9 goal time outperforms or matches every baseline on each of the 12 goals, which confirms that our method not only learns faster, but does so by exploring more efficiently. Maze Layout Goal Location Methods without Pretraining Online + RND ExPLORe Diffusion BC w/ JSRL Top Left Top Right Bottom Right Center Aggregated Top Left Top Right Bottom Right Top Center Aggregated Top Left Top Right Bottom Right Top Center Aggregated 71 5.0 100 16 230 38 210 32 150 14 72 10 220 20 280 15 220 28 200 8.9 76 7.0 300 0.0 300 0.0 230 35 230 9.3 190 6. Medium Large Ultra Aggregated 27 3.2 29 2.8 35 4.9 71 8.0 40 2.0 33 2.9 49 7.7 34 1.8 48 5.2 41 2.7 34 4.9 92 20 70 8.0 29 5.5 56 5.4 46 2.3 60 8.1 85 19 99 15 260 28 130 10 52 3.3 220 28 160 22 120 8.8 140 13 91 11 290 7.8 300 0.0 230 29 230 9.1 160 6. Methods with Pretraining Online w/ Trajectory Skills 21 4.1 76 26 77 34 26 3.4 50 11 22 4.2 190 27 140 22 59 12 100 13 36 11 120 14 130 16 75 16 90 7.1 80 5.9 Online w/ HILP Skills 120 47 160 40 300 0 260 28 210 17 300 0 280 20 280 21 240 23 270 12 39 21 260 19 240 28 100 32 160 14 210 11 HILP w/ Offline Data 27 6.2 72 36 270 33 300 0.0 170 13 300 0.0 110 36 260 19 33 8.5 180 13 15 5.3 150 32 67 12 17 1.7 61 8.2 130 7.4 Ours 14 3.1 22 3.2 22 4.4 18 1.7 19 1.8 21 2.8 27 2.6 21 1.8 39 6.2 27 1.7 17 3.6 37 5.5 34 6.0 22 4.4 27 2.4 25 1. Table 1: The number of environment steps (103) taken before the agent find the goal. Lower is better. The first goal time is considered to be 300 103 steps if the agent never finds the goal. We see that our method is the most consistent, achieving performance as good as or better than all other methods in each of the 4 goals across 3 different maze layouts. The error quantity indicated is standard error over 8 seeds. Figure 5: Coverage on three different AntMaze mazes, averaged over runs on four goals. Ours has the best coverage performance on the challenging antmaze-ultra, and is only passed by HILP w/ Offline Data on antmaze-large. Online w/ Traj. Skills and Online with HILP Skills struggle to explore after initial learning, and Online and Diffusion BC + JSRL generally perform poorly at all time steps. 5.5 DOES OFFLINE DATA HELP OUR METHOD TO COMPOSE SKILLS BETTER FOR FASTER EXPLORATION? Figure 5 shows the percentage of the maze that the agent has covered throughout the training. The coverage of skill-based methods that do not use prior data during online learning, Online w/ Trajectory Skills and Online w/ HILP Skills, significantly lags behind baselines that use offline data after 50,000 environment steps. Many methods achieve similar coverage on antmaze-medium, likely because the maze is too small to differentiate the different methods. Ours is able to achieve the highest coverage on the antmaze-ultra, and is only surpassed on antmaze-large by HILP w/ Offline Data, which has high first goal times and slow learning. Thus, the coverage difference can likely be at least partially attributed to HILP w/ Offline Data struggling to find the goal and continuing to explore after finding the goal. All non-skill based methods struggle to get competitive coverage levels on antmaze-large and antmaze-ultra. This suggests both pretraining skills and the ability to leverage prior data online are crucial for efficient exploration, and our method effectively compounds their benefits. For completeness, we include the success rate and coverage results for each maze layout and goal location in Appendix F."
        },
        {
            "title": "6 DISCUSSION AND LIMITATIONS",
            "content": "In this work, we propose novel method, SUPE, that leverages unlabeled prior trajectory data to accelerate online exploration and learning. The key insight is to use unlabeled trajectories twice, to 1) extract set of low-level skills offline, and 2) serve as additional data for high-level offpolicy RL agent to compose these skills to explore in the environment. This allows us to effectively combine the strengths from unsupservised skill pretraining and sample-efficient online RL methods to solve series of challenging long-horizon sparse reward tasks significantly more efficiently than existing methods. Our work opens up avenues in making full use of prior data for scalable, online RL algorithms. First, our pre-trained skills remain frozen during online learning, which may hinder online learning when the skills are not learned well or need to be updated as the learning progresses. Such problems could be alleviated by utilizing better skill pretraning method, or allowing the lowlevel skills to be fine-tuned online. second limitation of our approach is the reliance on RND to maintain an upper confidence bound on the optimistic reward estimate. Although we find that RND works without ICVF on high-dimensional image observations in Visual AntMaze, the use of RND in other high dimensional environments may require more careful consideration. Possible future directions include examining alternative methods of maintaining this bound."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research used the Savio computational cluster resource provided by the Berkeley Research Computing program at UC Berkeley, and was supported by ONR through N00014-22-1-2773, AFOSR FA9550-22-1-0273, and the AI Institute. We would like to thank Seohong Park for providing his implementation of OPAL which was adapted and used for skill pretraining in our method. We would also like to thank Seohong Park, Fangchen Liu, Junsu Kim, Dibya Ghosh, Katie Kang, Oleg Rybkin, Kyle Stachowicz, Zhiyuan Zhou for discussions on the method and feedback on the early draft of the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv preprint arXiv:1703.01732, 2017. Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299, 2018. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2895528971. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/ba1c5356d9164bb64c446a4b690226b0-Paper-Conference.pdf. Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline In International Conferprimitive discovery for accelerating offline reinforcement learning. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= V69LGwJ0lIN. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. Akhil Bagaria and George Konidaris. Option discovery using deep skill chaining. In International Conference on Learning Representations, 2019. Akhil Bagaria, Ben Abbatematteo, Omer Gottesman, Matt Corsaro, Sreehari Rammohan, and George Konidaris. Effectively learning initiation sets in hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Philip Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In International Conference on Machine Learning, pp. 15771594. PMLR, 2023. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 14711479, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neural Information Processing Systems (NeurIPS), 2020. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Boyuan Chen, Chuning Zhu, Pulkit Agrawal, Kaiqing Zhang, and Abhishek Gupta. Self-supervised reinforcement learning that transfers using random features. Advances in Neural Information Processing Systems, 36, 2024. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Christian Daniel, Gerhard Neumann, Oliver Kroemer, and Jan Peters. Hierarchical relative entropy policy search. Journal of Machine Learning Research, 17(93):150, 2016a. Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104:337357, 2016b. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Aleksandr Ermolov and Nicu Sebe. Latent world models for intrinsically motivated exploration. Advances in Neural Information Processing Systems, 33:55655575, 2020. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without reward function. arXiv preprint arXiv:1802.06070, 2018. Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reIn International Conference on Learning Representations, 2017. URL inforcement learning. https://openreview.net/forum?id=B1oK8aoxe. Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options. arXiv preprint arXiv:1703.08294, 2017. Kevin Frans, Seohong Park, Pieter Abbeel, and Sergey Levine. Unsupervised zero-shot reinforcement learning via functional reward encodings. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1392713942. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/frans24a.html. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Dibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive data via latent intentions. In International Conference on Machine Learning, pp. 1132111339. PMLR, 2023. 12 Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016. Zhaohan Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent Altché, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. Advances in neural information processing systems, 35: 3185531870, 2022. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. PMLR, 2018. Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=BJeAHkrYDS. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: Implicit Q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 11091117, 2016. Hao Hu, Yiqin Yang, Jianing Ye, Ziqing Mai, and Chongjie Zhang. Unsupervised behavior extraction via random intent priors. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=4vGVQVz5KG. Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rocktäschel, Edward Grefenstette, and Yuandong Tian. Efficient planning in compact latent action space. arXiv preprint arXiv:2208.10291, 2022. Taesup Kim, Sungjin Ahn, and Yoshua Bengio. Variational temporal abstraction. Advances in Neural Information Processing Systems, 32, 2019. George Dimitri Konidaris. Autonomous robot skill acquisition. University of Massachusetts Amherst, 2011. Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Qlearning. arXiv preprint arXiv:2110.06169, 2021. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:11791191, 2020. Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic Q-ensemble. In Conference on Robot Learning, pp. 17021712. PMLR, 2022. Qiyang Li, Yuexiang Zhai, Yi Ma, and Sergey Levine. Understanding the complexity gains of singletask RL with curriculum. In International Conference on Machine Learning, pp. 2041220451. PMLR, 2023. Qiyang Li, Jason Zhang, Dibya Ghosh, Amy Zhang, and Sergey Levine. Accelerating exploration with unlabeled prior data. Advances in Neural Information Processing Systems, 36, 2024. Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= fIn4wLS2XzU. 13 Sam Lobel, Akhil Bagaria, and George Konidaris. Flipping coins to estimate pseudocounts for In International Conference on Machine Learning, pp. exploration in reinforcement learning. 2259422613. PMLR, 2023. Shie Mannor, Ishai Menache, Amit Hoze, and Uri Klein. Dynamic abstraction in reinforcement learning via clustering. In Proceedings of the twenty-first international conference on Machine learning, pp. 71, 2004. Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cutdynamic discovery of sub-goals in reinforcement learning. In Machine Learning: ECML 2002: 13th European Conference on Machine Learning Helsinki, Finland, August 1923, 2002 Proceedings 13, pp. 295306. Springer, 2002. Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807814, 2010. Mitsuhiko Nakamoto, Simon Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-QL: Calibrated offline RL pre-training for efficient online finetuning. Advances in Neural Information Processing Systems, 36, 2024. Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning, 2022. Georg Ostrovski, Marc Bellemare, Aäron Oord, and Rémi Munos. Count-based exploration with neural density models. In International conference on machine learning, pp. 27212730. PMLR, 2017. Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. HIQL: Offline goalIn Thirty-seventh Conference on Neural Inconditioned RL with latent states as actions. formation Processing Systems, 2023a. URL https://openreview.net/forum?id= cLQCCtVDuW. Seohong Park, Oleh Rybkin, and Sergey Levine. METRA: Scalable unsupervised RL with metricIn NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning, aware abstraction. 2023b. URL https://openreview.net/forum?id=YgZNmDqyR6. Seohong Park, Tobias Kreiman, and Sergey Levine. Foundation policies with hilbert represenIn Forty-first International Conference on Machine Learning, 2024. URL https: tations. //openreview.net/forum?id=LhNsSaAKub. Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1617, 2017. Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pp. 188204. PMLR, 2021. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Tanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference. In International Conference on Machine Learning, pp. 86248633. PMLR, 2020. Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgLZR4KvH. Özgür Simsek and Andrew Barto. Using relative novelty to identify useful temporal abstractions in reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 95, 2004. Özgür Simsek and Andrew G. Barto. Betweenness centrality as basis for forming skills. Workingpaper, University of Massachusetts Amherst, April 2007. 14 Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. In International ConferParrot: Data-driven behavioral priors for reinforcement learning. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= Ysuv-WOFeKR. Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid RL: Using both offline and online data can make RL efficient. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=yyBis80iUuU. Aravind Srinivas, Ramnandan Krishnamurthy, Peeyush Kumar, and Balaraman Ravindran. Option discovery in hierarchical reinforcement learning using spatio-temporal clustering. arXiv preprint arXiv:1605.05359, 2016. Bradly Stadie, Sergey Levine, and Pieter Abbeel."
        },
        {
            "title": "Incentivizing exploration in reinforcement",
            "content": "learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015. Richard Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181 211, 1999. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: study of count-based exploration for deep reinforcement learning. Advances in neural information processing systems, 30, 2017. Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Ahmed Touati, Jérémy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In The Eleventh International Conference on Learning Representations, 2022. Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Joséphine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et al. Jump-start reinforcement learning. In International Conference on Machine Learning, pp. 3455634583. PMLR, 2023. Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. Advances in neural information processing systems, 34:2739527407, 2021. Haichao Zhang, Wei Xu, and Haonan Yu. Policy expansion for bridging offline-to-online reinforceIn The Eleventh International Conference on Learning Representations, 2023. ment learning. URL https://openreview.net/forum?id=-Y34L45JR6z. Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy learning for offline-to-online reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1137211380, 2023."
        },
        {
            "title": "A COMPUTE RESOURCES",
            "content": "We run all our experiments on NVIDIA A5000 GPUs. We first calculate required compute for AntMaze experiments. For each maze-goal configuration, each of our pretraining run takes about two hours and online training also takes about two hours each. To reproduce the results of our methods, it requires 8 (seeds) 3 (maze layouts)(1 (pretraining)+ 4 (online learning) )2 (hours per run) = 240 GPU hours. We have eight baselines that take similar GPU hours, which bring the total estimated GPU hours required to be around 2160. The runtime per Kitchen experiment is similar. There are only 3 environments, but we do 16 seeds. This means we need about 16 (seeds)3 (Kitchen tasks) (1 (pretraining) +1 (online learning) ) 2 (hours per run) = 192 GPU hours for our method. We only train six baselines (we do not run ExPLORe (No Online RND) ablation, or KL ablation), so this gives 1344 hours. We also calculate the compute used for the Visual AntMaze experiments. On average it takes approximately 8 hours to train pretraining checkpoint and approximately 24 hours to do online learning. This means it requires 8 (seeds) (1 (pretraining) 8 (hours per run)+4 (online learning) 24 (hours per run)) = 832 hours for our method. We have six baselines in the main figure. Additionally, we add 4 additional baselines in the ICVF ablation. This gives total of 9152 hours for the Visual AntMaze results. Finally, we look at the compute required for the data ablation experiments. These are additional AntMaze experiments on just the goals in antmaze-large maze. Thus, we have approximately 8 (seeds) 1 (maze layouts) 2 (data ablations) (1 (pretraining) + 4 (online learning) ) 2 (hours per run) = 160 GPU hours to reproduce our method. We include 5 additional baselines, bringing the total compute for data ablations to 960 GPU hours. Thus, in total the results in this paper required approximately 13616 GPU hours, or about 1.5 GPU years. Note this is an approximate upper bound, since not all methods required training checkpoints, and checkpoints were shared between different baselines that both uses trajectory skills or both used HILP skills."
        },
        {
            "title": "B VAE ARCHITECTURE AND HYPERPARAMETERS",
            "content": "We use VAE implementation from Park et al. (2024). The authors kindly shared with us their OPAL implementation (which produces the results of the OPAL baseline in the paper). In this implementation, the VAE encoder is recurrent neural network that uses gated-recurrent units (Cho et al., 2014) (GRU). It takes in short sequence of states and actions, and produces probabilistic output of latent z. The reconstruction policy decoder is fully-connected network with ReLU activation (Nair & Hinton, 2010) that takes in both the state in the sequence as well as the latent to output an action distribution. Parameter Name Batch size Optimizer Learning rate GRU Hidden Size GRU Layers KL Coefficient (β) VAE Prior VAE Posterior Reconstruction Policy Decoder Latent Dimension Trajectory Segment Length (H) Image Encoder Latent Dim Value 256 Adam 3 104 256 2 hidden layers 0.1 state-conditioned isotropic Gaussian distribution over the latent isotropic Gaussian distribution over the latent isotropic Gaussian distribution over the action space 8 4 50 Table 2: VAE training details. In the online phase, our high-level policy is Soft-Actor-Critic (SAC) agent (Haarnoja et al., 2018) with 10 critic networks, entropy backup disabled and LayerNorm added to the critics following the architecture design used in RLPD (Ball et al., 2023). We follow similar strategy in ExPLORe (Li et al., 2024) where we sample 128 offline samples and 128 online samples and add RND reward bonus to all of the samples. The main difference is in the original ExPLORe paper is that they only add reward bonus to the offline data as additionally adding the bonus to the online replay buffer does not help for the maze goals they tested. In our experiments, we add the reward bonus to both offline data and online data, as it leads to better performance in goals where there is limited offline data coverage (see Appendix F). Parameter Batch size Discount factor (γ) Optimizer Learning rate Critic ensemble size Critic minimum ensemble size UTD Ratio Actor Delay Network Width Network Depth Initial Entropy Temperature Target Entropy Entropy Backups Start Training RND coefficient (α) Value 256 0.99 Adam 3 104 10 1 for all methods on AntMaze, 2 for all on Kitchen, 1 for non-skill based methods on Visual AntMaze, 2 for skill-based methods on Visual AntMaze. 20 for AntMaze and Kitchen, 40 for Visual AntMaze 20 256 3 hidden layers. 1.0 on Kitchen, 0.05 on Visual AntMaze/AntMaze dim(A)/2 False after 5K env steps (RND update starts after 10K steps) 2.0 for non-skill based, 8.0 for skill based methods Table 3: Hyperparameters for the online RL agent following RLPD (Ball et al., 2023)/ExPLORe (Li et al., 2024). For Diffusion BC + JSRL on AntMaze, we use an initial entropy temperature of 1.0 because it works much better than 0.05. We use 4 larger RND coefficient in skill-based methods such that the reward bonus we get for each step in the skill horizon stays roughly proportional to the non-skill-based methods."
        },
        {
            "title": "C IMPLEMENTATION DETAILS FOR BASELINES",
            "content": "Diffusion BC + JSRL. We use the diffusion model implementation from (Hansen-Estruch et al., 2023). Following the papers implementation, we train the model for 3 million gradient steps with dropout rate of 0.1 and cosine decaying learning rate schedule from the learning rate of 0.0003. In the online phase, in the beginning of every episode, with probability p, we rollout the diffusion policy for random number of steps that follows geometric distribution Geom(1 γ) before sampling actions from the online agent (inspired by (Li et al., 2023)). RND bonus is also added to the online batch on the fly with coefficient of 2.0 to further encourage online exploration of the SAC agent. The same coefficient is used in all other non-skill based baselines. For skill based baselines, we scale up the RND coefficient by the horizon length (4) to account for different reward scale. Following the BC + JSRL baseline used in ExPLORe (Li et al., 2024), we use SAC agent with an ensemble of 10 critic networks, one actor network, with no entropy backup and LayerNorm in the critic networks. This configuration is used for all baselines on all environments. On AntMaze, We perform hyperparameter sweep on both = {0.5, 0.75, 0.9} and the geometric distribution parameter γ = {0.99, 0.995, 0.997} on the large maze with the top right goal and find that = 0.9 and γ = 0.99 works the best. We also use these parameters for the Visual AntMaze experiments. For Kitchen, we perform sweep on the parameter = {0.2, 0.5, 0.75, 0.9} and find that 0.75 works best. We still use γ = 0.99. We take the minimum of one random critic for AntMaze and Visual AntMaze, and the minimum of two random critics for Kitchen. For all methods, we use the same image encoder used in RLPD (Ball et al., 2023) for the Visual AntMaze task, with latent dimension of 50 (encoded image is 50 dimensional vector), which is then concatenated with proprioceptive state observations. ExPLORe. We directly use the open-source implementation from https://github.com/ facebookresearch/ExPLORe/. The only difference we make is to adjust the RND coefficient from 1.0 to 2.0 and additionally add such bonus to the online replay buffer (the original 17 method only adds to the offline data). Empirically, we find slightly higher RND coefficient improves performance slightly. Setting the value too high (e.g., 10) can in turn hurt performance. The SAC configuration is the same as that of the Diffusion BC + JSRL agent. We found that taking the minimum of one random critic on Visual AntMaze worked better for ExPLORe than taking the minimum of two, so all non-skill based baselines use this hyperparameter value. Online RL with trajectory skills. This baseline is essentially our method but without using the trajectory encoder in the VAE to label trajectory segments (with high-level skill action labels), so all stated implementation decisions also apply to Ours. Instead, we treat it directly as high-level RL problem with the low-level skill policy completely frozen. The SAC agent is the same as the previous agents, except for on Visual AntMaze, where taking the minimum of 2 critics from the ensemble leads to better performance for Ours, so we use this parameter setting for all skill-based benchmarks. We compute the high-level reward as the discounted sum of the rewards received every environment steps. During the 5 103 steps before the start of training, we sample random actions from the state-based prior. For Visual AntMaze, we use the learned image encoder from the VAE to initialize both the critic image encoder and the RND network. If using ICVF, we initialize the RND network with the ICVF encoder instead. Online RL with HILP skills. This baseline is the same as the one above but with the skills from recent unsupervised offline skill discovery method, HILP (Park et al., 2024). We use the official open-source implementation https://github.com/seohongpark/HILP and run the pretraining to obtain the skill policies. Then, we freeze the skill policies and learn high-level RL agent to select skills every steps. HILP w/ offline data. This novel baseline is the same as Online w/ HILP Skills, except that we also relabel the offline trajectories and use them as additional data for learning the high-level policy online (similar to our proposed method). To relabel trajectories with the estimated HILP skill, we compute the difference in the latent representation of the final state sH and initial state s0 in the trajectory, so ˆz ϕHILP(sH )ϕHILP(s0) . We normalize the skill vector since the pretrained HILP ϕHILP(sH )ϕHILP(s0)2 policies use normalized vector as input. The high-level RL agent is the same as our method, except the skill relabeling is done using the latent difference rather than the trajectory encoder. Ours. We follow Li et al. (2024) (ExPLORe) to relabel offline data with optimistic reward estimates using RND and reward model. For completeness, we describe the details below. We initialize two networks gϕ(s, z), g(s, z) that each outputs an L-dimensional feature vector predicted from the state and (tanh-squashed) high-level action. During online learning, g(s, z) is fixed and we only update the parameters of the other network gϕ(s, z) to minimize the L2 distance between the feature vectors predicted by the two networks on the new high-level transition (snew , znew, rnew, snew ): L(ϕ) = gϕ(snew , znew) g(snew 0 0 , znew)2 2. In addition to the two networks, we also learn reward model rψ(s, z) that minimizes the reward loss below on the transitions (s0, z, r, sH ) from online replay buffer: L(ψ) = rψ(s0, z) r2 2. We then form an optimistic estimate of the reward value for the offline data as follows: rUCB(s, z) rψ(s0, z) + αgϕ(s0, z) g(s0, z)2 2, where α controls the strength of the exploration tendency (RND coefficient). For AntMaze environments (both state-based and visual), we find that it is sufficient to use the minimum reward, 1, to label the offline data without performance drop, so we opt for such simpler design for our experiments."
        },
        {
            "title": "VISUAL ANTMAZE",
            "content": "We use the public implementation from the authors of (Ghosh et al., 2023) at https://github. com/dibyaghosh/icvf_release and run the ICVF training for 75, 000 gradient steps to obtain the pre-trained encoder weights, following (Li et al., 2024). Then, we initialize the encoder of the RND network with these weights before online learning. It is worth noting that this is slightly 18 Figure 6: Success rate on Visual AntMaze environment with and without ICVF. Ours works well without ICVF, almost matching the original performance. However, the other baselines Online w/ Trajectory Skills and ExPLORe achieve far worse performance without ICVF, which shows that using offline data both for extracting skills and online learning leads to better utilization of noisy exploration bonuses. Initializing ExPLORe critic with ICVF helps, but does not substantially change performance. different from the prior work (Li et al., 2024) that initializes both the RND network and the critic network. In Figure 6, we examine the performance of Ours, Online w/ Trajectory Skills, and ExPLORe with and without ICVF. Both of the baselines perform much better with the ICVF initialization, suggesting that ICVF might play an important role in providing more informative exploration signal. Ours, without using ICVF, can already outperform the baselines with ICVF. By both extracting skills from offline data and training with offline data, we are able to learn better from less informative exploration signals. We also observe that initializing the critic with ICVF (as done in the original paper (Li et al., 2024)) helps improve the performance of ExPLORe some, but does not substantially change performance."
        },
        {
            "title": "E KL PENALTY ABLATION",
            "content": "In Figure 7, we compare the performance of Ours with version of our method that uses KLdivergence penalty with the state-based prior (as used in previous skill-based method (Pertsch et al., 2021)), Ours (KL). In Ours, as discussed in Section 4, we borrow the policy parameterization from Haarnoja et al. (2018) and adopt tanh policy parameterization with entropy regularization on the squashed space. Pertsch et al. (2021) parameterize the higher level policy as normal distribution and is explicitly constrained to learned state-dependent prior using KL-divergence penalty, with temperature parameter that is auto-tuned to match some target value by using dual gradient descent on the temperature parameter. They do not use entropy regularization. Keeping everything else about our method the same, we instantiate this alternative policy parameterization in Ours (KL). We sweep over possible target KL-divergence values (5, 10, 20, 50) and initial values for the temperature parameter (100, 1, 0.1) using the performance on antmaze-large, but find that these parameters do not substantially alter performance. As shown in Figure 7, Ours performs at least as well as Ours (KL) in the initial learning phase, and has better asymptotic performance on all three mazes, matching or beating ExPLORe, on all three mazes. It seems likely that not having entropy regularization makes it difficult to appropriately explore online, and that explicitly constraining to the prior may prevent further optimization of the policy. Attempts at combining 19 Figure 7: Normalized return on three AntMaze mazes, comparing Ours with KL regularized alternative (Ours (KL)). We that Ours consistently outperforms Ours (KL) on all three mazes, with initial learning that is at least as fast and significantly improved asymptotic performance. Only Ours is able to meet or surpass the asymptotic performance of ExPLORe on all mazes. an entropy bonus and KL-penalty lead to instability and difficulty tuning two separate temperature parameters. Additionally, in the Kitchen domain, the KL objective is unstable, since at some states the prior standard deviation is quite small, leading to numerical instability. In contrast, adopting the tanh policy parameterization from Haarnoja et al. (2018) is simple, performs better, and encounters none of these issues in our experiments."
        },
        {
            "title": "LOCATIONS",
            "content": "We evaluate the success rate of the our algorithm compared to the same baseline suite as in the main results section for each individual goal and maze layout and report the results in Figure 8. We also include ExPLORe both with and without an online RND bonus. Online RND helps ExPLORe the most for the antmaze-medium bottom-right goal, where there is sparse offline data coverage for considerable radius around the goal. We hypothesize that with the absence of online RND, the agent is encouraged to only stay close to the offline dataset, making it more difficult to find goals in less well-covered regions. On the flip side, for some other goals with better offline data coverage, like the antmaze-large top-right goal, online RND can make the performance worse. For every goal location, Ours consistently matches or outperforms all other methods throughout the training process. We also evaluate the coverage at every goal location for every method for each maze layout and show the result in Figure 9. The coverage varies from goal location to goal location as some goal locations are harder to reach. Generally, the agent stops exploring once it has learned to reach the goal consistently. Ours consistently has the best initial coverage for 11 out of 12 goals, though sometimes has lower coverage compared to other methods later in training. However, this is likely due in large part to successfully learning how to reach that goal quickly, and thus not exploring further."
        },
        {
            "title": "G ROBUSTNESS AGAINST OFFLINE DATA CORRUPTIONS",
            "content": "To study how robust our method is, we perform an ablation study on the AntMaze domain on the large maze layout with two types of data corruption applied to the offline data: 1. Insufficient Coverage: All the transitions close to the goal (within circle with radius of 5) are removed. 2. 5% Data: We subsample the dataset where only 5% of the trajectories are used for skill pretraining and online learning. Figure 8: Success rate by goal location. The addition of online RND in ExPLORe leads to better performance on goals with less offline data coverage, and slightly worse performance on goals well-represented in the dataset. Ours consistently matches are outperforms all other methods on all goals throughout training. 21 Figure 9: Coverage for every goal location on three antmaze environments. There is significant variation between goals, and Ours consistently has the best initial coverage performance on 11 of 12 goals. Flattening coverage compared to other methods can be at least partially attributed to having already found the goal, and sucessfully optimizing reaching that goal, rather than continuing to explore after already finding the goal. 22 We report the performance on both settings in Figure 10. For the Insufficient Coverage setting, our method learns somewhat slower than the full data setting, but can still reach the same asymptotic performance, and outperforms or matches all baselines in the same data regime throughout the training process. For the 5% Data setting, our method also reaches the same asymptotic performance as in the full data regime, and outperforms or matches all baselines throughout training. The gap between Ours and baseline performance (in particular, ExPLORe) is smaller than in the full data regime, which is to be expected as we have less data to learn the prior skills, so the skills are likely not as good. Overall, among the top performing methods in the AntMaze domain, our method is the most robust, consistently outperforming the other baselines that either do not use pre-trained skills (ExPLORe) or do not use the offline data during online learning (Online w/ Trajectory Skills) in these data corruption settings. Figure 10: Data corruption ablation on state-based antmaze-large. Top: The success rate of different methods on these data corruption settings. Bottom: Visualization of the data distribution for each corruption setting. We experiment with two data corruption settings. Our method performs worse than the full data setting but still consistently outperforms all baselines."
        }
    ],
    "affiliations": [
        "UC Berkeley"
    ]
}