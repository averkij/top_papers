{
    "paper_title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "authors": [
        "Lunbin Zeng",
        "Jingfeng Yao",
        "Bencheng Liao",
        "Hongyuan Tao",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL."
        },
        {
            "title": "Start",
            "content": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models Lunbin Zeng1,, Jingfeng Yao1,, Bencheng Liao1, Hongyuan Tao1, Wenyu Liu1, Xinggang Wang1, 1Huazhong University of Science and Technology 5 2 0 2 7 1 ] . [ 1 3 1 7 5 1 . 2 1 5 2 : r Figure 1. Performance Comparison. Our DiffusionVL achieve state-of-the-art (SOTA) performance among diffusion vision language models including [18, 41] and competitive performance to Qwen2.5-VL [29]."
        },
        {
            "title": "Abstract",
            "content": "In recent multimodal research, the diffusion paradigm has emerged as promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, dVLM family that could be translated from any powerful AR models. Through simple finetuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key ob- (1) The paradigm shift from AR-based mulservations: timodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving Equal Contribution; Corresponding author: Xinggang Wang (xgwang@hust.edu.cn). significant inference speedup. We conduct large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside 2 inference speedup. The model and code are released at https://github. com/hustvl/DiffusionVL. 1. Introduction Vision language models (VLMs) [4, 7, 22] have achieved significant achievements in multimodal understanding. Most of them are autoregressive (AR) models with the nexttoken prediction (NTP) paradigm. To further speed up the decoding of autoregressive models for real-time inference, some studies [19, 21] have tried to introduce speculative decoding methods to improve the NTP paradigm through parallel decoding. Recently, the diffusion paradigm [18, 41, 42] has emerged as promising alternative, offering more efficient parallel decoding potential than speculative decoding methods. However, existing diffusion vision language models (dVLMs) still face critical challenges. As shown in Figure 1, existing dVLMs consistently underperform advanced autoregressive vision language models (AR-VLMs) on multimodal benchmarks, highlighting significant performance gap. Meanwhile, existing dVLMs cannot support variablelength generation and exhibit low reuse efficiency of keyvalue caching, resulting in inferior practical inference speed compared to AR-VLMs. We argue that the performance gap is largely due to the gap between base diffusion language models (dLLMs) and autoregressive language models (ARLMs). For instance, LLaDA-8B lags behind Qwen2.5-7B by 42.0% in performance on the code task HumanEval [6]. In fact, dVLMs and AR-VLMs are structurally identical; the only difference between them lies in the different attention patterns and behavior patterns between training and inference. Given these challenges and findings, compelling question emerges: Can we directly translate any existing autoregressive models into powerful diffusion vision language models? To answer this question, we explore the way to use any autoregressive models that you find and translate them into diffusion vision language models (see Figure 2). We propose DiffusionVL, with the core technical contribution lying in demonstrating that simple diffusion finetuning approach can achieve this translation. Specifically, we convert the next-token prediction paradigm of the original AR model into diffusion paradigm, which we refer to as diffusion finetuning. Its key advantage is that it enables the construction of DiffusionVL from any AR model without modifying the architecture whatsoever. For AR-VLM, since these models are already vision-language aligned, we directly apply full-parameter diffusion finetuning to convert AR-VLMs to dVLMs. For AR-LMs, we follow the LLaVA [22] paradigm to build vision language model. We first train only the connector during the pretraining stage: this stage aligns the vision and text spaces, with the autoregressive paradigm ensuring training stability. We then conduct diffusion finetuning in the second stage to complete the paradigm conversion. Furthermore, we adopt the block diffusion strategy consistent with [1], which not only supports response generation of arbitrary lengths but also effectively reuses KV-cache for enhanced efficiency. Building on these technical designs, we introduce the DiffusionVL family, which can be translated from AR models of any scale and modality, while maintaining efficient inference speeds. Our experimental results strongly validate the efficacy of DiffusionVL. When we translate AR-VLMs to DiffusionVL, our method achieves state-of-the-art performance among current diffusion vision language models using less than 5% of the data compared to prior methods, significantly narrowing the gap with advanced AR-VLMs. Specifically, DiffusionVL achieves comprehensive performance improvementa 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench. When translating AR-LMs to DiffusionVL, we have drawn the Figure 2. Paradigm Shift and Modality Shift. We demonstrate that any autoregressive models with different modalities can be translated to the diffusion vision language models effectively. following conclusions through detailed controlled experiments: under the same training data and configuration, the DiffusionVL obtained by diffusion finetuning on ARLM consistently outperforms the dVLM obtained by diffusion finetuning on dLLM in downstream multimodal benchmarks. Furthermore, even when compared to AR-VLMs with autoregressive finetuning of the same paradigm, our DiffusionVL can still achieve comparable performance on downstream benchmarks. For inference, equipped with the block diffusion strategy, DiffusionVL enjoys 2.0 speedup over previous dVLMs in the detailed image captioning task. In summary, our contributions are threefold. We validate the effectiveness and feasibility of translating any pretrained autoregressive models into diffusion vision language models, providing an efficient and low-cost approach for developing high-performance diffusion vision language models. Our model supports flexible generation lengths and efficient KV-cache reuse via block diffusion strategy, two advantages that are absent in existing diffusion vision language models. Extensive experimental results demonstrate that our method yields state-of-the-art DiffusionVL, which not only narrows the performance gap with advanced autoregressive vision language models but also achieves 2.0 speedup compared to existing diffusion vision language models. 2. Related Work 2.1. Masked Diffusion Models Diffusion models have achieved great success in vision generation and other computer vision tasks [10, 20, 28, 38, 39, 45]. At the same time, recent work is beginning to explore the potential of diffusion in text generation. Prior work [3, 12, 25] on masked discrete diffusion models (MDMs) has already validated the effectiveness of this paradigm in small-scale text pretraining scenarios. These early studies demonstrated that MDMs can achieve perplexity levels comparable to those of autoregressive language models (AR-LMs) while enabling parallel inference. Building on this basis, recent advanced models such as LLaDA [27] and Dream [40] have further validated the effectiveness and scalability of MDMs in modern large language model (LLM) settings. Represented by models such as Dimple [42], LLaDAV [41], and LaViDa [18], they follow the LLaVA [22] paradigm to build diffusion vision language model (dVLM). These models have well explored the potential of the MDM paradigm in the visual and multimodal domains; however, they are also limited by their inability to perform variable-length generation and reuse the KV-cache, and certain performance gap still exists compared with advanced autoregressive VLMs. 2.2. Interpolation between AR and Diffusion Constrained by the inability to reuse KV-cache and support arbitrary generation, recent works have explored balancing the autoregressive (AR) and masked diffusion paradigms for text generation. SSD-LM [13], AR-Diffusion [36], and BD3-LM [1] (a foundation for follow-up research) proposed block-based text diffusion, but these are limited to small-scale text pre-training. Their scalability to large models and vision domains remains unproven. To scale this block diffusion paradigm to large text models, SDAR [8], Fast-dLLM-V2 [34], and SDLM [24] verified that fine-tuning from AR models can build efficient block diffusion language models, with SDLM extending it to next-sequence prediction. However, such AR-diffusion interpolation research is still scarce in vision language models (VLMs). Concurrent with our work, A2D-VL [2] successfully finetuned existing AR-VLMs into block diffusion VLMs. In contrast, our work more systematically explores leveraging AR models for efficient block diffusion VLMs: We first extended diffusion fine-tuning to directly convert AR-VLMs into block diffusion VLMs. To validate the broader applicability of this paradigm-shift strategy, we further verified on AR-LMs that diffusion finetuning achieves performance comparable to autoregressive finetuning. More importantly, we found the gap between AR-VLMs and diffusion VLMs is minimal, which eliminates the need for complex annealing, yet our finetuned models still outperform A2D-VL. 3. Method 3.1. Preliminaries In the autoregressive paradigm, text generation is modeled under the next-token prediction. Given sequence of input {x1, . . . , xL}, the training objective is to minimize the cross-entropy loss. LAR(x; θ) = Ex (cid:34) (cid:88) (cid:35) log Pθ(xi x<i) , (1) i=1 where the model Pθ(x<i) aims to maximize the conditional probability of the current position by using the preceding context x<i = x0, . . . , xi1. By contrast, masked diffusion models can be further subdivided into two paradigms: full diffusion and block diffusion. The traditional full diffusion paradigm adds and removes noise simultaneously throughout the entire sequence. For time (0, 1), the input sequence is masked with probability of t, yielding noisy sequence xt. And the model Pθ(xt) is trained to minimize the expected masked position prediction loss. LDM(x; θ) = Et,x0,xt (cid:34) 1 (cid:88) i=1 log Pθ(xi (cid:35) 0 xt) , (2) where the U(0, 1) and the loss is only calculated for masked positions The block diffusion paradigm divides the entire sequence into several blocks of equal size and performs block-wise noise addition and denoising within each block. And the loss is also only calculated for masked positions. LBDM(x; θ) = L/D (cid:88) α log Pθ(xi 0 x<i, xD(i)) , (3) i=1 where the xD(i) denotes all contexts in the block containing position i, x<i are clean contexts from earlier blocks, α represents the loss scale which is calculated by α , and 1αt α is the instantaneous rate of change of αt in continuous time. 3.2. Architecture Architectures and paradigms are largely decoupled. This means that single architecture can yield diverse models by applying different paradigms. Taking autoregressive models and masked diffusion models as examples, although both of them are built upon the modern transformer architecture [31], they follow different paradigms for both training and inference. Based on the above observations, our model architecture completely adopts the same architecture as existing autoregressive models. We only equip the diffusion training and inference with different attention mechanism compared to autoregressive models, which will be detailed discussed later. Figure 3. The diffusion finetuning framework of our model. After the input is converted into the embedding space, block-wise noise is added to the answer text sequence within this space. The noise sequence xi 0 and fed into the language model. noisy block can see information about the preceding blocks in the corresponding clean sequence (offset block causal) and other positions within the same block (block diagonal). During inference, the attention pattern of the clean sequence is used (block causal). The model performs denoising prediction and finally computes the loss at the masked noisy positions. is concatenated with the original sequence xi 3.3. Modalities and Paradigms Since we are using the same architecture as autoregressive models, we discuss how can maximize the capabilities of autoregressive models to build diffusion vision language models. We design different diffusion finetuning pipelines for autoregressive vision language models and autoregressive language models, depending on whether the pretrained autoregressive model has been vision-language aligned. First, we describe the process of finetuning an autoregressive vision language model into diffusion vision language model as paradigm shift. We demonstrate that this direct paradigm shift is an efficient way to build diffusion vision language models, which aligns with the findings of [8, 34] in the text domain. Since our model has already been vision-language aligned, no additional pretraining stage is required for alignment. At this stage, we directly finetune the entire AR-VLM into dVLM in an end-to-end manner using Eq. (3). Furthermore, we extend the approach to build diffusion vision language model based on an autoregressive language model, and describe it as process of modality shift and paradigm shift. We adopt the traditional two-stage training approach similar to LLaVA [22]. In the pretraining stage, we only train the connector to align the vision embedding space and text embedding space. Since this connector is randomly initialized to align vision and text embeddings, we use the standard autoregressive objective Eq. (1) to staIn the finetuning stage, bilize the modality shift process. all components are jointly trained end-to-end using the diffusion finetuning strategy Eq. (3) to achieve modality shift and paradigm shift at the same time. 3.4. Diffusion Finetuning In this section, we will elaborate on the training framework of our diffusion finetuning. The overall training framework is illustrated in Figure 3. For the input visual finetuning data, the image is processed through the vision encoder and projector to obtain vision embeddings, and then the vision embeddings are concatenated with text embeddings. Each sequence is padded with <EOS> tokens to length divisible by the block-size, and further split into = L/D non-overlapping blocks of size D. pivotal design choice is our block-wise noise schedule. Unlike the sequence-level noise used in prior dVLMs, noise is applied uniformly to entire blocks containing the models response and the <EOS> padding tokens. This design applies uniform noise level within each block, which aligns naturally with the block-level denoising process during inference. Regarding the attention mechanism, we adopt the hybrid attention pattern like [1]: for each input embedding sequence, the noised sequence and original clean embedding are concatenated along the sequence dimension. specialized attention mask is constructed to enforce bidirectional attention within each block and causal attention between blocks. This design enables the current blocks denoising to be guided by preceding context while maintaining intrablock parallelism. Guided by this architecture, the model is trained to minimize the cross-entropy loss specifically at the masked token positions using Eq. (3) 3.5. Diffusion Inference During the inference stage, our model employs blockdecoding strategy. This strategy naturally supports KVcache reuse and arbitrary-length generation through intrablock diffusion parallel generation and inter-block autoregressive generation strategies. For the inter-block autoregressive decoding, we first encode the input image and text prompt to initialize the prompt cache C0. Himg = Evision(I), Htxt = Etxt(P ) C0 = [Himg; Htxt] (4) (5) where [; ] denotes concatenation along the sequence dimension and the represents the input image, the represents the input text prompt. For the m-th block to be decoded, we initialize the block with fully noisy embeddings and perform iterative denoising. At each step, the current block keys and values are concatenated with the previous cached context: Kin, Vin = [Kcache; k], [Vcache; v] (6) where the mechanism [Kcache; k] denotes the KV-Cache reuse strategy, allowing the current block to attend to previous contexts. Then the model uses the previous context to guide the generation of this block. Upon obtaining the clean block Hm, we append it to the cache context for the next generation cycle: Cm = [Cm1; Hm] (7) This decoding process repeats until an EOS token is detected in the fully denoised block. Within each block, two strategies similar to those in [8] are adopted. We set our default strategy as static lowconfidence remasking: fixed number of tokens are decoded in each decode step. Specifically, given block size and denoise step S, the B/S positions (or B/S for non-divisible cases) with the highest prediction confidence are selected for decoding at each step. Furthermore, we introduce the dynamic low-confidence remasking strategy. This strategy decodes tokens whose prediction confidence exceeds preset threshold in each step, endowing the model with dynamic decoding steps. For simpler content, the model can complete predictions more rapidly. This not only reduces the generation time of single block but also achieves more extreme inference acceleration. 4. Experiment 4.1. Implementation Details For the model architecture of building diffusion vision language models (dVLMs) from autoregressive vision language models (AR-VLMs) in Section 3.3, we select Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct [4] as our base model to conduct experiments. For the experiment of finetuning autoregressive language models (ARLMs) to build dVLMs and AR-VLMs, we select Qwen2.57B-Instruct [29] as our base language model. And for the experiment of finetuning diffusion language models (dLLMs) to build dVLMs, we select LLaDA-8B-Instruct [27] as our base language model. For the vision encoder, we adopt SigLip2-400M [30] as the base vision encoder. The projector is implemented as randomly initialized twolayer MLP projector. For the pretraining stage of building dVLMs from ARVLMs, we adopt the 580K-sample pretraining dataset from LLaVA-Pretrain [22]. For finetuning data used in building dVLMs, we uniformly use the open-source 738K data instruction-follow samples from LLaVA-Next [17]. To better balance model performance and parallelism, we conducted our training experiments with default blocksize=8 configuration. During inference, to avoid sacrificing model performance, we adopted the static low-confidence remasking strategy by default and used the same denoise steps as in the block-size configuration. To conduct comprehensive evaluation of our models performance, we incorporated multiple vision-language benchmarks spanning various categories: General Knowledge: MMMU [43], MMMU-Pro [44], MMStar [5], MME [11], SeedBench [16], MMBench [23], RealworldQA [37]. Chart and Doc Understanding: AI2D [15], ChartQA [26]. Multi-image Understanding: Muirbench [32]. 4.2. Efficient dVLM Construction from AR-VLMs Table 1 and Table 2 present the downstream multimodal benchmark results of DiffusionVL-3B and DiffusionVL7B, which are derived from diffusion finetuning of Qwen2.5VL-3B-Instruct and Qwen2.5VL-7B-Instruct. We compare our models results with some previous AR-VLMs and open-source dVLMs. As shown in the Table 1 and Table 2, DiffusionVL7B achieves comprehensive performance surpassing existing open-source dVLMs LaViDa-L [18], Dimple [42], LLaDA-V [41] on multimodal benchmarks. Notably, this is achieved despite being finetuned with only 738K samples, which is less than 5% of the data used for LLaDAV. This demonstrates its strong vision-language capability and high training efficiency. Notably, leveraging the strong pretrained foundation of AR-VLMs, DiffusionVL-3B even outperforms the larger LaViDA-L-8B and Dimple-7B with Model Size Type Samples MMBench [en-dev] MMMU [val] MMMU-Pro [std.] MMMU-Pro [vision] MMStar [test] MME [cog.] MME [perp.] LLaVA LLaVA-1.5 Cambrian-1 LLaVA-OV Qwen2.5VL LaViDa-L Dimple LLaDA-V DiffusionVL 7B 7B 8B 7B 3B 7B 8B 7B 8B 3B 7B AR AR AR AR AR AR Diff. Diff. Diff. Diff. Diff. - - - 7.8M >9M >9M 1.6M 1.3M 16.5M 738K 738K AutoRegressive Vision Language Models 38.7 64.3 75.9 80.8 79.1 83.5 - - 42.7 48.8 46.8 51.1 - - - - 31.2 36.7 Diffusion Vision Language Models 70.5 - 82.9 80.1 83.5 43.3 45.2 48.6 47.2 49.3 28.6 - 35.2 31.2 36.9 - - - - 22.1 33.4 - - 18.6 20.2 25.0 - - - 61.7 55.9 63. - - 60.1 55.9 63.2 - - - 418 620 646 341 432 491 594 675 809 1510 1547 1580 1533 1680 1365 1514 1507 1539 1519 Table 1. Benchmark Performance Comparison (Part 1). The top-2 results are highlighted separately for AR and Diffusion models. Best AR / 2nd AR are in green, while Best Diffusion / 2nd Diffusion are in blue. Model Size Type Samples SeedBench [img] SeedBench [vid] AI2D ChartQA Realworld QA MuirBench AutoRegressive Vision Language Models LLaVA LLaVA-1.5 Cambrian-1 LLaVA-OV Qwen2.5VL LaViDa-L Dimple LLaDA-V DiffusionVL 7B 7B 8B 7B 3B 7B 8B 7B 8B 3B 7B AR AR AR AR AR AR Diff. Diff. Diff. Diff. Diff. - - - 7.8M >9M >9M 1.6M 1.3M 16.5M 738K 738K 37.0 66.1 74.7 75.4 74.8 77.5 23.8 37.3 - 56.9 55.1 61.3 - - 73.0 81.4 81.6 83.9 Diffusion Vision Language Models 66.5 - 74.8 73.9 75. - - 53.7 52.2 54.4 70.0 74.4 77.8 78.4 82.2 - - 73.3 80.0 84.0 87.3 64.6 63.4 78.3 79.9 84.2 - - 64.2 66.3 65.4 68.5 - - 63.2 61.6 68. - - - 41.8 47.7 59.6 - - 48.3 47.2 44.8 Table 2. Benchmark Performance Comparison (Part 2). Color-coded rankings show DiffusionVL-7B achieving top-tier performance among Diffusion models (blue) and closing the gap with top AR models (green). less training data, further validating the effectiveness of our proposed method. Moreover, our DiffusionVL significantly narrows the gap between existing dVLMs and advanced AR-VLMs, demonstrating an efficient approach to building dVLMs. 4.3. Feasible dVLM Conversion from AR-LMs In this section, we conduct multiple groups of experiments on different base language models and various finetuning paradigms to investigate the feasibility of finetuning dVLMs from AR-LMs. We perform autoregressive finetuning, block diffusion finetuning, and full diffusion finetuning based on the autoregressive language model Qwen2.57B-Instruct and the diffusion language model LLaDA-8BInstruct, respectively, and report the differences in their performance on the downstream multimodal benchmarks. As shown in the Table 3, DiffusionVL significantly outperforms LLaDA-V finetuned from LLaDA using block diffusion and full diffusion paradigms. This indicates that building high-performance dVLM does not require Model Size Paradigm Base LLM Performance (MMLU / BBH) MMMU (val) MMMU-Pro (std.) AI2D ChartQA RealWorld QA MME (cog.) MME (perp.) Conversion from LLaDA-8B (dLLM base) LLaDA-V LLaDA-V Full-Diff 8B 8B Block-Diff. 65.9 / 47.4 42.4 32.6 26.0 17. 60.1 51.2 Conversion from Qwen2.5-7B (AR-LM base) LLaVA 7B DiffusionVL 7B Block-Diff. AR 71.9 / 63.9 45.4 43. 28.1 28.4 74.0 70.6 20.2 29.8 52.8 53.6 60.1 44.2 60.4 60. 342 261 356 371 1337 1186 1479 1457 Table 3. comparison of dVLM construction using different models and paradigms. We compare dVLMs converted from AR-LMs versus those from dLLMs. The gray column indicates the text capabilities (MMLU / BBH) between the base language models. The best results across all models are highlighted in bold, and the second-best results are marked with underline. The AR, Block-Diff, and Full-Diff paradigms represent the different paradigms we discussed in Section 3.1. pre-existing dLLM; we can fully leverage existing powerful AR-LMs to develop such dVLMs. Notably, DiffusionVL exhibits negligible differences from LLaVA on downstream benchmarks. This further confirms that constructing dVLMs from AR-LMs is both feasible and effective. We must emphasize that the performance difference between the DiffusionVL here and DiffusionVL in Section 4.2 does not imply that finetuning from AR-VLMs to dVLM is the best approach. The DiffusionVL in Section 4.2 benefits more from the fact that its base model has already undergone extensive, high-quality vision-language alignment training. We believe that AR-LMs, with longer and higherquality visual finetuning, also have the potential to build dVLMs achieving performance comparable to dVLMs from AR-VLMs on downstream benchmarks. 4.4. Ablation Study Different denoising steps. We investigate the impact of denoising steps during inference on performance and efficiency, regarding such steps as form of latent reasoning like [14] that differs from explicit chain-of-thought (CoT) [33]. For the inference strategy, we adopt static lowconfidence remasking strategy to control the number of decoded tokens at each denoising step. Based on the detailed image caption task detailcaps [9], we compare the models response ( 512 tokens) with the labeled captions using BERTScore. We compare our model with LLaDA-V-8B. For LLaDA-V-8B, we follow the recommended settings: approximate fast-dllm [35] KV caching with recomputation every 32 steps. As shown in the Figure 4, on the detailed image caption task, our DiffusionVL-7B achieves BERTscore 2.02 better than LLaDA-V-8B while enjoying the 2.0 faster inference speed when using the same parallelism setting. Additionally, our DiffusionVL-7B exhibits similar testscaling law: as the number of denoising steps increases, Figure 4. Balancing speed and quality for detailed image captioning. We define the parallelism factor for dVLMs as the average number of tokens generated simultaneously throughout the sequence (for instance, 1 parallelism corresponds to single-token sampling). Speed metrics were collected using 8 GPUs, with results reported as the average per device. the models descriptive performance improves, accompanied by certain inference speed loss. Different block size performance. To further explore the impact of different block sizes on the performance of diffusion finetuning, we conduct four groups of diffusion finetuning ablation experiments based on the Qwen2.5VL-3BInstruct, setting the training block size from 1 to 16. During inference, we align denoising steps with block size to ensure models with different block sizes use the same number of denoising steps when generating the same tokens. As shown in Table 4, the downstream benchmarks indicate that smaller training block size yields better performance, though the differences are marginal, reflecting classic trade-off. When finetuning an autoregressive model into dVLM, smaller block size makes the model behave Models A2D-VL-7B [2] DiffusionVL-7B Training Samples Annealing MMMU-PRO[Standard] MMMU[Val] 400K 35 49.1 400K 35.1 50.7 Table 5. Comparison of Our DiffusionVL-7B with A2D-VL7B. DiffusionVL-7B outperforms A2D-VL-7B on downstream benchmarks with relatively lower-quality training samples, and without the need for annealing. 5. Discussion Concurrent with our work, A2D-VL [2] successfully finetuned existing autoregressive vision language models (ARVLMs) into diffusion vision language models (dVLMs). In this section, we will focus on discussing the differences between our DiffusionVL and A2D-VL. Firstly, compared to A2D-VL, our work more systematically explores how to finetune any AR models into dVLM, resulting in the DiffusionVL series of models. To our knowledge, we are the first work to demonstrate that the efficient construction of dVLM from autoregressive language models (AR-LMs) is feasible. We achieved this by simultaneously performing modality-shift and paradigm-shift from AR-LMs, and we dont use any complex additional annealing strategies. Furthermore, we conduct experiment with the same training data volume samples and compare the downstream metrics with those reported in the A2D-VL official blog. Table 5 shows that our DiffusionVL-7B still outperforms A2D-VL-7B on downstream benchmarks in setting the same data volume but with relatively lower data quality, without requiring additional annealing strategies. This validates the core insight of our paper: the difference between dVLMs and AR-VLMs is minimal, enabling us to construct dVLMs from any AR models simply and efficiently. 6. Conclusion This paper focuses on novel problem in the building of Is it possidiffusion vision language models (dVLMs): ble to construct dVLMs based on existing powerful autoregressive models? In response, we propose DiffusionVL, dVLM family that translates from any powerful autoregressive models. DiffusionVL proposes two key observations: (1) The paradigm shift from autoregressive vision language models (AR-VLMs) to dVLMs is remarkably effective. (2) Direct conversion of an autoregressive language model (AR-LM) to dVLM is also feasible. Furthermore, we introduce block diffusion strategy into dVLMs that supports arbitrary-length generation and KV-cache reuse, achieving significant inference speedup compared with Figure 5. Performance and speed between different thresholds for dynamic low-confidence remasking. By adjusting the thresholds, DiffusionVL can achieve extreme acceleration contrast to static low-confidence remasking. And it also offers tunable balance between speed and output quality (BERTScore). more like an autoregressive model, resulting in slightly better performance but poorer parallelism. To better improve the models parallelism while maintaining its performance, we choose block size of 8 as our default configuration. Block Size 1 4 8 ChartQA RealWorldQA MMMU-PRO[Standard] MMMU[Val] 82.8 59.5 32.1 47.9 80.9 61.7 31.4 47.3 79.9 61.6 31.2 47.2 16 77.4 61.6 31.0 46.6 Table 4. Different block size performance comparison. We evaluate the impact of different block sizes on benchmark performance. Our results show that smaller block size achieves slightly better performance but poorer parallelism. Different thresholds for dynamic remasking. To explore more extreme acceleration, we recommend adopting the dynamic low-confidence remasking strategy. We further conducted an ablation study on the relationship between the BERTScore / Token Per Second(TPS) of detailcaps and different thresholds under the condition of fixed denoising step of 8. The choice of setting the denoising step to 8 is intended to minimize the number of tokens denoised in each fixed-schedule denoising step, which allows for more intuitive demonstration of the differences between the dynamic remasking strategy and the static remasking strategy. Figure 5 shows that smaller dynamic thresholds allow the model to decode all tokens meeting the threshold condition at each denoising step, thereby achieving more significant acceleration. However, such acceleration comes at the cost of certain degree of performance degradation for the model. other dVLMs. With this integrated design, despite training with less than 5% of the data required by prior methods, DiffusionVL translated from AR-VLMs achieves stateof-the-art performance among existing diffusion vision language models, alongside 2.0 inference speedup in the detailed image captioning task. And DiffusionVL translated from the AR-LMs not only outperforms the dVLMs built from diffusion language models but also achieves performance competitive with AR-VLMs finetuned under the same autoregressive paradigm."
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. 2, 3, 4 [2] Marianne Arriola, Naveen Venkat, Jonathan Granskog, and Anastasis Germanidis. Adapting autoregressive vision language models for parallel diffusion decoding, 2025. 3, 8 [3] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 2 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 5 [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 5 [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. 2 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 1 [8] Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenghai Wang, Qipeng Guo, Kai Chen, Biqing Qi*, and Bowen Zhou. Sdar: synergistic diffusionautoregression paradigm for scalable sequence generation, 2025. 3, 4, 5 [9] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. 7 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, Caifeng Shan, and Ran He. Mme: comprehensive evaluation benchmark for multimodal large language models, 2025. 5 [12] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. 2 [13] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssdlm: Semi-autoregressive simplex-based diffusion language arXiv model for text generation and modular control. preprint arXiv:2210.17432, 2022. [14] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 7 [15] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. 5 [16] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 5 [17] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 5 [18] Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025. 1, 3, 5 [19] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 1 [20] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, et al. Diffusiondrive: Truncated diffusion model In Proceedings of the for end-to-end autonomous driving. Computer Vision and Pattern Recognition Conference, pages 1203712047, 2025. 2 [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 1 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 2, 3, 4, 5 [23] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 5 [24] Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, et al. Sequential diffusion language models. arXiv preprint arXiv:2509.24007, 2025. 3 [25] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 2 [26] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [27] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 3, 5 [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [29] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 1, 5 [30] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 5 [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [32] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 5 [33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 7 [34] Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, and Enze Xie. Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025. 3, 4 [35] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. 7 [36] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. 3 [37] xAI. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model, 2024. 5 [38] Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. 2 [39] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 2 [40] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream arXiv preprint 7b: Diffusion large language models. arXiv:2508.15487, 2025. 3 [41] Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. 1, 3, 5 [42] Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. 1, 3, 5 [43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. [44] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 5 [45] Ya Zou, Jingfeng Yao, Siyuan Yu, Shuai Zhang, Wenyu Liu, and Xinggang Wang. Turbo-vaed: Fast and stable transfer of video-vaes to mobile devices. arXiv preprint arXiv:2508.09136, 2025."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology"
    ]
}