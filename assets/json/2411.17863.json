{
    "paper_title": "LongKey: Keyphrase Extraction for Long Documents",
    "authors": [
        "Jeovane Honorio Alves",
        "Radu State",
        "Cinthia Obladen de Almendra Freitas",
        "Jean Paul Barddal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents. In this paper, we introduce LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKey's versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains."
        },
        {
            "title": "Start",
            "content": "LongKey: Keyphrase Extraction for Long Documents Jeovane Honorio Alves and Radu State SEDAN - SnT University of Luxembourg jeovane.alves,radu.state@uni.lu Cinthia Obladen de Almendra Freitas Graduate Program in Law (PPGD) Pontifícia Universidade Católica do Paraná cinthia.freitas@pucpr.br Jean Paul Barddal Graduate Program in Informatics (PPGIa) Pontifícia Universidade Católica do Paraná jean.barddal@ppgia.pucpr.br 4 2 0 2 6 ] . [ 1 3 6 8 7 1 . 1 1 4 2 : r AbstractIn an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving gap in processing long-context documents. In this paper, we introduce LongKey, novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKeys versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains. I. INTRODUCTION Efficient extraction of vital information from textual documents across diverse domains is essential for effective information retrieval, especially given the vast volume of data on the internet and within organizational datasets. In response to this need, Keyphrase Extraction (KPE) aims to identify representative keyphrases that enhance document comprehension, retrieval, and information management [1], [2]. keyword encapsulates the central theme or distinct element of documents subject matter. When multiple words are used, this term is referred to as keyphrase. In practice, the terms keyword and keyphrase are often used interchangeably. This paper adopts this convention, treating keyword and keyphrase extraction as synonymous, applicable to terms of any length [3]. Keyphrase extraction techniques are commonly categorized based on their underlying principles [3]. For example, unsupervised methods like TF-IDF [4] calculate term importance based on term frequency within document and across the corpus. RAKE [5] assesses word relevance through cooccurrence ratios, while TextRank [6] uses graph-based structure to measure word strength and similarity. KeyBERT [7], unlike unsupervised methods, employs supervised learning with pre-trained BERT embeddings [8] and cosine similarity to determine importance and relevance. PatternRank [9] is similar to KeyBERT, yet it uses part-of-speech (POS) module to reduce the number of keyphrase candidates evaluated. recent relevant work in keyphrase extraction is JointKPE [10], which finetunes BERT model for keyphrase extraction based on two strategies: global informativeness and keyphrase chunking. Different algorithms served as baselines. ChunkKPE only uses keyphrase chunking as its strategy. Likewise, RankKPE uses only global informativeness as its strategy. TagKPE considers five-tagging approach to facilitate n-grams extraction. And then SpanKPE, which employs span self-attention mechanism. HyperMatch, new hyperbolic matching model proposed in [11], advances keyphrase extraction beyond Euclidean space, evaluating the relevance of keyphrase candidates using the Poincaré distance. The authors also combine intermediate layers of the RoBERTa [12] model through an adaptive mixing layer to enhance representation. Aimed in long-context documents, GELF [13] is based on graph-enhanced sequence tagging, using the Longformer [14] encoder. The authors constructed text co-occurrence graph and utilized graph convolutional network (GCN), focusing on edge prediction, to augment Longformer model embeddings. Although KPE is powerful tool, most research has focused on short-context documents, such as abstracts and news articles. While many methods focus on short texts, challenges remain for longer documents. These challenges encompass diverse content structures, increased syntactic complexity, varying contexts within the same document, and limited compatibility with long-context language models. Addressing these intricacies demands developing advanced approaches explicitly tailored for the nuances of handling long-context data [2], [15]. To address these challenges, in this paper, we present LongKey1, novel framework that extends keyphrase extraction to long documents through two key contributions. First, LongKey expands token support for encoder models like Longformer, capable of processing up to 96K tokens, ideal for inference on lengthy documents. Second, it introduces new strategy for keyphrase candidate embedding that captures and 1Code available at https://github.com/jeohalves/longkey. consolidates context across the document, enabling more accurate, context-aware extraction. The remainder of this paper is organized as follows: Section II details the LongKey methodology, Section III presents the experimental setup, Section IV discusses the results, and Section concludes the study. II. PROPOSED APPROACH Our proposed methodology, dubbed LongKey, is outlined in this section. LongKey operates considering three stages: initial word embedding, keyphrase candidate embedding, and candidate scoring, as shown in Figure 1. Each stage is designed to refine the selection and evaluation of keyphrases. A. Word Embedding To generate embeddings for long-context documents, our proposal uses the Longformer model [14]. Longformer is an encoder-type language model that uniquely supports extended contexts through two innovative mechanisms: sliding local windowed attention with default span of 512 tokens and task-specific global attention mechanism. By default, each of the models twelve attention layers produces an output embedding size of 768. Furthermore, Longformer has positional embedding size of 4,096. We extended it to 8,192 by duplicating the same weights to the next 4,096 elements. For global attention, preliminary experiments have demonstrated optimal results by designating the initial token ([CLS]) as the token for global attention, i.e., the token that attends to every document token and vice-versa. First, tokenizer converts the input document to numeric representation. Our approach uses the Longformer model as the encoder, with tokens defined by the RoBERTa [12] tokenizer. This token representation is then processed by Longformer to generate embeddings, capturing the contextual details of each token within the document. Even with Longformer, processing of large documents would not be possible with our current computational resources if they are not chunked. Therefore, documents larger than 8K tokens are split in equally sized chunks (with maximum size of 8192 tokens). Each document is divided into chunks for processing by Longformer, and its embeddings are concatenated to create unified representation of the entire texts tokens. Given document = {w1, . . . , wi, . . . , wN } containing words, we use an encoder-type model to generate the token embeddings ET : ET = Encoder({w1, . . . , wi, . . . , wN }). (1) The resulted operation can be represented as follows: ET = {e1,1, e1,2, . . . , e1,M1 , e2,1 . . . , ei,j, . . . , eN,MN }, (2) where ei,j represents the embeddings of the jth token from the ith word in document D. Each embedding ei,j has size of 768, which is omitted in the explanation for better clarity. If > 8192, is grouped in chunks which are processed separately and the resulting token embeddings are concatenated together. B. Keyphrase Embedding Keyphrase embeddings are context-sensitive, meaning the same keyphrase can yield different embeddings based on its surrounding textual environment. Once these embeddings are crafted, they are combined into unique embeddings for each keyphrase candidate, taking into account the documents overarching thematic and semantic landscape. token embeddings to represent Since specific word may contain more than one token, its necessary to create single embedding for this word. Like JointKPE and other similar methods, we used only the first the word, since there was no significant difference between this strategy and other simple combinations evaluated, thus reducing computational calculation. These word embeddings are used as the input of our keyphrase embedding module. Given the token embeddings ET , the word embeddings are given by preserving only the first token embeddings for each word, given as follows: EW = {e1,1, e2,1 . . . , ei,1, . . . , eN,1}, (3) which, for simplicity, we can omit the token index j. Then, we employ convolutional network to construct embeddings for each potential n-gram keyphrase. For n-grams up to predetermined maximum length, e.g., = 5, we use distinct 1-D convolutional layers, each with kernel size corresponding to its n-gram size (i.e., = n), ranging from [1, n], and no padding, to generate the keyphrase embeddings from the pregenerated word embeddings. The n-gram representation of the keyphrase occurrence from words wi to wi+k1 is given by the convolutional module with kernel size is calculated as follows: hi:k = CNNk({ei, . . . , ei+k1}), (4) where H, the set of keyphrase embeddings, can be represented as: = {h1:1, h2:1, . . . , h1:2, . . . , hi:k, . . . , hN k,n}. (5) The convolutional module generates embeddings for each keyphrase occurrence in the text. To capture the relevance of each keyphrase across the document, LongKey uses keyphrase embedding pooler that combines all occurrences of keyphrase candidate into single, comprehensive representation. This approach helps emphasize the most contextually significant keyphrases. computationally efficient max pooling operation aggregates the diverse embeddings of the keyphrase candidates occurrences from various text locations into singular, comprehensive representation. Given KP as the set the unique possible keyphrases found in with maximum size of words Fig. 1. Overall workflow of the LongKey approach. KP = {kp1, kp2, . . . , kpi, . . . , kpM }, (6) where is the number of unique keyphrases found in D, from unigrams to n-grams, the embeddings of every occurrence of kpi are defined as follows: KPl = hi:k where wi:k = KPl, thus, for simplicity, KPl can be also represented as: KPl = {hl 1, hl 2, . . . , hl i, . . . , hl Sl }, (7) (8) where Sl is the number of occurrences in the document for specific KPl. To generate the candidate embeddings Cl for the unique keyphrase l, max pooling is employed as follows: = max({hl 1, hl 2, . . . , hl i, . . . , hl Sl}). (9) An overall presentation of the candidate embedding calculation is shown in the bottom-left part of Figure 1. For clear explanation, we show an illustration with the embedding size of 1. This calculation is employed separately for each embedding position. In practice, for each keyphrase candidate, we select its occurrences and get the maximum value. In our example, the keyphrase candidate has three occurrences with values (3, 3, 4) in position = 0. After max pooling, the value for this candidate in the position = 0 is j=0 = 4. Even though this illustration only has integers, floating-point numbers are employed. In summary, this cohesive representation encapsulates the essential details from multiple instances, facilitating robust evaluation of keyphrase relevance for accurate ranking. Consequently, this pooling mechanism strengthens the models ability to identify the most relevant keyphrases based on context, improving the precision and relevance of the extraction process. C. Candidate Scoring In the LongKey approach, candidate embeddings are each assigned ranking score, with higher scores indicating keyphrases that more accurately represent the documents content. LongKey fine-tunes its performance during training by optimizing ranking and chunking losses, aligning closely with ground-truth keyphrases to ensure relevance. For both losses, ground-truth keyphrases are positive samples. Remaining instances are considered as negative samples. To generate the scores for both ranking and chunking parts, we employ linear layers for different inputs. For the ranking score, we use the candidate embeddings as the input of linear layer which converts the embedding to single value (i.e., ranking score). Given as the embedding of candidate keyphrase l, we calculate the ranking score as follows: rank = Linearrank(C l) sl (10) Unlike JointKPE, which might assign multiple scores to single keyphrase candidate based on its occurrences, LongKey assigns singular score per candidate, facilitated by the efficient proposed keyphrase embedding pooler. Each candidates score is then optimized through Margin Ranking loss, enhancing the distinction between positive y+ and negative samples by elevating the scores of the true keyphrases. This loss is defined as follows: MRloss(s+ rank, rank) = max(0, s+ rank + rank + 1) (11) As for the chunking score, we use the keyphrase embeddings as the input of linear layer. Given as the embedding of keyphrase i, we calculate the chunking score as follows: chunk = Linearchunk(H i) si (12) One thing to note is that LongKey maintains the same objective as JointKPE for keyphrase chunking, utilizing binary classification optimized with Cross-Entropy loss. Given probability p+ p+ = Softmax(schunk)+, (13) representing the likelihood of sample belonging to the positive class, and z, the actual binary class label of the sample (where 1 indicates positive and 0 indicates negative), the BCE loss is calculated using the formula: BCEloss = [z log(p+) + (1 z) log(1 p+)] (14) Both losses are added together and jointly optimized across model training, similar to JointKPE. The formula is given as follows: LongKeyloss = MRloss + BCEloss (15) However, distinctively, LongKey diverges from JointKPE in the objectives of its loss functions. Regarding the ranking loss function, LongKey is specifically designed to refine the embeddings of keyphrase candidates, in contrast to JointKPEs focus on optimizing the embeddings of individual keyphrase instances, thereby enhancing the models overall precision and contextual sensitivity in keyphrase extraction. III. EXPERIMENTAL SETUP the This section outlines the empirical evaluation of LongKey method, providing comprehensive overview of the experimental datasets and the specific configurations underpinning our analysis. A. Datasets Robust and large datasets must be employed to train language models and evaluate the capability of an approach in extracting relevant keyphrases from an input document. Many large datasets typically only contain the title and abstract of scientific papers. They are sub-optimal in evaluating long context-based keyphrase extractors since they generally have samples with less than 512 tokens. Due to the scarcity of datasets containing high volume of lengthy documents, the Long Document Keyphrase Identification Dataset (LDKP) was formulated specifically for extracting keyphrases from full-text papers (which generally surpass 512 tokens) [15]. LDKP has two datasets: LDKP3K: variation of the KP20K dataset [16], which contains approximately 100 thousand samples and an average of 6027 words per document. LDKP10K: variation of the OAGKx dataset [17], containing more than 1.3M documents, averaging 4384 words per sample. Other datasets are employed in zero-shot fashion, i.e., inference only, to assess the capability of different methods trained on both datasets to adapt to different domains and patterns. These datasets are the following: Krapivin [18]: Features 2,304 full scientific papers from the computer science domain published by ACM. SemEval2010 [19]: Comprises 244 ACM scientific papers across four distinct sub-domains: distributed systems; information search and retrieval; distributed artificial intelligence multiagent systems; social and behavioral sciences economics. NUS [20]: This dataset contains 211 scientific conference papers with keyphrases annotated by student volunteers, offering unique perspective on keyphrase relevance. FAO780 [21]: With 780 documents from the agricultural sector labeled by FAO staff using the AGROVOC thesaurus, this dataset tests the models performance on domain-specific terminology. NLM500 [22]: This collection of 500 biomedical papers, annotated with terms from the MeSH thesaurus, assesses the methods capability in the biomedical domain. TMC [23]: Including 281 chat logs related to child grooming from the Perverted Justice project, this dataset, with documents and keyphrases based on the formatting from [24], introduces the challenge of informal text and sensitive content. Although the focus is on long-context documents, its possible to use the evaluated methods on short documents. To assess the effectiveness of the models trained on the LDKP datasets, we evaluate them on two of the most popular short-context datasets: KP20k and OpenKP: KP20k [16]: Highly correlated with the LDKP3K dataset, the KP20k is dataset containing more than 500 thousand abstracts of scientific papers (20 thousand of abstracts for the validation and test subsets each). OpenKP [25]: The OpenKeyPhrase (OpenKP) is popular short-context dataset containing more than 140 thousand of real-world web documents, where their keyphrases were human-annotated. B. Experimental Settings Our experiments utilized two NVIDIA RTX 3090 GPUs, with 24GB VRAM each. The training regimen was guided by the AdamW optimizer, combined with cosine annealing learning rate scheduler, with learning rate value of 5 105, and warm-up for the initial 10% training iterations. To circumvent VRAM constraints, we employed gradient accumulation, achieving an effective batch size of 16 in the training phase. To maintain clarity and consistency in our reporting, we use the terms iterations and gradient updates interchangeably. We set maximum token limit of 8,192 during training to accommodate the length of the documents within our available computational resources. The positional embedding was expanded to 8,192, duplicating the original size used by Longformer, which enhances support for longer chunks in inference mode (tested up to 96K in total). We limit keyphrases to maximum of five words (k = [1, 5]) to maintain computational efficiency and align with standard practices in keyphrase extraction. In the evaluation, longer ground-truth keyphrases are considered as false negatives. Moreover, models were trained on LDKP3K for 25 thousand iterations. Since LDKP10K had substantially higher number of samples, we trained it for 78,125 iterations (i.e., almost an entire epoch). We also evaluated some methods with the BERT model, where we also used chunking to extend training to 8,192 tokens. To maintain consistency in our analysis and ensure fair comparisons, we used the Longformer model for all supervised approaches that are encoder-based and fine-tuned on the LDKP datasets. Moreover, we employed the same global attention mask as used in LongKey. Model performance was quantitatively assessed using the F1-score, the harmonic mean between precision and recall, for the most significant keyphrase candidates (F1@K), with Ks value determined based on the overall average of keyphrases per document in each dataset, also following choices of related works, e.g., [26]. Given ˆY = [ˆy1, ˆy2, . . . , ˆyM ] (16) as the predicted keyphrases sorted by their ranking scores in decreasing order, and as the ground-truth keyphrases of given document (with no specific order), we can calculate the F1-score and its intermediary metrics, i.e., precision and recall; using the top-K predicted keyphrases, given by ˆY:k = [ˆy1, ˆy2, . . . , ˆymin(K,M )]. (17) We calculate the intermediary metrics as follows: Precision@K = ˆY:k ˆY:k , Recall@K = ˆY:k , (18) then, with these two metrics, we calculate the F1-score at the top-K keyphrases as follows: F1@K = 2 Precision@K Recall@K Precision@K + Recall@K . (19) Another relevant metric, proposed in [27], is variation of the F1@K defined as F1@O. Here, is the number thus = , of ground-truth keyphrases (i.e., oracle), which is dynamically calculated depending on the document. This metric is independent to the methods output, given the effectiveness of each method only with the needed predicted keyphrases. We also employed an additional evaluation: F1@Best. Basically, we evaluate which is the that have the best harmonic mean between recall and precision, i.e., best F1-score. The purpose of this additional evaluation is to verify how far is the optimum for specific method in specific dataset is from the selected Ks. We put threshold of 100 to not deviate strongly from the default Ks. Furthermore, we employed the Porter Stemmer, from the NLTK package [28], for all experiments, but no lemmatization was applied. Stemming was applied for both candidate and ground-truth keyphrases. Duplicated ground-truth keyphrases were cleaned, removing the possibility of duplicated keyphrases erroneously improving the F1-score. IV. RESULTS AND DISCUSSION In this section, we delve into the performance outcomes on two primary datasets, extending our analysis to encompass zero-shot learning scenarios and domain-shift adaptability. Moreover, we unravel the contribution of the keyphrase embedding pooler, performance estimation, and inference on short-context documents. A. LDKP Datasets Table presents the comparative results on the LDKP3K test subset, encompassing both unsupervised methods and models finetuned on the LDKP3K and LDKP10K training subsets. Its noteworthy that, aside from GELF, standard benchmark model, all fine-tuned methods are tailored adaptations designed to handle extensive texts, utilizing the BERT (only when trained on LDKP3K) and Longformer architecture for enhanced context processing. Our approach was also evaluated without chunking, identified as LongKey8K. i.e., max of 8192 tokens, Among the evaluated methods, LongKey8K emerged as the best, achieving an F1@5 of 39.55% and F1@O of 41.84%. Remarkably, even under domain shift when trained on the broader LDKP10K dataset, which includes more comprehensive array of topics beyond computer science, LongKey maintained its lead with an F1@5 of 31.94% and F1@O of 32.57%. Performance metrics on the LDKP10K test subset are also provided in Table I, where LongKey emerges as the leading method, achieving an F1@5 of 41.81%. While LongKey trained on the LDKP3K dataset outperformed other models trained on the same dataset, it scored significantly lower when compared to its performance on the LDKP10K dataset, indicative of dataset-specific variations in effectiveness. This discrepancy, especially the reduced efficacy on the LDKP10K subset, could be attributed to the significant skew towards computer science papers within the LDKP3K dataset, as detailed in the LDKP study. Generally, the evaluated methods had superior F1@O than F1s at specific Ks, suggesting that, for the LDKP datasets, ground-truth keyphrases were ranked higher in prediction. TABLE RESULTS OBTAINED ON LDKP TEST SUBSETS. VALUES IN %. THE BEST SCORES FOR EACH ARE IN BOLD. BEST SCORES ONLY IN SPECIFIC SECTION ARE UNDERLINED. * GELF SCORE WAS REPORTED IN ITS PAPER WITHOUT SPECIFIC VALUE. F1@K TF-IDF TextRank PatternRank GELF* SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch BERT-SpanKPE BERT-TagKPE BERT-ChunkKPE BERT-RankKPE BERT-JointKPE BERT-HyperMatch LongKey BERT-LongKey LongKey8K SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch LongKey @4 8.64 6.28 7.50 - 30.27 34.50 31.43 36.83 37.50 36.34 29.80 34.13 31.80 36.28 37.19 36.17 39.50 38.67 39.55 25.83 30.06 23.93 28.20 29.79 27.98 31. @5 9.08 6.90 8.24 - 30.08 34.52 31.17 36.61 37.23 36.37 30.00 34.15 31.77 36.43 37.28 36.31 39.50 38.68 39.54 25.81 30.12 23.70 28.39 29.78 28.21 31. LDKP3K LDKP10K @6 9.40 7.19 8.56 - 29.43 33.94 30.55 35.81 36.54 35.78 29.51 33.49 31.35 35.53 36.59 35. 38.57 37.98 38.57 25.49 29.58 23.11 28.08 29.44 28.07 31.69 8.75 6.68 7.33 @Best 9.72@9 8.01@12 8.65@ Trained on LDKP3K 27.10 31.08 36.58 32.81 38.38 39.41 38.23 31.08 36.09 33.89 38.38 39.94 38.27 41.84 40.43 41.84 - 30.27@4 34.52@5 31.43@4 36.83@4 37.50@4 36.37@5 30.00@5 34.15@5 31.80@4 36.43@5 37.28@5 36.31@ 39.50@5 38.68@5 39.55@4 Trained on LDKP10K 26.54 31.48 24.65 29.04 30.61 29.11 32.57 25.83@4 30.12@5 23.93@4 28.39@5 29.79@4 28.21@5 31.94@5 @4 7.45 5.11 5. - 19.99 21.48 20.12 23.14 23.67 23.20 20.94 21.03 19.19 23.32 23.66 23.63 25.17 25.36 25.15 32.17 41.12 36.22 37.98 39.86 37.44 41.57 @5 7.88 5.47 6. - 20.37 21.84 20.45 23.70 24.23 23.64 21.46 21.40 19.68 23.77 24.25 24.10 25.78 26.00 25.75 32.21 40.68 35.42 38.23 39.95 37.52 41.81 @6 8.12 5.82 6. - 20.39 21.92 20.50 23.84 24.37 23.77 21.50 21.40 19.74 23.89 24.26 24.16 25.77 26.10 25.77 31.75 39.64 34.43 37.89 39.45 37.25 41.00 7.77 5.48 6. @Best 8.41@9 6.54@14 7.23@14 - 21.00 22.56 21.06 24.31 24.98 24.20 21.97 21.87 20.36 24.35 25.08 24.74 26.45 26.58 26.50 34.90 46.47 40.55 42.37 44.73 41.67 47. - 20.39@6 21.92@6 20.50@6 23.84@6 24.37@6 23.77@6 21.50@6 21.40@5 19.74@6 23.89@6 24.26@6 24.16@6 25.78@5 26.10@6 25.77@6 32.21@5 41.12@4 36.22@4 38.23@5 39.95@5 37.52@5 41.81@5 B. Unseen Datasets Without any finetuning on their respective data, LongKey and related methods were evaluated across six diverse domains, as shown in Tables II and III. Remarkably, LongKey outperformed other methods in nearly all tested datasets, with the exception of SemEval2010 and TMC where its results were slightly below the top performers (HyperMatch and RankKPE, respectively)."
        },
        {
            "title": "The",
            "content": "choice of LDKP training datasetLDKP3K or LDKP10Ksignificantly influenced performance across the unseen datasets, with LDKP3K-trained models excelling in every dataset with the exception of the NLM500 dataset. Although LDKP10K had broader areas of study, LDPK3K had overall longer samples, with an average of 6,027 words per document against an average of 4,384 words in the LDKP10K. Further studies are encouraged to assess the influence of study areas and sample size. Another thing to note is that, for the unseen datasets, there was balance dispute between BERT and Longformer-based methods as the best one, even for LongKey. Although access the robustness of BERT with chunking approach, it also show room for improvements regarding long-context encoders. Figure 2 presents the performance of LongKey and JointKPE on the LDKP3K dataset, categorized by document length. Overall, LongKey achieved consistently high scores across different encoder models, while JointKPEs performance was more variable. Notably, LongKeys Longformer model performed better on longer documents, while the BERT model maintained more balanced results across various lengths. Additionally, LongKey showed particularly strong results for documents between 512 and 1024 tokens, suggesting potential areas for optimization when handling even longer documents. Overall, LongKeys robustness was evident as it consistently outperformed other models in nearly all benchmarks, showcasing its broad applicability and strength in keyphrase extraction across varied domains. C. Component Analysis To assess the keyphrase embedding pooler (KEP) contribution, we undertook component analysis using the LDKP3K validation subset. This analysis involved evaluating the LongKey approach with different aggregation functions, i.e., average, sum and maximum; but also the improvement obtained compared to the JointKPE approach. We used the configuration outlined in the experimental settings, with each model configuration undergoing 12,500 iterations. Table IV shows each configurations average and TABLE II RESULTS OBTAINED IN UNSEEN DATASETS WITH MODELS TRAINED ON LDKP3K AND LDKP10K TRAINING SUBSETS. VALUES IN %. BEST SCORES, FOR EACH AND DATASET, ARE IN BOLD. BEST SCORES ONLY IN SPECIFIC SECTION ARE UNDERLINED. * GELF SCORES WERE REPORTED IN ITS PAPER WITHOUT SPECIFIC VALUE. Unseen datasets Krapivin SemEval2010 NUS F1@K TF-IDF TextRank PatternRank GELF* SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch BERT-SpanKPE BERT-TagKPE BERT-ChunkKPE BERT-RankKPE BERT-JointKPE BERT-HyperMatch LongKey BERT-LongKey LongKey8K SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch LongKey @4 6.30 4.87 6.72 - 27.59 29.87 27.90 32.00 32.55 31.22 27.18 26.20 24.79 31.20 32.06 32.16 34.96 34.67 34.94 24.63 26.22 21.37 25.56 26.68 25.23 29. @5 7.02 5.26 7.17 - 27.62 29.72 27.74 31.82 32.42 31.44 27.16 26.30 24.67 31.43 32.17 32.14 34.82 34.86 34.85 25.13 26.57 21.50 26.05 27.04 25.70 30. @6 7.45 5.77 7.61 - 27.22 29.32 27.50 31.19 32.10 31.27 26.82 25.85 24.38 31.04 31.80 31.79 34.21 34.30 34.23 24.91 26.38 21.23 26.15 27.11 26.01 30. @O 6.40 5.23 6.81 @5 @10 @15 6.62 6.53 6.24 8.80 8.95 7.91 10.07 10.11 9. Trained on LDKP3K - 28.62 31.01 28.89 33.32 33.73 32.79 28.15 27.33 25.72 32.49 33.45 33.47 36.31 36.07 36.29 - 20.78 21.81 20.32 20.43 19.08 22.20 20.78 19.00 18.35 20.38 22.45 24. 22.31 19.93 22.31 16.70 24.81 24.72 23.58 24.99 25.10 26.64 25.50 22.41 21.93 24.95 26.09 27.62 26.36 24.06 26.36 Trained on LDKP10K 25.52 27.43 22.30 26.88 27.68 26.65 31. 22.02 21.54 18.57 16.47 18.23 16.94 22.26 25.35 25.82 20.97 20.58 21.69 21.11 25.77 - 25.42 25.14 23.73 25.22 25.73 26.75 25.63 22.63 22.13 25.94 25.68 26.85 27.37 25.34 27.31 26.17 26.02 20.54 22.59 23.23 23.37 26. @O 9.42 9.54 8.16 - 25.72 25.57 24.29 25.53 25.80 26.82 26.45 22.53 22.61 25.94 26.91 27.85 27.74 25.69 27.60 26.29 26.59 20.80 22.06 23.02 23.26 26. @5 @10 @15 @O 10.44 7.83 8.53 12.22 10.63 9.89 12.38 11.73 11.15 11.98 9.47 10. - 29.68 28.78 27.77 29.22 28.22 31.27 29.91 27.51 26.32 26.07 26.57 28.98 30.02 24.46 30.09 26.00 25.86 24.56 25.18 25.43 24.50 27.93 21.50 30.47 31.25 28.66 31.64 31.12 33.53 30.96 27.81 27.70 30.05 30.34 31. 33.32 28.60 33.19 28.19 27.16 26.11 26.57 26.42 26.08 29.20 - 28.30 29.09 26.84 30.30 30.54 32.23 28.34 26.46 26.71 29.59 29.62 31.08 32.51 29.34 32.47 26.48 26.63 24.08 26.34 25.76 25.60 28. - 33.04 32.12 30.46 33.32 33.61 35.14 31.30 30.43 27.70 30.95 31.06 33.27 34.95 29.43 34.95 29.56 29.13 26.85 27.78 27.81 27.16 30.34 Fig. 2. F1 scores based on the document length of the LongKey and JointKPE methods with different encoders applied to the LDKP3K dataset. F1@K for six range of document length (from less than 512 words to more than 8192), where = [1, 10]. Dashed lines are the F1@O for the specific interval. standard deviation results that were computed considering five runs per method. Overall JointKPE F1@5 score was around 36% with high std dev. of 0.50%. Using the KEP proposed in LongKey, but with the average reduction, significantly impaired performance, resulting in an F1@5 score of 29.15%, but with std dev. of 0.23%, lower than JointKPE. Using the summation aggregator improved F1@5 little (32.76% 0.20), but still inferior to JointKPE. The best F1@5 score was obtained using max pooling, TABLE III RESULTS OBTAINED IN UNSEEN DATASETS WITH MODELS TRAINED ON LDKP3K AND LDKP10K TRAINING SUBSETS. VALUES IN %. BEST SCORES, FOR EACH AND DATASET, ARE IN BOLD. BEST SCORES ONLY IN SPECIFIC SECTION ARE UNDERLINED. F1@K TF-IDF TextRank PatternRank SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch BERT-SpanKPE BERT-TagKPE BERT-ChunkKPE BERT-RankKPE BERT-JointKPE BERT-HyperMatch LongKey BERT-LongKey LongKey8K SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch LongKey @4 7.21 9.62 1. 15.42 18.85 16.53 19.34 19.35 19.50 16.08 17.22 13.96 17.26 17.58 18.77 20.90 22.20 20.91 17.40 19.89 13.17 18.11 18.03 17.98 20.00 Unseen datasets FAO NLM500 TMC @5 @6 @O @5 @10 @ @O @40 @50 @60 7.97 10.23 1.62 8.31 10.65 1.84 8.37 10.95 1.86 4.66 4.30 2.00 5.69 5.35 3. 5.90 5.82 3.55 5.61 5.32 2.71 1.92 4.83 6.91 2.14 5.70 7.27 2.41 6.30 7.45 16.23 19.31 17.13 19.87 19.88 19. 16.45 17.77 14.49 18.68 18.74 19.25 21.70 22.93 21.77 18.07 20.72 13.18 19.01 19.05 18.74 21.02 16.34 19.29 17.41 20.42 19.98 20.23 17.03 17.82 14.59 19.42 18.99 19.35 21.87 22.67 21. 18.12 20.69 13.00 19.45 19.54 18.95 21.20 Trained on LDKP3K 16.83 20.42 17.89 20.64 20.19 20.76 17.26 18.10 14.10 19.25 19.29 20.16 22.34 23.18 22.23 11.15 13.57 11.85 14.08 14.16 13. 11.97 12.88 11.90 13.43 14.74 13.11 14.24 14.94 14.25 12.29 13.85 12.52 14.78 15.14 14.38 12.35 13.56 12.32 13.98 14.64 14.32 14.96 15.80 15.00 Trained on LDKP10K 18.45 21.47 14.17 19.77 19.88 19.63 21.92 15.00 16.23 13.27 15.96 16.24 14.96 16.49 16.68 17.57 14.13 18.94 17.92 18.43 19.19 11.94 13.01 12.15 14.11 14.52 13.91 12.04 13.38 11.82 13.75 14.11 13.70 14.21 15.04 14. 16.49 17.04 13.01 18.64 17.67 18.53 18.78 12.34 14.34 12.77 15.12 15.37 14.59 12.81 14.34 12.42 14.13 15.27 14.72 15.41 16.12 15.35 16.63 17.52 14.56 18.86 17.96 18.00 18.86 12.10 14.85 13.63 16.21 15.26 15. 15.26 13.50 13.78 16.80 15.86 15.23 15.89 16.69 15.92 11.50 12.09 1.20 8.53 9.47 9.95 10.87 13.04 15.63 14.37 17.09 16.11 16.02 16.28 14.48 14.57 17.44 16.51 16.31 16.43 17.31 16. 12.04 12.98 1.53 9.47 10.49 10.97 11.51 13.63 16.24 14.99 17.62 16.65 16.16 16.64 15.10 14.94 17.78 17.05 16.81 16.75 17.68 16.75 12.31 13.75 1.83 10.13 10.80 11.65 11.80 @O 2.09 5.35 6.93 12.58 15.15 13.94 16.53 15.38 15.89 15.92 13.86 14.43 17.75 16.71 16.09 16.20 17.13 16.26 11.82 12.31 0.82 9.01 9.67 10.33 10.88 TABLE IV OVERALL RESULTS OBTAINED IN OUR COMPONENT ANALYSIS. SCORES IN %. THE BEST SCORES FOR EACH ARE IN BOLD. Component Analysis F1@K @4 @5 @6 JointKPE + avg KEP + sum KEP 36.030.50 28.600.23 32.540.36 36.000.50 29.150.23 32.760.20 35.240.48 29.210.34 32.540.16 LongKey 39.040.18 38.940. 38.130.10 achieving almost 39%, with the lowest std. dev. of 0.07%. These findings underscore the KEPs substantial impact on LongKeys success, which is contingent on the appropriate reduction choice. We suggest that the max aggregator can especially highlight salient features present in different occurrences of specific keyphrase around the document, thus contributing to more effective extraction of representative keyphrases. D. Performance Evaluation We also evaluate the performance of each method in inference. We calculate the performance for each dataset based on the number of processed documents per second using single RTX 3090. The overall results can be seen in Table V. As we can see, LongKey performed slightly inferior to the supervised methods. This was basically caused by the keyphrase embedding pooler. However, this performance loss is minor compared with how much the overall F1 increased with the proposed module. Robust approaches with as little bottleneck as possible are encouraged. Also, though in some cases BERT-based methods had inferior results, they have little boost in performance in comparison with Longformerbased. E. Short Documents In Table VI, we show the results of the evaluated methods in two short-context datasets, KP20k and OpenKP. Three methods were generally competitive: RankKPE, JointKPE, and LongKey. Overall, JointKPE was superior on the KP20k (which was originally developed using it). Since KP20k has high correlation with LDKP3K, better results are expected in models trained with the latter. For the OpenKP, models trained on the LDKP10K were generally better, especially RankKPE. Here, SpanKPE also had results similar to those of the other three. Overall, LongKey improvements on long-context datasets (except the TABLE PERFORMANCE EVALUATION OF EACH METHOD TESTED ON EACH DATASET USING SINGLE GPU USING DOCUMENTS PER SECOND. * DENOTES CPU-ONLY METHODS. Performance Evaluation (docs/sec) LDKP3K LDKP10K Krapivin SE2010 TF-IDF* TextRank* PatternRank SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch BERT-SpanKPE BERT-TagKPE BERT-ChunkKPE BERT-RankKPE BERT-JointKPE BERT-HyperMatch LongKey BERT-LongKey LongKey8K TF-IDF* TextRank* PatternRank SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch BERT-SpanKPE BERT-TagKPE BERT-ChunkKPE BERT-RankKPE BERT-JointKPE BERT-HyperMatch LongKey BERT-LongKey LongKey8K 33.42 3.70 1.35 1.20 3.96 4.11 4.16 4.13 4. 0.99 5.29 5.67 5.83 5.57 5.59 4.02 5.59 4.11 41.90 4.26 1.62 24.89 2.57 1.12 26.78 2.39 1.10 1.79 5.06 5.25 5.26 5.25 5. 1.62 6.87 7.38 7.46 7.11 7.14 5.06 7.17 5.20 1.10 3.02 3.16 3.22 3.21 3.22 0.71 3.79 4.26 4.26 4.31 4.20 3.10 4.15 3.18 1.26 3.03 3.15 3.19 3.17 3. 0.60 3.70 4.30 4.34 4.17 4.15 3.07 4.17 3.16 NUS FAO780 NLM500 TMC 43.35 4.07 1.49 1.08 4.59 4.82 4.84 4.81 4.78 1.13 5.95 6.70 6.59 6.57 6.41 4.60 6.42 4.70 39.79 2.52 1.47 41.11 3.36 1. 32.50 2.71 1.33 1.20 4.61 4.82 4.81 4.79 4.64 1.28 6.45 6.78 6.90 6.89 6.64 4.65 6.59 4.73 1.56 4.51 4.71 4.73 4.66 4.62 1.40 6.24 6.38 6.54 6.46 6. 4.54 6.23 4.66 1.29 3.13 3.20 3.04 3.01 3.06 1.26 2.86 3.98 3.67 3.49 3.48 2.87 3.44 2.96 TMC dataset, which has quite different domain) are not seen in short-context documents. These improvements should be related to the proposed keyphrase embedding pooler. Still, LongKey may also be more biased toward long-context documents, which were not generally seen in the training datasets. Further experiments should be employed, increasing length and content variability in the training stage, to evaluate the capabilities of the keyphrase embedding pooler. V. CONCLUSION Automatic keyphrase extraction is crucial for summarizing and navigating the vast content within documents. Yet, prevalent methods fail to analyze long-context texts like books and technical reports comprehensively. To bridge this gap, we introduce LongKey, novel keyphrase extraction framework specifically designed for the intricacies of extensive documents. LongKeys robustness stems from its innovative architecture, which is specifically designed for long-form content and rigorously validated on extensive datasets crafted for long-context documents. To validate its efficacy, we conducted simple component analysis and further assessments of the LDKP datasets, followed by testing across six diverse and previously unseen longcontext datasets and two short-context datasets. The empirical results highlight LongKeys capability in long-context KPE, setting new benchmark for the field and broadening the horizon for its application across extensive textual domains. Selecting the appropriate LDKP training dataset was crucial for LongKeys performance on unseen data, highlighting the need for strategic modifications to improve generalization without sacrificing the effectiveness of keyphrase extraction. Slightly inferior results in the short-context datasets also indicate the necessity of improvements for better generalization. Furthermore, the restriction on the maximum number of words per keyphrase inherently focuses the method on extracting keyphrases of specific lengths. Further adjustments to accommodate longer keyphrases should be explored, as simply increasing keyphrase length may not improve results without careful evaluation. Although this is common pattern in KPE methods, future work must carefully consider the impact of different keyphrase lengths on overall performance. Additionally, the context size limitation to 8K tokens and similarly sized chunks during inference may restrict LongKeys ability (through not restricted only to our approach) to fully capture and process extensive document content. However, any plans to expand this limit must carefully balance the increased computational demands with available resources. In summary, LongKey sets new benchmark in keyphrase extraction for long documents, combining adaptability with high accuracy across various domains. Its superior embedding strategy contributes to its effectiveness, suggesting significant potential for enhancing document indexing, summarization, and retrieval in diverse real-world contexts."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This study has been funded by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) via the Programa Nacional de Cooperação Acadêmica (PROCADSPFC) program."
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, Recent advances in natural language processing via large pre-trained language models: survey, ACM Computing Surveys, vol. 56, no. 2, pp. 140, 2023. [2] M. Song, Y. Feng, and L. Jing, survey on recent advances in keyphrase extraction from pre-trained language models, Findings of the Association for Computational Linguistics: EACL 2023, pp. 21532164, 2023. [3] S. Siddiqi and A. Sharan, Keyword and keyphrase extraction techniques: literature review, International Journal of Computer Applications, vol. 109, no. 2, 2015. [4] J. Ramos et al., Using tf-idf to determine word relevance in document queries, in Proceedings of the first instructional conference on machine learning, vol. 242, no. 1. Citeseer, 2003, pp. 2948. TABLE VI RESULTS OBTAINED IN SHORT DOCUMENT DATASETS WITH MODELS TRAINED ON LDKP3K AND LDKP10K TRAINING SUBSETS. VALUES IN %. BEST SCORES, FOR EACH AND DATASET, ARE IN BOLD. BEST SCORES ONLY IN SPECIFIC SECTION ARE UNDERLINED. KP20k OpenKP F1@K @3 @4 @5 @Best @3 @4 @5 @Best TF-IDF TextRank PatternRank 15.43 2.94 13.30 15.22 3.11 14.96 13.03 2.87 14.52 15.28 2.97 12.38 15.45@4 3.11@5 15.19@ 12.48 5.39 7.40 15.06 7.54 9.98 13.78 7.56 9.90 15.17 6.86 9.49 15.06@3 7.70@4 10.12@4 SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch LongKey SpanKPE TagKPE ChunkKPE RankKPE JointKPE HyperMatch LongKey 30.65 35.23 33.66 34.77 36.36 35.08 35.32 28.40 28.19 25.03 28.38 29.20 28.02 29.19 30.31 34.74 33.11 34.45 35.74 34.59 35.00 28.25 28.21 24.56 28.33 29.06 28.35 29.26 29.28 33.59 31.98 33.35 34.45 33.51 33. 27.60 27.66 23.72 27.74 28.37 27.79 28.65 Trained on LDKP3K 32.31 37.51 35.88 36.76 38.63 37.06 37.21 30.65@3 35.23@3 33.66@3 34.77@3 36.36@3 35.08@3 35.32@3 16.87 15.93 16.05 16.82 17.24 18.58 16.73 Trained on LDKP10K 28.84 29.14 26.10 28.85 29.84 28.14 29.87 28.40@3 28.21@4 25.03@3 28.38@3 29.20@3 28.35@4 29.26@4 19.07 18.18 15.57 18.79 17.84 20.60 17.73 19.35 17.42 18.56 20.31 21.25 18.09 20.44 22.12 21.30 16.51 22.71 22.57 20.49 22.31 17.84 16.06 17.01 18.68 19.71 17.41 19. 20.27 19.61 14.80 21.07 21.05 19.89 20.90 19.88 18.21 18.68 20.30 20.89 18.20 20.30 22.61 22.02 17.38 22.86 22.32 20.02 22.23 19.41@2 17.52@2 18.56@3 20.31@3 21.26@2 18.58@3 20.44@3 22.34@2 21.42@2 17.07@2 22.71@3 22.57@3 20.60@3 22.31@3 [5] S. Rose, D. Engel, N. Cramer, and W. Cowley, Automatic keyword extraction from individual documents, Text mining: applications and theory, pp. 120, 2010. [6] R. Mihalcea and P. Tarau, Textrank: Bringing order into text, in Proceedings of the 2004 conference on empirical methods in natural language processing, 2004, pp. 404411. [7] M. Grootendorst, Keybert: Minimal keyword extraction with bert. 2020. [Online]. Available: https://doi.org/10.5281/zenodo.4461265 [8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. [9] T. Schopf, S. Klimek, and F. Matthes, Patternrank: Leveraging pretrained language models and part of speech for unsupervised keyphrase extraction, in Proceedings of the 14th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - KDIR, INSTICC. SciTePress, 2022, pp. 243248. [10] S. Sun, Z. Liu, C. Xiong, Z. Liu, and J. Bao, Capturing global informativeness in open domain keyphrase extraction, in Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 1317, 2021, Proceedings, Part II 10. Springer, 2021, pp. 275287. [11] M. Song, Y. Feng, and L. Jing, Hyperbolic relevance matching for neural keyphrase extraction, in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, Eds. Seattle, United States: Association for Computational Linguistics, Jul. 2022, pp. 57105720. [Online]. Available: https://aclanthology.org/2022.naacl-main.419 [12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: robustly optimized BERT pretraining approach, CoRR, vol. abs/1907.11692, 2019. [Online]. Available: http://arxiv.org/abs/1907.11692 [13] R. Martínez-Cruz, D. Mahata, A. J. López-López, and J. Portela, Enhancing keyphrase extraction from long scientific documents using graph embeddings, arXiv preprint arXiv:2305.09316, 2023. [14] I. Beltagy, M. E. Peters, and A. Cohan, Longformer: The longdocument transformer, arXiv preprint arXiv:2004.05150, 2020. [15] D. Mahata, N. Agarwal, D. Gautam, A. Kumar, S. Parekh, Y. K. Singla, A. Acharya, and R. R. Shah, LDKP: Dataset for Identifying Keyphrases from Long Scientific Documents, arXiv preprint arXiv:2203.15349, 2022. [16] R. Meng, S. Zhao, S. Han, D. He, P. Brusilovsky, and Y. Chi, Deep keyphrase generation, arXiv preprint arXiv:1704.06879, 2017. [17] E. Çano and O. Bojar, Two huge title and keyword generation corpora of research articles, arXiv preprint arXiv:2002.04689, 2020. [18] M. Krapivin, A. Autaeu, M. Marchese et al., Large dataset for keyphrases extraction, 2009. [19] S. N. Kim, O. Medelyan, M.-Y. Kan, and T. Baldwin, Semeval2010 task 5: Automatic keyphrase extraction from scientific articles, in Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, 2010, pp. 2126. [20] T. D. Nguyen and M.-Y. Kan, Keyphrase extraction in scientific publications, in International conference on Asian digital libraries. Springer, 2007, pp. 317326. [21] O. Medelyan and I. H. Witten, Domain-independent automatic keyphrase indexing with small training sets, Journal of the American Society for Information Science and Technology, vol. 59, no. 7, pp. 10261040, 2008. [22] A. R. Aronson, O. Bodenreider, H. F. Chang, S. M. Humphrey, J. G. Mork, S. J. Nelson, T. C. Rindflesch, and W. J. Wilbur, The nlm indexing initiative. in Proceedings of the AMIA Symposium. American Medical Informatics Association, 2000, p. 17. [23] A. Kontostathis, L. Edwards, and A. Leatherman, Text mining and cybercrime, Text mining: Applications and theory, pp. 149164, 2010. [24] J. H. Alves, H. A. C. G. Pedroso, R. H. Venetikides, J. E. M. Köster, L. R. Grochocki, C. O. A. Freitas, and J. P. Barddal, Detecting relevant information in highvolume chat logs: Keyphrase extraction for grooming and drug dealing forensic analysis, in 2023 International Conference on Machine Learning and Applications (ICMLA), 2023, pp. 19791985. [25] L. Xiong, C. Hu, C. Xiong, D. Campos, and A. Overwijk, Open domain web keyphrase extraction beyond language modeling, arXiv preprint arXiv:1911.02671, 2019. [26] A. Kong, S. Zhao, H. Chen, Q. Li, Y. Qin, R. Sun, and X. Bai, Promptrank: Unsupervised keyphrase extraction using prompt, arXiv preprint arXiv:2305.04490, 2023. [27] X. Yuan, T. Wang, R. Meng, K. Thaker, P. Brusilovsky, D. He, and A. Trischler, One size does not fit all: Generating and evaluating variable number of keyphrases, arXiv preprint arXiv:1810.05241, 2018. [28] E. Loper and S. Bird, Nltk: The natural language toolkit, arXiv preprint cs/0205028, 2002."
        }
    ],
    "affiliations": [
        "Graduate Program in Informatics (PPGIa) Pontifícia Universidade Católica do Paraná",
        "Graduate Program in Law (PPGD) Pontifícia Universidade Católica do Paraná",
        "SEDAN - SnT University of Luxembourg"
    ]
}