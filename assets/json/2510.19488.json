{
    "paper_title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
    "authors": [
        "Dunjie Lu",
        "Yiheng Xu",
        "Junli Wang",
        "Haoyuan Wu",
        "Xinyuan Wang",
        "Zekun Wang",
        "Junlin Yang",
        "Hongjin Su",
        "Jixuan Chen",
        "Junda Chen",
        "Yuchen Mao",
        "Jingren Zhou",
        "Junyang Lin",
        "Binyuan Hui",
        "Tao Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation."
        },
        {
            "title": "Start",
            "content": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos 2025-10-23 Dunjie Lu1,2*, Yiheng Xu1,2*, Junli Wang2*, Haoyuan Wu1, Xinyuan Wang1, Zekun Wang2, Junlin Yang1, Hongjin Su1, Jixuan Chen1, Junda Chen1, Yuchen Mao1, Jingren Zhou2, Junyang Lin2, Binyuan Hui2, Tao Yu1 1The University of Hong Kong 2Qwen Team, Alibaba Group 5 2 0 O 2 2 ] . [ 1 8 8 4 9 1 . 0 1 5 2 : r Project Page: https://videoagenttrek.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VIDEOAGENTTREK, scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop VIDEO2ACTION, an inverse dynamics module (IDM) with two components: (1) video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing scalable alternative to expensive manual annotation."
        },
        {
            "title": "Introduction",
            "content": "Teaching machines to use computers like humans do (clicking buttons, typing text, navigating interfaces) represents fundamental challenge in AI. While recent advances in vision-language models have made computer-use agents increasingly feasible (Bai et al., 2025; Qin et al., 2025; Team et al., 2025; Wang et al., 2025b), their development remains bottlenecked by data availability. Training these agents requires extensive trajectories that precisely document GUI interactions: screenshots paired with exact action parameters like click coordinates (x, y) and typed strings. However, creating such datasets through manual annotation is extraordinarily expensive, making it impractical to achieve the scale necessary for robust generalization across diverse applications and operating systems. Meanwhile, the internet hosts millions of screen-recorded tutorials where humans demonstrate computer use, from Excel tutorials to software walkthroughs. These videos implicitly contain the supervision we need: they show where users click, what they type, and how interfaces respond. Yet this resource remains untapped because videos lack the structured action labels required for training. The cursor movements are visible but not tracked; the typed text appears but isnt extracted; the timing of actions is implicit but not annotated. We can learn to automatically extract structured action trajectories from raw videos by training specialized models to detect when actions occur and infer what their parameters are, effectively converting passive recordings into active training data. We introduce VIDEOAGENTTREK, scalable pipeline that mines computer-use trajectories from publicly available unlabeled videos without manual annotation. Our approach employs VIDEO2ACTION, an inverse dynamics module (IDM) with two stages: First, an action event detection model performs dense event detection, identifying action types and their precise temporal boundaries (e.g., click at [1.5, 2.0]s, type at [3.5, 5.5]s). Second, the action parameterization model, an action-content recognizer, analyzes these localized segments to extract structured parameters (pointer coordinates for clicks, literal text for typing), yielding complete (screenshot, action, parameters) trajectories suitable for training. VIDEOAGENTTREK enables large-scale computer-use pretraining with unlabeled web videos. From 39,000 YouTube videos, we automatically extract 1.52 million interaction steps. This represents not just more data, but more diverse data: the trajectories span hundreds of applications across Windows, macOS, and web platforms, capturing interaction patterns that would be infeasible to annotate manually. We validate VIDEOAGENTTREK with two-stage training recipe: continued pretraining on the mined trajectories followed by supervised fine-tuning on curated dataset. This combination leverages the broad coverage from videos to learn fundamental GUI interaction patterns, while supervised fine-tuning sharpens task-specific performance. *Equal contribution. Corresponding authors. 1 Figure 1: Overview of VIDEOAGENTTREK. (1) Video Collection: crawl screen-recorded tutorials and filter GUI footage with SCREENFILTER. (2) Video2Action: an inverse dynamics module that first performs dense action-event detection to localize clips and assign action types, then action parameterization (e.g., click coordinates, typed text) to yield structured (screenshot, action, parameters) trajectories. (3) Agent Training: use the mined trajectories for continued pretraining and supervised finetuning of computer-use agents. Our models achieve 15.8% task success on OSWorld-Verified compared to 9.3% for baselines, 70% relative improvement. The gains are particularly pronounced in online environments where robustness to visual variation matters most. We summarize our main contributions and findings below: We propose VIDEOAGENTTREK, an unsupervised approach to training computer-use agents that automatically converts screen-recorded videos into structured training data through learned inverse dynamics, thereby eliminating the need for manual annotation. Our VIDEO2ACTION module implements inverse dynamics, combining action event detection with millisecondprecision temporal localization and action parameter extraction. It enables accurate reconstruction of GUI interactions (clicks, typing,...) from raw video without ground-truth labels. Experiments demonstrate that our approach achieves 15.8% task success on OSWorld-Verified compared to 9.3% for SFT-only baselines (70% relative improvement), and improves step accuracy on AgentNetBench from 64.1% to 69.3%, validating that passive internet videos can provide effective supervision at scale. We provide reproducible pipeline and training methodology that enables researchers to leverage publicly available screen recordings for computer-use agent training. To facilitate future research, we release SCREENFILTER for efficient GUI filtering and VIDEO2ACTION for action extraction as open-source tools."
        },
        {
            "title": "2 VideoAgentTrek",
            "content": "We introduce VIDEOAGENTTREK, video-driven pipeline that turns web tutorials into training supervision for computer-use agents. Each trajectory is sequence = {(Ik, rk, ak, πk)}K k=1 following Yao et al. (2023), where Ik is representative screenshot, rk is brief inner monologue, ak is the action type (e.g. click, type), and πk is the action content (e.g., pointer (x, y) or typed text). The pipeline has three parts: Video collection and preprocessing. We crawl tutorial videos with seeded queries and tag expansion, apply human-in-the-loop screening, and use cursor-based filtering to retain screen segments with GUI interactions (Section 2.1). VIDEO2ACTION. From raw video, we recover stepwise supervision without manual labels: (i) dense event detection produces typed segments with tight start/end times; (ii) action identification infers parameters πk (e.g., click coordinates, typed strings); and (iii) short inner monologue rk makes the intent explicit. Assembling these per-clip steps yields ReAct tuples for training (Section 2.2). Agent training. We combine large-scale agentic data produced by the method with human demonstrations and targeted GUI grounding pairs, and train an end-to-end agent in two stages: interleaved videotext pretraining followed by instruction-style finetuning (Section 2.3). This structure scales supervision to web-scale while preserving the stepwise semantics needed for robust computeruse policies. Figure 2: Video candidate auto-discovery. From seed keywords and tags, we search and evaluate videos, expand to related videos and high-quality channels (80% pass), and iteratively collect GUI-containing videos for VAT. 2.1 Video collection and preprocessing 2.1.1 Video Candidate Auto-Discovery We employ scalable pipeline for video collection that leverages channel coherencethe observation that YouTube channels typically maintain consistent content types and quality. Starting from seed keywords such as Excel tutorial and How to use Windows, we validate initial results and extend to entire channels when sampling indicates high quality (i.e., when 80% of samples meet our criteria). This channel-based expansion enables efficient scaling: validated channels become trusted sources, while their tags and metadata enable iterative discovery. When we identify high-quality channels through seed validation, we include all their videos as candidates rather than individually vetting each one. This approach deliberately optimizes recall over precision, as the subsequent SCREENFILTER stage ensures final data quality. The channel coherence propertywhere content creators typically focus on consistent topicsmakes this expansion particularly effective. Through iterative rounds of keyword search, channel expansion, and tag extraction, we transform small set of manually validated seeds into 55,000 candidate videos (10,000 hours). This process requires minimal human oversight: initial quality validation on seed videos and periodic verification of expansion effectiveness. The resulting candidate pool intentionally includes some non-GUI content (presentations, tutorials with mixed content), which our filtering stage handles efficiently. 2.1.2 Video Preprocessing with SCREENFILTER Although keyword-based searches typically retrieve relevant computer operations, they also include non-interactive segments, such as explanatory sections where the presenter uses PowerPoint or other presentation tools. Additionally, some of the videos retrieved through this method may not meet the standards for GUI interaction content. To address this, we developed SCREENFILTER, lightweight cursor detection model upon YOLOv8x (Reis et al., 2024) to efficiently extract video segments that focus exclusively on GUI interactions. Using the detection results, we retain video segments where at least 80% of the frames contain cursor for 6 seconds or more, with 2-second merge gap for temporal smoothing. When applied to our corpus, SCREENFILTER successfully extracts 7,377 hours of verified GUI interactions from 10,000 hours of raw video. SCREENFILTERs details are in Appendix B. 2.1.3 VideoAgentTrek Data Analysis Quality and relevance. We collected 55k screen-capture videos (about 10,000 hours) from 50+ channels. The corpus is predominantly clear (about 97% are 720p or higher) and most clips are minutes long, yielding sustained, readable interactions suitable for our pipeline  (Table 6)  . lightweight title/description audit groups videos into tutorials, background pieces, tech talks, and unrelated; tutorials dominate (69.6%), with the remainder used mainly for tag mining or removed during filtering  (Table 7)  . Together, these checks indicate that the collected data are both visually clean and topically aligned with computer-use supervision. Data classification. We label each video as daily, office, workflow, professional, operating-system (OS), or other using lightweight GPT-4.1 pass over the title and short transcript snippet. The distribution (Figure 3) is skewed toward OS-level operations (36%), followed by professional (19%), daily (18%), and office (16%); workflow is smaller (7%) with small remainder labeled as other (4%). This indicates broad coverage with bias toward system and professional use cases. Figure 3: Domain distribution. 3 Figure 4: Overview of VIDEO2ACTION: Given screen-capture video (with optional subtitles), the module (1) detects GUI action events and segments clips, (2) parameterizes each action (type and arguments), and (3) generates step-level thoughts, yielding training-ready sequences of {action clip, action, thought} 2.2 VIDEO2ACTION: Inverse Dynamics Module We develop VIDEO2ACTION, an inverse dynamics module (IDM) that extracts structured action supervision from unlabeled GUI videos. Following insights from robotics where inverse dynamics recovers actions from observations (Nguyen-Tuong & Peters, 2010), VIDEO2ACTION detects GUI events (clicks, drags, scrolls, typing) and infers their parameters directly from pixel changes. This yields training-ready (screenshot, action) pairs without manual annotation, forming the second core component of our VideoAgentTrek toolkit. 2.2.1 Action Event Detection Task. Given an unlabeled screen-capture video (length T), perform prompt-free, dense event detection: predict set of typed GUI interactions with tight temporal bounds, k)}K k, te T. < te ak A, 0 ts fθ(v) = {(ak, ts k=1, Unlike query-based setups, our input contains only v; the output is multi-event set with both action types and start/end timestamps. Approach. We equip VLM with video grounding so that, given clip, it emits sequence of (ak, ts k) for all GUI actions, reframing keyframe detection as multi-class temporal event detection with tight bounds. (1) Training data: We utilize the annotation tool provided by OpenCUA (Wang et al., 2025b) to obtain synchronized screen videos and timestamped GUI interactions (mouse/keyboard events). These raw demonstration logs are then used to create temporal-grounding supervision, allowing precise event detection without manual annotation. (2) Model training: We leverage Qwen2.5-VL (Bai et al., 2025) as the base model, benefiting from its multimodal understanding and fine-grained spatiotemporal capabilities. We perform full-parameter supervised fine-tuning on the Qwen2.5-VL-7B-Instruct model to enable it to generate ordered, typed event spans directly from raw video clips. (3) Evaluation: We evaluate the detector in two phases. First, we check its performance on small curated subset from the source corpus, ensuring tight boundaries and full recovery of relevant GUI actions. Second, we apply the model to unseen web tutorials and conduct blinded manual review to assess its robustness and real-world usability. Details are provided in Appendix C. k, te 2.2.2 Action Parameterization Task. Given detected action segment vk = v[ts k] with type ak A, predict the action content (parameters) πk: : te hϕ(vk) ( ˆak, πk). For example, click segment yields hϕ(vk) (click, (x, y)), while typing segment yields hϕ(vk) (type, content). Approach. We build recognizer hϕ that, for each detected segment vk, predicts both the action type and its parameters ( ˆak, πk). (1) Training data: We start from the OpenCUA raw demonstration logs, which pair screencapture video with timestamped mouse and keyboard events. Each event is converted into type-specific parameter labels and temporally aligned to its clip, yielding prompt-free supervision that captures the exact content of the interaction. (2) Model training: Using Qwen2.5-VL (7B Instruct) as the base, we perform full-parameter supervised fine-tuning so the model maps vk directly to ( ˆak, πk); when available, we optionally condition on the detectors ak to stabilize type predictions. (3) Evaluation: Because ground-truth object boxes are unavailable, we evaluate only on unseen web tutorials via blinded manual review, assessing whether the predicted action type and parameters are correct and practically actionable. Details are provided in Appendix D. 4 2.2.3 Inner Monologue Generation Dense event detection and action identification recover what happened on screen but omit the stepwise rationale. We therefore generate brief inner monologue rk before each action to make explicit the intent, the local plan, and the expected state change (e.g., type query into the search box to reveal results, scroll to bring the Settings button into view). Explicit rationales provide structured supervision for planning and credit assignment, tie cursortarget grounding to goals and affordances, and improve robustness on long-horizon tasks via better error detection and recovery. Recent GUI-agent work that injects step-level thoughts or System-2 reasoning reports notable gains in perception, grounding, and task execution, motivating our inclusion of rk in ReAct-style trajectories (Xu et al., 2025b; Qin et al., 2025; Wang et al., 2025b). We cast inner-monologue generation as conditional paraphrasing with GPT-5 Medium. For each step k, we build structured prompt that includes: (i) the detected action type ak; (ii) its parameters πk (e.g., typed text, cursor coordinates); (iii) the screen state immediately before and after the action (keyframes or thumbnails); and (iv) short ASR transcripts spanning 1-minute window before the action, the during span [ts k], and 1-minute window after. Conditioned on these inputs, the model outputs concise rationale rk that states the intent, the local plan, and the expected state change (grounded to visible UI). Additional prompt templates and representative inner-monologue examples are provided in Appendix E. k, te 2.3 Computer Use Model Pretraining We demonstrate VIDEOAGENTTREKs effectiveness by training an end-to-end computer-use agent with our video-driven data and high-quality supervised finetuning set. On this strong finetuning basis, VIDEOAGENTTREK improves performance on online and offline agent evaluations. 2.3.1 Agentic Data Collection VideoAgentTrek Data. We apply VIDEO2ACTION to the collected tutorial videos and convert them into agentic supervision. For each processed clip, we (i) run dense event detection to obtain typed, tightly bounded segments, (ii) infer action parameters with the action-identification recognizer, and (iii) generate brief inner monologue for intent and expected state change. We then assemble the resulting steps (Ik, rk, ak, πk) into trajectories and serialize them for downstream training. In total, we processed 39,000 videos; each video produces on average 39 steps, yielding approximately 1.52 million ReAct steps overall, and about 26 billion training tokens. Detailed data statistics and examples will be provided in the Appendix H. Human demonstrations Data We sample human-annotated trajectories from OpenCUA (Wang et al., 2025b) and AGUVIS (Xu et al., 2025b), harmonizing formats and labels into single schema. The corpus spans Windows, macOS, and Android, contributing about 8B tokens to training. GUI Grounding Data. We include focused subset of GUI grounding pairs from the OSWorld-G dataset (Xie et al., 2025a) to strengthen pointertarget alignment and layout-aware perception. This contributes roughly 1B tokens to training. 2.3.2 Training strategy. Automatically mined trajectories, while large-scale, inevitably contain residual noise. Motivated by prior findings that decoupling perception/grounding from policy learning improves robustness (Xu et al., 2025b; Wang et al., 2025b), we adopt two-stage schedule that first stabilizes grounding on broad but imperfect supervision and then consolidates policy on clean subset. Foundation Qwen2.5-VL-7B (Bai et al., 2025) is general vision-language model with superior vision understanding capability, but it is not sufficiently pretrained on computer-use tasks with an end-to-end success rate of 4.5% on OSWorld (Xie et al., 2024), which makes it proper starting point (base) for evaluating the data generated by VIDEOAGENTTREK. Stage 1 training. We train for one epoch over 26B tokens drawn from the VideoAgentTrek trajectories, augmented with small number of GUI grounding pairs. Trajectories are formatted as interleaved visiontext sequences: frames (or frame-equivalent images) appear inline with the stepwise textual outputs, preserving temporal order across the entire clip. Loss is masked to the textual portions only; images are conditioning context and are not predicted. Please refer to Appendix for representative formatting examples and complete training configurations including hardware, batch sizes, and optimization details. Stage 2 training. We continue training for 8B tokens on curated set of clean, human-annotated trajectories. Here we reformat the data into chat template with user prompts and assistant responses that describe or execute the next action. We apply standard supervised finetuning with loss computed only on the assistant turns, leaving user turns as pure conditioning. Representative formatting examples and training details are provided in the Appendix F. 5 Figure 5: Experimental Results on OSWorld-Verified (Xie et al., 2025b) and AgentNetBench (Wang et al., 2025b). VideoAgentTrek demonstrates significant improvements over baseline models, with test-time scaling providing additional performance gains"
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Computer Use Agent Performance 3.1.1 Experiment Setup. We evaluate the performance of our model on two computer-use agent benchmarks: OSWorld-Verified (Xie et al., 2025b; 2024) for online settings and AgentNetBench (Wang et al., 2025b) for offline settings. Further protocol, metrics, and computational details are provided in Appendix G. 1. OSWorld-Verified. OSWorld (Xie et al., 2024) is an online computer-use agent evaluation benchmark that includes 369 human-crafted Ubuntu desktop tasks. OSWorld-Verified (Xie et al., 2025b) is more stable version, with updated evaluation scripts, environments, and clarified instructions, designed to measure CUAs task-solving capabilities in dynamic, real-world environments. 2. AgentNetBench. AgentNetBench is an offline benchmark is based on 100 representative tasks from the AgentNet dataset, covering wide range of applications and websites on Windows and macOS. The tasks are manually refined and offer multiple valid action options for each step to reflect the variety of correct interactions. 3.1.2 Main Results. Video pretraining enhances performance on offline benchmarks. On AgentNetBench, incorporating VideoAgentTrek pretraining achieves step success rate of 69.3%, representing 5.2 percentage point improvement over the SFT-only baseline (64.1%) and substantial 30.8 percentage point gain over the base model (38.5%). This consistent improvement demonstrates that video pretraining effectively transfers knowledge to structured offline evaluation scenarios. Video pretraining delivers greater improvements on online benchmarks. On OSWorld-Verified, our complete approach achieves task success rate of 14.13%, demonstrating 4.83 percentage point improvement (+52% relative) over SFT-only training (9.3%) and more than tripling the performance of the base model (4.5%). Video pretraining enables effective test-time scaling for computer-use agents. As shown in Figure 5, the performance of model trained with stage 1 and stage 2 improved from 14.13% to 15.78 % when the action step budget increases from 20 to 50 steps on OSWorld-Verified. This 1.65 percentage point improvement demonstrates the models ability to effectively utilize additional exploration opportunities. This test-time scaling benefit emerges specifically from our video pretraining: models trained on longer video trajectories learn to effectively utilize extended planning horizons, while the SFT-only baseline shows no improvement with additional steps (see Section 4.1). 3.2 VIDEO2ACTION Performance 3.2.1 Action Event Detection We assess VIDEO2ACTION with two-part protocol: held-out, annotated test set and an in-the-wild manual validation:"
        },
        {
            "title": "GT Precision Recall",
            "content": "F"
        },
        {
            "title": "Click\nDrag\nPress\nScroll\nType",
            "content": "12,222 14,247 1,462 842 1,691 2,040 971 177 1,448 1,"
        },
        {
            "title": "Total",
            "content": "17,298 20,282 0.88 0.78 0.40 0.93 0.89 0.88 0.76 0.82 0.52 0.62 0.08 0.14 0.80 0.86 0.64 0.75 0.70 0.78 Action type Samples Accuracy Click Drag Press Scroll Type Overall 324 22 47 34 73 500 0.713 0.366 0.362 0.735 0.671 0. Table 1: Action-event detector evaluation: held-out testset results by action type. Table 2: Action parameterization evaluation: manual in-the-wild assessment. Held-out test set. We hold out 23 hours of screen-capture videos with 20,282 annotated GUI events. Each event is tuple (type, ts, te). prediction counts as hit iff its type matches and its interval has any temporal overlap with ground-truth event; unmatched predictions are false positives and unmatched ground truths are false negatives. We report per-type Precision/Recall/F1 and micro/macro aggregates. Manual validation (in-the-wild). On 10 unseen YouTube tutorials, we apply the same overlap criterion and estimate recovery rates by human review to assess robustness outside the curated set. Results. As the results shown in Table 1, Overall precision is high (0.88) with solid recall (0.71). Pointer-centric actions (click, scroll) are reliably localized; keystroke-only actions show lower recall/precision due to subtle visual evidence. In the manual study, the detector recovers 70% of actions under the same criterion, consistent with in-house results. 3.2.2 Action Identification Evaluating action identification automatically is difficult because target-element boxes are unavailable. We therefore apply the identifier to in-the-wild videos and perform blinded manual assessment. An action is judged proper if, when executed, it would plausibly produce the observed on-screen transition (for example, the clicked control changes state, typed text appears in the focused field, or the page scrolls). We evaluate 500 predictions sampled across action types. Results. Annotators review pre/post frames and verify whether predicted parameters explain the observed changes, with disagreements resolved through second-pass review. Performance varies by action type: pointer-based actions (click, scroll) achieve highest accuracy, typing shows moderate accuracy despite OCR noise, while drag/press actions struggle with subtle visual cues. Despite these challenges, the predicted parameters are accurate enough for trajectory construction and downstream training; detailed counts and validation rates appear in Table 2."
        },
        {
            "title": "4 Analysis",
            "content": "4.1 Effectiveness of Data Scaling To assess the impact of Stage-1 data scale, we train models using 0%, 50%, and 100% of the video tokens, then apply identical Stage-2 SFT to each variant. With increasing tokens, performance scales consistently across both benchmarks. AgentNetBench step success rates increase from 64.1% to 68.1% and 69.3%, while OSWorld-Verified task SR@50 grow from 9.3% to 13.3% and 15.7% (Figure 6). These findings establish clear relationship between pretraining data size and computer-use agent performance, demonstrating the benefits of scaling video pretraining data. 4.2 Improving Long Horizon planning Figure 6: Performance Scaling VIDEOAGENTTREK provides substantially longer trajectories than previous CUA corpora. As illustrated in Figure 10, 42.1% of trajectories exceed 20 steps, while 14.5% contain 50 or more steps, yielding an average trajectory length of 39.25 steps. Cross-dataset comparisons  (Table 9)  reveal that this average substantially exceeds those of established benchmarks, demonstrating that VIDEOAGENTTREKcorpus emphasizes supervision of complex, multi-step workflows rather than brief, single-interaction sequences. The benefits of long-horizon supervision become evident when evaluating planning capabilities under varying step budgets. On OSWorld-Verified, we observe striking difference in how models respond to increased action 7 budgets. The Stage2-only model shows no performance improvement when the step budget expands from 20 to 50 steps, remaining flat at 9.3% task success, indicating it cannot effectively plan beyond its training horizon or recover from early mistakes. In contrast, after Stage-1 pretraining on VideoAgentTreks long video trajectories, the agent demonstrates true test-time scaling: task success rate increases from 14.13% at 20 steps to 15.78% at 50 steps, +1.65 point absolute improvement (+11.7% relative gain, Figure 5). This differential reveals that exposure to extended video demonstrations during pretraining teaches the model to decompose complex tasks into subgoals, persist through intermediate failures, and leverage additional computational budget for exploration and error correction, capabilities that supervised fine-tuning on shorter trajectories fails to instill."
        },
        {
            "title": "5 Related Work",
            "content": "Generating agent trajectories. Computer-use trajectories have been obtained through human annotation, programmatic synthesis in instrumented environments, and web-scale mining of public resources. Human annotation, often aided by instrumentation to log pointer coordinates and keystrokes, yields precise labels but is costly and narrow in coverage (Qin et al., 2025; Wang et al., 2025a;b). Programmatic synthesis inside headless browsers or scripted desktop flows can generate large volumes with exact parameters, yet coverage is constrained by simulator APIs and may diverge from real-world UI variability (Su et al., 2025; Sun et al., 2025). Web-scale mining taps tutorials, RPA logs, and screen recordings to obtain diverse trajectories, but typically lacks precise temporal boundaries or action parameters (Xu et al., 2025a; Jang et al., 2025). Precise Event Grounding in Video. Temporal grounding approaches such as temporal action localization, moment retrieval, keyframe detection, and dense video captioning seek to determine when events take place and provide corresponding descriptions (Lin et al., 2019; Zhuang et al., 2025; Wasim et al., 2024). Meanwhile, recent multimodal systems (e.g., Qwen2.5-VL (Bai et al., 2025), Gemini 2.5 Pro (Comanici et al., 2025)) have advanced the field by enabling more detailed spatiotemporal understanding and long-horizon video reasoning. Nonetheless, most general-purpose grounding frameworks focus primarily on semantic interpretation, rather than achieving the millisecond-level precision and parameter extraction required to faithfully reconstruct GUI interactions. Learning from unlabeled video to act in environments. VPT demonstrated that large-scale unlabeled videos can be converted into effective training signals (e.g., via inverse-dynamics auto-labeling followed by behavior cloning), substantially improving an agents ability to act (Baker et al., 2022).Building on this idea, subsequent work leverages internet-scale human videos to distill human policy priors that transfer to interactive environments, including learning action-centric latent spaces without action labels (Ye et al., 2025) and scaling to humanoid control (Mao et al., 2024)."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented VideoAgentTrek, scalable pipeline that transforms publicly available screen recordings into structured supervision for computer-use agents without manual annotation. By developing an inverse dynamics module that accurately detects GUI events and extracts action parameters from raw video, we demonstrate that the implicit supervision in tutorial videos can be effectively harvested at scale. Our experiments on 39,000 YouTube videos yielded 1.52 million interaction steps, enabling continued pretraining that improved task success rates by 70% on OSWorld-Verified (9.3% to 15.8%) and increased step accuracy on AgentNetBench from 64.1% to 69.3%. These results establish that unlabeled internet videos, when processed through learned inverse dynamics, provide viable and cost-effective alternative to expensive manual annotation for training robust computer-use agents. The open-source release of our ScreenFilter and Video2Action tools enables the community to leverage this abundant resource for advancing GUI automation research."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos, 2022. URL https://arxiv.org/abs/2206.11795. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Guicourse: From general vision language models to versatile gui agents, 2025. URL https://arxiv.org/abs/2406.11317. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, and Ice Pasupat.etc. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web, 2023. URL https://arxiv.org/abs/2306.06070. Yunseok Jang, Yeda Song, Sungryull Sohn, Lajanugen Logeswaran, Tiange Luo, Dong-Ki Kim, Kyunghoon Bae, and Honglak Lee. Scalable video-to-dataset generation for cross-platform mobile agents, 2025. URL https://arxiv.org/abs/2505.12632. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents, 2024. URL https://arxiv.org/abs/2406.03679. Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action proposal generation, 2019. URL https://arxiv.org/abs/1907.09702. Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, and Yue Wang. Learning from massive human videos for universal humanoid pose control, 2024. URL https://arxiv.org/abs/2412.14172. Duy Nguyen-Tuong and Jan Peters. Using model knowledge for learning inverse dynamics. In 2010 IEEE international conference on robotics and automation, pp. 26772682. IEEE, 2010. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents, 2025. URL https://arxiv.org/abs/2501. 12326. Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. Real-time flying object detection with yolov8, 2024. URL https://arxiv.org/abs/2305.09972. Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan O. Arık. Learn-by-interact: datacentric framework for self-adaptive agents in realistic environments, 2025. URL https://arxiv.org/ abs/2501.10893. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis, 2025. URL https://arxiv.org/ abs/2412.19723. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinhao Li, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yuhao Dong, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, and Zongyu Lin. Kimi-vl technical report, 2025. URL https://arxiv.org/abs/2504.07491. 9 Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, ..., Qinghao Zhao, and Guang Shi. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning, 2025a. URL https://arxiv.org/abs/2509.02544. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025b. URL https://arxiv.org/abs/2508.09123. Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Videogroundingdino: Towards open-vocabulary spatio-temporal video grounding, 2024. URL https://arxiv. org/abs/2401.00901. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. URL https://arxiv.org/abs/2404.07972. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025a. URL https://arxiv.org/abs/2505. 13227. Tianbao Xie, Mengqi Yuan, Danyang Zhang, Xinzhuang Xiong, Zhennan Shen, Zilong Zhou, Xinyuan Wang, Yanxu Chen, Jiaqi Deng, Junda Chen, Bowen Wang, Haoyuan Wu, Jixuan Chen, Junli Wang, Dunjie Lu, Hao Hu, and Tao Yu. Introducing osworld-verified. xlang.ai, July 2025b. URL https://xlang.ai/blog/ osworld-verified. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials, 2025a. URL https://arxiv. org/abs/2412.09605. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2025b. URL https://arxiv. org/abs/2412.04454. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, and Minjoon Seo. Latent action pretraining from videos, 2025. URL https://arxiv.org/abs/2410.11758. Weijun Zhuang, Qizhang Li, Xin Li, Ming Liu, Xiaopeng Hong, Feng Gao, Fan Yang, and Wangmeng Zuo. Grounding-md: Grounded video-language pre-training for open-world moment detection, 2025. URL https: //arxiv.org/abs/2504.14553."
        },
        {
            "title": "A YouTube Video Quality Standards",
            "content": "To ensure consistency and usability in selecting high-quality instructional videos from YouTube for research purposes, the following standards must be met: 1. Minimal Overlays. If overlays, such as face cams or titles, are present, they must occupy no more than 1 10 of the screen area to avoid obstructing the primary content. 2. Primary Focus on Screen Recording. The video should predominantly feature clean screen recordings. Brief transitions to other scenes, such as PowerPoint slides or face capture, are permissible but should be limited to introductory or concluding segments. 3. Screen Recording Method. The video must consist of direct screen recordings rather than footage captured by an external camera. 4. Language Requirement. The video must be in English to facilitate monolingual captioning in subsequent processing steps. 5. Stable Visual Presentation. Frequent zooming in or out should be avoided. The entire screen or application window must be visible for the majority of the video duration. 6. Caption Availability. The video must include captions, indicated by the availability of the closed caption (CC) icon in the bottom right corner of the player. Captions may be auto-generated or manually annotated. 7. Orientation. The video must be recorded in horizontal format, as vertical videos often fail to capture complete desktop screens, limiting their utility. 8. Recency. Videos must be no older than five years to ensure that the user interfaces depicted remain relevant and applicable. These criteria ensure that selected videos are suitable for detailed analysis and processing in research contexts."
        },
        {
            "title": "B SCREENFILTER Details",
            "content": "SCREENFILTER is trained on 15,000 synthetic images generated by compositing cursor sprites onto GUI screenshots from the GUIEnv (Chen et al., 2025) dataset. To enhance its generalization across different platforms, we incorporate various cursor patterns from both Windows and macOS. On the held-out test set, SCREENFILTER achieves an F1 score of 89.58%, with 90.64% precision and 88.54% recall, demonstrating its effectiveness in accurately separating computer-use content from unrelated material. For video processing, SCREENFILTER operates at 1-2 frames per second to balance both accuracy and efficiency. The model retains segments where at least 80% of the frames contain cursor for minimum of 6 seconds, with 2-second temporal smoothing gap to merge frames. This design allows SCREENFILTER to process approximately 840 hours of video per GPU-day, facilitating large-scale filtering."
        },
        {
            "title": "C Dense Event Detection",
            "content": "Our dense detector is trained on 154 hours of screen-capture video paired with raw interaction logs from OpenCUA (Wang et al., 2025b). The logs contain complete demonstrations with precisely timestamped GUI interactions (mouse and keyboard). We convert these logs into prompt-free temporal grounding supervision by mapping lowlevel events to our action taxonomy, merging short consecutive micro-events into typed spans with start and end timestamps, and discarding segments without actionable GUI operations. For training-set preparation, we downsample videos to 4 fps, segment them into non-overlapping 10 clips, and align the interaction logs within each clip to obtain typed spans with start and end timestamps. We adopt temporal patch size of 2 frames for modeling efficiency. Label names are normalized to our action taxonomy. We visualize the data sample in Figure 7 Model training. We perform full-parameter supervised fine-tuning of Qwen2.5-VL-7B-Instruct. The training configuration and loss curve are shown side-by-side in Figure 8."
        },
        {
            "title": "D Action Identification",
            "content": "Training data. Our dense detector is trained on 512,000 screen-capture clips paired with raw interaction logs from OpenCUA (Wang et al., 2025b). To preprocess action segments, we adopt dynamic frame-rate policy that caps frames per 11 Table 3: Event distribution in the training data (154 hours). Action type Count click key write scroll moveTo dragTo doubleClick rightClick hscroll hotkey tripleClick middleClick Total 410,101 138,660 80,749 46,597 32,840 32,840 14,241 7,451 3,411 2,570 2,428 57 771,945 Figure 7: Example training sample for the dense event detector. clip at 20 while preserving short, fast interactions. For segment of duration (seconds), we set k]. This yields, for example, =30 for brief clicks/scrolls (t 0.5 s, then sample frames uniformly within [ts 15 frames), 20 for 1.0 (20 frames), and =4 for extended typing segments (t 5 s, 20 frames). We visualize the data sample in Figure = min{30, max{4, 20/t}}, k, te"
        },
        {
            "title": "E Inner Monologue Generation",
            "content": "Prompt Content. Parameters: πk , Ipost k, te k], [0, 60s] Inputs: Action type: ak Before/after keyframes: Ipre ASR windows: [60s, 0], [ts Instruction (to model): You are generating inner-monologue annotations for dataset of GUI agent trajectories built from in-the-wild screen recordings. End-to-end setting. Source: real GUI screen recordings from the wild. Extraction: each GUI interaction (an action) is automatically detected from video/audio. For every detected action, you receive three kinds of evidence: Action details: {action type} and {action content}. action content may contain: coordinates (absolute or normalized) and/or bbox; typed text; 12 Megatron-LM Framework 32 H100 GPUs Hardware Tensor parallelism TP = 4 Pipeline parallelism PP = 1 Global batch size Training iterations LR decay iterations Wall-clock time 256 2000 2000 15 Figure 8: Dense event detector: training loss (left) and training configuration (right). Figure 9: Example training sample for the action parametrization model. pressed keys; scroll amount/direction; drag start/end; and similar specifics. Keyframes: start screenshot and, if available, an end screenshot right after the action executes. Surrounding transcripts: short snippets of narration or speech immediately before, during, and after the action. Action validation (optional): brief validator description summarizing what occurred. Your task. For each action, output exactly one JSON object with two fields: action description and thought. Field definitions (strict). action description: concise natural-language description of what do in the UI at this step. Name the target UI element if inferable (button, menu, tab, field); otherwise describe by role/label/relative position. Mention the immediate visible outcome only if it is clearly observable. Forbidden: raw coordinates, code, function/method names, automation tokens, keyvalue argument lists. thought: my first-person inner monologue (48 sentences) as the demonstrator (use I, me, my). Provide substantive reasoning. Include: (a) what aim to accomplish and why now; (b) how the speech context informs my intent (weave naturally); (c) brief summary of what likely changes from start to end if both frames exist; (d) short breakdown of the atomic actions in this step (e.g., type + press) and why each is needed; (e) what expect to verify or do next. Prefer present tense when natural. General rules. 13 (a) Stage-1 training (b) Stage-2 training Figure 10: Computer Use Agent Training Data (a) Stage-1 training, (b) Stage-2 training. The thought must be in first person; never switch to third person. Evidence priority: prefer visual evidence from start/end keyframes; treat speech as weak hint for why. If they conflict, prefer visuals. Weave evidence naturally without naming transcripts or frames. For coordinate-based actions, red hollow circle may mark the interaction point; do not mention the marker, describe the target element instead. If only start keyframe is available, focus on intent; if an end keyframe exists, you may include the immediate visible result. When step bundles multiple atomic actions, reason across them as one coherent operation. Keep action description concise; let thought carry the details; avoid hedging and boilerplate. Output format: exactly one valid JSON object with only action description and thought; no extra keys or commentary. Output: rk: inner-monologue JSON with fields action description and thought."
        },
        {
            "title": "F Computer Use Agent Training",
            "content": "Training Data. we visualize the training data samples in stage-1 and stage-2 training in Figure 10. Stage-1 training. We perform full-parameter continue-pretraining Qwen2.5-VL-7B-Instruct. The training configuration and loss curve are shown side-by-side in Figure 11. Stage-2 training. We continue full-parameter supervised fine-tuning on the stage-1-trained checkpoint. The training configuration and loss curve are shown side-by-side in Figure 12."
        },
        {
            "title": "G Computer Use Agent Evaluation",
            "content": "Evaluation Setting. We follow the OSWorld-Verified protocol: the agent interacts with live desktop given natural-language instruction and the full history of prior states and actions. At each step, the policy conditions on the instruction and bounded visual context of up to five recent screenshots (FIFO window) together with the action/rationale history, then emits the next action. For the 20-step budget, we conduct three independent runs per model and report the average Task SR. For the 50-step budget, we perform single run. All models use identical 14 Megatron-LM Framework 32 H100 GPUs Hardware Tensor parallelism TP = 4 Pipeline parallelism PP = 1 Global batch size Training iterations LR decay iterations Wall-clock time 512 6500 6500 60 Figure 11: CUA Stage-1 Training: training loss (left) and training configuration (right). Megatron-LM Framework 64 H100 GPUs Hardware Tensor parallelism TP = 4 Pipeline parallelism PP = 1 Global batch size Training iterations LR decay iterations Wall-clock time 512 3000 3000 16 Figure 12: CUA Stage-2 Training: training loss (left) and training configuration (right). inference settings and action executors; no manual interventions are allowed during evaluation. AgentNet Bench summary. Overall step SR rises from 0.385 (base) to 0.641 with SFT-only and to 0.693 with Stage 1 + Stage 2. These trends suggest that video pretraining notably strengthens grounding and multi-action control, especially for less frequent or harder motor primitives. OSWorld-Verified summary. Table 4 reports task success across turns and step budgets. With SFT-only (stage2 only), Task SR hovers around 9.19.4% at 20 steps and shows no improvement at 50 steps (9.27%), indicating limited ability to leverage longer budgets. Adding VideoAgentTrek pretraining (stage1 + stage2) raises Task SR to 13.614.7% at 20 steps and further to 15.78% at 50 steps. Per-domain counts improve most for chrome/46 (up to 15 solved) and workflow/92 (up to 8 solved), with steady gains in os/24 and authoring apps (writer, impress). Across three 20-step runs, variance is modest, suggesting stable benefits from Stage 1. Overall, the results show that large-scale video pretraining yields higher step quality and makes the agent budget-sensitiveable to convert extra steps into additional task completions. Model Eval Task SR (%) calc/46 chrome/46 gimp/ vscode/23 writer/23 tbird/15 os/24 impress/47 workflow/92 vlc/17 stage2 only stage1 + stage2 turn1 (20) turn2 (20) turn3 (20) turn4 (50) turn1 (20) turn2 (20) turn3 (20) turn4 (50) 9.42 9.13 9.42 9.27 14.68 13.57 14.13 15.78 1 2 1 2 2 2 1 8 8 8 12 13 13 12 15 3 1 2 2 2 2 2 2 6 6 6 6 5 7 6 2 2 2 2 5 5 7 6 3 2 2 2 6 6 6 6 2 2 2 4 3 3 4 3 3 4 4 6 6 5 6 5 5 5 4 7 5 5 8 1 2 2 2 2 2 3 Table 4: OSWorld-Verified full results. Counts indicate solved tasks per application bucket (denominators shown in headers). Model Step SR click write press scroll moveTo dragTo hotkey dbClick rClick terminate base stage2 only stage1 + 2 0.385 0.641 0.693 0.402 0.671 0.767 0.605 0.719 0.733 0.286 0. 0.615 0.500 0.600 0.189 0.300 0.502 0.000 0.145 0.264 0.250 0.484 0.562 0.000 0.526 0.650 0.000 0.214 0. 0.188 0.588 0.237 Table 5: AgentNet Bench: step success rate (overall and per action type). indicates the metric was not applicable/recorded."
        },
        {
            "title": "H VideoAgentTrek Data Analysis",
            "content": "Resolution and scale. We downloaded 55,603 screen-capture videos (about 10,000 hours) from 50+ channels. The corpus is predominantly clear: 97% are 720p or higher  (Table 6)  . Most videos are minutes long, providing sustained interactions suitable for dense detection and action identification. Resolution bucket High (1080p+) Standard (720p1080p) Low (<720p) Count 2,322 49,589 1,464 Table 6: Resolution distribution of downloaded videos. Title/description-based content classification. To quickly audit topical relevance at scale, we apply lightweight classifier to each videos title and brief description. Labels. tutorial: hands-on screen tutorials. * Include: step-by-step demonstrations, cursor-driven walkthroughs, how to . . . tasks; frequent UI focus changes; imperative phrasing in titles (Create. . . , Install. . . , Fix. . . ). * Exclude: talk-style narrations with few concrete on-screen actions; marketing teasers without real steps. * Signals: verbs tied to UI operations (open, click, type), timestamps/chapters per step, tool/app names plus action verbs. background: expository background with incidental screen use. * Include: market share reports, product overviews, concept explainers where the desktop appears only as backdrop. * Exclude: segments that actually show multi-step operations (move to tutorial). * Signals: nouns like overview, history, comparison, review, charts/stats in title/description, little or no cursor interaction. tech talk: talks or presentations with slides. * Include: conference talks, webinars, lectures; slide navigation with limited live demos. * Exclude: talks that transition into substantial live step-by-step demos (then split or relabel tutorial). * Signals: keynote, webinar, seminar, lecture, speaker names/affiliations, slide thumbnails. unrelated: off-topic for computer-use learning. * Include: content where screen appears but no actionable computer-use task is taught (e.g., pure entertainment, face-cam only). * Exclude: any clip with consistent stepwise UI operations (move to tutorial). * Signals: lifestyle/vlog tags, gameplay without UI instruction, no app/task keywords. 16 Procedure. Single-pass GPT-4.1 classification with short instruction to choose exactly one of the four labels given the title and short description; no transcript or frames are used. We use the result only for corpus auditing, tag mining, and optional down-weighting in later filters, not as hard accept/reject gate. Limitations. Metadata-only classification can mislabel borderline cases (e.g., talks that include substantial demos). Final training sets are still screened by cursor gating, license/PII checks, and downstream detectors. Distribution. Class counts and shares are shown in Table 7. The majority are tutorials, indicating strong alignment with our target use case. Label Count Share tutorial background tech talk unrelated 38,700 12,900 2,391 1,612 69.6% 23.2% 4.3% 2.9% Total 55, 100% Table 7: distribution from title/description classification. Action type Count Share (%) left click type key scroll right click double click mouse move drag hscroll Total 1,037,617 214,816 145,860 111,203 24,111 11,848 8,441 6,372 196 1,547,092 67.1 13.9 9.4 7.2 1.6 0.8 0.1 0.1 0.0 100.0 Table 8: Action distribution in the VideoAgentTrek agentic dataset. Action distribution. Table 8 summarizes action counts in the VideoAgentTrek agentic data. Cross-dataset comparison. We summarize reported average step counts (and task counts when available) for common CUA datasets and include our corpus for context. Dataset Tasks Avg. Step AndroidControl (Li et al., 2024) OS-Genesis (Sun et al., 2025) AgentTrek (Xu et al., 2025a) Mind2Web (Deng et al., 2023) AgentNet (Wang et al., 2025b) 15,283 2,451 10,398 2,350 22,625 5.5 6.4 12.1 7.3 18.6 VideoAgentTrek 39,000+ 39.25 Table 9: Average steps across datasets (as reported in their papers). Estimated from 5,416-trajectory sample in our corpus. Table 10: VideoAgentTrek data distribution of step number"
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Group",
        "The University of Hong Kong"
    ]
}