{
    "paper_title": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification",
    "authors": [
        "Xu Xu",
        "Xin Li",
        "Xingwei Qu",
        "Jie Fu",
        "Binhang Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification generation in Dafny. Unlike prior benchmarks that focus on single-function tasks, DafnyCOMP targets programs composed of multiple interacting functions with data dependencies, requiring reasoning across component boundaries. The benchmark consists of 300 automatically synthesized multi-function programs. We evaluate several state-of-the-art LLM families and find that, while they perform well on single-function verification, their performance drops sharply on compositional tasks. Analysis reveals systematic failures in cross-functional reasoning, including fragile specifications, misalignment between implementations and proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for measuring progress toward reliable, verifiable, and compositional code generation with LLMs."
        },
        {
            "title": "Start",
            "content": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification Xu Xu1,, Xin Li2,, Xingwei Qu3, Jie Fu4 Binhang Yuan1, 1HKUST, 2NTU, 3UoM, 4Shanghai AI Lab Abstract Despite rapid advances in code generation, current Large Language Models (LLMs) still lack an essential capability for reliable and verifiable code generation: compositional reasoning across multifunction programs. To explore this potential and important gap, we introduce DAFNYCOMP, benchmark designed to systematically evaluate LLMs on the generation of compositional specifications in Dafny. Unlike prior benchmarks that primarily target single-function annotation, DAFNYCOMP focuses on programs composed of multiple interacting functions with necessary data dependencies, requiring LLMs to produce specifications that ensure correctness across component boundaries. Our benchmark comprises 300 automatically synthesized programs, each carefully constructed by combining 25 originally independent functions in chain-based manner through LLM-driven synthesis. We evaluate LLMs from five leading research groups that represent the current frontier of reasoning-centric AI, including the GPT, CLAUDE, GEMINI, DEEPSEEK, and QWEN families. Our results reveal striking dichotomy: while LLMs achieve both high syntax correctness (>99%) and moderate verification rates (>58%) in prior single-function benchmarks, they exhibit degraded syntax correctness (95.67%) and catastrophic verification failure (3.69%) in DAFNYCOMPs compositional tasksa 92% performance gap. Even the most powerful LLM achieves only 7% verification at Pass@8, with most LLMs below 2%. Further analysis reveals that LLMs systematically fail at cross-functional reasoning through three primary failure modes: specification fragility (39.2%), implementation-proof misalignment (21.7%), and reasoning instability (14.1%). These failures clearly reveal the absence of compositional reasoning capabilities in current LLMs. DAFNYCOMP thus establishes diagnostic benchmark for tracking progress in verifiable code generation with LLMs, highlighting that the path from local to compositional verification remains largely uncharted. Project homepage: https://DafnyComp.github.io 5 2 0 2 7 ] . [ 1 1 6 0 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have transformed software development through their remarkable code generation capabilities, enabling developers to produce complex programs from natural language descriptions [3, 4]. These advances have driven widespread adoption of programming assistants and development environments, fundamentally transforming how modern software is developed. As LLM-generated code becomes increasingly integrated into production systems, critical question emerges: how to ensure the correctness of automatically *Equal contribution. Correspondence to: Binhang Yuan <biyuan@ust.hk>. 1 Figure 1 The formal verification gap: high syntax success versus low verification rates. (a) Benchmark performance reveals dramatic gap between syntax correctness and verification success, with DAFNYCOMP showing 92% drop from 95.67% to 3.69%. (b) All models converge to high syntax correctness at @8 shots, with performance ranging from 92% to 99%. (c) Verification rates remain critically low (<8%) across all models despite increased sampling, with Claude-3.5 achieving the highest rate at only 7%. synthesized programs. Unlike human-written code that can be manually reviewed and tested, the scale and complexity of LLM outputs demand systematic approaches to verification that go beyond traditional debugging methods. On the other hand, conventional testing provides only partial confidence and cannot rule out rare corner cases or subtle specification mismatches. Formal verification provides principled solution to this challenge by offering mathematical guarantees of program correctness through rigorous specification and proof techniques. Programming languages like Dafny enable developers to express precise contractspreconditions, postconditions, and invariantsthat can be mechanically verified against implementations [17]. However, the adoption of formal verification has historically been constrained by what we often refer to as the specification bottleneck: writing comprehensive annotations not only demands specialized expertise but also produces specification code that is comparable in size to the implementation itself [18, 24]. Recent research has explored the use of LLMs to automate this specification generation process, demonstrating promising results where models can complete missing annotations for individual functions and achieve moderate verification success rates [23, 35]. However, current work in this area often suffers from critical limitation: existing benchmarks, such as DAFNYBENCH, primarily evaluate annotation completion within isolated functions [23], failing to address the compositional reasoning ability required for real-world, sophisticated software systems, where correctness emerges from complex interactions between multiple components [15]. To fill this gap, we introduce DAFNYCOMP, the first benchmark explicitly designed to evaluate the generation of compositional specifications for programming languages equipped with formal verification. Concretely, we make the following contributions: Contribution 1. To address the limitations of prior verification benchmarks, we present DAFNYCOMP, new benchmark explicitly designed for compositional formal verification (3). Unlike existing datasets such as DAFNYBENCH [23] that focus on specification generation for isolated single functions, DAFNYCOMP evaluates LLMs on programs composed of multiple interacting functions with real data dependencies. The benchmark consists of 300 Dafny programs synthesized by combining 25 independent functions, forcing models to reason across function boundaries to ensure end-to-end correctness. Note that our design is the first to require actual compositional reasoning in specification generation, bridging critical gap left by prior benchmarks and reflecting the complexities of real-world software systems. Contribution 2. We comprehensively evaluate 13 state-of-the-art LLMs on the constructed DAFNYCOMP (4) benchmark, including advanced models like GPT-4O, CLAUDE 3.5, GEMINI 2.5, DEEPSEEK-V3.1, and QWEN3CODER. The results reveal dramatic collapse in verification performance despite high syntactic accuracy: while the models produce syntactically correct code for approximately 95.7% of the tasks, only 3.7% of their 2 outputs actually pass the formal verifier. This staggering 92% gap between syntax success and semantic correctness persists across all model families, prompt settings, and sampling strategies. Even with up to 8 attempts per problem, the best model attains only around 7% verification success, indicating that increasing sampling or context does not remedy the fundamental limitation. Contribution 3. We carefully analyze the failure cases in the benchmark, which pinpoints three primary failure modes underlying this breakdown (5), highlighting systemic obstacles to compositional reasoning in current LLMs: (i) Specification fragility: we observe the brittleness of generated specifications wherein small omissions, over-/under-strengthening, or inconsistent framing (reads/modifies) clauses can invalidate downstream proofs. Concretely, in compositional settings, missing or slightly weaker postcondition at one stage can fail to imply callees precondition, triggering domino effect along the call chain even when each component appears locally reasonable. (ii) Implementationproof misalignment: We identify inconsistencies between the produced code and its associated specifications or proofs, indicating that models often generate implementations and annotations via largely independent pathways. Typical symptoms include plausible-but-false loop invariants, contradictory requires/ensures obligations, or termination metrics that do not match control flow, any of which cause verification to fail despite syntactic well-formedness. (iii) Reasoning instability: we witness tendency to lose the inductive thread of the argument over multiple steps, leading to invariants that are not preserved, incomplete coverage of cases, or missing well-founded decreases measures. These errors are most evident in composition, where maintaining complex state relationships across iterations and function boundaries is crucial for end-to-end correctness. These failure modes were pervasive in the models outputs, revealing fundamental absence of robust compositional reasoning capabilities. By identifying these issues, DAFNYCOMP serves as diagnostic benchmark for the community, enabling systematic measurement of progress toward LLMs that can verify complex multicomponent programs."
        },
        {
            "title": "2 Related Work",
            "content": "Formal Verification Benchmarks. Existing benchmarks for verifiable code generation can be categorized into two types. Single-function benchmarks, such as DAFNYBENCH [23] and MBPP-DFY [26], evaluate annotation completion within isolated methods, achieving moderate success rates (50-60%) but failing to capture interfunction dependencies. Interactive theorem proving benchmarks (miniCodeProps [22], FVAPPS [8]) target proof synthesis in systems like Lean [6] but require extensive manual validation and remain disconnected from practical programming. DAFNYCOMP bridges this gap by evaluating compositional specification generationa prerequisite for scaling verification beyond toy programs to production systems. Unlike prior work, we explicitly construct multi-function programs with data dependencies, exposing the compositional reasoning deficit in current models (see Appendix for detailed comparison). Dynamic Benchmark Generation. Static benchmarks suffer from contamination and overfitting [12, 36]. Dynamic generation techniquescreating new tasks, transforming problems, or perturbing reasoning structures [37, 38]show promise in mathematics and logic but neglect formal verifications unique demands: specifications must be syntactically valid, semantically precise, and correct across all execution paths. Our synthesis pipeline addresses this by generating verifiable multi-function Dafny programs through controlled composition, ensuring both novelty and correctness while maintaining the semantic complexity that exposes compositional reasoning gaps. Compositional Reasoning in LLMs. The ability to systematically combine simpler units into correct larger structures remains frontier challenge [9, 20]. While progress exists in natural language and symbolic domains, formal verification imposes stricter demands: specifications must preserve invariants across components and ensure global correctness. Existing training paradigms favor pattern matching over principled proof construction. By requiring models to generate specifications bridge function boundaries with explicit data 3 dependencies, DAFNYCOMP provides the first diagnostic benchmark for compositional reasoning in formal verification."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "DAFNYCOMP synthesizes 300 verified multi-function programs through two-stage pipeline (Figure 2), including program assembly (3.1) and formal translation (3.2), which bridges the gap between practical Python implementations and verification-oriented Dafny specifications. Within this pipeline, program assembly ensures the construction of compositional Python programs with functional correctness, while specification translation with refinement ensures the quality and reliability of the resulting data. We also provide format details of evaluation tasks (3.3) and the key characteristics of the benchmark (3.4). Figure 2 Two-stage benchmark synthesis: (1) Assembly combines independent Python functions with controlled data flow, ensuring algorithmic complexity while maintaining tractability; (2) Formal translation converts to verified Dafny through incremental AST-guided transformation."
        },
        {
            "title": "3.1 Program Assembly",
            "content": "We construct compositional programs by systematically combining functions from LEETCODEDATASET [34], selected for their algorithmic depth and verification challenges. Function Selection. We filter the corpus using McCabes cyclomatic complexity [25] as proxy for verification difficulty, retaining only functions with complexity >5 (around top 30% of the dataset) and at least 10 lines of code. This threshold ensures non-trivial control flowloops with complex termination conditions, nested conditionals, recursive patternsthat stress specification generation. For tractability, we restrict to single-input/single-output functions, yielding 1,847 candidate functions. Compositional Strategy. Following Hu et al. [12], we employ chain-based composition where each functions output feeds the nexts input, creating explicit data dependencies. While more complex call graphs (trees, DAGs) are theoretically richer, empirical trials showed synthesis success drops from 47% (chains) to <8% (arbitrary graphs) while providing no additional diagnostic valuethe chains suffice to expose compositional failures. After composition, we further identify the minimal set of shared import dependencies across the combined Python functions. This step is essential because the original LEETCODEDATASET often relies on broad import * statements, which obscure library ownership and names. Without explicit mappings, Dafny cannot interpret external libraries, preventing the synthesis of intermediate functions to replace missing third-party features. We generate programs with 25 functions, exploring multiple permutations since function ordering affects both data flow and verification complexity. 4 Validation Pipeline. After composition, the resulting Python code is subjected to three-stage validation pipeline, which filters candidates before their use in Section 3.2. (i) Type checking via constraint propagation: We statically infer candidate types and shapes for each functions inputs and outputs and propagate these constraints along the composition chain. This pass rejects compositions with incompatible interfaces (e.g., scalar-sequence or element-type mismatches) and flags violations of simple value constraints inferred from guards (such as non-negativity or length bounds). The result is set of compositions whose interfaces are consistent end-to-end, providing reliable basis for subsequent translation and verification. (ii) Formatting standardization: We apply deterministic rewriter, implemented with tools such as Black and isort, to normalize code style, including indentation, whitespace, line breaking, and import organization. Canonicalizing these incidental variations yields stable, diff-friendly artifacts and reduces prompt variance in later stages. This step preserves semantics while producing uniform program layouts that are easier to parse, translate, and verify. (iii) Test validation: We execute each composed program against the reference unit tests from LEETCODEDATASET to confirm functional correctness and basic executability. Programs that raise runtime exceptions, fail assertions, or produce incorrect outputs are discarded, ensuring only behaviorally sound compositions advance. This filtering isolates verification challenges to specification and reasoning rather than implementation errors during the Dafny translation stage. Following this procedure, we obtain 1,200 valid Python programs with 25 functions, which will be used in the next stage."
        },
        {
            "title": "3.2 Formal Translation",
            "content": "We translate validated Python compositions into Dafny implementations with formal guarantees, focusing on the verification-oriented aspects of the benchmark here. Translation Challenges. Direct end-to-end translation from Python to Dafny proved largely ineffective, with empirical success rates below 5%. The core difficulty lies in Dafnys demand for explicit specifications, invariants, and termination argumentssemantic elements absent in Python. This semantic gap makes single-pass translation infeasible for non-trivial programs. Incremental Pipeline. Inspired by Wen et al. [33], we adopt an incremental approach: the abstract syntax tree (AST) of each Python program is decomposed into functionor control-structurelevel fragments. Each fragment is translated into Dafny and immediately verified, localizing errors to the smallest possible unit. Verified fragments are then progressively reassembled according to the AST hierarchy, culminating in complete Dafny program. Importantly, although translation proceeds incrementally, the Python program must be composed in its entirety before it can be executed. Whole-program composition in Python provides two benefits: (i) Pythons explicit AST nodes and mature tooling make program assembly more reliable and transparent; and (ii) having coherent Python blueprint ensures that the incremental Dafny translation preserves global logical relationships, rather than producing isolated fragments that fail to compose. Thus, whole-program composition and incremental translation are complementary design choices. To further improve reliability, each candidate Dafny program undergoes up to ten refinement iterations, where specifications are strengthened in response to verifier feedback (e.g., adding loop invariants, refining postconditions, or inserting assertions). The entire synthesis and refinement process is carried out by CLAUDE-4-SONNET-20250514, with the exact prompts provided in Appendix E. In total, the pipeline ultimately yields 564 verified Dafny programs from 1,200 attempts (corresponding to an overall 47% success rate). Translation synthesis errors primarily arise from incomplete specifications (31%), type inference errors (22%), timeouts (18%), and irreconcilable semantic gaps (29%). From these, we retain 300 programs carefully balanced across complexity levels (100 each with 23, 34, and 45 functions). To ensure evaluation integrity, we conduct thorough contamination analysis against MBPP-DFY [26], which is similarly synthesized from Python code as Dafny benchmark dataset. The results in Appendix provide strong evidence that our test set is indeed free from bias due to data overlap."
        },
        {
            "title": "3.3 Evaluation Task Format",
            "content": "We adopt specification reconstruction task inspired by Loughridge et al. [23]. Still, with crucial difference: rather than removing all assert and invariant statements, we strip away the contract clauses (requires, ensures, reads, modifies, decreases) that appear before the opening brace of each method or function. LLMs to be evaluated are then required to regenerate these specifications to enable verification. This design isolates the challenge of reconstructing cross-function contracts from implementation concerns, focusing evaluation on whether models can generate specifications that capture emergent correctness properties across component boundaries. Unlike annotation completion tasks that permit purely local reasoning, our multifunction programs require understanding how data flows and invariants propagate through compositions. We employ unified prompt across all evaluations (see Appendix G)."
        },
        {
            "title": "3.4 Benchmark Statistical Summary",
            "content": "The resulting benchmark comprises 300 mechanically verified Dafny test cases that jointly capture three key dimensions: compositional complexity from function-to-function call dependencies, algorithmic diversity across multiple categories, and verification challenges arising from the increased specification burden. Compositional Complexity. Each program contains 25 functions (mean = 3.2) with an average of 8.4 crossfunction data dependencies, requiring models to reason about specification alignment across component boundaries. Unlike single-function benchmarks where specification generation is largely local, our programs demand that preconditions of called functions be implied by postconditions of their callersa requirement that introduces cascading verification challenges when specifications fail to propagate correctly. Algorithmic Diversity. The benchmark spans 15 algorithmic categories with balanced representation: dynamic programming (18%), string manipulation (20%), number theory (15%), and graph algorithms (12%) constitute the primary categories, with the remainder distributed across sorting, searching, and combinatorial problems. Beyond the balance of individual categories, diverse permutations and combinations of these types yield composed programs with more intricate, layered structures. Consequently, the target of composing function calls is reinforced by the characteristics of the source dataset (LEETCODEDATASET [34]), which in turn induces composition at the level of algorithmic logic. This design ensures models must develop general compositional reasoning rather than memorizing category-specific patterns. Verification Challenges. Every program is mechanically verified by Dafny 4.10.0, thereby providing ground-truth correctness. The median program requires 7 loop invariants and 4 assertions for verification, with 23% demanding intricate termination arguments via decreases clausesa 3.5 increase in annotation density compared to DAFNYBENCHs average of 2 per program. This added specification burden reflects the extra complexity of compositional verification, creating graduated challenge that pinpoints where current models shift from local reasoning to compositional failure."
        },
        {
            "title": "4 Experimental Setup And Results",
            "content": "In this section, we enumerate the evaluation metrics (4.1), LLM model selection for the benchmark (4.2), and the corresponding evaluation results (4.3)."
        },
        {
            "title": "4.1 Metrics",
            "content": "We evaluate two complementary aspects of model performance: Syntax Correctness: measures whether generated specifications parse successfully in Dafny. This baseline metric captures models grasp of the formal language syntax. Verification Rate: measures the fraction of syntactically correct programs that pass Dafnys verifierthe ultimate test of semantic understanding. This metric is computed only over syntactically valid outputs, as verification requires parseable code. Following Chen et al. [4], we report Pass@k for {1, 2, 4, 8}, measuring the overall probability of successfully solving problem within attempts. Pass@1 provides strict zero-shot baseline of immediate reasoning ability, Table 1 Model performance reveals high syntax mastery but catastrophic verification failure. While syntax correctness reaches 99% with sufficient sampling, verification rates remain below 7% even for the best models, exposing the compositional reasoning gap."
        },
        {
            "title": "OpenAI Models",
            "content": "GPT-4O O4-MINI GPT-4."
        },
        {
            "title": "Anthropic Models",
            "content": "CLAUDE-3.5-SONNET CLAUDE-4-SONNET"
        },
        {
            "title": "Google Models",
            "content": "GEMINI-2.5-FLASH GEMINI-2.5-PRO DeepSeek Models DEEPSEEK-R1 DEEPSEEK-V3 DEEPSEEK-V3."
        },
        {
            "title": "Alibaba Models",
            "content": "Syntax Correct Rate (%) Verified Rate (%) @1 @2 @4 @ @1 @2 @4 @8 94.33 80.00 59.00 98.67 92.67 69.67 99.33 98.00 79.33 99.67 99.00 86.33 0.33 0.00 0. 0.33 0.00 0.00 0.33 0.67 0.00 0.33 0.67 0.00 90.67 95.67 96.33 97.33 98.67 98. 99.00 98.33 3.67 2.33 4.67 3.00 5.00 3.00 7.00 3.33 54.00 69. 64.00 81.00 81.00 91.67 89.67 96.00 0.00 0.00 0.00 0.33 0.00 0. 0.00 2.00 85.67 77.33 54.67 95.33 88.67 72.33 98.33 95.33 83.33 99.00 97.33 92.00 0.33 0.00 0. 0.00 0.00 0.00 0.33 0.00 0.00 0.33 0.33 0.00 0.33 0.33 0.00 0.33 0.33 0.00 0.33 0.33 0. 1.00 0.67 0.00 QWEN3-CODER-480B-A35B-INSTRUCT QWEN2.5-CODER-32B-INSTRUCT QWQ-32B 85.33 62.00 46.67 94.00 74.67 61.33 98.00 85.00 78.00 99.00 89.00 91. claude-3.5-sonnet-20241022, claude-4-sonnet-20250514 whereas larger values further exploit additional test-time compute to improve success on compositional tasks [32]. In this setting, Pass@8 is particularly informative for clearly distinguishing model robustness and adaptability."
        },
        {
            "title": "4.2 Model Selection",
            "content": "We evaluate 13 frontier models spanning five architectural families, chosen for their demonstrated strength in code generation and general reasoning: OpenAI: GPT-4O [14], GPT-4.1 [27], O4-MINI [28] Anthropic: CLAUDE-3.5-SONNET [1], CLAUDE-4-SONNET [2] Google: GEMINI-2.5-PRO, GEMINI-2.5-FLASH [10] DeepSeek: DEEPSEEK-R1 [11], DEEPSEEK-V3 [21], DEEPSEEK-V3.1 [7] Alibaba: QWEN2.5-CODER-32B [13], QWEN3-CODER-480B [30], QWQ-32B [31]"
        },
        {
            "title": "4.3 Results and Discussion",
            "content": "Table 1 presents our main experimental findings. We observe systematic verification collapse across all evaluated models, with four interesting observations: Observation 1. Universal verification failure despite syntactic mastery. The most striking result is the consistent 92percentage-point gap between syntax correctness and verification success across all models. At Pass@8, models achieve µ = 95.67% (SD = 4.21%) syntax correctness but only µ = 3.69% (SD = 2.14%) verification. This gap persists independent of: (i) model scale (480B vs 32B parameters, > 0.05), (ii) training specialization (code-specific vs general-purpose), (iii) architectural family (dense, MoE, constitutional), and (iv) increased 7 sampling (Pass@1 to Pass@8). The universality of this failure suggests fundamental architectural limitation rather than an optimization or data issue. Observation 2. Non-linear scaling reveals compositional breakdown. Comparing performance degradation from single-function (DAFNYBENCH) to multi-function (DAFNYCOMP) tasks reveals super-linear complexity scaling. With 3.2 increase in functions, we observe 14.4 decrease in verification success (from 53% to 3.69%). This disproportionate degradation cannot be explained solely by additive difficulty. Instead, it suggests that specification requirements grow combinatorially with function compositioneach function boundary introduces O(n2) potential specification dependencies that models fail to capture. Observation 3. Sampling saturation indicates capability ceiling, not search limitations. The verification-sampling curve plateaus by Pass@4 for all models, with the marginal improvement from Pass@4 to Pass@8 averaging only 0.8%. In contrast, syntax correctness continues improving (+7.3% on average), demonstrating that models can explore the output space but cannot discover valid specifications. This divergent behavior between syntax and semantics strongly suggests that current architectures lack the inductive biases necessary for compositional reasoning, rather than merely requiring better search strategies or more compute. Observation 4. Reasoning-specialized models show no clear advantage, thereby confirming architectural barriers. Models explicitly optimized for reasoning (QWQ-32B with chain-of-thought focus, DEEPSEEK-R1 with reinforcement learning) perform no better than general-purpose models, with QWQ-32B still achieving 0% verification even at Pass@8. The tight clustering of verification rates (coefficient of variation = 0.58) across diverse training objectives strongly indicates that compositional verification requires fundamentally different architectural primitivesnot refinements of existing transformer-based reasoning. This null result is particularly informative: it clearly demonstrates that neither extended reasoning traces nor reward-based optimization can overcome the absence of compositional inductive biases."
        },
        {
            "title": "5 Failure Case Analysis and Discussion",
            "content": "Table 2 Distribution of verification failure modes across 900 analyzed cases from DAFNYCOMP. Categories determined through automated error analysis and manual validation on 10% sample."
        },
        {
            "title": "Failure Mode",
            "content": "Frequency % of Total"
        },
        {
            "title": "Primary Mechanism",
            "content": "Specification Fragility ImplementationProof Misalignment Reasoning Instability Other (syntax, timeout, misc.) 353/900 195/900 127/900 225/900 39.2 21.7 14.1 25."
        },
        {
            "title": "Contract propagation failure\nIndependent generation pathways\nInductive chain breakdown\nVarious",
            "content": "The significant gap between syntax correctness and ultimate verification success demands clear mechanistic explanation. Through systematic analysis of 900 observed verification failures across three representative model families, we identify several distinct failure modes that reveal fundamental limitations in how transformers process compositional specifications. Table 2 presents the overall distribution of these failures, which we analyze in detail below."
        },
        {
            "title": "5.1 Specification Fragility: The Domino Effect",
            "content": "Specification fragility, the inability to generate contracts that remain valid across function compositions, constitutes the plurality of failures. Consider representative case from our benchmark: digitSum function correctly implemented but missing the postcondition ensures result >= 0. In isolation, this omission appears minor. In composition, it cascadeswhen digitSums output feeds downstream function expecting non-negative input, verification fails globally despite both functions being locally correct. Note that this pattern recurs throughout our dataset. Models generate specifications sufficient for local correctness but insufficient for compositional soundness. requires >= 0 precondition absent from one function invalidates the entire pipelines verification, even when each component individually passes most test cases. The fragility stems from fundamental mismatch: LLMs learn specifications as local patterns rather than global contracts. They 8 lack the architectural machinery to reason about how data constraints propagate through function callsa capability essential for modular verification. The implications extend beyond Dafny. Any system requiring compositional correctnessfrom distributed systems protocols to smart contract verificationwill face similar failures until models can reason about specification flow across component boundaries. The first key takeaway insight about specification fragility is summarized below: Takeaways (i): LLMs handle local specs but fail under composition. Missing contract propagation is the main cause of verification breakdowns."
        },
        {
            "title": "5.2 Implementation–Proof Misalignment: The Independence Assumption",
            "content": "The second failure mode reveals deeper architectural issue: LLMs treat implementation and specification as independent generation tasks rather than coupled constraints. In 21.7% of failures, syntactically valid code contradicts its own specifications. One striking example: model generated assert 0 >= 1; within otherwise reasonable codenot typo but systematic failure to maintain logical consistency. More subtle misalignments prove equally fatal. Loop invariants like forall :: 0 <= < ==> cnt[k] >= 0 appear plausible but fail verification because the implementations array access patterns violate the stated bounds. The model generates invariants that look right based on training patterns but dont correspond to the actual code behavior. This isnt surprising given transformer architecture: attention mechanisms excel at capturing local dependencies but struggle with the bidirectional constraints between specifications and implementations. Current training paradigms exacerbate this issue. Models learn from code-specification pairs without explicit feedback on the mutual consistency between them. The result: impressive performance on syntax and moderate success on individual functions, but catastrophic failure when consistency is required across boundaries. We summarize the second key takeaway about implementation-proof misalignment as: Takeaways (ii): Code and specs are generated independently, leading to plausible but inconsistent invariants. Future training for this task should enforce better alignment."
        },
        {
            "title": "5.3 Reasoning Instability: Induction as Achilles’ Heel",
            "content": "The third failure pattern, which we refer to as reasoning instability, exposes perhaps the most fundamental limitation. Formal verification relies on inductive reasoning: proving properties hold initially, maintain their validity through iterations, and compose across calls. LLMs consistently fail this inductive chain. Loop invariants that should accumulate state (e.g., invariant res == stringToIntHelper(s[..i])) break because models cannot track how program state evolves through iterations. Recursive functions lack proper termination arguments. Properties proven for base cases fail to extend inductively. This instability reflects the inherently statistical nature of reasoning exhibited by transformer architectures. While capable of patternmatching similar invariants from training data, models cannot construct the inductive proofs verification demands. They approximate rather than prove, which is sufficient for typical NLP tasks but inadequate for verifiable code generation, where formal verification is required. We summarize the third insight about reasoning instability as: Takeaways (iii): LLMs approximate base cases but fail to sustain inductive reasoning, exposing structural gap in formal verification."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "While DAFNYCOMP exposes fundamental limitations in compositional reasoning, we want to gently mention several constraints of our evaluation, which indicate some interesting future work. 9 Compositional Patterns. We restrict to chain-based compositions (sequential function calls) rather than complex topologies (recursive compositions, mutual dependencies) due to synthesis tractability. While chains suffice to demonstrate compositional failure, real systems exhibit richer patterns. Extending to arbitrary call graphs requires solving verification tractability for cyclic dependenciesa challenge independent of LLM capabilities. Specification Types. Our benchmark tests functional correctness (preconditions, postconditions, invariants) but not liveness properties, resource bounds, or security policies. These orthogonal concernse.g., proving memory consumption remains constant across compositionsrequire different verification techniques and evaluation metrics. Data Scarcity. The core challenge may be training data availability. Repositories contain only few verified multi-function programs with compositional specifications. Synthetic data generation or bootstrapped program synthesis could address this gap, although ensuring semantic diversity remains challenge."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce DAFNYCOMP, the first benchmark specifically designed to evaluate the generation of compositional specifications for formal verification. Through 300 synthesized multi-function Dafny programs, we systematically assessed 13 state-of-the-art LLMs on their ability to generate specifications that ensure correctness across function boundaries. Our results reveal fundamental capability gap: while models achieve greater than 99% syntax correctness and more than 58% verification on single-function benchmarks, they collapse to 3.69% verification on compositional tasksa 92% degradation. This performance cliff persists across all model families despite increased sampling (Pass@8), indicating an architectural rather than search limitation. Error analysis identifies three systematic failure modes: specification fragility (39.2%), implementation-proof misalignment (21.7%), and reasoning instability (14.1%), each reflecting the inability to maintain logical commitments across functional boundaries. In conclusion, DAFNYCOMP provides both diagnostic tool for current systems and concrete target for future research. We release the benchmark, evaluation framework, and synthesis pipeline to accelerate progress on this critical challenge."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. [2] Anthropic. Claude 4 sonnet. https://www.anthropic.com/news/claude-4-sonnet, 2025. Accessed: 2025-09-21. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient smt solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 337340. Springer, 2008. [6] Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In International Conference on Automated Deduction, pp. 378388. Springer, 2015. [7] DeepSeek-AI. Deepseek-v3.1 model introduction. https://www.deepseek.com/, 2025. Accessed: 2025-09-21. [8] Quinn Dougherty and Ronak Mehta. Proving the coding interview: benchmark for formally verified code In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. generation. 7279. IEEE, 2025. [9] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36:7029370332, 2023. [10] Google DeepMind. Gemini 2.5 models. https://deepmind.google/technologies/gemini/, 2025. 10 [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: dynamic complexityaware code benchmark for evaluating large language models in code generation. arXiv preprint arXiv:2503.10452, 2025. [13] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [15] Daniel Keysers et al. Compositional generalization in natural language processing. Transactions of the Association for Computational Linguistics, 8:1123, 2020. [16] K. Rustan M. Leino. Dafny: An automatic program verifier for functional correctness. In Logic for Programming, Artificial Intelligence, and Reasoning, pp. 348370. Springer, 2010. [17] K. Rustan M. Leino. Tour of the Dafny Program Verifier. Verified Software: Theories, Tools, Experiments, 2010. [18] K. Rustan M. Leino et al. Compositional verification of railway protection system with Dafny. Formal Aspects of Computing, 2017. [19] Yue Chen Li, Stefan Zetzsche, and Siva Somayyajula. Dafny as verification-aware intermediate language for code generation. arXiv preprint arXiv:2501.06283, 2025. [20] Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, and Ying Wei. Understanding and patching compositional reasoning in llms. arXiv preprint arXiv:2402.14328, 2024. [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [22] Evan Lohn and Sean Welleck. minicodeprops: minimal benchmark for proving code properties. arXiv preprint arXiv:2406.11915, 2024. [23] James Loughridge et al. DafnyBench: Benchmark for Formal Software Verification. Transactions on Machine Learning Research, 2024. Describes the fill annotations task. [24] James Loughridge et al. DafnySynth: Synthetic Dataset for Formal Verification. arXiv preprint arXiv:2411.15143, 2024. [25] Thomas McCabe. complexity measure. IEEE Transactions on software Engineering, (4):308320, 1976. [26] Md Rakib Hossain Misu, Cristina Lopes, Iris Ma, and James Noble. Towards ai-assisted synthesis of verified dafny methods. Proceedings of the ACM on Software Engineering, 1(FSE):812835, 2024. [27] OpenAI. Gpt-4.1 system card. https://openai.com/index/introducing-gpt-4-1/, 2024. [28] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. Accessed: 2025-09-21. [29] Gabriel Poesia, Chloe Loughridge, and Nada Amin. dafny-annotator: Ai-assisted verification of dafny programs. arXiv preprint arXiv:2411.15143, 2024. [30] Qwen Team. Qwen3-coder: Agentic coding in the world. https://qwenlm.github.io/blog/qwen3-coder/, 2025. Accessed: 2025-09-21. [31] Qwen Team. Qwq-32b: Large-scale reinforcement learning for reasoning models. https://qwenlm.github.io/zh/ blog/qwq-32b/, 2025. Accessed: 2025-09-21. [32] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [33] Cheng Wen, Jialun Cao, Jie Su, Zhiwu Xu, Shengchao Qin, Mengda He, Haokun Li, Shing-Chi Cheung, and Cong Tian. Enchanting program specification synthesis by large language models using static analysis and program verification. In International Conference on Computer Aided Verification, pp. 302328. Springer, 2024. 11 [34] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms. arXiv preprint arXiv:2504.14655, 2025. [35] Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Zhuangzhuang He, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, and Jie Fu. Re:Form Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: Preliminary Study on Dafny. arXiv preprint arXiv:2507.16331, 2025. [36] Zhehao Zhang, Jiaao Chen, and Diyi Yang. Darg: Dynamic evaluation of large language models via adaptive reasoning graph. Advances in Neural Information Processing Systems, 37:135904135942, 2024. [37] Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval: Dynamic evaluation of large language models for reasoning tasks. arXiv preprint arXiv:2309.17167, 2023. [38] Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. Dyval 2: Dynamic evaluation of large language models by meta probing agents. arXiv preprint arXiv:2402.14865, 3, 2024."
        },
        {
            "title": "A The Use of LLMs in Writing",
            "content": "During the preparation of this manuscript, we employed large language model (OpenAI GPT-5) to assist with language refinement and editorial improvements. Specifically, the LLM was used to enhance sentence fluency, improve clarity of expression, and ensure consistency with academic writing conventions. The tool was applied exclusively for linguistic polishingall research design, experimental work, data analysis, and core intellectual contributions remain entirely original."
        },
        {
            "title": "B Broader Impact and Societal Implications",
            "content": "Our benchmark exposes fundamental gaps in LLM compositional reasoning, underscoring both technical limitations and important societal implications. Positive impacts: Advancing automated verification could enhance software reliability in safety-critical systems (medical devices, autonomous vehicles) and democratize formal methods for resource-constrained teams. Risks: The 92% verification failure rate we document warns against premature deployment in safety-critical contexts. The specification fragility (39.2% of failures) is particularly concerninglocally correct but compositionally invalid specifications could create false confidence in system safety. Accessibility: While our benchmark is open-source, evaluation requires costly API access to frontier models, potentially creating verification disparities. Environmental: Extensive model evaluation across 13 LLMs carries computational costs we acknowledge. By quantifying the compositional reasoning gap, this work aims to guide development of more reliable verification systems while emphasizing the continued necessity of human expertise in safety-critical software development. This gap may naturally hinder automated malicious system development, while stronger verification could support both good and harmful uses."
        },
        {
            "title": "C Introduction to Dafny",
            "content": "Dafny [16], developed at Microsoft Research, is verification-oriented programming language specifically designed to support formal reasoning about software. Unlike conventional languages where correctness is primarily assessed through testing, Dafny integrates an automated program verifier directly into the development workflow, enabling developers to construct code that is mathematically proven to satisfy its specifications. This approach shifts the discovery of defects from the testing phase to the design and implementation phases, thereby improving software reliability. distinctive feature of Dafny is that specifications are treated as first-class citizens. Methods can be annotated with preconditions, postconditions, and logical properties that describe intended behavior. For example: method Example ( : array < int >) returns ( : array < int >) requires . Length > 0 ensures . Length == . Length ensures forall :: 0 <= < . Length == > [ ] >= 0 // Precondition // Postcondition // Property { } // Implementation The Dafny verifier relies on automated theorem proving (via Z3 solver [5]) to ensure that implementations conform to these specifications, providing mathematical certainty about program behavior. Crucially, the ability to reason about the composition of verified components determines whether verification can scale from toy examples to real-world systems. Without compositional reasoning, verification remains confined to small, isolated programs rather than production-level software."
        },
        {
            "title": "D Automated Theorem Proving",
            "content": "A complementary line of work contrasts automated verification frameworks with interactive theorem proving (ITP) systems. Languages such as Dafny and Verus rely on SMT solvers to discharge proof obligations, requiring only lightweight annotations (e.g., invariants, assertions). This design lowers the barrier to entry but is constrained by the solvers limited reasoning scope and opaque failure modes. In contrast, ITPs such as Lean expose every proof step explicitly, enabling iterative refinement and error diagnosis. Recent studies even show that LLMs can generate competition-level mathematical proofs in Lean. However, existing Leanbased benchmarks (e.g., miniCodeProps, FVAPPS) either focus narrowly on proof synthesis or lack human validation. By comparison, Dafny offers more balanced environment for benchmarking LLMs: it combines code, specifications, and automated verification in way that remains close to mainstream programming practice. How Dafny Works and Its Core Strengths. Dafnys approach stems from its verification-aware design. Developers embed formal specifications, such as preconditions, postconditions, and loop invariants, directly within the code [16]. These specifications are not merely comments; they are integral components checked by the built-in verifier. The verifier translates Dafny code and its specifications into an intermediate verification language, Boogie, which then generates proof obligations. These obligations are processed by an SMT solver (e.g., Z3) to prove their validity. If all obligations are proven, the code is confirmed to be correct according to its specifications. If proof fails, Dafny provides precise feedback on the inconsistencies. This methodology supports correctness by construction, helping to reduce common errors like null pointer dereferences or array out-of-bounds access [29]. Once verified, Dafny code can be translated into mainstream languages such as Python for execution [19]. Dafny vs. Python: Fundamental Difference in Approach. To understand Dafnys position, its useful to compare it with widely used language like Python. While both are effective, their fundamental design philosophies and primary objectives differ, as shown in Table 3."
        },
        {
            "title": "Year Introduced\nType System\nFormal Verification\nMain Use\nExecution Model",
            "content": "2010 (Microsoft Research) Static typing, compile-time checks Yes built-in contracts and proofs Verified algorithms, critical systems General-purpose programming Compiled with verification 1991 (Guido van Rossum) Dynamic typing, run-time checks No only basic assert Interpreted (e.g., CPython) Table 3 Key differences between Dafny and Python. In summary, Dafny offers distinct approach to software development by integrating formal verification into the language itself. While Python excels in agile development and broad applicability, Dafny is particularly suited for domains where software correctness and formal guarantees are critical. For more, please refer to the Dafny official website."
        },
        {
            "title": "E Prompt for Synthesis",
            "content": "The prompt templates used for annotating data with Claude 3.5 Sonnet are shown in the following boxes. https://dafny.org/dafny/OnlineTutorial/guide"
        },
        {
            "title": "Prompt for Inital Dafny Code Generation",
            "content": "SYSTEM You are an expert AI assistant that writes Dafny programs. You excel at writing code with formally verified correctness, providing precise preconditions and postconditions, and finding the appropriate loop invariants to ensure all verification conditions are met. TASK Below is the Python code: ```python <python_code> ``` Please translate this Python code into Dafny, ensuring: 1. Method Signatures: Each piece of functionality should be expressed as Dafny method (or set of methods) with well-defined signature. 2. Preconditions: Clearly state any requires clauses for each method (e.g., array length constraints, non-null references, numeric domain restrictions, etc.). 3. Postconditions: State the logical guarantees about the returned values or final state as ensures clauses (e.g., correctness of returned results, absence of side effects, etc.). 4. Verification Details: Include all necessary loop invariants (or other verification hints) so Dafny can prove the postconditions, along with brief explanation. For example: - Explain how you chose your invariants. - Describe how they ensure the correctness of the loop. Return the final Dafny code as self-contained snippet that can be verified by Dafny as-is, with short explanation of how it connects to the original Python functionality. AI ASSISTANT <The LLMs generated Dafny code with specifications here.>"
        },
        {
            "title": "Dynamic Debugging Prompt for Code Generation",
            "content": "SYSTEM You are an expert AI assistant that writes and debugs Dafny programs. You excel at diagnosing and fixing verification errors based on Dafny solver messages, while maintaining correct preconditions, postconditions, and loop invariants. TASK Below is the Python code: ```python <python_code> ``` And the Dafny code you previously provided (which tried to verify): ```dafny <main_spec> ``` ran dafny verify *.dfy and received this error message: ``` <dafny_analysis_result> ``` Can you please fix the main function specification so that it parses successfully? Output the corrected main function specification only, without any other text. AI ASSISTANT <The LLMs generated Dafny code with specifications here.>"
        },
        {
            "title": "F Data Contamination Analysis",
            "content": "To validate the novelty of DAFNYCOMP, we conducted rigorous data contamination analysis against the widely-used MBPP dataset [3], used to assess contamination in Python source data. We confirm that our benchmark source data shows no significant overlap, ensuring model performance reflects genuine reasoning capabilities rather than memorization. Our analysis, focusing solely on code, employs two standard metrics: Exact Match to detect verbatim copies, and n-gram Jaccard Similarity to identify structurally similar code. We performed this analysis under four distinct configurations, the results of which are summarized in Table 4. Across all scenarios, we found zero exact matches. The n-gram Jaccard similarity remains negligible, peaking at mere 0.0078 even under the most aggressive settings. These findings provide strong evidence that DAFNYCOMP is free from training data contamination."
        },
        {
            "title": "G Prompt for Evaluation",
            "content": "The prompt template used for evaluation is shown in the following box. Note that all model outputs are used directly for Dafny verification. 16 Table 4 Summary of Data Contamination Analysis. The table shows results for four testing configurations: (Conservative) with minimal preprocessing; (Default) with moderate preprocessing; (Aggressive) with extensive preprocessing; and (Holistic) for structure-level check. Across all configurations, results show zero exact matches and negligible n-gram similarity when comparing DAFNYCOMP source data against MBPP, confirming the benchmarks integrity. Analysis Configuration N-gram (n)"
        },
        {
            "title": "Exact Overlap Max Jaccard",
            "content": "vs. sanitized-mbpp A: Conservative B: Default C: Aggressive D: Holistic 15 11, 13, 15 9, 11, 13 11, 13, 15 0 0 0 0 0.000078 0.000389 0.007757 0.000234 0 0"
        },
        {
            "title": "Evaluation Prompt for Dafny Specification Generation",
            "content": "SYSTEM You are an expert in Dafny. You will be given tasks dealing with Dafny programs including precise annotations. You should only return code body in all circumstances. No text is allowed. TASK Given Dafny program with function signature, preconditions, postconditions, and code, but with annotations missing. Please return complete Dafny program with the strongest possible annotation (loop invariants, assert statements, etc.) filled back in. Do not explain or output any text. If you have to explain, put all explanations in comments form. There should only be code body in your output. Please use exactly the same function signature, preconditions, and postconditions. Do not ever modify the given lines. Below is the program: ```dafny <dafny_program_with_missing_annotations> ```"
        },
        {
            "title": "AI ASSISTANT",
            "content": "```dafny <The LLMs generated Dafny code with specifications here.> ```"
        },
        {
            "title": "H Real Examples from Benchmark Data",
            "content": "H.1 Challenge 1: Formal Specification Synthesis The following examples demonstrate failures in generating correct formal specifications, particularly preconditions and postconditions. H.1.1 Example 1.1: Postcondition Failure (org_input_id: 2, self_id: 20001) LLM Response: method digitSum ( : int ) returns ( sum : int ) requires >= 0 ensures sum >= var temp := ; sum := 0; while temp > 0 invariant temp >= 0 invariant sum >= 0 sum := sum + ( temp % 10) ; temp := temp / 10; { } { } Dafny Error: ex . dfy (137 ,0) : Error : postcondition could not be proved on this return path ex . dfy (136 ,25) : Related location : this is the postcondition that could not be proved H.1.2 Example 1.2: Missing Precondition (org_input_id: 6, self_id: 60005) LLM Response: method DigitSum ( : int ) returns ( sum : int ) requires >= 0 ensures sum >= 0 var num := ; sum := 0; while num > 0 invariant sum >= 0 invariant num >= 0 decreases num sum := sum + ( num % 10) ; num := num / 10; { } { } Dafny Error: ex . dfy (177 ,36) : Error : precondition for this call could not be proved H.2 Challenge 2: Ensuring Code Correctness These examples show implementation issues where the code logic doesnt satisfy the specifications. 18 H.2.1 Example 2.1: Array Bounds Error (org_input_id: 1, self_id: 10002) LLM Response: method al t b _ 2 1 6 5 ( num : int ) returns ( result : int ) { var neg := num < 0; var := if num < 0 then - num else num ; var cnt := new int [10]; var := 0; while < 10 invariant 0 <= <= 10 invariant cnt . Length == 10 invariant forall :: 0 <= < == > cnt [ ] >= { cnt [ ] := 0; := + 1; } // ... rest of implementation } Dafny Error: ex . dfy (44 ,30) : Error : this loop invariant could not be proved on entry Related message : loop invariant violation H.2.2 Example 2.2: Assertion Failure (org_input_id: 6, self_id: 60001) LLM Response: method DigitSum ( : int ) returns ( sum : int ) requires >= 0 ensures sum >= 0 var num := ; sum := 0; while num > 0 invariant sum >= 0 invariant num >= 0 decreases num sum := sum + ( num % 10) ; num := num / 10; { } { } Dafny Error: ex . dfy (181 ,4) : Error : assertion might not hold assert o2 >= 1; 19 H.3 Challenge 3: Proof Generation and Formal Reasoning Examples demonstrating failures in generating proper loop invariants and formal proofs. H.3.1 Example 3.1: Invariant Not Maintained (org_input_id: 0, self_id: 6) LLM Response: method stringToInt ( : seq < char >) returns ( result : int ) requires forall :: 0 <= < == > 0 <= [ ] <= 9 ensures result >= 0 { var res := 0; var := 0; while < invariant 0 <= <= invariant res >= 0 invariant res == tri ngT oIn tHe lpe ( [.. ]) { res := res * 10 + digitToInt ( [ ]) ; := + 1; } result := res ; } Dafny Error: ex . dfy (25 ,18) : Error : this invariant could not be proved to be maintained by the loop Related message : loop invariant violation H.3.2 Example 3.2: Recursion Termination (org_input_id: 4, self_id: 40003) LLM Response: function gcd_func ( : int , : int ) : int { if == 0 then else gcd_func (b , % ) } method gcd ( : int , : int ) returns ( result : int ) requires > 0 && >= 0 ensures result == gcd_func (a , ) { var := ; var := ; while != 0 invariant > 0 && >= 0 invariant gcd_func (a , ) == gcd_func (x , ) decreases { var temp := % ; := ; := temp ; } result := ; } Dafny Error: ex . dfy (3 ,34) : Error : cannot prove termination ; try supplying decreases clause if == 0 then else gcd_func (b , % ) H.4 Challenge 4: Spec-Code-Proof Alignment These examples show misalignment between specifications and implementation, particularly method vs. function usage. H.4.1 Example 4.1: Method in Expression Context (org_input_id: 0, self_id: 5) LLM Response: method stringToInt ( : seq < char >) returns ( result : int ) requires forall :: 0 <= < == > 0 <= [ ] <= 9 ensures result >= 0 { var res := 0; var := 0; while < invariant 0 <= <= invariant res >= 0 invariant res == if == 0 then 0 else stringToInt ( [.. ]) { res := res * 10 + digitToInt ( [ ]) ; := + 1; } result := res ; } Dafny Error: ex . dfy (26 ,43) : Error : expression is not allowed to invoke method ( stringToInt ) invariant res == if == 0 then 0 else stringToInt ( [.. ]) 21 H.4.2 Example 4.2: Function vs Method Confusion (org_input_id: 16, self_id: 160000) LLM Response: method reverse_7 ( : int ) returns ( ans : int ) { ans := 0; var temp_x := ; while temp_x != invariant ans >= 0 decreases if temp_x >= 0 then temp_x else - temp_x ans := ans * 10 + temp_x % 10; temp_x := temp_x / 10; { } } method isBeautiful ( : int ) returns ( result : bool ) { // Implementation for checking if number is beautiful result := true ; // Simplified } method t u u m _ 7 6 9 ( : int ) returns ( result : int ) { var := + 1; while true invariant >= + 1 invariant forall :: + 1 <= < == > ! isBeautiful ( ) decreases * { } var beautiful := isBeautiful ( ) ; if beautiful { result := ; return ; } := + 1; } Dafny Error: ex . dfy (248 ,46) : Error : expression is not allowed to invoke method ( isBeautiful ) invariant forall :: + 1 <= < == > ! isBeautiful ( ) 22 H.5 Challenge 5: Complexity and Scalability Issues Examples where complex programs or large data structures cause verification failures. H.5.1 Example 5.1: Large Array Allocation (org_input_id: 1, self_id: 10002) LLM Response: method al t b _ 2 1 6 5 ( num : int ) returns ( result : int ) { var neg := num < 0; var := if num < 0 then - num else num ; var cnt := new int [10]; var := 0; while < 10 invariant 0 <= <= 10 invariant cnt . Length == 10 invariant forall :: 0 <= < == > cnt [ ] >= 0 { } cnt [ ] := 0; := + 1; // Count digits var temp := ; while temp > 0 invariant temp >= 0 invariant forall :: 0 <= < 10 == > cnt [ ] >= 0 decreases temp { } var digit := temp % 10; cnt [ digit ] := cnt [ digit ] + 1; temp := temp / 10; // Construct result result := 0; := if neg then 1 else 0; while < invariant 0 <= <= 10 invariant result >= 0 decreases 10 - { } while cnt [ ] > 0 invariant cnt [ ] >= 0 invariant result >= 0 decreases cnt [ ] { result := result * 10 + ; cnt [ ] := cnt [ ] - 1; } := + 1; if neg { result := - result ; } } Dafny Error: ex . dfy (44 ,30) : Error : this loop invariant could not be proved on entry Related message : loop invariant violation invariant ans >= 0 23 H.6 Challenge 7: Avoiding Trivial Solutions Note: This category had limited representation in the dataset. The theoretical examples that would fit this category include: Overly permissive postconditions like ensures true Methods with missing functional specifications Weak preconditions that dont prevent edge cases Specifications that allow degenerate solutions Missing bounds on return values Incomplete coverage of input space Trivial loop invariants that dont capture semantics Weak assertions that dont prevent bugs Generic error handling without specific constraints H.7 Challenge 8: Limited Training in Formal Domains Examples showing basic syntax errors and misuse of Dafny constructs. H.7.1 Example 8.1: Parse Error (org_input_id: 0, self_id: 0) LLM Response: function digitToInt ( : char ) : int requires 0 <= <= 9 ensures 0 <= digitToInt ( ) <= 9 { } ( as int ) - ( 0 as int ) function intToChar ( : int ) : char requires 0 <= <= 9 ensures 0 <= intToChar ( ) <= 9 { } (( 0 as int ) + ) as char method stringToInt ( : string ) returns ( result : int ) requires forall :: 0 <= < == > 0 <= [ ] <= 9 ensures result >= { } var res := 0; var := 0; while < invariant 0 <= <= invariant res >= 0 { res := res * 10 + digitToInt ( [ ]) ; := + 1; } result := res ; Dafny Error: ex . dfy (16 ,35) : Error : type string is not expected in Dafny ; use seq < char > instead method stringToInt ( : string ) returns ( result : int ) 2 parse errors detected in ex . dfy 24 H.7.2 Example 8.2: Syntax Error (org_input_id: 0, self_id: 2) LLM Response: function digitToInt ( : char ) : int requires 0 <= <= 9 ensures 0 <= digitToInt ( ) <= 9 { } ( as int ) - ( 0 as int ) function intToChar ( : int ) : char requires 0 <= <= 9 ensures 0 <= intToChar ( ) <= 9 { } (( 0 as int ) + ) as char method stringToInt ( : seq < char >) returns ( result : int ) requires forall :: 0 <= < == > 0 <= [ ] <= 9 ensures result >= 0 { } var res := 0; var := 0; while < invariant 0 <= <= invariant res >= 0 { res := res * 10 + digitToInt ( [ ]) ; := + 1; } result := res ; Dafny Error: ex . dfy (29 ,5) : Error : \" closeparen \" expected result := res ; 2 parse errors detected in ex . dfy"
        }
    ],
    "affiliations": [
        "HKUST",
        "NTU",
        "Shanghai AI Lab",
        "UoM"
    ]
}