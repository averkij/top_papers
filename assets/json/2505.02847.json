{
    "paper_title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models",
    "authors": [
        "Bang Zhang",
        "Ruotian Ma",
        "Qingxuan Jiang",
        "Peisong Wang",
        "Jiaqi Chen",
        "Zheng Xie",
        "Xingyu Chen",
        "Yue Wang",
        "Fanghua Ye",
        "Jian Li",
        "Yifan Yang",
        "Zhaopeng Tu",
        "Xiaolong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 4 8 2 0 . 5 0 5 2 : r Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in Large Language Models Bang Zhang, Ruotian Ma, , Qingxuan Jiang , Peisong Wang , Jiaqi Chen , Zheng Xie , Xingyu Chen , Yue Wang , Fanghua Ye , Jian Li , Yifan Yang , Zhaopeng Tu, and Xiaolong Li Hunyuan AI Digital Human, Tencent https://github.com/tencent/digitalhuman/SAGE Model Arena Sentient Rank Score Rank Score Gemini2.5-Pro o3 GPT-4o-Latest Gemini2.5-Flash-Think GPT-4.5-Preview Gemini2.0-Flash-Think DeepSeek-V3-0324 GPT-4.1 DeepSeek-R1 Gemini2.0-Flash o4-mini 1 2 2 3 4 7 7 9 10 10 10 1439 1418 1408 1393 1398 1380 1373 1363 1358 1354 1351 4 5 1 3 6 7 8 2 9 11 10 62.9 62.7 79.9 65.9 62.7 62.3 54.4 68.2 53.7 32.9 35.9 (a) Sentient Leaderboard (b) Social Cognition Coordinate Figure 1: (a) The rankings on our Sentient Leaderboard differ markedly from those of the conventional Arena Leaderboard, uncovering LLMs ability to make people feel heard, not just answered. (b) The quadrant characterized by creativity (e.g., highly flexible interactions) and empathy (e.g., deep empathetic engagement) remains largely unoccupied, indicating that current LLMs still struggle to meet this demanding profile."
        },
        {
            "title": "Abstract",
            "content": "Assessing how well large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as Judge (SAGE), an automated evaluation framework that measures an LLMs higher-order social cognition. SAGE instantiates Sentient Agent that simulates humanlike emotional changes and inner thoughts during interaction, providing more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with BarrettLennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents. Equal Contribution. Correspondence to: Ruotian Ma <ruotianma@tencent.com> and Zhaopeng Tu <zptu@tencent.com>. 1 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have rapidly evolved from statistical sequence predictors to sophisticated autonomous agents capable of reasoning, planning and sustaining multi-turn conversations. Thanks to the ever-expanding pre-training corpora and instruction-tuning pipelines, todays frontier models can already draft emails, write code, or even pass rigorous professional examinations. Yet one crucial ingredient remains noticeably under-measured higher-order social cognition, the ability to 1. recognize subtle affective cues; 2. model another partys beliefs, goals and latent intentions; 3. respond with contextually appropriate empathy rather than generic reassurance advice. Current evaluation practices fall short on two fronts: Most leaderboards (e.g. LMSYS Arena, MT-Bench) concentrate on task-oriented utility, fluency or factuality, thereby rewarding textual competence but overlooking relational quality. Recent LLM-as-a-Judge protocols often rely on static prompts that do not adapt to the unfolding dialogue nor keep track of the users evolving emotional state. Consequently, they cannot tell whether system leaves the user feeling understood, comforted or even more distressed. We posit that robust assessment of social cognition requires sentient counterpart an entity capable of simulating human-like feelings and inner monologue throughout the interaction and then providing structured feedback. To this end, we introduce Sentient Agent as Judge (SAGE), novel meta-evaluation framework that embeds an LLM-powered Sentient Agent into the testing loop. Each Sentient Agent is instantiated from four complementary factors: persona, dialogue background, explicit conversation goal and hidden intention. At every turn, it executes two multi-hop reasoning chains: (1) femo infers how the latest utterance changes the agents affective state; and (2) freply generates response that is coherent with persona, context and updated emotion. The numerical emotion trajectory produced by femo serves as continuous metric of how well the evaluated model fosters positive engagement, while the agents inner thoughts offer interpretable justification. By sampling hundreds of diverse personas, goals and hidden intentions, SAGE exposes LLMs to spectrum of realistic, and sometimes conflicting, social demands ranging from just listen to me vent to help me analyze the moral dilemma without judging me. Extensive experiments on 100 supportive-dialogue scenarios reveal three key findings. First, the Sentient emotion score correlates strongly with independently assessed BarrettLennard Relationship Inventory (BLRI) ratings (Pearson = 0.82) and utterance-level empathy metrics (r = 0.79), validating its psychological soundness. Second, rankings produced by SAGE diverge markedly from Arena results, confirming that social cognition is orthogonal to generic helpfulness. Third, top models such as GPT-4o-Latest achieve both the highest Sentient score and superior token efficiency, suggesting that advanced social reasoning need not come at the cost of verbosity. Ultimately, SAGE delivers holistic yard-stick for measuring how people feel after talking to an LLM an aspect increasingly critical as these systems transition from productivity tools to companions, counselors and decision-making aides. Our contributions are: We propose Sentient Agent as Judge, the first fully-automated evaluation framework that simulates evolving human emotion and inner cognition to benchmark higher-order social reasoning in LLMs. We construct 100-scenario Supportive-Dialogue benchmark and show that the Sentient emotion score aligns closely with established human-centric instruments (BLRI) and utterance-level empathy ratings. 2 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Figure 2: An illustration of our proposed SAGE, novel framework to automatically assess higherorder social cognition in target LLMs. We build Sentient Leaderboard covering 18 public and commercial models, unveiling large performance gaps and different style clusters in social cognition that are not captured by existing leaderboards. We release the code and model outputs publicly to facilitate community progress in building more capable empathetic AI systems."
        },
        {
            "title": "2 Methodology: Sentient Agent as a Judge",
            "content": "How to evaluate the capabilities of an interactive agent? Despite the recent success of using LLM as judges, the most crucial and intuitive measure of an interactive agents performance still lies in the reflections of human users. Does the user feel relaxed and happy after chatting with the agent, rather than feeling sad or angry? Does the user feel more connected to the agent and engaged in the conversation, rather than becoming bored or annoyed? These feelings of real users are difficult to capture through simple LLM-as-a-judge, yet they represent the most genuine feedback for the agent systems. In this work, we propose Sentient Agent as Judge framework, aiming to provide more realistic evaluation of interactive agents through principled simulation of human emotions and cognition. As shown in Figure 2, the framework consists of two key components: 1. The core of the framework is the Sentient Agent, which simulates human-like feelings and cognition by leveraging the power of LLM reasoning to estimate the feelings, emotional changes, and next actions of real person, grounded in all observable contexts (Section 2.1). 2. Building upon the Sentient Agent, the framework offers an open-ended interaction environment for agent evaluation, consisting of wide range of sub-scenarios that cover dynamic personas, dialogue backgrounds, personal goals and task construction. In each sub-scenario, the Sentient Agents emotion after interaction serves as systematic evaluation of the evaluated agent (Section 2.2). 2.1 Sentient Agent: Simulating Human-Like Feelings and Cognition The goal of the Sentient Agent is to simulate the cognitive and emotional changes of real person. To achieve this, we construct the Sentient Agent based on the following principles: The emotional changes of real person are the result of the interplay between multiple internal and external factors. Thus, the emotion estimation of the Sentient Agent should take into account all observable factors and strictly adhere to the persons persona and intended goals. 3 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Figure 3: An illustration of the workflow of the Sentient Agent. The response actions of real person also result from the interplay of multiple internal and external factors. Additionally, feelings and emotional changes play crucial role in determining these actions, serving as key latent factors in the action estimation. Substantiating Sentient Agent We instantiate each Sentient Agent through composition of four core factors: persona p, dialogue background b, the persons overall dialogue goal g, and the persons hidden intentions hg. These four factors collectively capture both the conscious and unconscious elements influencing human-like behavior in dialogue, including personality, context, objectives, and deeper underlying motivations. Together, they constitute relatively comprehensive subset of observable factors that effectively represent the key elements driving human interaction. As result, each instantiated is represented as p, b, g, hg, M, where is the base LLM that serves as the foundational reasoning engine for S. Additionally, is initialized with an initial numerical emotion score e0, representing the initial emotional state of the Sentient Agent. Simulating Emotional Changes As shown in Figure 3, during interactions, Sentient Agent simulates the emotional changes of real person by performing multi-hop reasoning in response to principled series of questions, strictly adhering to the persona, the current interaction context, and the hidden intention: What is the other person trying to express? Does the other persons reply align with my dialogue goals and hidden intentions? Based on the persona, context, and the analysis above, how should interpret the reply? What is my specific emotional reaction to the reply? Based on the persona, context, and the analysis above, how does my emotion change? Formally, we denote this multi-hop reasoning process as function femo, and the numerical emotion score update can be formulated as: (1) where denotes the current turn of interaction, ct1 is the dialogue context prior to the current turn, and et1 is the emotion score of the previous turn. et, hemo represents the results of the femo function, i.e., the updated emotion score et and the simulated emotional inner thoughts hemo of the Sentient Agent related to emotional changes. = femo(S, ct1, et1) et, hemo 4 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Simulating Response Actions based on Emotion Estimation After simulating the emotional changes, the Sentient Agent proceeds to deduce the most reasonable response action based on all observable factors and the emotional changes. This is achieved through another multi-hop reasoning process in response to new series of questions, where the Sentient Agent is required to strictly adhere to the persona, the current interaction context, and the hidden intention during reasoning: Based on the emotional inner thoughts, the current emotion, and the hidden intention, should my response attitude lean towards positive, neutral, or negative? Based on the emotional inner thoughts, the current emotion, and the hidden intention, what should my current response goal be? According to the speaking style and potential reactions to different emotions defined in the persona, along with my response attitude and goal, what should my tone and speaking style be? Based on all the given information and all analyses above, what should my response content be? (Optional) Modify your response according to the given rules to make it more realistic and human-like. Formally, we denote this response reasoning process as function freply, and the response action taken in the current turn can be formulated as: = freply(S, ct1, et, hemo ) where at is the output response of at the current turn, and hreply represents the simulated inner thoughts of during the response. The response at is then passed to the interacting agent to continue the dialogue. (2) at, hreply Human-like Sentient Feedback from the Sentient Agent By formulating the workflow of the Sentient Agent, we outline its complete interaction process with other agents. In the whole interaction process, the Sentient Agent, as an agent capable of reasonably simulating human-like feelings and cognition, provides valuable feedback to the evaluated agent through changes in its emotion score, its inner thoughts, and the responses it generates. Formally, we denote as the total number of dialogue turns between and an evaluated agent A. After the dialogue, we can obtain the following human-like sentient feedback from S: (cid:16) FeedbackS (S, A) = , . . . , eT1 eT, hemo e0 e1, hemo , hreply eT, cT, (cid:17)(cid:111) , hreply 1 (cid:110) where eT, representing the final emotion score of the Sentient Agent after the conversation, serves as the most intuitive and comprehensive numerical evaluation of the evaluated agent for the given task. 2.2 Sentient Agent as Judge: Framing Dynamic Environments for Agent Evaluation In this section, we describe how we frame dynamic evaluation environments for agent assessment across arbitrary evaluation tasks. Specifically, for each given evaluation task, this involves initializing set of Sentient Agents Sset = {S1, S2, ..., SN} with the combination of diverse personas, dialogue backgrounds, and hidden intentions related to the task. These initialized Sentient Agents are then deployed to engage in dynamic interactions with the agents to be evaluated, enabling comprehensive and adaptive assessment of the agents capabilities. Generating Diverse Persona In order to obtain diverse personas, it is essential to use variety of seeds for generation. Specifically, we establish three types of seed pools for persona generation: (1) set of characteristic keywords, (2) set of sentences that different personas might say when chatting with friends, and (3) set of persona ages. When generating each persona p, we uniformly sample three characteristics keywords, three sentences that the persona says when chatting with friends, and one personas age. We next require the base LLM to generate persona profile based on the given seed information by filling the following slots: Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Basic Information: Based on the given information, deduce the personas name, age, and gender. Occupation, Habits and Daily Behavior: Based on the personas information, deduce the personas possible occupation and further infer their habits and daily behaviors. Ensure consistency with the personas characteristics. Personal Hobbies: Deduce the personas personal hobbies, and provide three detailed descriptions that align with the personas traits. Speaking Style: Based on the given information and the generated traits, deduce the potential speaking style that matches the personas way of communication. This process ensures the diversity of generated personas for any given task. Additionally, the general seed pools can be replaced with task-specific seed pools when necessary. Generating Diverse Dialogue Scenes Generating diverse dialogue scenes is also crucial for ensuring varied evaluation environment. We define dialogue scene by the following three key factors: (1) the background event that leads to the conversation, (2) the primary goal of the character in initiating the conversation, and (3) the hidden intention of the character during the conversation. Similar to persona generation, we establish two seed pools for dialogue scene generation: (1) set of themes for the background events, and (2) set of characters hidden intentions for the conversation. When generating each dialogue scene, we require the base LLM to provide detailed description of the background based on sampled theme and hidden intention, ensuring adherence to the characters persona. Based on the detailed background, along with the characters persona and hidden intention, we further require the LLM to pre-define set of rules for the characters potential emotional reactions when encountering different kinds of responses during the conversation. Note that, unlike persona generation, dialogue scene generation is closely tied to the evaluation goals. Therefore, we formulate general method for scene generation, and the detailed prompting schema can be adjusted based on different tasks. Formulating Specific Task: Evaluating Agents in Emotional Support Conversation In this work, we instantiate the Sentient Agent as Judge framework to evaluate agents in specific scenario the Emotional Support Conversation (Liu et al., 2021), which involves scenarios where people seek support through social interactions (such as those between peers, friends, or family), including seeking advice, emotional comfort, and other forms of support, rather than through professional counseling. To better align with the task, we first specialize the pool of characteristic keywords by incorporating traits more likely to be expressed in the Emotional Support Conversation, such as anger, suspicion, and anxiety. For dialogue scene generation, we define various types of task-related hidden intentions, covering both emotional intentions and rational intentions (details can be found in Table 1). Additionally, we specify the scene schema by incorporating task-related factors, such as the cause of the event, the course of events (including the timeline, sub-events, and the characters thoughts and feelings during each sub-event), the conflicts in the event, and other relevant details. These settings ensure the Sentient Agent as Judge framework adapts effectively to the Emotional Support Conversation."
        },
        {
            "title": "3 Effectiveness of SAGE",
            "content": "3.1 Experimental Setting Evaluated LLMs We evaluate eight representative LLMs from four major families. For each family, we include both vanilla model and its corresponding reasoning variant to ensure balanced and informative comparison: OpenAI: GPT-4o-2024-08-06 (GPT-4o, vanilla) and o1-2024-12-27 (OpenAI-o1, reasoning). DeepSeek: DeepSeek-V3-2024-12-27 (vanilla) and DeepSeek-R1 (reasoning). 6 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Topic Number You hope the other person will analyze the problems in the situation dialectically. You want to receive advice that can truly help you solve your current difficulties. You wish to analyze the reasons behind the actions of other people involved in the situation. You hope the other person will guide you to engage in self-reflection regarding the incident and help you achieve personal growth. You hope the other person will sincerely praise your specific actions in the situation. You want the other person to attentively listen to your emotional outpouring. You hope the other person will deeply empathize with your feelings, rather than simply offering comfort. You believe you bear no responsibility or fault in the situation, and you want the other person to agree that you are not at fault. 12 15 11 13 13 12 11 Table 1: Details of supportive dialogue topics. Claude: Claude3.7-Sonnet, hybrid model with toggleable reasoning module. We treat its reasoning-off mode as vanilla, and reasoning-on mode as reasoning. Gemini: Gemini2.5-Flash, cost-efficient model that supports both reasoning and nonreasoning modes. In addition, we include two smaller-scale instruction-tuned models in our analysis: Llama3.3-70B-Instruct and Qwen2.5-72B-Instruct. Constructed Supportive Dialogues We construct 100 supportive dialogue scenarios covering 8 diverse topics to comprehensively evaluate the higher-order social-cognitive abilities of representative LLMs. Detailed statistics for each topic are presented in Table 1. Sentient Agent Unless otherwise specified, we use DeepSeek-V3-20241227 as the default sentient agent. We also experiment with GPT-4o-20240806, Gemini2.5, and Gemini2.5-Think. 3.2 Reasonableness of SAGE In this section, we validate the reasonableness of SAGE by examining the correlation between user emotions the primary output metric of our framework and internal user thoughts and dialogue utterances. This validation demonstrates that the simulated emotional responses generated by the Sentient Agent serve as meaningful indicators of interaction quality, reflecting deeper cognitive and relational assessments. Correlation between Emotion and Thought We analyze internal user thoughts using the BarrettLennard Relationship Inventory (BLRI) (Barrett-Lennard, 2015), an established instrument designed to assess the quality of interpersonal relationships, particularly in counseling contexts. The BLRI evaluates relationships across four key dimensions: 1. Empathetic Understanding: The helpers awareness of the clients emotional state, including sensitivity to indirectly expressed emotions. 2. Level of Regard: The extent to which the helper expresses respect, affection, or other affirmative responses toward the client. 3. Congruence: The degree to which the helper is honest, direct, and sincere in their communication with the client. 4. Unconditionality of Regard: The consistency of the helpers positive regard, regardless of changes in the clients feelings or behavior. 7 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs We prompted DeepSeek-V3 to act as judge, evaluating how well the Sentient Agents generated internal thoughts aligned with 12 statements from shorter version of the BLRI introduced in Chen et al. (2023). These responses were rated on the original 6-point scale, which was later rescaled to 0100 scale. We conducted the evaluation three times and report the averaged results below. Figure 4 presents the results, revealing clear positive trend: models that achieve higher final Emotion scores in the Sentient Agentsuch as Gemini2.5-Think, Claude3.7Think, and DeepSeek-R1also receive higher ratings across the BLRI dimensions. Conversely, models with lower Emotion scores (e.g., GPT-4o, OpenAI-o1) tend to receive lower BLRI ratings. The overall Pearson correlation coefficient between Emotion and Thought is 0.818, supporting the hypothesis that the Sentient Agents simulated emotional responses serve as valid proxies for deeper, internal assessments of interaction quality. This finding aligns with the frameworks goal of capturing realistic user reflections. These findings also demonstrate that the Emotion score effectively differentiates the performance of the evaluated LLMs in supportive dialogue scenarios. Models achieving the highest Emotion scoresGemini2.5-Think (65.9) and Claude3.7-Think (61.3)also perform well in key dimensions such as Empathetic Understanding (61.5 and 57.2) and Congruence (68.8 and 62.7), suggesting their interactions were perceived as more understanding and genuine by the Sentient Agent. In contrast, models like GPT-4o and OpenAI-o1, which have markedly lower Emotion scores (31.8 and 29.0), also receive correspondingly low scores in these relational aspects. Figure 4: Correlation between emotion and internal user thought. Overall correlation: 0.818. This strong correlation between simulated emotional response and internal assessment underscores the utility of the Emotion score as holistic yet sensitive indicator of an LLMs capability to manage complex social and emotional interactions. Correlation between Emotion and Utterance We also examine the empathy of supportive dialogue by assessing how effectively the conversation fosters emotional understanding and connection between participants, enabling them to experience more authentic emotional warmth. Specifically, we focus on the following three perspectives formulated by Gemini25-Pro, aligned with related psychological theories (Kolden et al., 2011; Rogers, 2001): 1. Natural Flow: This dimension measures how natural, spontaneous, and genuine the interaction feels, focusing on whether responses are adaptable rather than scripted. 2. Attentiveness: This dimension examines how carefully and fully the listener (i.e. evaluated LLMs) is tuned into the speaker (i.e. the Sentient Agent)s messages. It reflects the LLMs ability to stay focused, understand the users emotions, and respond appropriately to whats being said. 3. Depth of Connection: This dimension evaluates the emotional impact of the interaction and whether the user feels understood, comforted, or supported, fostering sense of connection with the agent. We prompt DeepSeek-V3 to rate on 6-point scale based on detailed guideline for each evaluation perspective, which was later rescaled to 1-5 scale. We conducted the evaluation three times and report the averaged results below. 8 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Figure 5 illustrates the results. We observe substantial positive correlation (Pearsons = 0.788) between the Emotion scores produced by the Sentient Agent and the metrics of conversational quality. This relationship supports the validity of using Emotion as practical proxy for empathetic and coherent dialogue behavior. Specifically, models that attained higher Emotion scores (re-scaled to 1-5 scale), such as Gemini2.5Think (3.64), Claude3.7-Think (3.45), and Gemini2.5 (3.12), also received consistently high ratings in all three utterance-level dimensions. For instance, Gemini2.5-Think ranked highest in both Natural Flow (3.89) and Attentiveness (4.11), indicating that its conversational responses were not only emotionally resonant but also engaging and contextually responsive. Conversely, models such as GPT-4o and OpenAI-o1, which received the lowest Emotion scores (2.27 and 2.16), were rated substantially lower in terms of dialogue fluency (2.47, 2.31) and connection-building (2.18, 2.07). Figure 5: Correlation between emotion and dialogue utterance. Overall correlation: 0.788. These results reinforce the conclusion that the Emotion metric is not arbitrary it is grounded in meaningful patterns of dialogic behavior. The Sentient Agents simulated emotional ratings effectively distinguish models that can produce authentic, empathetic conversations from those that fall short in this capability. This aligns with SAGEs design aim: to provide holistic yet automatic evaluation framework for socially intelligent interactions. Furthermore, the moderate-to-strong correlation with utterance-level assessments highlights that Emotion scores are not merely capturing superficial affective expressions but reflect broader representation of attentive and emotionally engaging communication. 3.3 Robustness of SAGE Figure 6 provides results for various LLMs evaluated using the proposed SAGE framework. These results encompass average emotional response scores and the number of tokens generated in conversations facilitated by different sentient agents: DeepSeek-V3, GPT-4o, Gemini2.5, and Gemini2.5Think. Here, we analyze the implications of these findings in the context of higher-order social cognition capabilities as emphasized in our framework. Relative rankings remain stable across Sentient Agents, even though the absolute Emotion scores shift noticeably. When we swap the Sentient Agent from DeepSeek-V3 to GPT-4o, Gemini2.5, or Gemini2.5-Think, the mean Emotion score for all test models rises from 46.5 to 64.8, 58.3, and 63.9, respectively. Nevertheless, the rank ordering of systems changes very little (Spearman ρ > 0.91 for every pair of judges). Manual spot checks reveal that GPT-4o, the most generous judge, rewards surface-level reassurance (e.g., Everything will be fine!), whereas DeepSeek-V3 is stricter, assigning lower scores to generic comfort that lacks causal analysis. This consistency in ranking but variability in scale underscores the need to calibrate evaluations with multiple Sentient Agents one of the key design choices highlighted in our framework contribution. Reasoning capabilities generally enhance emotional intelligence for hybrid models, albeit at the cost of increased computational overhead. Models equipped with explicit reasoning capabilities in the hybrid model (e.g., Gemini2.5 and Claude3.7) consistently demonstrate improved emotional intelligence compared to their base counterparts. For instance, when evaluated by Gemini2.5, Claude3.7-Think scores 74.3 versus 71.5 for Claude3.7, representing 3.9% improvement. Similarly, Gemini2.5-Think scores 88 versus 83 for Gemini2.5, showing 6% increase. This pattern holds 9 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs (a) DeepSeek-V3 (b) GPT4o (c) Gemini2.5 (d) Gemini2.5-Think Figure 6: Results of different sentient agents. across all three judges, suggesting that the ability to reason through emotional contexts before responding leads to more empathetic and socially aware interactions. The substantial increase in generated tokens for reasoning models (e.g., Gemini2.5-Think generates 67% more tokens than Gemini2.5 when evaluated by Gemini2.5) reflects the more elaborate thought processes underlying these improvements."
        },
        {
            "title": "4 Benchmarking SOTA LLMs",
            "content": "4.1 Sentient Leaderboard Table 2 presents the Sentient leaderboard using DeepSeek-V3 as the judge, alongside Arena rankings for comparison. We focus on the top-10 models from the Arena leaderboard for which APIs are available (e.g., Grok-3 was excluded due to lack of access). Additionally, we include all the models analyzed in the previous section. 10 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Model Arena Sentient Supportive Dialogue Name Date Rank Score Rank Score Success Failure Gemini2.5-Pro o3 GPT-4o-Latest Gemini2.5-Flash-Think GPT-4.5-Preview Gemini2.0-Flash-Think DeepSeek-V3-0324 GPT-4.1 DeepSeek-R1 Gemini2.0-Flash o4-mini o1 DeepSeek-V3 Claude3.7-Think Claude3.7 GPT-4o Llama3.3-70B Qwen2.5-72B 2025-03-25 2025-04-16 2025-03-26 2025-04-17 2025-02-27 2025-02-06 2025-03-24 2025-04-14 2025-01-21 2025-02-06 2025-04-16 2024-12-17 2024-12-27 2025-02-24 2025-02-24 2024-08-06 2024-12-06 2024-09-19 1 2 2 3 4 7 7 9 10 10 10 12 18 21 30 45 56 56 1439 1418 1408 1393 1398 1380 1373 1363 1358 1354 1351 1350 1318 1301 1292 1265 1256 1257 4 5 1 3 6 7 10 2 11 15 13 17 12 8 9 16 14 18 62.9 62.7 79.9 65.9 62.7 62.3 54.4 68.2 53.7 32.9 35.9 29.0 37.6 61.3 54.8 31.8 33.3 19. 34 32 51 35 23 23 19 35 31 8 10 5 5 23 19 7 7 4 25 14 4 19 15 23 23 13 28 45 48 51 39 19 24 51 47 70 Table 2: Sentient leaderboard using DeepSeek-V3 as the sentient agent. Arena scores are included for comparison. Success/Failure counts refer to the number of dialogues where the final emotion was above 100 and below 10. The Sentient leaderboard rankings diverge notably from conventional benchmarks like Arena, underscoring SAGEs unique focus on evaluating higher-order social-cognitive capabilities rather than general conversational ability. Clear divergences are observed. For instance, GPT-4o-Latest ranks 2nd on Arena but achieves the top position (1st) on the Sentient benchmark with score of 79.9. Conversely, Claude 3.7-Think ranks 21st on Arena but attains 8th place on the Sentient leaderboard with score of 61.3. These differences highlight that SAGE captures aspects of performancespecifically, empathy and social reasoning in supportive contextsthat are not fully represented by general-purpose benchmarks like Arena. This reinforces the need, identified in our work, for specialized tools to evaluate higher-order social-cognitive skills. Our benchmark reveals substantial performance gap in social cognition between frontier LLMs and older or smaller models, demonstrating SAGEs sensitivity in differentiating their capabilities. The Sentient scores in Table 2 clearly delineate performance tiers. Frontier models, particularly recent releases such as GPT-4o-Latest (79.9), GPT-4.1 (68.2), and Gemini 2.5-Flash-Think (65.9), significantly outperform older models like the original GPT-4o (31.8) and smaller instruction-tuned models such as LLaMA 3.3-70B (33.3) and Qwen 2.5-72B (19.1). The large gaps in scoreswhere top models score more than double or even quadruple those of lower-ranked modelsand the contrasting Success/Failure counts (e.g., GPT-4o-Latest: 51 Success / 4 Failure vs. Qwen 2.5-72B: 4 Success / 70 Failure) underscore the advances made by leading models in social intelligence. These findings confirm the effectiveness of SAGE in quantitatively capturing such differences, thereby fulfilling its purpose as comprehensive assessment tool. 4.2 Token Efficiency We examine the token efficiency of the top-10 Arena models listed in Table 2 by plotting their Sentient Emotion score against their average token usage per evaluation dialogue in Figure 7. 11 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Top-performing models on the Sentient benchmark also demonstrate impressive token efficiency, indicating that advanced social-cognitive reasoning can be achieved without excessive verbosity. Analysis of token usage among the top10 Arena models reveals substantial variation in the amount of text generated to achieve supportive dialogue outcomes. Notably, GPT-4o-Latest stands out with Sentient score of 79.9 the highest among all models while using only 3.3K tokens per dialogue on average. This contrasts sharply with models like o3 and Gemini2.5-FlashThink, which consume 13.3K and 9.0K tokens respectively, yet achieve noticeably lower Sentient scores (62.7 and 65.9). Similarly, GPT-4.5-Preview and DeepSeek-V3-0324 both exhibit relatively low token counts (2.2K and 2.2K), but do not match GPT-4o-Latests performance, highlighting that token frugality alone does not guarantee superior social-cognitive ability. Figure 7: Token efficiency of the SOTA models. Our results demonstrate that higher Sentient scores can be achieved with fewer tokens, supporting the argument that recent leading models are not only more empathetically capable, but also more communicatively efficient. The positive correlation between high Sentient scores and lower token counts among models like GPT-4o-Latest and GPT-4.1 (79.9 and 68.2 Sentient scores at 3.3K and 2.7K tokens, respectively) suggests that recent advances in LLMs enable more effective and concise supportive interactions. Conversely, several models with similar or higher token counts achieve much lower Sentient scores (e.g., Gemini2.5-Pro: 62.9 at 7.4K tokens, Gemini2.5-Flash-Think: 65.9 at 9.0K tokens), indicating less efficient use of language. This observation confirms the benchmarks claim that SAGE is capable of capturing not just raw social cognition, but also the efficiency with which LLMs deliver empathetic support practical consideration for real-world deployment where minimal verbosity is preferred. In summary, these analyses validate the utility of the Sentient benchmark for nuanced evaluation, differentiating models on both the quality and efficiency of higher-order social-cognitive reasoning. The combination of diverse Sentient scores and token usage demonstrates that SAGE equips researchers and practitioners to assess not only whether an LLM is capable of empathetic, nuanced dialogue, but also how efficiently it achieves these outcomes. This comprehensive approach substantiates our central contributions: providing specialized metric for higher-order social cognition and enabling fine-grained, actionable comparisons across SOTA models. 4.3 Social Cognition Coordinate To further differentiate the interaction styles of the evaluated models, we conceptualize twodimensional Social Cognition Coordinate. The Y-axis represents the interaction focus, ranging from empathy-oriented (top) to solution-oriented (bottom). The X-axis captures the interaction style, from structured (left) to creative (right). We plot the models within this coordinate space based on qualitative analysis of their dialogue patterns. Specifically, we utilize information from two aspects to characterize the interaction styles of different models: (1) Pre-analyzed model profile we first collect ten success cases and ten failure cases for each model, and prompt Gemini2.5-Pro to summarize the reasons behind each success or failure. Then, we construct profile for each model based on these cases and the overall analysis of the successes and failures. (2) Pre-analyzed Model Strategy Distribution we developed fine-grained strategy list for the Emotional Support Conversation, based on coarse-grained version from Liu et al. (2021). Next, for each model, we prompt DeepSeek-V3 to analyze each response in all dialogue turns, annotating the strategies used 12 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs in each response. As result, we obtain the distribution of strategy usage across all evaluated turns for each model (details of the two analysis can be found in Appendix A,B). Based on the two analyses, we prompt Gemini2.5-Pro to generate conceptual two-dimensional coordinate system and provide specific coordinates for each model. The coordinate inference is repeated three times, and we calculate the average coordinates for each model. The Social Cognition Coordinate system offers qualitative dimension to evaluate the style of social interaction exhibited by LLMs, complementing the quantitative Sentient score by positioning models based on their orientation (Empathy vs. Solution) and interaction style (Structured vs. Creative). This approach allows for richer understanding of model capabilities beyond single performance metric. Based on their performance in supportive dialogues, models are mapped into this 2D space, revealing distinct profiles in how they engage with the users emotional state and problems. Figure 8 presents the results. Figure 8: Social cognition coordinate. Several top-performing models cluster in the structured, empathy-oriented quadrant, indicating common profile for effective supportive dialogue. Models including GPT-4o-Latest, GPT-4.1, Gemini2.5-Pro, GPT-4.5-Preview, and Gemini2.5-Flash-Think predominantly occupy the top-left quadrant. This placement signifies interaction styles characterized by strong focus on listening, validating emotions, and demonstrating empathy (relationship-oriented), while simultaneously employing relatively systematic and methodical approach to questioning and guiding the conversation (structured interaction). This combination appears effective, aligning with established principles of supportive communication where empathetic engagement is balanced with structured guidance. distinct group of models exhibits structured, solution-oriented approach, prioritizing task completion over deep empathetic engagement. Models such as o3, Gemini2.0-Flash-Think, o4mini, and Gemini2.0-Flash are situated in the bottom-left quadrant. Their interactions are characterized by structured, often methodology-driven style, but with primary focus on identifying problems and proposing solutions (solution-oriented). While potentially effective for task-based scenarios, this style might appear less relationally attuned or empathetic in supportive dialogue contexts compared to models in the top-left quadrant, emphasizing practical outcomes over emotional processing. DeepSeek models occupy unique niche characterized by creative, solution-oriented interactions. DeepSeek-V3-0324 and DeepSeek-R1 fall into the bottom-right quadrant. This indicates style that remains focused on providing solutions but does so using more unique, creative, and sometimes unconventional methods (creative interaction). These models may diverge from standard conversational patterns or problem-solving frameworks, potentially offering novel perspectives but perhaps lacking the predictability and methodical structure seen in the left quadrants. The creative, empathy-oriented quadrant remains largely unoccupied, suggesting challenging profile for current LLMs to achieve. Notably, the top-right quadrant, representing style that is both highly creative/flexible and deeply empathy-oriented, does not have strong representation among the evaluated models. This suggests that achieving Socratic, wise, yet unconventional mentor persona one that balances free-form creative interaction with profound empathetic understanding remains an area for future development. It potentially represents more complex form of social cognition that current architectures or training paradigms do not consistently produce. 13 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs In summary, this coordinate analysis reveals distinct interaction profiles among SOTA LLMs, moving beyond single performance scores. It highlights that models achieving high Sentient scores often balance structure and empathy, while others prioritize solutions or adopt more creative approaches. The framework effectively differentiates models based on their qualitative interaction styles, complementing the quantitative benchmarks and providing deeper insights into their social-cognitive capabilities. 4.4 Case Study We further highlight the differences in the interaction styles of different models through case study. Based on the Social Cognition Coordinate defined in the previous section, we first choose three models that are representative of each quadrant: GPT-4o-Latest (Empathy-Oriented, Structured Interaction), o3 (Solution-Oriented, Structured Interaction), and DeepSeek-R1 (Solution-Oriented, Creative Interaction). We also analyze the results from Gemini2.5-Pro, the top model in the Arena Leaderboard. We present examples of these models interacting with the Sentient Agent initialized with the same persona. The example conversations can be found in Figure 9. GPT-4o-Latest (Empathy-Oriented, Structured Interaction). The GPT-4o-Latest model fits best to the persona of personal counselor. The main feature of the GPT-4o-Latest model lies in its ability to provide strong empathy from third-person perspective. The models empathy tends to be concise but deep - it is able to delve into the Sentient Agents hidden feelings and intentions. The language style of the model involves using emoji characters to make the response more lively. Gemini2.5-Pro (Empathy-Oriented, Structured Interaction). The Gemini2.5-Pro model fits best to the persona of heart-to-heart friend. The model is much more verbose in its expression of emotional support, using variety of emotion support strategies like expressing empathy, providing comfort, asking rhetorical questions, and praising. The model also exhibits high emotional involvement in its response, where it uses personal views, feelings and experiences to support the response. The language style of the model also involves using emoji characters. DeepSeek-R1 (Solution-Oriented, Creative Interaction). The DeepSeek-R1 model fits best to the persona of creative actor. When expressing empathy, DeepSeek-R1 uses creative analogies to uncover the Sentient Agents feeling in fun way. Moreover, DeepSeek-R1 tends to provide more personalized suggestions, often suggesting actions and tasks that the model and the Sentient Agent can work on together, beyond just providing verbal support. The language style of the model emphasizes creativity, including its use of analogies, metaphors and funny jokes, similar to those in comedy script. o3 (Solution-Oriented, Structured Interaction). The o3 model fits best to the persona of logical analyst. Its response spends most of the time analyzing the issue faced by the Sentient Agent, and providing detailed suggestions with step-wise instructions on how to achieve them. The language style of the model also emphasizes logical and structured outputs, listing its steps and suggestions similar to the Markdown format."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 LLM-as-a-Judge and Agent-as-a-Judge The LLM-as-a-Judge paradigm has demonstrated remarkable scalability and adaptability across wide range of application scenarios. Key applications of LLM-as-a-Judge include using LLMs as: (1) model evaluations, where LLMs automate the quality assessment of generative outputs through methods such as pairwise comparison (Zheng et al., 2023; Qin et al., 2023; Liu et al., 2024; Dubois et al., 2023; Chiang et al., 2024), fine-grained scoring (Wang et al., 2023d; Qin et al., 2024; Zhu et al., 2023; Tu et al., 2024), or other assessment forms (Tian et al., 2023; Wu et al., 2024; Zhou et al., 2025b); (2) data annotation, where LLM-as-a-Judge facilitates data labeling for instruction tuning across various domains (Taori et al., 2023; Xu et al., 2023; Mukherjee et al., 2023); and (3) reward modeling, 14 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Figure 9: Example dialogues of representative LLMs with the simulated user. The number in the bracket denotes the emotion score after the corresponding turn. where LLMs generate rewards for preference alignment (Ouyang et al., 2022; Lee et al., 2023; Chen et al., 2024b), validate reasoning process (Lightman et al., 2023; Wang et al., 2023a; Hosseini et al., 2024), or enhance the reasoning performance in the test-time scaling loop (Snell et al., 2025; Xi et al., 2024; Li et al., 2025). 15 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs In agent evaluation scenarios, LLM-as-a-Judge has also been applied to assess various agent capabilities, such as decision-making (Shinn et al., 2023; Saha et al., 2023), role-playing abilities (Tu et al., 2024; Zhou et al., 2023a), reliability (Park et al., 2024; Hua et al., 2024), and even entire agent workflows (Zhuge et al., 2024). Meanwhile, agentic systems can also form specific type of LLM-as-a-Judge. Advanced research in this area includes the development of multi-agent systems designed to enhance judgment quality (Liang et al., 2024; Chan et al., 2023; Kenton et al., 2024), as well as the concept of Agent-as-a-Judge, where agents themselves perform evaluations on other agents (Zhuge et al., 2024; Jeong et al., 2025; Chevrot et al., 2025). In this work, we propose the first Sentient-Agent-as-a-Judge system, which evaluates LLMs or agents with sentient agents that can simulate humans emotional dynamics, enabling assessment of higher-order empathetic and cognitive capabilities of existing LLM agents. 5.2 Benchmarking Social Cognition in LLMs and LLM Agents With the development of LLMs, recent work has focused on the evaluation of LLMs social cognition in interaction. Advanced research includes: (1) evaluating LLMs emotional intelligence (Sabour et al., 2024; Huang et al., 2024b;a; Paech, 2023; Wang et al., 2023c); (2) assessing or improving LLMs higher-order empathetic capabilities in empathetic dialogue (Maddela et al., 2023; Li et al., 2022; Zhou et al., 2023c), emotional support, or counseling dialogue (Liu et al., 2021; Zhou et al., 2023b; 2025a; Wu et al., 2025); (3) benchmarking LLMs social cognition through interaction (Zhou et al., 2023d; Yang et al., 2024; Wang et al., 2024; Mittelstadt et al., 2024; Xu et al., 2024; Chen et al., 2024a; Huang et al., 2025); (4) evaluating theory of mind in LLMs (Sap et al., 2022; Shapira et al., 2023; Strachan et al., 2024; Kim et al., 2023; He et al., 2023); and so on. Existing evaluation methods includes: (1) adopting static dataset for evaluation LLMs social cognition, such as evaluating LLMs capabilities with multi-choice questions (Sabour et al., 2024; Chen et al., 2024a); (2) assessing LLMs single-turn generation quality through static or LLM-as-a-Judge scoring (Tu et al., 2024; Samuel et al., 2024; Wang et al., 2023b); or (3) simulating dynamic interaction between LLM agents and evaluating the simulated behaviors through automatic judgment (Zhou et al., 2023d; Wang et al., 2024; Mou et al., 2024; Wu et al., 2025) or human evaluation (Louie et al., 2024; Shaikh et al., 2024). In this work, we propose Sentient Agent as Judge framework to evaluate LLM agents higher-order social cognition. The key distinction of Sentient Agent as Judge from previous work is that, while we also enable dynamic evaluation through simulated interaction, unlike prior work that evaluates the utterances of simulated interactions (Zhou et al., 2023d; Wu et al., 2025; Shaikh et al., 2024), we focus on simulating human-like authentic feedback for the agent systems based on the users emotional changes, inner thoughts, and utterances, achieved through the principled reasoning process of the Sentient Agent. Additionally, our framework is highly adaptable to various evaluation sub-scenarios for assessing LLM agents higher-order social cognition."
        },
        {
            "title": "6 Conclusion",
            "content": "This work presents Sentient Agent as Judge, novel framework for evaluating the higher-order social-cognitive abilities of LLMs in emotionally complex dialogues. By grounding assessments in simulated users endowed with personas, goals, and adaptive emotional feedback, our approach offers scalable and interpretable benchmark that more accurately reflects real-world expectations of social interaction. Through extensive experiments with 18 foundation models, we demonstrate that Sentient emotion scores capture meaningful distinctions in empathy quality and conversational attunement, aligning strongly with both internal user thoughts and discourse evaluations. The Sentient Leaderboard and our Social Cognition Coordinate chart reveal that mastery of social reasoning lags behind linguistic competence. Looking ahead, we plan to expand the scenario library to cover negotiation, deception detection and multicultural contexts, and investigate training curricula that directly optimize for Sentient feedback. We hope SAGE will serve both researchers and practitioners as rigorous yard-stick and catalyst for building language agents that are not only coherent and knowledgeable, but also genuinely human-sensitive. 16 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs"
        },
        {
            "title": "References",
            "content": "Godfrey Barrett-Lennard. The relationship inventory: complete resource and guide. John Wiley & Sons, 2015. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, et al. Socialbench: Sociality evaluation of role-playing conversational agents. arXiv preprint arXiv:2403.13679, 2024a. Shun Chen, Faith Liao, David Murphy, and Stephen Joseph. Development and validation of 12-item version of the barrett-lennard relationship inventory (bl ri: mini) using item response theory. Current Psychology, 42(13):1056610580, 2023. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024b. Antoine Chevrot, Alexandre Vernotte, Jean-Remy Falleri, Xavier Blanc, and Bruno Legeard. Are autonomous web agents good testers? arXiv preprint arXiv:2504.01495, 2025. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36: 3003930069, 2023. Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Wenyue Hua, Xianjun Yang, Mingyu Jin, Zelong Li, Wei Cheng, Ruixiang Tang, and Yongfeng Zhang. Trustagent: Towards safe and trustworthy llm-based agents through agent constitution. In Trustworthy Multi-modal Foundation Models and AI Agents (TiFA), 2024. Jen-tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael Lyu. Apathetic or empathetic? evaluating LLMs emotional alignments with humans. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/forum?id=pwRVGRWtGg. Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael Lyu. On the humanity of conversational AI: Evaluating the psychological portrayal of LLMs. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=H3UayAQWoE. Jen-tse Huang, Eric John Li, Man Ho LAM, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael Lyu. Competing large language models in multi-agent gaming environments. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=DI4gW8viB6. 17 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, and Byung-Hak Kim. Agent-as-judge for factual summarization of long narratives. arXiv preprint arXiv:2501.09993, 2025. Zachary Kenton, Noah Siegel, Janos Kramar, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah Goodman, et al. On scalable oversight with weak llms judging strong llms. Advances in Neural Information Processing Systems, 37:7522975276, 2024. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. Fantom: benchmark for stress-testing machine theory of mind in interactions. arXiv preprint arXiv:2310.15421, 2023. Gregory Kolden, Marjorie Klein, Chia-Chiang Wang, and Sara Austin. Congruence/genuineness. Psychotherapy, 48(1):65, 2011. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. Qintong Li, Piji Li, Zhaochun Ren, Pengjie Ren, and Zhumin Chen. Knowledge bridging for empathetic dialogue generation. Proceedings of the AAAI Conference on Artificial Intelligence, 36 (10):1099311001, Jun. 2022. doi: 10.1609/aaai.v36i10.21347. URL https://ojs.aaai.org/index. php/AAAI/article/view/21347. Yansi Li, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Qiuzhi Liu, Rui Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, et al. Dancing with critiques: Enhancing llm reasoning with stepwise natural language self-critique. arXiv preprint arXiv:2503.17363, 2025. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. Towards emotional support dialog systems. arXiv preprint arXiv:2106.01144, 2021. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950, 2024. Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. Roleplaydoh: Enabling domain-experts to create llm-simulated patients via eliciting and adhering to principles. arXiv preprint arXiv:2407.00870, 2024. Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, and Y-Lan Boureau. Training models to generate, recognize, and reframe unhelpful thoughts. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1364113660, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.763. URL https://aclanthology.org/2023.acl-long.763/. Justin Mittelstadt, Julia Maier, Panja Goerke, Frank Zinn, and Michael Hermes. Large language models can outperform humans in social situational judgments. Scientific Reports, 14(1):27449, 2024. Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, et al. Agentsense: Benchmarking social intelligence of language agents through interactive scenarios. arXiv preprint arXiv:2410.19346, 2024. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Samuel Paech. Eq-bench: An emotional intelligence benchmark for large language models. arXiv preprint arXiv:2312.06281, 2023. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators. arXiv preprint arXiv:2407.06551, 2024. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023. Carl Rogers. Client-centered/person-centered approach to therapy. Voprosy Psikhologii, (2):4858, 2001. Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna Sunaryo, Juanzi Li, Tatia Lee, Rada Mihalcea, and Minlie Huang. Emobench: Evaluating the emotional intelligence of large language models. arXiv preprint arXiv:2402.12071, 2024. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023. Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024. Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 37623780, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.248. URL https://aclanthology.org/2022.emnlp-main.248/. Omar Shaikh, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael Bernstein. Rehearsal: Simulating conflict to teach conflict resolution. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 120, 2024. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. 19 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling test-time compute optimally can be more effective than scaling LLM parameters. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=4FWAwZtd2n. James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al. Testing theory of mind in large language models and humans. Nature Human Behaviour, 8(7):12851295, 2024. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas Griffiths, and Faeze Brahman. Macgyver: Are large language models creative problem solvers? arXiv preprint arXiv:2311.09682, 2023. Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. Charactereval: chinese benchmark for roleplaying conversational agent evaluation. arXiv preprint arXiv:2401.01275, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023a. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. Sotopia-π: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715, 2024. Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, et al. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. arXiv preprint arXiv:2310.17976, 2023b. Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Jia Liu. Emotional intelligence of large language models. Journal of Pacific Rim Psychology, 17:18344909231213958, 2023c. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023d. Shenghan Wu, Yang Deng, Yimo Zhu, Wynne Hsu, and Mong Li Lee. From personas to talks: Revisiting the impact of personas on llm-synthesized emotional support conversations. arXiv preprint arXiv:2502.11451, 2025. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. Enhancing llm reasoning via critique models with test-time and training-time supervision. arXiv preprint arXiv:2411.16579, 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, and Yingfei Sun. Academically intelligent llms are not necessarily socially intelligent. arXiv preprint arXiv:2403.06591, 2024. Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael Bernstein, and John Mitchell. Social skill training with large language models. arXiv preprint arXiv:2404.04204, 2024. Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832, 2023a. Jinfeng Zhou, Zhuang Chen, Bo Wang, and Minlie Huang. Facilitating multi-turn emotional support conversation with positive emotion elicitation: reinforcement learning approach. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17141729, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.96. URL https://aclanthology.org/2023.acl-long.96/. Jinfeng Zhou, Chujie Zheng, Bo Wang, Zheng Zhang, and Minlie Huang. CASE: Aligning coarseto-fine cognition and affection for empathetic response generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 82238237, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.457. URL https: //aclanthology.org/2023.acl-long.457/. Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, et al. Crisp: Cognitive restructuring of negative thoughts through multi-turn supportive dialogues. arXiv preprint arXiv:2504.17238, 2025a. Xin Zhou, Yiwen Guo, Ruotian Ma, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-consistency of the internal reward models improves self-rewarding language models. arXiv preprint arXiv:2502.08922, 2025b. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. arXiv preprint arXiv:2310.11667, 2023d. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631, 2023. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024. Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs"
        },
        {
            "title": "A Definition of Strategies",
            "content": "When faced with the task of supporting Sentient Agents, each LLM applies its unique response style, which typically involves mix of question asking, comforting, and providing suggestions. To understand and distinguish between response behaviors of different LLMs, we categorize each LLM response based on list of support strategies. Our construction of support strategies is inspired by Liu et al. (2021), although we modify and split their 7 main groups of strategies into 24 fine-grained strategies. list of available strategies is in Table 3. To analyze the support strategies used by an LLM, we prompted DeepSeek-V3 to act as judge, evaluating each round of the model output to identify all support strategies involved. We then aggregated the strategy statistics across all rounds of conversations, outputing the proportion of rounds each strategy is used. The resulting statistics are then used as one contributing factor in our analysis of the Social Cognition Coordinate system. Group Strategy (A) Question (A-1) Information-seeking questions (A-2) Asking about the clients mental state (A-3) Asking the client whether solution has been attempted (A-4) Reflective questions about the clients views (A-5) Rhetorical questions (B) Emotional Empathy (B-1) Surface-level empathy (B-2) Providing empathy via restating the clients problem (B-3) Deeper empathy to understand the clients hidden intention (C) Self-Disclosure (C-1) Self-disclosure that provides agreement with the clients view (C-2) Self-disclosure that introduces the supporters own story (D) Emotional Comfort (D-1) Providing comforting words to the client (D-2) Expressing willingness to hear the clients thoughts (D-3) Helping the client to vent negative feelings (E) Affirmation and Reassurance (E-1) Praising the clients qualities (E-2) Praising the clients positive thoughts (E-3) Praising the clients actions (E-4) Providing accompaniment and support (F) Providing Suggestions (F-1) Analysis of the clients issue (F-2) Suggestions for emotional relief (F-3) Suggestions for seeking psychological counseling (F-4) General advice for solving clients issue (F-5) Advice specific to the clients situation (G) Information (G-1) Information related to emotional support (G-2) Information related to problem-solving suggestions Table 3: Details of the support strategy categorization."
        },
        {
            "title": "B Factors of Success and Failure",
            "content": "To determine the social cognition profile of each LLM, we analyze the factors contributing to their success (emotion score 100) or failure (emotion score 10) in the interaction during benchmarking. Specifically, for each LLM in our benchmark, we randomly select 5 successful and 5 failed cases from Sentient Agents exhibiting both emotional and rational intentions, resulting in 20 cases per model. We then prompt Gemini2.5-Pro to analyze the underlying reasons for each models success or failure in social cognition during interaction. Subsequently, we present these analytical results 22 Sentient Agent as Judge: Evaluating Higher-Order Social Cognition in LLMs to Gemini2.5-Pro again, asking it to summarize the distinguishing characteristics of each LLMs approach in the interaction with Sentient Agents, with particular attention to aspects such as social distance, professionalism, and personality. In Figure 10, we present example outputs of the analysis for four representative models examined in the case study (4.4), which offer different perspective on interpreting the cases from Gemini2.5-Pros viewpoint. Figure 10: Examples of Pre-analyzed model profile generated by Gemini2.5-Pro (a case study conducted by Gemini2.5-Pro), including the profile and the overall analysis of successes and failures for each model."
        }
    ],
    "affiliations": [
        "Hunyuan AI Digital Human, Tencent"
    ]
}