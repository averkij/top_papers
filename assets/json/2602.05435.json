{
    "paper_title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity."
        },
        {
            "title": "Start",
            "content": "Stable Velocity: Variance Perspective on Flow Matching Donglin Yang 1 Yongxing Zhang 2 3 Xin Yu 1 Liang Hou 4 Xin Tao 4 Pengfei Wan 4 Xiaojuan Qi 1 Renjie Liao 2 3 5 Abstract While flow matching is elegant, its reliance on single-sample conditional velocities leads to highvariance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) highvariance regime near the prior, where optimization is challenging, and 2) low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variancereduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), finetuning-free acceleration. Extensive experiments on ImageNet 256 256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2 faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/ linYDTHU/StableVelocity. 6 2 0 2 5 ] . [ 1 5 3 4 5 0 . 2 0 6 2 : r 1. Introduction Recent years have seen major advances in generative modeling driven by diffusion (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020; Karras et al., 2022b), flow This work was conducted during the authors internship at Kling Team. Project Lead. 1University of Hong Kong, Hong Kong 2University of British Columbia, Canada 3Vector Institute for AI, Toronto, Canada 4Kling Team, Kuaishou Technology 5Canada CIFAR AI Chair. Correspondence to: Xiaojuan Qi <xjqi@eee.hku.hk>, Renjie Liao <rjliao@ece.ubc.ca>. Preprint. February 6, 2026. 1 matching (Lipman et al., 2022; Liu et al., 2022), and stochastic interpolants (Albergo et al., 2023; Ma et al., 2024; Yu et al., 2024). These approaches transform simple prior distribution, e.g., standard normal, into complex data distribution through stochastic or deterministic dynamic process, admitting unified formulation that has enabled scalable training and state-of-the-art performance across image generation (Labs, 2024; Esser et al., 2024; Wu et al., 2025a) and restoration (Rombach et al., 2022; Lin et al., 2024), and video generation (Brooks et al., 2024; Wan et al., 2025). Among these approaches, Conditional Flow Matching (CFM) (Tong et al., 2023) provides an elegant objective for learning the probability flow without explicitly simulating the forward SDE or PF-ODE. By training neural network to predict conditional velocity fields, CFM enjoys both theoretical guarantees and practical scalability. Despite its elegance, CFM suffers from fundamental yet underexplored limitation: the variance of its training target. In practice, the conditional velocity vt(xt x0) is single-sample Monte Carlo estimate of the true marginal velocity vt(xt), which can exhibit high variance, particularly at timesteps where the marginal distribution remains close to the prior. Such high-variance targets not only slow convergence, but also induce mismatch between the empirical optimization dynamics and the ideal population objective. While prior work has empirically observed variance-related inefficiencies in diffusion training (Karras et al., 2022a; Choi et al., 2022; Xu et al., 2023), principled variancetheoretic understanding within flow matching and stochastic interpolants has been largely missing. In this work, we develop variance-based perspective on stochastic interpolants. By explicitly characterizing the variance of conditional velocity targets, we reveal two-regime structure that governs both training and inference: highvariance regime near the prior, where optimization is inherently noisy, and low-variance regime near the data distribution, where conditional and marginal velocities coincide. This insight naturally leads to the Stable Velocity framework. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, together with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary superStable Velocity: Variance Perspective on Flow Matching vision when the variance is low. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling finetuning-free acceleration via Stable Velocity Sampling (StableVS). We validate our proposed approach on ImageNet 256 256 and standard text-to-image and text-to-video benchmarks using state-of-the-art pretrained models. StableVM and VA-REPA consistently outperform REPA (Yu et al., 2024) across wide range of model scales and training variants, including REG (Wu et al., 2025b) and iREPA (Singh et al., 2025) (Sec. 4.2). Meanwhile, StableVS achieves more than 2 inference acceleration in the low-variance regime for recent flow-based models, such as SD3.5 (Esser et al., 2024), Flux (Labs, 2024), Qwen-Image (Wu et al., 2025a), and Wan2.2 (Wan et al., 2025), without perceptible degradation in sample quality (Sec. 4.3). 2. Variance Analysis of Flow Matching We first briefly review flow matching and stochastic interpolants. Details as well as their connections to score-based diffusion models are deferred to Appendix A. Flow Matching and Stochastic Interpolants. Given samples from an unknown data distribution q(x0) over Rd, flow matching (Lipman et al., 2022; Liu et al., 2022; Lipman et al., 2024a) and stochastic interpolants (Albergo et al., 2023; Albergo & Vanden-Eijnden, 2023) define continuous-time corruption process xt = αtx0 + σtε, ε (0, I), (1) where αt and σt are differentiable functions satisfying α2 + σ2 > 0 for all [0, 1], with boundary conditions α0 = σ1 = 1 and α1 = σ0 = 0. Most works (Ma et al., 2024; Yu et al., 2024; Lipman et al., 2022; Liu et al., 2022; Lipman et al., 2024a) adopt simple linear interpolant αt = 1 t, σt = t. This process induces conditional velocity field vt(xt x0) = σ σt (xt αtx0) + α tx0, (2) where α and σ ing marginal velocity field denote the time derivative. The correspondvt(xt) = Ept(x0xt)[vt(xt x0)] . (3) Conditional Flow Matching (CFM). Training typically uses the CFM objective (Tong et al., 2023): min θ Et, q(x0), pt(xtx0)λt vθ(xt, t) vt(xtx0)2 , (4) where λt is positive weighting function, and vθ(, ) : [0, 1] Rd Rd is neural velocity field parameterized by θ. Here, the conditional path distribution is pt(xt x0) = Figure 1. Variance curves of VCFM(t) with 15%85% quantile bands. Evaluated on GMMs of varying dimensionality, CIFAR-10 images, and 256 256 ImageNet latents obtained by the Stable Diffusion VAE. The y-axis reports VCFM(t) normalized by the square root of the data dimension. See Appendix F.2 for details. (xt αtx0, σ2 I). The minimizer of Eq. (4) is provably the true marginal velocity field (Tong et al., 2023; Xu et al., 2023; Ma et al., 2024): θ(xt, t) = Ept(x0xt) [vt(xt x0)] = vt(xt). (5) Variance of CFM. Although unbiased, the CFM target is only single-sample Monte Carlo estimator of Eq. (3), which can exhibit high variance (Owen, 2013; Elvira & Martino, 2021). Following Xu et al. (2023), we quantify this variance by the average trace of the conditional velocity covariance at time t: CFM(t) = Ept(xt) (cid:2)Tr(cid:0)Covpt(x0xt) (cid:104) (cid:0)vt(xt x0)(cid:1)(cid:1)(cid:3) = Eq(x0), pt(xtx0) vt(xt x0) vt(xt)2(cid:105) . (6) To characterize the behavior of VCFM(t), we evaluate it on Gaussian mixture models (GMMs), CIFAR-10 (Krizhevsky, 2009), and ImageNet latent codes produced by the pretrained Stable Diffusion VAE (Rombach et al., 2022). As shown in Fig. 1, two consistent patterns emerge: Lowvs. high-variance regimes. VCFM(t) remains close to zero at small but increases rapidly as grows, naturally separating the process into low-variance regime (0 < ξ) and high-variance regime (ξ 1). Effect of dimensionality. As data dimensionality increases, the split point ξ shifts toward 1, enlarging the low-variance regime while also increasing the overall variance magnitude. Fig. 2 illustrates the two regimes: in the low-variance regime the posterior concentrates on single reference sample, while in the high-variance regime it spreads over multiple samples, leading to large variance. Increasing dimensionality delays this mixing effect and shifts the split point ξ closer to 1. Although the exact split point ξ depends on the 2 Stable Velocity: Variance Perspective on Flow Matching unknown data distribution, our variance analysis suggests clear dimensionality-dependent trend, which provides practical guidance for choosing ξ in real-world settings. In summary, these observations naturally suggest two key questions: (1) How can we reduce training variance in the high-variance regime without altering the global minimizer? (2) How can the low-variance regime be exploited for stronger supervision and faster sampling? 3. Variance-Driven Optimization of Training and Sampling In this section, we address the two questions posed in Sec. 2 from unified, variance-driven perspective. 3.1. Stable Velocity Matching We propose Stable Velocity Matching (StableVM), variance-reduced yet unbiased alternative to CFM. The key idea is to replace the single-sample conditional velocity target with multi-sample, self-normalized aggregation over reference data points under the multi-sample conditional path. This reduces training variance while preserving the exact same global minimizer as CFM in Eq. (5). 0}n (cid:0)xt (cid:8)xi Multi-sample conditional path. Inspired by (Xu et al., 2023), we introduce reference samples {xi i=1, drawn i.i.d. from the data distribution q(x0). We then define composite conditional probability path pGMM 0), which is essentially mixture of conditional probabilities associated with each reference sample. The posterior under the GMM path admits simple mixture form (Prop. E.1) , and preserves the original marginal path in expectation, property that is crucial for unbiasedness. pt(xt xi (cid:1) := (cid:80)n (cid:9)n i= i=1 1 0 StableVM target. Based on the reference samples, we define the StableVM target (cid:98)vStableVM as the self-normalized importance weighted average of the conditional velocities: (cid:98)vStableVM(xt; {xi 0}n i=0) := (cid:80)n k=1 pt(xt xk 0 )vt(xt xk 0 ) j=1 pt(xt xj 0) (cid:80)n . (7) Compared to the CFM target vt(xt x0), this can be viewed as multi-sample Monte Carlo estimator of the same marginal velocity field vt(xt). Training objective. We train neural velocity field vθ(xt, t) by minimizing interpolant framework. Under the special case of VP diffusion, it resembles the STF objective (Eq. (22)) in form, but differs fundamentally in how the noisy training input xt is constructed. STF generates xt by perturbing single reference sample, i.e., xt pt(xt x1 0), and then forms self-normalized weighted target over the remaining references. In contrast, StableVM explicitly samples xt from composite conditional path pGMM over the entire reference batch, which yields an unbiased target and extends naturally beyond VP diffusion. Details are provided in Appendix C. Unbiasedness and optimality. The following theorem establishes two key properties of StableVM: it remains unbiased and admits the same global minimizer as CFM. Theorem 3.1.(a) The StableVM target is unbiased. That is, for any xt, we have {xi 0}pGMM (cid:104) (xt) (cid:98)vStableVM(xt; {xi (cid:105) i=0) 0}n = vt(xt). (b) The global minimizer v(xt, t) of the StableVM objective LStableVM is the true velocity field vt(xt). Proofs are provided in Appendix E.2. Variance of StableVM. While remaining unbiased, StableVM strictly reduces the variance of the training target. Following Eq. (6), we derive the average trace-of-covariance VStableVM(t) (cid:20) = Extpt (cid:18) Tr Cov{xi 0}pGMM (cid:17)(cid:19)(cid:21) (cid:16) (cid:98)vStableVM = xtpt {xi 0}pGMM (cid:13) (cid:13)vt(xt) (cid:98)vStableVM(xt; {xi (cid:13) 0}n (cid:13) 2 (cid:13) i=0) (cid:13) (9) . Theorem 3.2. Fix [0, 1]. We always have VStableVM(t) VCFM(t). In fact, we can prove the following stronger variance bound stating that the variance decays in the rate of O(1/n). Proofs are provided in Appendix E.3. Theorem 3.3. Fix [0, 1]. Let vt be bounded. Assume (cid:0)(n 1)(cid:98)vt vt(xt)2(cid:1) n=1 is uniformly integrable. Let ε (0, 1). Assume (cid:90) := {x:pt(x)ε} Ex0pt(xt) (cid:104) vt(xt x0) vt(xt)2(cid:105) dxt < . LStableVM(θ) = t,{xi 0}qn xtpGMM (cid:13) (cid:13)vθ(xt, t) (cid:98)vStableVM(xt; {xi (cid:13) 0}n i=0) Then, for large enough n, we have VStableVM(t) 1 1 (cid:18) 1 ε (cid:19) VCFM(t) + + (cid:19) . (cid:18) 1 (cid:13) 2 (cid:13) (cid:13) . (8) Our StableVM target is compatible with general stochastic The training procedure is summarized in Alg. 1. 3 Stable Velocity: Variance Perspective on Flow Matching Figure 2. Illustration of CFM variance VCFM(t). (a) The low-variance regime (t ξ), where the posterior pt(x0 xt) is sharply concentrated and the conditional velocity vt(xt x0) nearly coincides with the true velocity vt(xt), yielding VCFM(t) 0. (b) The high-variance regime (t > ξ), the posterior spreads over multiple reference samples, causing the conditional velocity to fluctuate and resulting in large VCFM(t). Extension to Class-Conditional Generation with Classifier-Free Guidance. Extending StableVM to conditional generation introduces sparsity challenges, as only small subset of reference samples may match given label or prompt. To address this, we maintain class-conditional memory bank of capacity K, pre-populated from the training dataset and updated using FIFO policy. This allows StableVM to construct the mixture-based noisy input and target field using sufficiently large and diverse reference set even when per-batch class frequency is low. Crucially, this mechanism preserves unbiasedness, as all references are drawn from the true data distribution, while effectively amortizing reference sampling over time. The full algorithm is provided in Alg. 2. 3.2. Variance-Aware Representation Alignment Recent work on representation alignment (REPA) (Yu et al., 2024) and its variants (Leng et al., 2025; Wu et al., 2025b; Singh et al., 2025) demonstrates that auxiliary semantic supervision can substantially accelerate the training of diffusion transformers (Peebles & Xie, 2023a; Ma et al., 2024). From variance-regime perspective, we empirically find that the effectiveness of REPA largely arises when applied in the low-variance regime. As shown in Sec. 2, the generative process decomposes into lowand high-variance regimes. In the low-variance regime, xt remains strongly coupled to x0, preserving semantic information and making representation alignment well-conditioned. In contrast, in the high-variance regime near pure noise, xt carries little information about x0, and the posterior p(x0 xt) becomes highly multimodal, rendering deterministic alignment ill-posed. Fig. 3 empirically illustrates this regime dependence. When evaluated on well-trained model, the per-timestep representation-alignment loss remains low and learnable in the low-variance regime, but saturates at high values in the high-variance regime. Consistently, restricting representation alignment to early timesteps yields substantially better FID than applying it uniformly, while applying it only in the high-variance regime provides negligible benefit. Motivated by these observations, we propose variance-aware representation alignment (VA-REPA): semantic alignment should be applied selectively in the low-variance regime where the supervision signal is informative. This principle is independent of the specific representation or loss formulation and applies uniformly to REPA and its variants. Let ℓRA(xt) denote per-sample representation-alignment loss. We introduce nonnegative weighting function w(t) [0, 1] and define the overall training objective as = LStableVM + λRA Et,xt[w(t) ℓRA(xt)] Et[w(t)] . (10) The normalization by Et[w(t)] ensures that the alignment term is scaled by the number of effective samples, preventing vanishing gradients when most samples fall in the highvariance regime. Weighting functions. We consider three weighting schemes to modulate the alignment objective: (i) hard threshold whard(t) = I[t < ξ] for explicit ablation; (ii) sigmoid relaxation wsigmoid(t) = σ(k(ξ t)), where controls the sharpness of the transition; and (iii) an SNR-based weighting SNR(t)+SNR(ξ) , with SNR(t) = α2 wSNR(t) = , which anchors the midpoint (w = 0.5) at = ξ. All weights are applied per sample and normalized within each minibatch. /σ2 SNR(t) 3.3. Stable Velocity Sampling We introduce Stable Velocity Sampling (StableVS), principled, finetuning-free acceleration strategy for sampling in the low-variance regime. As shown in Sec. 2  (Fig. 2)  , the conditional variance VCFM(t) becomes negligible over range of early timesteps. In this regime, the instantaneous 4 Stable Velocity: Variance Perspective on Flow Matching Figure 3. Motivation for variance-aware representation alignment. (a) In the low-variance regime, the alignment loss remains consistently low on pretrained model from REPA (Yu et al., 2024), indicating learnable and informative supervision signal. In contrast, in the high-variance regime, the loss stays high, reflecting the ill-posed nature of semantic recovery from noise. (b) Restricting representation alignment to the low-variance regime yields the best FID, while applying it only in the high-variance regime provides minimal meaningful improvement over the baseline. These results indicate that representation alignment should be activated adaptively rather than uniformly along the diffusion trajectory. velocity vt(xt) is effectively determined by single dominant data point x0, allowing the true velocity field to be well approximated by the conditional velocity vt(xt x0). StableVS exploits this structure to enable stable, large-step integration without degrading sample quality, provided the model accurately recovers vt(xt). StableVS for SDE. For the reverse SDE (Eq. (16)), we derive the following DDIM-style (Song et al., 2021a) posterior: In the low-variance regime, the probability flow trajectory reduces to straight line with constant velocity, allowing exact integration via Euler steps of arbitrary size. 4. Experiments In this section, we empirically validate the three components of our variance-driven framework. Specifically, we address the following questions: pτ (xτ xt, vt(xt)) = (cid:0)µτ t, β2 I(cid:1) , (11) 1. Can StableVM and VA-REPA improve generation performance and training speed? (Tab. 1, 2) where βt = fβστ and fβ [0, 1] lable parameter. We then define the noise ratio ρt (cid:112)(σ2 )/σ2 τ β2 (ατ αtρt)/(α then given by: is control- := and the velocity coupling coefficient λt := αtσ t/σt). The posterior mean µτ is 2. Can StableVM and VA-REPA generalize across different training settings? (Tab. 2, 3, 4, Fig. 4) 3. Can StableVS greatly reduce the sampling steps within the low-variance regime without performance degradation on pretrained T2I and T2V models? (Tab. 5, 6, Fig. 5) (cid:18) µτ = ρt λt (cid:19) σ σt xt + λtvt(xt). (12) 4.1. Setup The full derivation is provided in Appendix E.4. StableVS for ODE. For the probability flow ODE, we define the integral factor Ψt,τ := 1 ds, where Ct C(s) = α s/σs. The exact solution at timestep τ is: αsσ C(s) σs (cid:82) τ xτ = στ (cid:20)(cid:18) 1 σt σ σt (cid:19) Ψt,τ xt + Ψt,τ vt(xt) (cid:21) . (13) The derivation is in Appendix E.5. In the special case of linear interpolant (i.e., αt = 1 t, σt = t), setting βt = 0 in Eq. (11) makes two samplers coincide: xτ = xt + (τ t)vt(xt). (14) 5 Implementation Details. Unless otherwise specified, the implementations of StableVS and VA-REPA follow the configuration of REPA (Yu et al., 2024). All models are trained on the ImageNet (Deng et al., 2009) training split. Input images are encoded into latent representations R32324 using the pre-trained VAE from Stable Diffusion (Rombach et al., 2022). For StableVM and VA-REPA, we set the split point ξ = 0.7, the bank capacity = 256, and adopt the sigmoid weighting function wsigmoid(t). StableVS uses 9 steps in the low-variance regime, with ξ = 0.85 and fβ = 0. Evaluation. For StableVM and VA-REPA, we follow the ADM evaluation protocol (Dhariwal & Nichol, 2021). Generation quality is assessed using FID (Heusel et al., 2017), IS (Salimans et al., 2016), sFID (Nash et al., 2021), and precision/recall (Kynkaanniemi et al., 2019), all computed Stable Velocity: Variance Perspective on Flow Matching over 50K generated samples. Following Yu et al. (2024), we employ an SDE Euler-Maruyama sampler with 250 steps. For experiments with classifier-free guidance (CFG) (Ho & Salimans, 2022), we use guidance scale of = 1.8 with interval-based CFG schedule (Kynkaanniemi et al., 2024). We further evaluate StableVS on standard text-to-image (T2I) and text-to-video (T2V) benchmarks. For T2I, we adopt the GenEval benchmark (Ghosh et al., 2023), which comprises 553 prompts spanning 6 categories. Following the official protocol, we generate four samples per prompt using random seeds {0, 1000, 2000, 3000}. For T2V, we evaluate on T2V-CompBench (Sun et al., 2025), which contains 1,400 text prompts covering seven aspects of compositionality. For both tasks, we additionally report referencebased metrics, including PSNR, SSIM, and LPIPS (Zhang et al., 2018), computed with respect to 30-step baseline. 4.2. Evaluation of StableVM and VA-REPA Quantitative Evaluation. We evaluate StableVM and VAREPA against state-of-the-art latent diffusion transformers under both classifier-free guidance (CFG) and non-CFG settings. Tab. 1 reports CFG results using the SiT-XL backbone for all methods. With only 80 training epochs, our approach achieves the strongest overall performance among all compared methods, outperforming prior REPA-based approaches in both FID and IS, while remaining highly competitive with REPA-E (Leng et al., 2025). Although REPA-E attains slightly lower FID, the gap is marginal, and our method reaches comparable generation quality at substantially lower training cost. Notably, REPA-E relies on expensive end-to-end fine-tuning of both the autoencoder and the diffusion transformer, whereas our pipeline keeps the autoencoder fixed, resulting in more modular and computationally efficient training procedure. Tab. 2 reports results without CFG over multiple SiT architectures and training checkpoints. Across all reported settings, StableVM and VA-REPA consistently improve FID, IS, precision, and recall over vanilla REPA, demonstrating strong scalability and stability with respect to both model size and training duration. Variation in REPA-based Methods. StableVM and VAREPA can plug into existing REPA-style pipelines without modifying the underlying method. Tab. 3 reports results at 100k iterations for vanilla REPA, REG, and iREPA. Integrating our approach consistently improves the performance across all variants, demonstrating that our contributions are orthogonal and provide reliable, drop-in performance gains. Ablation on Split Point ξ. The split point ξ defines the boundary between the low-variance regime and the highvariance regime, and determines the extent of representation alignment. Tab. 4 analyzes the effect of different ξ values Table 1. Comparison of latent diffusion transformers with CFG. We compare our method against baselines including MaskDiT (Zheng et al., 2023), DiT-XL/2 (Peebles & Xie, 2023b), SiT-XL/2 (Ma et al., 2024), Faster-DiT (Yao et al., 2024), REPA (Yu et al., 2024), iREPA (Singh et al., 2025), REG (Wu et al., 2025b), and REPA-E (Leng et al., 2025). The first Ours block uses the standard REPA sampling protocol, while the second adopts class-balanced sampling (marked with ) following REPAE. Methods marked with require fine-tuning autoencoders. Model Epochs FID sFID IS Prec. Rec. Latent Diffusion Transformers MaskDiT DiT-XL/2 SiT-XL/2 Faster-DiT 1600 1400 1400 400 2.28 2.27 2.06 2.03 5.67 4.60 4.50 4. Representation Alignment Methods 4.60 80 REPA 4.70 800 1.98 1.42 iREPA REG Ours REPA-E Ours 80 80 480 80 400 80 800 80 1.93 1.86 1.40 1.80 1.47 1.67 1.12 1.71 1.34 4.59 4.49 4. 4.52 4.51 4.12 4.09 4.54 4.53 276.6 278.2 270.3 264.0 263.0 305.7 268.8 321.4 296. 272.4 300.3 0.80 0.83 0.82 0.81 0.80 0.80 0.80 0.76 0.77 0.81 0. 0.61 0.57 0.59 0.60 0.61 0.65 0.60 0.63 0.66 0.60 0.63 266.3 302.9 274.2 305.0 0.80 0.79 0.81 0.80 0.63 0.66 0.61 0.64 at multiple training stages. At early training (100k iterations), smaller split point (ξ = 0.6) achieves the best FID, indicating that weaker alignment provides an easier supervisory signal that accelerates initial convergence. As training progresses, ξ = 0.7 consistently yields the best overall performance, particularly at 400k iterations, where it delivers the best performance. In contrast, larger split point (ξ = 0.8) degrades performance, due to noisy supervision introduced from the high-variance regime. Based on these results, we adopt ξ = 0.7 as the default setting. Ablations on weighting schemes w(t) and bank capacity K. Fig. 4 studies the sensitivity of VA-REPA weighting strategies and the StableVM memory bank capacity. Across all weighting strategies, incorporating VA-REPA consistently improves performance over the REPA baseline. Among them, soft weighting schemes outperform the hard threshold, with wsigmoid(t) achieving the best overall results. For the memory bank, = 256 is sufficient to obtain stable variance reduction, while increasing to = 1024 yields only marginal gains, indicating diminishing returns. 4.3. Evaluation of StableVS StableVS reduces sampling cost while preserving content across solvers and modalities. Tab. 5 and 6 show that replacing the base solver with StableVS only in the lowvariance regime substantially reduces the total number of 6 Stable Velocity: Variance Perspective on Flow Matching Table 2. Variation in Model Scale and Checkpoints. Comparison of our full method (StableVM + VA-REPA) against vanilla-REPA. Results are reported without CFG. Table 4. Ablation on split point ξ. Impact of different split points across training stages. The default setting (ξ = 0.7) is highlighted. Method Iter FID sFID IS Prec. Rec. SiT-B/2 (130M) 100k REPA Ours 100k SiT-L/2 (458M) 100k REPA Ours 100k SiT-XL/2 (675M) 100k REPA Ours 100k 200k REPA Ours 200k 400k REPA Ours 400k 52.06 49.69 22.75 21.03 18.59 17.12 11.04 10.56 8.13 7.58 8.18 8.18 5.52 5. 5.39 5.39 5.02 5.03 5.01 5.03 26.8 28.5 59.9 63.9 70.6 74.8 101.9 105.4 124.5 127.6 0.45 0.46 0.61 0. 0.64 0.65 0.68 0.69 0.69 0.70 0.59 0.60 0.63 0.63 0.62 0.63 0.64 0.64 0.66 0.66 Table 3. Compatibility of StableVM + VA-REPA with REPA variants. Integration results for vanilla REPA, REG, and iREPA. All metrics are reported at 100k iterations. Methods FID sFID REPA +Ours REG +Ours iREPA +Ours 18.59 17.12 8.90 8. 16.62 16.02 5.39 5.39 5.50 5.34 5.31 5.30 IS 70.6 74. 125.3 128.8 76.7 78.6 Prec. Rec. 0.64 0.65 0.72 0.74 0.65 0. 0.62 0.63 0.59 0.60 0.63 0.63 sampling steps without degrading generation quality. Across SD3.5, Flux, Qwen-Image-2512, and Wan2.2, StableVS with 9 low-variance steps consistently matches or exceeds 30-step baselines, while naively shortening the base solver leads to noticeable drops in reference metrics. This behavior is solver-agnostic and holds for both images and videos: similar gains are observed across Euler, DPM-Solver++, and UniPC. On GenEval, StableVS recovers the perceptual fidelity lost by short-step baselines, producing outputs indistinguishable from 30-step results; on T2V-CompBench, it maintains generative performance while significantly improving reference metrics, indicating that step reduction doesnt alter spatial structure, motion patterns, or semantic content. These findings directly support our analysis in Sec. 3.3: in the low-variance regime, the posterior effectively collapses, rendering the sampling trajectory deterministic. Consequently, replacing the base solver with StableVS in this regime changes the numerical integration path but not the resulting sample, conclusion confirmed by qualitative comparisons in Fig. 5 under identical random seeds. Choice of split point ξ for StableVS. We note that the split point ξ used for StableVS differs from that used for VA-REPA. Empirically, we find that smaller split point adopted in VA-REPA (ξ = 0.7) yields samples that are closer to the original 30-step baseline, whereas larger split point (e.g., ξ = 0.85) allows more aggressive step reduction with minimal quality degradation. detailed ablation over Iter Split point ξ FID sFID 100k 200k 400k 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 17.38 17.63 17.85 10.57 10.56 10.73 7.97 7.58 7. 5.36 5.33 5.34 5.02 5.03 5.02 5.04 5.03 4.97 IS 73.7 73.2 72.3 104.2 105.4 103. 124.3 127.6 127.0 Prec. Rec. 0.65 0.65 0.65 0.69 0.69 0.68 0.70 0.70 0.70 0.63 0.62 0. 0.64 0.64 0.65 0.66 0.66 0.66 Figure 4. Ablation on VA-REPA weighting and StableVM bank capacity. Left: effect of different weighting schemes w(t), showing that soft weightings outperform hard thresholding. Right: effect of memory bank capacity K, where = 256 already achieves near-optimal performance. All results are evaluated at 100k iterations. REPA baseline is shown as dashed line. ξ and other StableVS hyperparameters is provided in Tab. 9. 5. Related Works Training Acceleration of Diffusion and Flow Models. Diffusion and flow matching models exhibit strong generative performance but incur substantial computational costs when trained on high-resolution data. To mitigate this burden, prior work has explored three complementary directions. First, dimensionality reduction methods compress highresolution data into lower-dimensional representations to reduce training cost. Latent diffusion (Rombach et al., 2022) pioneered this paradigm, with subsequent improvements focusing on more efficient autoencoders (Chen et al., 2025) or localized modeling (Wang et al., 2023). Second, several works incorporate auxiliary regularization or self-supervised objectives to stabilize optimization and accelerate convergence (Zheng et al., 2023; Yu et al., 2024; Wu et al., 2025b; Leng et al., 2025; Singh et al., 2025). Third, improvements to the training objective itself aim to reduce variance or imbalance across timesteps, including log-normal timestep sampling and SNR-aware weighting (Karras et al., 2022b; Choi et al., 2022). More closely related to our work, recent studies employ self-normalized importance sampling (SNIS) estimators (Hesterberg, 1995) to reduce the variance of score estimation (Xu et al., 2023; Niedoba et al., 2024), albeit at the cost of introducing bias. In contrast, 7 Stable Velocity: Variance Perspective on Flow Matching Table 5. Evaluation on T2V-CompBench at 640 480p for Wan2.2 (Wan et al., 2025). StableVS replaces the base solver in the low-variance regime [0, ξ] (steps in parentheses), keeping the high-variance regime [ξ, 1] unchanged. Split point fixed at ξ = 0.85. Highlighted rows show StableVS matches or exceeds 30-step baselines with fewer steps. Solver configuration Reference metrics T2V-CompBench metrics Base solver Solver in [0, ξ] Total steps PSNR SSIM LPIPS Consist Dynamic Spatial Motion Action Interact Numeracy UniPC UniPC(19) UniPC(13) StableVS(9) 30 20 20 15.61 31.10 0.593 0.942 0.377 0. 0.842 0.821 0.843 0.120 0.123 0.123 0.607 0.618 0.610 0.299 0.265 0.289 0.749 0.720 0.753 0.708 0.703 0. 0.476 0.462 0.476 Table 6. Evaluation on GenEval at 1024 1024 resolution. StableVS replaces the base solver in the low-variance regime [0, ξ], while keeping the high-variance regime [ξ, 1] unchanged. Numbers in parentheses (e.g., Euler(19)) denote the number of sampling steps in the low-variance regime. The split point is fixed to ξ = 0.85 for all models. Highlighted rows demonstrate that StableVS achieves comparable results to the 30-step baseline with fewer total sampling steps. Full results are reported in Tab. 10. Solver configuration Overall Reference metrics Base solver Solver in [0, ξ] Total steps PSNR SSIM LPIPS SD3.5-Large Euler DPM++ Flux-dev Euler Euler(19) Euler(13) StableVS(9) DPM++(19) DPM++(13) StableVS(9) Euler(19) Euler(13) StableVS(9) Qwen-ImageEuler Euler(22) Euler(12) StableVS(9) 30 20 20 30 20 20 30 20 20 30 17 0.723 0.710 0.723 0.724 0.717 0.719 0.660 0.659 0.666 0.733 0.721 0.731 16.93 36.92 17.42 32. 19.74 35.45 17.01 32.27 0.753 0.980 0.784 0.957 0.820 0.968 0.767 0. 0.333 0.021 0.287 0.063 0.244 0.025 0.277 0.031 prove integration accuracy, they treat the entire diffusion trajectory uniformly. Our StableVS departs from this view by exploiting the low-variance regime identified in our analysis, where the sampling dynamics become effectively deterministic. This regime-aware perspective enables aggressive step reduction without retraining, while remaining compatible with existing solvers in the high-variance regime. 6. Conclusion In this work, we develop variance-based perspective on stochastic interpolants and reveal two-regime structure that fundamentally governs both training and sampling dynamics. Our analysis shows that high-variance conditional velocity targets hinder optimization, whereas in the low-variance regime the conditional and true velocities coincide, yielding both stable supervision and predictable dynamics. Building on this insight, we introduce the Stable Velocity framework, comprising StableVM for unbiased variance-reduced training, VA-REPA for selectively applying auxiliary supervision, and StableVS for finetuning-free acceleration at inference. Extensive experiments on ImageNet 256 256 and large pretrained T2I and T2V models demonstrate consistent improvements in training stability and substantial sampling speedups without sacrificing sample quality. Beyond the specific methods introduced here, our results suggest that explicitly modeling variance structure along the generative Figure 5. Visual comparison across prompts on SD3.5 (Esser et al., 2024). Results are generated using the Euler solver with 30 and 20 steps, and with StableVS replacing Euler in the lowvariance regime, all under the same random seeds. Compared to the standard 20-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details. Additional qualitative comparisons are provided in Appendix H. our work provides variance-centric analysis that explicitly reveals two-regime structure in stochastic interpolants, and introduces an unbiased, variance-reducing training objective StableVM together with VA-REPA, unifying variance reduction and auxiliary supervision under single principle. Sampling Acceleration of Diffusion and Flow Models. Reducing the number of sampling steps is critical for practical deployment of diffusion and flow-based generative models. Existing approaches can be broadly categorized into training-required and training-free methods. Trainingrequired approaches leverage additional learning to enable few-step generation, including progressive distillation (Salimans & Ho, 2022; Sauer et al., 2024), consistency models (Song et al., 2023), inductive moment matching (Zhou et al., 2025), and mean flow models (Geng et al., 2025). Training-free approaches instead focus on improved numerical solvers for the reverse ODE, such as DDIM (Song et al., 2021a), DPM-Solver and its variants (Lu et al., 2022; 2025), and UniPC (Zhao et al., 2023). While these methods im8 Stable Velocity: Variance Perspective on Flow Matching trajectory provides principled foundation for designing more efficient training objectives and sampling algorithms in diffusion and flow-based generative models."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by Kuaishou Technology and also funded, in part, by the NSERC DG Grant (No. RGPIN2022-04636), the Vector Institute for AI, Canada CIFAR AI Chair, NSERC Canada Research Chair (CRC), NSERC Discovery Grants, and Google Gift Fund. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through the Digital Research Alliance of Canada alliance.can. ca, and companies sponsoring the Vector Institute www. vectorinstitute.ai/#partners, and Advanced Research Computing at the University of British Columbia. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant. 9 Stable Velocity: Variance Perspective on Flow Matching"
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. In 11th International Conference on Learning Representations, ICLR 2023, 2023. Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Anderson, B. D. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313 326, 1982. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators. 2024. URL https://openai.com/ research/video-generation-models-as -world-simulators. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models, 2025. URL https://arxiv.org/abs/2410.10733. Choi, J., Lee, J., Shin, C., Kim, S., Kim, H., and Yoon, S. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1147211481, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. Elvira, V. and Martino, L. Advances in importance sampling. arXiv preprint arXiv:2102.05407, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 10 Hesterberg, T. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2): 185194, 1995. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Huang, C.-W., Lim, J. H., and Courville, A. C. variational perspective on diffusion-based generative models and score matching. Advances in Neural Information Processing Systems, 34:2286322876, 2021. Jeha, P., Grathwohl, W., Andersen, M. R., Ek, C. H., and Frellsen, J. Variance reduction of diffusion models gradients with taylor approximation-based control variate. In ICML 2024 Workshop onStructured Probabilistic Inference & Generative Modeling, 2024. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, 2022a. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022b. Kingma, D. P. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. Krizhevsky, A. Learning multiple layers of features from tiny images. University of Toronto, 2009. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, 2019. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Stable Velocity: Variance Perspective on Flow Matching J. Lehmann, E. and Romano, Testing Statistical Hypotheses. Springer Texts in Statistics Series. Springer ISBN International Publishing, 2023. 9783030705800. URL https://books.google. com.hk/books?id=NIDzzwEACAAJ. Leng, X., Singh, J., Hou, Y., Xing, Z., Xie, S., and Zheng, L. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. Lin, X., He, J., Chen, Z., Lyu, Z., Dai, B., Yu, F., Qiao, Y., Ouyang, W., and Dong, C. Diffbir: Toward blind image restoration with generative diffusion prior. In European conference on computer vision, pp. 430448. Springer, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Lipman, Y., Havasi, M., Holderrieth, P., Shaul, N., Le, M., Karrer, B., Chen, R. T., Lopez-Paz, D., Ben-Hamu, H., and Gat, I. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024a. Lipman, Y., Havasi, M., Holderrieth, P., Shaul, N., Le, M., Karrer, B., Chen, R. T. Q., Lopez-Paz, D., Ben-Hamu, H., and Gat, I. Flow matching guide and code, 2024b. URL https://arxiv.org/abs/2412.06264. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Niedoba, M., Green, D., Naderiparizi, S., Lioutas, V., Lavington, J. W., Liang, X., Liu, Y., Zhang, K., Dabiri, S., Scibior, A., et al. Nearest neighbour score estimators for diffusion generative models. In Proceedings of the 41st International Conference on Machine Learning, pp. 3811738144, 2024. Owen, A. B. Monte Carlo theory, methods and examples. Stanford, 2013. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision, 2023a. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023b. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022. Progressive distillation for arXiv preprint Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016. Loshchilov, I. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pp. 122, 2025. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W. Generating images with sparse representations. In International Conference on Machine Learning, 2021. Singh, J., Leng, X., Wu, Z., Zheng, L., Zhang, R., Shechtman, E., and Xie, S. What matters for representation alignment: Global information or spatial structure? arXiv preprint arXiv:2512.10794, 2025. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 11 Stable Velocity: Variance Perspective on Flow Matching Song, Y., Durkan, C., Murray, I., and Ermon, S. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34: 14151428, 2021b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In International Conference on Machine Learning, pp. 3221132252. PMLR, 2023. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., ming Yin, S., Bai, S., Xu, X., Chen, Y., Chen, Y., Tang, Z., Zhang, Z., Wang, Z., Yang, A., Yu, B., Cheng, C., Liu, D., Li, D., Zhang, H., Meng, H., Wei, H., Ni, J., Chen, K., Cao, K., Peng, L., Qu, L., Wu, M., Wang, P., Yu, S., Wen, T., Feng, W., Xu, X., Wang, Y., Zhang, Y., Zhu, Y., Wu, Y., Cai, Y., and Liu, Z. Qwen-image technical report, 2025a. URL https://arxiv.org/abs/2508.02324. Sun, K., Huang, K., Liu, X., Wu, Y., Xu, Z., Li, Z., and Liu, X. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 84068416, 2025. Wu, G., Zhang, S., Shi, R., Gao, S., Chen, Z., Wang, L., Chen, Z., Gao, H., Tang, Y., Yang, J., et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025b. Xu, Y., Tong, S., and Jaakkola, T. Stable target field for reduced variance score estimation in diffusion models, 2023. URL https://arxiv.org/abs/2302. 00670. Yao, J., Wang, C., Liu, W., and Wang, X. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. Zheng, H., Nie, W., Vahdat, A., and Anandkumar, A. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. Zhou, L., Ermon, S., and Song, J. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. Tong, A., Fatras, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. Vincent, P. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., Nair, D., Paul, S., Liu, S., Berman, W., Xu, Y., and Wolf, T. Diffusers: State-of-theart diffusion models. URL https://github.com/ huggingface/diffusers. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, R. and He, K. Diffuse and disperse: Image generation with representation regularization, 2025. URL https: //arxiv.org/abs/2506.09027. Wang, Z., Cheng, S., Yueru, L., Zhu, J., and Zhang, B. wasserstein minimum velocity approach to learning unnormalized models. In International Conference on Artificial Intelligence and Statistics, pp. 37283738. PMLR, 2020. Wang, Z., Jiang, Y., Zheng, H., Wang, P., He, P., Wang, Z., Chen, W., Zhou, M., et al. Patch diffusion: Faster and more data-efficient training of diffusion models. Advances in neural information processing systems, 36: 7213772154, 2023. Wang, Z., Zhao, W., Zhou, Y., Li, Z., Liang, Z., Shi, M., Zhao, X., Zhou, P., Zhang, K., Wang, Z., Wang, K., and You, Y. Repa works until it doesnt: Early-stopped, holistic alignment supercharges diffusion training, 2025. URL https://arxiv.org/abs/2505.16792. 12 Stable Velocity: Variance Perspective on Flow Matching A. Preliminaries The probability flow ordinary differential equation (PF-ODE) for flow matching is defined as follows, dxt = vt(xt) dt (15) induces marginal distributions that match that of Eq. (1) for all time [0, 1]. In addition, there exists reverse stochastic differential equation (SDE) whose marginal pt(x) coincides with that of the PF-ODE in Eq. (15), but with an added diffusion term (Ma et al., 2024): dxt = vt(xt) dt 1 2 wtst(xt) dt + wt dwt, (16) where wt is standard Wiener process in backward time, xt log pt(xt) can be re-expressed using the velocity field (Ma et al., 2024): wt is the diffusion coefficient, and the score st(xt) = st(xt) = σ1 (αtvt(xt) α txt)/(α tσt αtσ t). (17) In diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), the forward process can be modeled as an Itˆo SDE: dx = (x, t) dt + g(t) dw, (18) where is the standard Wiener process, (, t) : Rd Rd is vector-valued function called drift coefficient of xt and g() : is scalar function known as the diffusion coefficient of xt. It gradually transforms the data distribution to known prior as time goes from = 0 to 1. With stochastic process defined in Eq. (18), Variance-Preserving (VP) diffusion model (Ho et al., 2020; Song et al., 2020; Karras et al., 2022a) implicitly define both αt and σt in Eq. (1) with an equilibrium distribution as prior. VP diffusion commonly chooses αt = cos( π 2 t), σt = sin( π 2 t). The diffusion model is trained to estimate the score of the marginal distribution at time t, xtpt(xt), via neural network. Specifically, the training objective is weighted sum of the denoising score matching (DSM) (Vincent, 2011): Et,q(x0),pt(xtx0)λt sθ(xt, t) st(xt x0)2 , min θ (19) where λt is positive weighting function, sθ(, ) : [0, 1] Rd Rd is time-dependent vector field parametrized as neural network with parameters θ, the conditional probability path pt(xt x0) = (xt αtx0, σ2 I), and the conditional score function st(xt x0) = xt log pt(xt x0). The Eq. (19) shares very similar form with CFM target in Eq. (4). Also, similar to Eq. (5), the objective in Eq. (19) admits closed-form minimizer (Song et al., 2020; Xu et al., 2023): θ(xt, t) = Ept(x0xt) [st(xt x0)] = st(xt) (20) Here, the marginal probability path pt(xt) is mixture of conditional probability paths pt(xt x0) that vary with data points x0, that is, (cid:90) pt(xt) = pt(xt x0)q(x0) dx0. (21) B. More Discussion on Related Works Representation Alignment and Training Acceleration. growing body of work shows that shaping intermediate representations can substantially accelerate the training of diffusion and flow-based generative models. REPA (Yu et al., 2024) introduces an auxiliary objective that aligns hidden states of diffusion transformers with features from pretrained visual encoders, yielding significant gains under limited training budgets. Several follow-up methods explore alternative alignment strategies and regularization mechanisms. Dispersive Loss (Wang & He, 2025) removes the need for external teachers by encouraging internal feature dispersion, providing lightweight, encoder-free form of representation regularization. HASTE (Wang et al., 2025) addresses over-regularization by restricting alignment to early training stages and disabling it later for improved stability. REG (Wu et al., 2025b) departs from explicit alignment altogether, instead entangling noisy latents with compact semantic tokens throughout the denoising trajectory. REPA-E (Leng et al., 2025) further extends REPA 13 Stable Velocity: Variance Perspective on Flow Matching by jointly fine-tuning both the VAE and diffusion model, substantially improving performance at the cost of increased training complexity. More recently, iREPA (Singh et al., 2025) demonstrates that the effectiveness of representation alignment is driven primarily by spatial structure rather than high-level semantics, enhancing REPA through spatially aware projections and normalization. Our variance-aware perspective is orthogonal to these approaches: rather than modifying the form or source of alignment, we identify when semantic supervision is well-defined along the generative trajectory. As result, VA-REPA can be naturally combined with existing alignment methods to further improve training efficiency and stability. Variance Reduction for Diffusion and Flow Models. Several works have explored variance reduction for diffusion models through importance sampling over timesteps, primarily aiming to reduce the variance of the diffusion ELBO and empirically improving training efficiency (Huang et al., 2021; Song et al., 2021b). Alternative approaches leverage control variates instead of importance sampling to stabilize training objectives (Wang et al., 2020; Jeha et al., 2024). More closely related to our work, recent studies employ self-normalized importance sampling (SNIS) estimators (Hesterberg, 1995) to reduce the variance of score estimation (Xu et al., 2023; Niedoba et al., 2024). These methods achieve variance reduction by aggregating multiple conditional scores, but typically introduce bias due to self-normalization. In contrast, StableVM focuses on different source of variance that arises in flow-matching objectives, namely the variability induced by individual reference pairs (x0, ϵ) during target construction. We reduce this variance by aggregating reference samples into composite conditional formulation, which enables variance reduction while preserving unbiasedness. As result, StableVM complements existing diffusion-based variance reduction techniques and is particularly effective in settings where variance originates from reference-level stochasticity rather than timestep sampling. C. Comparison with Stable Target Field The standard training objective for diffusion models (Ho et al., 2020; Song et al., 2020; Karras et al., 2022b) is based on denoising score matching (DSM) (Vincent, 2011), also suffers from high variance. To address this issue, Xu et al. (2023) proposed the Stable Target Field (STF), which stabilizes training by leveraging reference batch = {xi i=1 q(x0). The STF objective is defined as 0}n LSTF(θ, t) = {xi 0}n i=1q(x0), pt(xtx1 0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vθ(xt, t) (cid:88) k= pt(xt xk 0) j=1 pt(xt xj 0) (cid:80)n st(xt xk 0) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) . (22) Unlike DSM (n = 1), STF forms weighted average of scores over the reference batch, with weights determined by the conditional likelihoods pt(xt xk 0). This reduces the covariance of the target by factor of n, thereby lowering variance. While STF introduces bias, the minimizer of LSTF is given by v(xt, t) = p(x0xt), {xi 0}n i=2q(x0) (cid:88) k=1 pt(xt xk 0) j=1 pt(xt xj 0) (cid:80)n st(xt xk 0), (23) which deviates from the true score st(xt). However, as , this bias vanishes and the weighted estimator converges to the true score. Our approach differs from STF in three important ways: 1. General framework. We extend the variance analysis and variance-reduction strategy to the flow matching as well as stochastic interpolant framework, which generalizes beyond VP diffusion and exhibits distinct variance structure. 2. Unbiased objective. Instead of relying on finite-sample weighted average, we propose mixture of conditional probabilities that eliminates bias while still achieving variance reduction. 3. Class-conditional extension. While STF does not naturally extend to class-conditional settings, we design tailored algorithm that maintains variance reduction under classifier-free guidance, improving both convergence and training efficiency. To further elucidate these differences, we evaluate unconditional generation on CIFAR-10 (Krizhevsky, 2009). For fair comparison, both STF and StableVM use the same reference batch size = 2048. All metrics are computed over 50K generated samples. 14 Stable Velocity: Variance Perspective on Flow Matching Table 7. Unconditional CIFAR-10 generation. Performance comparison between CFM, STF, and StableVM. STF instantiated directly from Eq. 22 performs poorly, whereas StableVM achieves faster convergence and better sample quality via unbiased variance reduction. All results use = 2048 and 50k generated samples. Iter 30k 50k 200k Model FID IS sFID Prec. Rec. CFM STF (Eq. 22) STF (original impl.) StableVM CFM STF (Eq. 22) STF (original impl.) StableVM CFM STF (Eq. 22) STF (original impl.) StableVM 10.76 38.37 9.91 9.31 7.50 29.18 7.07 6.87 3.58 13.50 3.93 3. 7.97 6.02 8.01 8.02 8.30 6.59 8.24 8.38 9.06 7.69 8.92 9.05 4.55 9.94 4.51 4.62 4.25 7.65 4.22 4.34 3.94 4.88 4.06 3. 0.58 0.52 0.58 0.59 0.60 0.52 0.60 0.61 0.65 0.56 0.65 0.65 0.57 0.45 0.58 0.57 0.59 0.50 0.59 0.59 0.61 0.57 0.61 0. Figure 6. Qualitative comparison of generated samples on CIFAR-10. Samples generated by the checkpoint at 50k iterations (a) CFM, (b) STF instantiated directly from Eq. 22, (c) the original STF implementation, and (d) our StableVM. StableVM produces sharper and more coherent samples, consistent with its improved convergence and variance reduction. In this experiment, the STF baseline strictly follows Eq. 22: the reference batch = {xi i=1 is sampled from the data distribution, and the noisy input xt is generated by applying the forward process to the single reference sample x1 0. While this matches the theoretical formulation in the original paper, it differs from the implementation used by Xu et al. (2023), which instead applies noise to the first samples in the reference batch and treats them as training inputs. This modification yields near-unbiased estimator, since xt is no longer conditioned on single reference point but effectively drawn from composite distribution over multiple samples. This behavior closely resembles the composite conditional distribution (cid:1) introduced in Sec. 3.1. By contrast, StableVM explicitly samples xt from Gaussian mixture model pGMM constructed over the reference batch, ensuring unbiased targets while reducing the variance. (cid:0)xt {xi 0}n 0}n i= As shown in Tab. 7 and Fig. 6, STF instantiated directly from Eq. 22 underperforms even standard CFM, whereas StableVM consistently accelerates convergence and improves sample quality. While the original STF implementation performs substantially better than its direct instantiation from Eq. 22, yet still lags behind StableVM, this comparison highlights key distinction: STFs empirical gains arise from an implicit deviation from its theoretical objective, whereas StableVM is explicitly designed to be unbiased, variance-reduced, and readily extensible to flow-matching settings. 15 Stable Velocity: Variance Perspective on Flow Matching D. Algorithms For clarity of presentation, we provide the detailed training procedures of StableVM in this appendix. The main text focuses on the variance-driven formulation and theoretical properties, while Algorithm 1 summarizes the core StableVM training loop. We further extend StableVM to the class-conditional setting with classifier-free guidance, with the complete procedure given in Algorithm 2. StableVM introduces additional computation and memory overhead that scales linearly with the bank capacity, i.e. O(K) . Specifically, the self-normalized target requires computing weighted average over the reference batch for each class, and in class-conditioned generation, the memory bank stores latent representations per class. For example, in ImageNet generation, storing 256 latents for each of the 1,000 classes in fp16 precision requires approximately 2 GB of additional memory. The associated computation incurs only minor latency, which is negligible compared to the models forward and backward passes. Algorithm 1 Stable Velocity Matching Require: Training iteration , initial model vθ, dataset D, learning rate η 1: for iter = 1 . . . do Sample batch {xi 0}n 2: Uniformly sample time qt(t) from [0, 1] 3: Sample perturbed batch {xj 4: i=0 from }M j=1 from pGMM 5: Calculate stable vector field for all xj : (xj {xi 0}n i=0) = (cid:88) i=0 1 pt(xj xi 0) ˆvStableVM(xj ; {xi 0}n i=0) := (cid:80)n k=1 pt(xt xk 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:80)n 6: Calculate loss: L(θ) = 1 (cid:88) j= Update model: θ θ ηL(θ) 7: 8: end for 9: Return vθ λ(t) vθ(xj , t) ˆvStableVM(xj ; {xi 0}n i=0)2 16 Stable Velocity: Variance Perspective on Flow Matching Algorithm 2 Stable Velocity Matching with Classifier-free Guidance Input: training iterations , model vθ, dataset D, learning rate η, batch size B, number of classes C, per-class bank capacity K, CFG dropout probability pcfg Initialize memory bank = {Mc}C for = 0, . . . , do c=0, where c=0, . . . , C1 denote class-conditional banks and c=C is the unconditional bank Mc prefilled FIFO queue of capacity end for for iter = 1 . . . do Sample times {ti}B Uniformly sample labels {yi}B Set yi with probability pcfg for = 1 . . . do i=1 qt([0, 1]) Sample perturbed sample xi ti from i=1 from {0, . . . , C1} pGMM(xi ti Myi ) = 1 Myi (cid:88) pt(xi ti xref 0 ) xref 0 yi Compute stable field end for Compute loss vM yi (xi ti ) = (cid:88) xref 0 yi pt(xi ti xref 0 ) pt(xi ti yref 0 yi (cid:80) yref 0 ) vt(xi ti xref 0 ) L(θ) = 1 (cid:88) i=1 Update parameters θ θ ηθL(θ) Sample = {(xi 0, yi)}B for = 1 . . . do i=1 from Push xi Push xi 0 into Myi (evict oldest if full) 0 into MC (unconditional bank) λ(ti)(cid:13) (cid:13)vθ(xi ti , ti, yi) vM yi (xi ti )(cid:13) (cid:13) 2 end for end for Output: trained velocity field vθ E. Proofs E.1. pGMM Posterior Proposition E.1. The posterior pGMM (cid:0)(cid:8)xi 0 (cid:9)n i=1 xt (cid:1) = 1 (cid:80)n i=1 (cid:16) pt (cid:0)xi 0 xt (cid:1) (cid:81) (cid:17) j=i q(xj 0) . Proof. This is simple application of the Bayes rule. Note that we have pGMM (cid:16)(cid:8)xi (cid:9)n i=1 xt (cid:17) = pGMM (cid:0)xt (cid:8)xi (cid:9)n (cid:1) (cid:81)n i=1 q(xi 0) 0 i=1 pt(xt) = 1 pt(xt) (cid:32) (cid:88) i=1 1 pt(xt xi 0) (cid:33) (cid:32) (cid:89) (cid:33) q(xi 0) (24) = = 1 1 (cid:88) i=1 (cid:88) i=1 as desired. pt(xt xi 0) pt(xt) (cid:89) j=1 i=1 q(xj 0) pt(xi 0 xt) q(xj 0) , (cid:89) j=i 17 Stable Velocity: Variance Perspective on Flow Matching E.2. Proof of Unbiasedness (Theorem 3.1) Theorem 3.1.(a) The StableVM target is unbiased. That is, for any xt, we have {xi 0}pGMM (cid:104) (cid:98)vStableVM(xt; {xi (cid:105) i=0) 0}n (xt) = vt(xt). (b) The global minimizer v(xt, t) of the StableVM objective LStableVM is the true velocity field vt(xt). Proof. (a) Using Eq. (24), we obtain {xi 0}n i=1 pGMM (xt) (cid:34) (cid:88) k= (cid:32) (cid:88) i=1 (cid:90) (cid:32) (cid:89) (cid:90) = 1 pt(xt) = = 1 pt(xt) 1 pt(xt) (cid:88) k=1 = 1 pt(xt) = 1 pt(xt) (cid:88) (cid:90) k= (cid:88) k=1 i=k pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:33) (cid:33) (cid:32) (cid:89) q(xi 0) pt(xt xi 0) i=1 (cid:35) (cid:88) k= pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:33) dx1:n 0 (cid:33) (cid:32) (cid:88) q(xi 0) pt(xt xk 0)vt(xt xk 0) dx1:n 0 i=1 (cid:90) (cid:32) (cid:89) k=1 (cid:33) q(xi 0) pt(xt xk 0)vt(xt xk 0) dx1:n 0 i=1 (cid:89) i=k pt(xt, xk q(xi 0) 0)vt(xt xk 0) dx1:n 0 (cid:90) (cid:89) q(xi 0) dxi 0 (cid:90) pt(xt, xk 0)vt(xt xk 0) dxk 0 = 1 (cid:88) (cid:90) k=1 as desired. pt(xk 0 xt)vt(xt xk 0) dxk 0 = vt(xt), (b) Recall the StableVM objective Eq. (8) LStableVM(θ, t) = xtpt, {xi 0}n i=1 pGMM (xt) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vθ(xt, t) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 = Extpt (cid:90) pGMM (cid:16)(cid:8)xi 0 (cid:9)n i=1 xt (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vθ(xt, t) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) dx1:n 0 . For each xt, let Lxt(v) := (cid:90) pGMM (cid:16)(cid:8)xi 0 (cid:9)n i=1 xt (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) dx1:n 0 . It suffices to show that for each xt, setting to vt(xt) above minimizes Lxt. Note that Lxt is differentiable and strictly 18 Stable Velocity: Variance Perspective on Flow Matching convex in v, so the minimizer must satisfy Lxt(v) = 0. It follows that (cid:90) (cid:90) 0 = 2 = 2 pGMM (cid:16)(cid:8)xi 0 (cid:9)n i=1 xt pGMM (cid:16)(cid:8)xi (cid:9)n i=1 xt (cid:17) (cid:17) (cid:32) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:33) dx1:n 0 dx1:n 0 (cid:90) 2 pGMM (cid:16)(cid:8)xi (cid:9)n i=1 xt (cid:17) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) dx1:n = 2v 2vt(xt), where we have used part (a) in the last step. Therefore, vt(xt) is the unique minimizer of Lxt. This finishes the proof. E.3. Proofs of the Variance Bounds (Theorem 3.2 and Theorem 3.3) In this section, we prove the variance reduction bound of our StableVM target as in Eq. (9). We first recall that the StableVM target is the following estimator (cid:98)vt := (cid:88) k=1 pt(xt xk (cid:80)n 0) vt(xt xk 0) j=1 pt(xt xj 0) Rd, (25) where xt pt and (cid:8)xi We first prove the weaker version of the variance bound. Theorem 3.2. Fix [0, 1]. We always have VStableVM(t) VCFM(t). (cid:9)n i=1 pGMM ( xt). 0 Proof. We have VStableVM(t) = {xi 0}n i=1qn xtpGMM ({xi 0}n i=1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vt(xt) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) pt(xt xi 0) vt(xt) (cid:88) k=1 pt(xt xk (cid:80)n 0)vt(xt xk 0) j=1 pt(xt xj 0) dxt (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) pt(xt xi 0) i=1 k= pt(xt xk 0) j=1 pt(xt xj 0) (cid:80)n (cid:13) (cid:13)vt(xt) vt(xt xk 0)(cid:13) 2 (cid:13) dxt (cid:35) (cid:33) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:33) (cid:88) = {xi 0}n i=1qn {xi 0}n i=1qn = {xi 0}n i=1qn (cid:32) (cid:88) i=1 (cid:32) (cid:88) (cid:90) 1 (cid:34)(cid:90) 1 (cid:34)(cid:90) 1 (cid:34) (cid:88) (cid:88) k=1 (cid:90) = = = 1 1 1 (cid:88) k= VCFM(t) = VCFM(t), pt(xt xk 0) (cid:13) (cid:13)vt(xt) vt(xt xk 0)(cid:13) 2 (cid:13) (cid:35) dxt pt(xt xk 0) (cid:13) (cid:13)vt(xt) vt(xt xk 0)(cid:13) 2 (cid:13) (cid:35) dxt {xi 0}n i=1qn k=1 (cid:34) (cid:88) k= xtpt(xk 0 ) (cid:13) (cid:13)vt(xt) vt(xt xk 0)(cid:13) 2 (cid:13) (cid:35) {xi 0}n i=1qn where the inequality is by Jensens inequality and the convexity of 2. Before proceeding to proving the stronger bound, we first give an alternative interpretation of the posterior pGMM (cid:1). Consider the following procedure conditioned on xt: (cid:0)(cid:8)xi 0 (cid:9)n i=1 xt 19 Stable Velocity: Variance Perspective on Flow Matching 1. Sample latent index uniformly from {1, . . . , n}. 2. Sample xI 0 pt( xt) and xj 0 for all = I. We claim the following: Lemma E.2. (a) The joint distribution of (cid:8)xi (cid:1). (cid:0)(cid:8)xi (cid:9)n i=1 xt pGMM 0 0 (cid:9)n i=1 sampled from the above procedure conditioned on xt is exactly (b) (cid:8)xi 0 (cid:9)n i=1 are independent conditioned on xt and I. Proof. (a) Note that the joint distribution of (cid:8)xi 0 (cid:9)n i=1 sampled from the above procedure is 1 (cid:88) i= pt(xi 0 xt) q(xj 0) , (cid:89) j=i which matches the joint distribution given by the posterior of pGMM . (b) This is clear by construction. The following lemmas compute the variance of the StableVM estimator step-by-step. Lemma E.3. We have where the expectation on the right-hand side is over the random variable I. Cov ((cid:98)vt xt) = [Cov ((cid:98)vt xt, I) xt] , Proof. By the law of total covariance, we have Cov ((cid:98)vt xt) = EI (cid:2)Cov(cid:0) (cid:98)vt xt, I(cid:1) xt (cid:3) + CovI (cid:0)E(cid:2) (cid:98)vt xt, I(cid:3) xt (cid:1) . We claim that E(cid:2) (cid:98)vt xt, I(cid:3) does not depend on I, so the second term above would be 0. Let [n]. We have E(cid:2) (cid:98)vt xt, = i(cid:3) = (cid:90) pt(xi 0 xt) q(xj 0) (cid:89) j=i (cid:88) k=1 pt(xt xk (cid:80)n 0) vt(xt xk 0) j=1 pt(xt xj 0) dx1:n 0 . Let π : Rnd Rnd that swaps x1 det Dπ = 1 since the Jacobian Dπ is permutation matrix. Then by the change of variable formula, we have 0, i.e. π(x1 0 ) = (xi 0, . . . , xn 0, . . . , xn 0, . . . , x1 0, . . . , xi 0 and xi 0 ). Note that we have E(cid:2) (cid:98)vt xt, = i(cid:3) = (cid:90) pt(x 0 xt) q(xj 0) (cid:89) j=1 (cid:88) k=1 = E(cid:2) (cid:98)vt xt, = 1(cid:3). pt(xt xk (cid:80)n 0) vt(xt xk 0) j=1 pt(xt xj 0) dx1:n 0 This shows that CovI (cid:0)E(cid:2) (cid:98)vt xt, I(cid:3) xt (cid:1) = 0, which finishes the proof. For the next two lemmas, we fix ℓ [d] and only consider the ℓth component (cid:98)v(ℓ) (cid:88) (cid:88) pt(xt xk 0)v(ℓ) (xt xk 0), Pn1 := Vn1 := for simplicity. Define pt(xt xk 0), k=i k=i vi := pt(xt xi 0)v(ℓ) (xt xi 0), pi := pt(xt xi 0). 20 Stable Velocity: Variance Perspective on Flow Matching"
        },
        {
            "title": "Then we have",
            "content": "(cid:98)v(ℓ) = Vn1 + vi Pn1 + pi . Note that conditioned on xt and = i, the random variables xk by Lemma E.2. 0 for = are all independent and follow the data distribution Let us also first compute some expectations that will be useful later. We have for = i, (cid:2)pt(xt xk 0) xt, = i(cid:3) = (cid:90) pt(xt xk 0)q(xk 0) dxk 0 = (cid:90) pt(xt, xk 0) dxk 0 = pt(xt), (cid:2)pt(xt xk 0)vt(xt xk 0) xt, = i(cid:3) = = (cid:90) (cid:90) pt(xt xk 0)vt(xt xk 0)q(xk 0) dxk 0 pt(xt, xk 0)vt(xt xk 0) dxk = pt(xt) = pt(xt) (cid:90) (cid:90) pt(xt, xk 0) pt(xt) vt(xt xk 0) dxk pt(xk 0 xt)vt(xt xk 0) dxk 0 Ex0pt(xt) [vt(xt x0) xt] = (cid:90) pt(x0 xt) vt(xt x0) dx0 = vt(xt), = pt(xt)vt(xt), and Ex0q (cid:20) pt(x0 xt) q(x0) (cid:21) (cid:90) = pt(x0 xt) dx0 = 1. We then continue with the lemmas. Lemma E.4. As , we have (cid:18) Vn1 Pn1 (cid:18) 0, Ex0pt(xt) (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21)(cid:19) , v(ℓ) (cid:19) (xt) (cid:20) pt(xt x0) pt(xt) where the random variables Vn1 and Pn1 are conditioned on xt and = i. Proof. Write Then we have (x0) := v(ℓ) (xt x0), w(x0) := pt(x0 xt) q(x0) . Vn1 Pn1 = = = (cid:80) (cid:80) (cid:80) k=i pt(xt xk (cid:80) pt(xtxk 0) v(ℓ) (xt xk 0) k=i pt(xt xk 0) 0 ) q(xk 0 ) 0 ) (xk 0) 0 ) q(xk 0 ) pt(xt) q(xk 0 ) 0) (xk 0) k=i w(xk 0) pt(xt) q(xk pt(xtxk = . k=i k=i w(xk (cid:80) k=i (cid:80) (cid:80) = (cid:80) k=i (cid:80) k=i pt(xt xk (cid:80) 0) (xk 0) k=i pt(xt xk 0) pt(xk 0 xt) (xk 0) q(xk 0 ) pt(xk 0 xt) q(xk 0 ) k=i (26) (27) (28) (29) (30) Recall from Lemma E.2 that conditioned on xt and = i, the xi 0s are all independent. Therefore, Eq. (30) is selfnormalized importance sampling estimator (Chapter 9 of (Owen, 2013)) with importance distribution q(x0), nominal distribution pt(x0 xt), and importance weight ratio w(x0). Stable Velocity: Variance Perspective on Flow Matching Note that by Eq. (28) and Eq. (29), we have Ex0pt(xt)[f (x0)] = v(ℓ) (xt), Ex0q[w(x0)] = 1. Following results using the delta method as in (Lehmann & Romano, 2023) (Theorem 11.2.14) and (Owen, 2013) (Eq. (9.8)), we have that 1 (cid:18) Vn1 Pn1 (cid:19) v(ℓ) (xt) (cid:18) 0, Ex0q (cid:20) w(x0)2 (cid:16) (x0) v(ℓ) (xt) (cid:17)2(cid:21)(cid:19) ."
        },
        {
            "title": "Expanding the variance term above gives us",
            "content": "f (x0) v(ℓ) (xt) (cid:17)2(cid:21) (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17) dx0 Ex0q = Ex0q (cid:20) w(x0)2 (cid:16) (cid:20) pt(x0 xt)2 q(x0)2 (cid:16) (cid:90) pt(x0 xt)2 q(x0) (cid:90) pt(x0 xt) = = = = (cid:90) 1 pt(xt) 1 pt(xt) 1 pt(xt) pt(x0 xt) pt(xt) q(x0) (cid:16) (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2 dx pt(x0 xt) pt(xt x0) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2 dx0 Ex0pt(xt) (cid:20) pt(xt x0) (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) , as desired. Lemma E.5. Assume (cid:0)(n 1)(cid:98)vt vt(xt)2(cid:1) n=1 is uniformly integrable. Then we have Var (cid:16) (cid:98)v(ℓ) (cid:17) xt, = = 1 1 Ex0pt(xt) (cid:20) pt(xt x0) pt(xt) (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) + (cid:19) (cid:18) 1 for large n. Proof. Let g(x, y) = x/y so that (cid:98)v(ℓ) (Vn1, Pn1) gives us = g(Vn1 + vi, Pn1 + pi). Performing Taylor expansion of at the point (cid:98)v(ℓ) = g(Vn1, Pn1) + g(Vn1 + λn1vi, Pn1 + λn1pi) (cid:21) (cid:20)vi pi = Vn1 Pn1 + vi Pn1 + λn1pi Vn1 + λn1vi (Pn1 + λn1pi)2 pi for some [0, 1]-valued random variable λn1. It follows that (cid:16) 1 = 1 (cid:98)v(ℓ) v(ℓ) (cid:18) Vn1 Pn1 (xt) (cid:17) (cid:19) v(ℓ) (xt) 1 vi Pn1 + λn1pi + 1 Vn1 + λn1vi (Pn1 + λn1pi)2 pi (31) For the second term in Eq. (31), we first note that by the law of large numbers and Eq. (26), we have Pn1 pt(xt) as n1 . Also note that λn1 pt(xt). We also have n1 0 as deterministically. It follows that 0 deterministically since vt is bounded. Hence, we have 1 n1 (Pn1 + λn1pi) vi n1 1 vi Pn1 + λn1pi = 1 vi n1 1 n1 (Pn1 + λn1pi) 0 Stable Velocity: Variance Perspective on Flow Matching by Slutskys theorem, so the second term of Eq. (31) converges to 0 in probability. For the third term in Eq. (31), we have by the law of large numbers and Eq. (27), we have Vn1 n1 deterministically. We also have pi n1 (n1)2 (Pn1 + λn1pi)2 pt(xt)v(ℓ) pt(xt)2 by Slutskys theorem. At the same time, (xt). Since vt is bounded, we have λn1vi n1 0 0 deterministically. Therefore, we get that 1 (n 1)3/2 (Vn1 + λn1vi) pi = pi (cid:18) Vn1 1 + (cid:19) λn1vi 1 (cid:17) 1 (cid:16) 0 pt(xt)v(ℓ) (xt) + 0 = 0, by Slutskys theorem. Then by Slutskys theorem again, we have 1 Vn1 + λn1vi (Pn1 + λn1pi)2 pi = 1 (n1)3/2 (Vn1 + λn1vi) pi (n1)2 (Pn1 + λn1pi)2 0, so the third term in Eq. (31) also converges to 0 in probability. Therefore, combining Lemma E.4 and the results above using Slutskys theorem, we conclude that (cid:16) v(ℓ) (cid:98)v(ℓ) (xt) (cid:18) (cid:17) 0, Ex0pt(xt) (cid:20) pt(xt x0) pt(xt) (cid:16) Then by the uniform integrability assumption, we have that (cid:98)v(ℓ) has variance (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21)(cid:19) . 1 1 Ex0pt(xt) (cid:20) pt(xt x0) pt(xt) (cid:16) as desired. (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) + (cid:19) , (cid:18) 1 We are now ready to state and prove the main theorem. Theorem 3.3. Fix [0, 1]. Let vt be bounded. Assume (cid:0)(n1)(cid:98)vt vt(xt)2(cid:1) Assume (cid:90) := Ex0pt(xt) vt(xt x0) vt(xt)2(cid:105) (cid:104) dxt < . n=1 is uniformly integrable. Let ε (0, 1). Then, for large enough n, we have {x:pt(x)ε} VStableVM(t) 1 (cid:18) 1 ε (cid:19) VCFM(t) + + (cid:19) . (cid:18) 1 Proof. Recall from Eq. (9) that we have (cid:104) VStableVM(t) = (cid:104) (cid:104) = = (cid:105) Tr Cov ((cid:98)vt xt) (cid:104) (cid:16) Tr (cid:104) Cov ((cid:98)vt xt, I) xt (cid:105)(cid:105) (cid:98)vt xt, I(cid:1) xt Tr Cov(cid:0) , (cid:105)(cid:17)(cid:105) where the second line follows from Lemma E.3, and the third line follows from the linearity of expectation. Note that the inner expectation is over the random variable I, and the outer expectation is over the random variable xt. From Lemma E.5, we have Var (cid:16) (cid:98)v(ℓ) (cid:17) xt, = = 1 1 Ex0pt(xt) (cid:20) pt(xt x0) pt(xt) (cid:16) 23 (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) + (cid:19) . (cid:18) 1 Stable Velocity: Variance Perspective on Flow Matching"
        },
        {
            "title": "Then",
            "content": "Tr Cov(cid:0) (cid:98)vt xt, = i(cid:1) = (cid:88) ℓ="
        },
        {
            "title": "Var",
            "content": "(cid:16) (cid:98)v(ℓ) (cid:17) xt, = (cid:88) ℓ=1 = = 1 1 1 1 Ex0pt(xt) (cid:20) pt(xt x0) pt(xt) (cid:20) pt(xt x0) pt(xt) Ex0pt(xt) vt(xt x0) vt(xt)2 (cid:16) (xt x0) v(ℓ) v(ℓ) (xt) (cid:17)2(cid:21) + (cid:19) (cid:18) 1 (cid:21) + (cid:19) . (cid:18) 1 Since does not appear in the last line above, we get VStableVM(t) = 1 1 1 Extpt Extpt (cid:20) Ex0pt(xt) (cid:20) (cid:20) pt(xt x0) pt(xt) (cid:104) Ex0pt(xt) 1 pt(xt) vt(xt x0) vt(xt)2 vt(xt x0) vt(xt)2(cid:105)(cid:21) + (cid:21)(cid:21) + (cid:19) (cid:18) 1 (cid:19) , (cid:18) 1 where we have used pt(xt x0) 1 in the second line. Now we fix some ε > 0. Define P>ε := {x Rd : pt(x) > ε} and Pε := Rd P>ε. Then we have VStableVM(t) 1 = 1 1 (cid:90) Ex0pt(xt) (cid:104) vt(xt x0) vt(xt)2(cid:105) Rd (cid:90) Ex0pt(xt) (cid:104) pt(xt) vt(xt x0) vt(xt)2(cid:105) P>ε (cid:90) pt(xt) vt(xt x0) vt(xt)2(cid:105) (cid:104) Ex0pt(xt) pt(xt) dxt + (cid:19) (cid:18) 1 pt(xt) dxt dxt + (cid:19) (cid:18) 1 ε (cid:90) P>ε (cid:90) Rd + 1 1 1 ε(n 1) 1 ε(n 1) 1 ε(n 1) (cid:18) 1 ε 1 = = as desired. Ex0pt(xt) (cid:104) vt(xt x0) vt(xt)2(cid:105) pt(xt) dxt + 1 + Ex0pt(xt) (cid:104) vt(xt x0) vt(xt)2(cid:105) pt(xt) dxt + (cid:19) (cid:18) 1 (cid:19) (cid:18) 1 + M 1 (cid:18) 1 (cid:19) + 1 + Extpt,x0pt(xt) (cid:19) VCFM(t) + + (cid:104) vt(xt x0) vt(xt)2(cid:105) (cid:18) 1 (cid:19) , We note that we can choose suitable ε in practice such that the set {x : pt(x) ε} is small (in the sense that it has Lebesgue measure close to 0), which would lead to being close to 0. E.4. Simulating the Reverse SDE in Low-Variance Regime Ma et al. (2024) show that the reverse-time SDE (Eq. (16)) with score function st(x) = log pt(x) and arbitrary diffusion strength wt 0 yields the correct marginal density pt(x) at each time t. Furthermore, as established in Anderson (1982); Ma et al. (2024), if xt pt(x), then the reverse-time solution xτ at any τ [0, t] is distributed according to the posterior: pτ (xτ xt) = Ept(x0xt) [pτ (xτ x0, xt)] pτ (xτ x0, xt). (32) Proposition E.6. Let xt (αtx0, σ2 sample. For any fixed variance parameter β I) and xτ (ατ x0, σ2 τ I), where τ < t, and x0 p(x0) is the clean data (0, σ2 τ ), define the posterior distribution as τ (xτ xt, x0) = (ktxt + λtx0, β2 pαt I), 24 then the coefficients Stable Velocity: Variance Perspective on Flow Matching (cid:115) kt = τ β2 σ2 σ2 , λt = ατ αt (cid:115) τ β2 σ2 σ2 guarantee that the marginal of xτ is (ατ x0, σ2 τ I). Proof. We begin by expressing xt using the forward diffusion process: We define the reverse model as Gaussian conditional: xt = αtx0 + σtϵ, ϵ (0, I). xτ = ktxt + λtx0 + η, η (0, β I). Substituting xt yields: xτ = kt(αtx0 + σtϵ) + λtx0 + η = (ktαt + λt)x0 + ktσtϵ + η. Hence, the conditional distribution of xτ given x0 is: xτ x0 (cid:0)(ktαt + λt)x0, (k2 σ + β2 )I(cid:1) . To match the desired marginal xτ (ατ x0, σ2 τ I), we require: ktαt + λt = ατ , = σ2 σ2 k2 τ . + β Solving the second equation above for kt, we obtain: Substituting into first equation, we get: (cid:115) kt = τ β2 σ2 σ2 . λt = ατ αt (cid:115) τ β2 σ2 σ2 . Thus, the choice of kt and λt ensures that the conditional distribution of xτ is consistent with the marginal. Within this low variance area, we also have vt(xt) vt(xt x0) = σ σt (xt αtx0) + α tx0, Thus, given the velocity field vt(xt) and the current state xt, the target x0 can be extracted as: x0 = vt(xt) σ σt σ α αt σt xt (33) (34) Plugging in this equation into the original expression, thus the posterior distribution with x0 eliminated via vt(xt), is given by: pτ (xτ xt, vt(xt)) = (cid:0)µτ t, β2 I(cid:1) where the posterior mean is explicitly: (cid:115) µτ = τ β2 σ2 σ2 (cid:32) ατ αt (cid:115) (cid:33) τ β2 σ2 σ2 αt (cid:32) xt + ατ αt (cid:115) (cid:33) τ β2 σ2 σ2 vt(xt) σ σt α αt σ σt σ σt α 25 Stable Velocity: Variance Perspective on Flow Matching Assuming αt = 1 and σt = t, the DDIM-style posterior becomes: pτ (xτ xt, vt(xt)) = (cid:0)µτ t, β2 I(cid:1) with mean: µτ = (cid:32)(cid:114) τ 2 β2 (cid:32) + (1 τ ) (1 t) (cid:114) τ 2 β2 t2 (cid:33)(cid:33) (cid:32) xt (1 τ ) (1 t) (cid:33) (cid:114) τ 2 β2 tvt(xt) If we set βt = 0, we obtain the deterministic sampler: xτ = xt + (τ t)vt(xt) E.5. Explicit PF-ODE Solution in Low-Variance Regime In low-variance regime (0 ξ), the conditional velocity field simplifies as vt(xt) vt(xt x0). We can thus derive explicit solutions to the Probability Flow ODE (PF-ODE) under both the stochastic interpolant and VP diffusion frameworks. We consider the Probability Flow ODE (PF-ODE) under the stochastic interpolant framework: dxt dt = vt(xt) vt(xt x0) = σ σt (xt αtx0) + α tx0, (35) where αt and σt define stochastic interpolant, and x0 is the data point to be matched. Closed-form of the Target x0 Given the velocity field vt(xt) and the current state xt, the target x0 can be extracted as: x0 = where we define the coefficient vt(xt) σ σt σ α αt σt xt vt(xt) σ σt Ct xt , = (36) Ct := α σ σt αt. Solving the PF-ODE from to 0 τ < We aim to integrate the PF-ODE backward in time from known terminal state xt. The PF-ODE can be written as: dxt dt + a(t)xt = b(t), where a(t) = σ σt , b(t) = Ctx0. This is linear nonhomogeneous first-order ODE. The integrating factor is: (cid:90) σ σt µ(t) = exp = exp a(t)dt (cid:18)(cid:90) (cid:19) (cid:18) (cid:19) dt = 1 σt . Multiplying both sides by µ(t) yields: Integrating both sides from to τ < t: which gives: where (cid:19) dt (cid:18) xt σt = Ct σt x0. xτ στ = xt σt + (cid:90) τ C(s) σs ds x0, xτ = στ (cid:18) xt σt (cid:19) + I(t, τ ) x0 , I(t, τ ) := C(s) σs ds. (cid:90) τ 26 (37) (38) (39) Stable Velocity: Variance Perspective on Flow Matching Substituting x0 in Closed Form We now substitute the expression for x0 evaluated at time t: Substitute this into the solution: xτ = στ = στ x0 = vt(xt) σ σt Ct xt . + I(t, τ ) vt(xt) σ σt Ct xt (cid:19) σ σt I(t, τ ) Ct xt + I(t, τ ) Ct (cid:21) vt(xt) . xt σt (cid:20)(cid:18) 1 σt Final Expression (Only in Terms of xt) xτ = στ (cid:20)(cid:18) 1 σt σ σt I(t, τ ) Ct (cid:19) xt + (cid:21) vt(xt) I(t, τ ) Ct This provides fully explicit backward solution to the PF-ODE, depending only on xt and the velocity field vt(xt). Special Case: Linear Interpolant For the linear interpolant with αt = 1 and σt = t, we have: α = 1, σ = 1, Ct = 1 , Ct σt = 1 t2 . Then: Also note: (cid:90) τ I(t, τ ) = 1 s2 ds = 1 τ 1 . 1 σ σt = , Ct = 1 . Plug into the general expression: xτ = τ = τ (cid:20)(cid:18) 1 (cid:20)(cid:18) 1 + 1 (cid:18) 1 τ 1/τ 1/t 1/t (cid:19)(cid:19) 1 (cid:19) xt + (cid:18) 1/τ 1/t 1/t (cid:19) (cid:21) vt(xt) (cid:18) 1 1 τ xt + (cid:19) (cid:21) vt(xt) = xt + (τ t)vt(xt). Final Linear Interpolant Result xτ = xt + (τ t)vt(xt) (40) (41) (42) (43) (44) (45) (46) In the case of the linear interpolant, the PF-ODE corresponds to straight-line trajectory with constant velocity vt(xt), enabling exact integration via Euler steps of arbitrary size. F. More Experimental Results F.1. Two-Stage Training and Sampling For the two-stage experiment, we set ξ = 0.7 as the split point between the low-variance and high-variance regimes. During training, timesteps are sampled uniformly from [ξ, 1], and optimization is performed using either the standard CFM loss or our StableVM loss, while all other configurations follow Sec. 4.1. Specifically, an SiT-XL/2 model is trained from scratch on [ξ, 1], while pretrained SiT-XL/2 model from REPA is used for stepping in [0, ξ]. Each model is trained for 500k steps, with checkpoints evaluated every 40k steps. As shown in Tab. 8, StableVM consistently outperforms CFM across training stages, indicating improved optimization stability in the high-variance regime. 27 Stable Velocity: Variance Perspective on Flow Matching Table 8. Two-stage training experiment, comparing CFM and StableVM (bank size = 256). Models are trained for 500k steps with checkpoints evaluated every 40k steps. The results are reported with classifier-free guidance. StableVM consistently achieves better FID and IS across training, demonstrating improved optimization stability when training is restricted to the high-variance regime. # Steps SiT StableVM (K = 256) IS FID sFID Precision Recall IS FID sFID Precision Recall 40k (SiT) / 50k (StableVM) 80k 120k 160k 200k 240k 280k 320k 360k 400k 440k 480k 500k 212.5 223.8 234.4 239.7 244.7 247.4 250.9 251.7 255.4 258.6 259.3 259.7 260.1 4.33 3.53 3.07 2.81 2.62 2.54 2.47 2.39 2.35 2.28 2.29 2.26 2.26 6.29 5.61 5.34 5.24 5.20 5.17 5.14 5.17 5.16 5.17 5.20 5.22 5.23 0.71 0.73 0.74 0.74 0.75 0.75 0.75 0.75 0.75 0.76 0.76 0.76 0.76 0.69 0.68 0.67 0.67 0.67 0.67 0.66 0.67 0.67 0.67 0.67 0.67 0. 219.2 227.3 234.8 238.9 247.0 250.3 253.3 255.4 258.5 261.7 262.3 263.1 262.7 3.92 3.35 3.02 2.87 2.59 2.48 2.40 2.34 2.28 2.17 2.16 2.16 2.17 6.04 5.61 5.40 5.74 5.21 5.20 5.13 5.13 5.11 4.98 5.00 5.03 5.06 0.72 0.73 0.74 0.74 0.75 0.75 0.75 0.75 0.76 0.76 0.76 0.76 0.76 0.69 0.67 0.67 0.66 0.67 0.67 0.67 0.66 0.66 0.67 0.67 0.66 0.67 Figure 7. Comparison of CFM and StableVM with = 2048 on the synthetic GMM distribution. We plot the second-order moment as function of training iterations at four time steps: = 0.20, 0.30, 0.40, and 0.50. F.2. Unconditional Synthetic GMM Generation We also evaluate Algorithm 1 in the unconditional generation setting. Specifically, we construct synthetic Gaussian Mixture Model (GMM) distribution and train the model to learn it using either the standard CFM loss or our proposed StableVM loss. The GMM is defined with 100 modes. For each component k, the mean vector µk is sampled independently from uniform distribution over [1, 1] in each dimension. The variances are drawn independently per component and per dimension from uniform distribution over [102, 101], yielding anisotropic Gaussian components. The mixing coefficients π are obtained by sampling each entry from Uniform(0.1, 1.0) and normalizing so that they sum to one. To generate samples, we first draw component index via multinomial sampling according to π, then sample from the corresponding Gaussian using the 28 Stable Velocity: Variance Perspective on Flow Matching Table 9. Ablation study on hyperparameters of StableVS. We analyze the effects of the split point ξ, the number of steps in the low-variance regime [0, ξ], and the variance factor fβ of Stable Velocity Sampling for SD3.5-Large (Esser et al., 2024) at 1024 1024 resolution. All experiments use Euler as the base solver. [0, ξ] steps fβ Overall PSNR SSIM LPIPS Total steps Baseline ξ Default StableVS setting 20 0.85 Variance factor fβ 0.85 0. 20 20 9 9 9 Low-variance regime steps [0, ξ] 0.85 0.85 15 4 14 Split point ξ 26 22 17 0.70 0.80 0.90 9 9 9 0. 16.93 0.753 0.333 0.0 0.723 36. 0.980 0.021 0.1 0.2 0.0 0.0 0.0 0.0 0.0 0.719 0. 0.708 0.719 0.726 0.724 0.728 32.69 29.08 29.30 39.79 43.65 39.15 31.99 0.956 0. 0.873 0.988 0.992 0.974 0.959 0.036 0.062 0.155 0.010 0.006 0.014 0.045 reparameterization trick: = µk + σk ϵ, where ϵ (0, I), and denotes element-wise multiplication. We fix the data dimensionality to 10 and evaluate both the CFM loss and our proposed StableVM loss with reference batch size of 2048 on this distribution. Model performance is assessed by computing the second-order moment of the discrepancy between the models predicted velocity field vθ(xt, t) and the true velocity field vt(xt), under the marginal distribution pt(xt): LMSE(t) = 1 2 Extpt (cid:104)(cid:13) (cid:13)vθ(xt, t) vt(xt)(cid:13) (cid:13) 2(cid:105) . (47) However, the exact velocity field vt(xt) = Ept(x0xt)[vt(xt x0)] is intractable. We therefore approximate it using self-normalized importance sampling estimator (Hesterberg, 1995). Concretely, given = 50,000 samples {xi i=1 drawn from the data distribution, we approximate: 0}N vt(xt) (cid:88) i=1 wi(xt) vt(xt xi 0), where wi(xt) = pt(xt xi 0) j=1 pt(xt xj 0) (cid:80)N . This approximation enables us to estimate the second-order moment at any time step t. As shown in Fig. 7, StableVM consistently achieves faster convergence and lower final error than standard CFM across all evaluated timesteps. This behavior directly reflects the variance reduction predicted by our analysis, even in fully controlled setting where the true velocity field is accessible. Reproduction of Fig. 1. To reproduce the variance curves in Fig. 1, we follow the same procedure described above but increase the dimensionality of the synthetic GMM to 100 and 500, using identical configuration settings. The true velocity field is again approximated using the self-normalized importance estimator with = 10,000 samples. For CIFAR-10 and ImageNet latents, we evaluate on the full training set or 50,000-sample subset. F.3. Ablation Studies of Hyperparameters in StableVS Tab. 9 presents the effect of three key hyperparametersthe variance factor fβ, the number of steps in the low-variance regime, and the split point ξon Stable Velocity Sampling with SD3.5-Large at 1024 1024 resolution on the GenEval benchmark. 29 Stable Velocity: Variance Perspective on Flow Matching Table 10. Detailed evaluation on GenEval at 1024 1024 resolution. We report the overall GenEval score together with per-category breakdowns. StableVS replaces the base solver in the low-variance regime [0, ξ] (with the number of steps indicated in parentheses), while keeping the high-variance regime [ξ, 1] unchanged. The split point is fixed to ξ = 0.85 for all models. Solver configuration GenEval Metrics Reference metrics Base solver Solver in [0, ξ] Total steps Overall Single Two Counting Colors Position Color Attr PSNR SSIM LPIPS SD3.5-Large (Esser et al., 2024) Euler DPM++ Euler(19) Euler(13) StableVS(9) DPM++(19) DPM++(13) StableVS(9) Flux-dev (Labs, 2024) Euler Euler(19) Euler(13) StableVS(9) 30 20 20 30 20 20 30 20 Qwen-Image-2512 (Wu et al., 2025a) Euler Euler(22) Euler(12) StableVS(9) 30 17 17 0.723 0.710 0.723 0.724 0.717 0. 0.660 0.659 0.666 0.733 0.721 0.731 0.994 0.994 0.994 0.997 0.994 0.997 0.984 0.991 0.981 0.988 0.994 0. 0.907 0.884 0.891 0.917 0.889 0.889 0.828 0.838 0.833 0.907 0.899 0.907 0.697 0.650 0.700 0.700 0.681 0. 0.700 0.694 0.697 0.572 0.556 0.569 0.843 0.838 0.838 0.825 0.717 0.806 0.801 0.785 0.798 0.859 0.859 0. 0.293 0.290 0.298 0.278 0.278 0.275 0.205 0.203 0.210 0.450 0.430 0.450 0.608 0.603 0.615 0.630 0.620 0. 0.443 0.445 0.475 0.623 0.590 0.615 16.93 36.92 17.42 32.61 19.74 35.45 17.01 32. 0.753 0.980 0.784 0.957 0.820 0.968 0.767 0.962 0.333 0.021 0.287 0. 0.244 0.025 0.277 0.031 Tab. 9 shows that StableVS is robust to moderate variations in all three hyperparameters. Increasing the variance factor fβ introduces additional stochasticity and degrades reference metrics, consistent with our analysis that the low-variance regime admits nearly deterministic dynamics. Similarly, allocating too few steps to the low-variance regime reduces fidelity, while overly aggressive step reduction beyond the regime boundary (ξ too large) leads to quality degradation. These trends support the variance-regime interpretation underlying StableVS and justify our default configuration. F.4. Full Results on Geneval Tab. 10 provides the full GenEval category-wise breakdown for all models and solver configurations. G. Experimental Details Data preprocessing follows the ADM protocol (Dhariwal & Nichol, 2021), where original images are center-cropped and resized to 256 256 resolution. For optimization, we employ AdamW (Kingma, 2015; Loshchilov, 2017) with constant learning rate of 1 104 and global batch size of 256. Training efficiency and numerical stability are enhanced via mixed-precision (fp16) training, gradient clipping, and an adaptive exponential moving average (EMA) decay schedule following Lipman et al. (2024b). For sigmoid weighting, the sharpness hyperparameters = 20. StableVS is implemented on top of the Hugging Face diffusers library (von Platen et al.), which provides robust support for state-of-the-art pre-trained models and flexible sampling pipelines. H. More Qualitative Results We provide additional qualitative results for SD3.5 (Esser et al., 2024), Flux (Labs, 2024), SD3 (Esser et al., 2024), and Wan2.2 (Wan et al., 2025), shown in Fig. 8, Fig. 9, Fig. 10, Fig. 11, and Fig. 12, respectively. 30 Stable Velocity: Variance Perspective on Flow Matching Figure 8. Visual comparison of SD3.5-Large (Esser et al., 2024) on different prompts. Results are generated using the Euler solver with 30 and 20 steps, and with StableVS replacing Euler in the low-variance regime, all under the same random seed. Compared to the standard 20-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details. 31 Stable Velocity: Variance Perspective on Flow Matching Figure 9. Visual comparison of Flux-dev (Labs, 2024) on different prompts. Results are generated using the Euler solver with 30 and 20 steps, and with StableVS replacing Euler in the low-variance regime, all under the same random seed. Compared to the standard 20-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details. 32 Stable Velocity: Variance Perspective on Flow Matching Figure 10. Visual comparison of Qwen-Image-2512 (Wu et al., 2025a) on different prompts. Results are generated using the Euler solver with 30 and 17 steps, and with StableVS replacing Euler in the low-variance regime, all under the same random seed. Compared to the standard 17-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details. 33 Stable Velocity: Variance Perspective on Flow Matching Figure 11. Visual comparison of Wan2.2 (Wan et al., 2025) on different prompts. Results are generated using UniPC solver with 30 and 20 steps, and with StableVS replacing UniPC in the low-variance regime, all under the same random seed. Compared to the standard 20-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details. 34 Stable Velocity: Variance Perspective on Flow Matching Figure 12. Visual comparison of Wan2.2 (Wan et al., 2025) on different prompts. Results are generated using UniPC solver with 30 and 20 steps, and with StableVS replacing UniPC in the low-variance regime, all under the same random seed. Compared to the standard 20-step solver, StableVS yields outputs that more closely resemble the 30-step results. Zoom in for details."
        }
    ],
    "affiliations": [
        "CIFAR",
        "Kling Team, Kuaishou Technology",
        "University of British Columbia",
        "University of Hong Kong",
        "Vector Institute for AI"
    ]
}