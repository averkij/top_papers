{
    "paper_title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
    "authors": [
        "Zhefei Gong",
        "Pengxiang Ding",
        "Shangke Lyu",
        "Siteng Huang",
        "Mingyang Sun",
        "Wei Zhao",
        "Zhaoxin Fan",
        "Donglin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 2 8 7 6 0 . 2 1 4 2 : r CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction Zhefei Gong1, Pengxiang Ding12*, Shangke Lyu1, Siteng Huang12, Mingyang Sun12, Wei Zhao1, Zhaoxin Fan3, Donglin Wang1 1Westlake University 2Zhejiang University 3Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing Project Webpage"
        },
        {
            "title": "Abstract",
            "content": "In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as coarse-tofine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, GPT-style transformer refines the sequence prediction through coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as realworld tasks. CARP achieves competitive success rates, with up to 10% improvement, and delivers 10 faster inference compared to state-of-the-art policies, establishing high-performance, efficient, and flexible paradigm for action generation in robotic tasks. 1. Introduction Policy learning from demonstrations, formulated as the supervised regression task of mapping observations to actions, has proven highly effective across various robotic tasks, * Project lead. Corresponding author. Figure 1. Policy Comparison. The representative performance among Behavior Transformer [52] served as an autoregressive policy, Diffusion Policy [11], and our approach in the state-based Robomimic square task experiment. CARP achieves superior balance of performance and efficiency. even in its simplest form. Replacing the policies with generative models, particularly those stemming from the vision community, has opened new avenues for improving performance, enabling robots to achieve the high precision required for complex tasks. Existing approaches have explored different generative modeling techniques to address challenges in visuomotor policy learning. Autoregressive Modeling (AM) [12, 30, 52, 70] provides straightforward and efficient out-of-the-box solution, benefitting from its scalability, flexibility, and mature exploration at lower computation requirements. However, AMs next-token prediction paradigm often fails to 1 (a) Autoregressive Policy (b) Diffusion Policy (c) CARP (Ours) Figure 2. Structure of Current Policies. ˆa is the predicted action, ak denotes the refining action at step k, is the historical condition. a) Autoregressive Policy predicts the action step-by-step in the next-token paradigm. b) Diffusion Policy models the noise process used to refine the action sequence. c) CARP refines action sequence predictions autoregressively from coarse to fine granularity. capture long-range dependencies, global structure, and temporal coherence [26], leading to poor performance, which is essential for many robotic tasks. Recently, Diffusion Modeling (DM) [11, 51, 61] has emerged as promising alternative, bridging the precision gap in AM by modeling the gradient of the action score function to learn multimodal distributions. Nevertheless, DM requires multiple steps of sequential denoising, making it computationally prohibitive for robotic tasks, especially for robotic tasks requiring efficient real-time inference in on-board compute-constrained environments. Additionally, DDPM [19]s rigid generative process lacks flexibility and adaptability for tasks with longterm dependencies, often leading to cumulative errors and reduced robustness over extended time spans. Both AM and DM have their respective advantages and limitations, which are often orthogonal and difficult to balIn this work, we aim to ance in practical applications. resolve this trade-off by introducing novel generative paradigm for robot visuomotor policy learning that predicts entire action sequences from coarse-to-fine granularity in next-scale prediction framework. This approach allows our model to achieve performance levels comparable to DM while achieving AMs efficiency and flexibility. Our primary contribution is the introduction of Coarseto-Fine AutoRegressive Policy (CARP), hybrid framework that combines AMs efficiency with DMs high performance to meet the demands of real-world robotic manipulation. Specifically, our contributions are as follows: Multi-scale action tokenization: We propose multiscale tokenization method for action sequences that captures the global structure and maintains temporal locality, effectively addressing AMs myopic limitations. Coarse-to-fine autoregressive prediction: This mechanism refines action sequences in the latent space using Cross-Entropy loss with relaxed Markovian assumptions during iterations, achieving DM-like performance with high efficiency. Comprehensive sim & real experiments: Extensive experiments demonstrate CARPs effectiveness in both simulated and real-world robotic manipulation tasks. In summary, we present CARP, novel visuomotor policy framework that synergizes the strengths of AM and DM, offering high performance, efficiency, and flexibility. 2. Background We start by introducing the background, focusing on three key areas: problem formulation, conventional autoregressive policies, and diffusion-based policies. 2.1. Problem Formulation Problem formulation will consider task , where there are expert demonstrations {τi}N i=1. Each demonstration τi is sequence of state-action pairs. We formulate robot imitation learning as an action sequence prediction problem [11, 46, 61], training model to minimize the error in future actions conditioned on historical states. Specifically, imitation learning minimize the behavior cloning loss Lbc formulated as Lbc = Es,aT (cid:34) (cid:88) t=0 (πθ(aH sO), aH ) , (1) (cid:35) where represents the action, denotes the state or observation according to the specific task description, is the current time step, is the prediction horizon, and is the historical horizon. For notational simplicity, we denote the action sequence at:t+H1 as aH and the state sequence stO+1:t as sO. Here, Lbc represents supervised action prediction loss, such as mean squared error or negative loglikelihood, is the length of the demonstration, and θ represents the learnable parameters of the policy network πθ. 2.2. Autoregressive Policy Autoregressive policies leverage the efficiency and flexibility of autoregressive models (GPT-style Decoders). Recent advancements, such as action chunking [52, 70], redefine this paradigm as multi-token, one-pass prediction method (see Appendix for further details). We refer to this approach as Autoregressive Policy (AP). In this approach, the 2 (a) Multi-Scale Action Tokenization (b) Coarse-to-Fine Autoregressive Prediction Figure 3. Overview of the Two Stages of CARP. a) multi-scale action autoencoder extracts token maps r1, r2, . . . , rK to represent the action sequence at different scales, trained using the standard VQVAE loss. b) The autoregressive prediction is reformulated as coarse-to-fine, next-scale paradigm. The sequence is progressively refined from coarse token map r1 to finer granularity token map rK , where each rk contains lk tokens. An attention mask ensures that each rk attends only to the preceding r1:k1 during training. standard Cross-Entropy loss is used for training. During inference, the final token map rK is decoded into continuous actions for execution. next-token posits the probability of observing the current action at depends solely on its previous states sO, which allows for the factorization of the likelihood of sequence with length as: (at, at+1, ..., at+H1) = t+H1 (cid:89) k=t (aksO) . (2) However, it introduces several issues that hinder its performance. The linear, step-by-step unidirectional dependency of action prediction may overlook the global structure [26], making it challenging to capture long-range dependencies and holistic coherence or temporal locality [22] in complex scenes or sequences, which restricts their generalizability in tasks requiring bidirectional reasoning, as shown in Fig. 2a. For example, it cannot predict the former actions given the near-terminal state. 2.3. Diffusion Policy Diffusion-based policies [11] utilize Denoising Diffusion Probabilistic Models [19] to approximate the conditional distribution p(aH sO), through modeling the noise during the denoising process from Gaussian noise to noise-free output as ak+1 = α(ak γϵθ(sO, ak , k) + ), (3) where ϵθ is the learnable noise network, is the current denosing step, is the Gaussian noise, α and γ are the hyper-parameters. Diffusion-based policies show impressive performance in robotic manipulation tasks due to their action generation which gradually refines from random samples which we can abstract as coarse-to-fine process. This kind of 3 process alike human movement or natural thinking flows in line with human intuition. However, they suffer from poor convergence due to their design. To address this, multiple denoising steps are required, constrained by the Markovian assumption, where ak+1 depends only on the previous step ak (as shown in Fig. 2b), which leads to significant runtime inefficiency. Moreover, they lack the ability to leverage generation context effectively, hindering scalability to complex scenes [16]. 3. Method To address the limitations of existing methods, we propose Coarse-to-Fine AutoRegressive Policy (CARP), novel visuomotor policy framework that combines the high performance of recent diffusion-based policies with the sample efficiency and flexibility of traditional autoregressive policies. CARP achieves these advantages through redesigned autoregressive modeling strategy. Specifically, we shift from conventional next-token prediction to coarse-to-fine, next-scale prediction approach, using multi-scale action representations. CARPs training is divided into two stages: multi-scale action tokenization and coarse-to-fine autoregressive prediction, as shown in Fig. 3. In this section, we first explain the construction of multiscale action token maps, followed by the coarse-to-fine autoregressive prediction approach for action generation. Key implementation details are provided at the end. 3.1. Multi-Scale Action Tokenization Instead of focusing on individual action steps, we extract representations at multiple scales across the entire action sequence. We propose novel multi-scale action quantization autoencoder that encodes sequence of actions into discrete token maps, = (r1, r2, . . . , rK), which are used for both training and inference. Our approach builds on the VQVAE architecture [60], incorporating modified multiscale quantization layer [58] to enable hierarchical encoding. Encoder and Decoder. As illustrated in Fig. 3a, the actions are first organized into an action sequence AHD, where denotes the prediction horizon and represents the dimensionality of the action a. Given that each dimension in the action space is orthogonal to the others, and that there exists natural temporal dependency within an action sequence, we employ 1D temporal convolutional network (1D-CNN) along the time dimension, as used in [22], for both the encoder E() and decoder D(). Let = E(A) RLC, where denotes the compressed length of the temporal dimension with H, and represents the dimensionality of the feature map . Quantization. We introduce quantizer with learnable codebook RV C, containing code vectors, each of dimension C. The quantization process (lines 4-11 in Algorithm 1) generates the action sequence representation iteratively across multiple scales. At scale k, it produces action token map rk to represent the sequence, which consists of lk tokens q. In our implementation, we set lk = k. The function Lookup(Z, v) retrieves the v-th code vector from Z. The quantization function is defined by = Q(F ) as following: = arg min dist (Lookup(Z, v), ) [V ], (4) v[V ] where token represents the nearest vector in for given feature vector R1C in the feature map , based on distance function dist(). Specifically, we adopt residual-style design [29, 58] for feature maps and ˆF , as shown in lines 4-11 of Algorithm 1. This design ensures that each finer-scale representation rk depends only on its coarser-scale predecessors (r1, r2, . . . , rk1), facilitating multi-scale representation. shared codebook is used across all scales, ensuring that multi-scale token maps = (r1, r2, . . . , rK), where is the number of scales, are drawn from consistent vocabulary [V ]. To preserve information during upsampling, we employ additional 1D convolutional layers {ϕk}K k=1 [58], as illustrated in lines 9-10 of Algorithm 1. Loss. The final approximation ˆF of the original feature map is combined residually with the multi-scale representation derived from the codebook Z, based on each r. The reconstructed action sequence is then obtained as ˆA = D( ˆF ). To train the quantized autoencoder, typical VQVAE loss is minimized as following = ˆA2 (cid:125) (cid:124) (cid:123)(cid:122) Lrecon + sg(F ) ˆF 2 (cid:123)(cid:122) (cid:125) Lquant (cid:124) + sg( ˆF )2 , (cid:125) (cid:123)(cid:122) Lcommit (cid:124) (5) 4 where Lrecon minimizes the difference between the original action sequence and the reconstruction ˆA. sg() denotes stop-gradient. Lquant aligns the quantized feature map ˆF with the original , and Lcommit encourages the encoder to commit to codebook entries, preventing codebook collapse. For Lquant and Lcommit, we calculate every residual calculating moment of each scale rk as in Algorithm 1. After training, the autoencoder {E, Q, D} tokenizes actions for subsequent coarse-to-fine autoregressive modeling. Discussion. The tokenization strategy described above allows the multi-scale tokens R, extracted from the action sequence RHD via temporal 1D convolutions, to inherently preserve temporal locality [22]. Additionally, the hierarchical extraction captures the global structure, enabling the model to treat the action sequence as unified entity. Unlike traditional autoregressive policies that predict each action token independently (see Eq. (2) and Fig. 2a), CARP leverages dual capabilities to capture both local temporal dependencies and global structure across the entire action sequence. This approach produces smoother transitions and more stable action sequences. Through multi-scale encoding, CARP overcomes the short-sighted limitations of conventional autoregressive models, yielding more robust, coherent, and precise behaviors over extended time horizons. Algorithm 1 Multi-Scale Action VQVAE (lk)K 1: Inputs: Action sequence 2: Hyperparameters: Number of scales K, length of each scale k=1 , length of feature maps temporal dimension rk Q(Interpolate(F , lk)) 3: Initialize: E(A), ˆF 0, [] 4: for = 1 to do 5: 6: {rk} 7: 8: 9: 10: 11: end for 12: ˆA D( ˆF ) 13: Return: Multi-scale token maps R, reconstructed action seZ Lookup(Z, rk) Interpolate(Z k, L) ϕk(Z k) ˆF ˆF + ϕk(Z k) quence ˆA 3.2. Coarse-to-Fine Autoregressive Prediction Using multi-scale action sequence representations, we shift from traditional next-token prediction to next-scale prediction approach, progressing from coarse to fine granularity. Prediction. As we receive the multi-scale representation tokens (r1, r2, . . . , rK), each represents the same action sequence in different scales. The autoregressive likelihood can be formulated as p(r1, r2, . . . , rK) = (cid:89) p(rk r1, r2, . . . , rk1; sO), k= (6) where each autoregressive unit rk [V ]lk is the token map at scale containing lk tokens q. The predix sequence (r1, r2, . . . , rk1) is served as the condition for rk, accompanying with the historical state sequence sO. This kind of next-scale prediction methodology is what we define as coarse-to-fine autoregressive prediction. Due to the residual-style quantization, during autoregressive prediction, we first embed the previous scale token map rk1 and then reconstruct the next scale feature map ek, as shown in Fig. 3b. This feature map ek is then used as input for predicting the next scale token map rk. The final token map rK is decoded by the multi-scale autoencoder into continuous actions for execution. Loss. During the k-th autoregressive step, all distributions over the lk tokens in rk will be generated in parallel, with the coarse-to-fine dependency ensured by block-wise causal attention mask [58]. To optimize the autoregressive model, we utilize the standard Cross-Entropy loss to capture the difference between the predicted token map ˆr and the token map from the ground truth action sequence: LCross-Entropy = (cid:88) lk(cid:88) k=1 i=1 (cid:34) (cid:88) v= (cid:35) log ˆri,v ri,v , (7) where lk is the length of each scale, is the action vocabulary size, and is the number of scales. Discussion. CARP models the entire trajectory holistically, progressively refining actions from high-level intentions to fine-grained details (see Fig. 2c). This approach aligns more closely with natural human behavior, where movement is guided by overarching intentions rather than step-by-step planning. Instead of the commonly used MSE loss [5, 59, 70], CARP employs Cross-Entropy loss (Eq. (7)) in behavior cloning, as MSE tends to enforce unimodal distribution that can be detrimental to manipulation tasks [52]. CARPs iterative refinement process resembles the denoising steps in diffusion models to achieve high accuracy. Furthermore, our approach models actions directly rather than modeling noise, enabling faster convergence in lowdimensional manifolds [36, 64]. By operating in the latent space with action sequence tokens, CARP mitigates trajectory anomalies that can disrupt prediction, unlike methods that work on raw actions [11]. This latent-space representation allows CARP to focus on the essential components of actions, resulting in smoother and more efficient predictions. In contrast, traditional diffusion models rely on Markovian processes, which compress all information into progressively noisier inputs from previous levels, often hindering efficient learning and requiring more inference steps [16] (see Eq. (3) and Fig. 2b). CARP relaxes this constraint by allowing each scale rk to depend on all prior scales r1:k1 instead of the only previous scale rk1. This structure enables CARP to generate high-quality trajectories with significantly fewer steps. 3.3. Implementation Details Our primary focus is on the CARP algorithm, and we adopt simple model architecture design. Tokenization. Due to the disentangling of the action prediction, imprecise coarse-to-fine actions representation can decrease the upper limit of model performance. Considering the discontinuity of the most representation for rotation space like Euler angle or quaternion will increase the unstable training process, we utilize rotation6d [71] to get stable multi-scale tokens. And for the distance function dist(), we use cosine similarity rather than Euclidean distance which is the cause of unstable training based on our observations. In practice, due to the orthogonality between dimensions, we use separate VQVAE [58] for each dimension of the action space, to gain stable training. All convolutions we used in CARP are 1D to capture the timedimension features. Autoregressive. We adopt the architecture of standard decoder-only transformers akin to GPT-2 [9, 58]. The state sO is used to generate the initial coarse-scale token map and then is utilized as adaptive normalization [40] for the subsequent predictions. During training, we observe that incorporating Exponential Moving Average (EMA) [18] enhances both training stability and performance, yielding 4-5% improvement, consistent with findings in [11]. During the inference, kv-caching can be used and no mask is needed, as shown in Fig. 10. 4. Experiment In this section, we evaluate CARP on diverse robotics tasks, including state-based and image-based benchmarks in single-task and multi-task settings. CARPs performance is assessed based on task success rates, inference speed, and model parameter scale. Additionally, we validate CARPs practical effectiveness by deploying it on realworld tasks using UR5e robotic arm, comparing its performance against state-of-the-art diffusion-based policies."
        },
        {
            "title": "Our experiments are structured to address the following",
            "content": "key research questions: RQ1: Can CARP match the accuracy and robustness of current state-of-the-art diffusion-based policies? RQ2: Does CARP maintain high inference efficiency and achieve fast convergence? RQ3: Does CARP leverage the flexibility benefits of GPT-style architecture? 5 Figure 4. Single-Task Simulation Setup. We evaluate three tasks from the Robomimic [38] benchmarkLift, Can, and Squareordered by increasing difficulty, along with Kitchen task [17] on the far left. Baselines. We compare CARP with previous autoregressive policies as well as recent diffusion policies. Behavior Transformer (BET) [52] is an autoregressive policy with action discretization and correction mechanisms, similar to offset-based prediction. Implicit Behavior Cloning (IBC) [15] utilizes energy-based models for supervised robotic behavior learning. Diffusion Policy (DP) [11] combines denoising process with action prediction, implemented in two variants: CNN-based (DP-C) and Transformer-based (DP-T). 4.1. Evaluation on Simulation Benchmark Policy p2 p3 p4 Params Speed We first evaluate CARP on set of simulated tasks commonly used to benchmark diffusion-based models, assessing its ability to match their performance while significantly improving computational efficiency. BET [52] DP-C [11] DP-T [11] CARP (Ours) 0.96 1.00 1.00 1.00 0.84 1.00 0.99 1.00 0.6 1.00 0.98 0.98 0.20 0.96 0.96 0.98 0.30 66.94 80.42 3. 1.95 56.14 56.32 2."
        },
        {
            "title": "Square",
            "content": "Params/M Speed/s BET [52] DP-C [11] DP-T [11] CARP (Ours) 0.96 1.00 1.00 1.00 0.88 0.94 1.00 1.00 0.54 0.94 0.88 0.98 0.27 65.88 8.97 0. 2.12 35.21 37.83 3.07 Table 1. State-Based Simulation Results (State Policy). We report the average success rate of the top 3 checkpoints, along with model parameter scales and inference time for generating 400 actions. CARP significantly outperforms BET and achieves competitive performance with state-of-the-art diffusion models, while also surpassing DP in terms of model size and inference speed."
        },
        {
            "title": "Square",
            "content": "Params/M Speed/s IBC [15] DP-C [11] DP-T [11] CARP (Ours) 0.72 1.00 1.00 1.00 0.02 0.97 0.98 0.98 0.00 0.92 0.86 0.88 3.44 255.61 9.01 7. 32.35 47.37 45.12 4.83 Table 2. Image-Based Simulation Results (Visual Policy). Results show that CARP consistently balances high performance and high efficiency. We highlight our results in light-blue. Experimental Setup. We use the Robomimic [38] benchmark suite, which is widely adopted for evaluating diffusion-based models [11, 42, 61]. We evaluate CARP on both state-based and image-based datasets collected from expert demonstrations, with each task containing 200 demonstrations. The selected tasks are standard in singletask evaluations, as shown in Fig. 4 and Fig. 12. For evaluating the ability to learn multiple long-horizon tasks, we utilize the Franka Kitchen environment [17] which contains 7 objects for interaction and comes with human demonstration dataset of 566 demonstrations, each completing 4 tasks in arbitrary order (see Fig. 14 for details). Table 3. Multi-Stage Task Results (State Policy). In the Kitchen, px represents the frequency of interactions with or more objects. CARP outperforms BET, especially on challenging metrics like p4, and achieves competitive performance compared to DP, with fewer parameters and faster inference speed. Metrics. For each task, we evaluate the policies by running 50 trials with random initializations to compute the success rate and report the average success rate across the top 3 checkpoints. For the Kitchen task, success rates are reported with increasing difficulty, determined by the number of interactive objects (from p1 to p4). Inference speed is measured on an A100 GPU by averaging the time required for 400 action predictions (280 for the Kitchen task) over 5 runs to ensure robustness. We also record the parameter count for each model using the same PyTorch implementation interface. Implementation Details. For baseline models, we follow the same implementation and training configurations provided by [11]. In state-based experiments, we set the observation horizon = 2 and the prediction horizon = 16 across all models. For image-based experiments, we set = 1 and = 16 for better transferability to real-world scenarios. As per the benchmark, only the first 8 actions in the prediction horizon are executed starting from the current step (see Appendix C). For CARP, we first train an action VQVAE model (see Sec. 3.1) following [29], using = 512, = 8, batch size of 256, and 300 epochs per task Given horizon = 16, we design multi-scale representations with scales of 1, 2, 3 and 4 to capture coarse-tofine information across the action sequence. We then train an autoregressive GPT-2 style, decoder-only transformer (see Sec. 3.2), based on [58], using the same training settings as the benchmark, with batch size of 256 for statebased experiments (4000 epochs) and batch size of 64 for image-based experiments (3000 epochs). Results. As demonstrated in Tables 1, 2, and 3, CARP 6 Figure 5. Visualization of the Trajectory and Refining Process. The left panel shows the final predicted trajectories for each task, with CARP producing smoother and more consistent paths than Diffusion Policy (DP). The right panel visualizes intermediate trajectories during the refinement process for CARP (top-right) and DP (bottom-right). DP displays considerable redundancy, resulting in slower processing and unstable training, as illustrated by 6 selected steps among 100 denoising steps. In contrast, CARP achieves efficient trajectory refinement across all 4 scales, with each step contributing meaningful updates. consistently achieves comparable performance to state-ofthe-art diffusion models across both state-based and imagebased tasks, answering RQ1. Notably, CARP significantly outperforms diffusion policies in terms of inference speed, being approximately 10 times faster, with only 1-5% of the parameters required by diffusion models, thereby strongly supporting RQ2 regarding CARPs efficiency. Analysis. To further examine CARPs stability and efficiency, we visualize spatial trajectories along the xyz axes for the Can and Square tasks in Fig. 5. In each tasks left panel, CARP consistently reaches specific regions (light grey area) to perform task-related actions, such as positioning for object grasping or placement. Compared to diffusion-based models, CARPs trajectories are smoother and more consistent, underscoring its stability and accuracy (supporting RQ1). The right panels of Fig. 5 compare CARPs coarse-to-fine action predictions with 6 selected denoising steps of the diffusion model. CARP achieves accurate predictions within just 4 coarse-to-fine steps, whereas the diffusion model requires numerous denoising iterations, with many early steps introducing redundant computations. This analysis further supports RQ2 and highlights CARPs efficiency and fast convergence advantage over diffusion policies in generating accurate actions with fewer refinement steps, as detailed in Sec. 3.2. 4.2. Evaluation on Multi-Task Benchmark We evaluate CARP on the MimicGen [39] multi-task simulation benchmark, widely used by the state-of-the-art Sparse Diffusion Policy (SDP) [61], to demonstrate CARPs flexibility, result of its GPT-style autoregressive design. Figure 6. Multi-Task Simulation Setup. We evaluate eight tasks from the MimicGen [39] benchmark: Coffee, Hammer Cleanup, Mug Cleanup, Nut Assembly, Square, Stack, Stack Three, and Threading, listed left-to-right and top-to-bottom. Experimental Setup. MimicGen extends benchmark Robomimic [38] by including 1K10K human demonstrations per task, with diverse initial state distributions for enhanced generalization in multi-task evaluation. It consists of 12 robosuite [72] tasks powered by MuJoCo and 4 highprecision tasks from Isaac Gym Factory. We select 8 robosuite tasksCoffee, Hammer, Mug, Nut, Square, Stack, Stack Three, and Threadingfor evaluation, each with 1K training trajectories, following the settings in SDP [61]. Visualization is shown in Fig. 6 and Fig. 13. Baselines. We compare CARP with two baselines: Task-Conditioned Diffusion (TCD) [2, 34]: basic diffusion-based multi-task policy and Sparse Diffusion Policy (SDP) [61]: transformer-based diffusion policy that leverages Mixture of Experts (MoE) [53]. Both baselines 7 Policy Prams/M Speed/s Coffee Hammer Mug Nut Square Stack Stack three Threading Avg. TCD [34] SDP [61] CARP (Ours) 156.11 159.85 16.08 107.15 112.39 6.92 0.77 0.82 0.86 0.92 1.00 0. 0.53 0.62 0.74 0.44 0.54 0.78 0.63 0.82 0.90 0.95 0.96 1.00 0.62 0.80 0.82 0.56 0.70 0. 0.68 0.78 0.85 Table 4. Multi-Task Simulation Results (Visual Policy). Success rates are averaged across the top three checkpoints for each task, as well as the overall average across all tasks. We also report parameter count and inference time for generating 400 actions. CARP outperforms diffusion-based policies by 9%-25% in average performance, with significantly fewer parameters and over 10 faster inference. are trained using visual inputs. Metrics. Success rates are reported for each task as the average of the best three checkpoints. For Nut Assembly, partial success (e.g., placing one block inside the cylinder) is assigned score of 0.5, while full success is scored as 1. For all other tasks, score of 1 is awarded only when strict success criteria are fully satisfied. Additionally, we calculate the average success rate across all tasks. To evaluate model efficiency, we report both the parameter scale calculated by the Pytorch interface and the inference time required to predict 400 actions on single A100 GPU. Implementation Details. For CARP, we extend the single-task implementation by incorporating task embedding as an additional condition, alongside the observation sequence s. We also use moderately deeper decoder-only transformer in GPT-2 style. CARP is trained with batch size of 512 for 200 epochs on an A100 GPU. Baseline models follow the same training settings as SDP [61]. This minimal modification enables CARP to adapt to multi-task learning seamlessly. Results. As shown in Tab. 4, CARP achieves up to 25% average improvement in success rates compared to state-of-the-art diffusion-based policies, highlighting its strong performance. With minimal modification, CARP seamlessly transitions from single-task to multi-task learning, further demonstrating its flexibility, benefit of its GPT-style architecture. Additionally, as shown in Tab. 4, CARP achieves over 10 faster inference speed and uses only 10% of the parameters compared to SDP. Leveraging its GPT-style autoregressive design [7, 47], CARP can seamlessly adapt to multi-task settings without complex design modifications [61], demonstrating the flexibility of this architecture, which strongly supports RQ3. These results strongly demonstrate CARPs flexibility, solidifying its role as high-performance and high-efficiency approach for visuomotor robotic policies. 4.3. Evaluation on Real-World In this section, we evaluate our approach, CARP, on realworld tasks under compute-constrained conditions, comparing its performance and efficiency against baseline methods. Experimental Setup. To validate CARPs real-world applicability, we design two manipulation tasks: Figure 7. Real-World Setup. The left panel shows the environment used for the experiment and demonstration collection. The right panel shows the trajectory from the Cup and Bowl datasets. 1. Cup: The robot must locate cup on the table, pick it up, move to the right area of the table, and put it down steadily (top-right, Fig. 7). 2. Bowl: The robot needs to identify smaller bowl and larger pot on the table, pick up the bowl, and place it inside the pot (bottom-right, Fig. 7). We use UR5e robotic arm with Robotiq-2f-85 gripper, equipped with two RGB cameras: one mounted on the wrist and one in third-person perspective (left panel, Fig. 7). The robot is controlled through 6D end-effector positioning, with inverse kinematics for joint angle calculation. For teleoperation, we collected 50 human demonstration trajectories for each task using 3D Connexion space mouse. Baselines. As baseline, we reproduce the CNN-based image-based diffusion policy [11], adapting the models input size to accommodate our observational setup. Metrics. For each trained policy, we report the average success rate across 20 trials per task, with the initial positions randomized. We also measure inference speed on an NVIDIA GeForce RTX 2060 GPU, reporting action prediction frequency in Hertz. Implement Details. For both models, the input consists of current visual observations from the wrist and scene cameras (resolution: 120 160), as well as proprioceptive data from the robotic arm. We execute 8 predicted actions out of horizon of 16 predictions. We train the diffusion policy for 3000 epochs with batch size of 64. For CARP, we use 8 strong performance by generating tokens sequentially. Recent work has scaled these models to billions of parameters, achieving impressive text-to-image synthesis results [35, 69] and robotic action generation [13, 54]. VAR [58] introduces new next-scale autoregressive paradigm that shifts image representation from patches to scales. This framework [58], has been applied across tasks [16, 32, 33, 37, 43, 66, 67]. Studies [56, 58] show that autoregressive models can surpass diffusion models in achieving state-of-the-art performance, serving as key inspiration for our work. 5.2. Visuomotor Policy Learning Behavior cloning [8] has proven effective, especially in autonomous driving [41] and manipulation [68], offering simpler alternative to complex reinforcement learning. Explicit policies directly map states or observations to actions [68], enabling efficient inference. However, they struggle with complex tasks. Techniques like action space discretization [65] and Mixture Density Networks (MDNs) [12, 52] have been proposed, though they face issues with exponential action space growth and hyperparameter sensitivity. Implicit policies, often based on EnergyBased Models (EBMs) [15, 23], offer flexibility but are challenging to train due to optimization instabilities. Diffusion models have been applied to robotic policy learning [2, 11, 50], showing their effectiveness for decisionmaking tasks. However, their multi-step denoising process is computationally expensive. Recent efforts focus on improving generalization [62], adapting to 3D environments [61, 64], and enhancing modularity via Mixture of Experts (MoE) [61]. Consistency models [36, 55] accelerate inference but often compromise action prediction accuracy, along with inflexible model design. 6. Conclusion In this work, we introduce Coarse-to-Fine Autoregressive Policy (CARP), novel paradigm for robotic visuomotor policy learning that combines the efficiency of autoregressive modeling (AM) with the high performance of diffusion modeling (DM). CARP incorporates: 1) multi-scale action tokenization to capture global structure and temporal locality, addressing AMs limitations in long-term dependency; 2) coarse-to-fine autoregressive prediction that refines actions from high-level intentions to detailed execution, achieving DM-like performance with AM-level efficiency through latent space prediction and relaxed Markovian constraints. The comprehensive evaluations, from simulation to real-world, demonstrate CARPs effectiveness in balancing high performance, efficiency, and flexibility. We hope this work will inspire further exploration into next-generation policy learning by leveraging GPT-style autoregressive models, advancing more unified perspective on current generative modeling techniques (see Appx.A). Figure 8. Real-World Results (Visual Policy). We report the average success rate across 20 trials and the inference speed as action prediction frequency. CARP achieves competitive success rates with significantly faster inference compared to DP. the same visual policy structure as in the simulation tasks, training the action VQVAE for 300 epochs with batch size of 256, and the decoder-only transformer for 3000 epochs with batch size of 64. Results. As shown in Fig. 8 and Fig. 9, CARP achieves comparable or superior performance, with up to 10% improvement in success rate over the diffusion policy across all real-world tasks, supporting RQ1. Additionally, CARP achieves approximately 8 faster inference than the baseline on limited computational resources, demonstrating its suitability for real-time robotic applications, thus supporting RQ2 (see Fig. 15 for further visualization). Figure 9. Visualization of CAPR on Real-World Tasks. CARP generates smooth and successful trajectories for the Cup and Bowl tasks, with temporal progression from left to right. 5. Related Work 5.1. Visual Generation Advancements in generative models for visual generation have significantly influenced the robotics community. Autoregressive models generate images in raster-scan fashion using discrete tokens from image tokenizers [14, 14, 20, 60]. GPT-2-style transformers [4, 29, 48, 63] demonstrate"
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 13 [2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. 7, 9 [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 13 [4] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. 9 [5] Mariusz Bojarski. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 13 [7] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 8, 13 [8] Harish chaandar Ravichandar, Athanasios S. Polydoros, Sonia Chernova, and Aude Billard. Recent advances in robot learning from demonstration. Annu. Rev. Control. Robotics Auton. Syst., 3:297330, 2020. 9 [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 5 [10] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. 13 [11] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023. 1, 2, 3, 5, 6, 8, 9, 13, 14 [12] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022. 1, 9, [13] Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. Quar-vla: Vision-language-action model for quadruped robots. In European Conference on Computer Vision, pages 352367. Springer, 2025. 9 [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 9 [15] Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral In Proceedings of the 5th Conference on Robot cloning. Learning, pages 158168. PMLR, 2022. 6, 9 [16] Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation. arXiv preprint arXiv:2410.08159, 2024. 3, 5, 9 [17] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 6 [18] David Haynes, Steven Corns, and Ganesh Kumar Venayagamoorthy. An exponential moving average algorithm. In 2012 IEEE Congress on Evolutionary Computation, pages 18. IEEE, 2012. 5 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. 2, 3, [20] Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yongdong Zhang. Towards accurate image coding: Improved autoregressive image generation with dynamic vector quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2259622605, 2023. 9 [21] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34: 12731286, 2021. 13 [22] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. 3, 4 [23] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-based distribution matching. Advances in Neural Information Processing Systems, 33:73547365, 2020. 9 [24] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot arXiv preprint manipulation with multimodal prompts. arXiv:2210.03094, 2(3):6, 2022. 13 [25] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [26] Maciej Kilian, Varun Jampani, and Luke Zettlemoyer. tradeoffs in image synthesis: Diffusion, Computational masked-token, and next-token prediction. arXiv preprint arXiv:2405.13218, 2024. 2, 3 10 [27] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 13 [28] Lucy Lai, Ann Zixiang Huang, and Samuel Gershman. Action chunking as policy compression. PsyArXiv, 2022. 13 [29] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 4, 6, [30] Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. BearXiv preprint havior generation with latent actions. arXiv:2403.03181, 2024. 1, 13 [31] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. 13 [32] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. 9 [33] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj. Controlvar: Exploring conarXiv preprint trollable visual autoregressive modeling. arXiv:2406.09750, 2024. 9 [34] Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, and Ping Luo. Skilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16467 16476, 2024. 7, 8 [35] Yang Liu, Pengxiang Ding, Siteng Huang, Min Zhang, Han Zhao, and Donglin Wang. Pite: Pixel-temporal alignment for large video-language model. In European Conference on Computer Vision, pages 160176. Springer, 2025. [36] Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, and Yansong Tang. Manicm: Real-time 3d diffusion policy via consistency model for robotic manipulation. arXiv preprint arXiv:2406.01586, 2024. 5, 9 [37] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. 9 [38] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298, 2021. 6, 7 [39] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. In 7th Annual Conference on Robot Learning, 2023. 7 [40] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23372346, 2019. 5 [41] Dean Pomerleau. Alvinn: An autonomous land vehicle in neural network. Advances in neural information processing systems, 1, 1988. [42] Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation. arXiv preprint arXiv:2405.07503, 2024. 6 [43] Kai Qiu, Xiang Li, Hao Chen, Jie Sun, Jinglu Wang, Zhe Lin, Marios Savvides, and Bhiksha Raj. Efficient autoregressive audio modeling via next-scale prediction. arXiv preprint arXiv:2408.09027, 2024. 9 [44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 13 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 13 [46] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with sensorimotor pre-training. arXiv:2306.10007, 2023. 2 [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [48] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 9 [49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias SprinarXiv preprint genberg, et al. generalist agent. arXiv:2205.06175, 2022. 13 [50] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation learning using scorebased diffusion policies. arXiv preprint arXiv:2304.02532, 2023. 9 [51] Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. 2 [52] Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning modes with one stone. In Thirty-Sixth Conference on Neural Information Processing Systems, 2022. 1, 2, 5, 6, 9, 13 [53] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra- [67] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024. 9 [68] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE international conference on robotics and automation (ICRA), pages 56285635. IEEE, 2018. 9 [69] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. 9 [70] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 1, 2, 5, 13 [71] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural In Proceedings of the IEEE/CVF conference on networks. computer vision and pattern recognition, pages 57455753, 2019. 5 [72] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto MartınMartın, Abhishek Joshi, Soroush Nasiriany, Yifeng Zhu, robosuite: modular simulation frameand Kevin Lin. In arXiv preprint work and benchmark for robot learning. arXiv:2009.12293, 2020. geously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 7 [54] Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, and Donglin Wang. Germ: generalist robotic model with mixture-of-experts for quadruped robot. arXiv preprint arXiv:2403.13358, 2024. 9 [55] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 9 [56] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 9 [57] Garrett Thomas, Ching-An Cheng, Ricky Loynd, Felipe Vieira Frujeri, Vibhav Vineet, Mihai Jalobeanu, and Andrey Kolobov. Plex: Making the most of the available data for robotic manipulation pretraining. In Conference on Robot Learning, pages 26242641. PMLR, 2023. 13 [58] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 4, 5, 6, [59] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint arXiv:1805.01954, 2018. 5 [60] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 4, 9 [61] Yixiao Wang, Yifei Zhang, Mingxiao Huo, Ran Tian, Xiang Zhang, Yichen Xie, Chenfeng Xu, Pengliang Ji, Wei Zhan, Mingyu Ding, et al. Sparse diffusion policy: sparse, reusable, and flexible policy for robot learning. arXiv preprint arXiv:2407.01531, 2024. 2, 6, 7, 8, 9 [62] Jingyun Yang, Zi-ang Cao, Congyue Deng, Rika Antonova, Shuran Song, and Jeannette Bohg. Equibot: Sim (3)- equivariant diffusion policy for generalizable and data efficient learning. arXiv preprint arXiv:2407.01479, 2024. 9 [63] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 9 [64] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation, 2024. 5, 9 [65] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipuIn Conference on Robot Learning, pages 726747. lation. PMLR, 2021. [66] Jinzhi Zhang, Feng Xiong, and Mu Xu. G3pt: Unleash the power of autoregressive modeling in 3d generation via cross-scale querying transformer. arXiv preprint arXiv:2409.06322, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Limitations and Future Work In this work, we propose CARP, next-generation paradigm for robotic visuomotor policy learning, which effectively balances the long-standing trade-off between high performance and high efficiency seen in previous autoregressive modeling (AM) and diffusion modeling (DM) approaches. Despite these advancements, there remain several limitations and opportunities for improvement in future research. First, the model structure of CARP can be further optimized for simplicity. Currently, CARP employs two-stage design, where the first stage utilizes separate multi-scale action VQVAE modules for each action dimension to address their orthogonality. Future work could focus on developing unified one-stage method that integrates multi-scale tokenization with the coarse-to-fine prediction process, resulting in more efficient and streamlined framework without compromising performance. Second, the multimodal capabilities of CARP remain underdeveloped. To mitigate the unimodality issue inherent in traditional autoregressive policies trained with MSE loss, CARP adopts cross-entropy loss, which retains the potential for multimodality. Compared to the Diffusion Policys multimodal ability [11] likely coming from DDPMs integration over Stochastic Differential Equation [19], CARPs approach is relatively straightforward. Further enhancements are needed to fully realize and leverage its multimodal potential. Third, CARPs adoption of the GPT-style paradigm opens up promising yet unexplored possibilities. Beyond the flexibility already demonstrated, the contextual understanding capabilities inherent in GPT-style architectures [44] suggest that CARP could be extended to support multimodal inputs [3, 45] like tactile and auditory information and address robotic tasks requiring long-term dependency reasoning [1]. Additionally, its potential for generalization in few-shot and zero-shot learning scenarios [7] represents particularly valuable direction for future exploration. Finally, but not exhaustively, the scaling properties of CARP represent promising frontier. The scaling laws established in existing GPT-style models [25] could be seamlessly applied to CARP, suggesting that increasing network capacity and leveraging larger pre-training datasets could lead to substantial performance gains. Furthermore, recent advances in Vision-Language-Action (VLA) [6, 27, 31] models present an opportunity to integrate CARP into such frameworks, which could further validate its scalability and capabilities. Figure 10. Coarse-to-Fine Autoregressive Inference. During inference, we leverage kv-caching to enable coarse-to-fine prediction without the need for causal masks. The finest-scale token map rK is subsequently decoded by the action VQVAE decoder into executable actions for the robotic arm. Figure 11. Conventional Autoregressive Policy. In reinforcement learning, conventional autoregressive policies generate action tokens sequentially, where each token is predicted based on the previously generated tokens. This differs from the action chunking prediction shown in Fig. 2a. B. Definition of Autoregressive Policy Autoregressive policies naturally capitalize on the efficiency and flexibility of autoregressive models. Initial works from reinforcement learning perspective applied models like Transformers to predict the next action using states or rewards as inputs [10, 21], as shown in Fig. 11 and formalized as: (at, at+1, ..., at+H1) = t+H1 (cid:89) (akakH:k1, skH:k) , k=t (8) where skH:k1 represents the states or observations corresponding to the previous actions akH:k1, and sk is the current state or observation. Following this paradigm, several subsequent works [24, 49, 57] employ autoregressive models to predict one action at time during inference. More recently, the concept of action chunking [28], derived from neuroscience, has demonstrated notable benefits In action chunkfor imitation learning [12, 30, 52, 70]. ing, individual actions are grouped and executed as cohesive units, leading to improved efficiency in storage and 13 execution, as depicted in Fig. 2a. This paradigm extends the capabilities of GPT-style decoders by modifying them to generate chunks of actions in one forward pass, replacing the traditional single-step autoregressive operation with multi-token, pseudo-autoregressive process. The action generation process for this paradigm is described as: (at, at+1, . . . , at+H1) = t+H1 (cid:89) k=t (aksO) , (9) where is the historical horizon. The model predicts the entire action sequence in one forward pass without strictly adhering to step-by-step autoregressive operations. Given the significant performance improvements enabled by action chunking, we adopt this multi-token, oneforward-pass framework throughout the article and experiments when referring to autoregressive policies (AP). C. Implementation Consistent with Diffusion"
        },
        {
            "title": "Policy",
            "content": "We adopt similar experimental settings with 1 or 2 observations, prediction horizon of 16, and an executable action length of 8, following the standard setup used in Diffusion Policy [11]. It is important to note that our classical formulation introduces minor discrepancy in the horizon definition compared to the implementation of Diffusion Policy [11]. Specifically, in our formulation, the horizon encompasses the past observed steps, meaning that the index of the current next predicted action is O, rather than 0. In contrast, as outlined in formulation Eq. (1), the horizon does not include past observations, with the first prediction step corresponding to the next time step. While the rationale behind this design remains unclear due to limitations in the authors understanding, we retain the horizon definition introduced by Diffusion Policy [11] to ensure consistency in our experimental comparisons. 14 Figure 12. Visualization of Tasks in Single-Task Experiment. Figure 13. Visualization of Tasks in Multi-Task Experiment. Figure 14. Visualization of All Interaction Tasks in Kitchen Experiment. Figure 15. Visualization of Tasks in Real-World Setup."
        }
    ],
    "affiliations": [
        "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing",
        "Westlake University",
        "Zhejiang University"
    ]
}