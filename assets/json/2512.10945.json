{
    "paper_title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
    "authors": [
        "Henghui Ding",
        "Chang Liu",
        "Shuting He",
        "Kaining Ying",
        "Xudong Jiang",
        "Chen Change Loy",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/"
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 MeViS: Multi-Modal Dataset for Referring Motion Expression Video Segmentation Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Fellow, IEEE, Chen Change Loy, Senior Member, IEEE, Yu-Gang Jiang, Fellow, IEEE AbstractThis paper proposes large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the methods source code are publicly available at https://henghuiding.com/MeViS/. Index TermsMotion Expression Video Segmentation, MeViS Dataset, Referring Video Object Segmentation, Audio-guided Video Object Segmentation, Referring Multi-object Tracking, Referring Motion Expression Generation, LMPM++."
        },
        {
            "title": "R EFERRING video segmentation is an emerging field that aims",
            "content": "at segmenting and tracking the specific target object referred by given natural language expression [1], [2], [3], [4]. This task has traditionally been subset of semi-supervised video object segmentation, where the clue of target object is provided through means such as mask, scribble, or sentence in the first frame. Existing datasets in this context, such as DAVIS16-RVOS [3] and Refer-YouTube-VOS [4], typically encompass videos featuring isolated and salient objects with evident static characteristics. The corresponding expressions frequently contain static attributes like the objects color and shape, which can be identified from single frame. Consequently, motion properties of videos are less pronounced in these expressions, and methods designed for referring image segmentation can effectively be applied to referring video segmentation, yielding favorable performance [3], [5], [6], [7]. The motivation of this work is to emphasize the importance of temporal motion characteristics in videos and explore the feasibility of employing motion-related expressions to identify and segment objects within video content. To this end, we propose new large-scale dataset named Motion expressions Video Segmentation (MeViS). Some samples of MeViS are shown in Fig. 1. The MeViS dataset contains 2,006 videos with total of 8,171 distinct objects. In the conference version, MeViSv1 [1], 28,570 motion-related expressions are provided for referring and delineating these objects, focusing on direct motion descriptions Henghui Ding, Kaining Ying, and Yu-Gang Jiang are with Fudan University, China 200433. (e-mail: henghui.ding@gmail.com) Chang Liu and Shuting He are with Shanghai University of Finance and Economics, Shanghai, China, 200433. Xudong Jiang and Chen Change Loy are with Nanyang Technological University, Singapore 639798. of single or multiple targets. Compared to MeViSv1 [1], the updated MeViSv2 in this work significantly expands the dataset with more challenging motion expressions, adding audio format expressions, providing tracking annotations, and supporting more tasks. First, the updated dataset includes 4,502 new challenging expressions, bringing the total to 33,072 expressionsthe largest in the field of referring video. These additions include motion reasoning expressions, which involve implicit queries requiring complex reasoning, and no-target expressions, which are deceptive motion descriptions that relate to the video but do not refer to any actual object, as shown in Fig. 3. In addition to text expressions, the updated MeViSv2 further provides above 150,000 seconds audio expressions, facilitating the study of audio-guided video object segmentation (AVOS) and multi-modal referring expressions. Furthermore, we provide tracking annotations in MeViSv2, establishing it as the largest referring multi-object tracking (RMOT) dataset. Beyond perception tasks, we introduce new task based on MeViS: Referring Motion Expression Generation (RMEG). This task aims to generate an unambiguous and concise motion expression for the selected objects in given video. In the construction of the MeViS dataset, several steps are undertaken to highlight the temporal motions inherent to videos. First, an assortment of videos is selected with the criterion that they showcase multiple interacting objects in motion, deliberately excluding low-quality videos where isolated objects could be easily described through static attributes alone. Second, the dataset prioritizes language expressions that focus on motion clues (e.g., walking, moving) rather than static clues (e.g., color, shape). These rules distinguish our MeViS from earlier datasets like [3], [4], [8], which contain salient targets in their videos or include obvious static clues in their sentence annotations. MeViS also sets itself 5 2 0 2 1 ] . [ 1 5 4 9 0 1 . 2 1 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 1: Examples from Motion expressions Video Segmentation (MeViS) showing the datasets nature and complexity. The expressions in MeViS primarily focus on motion attributes, making it impossible to identify the target object from single frame. For example, the first example has three parrots with similar appearances, and the target object is identified as The bird flying away. This object can only be recognized by capturing its motion throughout the video. The updated MeViSv2 further provides motion-reasoning and no-target expressions (see Fig. 3), adds audio ))) expressions alongside text, and provides mask and bounding box trajectory annotations. apart from referring image segmentation datasets, such as [9], [10], [11], [12], [13], by considering the temporal properties of video, which is overlooked in these datasets. Uniquely, unlike existing referring video segmentation datasets [3], [4], [8] that are limited to single-object expressions, i.e., one expression refers to only one target object, MeViS broadens the scope to further support generalized expressions that can refer to an unlimited number of target objects, including no target, thereby enhancing the generalizability and real-world applicability of the MeViS dataset and referring video segmentation. The proposed MeViS dataset presents significant challenges in capturing and understanding motion in both video and language. Language expressions may refer to actions spanning varying numbers of frames, necessitating the capture of both fleeting movements and long-term actions throughout the entire video. This requirement introduces substantial challenges in comprehending motion within the video content and the corresponding language expressions. Capturing fleeting movements necessitates detailed attention to individual frames, whereas understanding long and complex movements requires maintaining temporal context across the entire video. To evaluate the effectiveness of existing methods in addressing the challenges posed by MeViSv2, we benchmark 15 existing methods across 4 tasks and conduct comprehensive comparisons, including 6 referring video object segmentation (RVOS) methods [4], [5], [14], [15], [16], [17], 3 Audio-guided Video Object Segmentation (AVOS) methods [1], [18], [19], 2 Referring Multi-Object Tracking (RMOT) methods [20], [21], and 4 video captioning methods [22], [23], [24], [25]. The experimental results show that MeViS presents greater challenges than existing datasets, revealing that existing methods are insufficient in effectively addressing motion expression-related video understanding. In addition to introducing the MeViS dataset, we propose baseline method: Language-guided Motion Perception and Matching (LMPM++). LMPM++ utilizes language-conditional queries to detect potential target objects within the video, and represents these objects with object embeddings. In this way, it enhances robustness and computational efficiency compared to using frame features [26]. We then feed object embeddings into large language model (LLM), capturing and reasoning temporal context and achieving comprehensive understanding of the video. Unlike existing methods [27], [28], we generate and input object tokens instead of frame features into LLM, enabling it to process much longer sequences, e.g., 200 frames compared to 3 [27] or 13 frames [28] in previous methods. Understanding the sequence of movements is crucial. For example, the actions of first jumping high and then jumping far vs. first jumping far and then jumping high, though similar in overall words, represent distinct motion patterns. To address this, we introduce temporal-level contrastive loss, enabling the model to differentiate motions with different temporal orders or sequences. In summary, our main contributions are as follows: We build MeViSv2, large-scale multi-modal referring motion expression video segmentation dataset focusing on segmenting and tracking object(s) in the given video indicated by motion expression in either text or audio format. The proposed MeViSv2 dataset can support at least 4 different referring video tasks: referring video object segmentation (RVOS), audio-guided video object segmentation (AVOS), referring multi-object tracking (RMOT), and referring motion expression generation (RMEG). We benchmark 15 methods across RVOS, AVOS, RMOT, and RMEG tasks on the proposed MeViSv2 dataset, serving as reference for future works in these 4 tasks on MeViSv2. Taking close look at the proposed MeViS dataset, we identify several challenges and develop baseline approach for perception tasks, named Language-guided Motion Perception and Matching (LMPM++), to meet these challenges. We discuss potential directions for future video-language motion understanding research."
        },
        {
            "title": "2 RELATED WORK\n2.1 Referring Image Segmentation",
            "content": "Referring image segmentation [5], [13], [29] aims at segmenting the target object in the given image referred by natural language expression describing the targets properties, e.g., location and color. Since introduced by Hu et al. in 2016 [30], this task has attracted significant interest and attention. Before the advent IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3 of Transformer-based models, conventional methods [31], [32] commonly relied on Fully Convolutional Networks (FCN) [33], [34] and Recurrent Neural Networks (RNN) to extract image and language features, respectively. Subsequently, these multi-modal features were integrated using specifically designed modules. For example, Liu et al. [32] present the Recurrent Multimodal Interaction (RMI) module to iteratively merge the features of individual words into the image features. Apart from one-stage methods, there are methods that decompose the task into two stages: instance segmentation and language-object matching [35], [36], [37]. For example, Yu et al. employ the pre-trained instance segmentation model Mask R-CNN [38] to detect all instances within an image. Then, they select the instance that best matches the given expression as the final output. Contextual modeling of both language and visual information is essential, and numerous studies have explored this direction [39], [40], [41]. As an example, Ye et al. introduce Cross-Modal Self-Attention (CMSA) [41] to identify the most relevant words within the language expression and pixels within the image, enhancing contextual comprehension. Recently, the impressive achievements of Transformer [42] in various vision-related tasks have inspired many works in referring image segmentation [43]. Ding et al. [5], [29] are the pioneers in introducing Transformer into referring segmentation and introduce Vision-Language Transformer (VLT). Following Ding et al. [5], [29], more Transformer-based methods have emerged in the field [13], [44], [45], [46], [47], [48], [49]. For example, Wang et al. [45] present Vision-Language Decoder to handle visual and text tokens extracted using CLIP [50]. Yang et al. [44] focus on the fusion of multi-modal features and introduce Language-Aware Vision Transformer (LAVT). These advancements showcase the growing influence of Transformer models in this area."
        },
        {
            "title": "2.2 Referring Video Segmentation",
            "content": "Referring video object segmentation is an emerging multi-modal video understanding task [7], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61] that focuses on segmenting the target object specified by given expression throughout video. It is first introduced in 2018 by A2D [8] and DAVIS17-RVOS [3]. The A2D dataset [8] aims to segment actors based on descriptions of their actions within video content, whereas DAVIS17-RVOS [3] utilizes language, rather than masks, as the reference for the target object in video object segmentation. Subsequently, Seo et al. [4] developed Refer-YouTube-VOS, which is based on the YouTube-VOS-2019 dataset [62]. These datasets usually provide expressions rich in static attributes describing single object. To emphasize motion, Ding et al. [1] introduce MeViS dataset with numerous motion expressions. Existing methods typically treat referring video segmentation as variant of semi-supervised video object segmentation [63] by replacing mask references with language references. For example, Khoreva et al. [3] adapt the referring image segmentation method MAttNet [35] to achieve frame-level segmentation, followed by post-processing to ensure temporal consistency. URVOS [4] utilizes cross-modal attention to perform per-frame segmentation, propagating the mask across frames using memory attention module. RefVOS [6] segments each frame independently based on fused language and image/frame features, without leveraging temporal information. Liang et al. [64] propose top-down approach that first detects all object tracklets and then selects the target object by matching language and tracklet features. More recently, ReferFormer [14], MTTR [15], and DsHmp [17] employ Transformers [42] to address referring video object segmentation."
        },
        {
            "title": "2.3 Audio-guided Video Segmentation",
            "content": "Audio-guided Video Segmentation introduces the audio modality into video segmentation, with three main settings: audio-visual segmentation (AVS) [65], [66], audio-guided video object segmentation (AVOS) [18], and referring audio-visual segmentation (RefAVS) [67]. The goal of AVS [65], [66] is to segment the soundemitting objects in video, such as birds chirping, cars honking, or dogs barking. In AVOS [18], the audio is human speech, and the goal is to segment the object described by the speech. The recently proposed Ref-AVS [67] focuses on segmenting objects in videos based on referring expressions that consider both visual and audio signals, unlike RVOS, which considers visual and textual signals. MeViSv2 focuses on AVOS, which has significant applications in embodied scenarios. The AVOS-Bench [18] dataset, derived from existing RVOS datasets with text converted to speech through human narration, has noisy speech and limits model complexity. MeViSv2 increases the challenge by using both TTS [68] and human narration for speech generation, inheriting the complexity of MeViSs videos and descriptions, which further enhances the difficulty and practicality of MeViSv2."
        },
        {
            "title": "2.4 Referring Object Tracking",
            "content": "Different from the above mentioned referring segmentation tasks, referring object tracking [20], [69] aims to detect the corresponding bounding box tracklets based on the language description. There are two common settings: referring single-object tracking (RSOT) [69], [70], [71], [72] and referring multi-object tracking (RMOT) [20], [21], [73]. RSOT, defined by Li et al. [70], focuses on localizing single target object in video based on sentence for the first frame. Yang et al. [71] and Feng et al. [72] divide this task into grounding and tracking. The grounding stage detects the language referred target, while the tracking stage tracks the target in subsequent based on the first-frame grounding result. [71] also performs visual matching based on the history of grounded objects and language-based grounding for each frame. Despite significant advancements in RSOT, current RSOT methods are constrained by their ability to describe only single target per expression, and the language description is typically provided solely for the first frame. These limitations hinder their effectiveness in real-world applications. To address these issues, Wu et al. [20] propose RMOT and introduce the Refer-KITTI benchmark, designed to handle multi-object and temporally statusvariant scenarios. Their baseline method extends the end-to-end multi-object tracking framework, MOTR [74], to accommodate cross-modal input. Subsequently, iKUN [73] adopts two-stage approach, initially extracting object tracklets explicitly and then selecting those that correspond to the given language expression. Later, Zhang et al. [21] propose query-based temporal enhanced framework, modeling long-term spatial-temporal interactions through transformer query features. Alongside this solution, they introduce RMOT dataset, Refer-KITTI-V2, which includes diverse and multifaceted textual descriptions encompassing appearance, complex motion, position, etc. MeViS supports more generalized referring understanding between expressions and long-term target states by including singletarget, multi-target, and no-target expressions. Unlike previous RSOT and RMOT datasets, MeViS is designed for broader range IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 A2. If an object can be unambiguously described by its motion or action, static attributes such as shape and color should not be included in the expression. A3. If multiple objects cannot be differentiated based solely on their motion or action, they can be described together if their motion or action can unambiguously identify them, such as The two lions fighting and running amidst group of lions. A4. If it is not possible to differentiate single or multiple objects based solely on their motion or action, limited static attributes can be included in the expression. A5. No-target expression must also describe motion like other single-/multi-target expressions, and cannot be entirely unrelated to the video. Annotators can derive no-target expressions by modifying existing single-/multi-target expressions. Language Expression Validation. Upon receiving annotated video-object-expression samples from the annotators, the validation process begins by displaying the video and expression and prompting the validator to select and submit the objects referred to in the expression. The validator must find the targets independently and submit their selection. The system then compares the targets chosen by the validator with the annotations submitted by the annotator. sample is considered valid if the validator and annotator independently selected the same target object(s) using the same expression. If the targets selected by the validator do not match the annotation submitted by the annotator, the sample will be forwarded to another validator for second opinion. If the second validator also fails to identify the correct targets, the sample will be considered invalid and excluded from the dataset. Validators have the authority to reject samples that are deemed inappropriate or fall short of quality standards. Moreover, we stress the importance of the following validation criterion: V1. The corresponding sentence will be removed from the dataset when the target object described by sentence can be easily identified through single frame. V2. No-target expressions that are unrelated to the corresponding video or do not describe motion will be discarded. By establishing these validation criteria, we aim to ensure that the language sentences in our dataset accurately express motion and are of high quality, while also increasing the difficulty of the language-guided video segmentation task, thereby enabling more robust evaluation of model performance. Audio Annotation ))). After the text expressions are created, we further add speech recordings for every language expression. To ensure voice variety, the audio is mix of automatic synthesis and human recordings. For the human-recorded portion, 10 speakers are employed to read and record the sentences. The speakers come from diverse backgrounds, including native and non-native speakers of different genders and age groups. They are required to read the sentences naturally at normal talking speed of 100-150 words per minute, and record the audio using microphones with sampling rate higher than 44.1KHz. Slight pauses, stutters, and background noises are allowed to simulate practical user cases, as long as the speech is recognizable and true to the original text. For the synthesized portion, we use six state-of-the-art TextTo-Speech (TTS) models and 3 public TTS services. All audio clips are verified twice, by human verifiers and speech recognition models, to ensure they are consistent with the text expression. The total recording time of the dataset is above 150,000 seconds. Fig. 2: Flowchart of the language expression annotation and validation process of MeViS. Samples that are found ambiguous or too simple by validators will be rejected and discarded. of scenarios and emphasizes finer-grained pixel-level perception, enhancing its applicability in real-world applications."
        },
        {
            "title": "3 MEVIS DATASET\n3.1 Motion Expression Annotation",
            "content": "Video Collection. We aim to build challenging video dataset that includes wide range of scenes to facilitate motion understanding. Based on publicly available video segmentation datasets with high-quality mask annotations [75], [76], [77], [78], we select those that satisfy our criteria for motion and object complexity. The video selection process adheres to the following rules: R1. In MeViS, we select videos having multiple objects within the frame; videos containing only one or two salient objects are not considered. We particularly seek videos that contain objects with similar appearances, exemplified by the first video in Fig. 1, which shows three similar looking parrots. R2. We select videos containing objects that demonstrate substantial motion and movement. Videos with objects that display minimal or no movement are excluded from MeViS. After reviewing over 4,000 candidate videos, we carefully selected the most appropriate and suitable videos that meet our requirements. By prioritizing quality over quantity, we finally chose 2,006 videos to create dataset that is diverse and representative of wide range of real-world complex video scenarios. The language expression annotation procedure for MeViS follows GRES [13] and ReferIt [12], using an interactive gamelike approach that involves two players taking turns to annotate and validate. An overview of the language expression annotation and validation process is shown in Fig. 2. The process of language expression annotation and validation is detailed as follows. Language Expression Annotation. We developed web-based annotation system for annotating language expressions in text format. The system randomly selects video from the MeViS dataset and displays all object masks of the selected video on the webpage. For single-target and multi-target expressions, the annotator needs to choose one or several objects from the video and write the corresponding referring expression according to the annotation guidelines. For no-target expressions, the annotator needs to write deceptive expressions without choosing any object. To ensure that the language expressions in MeViS align with our focus on motion-based video segmentation, we established several guidelines for annotating the language expressions: A1. Target objects must exhibit significant motion. Objects that remain stationary and have no motion interactions with other objects should be disregarded. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5 TABLE 1: Statistics of representative language-guided video segmentation datasets. The newly constructed MeViSv2 has the largest number of objects and language expressions. More importantly, MeViS specifically focuses on segmenting objects in videos as indicated by motion expressions, supports generalized referring expressions for multi-target/no-target, and provide audio format. This dataset facilitates the exploration of using motion expressions for object segmentation and grounding in videos. Dataset Year Pub. Video Object Expression Mask Object/ Video Target Object/ Experission Singletarget A2D Sentence [8] J-HMDB Sentence [8] DAVIS16-RVOS [3] DAVIS17-RVOS [3] Refer-YouTube-VOS [4] MeViSv1 [1] MeViSv2 2018 2018 2018 2018 2020 2023 [CVPR] [CVPR] [ACCV] [ACCV] [ECCV] [ICCV] [TPAMI] 3,782 928 50 90 3,978 2,006 2,006 4,825 928 50 205 7,451 8,171 8,171 6,656 928 100 1,544 15,009 28, 33,072 58k 31.8k 3.4k 13.5k 131k 443k 443k 1.28 1 1 2.27 1.86 4.28 4.28 1 1 n/a 1 1 1. 1.58 6,656 928 100 1,544 15,009 21,031 21,541 Multitarget 7,539 8, Notarget 3,503 Audio ))) 33,072 Fig. 3: Examples of the newly added motion reasoning and notarget expressions in MeViSv2. Motion reasoning expressions refer to the targets the masked in orange, while no-target expressions, though deceptive, do not refer to any objects."
        },
        {
            "title": "3.2 Dataset Analysis and Statistics",
            "content": "In TABLE 1, we present statistical analysis of the newly proposed MeViS dataset, using 5 previous referring video object segmentation datasets as references, including A2D Sentence [8], J-HMDB Sentence [8], DAVIS16-RVOS [3], DAVIS17RVOS [3], and Refer-YouTube-VOS [4]. As shown in TABLE 1, MeViS contains 2,006 videos and 8,171 objects. Compared to Refer-YouTube-VOS [4], which is based on the existing VOS dataset [62], MeViS has more objects (8,171 vs. 7,451), more expressions (33,072 vs. 15,009), and more annotation masks (443k vs. 131k). Compared to the previous conference version MeViSv1 [1], MeViSv2 offers more expressions (33,072 vs. 28,570) by including: 1) 999 motion reasoning expressions that through complex motion reasoning, and 2) 3,503 no-target expressions that are deceptive but do not refer to any objects in the video, as shown in Fig. 3. Motion reasoning expression necessitates reasoning based on implicit motion clues, while no-target expressions support the robustness study in language-guided video segmentation. Both additions expand the real-world applications of the MeViS dataset. Moreover, MeViSv2 provides audio format referring expressions to support multi-modal studies in the field. In the following, we discuss how the proposed dataset MeViS intentionally increases the complexities of language-guided video segmentation by considering the challenges of both linguistic and visual modalities. implicitly refer to the target More Challenging Videos. As shown in TABLE 1, MeViS has Fig. 4: The duration of videos and objects of MeViS and ReferYouTube-VOS [4], in seconds. The vertical lines and values in the legends represent the mean duration across the two datasets. The duration of both videos and objects in MeViS is significantly longer than Refer-YouTube-VOS. an average of 4.28 objects per video. This is significantly higher than all previous datasets and is more than twice the number in the largest previous dataset, Refer-YouTube-VOS. The increased number of objects per video introduces more complex relationships among objects and poses greater challenges for understanding video content. Furthermore, as shown in Fig. 4, MeViS contains considerably longer videos, with an average duration of 13.16 seconds, which is significantly longer than the 5.45 seconds of ReferYouTube-VOS [4] dataset. These long-term videos introduce unique challenges, such as frequent disappearance-reappearance and prolonged confusion among similar-looking objects. These intentional design choices make MeViS more complex and challenging for language-guided video segmentation. This is in contrast to existing datasets such as A2D Sentence [8] and DAVIS16RVOS [3], where only one or two salient objects per category are present, and the model can choose the most salient object as the target or identify the target object based on the category name. For example, in Fig. 5(b), there is only one person in the foreground, and the model can simply identify the target by the term person while ignoring skateboarding. The proposed MeViS dataset addresses this limitation by selecting videos with more objects that have diverse and dynamic motions. Moreover, MeViS includes many videos with objects of the same category, such as group of tigers or rabbits. By including more challenging videos, MeViS better simulates real-world scenarios, making it valuable resource for studying motion expression-guided video understanding in complex environments. More Challenging Target Objects. As we have included longer videos in our MeViS dataset, we have also observed significant increase in the duration of target objects. As shown in Fig. 4(b), the object durations in our dataset have an average IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6 Fig. 5: Comparison of MeViS and Refer-YouTube-VOS [4]: (a) Example from MeViS. (b) Example from Refer-YouTubeVOS [4]. Compared to Refer-YouTube-VOS: Videos in MeViS contain more objects in complex environments, making it impossible to identify the target object via saliency or category information alone. The number of target objects indicated by language expression in MeViS is arbitrary, from 0 to many. of 10.88 seconds, which is more than two times longer than the average duration of Refer-YouTube-VOS. The longer duration of target objects ensures sufficient object motions and increases the difficulty of motion understanding. In previous datasets, target objects are typically salient, dominant, and isolated. For example, in Fig. 5(b), the target person is the absolute only protagonist of the video. Inconspicuous objects are rarely the referred targets, which does not align with real-world applications. In contrast, the proposed MeViS dataset includes numerous targets that are inconspicuous, small, entangled with other objects, or in the background. For example, in Fig. 5(a), there are three giraffes with highly similar appearances, and the most salient/foreground one is not the target object in this sample, making it challenging to identify the target object(s) through saliency or category information alone. Then compared to previous datasets, such as A2D Sentence [8] and J-HMDB Sentence [8], which focus on few categories [4], our MeViS dataset includes more categories from open-world [75], [76], [77], [78], [79], presenting improved difficulties in the diversity of target objects. Generalized Referring Expressions. As shown in the Target Object/Expression of TABLE 1, previous datasets typically have one sentence referring to single object, i.e., one expression, one object has become de-facto rule. This implies that finding multiple objects requires multiple expressions, with each object being searched for individually. In contrast, we add more natural way of selecting target objects, allowing one expression to refer to multiple target objects, denoted as multi-target expression. An example of multi-target expression is shown in Fig. 5(a), where Giraffes turning around refers to two giraffes. As shown in TABLE 1, on average, each expression in MeViSv1 refers to 1.59 objects, which is larger than existing datasets where the average is only 1 object per expression. However, in previous version no-target expression is not considered, leading to undefined behavior when the given sentence does not match any object in the given video. To address this issue and enhance the practical applications of referring video segmentation, we further add no-target expressions through human annotation in MeViSv2, especially focusing on motion confusion like Moving coins from Fig. 6: Word cloud of the top 100 words in the MeViS dataset. MeViS has large number of words that describe motions, like walking, moving, playing, and many position words that are related to motions, such as left, right. right pile to left pile in Fig. 3. Allowing multi-target and notarget expressions makes MeViS more practical and generalized to real-world scenarios. Generalized referring expressions [13] help to enhance the models reliability and robustness in realistic scenarios, where any type of expression can occur unexpectedly. More Challenging Motion Clues. One of the key distinguishing aspects of the MeViS dataset is its emphasis on describing object motions in language expressions. The previous largest RVOS dataset Refer-YouTube-VOS [4] provides two types of language annotations: full-video expression and first-frame expression. The first-frame expression is based solely on static attributes of the first frame image, whereas the full-video expression considers the entire video. However, in many cases, even the full-video expressions contain static attributes that could potentially enable the target object to be identified in single frame, for example, person on the right dressed in blue black.... In contrast, to explore the practicality of employing motion expressions for object localization and segmentation in videos, MeViS is intentionally designed to include range of diverse and dynamic object motions, making it more challenging to identify the target object based on static attributes alone. In MeViS, there are significantly more motion expressions that explicitly identify the target object based on its distinctive actions or movements. The language expressions in the proposed MeViS contain more motion attributes, such as object position moving through the video and actions that span several frames. The word cloud of the newly proposed MeViS is visualized in Fig. 6. From the word cloud figure, we can observe that MeViS dataset has large number of words that describe motions, like walking, moving, playing, and many relative directions that are related to motions, such as left, right, etc. Multi-modal Referring Expressions: Text & Audio. Besides the text-based referring expressions, we further add audio-based referring expressions in the updated MeViSv2. Audio, as reflection of human cognition, is more natural, common, and convenient in daily interactions compared to text. It carries rich semantic information and captures nuances of tone, emotion, and emphasis that text alone cannot convey. These qualities aid in more precise target identification and segmentation. The newly added audio format in MeViSv2 supports not only audio-guided video object segmentation but also multi-modal referring expression tasks. By leveraging the strengths of both text and audio, multi-modal referring expressions offer significant advantages and flexibility in enhancing video understanding and supporting more natural and intuitive interactions. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
        },
        {
            "title": "3.3 Tasks Supported by MeViS",
            "content": "1) Referring Video Object Segmentation (RVOS). The proposed MeViS dataset is originally designed for referring video object segmentation (RVOS), emphasizing motion understanding of both linguistic and visual contents. Besides RVOS, MeViS is versatile and applicable to variety of other tasks, as outlined below. 2) Audio-guided Video Object Segmentation (AVOS). As described in Sec. 3.1, we further provide corresponding speech recordings to each textual expression in MeViSv2, enabling MeViS to be used for Audio-guided Video Object Segmentation (AVOS) [18], [80]. This task suits future embodied scenarios, where using speech to command robot is more convenient than inputting text. An intuitive solution is directly using automatic speech recognition models to convert audio to text, thus degenerating it to the aforementioned RVOS. However, this method overlooks the rich semantic information in audio, such as accent, emotion, speed, and noise [18]. Recent works [18], [81] demonstrate the potential of using audio as single modality without the need for text as an intermediary. Therefore, directly integrating audio with visual signals for achieving efficient referring segmentation is good direction. Additionally, compared to earlier AVOS datasets [18], [80], which are extended from simple RVOS [4], [82], [83] and have the drawback of relying on target saliency that could be judged from single frame, the audio version of MeViS inherits the complex relationships between expressions and targets from MeViS. This increases the tasks complexity and evaluates the models generalization in real-world scenarios. 3) Referring Multi-Object Tracking (RMOT). RMOT aims to detect and track objects by generating bounding box trajectories based on natural language descriptions. MeViS can be seamlessly adapted to RMOT by converting segmentation masks into bounding boxes. Unlike previous RMOT datasets such as Refer-KITTI [20], which focus on autonomous driving, MeViS includes wider variety of scenes, enhancing its relevance to real-world applications and its ability to test model generalization. Additionally, MeViS introduces several unique features, such as the no-target expressions and an emphasis on understanding long-term actions through natural language expressions. Moreover, MeViS offers significantly larger dataset, comprising 2,006 videos, 136,102 frames, and 33,072 expressions, in comparison to Refer-KITTIs [20] 18 videos, 6,650 frames, and 818 expressions. This makes MeViS better suited for large-scale model training. 4) Referring Motion Expression Generation (RMEG). Besides the aforementioned perception tasks, the MeViS dataset is also suitable for language generation tasks. One of the relevant tasks is video captioning [22], [84], [85], which aims to generate descriptive expressions given video. Traditional video captioning can be divided into two categories: single sentence video captioning [86], [87] and dense video captioning [88], [89], [90], [91], [92]. The former one requires to generate one expression that describes the video globally, which is only applicable for videos with an explicit theme or salient subject. In contrast, dense video captioning methods generate multiple expressions that caption multiple events [90] or objects [91] in the video. However, they both focus on description without the need to differentiate between different objects or events, meaning that the generated expressions are not unambiguously affiliated with the object. Different from traditional video captioning, we propose new task with the help of the MeViS dataset, namely Referring Motion Expression Generation (RMEG). The input of RMEG is video along with set of masks of specific target object(s) in this video. The model is expected to generate referring expression that unambiguously describes the targets motion and distinguishes it from other objects. This poses higher challenge to the methods regarding their scene and object understanding capability. Recently, object-oriented [93] and controllable [94] video captioning are proposed. However, they require an explicit object list input to decide which objects will appear in the output expression. In RMEG, we expect the model to find all relevant objects and form an appropriate expression by itself. 5) Applications in Additional Tasks such as AIGC and Beyond. MeViSv2 includes referring expressions in both text and audio formats, along with segmentation masks and bounding boxes, making it applicable to wide range of areas. We have already seen MeViS being used for tasks beyond those previously mentioned. For example, VIDiff [95] employs our dataset to train diffusion-based models for generative video editing, enabling the modification and translation of content based on user instructions. Merlin [96] uses our dataset to train Multi-modal Large Language Models (MLLMs) that can foresee the future based on present observations. These examples demonstrate the extensive potential and versatility of our dataset in various applications."
        },
        {
            "title": "4 LMPM++: A BASELINE APPROACH",
            "content": "The MeViS dataset introduces unique challenges in detecting and understanding object motions in both video and language contexts. Motions described by language expressions can occur over random number of frames, making it necessary to capture fleeting actions and movements that occur throughout the entire video. This presents significant challenges for recognizing motions in the video content and the corresponding language expressions. Detecting fleeting actions requires meticulous perceiving of every frame while comprehending complex and extended motion spanning multiple frames requires contextual understanding across the entire duration of the video. Current state-of-the-art methods [14], [15], [16] rely on random sampling of few frames, which may miss frames containing crucial information described by the given expression. Furthermore, these methods fail to effectively extract temporal contextual information and instead simply use spatialtemporal feature extractors due to the significant burden on computational resources of temporal communication. Additionally, as illustrated in Sec. 3, objects described by language expressions can vary from zero to multiple, requiring the output to cover from zero to an arbitrary number of objects. To address the challenges posed by MeViS, we propose new approach called Language-guided Motion Perception and Matching (LMPM++), as shown in Fig. 7. LMPM++ generates N1 language-based queries to identify potential target objects in the video, across frames, and produces object embeddings to represent each of them. Using language queries instead of conventional object queries can filter out irrelevant objects and ensure the efficiency and effectiveness of subsequent operations [5], [29]. Inspired by VITA [26], we represent objects using object embeddings, which provide instance-specific information, to reduce computational requirements [97], [98]. Recent developed large language models (LLM) have the ability to reason about complex sentences [99], [100], [101], [102]. Given their strengths, we employ LLM for motion modeling of both long-term motion and fleeting motion. After obtaining object embeddings from frames IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8 Fig. 7: The overview architecture of the proposed Language-guided Motion Perception and Matching (LMPM++). We first detect all possible target objects in each video frame via Language-Guided Extractor and represent them using object embeddings. Then, large language model is used to capture and reason the global temporal context from object embeddings, outputting the number of target objects No and the corresponding No <SEG> tokens for subsequent mask generation. Temporal-level contrastive loss is designed to enhance the understanding of temporal structure. Finally, object mask trajectories are generated using <SEG> token with Mask Decoder. in the video, we utilize LLM [99], [100] to obtain global view across frames with LoRA [103] fine-tuning strategy."
        },
        {
            "title": "To support multimodal",
            "content": "input, we design separate feature extraction branches for text and audio. For text prompts, we introduce <Text> and </Text> for LLM to recognize textreferring inputs. Extracted text features are projected through text-language projection layer and inserted between prompt tokens, forming the instruction: Can you track and segment <Text><Text Embedding></Text> in this video? The text embedding is generated by the Text Encoder ET . Similarly, for audio prompts, we use <Audio> and </Audio> tags, with audio features processed through an audio-language projection layer and inserted into prompt tokens. The instruction is: Can you track and segment <Audio><Audio Embedding></Audio> in this video? Audio embedding is produced by the Audio Encoder EA. In this way it unifies referring representations across modalities, enabling the LLM to handle them like language instructions. To deal with the newly added generalized referring expressions in MeViSv2 and further support multi-target and no-target outputs, we design the answer template of LLM as: No <SEG> . No denotes the predicted number of target objects as well as the number of <SEG> tokens. If No = 0, there is no target object, and No > 1 indicates multiple target objects. This answer template facilitates the LLM to better understand the generalized referring expressions and enhances the practicality of LMPM++. Another challenge is how to facilitate LLM to understand visual-temporal information, such as first jumping high and then jumping far or first jumping far and then jumping high. The model struggles to effectively construct temporal structure from the input object embeddings, leading to potential ambiguities and false positives during inference. To address this issue, we propose temporal-level contrastive loss to encode temporal knowledge. First, we randomly disrupt the object embeddings along the time axis, breaking the original sequential order. We then apply contrastive learning to distinguish the target <SEG> token from the disrupted <SEG> token. This is done by maximizing the similarity between the target <SEG> and the corresponding text or audio expression while minimizing the similarity between the disrupted token and its associated text or audio expression. Additionally, to increase the number of samples, we gather tokens across different GPUs and incorporate them into the process. The process can be TABLE 2: Temporal Context (TC) shows varying impacts on 3 datasets. Image-based methods, like VLT [5], can achieve stateof-the-art performance on DAVIS17-RVOS (D17R) [3] and ReferYouTube-VOS (RYV) [4], but cannot well handle the harder motion challenges in MeViS that require temporal context. Methods VLT [5] RFormer [14] VLT+TC RFormer+TC Type Image Video Video Video Temporal 1 frame 5 rand. frames All frames All frames D17R 60.4 60.2 60.3 59.9 RYV MeViS 63.1 62.8 62.7 63.0 27.8 31.0 35.5 36.3 formulated as multi-positive supervised contrastive learning: Lcon = 1 (cid:88) log a+P exp(< a, a+ > / ) aP,N exp(< a, > / ) (cid:80) , (1) where represents the anchor item, text or audio embedding, and a+ and are <SEG> embeddings. is the collection of positive samples matched to the language embedding. is the collection of negative samples that come from different objects and disrupted embeddings. The primary goal of Lcon is to refine the embedding space so that text or audio embedding align with the <SEG> embeddings that have the correct temporal order, while staying distanced from embeddings with incorrect temporal orders or embeddings of different objects. This approach effectively mitigates ambiguities caused by distractor embeddings and temporal inconsistencies, and significantly enhances the referring video understanding performance. Our model is trained end-to-end using combination of text classification loss, contrastive loss, and segmentation mask loss: = txtLtxt + bceLbce + diceLdice + conLcon, (2) where Ltxt denotes the auto-regressive cross-entropy loss, optimizing text generation accuracy for No and <SEG> tokens. The segmentation loss includes binary cross-entropy loss Lbce and DICE loss Ldice, which work together to enhance segmentation quality. These loss items are balanced by the weights txt, bce, dice, and con. During training, the model is guided by the ground-truth labels ytxt for text and for segmentation masks."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Dataset Setting. The proposed MeViSv2 dataset consists of total of 2,006 videos along with 33,072 sentences. These videos IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 3: Image-video cross-dataset validation. We train the models on referring image segmentation dataset Ref-COCO/+/g, and test their performance on video datasets. The models trained on images perform worse on MeViS than on the other two datasets. Training on Referring Image Segmentation Dataset Methods VLT [5] RFormer [14] Type Image Video DAVIS17-RVOS 54.2 55.6 RYV 46.1 45.2 MeViS 22.5 27.0 TABLE 4: Ablation study of LMPM++ in &F . ID LMPM Large language model Lcon MeViSv1 MeViSv2 ii iii 42.2 45.9 47.6 38.3 42.4 43.9 are split into three subsets, i.e., training set, validation set for daily online evaluation, and testing set for competition 1, which contain 1,712 videos, 140 videos, and 154 videos, respectively. Evaluation Metrics. For RVOS and AVOS tasks, following [3], [4], we employ and to assess segmentation performance on the newly proposed MeViS dataset. The region similarity metric computes the Intersection over Union (IoU) of the predicted and ground-truth masks, reflecting the segmentation quality. The F-measure reflects the contour accuracy of the prediction. To evaluate the overall performance, we calculate the average of these two metrics, denoted as &F . It is worth noting that for samples where no target is present, true positives are assigned and value of 1, whereas false negatives are given and value of 0. Besides, N-acc. and T-acc. [13] are employed to assess the models ability to identify no-target scenarios. N-acc. (Notarget accuracy) measures the models ability to identify no-target TP+FN , where TP is the number of correctly samples: N-acc. = identified no-target samples and FN is the number of no-target samples misclassified as target samples. T-acc. (Target accuracy) reflects how no-target generalization affects target performance: T-acc. = TN TN +FP , where TN is the number of correctly identified target samples and FP is the number of target samples misclassified as no-target. For the evaluation metrics of RMOT and RMEG tasks, please refer to Sec. 5.5 and Sec. 5.7, respectively. TP Implementation Details. We set all the hyper-parameters of Language-Guided Extractor including Backbone, Mask Head, and Transformer Decoder to the default settings of Mask2Former [104]. We train 150,000 iterations using AdamW optimizer [105] with learning rate of 0.00005. Tiny Swin Transformer [106] is used as our backbone. The input frames are resized to have minimum size of 360 pixels on the shorter side and maximum size of 640 pixels on the longer side, to ensure efficient memory usage on the GPU. We use Video-LLaMA-7B [107] as our large language model, with its vision encoder removed. For audio encoding, we employ the pre-trained ImageBind [108], following Video-LLaMAs processing pipeline. Our model is trained end-toend on 8 NVIDIA A6000 GPUs. For the hyper-parameter settings, we set N1, txt, bce, dice, and con to 20, 1, 2, 0.5, and 0.3, respectively. We use RoBERTa [109] as text encoder that is consistent with the ReferFormer and is frozen all the time. 1. The testing set is used for evaluation during the competition periods, such as https://lsvos.github.io/ and https://pvuw.github.io/. TABLE 5: Computational cost comparison on MeViSv2. 9 Methods LMPM [1] VISA [28] LMPM++ (ours) Video-LLaMA-7B 2.86 FPS #Params. &F N-Acc. T-Acc 72.9 32.1 66.39M 38.3 87.2 Chat-UniVi-13B 1.72 22.98B 40.7 87.4 7.08B 43.9 Backbone Swin-T 20.1 1.9 45."
        },
        {
            "title": "5.1 Dataset Necessity and Challenges",
            "content": "To show the necessity and validity of MeViS in motion expression understanding, we compare the results of state-of-the-art referring image segmentation method VLT [5] and referring video segmentation method ReferFormer [14] on DAVIS17-RVOS [3], ReferYouTube-VOS [4], and MeViS, as shown in TABLE 2. When trained on referring video segmentation dataset, such as ReferYouTube-VOS [4] and testing on itself, the image-based method VLT [5] that does not use any temporal design can achieve exceptional results of 60.4% &F and 63.1% &F on video datasets DAVIS17-RVOS [3] and Refer-YouTube-VOS [4], respectively, which are even better than video method ReferFormer [14]. The results suggest that for DAVIS17-RVOS [3] and Refer-YouTubeVOS [4], the temporal context is not essential, and image-based methods that use static clues can achieve good performance on these two datasets. However, on the proposed MeViS, VLT [5] only achieves score of 27.8% &F , suggesting that referring image segmentation methods without temporal designs struggle to address the unique challenges presented by videos in our dataset, particularly in handling motion, despite their success on other benchmark datasets. Furthermore, by comparing the results of VLT [5] with ReferFormer [14], which is trained using five randomly selected frames from the video, we find that ReferFormer outperforms VLT by large margin of 3.2% in terms of &F . This further highlights the importance of analyzing long-term motions in the MeViS dataset. In order to further prove this point, we enhance VLT and ReferFormer by incorporating an attention module at the head to perceive and gather global temporal context (TC in TABLE 2). Adding temporal context results in both VLT and ReferFormer achieving performance gain of approximately 5% &F , underscoring the significance of temporal context for MeViS. However, it is worth noting that longer temporal information does not necessarily lead to better performance on DAVIS17-RVOS and Refer-YouTube-VOS. We also conduct cross-dataset experiment, where models are trained on referring image datasets and tested on referring video datasets. As shown in TABLE 3, both the image-based method VLT [5] and video-based method ReferFormer [14] achieve competitive results on Refer-YouTube-VOS [4] and DAVIS17RVOS [3] when trained on image datasets Ref-COCO, RefCOCO+, and Ref-COCOg. These results suggest that the expressions in Refer-YouTube-VOS [4] and DAVIS17-RVOS [3] provide static clues like in the image domain, and many target objects can be identified by examining single frame solely. In contrast, when trained on referring image segmentation datasets and tested on MeViS, both VLT [5] and ReferFormer [14] perform worse, indicating that there is significant expression-gap (e.g., static vs. motion) between MeViS and these image domain datasets."
        },
        {
            "title": "5.2 Ablation Study of LMPM++",
            "content": "In TABLE 4, we present an ablation study of the proposed approach LMPM++. We conduct the following three experiments: (i) First, we utilize language queries as cues to detect potential target object trajectories and capture temporal context through motion IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 6: MeViSv1 Benchmark Results. Methods Reference &F URVOS [4] LBDT [16] MTTR [15] ReferFormer [14] VLT+TC [5] DsHmp [17] LMPM (ours) LMPM++ (ours) [ECCV20] [CVPR22] [CVPR22] [CVPR22] [TPAMI23] [CVPR24] [ICCV23] [TPAMI] 27.8 29.3 30.0 31.0 35.5 46.4 42.2 47.6 25.7 27.8 28.8 29.8 33.6 43.0 39.3 44. 29.9 30.8 31.2 32.2 37.3 49.8 45.1 50.9 aggregation, outputting trajectories whose similarity to language is greater than matching threshold. This variant, denoted as LMPM [1], achieves &F score of 42.2% on MeViSv1 and 38.3% on MeViSv2. However, LMPM struggles to understand implicit language information in motion reasoning expressions. Additionally, despite using cross-attention mechanism to gather temporal context, it overlooks the temporal sequential order within the video. These limitations reduce its effectiveness in processing long-term and complex motions. (ii) By incorporating large language model for motion perception, which includes predicting the number of objects as part of the output, the &F score improves significantly by 3.7% and 4.1% on MeViSv1 and MeViSv2, respectively. This improvement is due to the models ability to capture temporal contextual information and embed reasoning capabilities, which are critical for MeViS. Additionally, this variant enables the model to handle single-object, no-target, and multiobject expressions effectively. (iii) Given that MeViS contains motion expressions within video sequences, it is crucial for the model to understand temporal motion sequence. To enhance this capability, we introduce temporal-level contrastive loss Lcon, which further improves the models comprehension of temporal information. This variant outperforms (ii) by 1.7% and 1.5% &F on MeViSv1 and MeViSv2, respectively. TABLE 5 compares computational costs. LMPM++ introduces only slight increase in trainable parameters (+12.76M) over LMPM [1] due to the use of LoRA, though with slower inference speed. Compared to LLM-based methods such as VISA [28], LMPM++ is both faster and more parameter-efficient, as it leverages object tokens rather than frame-level features for handling long video sequences."
        },
        {
            "title": "5.3 MeViS Benchmark Results",
            "content": "Quantitative Results. We conduct comprehensive evaluation of the MeViS dataset to benchmark the performance of existing methods on the challenging motion-expression MeViSv1 in TABLE 6 and the newly introduced MeViSv2, which further include no-target and motion reasoning scenarios, in TABLE 7. We evaluate 1 modified image-based method VLT [5] and 5 recent state-ofthe-art video-based methods, including URVOS [4], LBDT [16], MTTR [15], ReferFormer [14], and DsHmp [17] on the validation set of MeViS. MeViSv1. The evaluation results presented in TABLE 6 show that previous state-of-the-art methods could only achieve performance ranging from 27.8% &F to 31.0% &F on the validation set of MeViSv1, while their results on other conventional datasets like Refer-YouTube-VOS [4] and DAVIS17-RVOS [3] are usually above 60% &F (see TABLE 8). 10 TABLE 7: MeViSv2 Benchmark Results. &F and &F show performance for single-target and multi-target samples, respectively. N-acc. and T-acc. represent no-target performance. Methods &F &F &F N-acc. T-acc. URVOS [4] LBDT [16] MTTR [15] ReferFormer [14] VLT+TC [5] DsHmp [17] LMPM (ours) LMPM++ (ours) 23.4 25.1 25.9 26.7 30.1 40.8 38.3 43.9 20.3 26.5 21.8 28.4 22.5 29.3 23.1 30.3 26.5 33.7 36.9 44.7 35.6 40.9 40.8 47.0 25.7 27.5 28.3 28.9 31.9 39.7 37.1 41.2 22.1 24.3 25.2 25.8 32.5 49.4 46.7 51.6 0.0 0.0 0.0 0.0 0.0 21.0 20.1 45. 100.0 100.0 100.0 100.0 100.0 84.1 72.9 87.4 MeViSv2. The evaluation results presented in TABLE 7 show that previous state-of-the-art methods struggle with no-target and motion reasoning scenarios. In contrast, the proposed LMPM++ effectively addresses these challenges by leveraging the capabilities of an embedded large language model. We further decompose the overall performance into different expression types: singletarget, multi-target, and no-target cases, corresponding to &F s, &F m, and N-acc. & T-acc. in TABLE 7, respectively. The results show that multi-target and no-target samples present greater challenges for methods that only output the top-1 object mask, such as ReferFormer [14]. This limitation arises because the top-1 strategy assumes the presence of single target object, default assumption in previous RVOS datasets such as Refer-YouTube-VOS [4] and DAVIS17-RVOS [3]. As result, it is inherently incapable of handling no-target samples, leading to very low (even 0) Nacc. scores. For multi-target cases, even if the top-1 mask is highly accurate, it can only capture one of the target objects. In contrast, methods using adaptive output strategies, such as DsHmp [17] and the proposed LMPM++, perform much better in multi-target scenarios. In multi-target scenarios, partially erroneous predictions have smaller impact on performance due to the larger ground truth area, whereas an error in single-target cases leads to significantly lower score due to small target area. These results highlight that the proposed MeViSv2 dataset presents significant challenges for evaluating models generalization abilities across variety of complex scenarios. Our experiments on MeViSv1 and MeViSv2 show that while notable progress has been made in language-guided video object segmentation on existing benchmarks, the new challenges introduced by MeViS underline the need for further exploration of motion expression-guided video segmentation in complex scenarios. These challenges can arise from various factors across both linguistic and visual modalities, such as the use of motion expressions, highly dynamic objects, and fast-paced motions in videos, all of which can adversely affect overall performance. Besides, MeViSv1 primarily focuses on explicitly referring to single or multiple objects, neglecting real-world scenarios where expressions may involve implicit information or might be intentionally or unintentionally incorrect, resulting in no objects being referred to. In MeViSv2, we introduce no-target expressions and motion reasoning expressions to make the dataset more reflective of realworld situations. Consequently, MeViSv2 is more challenging than MeViSv1, as evidenced by DsHmps [17] &F score being 5.6% (40.8% vs. 46.4%) lower on MeViSv2 compared to MeViSv1. MeViSv2 offers dataset that is more representative of real-world complexities and closer to practical applications like embodied AI. Visualizations. Fig. 8 presents both successful and unsuccessful IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 8: Results on Refer-YouTube-VOS and DAVIS17-RVOS. Method Reference &F Refer-YouTube-VOS DAVIS17-RVOS &F ReferFormer [14] HTML [111] R2-VOS [112] SgMg [113] TempCD [114] SOC [115] DsHmp [17] LoSh [110] LMPM++ (ours) ReferFormer [14] OnlineRefer [116] HTML [111] SgMg [113] TempCD [114] SOC [115] DsHmp [17] LoSh [110] LMPM++ (ours) Video-Swin-Tiny 58.0 59.4 59.5 61.2 59.6 61.3 60.4 62.0 60.5 62.3 61.1 62.4 61.8 63.6 62.0 63.7 64.0 62.2 Video-Swin-Base 61.3 62.9 61.0 62.9 61.5 63.4 63.9 65.7 63.6 65.8 64.1 66.0 65.0 67.1 65.4 67.2 65.7 67.8 [CVPR22] [ICCV23] [ICCV23] [ICCV23] [ICCV23] [NeurIPS23] [CVPR24] [CVPR24] [TPAMI] [CVPR22] [ICCV23] [ICCV23] [ICCV23] [ICCV23] [NeurIPS23] [CVPR24] [CVPR24] [TPAMI] 60.9 63.0 63.1 63.5 64.0 63.7 65.4 65.4 65.8 64.6 64.7 65.2 67.4 68.0 67.9 69.1 69.0 69. 59.6 - - 61.9 62.2 63.5 64.0 62.9 64.2 61.1 62.4 62.1 63.3 64.6 64.2 64.9 64.3 65.0 56.5 62.7 - - - - 59.0 64.8 59.3 65.0 60.2 66.7 60.8 67.2 60.1 65.7 60.9 67. 58.1 64.1 59.1 65.6 59.2 65.1 60.6 66.0 61.6 67.6 61.0 67.4 61.7 68.1 61.8 66.8 61.9 68.1 the action in the video, such as distinguishing between lowering their heads to eat and lowering their heads to drink. In (h), challenges arise when the motion involves complex interactions with other objects. These failure cases underscore the challenges posed by MeViS, highlighting the need for models to possess strong understanding of motion expression, world knowledge for effective reasoning, and an unbiased approach to scenarios where no target may be present."
        },
        {
            "title": "5.4 Results on Previous RVOS Datasets",
            "content": "Refer-YouTube-VOS & DAVIS17-RVOS. In TABLE 8, we report our results on Refer-YouTube-VOS [4] and DAVIS17-RVOS [3] datasets, where our method surpasses all existing approaches across various metrics. For Refer-YouTube-VOS, our model LMPM++ with the Video-Swin-Tiny backbone achieves score of 64.0% &F , which is an improvement of 0.3% over the previous best LoSh [110]. When utilizing larger backbone, specifically the Video-Swin-Base, our models performance further elevates to 67.8% &F , consistently outperforming all other methods by at least 0.6%. On DAVIS17-RVOS dataset, LMPM++ achieves the best performance of 65.0% &F with Video-Swin-Base. A2D Sentence & J-HMDB Sentence. We further evaluate the performance of LMPM++ on A2D Sentence and J-HMDB Sentence datasets [8], as shown in TABLE 9. Consistent with the approach in [14], the models are initially pre-trained on RefCOCO/+/g and subsequently fine-tuned on A2D Sentence. The J-HMDB Sentence dataset is only used for evaluation purposes. The proposed LMPM++ achieves new state-of-the-art results, surpassing the closest competitor, DsHmp [17], by 0.9% mAP on A2D Sentence and 0.4% mAP on J-HMDB Sentence, respectively. The performance gains of LMPM++ on the above four datasets, while notable, are relatively modest when compared to those on MeViS. This could be attributed to the nature of these four datasets, which primarily contain sentences with image-level descriptions for the first frame and do not strictly necessitate motion expressions. Nevertheless, LMPM++ maintains its state-of-the-art status, demonstrating its broad applicability and effectiveness. Fig. 8: Examples (a)-(d) show success cases, while (e)-(h) display failure cases of LMPM++. Examples (c)-(d) and (f)-(h) correspond to no-target samples. cases using the proposed approach LMPM++. Examples (a) and (b) show successful cases where LMPM++ accurately interprets expressions requiring reasoning and long-term motion tracking, such as get their food and goes out of the screen. Examples (c) and (d) demonstrate successful cases for no-target expressions. For the expression Cow running very fast to distance in (c), LMPM++ correctly avoids outputting mask, indicating its ability to understand concepts like fast and distance and recognize their mismatch with the visual content. In (d), LMPM++ accurately perceives directional cues like right, showcasing its spatial understanding. On the other hand, examples (e) to (h) present failure cases. Example (e) involves sentence describing long-term motion with multiple target objects. While our method initially identifies the correct targets, i.e., the two goats walking from the distance, the targets are lost in the later stages of the video when the object motions become more complex and intertwined. (f)-(h) highlight the inherent difficulty of no-target scenarios, particularly when dealing with fine-grained actions, complex object relationships, and subtle contextual differences. In example (f), the expression contains misleading language, where the entity in danger, chased by lion, should be identified as zebra instead of dog. Our method fails to disambiguate this confusing term, leading to an incorrect prediction. In (g), the model fails when the described action is highly similar to IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 9: Results on A2D Sentence and J-HMDB Sentence. TABLE 11: Results of AVOS on MeViSv2. J-HMDB Sentence A2D Sentence Method Reference mAP oIoU mIoU mAP oIoU mIoU MTTR [15] ReferFormer [14] HTML [111] SOC [115] SgMg [113] DsHmp [17] LoSh [110] LMPM++ (ours) ReferFormer [14] OnlineRefer [116] HTML [111] SOC [115] SgMg [113] DsHmp [17] LoSh [110] LMPM++ (ours) Video-Swin-Tiny 72.0 46.1 [CVPR22] 77.6 52.8 [CVPR22] 77.6 53.4 [ICCV23] 78.3 [NeurIPS23] 54.8 78.0 56.1 [ICCV23] 79.0 57.2 [CVPR24] 79.3 57.6 [CVPR24] 79.4 57.8 [TPAMI] Video-Swin-Base 78.6 55.0 [CVPR22] 79.6 - [ICCV23] 79.5 56.7 [ICCV23] 80.7 [NeurIPS23] 57.3 79.9 58.5 [ICCV23] 81.1 59.8 [CVPR24] 81.2 59.9 [CVPR24] 81.8 60.7 [TPAMI] 64.0 69.6 69.2 70.6 70.4 71.3 71.6 71.6 70.3 70.5 71.2 72.5 72.0 72.9 73.1 73.9 39.2 42.2 42.7 42.7 44.4 44.9 - 45.3 43.7 - 44.2 44.6 45.0 45.8 - 46.2 70.1 71.9 - 72.7 72.8 73.1 - 73.4 73.0 73.5 - 73.6 73.7 73.9 - 74. 69.8 71.0 - 71.6 71.7 72.1 - 72.4 71.8 71.9 - 72.3 72.5 73.0 - 73.4 TABLE 10: Results of RMOT on MeViSv2. * indicates that the RMOT metrics are adapted to account for no-target samples. Method Reference HOTA* DetA* AssA* N-acc. T-acc. TransRMOT [20] TempRMOT [21] LMPM++ LMPM++det [CVPR23] [PrePrint] [TPAMI] [TPAMI] 18.6 30.0 38.1 38.8 9.2 17.7 28.1 29.0 38.1 51.3 52.5 52.7 56.9 18.2 45.7 45.7 52.3 72.3 87.4 87."
        },
        {
            "title": "5.5 Referring Multi-Object Tracking Results",
            "content": "As mentioned in Sec. 3.3, MeViSv2 supports the RMOT task. TABLE 10 presents RMOT results on MeViSv2, comparing LMPM++ with previous state-of-the-art methods [20], [21]. For LMPM++, bounding boxes are derived from the bounding rectangles of the masks, while LMPM++det integrates an additional detection head. The metrics HOTA*, DetA*, and AssA* are adapted from RMOT [20] to account for no-target samples in MeViSv2. Originally, HOTA scored 0 for no-target samples; we modify this so that score of 1 is awarded when the model accurately predicts an empty bounding box trajectory, otherwise scoring 0. N-acc. and T-acc. [13] are also employed to evaluate performance on no-target identification. As shown in TABLE 10, the proposed LMPM++ outperforms other methods with the highest performance across several metrics, including HOTA* (38.1%), DetA* (28.1%), and T-acc. (87.4%). While TransRMOT [20] slightly outperforms in N-acc. (56.9% vs. 45.7%), our model shows significant overall improvements, especially in detection and target prediction accuracy. Compared to LMPM++, LMPM++det achieves better performance with an additional detection head for direct bounding box prediction and supervision using ground truth annotations. The above indicates the superior transfer performance of our method among different tasks."
        },
        {
            "title": "5.6 Audio-Guided Video Object Segmentation",
            "content": "TABLE 11 benchmarks AVOS methods on MeViSv2 dataset. WNet [18] and MUTR [19] are models that originally support audio as input, but they only achieve &F scores of 16.5% and 33.6%, respectively, highlighting the difficulty of MeViS. MUTRs N-acc. of 0% and T-acc. of 100% indicate that the inclusion of no-target cases significantly increases the challenge of the MeViS dataset, especially for models that tend to output one target for any given expression. For LMPM [1], three variMethods Reference &F N-acc. T-acc. WNet [18] MUTR [19] LMPMAudioText [1] LMPMAudio [1] LMPMGT Text [1] LMPM++ (ours) [CVPR22] [AAAI24] [ICCV23] [ICCV23] [ICCV23] [TPAMI] 16.5 33.6 36.8 37.5 38.3 42.3 16.6 30.9 33.2 34.5 35.6 39. 16.3 36.3 40.5 40.4 40.9 45.5 0.5 0.0 22.1 11.0 20.1 43.2 99.3 100.0 64.7 78.8 72.9 85.4 TABLE 12: Results of RMEG on MeViS. Methods Reference METEOR CIDEr GIT [22] VAST [23] NarrativeBridge [24] VideoLLaMA 2 [25] [TMLR22] [NeurIPS23] [PrePrint] [PrePrint] 12.33 10.66 14.99 15.68 18.20 20.42 25.68 27. ants are tested: LMPMGT Text, LMPMAudioText, and LMPMAudio. LMPMGT Text uses ground truth text as input, achieving the highest &F score and highlighting the advantage of accurate text data. LMPMAudioText converts audio to text using the Whisper-Base [117] model before inputting it into LMPM, but it slightly underperforms due to transcription accuracy issues. Finally, LMPMAudio directly extracts audio features using Whisper, replacing RoBERTa [109], but this approach faces additional challenges and lags behind the text-based variant, reflecting the complexities of direct audio processing. LMPM++ achieves the highest &F score of 42.3%, outperforming all LMPM variants and other methods on the AVOS task. It excels in both N-acc. (43.2%) and Tacc. (85.4%), demonstrating superior robustness and accuracy, particularly in handling no-target cases. This highlights the significant improvements LMPM++ brings over previous approaches."
        },
        {
            "title": "5.7 Referring Motion Expression Generation Results",
            "content": "Herein we benchmark several existing methods on the proposed task of Referring Motion Expression Generation (RMEG). Following existing works in classic video and image captioning tasks, we employ two commonly used metrics to evaluate model performance: METEOR [118] and CIDEr [119]. As shown in TABLE 12, we benchmark four methods on MeViS, including two traditional video captioning methods GIT [22] and VAST [23], one video captioning model that involves Large Language Models (LLM), NarrativeBridge [24], and one native LLM VideoLLaMA2 [25]. As most of the traditional methods do not support specifying certain objects, we highlight the target object with semitransparent mask throughout the video to help the methods focus on the object. Additionally, we instruct the LLM-based methods with the language prompt: Write an unambiguous referring expression that unambiguously describes the motion of the masked object(s) in the video. From TABLE 12, the METEOR scores of most methods are less than 15, indicating that the generated motion expressions lack accuracy. The CIDEr scores are also low, with the highest score being 27.10. We find that the LLM-based methods, such as NarrativeBridge [24] and VideoLLaMA 2 [25], outperform traditional methods by significant margin, e.g., NarrativeBridge [24] exceeds VAST [23] by over 5 points on the CIDEr metric. This suggests that the RMEG task requires strong reasoning abilities to unambiguously describe the motion of the target object in the video, which LLM-based methods may handle more effectively. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] H. Ding, C. Liu, S. He, X. Jiang, and C. C. Loy, MeViS: large-scale benchmark for video segmentation with motion expressions, in ICCV, 2023. 1, 2, 3, 5, 9, 10, 12 H. Ding, S. Tang, S. He, C. Liu, Z. Wu, and Y.-G. Jiang, Multimodal referring segmentation: survey, arXiv preprint arXiv:2508.00265, 2025. 1 A. Khoreva, A. Rohrbach, and B. Schiele, Video object segmentation with language referring expressions, in ACCV, 2018. 1, 2, 3, 5, 8, 9, 10, 11 S. Seo, J.-Y. Lee, and B. Han, Urvos: Unified referring video object segmentation network with large-scale benchmark, in ECCV, 2020. 1, 2, 3, 5, 6, 7, 8, 9, 10, 11 H. Ding, C. Liu, S. Wang, and X. Jiang, VLT: Vision-language transformer and query generation for referring segmentation, IEEE TPAMI, 2023. 1, 2, 3, 7, 8, 9, [7] [6] M. Bellver, C. Ventura, C. Silberer, I. Kazakos, J. Torres, and X. Giro-i Nieto, closer look at referring expressions for video object segmentation, Multim. Tools Appl., 2022. 1, 3 S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, Cross-modal progressive comprehension for referring segmentation, IEEE TPAMI, 2021. 1, 3 K. Gavrilyuk, A. Ghodrati, Z. Li, and C. G. Snoek, Actor and action video segmentation from sentence, in CVPR, 2018. 1, 2, 3, 5, 6, 11 L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, Modeling context in referring expressions, in ECCV, 2016. 2 J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, Generation and comprehension of unambiguous object descriptions, in CVPR, 2016. 2 [10] [8] [9] [11] C. Wu, Z. Lin, S. Cohen, T. Bui, and S. Maji, Phrasecut: Languagebased image segmentation in the wild, in CVPR, 2020. 2 [12] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, ReferItGame: Referring to objects in photographs of natural scenes, in EMNLP, 2014. 2, 4 [13] C. Liu, H. Ding, and X. Jiang, GRES: Generalized referring expression [14] segmentation, in CVPR, 2023. 2, 3, 4, 6, 9, 12 J. Wu, Y. Jiang, P. Sun, Z. Yuan, and P. Luo, Language as queries for referring video object segmentation, in CVPR, 2022. 2, 3, 7, 8, 9, 10, 11, 12 [15] A. Botach, E. Zheltonozhskii, and C. Baskin, End-to-end referring video object segmentation with multimodal transformers, in CVPR, 2022. 2, 3, 7, 10, [16] Z. Ding, T. Hui, J. Huang, X. Wei, J. Han, and S. Liu, Languagebridged spatial-temporal interaction for referring video object segmentation, in CVPR, June 2022. 2, 7, 10 [17] S. He and H. Ding, Decoupling static and hierarchical motion perception for referring video segmentation, in CVPR, 2024. 2, 3, 10, 11, 12 [18] W. Pan, H. Shi, Z. Zhao, J. Zhu, X. He, Z. Pan, L. Gao, J. Yu, F. Wu, and Q. Tian, Wnet: Audio-guided video object segmentation via waveletbased cross-modal denoising networks, in CVPR, 2022. 2, 3, 7, 12 [19] S. Yan, R. Zhang, Z. Guo, W. Chen, W. Zhang, H. Li, Y. Qiao, H. Dong, Z. He, and P. Gao, Referred by multi-modality: unified temporal transformer for video object segmentation, in AAAI, 2024. 2, 12 [20] D. Wu, W. Han, T. Wang, X. Dong, X. Zhang, and J. Shen, Referring multi-object tracking, in CVPR, 2023. 2, 3, 7, 12 [21] Y. Zhang, D. Wu, W. Han, and X. Dong, Bootstrapping referring multiobject tracking, arXiv preprint arXiv:2406.05039, 2024. 2, 3, 12 J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, GIT: generative image-to-text transformer for vision and language, Trans. Mach. Learn. Res., 2022. 2, 7, [22] [23] S. Chen, H. Li, Q. Wang, Z. Zhao, M. Sun, X. Zhu, and J. Liu, VAST: vision-audio-subtitle-text omni-modality foundation model and dataset, in NeurIPS, 2023. 2, 12 [24] A. Nadeem, F. Sardari, R. Dawes, S. S. Husain, A. Hilton, and A. Mustafa, Narrativebridge: Enhancing video captioning with causaltemporal narrative, arXiv preprint arXiv:2406.06499, 2024. 2, 12 [25] Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao, and L. Bing, Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms, 2024. [Online]. Available: https://arxiv.org/abs/2406.07476 2, 12, 13 [26] M. Heo, S. Hwang, S. W. Oh, J.-Y. Lee, and S. J. Kim, Vita: Video instance segmentation via object token association, in NeurIPS, 2022. 2, 7 J. Zhu, Z.-Q. Cheng, J.-Y. He, C. Li, B. Luo, H. Lu, Y. Geng, and X. Xie, Tracking with human-intent reasoning, arXiv preprint arXiv:2312.17448, 2023. 2 [27] Fig. 9: REMG failure cases of VideoLLaMA 2 [25] on MeViS. Fig. 9 shows some failure cases of VideoLLaMA2 [25]. We observe two significant drawbacks of previous video captioning methods on the RMEG task. First, the generated expression may fail to describe the motion and only output the object state in certain time range. For example, in Fig. 9 (a), the predicted expression, The elephant in the back, is non-motion description and only valid for the target at the beginning of the video. Second, when multiple similar objects are present in the video, the generated expressions may fail to distinguish between them or may even produce identical expressions for different objects. This loss of the referring property is unacceptable for RMEG. For example, in Fig. 9 (b) and (c), two people with similar appearances and movement trajectories are located in different places. The model fails to distinguish between them and generates the same expression for both, which is inadequate for the RMEG task."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSION",
            "content": "This paper introduces large-scale multi-modal dataset MeViSv2, designed to advance research in referring video understanding, especially with motion-centric language descriptions across diverse and complex scenarios. Through extensive benchmarks on MeViS, we demonstrate the limitations of existing methods in RVOS, AVOS, RMOT, and RMEG tasks, showing that these methods fall short in effectively leveraging motion expressions for video understanding. Moreover, we analyze the challenges and propose baseline approach LMPM++ to meet the challenges of the proposed MeViS dataset. Future Directions. There are many interesting research directions and remaining challenges to be addressed with the MeViS dataset. These include but are not limited to: (i) developing techniques for enhanced motion understanding and representation in both visual and linguistic domains, (ii) designing more rigorous and robust models that can effectively handle diverse motion types spanning across range of frames, including long-term, shortterm, and complex motions, (iii) developing advanced models that can handle complex scenes with various types of objects and expressions, (iv) creating more efficient models that can effectively reduce the number of detected redundant objects, (v) designing effective cross-modal fusion methods to better align the information between language and visual signals, (vi) investigating the potential of transfer learning and domain adaptation in languageguided video segmentation, (vii) developing methods that can better handle the open-world concepts in both the visual and linguistic domain. These challenges require significant research efforts to advance the research in language-guided video understanding. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 [28] C. Yan, H. Wang, S. Yan, X. Jiang, Y. Hu, G. Kang, W. Xie, and E. Gavves, Visa: Reasoning video object segmentation via large language models, in ECCV, 2024. 2, 9, 10 [57] W. Zhao, K. Wang, X. Chu, F. Xue, X. Wang, and Y. You, Modeling motion with multi-modal features for text-based video segmentation, in CVPR, 2022. [29] H. Ding, C. Liu, S. Wang, and X. Jiang, Vision-language transformer and query generation for referring segmentation, in ICCV, 2021. 2, 3, 7 [58] M. Sun, J. Xiao, E. G. Lim, and Y. Zhao, Starting point selection and multiple-standard matching for video object segmentation with language annotation, IEEE TMM, 2023. 3 [30] R. Hu, M. Rohrbach, and T. Darrell, Segmentation from natural language expressions, in ECCV, 2016. 2 [31] R. Li, K. Li, Y.-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, Referring image segmentation via recurrent refinement networks, in CVPR, 2018. 3 [32] C. Liu, Z. Lin, X. Shen, J. Yang, X. Lu, and A. Yuille, Recurrent multimodal interaction for referring image segmentation, in ICCV, 2017. 3 J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in CVPR, 2015. [33] [34] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, Context contrasted feature and gated multi-scale aggregation for scene segmentation, in CVPR, 2018. 3 [35] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg, Mattnet: Modular attention network for referring expression comprehension, in CVPR, 2018. 3 [36] C. Liu, X. Jiang, and H. Ding, Instance-specific feature propagation for referring segmentation, IEEE TMM, 2023. 3 [37] Y. Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, Locate then segment: strong pipeline for referring image segmentation, in CVPR, 2021. [38] K. He, G. Gkioxari, P. Dollar, and R. Girshick, Mask r-cnn, in ICCV, 2017. 3 [39] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, Linguistic structure guided context modeling for referring image segmentation, in ECCV, 2020. 3 [40] S. Yang, M. Xia, G. Li, H.-Y. Zhou, and Y. Yu, Bottom-up shift and reasoning for referring image segmentation, in CVPR, 2021. 3 [41] L. Ye, M. Rochan, Z. Liu, and Y. Wang, Cross-modal self-attention network for referring image segmentation, in CVPR, 2019. 3 [42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin, Attention is all you need, in NeurIPS, 2017. 3 J. Wu, X. Li, S. Xu, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong, X. Jiang, B. Ghanem, and D. Tao, Towards open vocabulary learning: survey, IEEE TPAMI, 2024. [43] [44] Z. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, Lavt: Language-aware vision transformer for referring image segmentation, in CVPR, 2022. 3 [45] Z. Wang, Y. Lu, Q. Li, X. Tao, Y. Guo, M. Gong, and T. Liu, Cris: Clip-driven referring image segmentation, in CVPR, 2022. 3 [47] [46] N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, Restr: Convolutionfree referring image segmentation using transformers, in CVPR, 2022. 3 J. Tang, G. Zheng, C. Shi, and S. Yang, Contrastive grouping with transformer for referring image segmentation, in CVPR, June 2023. 3 J. Liu, H. Ding, Z. Cai, Y. Zhang, R. K. Satzoda, V. Mahadevan, and R. Manmatha, Polyformer: Referring image segmentation as sequential polygon generation, in CVPR, June 2023. [48] [49] B. Yan, Y. Jiang, J. Wu, D. Wang, Z. Yuan, P. Luo, and H. Lu, Universal instance perception as object discovery and retrieval, in CVPR, 2023. 3 [50] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. 3 [51] H. Wang, C. Deng, F. Ma, and Y. Yang, Context modulated dynamic networks for actor and action video segmentation with language queries, in AAAI, 2020. 3 [52] K. Ning, L. Xie, F. Wu, and Q. Tian, Polar relative positional encoding for video-language segmentation, in IJCAI, 2020. [53] H. Wang, C. Deng, J. Yan, and D. Tao, Asymmetric cross-guided attention network for actor and action video segmentation from natural language query, in ICCV, 2019. 3 [54] B. McIntosh, K. Duarte, Y. S. Rawat, and M. Shah, Visual-textual capsule routing for text-based video segmentation, in CVPR, 2020. 3 [55] T. Hui, S. Huang, S. Liu, Z. Ding, G. Li, W. Wang, J. Han, and F. Wang, Collaborative spatial-temporal modeling for language-queried video actor segmentation, in CVPR, 2021. 3 [56] D. Wu, X. Dong, L. Shao, and J. Shen, Multi-level representation learning with semantic alignment for referring video object segmentation, in CVPR, 2022. 3 [59] W. Chen, D. Hong, Y. Qi, Z. Han, S. Wang, L. Qing, Q. Huang, and G. Li, Multi-attention network for compressed video referring object segmentation, in ACM MM, 2022. [60] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, Tubedetr: Spatio-temporal video grounding with transformers, in CVPR, 2022. 3 [61] Z. Tang, Y. Liao, S. Liu, G. Li, X. Jin, H. Jiang, Q. Yu, and D. Xu, Human-centric spatio-temporal video grounding with visual transformers, IEEE TCSVT, 2021. 3 [62] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, Youtube-vos: large-scale video object segmentation benchmark, arXiv preprint arXiv:1809.03327, 2018. 3, 5 J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool, The 2017 davis challenge on video object segmentation, arXiv preprint arXiv:1704.00675, 2017. 3 [63] [65] [64] C. Liang, Y. Wu, T. Zhou, W. Wang, Z. Yang, Y. Wei, and Y. Yang, Rethinking cross-modal interaction from top-down perspective for referring video object segmentation, arXiv preprint arXiv:2106.01061, 2021. 3 J. Zhou, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang, and Y. Zhong, Audio-visual segmentation, in ECCV, 2022. 3 J. Zhou, X. Shen, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang et al., Audio-visual segmentation with semantics, arXiv preprint arXiv:2301.13190, 2023. [66] [67] Y. Wang, P. Sun, D. Zhou, G. Li, H. Zhang, and D. Hu, Ref-avs: Refer and segment objects in audio-visual scenes, in ECCV, 2024. 3 [68] P. Anastassiou, J. Chen, J. Chen, Y. Chen, Z. Chen, Z. Chen, J. Cong, L. Deng, C. Ding, L. Gao et al., Seed-tts: family of high-quality versatile speech generation models, arXiv preprint arXiv:2406.02430, 2024. 3 [69] Y. Shao, S. He, Q. Ye, Y. Feng, W. Luo, and J. Chen, Contextaware integration of language and visual references for natural language tracking, in CVPR, 2024. 3 [70] Z. Li, R. Tao, E. Gavves, C. G. Snoek, and A. W. Smeulders, Tracking by natural language specification, in CVPR, 2017. 3 [71] Z. Yang, T. Kumar, T. Chen, J. Su, and J. Luo, Grounding-trackingintegration, IEEE TCSVT, 2020. 3 [72] Q. Feng, V. Ablavsky, Q. Bai, G. Li, and S. Sclaroff, Real-time visual object tracking with natural language description, in WACV, 2020. 3 [73] Y. Du, C. Lei, Z. Zhao, and F. Su, ikun: Speak to trackers without retraining, in CVPR, 2024. 3 [74] F. Zeng, B. Dong, Y. Zhang, T. Wang, X. Zhang, and Y. Wei, Motr: End-to-end multiple-object tracking with transformer, in ECCV, 2022. 3 J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille, P. H. Torr, and S. Bai, Occluded video instance segmentation: benchmark, IJCV, 2022. 4, [75] [76] W. Wang, M. Feiszli, H. Wang, and D. Tran, Unidentified video objects: benchmark for dense, open-world segmentation, in ICCV, 2021. 4, 6 [77] P. Voigtlaender, L. Luo, C. Yuan, Y. Jiang, and B. Leibe, Reducing the annotation effort for video object segmentation datasets, in WACV, 2021. 4, 6 [78] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai, MOSE: new dataset for video object segmentation in complex scenes, in ICCV, 2023. 4, 6 [80] [79] H. Ding, K. Ying, C. Liu, S. He, X. Jiang, Y.-G. Jiang, P. H. Torr, and S. Bai, MOSEv2: more challenging dataset for video object segmentation in complex scenes, arXiv preprint arXiv:2508.05630, 2025. 6 J. Lin, J. Chen, K. Peng, X. He, Z. Li, R. Stiefelhagen, and K. Yang, Echotrack: Auditory referring multi-object tracking for autonomous driving, arXiv preprint arXiv:2402.18302, 2024. 7 J. Zhan, J. Dai, J. Ye, Y. Zhou, D. Zhang, Z. Liu, X. Zhang, R. Yuan, G. Zhang, L. Li et al., Anygpt: Unified multimodal llm with discrete sequence modeling, arXiv preprint arXiv:2402.12226, 2024. 7 [82] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, Towards [81] understanding action recognition, in ICCV, 2013. 7 [83] C. Xu, S.-H. Hsieh, C. Xiong, and J. J. Corso, Can humans fly? action understanding with multiple classes of actors, in CVPR, 2015. 7 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15 [110] L. Yuan, M. Shi, Z. Yue, and Q. Chen, Losh: Long-short text joint prediction network for referring video object segmentation, in CVPR, 2024. 11, [111] M. Han, Y. Wang, Z. Li, L. Yao, X. Chang, and Y. Qiao, Html: Hybrid temporal-scale multimodal learning framework for referring video object segmentation, in ICCV, 2023. 11, 12 [112] X. Li, J. Wang, X. Xu, X. Li, B. Raj, and Y. Lu, Robust referring video object segmentation with cyclic structural consensus, in ICCV, 2023. 11 [113] B. Miao, M. Bennamoun, Y. Gao, and A. Mian, Spectrum-guided multi-granularity referring video object segmentation, in ICCV, 2023. 11, 12 [114] J. Tang, G. Zheng, and S. Yang, Temporal collection and distribution for referring video object segmentation, in ICCV, 2023. 11 [115] Z. Luo, Y. Xiao, Y. Liu, S. Li, Y. Wang, Y. Tang, X. Li, and Y. Yang, Soc: Semantic-assisted object cluster for referring video object segmentation, in NeurIPS, 2023. 11, [116] D. Wu, T. Wang, Y. Zhang, X. Zhang, and J. Shen, Onlinerefer: simple online baseline for referring video object segmentation, in ICCV, 2023. 11, 12 [117] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in ICML, 2023. 12 [118] M. Denkowski and A. Lavie, Meteor universal: Language specific translation evaluation for any target language, in Proceedings of the ninth workshop on statistical machine translation, 2014. 12 [119] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, Cider: Consensusbased image description evaluation, in CVPR, 2015. 12 [84] T.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y. Wang, L. Wang, and Z. Liu, An empirical study of end-to-end video-language transformers with masked visual modeling, in CVPR, 2023. [85] K. Yamazaki, S. Truong, K. Vo, M. Kidd, C. Rainwater, K. Luu, and N. Le, Vlcap: Vision-language with contrastive learning for coherent video paragraph captioning, in ICIP, 2022. 7 [86] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, Video captioning with attention-based lstm and semantic consistency, IEEE TMM, 2017. 7 [87] W. Pei, J. Zhang, X. Wang, L. Ke, X. Shen, and Y.-W. Tai, Memoryattended recurrent network for video captioning, in CVPR, 2019. 7 [88] M. Suin and A. Rajagopalan, An efficient framework for dense video captioning, in AAAI, 2020. [89] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang et al., mplug-2: modularized multi-modal foundation model across text, image and video, in ICML, 2023. 7 [90] H. Xu, B. Li, V. Ramanishka, L. Sigal, and K. Saenko, Joint event detection and description in continuous video streams, in WACV, 2019. 7 [91] X. Zhou, A. Arnab, C. Sun, and C. Schmid, Dense video object captioning from disjoint supervision, arXiv preprint arXiv:2306.11729, 2023. 7 [92] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev, J. Sivic, and C. Schmid, Vid2seq: Large-scale pretraining of visual language model for dense video captioning, in CVPR, 2023. 7 [93] F. Liu, X. Ren, X. Wu, B. Yang, S. Ge, Y. Zou, and X. Sun, O2na: An object-oriented non-autoregressive approach for controllable video captioning, in ACL, 2021. 7 [94] M. Cornia, L. Baraldi, and R. Cucchiara, Show, control and tell: framework for generating controllable and grounded captions, in CVPR, 2019. 7 [95] Z. Xing, Q. Dai, Z. Zhang, H. Zhang, H. Hu, Z. Wu, and Y.-G. Jiang, Vidiff: Translating videos via multi-modal instructions with diffusion models, arXiv preprint arXiv:2311.18837, 2023. [96] E. Yu, L. Zhao, Y. Wei, J. Yang, D. Wu, L. Kong, H. Wei, T. Wang, Z. Ge, X. Zhang, and W. Tao, Merlin: Empowering multimodal llms with foresight minds, in ECCV, 2024. 7 [97] X. Li, H. Ding, W. Zhang, H. Yuan, J. Pang, G. Cheng, K. Chen, Z. Liu, and C. C. Loy, Transformer-based visual segmentation: survey, IEEE TPAMI, 2024. 7 [98] X. Li, H. Yuan, W. Zhang, G. Cheng, J. Pang, and C. C. Loy, Tubelink: flexible cross tube baseline for universal video segmentation, ICCV, 2023. 7 [99] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. 7, 8 [100] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and impressing gpt-4 E. P. Xing, Vicuna: An open-source chatbot with 90%* chatgpt quality, March 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/ 7, 8 [101] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. 7 [102] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia, Lisa: Reasoning segmentation via large language model, in CVPR, 2024. 7 [103] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, in ICLR, 2022. 8 [104] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Masked-attention mask transformer for universal image segmentation, in CVPR, 2022. 9 [105] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2019. 9 [106] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021. [107] H. Zhang, X. Li, and L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, in EMNLP, 2023. 9 [108] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, Imagebind: One embedding space to bind them all, in CVPR, 2023. 9 [109] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: robustly optimized bert pretraining approach, arXiv preprint arXiv:1907.11692, 2019. 9,"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanyang Technological University",
        "Shanghai University of Finance and Economics"
    ]
}