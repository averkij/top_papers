{
    "paper_title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents",
    "authors": [
        "Pengxiang Zhao",
        "Guangyi Liu",
        "Yaozhen Liang",
        "Weiqing He",
        "Zhengxi Lu",
        "Yuehao Huang",
        "Yaxuan Guo",
        "Kexin Zhang",
        "Hao Wang",
        "Liang Liu",
        "Yong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents."
        },
        {
            "title": "Start",
            "content": "MAS-Bench: Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents Pengxiang Zhao1*, Guangyi Liu1*, Yaozhen Liang1*, Weiqing He1, Zhengxi Lu1, Yuehao Huang1, Yaxuan Guo2, Kexin Zhang3, Hao Wang2, Liang Liu2, Yong Liu1 1Zhejiang University, 2vivo AI Lab, 3Huzhou Institute of Zhejiang University {zhaopengxiang, guangyiliu, liang.yaozhen}@zju.edu.cn, {liangliu.vivoai}@vivo.com,{yongliu}@iipc.zju.edu.cn 5 2 0 2 8 ] . [ 1 7 7 4 6 0 . 9 0 5 2 : r Abstract To enhance the efficiency of GUI agents on various platforms like smartphones and computers, hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as promising direction. However, framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agents capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agents shortcut generation capabilities. MAS-Bench fills critical evaluation gap, providing foundational platform for future advancements in creating more efficient and robust intelligent agents. The project page is available at https://pengxiang-zhao.github.io/MAS-Bench."
        },
        {
            "title": "Introduction",
            "content": "The rise of Large Language Models (LLMs) (et al. 2024; Zhao et al. 2023; Minaee et al. 2024; Chang et al. 2024; Xu et al. 2024) is driving the development of Graphical User Interface (GUI) Agents, enabling them to operate diverse digital platforms such as computers, web browsers, and smartphones (Liu et al. 2025b; Tang et al. 2025; Liu et al. 2025a). Early research on mobile agents primarily focused on replicating human-like flexibility through GUIonly interaction (Lee et al. 2025; Gou et al. 2024; Cheng et al. 2024; You et al. 2024; Zhang et al. 2024; Lu et al. 2025). This approach grants agents the generality to operate on any application, but it often overlooks the significant efficiency advantages offered by more direct methods. *These authors contributed equally. Project Lead. Corresponding authors. Figure 1: Workflow of GUI Only vs. GUI-Shortcut Hybrid Agent. Shortcuts improve agent execution efficiency by bypassing GUI operations. As GUI agents are increasingly applied to complex tasks, enhancing their efficiency has become central research focus. In this context, hybrid paradigms have emerged as promising solution, demonstrating effectiveness across wide range of platforms (Wang et al. 2025b; Zheng et al. 2025; Zhang et al. 2025a,b; Jiang et al. 2025; Lee et al. 2023). This approach combines the speed and reliability of shortcuts such as Application Programming Interface (API) calls, deep links, and Robotic Process Automation (RPA) scripts (Bridle and McCreath 2006; Guerreiro, Gamboa, and Jorge 2008; Kennedy and Everett 2011; Tripathi 2018; Ling, Gao, and Wang 2020; Agostinelli et al. 2022), with the flexibility of GUI operations. As Fig. 1 illustrates, hybrid agent can bypass tedious, multi-step GUI operations by invoking single, efficient shortcut, drastically reducing operational complexity and time. However, despite these advancements, framework for systematically evaluating and benchmarking hybrid agents is still lacking (Le et al. 2020; Roffarello, Purohit, and Purohit 2024; Zhang et al. 2025a,b). This gap leaves the full potential of GUI-Shortcut agents far from being thoroughly explored and assessed. While this is general challenge, the mobile domain represents critical and demanding environment for progress. Therefore, we present MAS-Bench, the first benchmark explicitly designed to evaluate GUI-shortcut hybrid mobile agents. The tasks within MAS-Bench are solvable entirely through GUI interactions, but integrating shortcuts can improve efficiency. MAS-Bench evaluates agents ability to make optimal decisions between different operational modes to enhance performance. Our experiments on MAS-Bench show that hybrid agents achieve up to 64.1% success rate, significant improvement over the 44.6% achieved by their GUI-only counterparts, and execute tasks with over 40% greater efficiency. Furthermore, we introduce novel framework to evaluate agents capacity to generate new shortcuts from interaction. This framework integrates agent-generated shortcuts into standard baseline agent and measures subsequent task performance. Our findings reveal performance gap: while our predefined shortcuts prove highly reliable (100% success rate), agent-generated shortcuts lag in robustness and efficiency. Dynamic shortcuts demonstrate significant potential. Despite their task completion rate of 38% (baseline 43%), they offer the highest efficiency, highlighting them as promising direction for future research into robust shortcut generation. Our contributions are threefold: We introduce MAS-Bench, the first benchmark for systematically evaluating GUI-shortcut hybrid mobile agents. It comprises 139 complex tasks spanning 11 realworld applications, supported by knowledge base of 88 predefined shortcuts and 7 distinct evaluation metrics. We establish baselines with several agents on MASBench, demonstrating that those leveraging GUI-shortcut hybrid operations achieve significantly higher success rates and efficiency than their GUI-only counterparts. We propose the first framework to evaluate an agents ability to generate shortcuts from interaction. Our experiments reveal substantial gap between agent-generated shortcuts and predefined ones in efficiency and robustness, highlighting key area for future research."
        },
        {
            "title": "2 Related Work\nMobile Task Automation. Mobile\ntask automation\nfrom methods based on predefined scripts\nevolves\nto more intelligent and adaptive agents driven by\nLLMs\n(Kirubakaran and Karthikeyani 2013; Amalfi-\ntano et al. 2014; Linares-VÂ´asquez, Moran, and Poshyvanyk\n2017; Kong et al. 2018; Zhao, Harrison, and Yu 2024).\nTraditional approaches, such as API calls, deep links, and\nRobotic Process Automation (RPA) scripts, offer direct\nexecution paths but suffer from significant\nlimitations,\nincluding invalidation from app updates, and a sensitivity\nto UI changes that hinders their ability to adapt (Bridle\nand McCreath 2006; Guerreiro, Gamboa, and Jorge 2008;\nKennedy and Everett 2011; Le et al. 2020).",
            "content": "The rapid development of LLM-based GUI agents significantly overcomes traditional automation challenges by simulating human interaction through visual or multimodal inputs (Gou et al. 2024; Cheng et al. 2024; Baechler et al. 2024; Liu et al. 2025a). However, complete reliance on LLMs for fine-grained GUI interaction presents new problems: step-by-step action for routine tasks leads to inefficiency and increased costs; in complex operations, LLM hallucinations or misinterpretations can cause cumulative erPreprint under review Benchmark AndroidArena AndroidWorld LlamaTouch AndroidLab SPA-Bench # Inst. DS DL EC SG 221 116 495 138 MAS-Bench (Ours) 139 Table 1: Comparison of MAS-Bench and other smartphone agent benchmarks with dynamic environments. Column definitions: # Inst. (number of instructions), # DS (support diverse shortcuts), # DL (difficulty level), # EC (efficiency&cost metrics), # SG (evaluate shortcut generation). rors, affecting task success rates and reliability (Wen et al. 2024; Wang et al. 2024a). Zhang et al. (Zhang et al. 2025a) provide detailed analysis of API and GUI agents; the former gain attention for their stability and programming integration capabilities, while the latter possesses strong adaptability. These considerations regarding the efficiency and robustness of GUI agents lead to an exploration of how to combine LLM with efficient shortcut methods, thereby promoting the rise of research in GUI-Shortcut agents. GUI-Shortcut Hybrid Operations. Researchers explore GUI-Shortcut methods to enhance agent capabilities and address the persistent challenges in task completion rate, costs, and operational efficiency for LLM-based GUI agents. For instance, UFO2 (Zhang et al. 2025b), desktop AgentOS, demonstrates robust task execution through its multi-agent architecture and unified GUI-API action layer that deeply integrates with operating system and application features. Inspired by such work, AppAgentX (Jiang et al. 2025) improves efficiency by evolving high-level actions into shortcuts based on task execution history. MobileAgent-E (Wang et al. 2025a) uses its self-evolving module, learns and stores general-purpose Tips and reusable Shortcuts from past experiences to continuously improve performance and efficiency. While these efforts focused on the mobile domain are promising, they also highlight the urgent need for systematic method to evaluate the effectiveness of these emerging hybrid operations on mobile devices. Mobile GUI Agent Benchmark. As agents evolve from GUI-based interaction to hybrid GUI-Shortcut operations, they place new demands on benchmarking. Among existing benchmarks, MobileAgentBench (Wang et al. 2024c) focuses on the foundational GUI navigation and task completion capabilities of LLM-powered mobile agents. MobileEval-E (Wang et al. 2025a) includes complex, real-world mobile tasks that require long-horizon, cross-app interactions. While these benchmarks advance research in various dimensions, there remains need for benchmark that can comprehensively evaluate the ability of agents to intelligently discover, decide upon, and execute diverse shortcuts within GUI interactions and also measure the overall task effectiveness of this GUI-Shortcut hybrid operation (Zhang et al. 2025b)."
        },
        {
            "title": "3.2 GUI-Shortcut Hybrid Action Space\nTo complete tasks, agents in MAS-Bench operate within a\nhybrid action space that combines conventional GUI inter-\nactions with programmatic shortcuts. This space comprises\ntwo primary action types:",
            "content": "GUI Action. GUI actions simulate direct human interaction with an applications interface. This action space consists of basic operations such as click, swipe, and type, enabling the agent to navigate and interact with UI elements. Detailed definitions for each action are provided in Appendix A.2. GUI actions are essential for dynamic scenarios with unavailable predefined shortcuts. Shortcut Action. Shortcut actions can trigger specific functions or navigate to designated pages, bypassing multistep GUI actions. In MAS-Bench, shortcuts are categorized into two types: Predefined shortcuts and Agent-generated shortcuts. As illustrated in Fig. 2, the predefined shortcuts in MASBench primarily consist of APIs, deep-links, and RPA scripts. API: Direct programmatic control that executes specific functions by calling an applications internal services without UI interaction. This method can alter application state or retrieve data directly, as shown in Fig. 2(a). Deep Link: specialized URI that targets specific page or function within an application. As shown in Fig. 2(b), this enables direct navigation, bypassing multi-step GUI actions. RPA Script: An Automation script designed to handle specific, highly repetitive subtask. Unlike single action, an RPA script encapsulates complex workflow of GUI actions, API calls, or deep-links, consolidating multistep process into single, efficient shortcut action, as shown in Fig. 2(c). The agent-generated shortcuts are created dynamically by the agent itself. They are typically formed by identifying and abstracting repetitive subtasks from the agents execution history into new, executable routines. This capability allows the agent to learn from experience and progressively optimize its performance by creating customized shortcuts. Preprint under review Figure 2: Functional Comparison of APIs, Deep-Links, and RPA Scripts. The figure uses the Amazon app as an example: (a) the open cart() API call directly opens the shopping cart; (b) the search product() deep-link directly performs product search; and (c) an RPA script combines APIs, deep links, and GUI operations to automate complete workflow."
        },
        {
            "title": "3.3 Shortcut Knowledge Base",
            "content": "Predefined Shortcuts Knowledge Base. To validate the feasibility of hybrid GUI-shortcut operations on mobile devices, we construct predefined knowledge base of commonly used mobile shortcuts. Based on 11 popular apps, it includes 11 APIs, 70 Deep Links, and 7 custom RPA scripts, for total of 88 predefined shortcuts. The knowledge base provides agents with rich, queryable resource to select appropriate actions based on task instructions. To ensure real-world relevance, APIs were sourced from official documentation and static analysis of application packages. At the same time, Deep Links were identified by analyzing each applications declared URI schemes and entry points. The custom RPA scripts were designed to encapsulate common, high-repetition sub-tasks, testing an agents ability to leverage complex, pre-built automation routines. More details on the collection and design methodology are available in the Appendix A.3. Agent-Generated Shortcuts. Besides predefined shortcuts, MAS-Bench supports evaluating agent-generated shortcuts to test an agents learning and abstraction capabilities. In order to demonstrate the effectiveness of MASBench in evaluating agents capabilities of generating shortcuts, we have incorporated several contrasting shortcut generation methods for evaluation. We include Macro-level action trajectory replay (abbreviated as action replay) shortcuts that record and replay entire task or sub-task execution trajectories. We also incorporate dynamic shortcuts that require the agent to perform real-time grounding based on execution shortcuts for these recurrent operations. This design aims to test whether an agent, after completing sub-task via GUI operations for the first time, can identify the repetitive pattern and then autonomously generate an efficient shortcut for subsequent use."
        },
        {
            "title": "3.5 Systematic Hybrid Agent Evaluation\nGUI-Shortcut Hybrid Operation Evaluation. MAS-\nBench is designed to evaluate an agentâs ability to au-\ntonomously discover and utilize shortcuts to improve task\nsuccess, efficiency, and cost-effectiveness. This evaluation\nmandates that an agent possess sophisticated recognition and\ndecision-making capabilities. Agents must not only identify\navailable shortcuts based on the current task and environ-\nment but also critically assess their applicability and effi-\nciency. Furthermore, the agent must make optimal decisions\nbetween GUI operations and shortcuts to minimize opera-\ntional steps and time.",
            "content": "Shortcut Generation Evaluation. We expect GUI agents not only to rely on predefined shortcuts within the knowledge base but also to possess the capability for shortcut generation. The quality of agent-generated shortcuts serves as direct indicator of their generative capability. However, systematic framework to develop and evaluate the effectiveness of these generation methods is currently lacking. Therefore, MAS-Bench encourages and evaluates the agents ability to generate new shortcuts autonomously. As shown in Fig. 3, we design the following evaluation method. First, self-generation shortcut agent explores the MASBench environment and creates its shortcut knowledge base. To ensure fairness and eliminate interference from the intrinsic abilities of the agent under test, we use unified baseline GUI agent as the evaluated agent and import these shortcut knowledge bases into the baseline agent for task execution. The baseline agents performance on the tasks directly reflects the quality of the imported knowledge base, thus enabling an evaluation of each agents shortcut generation capabilities."
        },
        {
            "title": "3.6 Evaluation Metrics\nWe use the following key evaluation metrics to measure an\nagentâs performance on MAS-Bench comprehensively. We\nencourage agents to efficiently complete tasks by utilizing\nexisting and self-generated shortcuts to achieve lower costs\nand greater efficiency.",
            "content": "Success Rate (SR). We use SR to measure whether an agent completes task. Success is typically defined as the agent reaching the final goal state of the task while satisfying all of its requirements. Efficiency. We use the following metrics to evaluate the impact of shortcut actions on agents efficiency, measured by the time and operations required to complete task: Mean Steps (MS): The average number of steps an agent takes to complete task. Mean Step Ratio (MSR): The ratio of the number of steps an agent takes to complete task to the optimal Figure 3: Evaluation Workflow for Agents Shortcut Generation Capability. The process consists of two stages: (a) Shortcut Generation Stage, where the agent creates its shortcut knowledge base; and (b) Quality Evaluation Stage. The Agent-generated shortcuts is imported into baseline agent for performance testing. The quality of the Agent-generated shortcuts is then measured by comparing this performance against the GUI-only baseline agent and the baseline agent with predefined shortcuts. history, as well as shortcuts generated by the MobileAgent-E framework. This diverse set of generation strategies allows for comprehensive assessment of an agents ability to autonomously create efficient workflows."
        },
        {
            "title": "3.4 Tasks Design for GUI-Shortcut Hybrid Agent\nMAS-Bench comprises 139 complex tasks derived from\nreal-world scenarios across 11 Android applications, each\nfeaturing one or more invocable predefined shortcuts. These\ntasks reflect authentic user needs, with an average of 9.27\nsteps for single-app and 17.66 for cross-app workflows\nbased on human operation. A detailed list of the tasks is pro-\nvided in Appendix A.4.",
            "content": "Specifically, tasks in MAS-Bench maintain high realworld relevance. They are designed by considering user needs from daily life, such as searching for products in shopping app or navigating to location. The tasks typically involve multiple steps and sub-goals, and usable shortcuts exist within our established knowledge base for each task. However, our shortcut knowledge base for these complex tasks is intentionally incomplete. It offers only basic shortcuts, which are insufficient to complete an entire task independently. This design compels agents to synthesize solutions by combining shortcut invocations with multi-step GUI operations, thus providing more realistic test of their planning and reasoning abilities. To test the agents learning and generation capabilities, we introduce scenarios with repetitive sub-tasks. In these scenarios, the knowledge base intentionally lacks predefined Preprint under review Figure 4: The pipeline of MAS-Bench. The GUI-Shortcut agent first filters products using the search product shortcut, selects an item via GUI operations, and then adds it to the cart using the add to cart shortcut. The entire process is monitored by an automated evaluation module, which outputs metrics such as success rate and efficiency. number of steps required to complete the task. lower ratio indicates better efficiency in task execution. Mean Execution Time (MET): The average time an agent takes to complete task. Cost and Resource Utilization. Cost metrics evaluate the computational resources and overhead an agent consumes to complete task. high-performing agent should minimize the overall cost by intelligently utilizing or autonomously generating shortcuts. We use the following metrics to evaluate the cost of an agent: Mean kTokens Cost (MToC): The average number of kTokens the LLM consumes during the task execution. Mean Shortcut Call Count (MSC): The average number of shortcuts the agent calls during the task execution. Shortcut Success Rate (SSR): The ratio of successfully executed shortcut calls to the total number of shortcut calls made by the agent. GUI to Shortcut Action Ratio (GSAR): This metric reflects an agents operational strategy preference by calculating the ratio of GUI to shortcut operations. high ratio, when combined with low token consumption and short execution time, provides strong evidence that the agent utilizes shortcuts both efficiently and intelligently. For automated evaluation, we adapt the methodology from SPA-Bench (Chen et al. 2024), embedding shortcut action descriptions to determine task success or failure. MASBench provides fair, reproducible, and in-depth assessment of mobile agents comprehensive capabilities in complex GUI-Shortcut tasks through these multi-dimensional evaluation metrics. It specializes in agents potential for dynamic adaptation and innovative shortcut generation. By doing so, MAS-Bench offers valuable insights and direction for the future development of mobile agents."
        },
        {
            "title": "4 MAS: Shortcut-Augmented Hybrid Agents",
            "content": "To evaluate the effectiveness of GUI-Shortcut hybrid operation, we introduce MAS-MobileAgent, which serves as reference for GUI-shortcut Hybrid Agents. MASMobileAgent is based on MobileAgent-V2 (Wang et al. 2024b), which primarily relies on visual screenshots for UI understanding and automation. MAS-MobileAgent retrieves relevant shortcuts from knowledge base and inputs them, along with perceptual information from screenshots, into the model to inform action decisions. To test the generalization of our shortcut knowledge base and injection method further, we introduce MAS-T3A. Based on T3A (Rawles et al. 2024), this agent relies on structured UI tree instead of visual information. By applying the same shortcut knowledge base to this agent with different perceptual foundation, we aim to demonstrate the versatility of our approach, showing it can be seamlessly integrated into different agent architectures. Preprint under review"
        },
        {
            "title": "5.1 Experiment Setup\nEvaluation with Predefined Shortcut. We evaluate the\nperformance of GUI frameworks on the 139 tasks in\nin-\nMAS-Bench: 1) Frameworks with GUI-only action,\ncluding T3A (Rawles et al. 2024), M3A (Rawles et al.\n2024), and MobileAgentV2 (Wang et al. 2024b). 2) self-\ngenerated shortcut frameworks,\nincluding MobileAgent-\nE (Wang et al. 2025a). 3) shortcut-augmented Hybrid GUI\nAgents (GUI-shortcut agents), including MAS-T3A and\nMAS-MobileAgent mentioned in Sec . 4 with predefined\nshortcut. We employ Gemini-2.5-Pro as the base model for\nall frameworks. The evaluation utilizes metrics from Sec\n. 3.6, with detailed configurations in Appendix A.",
            "content": "Evaluation of Shortcut Generation. To assess the quality of shortcut knowledge bases generated by different strategies, we conduct two-stage evaluation on randomly selected test subset comprising 50% of the tasks in MAS-Bench (see Appendix C.2 for the task list). In the Shortcut Generation Stage, we construct several types of agent-generated shortcuts to form distinct knowledge bases. These are created using two methods: the first leverages MobileAgent-E to generate shortcuts (SMobileAgent-E). The second, based on the historical execution trajectories of M3A, generates three types of shortcuts: task-level action replays (SReplay-Task), subtask-level action replays (SReplay-Subtask), and dynamic shortcuts (SDynamic ) that require the model to perform real-time grounding, the construction of the agent-generated shortcut knowledge base is detailed in Sec. 3.3. In the Quality Evaluation Stage, each knowledge base is mapped to standardized action space and then integrated into the T3A baseline agent. This method treats the baseline agents performance as direct reflection of the imported knowledge bases quality, enabling an unbiased evaluation of the different generation strategies."
        },
        {
            "title": "5.2 Effectiveness of GUI-Shortcut Operation\nBased on our experimental results in Table 2, we identify\nthree key findings:",
            "content": "Finding 1: Integrating predefined shortcut knowledge base significantly enhances agent performance, thereby validating the effectiveness of the hybrid GUI-shortcut operation. Experiments demonstrate that using just the predefined knowledge base of 88 shortcuts can significantly improve an agents task success rate and efficiency. For single-app tasks, compared to the baseline T3A, MAS-T3A improves the success rate (SR) from 51.1% to 57.6% and reduces the token cost from 346.382k to 291.391k. For agents that may terminate prematurely on long-horizon tasks due to errors, such as MobileAgent-E in the Table 2, the Mean Preprint under review Figure 5: Performance comparison of MAS-MobileAgent with and without shortcuts. The base models are Gemini2.5-Pro and Gemini-2.0-Flash. Data points show the relationship between SR and MET for single-app and crossapp tasks, with circle size representing mean cost. Results demonstrate that shortcuts benefit both models, with more significant improvements for the weaker Gemini-2.0-Flash. Step Ratio on Successful tasks (MSRS) provides more accurate assessment of the efficiency gains from shortcuts. MAS-MobileAgent achieves an MSRS of 0.613 on singleapp tasks, which is significantly lower than the baseline versions 1.058, indicating that its execution path is closer to the optimal solution. The SR and MS results for Different-level Tasks can be found in the Appendix B.2. Finding 2: The effectiveness of predefined shortcuts is framework-agnostic and can benefit agents with different input modalities. Experiments show that MAS-T3A and MAS-MobileAgent benefit from predefined shortcuts, regardless of whether they are based on UI Tree or screenshots. The reason lies in that the use of API, Deeplink, and RPA scripts is independent of the agents perception modality. These scripts directly send commands to the operating system or application, bypassing the need for the agent to parse the UI layout in real-time. Finding 3: The performance gain of shortcuts is more significant when the base model is weak. For less capable models like Gemini-2.0-Flash, which often struggle with complex GUI navigation, shortcuts provide crucial way to bypass failure-prone steps. This directly translates into substantial increase in task success. For instance, on CROSS-app tasks, augmenting the Gemini-2.0-Flash agent with shortcuts boosts its success rate from 0% to 23.4% (see the Appendix B.3 for detailed experimental results). Shortcuts mainly contribute to efficiency improvements for strong models (e.g., Gemini-2.5-Pro). As shown in Fig. 5, the performance gain of using shortcuts is significantly greater for Gemini-2.0-Flash than for Gemini-2.5-Pro."
        },
        {
            "title": "SS VH",
            "content": "SR"
        },
        {
            "title": "Cost",
            "content": "MSRS MET MToC MSC GSAR T3A (Rawles et al. 2024) M3A (Rawles et al. 2024) MobileAgentV2 (Wang et al. 2024b) MobileAgent-E (Wang et al. 2025a) MAS-T3A (Ours) MAS-MobileAgent (Ours) Single-app Tasks (92 Tasks) - 0.511 0.565 0.446 0.359 0.576 0.641 1. 1.056 1.064 1.058 0.818 0.915 0.613 - - 137.641 192.775 1013.386 459.574 129.279 682.547 346.382 155.281 120.212 88.772 291.391 99.780 Cross-app Tasks (47 Tasks)"
        },
        {
            "title": "Human",
            "content": "T3A (Rawles et al. 2024) M3A (Rawles et al. 2024) MobileAgentV2 (Wang et al. 2024b) MobileAgent-E (Wang et al. 2025a) MAS-T3A (Ours) MAS-MobileAgent (Ours) - 0.340 0.383 0.170 0.064 0.511 0.617 1.000 1.087 1.262 1.247 0.934 0.643 0.829 - - 257.122 411.145 2053.133 469.109 185.911 1441. 625.970 288.833 227.128 85.859 440.222 189.836 - 0 0 0 0.378 1.043 1.348 - 0 0 0 2.250 2.213 3.128 - 0 0 0 0.081 0.117 0.345 - 0 0 0 0.177 0.185 0.320 Table 2: Performance comparison of our MAS agents and baseline methods on MAS-Bench with predefined shortcuts knowledge base. All agents utilize the Gemini-2.5-Pro. SS and VH refer to the Screenshot and View Hierarchy (UI Tree) input modalities. MSRS is the Mean Step Ratio on Successful tasks, MET is the Mean Execution Time in seconds, and MToC is the Mean Token Cost in thousands (kTokens). The SSR for the predefined shortcuts is 1.0. In our experiments, Predefined shortcuts achieve the best performance, improving the success rate (SR) by 9% over the baseline while maintaining perfect shortcut success rate (SSR) of 100%. Notably, agents using Predefined shortcuts reduce average execution steps by 25% and decrease total execution time by approximately 16%. Despite having the second-highest shortcut call count per task (1.45), Predefined shortcuts maintain excellent robustness, executing successfully across diverse task configurations. The increased success rate and reduced execution time validate that welldesigned shortcuts will enhance GUI task efficiency. In contrast, Replay-Task shortcuts exhibit poor robustness with only 10% success rate, indicating high susceptibility to environmental variations. This fragility negatively impacts both task completion rates and execution efficiency, underscoring that while effective shortcuts boost GUI performance, poorly designed ones are counterproductive. Our findings reveal substantial room for improvement in model-generated shortcuts. Among all variants, only Predefined and MobileAgent-E shortcuts improve model accuracy, with only Predefined shortcuts simultaneously enhancing both execution speed and success rate. These results indicate current limitations in shortcut generation capabilities within our framework. Future research should focus on developing more efficient, robust shortcuts with higher utilization rates that effectively reduce execution steps."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduce MAS-Bench, the first benchmark\ndesigned to evaluate the effectiveness of GUI-shortcut hy-",
            "content": "Preprint under review Shortcut SR SSR MSRS MSC MET Human Baseline SPredefined SReplay-Task SReplay-Subtask SDynamic SMobileAgent-E - 0.43 0.52 0.34 0.43 0.38 0.49 - - 1.00 0.10 0.73 0.75 0.71 1.00 0.96 0.71 0.91 1.13 0.82 1.00 - - 1.45 3.04 1.22 0.91 1.01 - 188.93 152.15 244.61 236.67 216.24 224.87 Table 3: The results of different shortcut generation methods. Column definitions: # SR (success rate), # MSRS (Mean Step Ratio on Successful tasks), # MSC (Mean Shortcut Call Count), # SSR (Shortcut Success Rate), # MET (Mean Execution Time). brid mobile agents. Furthermore, MAS-Bench provides framework to access the quality and validity of shortcuts that are autonomously generated by the agent. Our experiments demonstrate that the GUI-shortcut hybrid operational approach enhances both the success rate and efficiency of task execution. Moreover, we also validate that our benchmark effectively measures the quality and robustness of the agent-generated shortcuts. Limitations and Future Work. Despite our efforts to provide comprehensive evaluation, the range of comparative baselines remains limited due to the currently underexplored research in Mobile GUI-shortcut hybrid Agents. Therefore, we hope to pay more attention to the efficiency of mobile GUI agents in the future, promoting research into GUI-shortcut Hybrid Mobile Agents."
        },
        {
            "title": "7 Acknowledgments\nThis work was supported by Huzhou Natural Science\nFoundation under Grant 2024YZ01 and in part by the\nPostdoctoral Fellowship Program of CPSF under Grant\nGZC20241491.",
            "content": "References Agostinelli, S.; Lupia, M.; Marrella, A.; and Mecella, M. 2022. Reactive synthesis of software robots in RPA from user interface logs. Computers in Industry, 142: 103721. Amalfitano, D.; Fasolino, A. R.; Tramontana, P.; Ta, B. D.; and Memon, A. M. 2014. MobiGUITAR: Automated model-based testing of mobile apps. IEEE software, 32(5): 5359. Baechler, G.; Sunkara, S.; Wang, M.; Zubach, F.; Mansoor, H.; Etter, V.; Carbune, V.; Lin, J.; Chen, J.; and Sharma, A. 2024. Screenai: vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615. Bridle, R.; and McCreath, E. 2006. Inducing shortcuts on mobile phone interface. In Proceedings of the 11th international conference on Intelligent user interfaces, 327329. Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; et al. 2024. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3): 145. Chen, J.; Yuen, D.; Xie, B.; Yang, Y.; Chen, G.; Wu, Z.; Yixing, L.; Zhou, X.; Liu, W.; Wang, S.; et al. 2024. Spa-bench: comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents. Cheng, K.; Sun, Q.; Chu, Y.; Xu, F.; Li, Y.; Zhang, J.; and Wu, Z. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. et al., O. 2024. GPT-4 Technical Report. arXiv:2303.08774. Gou, B.; Wang, R.; Zheng, B.; Xie, Y.; Chang, C.; Shu, Y.; Sun, H.; and Su, Y. 2024. Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents. arXiv preprint arXiv:2410.05243. Guerreiro, T.; Gamboa, R.; and Jorge, J. 2008. MnemoniIn Procal body shortcuts: improving mobile interaction. ceedings of the 15th European conference on Cognitive ergonomics: the ergonomics of cool interaction, 18. Jiang, W.; Zhuang, Y.; Song, C.; Yang, X.; Zhou, J. T.; and Zhang, C. 2025. AppAgentX: Evolving GUI Agents as Proficient Smartphone Users. arXiv preprint arXiv:2503.02268. Kennedy, C.; and Everett, S. E. 2011. Use of cognitive shortcuts in landline and cell phone surveys. Public Opinion Quarterly, 75(2): 336348. Kirubakaran, B.; and Karthikeyani, V. 2013. Mobile application testingChallenges and solution approach through In 2013 International Conference on Pattern automation. Recognition, Informatics and Mobile Engineering, 7984. IEEE. Preprint under review Kong, P.; Li, L.; Gao, J.; Liu, K.; Bissyande, T. F.; and Klein, J. 2018. Automated testing of android apps: systematic literature review. IEEE Transactions on Reliability, 68(1): 4566. Le, H. V.; Mayer, S.; WeiÃ, M.; Vogelsang, J.; Weingartner, H.; and Henze, N. 2020. Shortcut gestures for mobile text editing on fully touch sensitive smartphones. ACM Transactions on Computer-Human Interaction (TOCHI), 27(5): 1 38. Lee, J.; Lee, D.; Choi, C.; Im, Y.; Wi, J.; Heo, K.; Oh, S.; Lee, S.; and Shin, I. 2025. Safeguarding Mobile GUI Agent via Logic-based Action Verification. arXiv:2503.18492. Lee, S.; Choi, J.; Lee, J.; Wasi, M. H.; Choi, H.; Ko, S. Y.; Oh, S.; and Shin, I. 2023. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation. arXiv preprint arXiv:2312.03003. Linares-Vasquez, M.; Moran, K.; and Poshyvanyk, D. 2017. Continuous, evolutionary and large-scale: new perspective In 2017 IEEE Internafor automated mobile app testing. tional Conference on Software Maintenance and Evolution (ICSME), 399410. IEEE. Ling, X.; Gao, M.; and Wang, D. 2020. Intelligent document In 2020 processing based on RPA and machine learning. Chinese Automation Congress (CAC), 13491353. IEEE. Liu, G.; Zhao, P.; Liu, L.; Chen, Z.; Chai, Y.; Ren, S.; Wang, H.; He, S.; and Meng, W. 2025a. LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark. arXiv preprint arXiv:2504.13805. Liu, G.; Zhao, P.; Liu, L.; Guo, Y.; Xiao, H.; Lin, W.; Chai, Y.; Han, Y.; Ren, S.; Wang, H.; et al. 2025b. Llm-powered gui agents in phone automation: Surveying progress and prospects. arXiv preprint arXiv:2504.19838. Lu, Z.; Chai, Y.; Guo, Y.; Yin, X.; Liu, L.; Wang, H.; Xiong, G.; and Li, H. 2025. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620. Minaee, S.; Mikolov, T.; Nikzad, N.; Chenaghlu, M.; Socher, R.; Amatriain, X.; and Gao, J. 2024. Large language models: survey. arXiv preprint arXiv:2402.06196. Rawles, C.; Clinckemaillie, S.; Chang, Y.; Waltz, J.; Lau, G.; Fair, M.; Li, A.; Bishop, W.; Li, W.; Campbell-Ajala, F.; et al. 2024. AndroidWorld: dynamic benchmarkarXiv preprint ing environment for autonomous agents. arXiv:2405.14573. Roffarello, A. M.; Purohit, A. K.; and Purohit, S. V. 2024. Trigger-Action Programming for Wellbeing: Insights From 6590 iOS Shortcuts. IEEE Pervasive Computing. Tang, F.; Xu, H.; Zhang, H.; Chen, S.; Wu, X.; Shen, Y.; Zhang, W.; Hou, G.; Tan, Z.; Yan, Y.; et al. 2025. Survey on (M) LLM-Based GUI Agents. arXiv preprint arXiv:2504.13865. Tripathi, A. M. 2018. Learning Robotic Process Automation: Create Software robots and automate business processes with the leading RPA toolUiPath. Packt Publishing Ltd. Wang, F.; Zhang, Z.; Zhang, X.; Wu, Z.; Mo, T.; Lu, Q.; Wang, W.; Li, R.; Xu, J.; Tang, X.; et al. 2024a. comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and trustworthiness. arXiv preprint arXiv:2411.03350. Wang, J.; Xu, H.; Jia, H.; Zhang, X.; Yan, M.; Shen, W.; Zhang, J.; Huang, F.; and Sang, J. 2024b. MobileAgent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration. arXiv preprint arXiv:2406.01014. Wang, L.; Deng, Y.; Zha, Y.; Mao, G.; Wang, Q.; Min, T.; Chen, W.; and Chen, S. 2024c. MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents. arXiv preprint arXiv:2406.08184. Wang, Z.; Xu, H.; Wang, J.; Zhang, X.; Yan, M.; Zhang, J.; Huang, F.; and Ji, H. 2025a. Mobile-Agent-E: SelfarXiv Evolving Mobile Assistant for Complex Tasks. preprint arXiv:2501.11733. Wang, Z. Z.; Gandhi, A.; Neubig, G.; and Fried, D. 2025b. arXiv Inducing programmatic skills for agentic tasks. preprint arXiv:2504.06821. Wen, H.; Tian, S.; Pavlov, B.; Du, W.; Li, Y.; Chang, G.; Zhao, S.; Liu, J.; Liu, Y.; Zhang, Y.-Q.; et al. 2024. AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation. arXiv preprint arXiv:2412.18116. Xu, Y.; Liu, X.; Sun, X.; Cheng, S.; Yu, H.; Lai, H.; Zhang, S.; Zhang, D.; Tang, J.; and Dong, Y. 2024. AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents. arXiv preprint arXiv:2410.24024. You, K.; Zhang, H.; Schoop, E.; Weers, F.; Swearngin, A.; Nichols, J.; Yang, Y.; and Gan, Z. 2024. FerretUI: Grounded Mobile UI Understanding with Multimodal LLMs. arXiv preprint arXiv:2404.05719. Zhang, C.; He, S.; Li, L.; Qin, S.; Kang, Y.; Lin, Q.; and Zhang, D. 2025a. API Agents vs. GUI Agents: Divergence and Convergence. arXiv preprint arXiv:2503.11069. Zhang, C.; Huang, H.; Ni, C.; Mu, J.; Qin, S.; He, S.; Wang, L.; Yang, F.; Zhao, P.; Du, C.; et al. 2025b. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603. Zhang, J.; Yu, Y.; Liao, M.; Li, W.; Wu, J.; and Wei, Z. 2024. UI-Hawk: Unleashing the screen stream understanding for gui agents. Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Zhao, Y.; Harrison, B.; and Yu, T. 2024. Dinodroid: Testing android apps using deep q-networks. ACM Transactions on Software Engineering and Methodology, 33(5): 124. Zheng, B.; Fatemi, M. Y.; Jin, X.; Wang, Z. Z.; Gandhi, A.; Song, Y.; Gu, Y.; Srinivasa, J.; Liu, G.; Neubig, G.; and Su, Y. 2025. SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills. arXiv:2504.07079. Preprint under review MAS-Bench Environment A.1 Observation Space MAS-Bench evaluates mobile agents within highly standardized and controllable environment using an Android Virtual Device (AVD). Specifically, is based on Pixel 7 emulator running Android 15 (API Level 35). Its screen resolution is set to 1080 2400 pixels, and the screen density is 420 dpi. The emulator has 24GB of RAM and 24GB of storage. The environment can support parallel execution of multi-threaded evaluations. the environment A.2 Details of Action Space Action Tap (x, y) Type (text) Swipe (x1, y1, x2, y2) Home Back Stop Definition Tap at coordinates (x, y). Enter text content into an input field. Swipe from (x1, y1) to (x2, y2). Go to the home screen. Go back to the previous app screen. Complete the current task. Table 4: Examples of the basic action space in MAS-Bench. The MAS-Bench environment facilitates agent-device interaction through low-level interface based on the Android Debug Bridge (ADB). Crucially, the specific action space available to an agent during evaluation ultimately depends on its architecture and capabilities. Table 4 shows examples of the basic action set. A.3 Details of Predefined Shortcuts Knowledge"
        },
        {
            "title": "Base",
            "content": "Details of Predefined Shortcuts. Our shortcut knowledge base was created to provide agents with an authentic yet challenging environment. We collect and design shortcuts for 11 selected applications, ensuring their relevance to realworld usage patterns. APIs Collection: The API shortcuts in our knowledge base are application-specific internal APIs. We identify these APIs through official Android documentation. We employ static analysis of the application packages for internal APIs that lack public documentation to identify key functional endpoints. Deep-Links Collection: We collect deep-links by analyzing each applications AndroidManifest.xml file. This file typically declares the URL schemes and paths the application can handle, providing direct mappings to specific pages or functions. RPAs Script Design: RPA scripts are designed for common, highly repetitive sub-tasks (e.g., adding an item with specific options to shopping cart). These scripts can be executed as single action, serving as high-level shortcut. This design enables us to test an agents ability to utilize complex, pre-built automation routines. Preprint under review Details of Agent-Generated Shortcuts. We incorporate several methods to create agent-generated shortcuts. The primary purpose for including this variety of shortcut types is to validate the effectiveness of the MAS-Bench framework itself in evaluating an agents shortcut generation capabilities. By creating test cases with diverse characteristics, we can verify our benchmarks ability to assess different shortcut generation strategies. Macro-level action trajectory replay (action replay) Shortcuts: This category of shortcuts is constructed by recording and replaying an agents historical action trajectories. They execute fixed sequence of low-level actions, such as clicks and swipes at specific coordinates, to automate previously completed workflow. This method is implemented at two distinct granularities: Task-Level Replay: Captures and replays the entire action sequence of full task, from start to finish. This results in single, high-level macro that is highly specific to one complete workflow. Subtask-Level Replay: Identifies and abstracts recurrent, multi-step operations within larger task. This approach generates shorter, more modular shortcuts that automate common sub-workflows. Dynamic Shortcuts: Unlike static macro-replays, these shortcuts are adaptive and robust to UI changes. Instead of replaying hardcoded coordinates, they utilize real-time grounding to semantically identify and interact with target UI elements based on the current screen context. This allows them to function correctly even if the position or layout of UI elements changes. Ultimately, this diverse suite of shortcut types validates that MAS-Bench provides comprehensive framework for evaluating agents shortcut generation capabilities. A.4 Details of Tasks in MAS-Bench We have developed specialized benchmark suite to validate the feasibility of GUI-shortcut hybrid actions for mobile agents and establish framework for the comprehensive evaluation of an agents ability to utilize and generate shortcuts. This suite comprises 139 complex tasks strategically designed across 11 popular real-world applications. The tasks are categorized into 92 single-app tasks and 47 cross-app workflows, with their difficulty distribution illustrated in Table 6. We have also curated corresponding knowledge base of predefined shortcuts to support these tasks. Table 9 presents breakdown of the selected applications and the number of single-app tasks and shortcuts associated with each. Details of Predefined Shortcut Evaluation B.1 Details of MAS-MobileAgent and MAS-T3A Compared to MobileAgentV2, MAS-MobileAgent acquires the standard screenshot and UI elements and retrieves relevant and potentially applicable shortcuts from the shortcut knowledge base, based on the current task goal and interface context. This retrieved shortcut information is injected"
        },
        {
            "title": "Base Model",
            "content": "SR"
        },
        {
            "title": "Cost",
            "content": "MSRS MET MToC MSC GSAR Single-app Tasks (92 Tasks) MobileAgentV2 Gemini-2.0-Flash MAS-MobileAgent Gemini-2.0-Flash Gemini-2.5-Pro MobileAgentV2 Gemini-2.5-Pro MAS-MobileAgent 0.250 0.402 0.446 0. 1.280 0.719 1.058 0.613 472.426 335.181 1013.386 682.547 40.406 39.597 120.212 99.780 Cross-app Tasks (47 Tasks) MobileAgentV2 Gemini-2.0-Flash MAS-MobileAgent Gemini-2.0-Flash Gemini-2.5-Pro MobileAgentV2 Gemini-2.5-Pro MAS-MobileAgent 0 0.234 0.170 0. - 0.738 1.247 0.829 952.660 403.798 2053.133 1441.586 106.501 71.350 227.128 189.836 0 3.565 0 1.348 0 5.362 0 3.128 0 0.505 0 0. 0 0.709 0 0.320 Table 5: Evaluation results of different base models. MSRS is the Mean Step Ratio on Successful tasks, MET is the Mean Execution Time in seconds, and MToC is the Mean Token Cost in thousands (kTokens). Difficulty Level Proportion (%) Human Steps Level 1 Level 2 Level 3 27.3 47.5 25.2 6.2 12.6 17. Table 6: Distribution of task difficulty levels in MAS-Bench. The table shows the proportion of tasks at each level and the average number of steps required for human to complete them. as an additional context into the prompt provided to the base model, along with screen information and the task goal. MAS-T3A operates on similar principle, but its core distinction lies in its input modality: it relies on the structured UI tree, whereas MAS-MobileAgent processes visual screenshots. By evaluating both agents, we aim to demonstrate that the effectiveness of predefined shortcuts is framework-agnostic and can benefit agents with fundamentally different perceptual modalities. B.2 Success Rate on Different-Level Tasks As shown in Table 7, an analysis of metrics such as MSRS reveals that the GUI-shortcut hybrid agent, by utilizing the predefined shortcuts knowledge base, demonstrates improvements in both success rate and efficiency across tasks of varying difficulty levels when compared to the GUI-only agent."
        },
        {
            "title": "Influence of Base Model Capability",
            "content": "B.3 To assess the impact of GUI-shortcut hybrid actions on base models with varying capabilities, we conducted experiments using Gemini 2.0-Flash and Gemini 2.5-Pro. Our experiments, detailed in Table 5, reveal that the performance gain from shortcuts is significantly more pronounced when the base model is weak. For the less capable Gemini 2.0-Flash, shortcuts are critical for task completion. This is most apparent in cross-app tasks, where shortcuts boost the Success Preprint under review Agent T3A M3A MobileAgentV MobileAgentE MAS-T3A MAS-MobileAgent DL SR MS MSRS Level 1 Level 2 Level Level 1 Level 2 Level 3 Level 1 Level 2 Level 3 Level 1 Level 2 Level 3 Level 1 Level 2 Level 3 Level 1 Level 2 Level 3 71.1 34.8 42. 65.8 51.5 31.4 52.6 34.8 17.1 44.7 20.0 17.1 76.3 53.0 37.1 84.2 65.2 37.1 7.5 17.6 21. 7.7 17.0 25.3 18.2 30.6 46.8 8.8 5.5 6.2 6.2 13.2 20.7 8.0 18.8 32.2 0.93 1.04 1. 1.02 1.19 1.04 1.04 1.10 1.24 0.84 0.82 0.60 0.78 0.82 0.96 0.54 0.76 0.79 Table 7: Performance comparison of our MAS agents and baseline methods on MAS-Bench across different difficulty levels. Column definitions: # DL (difficulty level), # SR (success rate), # MS (Mean step), # MSR (Mean Step Ratio on Successful tasks). Rate (SR) from 0 to 0.234, making previously impossible tasks solvable. Efficiency also improves dramatically, with Mean Execution Time (MET) cut by over 57% in these scenarios. Details of Shortcut Generation Evaluation C.1 Shortcut Generation Method To validate the effectiveness of the MAS-Bench framework in evaluating an agents shortcut generation capabilities, we C.2 Subset Task To validate our framework for evaluating an agents shortcut generation capability, we randomly selected subset of 50% from the 139 tasks in MAS-Bench. This subset comprises total of 69 tasks, including 46 single-app and 23 cross-app tasks. Bad Case Analysis We further analyze the performance of the GUI-shortcut hybrid agent on failed tasks, with particular focus on cases where the GUI-only agent succeeds, but the GUI-shortcut hybrid agent fails. Our analysis of these failure cases reveals three primary Selection and Planning Errors. This category of error pertains to flaws in the agents task comprehension and strategic planning. These errors typically occur when the agent fails to accurately map the natural language instruction to the correct shortcut, resulting in the invocation of an irrelevant shortcut that leads to task failure. For instance, in an experiment with MAS-MobileAgent using Gemini-2.0-Flash (Fig. 7(a)), the agent was given the instruction: Navigate to the Attractions section. Search for attractions near Berlin for the 6th of next month. After launching the application, the agent made planning error by incorrectly invoking the search hotel() shortcut, which was irrelevant to the Attractions goal, thus causing the task to fail. Behavioral and Adaptation Errors. This error class occurs when the agent fails to evaluate the outcome of an action and adjust its subsequent action. The deficiency typically stems from an inability to learn from the action histories and modify future behavior accordingly. As shown in Fig. 7(b), when tasked to Get the search results for the short videos Stephen Curry. Select the first Short in the results and like it. Leave comment good shot!, The agent successfully uses shortcuts to search for the video. However, at the step requiring it to select the first video, it fails to switch to GUI-based action and instead erroneously calls the open shorts() shortcut again. This results in an unproductive loop, demonstrating failure to adapt its operational mode based on the tasks state. Execution and Formulation Errors. This type of error is specific to the invocation of agent-generated shortcuts. As noted in the main text, the execution success rate for these shortcuts is considerably low. Failures typically result from the shortcut being poorly formulated during the generation phase, rendering it inherently flawed or not robust enough to handle slight variations in the UI during execution. Figure 6: Examples of the resulting shortcut types. Action Replay shortcuts (Task-Level and Subtask-Level) use sequence of actions with fixed indices, while Dynamic Shortcuts use variable arguments that correspond to UI elements. generated diverse set of agent-generated shortcuts using various methods. We employ M3A as our baseline agent to generate different types of shortcuts. Without altering its core operational logic, M3A autonomously explores the environment and generates task execution trajectories, utilizing Gemini2.5-Pro as its base model. Upon completing each task, we construct prompt containing the task instruction, the whole action history, and the reasoning for each step. This prompt is then provided to Gemini-2.5-Pro, generating the corresponding shortcuts. Examples of the resulting shortcut types are shown in Fig. 6. As detailed in Table 8, this process resulted in 46 Subtask-level Macro-Replay, 45 Dynamic, 39 Task-level Macro-Replay, and 36 MobileAgent-E shortcuts. Shortcut Type Number of Shortcuts SReplay-Task SReplay-Subtask SDynamic SMobileAgent-E 39 46 45 36 Table 8: Number of agent-generated shortcuts by different methods. Preprint under review App name"
        },
        {
            "title": "YouTube",
            "content": "Amazon Booking.com Google Calendar Google Maps BBC Fitbit Gmail Chrome Yelp Contacts Tasks Shortcuts Shortcut Example 11 10 10 9 9 8 8 7 8 4 10 10 2 8 16 3 4 6 3 youtube.search video query(\"cat\") youtube.open shorts() amazon.open amazon() amazon.open cart() booking.open hotelid() booking.open my trips() calendar.open calendar main() calendar.create event(title=\"Meeting\") google map.navigate to address(\"Beijing\") google map.search address(\"Square\") bbc.open news() bbc.open live() fitbit.open fitbit main() fitbit.open sleep log() gmail.open gmail() gmail.open setting() chrome.search query(\"Python\") chrome.open incognito() yelp.open yelp main() yelp.open events() contacts.open contacts main() contacts.open contact starred() Table 9: List of MAS-Bench apps and number of single-app tasks and shortcuts for each one. Preprint under review Figure 7: Examples of GUI-shortcut hybrid agent failure cases. (a) Selection and Planning Error: The agent incorrectly invokes search hotel() shortcut instead of searching for attractions, demonstrating failure in mapping natural language instructions to appropriate shortcuts. (b) Behavioral and Adaptation Error: The agent repeatedly calls open shorts() shortcut instead of switching to GUI-only action to select the video, showing an inability to adapt operational mode based on task state. Preprint under review"
        }
    ],
    "affiliations": [
        "Huzhou Institute of Zhejiang University",
        "Zhejiang University",
        "vivo AI Lab"
    ]
}