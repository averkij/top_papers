{
    "paper_title": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "authors": [
        "Ho Kei Cheng",
        "Masato Ishii",
        "Akio Hayakawa",
        "Takashi Shibuya",
        "Alexander Schwing",
        "Yuki Mitsufuji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio"
        },
        {
            "title": "Start",
            "content": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis Ho Kei Cheng1* Masato Ishii2 Alexander Schwing1 1University of Illinois Urbana-Champaign Akio Hayakawa2 Takashi Shibuya2 Yuki Mitsufuji2,3 2Sony AI 3Sony Group Corporation {hokeikc2,aschwing}@illinois.edu, {masato.a.ishii,akio.hayakawa,takashi.tak.shibuya,yuhki.mitsufuji}@sony.com 4 2 0 2 9 1 ] . [ 1 2 2 3 5 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using novel multimodal joint training framework (MMAudio). In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with largerscale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with flow matching objective, MMAudio achieves new video-to-audio stateof-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder singlemodality performance. Code and demo are available at: hkchengrex.github.io/MMAudio. 1. Introduction We are interested in Foley, i.e., for given video we want to synthesize ambient sound (e.g., rain, river flow) and sound effects induced by visible events (e.g., dog barks, racket hits tennis ball). Note, Foley does not focus on synthesizing background music or human speech, which is often added in post-processing. Importantly, Foley requires to synthesize convincing high-quality audio that is 1) semantically and 2) temporally aligned to an input video. For semantic alignment, methods need to understand scene contexts and their association with audio the visual concept of rain should be associated with the sound of splashing raindrops. For temporal alignment, methods need to understand audio-visual synchrony as humans can perceive audio-visual misalignment as slight as 25 ms [47]. Inspired by the efficacy of training data *Work done during an internship at Sony AI. Figure 1. In addition to training on audio-visual-(text) datasets, we perform multimodal joint training with high-quality, abundant audio-text data which enables effective data scaling. At inference, MMAudio generates conditions-aligned audio with video and/or text guidance. scaling demonstrated by recent works [7, 19, 49], we pursue data-driven approach to synthesize high-quality audio that respects these two types of alignment. Current state-of-the-art video-to-audio methods either train only on audio-visual data [1] from scratch [63] or train new control modules [18, 62, 65, 69] for pretrained textto-audio models on audio-visual data. The former is limited by the amount of available training data: the most commonly used dataset VGGSound [1] contains only around 550 hours of videos. Audio-visual data are expensive to curate at large scale, as in-the-wild videos from the Internet 1) mostly contain music and speech1, which has limited utility for training general Foley model, and 2) contain non-diegetic [54] sounds such as background music or sound effects added in post-processing that are not suited for Foley model. The latter, i.e., finetuning pretrained text-to-audio models (with some new adapter layers) on audio-visual data, enables to benefit from audio-generation knowledge obtained from larger-scale audio-text data (e.g., WavCaps [43] at 7,600 hours). However, adding control modules to pretrained textto-audio models complicates the network architecture and 1In AudioSet [8] (collected from YouTube), 82.48% of videos contain either human speech or music according to the provided audio labels. 1 limits the design space. It is also unclear whether pretrained text-to-audio models have sufficient degrees of freedom to support all video-to-audio scenarios, compared to training from scratch. To avoid these limitations, we propose multimodal joint training paradigm (Figure 1) that jointly considers video, audio, and text in single transformer network and masks away missing modalities during training. This allows us to train from scratch on both audio-visual and audio-text datasets with simple end-to-end framework. Jointly training on large multimodal datasets enables unified semantic space and exposes the model to more data for learning the distribution of natural audio. Empirically, with joint training, we observe significant relative improvement in audio quality (10% lower Fréchet Distance [21] and 15% higher Inception Score [52]), semantic alignment (4% higher ImageBind [10] score), and temporal alignment (14% better synchronization score). To further improve temporal alignment, we note that using attention layers only is ineffective at indicating precise timings in audio/visual signals. To tackle this issue, instead, we introduce conditional synchronization module that uses high frame-rate visual features (extracted from selfsupervised audio-visual desynchronization detector [17]) and operates in the space of scales and biases of adaptive layer normalization (adaLN) layers [46], leading to accurate frame-level synchronization (50% relative improvement in synchronization score). In summary, we first propose MMAudio, multimodal joint training paradigm for video-to-audio. It enables accessible data scaling and cross-modal understanding, significantly improving audio quality and semantic alignment. We also propose conditional synchronization module which enables more precise audio-visual synchrony. We train MMAudio on openly accessible datasets and scale it to two audio sampling rates (16kHz and 44.1kHz) and three model sizes (157M, 621M, 1.03B), with the smallest model already achieving new state-of-the-art performance in videoto-audio synthesis among public models. Surprisingly, our multimodal approach also achieves competitive performance in text-to-audio generation compared to dedicated text-toaudio methods, showing that joint training does not hinder single-modality performance. Our code and models will be released publicly. 2. Related Works Semantic alignment. Semantic alignment between audio and video is obtained by training on paired audio-visual data with generation objective [2, 5, 16, 42, 45, 56, 62, 67] or contrastive objective [40]. To further understand the semantics of audio, we additionally train on paired audiotext data. We argue that the semantic understanding learned from audio-text pairs can be transferred to video-text pairs, as joint training leads to shared semantic space (similar to ImageBind [10] and LanguageBind [70]) and enables the network to learn richer semantics from more diverse data. Temporal alignment. Besides learning temporal alignment directly from audio-visual pairs, some recent works first learn from videos separate model to predict handcrafted proxy features such as onsets [50, 69], energy [15, 18], and root-mean-square (RMS) of waveforms [3, 32]. We deviate from these explicit handcrafted proxy features and directly learn alignment from the deep feature embeddings of pre-trained self-supervised de-synchronization detector Synchformer [17], allowing more nuanced interpretation of the input signal. concurrent work V-AURA [61] also uses Synchformer [17] for synchronization in an autoregressive framework. However, V-AURA [61] does not perform multimodal training on text and has short context window (2.56s) while we produce longer-term (8-10s) temporally consistent generation. Multimodal conditioning. The most common way to support multimodal conditioned generation is to add control modules to pretrained text-to-audio network that injects visual and temporal features into the pretrained network [15, 18, 32, 44, 50, 69]. However, this usually further increases the number of parameters (the base text-to-audio network plus the control modules). Besides, as the text modality is fixed during video-to-audio training, it becomes more challenging to learn joint semantic space the video modality needs to bind to the semantics of text instead of both modalities learning to cooperate. In contrast, we train all modalities simultaneously in our multimodal training formulation to learn joint semantics and enable omnidirectional feature sharing among modalities. Alternatively, to align different modalities without training, Seeing-and-Hearing [65] uses pretrained text-to-audio model and performs gradient ascent on an alignment score (e.g., ImageBind [10]) at test-time. We note that test time optimization is typically slower and sometimes results in low-quality and temporally misaligned output. Indeed, our model is faster at test time and is more consistent in producing synchronized audio. concurrent work VATT [38] uses both video and text to generate audio and trains captioning model to label unlabeled videos. However, they always require video condition inputs during training which limits the amount of training data. In contrast, our multimodal formulation allows us to train on audio-text datasets. Multimodal generation. Related to multimodal conditioning, multimodal generation models generate samples composed of multiple modalities (e.g., video and audio at the same time). We note that multimodal generation is more challenging problem and existing approaches [23, 51, 57, 58] are not yet competitive with dedicated video-to-audio models. In this work, we focus on single-modal audio generation with multimodal conditions. We believe our multimodal formulation and architecture can serve as foundation for future works in multimodal generation. 3. MMAudio 3.1. Preliminaries Conditional flow matching. We use the conditional flow matching objective [35, 59] for generative modeling and refer readers to [59] for details. In short, at test time, to generate sample, we randomly draw noise x0 from the standard normal distribution and use an ODE solver to numerically integrate from time = 0 to time = 1 following learned time-dependent conditional velocity vector field vθ(t, C, x) : [0, 1]RC Rd Rd, where is the timestep, is the condition (e.g., video and text), and is point in the vector field. We represent the velocity vector field via deep net parameterized by θ. At training time, we find θ by considering the conditional flow matching objective Et,q(x0),q(x1,C)vθ(t, C, xt) u(xtx0, x1)2, (1) where [0, 1], q(x0) is the standard normal distribution, and q(x1, C) samples from the training data. Further, xt = tx1 + (1 t)x0 (2) defines linear interpolation path between noise and data, and u(xtx0, x1) = x1 x0 (3) denotes its corresponding flow velocity at xt. Audio encoding. For computational efficiency, we model the generative process in latent space, following the common practice [36, 63]. For this, we first transform audio waveforms with the short-time Fourier transform (STFT) and extract the magnitude component as mel spectrograms [53]. The spectrograms are then encoded by pretrained variational autoencoder (VAE) [25] into latents x1. During testing, the generated latents are decoded by the VAE into spectrograms which are then vocoded by pretrained vocoder [33] into audio waveforms. 3.2. Overview Following conditional flow matching, at test time, we numerically integrate noise x0 from = 0 to = 1 following flow vθ, which was learned at training time by optimizing Eq. (1). Numerical integration at test time arrives at latent x1 that is decoded to audio preferably of high-quality and preferably semantically and temporally aligned to the video and text conditions. To estimate flow vθ for the current latent x, MMAudio operates on the video/text conditions and the flow timestep t. Figure 2 illustrates our network architecture. To combine inputs from different modalities, MMAudio consists of series of (N1) multimodal transformer blocks [7] with visual/text/audio branches, followed by series of (N2) audioonly transformer blocks [30]. Additionally, for audio-visual synchrony, we devise conditional synchronization module that extracts and integrates into the generation process high frame rate (24 frames per second (fps)) visual feature for temporal alignment. Next, we describe both components in detail. 3.3. Multimodal Transformer Core to our approach is the desire to model the interactions between video, audio, and text modalities. For this purpose, we largely adopt the MM-DiT block design from SD3 [7] and introduce two new components for temporal alignment: aligned RoPE positional embeddings for aligning sequences of different frame rates and 1D convolutional MLPs (ConvMLPs) for capturing local temporal structure. Figure 2 (right) illustrates our block design. Note, we also include sequence of audio-only single-modality blocks following FLUX [30], implemented by simply removing the streams of the two other modalities (joint attention becomes selfattention). Compared to considering all modalities at every layer, this design allows us to build deeper network with the same parameter count and compute without sacrificing multimodality. This multimodal architecture allows the model to selectively attend to and focus on different modalities depending on the inputs, thus enabling effective joint training on both audio-visual and audio-text data. Next, we describe the feature representation in our network and then the core components of our block design. Representations. We represent all the features as onedimensional tokens. Note, we deliberately do not use any absolute position encoding which allows us to generalize to different durations at test time. Thus, we specify the sequences in terms of frame rates to determine the number of tokens for given duration. The visual features Fv (one token per frame, at 8 fps) and text features Ft (77 tokens) are extracted from CLIP [49] as 1024d features. The audio latents are in the VAE latent space (Section 3.1), at 31.25 fps as 20d latents by default. The synchronization features Fsyn are extracted from Synchformer [17] at 24 fps as 768d features, which we will detail in Section 3.4. Note, except for the text tokens, all other tokens follow the same temporal ordering, albeit at different frame rates. After the initial ConvMLP/MLP layers, all features will be projected to the hidden dimension h. Joint attention. These tokens from different modalities communicate via joint attention (Figure 2, right). Following [7], we concatenate the query, key, and value representations from the three different modalities and apply joint attention operation [60]. The output is split into three modalities, following the input partition. We refer readers to [7] for details. We note that joint attention alone does not capture 3 Figure 2. Overview of the MMAudio flow-prediction network. Video conditions, text conditions, and audio latents jointly interact in the multimodal transformer network. synchronization model (Section 3.4) injects frame-aligned synchronization features for precise audio-visual synchrony. temporal alignment which we will address next. Aligned RoPE position embedding. For audio-visual synchrony, precise temporal alignment is crucial. As typical in transformers [60], we adopt positional embeddings to inform the attention layers of time. Specifically, we apply the RoPE [55] embeddings on the queries and keys in the visual and audio streams before joint attention (Figure 2). Note that we do not apply it to the text stream since it does not follow the temporal order of video or audio. Further, since the frame rates do not align (8 fps for the visual stream, 31.25 fps for the audio stream), we scale the frequencies of the positional embeddings in the visual stream proportionally, i.e., by 31.25/8. We visualize the default (non-aligned) RoPE and our proposed aligned RoPE in Figure A5. We note that these aligned embeddings are beneficial yet insufficient for good synchrony. Therefore, we introduce an additional synchronization module, which we discuss in Section 3.4. ConvMLP. To better capture local temporal structure, we use ConvMLPs rather than MLPs in the visual and audio streams. Concretely, our ConvMLP uses 1D convolutions (kernel size = 3 and padding = 1) rather than linear layers. Again, this change is not made to the text stream since it does not follow the temporal order of video or audio. Global conditioning. Global conditioning injects global features into the network through scales and biases in adaptive layer normalization layers (adaLN) [46]. First, we compute global conditioning vector cg R1h shared across all transformer blocks from the Fourier encoding [60] of the flow timestep, the average-pooled visual features, and the average-pooled text features (Figure 2). Then, each adaLN layer modulates its input RLh (L is the sequence length) with the global condition cg as follows: adaLNg(y, cg) = LayerNorm(y) 1Wγ(cg) + 1Wβ(cg). (4) Here, Wγ, Wβ are MLPs, and 1 is 1 all-ones matrix, which broadcasts the scales and biases to match the sequence length such that the same condition is applied to all tokens in the sequence (hence global). Next, we describe how we design position-dependent conditions for precise audio-visual synchronization. 4 3.4. Conditional Synchronization Module We develop token-level conditioning to further improve audio-visual synchrony. While the visual and audio streams already communicate via cross-modality attention layers, these layers aggregate features via soft distribution, which we found to hamper precision. To address this issue, we first extract high frame rate (24 fps) features (Fsyn ) from the input video using the visual encoder of Synchformer [17]. We use Synchformer because it is trained in self-supervised manner to detect temporal misalignment between video and audio data, which we hypothesize will yield visual features relevant to audio events and hence benefit synchronization.We find the frame-aligned conditioning cf RLh via cf = Upsample (ConvMLP (Fsyn )) + 1cg. (5) The upsample operation uses nearest neighbor interpolation and matches the frame rate of the synchronization features Fsyn with that of the audio latent x. This frame-aligned conditioning cf is injected via the adaLN layers in the audio stream for precise feature modulation. Similar to Eq. (4), we apply cf via adaLNf (x, cf ) = LayerNorm(x) Aγ(cf ) + Aβ(cf ), (6) where Aγ, Aβ Rhh are MLPs. Different from Eq. (4), the scales and biases are applied per token without broadcasting, providing fine-grained control. 3.5. Training and Inference 3.5.1. Multimodal Datasets VGGSound. We train on VGGSound [1] as the only audiovisual dataset. It offers around 500 hours of footage. Additionally, VGGSound contains class label (310 classes in total) for each video and we use the class names as input following ReWaS [18] and VATT [38]. We set aside 2K videos from the training set for validation, resulting in training set of around 180K 10s videos. We truncate the videos to 8s from the end for training. Audio-text datasets. We use AudioCaps [22] (around 128 hours, manually captioned) and WavCaps [43] (around 7,600 hours, automatically captioned from metadata) as audio-text datasets for training. Since they do not contain the visual modality, we set all the visual features and synchronization features corresponding to these samples as learnable empty tokens and syn respectively. For short audios (<16s), we truncate them to 8s from the end for training as in VGGSound. For longer audios, we take at most five non-overlapping crops of 8s from each audio. This results in total of 951K audio clip-text pairs. Overlaps. We notice minor (<1% of the test sets) train/test data contamination among these datasets. For fair comparison, we have removed the test sets of VGGSound and AudioCaps from all training data. We provide more details in Appendix C. 3.5.2. Implementation Details Model variants. Our default model generates 16kHz audio encoded as 20-dimensional, 31.25fps latents (following Frieren [63]), with N1 = 4, N2 = 8, = 448. We refer to this default model as S-16kHz. We additionally train larger models and models with higher audio sampling rates: S-44.1kHz, M-44.1kHz, and L-44.1kHz, detailed in Appendix E.1. The parameter counts and running time of these models are summarized in Table 1. We describe additional implementation details in Appendices to F. Classifier-free guidance. To enable classifier-free guidance [12] in inference, we randomly mask away all visual tokens (Fv and Fsyn ) or all the text with 10% probability during training. The masked visual tokens are replaced with learnable embedding (v and syn ), while any masked text is replaced with the empty string t. Inference. By default, we use Eulers method for numerical integration with 25 steps, with classifier-free guidance strength of 4.5. Both video and text conditions are optional during test-time we replace the missing modalities with empty tokens v, syn , or t. Recall, we deliberately do not use any absolute position encoding and thus can generalize to different durations at test time (e.g., 8s in VGGSound and 10s in AudioCaps in Section 4.2). 4. Experiments 4.1. Metrics We assess the generation quality in four different dimensions: distribution matching, audio quality, semantic alignment, and temporal alignment. Distribution matching assesses the similarity in feature distribution between ground-truth audio and generated audio, under some embedding models. Following common practice [16, 63], we compute Fréchet Distance (FD) and KullbackLeibler (KL) distance. For FD, we adopt PaSST [28] (FDPaSST), PANNs [27] (FDPANNs), and VGGish [8] (FDVGG) as embedding models. Note, PaSST operates at 32kHz, while both PANNs and VGGish operate at 16kHz. Moreover, both PaSST and PANNs produce global features, while VGGish processes non-overlapping 0.96s clips. For the KL distance, we adopt PANNs (KLPANNs) and PaSST (KLPaSST) as classifiers. We follow the implementation of Liu et al. [36]. Audio quality assesses the generation quality without comparing it to the ground truth using the inception score [52]. We adopt PANNs as the classifier following Wang et al. [63]. Semantic alignment assesses the semantic similarity between the input video and the generated audio. We use ImageBind [10] following Viertola et al. [61] to extract visual features from the input video and audio features from the generated audio and compute the average cosine similarity as IB-score. 5 Method Distribution matching Audio quality Semantic align. Temporal align. Params FDPaSST FDPANNs FDVGG KLPANNs KLPaSST IS IB-score DeSync Time (s) - ReWaS [18] 619M 141.38 Seeing&Hearing [65] 415M 219.01 V-AURA [61] 695M 218.50 VATT [38] 131.88 Frieren [63] 159M 106.10 FoleyCrafter [69] 1.22B 140.09 V2A-Mapper [62] 229M 84.57 157M 70.19 MMAudio-S-16kHz MMAudio-S-44.1kHz 157M 65.25 MMAudio-M-44.1kHz 621M 61.88 60.60 MMAudio-L-44.1kHz 1.03B 17.54 24.58 14.80 10.63 11.45 16.24 8.40 5.22 5.55 4.74 4. 1.79 5.40 2.88 2.77 1.34 2.51 0.84 0.79 1.66 1.13 0.97 2.87 2.26 2.42 1.48 2.73 2.30 2.69 1.65 1.67 1.66 1.65 2.82 2.30 2.07 1.41 2.86 2.23 2.56 1.59 1.44 1.41 1.40 8.51 8.58 10.08 11.90 12.25 15.68 12.47 14.44 18.02 17.41 17.40 14.82 33.99 27.64 25.00 22.78 25.68 22.58 29.13 32.27 32.99 33.22 1.062 1.204 0.654 1.195 0.851 1.225 1.225 0.483 0.444 0.443 0. 15.97 14.55 16.55 - - 1.67 - 1.23 1.30 1.35 1.96 Table 1. Video-to-audio results on the VGGSound test set. Following the common practice [63], the parameter counts exclude pretrained feature extractors (e.g., CLIP), latent space encoders/decoders, and vocoders. Time is the total running time using the official code to generate one sample with batch size of one after warm-up and excludes any disk I/O operations on an H100 GPU. : reproduced using official evaluation code. : evaluated using generation samples obtained directly from the authors. : does not use text input during testing. Note, Seeing&Hearing [65] directly optimizes ImageBind score during test time, therefore attains the highest IB-score. Temporal alignment assesses audio-visual synchrony with synchronization score (DeSync). DeSync is predicted by Synchformer [17] as the misalignment (in seconds) between the audio and video. Note that Viertola et al. [61] also use the synchronization score but evaluate on audio (2.56s) that is shorter than the context window (4.8s) of Synchformer. Instead, we evaluate on longer (8s) audios by taking two crops (first 4.8s and last 4.8s) and averaging the results. Thus, the scores from Viertola et al. [61] are not directly comparable with ours. 4.2. Main Results Video-to-audio. Table 1 compares our main results on the VGGSound [1] test set (15K videos) with existing state-ofthe-art models. We evaluate all generations at 8s following Wang et al. [63] by truncating longer audio to 8 seconds. For V-AURA [61], we use the official autoregression code to generate 8s audio. ReWaS [18] only generates 5s audio thus we evaluate it as-is, by truncating the ground truth also to 5s we indicate this discrepancy via the gray font in the table. Our smallest model (157M) demonstrates better distribution matching, audio quality, semantic alignment, and temporal alignment than prior methods, while being fast. notable exception is the IB-score comparison with Seeingand-Hearing [65]. We note that Seeing-and-Hearing [65] directly optimizes the IB score during test time, which we do not perform. Further, our larger models continue to improve in FDPaSST and IB-score, though we observe diminishing returns potentially limited by data quality and the amount of audio-visual data. Note, while our method uses more data for multimodal joint training, we do not use more data overall than some of the existing methods: FoleyCrafter [69], V2AMapper [62], ReWaS [18], and Seeing-and-Hearing [65] all finetune/incorporate text-to-audio model that has been trained on audio-text data similar to the one we use. For fair evaluation, we use the precomputed samples provided by [38, 62, 63], and reproduce the results using the official inference code for [18, 61, 65, 69]. Figure 3 visualizes our results and compares them with prior works. We present the results of user study and comparison with Movie Gen Audio [48] in Appendices and B. Text-to-audio. Our multimodal framework can be applied to text-to-audio synthesis without fine-tuning. Table 2 compares our method with state-of-the-art text-to-audio models using the AudioCaps [22] test set. For fair comparison, we follow the evaluation protocol of GenAU [11] to evaluate 10s samples in the AudioCaps [22] test set without using CLAP re-ranking (used by AudioLDM [36]). We transcribe the baselines directly from Haji-Ali et al. [11], who have reproduced those results using officially released checkpoints under the same evaluation protocol. We assess FDPANNs, FDVGG, IS, and CLAP [64]. CLAP measures the semantic alignment between the generated audio and the input caption. While our main focus is on video-to-audio synthesis, MMAudio attains state-of-the-art semantic alignment (CLAP) and audio quality (IS), due to rich semantic feature space learned from multimodal joint training. We note that we attain worse FDVGG score compared to recent works. We hypothesize that this is because VGGish processes local features (clips of 0.96s) while our strength lies in generating globally and semantically consistent audio. 4.3. Ablations We base all ablations on the small-16kHz model and evaluate distribution matching (FDPaSST), audio quality (IS), semantic alignment (IB-score), and temporal alignment (DeSync) on the VGGSound [1] test set. We highlight our default setting using blue background. 6 Figure 3. We visualize the spectrograms of generated audio (by prior works and our method) and the ground-truth. Note our method generates the audio effects most closely aligned to the ground-truth, while other methods often generate sounds not explained by the visual input and not present in the ground-truth. Method Params FDPANNs FDVGG IS CLAP 712M 32.50 AudioLDM 2-L [37] 866M 26.13 TANGO [9] 866M 19.77 TANGO 2 [41] Make-An-Audio [14] 453M 27.93 Make-An-Audio 2 [13] 937M 15.34 16.51 1.25B GenAU-Large [11] 157M 14.42 MMAudio-S-16kHz MMAudio-S-44.1kHz 157M 15.26 MMAudio-M-44.1kHz 621M 14.38 15.04 MMAudio-L-44.1kHz 1.03B 5.11 1.87 2.74 2.59 1.27 1.21 2.98 2.74 4.07 4.03 8.54 8.23 8.45 7.44 9.58 11.75 11.36 11.32 12.02 12.08 0.212 0.185 0.264 0.207 0.251 0.285 0.282 0.331 0.351 0.348 Table 2. Text-to-audio results on the AudioCaps test set. For fair comparison, we follow the evaluation protocol of [11] and transcribe all baselines directly from [11], who have reproduced those results using officially released checkpoints under the same evaluation protocol. Training modalities FDPaSST IS IB-score DeSync AVT+AT AV+AT AVT+A AVT AV 70.19 72.77 71.01 77.38 77.27 14.44 12.88 14.30 12.53 12.69 29.13 28.10 28.72 27.98 28.10 0.483 0.502 0.496 0.562 0. Table 3. Results when we vary the training modalities. A: Audio, V: Video, T: Text. In the second and third rows, we mask away the text token in either audio-visual data or audio-text data. In the last two rows, we do not use any audio-text data. Cross-modal alignment. To elucidate the benefits of joint multimodal training, we mask away some modalities during training and observe the effects on the results. The results are summarized in Table 3. We denote the setting as (modalities used for audio-visual-text data + modalities used for audiotext data), where A: Audio, V: Video, T: Text. Our default setting (AVT+AT) means that we train on audio-visual-text data (VGGSound class labels as text input) and audio-text data. We make three observations: 1. Masking away the text modality from either the former (AV+AT) or the latter (AVT+A) leads to worse results. This suggests that having joint text feature space is beneficial for multimodality training. 2. Adding uncaptioned audio data improves results (AVT vs. AVT+A). This suggests that our network even benefits from training on unconditional generation data by learning the distribution of natural sounds. 3. When no audio-text data is used, using the simple class labels in VGGSound does not affect results significantly (AVT vs. AV). This suggests that training on large multi- % audio-text data FDPaSST IS IB-score DeSync 100% 50% 25% 10% None 70.19 71.03 71.67 79. 14.44 14.62 14.41 13.55 29.13 29.11 28.75 27.47 77.38 12.53 27.98 0.483 0.489 0.505 0. 0.562 Variant ConvMLP MLP N1 = 4, N2 = 8 N1 = 6, N2 = 3 N1 = 2, N2 = 13 FDPaSST IS IB-score DeSync 70.19 73.84 70.19 72.53 70.33 14.44 13.01 14.44 13.75 15.18 29.13 28. 29.13 29.06 29.39 0.483 0.533 0.483 0.509 0.487 Table 4. Results when we vary the amount of multimodal training data. For the first four rows, we sample audio-visual and audio-text data at 1:1 ratio during training. For the last row, only audiovisual data is used. Variant FDPaSST IS IB-score DeSync With sync module Sum sync with visual No sync features Aligned RoPE No RoPE Non-aligned RoPE 70.19 73.59 69.33 70.19 70.24 70. 14.44 16.70 15.05 14.44 14.54 14.54 29.13 28.65 29.31 29.13 29.23 29.25 0.483 0.490 0.973 0.483 0.509 0. Table 5. Results when we use synchronization features or RoPE embeddings differently. modal datasets, rather than adding transformer branch in the network or using the class labels, is key. Multimodal data. Training on large collection of multimodal data is crucial. Table 4 shows our models performance when we vary the amount of audio-text training data. Note, we always sample audio-visual data and audio-text data at 1:1 ratio except when we use no audio-text data (then we only train on audio-visual data). When we do not use audio-text data, we observe overfitting and stop training early. When more multimodal data is used, distribution matching (FDPaSST), semantic alignment (IB-score), and temporal alignment (DeSync) improve with diminishing returns. Conditional synchronization module. We compare several different methods for incorporating synchronization features: 1) our default of using the conditional synchronization module (Sec. 3.4); 2) incorporating the synchronization features into the visual branch of the multimodal transformer. Concretely, we upsample (with nearest neighbor) the CLIP features to 24fps, and then sum the CLIP features and Sync features after linear projections as the final visual feature. We illustrate this architecture in Appendix E.5; finally, 3) not using synchronization features. Table 5 (top) shows that our synchronization module attains the best temporal alignment. We note the sum sync with visual method achieves higher audio quality (IS) we hypothesize that since upsampling CLIP features increased the number of tokens in the visual stream by three times, the model benefits from using the longer sequence for more fine-grained computations. RoPE embeddings. We compare our aligned RoPE formulation with 1) not using RoPE embeddings and 2) nonTable 6. Results when we vary the MLP architecture or the ratio between multi-/single-modality transformer blocks. aligned RoPE embeddings, i.e., no frequency scaling in the visual branch. Table 5 (bottom) shows that using aligned RoPE embeddings [55] improves audio-visual synchrony. ConvMLP. Table 6 (top) summarizes the performance difference of using MLP vs. ConvMLP. For the MLP model, we increase from 448 512, N1 from 4 6, and N2 from 8 10 to roughly match the number of parameters in the ConvMLP model. The ConvMLP model is better at capturing local temporal structure and thus has better performance, especially in synchronization. Ratio between N1 and N2. Table 6 (bottom) compares different assignments of the number of multimodal (N1) and single-modal (N2) transformer blocks with roughly the same parameter budget. We note our default assignment (N1 = 4, N2 = 8) performs similarly as using more singlemodal blocks (N1 = 2, N2 = 13) and better than using fewer single-modal blocks (N1 = 6, N2 = 3). We think this is because using single-modal blocks allows us to build deeper network with the same parameter count. 4.4. Limitations Our model generates unintelligible mumbles when prompted to generate human speech (from seeing human talk or from text input). We believe human speech is inherently more complex (e.g., with languages, tones, and grammars) and our model aimed at general audio effects (Foley) fails to adequately accommodate. 5. Conclusion We propose MMAudio, the first multimodal training pipeline that jointly considers audio, video, and text modalities, resulting in effective data scaling and cross-modal semantic alignment. Combined with conditional synchronization module, our method achieves new state-of-the-art performance among public models and comparable performance with Movie Gen Audio. We believe multimodal formulation is key for the synthesis of data in any modality and MMAudio lays the foundation in the audio-video-text space. Acknowledgment. This work is supported in part by Sony. AS is supported by NSF grants 2008387, 2045586, 2106825, and NIFA award 2020-67021-32799."
        },
        {
            "title": "References",
            "content": "[1] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP, 2020. 1, 5, 6, 13, 14, 15 [2] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan. Generating visually aligned sound from videos. TIP, 2020. 2 [3] Yoonjin Chung, Junwon Lee, and Juhan Nam. T-foley: controllable waveform-domain diffusion model for temporalevent-guided foley sound synthesis. In ICASSP. IEEE, 2024. 2 [4] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP. IEEE, 2020. 15 [5] Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, and Andrew Owens. Conditional generation of audio from video via foley analogies. In CVPR, 2023. 2 [6] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang. Natural language supervision for general-purpose audio representations. In ICASSP, 2024. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 1, 3 [8] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP. IEEE, 2017. 1, 5, 14, 15 [9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023. 7 [10] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 2, 5, 13 [11] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, and Vicente Ordonez. Taming data and transformers for audio generation. arXiv preprint arXiv:2406.19388, 2024. 6, 7 [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [13] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-toaudio generation. arXiv preprint arXiv:2305.18474, 2023. 7, 15 [14] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In ICML, 2023. 7 [15] Zhiqi Huang, Dan Luo, Jun Wang, Huan Liao, Zhiheng Li, and Zhiyong Wu. Rhythmic foley: framework for seamless audio-visual alignment in video-to-audio synthesis. arXiv preprint arXiv:2409.08628, 2024. [16] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. In BMVC, 2021. 2, 5 [17] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP. IEEE, 2024. 2, 3, 5, 6, 13, 17 [18] Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. arXiv preprint arXiv:2407.05551, 2024. 1, 2, 5, 6 [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1 [20] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In CVPR, 2024. 15, 19 [21] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: metric for evaluating music enhancement algorithms. In Interspeech, 2018. [22] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT, 2019. 5, 6, 14, 15 [23] Gwanghyun Kim, Alonso Martinez, Yu-Chuan Su, Brendan Jou, José Lezama, Agrim Gupta, Lijun Yu, Lu Jiang, Aren Jansen, Jacob Walker, et al. versatile diffusion transformer with mixture of noise levels for audiovisual generation. In NeurIPS, 2024. 2 [24] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. ICLR, 2015. 18 [25] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 3, [26] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. NeurIPS, 2017. 16 [27] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. TASLP, 2020. 5 [28] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. In Interspeech, 2022. 5 [29] Etienne Labb, Thomas Pellegrini, and Julien Pinquier. Conette: An efficient audio captioning system leveraging multiple datasets with task embedding. TASLP, 2024. 14 [30] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [31] LAION-AI. Audio dataset project. https://github. com/LAION-AI/audio-dataset, 2024. 14, 15 [32] Junwon Lee, Jaekwon Im, Dabin Kim, and Juhan Nam. Video-foley: Two-stage video-to-sound generation via temarXiv preprint poral event condition for foley sound. arXiv:2408.11915, 2024. [33] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: universal neural vocoder with large-scale training. In ICLR, 2023. 3, 15 9 [34] Rensis Likert. technique for the measurement of attitudes. Archives of Psychology, 1932. 13 [35] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [36] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. In ICML, 2023. 3, 5, [37] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. TASLP, 2024. 7 [38] Xiulong Liu, Kun Su, and Eli Shlizerman. Tell what you hear from what you see video to audio generation through text. In NeurIPS, 2024. 2, 5, 6, 13 [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 18 [40] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. In NeurIPS, 2024. 2 [41] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, WeiNing Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization. In ACM MM, 2024. 7 [42] Xinhao Mei, Varun Nagaraja, Gael Le Lan, Zhaoheng Ni, Ernie Chang, Yangyang Shi, and Vikas Chandra. FoleyarXiv preprint gen: Visually-guided audio generation. arXiv:2309.10537, 2023. 2 [43] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. TASLP, 2024. 1, 5, 14, [44] Shentong Mo, Jing Shi, and Yapeng Tian. Text-to-audio arXiv preprint generation synchronized with videos. arXiv:2403.07938, 2024. 2 [45] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serrà. Masked generative video-to-audio transformers with enhanced synchronicity. In ECCV, 2024. 2 [46] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. 2, 4 [47] Karin Petrini, Sofia Dahl, Davide Rocchesso, Carl Haakon Waadeland, Federico Avanzini, Aina Puce, and Frank Pollick. Multisensory integration of drumming actions: musical expertise affects perceived audiovisual asynchrony. Experimental brain research, 198:339352, 2009. 1 [48] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 6, 13, [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICLR, 2021. 1, 3 [50] Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu, Rilin Chen, and Dong Yu. Sta-v2a: Video-to-audio generation with semantic and temporal alignment. arXiv preprint arXiv:2409.08601, 2024. 2 [51] Ludan Ruan, Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mmdiffusion: Learning multi-modal diffusion models for joint audio and video generation. 2023 ieee. In CVPR, 2023. 2 [52] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 2, 5 [53] Stanley Smith Stevens, John Volkmann, and Edwin Broomell Newman. scale for the measurement of the psychological magnitude pitch. The journal of the acoustical society of america, 1937. 3, 15 [54] Robynn Stilwell. The fantastical gap between diegetic and nondiegetic, 2007. 1 [55] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 4, 8, 17 [56] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, and Chuang Gan. Physics-driven diffusion models for impact sound synthesis from videos. In CVPR, 2023. 2 [57] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context interleaved and interactive any-to-any generation. In CVPR, 2024. 2 [58] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. In NeurIPS, 2024. 2 [59] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. 3 [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3, 4 [61] Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally aligned audio for video with autoregression. arXiv preprint arXiv:2409.13689, 2024. 2, 5, 6, [62] Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, and Weidong Cai. V2a-mapper: lightweight solution for vision-to-audio generation by connecting foundation models. In AAAI, 2024. 1, 2, 6, 13 [63] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation with rectified flow matching. In NeurIPS, 2024. 1, 3, 5, 6, 16 [64] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP, 2023. 6, 13 [65] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. In CVPR, 2024. 1, 2, 6, 13 [66] Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation. arXiv preprint arXiv:2401.01044, 2024. 14 [67] Qi Yang, Binjie Mao, Zili Wang, Xing Nie, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, and Shiming Xiang. Draw an audio: Leveraging multi-instruction for video-to-audio synthesis. arXiv preprint arXiv:2409.06135, 2024. 2 [68] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation. ECCV, 2024. 14 [69] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 1, 2, 6, 14 [70] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In ICLR, 2024."
        },
        {
            "title": "Table of Contents",
            "content": ""
        },
        {
            "title": "3 MMAudio",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Overview . . 3.3 Multimodal Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Conditional Synchronization Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Training and Inference . . ."
        },
        {
            "title": "3.5.1 Multimodal Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nImplementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.5.2",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Network Details",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Model Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Projection Layers . . E.3 Gating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Details on Synchronization Features . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Illustration of the sum sync with visual Ablation . . . . . . . . . . . . . . . . . . . E.6 Visualization of Aligned RoPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "G Additional Visualizations",
            "content": "12 1 2 3 3 3 3 5 5 5 5 5 5 6 6 8 . . . . . . . . . . . 13 13 14 16 . 16 . 16 . 16 . 17 . 17 . 17 18 19 A. User Study In addition to the objective metrics presented in Table 1, we have also performed user study for subjective evaluation on the VGGSound [1] test set. For comparisons, we have selected our best model (MMAudio-L-44.1kHz) and four best baselines: 1. Seeing and Hearing [65], as it has the highest ImageBind (i.e., best semantic alignment with videos) score, besides ours. 2. V-AURA [61], as it has the lowest DeSync (i.e., best temporal alignment) with videos, besides ours. 3. VATT [38], as it has the lowest KullbackLeibler divergence (i.e., KLPANNs and KLPaSST), besides ours. 4. V2A-Mapper [62], as it has the lowest Fréchet distances (i.e., FDPaSST, FDPANNs, and FDVGG), besides ours. We sample eight videos from the VGGSound [1] test set, after excluding videos that are of low-resolution (below 360p) or that contain human speech. In total, each participant evaluates 40 videos (8 videos 5 methods). We group the samples for the same video, and randomly shuffle the ordering in each group to avoid bias. We ask each participant to rate the generation in three aspects using the Likert scale [34] (1-5; strongly disagree, disagree, neutral, agree, strongly agree) providing the following instructions: (a) The audio is of high quality. Explanation: An audio is low-quality if it is noisy, unclear, or muffled. In this aspect, ignore visual information and focus on the audio. (b) The audio is semantically aligned with the video. Explanation: An audio is semantically misaligned with the video if the audio effects are unlikely to occur in the scenario depicted by the video, e.g., the sound of an explosion in library. (c) The audio is temporally aligned with the video. Explanation: An audio is temporally misaligned with the video if the audio sounds delayed/advanced compared to the video, or when audio events happen at the wrong time (e.g., in the video, the drummer hits the drum twice and stops; but in the audio, the sound of the drum keeps occurring). In total, we have collected 920 responses in each of these aspects from 23 participants. Table A1 summarizes the results from the user study. MMAudio receives significantly higher ratings in all three aspects from the users, which aligns with the objective metrics presented in Table 1 of the main paper."
        },
        {
            "title": "Method",
            "content": "Audio quality Semantic alignment Temporal alignment Seeing&Hearing [65] V-AURA [61] VATT [38] V2A-Mapper [62] MMAudio-L-44.1kHz 2.651.05 3.591.02 2.660.99 3.000.95 4.140.77 3.101.24 3.701.17 3.321.17 3.281.27 4.520.74 1.850.99 3.651.16 2.041.07 2.031.11 4.460. Table A1. Average ratings for each method from the user study. We show meanstd in each aspect. B. Comparisons with Movie Gen Audio Recently, Movie Gen Audio [48] has been introduced for generating sound effects and music for input videos. While Movie Gen Audios technical details are sparse, it represents the industrys current state-of-the-art video-to-audio synthesis algorithm. Its 13-billion parameters model has been trained on non-publicly accessible data that is > 100 larger than ours. Nevertheless, we compare MMAudio to Movie Gen Audio [48] to benchmark the differences between public and private models. At the time of writing, the only accessible outputs from Movie Gen Audio are 5272 generations in the Movie Gen Audio Bench dataset. All the videos from Movie Gen Audio Bench are generated by MovieGen [48], which we note is different from the distribution of real-world videos (e.g., over-smoothed textures, slow motions). Since these are synthetic videos, there is no corresponding ground-truth audio. We run our best model MMAudio-L-44.1kHz on these videos and the corresponding audio prompts (which Movie Gen Audio also uses) and compare our generations with Movie Gen Audio. Since there is no ground truth audio, among the standard metrics that we have used in the main paper, we can only evaluate Inception Score (IS, audio quality), IB-score (ImageBind [10] similarly, semantic alignment between video and audio), DeSync (misalignment predicted by SynchFormer [17] between video and audio), and CLAP [6, 64] (alignment between text and audio). Additionally, we have conducted user study following the protocol of Appendix A, and have excluded audios with 2While the MovieGen technical report mentioned 538 samples, only 527 were released at the time of writing. 13 very low volume (cannot be heard clearly at normal volume) generated by Movie Gen Audio to prevent bias. We sampled total of 5 videos and received 230 responses in each of the aspects from 23 participants. Table A2 summarizes our results. In subjective metrics, MMAudio is comparable to Movie Gen Audio slightly worse in semantic alignment and slightly better in temporal alignment. In objective metrics, we observe the same trend MMAudio and Movie Gen Audio obtain the same audio quality (IS) score, Movie Gen Audio has better semantic alignment (IB-score and CLAP), and MMAudio has better video-audio synchrony (DeSync). Method Param Training data Audio qual. Semantic align. Temporal align. IS IB-score CLAP DeSync Subjective metrics Objective metrics Movie Gen Audio [48] MMAudio-L-44.1kHz 1.03B 13B O(1, 000, 000)h 3.930.92 3.930.89 8, 200h 4.360.74 4.260.71 3.521.21 3.621.03 8.40 8.40 36.26 27.01 0.4409 0.4324 1.006 0. Table A2. Comparisons between Movie Gen Audio and MMAudio in both subjective metrics (from user study) and objective metrics. For the subjective metrics, we show meanstd. Further, in terms of IB-score, we find that MMAudio struggles more in some videos, while Movie Gen Audio delivers more consistent results. We plot the sorted IB-score comparing MMAudio and Movie Gen Audio in Figure A1 (left). Movie Gen Audio consistently performs better in the low-performance regime, but the gap narrows in the high-performance region. We believe this is due to our limited training data, which is unable to adequately cover the data in Movie Gen Audio Bench and thus falls short in unfamiliar video types. Note, our only video-audio dataset for training is VGGSound [1] which contains videos for 310 classes. We hypothesize that collecting open-world data beyond these classes can effectively reduce this performance gap. The same phenomenon occurs at much smaller scale for the CLAP score, which might be because we use more audio-text data. Figure A2 shows examples where we obtain substantially higher/lower IB-score on videos with concepts well/not well covered by the training data. Figure A1. Sorted MMAudio and Movie Gen Audio performance scores in Movie Gen Audio Bench. C. Details on Data Overlaps We note that there are training and testing data overlaps among commonly used datasets for video-to-audio generation. For example, AudioSet [8] is commonly used to train VAE encoders/decoders but it contains test set data from VGGSound [1] and AudioCaps [22]. Additionally, AudioCaps is often used to train text-to-audio models [66], which is then used as the backbone for video-to-audio models which evaluate on VGGSound [1] however, part of the VGGSound test set overlaps with the AudioCaps training set. Moreover, AVSync15 [68], which is sometimes used jointly with VGGSound for training/evaluating video-to-audio algorithms [69], contains severe cross-contamination with VGGSound. This results in biased evaluations in both VGGSound and AVSync15. To our best knowledge, this data contamination is not yet addressed in the video-to-audio community. We thank Labb et al. [29] for raising this issue in the audio captioning field, which has helped us identify this problem. Table A3 summarizes the observed overlaps. The overlaps with WavCaps [43] and Freesound [31] have been included as part of their release, which we do not repeat in our table. 14 Audio prompt: rhythmic splashing and lapping of water IB-score (Movie Gen Audio): 42.74 IB-score (MMAudio, ours): 53.95 Audio prompt: creamy sound of mashed potatoes being scooped IB-score (Movie Gen Audio): 30.94 IB-score (MMAudio, ours): 10.52 Figure A2. Examples of videos in Movie Gen Audio Bench that are well/not well covered by our training data. Left: with familiar concept in our training data (516 swimming videos in the VGGSound training set), MMAudio achieves higher IB-score. Right: with an unfamiliar concept (there are no videos about mashed potatoes in VGGSound [1], according to the provided labels), MMAudio attains significantly lower IB-score. We have carefully removed from our training data (AudioSet [8], AudioCaps [22], Freesound [31], WavCaps [43], and VGGSound [1]) anything that overlaps with any of the test sets (VGGSound and AudioCaps). Additionally, we have also removed from our training data the test set of Clotho [4]. Since most baselines have been trained on VGGSound, we elect not to evaluate on AVSync15. Test sets (number of samples) Training sets AudioSet AudioCaps VGGSound AVSync AudioCaps (975) VGGSound (15,496) AVSync-15 (150) 580 (59.5%) 132 (0.9%) - - 13 (0.1%) - 147 (15.1%) - 144 (96.0%) - 59 (0.4%) - Table A3. Overlaps between training and test sets of different datasets. The percentage denotes the proportion of overlapping data in the entire test set. - means that we did not compute this data (we do not train or test on AVSync15). D. Details on the Audio Latents As mentioned in the main paper, we obtain the audio latents by first transforming audio waveforms with the short-time Fourier transform (STFT) and extracting the magnitude component as mel spectrograms [53]. Then, spectrograms are encoded into latents by pretrained variational autoencoder (VAE) [25]. During testing, the generated latents are decoded by the VAE into spectrograms, which are then vocoded by pretrained vocoder [33] into audio waveforms. Table A4 tabulates our STFT parameters and latent information. For the VAE, we follow the 1D convolutional network design of Make-An-Audio 2 [13] with downsampling factor of 2 and trained with reconstruction, adversarial, and KullbackLeibler divergence (KL) objectives. We note that the default setting leads to extreme values in the latent at the end of every sequence (10σ away). To tackle this problem, we have applied the magnitude-preserving network design from EDM2 [20], by replacing the convolutional, normalization, addition, and concatenation layers with magnitude-preserving equivalents. While this change removes the extreme values, it leads to no significant empirical performance difference. We train the 16kHz model on AudioSet [8], following Make-An-Audio 2 [13]. For the 44.1kHz model, we increase the hidden dimension from 384 to 512 and train it on AudioSet [8] and Freesound [31] to accommodate the increased reconstruction difficulty due to higher sampling rate. For vocoders, we use the BigVGAN [33] trained by Make-An-Audio 2 [13] in our 16kHz model. For our 44.1kHz model, we use BigVGAN-v2 [33] (the bigvgan_v2_44khz_128band_512x checkpoint). Model variants Latent frame rate # latent channels # mel bins # FFTs Hop size Window size Window function 16kHz 44.1kHz 31.25 43.07 20 40 80 128 1024 2048 256 512 1024 Hann Hann Table A4. Short-time Fourier transform (STFT) parameters and latent information. E. Network Details E.1. Model Variants Our default model generates 16kHz audio encoded as 20-dimensional, 31.25fps latents (following Frieren [63]), with N1 = 4, N2 = 8, = 448. We refer to this default model as S-16kHz. To faithfully capture higher frequencies, we also train 44.1kHz model (S-44.1kHz) that generates 40-dimensional, 43.07fps latents while all other settings are identical to the default. To scale up the high-frequency model, we first double the hidden dimension to match the doubled latent dimension, i.e., we use N1 = 4, N2 = 8, = 896 and refer to this model using M-44.1kHz. Finally, we scale the number of layers, i.e., N1 = 7, N2 = 14, = 896 and refer to this model via L-44.1kHz. These model variants are summarized in Table A5. Model variants Params # multimodal blocks N1 # single-modal blocks N2 Hidden dim Latent dim Time (s) S-16kHz S-44.1kHz M-44.1kHz L-44.1kHz 157M 157M 621M 1.03B 4 4 4 7 8 8 8 14 448 448 896 896 20 40 40 40 1.23 1.30 1.35 1.96 Table A5. Summary for different MMAudio model variants. Time is the total running time to generate one sample with batch size of one after warm-up and excludes any disk I/O operations on an H100 GPU. E.2. Projection Layers We use projection layers to project input text, visual, and audio features to the hidden dimension and for initial aggregation of the temporal context. Text feature projection. We use linear layer that projects to h, followed by an MLP. Clip feature projection. We use linear layer that projects to h, followed by ConvMLP with kernel size of 3 and padding of 1. Sync feature projection. We use 1D convolutional layer with kernel size of 7 and padding of 3 that projects to h, an SELU [26] activation layer, followed by ConvMLP with kernel size of 3 and padding of 1. Audio feature projection. We use 1D convolutional layer with kernel size of 7 and padding of 3 that projects to h, an SELU [26] activation layer, followed by ConvMLP with kernel size of 7 and padding of 3. E.3. Gating The gating layers are similar to the adaptive normalization layers (adaLN). Each global gating layer modulates its input RLh (L is the sequence length) with the global condition cg as follows: Gatingg(y, cg) = 1Wg(cg). (A1) Here, Wg Rhh is an MLP, and 1 is 1 all-ones matrix, which broadcasts the scales to match the sequence length such that the same condition is applied to all tokens in the sequence (hence global). Similarly, for per-token gating layers, the frame-aligned conditioning cf is injected into the audio stream for precise feature modulation via Gatingf (y, cf ) = Wf (cf ), (A2) where Wf Rhh is an MLP. Different from Equation (A1), the scales are applied per token without broadcasting, 16 E.4. Details on Synchronization Features Figure A3. Synchformer feature extraction. We use the visual encoder of Synchformer [17] to extract synchronization features. We use the pretrained audio-visual synchronization model trained on AudioSet, provided by Iashin et al. [17]. As input, we obtain frames at 25 fps. Synchformer partitions these frames into overlapping clips of 16 frames with stride 8 and produces features of length 8 for each clip. Thus, for video of length Tsec seconds, the sequence length of the synchronization features is"
        },
        {
            "title": "The corresponding feature fps is",
            "content": "Lsync = 8 (cid:18)(cid:22) 25Tsec 16 (cid:23) 8 (cid:19) + . FPSsync ="
        },
        {
            "title": "Ltext\nTsec",
            "content": ". (A3) (A4) In this paper, we experimented with Tsec = 8 and Tsec = 10. In both cases, FPSsync is exactly 24. Additionally, we introduce learnable positional embedding of length 8 (matching the number of features in each clip processed by Synchformer) that is added to the Synchformer features, as illustrated in Figure A3. E.5. Illustration of the sum sync with visual Ablation Figure A4 illustrates the network architecture for the sum sync with visual ablation in the conditional synchronization module paragraph. The visual features are upsampled using the nearest neighbor to match the frame rate of the synchronization features. This architecture has worse FDPaSST, IB-score, synchronization (DeSync) but better inception score (IS), which we hypothesize is due to the increased number of visual tokens in the upsampling step, leading to finer-grained computations. E.6. Visualization of Aligned RoPE To visualize the effects of using aligned RoPE [55], we compare the dot-product affinity of two sequences 1250C and 164C when RoPE is applied. Here, 250 represents the audio sequence length (31.25 fps for 8 seconds), 64 represents the visual sequence length (8 fps for 8 seconds), and = 64 is the channel size. Concretely, we visualize and, RoPEdefault(1250C) (cid:0)RoPEdefault(164C)(cid:1)T , RoPEaligned(1250C) (cid:0)RoPEaligned(164C)(cid:1)T , (A5) (A6) in Figure A5. Temporal alignment is attained when we use aligned RoPE. 17 Figure A4. Illustration of the sum sync with visual ablation. Figure A5. Affinity visualizations between two sequences with different frame rates when default/aligned RoPE embeddings are used. Left: with default RoPE, the sequences are not aligned. Right: with our proposed aligned RoPE, we attain temporal alignment. F. Training Details We use base learning rate of 1e-4, with linear warmup schedule of 1K steps to train our model for 300K iterations. We use the AdamW optimizer [24, 39] with β1 = 0.9 and β2 = 0.95. We notice occasional training collapse (to NaN) if the default 18 β2 = 0.999 was used instead. We reduce the learning rate to 1e-5 after 80% of the training steps, and once again to 1e-6 after 90% of the training steps. We use post-hoc EMA [20] with relative width σrel = 0.05 for all models. For efficiency, we use bf16 mixed precision training in all training runs. All the audio latents and visual embeddings are precomputed offline and loaded during training. Table A6 summarizes the training resources we used for each model size."
        },
        {
            "title": "Model",
            "content": "Number of GPUs used Number of hours to train Total GPU-hours MMAudio-S-16kHz MMAudio-S-44.1kHz MMAudio-M-44.1kHz MMAudio-L-44.1kHz 2 2 8 8 22 26 21 38 44 52 168 304 Table A6. The amount of training resources used for each model size. H100 GPUs are used in all settings. G. Additional Visualizations We provide generated samples and comparisons with state-of-the-art methods on our project page https://hkchengrex. com/MMAudio/video_main.html. Below, we provide additional spectrogram visualizations comparing our method with prior works in Figures A6 and A7. 19 Figure A6. Left: our method can precisely capture the distinct audio event of striking golf ball. Right: dog barks in successive bursts. Our generation does not line up with the ground-truth as precisely due to the ambiguous nature of video-to-audio generation, but does capture the rapid bursts. 20 Figure A7. Left: when visible audio events (e.g., when string is played) can be clearly seen, MMAudio captures them much more precisely than existing methods. Right: in complex scenario, MMAudio does not always generate audio aligned to the ground-truth (as common in the generative setting) but the generation is often still plausible."
        }
    ],
    "affiliations": [
        "Sony AI",
        "Sony Group Corporation",
        "University of Illinois Urbana-Champaign"
    ]
}