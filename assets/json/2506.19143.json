{
    "paper_title": "Thought Anchors: Which LLM Reasoning Steps Matter?",
    "authors": [
        "Paul C. Bogdan",
        "Uzay Macar",
        "Neel Nanda",
        "Arthur Conmy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified \"broadcasting\" sentences that receive disproportionate attention from all future sentences via \"receiver\" attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 2 3 4 1 9 1 . 6 0 5 2 : r Thought Anchors: Which LLM Reasoning Steps Matter? Paul C. Bogdan, Duke University Uzay Macar, Aiphabet Neel Nanda Arthur Conmy"
        },
        {
            "title": "Abstract",
            "content": "Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) black-box method measuring each sentences counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with different meaning; (2) white-box method of aggregating attention patterns between pairs of sentences, which identified broadcasting sentences that receive disproportionate attention from all future sentences via receiver attention heads; (3) causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentences tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods, and present case study showing converging patterns across methods that map how model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for deeper understanding of reasoning models."
        },
        {
            "title": "Introduction",
            "content": "Training large language models to reason with chain-of-thought [28, 23, 34] has led to significant advances in capabilities [26]. The resulting reasoning traces are already used in safety research [3, 29]. However, there has been little work adapting interpretability methods to this new paradigm (though see [31, 14]). Traditional mechanistic interpretability [25, 24] methods often focus on single forward pass of the model: understanding the activations, how they are processed by each layer, and how they are converted to the final output [33, 16, 15]. However, this framework is too fine-grained for autoregressive reasoning models, which consume their own output tokens. core step in many interpretability strategies is to decompose the model into smaller parts that can be analyzed independently [20]. natural decomposition for chain-of-thought is into individual sentences and how they depend on each other. Interpretations of neural network behavior operate at varying levels of abstraction [11, 12], and sentence-level explanations strike an intermediate abstraction depth. Compared to tokens, sentences are more coherent and often coincide with reasoning steps extracted by an LLM [31, 2]. Compared to paragraphs, sentences are less likely to conflate reasoning steps and may serve as an effective target for linking different steps. Equal contribution (author order determined by coinflip) Equal senior author contribution (author order determined by coinflip) *Correspondence to: Paul C. Bogdan <paulcbogdan@gmail.com>, Uzay Macar <uzaymacar@gmail.com> Preprint. Under review. Figure 1: Summary of our three methods for principled attribution to important sentences in reasoning traces. A. An example reasoning trace with sentences labeled per our taxonomy. B. Our proposed methods are: black-box resampling, receiver heads, and attention suppression. C. directed acyclic graph among sentences prepared by one of our techniques, made available open source. Prior work has established that different sentences within reasoning traces perform distinct functions. Backtracking sentences (e.g., Wait. . . ) cause the model to revisit earlier conclusions, which boosts final-answer accuracy [22]. Other research has distinguished sentences based on whether they retrieve new information or execute deduction with existing information [31]. Hence, reasoning may follow an overarching structure, where sentences can introduce and pursue high-level computational goals. Yet, approaches for mapping this structure remain limited. We propose three complementary methods for mapping the structure of reasoning traces that focus on what we term thought anchors: critical reasoning steps that guide the rest of the reasoning trace. We provide evidence for this type of anchoring based on black-box evidence from resampling and white-box evidence based on attention patterns. First, in section 3 we present black-box method for measuring the counterfactual impact of sentence on the models final answer and future sentences. We repeatedly resample reasoning traces from the start of each sentence. Based on resampling data, we can quantify the impact of each sentence on the likelihood of any final answer or the likelihood of producing any subsequent sentence. Via this resampling approach, we can additionally distinguish planning sentences that initiate computations leading to some answer from sentences performing computations necessary for the answer but which are predetermined. Second, in section 4 we present white-box method for evaluating importance based on the sentences most attended. Our analyses reveal receiver heads that narrow attention toward particular past broadcasting sentences. Compared to base models, where attention is more diffuse, reasoning models display overall greater attentional narrowing through receiver heads, and these heads have an outsized impact on the models final answer. We develop systematic approach to identifying receiver heads and show how evaluating sentences on the extent to which they are broadcast by these heads provides mechanistic measure of importance. Third and finally, in section 5 we present method that measures the causal dependency between specific pairs of sentences in reasoning trace. For each sentence in trace, we intervene by masking all attention to it from subsequent tokens. We then measure the effect of suppression on subsequent token logits (KL divergence) compared to those generated during the absence of suppression. Averaging token effects by sentence, this strategy measures each sentences direct causal effect on each subsequent sentence. Applying these techniques, our work suggests that analyzing reasoning through sentence-level units introduces new domains through which reasoning models can be understood. Our work also opens the door to more precise debugging of reasoning failures, identification of sources of unreliability, and the development of techniques to enhance the reliability of reasoning models."
        },
        {
            "title": "2 Setup",
            "content": "2.1 Sentence taxonomy for reasoning functions We categorized sentences using the framework by [31], which describes distinct reasoning functions within reasoning trace. We define eight categories (see examples and frequencies in Appendix A): 1. Problem Setup: Parsing or rephrasing the problem 2. Plan Generation: Stating or deciding on plan of action, meta-reasoning 3. Fact Retrieval: Recalling facts, formulas, problem details without computation 4. Active Computation: Algebra, calculations, or other manipulations toward the answer 5. Uncertainty Management: Expressing confusion, re-evaluating, including backtracking 6. Result Consolidation: Aggregating intermediate results, summarizing, or preparing 7. Self Checking: Verifying previous steps, checking calculations, and re-confirmations 8. Final Answer Emission: Explicitly stating the final answer Each sentence in the analyzed response is assigned to one of these categories using an LLM-based auto-labeling approach. The prompt used to label sentence categories is provided in Appendix B. Some categories that rarely appear are omitted from the figures below. Probing classifiers achieve high accuracy in distinguishing sentences and their corresponding categories (see Appendix C). 2.2 Model and dataset Our analyses are based on the DeepSeek R1-Distill Qwen-14B model (48 layers) [8]. We used temperature of 0.6 and top-p value of 0.95 in all of our experiments. We used the MATH dataset [17] specifically targeting 10 challenging mathematics questions that the model correctly solves 25-75% of the time, identified by testing the model on 1,000 problems 10 times each. For each selected problem, we generated one correct and one incorrect reasoning trace, producing 20 responses. The average response is 144.2 sentences long (95% CI: [116.7, 171.8] and 4208 tokens (95% CI: [3479, 4937]). We focus only on sentences before the resampling method indicates the model has converged on an answer (i.e., after which it gives the same response in >98% of resamples). In Appendix D, we provide results from applying our techniques to other reasoning models."
        },
        {
            "title": "3 Measuring counterfactual influence",
            "content": "Some sentences matter more than others, but which ones matter most depends on how we define and measure importance. We frame sentence-level importance as question of counterfactual influence: how does including or excluding sentence affect subsequent steps and the models final output? 3.1 Forced answer importance In earlier work, importance has often been approximated by forcing the model to give an answer before completing its full reasoning trace [18]. We tested this strategy. For each sentence in our examined model responses, we interrupt the model at that point and append prompt inducing it to give final answer (Therefore, the final answer is boxed{). This is performed 100 times at each sentence to generate distribution of answers linked to each sentence position. To evaluate the impact of sentence on the final answer, we measure the KL Divergence between the distribution before sentence has been stated and the distribution after sentence has been stated. The taxonomic categories reveal that active computation statements produce the greatest distributional shifts in this forced answer importance (Figure 3A). This aligns with prior findings that reasoning models update their beliefs throughout the trace, and suggests intermediate calculations alter the models answer responses. However, active computation steps may not provide comprehensive view on importance, as they may be predetermined by upstream statements. 3.2 Counterfactual importance natural limitation of the forced-answer approach is that sentence may be necessary for some final answer, but consistently produced by the LLM late in the reasoning trace. This means that 3 Figure 2: Accuracy over 100 rollouts at each sentence for (A) one correct and (B) one incorrect base solution. Red dots mark significant spikes or dips. Local minima and maxima sentences are annotated with category initials (e.g., PG = plan generation). The analyses below focus on the counterfactual KL-divergence between sentences, but resampling accuracy is visualized here as it is more intuitive. forced answer accuracy will be low for all sentences that occur before S, not allowing the importance of these earlier steps to be accurately determined. Consider rollout which consists of sentences S1, S2, . . . , Si, . . . , SM and final answer A. We use resampling to define metric capturing the extent to which sentence causes to be incorrect.1 We call this metric counterfactual importance. We can motivate and define this metric with three steps: 1. Rollout sampling. For given sentence Si, we generate distribution over final answers by generating 100 rollouts both without sentence Si (the base condition, with rollouts of the form S1, S2, . . . , Si1, Ti, . . . , TN , ), and another distribution with sentence (the Si intervention condition, with rollouts of the form S1, S2, . . . , Si1, Si, . . . , SM , ASi). 2. Naive distributional comparison. We can compute the KL Divergence between the final answer distributions in the two conditions, i.e., importancer := DKL[p(A )p(ASi)], Si providing scalar measure of how much sentence Si changes the answer. We call this the resampling importance metric. 3. Semantic filtering. The problem with resampling importance is that in rollout resampling, if Ti is identical or similar to Si then we do not get much information about whether Si is important or not. Therefore, we write if two sentences and are dissimilar, defined as having cosine similarity less than the median value across all sentence pairs in our dataset, when fed into an embeddings model (see Appendix for details). Therefore we can finally define the counterfactual importance by conditioning on Ti Si i.e., importance := DKL[p(A Si Ti Si)p(ASi)]. Since we resample all steps after given sentence Si, we avoid the aforementioned limitation of forced-answering. We also provide empirical evidence that the principled counterfactual importance definition in 1-3 above is useful, by comparing it to the resampling importance in Appendix G. Across our dataset, we find that plan generation and uncertainty management (e.g., backtracking) sentences consistently show higher counterfactual importance than other categories like fact retrieval or active computation (see Figure 3B). This supports the view that high-level organizational sentences anchor, organize, and steer the reasoning trajectory. We argue that this approach provides more informative results compared to forced answer importance and prior tokenor attention-based metrics. 3.3 Sentence-to-sentence importance In addition to estimating sentences effect on the final answer, we extend our framework to quantify its causal influence on each individual future sentence. This sentence-level attribution helps uncover the structure of the models chain-of-thought: which sentences initiate subroutines, influence future claims, or act as attractors for subsequent steps. We estimate the counterfactual importance of sentence Si on future sentence SFut. formally with: 1All Sj and Tk as well as in this section are random variables, since we sample with non-zero temperature. 4 Figure 3: The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance, per the resampling method, plotted against the sentence categorys mean position in the reasoning trace. Only the 5 most common sentence types are shown (see Appendix F). importance(Si SFut.) = P(SFut. {Si, . . . , SM }) P(SFut. {Ti, . . . , TN }Ti Si) (1) Intuitively, on the right-hand side of Equation (1), the first term is the probability that future sentence SFut. will semantically occur given that Si was present in the trace, and the second term is the corresponding probability when Si is resampled with non-equivalent sentence. positive score indicates that sentence Si increases the likelihood of producing SFut. (i.e., Si upregulates SFut.), while negative score suggests that it suppresses or inhibits it. In this context SFut. semantically occurs if, when we extract the sentences and identify the best candidate match for SFut. using cosine similarity between sentence embeddings, it has greater than 0.8 cosine similarity (i.e., the median value in our dataset) to that sentence. Pseudocode for estimating sentence-to-sentence importance and empirical values of this metric can be found in Appendix G."
        },
        {
            "title": "4 Attention aggregation",
            "content": "We hypothesize that important sentences may receive heightened attention from downstream sentences. Although attention weights do not necessarily imply causal links (see Section 5), heightened attention is plausible mechanism by which important sentence may exert their influence on subsequent sentences. We conjecture further that high focus on important sentences may be driven by specific attention heads, and by tracking such heads, we may pinpoint key sentences. We assessed the degree different heads narrow attention toward particular sentences. First, for each reasoning trace, we averaged each attention heads token-token attention weight matrix to form sentence-sentence matrix, where each element is the mean across all pairs of tokens between two sentences. Based on each attention matrix, we computed the mean of its column below the diagonal to measure the extent each sentence receives attention from all downstream sentences; averaged only among pairs at least four sentences apart to focus on distant connections. This generates distribution for each head (e.g., Figure 4A), and the extent each head generally narrows attention toward specific sentences can be quantified as its distributions kurtosis (computed for each reasoning trace, then averaged across traces). Plotting each heads kurtosis reveals that some attention heads strongly narrow attention toward specific, possibly important, sentences in the reasoning trace (Figure 4B). 4.1 The identification of receiver heads We refer to attention heads that narrow attention toward specific sentences as receiver heads. These heads are more common in later layers (Appendix H). To formally assess the existence of receiver heads, we tested whether some attention heads consistently operate in this role. Specifically, we measured the split-half reliability of heads kurtosis scores. We found strong head-by-head 5 Figure 4: A. Lines show the vertical attention scores for each sentence by the 40 different heads in layer 36. Head 6 has been highlighted as receiver head, and its corresponding attention weight matrix is shown for reference. Its prominent spikes cause the distribution to have high kurtosis. B. Histogram of these kurtosis values across all attention heads, averaged across all reasoning traces. correlation (r = .67) between kurtosis scores computed for half of the problems with kurtosis scores for the other half of problems. Thus, some attention heads consistently operate as receiver heads, albeit with some heterogeneity across responses in which heads narrow attention most. Receiver heads usually direct attention toward the same sentences. Among the 16 heads with the highest kurtoses, we computed the sentence-by-sentence correlation between the vertical-attention scores for each pair of heads; calculated separately for each reasoning trace, then averaged. This produced an large correlation (mean = .60). Thus, receiver heads generally attend the same sentences, albeit with some heterogeneity (this correlation among randomly selected heads is = .45). This convergence is consistent with the existence of sentence importance, which these heads identify. Attentional narrowing toward particular sentences may be feature specifically of reasoning models that enhances their performance. Comparing R1-Distill-Qwen-14B (reasoning) and Qwen-14B (base) suggests that the reasoning models receiver heads will narrow attention toward singular sentences to greater degree (appendix I). Furthermore, ablating receiver heads leads to greater reduction in accuracy than ablating self-attention heads at random (appendix J). Altogether, these findings are consistent with receiver heads and thought-anchor sentences playing special roles in reasoning. 4.2 Links to resampling importance and sentence types Plan generation, uncertainty management, and self checking sentences consistently receive the most attention via receiver heads (Figure 5), whereas active computation sentences receive minimal attention. Further consistent with this, sentences receiving high receiver-head attention tend to also exert larger effect on downstream sentences per the resampling method (mean Spearman ρ = .22; 95% CI: [.17, .30]; detailed further in Appendix K). These findings are hence consistent with reasoning traces being structured around high-level sentences that initiate computations that may link high-level statements but may minimally impact the overall reasoning trajectory."
        },
        {
            "title": "5 Attention suppression",
            "content": "5.1 Approach natural concern with examining attention weights is that they do not measure causality. Further, receiver heads are not necessarily suited for identifying sentences links to individual future sentences. Hence, we next focused on causal attribution for sentence-sentence dependencies. The sentencesentence analyses based on resampling also examined such dependencies but had limited precision for mapping logical connections, as counterfactual importance represents the total effect (direct and indirect) of one sentence on another. The present method instead aims to isolate the direct effects between sentences, which we expect to permit more precise modeling of logical connections. 6 Figure 5: The boxplot shows the average top-16 receiver-head score for each sentence type. The boxes correspond to the interquartile range across different reasoning traces. We suppress all attention (all layers and heads) toward given sentence and examine the impact on future sentences. The impact is defined as the KL divergence between tokens logits and its baseline logits without suppression. The effect on future sentence is calculated as the average of its constituent token log-KL divergences. This technique assumes (i) token logits accurately capture sentences semantic content and (ii) suppressing attention does not problematically induce out-ofdistribution behavior. Hence, we must validate correspondence between the attention-suppression measure and resampling measure, which, although less precise, does not hinge on these assumptions. 5.2 Correlations with the resampling-based importance matrix The attention-suppression matrix values correlate with those of the resampling-method matrix. Specifically, the two matrices were positively correlated for 19 of the 20 examined reasoning traces (mean: Spearman ρ = .20, 95% CI: [.12, .27]). This association is stronger when considering only cases fewer than five sentences apart in the reasoning trace, which may better track direct rather than indirect effects represented by the resampling method (mean: ρ = .34 [.27, .40]). The magnitudes of these correlations are substantial, given that measures measure partially different aspects of causality and the resampling measure itself contains stochastic noise. Hence, these results give weight to the validity of the resampling approach, whose precision we leverage for the forthcoming case study."
        },
        {
            "title": "6 Case study",
            "content": "The three presented techniques cover different aspects of attribution within reasoning trace. We will now illustrate the utility and the complementary nature of these techniques by applying them to the models response for one problem: When the base-16 number 6666616 is written in base 2, how many base-2 digits (bits) does it have? (Figure 6) (see Appendix for the full transcript). 6.1 Resampling The model approaches this problem by first considering that 6666616 contains five base-16 digits and any base-16 digit can be represented with four base-2 digits. Following this logic, the CoT starts by pursuing the answer 20 bits. However, this initial answer overlooks that 616 is 1102 (i.e., not 01102 due to the leading zero), so the correct answer is actually 19 bits. In sentence 13, the model pivots toward this correct answer by initiating computations to convert 6666616 to decimal and then binary. The resampling method (section 3) reveals this initial poor trajectory and pivot upwards (shown previously in Figure 2A). Specifically, from sentences 6-12, expected accuracy steadily declines, but sentence 13 causes counterfactual accuracy to drastically increase. The pivotal role of 13 is notably missed if one instead evaluates accuracy by forcing the model to give response immediately, as some prior studies have done, which instead yields 0 6.2 Receiver heads The trajectory toward the final correct answer can be understood as series of computational chunks (see flowchart in Figure 6). First, the model prepares formula for converting 6666616 to decimal (sentences 13-19). Next, the model computes the answer to that formula, finding that 6666616 is 7 Figure 6: Case study: problem #4682 (correct). Red matrix shows the effect of suppressing one sentence (x-axis) on future sentence (y-axis). Darker colors indicate higher values. Bottom-left line plot shows the average attention toward each sentence by all subsequent sentences via the top-32 receiver heads (32 attention heads with the highest kurtosis score). Flowchart summarizes the models CoT with chunks defined around key sentences receiving high attention via receiver heads. Sentence 13 is emphasized as it has high counterfactual importance per the resampling method (see Figure 2A). 419,430 in decimal (sentences 20-33). The model subsequently converts that number to binary by putting forth another formula and solving it, loor(log2(419, 430)) + 1 = 19, to derive that the answer is 19 bits (sentences 34-41). The model then notes discrepancy with the earlier 20-bit solution (sentences 42-45). The model hence initiates new computations that verify that it computed the decimal value of 6666616 correctly (sentences 46-58) and that it computed the binary conversion accurately (sentences 59-62). Equipped with this increased certainty about 19-bit answer, the model discovers why its initial 20-bit idea was incorrect: because leading zeros are not counted (Sentence 66). This overall narrative is based on our analysis of attention patterns (section 4): Receiver attention heads pinpoint sentences initiating computations or stating key conclusions, thereby segmenting the reasoning trace into seemingly meaningful chunks (Figure 6). 6.3 Attention suppression Along with being organized into computational chunks, the reasoning displays scaffold related to sentence-sentence dependencies (Figure 6). One piece of this structure is self-correction pattern involving an incorrect proposal, detected discrepancy, and final resolution. Specifically, the model initially proposes an incorrect answer of 20 bits, which it decides to recheck (sentence 12). This leads to discrepancy with the 19 bits answer computed via decimal conversion (sentences 43 & 44). After rechecking its arithmetic supporting the 19 bit answer, the model returns to the discrepancy (sentence 65) and then produces an explanation for why the 20 bits answer is incorrect (sentence 66). This can be seen as tentative CoT circuit, where two conclusions conflict to produce discrepancy, which in turn encourages the model to resolve the discrepancy. Within this wide-spanning scaffold, there exist further dependencies, corresponding to verifying an earlier computation. Specifically, the model finishes computing the decimal value of 6666616 as 419,430 (sentence 32), later decides to verify that decimal conversion (sentence 46), and finally confirms that the original value is correct (sentence 59). This can be seen as further indication of CoT circuitry. We identified these linkages based on the attention-suppression matrix (section 5), which contains local maxima at these linkages (12 43, 43 65, 12 66; 32 46, 32 59). Notice that many of the sentences pinpointed by the attention-suppression technique overlap with the sentences receiving high attention from receiver heads. Adding to the receiver-head conclusions, the attention suppression technique shows how information flows between these key sentences that structure the reasoning trace. 6.4 Open source interface We released an open source interface (thought-anchors.com) for analyzing and visualizing reasoning traces. The reasoning trace can be visualized as an annotated directed acyclic graph using the importance metrics from Section 3, which may benefit interpretability and reasoning model debugging. We show our proof-of-concept interface in Figure 1C, where important sentences are represented by larger nodes and sentences that causally affect each other are connected with dashed gray lines. Hovering over node lets users inspect sentence properties or compare alternative rollouts."
        },
        {
            "title": "7 Related work",
            "content": "Reasoning advances in LLMs. Chain-of-thought reasoning [34, 23, 28] has driven significant capabilities improvements in large language models. State-of-the-art performance on complex tasks comes from thinking models like DeepSeek-R1 [8], OpenAIs o1 [26], and Claude 3.7 [1] are explicitly optimized for this with RLVR[36]. These increased reasoning capabilities introduce novel safety challenges, including potential unfaithfulness in the reasoning itself [19]. While current alignment evaluation methods often rely on examining reasoning traces [3, 29], we cannot rely on the models reasoning being faithful to the internal reasoning[7, 30, 2], creating need for more mechanistic analyses. Importance of individual steps. Many CoT sentences can be deleted or rewritten with limited impact on task accuracy [32, 21]. Prior attribution methods suggest that subset of steps disproportionately drive the final answer: Shapley values [10], ROSCOE metrics [13], gradient-based scores [35], and resampling at fork tokens [4]. Complementing these, our approach provides more principled framework for understanding how reasoning traces are constructed around key sentences. White-box analyses of reasoning. Mechanistic studies locate specialised components that propagate information across steps: iteration or receiver heads [6], small sets of heads necessary for CoT performance [37], steerable activation directions for planning and backtracking [31], and deduction circuits in symbolic tasks [5]. Other work identifies phase shifts separating pre-training priors from contextual reasoning [9]. These findings motivate our focus on sentence-level structure rather than token-level saliency. However, they do not provide general methods for understanding the causal interdependence between sentences."
        },
        {
            "title": "8 Discussion and Limitations",
            "content": "This work presents initial steps towards principled decomposition of reasoning traces with focus on identifying thought anchors: sentences with outsized importance on the models final response, specific future sentences, and downstream reasoning trajectory. We have also begun unpacking the attentional mechanisms associated with these important sentences. We expect that understanding thought anchors will be critical for interpreting reasoning models and ensuring their safety. We view this as preliminary work. Our analyses require refinement to fully grapple with how downstream sentences may be overdetermined by different trajectories in reasoning trace or independent sufficient causes. Additionally, we do not formally examine the role of error correction. Our receiver-head analyses are confounded by sentences position in the reasoning trace (see Appendix M). Further, our attention-suppression work is limited because it effectively requires the model to process out-of-distribution information. Nonetheless, we believe that we have demonstrated that our metrics are an advance on prior work, interrupting models and forcing final answers. The surprising degree of shared structure we have found across our three methods illustrates the potential value of future work in this area and points to the possibility of more powerful interpretability techniques to come."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was conducted as part of the ML Alignment & Theory Scholars (MATS) Program. We would like to thank Iván Arcuschin, Constantin Venhoff, and Samuel Marks for helpful discussions and feedback. We particularly thank Stefan Heimersheim for his valuable feedback and insightful suggestions, including ideas for experimental approaches that helped strengthen our analysis and contributed to the clarity of our presentation. We also thank members of Neel Nandas MATS stream for engaging brainstorming sessions, thoughtful questions during our presentations, and ongoing discussions that helped shape our approach."
        },
        {
            "title": "Author Contributions",
            "content": "Both first authors, Paul C. Bogdan and Uzay Macar, contributed to the research, engineering, and writing of the paper. Neel Nanda was the main supervisor and provided feedback and guidance throughout the project. Arthur Conmy proposed the black-box resampling method for measuring the importance of sentences and also provided feedback and guidance throughout the project."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 Sonnet. https://www.anthropic.com/claude/sonnet, 2025. Accessed 15 May 2025. [2] Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy. Chain-of-thought reasoning in the wild is not always faithful, 2025. URL https://arxiv.org/abs/2503.08679. [3] Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, and David Farhi. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation, 2025. URL https://arxiv.org/abs/ 2503.11926. [4] Eric Bigelow, Ari Holtzman, Hidenori Tanaka, and Tomer Ullman. Forking paths in neural text generation, 2024. URL https://arxiv.org/abs/2412.07961. [5] Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. mechanistic analysis of transformer trained on symbolic multi-step reasoning task. arXiv preprint arXiv:2402.11917, 2024. [6] Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Xingyu Alice Yang, Francois Charton, and Julia Kempe. Iteration head: mechanistic study of chain-of-thought. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=QBCxWpOt5w. [7] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning models dont always say what they think, 2025. [8] DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [9] Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty. How to think step-by-step: mechanistic understanding of chain-of-thought reasoning. arXiv preprint arXiv:2402.18312, 2024. [10] Leo Gao. Shapley value attribution in chain of thought, 2023. https://www.lesswrong.com/ posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought. [11] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks, 2021. URL https://arxiv.org/abs/2106.02997. 10 [12] Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal abstraction: theoretical foundation for mechanistic interpretability, 2025. URL https: //arxiv.org/abs/2301.04709. [13] Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: suite of metrics for scoring step-by-step reasoning, 2023. URL https://arxiv.org/abs/2212.07919. [14] Goodfire. Under the hood of reasoning model. https://www.goodfire.ai/blog/ under-the-hood-of-a-reasoning-model, 2025. Accessed: May 15, 2025. [15] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greaterthan?: Interpreting mathematical abilities in pre-trained language model, 2023. URL https: //arxiv.org/abs/2305.00586. [16] Stefan Heimersheim and Jett Janiak. circuit for python docstrings in 4-layer attention-only transformer, 2023. https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/ a-circuit-for-python-docstrings-in-a-4-layer-attention-only. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. [18] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukošiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning, 2023. URL https://arxiv.org/abs/2307.13702. [19] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. [20] Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. On the biology of large language model. Transformer Circuits Thread, 2025. URL https: //transformer-circuits.pub/2025/attribution-graphs/biology.html. [21] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686. [22] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [23] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. [24] Chris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases. https://www.transformer-circuits.pub/2022/mech-interp-essay, 2022. [25] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. [26] OpenAI. o1: Introducing our first reasoning model. https://openai.com/o1/, 2024. Accessed: 2025-05-15. 11 [27] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019. [28] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm, 2021. [29] Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, Sébastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, and Anca Dragan. An approach to technical agi safety and security, 2025. URL https://arxiv.org/abs/2504.01849. [30] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023. URL https://arxiv.org/abs/2305.04388. [31] Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, and Neel Nanda. Understanding reasoning in thinking language models via steering vectors. In Workshop on Reasoning and Planning for Large Language Models, 2025. [32] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters, 2023. URL https://arxiv.org/abs/2212.10001. [33] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small, 2022. URL https://arxiv.org/abs/2211.00593. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [35] Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju. Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions, 2023. URL https://arxiv.org/abs/2307.13339. [36] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [37] Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, and Zhiyu Li. Attention heads of large language models: survey, 2024."
        },
        {
            "title": "A Sentence taxonomy",
            "content": "Building on top of the framework presented by [31], we developed taxonomy consisting of eight distinct sentence categories that capture reasoning functions in mathematical problem-solving. Each category represents specific cognitive operation. The functions and examples for each category are given in Table 1. Notably, the uncertainty management category includes backtracking sentences. Table 1: Sentence taxonomy with reasoning functions in problem-solving Category Problem Setup Plan Generation Fact Retrieval Active Computation Function Examples Parsing or rephrasing the problem (e.g., initial reading) need to find the area of circle with radius 5 cm. Stating or deciding on plan of action, meta-reasoning Ill solve this by applying the area formula. Recalling facts, formulas, problem details without computation The formula for the area of circle is = πr2. Algebra, calculations, or other manipulations toward the answer Substituting = 5: = π 52 = 25π. Uncertainty Management Expressing confusion, re-evaluating, including backtracking Wait, made mistake earlier. Let me reconsider... Result Consolidation Self Checking Aggregating intermediate results, summarizing, or preparing Verifying previous steps, checking calculations, and re-confirmations So the area is 25π square cm which is approximately... Let me verify: πr2 = π 52 = 25π. Correct. Final Answer Emission Explicitly stating the final answer Therefore, the answer is... The distribution of categories across our dataset as shown in Figure 7 reveals that active computation constitutes the largest proportion (32.7%), followed by fact retrieval (20.1%), plan generation (15.5%), and uncertainty management (14.0%). The sequential structure of reasoning is reflected in the rarity and positioning of problem setup (2.4%), which typically occurs at the beginning, and final answer emission (0.7%), which predominantly appears toward the end of the reasoning process. Figure 7: Counts and frequencies of taxonomic sentence categories in our dataset."
        },
        {
            "title": "B Prompt information",
            "content": "We used the following prompt with OpenAI GPT-4o (April-May, 2025) to annotate each sentence: You are an expert in interpreting how LLMs solve math problems using multi-step reasoning. trace, broken into discrete text sentences, and label each sentence with: Your task is to analyze chain-of-thought reasoning **function_tags**: One or more labels that describe what this sentence 1. is *doing* functionally in the reasoning process. 2. directly depends on, e.g., uses information, results, or logic introduced in earlier sentences. list of earlier sentence indices that this sentence **depends_on**: This annotation will be used to build dependency graph and perform causal analysis, so please be precise and conservative: dependent on another if its reasoning clearly uses previous sentences result or idea. only mark sentence as Function Tags: Parsing or rephrasing the problem (initial reading or Stating or deciding on plan of action (often Recalling facts, formulas, problem details (without Performing algebra, calculations, manipulations Aggregating intermediate results, summarizing, Expressing confusion, re-evaluating, proposing Explicit statement of the final boxed answer or problem_setup: plan_generation: result_consolidation: 1. comprehension). 2. meta-reasoning). 3. fact_retrieval: immediate computation). 4. active_computation: toward the answer. 5. or preparing final answer. 6. alternative plans (includes backtracking). 7. earlier sentences that contain the final answer. 8. re-confirmations. 9. or is purely stylistic or semantic. uncertainty_management: final_answer_emission: self_checking: Verifying previous steps, checking calculations, and unknown: Use only if the sentence does not fit any of the above tags Dependencies: For each sentence, include list of earlier sentence indices that the reasoning in this sentence *uses*. For example: - If sentence 9 performs computation based on plan in sentence 4 and recalled rule in sentence 5, then depends_on: - If sentence 24 plugs in final answer to verify correctness from sentence 23, then depends_on: - If theres no clear dependency use an empty list: - If sentence 13 performs computation based on information in sentence 11, which in turn uses information from sentence 7, then depends_on: [11, 7] [4, 5] [23] [] Important Notes: - Make sure to include all dependencies for each sentence. - Include both long-range and short-range dependencies. - Do NOT forget about long-range dependencies. - Try to be as comprehensive as possible. - Make sure there is path from earlier sentences to the final answer. 14 Output Format: Return dictionary with one entry per sentence, where each entry has: - the sentence index (as the key, converted to string), - dictionary with: - \"function_tags\": - \"depends_on\": list of sentence indices, converted to strings list of tag strings Here is the expected format: { \"1\": { \"function_tags\": [\"problem_setup\"], \"depends_on\": [\"\"] }, \"4\": { \"function_tags\": [\"plan_generation\"], \"depends_on\": [\"3\"] }, \"5\": { \"function_tags\": [\"fact_retrieval\"], \"depends_on\": [] }, \"9\": { \"function_tags\": [\"active_computation\"], \"depends_on\": [\"4\", \"5\"] }, \"24\": { \"function_tags\": [\"uncertainty_management\"], \"depends_on\": [\"23\"] }, \"32\": { \"function_tags\": [\"final_answer_emission\"], \"depends_on\": [\"9, \"30\", \"32\"] }, } Here is the math problem: <PROBLEM> Here is the full chain-of-thought, broken into sentences: <SENTENCES> Now label each sentence with function tags and dependencies."
        },
        {
            "title": "C Sentence category probing",
            "content": "We trained linear classifier to identify sentence categories based on activations. We employed multinomial logistic regression with L2 regularization (C = 1.0) on the residual stream activity from layer 47 (last layer) of R1-Distill-Qwen-14B. For evaluating accuracy, we implemented group-5-fold cross-validation that ensured examples from the same problem response remained in either the training or testing set to prevent data leakage. We averaged the residual stream activity across tokens to create sentence-level representations, whose dimensions were then standardized. To address class imbalance in the training data, we employed balanced class weights. The model demonstrated strong discriminative power across all reasoning categories, achieving macro-F1 score of 0.71. The confusion matrix presented in Figure 8 reveals high classification accuracies for categories such as active computation (0.74), uncertainty management (0.79), and problem setup (0.83), while showing some confusion between functionally related categories. Figure 8: Confusion matrix showing the sentence category classification performance of logistic regression probe trained on activations from layer 47 of the R1-Distill-Qwen-14B model. Values represent the proportion of examples from each true category (rows) classified as each predicted category (columns). Diagonal elements indicate correct classifications."
        },
        {
            "title": "D Other reasoning model",
            "content": "D.1 Measuring counterfactual influence To assess the generalizability of our counterfactual importance findings, we replicated our resampling methodology on R1-Distill-Llama-8B, applying the same experimental parameters (e.g., temperature = 0.6 and top-p = 0.95) used for R1-Distill-Qwen-14B. We collected 100 rollouts for 10 correct and 10 incorrect base solutions using the identical question set described in Section 2. The resampling accuracy trajectories for R1-Distill-Llama-8B (Figure 9) demonstrate patterns that are similar to those observed in R1-Distill-Qwen-14B (Figure 2). Specifically, we observe similar characteristic accuracy fluctuations throughout the reasoning traces, with notable spikes and dips occurring at sentences corresponding to critical reasoning transitions. Figure 10 shows that R1-Distill-Llama-8B exhibits similar sentence category effects whereby plan generation and uncertainty management sentences demonstrate higher counterfactual importance compared to active computation and fact retrieval sentences (see Figure 3 for R1-Distill-Qwen-14B). 16 Figure 9: Accuracy over 100 rollouts at each sentence for (A) one correct and (B) one incorrect base solution for R1-Distill-Llama-8B. Red dots mark significant spikes or dips. Local minima and maxima sentences are annotated with category initials. Our analyses focus on the counterfactual KL-divergence between sentences, but resampling accuracy is visualized here as it is more intuitive. Figure 10: The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance for R1-Distill-Llama-8B, per the resampling method, plotted against the sentence categorys mean position in the reasoning trace. Only the 5 most common sentence types are shown. This cross-model validation supports our claim that reasoning traces are structured around high-level organizational sentences rather than low-level computational steps. The consistency of counterfactual importance patterns suggests that our sentence-level attribution framework captures fundamental properties of chain-of-thought reasoning that generalize beyond specific model implementations. D.2 Attention aggregation R1-Distill-Llama-8B displayed receiver-head patterns largely consistent with those of R1-DistillQwen-14B. The histogram of attention heads vertical-attention scores displays right tail, indicating that some attention heads tend to particularly focus attention on subset of sentences (Figure 12A). Interestingly, the R1-Distill-Qwen-14B receiver-heads tended to be more frequent in later layers (see below, Figure 17), which was not evident in R1-Distill-Llama-8B (Figure 11). The R1-Distill-Qwen-14B and R1-Distill-Llama-8B receiver heads displayed consistent patterns related to sentence types, such that plan generation, uncertainty management, and self checking sentences received heightened attention; although visually, the differences to fact retrieval and active computation may be less prominent, paired t-tests (paired with respect to given response) showed that plan generation, uncertainty management, and self checking always significantly surpassed fact retrieval and active computation (six paired t-tests: ps .01). No R1-Distill-Llama-8B results are provided for the attention suppression analysis, as that method was principally used for the case study, and no new case study was performed for R1-Distill-Llama-8B. 17 Figure 11: The plots here show the vertical-attention score patterns associated with the R1-DistillLlama-8B data. A. This histogram shows the kurtosis values across all attention heads, averaged across all reasoning traces; parallels Figure 4 based on the R1-Qwen-14B data. B. This scatterplot shows the kurtosis of each heads vertical-attention score, organized by layer. Figure 17 is the R1-Distill-Qwen-14B version of this figure, which showed an upward trend into later layers that is not evident here. Figure 12: Based on the R1-Distill-Llama-8B data, the boxplot shows the average top-64 receiverhead score for each sentence type. The boxes correspond to the interquartile range across different reasoning traces. Figure 5 is the R1-Distill-Qwen-14B version of this figure; note that for the R1Distill-Qwen-14B figure, the top-16 heads were used. We found that for Llama 8B, examining the top-64 heads yielded more pronounced differences, although the sentence types with the highest scores remain the same."
        },
        {
            "title": "E Embeddings model",
            "content": "We used all-MiniLM-L6-v2 with maximum sequence length of 256 tokens and hidden dimension of 384 as our sentence embeddings model from the sentence-transformers [27] library. We picked cosine similarity threshold of 0.8, which is the median similarity value between all sentence removed (i.e., original sentence) and sentence resampled pairs in our dataset."
        },
        {
            "title": "F Additional resampling results",
            "content": "Figure 13 presents mean counterfactual importances across all eight taxonomic categories for R1Distill-Qwen-14B, extending the main text results (Figure 3) which showed only the five most frequent sentence types. The expanded view includes three additional categories with lower frequencies. Problem setup sentences occur predominantly at trace beginnings (mean normalized position 0.1) with moderate-high counterfactual importance. Self checking sentences tend to occur in the secondhalf of the traces and show lower counterfactual importance. Final answer emission sentences appear late in traces (mean normalized position 0.9) and show the lowest counterfactual importance. The patterns observed in the five-category analysis remain consistent when examining the full taxonomy. 18 Figure 13: The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance for R1-Distill-Qwen-14B, per the resampling method, plotted against the sentence categorys mean position in the reasoning trace. All sentence types are shown."
        },
        {
            "title": "G Additional resampling details",
            "content": "G.1 Counterfactual versus resampling importance The resampling importance metric introduced in Section 3.2 treats all resampled sentences as equally informative, but different sentence types may exhibit varying degrees of overdetermination during resampling. Overdetermination occurs when resampled sentences Ti are frequently similar to the original sentence Si (i.e., Ti Si), indicating that the reasoning context strongly constrains what can be expressed at that position. We present empirical evidence that counterfactual importance is more nuanced measure by accounting for semantic divergence in resampled content. Some sentences are more overdetermined than others. Figure 14A shows that uncertainty management and plan generation sentences produce semantically different alternatives in large proportion of resamples, while active computation and problem setup sentences show lower divergence rates. The transition matrix in Figure 14B shows how sentence categories change under resampling. For instance, uncertainty management and active computation sentences are usually replaced by sentences of the same category, whereas plan generation and fact retrieval sentences are more often resampled into variety of other categories. Figure 14: (A) Fraction of semantically different resampled sentences by category, showing that uncertainty management and plan generation sentences produce more divergent alternatives when resampled. (B) Transition probabilities between original and resampled sentence categories. These resampling behaviors create systematic differences between our counterfactual and resampling importance metrics. Figure 15 demonstrates that the relationship between the two metrics varies substantially across sentences and sentence categories. The counterfactual importance metric aims to address overdetermination by explicitly filtering for semantically different resamples, providing more targeted measure of causal influence. In contrast, the resampling metric potentially overestimates the importance of sentences that consistently produce similar content when resampled. Figure 15: Comparison between counterfactual and resampling importance metrics across sentence categories. Each point represents single sentence and the dashed gray line is the = line. However, the counterfactual importance metric can yield high-variance estimates when the number of semantically divergent resampled sentences is low (e.g., < 10), as the conditional probability estimates become less reliable with limited data. Alongside the limitations discussed in Section 8, this represents another constraint of our approach that future work should investigate further. G.2 Sentence-to-sentence importance Beyond measuring individual sentence importance, our framework quantifies causal dependencies between specific sentence pairs within reasoning traces. Figure 16 displays the sentence-to-sentence importance matrix for problem #2236 (incorrect) (Each page number of 488-page book is printed one time in the book. The first page is page 1 and the last page is page 488. When printing all of the page numbers, how many more 4s are printed than 8s?), showing how individual sentences influence downstream reasoning steps. Below we list few illustrative cases. 12-PG 16-PG. The planning in sentence 12 (1. Count the number of 4s in the units place across all page numbers) raises the probability that the model produces sentence 16 (Starting with the 4s.) by 0.39. plan statement triggers subordinate planning step. 8-FR, 9-PG, 12-PG, 14-PG 32-UM. The uncertainty management in sentence 32 (However, need to check if 440-449 is fully included.) receives sizeable positive influence from several earlier sentences: 8-FR (+0.11), 9-PG (+0.06), 12-PG (+0.12), 14-PG (+0.10). This forms the dense horizontal band at row index 32. 39-RC 83-UM. The result consolidation in sentence 32 (Now, summing up all the 4s: - Units: 48 - Tens: 50 - Hundreds: 89. Total 4s = 48 + 50 + 89 = 187.) decreases the likelihood (i.e., inhibits) of 83-UM (Wait, but just to be thorough, let me check the hundreds place for 4s again.) by 0.22. 52-AC 65-SC. The computation in sentence 52 (The first four blocks 80-89, 180-189, 280-289, 380-389 each contribute 10 eights in the tens place.) decreases the likelihood of 65-SC (Let me go through each step again to make sure didnt make mistake.) by 0.16. 63-AC 64-UM, 65-SC, 69-SC, 75-SC, 83-UM, 86-SC. The computation in sentence 63 (So, the difference is 187 98 = 89.) propagates forward, increasing the likelihood of 64-UM (+0.24), 65-SC (+0.17), 69-SC (+0.16), 75-SC (+0.28), 83-UM (0.23), and 86-SC (0.16). This forms the dense vertical band originating from column index 63. 64-UM 65-SC, 69-SC, 75-SC, 83-UM, 86-SC. The uncertainty management in sentence 64 (Wait, that seems quite large difference.) further amplifies the same downstream block: 65-SC (+0.32), 69-SC (+0.25), 75-SC (+0.26), 83-UM (0.25), and 86-SC (0.25). 83-UM 86-SC, 90-FAE. Even very late checks matter. Sentence 83 (Wait, but just to be thorough, let me check the hundreds place for 4s again.) increases the chance of 86-SC (Correct. And for the tens place...) by 0.43 and of the final answer in 90-FAE by 0.41. 20 Figure 16: Sentence-to-sentence importance matrix for the 32 most important sentences in problem #2236 (incorrect), selected based on total outgoing and incoming importance. Each cell (i, j) shows the causal importance of sentence on sentence j, calculated as the difference in the probability sentence semantically occurs (> 0.8 cosine similarity) when sentence is present versus resampled. We provide the following pseudocode for estimating sentence-to-sentence importance: Input: Sentence index i, target sentence index (where > i), threshold = 0.8 Output: Importance score importance(i -> j) 1. Get rollouts R_keep where sentence was kept (resampling from i+1) 2. Get rollouts R_remove where sentence was removed (resampling from i) 3. For each rollout in R_keep: a. Extract all sentences S_r from rollout b. Find best matching sentence to target sentence j: - Compute sentence embeddings - Calculate cosine similarity between each in S_r and target - Select sentence with highest similarity if similarity >= c. Add to matches_keep if valid match found 4. For each rollout in R_remove: a. Extract all sentences S_r from rollout b. Find best matching sentence to target sentence (same process as step 3b) c. Add to matches_remove if valid match found 5. Calculate match rates: match_rate_keep = matches_keep / R_keep match_rate_remove = matches_remove / R_remove 6. Return importance(i -> j) = match_rate_keep - match_rate_remove"
        },
        {
            "title": "H Additional receiver head information",
            "content": "Receiver heads heads receiving high kurtosis scores are more common in late layers (Figure 17). Examples of receiver heads are shown in Figure 18, showing how the highest kurtosis head consistently narrows attention on particular sentences, and Figure 19, showing how there exist many heads that narrow attention on particular sentences. 21 Figure 17: This scatterplot shows the kurtosis of each heads vertical-attention score, organized by layer. There is an upward trend across layers and strong uptick among some late-layer heads. Figure 18: The attention weight matrices for the receiver head with the highest kurtosis score are shown here for all twenty responses. The coloring was defined such that the darkest navy corresponds to values surpassing 99.5th percentile value of each matrix. White is zero. 22 Figure 19: The attention weight matrices for response #1591 (incorrect) are shown here for the 20 attention heads yielding the highest kurtosis score across all responses. No effort was taken to cherry-pick responses showing prominent receiver head patterns; we are showing #1591 (incorrect) because it corresponded to the alphabetically earliest problem number among the ten problems analyzed (correct/incorrect chosen randomly). The coloring was defined such that the darkest navy corresponds to values surpassing 99.5th percentile value of each matrix. White is zero. 23 Figure 20: The navy and red lines on the left show the receiver-head scores assigned to sentences, averaged across the 16 heads with the highest kurtoses. The green lines on the right represent the ratio of the navy and blue lines for given sentence rank. Sentences with high receiver head scores receive more attention in the reasoning model compared to the base model."
        },
        {
            "title": "I Reasoning versus base model differences in receiver heads",
            "content": "Attentional narrowing toward particular sentences may be feature specifically of reasoning models. We submitted the reasoning traces to base model version of Qwen-14B and identified receiver heads. For both models, we sorted all sentences by their mean receiver-head score using the 16 attention heads with the highest kurtoses. The highest percentile sentences received greater attention by the reasoning model - e.g., the highest-percentile sentences receive 1.8x more attention via top-16 heads in the reasoning model compared to the base model (Figure 20). Additionally, lower percentile sentences receive less attention through the top-16 heads. This conclusion is somewhat tenuous, as no base-model difference is seen when this result is tested using R1-Distill-Llama-8B. Nonetheless, based on the Qwen-14B data, it appears the model has learned to narrow its attention toward particular sentences."
        },
        {
            "title": "J Effects of ablating receiver heads",
            "content": "To test the causal hypothesis that the receiver heads identified in Section 4 are functionally important for reasoning, we performed an experiment ablating receiver heads and evaluating how this impacts model accuracy. This intervention is designed to measure the direct impact of removing these heads on task performance and to evaluate the possibility that they may be more important than typical heads. J.1 Methodology We continue to use problems from the MATH dataset. We selected 32 problems where the non-ablated model achieves 10-90% accuracy on average. For each problem, we ran R1-Distil-Qwen-14B sixteen times, while allowing the model to output up to 216 (16,384) tokens. Responses that did not produce an answer by that point were marked as incorrect. We compared the effect of ablating 128 attention heads (approx. 7% of all heads), 256 heads (approx. 13%), or 512 heads (approx. 27%). The ablation strategies were: 1. Receiver head ablation: We ablated the top-N heads with the highest average kurtosis scores. 2. Random non-receiver (control) ablation: For each layer where receiver heads were ablated, we ablated heads chosen randomly from the set of heads not selected from that same layer. This ensures matched comparison with no overlap. Note that receiver heads are more common in late layers (see above, Figure 17). By ensuring that both conditions included an equal number of heads from each layer (rather than selecting 128, 256, or 512 heads randomly across all layers), this ensures that differences cannot be explained simply by differences in the layers selected. In the 512-head ablation condition, majority of attention heads in some late layers were marked as receiver heads. For these layers, the non-receiver control condition was modified to ablate the corresponding number of heads with the lowest kurtosis scores to ensure valid comparison set. For instance, if 60% of layer 43 heads are in the top-512, then the control condition included the 60% with the lowest kurtosis score, meaning that there is 20% overlap for that layer. 24 J.2 Results and Discussion Our experiments show that large number of heads must be ablated to induce significant drop in performance compared to the baseline level of accuracy (baseline = 64.1%, 95% CI: [56.0%, 72.1%]). Regardless of whether receiver heads or non-receiver heads are targeted, ablating 128 heads produces differences in accuracy that insignificantly differ from baseline accuracy, and ablating 256 heads still produces only small drop in accuracy  (Table 2)  . Table 2: Answer accuracy on MATH problems for different self-attention-head ablation conditions. The brackets show the 95% confidence interval for each accuracy estimate. Heads Ablated Receiver heads Random heads 256 512 48.8% [39.3%, 58.3%] 27.7% [17.2%, 38.2%] 52.7% [43.0%, 62.5%] 37.3% [27.5%, 47.1%] The importance of receiver heads emerges when large number of heads are ablated. When ablating 512 heads (over quarter of the models 1920 heads), targeting receiver heads caused performance to fall to 28% accuracy. Removing the same number of control heads resulted in less severe drop to 37% accuracy. There is significant difference between these percentages (t[31] = 2.55, = .02), suggesting receiver heads are more critical for reasoning than other heads. As mentioned, this analysis treats responses as incorrect if they do not produce final answer by 16,384 tokens. If the analysis is changed to instead simply omit those responses entirely from the analysis, there remains significant difference in accuracy when ablating top-512 receiver heads (29% accuracy) versus random non-receiver heads (39% accuracy) (t[31] = 2.66, = .02). Hence, regardless of whether non-completed responses are marked as incorrect or ignored, ablating receiver heads is found to exert larger impact on model accuracy than ablating random non-receiver heads. Receiver head correlations with sentence-sentence resampling importance sentences receiver-head score predicts its impact on downstream sentences per the resampling method. Specifically, our resampling approach (Section 3) produces matrix describing each sentences counterfactual importance on each future sentence. Based on said matrix, we can compute sentence-level measure analogous to the receiver head score by taking the average value below the matrix diagonal; as with the receiver head score, we omit the four entries immediately below the diagonal to avoid simply capturing proximal effects and focus on farther downstream impacts; we take the absolute value of the resampling matrix to produce an estimate of how given sentence may both upregulate or downregulate future sentences. We correlated sentence (top-64-head) receiver scores with their resampling scores calculated here - correlated separately for each response - yielding positive association (mean Spearman ρ = .22; 95% CI: [.17, .30]). Although the correlation is nominally weak/moderate, this is consistent trend, and 18 of the 20 responses tested displayed positive association (ρ > 0). sentences receiver-score additionally correlates with its downstream effect as computed by the attention suppression measure introduced in Section 5 (mean >4 below the diagonal) (mean Spearman ρ = .19; 95% CI: [.10, .28]). Although attention weights alone are not causal measure, they are computationally cheap to examine, requiring no post hoc forward pass, unlike the other two methods. Grounded in these other techniques, the receiver-head scores may hence prove useful by virtue of their limited cost. 2We are not aware of prior studies on attention head ablation for models generating long chain-of-thought reasoning, making it difficult to establish what is typical number of heads to ablate. Potentially, large number is necessary because the long reasoning traces (sometimes exceeding 10,000 tokens) provide extensive opportunities for error correction and compensatory computation."
        },
        {
            "title": "L Case study transcript",
            "content": "Below is transcript of the models response used for the case study (Section 6). The first 74 sentences following <think> are shown. After this point, the model converged on 100% resampling accuracy for all subsequent sentences. 0. (problem_setup): Okay, so have this problem where need to find out how many bits the base-16 number 6666616 has when its converted to base-2. 1. (uncertainty_management): Hmm, lets see. 2. (fact_retrieval): remember that each hexadecimal digit corresponds to exactly 4 binary digits, or bits. 3. (plan_generation): So, maybe can just figure out how many hexadecimal digits there are and multiply by 4 to get the number of bits. 4. (uncertainty_management): Let me check that. 5. (fact_retrieval): First, the number given is 66666 in base 16. 6. (active_computation): Let me count the digits: 6, 6, 6, 6, 6. 7. (result_consolidation): Thats five digits in total. 8. (active_computation): So, if each digit is 4 bits, then 5 digits would be 54=20 bits. 9. (uncertainty_management): Wait, is that always the case? 10. (fact_retrieval): think so because each hex digit maps directly to 4 bits without any overlap or sharing. 11. (result_consolidation): So, 6666616 would be 20 bits in binary. 12. (uncertainty_management): But wait, should make sure that this is correct by actually converting the number or at least checking if theres any leading zero that might affect the bit count. 13. (plan_generation): Alternatively, maybe can calculate the value of 6666616 in decimal and then find out how many bits that number would require. 14. (plan_generation): Lets try that approach to cross-verify. 15. (fact_retrieval): So, to convert 6666616 to decimal, can expand it as follows: 16. (fact_retrieval): Each digit represents power of 16. 17. (fact_retrieval): Starting from the right, the digits are 160,161,162,163,164. 18. (fact_retrieval): So, the number is: 19. (active_computation): 6164+6163+6162+6161+6160 20. (plan_generation): Let me compute each term: 21. (fact_retrieval): First, 164 is 16161616. 22. (active_computation): 162 is 256, so 163 is 25616=4096, and 164 is 409616=65536. 23. (active_computation): So, 665536=393216 24. (active_computation): Next, 163 is 4096, so 64096=24576 25. (active_computation): Then, 162 is 256, so 6256=1536 26. (active_computation): 161 is 16, so 616=96 27. (active_computation): And 160 is 1, so 61=6 28. (result_consolidation): Now, adding all these up: 29. (active_computation): 393216+24576=417,792 30. (active_computation): 417,792+1536=419,328 31. (active_computation): 419,328+96=419,424 32. (active_computation): 419,424+6=419,430 33. (result_consolidation): So, 6666616 is equal to 419,430 in decimal. 26 34. (fact_retrieval): Now, to find out how many bits this number requires, can use the formula for the number of bits needed to represent number in binary, which is loor(log2(N )) + 1. 35. (plan_generation): So, lets compute log2(419, 430). 36. (uncertainty_management): Hmm, dont have calculator here, but can estimate it. 37. (fact_retrieval): know that 218 =262,144 38. (fact_retrieval): 219 =524,288 39. (result_consolidation): So, 419,430 is between 218 and 219. 40. (result_consolidation): Therefore, log2(419, 430) is between 18 and 19. 41. (active_computation): The floor of that would be 18, so the number of bits is 18+1=19 bits. 42. (uncertainty_management): Wait, but earlier thought it would be 20 bits because its hex digits each being 4 bits. 43. (result_consolidation): Now, according to this, its 19 bits. 44. (uncertainty_management): Theres discrepancy here. 45. (uncertainty_management): must have made mistake somewhere. 46. (self_checking): Wait, let me check my calculations again. 47. (uncertainty_management): Maybe messed up the decimal conversion. 48. (fact_retrieval): Wait, 66666 in hex. 49. (plan_generation): Let me convert it step by step again. 50. (active_computation): 6164: 665536=393216 51. (active_computation): 6163: 64096=24576 52. (active_computation): 6162: 6256=1536 53. (active_computation): 6161: 616=96 54. (active_computation): 6160: 61=6 55. (active_computation): Adding them up: 393216+24576=417,792 56. (active_computation): 417,792+1536=419,328 57. (active_computation): 419,328+96=419,424 58. (active_computation): 419,424+6=419,430. 59. (result_consolidation): That seems correct. 60. (result_consolidation): So, 419,430 in decimal. 61. (fact_retrieval): Now, lets check 218 is 262,144, 219 is 524,288. 62. (result_consolidation): So, 419,430 is less than 524,288, so log2(419, 430) is less than 19. 63. (active_computation): So, loor(log2(419, 430)) is 18, so number of bits is 19. 64. (uncertainty_management): But wait, when thought about each hex digit being 4 bits, 5 hex digits would be 20 bits. 65. (uncertainty_management): So, why is there difference? 66. (plan_generation): Ah, perhaps because leading zeros are not counted. 67. (fact_retrieval): So, when converting from hex to binary, each digit is 4 bits, but if the leading hex digits are such that when converted to binary, they dont add any leading zeros beyond the 4 bits per digit. 68. (uncertainty_management): Wait, but 66666 in hex is 5 digits, so 54=20 bits. 69. (result_consolidation): But according to the decimal conversion, its 19 bits. 70. (result_consolidation): That suggests that when converted to binary, its 19 bits, not 20. 27 71. (uncertainty_management): So, which is correct? 72. (plan_generation): Wait, perhaps when converting 6666616 to binary, its exactly 20 bits, but the leading bits are zeros, so when counting the number of bits, we dont count leading zeros. 73. (fact_retrieval): Wait, but in the problem, its just asking how many bits it has when written in base 2. Sentence 13 (Alternatively, maybe can calculate the value of 6666616 in decimal and then find out how many bits that number would require.) was found to have the highest counterfactual importance among any sentence in the response (see Figure 2A). Five alternative possible sentences resampled at the sentence 13 position, marked by whether they eventually led to correct or incorrect answer, are as follows: 1. () Let me think. The first digit is 6, which in binary is 0110. 2. () Let me try converting the number to decimal first. . . 3. () Let me try converting the first few digits to binary to see how it goes. 4. () Let me think about the conversion process. 5. () Let me try converting the number step by step. Sentence position effects on receiver-head scores sentences position within the reasoning trace will tend to influence its measured receiver score. As reasoning trace progresses, the number of possible broadcasted sentences will necessarily increase. For instance, by sentence 20, there might be only two broadcasted sentences (each receiving 50% of attention from sentences 21-29), whereas by sentence 100, there could be ten broadcasted sentences (each receiving 10% of attention from sentences 101-109). As the sum of an attention weight row will sum to 1 (at the token level), later sentences will distribute their attention across larger number of past sentences. This dilution of attention creates downward pressure on the receiverhead scores of later sentences. This is the case even though receiver head score extends through all subsequent low-competition or high-competition periods. For example, broadcasting sentence 20 will face limited competition from receiving sentence 21-29 attention and high competition for sentences 101-109, whereas broadcasting sentence 100 will exclusively face high competition, pushing its score downward as broadcasting-sentence position increases. There also exists proximity effect on receiver-head scores that operates in the opposite direction of the above effect. Although broadcasted sentences are attended by all subsequent sentences to some degree, this will be more so the case for more recently subsequent sentences (e.g., receiving more attention from sentence 10 sentences downstream than one 20 sentences downstream). For sentences late in the reasoning trace, the average distance to future sentences will be shorter. For example, if reasoning trace contains 120 sentences, then sentence 100 will be at most 19 sentences apart from any given future sentence, whereas sentence 20 will be at most 99 sentences apart. To degree, the analyses in the report account for proximity effects by ignoring the 4 sentences immediately proximal to given sentence when calculating vertical-attention scores. However, this will not fully address proximity effects. We see no reason why the downward pressure of sentence position on receiver-head scores (attention dilution) will be equal in magnitude to the upward pressure of sentence position (proximity effects). For the preparation of the present report, we conducted exploratory analyses evaluating whether the above confounding factors invalidate any presented finding, and we did not find evidence that this is the case. Thus, rather than pursuing some technique to account for the above pressures (e.g., linearly weighing attention weight matrices based on their position), we opted to only account for these factors in minimal fashion by ignoring the attention among sentences just 4 sentences apart."
        }
    ],
    "affiliations": [
        "Aiphabet",
        "Duke University"
    ]
}