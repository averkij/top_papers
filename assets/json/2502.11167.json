{
    "paper_title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
    "authors": [
        "Bohan Lyu",
        "Siqiao Huang",
        "Zichen Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE."
        },
        {
            "title": "Start",
            "content": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors Bohan Lyu1 * Siqiao Huang2 * Zichen Liang1 * 1Department of Computer Science and Technology, Tsinghua. 2Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua. lyubh22@gmail.com, {huang-sq23, liang-zc22}@mails.tsinghua.edu.cn *Equal contribution Corresponding author 5 2 0 2 6 ] . [ 1 7 6 1 1 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of program without actually running it. To systematically investigate this capability, we introduce SURGE, comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, highcost scientific computing, time-complexityintensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple opensource and proprietary LLMs on SURGE and conduct scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Reid et al., 2024; Meta, 2024; Anthropic, 2024b; Hui et al., 2024; Bi et al., 2024) have demonstrated remarkable capabilities in code-related tasks (Lu et al., 2021a; Zheng et al., 2023; Luo et al., 2023; Team, 2024a; Guo et al., 2024), including code understanding (Ahmad et al., 2020; Chakraborty et al., 2020) and code generation (Li et al., 2018a; Parvez et al., 2018). However, an equally important yet underexplored question is whether LLMs can serve as generalFigure 1: Performance of 6 typical models on 8 subsets of SURGE. purpose surrogate code executors, which predict the output and behavior of program without actually running it. Recent study (Lyu et al., 2024) acknowledges its importance, however, it focuses on case study rather than systematic analysis. The ability to predict code execution outcomes without execution has tremendous siginficance. In scientific computing, running simulations often requires substantial computational resources and is time-consuming, making it impractical to test every possible configuration (Lu and Ricciuto, 2019; Hesthaven and Ubbiali, 2018; Benner et al., 2015). In security-sensitive environments, executing untrusted code poses inherent risks, necessitating alternative mechanisms for assessing program behavior without exposing the system to potential vulnerabilities (Nebbione and Calzarossa, 2023; Shirazi et al., 2017; Wang et al., 2024). Additionally, some code requires highly specific execution environments, which may not always be available, making surrogate execution valuable alternative (Queiroz et al., 2023; Gu et al., 2025). Moreover, accurately predicting models potential outputs or errors is crucial for improving traditional tasks such as code understanding, code generation, and even math reasoning (Li et al., 2025). Lastly, many works use LLMs as reward models (RMs) in reinforcement learning. For code tasks, accurate execution prediction is key to reliable RM (Ouyang et al., 2022). Traditional approaches to surrogate code executing (King, 1976; Cadar and Sen, 2013) struggle to generalize across languages and suffer from scalability issues when applied to complex real-world codebases. Containerized environments (Merkel, 2014) mitigate dependency issues but still require full code execution. Recent efforts to train neural executors (Yan et al., 2020) focus on narrow tasks and lack the generality needed for real-world code. In contrast, LLMs capacity to internalize patterns from vast code corpora (Lu et al., 2021b; Chaudhary, 2023) suggests path toward general-purpose surrogate code execution, though their limits remain unquantified. Therefore, its crucial to understand the potential of LLMs as general-purpose surrogate code executors. In this paper, we try to figure out how well current LLMs function as general-purpose surrogate code executors and whether this ability can be improved through training. To systematically investigate LLMs as GEneralpurpose SURrogate code executors, we introduce SURGE. It includes 8 components: (1) fundamental programming tasks in multiple languages, (2) competition programming problems requiring deep logical inference, (3) repository-level codebases that test long-range dependencies, (4) scientific simulations and optimizations where direct execution is high-cost, (5) time-consuming logical algorithms that have high time-complexity, (6) buggy code that examines LLMs ability to predict runtime errors, (7) programs whose behavior depends on specific compiler versions or execution environments and (8) math theorem proving in formal language (LEAN4) (De Moura et al., 2015; Moura and Ullrich, 2021) which expects compilers to testify the proofs. By evaluating both open-source and proprietary LLMs on this benchmark, we provide the first comprehensive study of their potential as general-purpose code execution surrogates. The performance of typical models on SURGE is shown in Figure 1. Beyond evaluating existing models, we conduct Figure 2: Performance scaling across model sizes and training steps. scaling law study to determine whether models performance improves with increased model size and data availability. Specifically, we train models with sizes ranging from {0.5/1.5/3/7}B on different scales of training data from the formal language subset of SURGE and analyze how model capacity and training data scale affect surrogate execution accuracy. Our scaling experiments demonstrate that performance consistently improves with both model size (from 0.5B to 7B parameters) and training steps, with larger models showing stronger learning capacity and higher performance ceilings throughout the training process (Figure 2). In short, our work makes the following key contributions: We introduce SURGE, the first but holistic benchmark for evaluating LLMs as generalpurpose surrogate code executors. We conduct an extensive evaluation of both open-source and proprietary LLMs on SURGE, providing the first large-scale study assessing their capabilities. We present scaling law study with models of varying sizes and data of different scale, providing empirical insights into whether LLMs systematically improve with more capacity and training data."
        },
        {
            "title": "2 Related Works",
            "content": "Neural Surrogate Models. Neural surrogate models are neural network-based approximations used to replace computationally expensive simulations in various scientific and engineering domains (Zhang et al., 2024; Sun and Wang, 2019). These models act as efficient emulators by learning complex input-output relationships from highfidelity data, significantly reducing computational Figure 3: The construction Pipeline of SURGE. costs while maintaining accuracy (Raissi et al., 2020; Sun et al., 2020; Bessa et al., 2017; Thuerey et al., 2020; Raissi et al., 2019; Willard et al., 2022). Recently, generative models (e.g. pre-trained language models) have been incorporated into surrogate modeling. Some equip language models with traditional surrogate models to facilitate iterative optimization (Ma et al., 2024; Lyu et al., 2025), and some use generative models to realize the endto-end surrogate process (Gruver et al., 2024; Hao et al., 2024; Wimmer and Rekabsaz, 2023; Che et al., 2024). While these studies primarily focus on natural sciences, time series, and multimodal gaming, the application of surrogate modeling to code execution, where both input and output exist in the modality of language, remains unexplored. LLMs for Code. LLMs are widely used in coderelated tasks (Lu et al., 2021a; Zheng et al., 2023; Luo et al., 2023; Team, 2024a; Guo et al., 2024), which can be fundamentally categorized into code understanding and code generation. Code understanding tasks include code summarization (?Hu et al., 2018; Harer et al., 2019; Ahmad et al., 2020), bug detection (Li et al., 2018b; Russell et al., 2018; Zhou et al., 2019; Chakraborty et al., 2020), duplication detection (Zhang et al., 2019; Yu et al., 2019; Wang et al., 2020), code retrieval (Husain et al., 2020; Lu et al., 2021a), etc. Code generation tasks include code completion (Li et al., 2018a; Parvez et al., 2018), code repair (Chen et al., 2019; Chakraborty et al., 2020; Lutellier et al., 2020), test generation (Watson et al., 2020; Siddiq et al., 2024; ?), etc. When evaluating LLMs on these tasks, some datasets provide broad evaluations across general tasks (Chen et al., 2021; Austin et al., 2021; Liu et al., 2024; Muennighoff et al., 2023), some focus on specific dimensions such as multi-lingual capabilities (Athiwaratkun et al., 2022; Zheng et al., 2023; Cassano et al., 2022; Khan et al., 2023), competition problems (Hendrycks et al., 2021; Li et al., 2022), data science tasks (Chandel et al., 2022; Lai et al., 2023; Huang et al., 2022), repositorylevel understanding (Zhang et al., 2023a; Shrivastava et al., 2023; Liu et al., 2023; Li et al., 2024; Jimenez et al., 2023; Ding et al., 2024; Zan et al., 2024), and reasoning abilities (Athiwaratkun et al., 2022; Austin et al., 2021; Cobbe et al., 2021; Gao et al., 2023; Gu et al., 2024). However, while the potential execution result of code is important for both code understanding and generation, this aspect remains largely unexplored (Weber et al., 2024)."
        },
        {
            "title": "SURGE",
            "content": "To comprehensively evaluate the potential of LLMs as surrogate code executors, we construct diverse benchmark, SURGE, that covers various code execution scenarios. Our dataset is designed to assess the models ability to approximate execution results across multiple dimensions, including multilingual diversity, repository-level complexity, computational intensity, error handling, and scenariodependent variability. Below, we describe each component of SURGE, explaining the motivation behind its inclusion."
        },
        {
            "title": "3.1 Dataset Construction",
            "content": "As illustrated in Figure 3, various construction pipelines are applied in constructing the SURGE benchmark. For ML, CL, BG, we first curate an original dataset through manual construction or development from existing coding problems. Then an iterative process of refinement is applied, namely, we iterate between code execution, utilizing the power of LLMs to refactor the code and manually checking the code to ensure no compile error, bias for certain answers and other falsities would occur. For RL, as we are focusing on repository-level coding, we turn to public repositories as well as custom repositories. Then, based on these repositories, we construct test cases by injecting human bias of the suitable parameters and executing the code to obtain the ground truth values. Regarding SC, TC and DR, we first carefully select series of areas and questions concerning the related component, spanning wide spectrum from theory to practice. The collection of questions is then implemented through handcrafted coding and checked for implementation mistakes before finally executing the code. Lastly, concerning FL, we adapt datasets in mathematics as well as formal math prover. Then, the compilation results and formal proofs are aggregated to form the final component of SURGE."
        },
        {
            "title": "3.2 Dataset Components",
            "content": "Multi-lingual Code (ML). The most fundamental nature of general-purpose surrogate executor is its ability to handle multiple programming languages. Since we are exploring the models execution capabilities, we do not focus on rendering languages such as HTML, but concentrate on computational languages. Our dataset covers 7 such languages, including C, C++, C#, Java, Rust, Python, and Julia. Our dataset is adapted from McEval (Chai et al., 2024). The original dataset does not provide executable code, so we used an LLM to generate executable code by providing it with prompts, ground truth, and test cases in the original dataset. This generated code was then manually processed. We manually modified code that failed to compile, such as adding missing headers. And then we took precautions to prevent answers from being leaked in assert statements or comments. Competition-level Code (CL). Next, we consider competition-level code, which presents higher level of coding difficulty. We collect these tasks from 2 public repositories 12, which contain problems from open coding platforms (e.g. LeetCode 3, Luogu 4). The dataset includes problems in 3 languages, C++, Java, and JavaScript. Since the original repositories only provide par1https://github.com/azl397985856/leetcode 2https://gitee.com/shanire/OJCode 3https://leetcode.com 4https://www.luogu.com.cn tial solutions, we first use an LLM to generate complete, executable code that prints the expected output. This generated code is then manually verified. To investigate whether problem difficulty affects the performance of surrogate models, we further employ an LLM to automatically classify problems into 5 different difficulty levels, followed by human verification to ensure accuracy. Repository-level Code (RL). In real-world scenarios, most code exists at the repository level, making repository-level code execution prediction equally important for general-purpose surrogate model. We manually collect computational repositories that fit within the input length constraints of LLMs. These repositories include tasks such as solving the 24-point problem, Sudoku solving, and converting Python code to LaTeX. These tasks exhibit complex logic but do not rely on sophisticated models or external inputs. To assess the models ability to understand multi-file structures, we also manually construct two repositories containing advanced C++ syntax and multiple files. Scientific Computing (SC). Scientific computing has long been adopting neural surrogate models. Its crucial to investigate whether LLMs can approximate execution results for these non-trivial tasks and hold the potential to serve as efficient surrogate models. We introduced tasks ranging from solving ordinary differential equations (ODEs) to optimization problems and signal processing. These tasks are motivated by and widely used in real-world scientific challenges, including areas where increasing research has been done on solving these computational tasks through building efficient surrogate models (Wu et al., 2023; Zhang et al., 2023b). comprehensive overview of the setup for each task, along with the corresponding algorithms, can be found in Appendix B.2. Time-Consuming Algorithms (TC). Surrogate models were originally motivated by real-world applications where program execution is high-cost and time-consuming. Its necessity for LLms to generalize well to strongly computation-powerdependent and time-consuming tasks. We include examples from linear algebra, sorting, searching, Monte Carlo simulations, and string matching programs, ensuring broad representation of computationally intense tasks. These tasks cover various complexity classes, including (e.g. sorting an array), NP (e.g. Hamiltonian Cycle), and NP-Hard (e.g. Traveling Salesmans Problem). detailed description of each tasks setup and corresponding algorithms is provided in Appendix B.3. Buggy Code (BG). Real-world code execution often encounters errors due to syntax mistakes, logical flaws, or runtime exceptions. Code errors pose risks in sensitive scenarios, therefore, we aim for the code surrogate model to recognize the presence of bugs. This dataset is adapted from DebugBench (Tian et al., 2024), which extracted Java, Python, and C++ code from LeetCode and manually inserted errors from 4 major bug categories and 18 minor types. Since DebugBench only provides erroneous code snippets rather than complete executable programs, we first used an LLM to automatically complete the error-free code into fully runnable versions. After verifying their correctness, we replaced relevant parts with buggy code and executed them again to capture the corresponding error outputs. Some errors resulted in infinite loops causing timeouts, so we set 30-second execution filter out such cases. Code with Differential Results under Different Scenarios (DR). Various contextual factors, such as compiler versions, optimization levels, and language standards often influence code execution. These variations can lead to different outputs for the same code snippet. It is crucial for surrogate models to recognize such discrepancies and adapt to different settings. We focus specifically on C++ and manually collect code snippets from textbooks and online sources (Bryant and OHallaron, 2010; Lippman et al., 2012) that exhibit different behaviors under varying compilation settings. We consider multiple compilers (g++, clang++), C++ standards (03, 11, 14, 17), and optimization levels (-O0, -O1, -O2, -O3, -Os). Each snippet is executed across these different settings, and we retain only those that produce varying outputs through different configurations while discarding cases that yield identical results across all settings. Mathematics Formal Language (FL). MathProving Formal Languages are specialized programming languages designed for mathematical proof verification through compilers (De Moura et al., 2015; Moura and Ullrich, 2021; Paulson, 1994; Barras et al., 1997). These compilers can determine whether proof is correct and identify specific errors if present. Formal theorem proving is gaining increasing attention, as constructing valid proof requires extensive trial and verification, which can be highly time-consuming. We aim to explore whether surrogate model can assist in this verification process. Unlike conventional code execution, formal language verification follows distinct paradigm, therefore we categorize this task separately. In this study, we focus on Lean 4 which is the most widely used proof assistant language. To build our dataset, we use state-of-the-art formal math prover, Goedel-Prover (Lin et al., 2025), to conduct large-scale reasoning on LeanWorkbook (Ying et al., 2024), extracting an equal proportion of correct and incorrect proofs. This balanced dataset allows us to evaluate the surrogate models ability to assess proof validity effectively."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "We design different evaluation metrics tailored to each subset of SURGE to ensure accurate evaluation. In ML and CL, the outputs are simple numerical values or formatted strings and contain no error or warning message, we employ exact string matching to measure correctness. For RL, we employ different evaluation methods for different tasks. For structured repositories, we use exact character matching to compare outputs. For Sudoku and 24-point problems, we use edit distance to compare results. For other types of repositories, we apply the Ratcliff/Obershelp (Ratcliff and Metzener, 1988) algorithm, which measures the similarity between two sequences by finding the longest common subsequence and computing similarity ratio. For SC, various tasks necessitate distinct evaluation methods. Specifically, (1) numerical simulations are evaluated using the average Relative Absolute Error (RAE), widely used metric that measures the deviation between the estimated values and the ground truth values; (2) position-based tasks, such as binary search, are assessed through exact string matching; and (3) sorting tasks are evaluated by the rank correlation coefficient (Spearman, 1904), which quantifies the similarity in the ordering of elements. Details regarding the evaluation metrics, and their correspondence to specific tasks, can be found in Appendix B.4. For BG, where outputs contain error messages, we use the Jaccard similarity (Jaccard, 1901) beTable 1: Holistic results of different models and different prompting strategies on SURGE. Model ML CL RL SC TC BG DR FL Avg. Claude-3.5-Sonnet DeepSeek-V3 GPT-4o GPT-4o-Mini Qwen-Max LLaMA-3.1-8B-Instruct Qwen-2.5-0.5B-Instruct Qwen-2.5-1.5B-Instruct Qwen-2.5-3B-Instruct Qwen-2.5-7B-Instruct Qwen-2.5-Coder-0.5B-Instruct Qwen-2.5-Coder-1.5B-Instruct Qwen-2.5-Coder-3B-Instruct Qwen-2.5-Coder-7B-Instruct Claude-3.5-Sonnet DeepSeek-V3 GPT-4o GPT-4o-Mini Qwen-Max LLaMA-3.1-8B-Instruct Qwen-2.5-1.5B-Instruct Qwen-2.5-3B-Instruct Qwen-2.5-7B-Instruct Qwen-2.5-Coder-0.5B-Instruct Qwen-2.5-Coder-1.5B-Instruct Qwen-2.5-Coder-3B-Instruct Qwen-2.5-Coder-7B-Instruct Claude-3.5-Sonnet DeepSeek-V3 GPT-4o GPT-4o-Mini Qwen-Max LLaMA-3.1-8B-Instruct Qwen-2.5-0.5B-Instruct Qwen-2.5-1.5B-Instruct Qwen-2.5-3B-Instruct Qwen-2.5-7B-Instruct Qwen-2.5-Coder-0.5B-Instruct Qwen-2.5-Coder-1.5B-Instruct Qwen-2.5-Coder-3B-Instruct Qwen-2.5-Coder-7B-Instruct 0-shot 21.67 11.67 13.33 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.00 42.67 36.67 28.67 42.67 3.33 4.67 7.33 14.00 16.67 4.67 16.67 30.00 33.33 81.33 56.67 66.67 84.00 38.67 12.67 19.33 41.33 35.33 27.33 27.33 54.00 68.67 74.67 0-shot Chain-of-Thought 82.00 76.67 80.00 76.00 71.33 41.33 26.67 24.67 52.67 2.00 18.67 53.33 45. 25.00 25.00 16.67 0.00 16.67 0.00 0.00 0.00 1.67 0.00 0.00 0.00 0.00 50.67 42.00 38.00 28.67 48.00 19.33 4.67 12.67 18.00 1.33 14.67 26.67 30.67 few-shot Chain-of-Thought 82 80.67 75.33 70.67 82.00 30.67 16.67 20.00 28.67 49.33 1.33 2.00 50.67 58.67 45.00 41.67 35.00 26.67 36.67 6.67 0.00 0.00 5.00 11.67 0.00 0.00 13.33 11.67 50.00 51.33 47.33 38.67 47.33 29.33 24.67 24.00 28.67 30.67 4.00 31.33 30.00 40. 18.00 18.67 18.00 16.67 18.00 12.00 1.33 5.33 5.33 15.33 0.67 2.67 12.67 16.00 22.00 18.00 17.33 15.33 22.00 10.00 4.67 6.00 16.00 0.00 6.00 12.00 16.67 24.00 20.67 20.00 16.67 19.33 10.67 3.33 4.00 6.00 16.67 1.33 4.00 11.33 16.67 74.00 63.33 53.33 57.33 41.33 5.33 8.00 25.33 26.00 12.00 16.67 33.33 37.33 44.67 83.33 80.67 76.67 72.67 78.67 27.33 22.00 34.00 34.67 6.00 20.00 35.33 51.33 79.33 82.67 74.67 69.33 78.67 19.33 8.00 20.00 26.00 34.00 2.00 8.00 35.33 43. 4.67 2.00 3.33 2.00 3.33 0.67 1.33 1.33 1.33 1.33 0.00 0.67 2.00 1.33 6.00 2.00 5.33 2.00 4.67 2.00 1.33 0.67 1.33 0.00 1.33 2.00 1.33 4.67 3.33 2.67 2.00 3.33 0.67 0.67 0.67 1.33 1.33 0.00 1.33 2.00 1.33 8.00 7.00 11.5 9.5 7.00 0.5 1.00 5.00 2.5 4.00 3.00 5.5 6.00 11. 7.5 11.5 10.00 10.00 10.5 6.00 2.00 5.00 6.5 2.00 6.00 4.5 11.5 11.5 12.00 12.00 9.5 11.5 6.00 0.00 2.00 4.00 5.5 0.00 6.00 5.00 9.5 18.00 32.67 22.00 39.33 30.00 0.00 5.33 12.13 15.98 23.01 41.33 34.67 41.33 0.67 34.67 35.33 27.33 33.33 22.00 5.33 40.00 8.00 5.33 34.00 24.00 40.00 25.33 32.00 35.33 27.33 24.67 24.67 17.33 20.00 42.00 36.00 37.33 38.00 34.00 39.33 15.33 34.31 29.74 28.53 31.12 24.22 4.48 5.34 16.72 11.03 10.09 12.24 19.31 25.86 24. 38.62 36.21 34.22 31.21 34.57 14.66 13.19 11.98 17.76 5.95 11.98 22.67 24.05 39.48 39.66 35.86 31.72 36.9 15.34 9.48 14.66 17.33 23.45 6.03 11.47 23.36 25.00 tween predicted and ground truth error messages. Jaccard similarity measures the overlap between two sets and is defined as: J(A, B) = B B , (1) where and represent the sets of words in the predicted and ground truth error messages respectively. Jaccard similarity well captures keywords in strings, thus suitable for error message comparison. For DR, since the same code can produce different outputs in varying settings, which sometimes include warnings or errors, we again utilize Jaccard similarity. This metric appropriately handles cases where the ground truth is not an error message. For FL, the results consist of two parts: (1) whether the proof passes or not, and (2) if it fails, the associated error message. The evaluation proceeds as follows. If both predicted and ground truth results indicate successful proof, the prediction is considered correct. If one is passed and the other is not, the prediction is incorrect. If both fail, we evaluate the accuracy of the error message. The error message consists of list containing the error locations and descriptions. We compute the score of prediction as: Figure 4: Models Performance on TC subset across programs with different run time on CPU."
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 1[ˆpj ] J( ˆmj, mj), (2) V38 (671B). where is the number of errors in the ground truth, is the set of predicted error positions, pj represents the j-th ground truth error position, ˆpj represents the predicted error position corresponding to pj, 1[ˆpj ] is the indicator function which equals to 1 when there exists ˆpj and equals to 0 when theres not, mj is the ground truth error message for position pj, ˆmj is the predicted error message for position ˆpj, is the Jaccard similarity function as defined in Equation 1."
        },
        {
            "title": "3.4 Dataset Statistics",
            "content": "Except for RL, which has 60 questions, and DR, which has 200 questions, all other subsets have 150 questions. Our dataset contains total of 1,160 questions."
        },
        {
            "title": "4.1 Setup",
            "content": "Models. We tested SURGE on 10 open-source and 4 closed-source models of different sizes, including both chat models and code models. The closed-source models include GPT-4o (202408-06) (OpenAI, 2024b), GPT-4o-mini (2024Claude-3.507-18) Sonnet (2024-10-22) (Anthropic, 2024a), and Qwen-Max (2025-01-25) (Team, 2024b). The open-source models include LLaMA-3.1-8BInstruct5, Qwen-2.5-{0.5, 1.5, 3, 7}B-Instruct6, Qwen-2.5-Coder-{0.5, 1.5, 3, 7}B-Instruct7 and DeepSeek- (OpenAI, 2024a), 5https://huggingface.co/collections/ meta-llama/llama-31-669fc079a0c406a149a5738f 6https://huggingface.co/collections/Qwen/ qwen25-66e81a666513e518adb90d9e Settings. We tested the above models on SURGE under 3 settings: 0-shot w/o CoT, 0-shot w/ CoT, and few-shot w/ CoT. CoT here means whether we use Chain-of-Thought (Wei et al., 2022) prompting, allowing the models to think step by step, or ask the models to answer directly. We set the temperature to 0, i.e. employing greedy decoding."
        },
        {
            "title": "4.2 Results",
            "content": "From the experimental results, there are several notable findings: (1) Our benchmark, especially certain subsets, exhibits strong discriminative ability. Even the strongest models perform only moderately well, which highlights the value of our benchmark. (2) We found that for the task of code execution surrogate, both Chain-of-Thought (CoT) and fewshot learning can enhance model performance. (3) We observed that for models of the same size, code models outperform chat models in the zero-shot setting for this task, whereas in the other two cases, chat models perform better."
        },
        {
            "title": "5.1 Model Prediction’s Accuracy and\nProgram’s Actual Execution Time",
            "content": "In this section, we explore the relationship between the execution time of the given code through compiling and the accuracy of using large language models as surrogate model to acquire the output. Specifically, we observed the trend of prediction accuracy of the model falling as the actual execution time required for the corresponding program prolongs. Its especially worth noting that for computational tasks that require execution time longer 7https://huggingface.co/collections/Qwen/ 8https://huggingface.co/deepseek-ai/ qwen25-coder-66eaa22e6f99801bf65b0c2f DeepSeek-V3 Figure 5: Breakdown of error types across different language models and prompting methods. than 1 second, state-of-the-art models still struggle significantly to obtain even one correct answer. To derive these results, we first recorded the actual execution times of the programs in TC. These inputs were then sorted into distinct bins based on their execution times. Next, we calculated the average accuracy for each model across all samples within the same bin, and aggregated these averages to generate Figure 4. As depicted in the figure, higher prediction accuracy is generally associated with shorter execution times. This indicates while large language models are much anticipated for their potential of serving as general surrogate models, current LLMs capacities are better suited for more time-efficient and computationally light tasks."
        },
        {
            "title": "5.2 Error Analysis",
            "content": "To further understand the models performance and limitations regarding serving as general surrogate models, we categorize the Errors made by Claude-3.5-Sonnet, GPT-4o and Llama3.1-8B-Instructin the CL component of our SURGE benchmark. To gain detailed understanding of the models coding abilities, we used combination of machineassisted annotation and manual verification to classify the errors. We specifically designed several error keywords to reflect the models varying capabilities across different tasks. For example, Code Language Knowledge is used to assess the models foundational programming language abilities, Calculation Accuracy measures its performance in scientific computations, Context Awareness demonstrates its ability in long-text, repository-level coding, while Conceptual Understanding and Causal Reasoning represent the logical coherence of the code. The categorized error statistics of the three models can be viewed in Figure 5. As models prompted with CoT exhibit fewer instances of most error types compared to zero-shot, especially for the Code Understanding capability of Llama, which shows significant improvement over zero-shot, CoT prompting results in universal performance gain on the overall performance. The main error types are Code Understanding, Calculation Process, and Calculation Accuracy. For zero-shot, the primary error is accuracy, but for CoT, the most frequent error is Calculation Process. This suggests that CoT can better grasp the overall code logic and produce more correct results, but it may still make mistakes in the chain of thought process. In general, CoT has fewer and smaller errors. From the model perspective, Llama has clear lead in Conceptual Understanding errors, indicating its weaker ability to understand concepts. The Claude model has the fewest errors, showing better performance under our criteria. Moreover, the error distribution of Claude and GPT is quite similar, which may suggest they share similar way of thinking."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce SURGE, holistic benchmark for evaluating LLMs as general-purpose surrogate code executors. The curated dataset spans multiple domains, bridging theoretical tasks with real-world applications. Through extensive empirical study, we argue that while current large language models possess the power to predict code execution results to certain extent, there remains significant room for further improvements on grounding LLMs to facilitate general surrogate model."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was independently conducted by the authors and is self-funded by BHL without institutional affiliation."
        },
        {
            "title": "References",
            "content": "Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. transformer-based approach for source code summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49985007, Online. Association for Computational Linguistics. Anthropic. 2024a. Introducing Claude 3.5 Sonnet. 2024b. Family: Anthropic. Model https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. 3 Haiku. Sonnet, Claude Opus, The Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. 1997. The Coq proof assistant reference manual: Version 6.1. Ph.D. thesis, Inria. Peter Benner, Serkan Gugercin, and Karen Willcox. 2015. survey of projection-based model reduction methods for parametric dynamical systems. SIAM Review, 57(4):483531. Miguel Bessa, Ramin Bostanabad, Zeliang Liu, Anqi Hu, Daniel Apley, Catherine Brinson, Wei Chen, and Wing Kam Liu. 2017. framework for datadriven analysis of materials under uncertainty: Countering the curse of dimensionality. Computer Methods in Applied Mechanics and Engineering, 320:633 667. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. Randal E. Bryant and David R. OHallaron. 2010. Computer Systems: Programmers Perspective, 2nd edition. Addison-Wesley Publishing Company, USA. Cristian Cadar and Koushik Sen. 2013. Symbolic execution for software testing: three decades later. Commun. ACM, 56(2):8290. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. 2022. scalable and extensible approach to benchmarking nl2code arXiv preprint for 18 programming languages. arXiv:2208.08227. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, Zekun Wang, Boyang Wang, Xianjie Wu, Bing Wang, Tongliang Li, Liqun Yang, Sufeng Duan, and Zhoujun Li. 2024. Mceval: Massively multilingual code evaluation. Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray. 2020. Codit: Code editing with tree-based neural models. IEEE Transactions on Software Engineering, pages 11. Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2020. Deep learning based vulnerability detection: Are we there yet? arXiv preprint arXiv:2009.07235. Shubham Chandel, Colin Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901. Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. https: //github.com/sahil280114/codealpaca. Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. 2024. Gamegen-x: Interactive openworld game video generation. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Transactions on Software Engineering. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. 2015. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pages 378388. Springer. Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. 2024. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language In International Conference on Machine models. Learning, pages 1076410799. PMLR. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2024. Large language models are zeroshot time series forecasters. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. 2024. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065. Ruizhen Gu, José Miguel Rojas, and Donghwan Shin. 2025. Software testing for extended reality applications: systematic mapping study. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196. Hao Hao, Xiaoqun Zhang, and Aimin Zhou. 2024. Large language models as surrogate models in evolutionary algorithms: preliminary study. Jacob Harer, Chris Reale, and Peter Chin. 2019. Treetransformer: transformer-based method for corarXiv preprint rection of tree-structured data. arXiv:1908.00449. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Jan Hesthaven and Stefano Ubbiali. 2018. Nonintrusive reduced order modeling of nonlinear problems using neural networks. Journal of Computational Physics, 363. Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension, page 200210, New York, NY, USA. Association for Computing Machinery. Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint arXiv:2211.09374. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2020. Codesearchnet challenge: Evaluating the state of semantic code search. Paul Jaccard. 1901. Étude comparative de la distribution florale dans une portion des alpes et des jura. Bulletin del la Société Vaudoise des Sciences Naturelles, 37:547579. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Mohammad Abdullah Matin Khan, Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. arXiv preprint arXiv:2303.03004. James C. King. 1976. Symbolic execution and program testing. Commun. ACM, 19(7):385394. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 1831918345. PMLR. Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599. Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018a. Code completion with neural attention and In Proceedings of the Twentypointer networks. Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 41594165. International Joint Conferences on Artificial Intelligence Organization. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. 2025. Codei/o: Condensing reasoning patterns via code input-output prediction. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018b. Vuldeepecker: deep learning-based sysarXiv preprint tem for vulnerability detection. arXiv:1801.01681. Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, and Chi Jin. 2025. Goedel-prover: frontier model for open-source automated theorem proving. Stanley B. Lippman, Jose Lajoie, and Barbara E. Moo. 2012. C++ Primer, 5th edition. Addison-Wesley Professional. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36. Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. Repobench: Benchmarking repository-level arXiv preprint code auto-completion systems. arXiv:2306.03091. Dan Lu and Daniel Ricciuto. 2019. Efficient surrogate modeling methods for large-scale earth system models based on machine learning techniques. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. 2021a. CodeXGLUE: machine learning benchmark dataset for code understanding and generation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021b. Codexglue: machine learning benchmark dataset arXiv for code understanding and generation. preprint arXiv:2102.04664. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations. Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. Coconut: combining context-aware neural translation models using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 101114, New York, NY, USA. Association for Computing Machinery. Bohan Lyu, Yadi Cao, Duncan Watson-Parris, Leon Bergen, Taylor Berg-Kirkpatrick, and Rose Yu. 2025. Adapting while learning: Grounding llms for scientific problems with intelligent tool usage adaptation. Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, and Longyue Wang. 2024. Large language models as code executors: An exploratory study. Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B. Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik. 2024. Llm and simulation as bilevel optimizers: new paradigm to advance physical scientific discovery. Dirk Merkel. 2014. Docker: lightweight linux containers for consistent development and deployment. Linux J., 2014(239). Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai. meta.com/blog/meta-llama-3/. Leonardo de Moura and Sebastian Ullrich. 2021. The Lean 4 theorem prover and programming language. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124. Giuseppe Nebbione and Maria Carla Calzarossa. 2023. methodological framework for ai-assisted security assessments of active directory environments. Ieee Access, 11:1511915130. OpenAI. 2024a. Gpt-4o mini: Advancing cost-efficient intelligence. OpenAI Blog. Accessed: 2025-02-16. OpenAI. 2024b. Hello gpt-4o. OpenAI Blog. Accessed: 2025-02-16. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Md Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2018. Building language models for text with named entities. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23732383, Melbourne, Australia. Association for Computational Linguistics. Lawrence Paulson. 1994. Isabelle: generic theorem prover. Rui Queiroz, Tiago Cruz, Jérôme Mendes, Pedro Sousa, and Paulo Simões. 2023. Container-based virtualization for real-time industrial systemsa systematic review. ACM Comput. Surv., 56(3). Maziar Raissi, Paris Perdikaris, and George Karniadakis. 2019. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686707. Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. 2020. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):10261030. John W. Ratcliff and David E. Metzener. 1988. Pattern matching: The gestalt approach. Dr. Dobbs Journal, 13(7):4651. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, and Marc McConley. 2018. Automated vulnerability detection in source code using deep represenIn 2018 17th IEEE International tation learning. Conference on Machine Learning and Applications (ICMLA), pages 757762. IEEE. Farid Shirazi, Adnan Seddighi, and Amna Iqbal. 2017. Cloud computing security and privacy: An empirical study. pages 534549. Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998. Mohammed Latif Siddiq, Joanna Cecilia Da Silva Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and Vinícius Carvalho Lopes. 2024. Using large language models to generate junit tests: An empirical study. In Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, EASE 2024, page 313322. ACM. C. Spearman. 1904. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72101. Gang Sun and Shuyue Wang. 2019. review of the artificial neural network surrogate modeling in aerodynamic design. Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering, 233(16):58635872. Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. 2020. Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data. Computer Methods in Applied Mechanics and Engineering, 361:112732. Qwen Team. 2024a. Code with codeqwen1.5. https: //qwenlm.github.io/blog/codeqwen1.5. Qwen Team. 2024b. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Nils Thuerey, Konstantin Weißenow, Lukas Prantl, and Xiangyu Hu. 2020. Deep learning methods for reynolds-averaged navierstokes simulations of airfoil flows. AIAA Journal, 58(1):2536. Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, and Maosong Sun. 2024. Debugbench: Evaluating debugging capability of large language models. Shang Wang, Tianqing Zhu, Bo Liu, Ming Ding, Xu Guo, Dayong Ye, Wanlei Zhou, and Philip S. Yu. 2024. Unique security and privacy threats of large language model: comprehensive survey. Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting code clones with graph neural network and flow-augmented abstract syntax tree. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER), pages 261271. IEEE. Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshyvanyk. 2020. On learning meaningful assert statements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE 20. ACM. Logan Weber, Jesse Michel, Alex Renda, and Michael Carbin. 2024. Learning to compile programs to neural networks. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. 2024. Artificial intelligence for science in quantum, atomistic, and continuum systems. Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin, Michael I. Jordan, and Jianmin Wang. 2023b. Skilful nowcasting of extreme precipitation with nowcastnet. Nat., 619(7970):526 532. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems, volume 32, pages 1019710207. Curran Associates, Inc. Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. 2022. Integrating scientific knowledge with machine learning for engineering and environmental systems. ACM Computing Surveys, 55(4):137. Christopher Wimmer and Navid Rekabsaz. 2023. Leveraging vision-language models for granular market change prediction. Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. 2023. Interpretable weather forecasting for worldwide stations with unified deep model. Nat. Mac. Intell., 5(6):602611. Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020. Neural execution engines: Learning to execute subroutines. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. 2024. Lean workbook: large-scale lean problem set formalized from natural language math problems. arXiv preprint arXiv:2406.03847. Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and Qianxiang Wang. 2019. Neural detection of semantic code clones via tree-based convolution. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC), pages 7080. IEEE Press. Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, et al. 2024. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint arXiv:2403.16443. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023a. Repocoder: Repository-level code completion through iterative retrieval and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 24712484. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. novel neural source code representation based on abstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering, ICSE 19, page 783794. IEEE Press. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers,"
        },
        {
            "title": "A Prompts",
            "content": "A.1 Prompts for Dataset Refactoring ML: will provide you with code problem with solution. You need to generate complete, executable code based on the raw json data, including all necessary package imports, the original code, the test cases, and the main function. You need to generate the executable code and expected result. Please choose test case according to the test field from raw json data, and the code should print the answer of the test case. The output should be json format, with code and expected_result fields. Please only generate the number or string answer in expected_result field without any extra description. CL: will provide you with the solution to code problem in cpp, python, and javascript. You need to score according to the difficulty of the problem from 1 to 5, while 5 means the hardest. And generate topic keywords for the problem. The output should only be json format, with difficulty and keywords fields. difficulty: 1-5, integer keywords: two or three words to best describe the problem, string list BG: will provide you with piece of code and some test cases. You need to generate complete, executable code based on these, including all necessary package imports, the original code, the test cases, and the main function. You should wrap the original code with ORIGINAL_CODE_START and ORIGINAL_CODE_END comments. Additionally, the program should output the results of the test cases. Do not include expected output in your answer. A.2 Prompts for Evaluation BG: will provide you with piece of code and some test cases. You need to generate complete, executable code based on these, including all necessary package imports, the original code, the test cases, and the main function. You should wrap the original code with ORIGINAL_CODE_START and ORIGINAL_CODE_END comments. Additionally, the program should output the results of the test cases. Do not include expected output in your answer."
        },
        {
            "title": "B Details of SURGE",
            "content": "sys_0shot: You are an expert in linear_algebra programming. Please execute the given code with the provided input and return the output. Make sure to return only the output in the exact format as expected. Output Format: Output: <result> sys_3shot: You are an expert in linear_algebra programming. Please execute the above code with the input provided and return the output. You should think step by step. Your answer should be in the following format: Thought: <your thought> Output: <execution result> Please follow this format strictly and ensure the Output section contains only the required result without any additional text. Here are some examples: Example 1: Input: python lu_u.py --A 15 23 ;48 41 Output: [[ 15. 23. ] [ 0. -32.6]] Example 2: Input: python lu_u.py --A 17 75 ;7 62 Output: [[17. 75. ] [ 0. 31.11764706]] Example 3: Input: python lu_u.py --A 15 51 ;18 63 Output: [[15. 51. ] [ 0. 1.8]] Please solve new problems following these examples exactly and ensure the Output section contains only the required result without any additional text. sys_cot: You are an expert in linear_algebra programming. Please execute the above code with the input provided and return the output. You should think step by step. Your answer should be in the following format: Thought: <your thought> Output: <execution result> Please follow this format strictly and ensure the Output section contains only the required result without any additional text. Table 2: Language usage count across different categories. Java C# Rust Julia Python CPP JavaScript Lean 25 20 20 26 51 49 18 50 24 150 150 45 21 51 36"
        },
        {
            "title": "ML\nCL\nRL\nSC\nTC\nBG\nDR\nFL",
            "content": "150 Table 3: Details of problems in different languages and different difficulty levels."
        },
        {
            "title": "JavaScript CPP Python",
            "content": "1 2 3 4 5 10 6 12 8 13 11 4 14 8 14 11 6 12 9 12 Table 4: Details of bug types in BG dataset and how many times each kind of bug appears in different languages. Error Type Java Python3 CPP == and = confusion undefined keywords parentheses mismatch indexing error undefined objects unclosed string conditional statement error undefined methods colon missing wrong comment mark variable value error operation error other error statement separation indentation error Double Bugs Triple Bugs Quadruple Bugs 5 6 5 10 11 7 10 8 5 9 2 2 4 4 0 10 12 8 6 3 5 9 9 5 8 3 7 1 2 2 2 0 4 8 10 5 5 5 6 11 8 7 9 6 8 9 4 3 1 7 0 10 11 9 B.1 Consists of SURGE B.2 Tasks Descriptions of Scientific Computing (SC) The scientific computing component of SURGE consists of 4 carefully curated areas, aiming to evaluate model performance on computational tasks that exhibit time-consuming nature as well as applicational values in scientific computing areas. In this section, we provide detailed description of each component. Numerical Optimization. In this task, the model is given program that solves an optimization problem through gradient descent. The query may be the optimized value (min) or the optimal point (argmin). We carefully select four functions, which consist of: simple quadratic function, Rosenbrock Function, Himmelblaus Function, and polynomial function with linear constraints. For each function, we will select multiple different hyperparameter configurations to assess the models performance. These four functions provide systematic evaluation of the models potential to serve as surrogate model in this field. As the quadratic function is solvable without need the to run the gradient descent, the model may solve it through world knowledge. The Rosenbrock function is known for its narrow, curved valley containing the global minimum, making it difficult for optimization algorithms to converge. Therefore the output is highly dependent on hyperparameters (initial point, learning rate, maximum steps), thus the model must execute code in its reasoning process to acquire the answer. Himmelblaus function has multiple local minima, also posing sensitivity to hyperparameters. PDE Solving. We consider three types of Partial Differential Equations: the 1D Heat Equation, the 2D Wave Equation, and the 2D Laplace Equation. For the 1D Heat Equation, we focus on solving the following equation: t = α 2u x2 . (3) For the 2D Laplace Equation, we aim to solve the equation: 2u x2 + 2u y2 = 0. (4) Lastly, for the 2D Wave Equation, we work on solving the following equation: 2u t2 = c2 (cid:18) 2u x2 + (cid:19) . 2u y2 (5) We solve 1D Heat Equation and 2D Wave Equation using the Explicit Finite Difference Method. For the 2D Laplace Equation, we solve it using the Gauss-Seidel Method. The model is then queried on the values of and x. Fourier Transform (FFT) We implement FFT using the Cooley-Tukey Algorithm and query the model to give the magnitude of the top 10 values. ODE Solving For solving ordinary differential equations, we constructed three different equations and implemented the Euler Method and the RungeKutta Method so solve these equations. B.3 Tasks Descriptions of Time Consuming (TC) The time consuming component of SURGE is comprised of 4 tasks in for computationally expensive areas, covering spectrum of Linear Algebra, Sorting, Searching, Monte Carlo Simulations and String Matching Programs. Some of these tasks take hours to complete, showing their potential to benchmark LLMs ability to reason through lengthy computations. Linear Algebra. In this task, we are focused on acquiring key properties in linear algebra given square matrices of varying sizes. In particular, we query the model on solving LU decomposition, QR decomposition, the largest eigenvalue and eigenvector using the power method, and the inversion matrix. Sorting And Searching. We include four classical algorithmic problems in this area, namely Hamiltonian Cycle, Traveling Salesman Problem (TSP), Sorting an array of real numbers and Searching. For Hamiltonian Cycle, we adopt the backtracking algorithm. Specifically, we randomly generate graphs with vertices from 4 to 100 and ask the model to find whether Hamiltonian cycle exists. For TSP, we implement naive brute-force algorithm and ask the model to find the length of the optimal path. For Sorting, we adopt the bubble sort, quick sort, and merge sort algorithms. For each algorithm, we consider different list sizes from 5 to 100 and generate 10 test cases for each list size. The evaluation metric is the rank correlation (also Spearmans ρ ). Lastly, for searching, we adopt binary search and query the model on randomly generated lists of varying sizes. Monte Carlo Estimation. We adopt Monte Carlo simulation to estimate the values of specific real numbers (e.g. π, e), as well as future stock price prediction that follows the Brownian motion. We alter the number of samples used in Monte Carlo estimation, resulting in varying program outcomes. String Matching Program. We adopt the naive string matching, KMP, and Rabin-Karp algorithms. For each algorithm, we randomly generate text and pattern with varying lengths, and query the model on the existence and position of the matching. B.4 Evaluation Metrics Relative Absolute Error (RAE). Given scalar ground truth value and model prediction ˆp, the Relative Absolute Error (RAE) is defined as: RAE(ˆp, p) = ˆp . (6) For cases involving multiple entries, such as tensors or vectors, the following alignment procedure is applied: (1) if the prediction contains fewer elements than the ground truth, the prediction is padded with zeros until it matches the length of the ground truth; (2) if the prediction has more elements than the ground truth, it is truncated to match the ground truth length. The average RAE is then computed by averaging the RAE for each corresponding element. Exact Matching. For tasks involving positionbased predictions, such as binary search, we adapt exact matching, as the accuracy of the algorithm is determined by comparing the exactness of the estimated result to the true result. This evaluation method checks if the estimated solution matches the ground truth exactly, typically using string or sequence matching. For such tasks, an exact match is considered success, and any discrepancy between the ground truth and the estimate results in failure. Formally, given string and the models prediction ˆs, the Exact Matching is given by: EM(s, ˆs) = 1[s = ˆs] (7) where 1[] is the indicator function. Rank Correlation. Rank Correlation (Spearman, 1904), also referred to as Spearmans ρ, is used to assess sorting tasks by measuring the correlation between the estimated ordinal ranking and the ground truth, which can be written as: RankCorr = Cov(x1:N , y1:N ) σ(x1:N )σ(y1:N ) (8) where x1:N and y1:N denote the true and estimated rankings, respectively, and Cov and σ represent the covariance and standard deviation of the respective sequences."
        },
        {
            "title": "C Training Details",
            "content": "For training, we employ Llama-Factory (Zheng et al., 2024) as the LLM training platform. Table 5 shows our training hyperparameters. Table 5: Hyperparameters for supervised fine-tuning."
        },
        {
            "title": "Value",
            "content": "128 1.0e-5 2.0 cosine 0.1 bf"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua",
        "Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua"
    ]
}