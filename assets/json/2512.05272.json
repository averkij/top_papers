{
    "paper_title": "Inferring Compositional 4D Scenes without Ever Seeing One",
    "authors": [
        "Ahmet Berke Gokmen",
        "Ajad Chhatkuli",
        "Luc Van Gool",
        "Danda Pani Paudel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven."
        },
        {
            "title": "Start",
            "content": "Inferring Compositional 4D Scenes without Ever Seeing One Ahmet Berke Gokmen Ajad Chhatkuli"
        },
        {
            "title": "Luc Van Gool Danda Pani Paudel",
            "content": "INSAIT, Sofia University St. Kliment Ohridski, Bulgaria {berke.gokmen, ajad.chhatkuli, luc.vangool, danda.paudel}@insait.ai 5 2 0 2 4 ] . [ 1 2 7 2 5 0 . 2 1 5 2 : r Figure 1. Given single video (bottom), our method reconstructs the entire 3D scene along with the individual dynamic objects (A), while maintaining spatial and temporal consistency through spatio-temporal attention mixing (C). The silhouettes (purple for human and orange for dog) correspond to the beginning of dynamic sequences. Our user study (for spatial correctness and temporal coherence) shows that the reconstructions obtained using the proposed attention mixing mechanism are clearly preferred over the baseline without mixing (B)."
        },
        {
            "title": "Abstract",
            "content": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven. Code will be released at https://github.com/insait-institute/COM4D. 1. Introduction Real scenes consist of multiple static and dynamic objects whose structures, compositional relationships, and spatio1 temporal configurations evolve continuously over time. Capturing these factors jointly, without restrictive assumptions, requires solving reconstruction, decomposition, and temporal reasoning simultaneously. Existing approaches often sidestep this by focusing on single object at time or relying on category-specific parametric models for dynamic entities, which may result in inconsistent scene geometry, limited generalization, and strong inefficiencies whenever real-world objects or motions deviate from modeled priors. Consequently, despite its importance, object-decomposed 4D scene modeling in-the-wild remains highly challenging. Single-object 3D from an image [43, 49, 93, 96, 98] has advanced rapidly, thanks to the large scale object [15], and powerful generative pipelines [16, 25, 48, 50] combining self-supervised visual priors [17, 59, 79], Variational Autoencoder (VAE) shape spaces [43, 85, 89, 92], and diffusion transformers [64]. Object-agnostic pointbased 3D/4D scene reconstruction has progressed through self-supervised image priors, predicting the joint structure [34, 40, 67, 81, 82] or structure with parametric 4D fitting [12]. These successes hinge on simplifying assumptions: objects are static, or of known parametric class, data scale is massive, and structure is globally consistent. Similarly restrictive, most approaches to dynamic 4D reconstruction are limited to single deforming object [7, 10, 65, 68, 94], multiple objects [12, 30] defined by categoryspecific parametric models [53, 63], or are object-unaware [11, 81, 83, 87, 95]. Typically, motion in training data is captured in controlled environments using active sensors, dedicated motion-tracking rigs, or via physical deformation models or carefully designed synthetic assets. Yet, realworld scenes violate all of these conveniences: multiple static and dynamic objects coexist, interact, occlude one another, and exhibit heterogeneous geometric and motion patterns that defy any unified prior. Consequently, as soon as objects move behind others, exhibit complex interactions, or undergo significant viewpoint changes, 4D structure can no longer be captured in many approaches, resulting in fragile representations. In other words, most 4D scene recovery often struggle with both consistency and persistence of objects. For the same reasons, large-scale, in-the-wild 4D multi-object data [32] is extremely scarce, making learning severely under-constrained. As result, progress on multiobject 4D scene reconstruction has lagged far behind that of simpler settings. To the best of our knowledge, no existing model can infer complete and persistent 4D representation of multiple static and dynamic objects in-the-wild using only monocular videos, without test-time optimization. To break through this barrier, we pursue fundamentally different approach. We show that the required spatiotemporal reasoning can be learned in the form of attentions. We do so separately from two sources that are easy to obtain: static multi-object observations for spatial structure, and single-object animations for temporal dynamics. Thus, we introduce COM4D, compositional 4D reconstruction method that unifies these independently learned attentions at inference time, guided by simple but powerful physical assumption: at every time instant, all scene elements are momentarily static, and their dynamics unfold by propagating object states forward in time. By iteratively alternating spatial and temporal reasoning process we call attention mixing (as illustrated in Figure 1.C), COM4D implicitly recovers the complete and persistent 4D structure of multiple interacting objects without ever seeing single example of such data during training. Realizing this intuition required series of careful design choices in representation, architecture, supervision, and inference, which collectively allow us to address problem long considered exceptionally hard. We summarize our main contributions as follows: We introduce attention parsing, simple yet effective strategy that disentangles the learning of spatial and temporal reasoning from separate, complementary data sources, without compromising the quality of either. We propose attention mixing, which unifies these independently learned attentions at inference time on an input of video of any length, to achieve compositional 4D scene reconstruction. Thus, our model recovers multiobject, spatio-temporal structure despite never being explicitly trained on such data. We show that this unified attention framework generalizes across diverse in-the-wild scenes, achieving coherent and persistent 4D reconstructions of multiple interacting objects, significantly outperforming specialized dynamic single-object or static-scene baselines. 2. Related Works In earlier works, 4D Object Reconstruction. termed as non-rigid structure-from-motion, 4D reconstruction approaches primarily relied on low-rank assumptions [6, 14, 57, 60, 75] and physics-based priors [1, 61, 69]. Due to more practical results, subsequent works favored categoryspecific approaches [2123, 38, 62, 99] using parametric human shape models [53, 63]. In order to handle any shape, methods [2, 31] later opted the score-distillation [26, 66, 84]. Considering both speed and generalizability to any shape, generative or diffusion-based approaches [10, 46, 68, 94] have become popular, as they have the potential to capture any shape persistently (including the occluded regions), while avoiding expensive test time adaptations. Noteably, [68] proposed to learn temporal self-attention in localglobal manner similar to [86] instead of borrowing the pretrained priors from video diffusion models [4, 20]. Composed 4D Scene Reconstruction. The steady advancement of 3D scene reconstruction [40, 56, 70, 73, 76, 79, 82] have enabled learning-based approaches to unify 2 static-dynamic scene 4D reconstruction [12, 18, 28, 81, 95, 97]. This has been largely possible through the novel unified appearance-3D representations [35, 55]. Still, most previous works target human motion [12, 23, 51, 71, 72, 74] or do not tackle the persistent object decomposed reconstruction [11, 37, 39, 80, 81, 83, 91]. Alternately, many current approaches [39, 44, 45, 52, 80] for 4D scenes use an online optimization paradigm limiting their efficiency. Specifically [13] reconstructs static-dynamic objects in their respective coordinates and recomposes them together with their poses and depth, through test-time optimization. Test-time optimization is also favored by other 4D complete scene reconstruction approaches [11, 39, 80]. Generative Scene Reconstruction. Despite novel representations [35, 55] unlocking new capabilities [27, 90], many real-world applications still favor persistent onemesh-per-object explicit geometry [96, 98]. Thanks to large scale data [15, 88], incredible advancements in fast single object reconstruction [43, 49, 89, 92, 93, 96, 98] have been made. However, surprisingly few works have tried integrating generative approaches for multi-object scene reconstruction, despite their potential in persistent, complete and object-aware results. Among the few, MIDI [29] proposes multi-instance attention in order to learn relative placement of objects from the object-wise masked conditional image embeddings. similar approach with multi-instance attention and object-wise mask inputs is followed by [54]. Closest to our approach, the recent PartCrafter [47] circumvents object-wise masks, and instead denoises multiple latents by alternating in-part and inter-part attention with the part embeddings for object localization. Nevertheless, to the best of our knowledge, previous test-time optimization-free approaches (diffusion or single-step) do not solve object decomposed 4D reconstruction of scenes from video. 3. Method Problem. Consider 4D scene composed of static (indexed by i) and dynamic (indexed by j) objects, recorded in fixed camera monocular video over frames (indexed by ). We represent the images by their DINOv2 [59] embeddings {fyj}, with the static objects separated in the embedding y. The goal of compositional 4D is to reconstruct the static object geometry latents = {zi} and the dynamic ones = {fzj} conditioned on the image embeddings. Note that, we use the VAE latent quantity to also refer to an object geometry, for convenience. Overview. In this work, we aim to learn compositional generative model for {S, D} without ever seeing such compositions. Consider single Diffusion Transformer (DiT), vθ with parameters θ, trained on the target distributions p({zi}y): the static geometry composition distribution conditioned on the image embedding, and p({fzfy}): the dynamic single object shape distribution over each video frame, conditioned on each frame embedding. Given the trainings, we want vθ to also naturally model the target joint static-dynamic distribution p(S, Dy, {fyj}). Here, may consist of multiple dynamic objects, unseen during the training of vθ. In the following, we describe how we tackle the problem by designing θ and its corresponding learning. The core of our method is single DiT architecture that first learns to perform two distinct but complementary tasks: object aware reconstruction of 3D scenes and modeling the temporal dynamics of deformable object. We achieve this through dual-objective training strategy called Attention Parsing. At inference, novel Attention Mixing mechanism allows us to combine these learned capabilities to conditionally generate complex 4D scenes with both static and dynamic components, task for which the model was never explicitly trained. Finally, we enhance the temporal coherence of our generations by fine-tuning with Diffusion Forcing, cf. next section. Fig. 2 illustrates how Attention Parsing and Attention Mixing works. 3.1. Preliminaries Diffusion Transformer for 3D. Our work builds upon TripoSG [43], state-of-the-art image-to-mesh generative model. It consists of shape Variational Autoencoder (VAE). The encoder of the VAE converts an object mesh into latent feature which can then be decoded into the Signed Distance Field (SDF) through its decoder similar to [92]. DiT [43, 64] conditioned on the DinoV2 [59] image embedding is then trained to denoise noisy latent to the object shapes VAE latent on Objaverse [15] and ShapeNet [8]. Diffusion Forcing. Different from the standard diffusionbased learning [25, 48], Diffusion Forcing [9] applies independent noise to the different latent vectors in the same data and denoises them together. The ability to process such mixed-noise inputs, where some latents are clean and others are noisy, is what makes this training scheme critical for our methods requirements. First, it directly addresses the need for stable static guidance by training the model to handle conditioning on fully denoised static objects (t = 0) while generating the remaining noisy dynamic latents. Second, it is essential for history-guided generation, as it enables previously denoised frame 1 to serve as clean (t = 0) context for generating the subsequent frame . This ensures strong temporal consistency and coherent evolution of the dynamic objects. 3.2. Attention Parsing: Dual-Objective Training Our key insight is to train single DiT model such that it understands both spatial composition and dynamics by alternately training the model on two distinct datasets, with 3 Figure 2. Our attention parsing and mixing strategy. single DiT model with shared weights is trained jointly on two datasets. (Top) During training with samples from DeformingThings [42], odd-indexed blocks perform multi-frame attention to capture temporal dynamics. (Bottom) When training with samples from 3D-FRONT [19], even-indexed blocks perform multi-instance attention to model spatial part decomposition. At inference, the same model applies an attention mixing mechanism. In each layer, spatial blocks (even-indexed) aggregate all latents from single frame and process them jointly, conditioned on the full-scene image at that timestep. Temporal blocks (oddindexed) then operate over all frames of each dynamic object separately, conditioned on their corresponding masked images. Masks are extracted from the video for each dynamic object using SAM [36], enabling temporally consistent object-specific reasoning. the transformer blocks performing different roles in each. Dual-Dataset Strategy. At each training step, we sample with equal probability from either the 3D-FRONT [19] dataset, which provides static scenes with object-level decompositions, or the DeformingThings [42] dataset, which contains dynamic sequences of single deforming object. Alternating Block Roles. The DiT backbone consists of 21 transformer blocks. We assign complementary roles to these blocks depending on the data source. When training on 3D-FRONT [19] sample, the even-indexed blocks are configured to perform multi-instance attention, enabling them to reason about the spatial relationships between different object parts in scene. Conversely, when training on DeformingThings [42] sample, the odd-indexed blocks are tasked with multi-frame attention, allowing them to capture the temporal dependencies across different frames of sequence. The blocks not assigned to multi-instance attention (even blocks in DeformingThings [42]) or multi-frame attention (odd blocks in 3D-Front [19]) default to local selfattention, see Fig. 2.A. Compositional Latent Space. Following our problem formulation, scene is represented as collection of + latents (objects and/or frames). Each latent/token is tensor RKC. To distinguish between the different objects/frames, we add unique, learnable embedding in each token: an object embedding ei for 3D-FRONT samples and frame embedding fe for DeformingThings samples. Similarly, single frame embedding is added to the 3D-FRONT object latents and single object embedding is added to the DeformingThings sample. Diffusion Training Objective. Our architectures ability to reason about multiple components is enabled by the dual attention strategy. Local self-attention is applied independently to each latents tokens z, while global reasoning is handled by transforming specific self-attention layers into multi-instance attention layers. The multi-instance attention allows the latent of each object zi to attend to the rest {zl}N l=1 while updating itself. Similarly, the multi-frame attention each latent fz using the rest l=1{lz}. We write the multi-instance attention for multi-objects as below. ziout = Attention(zi, {zl}N l=1). (1) The multi-frame attention follows similarly. To train this architecture, we adapt the rectified flow objective for our dual-task. For the sake of brevity and correctness, we describe how the process works for static multiple objects in scene. The rectified flow process for the multiframe object latents follows similarly. Crucially, to enable Diffusion Forcing [9], we sample an independent time step ti [0, 1] for each latent among . Each clean latent zi 0 is then perturbed along its own linear trajectory by ϵi (0, I), random Gaussian noise tensor: zi ti = tizi 0 + (1 ti)ϵi, (2) The flow network vθ is trained to predict the velocity vector ϵi zi 0 for each component based on its unique noisy state. 4 The overall loss is the sum over all latents: Algorithm 1 Single Denoising Pass with Attention Mixing LS = (cid:34) (cid:88) i=1 (cid:13) (cid:13)(ϵi zi 0) vθ(zi ti , ti, y)(cid:13) 2 (cid:13) (cid:35) . (3) # (N,M): Number of (Static, Dynamic) Objects # y: # Y: Masked frame-wise conditions (MxF) Full scene condition; Z: Latent matrix ((N+M)xF) The conditioning tensor is the background subtracted static object image embedding for eq. (3), where the samples are selected from the 3D-Front dataset [19]. During temporal training (on DeformingThings, see the equation in the supplementary), the conditioning tensor is fy, the unique image embedding corresponding to the -th frame. The independent noising used in training the spatial and temporal denoising is vital, as it forces the model to handle latents at different stages of the denoising process, directly preparing it for history-guided generation at inference. The exact loss used for training is LS/T /R: static, temporal or the standard loss from TripoSG [43] for regularization. 3.3. Attention Mixing: Compositional 4D Denoising As consequence of Attention Parsing, our model can separate spatial and temporal reasoning. At the inference time, on given compositional video input, the Attention Mixing strategy enables the joint denoising of complex 4D scenes of any combination of static and dynamic entities by modulating the information flow through the transformer blocks. During single denoising step at inference, the DiT blocks alternate their function to cohesively generate 4D scene. First, the even-indexed spatial blocks reason about the global scene layout. To do this, they receive the concatenated latents of all static objects along with the latent representing the current frame of each dynamic object. This combined set forms complete snapshot of the scene at single moment. The block performs multi-instance attention across this entire set, using cross-attention keys and values derived from the single, global scene image to ensure all elements are placed correctly relative to one another. Subsequently, the odd-indexed temporal blocks model motion and deformation. These blocks process the latent sequence for each dynamic object separately, performing multi-frame attention over its history. The cross-attention keys and values for this operation are derived from the corresponding per-frame conditioning embeddings for that specific object, allowing the model to capture its unique temporal dynamics. Note that the strategy can handle any number of frames by sequentially propagating attention through sliding temporal window (red block in Fig. 2 right) over all the frames, thus maintaining temporal consistency. Static object latents pass through these temporal blocks without being processed for motion. This flexible information routing allows the model to satisfy both the spatial constraints learned from 3D-FRONT [19] and the temporal dynamics learned from DeformingThings [42] within single denoising pass. Algo. 1 describes Attention Mixing in simple code. 5 for block in transformer.blocks: if is spatial(block): for in 1..F: Z[:,f] = block(Z[:,f],y,num instances=N+M) else: for in 1..N: Z[i,:] = block(Z[i,:],y,num instances=1) for in N..M: Z[j,:]=block(Z[j,:],Y[j-N,:],num instances=F) = return 3.4. Implementation Details We build on the 21-block DiT backbone of TripoSG [43]. Even-indexed blocks operate in the spatial mode, while odd-indexed blocks operate in the temporal mode. This depth-wise alternation follows the multi-instance reasoning intuition [47] while keeping the original DiT [43] depth and width unchanged. We fully fine-tune all weights of the pretrained TripoSG DiT. We cap the supervision targets at 8 parts for the spatial path and 8 frames for the temporal. At each iteration, with probability 0.3 we train on monolithic sample (single part, single frame) to regularize the model and preserve its learned object prior (LR). Training uses two stages on single NVIDIA H200 GPU. In stage 1 (8k steps), we fine-tune the model with batch size of 50 and learning rate of 1 104. In stage 2 (12k steps), we keep the batch size at 50, enable Diffusion Forcing [9], and lower the learning rate to 1 105. The full training time is about 2 days. 4. Experiments Test Datasets. For 3D evaluation, we use the 3DFRONT [19] test set following MIDIs protocol [29]. For 4D evaluation, we construct two complementary sets: (i) an Objaverse-based subset starting from the animated object list released with PUPPET-MASTER [41], from which we select 40 high-quality assets (clean geometry and faithful textures) from Objaverse [15]; and (ii) DeformingThings4D [42] subset comprising 30 human and 30 animal animations. For every object or sequence, we render image sequences from single fixed, calibrated camera and use the resulting frames as inputs for evaluation. Evaluation Protocol. We perform evaluation on three cases: Compositional 4D, 4D object and 3D scenes. For all methods we report runtime, fidelity, and task-specific structural quality. Runtimes are computed from the average inference time in NVIDIA H200. For the static 3D scenes, we compare against MIDI3D [29] and PARTCRAFTER [47]. For fair comparison, n u p s Figure 3. Qualitative results on temporal sequences. Top rows show input frames; bottom rows show our generated reconstructions from two vertically stacked camera views. The examples shown are from content produced by ChatGPT [58] and animated with Wan [78], the CMU Panoptic dataset [32] (sequences 160401 ian3 and 160906 ian2), and the PROX dataset [24] (N3OpenArea 00158 02). Our method maintains temporal consistency and spatial realism across both real and synthetic sources. we provide MIDI-3D with ground-truth instance masks at inference, step not required by PARTCRAFTER or our method. For the 4D animation task, we include state-of-theart generative approaches L4GM [68] and GVFD [94], the mesh-based V2M4 [10], and frame-wise TRIPOSG [43] baseline to ablate the effects of temporal modeling. The token budget is fixed at 512 per part for all 3D methods and 1024 per frame for all mesh-based 4D methods. Geometric fidelity is measured using the Chamfer Distance (CD) [3, 5] and F-Score [77] (at 0.1 threshold), where lower CD and higher F-Score are better. For 3D scenes, we concatenate all parts into single mesh before evaluation, whereas for 4D sequences, we compute the metrics per-frame and then average. To assess structural quality, we use two Intersection-over-Union (IoU) metrics based on consistent 643 voxelization. In 3D, we measure part independence via pairwise IoU, where lower scores indicate better separation between parts. In 4D, we evaluate accuracy with per-frame IoU against the ground truth, where higher scores are better. Note that for Gaussian-based baselines (L4GM, GVFD), we convert their outputs to point clouds for CD and F-Score evaluation, but omit the IoU metric due to lack of reliable watertight meshes. All reported scores are the mean values over the test set. 4.1. Compositional 4D Reconstruction Compositional 4D forms our primary experimental setup and is therefore our first task. We provide experimental results using various real and synthetic sequences. Fig. 3 provides the qualitative results on the synthetic, i.e., generated video on the top row and on the CMU Panoptic sequences [32] in the bottom row. Fig. 3 shows that our method completes the reconstruction capturing the interobject interactions with surprising accuracy, thanks to the attention mixing strategy where dynamic objects can attend to its history as well as the static objects in the scene. We provide more qualitative/quantitative results for our method, with the impact of attention mixing in 4.4. 6 w/o Mixing w/ Mixing w/o Mixing w/ Mixing Figure 4. Visualizations with and without our Attention Mixing strategy. Results are for 160401 ian3 at frame 1180 (starting frame: 1100) and 170915 office1 at frame 670 (starting frame: 590). Gray points denote ground truth. Table 1. Method comparison across datasets. Lower is better for CD; higher is better for F-Score and IoU. The best score is shown bold while the second best is shown italicized. Runtimes were averaged to reflect the reconstruction of single frame. DeformingThings [42] Objaverse [15] Runtime Method CD F-Score IoU CD F-Score IoU Sec. V2M4 [10] L4GM [68] GVFD [94] TripoSG [43] Ours 0.1678 0.2633 0.2806 0.1558 0.1144 0.4759 0.3167 0.2706 0.5179 0.8388 0.1533 0.1784 0.4191 0.1595 0.2308 0.2728 0.1107 0. 0.4903 0.3236 0.2856 0.6585 0.7349 0.2135 0.2874 0.3413 61.98 1.22 2.87 2.59 4.48 Input Ours TripoSG V2M GVFD L4GM 4.3. 3D Scene Reconstruction Fig. 6 provides visual comparison of our method against previous generative approaches: PartCrafter and MIDI, on the 3D-Front [19] examples. Our method reconstructs complete and detailed object layouts, being more faithful to the input. In contrast, PartCrafter [47] occasionally produces partial or low-quality meshes, while MIDI [29] struggles with occlusions. Table 2. 3D Scene Generation on 3D-FRONT and 3D-FRONTOccluded. Lower is better for CD and IoU; higher is better for F-Score. The best score is shown bold while the second best is shown italicized. 3D-FRONT [19] 3D-FRONT-Occluded [19] Runtime Method CD F-Score IoU CD F-Score Seconds MIDI [29] PartCrafter [47] Ours 0.1445 0.1751 0.0909 0.7829 0.7569 0. 0.0027 0.0017 0.0018 0.1781 0.1951 0.1256 0.7387 0.7461 0.7521 3.51 2.60 2.63 The quantitative evaluation makes these even clearer. Tab. 2 compares our method with recent baselines on the 3D-FRONT dataset [19]. Our approach demonstrates stateof-the-art performance, achieving the best Chamfer Distance (0.0909) and F-Score (0.8069). This represents significant improvement in geometric accuracy, surpassing the second-best method, MIDI [29]. This advantage holds even on the challenging 3D-FRONT-Occluded split."
        },
        {
            "title": "Ours",
            "content": "PartCrafter [47] MIDI [29] Figure 6. Qualitative comparison across methods. Our approach generates more consistent and detailed structures compared to PartCrafter [47] and MIDI [29]. 7 Figure 5. Qualitative 4D generation comparisons for two subjects (top two rows: Ninja, bottom two rows: Amy) at two time steps. The first column shows the input frames, and subsequent columns show fixed pose rendered view from each method. V2M4 fails in few samples, e.g., the last row input. 4.2. Single Object 4D Unlike the compositional case, several approaches exist for 4D object-aware generative reconstruction. We show qualitative comparisons in Fig. 5 in novel view. We observe that our method captures the shape details accurately. TripoSG [43] provides strong static reconstruction, while it along with V2M4 [10], struggle on temporal consistency for certain frames. Similarly, GVFD [94] and L4GM [68] produce textured Gaussians that look plausible from the input view, but their underlying 3D geometry shows inconsistency when rendered from novel viewpoints. The quantitative evaluations in Tab. 1 reflect the same observations. On DeformingThings [42], our approach significantly outperforms all baselines across every metric; for instance, our IoU of 0.4191 shows substantial gain over the next best method. On Objaverse [15], our approach again achieves the best F-Score and IoU, indicating better shape completeness and overlap, while remaining highly competitive in Chamfer Distance against TripoSG [43]. Overall, these results validate the geometric accuracy and completeness of our methods reconstructions on both moderate and large dynamics. 4.4. Ablation Study"
        },
        {
            "title": "Input",
            "content": "w/o Mixing w/ Mixing We conduct series of ablation studies to validate the effectiveness of our key architectural components: the use of distinct static/dynamic embeddings, the Diffusion Forcing training scheme, and our attention mixing strategy. Quantitative Analysis. The quantitative results in Tab. 3 validate our design choices against baseline model. Adding the Static/Dynamic Embeddings yields the most significant performance gain; on DeformingThings, this drops the Chamfer Distance (CD) from 0.1525 to 0.1284 and nearly doubles IoU from 0.2018 to 0.4034, highlighting the importance of disentangling static and dynamic representations. Diffusion Forcing also provides notable improvement, particularly for IoU, which enhances temporal consistency. Our full model COM4D, combining both components, achieves the best overall performance. Analysis of Attention Mixing. Fig. 7 qualitatively validates our Attention Mixing strategy. Without it, the model fails to ground dynamic objects in their static context, resulting in artifacts where the cat overlaps with the lamp stand and humans are not sitting where they are supposed to. In contrast, by alternating between spatial and temporal attention in the Attention Mixing strategy, our model makes dynamic objects aware of static placements. This leads to reconstructions that are significantly more spatially accurate, validating our strategy for modeling plausible temporal evolutions in multi-object scenes. We report quantitative results in two forms. Because no compositional 4D dataset with object-level decomposition exists, our main evaluation relies on user studies in Fig. 1.B, where our method is preferred over standard generative baseline (without Attention Mixing) by factor of 12 (87% vs. 6.9%). We also evaluate on CMU Panoptic [32, 33] point clouds by registering only the first frame reconstruction, where Attention Mixing reduces the average CD from 35.91 cm to 7.42 cm. This demonstrates that COM4D can accurately capture multiple dynamic shape evolutions in long sequences > 90 frames, with surprisingly small drifts. Fig. 4 visualizes the reconstructions of ours and the baseline. Further details appear in the supplementary material. Table 3. Ablation study on 3D-FRONT and DeformingThings datasets. Baseline refers to the model without static/dynamic embeddings or diffusion forcing, while Ours uses both. Lower is better for CD and IoU in 3D-FRONT; higher is better for FScore and IoU in DeformingThings. 3D-FRONT [19] DeformingThings [42] Method CD IoU CD IoU Baseline + Static/Dynamic Emb. + Diffusion Forcing Ours 0.1668 0.1044 0.1247 0.0909 0.6347 0.7888 0.7152 0.8069 0.0025 0.0017 0.0021 0. 0.1525 0.1284 0.1488 0.1144 0.7854 0.9350 0.8189 0.8388 0.2018 0.4034 0.4271 0.4191 Figure 7. Qualitative comparisons on Attention Mixing. Mixing (right) provides far better static-dynamic compositions compared to without Mixing (middle). 5. Conclusion In this work, we introduced COM4D, novel approach for reconstructing compositional 4D scenes from monocular videos, without requiring direct supervision on such scarce data. Our work is motivated by the inherent difficulty of acquiring real world compositional 4D for training, which in fact, may not be resolved in the near future. We instead, address the problem by proposing unique training strategy called Attention Parsing, which factorizes the learning of spatial and temporal priors into disentangled attention pathways. At inference, our Attention Mixing mechanism effectively fuses these learned attentions to produce coherent, persistent, and compositionally consistent 4D reconstructions, even in scenes containing multiple interacting static and dynamic objects. COM4D achieves state-of-the-art performance on both 3D compositional scene reconstruction and 4D dynamic object reconstruction, underscoring the versatility and effectiveness of the approach. Despite these promising results, certain limitations remain. The models understanding of motion is learned from data and does not incorporate an explicit physical causality. Consequently, when objects become occluded, it can struggle to reason about their continued trajectory and interaction in physically plausible manner. Furthermore, COM4D is designed to operate on videos captured from fixed camera perspective and does not currently support scenarios with camera motion. Future work could explore integrating physical causality to improve reasoning under occlusions and extending it to dynamic camera inputs. 8 research was Acknowledgements. This partially funded by the Ministry of Education and Science of Bulgaria (support the Bulgarian National Roadmap for Research Infrastructure). The research was also partly funded by INSAIT-VIVO project on 3D scene understanding. INSAIT, part of for"
        },
        {
            "title": "References",
            "content": "[1] Antonio Agudo, Lourdes Agapito, Begona Calvo, and J. M. M. Montiel. Good vibrations: modal analysis approach for sequential non-rigid structure from motion. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 15581565. IEEE Computer Society, 2014. 2 [2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B. Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 79968006, 2024. 2 [3] H. G. Barrow, J. M. Tenenbaum, R. C. Bolles, and H. C. Wolf. Parametric correspondence and chamfer matching: two new techniques for image matching. In Proceedings of the 5th International Joint Conference on Artificial Intelligence - Volume 2, page 659663, San Francisco, CA, USA, 1977. Morgan Kaufmann Publishers Inc. 6 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with laIn Proceedings of the IEEE/CVF tent diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), pages 2256322575, 2023. 2 [5] G. Borgefors. Hierarchical chamfer matching: parametric edge matching algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(6):849865, 1988. 6 [6] Christoph Bregler, Aaron Hertzmann, and Henning Biermann. Recovering non-rigid 3d shape from image streams. Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662), 2:690 696 vol.2, 2000. 2 [7] Wei Cao, Chang Luo, Biao Zhang, Matthias Nießner, and Jiapeng Tang. Motion2vecsets: 4d latent vector set diffusion for non-rigid shape reconstruction and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2049620506, 2024. 2 [8] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [9] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion, 2024. 3, 4, 5 9 [10] Jianqi Chen, Biao Zhang, Xiangjun Tang, and Peter Wonka. V2m4: 4d mesh animation reconstruction from single monocular video, 2025. 2, 6, 7 [11] Weirong Chen, Ganlin Zhang, Felix Wimbauer, Rui Wang, Nikita Araslanov, Andrea Vedaldi, and Daniel Cremers. Back on track: Bundle adjustment for dynamic scene reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 49514960, 2025. 2, 3 [12] Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, and Gerard Pons-Moll. Human3r: Everyone everywhere all at once. arXiv preprint arXiv:2510.06219, 2025. 2, 3 [13] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. NeurIPS, 2024. [14] Yuchao Dai, Hongdong Li, and Mingyi He. simple priorfree method for non-rigid structure-from-motion factorization. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 20182025, 2012. 2 [15] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects, 2022. 2, 3, 5, 7, 1, 4 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [17] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806, 2024. 2 [18] Haiwen Feng*, Junyi Zhang*, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J. Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 3 [19] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang Cao Li, Zengqi Xun, Chengyue Sun, Rongfei Jia, Binqiang Zhao, and Hao Zhang. 3d-front: 3d furnished rooms with layouts and semantics, 2021. 4, 5, 7, 8 [20] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Factorizing text-toIn Comvideo generation by explicit image conditioning. puter Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXII, page 205224, 2024. 2 [21] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 2 [22] Shreyas Hampali, Sinisa Stekovic, Sayan Deb Sarkar, Chetan Kumar, Friedrich Fraundorfer, and Vincent Lepetit. Monte carlo scene search for 3d scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380413813, 2021. [23] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3d human pose ambiguities with 3d scene constraints. In ICCV, pages 22822292, 2019. 2, 3 [24] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J. Black. Resolving 3D human pose ambiguities with 3D scene constraints. In International Conference on Computer Vision, pages 22822292, 2019. 6 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [26] Shuo Huang, Shikun Sun, Zixuan Wang, Xiaoyu Qin, Yanmin Xiong, Yuan Zhang, Pengfei Wan, Di Zhang, and Jia Jia. Placiddreamer: Advancing harmony in text-to-3d generation. In Proceedings of the 32nd ACM International Conference on Multimedia, page 68806889, 2024. 2 [27] Xincheng Huang, Dieter Frehlich, Ziyi Xia, Peyman Gholami, and Robert Xiao. Gaussiannexus: Room-scale realtime ar/vr telepresence with gaussian splatting. In Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology, New York, NY, USA, 2025. Association for Computing Machinery. 3 [28] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 42204230, 2024. 3 [29] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffusion for single image to 3d scene generation, 2025. 3, 5, 7, 1 [30] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis. Coherent reconstruction of multiple humans from single image. In CVPR, 2020. 2 [31] Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360 dynamic object generation from monocular video. In The Twelfth International Conference on Learning Representations, 2024. [32] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social interaction capture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 2, 6, 8, 1 [33] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total capture: 3d deformation model for tracking faces, hands, and bodies. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 8 [34] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv preprint arXiv:2509.13414, 2025. 2 [35] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 3 [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 4 [37] Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, and Xingang Pan. Stream3r: Scalable sequential 3d reconstruction with causal transformer. 2025. 3 [38] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template modIn Proceedings of the IEEE/CVF conference on comels. puter vision and pattern recognition, pages 1987619887, 2024. [39] Jiahui Lei, Yijia Weng, Adam W. Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61656177, 2025. 3 [40] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. 2 [41] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Puppet-master: Scaling interactive video generation as motion prior for part-level dynamics, 2025. 5 [42] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nießner. 4dcomplete: Non-rigid motion estimation beyond the observable surface. IEEE International Conference on Computer Vision (ICCV), 2021. 4, 5, 7, 8, 1 [43] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, and Yan-Pei Cao. Triposg: Highfidelity 3d shape synthesis using large-scale rectified flow models, 2025. 2, 3, 5, 6, 7, 1 [44] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 1048610496, 2025. [45] Zizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, and Jiajun Wu. Wonderplay: Dynamic 3d scene generation from single image and actions. 2025. 3 [46] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. 2 [47] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. 3, 5, 7, 1 [48] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for genera10 tive modeling. In The Eleventh International Conference on Learning Representations, 2023. 2, [49] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 2, 3 [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 1 [51] Zhizheng Liu, Joe Lin, Wayne Wu, and Bolei Zhou. Joint optimization for 4d human-scene reconstruction in the wild. arXiv preprint arXiv:2501.02158, 2025. 3 [52] Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, and Di Zhang. Unleashing the potential of multi-modal foundation models and video diffusion for 4d dynamic physical scene simulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1101611025, 2025. 3 [53] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, 2015. 2 [54] Yanxu Meng, Haoning Wu, Ya Zhang, and Weidi Xie. Scenegen: Single-image 3d scene generation in one feedforward pass. 2025. [55] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [56] David Nister. An efficient solution to the five-point relative IEEE transactions on pattern analysis and pose problem. machine intelligence, 26(6):756770, 2004. 2 [57] David Novotny, Nikhila Ravi, Benjamin Graham, Natalia Neverova, and Andrea Vedaldi. C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In Proceedings of the IEEE International Conference on Computer Vision, 2019. 2 [58] OpenAI. Chatgpt (gpt-5) conversation with the author. https://chat.openai.com, 2025. Accessed November 2025. 6 [59] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification. 2, [60] Marco Paladini, Alessio Del Bue, Marko Stosic, Marija Dodig, Joao Xavier, and Lourdes Agapito. Factorization for non-rigid and articulated structure using metric projections. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 28982905. IEEE, 2009. 2 [61] Shaifali Parashar, Daniel Pizarro, and Adrien Bartoli. Isometric non-rigid shape-from-motion in linear time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [62] Pramish Paudel, Anubhav Khanal, Danda Pani Paudel, Jyoti Tandukar, and Ajad Chhatkuli. ihuman: Instant animatable digital humans from monocular videos. In European Conference on Computer Vision, pages 304323. Springer, 2024. 2 [63] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. 2 [64] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [65] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animatable implicit neural representations for creating realistic avatars from videos. TPAMI, 2024. [66] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2023. 2 [67] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: In Advances Large 4d gaussian reconstruction model. in Neural Information Processing Systems, pages 56828 56858. Curran Associates, Inc., 2024. 2 [68] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: Large 4d gaussian reconstruction model, 2024. 2, 6, 7 [69] Mathieu Salzmann and Pascal Fua. Reconstructing sharply folding surfaces: convex formulation. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 10541061, 2009. 2 [70] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 2 [71] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings, 2024. 3 [72] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J. Black. WHAM: Reconstructing world-grounded humans with accurate 3D motion. In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2024. [73] N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world International Journal of from Internet photo collections. Computer Vision, 80(2):189210, 2008. 2 11 [74] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael J. Black. TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments. In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [75] Lorenzo Torresani, Aaron Hertzmann, and Christoph BreIn gler. Learning non-rigid 3d shape from 2d motion. Advances in Neural Information Processing Systems. MIT Press, 2003. 2 [76] Bill Triggs, Philip McLauchlan, Richard Hartley, and Andrew Fitzgibbon. Bundle adjustmenta modern synthesis. In International workshop on vision algorithms, pages 298372. Springer, 1999. 2 [77] C. J. van Rijsbergen. Information Retrieval. ButterworthHeinemann, London, 2 edition, 1979. 6 [78] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 6 [79] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2 [80] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In International Conference on Computer Vision (ICCV), 2025. 3 [81] Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 2, 3 [82] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. [83] Shizun Wang, Zhenxiang Jiang, Xingyi Yang, and Xinchao Wang. C4d: 4d made from 3d through dual correspondences. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 75707580, 2025. 2, 3 [84] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2 [85] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. In CVPR, pages 1109311103, 2025. 2 [86] Ross Wightman. https : / / github . com / rwightman / pytorch - image - models, 2019. 2 Pytorch image models. [87] Ren-Rong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, and Wangmeng Zuo. Deblur4dgs: 4d gaussian splatting from blurry monocular video. ArXiv, abs/2412.06424, 2024. 2 [88] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and In IEEE/CVF Conference on Computer Vision generation. and Pattern Recognition (CVPR), 2023. 3 [89] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 2, 3 [90] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, Dahua Lin, Michael Zollhofer, and Christian Richardt. VR-NeRF: High-fidelity virtualized walkable spaces. In SIGGRAPH Asia Conference Proceedings, 2023. 3 [91] Chengbo Yuan, Geng Chen, Li Yi, and Yang Gao. Selfsupervised monocular 4d scene reconstruction for egocentric videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 88638874, 2025. 3 [92] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 2, 3 [93] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance reparXiv preprint resentation for 3d generative modeling. arXiv:2403.19655, 2024. 2, [94] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. Gaussian variation field diffusion for high-fidelity video-to-4d synthesis, 2025. 2, 6, 7 [95] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimatarXiv preprint ing geometry in the presence of motion. arxiv:2410.03825, 2024. 2, 3 [96] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3 [97] Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel Angel Bautista, Joshua M. Susskind, and Alexander G. Schwing. Pseudo-Generalized Dynamic View Synthesis from Video. In ICLR, 2024. 3 [98] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 3 [99] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael Black. 3d menagerie: Modeling the 3d shape and pose of animals. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 63656373, 2017. 2 13 Inferring Compositional 4D Scenes without Ever Seeing One"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Training Details We provide details of the full training losses described in Sec. 3.2. Specifically, eq. (3) provides the equation for LS and thereafter describes LT in text. Here, we write LT formally: LT = (cid:88) =1 (cid:13)(fϵ fz0) vθ(fztf , tf , fy)(cid:13) (cid:13) 2 (cid:13) . (4) The above expression in eq. (4) for the temporal loss is almost identical to the spatial expression in eq. (3). The obvious change is the the frame index replacing the object index i, as each data: (fz0, fy) is sampled from DeformingThings [42]. Additionally, the conditional image embeddings fy are separate for each frame among , unlike in the spatial loss expression, where all objects use the same image embedding y. Finally for completeness, we formally write the regularization loss, i.e., the TripoSG [43] loss1 as follows: LR = (ϵ z0) vθ(zt, t, y)2(cid:105) (cid:104) . (5) Note that, each sample in eq. (5) only consists of one object with no temporal evolution. Samples (z0, y) are obtained from the Objaverse training set [15]. Finally, the overall loss is LS/T /R: LS, LT , LR are sampled with ratio of 0.35 : 0.35 : 0.3 respectively. The regularization and its sampling ratio of 0.3 is also used by PartCrafter [47] and MIDI [29]. B. User Study Details To quantitatively evaluate our methods perceptual quality, we conducted user preference study. The study was administered via Google Forms and compared our full model (with Attention Mixing) against an ablation baseline (without Attention Mixing). Procedure. As shown in Fig. 8, participants were presented with 2D input sequence indicating intended object motion and two corresponding 3D animated samples (labeled (1) and (2)). For each comparison, they answered the question: Which sample better matches the input in terms of object placement, motion, and scene structure? by rating their preference on 5-point Likert scale (1: Sample 1 is better, 3: Both are about the same, 5: Sample 2 is better). 1The training loss for DiT is not mentioned explicitly in the reference. Please refer to Algorithm 1 in [50] for the rectified flow loss. To prevent bias, the assignment of our method to Sample (1) or (2) and the order of the scenes were randomized for each participant. Figure 8. Our user study interface, run on Google Forms. Participants viewed 2D input sequence (top) and two 3D results (middle, bottom), then rated their preference on 5-point scale. Data Analysis. All ratings were included in our final analysis. The preference scores reported in the main paper show the complete distribution of judgments across the 5-point Likert scale. C. Evaluation on CMU Panoptic Dataset To quantitatively assess the temporal consistency and motion accuracy of our model, we performed an evaluation on the CMU Panoptic dataset [32]. This section details our protocol for preparing the data and computing the metrics. Ground Truth Point Cloud Generation. We first generated ground truth (GT) point clouds from the raw RGB-D Kinect data provided in the dataset. To ensure clean and fair comparison, we pre-processed these GT clouds in two steps: 1. Ground Removal: The ground plane was removed using simple height threshold. 1 2. Denoising: We applied statistical outlier removal filter to eliminate stray, floating points in the cloud. Alignment and Metric Computation. key challenge in evaluating generative models is that their outputs are not inherently aligned with the GT coordinate system; they may have an arbitrary scale, rotation, and translation. To address this, we adopted first-frame alignment protocol. For each sequence, we independently registered the initial generated mesh (frame 1) from both our full model and the baseline (without Attention Mixing) to the corresponding ground truth point cloud. This one-time alignment transformation (capturing scale, rotation, and translation) was then applied uniformly to all subsequent frames generated by that method for the entire sequence. Finally, we computed the Chamfer Distance (CD) between our transformed per-frame reconstructions and the GT point clouds. This metric effectively measures how much the predicted motion deviates from the ground truth over time, given an initial registration. lower accumulated CD indicates more accurate and temporally consistent motion prediction. Visual examples of these per-frame alignments are shown in Fig. 9, Fig. 10 and Fig. 11. We can observe that not only registered 1st frame, but also the remaining frames align well with the GT pointcloud despite the motion, with surprisingly small drift. n i / i o / t I x / i o / Figure 10. Ablation study on mixing components across five time steps. The top row shows the ground truth frames, followed by two views with our mixing strategy and two views without. Gray points denote the ground truth point cloud. p n M / i o / Figure 9. Ablation study on mixing components across five time steps in the CMU Panoptic [32] sample. The top row shows the ground truth frames, followed by two views with our mixing strategy and two views without. Gray points denote the ground truth point cloud. Figure 11. Ablation study on mixing components across five time steps. The top row shows the ground truth frames, followed by two views with our mixing strategy and two views without. Gray points denote the ground truth point cloud. 2 D. Additional Qualitative Results on Compositional 4D E. Additional Qualitative Results on 4D Object"
        },
        {
            "title": "Reconstruction",
            "content": "In Fig. 13, Fig. 14 and Fig. 15 we show more qualitative results of our model and baselines on single object 4D object reconstruction. Input / GT Ours TripoSG V2M4 GVFD L4GM n g i / i o / p n M / i o / Figure 12. Ablation study on mixing components for various sequences. For each sample, we show input frames, results with our mixing strategy, and results without. In particular, the chair pose and the interaction between the dynamics (the lady shaking hands with the man) and the dynamic and static (lady and the chair) are captured incorrectly without mixing. 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n Figure 13. Qualitative 4D generation comparisons from Objaverse [15] showing three subjects at two time steps. For each model, we show the reconstructed input view (top) and rendered novel view (bottom). We display the ground truth novel view in the bottom left. Input / GT Ours TripoSG V2M4 GVFD L4GM 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n Figure 14. Further qualitative 4D generation comparisons from Objaverse [15] showing two subjects of Objaverse [15], at three time steps. For each model, we show the reconstructed input view (top) and rendered novel view (bottom). We display the ground truth novel view in the bottom left. The novel view in particular highlights the discrepancies of the methods outputs from the ground truth. Both TripoSG and V2M4 show moderate and consistent misalignment. Similar misalignment, particularly in rotation and skeletal pose is apparent in GVFD, while L4GM often fails to provide good novel views. Typically, our method shows far less shape or pose misalignment, as reflected in the quantitative metrics. Input / GT Ours TripoSG GVFD L4GM 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n 0 p 0 9 n Figure 15. Qualitative 4D generation comparisons from DeformingThings [42] on two subjects at three time steps. For each model, we show the reconstructed input view (top) and rendered novel view (bottom). We display the ground truth novel view in the bottom left. Compared to Fig. 14, the sequences contain stronger motion thus showing even larger difference in performances of our method. In particular, V2M4 fails completely due to the large motion. F. Additional Qualitative Results on 3D Scene"
        },
        {
            "title": "Ours",
            "content": "PartCrafter [47] MIDI [29] Figure 16. Qualitative comparison across ours, PartCrafter [47] and MIDI [29]. We show qualitatively how our method shows better performance, as shown by the quantitative metrics. Largely, this is due to the consistent reconstruction of all parts and their accurate composition. Both PartCrafter [47] and MIDI [29] often miss large objects, e.g., the bed."
        }
    ],
    "affiliations": [
        "INSAIT, Sofia University St. Kliment Ohridski, Bulgaria"
    ]
}