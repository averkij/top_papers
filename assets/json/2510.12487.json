{
    "paper_title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
    "authors": [
        "Evgeniy Glukhov",
        "Michele Conti",
        "Egor Bogomolov",
        "Yaroslav Golubev",
        "Alexander Bezzubov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff understanding with three supervised tasks: apply (old code $+$ diff $\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code), and diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in the benchmark are triples $\\langle \\textit{old code}, \\textit{new code}, \\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with automatic metrics and a clear evaluation protocol. We use the benchmark to do a focused empirical study of the unified diff format and run a cross-format comparison of different diff representations. Our findings reveal that different formats should be used depending on the use case and model size. For example, representing diffs in search-replace format is good for larger models in the diff generation scenario, yet not suited well for diff analysis and smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing and improving diff handling in LLMs that can aid future development of diff formats and models editing code. The dataset is published on HuggingFace Hub: https://huggingface.co/datasets/JetBrains-Research/diff-xyz."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 8 4 2 1 . 0 1 5 2 : r Diff-XYZ: Benchmark for Evaluating Diff Understanding Evgeniy Glukhov JetBrains Research Amsterdam, the Netherlands evgeniy.glukhov@jetbrains.com Michele Conti JetBrains Research Amsterdam, the Netherlands michele.conti@jetbrains.com Egor Bogomolov JetBrains Research Amsterdam, the Netherlands egor.bogomolov@jetbrains.com Yaroslav Golubev JetBrains Research Belgrade, Serbia yaroslav.golubev@jetbrains.com Alexander Bezzubov JetBrains Research Amsterdam, the Netherlands alexander.bezzubov@jetbrains.com"
        },
        {
            "title": "Abstract",
            "content": "Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We introduce Diff-XYZ, compact benchmark for codediff understanding with three supervised tasks: apply (old code + diff new code), anti-apply (new code diff old code), and diff generation (new code old code diff). Instances in the benchmark are triples old code, new code, diff drawn from real commits in CommitPackFT, paired with automatic metrics and clear evaluation protocol. We use the benchmark to do focused empirical study of the unified diff format and run cross-format comparison of different diff representations. Our findings reveal that different formats should be used depending on the use case and model size. For example, representing diffs in search-replace format is good for larger models in the diff generation scenario, yet not suited well for diff analysis and smaller models. The Diff-XYZ benchmark is reusable foundation for assessing and improving diff handling in LLMs that can aid future development of diff formats and models editing code. The dataset is published on HuggingFace Hub: https://huggingface.co/datasets/JetBrains-Research/diff-xyz."
        },
        {
            "title": "Introduction",
            "content": "Modern code-capable language models increasingly interact with repositories by both analyzing and generating code diffs. In tasks such as issue resolution [5; 17], CI build repair [6], or bug repair [13], the models task is to produce patch for an existing codebase that is then evaluated for correctness. Tasks like commit message generation treat diffs as an input that model should analyze to generate summary [11; 29]. There exists multitude of formats for representing code diffs. Classical diff formats include the (i) normal format changes without surrounding context, (ii) context format changes with surrounding 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Deep Learning for Code in the Agentic Era. lines, and (iii) unified format1 compact, context-carrying variant with @@ hunk headers and +/- prefixes (see the example in Figure 6). These three formats are standardized in GNU diffutils [21] and remain the basis for machine-applicable patches. In modern tooling, git diff produces Git patch similar to unified diff but with additional Git-specific headers. While benchmarks such as SWE-bench evaluate patches in the unified diff format, the choice of the diff representation used by an LLM can influence the quality of generations and their cost. For example, Aider reports that switching from search/replace scheme to unified diffs cuts unhelpful outputs and raises benchmark scores [1]. OpenAI trains GPT-4.1 to generate and apply diffs in recommended V4A patch format, and reports large gains on Aiders polyglot diff benchmark [2; 23]. Existing works [25; 27; 28] report using different approaches, such as working with unified diffs, performing rewrites on specified ranges, supporting search-replace formats that specify anchored replacements. Despite this centrality, current evaluations make it hard to isolate the effect of diff format from other factors like retrieval or tool use. Wang et al. [26] analyze SWE-bench and highlight that there exist distinct failure modes: patch may fail because it is syntactically malformed, because it does not apply to the intended context, or because it applies but does not fix the issue. This shows that patch outcomes depend on more than just representation, making it difficult to isolate and evaluate the impact of diff formats within such end-to-end benchmarks. To enable studies targeting diff representations, we introduce Diff-XYZ, focused lightweight benchmark for exploring how well models work with various diff formats. Our benchmark allows varying the diff representation while keeping the rest of the context fixed. Diff-XYZ consists of three synthetic tasks that represent three possible problems of finding an unknown in the equation diff = new code old code. We hypothesize that solving complex generation and analysis tasks involving code diffs requires first mastering simpler variants of the problem. These include cases where two elements of the equation new code old code = diff are provided and the third must be inferred. Rewriting it as = Z, we obtain the following list of tasks. X. Apply Task new code is unknown. This tests format obedience and character-level fidelity. The model must analyze diff in given format and realize the exact edits, including whitespace and ordering. Y. Anti-Apply Task old code is unknown. This probes invertibility and losslessness of the chosen format as processed by the model. The model must reconstruct deletions and replacements precisely and align modified regions back to their original spans. This is strict operational check of diff understanding that goes beyond surface copying. Z. Diff Generation Task diff is unknown. This measures reliable diff synthesis: correct formatting and minimal, parseable edits. The outcome is directly relevant to code agents and patch-based evaluations where the system must emit patch that tools can apply and reviewers can inspect. First, we apply Diff-XYZ to analyze how well proprietary and open-source LLMs work with the most widespread unified diff format. Proprietary models consistently outperform open-source models, with Claude 4 Sonnet and GPT-4.1 achieving the highest scores across the tasks; GPT-4.1 is more sensitive to task prompt and tends to emit V4A diff format by default. For open models, scores improve as model size grows, yet performance is still short of strong results, especially for smaller open models. Diff-XYZ can help by providing focused, reproducible checks that highlight where formats and prompts make the most difference. This analysis clarifies model behavior on the benchmark tasks and establishes strong reference results. Building on these findings, we then compare alternative edit representations: udiff-based formats work best for Apply and Anti-Apply, search-replace excels for Diff Generation, but for smaller models modified udiff variants perform best."
        },
        {
            "title": "2 Data Collection",
            "content": "We construct Diff-XYZ, compact benchmark of 1,000 real-world code edits derived from the CommitPackFT dataset [22], large-scale corpus of open-source commits with paired before/after code and commit metadata. Each instance in Diff-XYZ is triple old code, new code, diff, where 1Throughout the work, we use unified diff or udiff for all the diffs that use the unified diff syntax. 2 diff is computed between the two code versions. This dataset serves as the common basis for all tasks presented in the next section. To ensure diversity and quality, we apply systematic filtering and sampling. We retain only commits that change single file and exclude binary files, generated code, vendor directories, and trivial whitespace-only changes. Changed files must contain at least 40 lines and no more than 1,000 lines in at least one version of the code (old or new). To balance edit complexity, we stratify commits by the number of changed hunks and change size (added + removed lines), targeting 50/50 split between single-hunk and multi-hunk edits and 40/40/20 distribution of small, medium, and large edits, defined by the 40th and 80th percentiles of change size (added + removed lines) within each bucket. In practice, edits with at most 7 changed lines are considered small (40th percentile), those with 8-24 lines medium (40th-80th percentile), and those with more than 24 lines large. We also cap the number of examples per repository at 5 to encourage repository diversity. The final dataset covers five programming languages, with 200 examples each: Python, JavaScript, and Java, which are widely used and appear in most existing benchmarks, and Kotlin and Rust, which are comparatively underrepresented but actively used in distinct contexts. Most edits include both additions and deletions (81.5%), while minority are add-only (16.3%) or delete-only (2.2%). In total, Diff-XYZ spans 891 unique repositories."
        },
        {
            "title": "3 Tasks Description",
            "content": "3.1 Apply Task and Anti-Apply Task Apply is straightforward task: given the old code and the diff, generate the new code. It can be viewed as special case of code generation task with all the needed information being already in the prompt, yet the model has to correctly interpret the diff to solve it. If an LLM is capable of understanding the diff and copying, we expect it to solve the task accurately, with larger models solving it near-perfectly. In case model scores low on the Apply task, it may require additional fine-tuning to work with diffs or an adaptation of the diff format. Note that Apply is natural simplification of the recently introduced problem of smart paste or fuzzy diff application with LLMs [12], i.e., generate new code state after applying patch in some insufficient format. Anti-Apply is the problem complementary to the Apply task: given the new code and the diff, generate the old code. Similar to the Apply task, all the data required to solve it is in the prompt. In case there is significant difference between the Apply and Anti-Apply scores for model, it is sign of models overfitting on one of the tasks. We evaluate performance on these tasks with slightly modified standard metrics. To calculate the following metrics for code snippets, we remove all the lines that contain only whitespace characters. Stripped Exact Match EM is 1 when two processed code snippets are exactly the same, 0 otherwise. Stripped Intersection over Union for lines IoU ratio of unique lines in intersection to unique lines in union for two processed code snippets. 3.2 Diff Generation Task Diff Generation Task is formulated as follows: given the old code and the new code, generate the diff in the specified format. This task is different from the first two, as it does not require generating long code sequences but rather shorter strictly formatted diffs. The choice of format is crucial for the Diff Generation Task, as the model may be more or less aware of the given format. The unified diff format is the most common format used by GitHub and SWE-bench, and LLMs have higher chances of seeing it during pretraining. This may be not the case for other formats, so we always add format description in the system prompt. Unlike Apply and Anti-Apply, diff calculation is not uniquely defined. Even for the same code change, different but valid diffs can be produced, depending on factors such as the number of context lines. Because of that, we cannot apply similarity metrics like EM or IoU to the Diff Generation task. To overcome this problem, we evaluate EM and IoU for the new code and the code that is the 3 result of applying the generated diff to the old code. Because diff application is strict, all-or-nothing procedure, EM and IoU after application only capture exact correctness when the generated diff applies successfully. To measure partial correctness even when application fails, we also compute F1-scores over the sets of added and deleted lines [20]. Finally, the standard unified diff hunk header contains line numbers, which are used to resolve uncertainties. However, it can be challenging to generate hunk header before the hunk itself, and such uncertainties represent less than 1% of the dataset. For this reason, we ignore hunk headers during patch application when they are not needed. Here is the formal definition for metrics that we use to evaluate the Diff Generation task. Parsing Rate and Applying Rate the fraction of generated diffs that can be parsed, and the fraction that can be successfully applied to the old code. EM and line-level IoU after application EM and IoU values of the new code and the code after applying diff to the old code, 0 if the generated diff cannot be parsed. F1-score on Addition Lines F1+ F1-score between the set of added lines in the generated diff and those in the reference. F1-score on Deletion Lines F1 F1-score between the set of deleted lines in the generated diff and those in the reference."
        },
        {
            "title": "4 Unified Diff Evaluation",
            "content": "In this section, we provide an extensive evaluation for the unified diff format on Diff-XYZ. The results are organized in two subsections: proprietary models and open-source models. All models are evaluated with fixed user prompt for each task (see Appendix A.2). We vary the system prompt in two ways: 1. w/o format generic system prompt (\"You are helpful assistant.\") with no explicit description of the edit representation. 2. w/ format system prompt that explicitly defines the unified diff format (see Figure 2 in Appendix A.1) 4.1 Proprietary Models Proprietary LLMs are usually more powerful than open-source ones [3; 10; 16; 23], and since the tasks are conceptually simple, we expect the best models to achieve near-perfect scores. Table 1 shows the results for the Apply and Anti-Apply tasks on the Diff-XYZ dataset. Claude 4 Sonnet outperforms all models, making almost no mistakes on Apply under either system prompt, though it shows some quality drop on Anti-Apply. GPT-4.1 also performs very well across both tasks, but its results vary more across prompts. For some models (e.g., GPT-4.1), quality drops on Apply when moving from the w/o format to the w/ format system prompt, while the scores on Anti-Apply remain unaffected. Such inconsistencies may indicate overfitting: the model focuses on the diff description and outputs diff instead of code (see Figure 10 in Appendix B). Although diff application is conceptually simple task, only the strongest proprietary models solve it perfectly. Open-source models, and even some smaller proprietary ones, still make mistakes. This confirms that the benchmark is well-calibrated: it exposes meaningful differences in models ability to interpret diff syntax, align edits with code, and produce the correct target version exactly. Table 2 shows the results for the Diff Generation task. Here, the effect of the system prompt is much stronger: explicitly describing the format narrows the models choices and reduces the likelihood of switching to alternative diff formats. This effect is especially noticeable for GPT 4.1, which, without explicit instructions, frequently outputs diffs in V4A, the format it was trained to produce, rather than unified diff (see Figure 11 in Appendix B). We observe that the Apply Rate is nearly identical to the IoU across models, indicating that whenever model produces an applicable diff, it is usually very close to the expected result. 4 Table 1: Results of closed models on the Apply and Anti-Apply tasks on Diff-XYZ. Model Prompt GPT 4o GPT 4o GPT 4o-mini GPT 4o-mini GPT 4.1 GPT 4.1 GPT 4.1 mini GPT 4.1 mini GPT 4.1 nano GPT 4.1 nano w/o format w/format w/o format w/format w/o format w/format w/o format w/format w/o format w/format Claude 4 Sonnet Claude 4 Sonnet Gemini 2.5 Flash w/o format Gemini 2.5 Flash w/format w/o format w/format Apply Anti-Apply EM IoU EM IoU 0.83 0.87 0.70 0.05 0.92 0.81 0.90 0.90 0.38 0.29 0.95 0.96 0.91 0.93 0.97 0.98 0.96 0.12 0.98 0.86 0.99 0.98 0.85 0. 0.99 0.99 0.97 0.98 0.88 0.89 0.57 0.34 0.93 0.95 0.86 0.85 0.03 0.00 0.87 0.87 0.77 0.81 0.97 0.98 0.85 0.49 0.98 0.98 0.97 0.97 0.63 0. 0.90 0.89 0.85 0.85 Table 2: Results of closed models on the Diff Generation task on Diff-XYZ. Model Prompt EM IoU f1+ f1 Apply Rate Parsing Rate GPT 4o GPT 4o GPT 4o-mini GPT 4o-mini GPT 4.1 GPT 4.1 GPT 4.1 mini GPT 4.1 mini GPT 4.1 nano GPT 4.1 nano w/o format w/format w/o format w/format w/o format w/format w/o format w/format w/o format w/format Claude 4 Sonnet Claude 4 Sonnet Gemini 2.5 Flash w/o format Gemini 2.5 Flash w/format w/o format w/format 0.34 0.33 0.17 0.16 0.34 0.76 0.11 0.60 0.25 0.21 0.64 0.85 0.69 0.66 0.50 0.53 0.38 0.27 0.35 0.78 0.12 0.64 0.34 0.27 0.66 0.89 0.79 0. 0.72 0.79 0.65 0.60 0.34 0.76 0.08 0.52 0.26 0.18 0.73 0.92 0.82 0.80 0.51 0.52 0.36 0.33 0.25 0.52 0.08 0.33 0.14 0.09 0.57 0.71 0.61 0. 0.52 0.55 0.40 0.28 0.35 0.79 0.12 0.78 0.35 0.28 0.67 0.89 0.80 0.77 0.89 0.98 0.92 0.90 0.44 0.99 0.15 0.65 0.47 0.53 0.80 1.00 0.93 0. 4.2 Open Source Models To analyze how model size affects diff understanding, we evaluate the Qwen2.5-Coder series [15], family of open-source, code-focused LLMs ranging from 0.5B to 32B parameters. Qwen2.5-Coder models achieve near state-of-the-art results on standard code generation and reasoning benchmarks among open-source models, making them strong testbed for scaling analysis on Diff-XYZ. Tables 3 and 4 report results for the Apply/Anti-Apply and Diff Generation tasks, respectively. We observe clear scaling trend: performance improves steadily with model size. On Apply and Anti-Apply, reliable results emerge around the 7B scale, with the largest Qwen Coder approaching GPT-4o. In contrast, none of the open-source models achieve comparable performance on Diff Generation. This gap suggests that handling diff syntax and formatting requires substantially more capacity than simply applying edits. It may also help explain why smaller models perform poorly on complex downstream benchmarks such as SWE-bench, where correctly generating patches is critical. 5 Table 3: Results of the Qwen Coder family of models on the Apply and Anti-Apply tasks on Diff-XYZ with w/format system prompt. Model Apply Anti-Apply EM IoU EM IoU Qwen2.5-Coder-0.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct 0.00 0.11 0.36 0.59 0.82 0.85 0.39 0.39 0.81 0.94 0.97 0. 0.00 0.05 0.15 0.64 0.82 0.86 0.46 0.70 0.71 0.93 0.97 0.98 Table 4: Results of the Qwen Coder family of models on the Diff Generation task on Diff-XYZ with w/format system prompt. Model EM IoU f1+ f1 Apply Rate Parsing Rate Qwen2.5-Coder-0.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct 0.00 0.01 0.00 0.03 0.14 0.24 0.00 0.04 0.12 0.17 0.35 0.47 0.01 0.07 0.18 0.29 0.47 0.61 0.02 0.10 0.15 0.24 0.37 0.50 0.01 0.04 0.15 0.19 0.38 0. 0.23 0.69 0.72 0.70 0.92 0."
        },
        {
            "title": "5 Diff Format Exploration",
            "content": "While the unified diff format is the most widely used, it is not the only way to represent edits. Different formats impose different structural constraints and tokenization patterns, which can affect how easily models generate, parse, and apply them. To probe these effects, we compare several representations side by side on Diff-XYZ. For each format, the system prompt included both description of the format and one example (see Appendix A.3). We evaluate the following formats: 1. udiff : the standard unified diff (Figure 6). 2. udiff-h: unified diff with relaxed hunk header, written as @@ ... @@ (Figure 7). 3. udiff-l: unified diff with verbose line markers ADD, DEL, CON instead of the single-character +, -, and leading space (Figure 8). 4. search-replace: sequence of edits where each search substring is replaced by replace substring, following Aider [1] and Li et al. [18] (Figure 9). We include udiff and search-replace because they are widely used. The two udiff variants address practical generation issues: udiff-h avoids committing to exact line numbers before the hunk body is produced, and udiff-l reduces ambiguity and token collisions by replacing single-character markers with explicit tags. The results are presented in Table 5. For Diff Generation, search-replace is strong default for most larger models, while udiff-l achieves the best scores for smaller models, result that is both consistent and unexpected. Another unexpected finding is that udiff-h is typically much worse than udiff, despite only relaxing the hunk header and making minimal change in format. For Apply and Anti-Apply, however, search-replace underperforms relative to more structured formats, highlighting trade-off between ease of generating edits and faithfulness of application. These results suggest that while searchreplace is attractive for generation, it is poor fit for tasks where faithful application matters. This may also affect downstream tasks such as commit message generation, which rely on diffs as structured inputs. For such tasks, explicit udiff variants remain the safer and more reliable choice. We also observe that the most effective representation depends on both model scale and task step (generation vs. application). We hypothesize three interacting causes: 6 Table 5: Results on Diff-XYZ for various diff formats. For each model, the best-performing format is highlighted per task: Apply, Anti-Apply, and Diff Generation. If format is best in multiple tasks, the corresponding styles are combined (e.g., udiff). Model Diff Format Apply Anti-Apply Diff Generation GPT 4o GPT 4.1 GPT 4.1-mini GPT 4.1-nano Claude 4 Sonnet Gemini 2.5 Flash Qwen2.5-Coder-0.5B Qwen2.5-Coder-1.5B Qwen2.5-Coder-3B Qwen2.5-Coder-7B Qwen2.5-Coder-32B udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace udiff udiff-h udiff-l search-replace EM 0.86 0.85 0.82 0.57 0.90 0.92 0.91 0.57 0.89 0.86 0.86 0.57 0.37 0.32 0.44 0. 0.95 0.95 0.95 0.57 0.90 0.91 0.79 0.57 0.01 0.01 0.00 0.00 0.16 0.16 0.11 0.20 0.38 0.41 0.34 0.28 0.58 0.57 0.57 0. 0.84 0.83 0.83 0.57 7 EM 0.85 0.89 0.88 0.60 0.88 0.93 0.88 0.56 0.81 0.85 0.83 0. 0.02 0.01 0.04 0.01 0.82 0.87 0.93 0.48 0.34 0.54 0.07 0.53 0.01 0.01 0.01 0.00 0.04 0.05 0.03 0.07 0.17 0.20 0.13 0. 0.65 0.64 0.62 0.50 0.87 0.86 0.82 0.53 EM f+ 0.43 0.03 0.03 0.74 0.77 0.06 0.06 0.95 0.73 0.03 0.04 0. 0.51 0.12 0.09 0.05 0.82 0.02 0.15 0.94 0.73 0.10 0.17 0.88 0.00 0.03 0.23 0.00 0.01 0.20 0.38 0.19 0.01 0.02 0.36 0. 0.05 0.01 0.07 0.28 0.23 0.00 0.03 0.68 0.89 0.20 0.11 0.93 0.92 0.25 0.21 0.97 0.87 0.28 0.10 0.94 0.81 0.47 0.10 0. 0.95 0.13 0.34 0.97 0.94 0.22 0.32 0.96 0.04 0.18 0.57 0.04 0.07 0.45 0.49 0.35 0.20 0.16 0.54 0.30 0.39 0.17 0.17 0. 0.73 0.25 0.18 0.92 0.83 0.61 0.25 0.89 0.91 0.61 0.39 0.94 0.84 0.62 0.14 0.94 0.78 0.49 0.11 0. 0.94 0.31 0.55 0.96 0.92 0.28 0.26 0.95 0.02 0.18 0.42 0.22 0.09 0.42 0.49 0.38 0.22 0.21 0.55 0.22 0.31 0.28 0.18 0. 0.63 0.67 0.39 0.86 1. Local vs. global constraints. search-replace avoids format-level global constraints such as predicting line numbers in hunk headers (e.g., @@ -a,b +c,d @@), matching hunk lengths, and preserving the number of context lines. Each replacement stands on its own, so an error in one edit does not invalidate the rest. Larger models are better at locating distinctive anchors and ordering small, local edits, which increases parse/apply success. 2. Marker collisions. Small models may confuse single-character udiff markers (+, -, leading space) with ordinary code characters. Replacing them with explicit tags (ADD/DEL/CON) in udiff-l makes the control tokens unambiguous and rare; the model mainly needs to decide, for each line, which tag to use and then emit the line, an easier decision for weaker models. 3. Header scaffolding & distribution shift. Standard udiff hunk headers (@@ -a,b +c,d @@) provide numeric anchors and implicit ordering cues. Even though our application step does not rely on these numbers, their presence appears to help models structure the patch: they act as scaffolding that encourages hunks to be segmented and emitted in file order. In udiff-h, this scaffold is removed, which not only introduces distribution shift relative to pretraining corpora (most public diffs include numbers) but may also increase the frequency of hunks being emitted out of order (e.g., an edit to line 120 preceding one to line 90). Both effects make the resulting patches less reliable, even though the format change is minimal."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "The tasks in Diff-XYZ are simplified proxies rather than full downstream applications. Establishing quantitative link between performance on these tasks and outcomes in real systems such as commit message generation, automated bug fixing, or code review is left for future work. As result, our findings should not be interpreted as direct predictions of production performance. All experiments use non-reasoning LLMs under greedy decoding. Evaluation of tool use, multistep reasoning, sampling strategies, or best-of-n decoding, is an important question that is out of scope of this work. The reported numbers therefore reflect single-pass behavior and likely represent conservative estimates rather than achievable upper bounds. The set of edit representations we study is limited to the most popular ones. Some alternatives remain unexplored, including richer treeor AST-based patches, structured search-replace variants, and error-tolerant or partially specified formats. Note, that they can be integrated into Diff-XYZ without modifying the benchmark."
        },
        {
            "title": "7 Related Work",
            "content": "Code generation benchmarks. long line of benchmarks evaluate LLMs on code generation from natural language. Chen et al. [9] and Austin et al. [4] introduced HumanEval and MBPP, which remain the most widely used, with extensions such as HumanEval+ and MBPP+ [19], HumanEval-XL [24], and multilingual variations such as MultiPL-E [7]. BigCodeBench [30] emphasizes more complex and library-heavy tasks. These benchmarks primarily test function-level synthesis and correctness, but do not address how edits are represented or applied. Editing and issue-resolution benchmarks. More recent work has evaluated language models in editing settings. Cassano et al. [8] introduces instruction-following edits, and Guo et al. [14] evaluate debugging, polishing and translation tasks. At larger scale, Jimenez et al. [17] proposed SWE-bench, which tasks models with resolving real GitHub issues by producing patches that apply and pass tests. These benchmarks reflect realistic workflows, but include many factors, from retrieval and long context reasoning to semantic correctness and patch formatting. These benchmarks capture realistic workflows, including patch application, but their evaluation mixes retrieval, long-context reasoning, semantic correctness, and patch formatting making it difficult to isolate the role of edit representation. Positioning. Despite this breadth, existing benchmarks all assume fixed diff format, typically unified diff. In reality, there are multiple representations in use: normal diff, context diff, unified diff, OpenAIs V4A diff format [23], and search/replace schemes, including anchored replacements used by some agents. Format is therefore real variable, not constant. Aiders benchmarks [2] 8 are notable for reporting differences between formats (whole file rewrite, search/replace, unified diff), but their evaluation was motivated by tool design choices rather than systematic study of edit representations. Our benchmark takes complementary approach: it decomposes the end-to-end code editing pipeline problem and focuses specifically on edit representation in isolation. By holding task context fixed, it enables us to measure how models handle alternative formats under controlled conditions. This design also makes evaluation lightweight and cheap: unlike agentic frameworks such as SWE-bench, it requires no repository setup or execution harness, yet still targets core capability that directly impacts downstream systems."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced Diff-XYZ, benchmark for evaluating LLMs ability to handle code diffs across multiple formats. The tasks isolate core subproblems that arise in downstream systems while remaining simple enough to serve as controlled probes for more complex workflows. Our main contribution is set of focused tasks with clearly specified inputs and targets, paired with automatic metrics. This framework offers reproducible setting for studying model behavior in diff-centric workflows. Possible extensions include connecting the benchmark to downstream tasks such as commit message generation, exploring corrupted or partial diff application, and incorporating structured code editing. We also establish baselines for the unified diff format by measuring how LLMs process udiff inputs directly. Frontier models perform strongly on the provided tasks, suggesting the value of introducing more challenging instances within the same framework. Beyond udiff, we compare several edit representations across range of models. The resulting trade-offs can guide representation design. Future work will address the observed flaws and iterate toward formats that improve faithfulness and applicability."
        },
        {
            "title": "References",
            "content": "[1] Aider. Unified diffs make GPT-4 turbo 3x less lazy, 2024. URL https://aider.chat/docs/ unified-diffs. [2] Aider. Aider polyglot benchmark, 2024. URL https://aider.chat/2024/12/21/ polyglot.html#the-polyglot-benchmark. [3] Anthropic. System card: Claude Opus 4 & Claude Sonnet 4, 2025. URL https://www-cdn. anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. SWErebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. arXiv preprint arXiv:2505.20411, 2025. [6] Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie Van Deursen, Maliheh Izadi, et al. Long code arena: set of benchmarks for long-context code models. arXiv preprint arXiv:2406.11612, 2024. [7] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. Multiple: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. 9 [8] Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Jane Anderson, and Arjun Guha. Can it edit? evaluating the ability of large language models to follow code editing instructions. In Conference on Language Modeling (COLM), 2024. [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [11] Aleksandra Eliseeva, Yaroslav Sokolov, Egor Bogomolov, Yaroslav Golubev, Danny Dig, and Timofey Bryksin. From commit message generation to history-aware commit message completion. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 723735. IEEE, 2023. [12] Simone Forte and Marcus Revaj. ments smart-paste-for-context-aware-adjustments-to-pasted-code/. pasted 2024. code, to Smart Paste URL adjusthttps://research.google/blog/ context-aware for [13] Gregory Gay and René Just. Defects4j as challenge case for the search-based software engineering community. In International Symposium on Search Based Software Engineering, pages 255261. Springer, 2020. [14] Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li, Ruibo Liu, Yue Wang, et al. Codeeditorbench: Evaluating code editing capability of large language models. arXiv preprint arXiv:2404.03543, 2024. [15] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [17] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [18] Hongwei Li, Yuheng Tang, Shiqi Wang, and Wenbo Guo. Patchpilot: stable and cost-efficient agentic patching framework. arXiv preprint arXiv:2502.02747, 2025. [19] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. [20] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval. Cambridge University Press, USA, 2008. ISBN 0521865719. [21] Jim Meyering and Paul Eggert. GNU diffutils, 2010. URL https://www.gnu.org/ software/diffutils/. [22] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. [23] OpenAI. Introducing GPT-4.1 in the API, 2025. URL https://openai.com/index/ gpt-4-1/. 10 [24] Qiwei Peng, Yekun Chai, and Xuhong Li. Humaneval-xl: multilingual code generation benchmark for cross-lingual natural language generalization. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 83838394, 2024. [25] Sergey Vakhreev. SWE-bench, Refact.ai is now the URL #1 agent open-source AI https://refact.ai/blog/2025/ on open-source-sota-on-swe-bench-verified-refact-ai/. 2025. [26] You Wang, Michael Pradel, and Zhongxin Liu. Are \"solved issues\" in SWE-bench really solved correctly? An empirical study. arXiv preprint arXiv:2503.15223, 2025. [27] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Demystifying LLMbased software engineering agents. Proceedings of the ACM on Software Engineering, 2(FSE): 801824, 2025. [28] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [29] Yuxia Zhang, Zhiqing Qiu, Klaas-Jan Stol, Wenhui Zhu, Jiaxin Zhu, Yingchen Tian, and Hui Liu. Automatic commit message generation: critical review and directions for future work. IEEE Transactions on Software Engineering, 50(4):816835, 2024. [30] Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking In The Thirteenth code generation with diverse function calls and complex instructions. International Conference on Learning Representations, 2025."
        },
        {
            "title": "A Inference Details",
            "content": "A.1 System prompts You are helpful assistant. Figure 1: w/o format System Prompt You are helpful assistant. When referred to unified diff format, the formatting must be as follows: or index ..... Use POSIX unified diff with headers Do NOT include or start with Git headers like diff git ... - <old> and +++ <new>, hunks @@ -old_start,old_count +new_start,new_count @@. -, additions +. lines. /dev/null. - /dev/null. Deleted file: 1-based numbering, LF newlines, 1 context Prefix context with space, removals New file: +++ Figure 2: w/format System Prompt 11 A.2 Task Prompts You need to write code that is result of applying the following diff in unified diff format to the following code snippet: Diff: {diff} Code: {old_code} Use triple backtick formatting for you answer (e.g., ```python...```). Figure 3: Prompt Template for the Apply Task You are given code snippet that results from applying unified diff. Your task is to reconstruct the original version of the code before the diff was applied. Diff: {diff} Code After Applying the Diff: {new_code} Use triple backtick formatting for you answer (e.g., ```python...```). Figure 4: Prompt Template for the Anti-Apply Task 12 You need to write diff in unified diff format that transforms code snippet 1 to code snippet 2: Code Snippet 1: {old_code} Code Snippet 2: {new_code} Use triple backtick formatting for you answer (e.g., ```diff...```). Figure 5: Prompt Template for the Diff Generation Task A.3 Examples of Diff Formats @@ -1,2 +1,4 @@ +import math + def calculate_area(radius): return 3.14159 * radius * radius return math.pi * radius * radius - + @@ -4,2 +6,0 @@ -def old_function(): - return \"deprecated\" Figure 6: Example of diff in udiff format. This example is included in system prompt for diff exploration. @@ ... @@ +import math + def calculate_area(radius): return 3.14159 * radius * radius return math.pi * radius * radius - + @@ ... @@ -def old_function(): - return \"deprecated\" Figure 7: Example of diff in udiff-h format. This example is included in system prompt for diff exploration. 13 @@ -1,2 +1,4 @@ ADD import math ADD CON def calculate_area(radius): DEL ADD @@ -4,2 +6,0 @@ DEL def old_function(): DEL return \"deprecated\" return 3.14159 * radius * radius return math.pi * radius * radius Figure 8: Example of diff in udiff-l format. This example is included in system prompt for diff exploration. <<<<<<< SEARCH return 3.14159 * radius * radius ======= return math.pi * radius * radius >>>>>>> REPLACE <<<<<<< SEARCH def old_function(): return \"deprecated\" ======= >>>>>>> REPLACE Figure 9: Example of diff in search-replace format. This example is included in system prompt for diff exploration."
        },
        {
            "title": "B Typical Errors",
            "content": "--- a/src/main.rs +++ b/src/main.rs @@ -20,5 +20,15 @@ let mut num = 0; + let mut prev = 0; - + + + + + + + + + + for in roman_number.chars() { num += roman_char_to_decimal(c); let digit = roman_char_to_decimal(c); num += digit; if prev != 0 && digit > prev { num -= 2 * prev; } prev = digit; } Figure 10: Typical incorrect answer for GPT 4.1 with w/format system prompt in Apply Task. Diff instead of new code is generated for more than 10% of data points. For these data points, answer with w/o format system prompt is correct, and answer for Anti-Apply is correct with either system prompt. --- snippet1.py 2024-06-13 12:00:00.000000000 +0000 +++ snippet2.py 2024-06-13 12:00:00.000000000 +0000 @@ setup( - - + + Development Status :: 4 - Beta, Development Status :: 5 - Production/Stable, classifiers=[ classifiers=[ --- old +++ new @@ -18,7 +18,7 @@ - + packages=[drupdates, drupdates.tests], classifiers=[ Development Status :: 4 - Beta, Development Status :: 5 - Production/Stable, Environment :: Console, Intended Audience :: Developers, Intended Audience :: System Administrators, Figure 11: Top Box. Typical incorrect answer for GPT 4.1 with w/o format system prompt in Diff Generation Task. Bottom Box. Answer with w/format system prompt. Incorrect diff is in V4A format instead of the unified diff format. Such answers are generated for more than 50% of data points."
        }
    ],
    "affiliations": [
        "JetBrains Research Amsterdam, the Netherlands",
        "JetBrains Research Belgrade, Serbia"
    ]
}