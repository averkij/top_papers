{
    "paper_title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training",
    "authors": [
        "Hongcheng Wang",
        "Yinuo Huang",
        "Sukai Wang",
        "Guanghui Ren",
        "Hao Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 9 4 4 2 . 9 0 5 2 : r Preprint. GRPO-MA: MULTI-ANSWER GENERATION IN GRPO FOR STABLE AND EFFICIENT CHAIN-OF-THOUGHT TRAINING Hongcheng Wang*1,2, Yinuo Huang*4,2, Sukai Wang3, Guanghui Ren(cid:134)3, and Hao Dong(cid:66)1,2 1CFCS, School of Computer Science, Peking University 2PKU-Agibot Joint Lab 3Agibot 4University of Electronic Science and Technology of China"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance."
        },
        {
            "title": "INTRODUCTION",
            "content": "The Deepseek-R1 model demonstrates that reinforcement learning (RL)specifically through Group Relative Policy Optimization (GRPO) Shao et al. (2024)is effective in training Chain-of-Thought (CoT). This method involves prompting the Large Language Model (LLM) to generate reasoning trace before producing final answer, which is then reinforced via reward signal to enhance the models reasoning capabilities. Subsequently, methods such as DAPO Yu et al. (2025), Dr.GRPO Liu et al. (2025), and GPG Chu et al. (2025) have improved upon GRPOs loss function from various perspectives, achieving more stable training curves and better results on mathematical problems. Beyond textual reasoning tasks, the GRPO paradigm has also been extended to multimodal scenarios Chen et al. (2025b); Shen et al. (2025); Huang et al. (2025b); Feng et al. (2025); Song et al. (2025); Kim et al. (2025). Compared to refinements of the algorithm itself, these multimodal applications have largely adopted task-specific reward functions to improve performance on specific objectives, such as adding temporal video reward in Video-R1 Feng et al. (2025) or trajectory distance reward in ManipVLM-R1 Song et al. (2025). These studies have shown that CoT, trained with verifiable reward functions in RL, significantly enhances multi-modal reasoning. Despite these successes, GRPO still suffers from inherent challenges that limit training stability, efficiency, and overall effectiveness. These challenges manifest in several distinct yet interrelated aspects of the GRPO frameworknamely, gradient coupling between thoughts and answers, inefficiencies in generation, and instability in advantage estimation. *Co-first Author. Yinuo Huang contributed during an internship at the PKU-agibot Joint Lab. (cid:134)Project Lead (cid:66)Corresponding Author: hao.dong@pku.edu.cn 1 Preprint. well-known issue is the mismatch between reasoning traces and final answers: the reasoning may be valid while the final answer is wrong, or conversely, flawed reasoning may still yield correct answer. This phenomenon can be observed in both pure textual reasoning tasks Simoni et al. (2025); Lin et al. (2025a); Paul et al. (2024); Turpin et al. (2023) and multi-modal tasks Chen et al. (2025b); Balasubramanian et al. (2025) including our experiments 5.4. Since the gradients of thoughts and answers are inherently coupled in GRPO, such inconsistencies can distort the gradient direction and consequently undermine training effectiveness. Although GRPO-CARE Chen et al. (2025b) introduces consistency reward to alleviate this, it risks reward hacking and is difficult to apply when semantic consistency is ill-defined (e.g., it is difficult to judge the consistency between CoT and the numerical coordinates of predicted bounding box). The second challenge arises from the high computational cost of response generation in GRPO. For each problem instance, the algorithm must sample multiple complete CoTanswer pairs, which becomes prohibitive when the task is difficult or the answer space is large. In such settings, the limited number of samples often fails to capture the rare successful outcomes, yielding groups where all rewards are zero. Since GRPO computes advantages by normalizing rewards within each group, an all-zero reward set collapses into uniform zero advantages, providing no relative signal. This collapse of advantage values eliminates informative gradients and stalls optimization. related approach, DAPO, attempts to address this issue by continually sampling until at least one positive reward is found, thereby avoiding advantage collapse. However, this strategy incurs extremely high computational overhead when rewards are sparse. Hence, an efficient mechanism is needed to enrich reward signals without incurring excessive generation cost. The third challenge concerns variance in advantage estimation. From probabilistic modeling perspective, good thought is fundamentally one that increases the likelihood of generating good answer. It follows that the most robust method for evaluating thoughts quality would be to assess the overall distribution of multiple answers sampled from it. The GRPO, in its current design, estimates thoughts advantage based on single sampled answer. consequence of this singlesample estimation, particularly when combined with the stochasticity of high-temperature sampling in LLMs and VLMs, is the potential for increased variance in the advantage estimation. Crucially, more accurate estimation of thought advantages is not only beneficial for reducing training instability, but also for guiding the model to internalize what constitutes genuinely good thought. This, in turn, enables the model to generate higher-quality answers more reliably. In this paper, we propose GRPO-MA (GRPO with Multi-Answer). For each of thoughts, we sample answers. thoughts value is the average reward of its answers, which is used to derive its advantage relative to other thoughts. while each of the answers also receives its own advantage. These two advantages are then used to update thought and answer tokens separately. Our theoretical analysis, based on the delta method Oehlert (1992), reveals the distinct effects of and on the variance of the thoughts advantage. The analysis shows that as increases, the variance monotonically decreases towards zero. In contrast, increasing only reduces the variance to non-zero constant. This design brings three benefits: (1) It reduces gradient coupling from noisy thoughtanswer mismatches by basing thought updates on an averaged reward. (2) The multi-answer estimate of thoughts value has lower variance, leading to more stable advantage estimation. (3) It is computationally efficient by amortizing the cost of generating thoughts across answers each, avoiding the higher cost of generating full reasoning responses, while still preserving diverse and informative set of reward signals. We evaluate the effectiveness of GRPO-MA on Code, Math, several distinct vision tasks (Object Detection, Affordance Prediction, Trajectory Prediction, Demand Prediction, OCR-based VQA) and simulator-based visual manipulation task. Compared to GRPO baseline with responses, GRPO-MA yields clear gains with only marginal increase in training time. Compared to baseline with responses, it achieves similar or slightly better performance using only about 60% of the training time, highlighting improved sample efficiency from more stable gradient estimation. In the visual manipulation task with extremely sparse rewards, GRPO-MA substantially outperforms the standard GRPO algorithm. Our ablation studies further show that increasing generally leads to improved model performance, and that the stability gained from more reliable thought advantage estimation may play an even more critical role than the sheer richness of reward signals. Finally, we also compare gradient spikes during training and find that GRPO-MA produces fewer gradient spikes, reflecting greater stability in the training progress. 2 Preprint. In summary, our contributions are as follows: We propose the GRPO-MA algorithm, simple but effective and general improvement strategy for GRPO that is compatible with other mainstream enhancements like DAPO. We provide theoretical analysis showing that our method can improve the stability of advantage estimation, leading to more stable gradients. Across multiple distinct tasks, GRPO-MA consistently achieves performance gains over the baseline GRPO."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The GRPO algorithm has inspired several works to enhance its stability and efficiency by refining its loss function and sampling strategies. DAPO Yu et al. (2025) introduces several tricks to stabilize training, such as Clip-Higher for exploration, Dynamic Sampling to filter uninformative samples, and Token-Level Policy Gradient Loss to properly weight complex reasoning chains. Dr. GRPO Liu et al. (2025) corrects inherent response length bias and question difficulty bias by removing specific normalization terms from the loss and advantage calculation, leading to more stable training. Generative Policy Gradient (GPG) Chu et al. (2025) simplifies the GRPO objective and introduces gradient rescaling method to counteract zero-gradient samples, ensuring more effective policy updates. Further research has focused on improving efficiency, with CPPO Lin et al. (2025b) pruning low-impact samples to reduce computational cost and Off-Policy GRPO Mroueh et al. (2025) using stale data to improve sample efficiency. Other works enhance stability, such as GSPO Zheng et al. (2025), which realigns importance sampling at the sequence level; GMPO Zhao et al. (2025), which uses geometric mean to mitigate sensitivity to outliers ; and GTPO Simoni et al. (2025), which resolves gradient conflicts and prevents policy collapse through trajectory analysis. Additionally, specialized solutions like Spectral Policy Optimization Chen et al. (2025a) create learning signals for all-negative sample groups using AI feedback."
        },
        {
            "title": "3 PRELIMINARY: GRPO",
            "content": "The GRPO algorithm Shao et al. (2024) is Proximal Policy Optimization (PPO) variant Schulman et al. (2017). As model-free method, GRPO omits value model and instead calculates advantage values by directly normalizing the rewards obtained from generated responses. For given prompt p, GRPO samples responses = {o1, . . . , oK} from πθ, obtain rewards {R1, . . . , RK}, and compute the advantage as A(oi) = RiMean({Rk}) . In general, response oi consists of thought thi and an answer ansi. Std({Rk}) The GRPO objective is built upon clipped surrogate objective that maximizes the expected advantage while regularizing the policy change. Formally, given generated sequence y, the clipped objective is defined as: Jclip(θ, y) = 1 y (cid:88) t=1 min(cid:0)rt(θ)A(y), clip(rt(θ), 1 ε, 1 + ε)A(y)(cid:1) βDKL(πθπref) (1) where rt(θ) = πθ(ytp,y<t) πθold (ytp,y<t) , represents sequence of generated tokens and πref is fixed reference model. Building on this definition, the overall GRPO objective aggregates over multiple sampled outputs: JGRPO(θ) = (p,a)D i=1πθold (op) {oi}K (cid:34) (cid:35) 1 (cid:88) i= Jclip(θ, oi) (2) As indicated by Equation 1, the advantage determines whether the probability of certain token increases or decreases, as well as the magnitude of this change. Therefore, more stable estimation of the advantage (with lower variance) is beneficial for more stable model parameter update. 3 Preprint. Figure 1: The operational flow of advantage estimation in GRPO and GRPO-MA. In the baseline GRPO framework (top), the advantage is computed from single thoughtanswer pair, inherently coupling the estimation of thought and answer advantages to single reward signal. In contrast, GRPO-MA (bottom) extends this setting by sampling multiple answers for each thought. This design decouples the estimation of thought and answer advantages and leverages aggregated information from multiple reward signals, thereby yielding richer supervision and enabling more robust and stable estimation of thought-level advantages."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we first describe the sampling and updating process of GRPO-MA, which builds upon the GRPO framework by introducing multi-answer sampling strategy. We then analyze the variance change of the advantages via the delta method. 4.1 PIPELINE OF GRPO-MA The core modification in GRPO-MA lies in its sampling pipeline, shown in Fig. 1. Given prompt p, it first generates thoughts {th1, . . . , thK} identically to GRPO. Then, for each thought thi, GRPO-MA then generates answers {ansi,1, . . . , ansi,M }, resulting in total answers where every answers share the same thought. After obtaining rewards {Ri,j}1iK, 1jM from reward function, we define the value of thought as (thi) = 1 j=1 Ri,j, and normalize it to compute the thought advantage A(thi) = (thi)Mean({V (thk)}1kK ) . Similarly, the advantage of an answer is A(ansi,j) = Ri,j Mean({Rk,l}1kK, 1lM ) Std({Rk,l}1kK, 1lM ) Std({V (thk)}1kK ) (cid:80)M . The GRPO-MA objective then combines the two levels of advantages: JGRPO-MA(θ) = {thi}K {ansi,j }K i= (p,a)D, i=1πθold (thp), j=1πθold (ansp,th) 1 (cid:88) i=1 Jclip(θ, thi) + 1 KM Jclip(θ, ansi,j) (cid:88) (cid:88) i=1 j= (3) where the first term updates the policy based on the thoughts, using the thought advantage A(thi) within its Jclip calculation, and the second term updates the policy based on the answers, using the answer advantage A(ansi,j). 4 Preprint."
        },
        {
            "title": "4.2.1 PRELIMINARIES",
            "content": "Fix prompt p. Thoughts th1, . . . , thK are sampled independently from distribution πθ( p) conditioned on the prompt p. For each thought thi we generate answers independently from the conditional policy, written as i.i.d. πθ( p, thi), = 1, . . . , . Here the index simply labels different samples, all drawn ansi,j from the same distribution π( p, thi). Each answer ansi,j is evaluated by reward function r. We denote the resulting reward as random variable Ri,j := r(ansi,j, p). Ri,j is random because the answer ansi,j itself is sampled. σ2 Ri . . The empirical value estimator of thought thi is the sample mean (thi) = 1 Conditioned on given thought thi and prompt p, the rewards Ri,j are i.i.d. with mean µRi and variance σ2 j=1 Ri,j, Ri with E[V (thi)] = µRi and Var(V (thi)) = We assume the thought-value estimates (th1), . . . , (thK) are independent, i.e., the covariance matrix is diagonal. In practice, small correlations may exist since all thoughts are sampled from the same prompt. However, Appendix A.2.5 shows that the diagonal entries capture most of the covariance energy, suggesting this assumption is largely reasonable. Finally, define the sample mean and standard deviation across thoughts as = 1 (cid:80)K k=1(V (thk) )2, and the thought advantage as A(thi) = (thi) k=1 (thk), . (cid:113) 1 SV = (cid:80)M (cid:80)K K1 SV 4.2.2 VARIANCE OF THE THOUGHT ADVANTAGE Using the first-order multivariate delta method, and noting that under the independence assumption the covariance matrix of {V (thk)} is diagonal, the variance of the standardized thought advantage is approximated as Var[A(thi)] 1 σ µR (cid:32) (cid:88) k=1 δik 1 µi µk K1 (cid:33)2 σ2 Rk (4) where δik is the Kronecker delta (δik = 1 if = k, and 0 otherwise), µ = 1 average true thought value, σ2 µR values across thoughts, µi = µRi µ k=1 µRk is the k=1(µRk µ R)2 is the variance of the true thought K1 is the normalized (expected) advantage of thought thi. = 1 (cid:80)K (cid:80)K σµR For the full derivation on the variance of though advantages and answer advantages, please refer to Appendix A.2. 4.2.3 ANALYSIS OF THE VARIANCE STRUCTURE This expression clearly shows that the variance of the thought advantage is inversely proportional to . Increasing the number of answers sampled per thought directly reduces the variance of the thoughts value estimate, leading to more stable thoughts advantage estimation. Additionally, let us analyze how variance changes as increases. As , by the law of large = 1 k=1(µRk µ R)2 converges numbers, the sample variance of the true thought values σ2 µR to the population variance σ2 π = Var[µRi], which characterizes the variability of the true thought values µRi across the population of thoughts sampled from the distribution πθ( p). Next, we analyze the sum by splitting it based on the Kronecker delta, δik. The single term where = converges to non-zero constant, since expressions like (δik 1 . . . ) approach 1. Conversely, the sum of the other 1 terms, where = i, vanishes because each term is of order O(1/K 2), making their total sum O(1/K). This leads to the final result where the variance converges to limit (cid:80)K K1 determined only by the properties of the thought thi itself: limK Var[A(thi)] = σ2 Ri σ2 π . 5 Preprint. The analysis of increasing and indicates that while increasing alone preserves minimum variance , increasing allows the variance to decrease continuously and approach 0."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate GRPO-MA on Math Yu et al. (2025), Code PrimeIntellect (2024); White et al. (2024), several distinct vision tasks (Object Detection contributors (2024), Affordance Prediction Myers et al. (2015); Luo et al. (2022), Trajectory Prediction Ji et al. (2025), Demand Prediction Wang et al. (2024), OCR-based VQA Biten et al. (2019); Tito et al. (2021)) and Simulator-based Visual Manipulation task Li et al. (2024). Our experiments use Qwen2.5-VL-3B-Instruct Bai et al. (2025) as the base model, with all training conducted on four H100 80G GPUs using LoRA Hu et al. (2022) for parameter-efficient fine-tuning. For each task, we conduct group of experiments separately. We introduce the TKAM notation to unify the representation of the GRPO and GRPO-MA methods. In this notation, represents the number of thoughts, and denotes the number of answers generated per thought. The notation corresponds to GRPO when = 1 and to GRPO-MA when > 1. For instance, T4A4 signifies process of generating 4 thoughts, with each thought producing 4 answers, resulting in total of 16 responses. More details (datasets, hyperparameters, training settings) are in the appendix A.3 and A.4. 5.1 TEXT AND VISION TASK 5.1.1 TASK SETTING AND METRIC Math Given math problem, output the correct solution. Metric: pass@10/32 Chen et al. (2021). Code Given programming problem, output the solution code. Metric: pass@10/32. Object Detection Given an image and specified object name, output bounding boxes. Metric: proportion of predictions with Intersection-over-Union (IoU) above threshold. Affordance Prediction Given an image and an affordance (e.g., grasping, holding), output 2D coordinates. Metric: proportion of matched points. Trajectory Prediction Given an image and manipulation instruction, output the 2D end-effector trajectory. Metrics: Discrete Frechet Distance (DFD) Eiter et al. (1994), Hausdorff Distance (HD) Huttenlocher et al. (2002), Root Mean Square Error (RMSE), EndPoint Distance. Demand Prediction Given an image and human demand instruction, output 2D coordinates of the demanded object. Metric: proportion of correct points. OCR-based VQA Given an image and question requiring text understanding (e.g., infographics, scene text, documents), output the answer. Metric: Average Normalized Levenshtein Similarity (ANLS) Biten et al. (2019). Please note that, unlike the <think> and <answer> tags used in GRPO, we design distinct structured output format for Math consisting of three tags: <analysis>, <process>, and <answer>. Multi-sampling is applied to both <process> and <answer>. For other tasks, Multi-sampling is applied only to the <answer>. We track the Gradient Spike Score (GSS) Huang et al. (2025a) to measure gradient stability, defined as GSS(gi) = , where gj represents the gradient at the j-th time step. We report gi (cid:80)T 1 +1 j=0 gj the number of spikes above 10 (GSS@10), where smaller is better. For all tasks, we also report the per-step training time (s). 5.1.2 BASELINES We adopt models from the Qwen2.5-VL-Instruct series (3B, 7B, and 72B) Bai et al. (2025) as baselines to evaluate the performance of general-purpose models on our tasks. In addition, we train Qwen2.5-VL-3B-Instruct with real labels using supervised fine-tuning (SFT) to compare against GRPO, denoted as SFT in the results. Finally, we compare our proposed GRPO-MA with GRPO 6 Preprint. Figure 2: case study comparing the baseline GRPO with our proposed GRPO-MA on referring expression grounding task. The prompt is to locate the purple bottled beverage. The baseline model, GRPO (T4A1), recognizes the targets existence but its reasoning is distracted by other salient objects (the snacks), leading to failure in grounding. In contrast, our GRPO-MA (T4A4) correctly reasons about the scenes context, focuses on the target object held by the robotic arm, and successfully provides the precise bounding box. This demonstrates the superior robustness of GRPO-MA in complex scene understanding and reasoning. Table 1: Combined Results for Math and Code Generation Benchmarks. TN: The number of thoughts; AN: The number of answers per thought; S/S: Second/Step during training; Bold indicates the best performance among the GRPO variants. Model TN AN S/S GSS Pass@10 Pass@32 S/S GSS Pass@10 Pass@32 Math Code Qwen2.5-VL-3B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-72B-Ins SFT GRPO GRPO GRPO GRPO-MA 4 8 16 4 1 1 1 4 111.24 140.05 225.43 132.87 5 13 15 5 9.27 9.97 33.07 11.07 11.78 11.16 12.89 14. 16.25 18.39 41.39 18.11 20.32 21.30 21.72 27.60 76.21 104.83 186.91 93.45 6 24 25 10 9.80 10.72 20.39 8.72 11.56 11.44 11.92 11. 11.67 11.31 22.37 10.59 13.70 13.39 14.12 14.70 under different numbers of responses to demonstrate the superiority of GRPO-MA in terms of training efficiency and performance. 5.1.3 MAIN RESULTS The experimental results are presented in Tab. 1 (Math and Code Problem), Tab. 2 (Object Detection, Affordance Prediction and Demand Prediction), and Tab. 3 (OCR-based VQA and Trajectory Prediction). Across multiple visual tasks, our proposed GRPO-MA outperforms both the GRPO and SFT under various settings, demonstrating its excellent versatility across diverse tasks. Compared to T4A1, T4A4 achieves significant performance gains with only about 15% increase in training time. Compared to T16A1, T4A4 achieves comparable or even slightly better performance with about 40% reduction in training time, which demonstrates that GRPO-MA does not involve trade-off between training efficiency and training performance, but rather enhances both simultaneously. Gradient Stability In most experiments, T4A4 achieves the lowest GSS@10, indicating the best gradient stability during training, consistent with our theoretical analysis: as crucial component of gradient magnitude, the more stable estimation of the advantage value also contributes to greater gradient stability. Case Study As illustrated in Fig. 2, we present case study to contrast the reasoning processes of T4A4 and T4A1 for the object detection task. T4A4 focuses on the general vicinity of the target object and its surrounding context. Conversely, T4A1 fails to detect the target, instead paying its attention on the central region of the image. Additional case studies are provided in the appendix A.5. 7 Preprint. Table 2: Combined Results for Object Detection, Affordance, and Demand Prediction. TN: The number of thoughts; AN: The number of answers per thought; S/S: Second/Step during training; UMD: UMD Part Afforance Dataset; AGD20K: AGD20K Dataset; Bold indicates the best performance among models of the same size. Model TN AN S/S GSS@10 IoU@0.5 IoU@0.6 IoU@0.7 IoU@0. S/S GSS@10 UMD AGD20K S/S GSS Accuracy Object Detection Affordance Prediction Demand Prediction Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B SFT GRPO GRPO GRPO GRPO-MA 4 8 16 4 1 1 1 4 13.86 18.86 26.99 15. 5 30 16 1 60.87 70.11 72.57 64.63 65.11 67.13 69.03 69.71 50.54 60.23 60.66 54.73 56.16 57.52 60.29 61.32 39.67 48.02 47. 42.43 43.88 42.62 45.16 46.77 21.32 25.89 24.48 22.96 23.02 22.82 24.04 25.64 14.34 18.92 24.27 15.86 11 14 22 5 38.98 34.65 59. 66.35 78.91 88.14 89.32 89.96 52.73 43.29 60.59 53.18 55.60 57.90 57.24 58.40 13.74 18.20 25.42 14.33 5 12 37 6 11.41 19.95 26. 36.97 38.13 40.81 42.47 42.63 Table 3: Combined Results for OCR-based VQA and Trajectory Prediction. TN: The number of thoughts; AN: The number of answers per thought; S/S: Second/Step during training. Bold indicates the best performance among models of the same size. Model TN AN S/S GSS@ Infographics St VQA Doc VQA S/S GSS@10 DFD HD RMSE EndPoint OCR-based VQA Trajectory Prediction Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B SFT GRPO GRPO GRPO GRPO-MA 4 8 16 1 1 1 4 14.79 19.62 26.79 17.17 42 88 68 17 73.10 78.94 79.72 74.77 73.70 76.33 76.65 76.69 67.63 74.03 74. 69.68 68.94 71.56 72.25 72.48 91.33 93.51 93.26 92.94 93.15 93.98 94.20 94.22 29.17 34.51 66.55 35.41 14 18 21 10 571.60 496.44 386. 277.68 187.99 172.41 165.16 151.10 537.63 451.19 352.18 261.86 172.58 157.09 149.59 138.29 404.40 340.33 263.61 196.55 140.80 130.09 122.95 111.59 429.93 354.03 300. 228.62 142.74 137.29 130.56 120.60 5.2 SIMULATOR-BASED MANIPULATION TASK 5.2.1 TASK SETTING We adapt most of the experimental settings introduced in ManipLLM Li et al. (2024), which provides simulator-based framework for visual manipulation tasks. To increase the difficulty of the task, we introduce two modifications to the experimental setup. First, to ensure greater observational diversity, the camera is reconfigured to view the target object from randomly sampled angle in each trial. Second, we adapt stricter success criterion: an attempt is immediately deemed failure if the predicted contact point does not lie on the surface of the target object. Following ManipLLM, when the model outputs grasping point on the image, we execute rule-based grasping strategy. Specifically, the sucker approaches along the surface normal at the predicted point, and the subsequent trajectory is adjusted depending on the object category. For evaluation, we report the proportion of predicted points that lead to successful manipulation. Through data collection and training, we observe that this task is highly reward-sparse, since solving it requires the model to reason about object-specific interaction dynamics. 5.2.2 BASELINES We adapt some of the same baselines used in visual tasks and added several additional baselines: ManipLLM-7B, CoT-SFT, and GRPO-NoThink. ManipLLM-7B They collects large number of successful samples in the simulator and constructs multiple task-specific question-answer pairs, utilizing the SFT training approach. We have finetuned their weights in the new settings. CoT-SFT We collect successful samples of GRPO-MA-T4A4 (including the chain of thoughts and answers), then fine-tune Qwen2.5-VL-3B using SFT. GRPO-NoThink We employ GRPO to train the Qwen2.5-VL-3B, but we do not require the model to generate thought process; instead, it directly produces the answers. 5.2.3 MAIN RESULTS The experimental results are presented in Tab. 4. direct comparison reveals that the performance of T4A4 is significantly superior to that of T4A1. This outcome demonstrates that in tasks with Preprint. Table 4: Manipulating Point Prediction. TN: The number of thoughts; AN: The number of answers per thought; S/S: Second/Step during training. Success Rate (%) Model TN AN Seen Unseen Qwen2.5-VL-3B ManipLLM-7B SFT CoT-SFT GRPO GRPO-NoThink GRPO-MA 4.73% 1.30% 22.80% 7.63% 9.17% 4.28% 28.18% 11.79% 10.75% 3.94% 10.60% 2.40% 31.40% 16.00% 4 0 4 1 16 4 extremely sparse rewards, such as multi-modal manipulation, employing multi-answer sampling strategy leads to more stable training process and facilitate sampling of effective responses. Furthermore, our experiments provide valuable insights into the indispensable role of the Chain of Thought (CoT) in this context. We observe that the GRPO-NoThink model, which ablates the CoT while sampling an equal number of answers as GRPO-MA-T4A4, suffers substantial degradation in performance. This result, along with the strong performance of the CoT-SFT model, clearly indicates that high-quality CoT is critical prerequisite for generating superior answers and effectively tackling such complex tasks. 5.3 ABLATION STUDY Figure 3: Ablation Study on Trajectory Prediction While maintaining the number of thoughts = 4, we gradually increase the number of responses per thought from 1 to 8 (i.e., the number of responses is 4, 8, 12...32). We conduct detailed ablation study on the trajectory prediction task to analyze the effect of the number of generated answers per thought, as shown in Fig. 3. The results indicate that as increases, all evaluation metrics decrease, although the rate of decline becomes progressively smaller. Surprisingly, T4A3 features 4 thoughts and 12 answers, outperforming T16A1s 16 thoughts and 16 answers across all metrics. One possible explanation for this finding is that the importance of reward signal richness (the number of answers) is less significant than the quality of thoughts; filtering out higher-quality thoughts has greater impact on the overall training process. Specifically, our method assesses thoughts quality by averaging the rewards of its subsequent answers (V (thi) = 1 j=1 Ri,j). With = 3, T4A3 obtains more stable and reliable estimate of each thoughts value, effectively reducing the noise from any single-answer evaluation. In contrast, T16A1s approach (M = 1) is far more susceptible to randomness, as single, potentially noisy reward is used to judge the entire thought. (cid:80)M 5.4 INCONSISTENCY ANALYSIS We quantify the inconsistency between thoughts and answers during training. For thought thi with answers, if sign(A(thi)) = sign(A(ansi,j)), we mark it as inconsistent. The inconsistency rate (cid:80)M is defined as InconsistencyRate = 1 j=1 1[A(thi)A(ansi,j) < 0], where 1[] denotes KM the indicator function, which equals 1 if the condition inside holds and 0 otherwise. (cid:80)K i=1 Under the T4A4 setting, the inconsistency rate is 25.65% for trajectory prediction and 24.83% for object detection. Notably, this ratio is also indicative for GRPO baselines (T4A1, T8A1, T16A1), even though they do not explicitly generate multiple answers per thought and thus cannot directly compute it, since they share the same generation hyperparameters (e.g., temperature, top-k, and top9 Preprint. sampling). This observation further supports our claim that inconsistency is common in GRPOs training. Moreover, this inconsistency implicitly undermines model training. Accuracy reward curves and richness of reward signal analysis are in the appendix A.6."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present GRPO-MA, simple yet theoretically grounded extension of GRPO that tackles three key challenges in training Chain-of-Thought models: unstable advantage estimation, gradient coupling between thoughts and answers, and sparse reward signals under limited sampling. By generating multiple answers per thought, GRPO-MA reduces the variance of advantage estimation, decouples the gradient between thoughts and answers, and densifies reward feedback. Our theoretical analysis further shows that increasing the number of answers per thought is principled way to stabilize gradients, which is corroborated by experiments on math, code, and multimodal tasks. Together, these results demonstrate that GRPO-MA improves both the stability and efficiency of GRPO-based reinforcement learning. Limitation Our study has several limitations. First, computational constraints prevent our experiments on larger-scale models. Second, our analysis relies on the simplifying assumption that thought values are independent, condition that may not hold true in practice. Finally, the lack of generalpurpose reward model means that our testing is confined to tasks with verifiable rewards."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Sriram Balasubramanian, Samyadeep Basu, and Soheil Feizi. closer look at bias and chain-ofthought faithfulness of large (vision) language models. arXiv preprint arXiv:2505.23945, 2025. Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Minesh Mathew, CV Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Icdar 2019 competition on scene text visual question answering. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 15631570. IEEE, 2019. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, and Tianyi Lin. Spectral policy optimization: Coloring your incorrect reasoning in grpo. arXiv preprint arXiv:2505.11595, 2025a. Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. GrpoarXiv preprint care: Consistency-aware reinforcement learning for multimodal reasoning. arXiv:2506.16141, 2025b. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. AgiBot World Colosseum contributors. Agibot world colosseum. https://github.com/ OpenDriveLab/AgiBot-World, 2024. Thomas Eiter, Heikki Mannila, et al. Computing discrete frechet distance. 1994. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 10 Preprint. Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, and Shiwei Liu. Spam: Spikeaware adam with momentum reset for stable llm training. arXiv preprint arXiv:2501.06842, 2025a. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025b. Daniel Huttenlocher, Gregory A. Klanderman, and William Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on pattern analysis and machine intelligence, 15(9): 850863, 2002. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17241734, 2025. Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1638416393, 2024. Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, and Younggyo Seo. Robot-r1: Reinforcement learning for enhanced embodied reasoning in robotics. arXiv preprint arXiv:2506.00070, 2025. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for objectcentric robotic manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1806118070, 2024. Zhenru Lin, Jiawen Tao, Yang Yuan, and Andrew Chi-Chih Yao. Existing llms are not self-consistent for simple tasks. arXiv preprint arXiv:2506.18781, 2025a. Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025. Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance grounding from exocentric images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22522261, 2022. Maxwell-Jia. Aime2024. https://huggingface.co/datasets/Maxwell-Jia/AIME_ 2024, 2024. Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, and Jesus Rios. Revisiting group relative policy optimization: Insights into on-policy and off-policy training. arXiv preprint arXiv:2505.22257, 2025. Austin Myers, Ching L. Teo, Cornelia Fermuller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In ICRA, 2015. Gary Oehlert. note on the delta method. The American Statistician, 46(1):2729, 1992. Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. Making reasoning matter: Measuring In Findings of the Association for and improving faithfulness of chain-of-thought reasoning. Computational Linguistics: EMNLP 2024, pp. 1501215032, 2024. PrimeIntellect. Synthetic-1: Scaling distributed synthetic data generation for verified reasoning. https://www.primeintellect.ai/blog/synthetic-1, 2024. 11 Preprint. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Marco Simoni, Aleksandar Fontana, Giulio Rossolini, and Andrea Saracino. Gtpo: Trajectory-based policy optimization in large language models. arXiv preprint arXiv:2508.03772, 2025. Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models. arXiv preprint arXiv:2505.16517, 2025. Rub`en Tito, Minesh Mathew, CV Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Icdar 2021 competition on document visual question answering. In International Conference on Document Analysis and Recognition, pp. 635649. Springer, 2021. Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36:7495274965, 2023. Hongcheng Wang, Peiqi Liu, Wenzhe Cai, Mingdong Wu, Zhengyu Qian, and Hao Dong. Mo-ddn: coarse-to-fine attribute-based exploration agent for multi-object demand-driven navigation. Advances in Neural Information Processing Systems, 37:6417664214, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contaminationfree llm benchmark. arXiv preprint arXiv:2406.19314, 4, 2024. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 12 Preprint."
        },
        {
            "title": "A APPENDIX",
            "content": "Below is the table of contents for the appendix. More Related Work A.1 Full Analysis of Variance A.2 The Multivariate Delta Method A.2.1 Asymptotic Normality of the Estimated Value Vector A.2.2 Application to the Thought Advantage Function A.2.3 Application to the Answer Advantage Function A.2.4 Diagonality Analysis of Matrices A.2.5 Details in Task Settings A.3 Math A.3.1 Code A.3.2 Object Detection A.3.3 Affordance Prediction A.3.4 Trajectory Prediction A.3.5 Demand Prediction A.3.6 Ocr-based VQA A.3.7 Simulator-based Visual Manipulation A.3.8 Details in Training A. Training Hyperparameters A.4.1 SFT Details A.4.2 Geneartion Configure A.4.3 More Case Study and Visualization A.5 More Experimental Analysis A.6 Accuracy Reward Curve A.6.1 Richness of Reward Signal A.6.2 Usage of LLMs A.7 A.1 MORE RELATED WORK: APPLICATIONS OF GRPO IN MULTIMODAL DOMAINS VLM-R1 Shen et al. (2025) applies general GRPO pipeline to Vision-Language Models, enabling smaller models to achieve competitive performance on complex visual reasoning tasks. Vision-R1 Huang et al. (2025b) generates high-quality multimodal Chain-of-Thought data and uses Progressive Thinking Suppression Training (PTST) to prevent the model from creating overly long reasoning paths. Video-R1 Feng et al. (2025) introduces Temporal-GRPO (T-GRPO), novel reward scheme that encourages the model to leverage temporal information in video sequences. ManipLVM-R1 Song et al. (2025) employs GRPO for robotic manipulation with new affordance-aware and trajectory matching reward functions to improve the localization of interactive parts and the physical plausibility of actions. Robot-R1 Kim et al. (2025) reframes robot learning as multiple-choice question answering task, using GRPO to optimize the reasoning for embodied manipulation. A.2 FULL ANALYSIS OF VARIANCE This document provides full derivation of the approximate variance for the Thought Advantage, A(thi), as presented in the main paper. We first review the multivariate Delta Method, establish the asymptotic normality of our estimators via the Central Limit Theorem (CLT), and finally present the detailed application and gradient calculation. A.2.1 THE MULTIVARIATE DELTA METHOD The Delta Method is fundamental result in statistics used to approximate the moments of function of one or more random variables. The multivariate version is central to our analysis. 13 Preprint. = (V1, V2, . . . , VK) be K-dimensional random vector of estimaGeneral Formulation. Let tors with true mean vector µ = (µ1, µ2, . . . , µK). Let denote the sample size used to compute each estimator Vk. To emphasize that these estimators are functions of the sample size, we denote ) as . the vector as Specifically, if . The Delta Method provides the asymptotic distribution of ( satisfies the condition for the Central Limit Theorem such that: µ ) (0, Σasymptotic) ( where denotes convergence in distribution, then the transformed variable ( in distribution: (5) ) also converges (f ( ) (µ )) (0, (µ )T Σasymptoticf (µ )) (6) From this formal result, we derive the practical formula for approximating the variance of ( for large but finite sample size . The term distribution has finite, non-zero variance. The variance of the estimator itself is given by: )) (µ )T Var( ) acts as scaling factor that ensures the limiting )f (µ ) Var(f ( (7) ) is the actual covariance matrix of the estimator vector, which is related to the where Var( asymptotic covariance by Var( ) Σasymptotic/M . A.2.2 ASYMPTOTIC NORMALITY OF THE ESTIMATED VALUE VECTOR Before applying the Delta Method, we must first establish that our core estimator, the vector of (th), satisfies the prerequisite of being asymptotically normal. This justification estimated values comes from the Central Limit Theorem (CLT). For each thought thk, its estimated value (thk) is the sample mean of i.i.d. random variables, the rewards {Rk,j}M j=1: (thk) = 1 (cid:88) j=1 Rk,j (8) The rewards have finite true mean µRk and finite true variance σ2 . According to the CLT, as Rk the sample size , the distribution of the standardized sample mean converges to normal distribution. This is formally stated as: (V (thk) µRk ) (0, σ2 Rk ) (9) to the = We now extend this (V (th1), . . . , (thK)). Since we have assumed that the estimated values for different thoughts are mutually independent, the joint asymptotic distribution of the vector is also normal. The mean of this limiting distribution is zero vector, and the covariance matrix is diagonal, composed of the individual variances. Therefore, the entire vector of estimators is asymptotically normal: full K-dimensional vector of estimators, (th) (10) where µ = (µR1, . . . , µRK ) is the vector of true means, and Σdiag is the diagonal covariance matrix of the limiting distribution: ( (th) µ ) (0, Σdiag) Σdiag = σ2 R1 0 ... 0 σ2 R2 ... 0 . . . 0 0 ... σ2 RK (11) This result formally justifies the application of the Multivariate Delta Method to the thought advantage function A(thi) = fi( (th)). 14 Preprint. A.2.3 APPLICATION TO THE THOUGHT ADVANTAGE FUNCTION Verification of Assumptions. The prerequisites for the Delta Method are satisfied. First, as estab- (th) is asymptotically normal. Second, the advantage function lished above, our estimator vector A(thi) = (V (thi) )/SV is continuously differentiable everywhere except where the denominator SV = 0, where = 1 k=1(V (thk) )2. We evaluate the gradient at µ , where the denominators analogue is σµR . The approximation is thus valid assuming σµR > 0, i.e., not all thoughts have the same true value. k=1 (thk) and SV = (cid:113) 1 (cid:80)K (cid:80)K Gradient Calculation. Let (th) = = (V1, . . . , VK) and define fi(V ) = A(thi) = Ni(V ) D(V ) = Vi SV , ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) j=1 Vj, Q(V ) ="
        },
        {
            "title": "1\nK − 1",
            "content": "K (cid:88) (Vj )2, j=1 D(V ) = SV = (cid:112)Q(V ). We compute fi/Vk in steps. First, Vk = 1 , Ni Vk = δik 1 . For we have, using (Vj )/Vk = δjk 1 , Vk = 1 = 2 1 (cid:88) 2(Vj )(cid:0)δjk 1 (cid:1) j=1 (Vk ) (cid:88) (Vj ) = j= 1 2 1 (Vk ), because (cid:80) j(Vj ) = 0. Therefore Vk = 1 2D Vk = Vk (K 1)D . Applying the quotient rule yields, for arbitrary , )D (Vi ) Vk (δik 1 (K1)D fi Vk = = δik 1 (Vi )(Vk ) (K 1)D3 . Evaluate at = µ and denote σµR := D(cid:12) (cid:12)V =µ = (cid:115) 1 1 (cid:88) (µj µ)2, µj := µj µ σµR . Then fi Vk (cid:12) (cid:12) (cid:12) (cid:12)V =µ = (cid:16) 1 σµR δik 1 µi µk 1 (cid:17) . (12) (13) (14) (15) (16) (17) Finally, by the first-order multivariate Delta method, with Var( diag(σ2 R1 , . . . , σ )), RK ) = 1 Σdiag (and Σdiag = Var[A(thi)] fi(µ)Var( ) fi(µ) = 1 σ2 µR (cid:88) (cid:16) k=1 δik 1 (cid:17)2 µi µk 1 σ2 Rk . (18) 15 Preprint. A.2.4 APPLICATION TO THE ANSWER ADVANTAGE FUNCTION For single answer ansi,j, the advantage is defined as A(ansi,j) = Ri,j SR , ="
        },
        {
            "title": "1\nKM",
            "content": "K (cid:88) (cid:88) k=1 m=1 Rk,m, SR = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nKM − 1",
            "content": "K (cid:88) (cid:88) k=1 m=1 (Rk,m R)2. (19) Using the first-order multivariate Delta method, the variance of A(ansi,j) can be approximated as (20) Var(cid:2)A(ansi,j)(cid:3) Rgi,j(µ) diag(σ2 R1 where gi,j(R) = A(ansi,j) and µ denotes the vector of reward means. , . . . ) Rgi,j(µ), , . . . , σ2 RK Evaluating the gradient at = µ and grouping by thought, we obtain Var(cid:2)A(ansi,j)(cid:3) KM 1 (K 1) σ2 µR (cid:88) (cid:88) k= m=1 (cid:18) δ(k,m),(i,j)"
        },
        {
            "title": "1\nKM",
            "content": "µi µk (K 1) (cid:19)2 σ2 Rk , (21) where δ(k,m),(i,j) is the Kronecker delta, µk = (µRk µ R)/σµR is the expected advantage of thought thk, µ = 1 k=1 µRk , and σ2 µR k=1(µRk µ R)2. = 1 (cid:80)K (cid:80)K K1 A.2.5 DIAGONALITY ANALYSIS OF MATRICES To examine whether the assumption of independence across thoughts (i.e., diagonal covariance matrix) holds in practice, we conducted numerical simulations and empirically estimated the covari- (th) = (V1, . . . , VK). Specifically, we generated independent replications ance structure of of the full K-dimensional estimator vector, denoted (n) for = 1, . . . , , and computed the empirical covariance matrix: (cid:98)Σ = 1 1 (cid:88) n=1 (cid:0)V (n) (cid:1)(cid:0)V (n) (cid:1) , = 1 (cid:88) n=1 (n). (22) We then assessed the degree of diagonal dominance using Row-wise strict diagonal dominance and Frobenius-norm based diagonal energy ratio. Row-wise strict diagonal dominance. For each row i, the covariance matrix is said to be strictly diagonally dominant if (cid:98)Σii > (cid:98)Σij. (cid:88) We summarize this property by the proportion of rows that satisfy the condition: j=i prow dom = 1 K (cid:88) i=1 1 (cid:110) (cid:98)Σii > (cid:88) (cid:111) , (cid:98)Σij j=i (23) (24) where 1{} denotes the indicator function. value prow dom 1 indicates strong diagonal dominance. Frobenius-norm based diagonal energy ratio. We also consider the proportion of squared Frobenius norm explained by the diagonal entries: (cid:80)K , 0 ρF 1. (25) ρF = (cid:80)K i=1 i=1 (cid:98)Σ2 ii (cid:80)K j=1 (cid:98)Σ2 ij Higher values of ρF indicate that the diagonal terms dominate the overall covariance energy. We select 50 samples from the Trajectory Prediction task and, at the 1500-step checkpoint, compute the covariance matrix of the thought-value estimates by performing = 10 independent replications per sample. The empirical results yield prow dom=63.65% and ρF = 70.71% averaged on 50 samples. Since our theoretical derivations rely on the assumption that the covariance matrix is diagonal, these diagnostics suggest that this assumption has certain degree of validity in practice, as the estimated covariance matrices exhibit clear tendency toward diagonal dominance. 16 Preprint. A.3 DETAILS IN TASK SETTINGS A.3.1 MATH We conduct our experiments using problems from the DAPO Yu et al. (2025) training set and evaluate on the AIME2024 test set Maxwell-Jia (2024). The Math training set is constructed by randomly sampling 1,000 problems from the DAPO training corpus. The model is trained for single epoch on these 1,000 training samples. We do not use validation set; instead, we select the final model parameters saved at the end of training (the last checkpoint) for testing. At test time, for each test problem from AIME2024 we generate = 100 independent candidate outputs (generations). From these 100 generations we compute the pass@k metrics for {10, 32}. The reward function is designed with two complementary components: format reward and an accuracy reward. The model is required to generate outputs in predefined structured format: <analysis> xxx </analysis> <process> xxx </process> <answer> </answer> where the answer is represented as single integes d. The format reward assigns value of 1 if and only if the output strictly follows the required format, and 0 otherwise. The accuracy reward is +1 if the predicted answer is identical to the true answer, and 0 otherwise. The full prompt template is shown below: {Question} You MUST structure your response using exactly threesections with XML-style tags in this exact order: 1) <analysis> ... </analysis> 2) <process> ... </process> 3) <answer> ... </answer> Roles and constraints: - <analysis>: State relevant concepts, theorems, formulas, and solution plan. Do NOT perform numeric calculations or write equations here. - <process>: Perform ALL detailed computations and step-by-step derivations based on the analysis. Show equations and numeric work here. - <answer>: Output ONLY the final integer (optional sign). No words, units, punctuation (except the sign), or explanations. Hard requirements: - All three tags must be present and appear in the exact order <analysis> -> <process> -> <answer>. - No calculations in <analysis>. - All computations must be in <process>. - <answer> must contain single integer only. Implementation Note: In our multi-sample framework, the sampled content encompasses both < answer > and < process > elements. A.3.2 CODE We conduct our experiments using the Python-code portion of the SYNTHETIC-1 dataset PrimeIntellect (2024) and evaluate on the LiveBench code test set White et al. (2024). The Code training set is constructed by randomly sampling 1,000 problems from the SYNTHETIC-1 Python-code corpus. The model is trained for single epoch on these 1,000 training samples. We do not use validation set; instead, we select the final model parameters saved at the end of training (the last checkpoint) for testing. 17 Preprint. At test time, for each test problem from LiveBench we generate = 100 independent candidate outputs (generations). From these 100 generations we compute the pass@k metrics for {10, 32} as described below. The reward function is designed with two complementary components: format reward and functional (accuracy) reward. The model is required to generate outputs in predefined structured format: <think> xxx </think> <answer> xxx </answer> The format reward assigns value of 1 if and only if the output strictly follows the required tag structure and the content within <answer> can be parsed as syntactically valid Python program. Otherwise the format reward is 0. The functional (accuracy) reward is +1 if the program inside <answer> executes successfully on the official hidden test inputs, terminates without runtime error, and produces outputs that exactly match the expected outputs for all test cases. Otherwise the accuracy reward is 0. The prompt used to condition the model for each problem is exactly: {Question} First output the thinking process in<think> </think> tags and then output the final code in <answer> </answer> tags. The answer should be complete Python code solution that solves the given problem. Make sure your code handles all edge cases and follows the input/output format specified in the problem. DONOT OUTPUT ANY CODE OR SOLUTION IN THE THINK TAGS. A.3.3 OBJECT DETECTION We conduct our experiments using the Agibot World dataset contributors (2024). The data is partitioned into training, validation, and test sets based on specific task ids from Agibot World dataset. Specifically, the training set is constructed from task ids 424, 480, and 507, comprising total of 3,000 images (randomly sampling). The validation and test sets are derived from task id 582 and 1352, respectively. For all images, the ground-truth bounding boxes and corresponding object labels are annotated through crowdsourcing process. The object detection model is trained for single epoch on the 3,000-image training set. After training, we perform model selection by evaluating checkpoints on the designated validation set. The model checkpoint that achieves the highest average IoU@0.5 (as defined below) on the validation data is selected for the final evaluation. The performance of this selected model is then reported on the test set. We evaluate the models performance using IoU rate metric, which measures the proportion of correctly localized objects based on the Intersection over Union (IoU). detection is considered positive if the IoU between the predicted bounding box (Bpred) and the ground-truth bounding box (Bgt) exceeds given threshold τ . The IoU rate at specific threshold τ , denoted as IoU@τ , is formulated as: IoU@τ = (cid:80)N pred, B(i) i=1 1(IoU(B(i) gt ) > τ ) (26) where is the total number of samples in the test set, and 1() is the indicator function. To provide comprehensive assessment, we report the performance across four different IoU thresholds: τ {0.5, 0.6, 0.7, 0.8}. The reward function is designed with two complementary components: format reward and an accuracy reward. The model is required to generate outputs in predefined structured format: <think> xxx </think> <answer> [d, d, d, d] </answer> 18 Preprint. where the bounding box is represented as list of four integers [d, d, d, d]. The format reward assigns value of 1 if and only if the output strictly follows the required format, and 0 otherwise. The accuracy reward is defined as the IoU between the predicted bounding box and the ground-truth bounding box. The full prompt template is shown below: {Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Only output the bounding box using [x min, min, max, max] format in the final answer. DO NOT OUTPUT ANY ANSWER OR CONCLUSION IN THE THINK TAGS. Output the final answer in List format. A.3.4 AFFORDANCE PREDICTION The task is defined as affordance prediction, where the model, given an image and specified affordance (e.g., grasping, holding), is required to predict pixel-wise mask indicating the corresponding region. We primarily use the UMD Part Affordance Dataset Myers et al. (2015). The official training split of this dataset is used to construct our training and validation sets. Specifically, we use 3,000 images for training and held-out portion of the original training split for validation. For evaluation, we use the official test split of the UMD dataset. To further assess the models generalization capabilities, we also use the entire AGD20K dataset Luo et al. (2022) as an additional, challenging test set. The affordance prediction model is trained for single epoch on the 3,000-image training set. After training, we perform model selection by evaluating checkpoints on the designated validation set. The model checkpoint that achieves the highest Success Rate (as defined below) on the validation data is selected for the final evaluation. The performance of this selected model is then reported on the test sets (UMD test and AGD20K). We evaluate the models performance using Success Rate metric. This metric measures the proportion of samples where the predicted point correctly falls within the ground-truth affordance mask. prediction is considered successful if the pixel value at the predicted 2D coordinate is 1 in the ground-truth binary mask. The Success Rate is formulated as: (cid:80)N Success Rate = gt (C (i) i=1 1(M (i) pred is the predicted 2D coordinate (x, y) is the corresponding ground-truth affordance mask. The notation pred) represents the value of the mask at the predicted coordinate. 1() is the indicator funcwhere is the total number of samples in the test set, (i) for the i-th sample, and (i) gt (i) tion, which is 1 if the condition is true and 0 otherwise. pred) = 1) gt (C (i) (27) The reward function consists of two complementary components: format reward and an accuracy reward. The model is required to generate outputs in the following structured format: <think> xxx </think> <answer> [d, d] </answer> where the final answer corresponds to 2D coordinate [d, d], with denoting an integer. The format reward assigns value of 1 if and only if the output strictly adheres to this format; otherwise, it is set to 0. The accuracy reward evaluates the correctness of the prediction by checking whether the predicted 2D point lies within the ground-truth affordance mask (i.e., region where the mask value equals 1). If the prediction falls inside the valid region, +1 reward is given; otherwise, it is not. The full prompt template is shown below: {Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> 19 Preprint. </answer> tags. y] format. THINK TAGS. Only output one affordance point using [x, DO NOT OUTPUT ANY ANSWER OR CONCLUSION IN THE A.3.5 TRAJECTORY PREDICTION The task is defined as trajectory prediction, where the model, given an image and manipulation instruction, is required to predict the two-dimensional trajectory of the robotic arms end-effector in the images pixel coordinate system. The trajectory is represented as sequence of coordinates, and the predicted path should follow the ground-truth trajectory to successfully complete the instructed manipulation. We primarily use the trajectory subset of the BAAI ShareRobot dataset Ji et al. (2025). The original dataset is partitioned into training, validation, and test sets. Specifically, we use 3,000 images for training, held-out portion of the training split for validation, and the test split for evaluation. The model is trained for single epoch on the 3,000-image training set. After training, we perform model selection by evaluating checkpoints on the designated validation set. The checkpoint that achieves the highest reward value (as defined below) on the validation data is selected for the final evaluation. The performance of this selected model is then reported on the held-out test set. We evaluate the models performance using multiple geometric similarity metrics, following the design in ManipVLM-R1 Song et al. (2025). These metrics measure how well the predicted trajectory matches the ground truth from different perspectives. Specifically, we use Discrete Frechet Distance (DFD), Hausdorff Distance (HD), Root Mean Square Error (RMSE), and Endpoint Distance as evaluation criteria. The model is required to generate outputs in the following structured format: <think> xxx </think> <answer> [[x1, y1], [x2, y2], ..., [xn, yn]] </answer> where the final answer corresponds to variable-length sequence of 2D coordinates [x, y], with and denoting integers. The reward function consists of two complementary components: format rewardand accuracy reward. The format reward assigns value of 1 if and only if the output strictly adheres to this format; otherwise, it is set to 0. To measure how well the predicted trajectory ˆT matches the groundtruth trajectory , we adopt an accuracy reward following the design in ManipVLM-R1 Song et al. (2025). Specifically, the reward is defined as Racc = exp (cid:0) DDFD( ˆT , )(cid:1) + exp (cid:0) DHD( ˆT , )(cid:1) + exp (cid:0) DRMSE( ˆT , )(cid:1) + exp (cid:0) ˆpN 2(cid:1), (28) where DDFD, DHD, and DRMSE denote the Discrete Frechet Distance, Hausdorff Distance, and Root Mean Square Error between the predicted trajectory ˆT and the ground-truth trajectory . The final term enforces endpoint accuracy by penalizing the distance between the predicted endpoint ˆpN and the ground-truth endpoint . The model is guided by carefully designed prompt that specifies both the reasoning and the answer requirements. The full prompt template is shown below: {Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. JSON format: each coordinate pair represents point in the images pixel space and the center of the end effector needs to follow the coordinates to complete the task. Each hand trajectory includes unknown number of [x, y] coordinate pairs.DO NOT OUTPUT ANY ANSWER OR CONCLUSION IN THE THINK TAGS. Output the final answer in the following Where [[x1, y1], [x2, y2], ..., [xn, yn]]. 20 Preprint. A.3.6 DEMAND PREDICTION The task is defined as demand prediction, where the model, given an image and human demand instruction (e.g., am thirsty), is required to output two-dimensional coordinate corresponding to an object in the image that fulfills the demand (e.g., water bottle or juice box). prediction is considered correct if the predicted point lies inside the ground-truth segmentation mask of the demanded object. We construct the dataset for this task based on MO-DDN Wang et al. (2024), which requires robots to ground natural demand instruction to objects in the environment. MO-DDN itself is built upon the HSSD scene dataset Khanna et al. (2024), together with custom demandobject dataset. To build our data, we randomly sample demand instruction and pair it with scene containing target object that satisfies the demand. We then crop and store the corresponding image, resulting in instructionimage pairs. Following the original MO-DDN splits, we collect data separately from the training and testing tasks. Specifically, we use 3,000 instructionimage pairs as the training set and 1,000 pairs as the validation set, both sampled from the training tasks. For evaluation, we construct test set of 5,000 instructionimage pairs sampled from thetesting tasks. We train the model for single epoch on the training set and perform model selection based on validation accuracy. The checkpoint achieving the highest validation performance is then used for testing, and we report results on the test set. We evaluate the models performance using Success Rate metric, defined as the proportion of samples where the predicted coordinate falls within the ground-truth mask of the demanded object. Formally: Success Rate = (cid:80)N gt (C (i) i=1 1(M (i) pred) = 1) , (29) where is the number of samples in the test set, (i) for the i-th sample, and (i) gt (i) equals 1 if the condition holds and 0 otherwise. pred denotes the predicted 2D coordinate (x, y) is the ground-truth binary mask of the demanded object. The notation pred) indicates the mask value at the predicted location. 1() is the indicator function that gt (C (i) The reward function for training consists of two complementary components: format reward and an accuracy reward. The model must output predictions in the following structured format: <think> xxx </think> <answer> [d, d] </answer> where the final answer corresponds to 2D coordinate [d, d], with denoting an integer. The format reward is assigned 1 if the output strictly follows this structure, and 0 otherwise. The accuracy reward is assigned if and only if the predicted coordinate lies within the ground-truth object mask. These two rewards jointly ensure syntactically valid outputs and semantic correctness. The model is guided by prompt template that specifies both the thinking process and the final answer format. The full prompt is given below: You are completing navigation task where you need to detect objects from the image that fulfill users demand. The users demand is {Question}. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. using [x, y] format that represents the target demanded object. TAGS. DO NOT OUTPUT ANY ANSWER OR CONCLUSION IN THE THINK Only output one point 21 Preprint. A.3.7 OCR-BASED VQA The task is defined as OCR-based Visual Question Answering (VQA), where the model, given an image containing textual information and natural language question, is required to output short natural language answer. The answer must be grounded in the image content and can involve both text extraction and reasoning over visual elements. We construct the dataset by combining three OCR-based VQA benchmarks: Document VQA Tito et al. (2021), Infographics VQA Tito et al. (2021), and Scene Text VQA Biten et al. (2019). Document VQA focuses on answering questions asked over document images, which may contain printed, typewritten, and handwritten content (e.g., letters, memos, reports). The answers are typically text spans taken verbatim from the document. Infographics VQA considers questions over infographic images containing charts, diagrams, or other structured visual data, where answers are not always explicitly extracted text but can include inferred information. Scene Text VQA consists of natural scene images with embedded text (e.g., storefronts, street signs). The model must jointly leverage OCR reading and visual understanding to answer the questions. From each of the three training sets, we randomly select 3,000 samples, resulting in combined training set of 9,000 samples. Additionally, we construct validation set of 1,500 samples (also drawn from the training splits), while the official validation sets of each benchmark are used as our test set. The model is trained for single epoch on the 9,000-sample mixed training set. Model selection is performed based on validation performance, and the checkpoint achieving the highest validation score is reported on the test sets. The evaluation metric is the Average Normalized Levenshtein Similarity (ANLS), which measures the string-level similarity between the predicted and ground-truth answers. ANLS accounts for OCR errors by softly penalizing recognition mistakes. threshold τ = 0.5 is applied to determine whether predicted answer is considered valid. Formally, ANLS is defined as: ANLS = 1 (cid:32) (cid:88) i=0 max (cid:33) , s(aij, oqi ) s(aij, oqi) = (cid:26)1 L(aij, oqi), 0, if L(aij, oqi) < τ, if L(aij, oqi) τ, (30) (31) where is the number of questions, is the number of ground-truth answers per question, aij is the j-th ground-truth answer for the i-th question qi, and oqi is the predicted answer. L() denotes the normalized Levenshtein distance. The reward function consists of format reward and an accuracy reward. The model must output answers in the following structured format: <think> xxx </think> <answer> xxx </answer> The format reward is 1 if the output strictly follows this structure, and 0 otherwise. The accuracy reward corresponds to the ANLS score of the predicted answer for the current question. The model is guided by the following prompt template: {Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. text. OUTPUT ANY ANSWER OR CONCLUSION IN THE THINK TAGS. The answer should be found in the image. The answer should be natural language DO NOT A.3.8 SIMULATOR-BASED VISUAL MANIPULATION The task is defined as simulator-based visual manipulation problem where, given single RGB observation of manipulation scene, the model must specify contact point (x, y) on the object at 22 Preprint. which sucker should attempt to manipulate. The models output must be grounded in the visual observation and may require reasoning about object geometry, affordances, and reachable contact locations. We construct the dataset and evaluation splits based on the PartNet Mobility dataset Xiang et al. (2020) and the ManipLLM experimental setup (A crucial point is that we have followed their setting by using suckers as the end effectors for the robotic arms.). For training, we adopt the same 20 training categories as ManipLLM, consisting of 1,043 object instances. Training scenes are generated following the SAPIEN simulator Xiang et al. (2020) setup and ManipLLM scene configurations. For testing, we use the open-sourced ManipLLM test set, which contains approximately 1,830 successful test samples spanning both Seen and Unseen objects. To better evaluate model generalization to novel viewpoints, we further construct camera-perturbed test set by modifying each test sample: the camera orientation vector [0, 0, 0] is replaced by [x, y, z] where each of x, y, is sampled uniformly from the signed interval [0.2, 0.6]. This perturbation preserves other scene properties while intentionally stressing viewpoint robustness. In order to simplify control and isolate contact selection, the sucker approach direction in all experiments is fixed to be the surface normal at the chosen manipulation point (x, y). The required output must follow strict format consisting of reasoning trace and final contact point, written as: <think> xxxx </think> <answer> (d,d) </answer> The evaluation metric is Success Rate, following ManipLLMs criterion based on the manipulated objects displacement after the scripted sucker motion. Formally, given trials, SuccessRate = 1 (cid:88) i=1 1{triali is successful according to ManipLLMs displacement criterion}, where 1{} is the indicator function. We report Success Rate on the camera-perturbed test sets, and further provide breakdowns by Seen vs. Unseen objects. The reward function during GRPO training consists of format reward and task reward. The format reward is 1 if the output strictly follows the required structure and 0 otherwise. The task reward is 1 if the manipulation attempt succeeds according to ManipLLMs displacement criterion and 0 otherwise. The overall reward is defined as Rtotal = Rformat + Rtask, so that only properly formatted and successful outputs receive credit. This ensures that malformed answers cannot be rewarded even if the manipulation itself succeeds. All experiments are conducted with Qwen2.5-VL-3B as the base model. We train using GRPO for 4,000 optimization steps, selecting checkpoints based on validation success rate. The validation set is constructed by sampling held-out scenes from the same 20 training categories without overlap with the test split. The prompt used in training is as follows: \"system\": \"You are an intelligent manipulator. The user conversation between User and Assistant. asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e. <think> reasoning process here </think><answer> answer here </answer>.\" \"user\": \"Specify the contact point (x, y) of manipulating the object. is:width: 336, height: format: <think>your thinking process</think> <answer>(x,y)</answer>\" 336, Output The camera resolution 23 Preprint. Table 5: Hyperparameters for GRPO training. Hyperparameter Group Parameter Value Training Configuration Model Optimizer Learning Rate (η) Batch Size Gradient Accumulation Steps Total Training Epochs Max Completion Length Data Seed Floating Point Precision Gradient Checkpointing Flash Attention 2 Qwen2.5-VL-3B-Instruct AdamW 1 105 1 1 1 4096 42 bfloat16 true true PEFT (LoRA) Configuration LoRA Rank (r) LoRA Alpha (α) LoRA Dropout GRPO-specific Configuration Beta (β) Epsilon High (ϵH ) Epsilon Low (ϵL) Model Specific Configuration Freeze Vision Modules 64 128 0.05 0.04 0.28 0.2 true A.4 DETAILS IN TRAINING A.4.1 TRAINING HYPERPARAMETERS We summarize the key hyperparameters used in our GRPO training experiments in Tab. 5. The settings are organized into general, training, and LoRA-related categories for clarity. A.4.2 SFT DETAILS For all Supervised Fine-Tuning (SFT) baselines, we train for 5 epochs. All other settings are kept consistent with GRPO, including the dataset, model selection criteria, and metric calculation. A.4.3 GENERATION CONFIGURE Our model is trained using the Hugging Face transformers library (version 4.51.3). During inference, we customize the decoding strategy via the GenerationConfig class. Specifically, we set temperature=1.0 and do sample=True to enable stochastic sampling. We also define stop strings=[\"</think>\", \"</analysis>\"] only when generating thoughts. The remaining parameters are maintained at their default settings. A.5 MORE CASE STUDY AND VISUALIZATION To provide more intuitive and in-depth analysis of our models performance, this section presents series of curated case studies and visualizations. These examples encompass range of key tasks, including object detection  (Fig. 4)  and trajectory prediction (Fig. 5 and Fig. 6). Our aim is to leverage these concrete scenarios to delve into the models behavior, decision-making logic, and inherent strengths and limitations. Specifically, in the simulator-based visual manipulation task, we visualize the distribution of the target operation points over multiple sampling attempts in Fig. 7. Green points indicate successful manipulations, while red points represent failures. This visualization demonstrates the robustness of our model. 24 Preprint. Table 6: NoZeroRate on Different Task. TN: The number of thoughts; AN: The number of answers per thought. Bold indicates the best performance and italics indicate the second-best performance."
        },
        {
            "title": "Sim Manip",
            "content": "GRPO GRPO GRPO GRPO-MA 4 8 16 4 1 1 1 4 46.37% 26.71% 19.14% 36.57% 27.71% 62.07% 43.71% 43.29% 97.17% 70.20% 66.47% 41.86% 34.71% 85.10% 94.37% 96.70% 38.20% / / 85.05% A.6 MORE EXPERIMENTAL ANALYSIS In this section, we further present some experimental results, including the accuracy reward curves during training, and an analysis of the richness of reward signal. A.6.1 ACCURACY REWARD CURVE We present the accuracy reward curves for five visual tasks in Object Detection  (Fig. 8)  , Affordance Prediction  (Fig. 9)  , Demand Prediction  (Fig. 10)  , OCR-based VQA  (Fig. 11)  and Trajectory Prediction  (Fig. 12)  . During the curve plotting process, we smooth the curve using moving average method with window size of 200. The curves demonstrate that T4A4 (red) exhibits performance comparable to that of T16A1 (blue) in the majority of cases, at times showing marginal advantage. A.6.2 RICHNESS OF REWARD SIGNALS For tasks with binary (0-1) rewards, such as Code, Math, Affordance Prediction, Demand Prediction and Simulator-based Visual Manipulation, we compute the proportion of samples whose total reward is positive, which we refer to as the NoZeroRate. Formally, it is defined as NoZeroRate = (cid:88) 1 t= 1 (cid:88) (cid:88) AccRt i,j i=1 j=1 > 0 , (32) where 1{} denotes the indicator function, which equals 1 if the condition inside holds and 0 otherwise. Here, is the total number of time steps, indexes specific time step, is the number of thoughts, is the number of answers per thought, and AccRt i,j denotes the accuracy reward associated with the j-th answer under the i-th thought at time step t. higher NoZeroRate indicates lower proportion of advantage collapses (where collapse means all advantage values become zero), and higher proportion of effective gradient information contribution. The statistical results are presented in Tab. 6. We observe that T4A4 achieves the second-highest proportion of non-zero accuracy rewards across all tasks, only behind T16A1. On the one hand, this indicates that under the T4A4 setting, the answers generated by each thought are largely different. On the other hand, it suggests that the diversity of generated answers can be substantially improved by generating additional answers per thought, as shown by the comparison between T4A4 and T4A1. A.7 USAGE OF LLMS We employ Large Language Model (LLM) to refine the manuscript, with focus on correcting grammatical errors and enhancing overall readability. 25 Preprint. Figure 4: Case Study on Object Detection Green text indicates key reasoning content. Figure 5: Case Study on Trajectory Prediction Green text indicates key reasoning content. Figure 6: Case Study on Trajectory Prediction Green text indicates key reasoning content. 26 Preprint. Figure 7: Visualization on Simulator-based Visual Manipulation Red dots indicate failures, while green dots represent successes. We can observe that most GRPO-MA-T4A4 points are located on the object. In contrast, GRPO-T4A1 frequently misses the object, resulting in lower success rate. Figure 8: Accuracy Reward Curve on Object Detection 27 Preprint. Figure 9: Accuracy Reward Curve on Affordance Prediction Figure 10: Accuracy Reward Curve on Demand Prediction 28 Preprint. Figure 11: Accuracy Reward Curve on OCR-based VQA Figure 12: Accuracy Reward Curve on Trajectory Prediction"
        }
    ],
    "affiliations": [
        "Agibot",
        "CFCS, School of Computer Science, Peking University",
        "PKU-Agibot Joint Lab",
        "University of Electronic Science and Technology of China"
    ]
}