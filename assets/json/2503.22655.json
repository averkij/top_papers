{
    "paper_title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
    "authors": [
        "Xiaomin Yu",
        "Pengxiang Ding",
        "Wenjie Zhang",
        "Siteng Huang",
        "Songyang Gao",
        "Chengwei Qin",
        "Kejian Wu",
        "Zhaoxin Fan",
        "Ziyue Qiao",
        "Donglin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git."
        },
        {
            "title": "Start",
            "content": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training Xiaomin Yu 1 * Pengxiang Ding 2 3 * Wenjie Zhang 2 Siteng Huang 2 3 Songyang Gao 4 Chengwei Qin 5 Kejian Wu 1 Zhaoxin Fan 6 Ziyue Qiao 7 Donglin Wang"
        },
        {
            "title": "Abstract",
            "content": "Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can highquality multimodal training data be synthesized purely from text? To tackle this, we propose cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds usIn Stage ing large language models (LLMs). 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instructiontuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers cost-effective and scalable solution for VLMs training. Code is available at https: //github.com/Yu-xm/Unicorn.git. 5 2 0 2 8 2 ] . [ 1 5 5 6 2 2 . 3 0 5 2 : r 1. Introduction The rapid advancement of VLMs (Liu et al., 2023; Zhu et al., 2023; Liu et al., 2024c) has underscored the critical importance of large-scale, high-quality image-text pair train1Xreal 2Westlake University 3Zhejiang University 4Shanghai AI Lab 5Nanyang Technological University 6Beihang University 7The Great Bay University. Correspondence to: Xiaomin Yu <xmyu02@gmail.com>. Figure 1. Unlike traditional image-text data synthesis frameworks, Unicorn removes the dependency on real image data, offering more efficient and scalable solution by cutting down API costs, synthesis time, and storage requirements. ing data (Chen et al., 2023; Zhao et al., 2024; Bai et al., 2024). However, scaling up such high-quality data remains an enduring challenge, constraining the potential for further breakthroughs in VLMs. Traditional methods for acquiring image-text pairs primarily rely on two strategies: (1) Manual annotation (Lin et al., 2015; Plummer et al., 2016). Manual annotation ensures quality but is constrained by cost, scale, and diversity. (2) Large-scale web crawling (Sharma et al., 2018) is scalable but introduces issues such as inconsistent data quality, compliance risks, and copyright concerns. Recent works, such as ShareGPT4V (Chen et al., 2023), have explored augmenting real images with fine-grained captions generated by advanced models like GPT-4v, resulting in highquality datasets that achieve notable improvements in visuallanguage tasks. However, such synthetic data methods still face significant cost constraints. In contrast, text data (Wu et al., 2024; Long et al., 2024) is abundant, inexpensive, and diversified, presenting an untapped opportunity. This raises fundamental question: Can we synthesize high1 Unicorn: Text-Only Data Synthesis for Vision Language Model Training quality multimodal training data for VLMs using only text, without relying on real/synthetic images? The answer is affirmative: recent studies have revealed that cross-modality representation transfer can be achieved in training-free way (Zhang et al., 2024) by exploiting the geometric structure of multimodal contrastive representation spaces (Liang et al., 2022; Zhang et al., 2023). While this phenomenon has been explored in small-scale tasks such as image captioning (Tewel et al., 2022; Li et al., 2023b) and visual question answering (Gu et al., 2023; Liu et al., 2024e), its potential for generating large-scale synthetic data has yet to be fully realized. Building on this insight, as shown in Fig. 1, we propose scalable multimodal synthetic data framework, termed Unicorn, that mitigates the modality gap (Liang et al., 2022; Zhang et al., 2023) to eliminate the dependency on real images. By utilizing the geometry of the shared representation space (Zhang et al., 2024), Unicorn generates high-quality synthetic pretraining and instruction-tuning data directly from text. Unicorn not only significantly lowers costs but also enables efficient construction of diverse, high-quality VLM training datasets, providing novel large-scale data synthesis method for VLM training. Unicorn adopts cross-integrated three-stage pipeline to construct diverse, high-quality multimodal datasets. Importantly, the first two stages synthesize data entirely within the textual modality, while the final stage performs the critical transfer from text representation space to visual representation space. This process produces two key datasets: Unicorn-1.2M for multimodal pretraining and Unicorn471K-Instruction for instruction-tuning. Specifically, in Stage 1: Diverse Caption Data Synthesis, Qwen2.5-72BInstruction (Yang et al., 2024) is used to add detailed information to 1.2M sparse caption seeds. The process ultimately yields 1.2M textual diverse captions. These diverse captions include both open-domain (Lin et al., 2015; Plummer et al., 2016; Sharma et al., 2018; Ordonez et al., 2011) and domain-specific content (Horn et al., 2021; Kaur et al., 2019), as the foundation for subsequent stages. In Stage 2: Instruction-Tuning Data Generation, 471K captions from Stage 1 are utilized with Qwen2.5-72B-Instruction (Yang et al., 2024) to generate instruction-tuning data encompassing three tasks: multiple-choice, question-answering, and complex reasoning. Finally, in Stage 3: Modality Representation Transfer, we transfer (Liang et al., 2022; Zhang et al., 2024) the diverse caption representations encoded by LLM2CLIP (Huang et al., 2024) into the visual representation space, obtaining synthetic image representations. Ultimately, we synthesize two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instructiontuning. Overall, Unicorn enables cost-efficient, scalable, and high-quality multimodal dataset generation without relying on real images. Using the synthetic datasets Unicorn-1.2M and Unicorn471K-Instruction, we train VLM, named Unicorn-8B, entirely without relying on real images. Experimental results demonstrate that, across multiple benchmark tests, the Unicorn-8B trained on text-only synthetic datasets achieves performance comparable to traditional methods that utilize image-text pairs. Our contributions can be summarized as: We propose Unicorn, three-stage text-only data synthesis framework for VLMs training. We synthesize the Unicorn-1.2M dataset for VLM pretraining and the Unicorn-471K-Instruction dataset for instruction fine-tuning, entirely without relying on real images. We train Unicorn-8B, cost-efficient VLM, that achieves competitive performance without relying on real images. 2. Backgrounds 2.1. Image-Text Data Synthesis In recent years, VLMs (Liu et al., 2023; Zhu et al., 2023) have made remarkable progress in integrating visual and textual information to address complex tasks. However, most approaches rely heavily on large-scale image-text pairs for modality alignment pretraining, which face challenges such as high costs, inconsistent quality, and limited diversity. To overcome these limitations, multimodal synthetic data generation has become key strategy for improving VLMs performance. Early methods, such as LLaVA (Liu et al., 2023), reconstruct existing image-text data to create instruction-tuning datasets but rely on extensive human annotations, often suffering from variable quality and scalability issues. Additionally, some works, like ShareGPT4V (Chen et al., 2023), leverage advanced vision-language models like GPT-4V for automated annotation to produce high-quality captions but depend on expensive closed-source models, making them cost-prohibitive. Furthermore, synthetic image captions often suffer from hallucinations (Huang et al., 2025; Li et al., 2023c), undermining data reliability. While these methods have advanced the field, no existing approach provides text-only solution for generating multimodal training data. To address this gap, we propose the first framework that synthesizes highquality multimodal datasets exclusively from text, offering scalable and cost-effective solution for training MLLMs without relying on real or generated images. 2.2. Modality Gap The modality gap refers to the distributional difference between representations of paired data from distinct modalities (e.g., image and text) within shared representation space, 2 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Figure 2. Unicorns text-only data synthesis pipeline, comprising three cross-integrated stages, (1) Diverse Caption Data Synthesis, (2) Instruction-Tuning Data Synthesis, and (3) Modality Presentation Transfer, ultimately yields two synthetic datasets: Unicorn-1.2M and Unicorn-471K-Instruction. Notably, unlike traditional approaches that rely on real images, Unicorn generates image representations paired with text, entirely free of real image data. despite the theoretical expectation of identical distributional properties (Radford et al., 2021; Zhai et al., 2023; Huang et al., 2024). Empirical studies have shown that this issue arises from the interplay between dimensional collapse during model initialization and collapsed gradients during optimization, which together inhibit the complete alignment of cross-modal representations (Liang et al., 2022; Zhang et al., 2023; 2024). Recent research has explored the geometric structure of the modality gap, demonstrating that for paired inputs (e.g., image-text pairs), their representations ex and ey satisfy the relationship (Zhang et al., 2024): ex ey = + ϵ, (1) where is constant orthogonal vector representing the modality gap, and ϵ denotes alignment noise approximated by Gaussian distribution. This theoretical insight has facilitated advancements in cross-modality tasks using unimodality data (Tewel et al., 2022; Gu et al., 2023; Li et al., 2023b; Liu et al., 2024e), providing foundation for addressing the modality gap. However, existing solutions largely rely on low-quality, small-scale downstream datasets, which fail to fully exploit the potential of large-scale synthetic data. We are the first to synthesize large-scale data for VLM training by mitigating the modality gap. 3. Data Synthesis Pipeline In this section, we introduce our data synthesis pipeline, which enables the creation of two datasets: Unicorn1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning. Specifically, the Unicorn framework synthesizes high-quality multimodal data through crossintegrated three-stage pipeline: Stage 1: Diverse Caption Data Synthesis, Stage 2: Instruction-Tuning Data Generation, and Stage 3: Modality Representation Transfer, as shown in Fig. 2. In the following, we detail each of these cross-integrated stages. 3.1. Seed Dataset To construct high-quality diverse captions, we first collect seed dataset with two main objectives: (1) to provide more diverse textual image scenarios for fine-grained alignment during VLMs pre-training, and (2) to incorporate domainspecific knowledge, enhancing the models understanding of specialized content. Importantly, we use only the textual captions from these datasets without involving any images. Specifically, given caption datasets {Dm}M m=1, where each Dm contains image captions {si}N i=1, we filter and merge the original textual captions from these datasets, resulting in seed dataset containing 1.2M high-quality samples, referred to as Unicorn-1.2M-Seed. This dataset is divided into two categories: Open-Domain Caption Seeds and Domain-Specific Knowledge Seeds. Open-Domain Caption Seeds. It includes two types of image caption datasets: manually annotated and web-crawled. Manually annotated caption datasets, such as Flickr30k (Plummer et al., 2016) and COCO Caption (Lin et al., 2015), provide basic information about image entities, events, and contexts. To ensure diversity, we retain only one caption per image. Web-crawled caption datasets, such as Conceptual Captions (Sharma et al., 2018), are sourced from large-scale web crawling, offering diverse semantic information. While these captions excel in scale and diversity, their quality is limited. To improve their usability, we filter and select captions that more closely resemble high-quality 3 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Figure 3. Data formats for the three instruction-tuning tasks. Each dialogues answer is derived from the textual content of diverse caption. image captions. Domain-Specific Knowledge Seeds. To inject domain knowledge, we collect 10K fine-grained image labels from six fine-grained image classification datasets across 18 domains. For each dataset, we design specialized prompt templates. For example, for the FoodX-251 (Kaur et al., 2019) dataset, we use templates like An image of delicious <label>, where <label> is dynamically replaced with the actual label. Each label is paired with 10 randomly selected templates. This approach generates 107K domainspecific knowledge seeds, enriching the dataset with domain knowledge. 72B-Instruction (Yang et al., 2024) Φ to enrich each si with additional details, generating the final textual diverse caption ci = Φ(si). For each si in the Unicorn-1.2M-Seed dataset, we assume that the generated diverse caption ci corresponds to unique imaginary image. Ultimately, based on Unicorn-1.2M-Seed, we have constructed 1.2M high-quality diverse captions set C. serves three functions: (1) synthesizing instruction-tuning data in Stage 2, (2) transferring to the visual representation space after encoding in Stage 3 to generate image representations with identical semantics, and (3) constructing the Unicorn-1.2M pretraining dataset. 3.2. Stage 1. Diverse Caption Data Synthesis Building on the previously introduced seed dataset, we aim to further expand its semantic richness and diversity to enhance the quality of synthetic data. To justify this expansion, we propose critical assumption: when additional details are continuously added to an initial textual caption seed, the set of imaginary images that correspond to it in semantic space will progressively shrink, to the point where it can be approximately regarded as corresponding to single imaginary image that does not actually exist. In other words, even without requiring real image or comparing it to any photographic or drawn image, simply by incrementally enriching the textual details, the description can increasingly approach uniquely defined virtual visual entity. Now, we move on to the details of the diverse caption generation process. Consider the Unicorn-1.2M-Seed dataset = {s1, s2, . . . , sN }, for each si, we employ the Qwen2.53.3. Stage 2. Instruction-Tuning Data Synthesis With the construction of 1.2M textual diverse captions in Stage 1, we move to Stage 2: Instruction-Tuning Data Generation. At this stage, we generate 471K instruction-tuning text samples designed to enhance the instruction-following and reasoning capabilities of VLMs. Notable, all the data generated at this stage is entirely in textual form. To accomplish this, we design three instruction-tuning tasks: (1) Multiple Choice. This task requires generating set of multiple-choice questions based on the given diverse caption ci. Each question has four options (A, B, C, D), with only one correct answer. (2) Question-Answering. This task involves generating series of open-ended questions and answers based on the image description. Each question has brief answer. (3) Complex Reasoning. This task requires generating complex reasoning questions, where the answers typically require multi-step reasoning or hypothetical deductions. 4 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Figure 4. Training and inference processes of Unicorn-8B. Training aligns synthetic image representations with LLM embeddings, while inference adjusts real image embeddings as input. For these tasks, we sample 171K, 199K, and 100K captions from the diverse caption set C. For each selected caption ci, it is inserted into task-specific template to serve as prompt for the Qwen2.5-72B-Instruction (Yang et al., 2024), thereby generating the instruction-tuned text sample ti corresponding to ci. As shown in Fig. 3, the resulting questions and answers are derived from the original diverse caption, ensuring consistency of content. This method effectively avoids the hallucination phenomena (Li et al., 2023c) present in the traditional image-text synthesis data process. Finally, we synthesize total of 471K high-quality instruction-tuning data set = {t1, t2, ..., ti}. 3.4. Stage 3. Modality Presentation Transfer After generating diverse caption set and instructiontuning data set in the previous stages, we synthesize image representations. Unlike traditional approaches that directly generate synthetic images, we adopt strategy based on modality gap theory (Liang et al., 2022; Zhang et al., 2024): synthesizing image embeddings within the shared representation space. This modality gap theorybased approach ensures the feasibility of generating meaningful image representations without the need for real images. We refer to this as Modality Representation Transfer. For the diverse caption set = {c1, c2, . . . , cN }, where each diverse caption ci is encoded by the text encoder of LLM2CLIP (Huang et al., 2024) to produce representation ui Rd. The set of all text representations is denoted by = {u1, u2, . . . , uN }. Unlike traditional models such as CLIP (Radford et al., 2021) or SigLIP (Zhai et al., 2023), which are constrained by fixed input sequence length, LLM2CLIP (Huang et al., 2024) is capable of processing long-form texts and complex semantic content through its extended input window, while also injecting open-world knowledge. Although the text representations in the shared representation space approximate the distribution of image representations , the modality gap phenomenon prevents directly using text representations to replace image representations with the same semantics. To synthesize image representations ˆvi that better approximate real image embeddings, we propose the following transformation: ˆvi = ui E[U ], (2) where ˆvi is the synthetic image representation for the paired diverse caption ci, and the subtraction of E[U ] adjusts for the global bias introduced by modality-specific embedding distributions. This operation ensures that the synthetic embeddings ˆvi are more closely aligned with the distribution of real image representations in the shared representation space H. Please refer to the Appendix for an expanded explanation. 3.5. Synthetic Dataset After three stages, we synthesize 1.2M Diverse Captions, 471K instruction-tuning data, and 1.2M synthetic image representations. Unicorn-1.2M. For each Diverse Caption ci, we pair it with semantically equivalent synthetic image representation ˆvi, forming pretraining dataset of 1.2M synthetic data pairs. This dataset is referred to as Unicorn-1.2M. Unicorn-471K-Instruction. For each instruction-tuning data ti, we pair it with the corresponding synthetic image representation ˆvi, creating an instruction-tuning dataset consisting of 471K synthetic data pairs. This dataset is referred to as Unicorn-471K-Instruction. 4. Unicorn-8B Based on the two synthetic datasets, we train VLM referred to as Unicorn-8B. This section presents the Unicorn-8Bs architecture and details of its training and inference processes. Fig. 4 shows the relevant details. 5 Unicorn: Text-Only Data Synthesis for Vision Language Model Training 4.1. Architecture Unicorn-8B is composed of two components: the projector and the backbone LLM fφ. For the projector , we adopt multilayer perceptron (MLP) architecture equipped with GELU activation functions, following the design in LLaVA (Liu et al., 2023). This component serves as modality adapter, effectively bridging the input representations to the latent space of the LLM. We utilize LLaMA3-8B as the backbone LLM. 4.2. Training & Inference Training. In the pretraining stage, synthetic image representations ˆvi are aligned with pre-trained LLM representation Hq through the projection matrix . During this phase, only is updated, while the weights of the LLM fφ remain frozen. Subsequently, in the instruction-tuning stage, the Unicorn-471K-Instruction dataset is employed to update both and the LLM fφ weights, thereby enhancing the models instruction-following capability. Inference. In the inference stage, real images from the test set are decoded by the LLM2CLIP (Huang et al., 2024) image encoder to generate real image representations. To mitigate the modality gap, the global mean of the test image representation set E[V ] is subtracted from each input test image representation vi Rh, resulting in transformed representation vi = vi E[V ]. 5. Experiments 5.1. Synthetic Data Quality Evaluation In this section, we evaluate Unicorn-1.2M and Unicorn471K-Instruction from three perspectives: Cost Analysis, Length Distribution, and Diversity. Cost Analysis. As shown in Table 1, we compare the cost efficiency of two synthetic datasets, ShareGPT4V and Unicorn-1.2M, across three dimensions: API call costs, time, and storage. The API call cost per sample for ShareGPT4V is $0.00684, while Unicorn-1.2M achieves much lower cost of $0.0003only 4% of the former. In terms of data generation time and storage, ShareGPT4V requires 44 days and 109 GB, whereas Unicorn-1.2M reduces this to 12 days and 4 GB, representing reductions of 73% in time and 96% in storage. These findings highlight Unicorn-1.2Ms significant advantages in cost reduction, faster data generation, and lower storage requirements, making it highly feasible and cost-effective for large-scale synthetic data applications. Length Distribution. In Fig. 5, we compare the length distributions of pre-training text datasets used of Unicorn and ShareGPT4V. Both datasets contain over 1.2 million samples (1,246,901 for Unicorn-1.2M and 1,243,195 for 6 Table 1. Comparison of Unicorn and ShareGPT4V in API, Time, and Storage costs. Cost API/1e3$ Time/Day Storage/GB ShareGPT4v Unicorn 6.84 0.36.54 44 1232 109 4 Figure 5. Comparison of the data length distributions between Unicorn-1.2M and ShareGPT4V. ShareGPT4V). Unlike ShareGPT4V, Unicorn-1.2M exhibits near-normal length distribution, better reflecting realworld data. The shortest sample in Unicorn-1.2M contains 51 words, compared to just 9 in ShareGPT4V, indicating richer semantic content. Despite this, both datasets show similar maximum lengths (507 for ShareGPT4V, 483 for Unicorn-1.2M) and comparable averages (144 vs. 129). This demonstrates that Unicorn effectively captures the complexity of image annotations, providing high-quality, diverse data for model training. Diversity Analysis. To systematically assess the diversity of the generated datasets, as shown in Table 2, we employed two diversity metrics, Type-Token Ratio (TTR) and Entropy, to evaluate both Unicorn-1.2M and ShareGPT4V. Our results unequivocally show that, at comparable data scale, Unicorn-1.2M achieves substantially higher level of diversity relative to ShareGPT4V. This finding reinforces Unicorn-1.2Ms capacity for synthesizing rich and varied data, which is primarily attributed to the open-domain diversity and extensive domain knowledge embedded in Unicorn1.2M-Seed. In effect, this indicates that broad coverage of domain knowledge can play crucial role in enhancing the diversity of synthetically generated datasets. Table 2. Comparison of Unicorn-1.2M and ShareGPT4V in data diversity. Dataset Scale TTR Entropy 1246901 ShareGPT-4v Unicorn-1.2M 1243195 0.61 0.68 5.91 6.07 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Table 3. Comparison of Unicorn-8B with state-of-the-art VLMs across four benchmarks. MMEC (Fu et al., 2024): MME Cognition, MMBD(Liu et al., 2024d): MMBench Development Split, SQAI (Lu et al., 2022): ScienceQA-IMG Test Split, and MM-Vet (Yu et al., 2024). : trained on real image-text pairs; : partially synthetic data. Model Scale Text-only Synthesis MMEC MMBD SQAI MM-Vet BLIP2-14B (Li et al., 2023a) MiniGPT-4-14B (Zhu et al., 2023) BLIP-2-Flan-T5-XXL (Li et al., 2023a) InstructBLIP-Flan-T5-XXL (Dai et al., 2023) BLIP-2-Vicuna-13B (Li et al., 2023a) SVIT-v1.5-13B (Zhao et al., 2023) LVIS-INSTRUCT4V-13B (Wang et al., 2023) ShareGPT4V-13B (Chen et al., 2023) LLaVA-v1.5-13B (LoRA) (Liu et al., 2024b) MiniGPT-4-8B (Zhu et al., 2023) InstructBLIP-Vicuna-7B (Dai et al., 2023) LLaVA-7B (Liu et al., 2023) SPHINX-Intern2 (Liu et al., 2024a) LLaVA-v1.5-7B (LoRA) (Liu et al., 2024b) >129M >5M >129M >15M >129M >5.8M >220K >1.2M >1.2M >5M >15M >1.2M >15.2M >1.2M Unicorn-8B w/o. Transfer Unicorn-8B 1.2M + 471K 1.2M + 471K 290.0 291.8 323.2 286.8 303.2 300.4 294.6 267.9 256. 37.5 68.0 68.5 68.5 32.7 36.0 38.7 57.9 66.1 52.4 61 64.5 70.6 61.0 70.0 70.6 71.2 71.2 60.5 70.4 68.4 69. 22.4 24.4 22.1 23.3 291.0+34.3 60.0+7.6 71.3+1.7 24.5+1.2 5.2. Quantitative VLM Performance Analysis To comprehensively evaluate the visual language understanding capabilities of the Unicorn-8B model, we conduct systematic experiments across five benchmarks: MMEC, MMBD, SQAI , POPE, and MM-Vet. The experiments are conducted under the zero shot setting. Unlike the traditional usage of \"zero shot,\" zero shot for Unicorn-1.2M refers to achieving zero-shot capability at the modality level by not using any images during training. Note our method enables the model to rely solely on synthetic text data for training, eliminating the need for real image-text pairs. The results, summarized in Table 3, highlight the effectiveness of our method. Despite being trained exclusively on synthetic pure text data, Unicorn-8B achieves performance comparable to or better than state-of-the-art VLMs trained on large-scale image-text datasets. Notably, on the challenging ScienceQA-IMG benchmark, Unicorn-8B achieves an accuracy of 71.3, surpassing the other baselines by 0.1. This remarkable result demonstrates the ability of our text-only training paradigm to capture complex multimodal reasoning capabilities. Furthermore, Unicorn-8B shows strong results across other benchmarks. For example, it achieves score of 75.0 on POPE and 24.5 on MM-Vet, further validating the robustness of our method. These results are particularly impressive considering that Unicorn-8B is trained on significantly smaller dataset (1.2M synthetic text samples and 471K additional data points) compared to competing models, which often rely on multimodal datasets containing millions or even billions of image-text pairs. In summary, Unicorn-8B demonstrates that purely textFigure 6. Performance of Unicorn-8B on the MMEC and ScienceQA benchmarks across different training data scales. based synthetic training methodology can achieve competitive or even superior performance to traditional multimodal training pipelines. This paradigm shift highlights the potential of scalable, text-only approaches in advancing visual language understanding, while significantly reducing the dependency on multimodal datasets. 5.3. Quantitative Ablation Study Impact of Modality Representation Transfer. Modality Representation Transfer is key component in addressing the modality gap by ensuring that synthesized image representations, derived from text-based training, align more closely with real image distributions. This alignment is critical for enabling the model to generalize effectively across multimodal tasks and achieve superior performance on benchmarks. As shown in Table 3, the importance of Modality Representation Transfer is evident when comparing Unicorn-8B to its variant without transfer (Unicorn-8B w/o. Transfer). The results show consistent and significant performance improvements across multiple benchmarks, including gains of 34.3 on MMEC, 7.6 on MMBD, 1.7 on SQAI , 2.9 on POPE, and 1.2 on MM-Vet. These im7 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Figure 7. Examples of Unicorn-8Bs correct responses on the ScienceQA and iNaturallist-VQA benchmarks. Table 4. Comparison of Unicorn-8B trained with text representations and synthetic image representations on the iNaturalist-VQA. Table 5. Performance of Unicorn-8B on the MMEP and GQA. LLaVA-NeXT-7B ShareGPT4V-13B Unicorn-8B Domain Plants Insects Birds iNaturalist-VQA w/o. Transfer Unicorn-8B 37.4 47.9 36.8 42.4 47.7 56.9 38.5 46.8 MMEP GQA 1519.0 64. 1618.7 64.8 841.0 25.2 provements highlight the effectiveness of this technique in reducing the modality gap and enhancing the models ability to reason across both visual and textual domains. Impact of Data Scale. We investigate the relationship between data scale and model performance by training the model on 5%, 30%, and 100% of the dataset. As shown in Figure 6, the models performance improves steadily as the data scale increases. For instance, training on 30% of the data already achieves competitive results, while using the full dataset further enhances performance. These findings validate the effectiveness of Unicorn and highlight the advantages of its low-cost scalability. By leveraging Unicorn-1.2Ms efficient data synthesis process, additional performance gains can be achieved simply by generating and incorporating more data, making the approach highly adaptable to larger scale applications. Ability of Fine-Grained Domain Knowledge Injection. To evaluate the injection of domain-specific knowledge, we construct iNaturalist-VQA, fine-grained VQA benchmark based on iNaturalist 21. It contains 10K images from 13 domains, with one image per fine-grained label. Each image has same question What is this image mainly about? and four answer choices, including one correct label and three distractors from the same domain. As shown in Table 4, Unicorn-8B achieves an overall score improvement of 8.3 after modality representation transfer. Notable gains include plants (37.4 47.9), insects (36.8 42.4), and birds (47.7 56.9), demonstrating its enhanced ability to incorporate fine-grained domain knowledge. 5.4. Qualitative VLM Performance Analysis As shown in Fig. 7, we provide qualitative results of Unicorn-8B on ScienceQA and iNaturalist-VQA benchmarks. ScienceQA features diverse range of questions across scientific domains, evaluating the models ability to reason and apply knowledge in fields such as physics, biology, and chemistry. In contrast, iNaturalist-VQA focuses on fine-grained visual question answering, requiring precise recognition of subtle differences within natural scenes. These results demonstrate Unicorn-8Bs strong performance across both general scientific reasoning tasks and domainspecific fine-grained visual understanding, highlighting its capability to adapt to diverse multimodal challenges. 6. Conclusion and Limitation Conclusion: In this work, we introduce novel threestage multimodal data synthesis framework that redefines the paradigm of vision-language model training by eliminating the dependency on real images. Leveraging the abundance of text-only data, we construct two high-quality datasets, Unicorn-1.2M for pretraining and Unicorn-471KInstruction for instruction-tuning, using advanced techniques for caption generation, instruction synthesis, and modality representation transfer. Our framework establishes scalable, cost-effective, and highly efficient approach to VLM training, while preserving data quality, diversity, and alignment across modalities. Limitation: Table 5 shows Unicorn-8Bs performance on 8 Unicorn: Text-Only Data Synthesis for Vision Language Model Training MMEP (Fu et al., 2024) and GQA (Hudson & Manning, 2019), scoring 841.0 and 25.2, respectivelysignificantly lower than LLaVA-NeXT-7B (Li et al., 2024) and ShareGPT4V-13B (Chen et al., 2023). The main reasons are: (1) There is noise between synthetic image representations used in training and real image representations, hindering the VLMs ability to handle fine-grained visual tasks like positional relationship and OCR; (2) lack of domainspecific knowledge, such as landmarks and artworks, limits performance in specialized tasks. Improving synthetic representations of quality and integrating more domain-specific knowledge is key to enhancing Unicorn-8Bs ability."
        },
        {
            "title": "References",
            "content": "Bai, T., Liang, H., Wan, B., Xu, Y., Li, X., Li, S., Yang, L., Li, B., Wang, Y., Cui, B., Huang, P., Shan, J., He, C., Yuan, B., and Zhang, W. survey of multimodal large language model from data-centric perspective, 2024. URL https://arxiv.org/abs/2405.16640. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multimodal models with better captions, 2023. URL https: //arxiv.org/abs/2311.12793. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2, 2023. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https: //arxiv.org/abs/2306.13394. Gu, S., Clark, C., and Kembhavi, A. cant believe theres no images! learning visual tasks using only language supervision, 2023. URL https://arxiv.org/abs/ 2211.09778. Horn, G. V., Cole, E., Beery, S., Wilber, K., Belongie, S., and Aodha, O. M. Benchmarking representation learning for natural world image collections, 2021. URL https: //arxiv.org/abs/2103.16483. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., and Liu, T. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, January 2025. ISSN 1558-2868. doi:10.1145/3703155. URL http://dx.doi.org/10.1145/3703155. Huang, W., Wu, A., Yang, Y., Luo, X., Yang, Y., Hu, L., Dai, Q., Dai, X., Chen, D., Luo, C., and Qiu, L. Llm2clip: Powerful language model unlocks richer visual representation, 2024. URL https://arxiv.org/abs/ 2411.04997. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering, 2019. URL https://arxiv.org/abs/ 1902.09506. Kaur, P., Sikka, K., Wang, W., Belongie, S., and Divakaran, A. Foodx-251: dataset for fine-grained food classification, 2019. URL https://arxiv.org/abs/1907. 06167. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models, 2024. URL https://arxiv.org/abs/2407.07895. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Li, W., Zhu, L., Wen, L., and Yang, Y. Decap: Decoding clip latents for zero-shot captioning via text-only training. arXiv preprint arXiv:2303.03032, 2023b. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionlanguage models, 2023c. URL https://arxiv. org/abs/2305.10355. Liang, W., Zhang, Y., Kwon, Y., Yeung, S., and Zou, J. Mind the gap: Understanding the modality gap in multimodal contrastive representation learning, 2022. URL https://arxiv.org/abs/2203.02053. Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollár, P. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405. 0312. Liu, D., Zhang, R., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., Zhang, K., et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023. URL https://arxiv.org/abs/2304. 08485. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. 9 Unicorn: Text-Only Data Synthesis for Vision Language Model Training Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2024c. URL https:// arxiv.org/abs/2310.03744. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. Mmbench: Is your multi-modal model an all-around player?, 2024d. URL https://arxiv.org/abs/ 2307.06281. Liu, Y., Yu, X., Zhang, G., Zhu, Z., Bergeles, C., Dasgupta, P., Granados, A., and Ourselin, S. Arcsin: Adaptive ranged cosine similarity injected noise for languagedriven visual tasks, 2024e. URL https://arxiv. org/abs/2402.17298. Long, L., Wang, R., Xiao, R., Zhao, J., Ding, X., Chen, G., and Wang, H. On llms-driven synthetic data generation, curation, and evaluation: survey, 2024. URL https: //arxiv.org/abs/2406.15126. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. URL https: //arxiv.org/abs/2209.09513. Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned photographs. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https://proceedings.neurips. cc/paper_files/paper/2011/file/ 5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper. pdf. Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016. URL https: //arxiv.org/abs/1505.04870. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556 2565, 2018. Tewel, Y., Shalev, Y., Schwartz, I., and Wolf, L. Zerocap: Zero-shot image-to-text generation for visual-semantic 10 arithmetic, 2022. URL https://arxiv.org/abs/ 2111.14447. Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., and Jiang, Y.-G. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. Wu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y., Zhou, T., Zhang, X., Gao, J., Xiao, C., and Sun, L. Unigen: unified framework for textual dataset generation using large language models, 2024. URL https://arxiv.org/abs/2406.18966. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. URL https: //arxiv.org/abs/2308.02490. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. Zhang, Y., HaoChen, J. Z., Huang, S.-C., Wang, K.-C., Zou, J., and Yeung, S. Diagnosing and rectifying vision models using language, 2023. URL https://arxiv.org/ abs/2302.04269. Zhang, Y., Sui, E., and Yeung-Levy, S. Connect, collapse, corrupt: Learning cross-modal tasks with uni-modal data, 2024. URL https://arxiv.org/abs/2401. 08567. Zhao, B., Wu, B., He, M., and Huang, T. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. Zhao, H. H., Zhou, P., and Shou, M. Z. Genixer: Empowering multimodal large language models as powerful data generator, 2024. URL https://arxiv.org/abs/ 2312.06731. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Nanyang Technological University",
        "Shanghai AI Lab",
        "The Great Bay University",
        "Westlake University",
        "Zhejiang University"
    ]
}