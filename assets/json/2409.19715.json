{
    "paper_title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
    "authors": [
        "Hyungjoo Chae",
        "Taeyoon Kwon",
        "Seungjun Moon",
        "Yongho Song",
        "Dongjin Kang",
        "Kai Tzu-iunn Ong",
        "Beong-woo Kwak",
        "Seonghyeon Bae",
        "Seung-won Hwang",
        "Jinyoung Yeo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and machine-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available."
        },
        {
            "title": "Start",
            "content": "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code Hyungjoo Chae1 Taeyoon Kwon1 Seungjun Moon"
        },
        {
            "title": "Yongho Song",
            "content": "Dongjin Kang1 Kai Tzu-iunn Ong1 Beong-woo Kwak1 Seonghyeon Bae1 Seung-won Hwang2 Jinyoung Yeo Yonsei University1 Seoul National University2 {mapoout, kwonconnor101, lune_blue, jinyeo}@yonsei.ac.kr seungwonh@snu.ac.kr 4 2 0 2 4 ] . [ 2 5 1 7 9 1 . 9 0 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper presents COFFEE-GYM, comprehensive RL environment for training models that provide feedback on code editing. COFFEE-GYM includes two major components: (1) COFFEE, dataset containing humans code edit traces for coding questions and machinewritten feedback for editing erroneous code; (2) COFFEEEVAL, reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, COFFEE-GYM addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying COFFEEGYM, we elicit feedback models that outperform baselines in enhancing open-source code LLMs code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have made great progress in code generation (Li et al., 2023; Rozière et al., 2023), e.g., achieving human-level performances in code generation benchmarks (Chen et al., 2021b). Such success makes them powerful tools for assisting human programmers (Köpf et al., 2023); however, they still produce errors (Guo et al., 2024a; OpenAI, 2023b). Therefore, code editing, i.e., resolving errors in code, remains an important task for code LLMs (Muennighoff et al., 2023). Studies have utilized natural language (NL) feedback from LLMs as descriptive guidance in editing wrong codes for code LLMs. For instance, SelfRefine (Madaan et al., 2023) largely improves their code editing using GPT-4s feedback. Yet, abilities to generate helpful feedback, as they report, are limited to powerful closed-source LLMs (e.g., GPT-4). Equal contribution 1https://huggingface.co/spaces/ Coffee-Gym/Project-Coffee-Gym Figure 1: motivating example (Top) and Pass@1 accuracy in HumanEvalFix (Bottom). We compare the feedback from our model and various other models, both paired with DeepSeekCoder-7B as the code editor. SFT denotes the model trained on Code-Feedback (Zheng et al., 2024) using the same backbone model as ours. This can lead to heavy reliance on closed-source LLMs that may cause not only high computational (e.g., API) cost but also security risks (Siddiq and Santos, 2023; Greshake et al., 2023), limiting their applicability for confidential codes. This work aims to foster building open-source feedback models that produce effective feedback for code editing. An intuitive approach is to apply supervised fine-tuning (SFT) on open-source code LLMs using feedback from GPT-4 (generated Figure 2: Comparison between COFFEE-GYM and the previous approach. based on machines code editing) (Zheng et al., 2024). However, this simplified approach poorly aligns editing performance with the helpfulness of feedback (Bottom of Figure 1) (Liu et al., 2022). Inspired by the success of RLHF (Ouyang et al., 2022), we reformulate feedback modeling with reinforcement learning (RL), where we align feedback models with the helpfulness of feedback during training. Since the success of RL highly depends on the initial SFT model and reliable reward function (Lightman et al., 2023; Lambert et al., 2024), we hereby identify 3 main challenges in applying RL to feedback generation for code editing: (1) limited scenarios of errors in modelgenerated code editing datasets for initializing SFT model, (2) the lack of pairwise (correct and wrong) feedback to train/test reward functions, (3) absence of validated implementation of reward models. We present COFFEE-GYM, comprehensive RL environment addressing the above challenges in training feedback models for code editing. First, to tackle data scarcity in SFT initialization and reward COFFEE, dataset for modeling, we curate code fixing with feedback, which consists of code editing traces of human programmers and human annotated feedback. Unlike model-generated data (Figure 2), COFFEE includes (1) problems across various difficulties, including those current LLMs (e.g., GPT-4) cannot solve; (2) pairs of correct and wrong feedback for reward modeling; (3) about 36 test cases per problem to measure the feedback helpfulness in code editing.2 2This work is substantially revised and extended version of our preprint (Moon et al., 2023). While both works use the same dataset, this submission presents significant advancements in methodology, analysis, and results. Next, to address the absence of validated (i.e., reliable) reward functions, we introduce COFFEEEVAL, reward function designed to reflect the helpfulness of feedback into reward calculation. Instead of directly assessing feedback quality (Rajakumar Kalarani et al., 2023), we simulate code editing based on generated feedback, conduct unit tests on the edited code, and use the test results to measure feedback helpfulness. With the pairwise feedback from COFFEE, we train given code editor to produce edited code that faithfully reflects the helpfulness of the given feedback. Through experiments, we validate COFFEEGYMs efficacy in training feedback models. We find that COFFEEEVAL provides more accurate rewards, compared to the current SOTA reward model, i.e., G-Eval (Liu et al., 2023c) with GPT-4. Also, we show that the feedback models trained with COFFEE-GYM generate more helpful feedback, achieving comparable performance to closedsource feedback models in code editing."
        },
        {
            "title": "2 Task Definition and Problem Statement",
            "content": "2.1 Code Editing with Natural Language Feedback The task of code editing aims to resolve errors in given codes to produce correct solution. Formally, given problem description and defective solution y, our goal is to learn feedback model θ that generates helpful feedback describing the errors in and provide helpful guidance on code editing: ˆc = θ(q, y). Then, an editor model ϕ that takes q, y, and the generated feedback ˆc as input and generates the edited code: = ϕ(q, y, ˆc). In evaluating the edited code y, the functionalFigure 3: Overview of the data collection process of COFFEE. ity of the edited code is measured with Pass@k, the standard metric that measures the number of passed test cases ti within the given set = {t1, t2, . . . , tk} (Li et al., 2022, 2023; Muennighoff et al., 2023). Each test case ti consists of an input xi and an expected output zi. 2.2 Learning Feedback Models In this paper, we consider two widely used learning approaches to build open-source feedback models. )}N Supervised fine-tuning. straightforward approach is to fine-tune an open-source code LLM θ on dataset = {(qi, yi, ci, i=1 of problem descriptions, incorrect codes, feedback annotations, and correct codes. The objective is to minimize the negative log-likelihood of the target feedback label given and y. However, simply training to optimize the probability of the target sequence does not achieve much improvement for code editing, because it does not consider the impact of feedback on code editing (Liu et al., 2022). Reinforcement learning. Inspired by Ouyang et al. (2022), we adopt reinforcement learning (RL) to further align feedback generation to correct code editing. Specifically, we choose PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023) as reference RL algorithms and apply them on the feedback model θ initialized via SFT. The two key factors of RL are (1) pairwise preference data and (2) reward modeling (Lambert et al., 2024). In our task, we consider preference dataset where each input and comes with pair of chosen and rejected feedback c+ and c, and their preference ranking c+ c. This dataset is then used to model the reward based on the preference ranking. While in PPO reward model is explicitly trained using c+ and c, DPO relies on implicit reward modeling and directly optimizes the feedback model using the preference dataset. 2.3 Problem Statement Our goal is to promote rapid development of opensource feedback models by facilitating RL for feedback generation on code editing. Specifically, we aim to provide the two key components in RL for feedback generation: Dataset. The dataset required for our RL approach covers the following key aspects: (1) Coverage of difficulty and diversity (q, y) to initialize good SFT model. (2) Pairwise feedback data (c+ q, y) to build datasets for training DPO and reward model for PPO. (3) Test cases for unit test (T ) are required to implement our R, for directly measuring the impact of on the correctness of code editing. Reward model. The current standard of using LLM as reward model (Lee et al., 2023) to evaluate LLM outputs do not sufficiently models the impact of feedback on code editing outcomes and requires powerful LLMs (e.g., GPT-4) that incur high API costs. Especially, the high computation costs significantly limits the application of online RL algorithms (e.g., PPO) in feedback modeling, which require frequent and continuous API calls for reward calculation."
        },
        {
            "title": "3 Constructing COFFEE-GYM",
            "content": "We introduce COFFEE-GYM, comprehensive RL environment for training NL feedback model for code editing. COFFEE-GYM consists of two major COFFEE, dataset of humancomponents: (1) written edit traces with annotated NL feedback, and (2) COFFEEEVAL, an accurate reward model that Figure 4: Example and statistics of COFFEE. measures feedbacks impact on code editing. 3.1 COFFEE: Human-written Code Edit Traces with Annotated Pairwise Feedback We curate COFFEE, dataset of code fixing with feedback, from human-written code edit traces. COFFEE consists of problems of diverse levels of difficulty, including challenging problems that only human programmers can solve, and provides test cases for reward functions (Section 3.2). The overview of constructing COFFEE, data examples, and statistics are in Figure 3 and 4. 3.1.1 Collecting Code Edit Traces from Human Programmers We collect human-authored code edits from an online competitive programming platform.3 In this platform, given problem description q, human programmers keep submitting new solution until they reach correct solution that passes all hidden test cases for q. Formally, for each and the correct submission n, we collect the submission n}, where {yk}n1 history {y1, y2, ..., k=1 are incorrect solutions. We then construct (q, y, y) triplets 3https://www.acmicpc.net/ Figure 5: Analysis results of details are in Appendix A.1.5. COFFEE. Experiment by pairing each incorrect solution yk with the correct one n, i.e., {(q, yk, n)}n1 k=1. To ensure COFFEE is not biased toward coding problems of specific difficulty level, we collect an equal number of problems from each of the five difficulty levels in the platforms, ranging from beginner to expert levels. We also ensure that COFFEE includes various solutions to each problem by collecting submission histories from 100 different users. Our analysis in Figure 5 shows that COFFEE (1) includes problems that are challenging for both human and LLMs and (2) covers more diverse error cases than machine-generated codes. 3.1.2 Annotating Pairwise Feedback Data We additionally annotate NL feedback that provides useful guidance on the necessary edits. For each triplet (q, y, y), we prompt GPT-3.5Turbo (OpenAI, 2023a) to describe how the correct solution differs from the wrong code y. The resulting description serves as the correct feedback that describes necessary changes on the wrong code to obtain the correct code y. Along with c, we also collect incorrect feedback c, which describes the difference between two wrong solutions, yk1 and yk (k = n), to provide pairwise labels for both correct and incorrect feedback to single wrong solution y. We discuss details on feedback annotation in Appendix A.1.1, including our prompt used mean std min 25% 50% 75% max Pass ratio 0.342 0.370 0.000 0.000 0.162 0. 0.985 Table 1: Pass ratio for incorrect code samples in the evaluation set of COFFEE dataset. ˆc from feedback model θ, an editor model ϕ generates an edited code by grounding on ˆc, i.e., = ϕ(q, y, ˆc). The COFFEEEVAL score is defined as the proportion of test cases for which the edited code produces the expected output: for feedback annotation and filtering techniques. COFFEEEVAL(q, y, ˆc, ϕ, ) 3.1.3 Augmenting Synthetic Test Cases Finally, we include set of hidden test cases = {t1, t2, . . . , tk} for each edit instance (q, y, y, c) in our dataset to assess whether the edited code is the correct solution to the problem. Each test case ti consists of an input xi and an expected output zi. As the programming platform does not make test cases publicly available, we annotate test cases by prompting GPT-3.5-Turbo to generate inputs xi for given and executing the correct code with xi to obtain the corresponding outputs zi. We filter out any invalid test cases with inputs that result in errors during execution. On average, we obtain 35.5 test cases per problem. critical question in evaluating our test suite is whether any incorrect solutions manage to pass all the test cases. To address this, we conduct an experiment using the evaluation set of the COFFEE dataset. We randomly sampled 200 wrong code instances and calculated the pass ratios of the wrong codes. We show the statistics of the distribution of pass ratios. As shown in Table 5, the maximum pass ratio is 0.985, which suggests that there are no wrong solutions that passed all the test cases. The mean score is 0.342, indicating that on average, wrong solutions fail the majority of the test cases. We further analyze the COFFEE-TEST and verified that no wrong solutions pass all the test cases. These test cases are used to measure the correctness of an edited code and estimate the helpfulness of the feedback as the COFFEEEVAL score, which we later use as supervision signals for training feedback models (3.2) in COFFEE-GYM. We provide details on test case generation in Appendix A.1.3. 3.2 COFFEEEVAL: Unit-test-driven Feedback Evaluation We present COFFEEEVAL as our reliable reward function in COFFEE-GYM. The key idea is to measure the helpfulness of feedback by gauging the correctness of the edited code produced by small, but cheap editor model that properly aligns editing with feedback. Specifically, given problem description q, wrong solution y, and feedback = 1 (cid:88) i=1 1 (ϕ(q, y, ˆc)(xi) = zi) (1) where each element ti consists of an input xi and an expected output zi, and 1 is binary indicator function that returns 1 if the output of matches the expected output zi. By reflecting the correctness of the edited code, the resulting score serves as an accurate measure for the effectiveness of the generated feedback in code editing. 3.2.1 Training Faithful Code Editor to Align Editing with Feedback General code LLMs are trained to produce only correct codes, resulting in bias toward correct editing regardless of feedback quality. To address this, we train code editor ϕ that aligns its output with the helpfulness of the feedback by training the model to generate both correct edits (q, y, c, y) Dcorrect and incorrect edits (q, y, c, y) Dwrong in COFFEE. The training objective is defined as: L(ϕ) = (cid:88) log pϕ(y q, y, c) (q,y,c,y)Dcorrect (cid:88) (q,y,c,y)Dwrong log pϕ(y q, y, c) (2) To prevent confusion during training, we follow Wang et al. (2023a) and indicate the correctness of the target code by prepending the keywords [Correct] and [Wrong] to the code sequence. By learning from both positive and negative examples, the editor learns to conduct code editing by faithfully following the given feedback. It allows us to use the editors output as reliable metric for evaluating feedback generation models in our COFFEE-GYM environment."
        },
        {
            "title": "4 Validating COFFEEEVAL",
            "content": "4.1 Experimental Setting Implementation details. We implement COFFEEEVAL with DeepSeekCoder-7B model as the backbone in all our experiments. For further details, please refer to Appendix A.2.1. Model GPT-4-Turbo GPT-3.5-Turbo GPT-4-Turbo GPT-3.5-Turbo DeepSeek-Coder-7B DeepSeek-COFFEEEVAL (w/o WF) DeepSeek-COFFEEEVAL (Ours) Evaluation Pass@1 Correct Feedback (TP) Wrong Feedback (FP) Scores Correlation Error Precision Recall F1 Pearson MSE G-Eval G-Eval Editing Editing Editing Editing Editing - - 53.0 43.4 36.0 36.4 52.0 - - 51.8 33.6 28.8 28.4 28.4 - - 50.6 56.4 55.6 56.2 64.7 - - 53.0 43.4 36.0 36.4 52.0 - - 51.8 49.0 43.7 44.2 57.7 0.135 -0. 0.012 0.101 0.077 0.085 0.149 0.415 0.575 0.450 0.417 0.428 0.418 0.408 Table 2: Performance of our evaluation protocol on the test sets of COFFEE compared to the baselines. Wrong Feedback is abbreviated as WF due to limited space. performance of our COFFEEEVAL validates its effectiveness in assessing the quality of NL feedback in the code editing task. Code LLMs are skewed toward correct editing, regardless of the feedback quality. While code LLMs have shown promising results in code generation tasks, they do not faithfully reflect the helpfulness of feedback on code editing. Especially, GPT-4-Turbo, the current SOTA code LLM, shows the highest Pass@1 among baselines, but it also tends to generate correct code even with wrong feedback. These results suggest that the training process with our pairwise feedback data is an essential step in building reliable reward model. The performance of COFFEEEVAL benefits from the number of test cases. Figure 6 compares the Pearson correlation coefficient and MSE with respect to the number of test cases. We observe that higher number of test cases leads to more accurate evaluation on the feedback quality, which validates our design choice of COFFEE."
        },
        {
            "title": "5 Benchmarking Reference Methods of",
            "content": "COFFEE-GYM In this section, we apply the feedback model trained using COFFEE-GYM on various opensource LLMs and assess its effectiveness in enhance code editing performance. Furthermore, we comprehensively explore wide range of training strategies available in our COFFEE-GYM to provide insights on building helpful feedback models. 5.1 Effectiveness of COFFEE-GYM in Training Feedback Models 5.1.1 Experimental Setting Implementation details. We train our feedback model based on DeepSeekCoder-7B using COFFEE-GYM by applying PPO. Further details are in Appendix A.3. Figure 6: Ablation results on the number of test cases used in COFFEEEVAL. The evaluation performance decreases as the number of test cases declines. 4.2 Reliability of COFFEEEVAL Baselines. We compare our COFFEEEVAL with two evaluation methods: G-Eval (Liu et al., 2023c) and Editing. For G-Eval, we directly assess feedback quality in Likert-scale (1 - 5) using score rubrics (Kim et al., 2023). Editing baselines follow the same evaluation scheme as COFFEEEVAL but use general code LLMs for the editor ϕ. We consider with three code LLMs, GPT-3.5-Turbo, GPT4-Turbo, and DeepSeek-Coder-7B. The prompt we use for G-Eval is in Appendix B.3. Evaluation. To measure the alignment between feedback generation and code editing, we use test COFFEE, where each is annotated with set of binary label on its helpfulness. For Editing methods (including ours), we regard the output as positive prediction when the edited code passes all test cases. Also, we provide Pearson correlation coefficients for both Editing and G-Eval methods to analyze the correlation between the predicted score and the ground-truth labels. 4.3 Results and Analysis COFFEEEVAL faithfully aligns feedback quality with editing performance. As shown in Table 2, DeepSeek-COFFEEEVAL achieves higher Pearson correlation and lower MSE than all G-Eval and Editing baselines. In particular, our approach shows even higher correlation than the G-Eval baseline implemented with GPT-4-Turbo. The strong Methods Params. Open-source HumanEvalFix COFFEE-TEST Average GPT-4-Turbo (OpenAI, 2023b) GPT-3.5-Turbo (OpenAI, 2023a) DeepSeek-Coder (Guo et al., 2024a) + Execution Feedback + Self-Feedback + OpenCodeInterpreter-DS-Coder Feedback + OURS + GPT-3.5-Turbo Feedback + GPT-4-Turbo Feedback CodeGemma (CodeGemma Team et al., 2024) + Execution Feedback + Self-Feedback + OpenCodeInterpreter-DS-Coder Feedback + OURS + GPT-3.5-Turbo Feedback + GPT-4-Turbo Feedback OpenCodeInterpreter-DS-Coder (Zheng et al., 2024) + Execution Feedback + Self-Feedback + DeepSeek-Coder Feedback + OURS + GPT-3.5-Turbo Feedback + GPT-4-Turbo Feedback - - 7B - 7B 7B 7B - - 7B - 7B 7B 7B - - 7B - 7B 7B 7B - - Pass@1 83.5 75.0 60.4 68.3 67.7 64.6 73.8 72.5 74.4 53.7 61.6 53 36.5 59.7 57.3 65.8 65.8 66.4 62.1 56.1 70.1 68.3 72.5 - - - + 7.9 + 7.3 + 4.2 + 13.4 + 12.1 + 14.0 - + 7.9 - 0.7 - 17.2 + 6.0 + 3.6 + 12.1 - + 0.6 - 3.7 - 9.7 + 4.3 + 2.5 + 6.7 Pass@1 43.8 32. 33.8 38.3 28.3 30.5 47.2 35.5 44.4 14.4 15.0 16.6 15 31.1 22.2 22.7 30.5 36.6 21.1 28.3 42.7 32.7 43.3 - - - + 4.5 - 5.5 - 3.3 + 13.4 + 1.7 + 10. - + 0.6 + 2.2 + 0.6 + 16.7 + 7.8 + 8.3 - + 6.1 - 9.4 - 2.2 + 12.2 + 2.2 + 12.8 Pass@1 63.6 53.6 47.1 53.3 48.0 47.5 60.5 54.0 59.4 34.1 38.3 34.8 25.8 45.4 39.8 44. 48.1 51.5 41.6 42.2 56.4 50.5 57.9 - - - + 6.2 + 0.9 + 0.5 + 13.4 + 6.9 + 12.3 - + 4.2 + 0.7 - 8.3 + 11.4 + 5.7 + 10.2 - + 3.4 - 6.5 - 5.9 + 8.3 + 2.4 + 9. Table 3: Code editing results of our feedback model trained with COFFEE-GYM, i.e., PPO-COFFEEEVAL, on HumanEvalFix and COFFEE-TEST. We pair our feedback model with an open-source code LLM as the code editor. Benchmarks. We test the feedback model trained using COFFEE-GYM on HumanEvalFix (Muennighoff et al., 2023), widely used code editing benchmark. The task is to fix the errors in given erroneous code and the correctness of the edited code is measures by running the annotated test cases. Then, if the submitted solution passes all testcases the solution is evaluated as success and pass@1 is calculated as the percentage of the passed solutions for all promplems. We carefully check if there is data leakage in COFFEE and verify there is no overlap between COFFEE and HumanEvalFix (Appendix A.1.6). Additionally, we assess the effectiveness of our approach on held-out test set named COFFEE-TEST. It consists of 180 instances of (q, y, y, ) pairs that are collected following the same process in 3.1 but with no overlapping problems with COFFEE. Baselines. We compare with the following baselines that provides feedback for code editing: (1) 4While we have considered other code editing benchmarks, DebugBench (Tian et al., 2024) and CodeEditorBench (Guo et al., 2024b), we find that these benchmarks have critical issue; even the ground-truth solution cannot pass the unit test. detailed discussion on this issue is in Appendix B.1. Execution Feedback (Chen et al., 2023): execution results of the generated code, e.g., error messages, without using any LLMs , (2) SelfFeedback (Madaan et al., 2023): NL feedback generated by the code editor itself, (3) OpenCodeInterpreter Feedback (Zheng et al., 2024): code LLM especially trained on Code-Feedback dataset. We also provide the results of feedback from closedsource LLMs, GPT-3.5-Turbo and GPT-4-Turbo, but these models are not our main focus as we aim to develop open-source feedback models. 5.1.2 Results In Table 3, we compare the performance of our best feedback model with other feedback methods using various open-source models. Consistent with the findings from Chen et al. (2023), we observe improvements across all code LLMs when using Execution Feedback. However, we find that opensource code LLMs, despite their capabilities in the code domain, struggle to generate helpful NL feedback for code editing (Self-Feedback), highlighting the complexity of producing effective feedback. Notably, our approach demonstrates comparable performance to GPT-3.5/4-Turbo, significantly closing the performance gap between closedsource and open-source models in the task of feedback generation for code editing. 5.2 Comparing Different Training Strategies in COFFEE-GYM 5.2.1 Experimental Setting Training strategies. For training algorithm, we explore DPO, PPO, and Rejection Sampling (RS). In RS, we sample 10 ˆc from SFT model, and collect ˆc with top-1 COFFEEEVAL score as labels for the next iteration of SFT. For PPO, we use COFFEEEVAL as the reward model. We use 3 variants for DPO: (1) DPO-TS: We construct preference pair by selecting the teacher models feedback (i.e., GPT3.5-Turbo) as c+, and the student models (SFT) response as (Tunstall et al., 2023), (2) DPO-CW: We directly use the labeled feedback pair (c, c). (3) DPO-COFFEEEVAL: We sample 10 ˆc, same as RS, and we construct preference pair with ˆc of top-1 and bottom-1 COFFEEEVAL score. 5.2.2 Results COFFEE provides helpful train data for SFT. In Figure 7, we find that SFT-COFFEE provides more helpful feedback than SFT-CODEFEEDBACK trained on Code-Feedback. This results suggest that COFFEE serves as valuable resource for fine-tuning feedback models. COFFEE and COFFEEEVAL allow informative preference pair construction for DPO. DPOCOFFEEEVAL achieves the best results among DPO variants, closely followed by DPO-CW, which utilizes correct-wrong pairs from COFFEE. However, DPO-TS significantly underperforms even with the correct feedback c+ sampled from the teacher. We conjecture that the teachers feedback may not always be superior to the students, leading to suboptimal preference pairs. PPO is the most effective training algorithm. PPO-COFFEEEVAL outperforms DPOCOFFEEEVAL and RS-COFFEEEVAL, despite using the same reward model. We hypothesize that online RL methods like PPO allow for continuous updates on the reference model and lead to better alignment compared to offline methods like DPO, which learn from fixed initial model. 5.3 Analysis Fine-grained analysis by error type. In Figure 8a, we compare the baselines with our approach Figure 7: End-to-end validation results of the reference methods in COFFEE-GYM on COFFEE-TEST. Figure 8: (a) Breakdown of editing performance on HumanEvalFix by different error types. (b) Human evaluation of the feedback generated on HumanEvalFix. See Appendix B.4 for details on human evaluation. across different error types. Our feedback model is particularly effective at correcting Missing logic and Function misuse errors, which can greatly benefit from NL feedback by providing detailed explanation for editing. In value misuse, our model shows slightly lower performance. We posit that this is due to the discrepancy between the distribution of errors from human-authored data (i.e., COFFEE) and synthetic data, where our model is tested. Human evaluation on feedback quality. To provide more accurate analysis of the feedback quality, we conduct human evaluation using qualified workers from MTurk.5 The results in Figure 8b show that the feedback from our model is rated as more helpful and informative compared to the baselines, supporting the findings in 5.2."
        },
        {
            "title": "6 Related Work",
            "content": "Code editing. Code LLMs have shown promising code generation capabilities by training on massive code corpora (Li et al., 2023; Wang et al., 2023b). Despite their promising capabilities, there remains possibility of errors, making code editing tasks essential for ensuring code quality and correctness (Muennighoff et al., 2023). In response to this necessity, recent studies have focused on as5The details of our human evaluation are in Appendix B.4. sessing the code editing capabilities of code LLMs, by proposing new benchmarks for the task (Tian et al., 2024; Guo et al., 2024b). Refining with external feedback. In code editing, two types of widely used external feedback are execution feedback (Gou et al., 2023; Chen et al., 2023) and NL feedback (Madaan et al., 2023; Shinn et al., 2023). Recently, Zheng et al. (2024) explored both types of feedback and demonstrate that NL feedback outperforms execution feedback. Concurrent to our work, Ni et al. (2024) explored building feedback model, but they do not provide the dataset used nor the model checkpoint. RL in code generation tasks. line of research has explored improving LLMs code generation with RL by leveraging the unit test results as reward (Le et al., 2022; Liu et al., 2023a; Shen et al., 2023). While the design of COFFEEEVAL is largely inspired by this line of work, we show that building reward model for feedback learning using unit test results is non-trivial, since code LLMs do not faithfully reflect feedback into editing  (Table 2)  ."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we present comprehensive study on building open-source feedback models for code editing. We introduce COFFEE-GYM, an environment for training and evaluating feedback models, and share valuable insights from our experiments. We hope our work will encourage researchers to further explore feedback model development using COFFEE-GYM and our findings, advancing the field of code editing with NL feedback."
        },
        {
            "title": "Limitations",
            "content": "Scope of editing. COFFEE-GYM tackles the task of code editing with particular focus on correcting errors in codes. This leaves room for improvement in our RL approach to consider the efficiency and readability of the edited codes. Also, we mainly focus on editing incorrect source codes in competitive programming setting. Some examples from our feedback model (Appendix C.2) suggest that our approach can be further applied to practical programming problems, e.g., those that involve machine learning libraries. In future studies, COFFEEGYM can be further expanded to real-world software engineering settings with additional training on general code corpora (Li et al., 2023). Using synthetic test cases for measuring reward. While running synthetic test cases and using the resulting pass rates might be promising proxy for reward calculation, there might be edge cases where even erroneous codes pass the synthetic test cases. Further research can incorporate Liu et al. (2023b) to make more challenging test cases that can rigorously identify erroneous codes. Single programming language. Our implementation of COFFEE-GYM is limited to single programming language, i.e., Python. However, future work might apply similar strategy as ours to expand our model to multilingual setting, where the model is capable of understanding and editing diverse programming languages such as Java. Single parameter size and architecture. Lastly, we implement the feedback models only with one parameter size and architecture. However, future work can apply our method to models with larger parameter sizes (e.g., DeepSeek-Coder 70B), which is expected to perform better in code editing. Our framework can also be further applied to other architectures, as our method is model-agnostic."
        },
        {
            "title": "Ethical Considerations",
            "content": "While our dataset originates from online competitive programming platforms, we have ensured the exclusion of personal information to maintain privacy standards. Additionally, we are aware of the potential risks associated with texts generated by language models, which can contain harmful, biased, or offensive content. However, based on our assessments, this risk is mostly mitigated in our work. Lastly, there exists risk of hallucination in the process of feedback generation and code editing, leading to incorrect edits. This emphasizes the need for careful application in our approach."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT)(No.RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University)) and (No.RS-2021-II212068, Artificial Intelligence Innovation Hub) and (2022-0-00077, RS2022-II220077,AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data). Jinyoung Yeo is the corresponding author."
        },
        {
            "title": "References",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large lanarXiv preprint guage models trained on code. arXiv:2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao et al. Gong. 2024. Codegemma: Open code models based on gemma. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024a. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. ArXiv, abs/2401.14196. Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li, Ruibo Liu, Yue Wang, et al. 2024b. Codeeditorbench: Evaluating code editing capability of large language models. arXiv preprint arXiv:2404.03543. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations democratizing large language model alignment. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 89388958. Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023a. Rltf: Reinforcement learning from unit test feedback. arXiv preprint arXiv:2307.04349. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023b. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. G-eval: Nlg evaluation using gpt-4 with better human alignment. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Seungjun Moon, Yongho Song, Hyungjoo Chae, Dongjin Kang, Taeyoon Kwon, Kai Tzu-iunn Ong, Seung-won Hwang, and Jinyoung Yeo. 2023. Coffee: Boost your code llms by fixing bugs with feedback. arXiv preprint arXiv:2311.07215. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124. Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2024. Next: Teaching large language models to reason about code execution. Augustus Odena, Charles Sutton, David Martin Dohan, Ellen Jiang, Henryk Michalewski, Jacob Austin, Maarten Paul Bosma, Maxwell Nye, Michael Terry, and Quoc V. Le. 2021. Program synthesis with large language models. OpenAI. 2023a. Chatgpt. https://openai.com/ blog/chatgpt. OpenAI. 2023b. Gpt-4 technical report. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Niyati Chhaya, and Sumit Shekhar. 2023. lets not quote out of context: Unified vision-language pretraining for context assisted image captioning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 695706, Toronto, Canada. Association for Computational Linguistics. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint arXiv:2307.14936. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Proceedings of NeurIPS. Mohammed Latif Siddiq and Joanna C. S. Santos. 2023. Generate and pray: Using sallms to evaluate the security of llm generated code. ArXiv, abs/2311.00889. Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023a. SCOTT: Selfconsistent chain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 55465558, Toronto, Canada. Association for Computational Linguistics. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023b. Codet5+: Open code large language models for code understanding and generation. arXiv preprint. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. 2022. CERT: Continual pretraining on sketches for library-oriented code generation. In The 2022 International Joint Conference on Artificial Intelligence. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658. Details of COFFEE-GYM Correctness Score Frequency (%) A.1 Details of COFFEE A.1.1 Feedback Annotation We annotate both correct and wrong feedback for our dataset using GPT-3.5-Turbo. We apply topp sampling and temperature, where = 0.95 and = 0.7. We limit the number of generation tokens to 500. We leave out submission histories where the LLM fails to find any errors. We also filter out submissions from different users whose correct solutions are identical, as these solutions are usually copied from the web without undergoing editing processes. With collected users submission history {y1, y2, ..., n}, we sample correct edit pairs n}n1 {yk, k=1 to annotate correct feedback. To annotate the wrong feedback, we use sequential pairs {yk, yk+1}n2 k=1 to capture transitions between consecutive incorrect solutions. The prompts used for annotating correct and wrong feedback are demonstrated in Appendix D.1 and Appendix D.2. A.1.2 Quality Analysis on Annotated Feedback To thoroughly analyze the quality of the feedback from GPT-3.5-Turbo, we conduct human evaluation. We ask human raters from Amazon Mechanical Turk (AMT) to score the quality of the feedback on Likert scale. To ensure proficiency, we filter out human raters who have not passed our qualification test, which assesses their knowledge of programming languages, especially Python. From the test set of COFFEE, we sample 100 instances for the evaluation. On average, the annotated feedback is scored 3.88 with 0.91 STD, which suggests that the quality of the annotated feedback is generally acceptable by humans. The full distribution of the evaluation results is shown in Table 4. A.1.3 Synthesizing Test Cases We prompt GPT-3.5-Turbo to synthesize input test cases given problem description with three demonstrations. For each test case, we execute the correct code to obtain the corresponding output. If execution was successful, we then pair these inputs and outputs to create sample input-output pairs. On average, we synthesize 35 test cases per problem. We provide the prompt for the test case generation in Appendix D.3. 1 2 3 4 5 2 (0.6%) 21 (7.0%) 70 (23.3%) 126 (42.0%) 81 (27.0%) Table 4: Distribution of human evaluation scores for GPT-3.5-Turbo feedback quality. mean std min 25% 50% 75% max Pass ratio 0.342 0.370 0.000 0.000 0.162 0. 0.985 Table 5: Pass ratio for incorrect code samples in the evaluation set of COFFEE dataset. A.1.4 Analysis on Machine-generated Test Cases To gain insights into the effectiveness of our machine-generated test cases, we conduct analyses exploring two key aspects: validity and diversity. Validity of test cases. critical question in evaluating our test suite is whether any incorrect solutions manage to pass all the test cases. To address this, we conducted an experiment using the evaluation set of the COFFEE dataset. We randomly sampled 200 wrong code instances and calculated the pass ratios of the wrong codes. We show the statistics of the distribution of pass ratios. As shown in Table 5, the maximum pass ratio is 0.985, which suggests that there are no wrong solutions that passed all the test cases. The mean score is 0.342, indicating that on average, wrong solutions fail the majority of the test cases. We further analyze the COFFEE-TEST and verified that no wrong solutions pass all the test cases. Diverse difficulty of test cases. To demonstrate that our generated test cases cover range of difficulties, we analyzed the pass ratio distribution for incorrect code samples annotated in the dataset. We focused on single problem from the COFFEE evaluation set. As shown in Figure 9, the results revealed that various incorrect solutions for this problem exhibited different pass ratios, indicating that our test cases encompass diverse difficulty levels. A.1.5 Data Analysis We conduct following experiments to explore original features in COFFEE dataset. Length of edit trace We analyze the distribution of average length of edit trace by problem level. In in COFFEE. The results (Figure 5.c) suggest that even the state-of-the-art LLM, i.e., GPT-4, struggles to produce correct solutions for problems in COFFEE and lags behind human programmers. A.1.6 Analysis on Train-test Overlap possible concern is that the training data in COFFEE might overlap with the test data in the code benchmark (i.e., HumanEval). Therefore, we follow Odena et al. (2021) and measure the amount of identical codes (based on the number of repeated lines) between the training and test data. Figure 10 reports both the fraction and the absolution number of line overlaps between COFFEE and HumanEval. We observe that most solutions in COFFEE do not contain lines that appear in the benchmark dataset which we evaluate our models on. A.2 Details of COFFEEEVAL Implementation Details A.2.1 We use DeepSeekCoder-7b6 as our backbone model using QLoRA (Dettmers et al., 2023), incorporating 4-bit quantization with learning rate of 5e-5 and batch size of 4 for 2 epochs. The training is run on 8 NVIDIA GeForce RTX 3090 GPUs. Regarding the LoRA configuration, we specify the dimension of low-rank matrices as 64, and alpha as 16. A.2.2 Training Details Following the approach of Wang et al. (2023a), we train the editor in two phases. The initial phase includes the keywords [Correct] and [Wrong] in the code sequence, while the second phase trains the model without these keywords. Phase I. We finetune our editor model ϕ using pairwise data of correct edits (q, y, c, y) Dcorrect and incorrect edits (q, y, c, y) Dwrong in COFFEE. During this phase, we additionally append keyword tokens and ([Correct] and [Wrong] respectively) with the target code sequences and y. Therefore, the training objective for the initial phase is defined as: L(ϕ) = (cid:88) log pϕ(t, q, y, c) (q,y,c,y)Dcorrect (cid:88) (q,y,c,y)Dwrong log pϕ(t, q, y, c) (3) 6https://huggingface.co/deepseek-ai/ deepseek-coder-6.7b-instruct Figure 9: Kernel Density Estimation plot of the pass ratio distribution for incorrect code samples. Figure 5.a, we observe steady increase in the average length of edit traces from human programmers with increasing difficulty levels. This suggests that problems in COFFEE are challenging for human programmers, as they tend to make more incorrect submissions for problems with higher difficulty levels. Code diversity. To assess the diversity of humanwritten codes compared to machine-generated codes, we conduct similarity analysis on error codes. Specifically, we sample problems from COFFEE where more than 100 users submitted solutions and collect the wrong code from these users. We also sample an equal number of wrong codes from ChatGPT and GPT-4 with top-p sampling of = 0.95 and temperature = 0.6. For each set of incorrect solutions sampled from user solutions, ChatGPT, and GPT-4, we use CodeBERT (Feng et al., 2020) to compute embeddings for incorrect solutions and measure cosine similarity for all possible pairs in the set. Figure 5.b shows the histogram of the number of problems by the average embedding similarity of incorrect solution pairs. We find that machinegenerated codes (i.e., ChatGPT, GPT4) tend to be more similar to each other than human-generated codes, indicating that collecting human-generated code allows for more diverse set of wrong code samples. Code complexity To show that problems in COFFEE are challenging for code LLMs, we measure the code generation performance of GPT-4 using Pass@1 and compare it with the solve rate of human programmers. Note that the latter is given as the metadata from the programming platform and computed as the proportion of correct solutions among all solutions submitted for problems Phase II. After training the editor in Phase I, we continually train the editor model using the same dataset but without the keyword tokens. Thereby, the training object for Phase II is defined as: library.7 Both libraries are licensed under Apache License, Version 2.0. We have confirmed that all of the artifacts used in this paper are available for non-commercial scientific use. L(ϕ) = (cid:88) log pϕ(y q, y, c)"
        },
        {
            "title": "B Experimental Details",
            "content": "(q,y,c,y)Dcorrect B.1 Benchmarks (cid:88) (q,y,c,y)Dwrong log pϕ(y q, y, c) (4) For our experiments, we consider the following benchmarks: We used the same hyperparameter settings in both phases and the prompt for training the code editor in Appendix D.3.1, A.3 Details of Reference Methods in COFFEE-GYM Preference Tuning. Given problem description, wrong code, and the corresponding preference set, we apply Direct Preference Optimization (DPO) (Rafailov et al., 2023) to train our critic. That is, we tune critic model to be biased towards helpful feedback. PPO. PPO optimizes the following objective: LPPO(θ) = (cid:104) (cid:16) min ˆEt rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17)(cid:105) (5) where rt(θ) is the probability ratio between the current policy θ and the old policy θold, ˆAt is an estimator of the advantage function at timestep t, and ϵ is hyperparameter that controls the clipping range. DPO. From SFT model we sample 10 feedback strings and score them with COFFEEEVAL. Among the 10 feedback collect feedback with top-1 score and bottom-1 score and construct preference pair, i.e., (c+, c), for DPO training. Using this dataset, we additionally conduct DPO training on SFT model. Rejection sampling. From SFT model we sample 10 feedback strings and score them with COFFEEEVAL. Among the 10 feedback we only collect feedback with top-1 score and construct dataset for further training. Using this dataset, we additionally conduct SFT. Terms and License. For our implementation and evaluation, we use Huggingface, TRL and vLLM HumanEvalFix HumanEvalFix is task of HumanEvalPack, manually curated using solutions from HumanEval (Chen et al., 2021a) for the task of code editing. Given an (i) incorrect code function, which contains subtle bug, and (ii) several unit tests (i.e., test cases), the model is tasked to correct/fix the function. The dataset consists of 164 samples from the HumanEval solutions, and each sample comes with human-authored bugs across six different programming languages, thus covering 984 bugs in total. The bugs are designed in way that the code is executed without critical failure but fails to produce the correct output for at least one test case. We have confirmed that the dataset is licensed under the MIT License and made available for noncommercial, scientific use. Reason for exclusion. We excluded DebugBench and CodeEditorBench for the following reasons: DebugBench (Tian et al., 2024) is debugging benchmark consisting of 4253 instances with 4 major categories and 18 minor types of bugs. The metric is based on the test suites provided by LeetCode, requiring API calls for evaluation. Due to the huge amount of API calls, LeetCode blocked the access during the evaluation, which lacked the accurate scoring. Also, some questions were graded incorrectly even though ground-truth solutions were given. Therefore, we decided not to use DebugBench for evaluation. CodeEditorBench (Guo et al., 2024b) is the framework designed for evaluating the performance of code editing. Code editing is categorized into four scenarios, debugging, translation, polishing, and requirement switching, where our main focus is on debugging. Similar to DebugBench, ground-truth solutions 7https://huggingface.co/ (a) Fraction of line overlaps. Neutral or somewhat accurate: Feedback is partially correct but contains significant inaccuracies or omissions. Mostly correct: Feedback is largely accurate with only minor mistakes or omissions. Completely correct: Feedback is entirely accurate and provides correct assessment of the code. We apply the same top-p sampling and temperature in Table A.1.1 and include the prompt used for the evaluation in Appendix D.3.2. B.4 Human Evaluation on Quality of (b) Absolute number of line overlaps. Feedback Figure 10: Analysis on train-test overlap between COFFEE and HumanEval. could not pass the unit test for some questions. Also, functions imported from external python files and some specific packages were used in questions without details, which made the question imprecise. So, we sent CodeEditorBench out of our scope. B.2 Metrics We use Pass@1 score to measure the code editing performance for all benchmarks. Specifically, Pass@1 is computed as the expected value of the correct rate per problem, when samples were generated to count the number of correct samples for each problem. Pass@1 = Problems (cid:105) (cid:104) (6) B.3 Feedback Quality Evaluation To assess the feedback quality in Likert-scale, we use G-Eval (Liu et al., 2023c) and prompt GPT-4Turbo to evaluate the feedback quality. Specifically, given problem description, input and output format, wrong code, and the corresponding feedback, we prompt GPT-4 to classify the feedback into one of the following five categories. Completely incorrect: Feedback has no valid points and is entirely misleading. Mostly incorrect: Feedback has some valid points but is largely incorrect or misleading. Task description. The error detection and correction scores were determined by human annotators evaluating feedback on incorrect code using Likert scale. The error detection score evaluates how accurately the feedback identifies errors in the incorrect code, while the error correction score assesses the correctness and effectiveness of the corrections suggested in the feedback. Preparing feedback for the evaluation. We aim to analyze the quality of the feedback generated for code editing. We randomly sample 100 codes from COFFEE-TEST to assure the correctness of our evaluation. For generating feedbacks, we use the erroneous codes provided in the dataset. Details on human evaluation. We conduct human evaluation by using Amazon Mechanical Turk (AMT), which is popular crowd sourcing platform. As we need workers who have enough experience with Python, we conduct qualification test to collect pool of qualified workers. In result, we recruit 186 workers who have passed the test, and task them to evaluate the quality of the feedback on Likert scale, ranging from 1 to 5. Each sample is evaluated by three different raters to ensure the reliability. Based on our estimates of time required per task, we ensure that the effective pay rate is at least $15 per hour. We use the evaluation interface in Figure 12."
        },
        {
            "title": "C Additional Analysis",
            "content": "C.1 Iterative Editing Inspired by Zheng et al. (2024), we consider practical setting where models are tasked with iterative code generation with feedback. We employed Model Pass@1 OpenCodeInterpreter-DS-Coder-7b + PPO-COFFEEEVAL 68.3 70.3 Table 6: The performance of different feedback models on NumpyEval. tail. As illustrated in Figure 14 and Figure 15, our model demonstrates the ability to generate helpful feedback even when the problem description is provided in Python comments rather than natural language format. In some instances, the feedback includes the necessary editing code. This capability highlights the potential for using our model in practical scenarios, where users queries can take various forms and formats, enhancing its applicability in real-world programming environments. C.3 Case Study on SFT vs. PPO In Figure 13, we present examples of generated feedback. Although the feedback generated by the SFT model appears plausible, it provides unnecessary feedback which may confuse the editor in feedback-augmented code editing. In contrast, our model (PPO) provides focused and helpful feedback on the incorrect part without unnecessary information. This result aligns with Figure 8, demonstrating that our model generates more accurate and helpful feedback compared to other models."
        },
        {
            "title": "D Prompts for Our Experiments",
            "content": "D.1 Correct Feedback Annotation Prompt Generate an explanation, analyzation, and plan to generate code prompt for the last task considering the example task instances. Your plan should show enough intermediate reasoning steps towards the answer. Construct the plan as much as you can and describe the logic specifically. When constructing the plan for the code prompt, actively use if else statement to take different reasoning paths based on the condition, loop to efficiently process the repititive instructions, dictionary to keep track of connections between important variables . [Example 1] Example task instances: {example_instances_of_task1} Output format: {output_format_of_task1} Figure 11: Performance on test cases from HumanEval, measured under the iterative edit setting. OpenCoderInterpreter-DS-7b as our codeLLM and used our feedback model to provide evaluations on the generated code. Our experiments included comparisons with reference methods in COFFEE-GYM. As shown in Figure 11, using our feedback model consistently enhanced performance over successive iterations. Consistent with our main experiment findings, both PPO and DPO improved feedback quality more effectively than rejection sampling. These results underscore the practical applications of our approach. C.2 Practical Programming Problems To further explore the applicability of our feedback model (PPO-COFFEEEVAL) to practical programming problems and assess its robustness across different domains, we conducted experiments using NumpyEval (Zan et al., 2022). This dataset focuses on the general coding domain, specifically involving problems related to the NumPy library. We chose this benchmark to test our models performance on unseen domains and evaluate its generalizability beyond our initial scope. We utilized OpenCodeInterpreter-DS-Coder-7b as both the generation and editing model, while PPO-CoffeeEval served as the feedback model. To establish baseline, we compared our approach against Self-Feedback method, which used OpenCodeInterpreter-DS-Coder-7b for feedback as well. As shown in Table 6, our PPO-CoffeeEval model outperforms the baseline. These results suggest that our feedback model is not overfitted to Coffee dataset, and did not lost generalization ability to unseen domains. For further analysis, we conducted case study to examine the models performance in more deExplanation: {analysis_of_task1} ... [Example 4] Example task instances: {example_instances_of_target_task} Output format: {output_format_of_target_task} Explanation: samples, please attach <start> token to indicate that the input string has started. Also, for every end of samples , please attach <end> token to indicate that the input string has ended. input format: {input format} python code: {python code} Sample: D.2 Wrong Feedback Annotation Prompt D.3.1 Code Editor Prompt Generate feedback that guides the refinement from Code before editing to Code after editing. Assume that the code after editing is 100% correct and your feedback should specifically guide the editing to the code after editing. Please point out only the guidance from the code before editing to the code after editing. Do not provide feedback on the code after editing or any feedback beyond the code after editing. [Example 1] Problem Description: {description} Code before editing: {wrong_code} Code after editing: {next_wrong_code} Feedback for Refining the Code: {feedback} ... [Example 4] Problem Description: {description} Code before editing: {wrong_code} Code after editing: {next_wrong_code} Feedback for Refining the Code: D.3 Test Case Generation Prompt Provide feedback on the errors in the given code and suggest the correct code to address the described problem. Description: {description} - output format: {output_format} - input format: {input_format} Incorrect code: python {wrong_code} Feedback:{feedback} Correct code: D.3.2 G-Eval Prompt You will be provided with feedback on the given incorrect code. Classify the accuracy of this feedback using Likert scale from 1 to 5, where: 1 (Completely incorrect): This feedback has no valid points and is entirely misleading. 2 (Mostly incorrect): This feedback has some valid points but is largely incorrect or misleading. 3 (Neutral or somewhat accurate): This feedback is partially correct but contains significant inaccuracies or omissions. 4 (Mostly correct): This feedback is largely accurate with only minor mistakes or omissions. 5 (Completely correct): This feedback is entirely accurate and provides correct assessment of the code. Just generate score from 1 to 5 based on the accuracy of the feedback. Description: {description} - output format: {output_format} - input format: {input_format} Given the input format and python code, please provide at least 30 challenging test input values to evaluate its functionality.For every start of Incorrect code: python {wrong_code} Feedback:{feedback} Score: Figure 12: The interface used for human evaluation on the feedback. Figure 13: Examples of the feedback from SFT and PPO model in COFFEE-GYM. Figure 14: Examples of the feedback from the PPO model on NumpyEval. Figure 15: Examples of the feedback from the PPO model on PandasEval."
        }
    ],
    "affiliations": [
        "Seoul National University",
        "Yonsei University"
    ]
}