{
    "paper_title": "Accelerated Preference Optimization for Large Language Model Alignment",
    "authors": [
        "Jiafan He",
        "Huizhuo Yuan",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 3 9 2 6 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Accelerated Preference Optimization for Large Language\nModel Alignment",
            "content": "Jiafan He and Huizhuo Yuan and Quanquan Gu Abstract Reinforcement Learning from Human Feedback (RLHF) has emerged as pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as proximal point method. Based on this observation, we propose general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterovs momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have emerged as pivotal technique in the era of artificial general intelligence and recently demonstrated impressive capabilities in tasks such as text generation (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023), coding (Chen et al., 2021; Austin et al., 2021), and problem solving (Cobbe et al., 2021; Wei et al., 2022). key element contributing to these achievements is the alignment of LLMs with human preference data, utilizing reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Munos et al., 2023). The standard RLHF method (Ouyang et al., 2022) involves three main steps: feedback collection, reward modeling, and policy optimization. Specifically, the LLM receives human-generated prompts Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: jiafanhe19@ucla.edu Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: hzyuan@cs.ucla.edu Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: qgu@cs.ucla.edu 1 and produces several possible responses. Subsequently, human preferences for these responses are collected and used to train reward model that matches these preferences. Finally, the policy optimization process updates the large language model using optimization algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) to produce responses with higher preference based on the trained reward model. Recently, Rafailov et al. (2023) introduced the Direct Preference Optimization (DPO) method based on the Bradley-Terry (BT) model (Bradley and Terry, 1952). This method skips the reward modeling process and replaces it with reparameterized reward with respect to the policy, allowing for direct optimization of the LLM. It simplifies the implementation, while maintaining comparable or even better performance than standard RLHF. Later, several works (Munos et al., 2023; Azar et al., 2024; Wu et al., 2024; Rosset et al., 2024) considered the general preference model rather than the BT model, and designed algorithms (Nash-MD, IPO, SPPO, and DNO) that more flexibly represent human preferences while maintaining simple implementation. Nonetheless, both the standard RLHF process and its variations rely on the policy optimization process. In fact, for general optimization problems, it is well-known that momentum techniques (Polyak, 1964; Nesterov, 2013; Bubeck et al., 2015b) can accelerate the convergence of the optimization algorithm in both theory and practice. Therefore, natural question arises: Can reinforcement learning from human feedback be accelerated? In this paper, we affirmatively answer this question. In details, following Xu et al. (2023); Yuan et al. (2024b); Chen et al. (2024); Wu et al. (2024), we consider the iterative preference optimization framework. In this framework, series of models is constructed, with each model being improved based on the previous one using preference optimization algorithm such as DPO and SPPO. We demonstrate that the policy optimization problem under this framework resembles the proximal point method (Bregman, 1967; Censor and Zenios, 1992; Kiwiel, 1997). Based on this observation, we adapt Nesterovs acceleration method (Nesterov, 1983, 2008, 2013; Lin et al., 2018), and introduce variant of the iterative preference optimization framework, named Accelerated Preference Optimization (APO). At the core of APO is an extrapolation step after each policy update. The contributions of our work are highlighted as follows: We propose general preference optimization framework, APO, based on Nesterovs momentum to accelerate preference optimization. Our theoretical analysis shows that iterative DPO achieves an (cid:101)O(1/t) sub-optimality gap from the optimal policy under the Bradley-Terry (BT) model. As comparison, our algorithm APO achieves smaller (cid:101)O(cid:0)(1 α)/t(cid:1) sub-optimality gap, where α is the extrapolation parameter in the momentum. To the best of our knowledge, our work provides the first convergence analysis of vanilla iterative DPO and the first accelerated preference optimization algorithm with provable guarantees. With an additional minimal sub-optimality gap assumption in the BT model, we prove that APO will convergence to the optimal policy in total variation (TV) distance at the rate of exp (cid:0) O(t/(1 α))(cid:1), improving the exp (cid:0) O(t)(cid:1) rate of iterative DPO. In addition, we extend our results to the general preference model and show that APO with the SPPO loss function can also accelerate SPPO under similar minimal sub-optimality gap assumption. Empirically, we verify the performance of APO when applied to DPO method on fine-tuning Mistral-7B-Instruct-v0.2. In the AlpacaEval 2.0 evaluation tasks, APO with 3 iterations 2 achieves length-controlled win rate of 31.73%, demonstrating 1.78% improvement over iterative DPO and 5.34% improvement over Snorkels Mistral-PairRM-DPO. Moreover, APO with 2 iterations obtains win rate of 37.53%, matching iterative DPOs 37.65% with 3 iterations, with noticeably shorter response lengths. This is consistent with our theoretical analysis. In addition, the evaluation on five general instruction-following tasks from MT-Bench shows an average score of 9.57 out of 10, further demonstrating APOs superior performance. Notation. For any positive integer n, we employ [n] to denote the set {0, . . . , n}. For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant such that an Cbn. We use (cid:101)O() to further hide the logarithmic factors."
        },
        {
            "title": "2 Preliminaries\nIn the setting of RLHF, we assume a finite context set X , and possible response set Y. For any\nprompts x ∈ X , a policy π : X → ∆(Y) maps the prompt x to the discrete distributions over the\nresponse set Y. For a given context x ∈ X collected from distribution ρ, we generate two responses\ny1, y2 with a reference policy µ and receive preferences from either humans or more advanced\nlanguage models between these two responses (yw ≻ yl), where yw and yl represent the preferred\nand dispreferred generated responses in {y1, y2}. Following Christiano et al. (2017); Ouyang et al.\n(2022); Rafailov et al. (2023), we assume the existence of a latent reward model r∗(x, y), and the\npreference distribution satisfies the Bradley-Terry (BT) model (Bradley and Terry, 1952):",
            "content": "P(y1 y2x) = exp (cid:0)r(x, y1)(cid:1) exp (cid:0)r(x, y1)(cid:1) + exp (cid:0)r(x, y2)(cid:1) . (2.1) Under this assumption, the standard RLHF first estimates the reward model by minimizing the following negative log-likelihood of BT model: L(r) = (x,yw,yl)D (cid:2) log σ(cid:0)r(x, yw) r(x, yl)(cid:1)(cid:3), (2.2) where is generated from distribution ρ, {yw, yl} are collected with reference policy µ and σ(z) = 1/(1 + exp(z)) is the Sigmoid function. After the reward modeling phase, the LLM (i.e., the policy) is fine-tuned with the learned reward r(x, y), which aims to maximize the expected reward with KL-regularization: π arg max πΠ Exρ,yπ(x)[r(x, y)] βExρ (cid:2)KL(cid:0)π(x)πref(x)(cid:1)(cid:3), (2.3) where Π denotes the policy class, ρ is the distribution of prompts, and the KL regularization with parameter β > 0 is used to control the deviation of the learned policy π from the reference policy πref. In detail, the optimization problem is usually solved with the PPO method (Schulman et al., 2017). Later, Rafailov et al. (2023) identified the following closed-form solution to the optimization problem in (2.3): π(x) ="
        },
        {
            "title": "1\nZ(x)",
            "content": "πref(x) exp (cid:18) r(x, ) β (cid:19) , 3 πref(x) exp (cid:0)r(x, y)/β(cid:1) is the partition function. By reparameterizing the reward where Z(x) = (cid:80) function by the policy and substituting it into the negative log-likelihood in (2.2), Rafailov et al. (2023) proposed the Direct Preference Optimization (DPO) method as follows: π arg max πΠ (x,yw,yl)D (cid:20) (cid:18) log σ β log π(ywx) πref(ywx) β log π(ylx) πref(ylx) (cid:19)(cid:21) , which avoids the explicit learning of reward model. Here, for finite dataset = {(xi, yw i=1} and function : R, we denote the empirical expectation of function with respect to the dataset by (cid:2)f (x, yw, yl)(cid:3) = (cid:80)N , yl i)N i=1 (xi, yw , yl i)/N . (x,yw,yl)D"
        },
        {
            "title": "3 Accelerated Preference Optimization\nIn this section, we present a general framework for language model alignment, namely accelerated\npreference optimization (APO), which is built upon the iterative preference optimization framework.",
            "content": "3."
        },
        {
            "title": "Iterative Preference Optimization Framework",
            "content": "Under the iterative Preference Optimization framework (Xu et al., 2023; Yuan et al., 2024b; Chen et al., 2024; Wu et al., 2024), the algorithm progressively updates the policy πt, aiming to converge to the optimal policy. In detail, for each iteration [T ], it designates the reference policy as the policy generated from the previous iteration, denoted by πt. It estimates the reward model by minimizing the expected loss function l(r, x, yw, yl) over the dataset Dt: (cid:2)ℓ(r, x, yw, yl, πt)(cid:3). (3.1) (x,yw,yl)Dt rt(, ) arg max r(,) Then, it updates the reference policy by solving the following KL-regularized optimization problem: (cid:98)πt+1 arg max πΠ Exρ,yπ(x)[rt(x, y)] βExρ (cid:2)KL(cid:0)π(x)πt(x)(cid:1)(cid:3). (3.2) According to Rafailov et al. (2023), for each iteration [T ], the optimization problem (3.2) has the following closed-form solution: (cid:98)πt+1(yx) πt(yx) exp (cid:18) rt(x, y) β (cid:19) . Thus, we can reparamterize the reward function for each policy π as follows: rπ(x, y) = β log π(yx) πt(yx) . With this reparameterized reward function, the previous two-step optimization process in (3.1) and (3.2) can be integrated into the following one-step direct preference optimization: (cid:98)πt+1 arg min rπRt (x,yw,yl)Dt (cid:2)ℓ(rπ, x, yw, yl, πt)(cid:3), (3.3) where Dt represents the data collected at iteration using the reference policy πt. For the vanilla iterative preference optimization framework, the updated policy (cid:98)πt+1 is directly used as the reference policy in the next iteration, where πt+1 = (cid:98)πt+1. In this situation, the iterative 4 optimization of the policy resembles the (Bregman) Proximal Point Method (Bregman, 1967; Censor and Zenios, 1992; Kiwiel, 1997), which iteratively minimizes the following proximal subproblem: πt+1 arg min πΠ (cid:110) ft(π) = Lt(π) + βD(π, πt) (cid:111) , (3.4) for given regularization parameter β and Bregman divergence D(, ). In this reduction, the expected reward Exρ,yπ(x)[rt(x, y)] corresponds to the target function Lt(π) in the proximal function ft(π), and the Bregman divergence D(π, πt) is chosen as the KullbackLeibler (KL) divergence to the behavior policy πt: Exρ The Choice of Loss Function By choosing different loss function ℓ(rπ, x, yw, yl, πt) in (3.3), we can recover many existing iterative preference optimization algorithms. In detail, the loss function depends on the preference model P, and we provide several concrete examples of the loss function and the corresponding preference optimization algorithms as follows. (cid:2)KL(cid:0)π(x)πt(x)(cid:1)(cid:3). Example 3.1 (DPO). If we choose the loss function in (3.3) as: ℓDPO(rπ, x, yw, yl, πt) = log σ(cid:0)rπ(x, yw) rπ(x, yl)(cid:1), it recovers DPO (Rafailov et al., 2023). Example 3.2 (SPPO). If we choose the loss function in (3.3) as: ℓSPPO(rπ, x, yw, yl, πt) = (cid:0)rπ(x, yw) 1 + log Zπt(x)(cid:1)2 + 1 (cid:0)rπ(x, yl) + log Zπt(x)(cid:1)2, 1 2 yY πt(yx) exp (cid:0)ηP(y πtx)(cid:1) represents the partition function for behavior where Zπt(x) = (cid:80) policy πt, it recovers the Self-Play Preference Optimization (SPPO) algorithm (Wu et al., 2024). (See Appendix B.1 for more detailed discussion.) Example 3.3 (IPO). If we choose the loss function in (3.3) as ℓIPO(rπ, x, yw, yl, πt) = (cid:0)rπ(x, yw) rπ(x, yl) τ 1(cid:1)2, where τ is regularization parameter, it recovers the Identity Preference Optimization (IPO) algorithm (Azar et al., 2024)."
        },
        {
            "title": "3.2 Accelerated Preference Optimization",
            "content": "So far, we have demonstrated that the iterative preference optimization framework resembles the proximal point method. For standard optimization problems, it is well known that Nesterovs momentum method (Nesterov, 1983, 2008, 2013) can accelerate the optimization algorithm both theoretically and empirically. In particular, Lin et al. (2018) proposed framework called Catalyst, which extends Nesterovs momentum method to the proximal point method and has shown that it can accelerate it provably. In the Catalyst method, after solving the proximal operator xt+1 = arg min (cid:110) ft(x) = (x) + κD(x, yt) (cid:111) , where (x) is the target function and D(x, yt) is the Bregman divergence, an extrapolation step is introduced as follows: yt+1 = xt+1 + αt(xt+1 xt), 5 Algorithm 1 APO (Accelerated Preference Optimization) 1: input: Reference policy πref, learning rate β, Nesterovs extrapolation parameter α, number of iterations 2: Initialize π0 = (cid:98)π0 = πref 3: for iteration = 0, . . . , do 4: Collect the dataset Dt = with prompts distribution ρ and current reference policy πt Set the reparameterized reward function class as following: 5: (cid:26) Rt rπ(x, y) = β log (cid:27) π(yx) πt(yx) (cid:12) (cid:12) (cid:12)π Π 6: Update the policy (cid:98)πt+1 as following: (cid:98)πt+1 arg min πΠ (x,yw,yl)Dt (cid:2)ℓ(rπ, x, yw, yl, πt)(cid:3) 7: Compute the policy πt+1 with an extrapolation step πt+1(yx) (cid:98)πt+1(yx) (cid:0) (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α (3.5) (3.6) 8: end for 9: output: final policy (cid:98)πT +1 where αt is the Nesterovs extrapolation parameter and xt+1 xt denotes the momentum from the previous update. Following the above idea, we introduce an extrapolation step after solving (3.3) to obtain πt+1: log πt+1(yx) = log (cid:98)πt+1 + α(cid:0) log (cid:98)πt+1 log (cid:98)πt where α > 0 is the fixed Nesterovs extrapolation parameter. After normalizing the policy πt+1(yx), we obtain (3.7) (cid:1), πt+1(yx) (cid:98)πt+1(yx) (cid:0) ="
        },
        {
            "title": "1\nZ′\nt(x)",
            "content": "(cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α (cid:98)πt+1(yx) (cid:0) (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α, where t(x) = (cid:80) (cid:98)πt+1(yx) (cid:0) (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α represents the partition function. Putting together all the key components discussed above, we present the APO framework in Algorithm 1. We notice that the extrapolation step in (3.7) is similar to the model extrapolation technique introduced by Zheng et al. (2024a), which aims to develop stronger model by extrapolating from the aligned model and the SFT model. However, there are several notable differences between APO and ExPO. First, the extrapolation step in ExPO is based on the strong assumption that medium-aligned model can be interpolated between weaker model and stronger modelan 6 assumption that lacks theoretical support. In contrast, APO is based on the observation that iterative preference optimization resembles the (Bregman) Proximal Point method and the extrapolation step follows the Nesterovs momentum technique (Nesterov, 1983; Lin et al., 2018), which has provable guarantee. Regarding algorithm design, APO is an iterative algorithm for policy optimization. For (cid:98)πT +1 before the the final iteration , similar to the Catalyst algorithm, APO outputs the policy extrapolation step. In contrast, ExPO is an one-shot algorithm which only has single iteration and outputs the final policy after an extrapolation step. In this sense, ExPO can be seen as special case of APO with only one iteration."
        },
        {
            "title": "4 Theoretical Analysis\nIn this section, we provide a theoretical analysis of APO in Algorithm 1.",
            "content": "We begin with the following theorem, which outlines the optimization dynamics of the policy (cid:98)πt+1 over different iterations [T ]. (cid:98)πt+1(yx) (cid:0) Theorem 4.1. Suppose that iteration [T ]. Then, the updated policy (cid:98)πt+1(yx) ="
        },
        {
            "title": "1\nZt(x)",
            "content": "πref(yx) exp (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α belongs to the policy class Π for each (cid:98)πt in Algorithm 1 satisfies (cid:18) 1 (cid:32) (cid:88) 1 β 1 α i=0 αt+1i 1 α (cid:19) (cid:33) , ri(x, y) where rt(x, y) = β log (cid:98)πt+1(yx) β log πt(yx) represents the reparameterized reward at iteration i=0(1/(1 α) αt+1i/(1 α)) ri(x, y)/β(cid:1) is the partition t, and Zt(x) = (cid:80) function. πref(yx) exp (cid:0) (cid:80)t Theorem 4.1 illustrates how the policy (cid:98)πt+1 evolves with respect to the reparameterized reward rt(x, y), which is highly dependent on the choice of the loss function ℓ in Algorithm 1. For the Bradley-Terry (BT) model with the loss function ℓDPO in Example 3.1, Theorem 1 in Rafailov et al. (2023) demonstrates that all reward functions compatible with the BT model can be expressed by the reparameterized reward. In addition, we introduce the following two assumptions, which are required by our analysis. Assumption 4.2 (Realizability). For each iteration [T ] and each policy π Π, the following updated policy belongs to the policy class Π: (cid:98)π(x) ="
        },
        {
            "title": "1\nZπ(x)",
            "content": "π(x) exp (cid:19) (cid:18) r(x, ) β Π, where Zπ(x) = (cid:80) Assumption 4.3 (Boundedness). For each iteration [T ] and each policy π, πt Π, we have π(x) exp (cid:0)r(x, y)/β(cid:1) is the partition function. for all , Y. β log π(yx) πt(yx) [R, R], Similar assumptions have been used in Rosset et al. (2024) to provide an analysis of the statistical error for the reparameterized reward. Equipped with these assumptions, we have the following performance guarantee for APO. Theorem 4.4 (APO with ℓDPO). For the Bradley-Terry (BT) model with loss function ℓDPO, under Assumptions 4.2 and 4.3, with probability at least 1 δ, the sub-optimality gap between (cid:98)πT +1 and the optimal policy π(x) = arg maxyY r(x, y) is bounded by (cid:2)r(x, y)(cid:3) Exρ,y(cid:98)πT +1(x) Exρ,yπ(x) (cid:101)O (cid:19) (cid:18) (1 α)β + (cid:32)(cid:115) (T + 1) (cid:80)T (cid:2)r(x, y)(cid:3) t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) , where the coverage coefficient κt is defined as: κt = max (x,y)X (cid:98)πT +1(yx)π π2 (yx) +1(yx) . (4.1) Remark 4.5. The sub-optimality gap in Theorem 4.4 consists of two terms: the optimization error (cid:101)O(cid:0)(1 α)β/T (cid:1) and the statistical error (cid:101)O(cid:0)(cid:112)T /(N β2(1 α)2)(cid:1), and there exists tradeoff between these two errors. Specifically, larger extrapolation parameter α 1 will decrease the optimization error but increase the statistical error. Moreover, when the dataset size is sufficiently large (N ), Theorem 4.4 suggests (cid:101)O(cid:0)(1 α)β/T (cid:1) sub-optimality gap for Algorithm 1 with the loss function ℓDPO. Compared with the standard iterative DPO method with α = 0, Algorithm 1 improves the sub-optimality gap by factor of 1/(1 α) thanks to the Nesterovs momentum technique. Theorem 4.4 only provides theoretical guarantee on the sub-optimality gap for the policy (cid:98)πT +1. (cid:98)πT +1 to the optimal policy π, we need the following In order to derive the convergence of policy minimal sub-optimality gap assumption. Assumption 4.6 (Minimal sub-optimality gap). For each prompt , let = arg maxyY r(x, y) be the optimal response. The sub-optimality gap between the optimal response and any other responses is strictly positive: miny=y x) r(x, y) > 0 r(x, Remark 4.7. For general Bradley-Terry (BT) model without the minimal sub-optimality gap, there may exist multiple responses that share the same maximum reward, i.e., r(x, y1) = r(x, y2) = maxyY r(x, y). In this case, the optimal policy is not unique, and it is impossible to show that the policy (cid:98)πT +1 converges to specific optimal policy. Theorem 4.8 (APO with ℓDPO). For the Bradley-Terry (BT) model with loss function ℓDPO, under Assumptions 4.2, 4.3 and 4.6, with probability at least 1 δ, the TV-distance between (cid:98)πT +1 and the optimal policy π(x) = arg maxyY r(x, y) is bounded by (cid:104) Exρ (cid:0) DTV (cid:32) (cid:98)πT +1(x), π(x)(cid:1)(cid:105) (cid:19)(cid:33) (cid:18) exp 1 β (1 α) (cid:32)(cid:115) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) , + where the coverage coefficient κt is defined in (4.1). (cid:98)πt+1 and the optimal policy π(x) = arg maxyY r(x, y) inRemark 4.9. The TV-distance between cludes both the optimization error exp (cid:0)O(T /(1α))(cid:1) and the statistical error (cid:101)O(cid:0)(cid:112)T /(N (1 α)2)(cid:1). 8 Similar to Theorem 4.4, there is tradeoff between these two errors based on the choice of α. In addition, when the dataset size is sufficiently large (N ), Theorem 4.8 suggests that APO converges to the optimal policy at rate of exp (cid:0) O(T /(1 α))(cid:1). In comparison, iterative DPO is special case of our algorithm with α = 0, which converges to the optimal policy at slower rate of exp (cid:0) O(T )(cid:1)."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Model and Datasets. We use Mistral AIs Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a) as our base model, which is fine-tuned version based on the pretrained Mistral-7B-v0.2 (Jiang et al., 2023a) on several publicly available datasets for instruction-following. This architecture has demonstrated robust performance improvements over Llama2 13B Chat(Touvron et al., 2023) in tasks such as Chatbot Arena(Chiang et al., 2024), MT-Bench (Zheng et al., 2024b), and other related evaluations. As result, Mistral-7B-Instruct-v0.2 has become popular choice for base models in recent reinforcement learning (RL) fine-tuning research (Hoang et al., 2023; Kawin et al., 2023). For training, we employ the UltraFeedback dataset (Cui et al., 2023) with loss function ℓDPO. Unlike traditional fine-tuning methods that depend on responses and preference labels generated by proprietary models like GPT-4, we utilize only the instruction set from UltraFeedback. All responses are autonomously generated by our model following an online principle, and the preference pairs are labeled using separate reward model, PairRM (Jiang et al., 2023b). The instruction set used for both training and validation includes total of 64k instructions that span diverse range of tasks, such as UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN, as detailed in Cui et al. (2023). Over three training iterations, we divided the instruction set into three folds as in Hoang et al. (2023); Kawin et al. (2023), allocating approximately 2k instructions per iteration, with an additional 1k left out for validation. Overall, our training pipeline is independent of human or GPT inputs. We provide details on hyperparameters in Appendix F.1. Evaluation. For evaluating the performance of our model, we utilize three common benchmarking tasks: the AlpacaEval 2.0 (Li et al., 2023b), the MT-Bench (Zheng et al., 2024b), and the Open LLM Leaderboard (Beeching et al., 2023; Gao et al., 2023). Among them, AlpacaEval 2.0 is the most indicative benchmark for our method with the current choice of experimental settings, focusing on general instruction-following capabilities as assessed by GPT-4-Turbo, with outcomes measured by weighted win-rate against GPT-4-Turbo. Another important benchmark is MT-Bench, which also targets instruction-following but offers less differentiation between models. Additionally, we present results from the Open LLM Leaderboard, which is based on accuracy of multiple-choice questions. AlpacaEval 2.0. As our primary evaluation metrics, AlpacaEval 2.0 incorporates an extensive set of 805 prompts. These prompts are simplified versions derived from the AlpacaFarm (Dubois et al., 2023) evaluation set, covering wide range of topics such as Health, Linguistics, Entertainment, Technology, Coding, Gaming, Arts, Sports, and more (Yuan et al., 2024b). During the evaluation, we consult the help of GPT-4-Turbo to compare the responses generated by our model with those produced by GPT-4-Turbo. The final win rate are weighted based on the uncertainty of the judge. 9 Employing the default AlpacaEval 2.0 pipeline, this metric demonstrates 0.93 Spearman correlation and 68.1% agreement rate with human annotators (Li et al., 2023b). MT-Bench: MT-Bench comprises of 80 multi-turn questions across eight distinct dimensions: Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, and Humanities. The evaluation process prompts the GPT-4 judge to assign scores from 1 to 10 to the generated responses. However, this scoring metric has limitations. Particularly, score saturation occurs quickly in certain dimensions, making it difficult to distinguish between models. Additionally, this can disadvantage models that perform exceptionally well in these saturated dimensions, as their distinctiveness is less recognizable. Open LLM Leaderboard: The Open LLM Leaderboard evaluates models across six different tasks, focusing on variety of language modeling capabilities. Each task consists of multiple-choice questions (MCQs), with the most probable choice being selected and compared against the correct answer. The final score is calculated based on accuracy. The ability for answering MCQs are not directly related with instruction-following abilities. The results are deferred to Appendix F.2. Baselines. Starting from Mistral-7B-Instruct-v0.2, we compare our method with existing iterative training results, including iterative DPO (Hoang et al., 2023) and iterative KTO (Ethayarajh et al., 2024; Kawin et al., 2023). We also present results from our own iterative DPO training. All three baselines utilize the same base model, training dataset, dataset splits, and training pipelines, which effectively highlights the differences in methodology. Additionally, as one of the pioneering approaches in iterative DPO, we include the Self-Rewarding algorithm (Yuan et al., 2024b) as another baseline. Note that different from other baselines, the training for the Self-Rewarding algorithm incorporates additional self-instruct data augmentation and is based on the LLama2 70B model, but requires no external reward model."
        },
        {
            "title": "5.2 Experimental Results",
            "content": "Having introduced our evaluation metrics and baselines, we now turn to our training pipeline In Algorithm 1, we begin by setting π0 = (cid:98)π0 = πref as the base model, and main results. Mistral-7B-Instruct-v0.2. At each iteration, we sample five pairs of responses under the current policy πt and rank them using their PairRM score (Jiang et al., 2023a). We designate the top-ranked response as the winner and the bottom-ranked response as the loser. Following the proximal point update described in (3.5), we proceed with momentum extrapolation. We note that when the policy is softmax linear function, update (3.6) reduces to momentum extrapolation in the parameter space. Consequently, we carry out an extrapolation in the parameter space to approximate the corresponding momentum step (3.6) in the probability space. AlpacaEval 2.0 Evaluation. Table 1 summarizes the AlpacaEval 2.0 results of different models. As shown, APO surpasses all other state-of-the-art iterative training models under the same experimental setting, with final length controlled (LC) win rate1 of 31.73%, and raw win rate of 39.38%. This demonstrates an increase of 1.78% in LC win rate and 1.74% in raw win rate over our implementation of iterative DPO, which only achieved an LC win rate of 29.95% and raw win rate of 37.64%. Delving deeper into the effects of extrapolation from one iteration to the next, we observe that starting from the same iteration 1 with an LC win rate of 25.35%, APO at iteration 2 achieves 1The length-controlled learning rate is calculated using logistic regression, with inputs including the model weights, the instruction, and the length difference between the model and baseline. The LC win rate is designed to correlate more strongly with human preferences and to be more robust against length attacks. Longer responses are penalized in manner that closely simulates (by causal inference) human length preferences. 10 Table 1: AlpacaEval 2.0 evaluation. Comparison of APO with state-of-the-art iterative training algorithms. Results are reported in both raw win rate (%) and length-controlled (LC) win rate (%). Additionally, the average response character length (Avg. Len) of each model is provided. The row highlighted in light red represents the results achieved by our APO. The highest LC win rate and raw win rate, both achieved by our APO at iteration 3, are emphasized in bold. Improvements of LC win rate from the previous iteration to the current iteration are calculated and marked by subscript (+) in red. Model Mistral-7B-Instruct-v0.2 Snorkel (Mistral-PairRM-DPO) Contextual AI (KTO-Mistral-PairRM) Self-Rewarding 70B Iter1 Self-Rewarding 70B Iter2 Self-Rewarding 70B Iter3 DPO Iter1 DPO Iter2 DPO Iter3 (cid:98)π1) APO Iter1 ( APO Iter1 (π1) APO Iter2 ( (cid:98)π2) APO Iter2 (π2) (cid:98)π3) APO Iter3 ( Epochs AlpacaEval 2.0 LC Win Rate Win Rate Avg. Len - - - - - - Epoch 1 Epoch 3 Epoch 4 Epoch 1 - Epoch 1 - Epoch 4 17.11 26.39 29.71 - - - 25.35 26.92 29.95 25.35(+8.24) 28.23 29.56(+4.21) 29.73 31.73(+2.17) 14.72 30.22 33.23 9.94 15.38 20. 30.71 32.54 37.64 30.71 35.75 37.53 38.65 39.38 1676 2736 2521 1092 1552 2369 2529 2736 2369 2640 2636 2799 2950 29.56% LC win rate, which is 4.21% higher than iteration 1, and 2.64% higher than the 26.92% attained by vanilla DPO. By iteration 3, APO exhibits further 2.17% improvement over its previous iteration, maintaining lead of 1.78% over vanilla DPO. This is significant, especially considering the increasing challenge of achieving gains at higher performance levels. Notably, the momentum acceleration step is both training-free and data-free. It maximizes the potential of single iteration of training data, advancing further toward optimality without requiring additional inputs. While iterative training across three iterations with approximately 20k data points each often plateaus at around 30%, APO effectively surpasses this threshold by large margin. Next, we discuss the effect of generation length. All iterative DPO training baselines, including Snorkels Mistral-PairRM-DPO and our DPO Iter3, exhibit trend of increasing response length. This is inevitable because all response pairs used for training are generated online and ranked by PairRM, where longer sequences are more likely to be chosen as winners. This effect accumulates as the model trains on its own generation. We discuss this effect from the following perspectives: First, both humans and GPT-4 exhibit length bias, and our goal is not to prevent any length growth but to evaluate it from perspective of fairness. Therefore, we primarily refer to the LC win rate1, which reflects the predicted win rate when response lengths are at the same level as the baseline. Second, we note that APO Iter2 achieves significantly higher LC win rate and overall win rate 11 Table 2: MT-Bench Evaluation. The MT-Bench comprises total of 8 tasks. We present the average score for all 8 tasks, as well as the average score across 5 dimensions relevant to our training dataset: Writing, Roleplay, Extraction, STEM, and Humanities. Each task includes 2 progressive turns, and we provide the average score for the first turn, the second turn, and the overall average of both turns."
        },
        {
            "title": "Model",
            "content": "Mistral-7B-Instruct-v0.2 DPO Iter1 DPO Iter2 DPO Iter3 APO Iter1 APO Iter2 APO Iter3 MT-Bench 1st Turn 2nd Turn Average Average of Five Tasks 1st Turn 2nd Turn Average 8. 7.98 7.80 7.61 7.98 7.95 7.72 7.20 7.29 7.26 7.25 7.29 7.26 7.33 7. 7.63 7.52 7.43 7.63 7.60 7.53 9.42 9.56 9.36 9.23 9.56 9.56 9.69 8. 9.06 8.98 9.04 9.06 9.23 9.44 9.14 9.31 9.17 9.14 9.31 9.40 9.57 compared to Snorkels iterative DPO implementation, while generating much shorter responses (2636 characters compared to 2736). When compared to our DPO Iter3, APO Iter2 achieves comparable win rate, but also with shorter response lengths. This verifies faster convergence rate and enhanced performance of our methodology. Finally, this length growth is not inherent to our method but is result of the online training and PairRM ranking mechanism. It can be mitigated if we query winner/loser pairs and preference choices from GPT-4 or human, with much higher cost. MT-Bench Evaluation. In addition, we also evaluate our model using the MT-Bench tasks. Among the eight tasks, threemath, coding, and reasoningare evaluated by GPT-4 math judge, while the other fivewriting, roleplay, extraction, STEM, and humanitiesare evaluated by GPT-4 general judge. Table 2 illustrates the results of iterative DPO and APO across all three iterations. Since the UltraFeedback dataset is not designed for math/code tasks, and our model has never seen correct answers, the ability to solve math questions is not directly related to our training objective. Therefore, we also provide an average of scores for the five tasks focusing on general instruction-following abilities. From Table 2, we see that APO exhibits superior performance on the five general tasks, showing consistent improvement with each iteration, with average scores progressing from 9.31 to 9.40 to 9.57. In contrast, both the base model and the iteratively trained DPO model maintain an average score of around 9.14. Moreover, even when math, coding and reasoning are included, the performance of APO in the second turn surpasses that of other models. However, because it is not specifically trained on math problems, the overall average score is slightly reduced. This decline in average is primarily due to saturation in the tasks where our method excels (achieving 9.57 out of 10), with limited room for further improvement in these areas. Consequently, the tasks where our model was not trained on become more detrimental when calculating an average of eight."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "Effect of Coefficient α. In Table 3, we explore how different choices of α might affect performance. At iteration 1, performance fluctuates when changing α, peaking at α = 0.3 with win rate of 12 28.23%. Lower and higher values of α yield less stable results, indicating that extreme values may either underutilize or overcompensate the momentum effect. At iteration 2, the model demonstrates performance improvements at both α = 0.3 and α = 0.6, with the highest win rate of 30.62% observed at α = 0.6. Although α = 0.3 continues to provide strong performance, it is surpassed by the higher α = 0.6, suggesting that the optimal α might increase with iterations as the model stabilizes. Nevertheless, we continue to use α = 0.3 at iteration 2, as our theoretical analysis assumes α as constant. While an adaptive approach to setting α might offer benefits, α = 0.3 has consistently shown to be the most stable and effective choice across different iterations. Table 3: The effect of different extrapolation strategy and parameters at iteration 1 and 2. Results are reported in terms of length-controlled win rate (%). The row labeled \"Start from (cid:98)π0\" represents (cid:98)π1\" corresponds to results at = 2 (π2). results at = 1 (π1). The row labeled \"Start from Additionally, the row labeled \"Start from π1\" shows results when the momentum vector is calculated using (cid:98)π2 π1 instead of (cid:98)π2 (cid:98)π1, which is not covered by our theoretical guarantee. Starting From α =0.1 α =0.2 α =0.3 α =0.4 α =0.5 α =0.6 26. 24.40 25.52 27.41 25.30 28.23 (cid:98)π0 (cid:98)π1 π 28.11 28.23 29.15 28.16 29.73 27.21 28.70 28.44 29.33 29.75 30.62 28. Effect of Extrapolation Strategy. When the momentum vector is calculated using (cid:98)π2 π1 (cid:98)π2 (cid:98)π1, the performance appears less stable, with the lowest LC win rate being 27.21. instead of Despite achieving competitive win rates at certain α values, the results are generally lower than those using the standard Nesterovs momentum scheme. Intuitively, (cid:98)π2 (cid:98)π1 = ((cid:98)π2 π1) + (π1 (cid:98)π1) includes accumulated information, acting as an exponential average of all previous updates. In contrast, if extrapolation is based on π1, the directions variance is significantly higher, leading to less predictable outcomes. This instability underscores the importance of adhering to the theoretically guaranteed strategy for consistent performance gains."
        },
        {
            "title": "6 Conclusions and Future Work\nIn this work, we studied the iterative preference optimization framework for aligning large language\nmodels (LLMs) with human preferences and showed that it resembles the proximal point method.\nBased on this observation, we introduced a general framework, APO, incorporating Nesterov’s\nmomentum technique. Theoretically, we show that our method achieves a faster convergence rate than\nthe standard iterative DPO and SPPO methods. Our experimental results demonstrate the superiority\nof APO over iterative DPO on the AlpacaEval 2.0 benchmark and on the instruction-following tasks\nof MT-Bench, achieving both accelerated convergence rate, and better final performance.",
            "content": "Limitation. Due to limited computational resources, we do not evaluate APO with the SPPO (Wu et al., 2024) loss function in the current experiments, and we plan to investigate it in our future work. Additionally, while our model demonstrates consistent improvements on instruction-following tasks, it faces challenges in solving math problems. This limitation is largely due to the choice of dataset and the restriction of not utilizing additional information from GPT-4 or human sources. In the future, we aim to address this by incorporating larger datasets and leveraging GPT-4 supervision."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Reinforcement Learning from Human Feedback Learning from human feedback in reinforcement learning can be traced back to Knox and Stone (2009); Wirth et al. (2017) and was later popularized by Christiano et al. (2017), which incorporated human preferences into deep reinforcement learning. Recently, RLHF has gained popularity in natural language processing and has become paradigmatic method for fine-tuning large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; OpenAI, 2023) to align with human objectives. The standard process for alignment with human feedback (Ouyang et al., 2022) requires both reward modeling and policy optimization. Recently, Rafailov et al. (2023) proposed the Direct Preference Optimization method, which replaces the reward modeling process with reparameterized reward and directly performs policy optimization using preference data. Similarly, Kawin et al. (2023) proposed the Kahneman-Tversky Optimization method (KTO) with reparameterized reward, while considering different human-aware loss functions. More recently, iterative variants of policy optimization have garnered increasing attention. Xu et al. (2023) investigated the iterative preference optimization method and proposed the PAIRWISE CRINGE algorithm, which iteratively generates new preference dataset using the current model and then updates the model with combination of the original preference data and the newly labeled data. Additionally, they studied the iterative version of Direct Preference Optimization (DPO) and demonstrated that it achieves better performance than standard PPO or DPO. Later, Yuan et al. (2024b) studied Self-Rewarding Language Models, where the algorithm iteratively updates the reference policy in the DPO method and uses LLM-as-a-Judge prompting (Zheng et al., 2024b) to provide its own rewards for the generated new preference data. Despite this work demonstrating the superiority of iterative optimization process, there is lack of theoretical foundations for these practical frameworks. Recently, Xiong et al. (2023) proposed Gibbs Sampling from Human Feedback, offering theoretical analysis with the aid of linear function approximation and the incorporation of an uncertainty bonus. Compared with previous methods, the reference policy remains fixed, the newly trained model is only used to generate new preference data, and this trained model will not be inherited in the subsequent iteration. On the contrary, our research focuses on studying the iterative DPO model without incorporating an uncertainty bonus, and it continually updates the reference policy throughout the process. Most of the works motioned above rely on the assumption that the latent preference distribution (y1 y2x) follows the Bradley-Terry (BT) model (Bradley and Terry, 1952)), and there exists series of works focusing on general preference, where human preference may not strictly be transitive. Under the general preference assumption, iterative optimization (Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Wu et al., 2024) is also employed to find the Nash-equilibrium policy. We leave the extension of our algorithm to general preference model in the future work. A.2 Accelerated Optimization with Momentum The idea of accelerating gradient methods has been extensively explored over the decades. One of the earliest contributions to this field was the Polyak momentum (Polyak, 1964), which achieved faster convergence by leveraging the history of previous iterates. However, this early approach sometimes failed to converge even for strongly convex objectives (Lessard et al., 2016). This was further refined by Nesterovs accelerated gradient (NAG) (Nesterov, 1983), with guarantee for 14 faster convergence rates for general smooth convex functions. Following these foundational works, other acceleration algorithms and analyses emerged for gradient descent and its proximal variants (Beck and Teboulle, 2009; Tseng, 2009; Taylor et al., 2017; Allen-Zhu and Orecchia, 2014; Bubeck et al., 2015a; Diakonikolas and Orecchia, 2017; Cohen et al., 2020). In addition, there has been another line of work in understanding Nesterovs acceleration through continuous analysis and Lyapunov function analysis (Su et al., 2016; Shi et al., 2019; Shi, 2021; Shi et al., 2022; Chen et al., 2022). Momentum techniques have also proven effective in accelerating minimax game settings (Jin et al., 2022; Kovalev et al., 2022; Thekumparampil et al., 2022; Li et al., 2023a; Yuan et al., 2024a). However, the application of acceleration techniques to LLMs remains relatively unexplored. The Proximal Point Method (PPM) (Moreau, 1965; Martinet, 1970; Rockafellar, 1976; Güler, 1992; Bauschke, 2011) is fundamental methodology that achieves optimality by iteratively solving auxiliary problems in regularized form. Generally, these auxiliary problems cannot be solved directly, and different approximation methods yield various types of acceleration (He and Yuan, 2012; Salzo et al., 2012). One of the connections between PPM and NAG is established in Ahn and Sra (2022), where the authors interpret the updates of NAG as different approximations of the proximal method. The Catalyst approach (Lin et al., 2015, 2018) represents another variant of PPM that achieves accelerated convergence through practical inexact schemes (Lin et al., 2015; Salzo et al., 2012). This method effectively integrates Nesterovs acceleration with the proximal point framework, making it applicable to wide range of machine learning algorithms, including gradient descent and block coordinate descent. It can be applied to variance reduction algorithms (Lin et al., 2015, 2018), demonstrating its strong adaptability to various machine learning scenarios. Despite the widespread adoption of momentum acceleration in both theoretical and practical applications, we are the first to establish connection between DPO and the proximal point method, linking iterative DPO with the Catalyst approach, and providing comprehensive theoretical analysis of this interpretation. Accelerated Preference Optimization with General Preferences In the previous sections, we assumed the existence of latent reward function and focused on the Bradley-Terry (BT) model. However, (Tversky, 1969) observed that the preferences across all possible responses may not be monotonous or transitive, which cannot be represented by latent reward function. In this section, we extend the analysis to environments with general preferences and focus on the win probability between different responses. In detail, for an environment with general preferences, we denote the probability that response y1 is preferred over response y2 given the context as P(y1 y2x), and we assume that the preference model is antisymmetric, such that P(y2 y1x) = 1 P(y1 y2x). Under this assumption, we (cid:2)P(y1 y2x)(cid:3) as the win probability for policy π against policy define the reward rπ,π = Ey1π,y2π π. Our goal is to identify the von Neumann winner (Dudík et al., 2015) with the preference model. Specifically, the von Neumann winner corresponds to the Nash equilibrium of the following two-player zero-sum game: (π, π) = arg max π = arg max π min π min π Ey1π,y2π (cid:2)P(y1 y2x)(cid:3) rπ,π. 15 B.1 Reduction to SPPO with ℓSPPO For the Self-Play Preference Optimization (SPPO) (Wu et al., 2024) algorithm, if we set the learning rate β in Algorithm 1 to be the inverse of the learning rate η in the SPPO algorithm, the preference optimization process can be expressed as follows: πt+1 = arg min π Exρ,yπt(x) (cid:34) (cid:34) = arg min π Exρ,yπt(x) β log log (cid:19) (cid:18) π(yx) πt(yx) η (cid:18) (cid:19) (cid:18) π(yx) πt(yx) (cid:18) P(y πtx) log Zπt(x) P(y πtx) log Zπt(x) (cid:19)(cid:35) (cid:19)(cid:35)2 (cid:105)2 Exρ,yπt(x) = arg min rπRt = arg min rπRt Exρ,y,yπt(x) Exρ,y,yπt(x) (cid:104) (cid:2)P(y yx)(cid:3) + log Zπt(x) rπ(x, y) Eyπt (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2 + Cπt (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2/2 = arg min rπRt (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2/2 + Cπt, + Exρ,y,yπt(x) yY πt(yx) exp (cid:0)ηP(y πtx)(cid:1) represents the partition function for behavior where Zπt(x) = (cid:80) (cid:2)P(y yx) 1(y yx)(cid:3)2 is the variance of behavior policy πt, policy πt, Cπt = Exρ,y,yπt(x) the second equation holds due to β = η1 and the last equation holds because y, collected under the same behavior policy. Therefore, the preference optimization process in SPPO is aligned with our Algorithm 1 using the SPPO loss function: ℓSPPO(rπ, x, yw, yl, πt) = (cid:0)rπ(x, yw) 1 + log Zπt(x)(cid:1)2 + 1 2 (cid:0)rπ(x, yl) + log Zπt(x)(cid:1)2. 1 2 B.2 Theoretical Analysis with General Preferences In Section 4, Theorems 4.4 and 4.8 analyze the performance of Algorithm 1 under the Bradley-Terry (BT) model. For general preference model, Theorem B.2 provides convergence rate for Algorithm 1 with the loss function ℓSPPO in Example 3.2, under the assumption of minimal sub-optimality gap. Assumption B.1 (Minimal sub-optimality gap with general preferences). For each prompt , we assume there exist unique optimal response such that for any other sub-optimal responses = , we have P(y yx) P(y yx) , Y. Theorem B.2 (APO with ℓSPPO). For general preference model with loss function ℓSPPO, under the mild assumptions of realizability (Assumption E.3), boundedness (Assumption E.4) and Assumption B.1, with probability at least 1 δ, the TV-distance between (cid:98)πT +1 and the optimal policy π(x) = is bounded by Exρ (cid:104) DTV (cid:32)(cid:115) (cid:0) (cid:98)πT +1(x), π(x)(cid:1)(cid:105) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) (cid:32) + exp (cid:18) (1 α) (cid:19)(cid:33) , 1 β 16 where the coverage coefficient κt is defined as: κt = max (x,y)X (cid:98)πT +1(yx)π π2 (yx) +1(yx) . Remark B.3. When the dataset size is sufficiently large (N ), Theorem B.2 suggests that APO converges to the optimal policy at rate of exp (cid:0) O(T /(1 α))(cid:1). When α = 0, our algorithm reduces to the standard SPPO algorithm in Wu et al. (2024), which only converges to the optimal policy at slower rate of exp (cid:0) O(T )(cid:1). It is worth noting that Wu et al. (2024) provides (cid:80)T t=1 πt. In comparison, our (cid:101)O(1/ result relies on minimal sub-optimality gap assumption (Assumption B.1) and provides faster convergence rate between the final policy (cid:98)πt+1 and the optimal policy π. As discussed in Remark 4.7, this assumption is used to ensure the uniqueness of the optimal policy, which is necessary for the convergence guarantee. ) sub-optimality gap guarantee for the average policy π = 1 T"
        },
        {
            "title": "C Proof of Main Results",
            "content": "C.1 Proof of Theorem 4.1 In this section, we provide the proof of Theorem 4.1, which is crucial for understanding the optimization dynamics of Algorithm 1. Proof of Theorem 4.1. Based on the definition of the reparameterized reward rt, we have rt(x, y) = β log (cid:98)πt+1(yx) β log πt(yx). (C.1) Furthermore, the extrapolation step (3.6) satisfies πt+1(yx) ="
        },
        {
            "title": "1\nZ′\nt(x)",
            "content": "(cid:98)πt+1(yx) (cid:0) (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α, (cid:98)πt+1(yx) (cid:0) where logarithm of both sides yields the following equation t(x) = (cid:80) (cid:98)πt+1(yx)/(cid:98)πt(yx)(cid:1)α represents the partition function. Taking the log πt+1(yx) = (1 + α) log (cid:98)πt+1(yx) α log (cid:98)πt(yx) log t(x). (C.2) For simplicity, we define lt(x, y) = β log (cid:98)πt+1(yx) β log (cid:98)πt(yx), and thus we have lt+1(x, y) = β log (cid:98)πt+2(yx) β log (cid:98)πt+1(yx) = rt+1(x, y) + β log πt+1(yx) β log (cid:98)πt+1(yx) = rt+1(x, y) + α(cid:0)β log (cid:98)πt+1(yx) β log (cid:98)πt(yx)(cid:1) β log = rt+1(x, y) β log t(x) + αlt(x, y), t(x) (C.3) where the second equation holds due to (C.1) and the third equation holds due to (C.2). Recursively using (C.3) over all iterations, we derive the following equation: lt(x, y) = rt(x, y) β log = rt(x, y) β log t1(x) + αlt1(x, y) t1(x) + αrt1(x, y) αβ log t2(x) + α2lt2(x, y) = . . . (cid:88) = αti rt(x, y) αti β log i1(x), (C.4) where we assume we have i=0 1(x) = 1 for simplicity. Finally, by summing (C.4) over all iterations 0 t, β log (cid:98)πt+1(yx) β log (cid:98)π1(yx) = = = (cid:88) (cid:88) j=0 i=0 (cid:88) (cid:88) αjiri(x, y) + αji β log i1(x) αjiri(x, y) + αji β log i1(x) i=0 (cid:88) j=i (cid:18) 1 1 α i=0 (cid:19) αt+1i 1 α ri(x, y) + (cid:18) 1 1 α (cid:19) αt+1i 1 α β log i1(x). (C.5)"
        },
        {
            "title": "Given that",
            "content": "(cid:98)π1 = πref, we have (cid:98)πt+1(yx) = πref(yx) exp β (cid:26) πref(yx) exp β (cid:26) (cid:88) (cid:18) 1 1 α (cid:18) 1 α i=0 (cid:88) i=0 (cid:19) αt+1i 1 α rt + (cid:18) 1 1 α (cid:19) αt+1i 1 α αt+1i 1 α (cid:19) (cid:27) rt . Thus, we complete the proof of Theorem 4.1. C.2 Proof of Theorem 4.4 β log i1(x) (cid:27) (cid:32) πref(yx) exp π t+1(yx) ="
        },
        {
            "title": "1\nZ∗\nt (x)\ny πref(y|x) · exp (cid:0) (cid:80)t",
            "content": "In this section, we provide the proof of Theorem 4.4. To simplify the notation, we define the auxiliary (cid:98)πt+1 in Theorem D.1, but using the latent reward policy, which is updated following the dynamics of r(x, y) instead of the reparameterized reward rt(x, y): (cid:88) αt+1i 1 α i=0(1/(1 α) αt+1i/(1 α)) r(x, y)/β(cid:1) is the partition where function for the auxiliary policy. With this notation, the following lemma provide upper bound for the statistical errors arising from the gap between the reparameterized reward rt(x, y) and the latent reward r(x, y). Lemma C.1. Under the mild assumptions of realizability (Assumption 4.2) and boundedness (cid:98)πT +1 and (Assumption 4.3), with probability at least 1 δ, the TV-distance between the policy auxiliary policy π is upper bounded by: (x) = (cid:80) r(x, y) (cid:18) (cid:33) , 1 α 1 β i=0 (C.6) (cid:19) +1 Exρ (cid:104) DTV (cid:0) (cid:98)πT +1(x), π +1(x)(cid:1)(cid:105) (cid:32)(cid:115) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α) (cid:33) , 18 where the coverage coefficient κt is defined as: κt = max (x,y)X (cid:98)πT +1(yx)π π2 (yx) +1(yx) . With the help of Lemma C.1, we start the proof of Theorem 4.4. Proof of Theorem 4.4. For each iteration [T ] and prompt , according to the definition of π t+1 in (C.6), we have π t+1(yx) = 1 (x) πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 1 α i=0 (cid:19) αt+1i 1 α (cid:33) r(x, y) (cid:88) (cid:18) 1 1 α (cid:19) αt+1i 1 α (cid:33) r(x, y) πref(yx) exp = πref(yx) exp (cid:32) (cid:32) 1 β 1 β i=0 (cid:18) + 1 1 α α (1 α)2 + αt+2 (1 α)2 (cid:19) r(x, y) (cid:33) . For simplicity, we set γ = ((t + 1)/(1 α) α/(1 α)2 + αt+1/(1 α)2)/β, and the sub-optimality gap for the policy (cid:98)πt+1 and fixed prompt can be denoted as: Eyπ(x) (cid:2)r(x, y)(cid:3) Eyπ (cid:80) t+1(x) (cid:2)r(x, y)(cid:3) yY πref(yx) exp (cid:0)γ r(x, y)(cid:1) r(x, y) yY πref(yx) exp (cid:0)γ r(x, y)(cid:1) (cid:80) = r(x, x) (cid:80) y=y = πref(yx) exp (cid:0)γ r(x, y) γ r(x, x) r(x, y)(cid:1) x)(cid:1) (cid:0)r(x, x)(cid:1) yY πref(yx) exp (cid:0)γ r(x, y) γ r(x, (cid:80) , where set z(x, y) = exp (cid:0)γ r(x, y) γ r(x, we have = arg maxyY r(x, y) denotes the optimal response given the prompt x. For simplicity, we πref(yx) z(x, y). With this notation, x)(cid:1) and Z(x) = (cid:80) y=y Eyπ(x) (cid:80) t+1(x) (cid:2)r(x, y)(cid:3) Eyπ (cid:2)r(x, y)(cid:3) πref(yx) z(x, y) log z(x, y) yY πref(yx) z(x, y) xx)(cid:1) (cid:80) y=y γ (cid:80) (cid:0)1 πref(y = = y=y γ (cid:80) (cid:80) y=y Z(x) log = πref(yx) z(x, y) log γ (cid:80) Z(x)/(cid:0)1 πref(y xx)(cid:1) γ (cid:0)Z(x) + πref(y (cid:16) yY πref(yx) z(x, y) xx)(cid:1)(cid:17) πref(yx)/(cid:0)1 πref(y xx)(cid:1) z(x, y) log z(x, y) yY πref(yx) z(x, y) (cid:16) (cid:80) y=y πref(yx) z(x, y)/(cid:0)1 πref(y xx)(cid:1)(cid:17) , (C.7) xx)(cid:1) = 1, and the last equation holds due to z(x, where the inequality holds due to (x) = log is convex function with the fact that (cid:80) πref(y auxiliary function for 0 < 1 πref(y xx)(cid:1)(cid:17) (cid:16)(cid:0)1 πref(y (cid:0)Z + πref(y Z/(cid:0)1 πref(y (Z) = log xx)(cid:1) log xx): (cid:16) xx)(cid:1)/πref(y (cid:17) xx) . x) = 1. Now, we consider the following πref(yx)/(cid:0)1 y=y With basic math calculation, we have (Z) = log (cid:32) (Z) (cid:18) 1 πref(y xx) (cid:19) (cid:18) 1 + log (cid:16)(cid:0)1 πref(y xx)(cid:1)/πref(y xx) (cid:17)(cid:19) (cid:0)1 πref(y xx)(cid:1)/ exp (cid:18) 1 + log (cid:16)(cid:0)1 πref(y xx)(cid:1)/πref(y xx) (cid:17)(cid:19)(cid:33) 0. Substituting the result (Z) 0 into (C.7), we have (cid:2)r(x, y)(cid:3) xx)(cid:1)(cid:17) Eyπ(x) (cid:2)r(x, y)(cid:3) Eyπ Z(x) log (cid:16) t+1(x) Z(x)/(cid:0)1 πref(y xx)(cid:1) xx) γ (cid:0)Z(x) + πref(y (cid:16)(cid:0)1 πref(y xx)(cid:1)/πref(y γ log (cid:17) = (cid:101)O (cid:18) (1 α)β (cid:19) . (C.8) Therefore, the sub-optimality gap between is bounded by (cid:98)πT +1 and the optimal policy π(x) = arg maxyY r(x, y) Exρ,yπ(x) = Exρ,yπ(x) + Exρ,yπ Exρ,yπ(x) (cid:104) + Exρ 2DTV (cid:2)r(x, y)(cid:3) Exρ,y(cid:98)πT +1(x) (cid:2)r(x, y)(cid:3) Exρ,yπ (cid:2)r(x, y)(cid:3) (cid:2)r(x, y)(cid:3) +1(x) (cid:2)r(x, y)(cid:3) Exρ,y(cid:98)πT +1(x) (cid:2)r(x, y)(cid:3) +1(x) (cid:2)r(x, y)(cid:3) Exρ,yπ (cid:98)πT +1(x), π (cid:0) +1(x)(cid:1)(cid:105) +1(x) (cid:2)r(x, y)(cid:3) Exρ,yπ(x) (cid:2)r(x, y)(cid:3) Exρ,yπ +1(x) (cid:2)r(x, y)(cid:3) + (cid:32)(cid:115) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) (cid:101)O (cid:19) (cid:18) (1 α)β + (cid:32)(cid:115) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) , where the first inequality holds due to the definition of TV-distance with the fact that r(x, y) [1, 1], the second inequality holds due to Lemma C.1 and the last inequality holds due to (C.8). Thus, we complete the proof of Theorem 4.4. 20 C.3 Proof of Theorem 4.8 In this section, we provide the proof of Theorem 4.8. Proof of Theorem 4.8. For each iteration [T ] and prompt , according to the definition of π t+1 in (C.10), we have π t+1(yx) = 1 (x) πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 1 α i=0 (cid:19) αt+1i 1 α (cid:33) r(x, y) (cid:88) (cid:18) 1 1 α (cid:19) αt+1i 1 α (cid:33) r(x, y) πref(yx) exp = πref(yx) exp (cid:32) (cid:32) 1 β 1 β i=0 (cid:18) + 1 1 α α (1 α)2 + αt+2 (1 α)2 (cid:19) r(x, y) (cid:33) . For simplicity, we set γ = (cid:0)(t + 1)/(1 α) α/(1 α)2 + αt+1/(1 α)2(cid:1)/β, and for each prompt , the KL-divergence between π t+1(x) and the optimal policy π(x) = arg maxyY r(x, y) can be denoted as: KL(cid:0)π(x)π t+1(x)(cid:1) = log = log t+1 π (cid:80) 1 (cid:0)y xx(cid:1) yY πref(yx) exp (cid:0)γ r(x, y)(cid:1) x)(cid:1) xx) exp (cid:0)γ r(x, πref(y πref(yx) exp (cid:0)γ r(x, y)(cid:1) xx) exp (cid:0)γ r(x, πref(yx) exp (cid:0)γ (cid:0)r(x, y) r(x, xx) exp (cid:0) γ (cid:0)r(x, πref(y x) r(x, y)(cid:1)(cid:1) πref(y x)(cid:1) xx) (cid:80) y=y πref(y (cid:80) y=y maxy=y x)(cid:1)(cid:1) = exp(γ) πref(y xx) (cid:32) = exp (cid:18) (1 α) (cid:19)(cid:33) , 1 β (C.9) = arg maxyY r(x, y) denotes the optimal response given the prompt x, the first inequality where holds due to the fact that log(1 + x) x, the second inequality holds due to (cid:80) πref(yx) 1 and the last inequality holds due to Assumption 4.6. Therefore, the TV distance between optimal policy π and y=y Exρ (cid:0) (cid:98)πT +1 can be upper bounded as following: (cid:104) DTV (cid:104) (cid:98)πT +1(x), π(x)(cid:1)(cid:105) (cid:98)πT +1(x), π"
        },
        {
            "title": "DTV",
            "content": "(cid:104) (cid:0) +1(x)(cid:1)(cid:105) t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 + Exρ (cid:33) (T + 1) (cid:80)T Exρ (cid:32)(cid:115) O"
        },
        {
            "title": "DTV",
            "content": "(cid:0)π(x), π +1(x)(cid:1)(cid:105) (cid:104) DTV (cid:0)π(x), π +1(x)(cid:1)(cid:105) + Exρ 21 (cid:32)(cid:115) (cid:32)(cid:115) (T + 1) (cid:80)T (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) (cid:33) + Exρ (cid:34)(cid:114) 1 DKL (cid:0)π(x)π +1(x)(cid:1) (cid:35) (cid:32) + exp (cid:18) (1 α) (cid:19)(cid:33) , 1 β where the first inequality holds due to DTV(X, ) DTV(X, Z) + DTV(Z, ), the second inequality holds due to Lemma C.1, the third inequality holds due to DTV(X, ) (cid:112)DKL(XY )/2 and the last inequality holds due to (C.9). Thus, we complete the proof of Theorem 4.8. C.4 Proof of Theorem B.2 In this section, we provide the proof of Theorem B.2. Similar to the proof of Theorem 4.4, we define the auxiliary policy, which is updated according to the dynamics of (cid:98)πt+1 as described in Theorem D.1. However, in environments with general preferences, latent reward r(x, y) may not exist. Instead, we use the win probability of response against the policy πt to replace the reparameterized reward rt(x, y). Specifically, we set: π t+1(yx) ="
        },
        {
            "title": "1\nZ∗\nt (x)",
            "content": "πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 α i=0 (cid:19) αt+1i 1 α (cid:33) , (x, y) (C.10) (cid:2)P(y yx)(cid:3) and where (x, y) = Eyπt i=0(1/(1 α) αt+1i/(1 (x, y)/β(cid:1) is the partition function for the auxiliary policy. With this notation, the folα)) lowing lemma provide upper bound for the statistical errors arising from the gap between the reparameterized reward rt(x, y) and the win probability πref(yx) exp (cid:0) (cid:80)t (x) = (cid:80) (x, y). Lemma C.2. Under the mild assumptions of realizability (Assumption E.3) and boundedness (Assumption E.4), the TV-distance between the policy is upper bounded by: (cid:98)πT +1 and auxiliary policy π +1 Exρ (cid:104) DTV (cid:0) (cid:98)πT +1(x), π +1(x)(cid:1)(cid:105) (cid:32)(cid:115) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:33) , where the coverage coefficient κt is defined as: κt = max (x,y)X (cid:98)πT +1(yx)π π2 (yx) +1(yx) . With the help of Lemma C.2, we start the proof of Theorem B.2. Proof of Theorem B.2. For each iteration [T ] and prompt , according to the definition of π t+1 in (C.6), we have π t+1(yx) ="
        },
        {
            "title": "1\nZ∗\nt (x)",
            "content": "πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 α i=0 (cid:19) αt+1i 1 α (cid:33) (x, y) 22 πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 αt+1i 1 α 1 α (cid:19) (cid:33) (x, y) , i=0 (cid:2)P(y yx)(cid:3). According to the Assumption B.1, for where the win probability (x, y) = Eyπt each prompt and behavior policy πt, the unique optimal response yx)(cid:3) Eyπt (C.11) For simplicity, we set γt = (cid:0)1/(1 α) αt+1i/(1 α)(cid:1)/β, and for each prompt , the KL-divergence between π t+1(x) and the optimal policy π(x) = x satisfies (x, y) + , = x. (cid:2)P(y yx)(cid:3) + = can be denoted as: x) = Eyπt (x, r (cid:2)P(y KL(cid:0)π(x)π t+1(x)(cid:1) = log = log t+1 π (cid:80) 1 (cid:0)y xx(cid:1) (cid:80) yY πref(yx) exp (cid:0) (cid:80)t xx) exp (cid:0) (cid:80)t πref(y πref(yx) exp (cid:0) (cid:80)t xx) exp (cid:0) (cid:80)t πref(yx) exp (cid:0) (cid:80)t y=y πref(y i=0 γi i=0 γi i=0 γi (x, y)(cid:1) x)(cid:1) (x, (x, y)(cid:1) x)(cid:1) (x, y) (cid:80) y=y i (x, x)(cid:1)(cid:1) maxy=y exp (cid:0) (cid:80)t x) (x, y)(cid:1)(cid:1) (x, (cid:0)r i=0 γi i=0 γi xx) (x, πref(y (cid:0)r xx) i=0 γi πref(y = exp( (cid:80)t πref(y (cid:20) i=0 γi) xx) (cid:18) = exp (cid:19)(cid:21) , 1 β (C.12) (1 α) where that log(1 + x) x, the second inequality holds due to (cid:80) holds due to (C.11). Therefore, the TV distance between optimal policy π and bounded as following: denotes the optimal response given the prompt x, the first inequality holds due to the fact πref(yx) 1 and the last inequality (cid:98)πT +1 can be upper y=y Exρ (cid:0) (cid:104) DTV (cid:104) (cid:98)πT +1(x), π(x)(cid:1)(cid:105) (cid:98)πT +1(x), π (cid:0)"
        },
        {
            "title": "DTV",
            "content": "Exρ (cid:32)(cid:115) (cid:32)(cid:115) (cid:32)(cid:115) (T + 1) (cid:80)T (T + 1) (cid:80)T (T + 1) (cid:80)T +1(x)(cid:1)(cid:105) t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:104)"
        },
        {
            "title": "DTV",
            "content": "(cid:0)π(x), π +1(x)(cid:1)(cid:105) + Exρ (cid:33) + Exρ (cid:104) DTV (cid:0)π(x), π +1(x)(cid:1)(cid:105) (cid:33) (cid:33) + Exρ (cid:34)(cid:114) 1 2 DKL (cid:0)π(x)π +1(x)(cid:1) (cid:35) (cid:32) + exp (cid:18) (1 α) (cid:19)(cid:33) , 1 β where the first inequality holds due to DTV(X, ) DTV(X, Z) + DTV(Z, ), the second inequality holds due to Lemma C.2, the third inequality holds due to DTV(X, ) (cid:112)DKL(XY )/2 and the last inequality holds due to (C.12). Thus, we complete the proof of Theorem B.2."
        },
        {
            "title": "D Proof of Lemmas in Appendix C",
            "content": "D.1 Proof of Lemma C.1 In this subsection, we provide the proof of Lemma C.1 and first propose the following lemmas. Lemma D.1. Under the mild assumptions of realizability (Assumption 4.2) and boundedness (Assumption 4.3), for each iteration [T ], with probability at least 1 δ, the estimation error can be upper bounded as follows: Exρ,(y1,y2)πt (cid:104)(cid:0)r(x, y1) r(x, y2) rt(x, y1) + rt(x, y2)(cid:1)2(cid:105) (cid:18) log (cid:0)Π/δ(cid:1) (cid:19) , where reparameterized reward rt(x, y) = β log (cid:98)πt+1(yx) β log πt(yx). Lemma D.2. For any R+, we have (1 + x) log x 1. With the help of Lemmas D.1 and D.2, we now begin the proof of Lemma C.1. Proof of Lemma C.1. For each iteration [T ] and prompt , the TV-distance between t+1(x) can be upper bounded as follows: (cid:98)πt+1(x) and π (cid:0) DTV t+1(x) (cid:98)πt+1(x), π (cid:20) 1 2 (cid:20) 1 2 t+1(x) = Eyπ Eyπ Eyπ t+1(x) 1 2 (cid:115) (cid:115) = 1 2 1 2 Eyπ t+1(x) Eyπ t+1(x) t+1(x)(cid:1) (cid:12) (cid:12) (cid:12) (cid:21) (cid:17) 1 (cid:21) (cid:12) (cid:98)πt+1(yx) (cid:12) (cid:12) π t+1(yx) (cid:16) 1 + (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:12) (cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) (cid:12) (cid:12) π t+1(yx) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) (cid:21) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) log (cid:98)πt+1(yx) π t+1(yx) Ey(cid:98)πt+1(x) 1 (cid:21) (cid:12) (cid:12) (cid:12) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) (cid:12) (cid:12) (cid:12) (cid:21)2 (cid:21)2 (cid:12) (cid:12) (cid:12) + 1 2 (cid:17)2(cid:21) + 1 2 Ey(cid:98)πt+1(x) Ey(cid:98)πt+1(x) (cid:17)2(cid:21) , (D.1) where the first inequality holds due to Lemma D.2, the second inequality holds due to + (cid:112)2(x2 + y2) for x, > 0, and the last inequality holds due to E[x]2 E[x2]. In addition, for each iteration [T ] and prompt , we have (cid:20)(cid:16) Eyπ t+1(x) Eyπ t+1(x) + 2DKL (cid:0) = Eyπ t+1(x) (cid:17)2(cid:21) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:98)πt+1(x)π (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) + Ey(cid:98)πt+1(x) (cid:17)2(cid:21) t+1(x)(cid:1) DKL (cid:17)2(cid:21) + Ey(cid:98)πt+1(x) (cid:0)π + Ey(cid:98)πt+1(x) (cid:20)(cid:16) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) t+1(x)(cid:98)πt+1(x)(cid:1) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) (cid:17)2(cid:21) (cid:17)2(cid:21) 24 (cid:20) + 2Eyπ t+1(x) = Eyπ t+1(x),y(cid:98)πt+1(x) (cid:21) (cid:20) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) Ey(cid:98)πt+1(x) log (cid:98)πt+1(yx) π t+1(yx) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) , (cid:21) (D.2) where the inequality holds due to KL-divergence is non-negative. According to Theorem 4.1 and definition of π in (C.6), we have t+ log (cid:98)πt+1(yx) π t+1(yx) = log (x) log Zt(x) + (cid:88) i= 1 β (cid:18) 1 1 α (cid:19) αt+1i 1 α (cid:0)ri(x, y) r(x, y)(cid:1). (D.3) Substituting (D.3) into (D.2), we have (cid:20)(cid:16) Eyπ t+1(x) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) Eyπ t+1(x),y(cid:98)πt+1(x) (cid:17)2(cid:21) (cid:20)(cid:16) + Ey(cid:98)πt+1(x) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) αt+1i (cid:0)ri(x, y) r(x, y)(cid:1) (cid:0)ri(x, y) r(x, y)(cid:1)(cid:17)2(cid:21) (cid:20)(cid:16) (cid:88) Eyπ t+1(x),y(cid:98)πt+1(x) Eyπ t+1(x),y(cid:98)πt+1(x) i=0 (cid:20) (t + 1) (cid:88) i=0 t+1i (cid:0)ri(x, y) ri(x, y) r(x, y) + r(x, y)(cid:1)2 α2 (cid:21) , (D.4) where αi = (cid:0)1/(1α)αi/(1α)(cid:1)/β and the last inequality holds due to CauchySchwarz inequality. According to the definition of coverage coefficient κt in (4.1), for policy (cid:17)2(cid:21) (cid:17)2(cid:21) (cid:98)πT +1, we have (cid:20)(cid:16) (cid:20)(cid:16) log (cid:98)πT +1(yx) π +1(yx) + Ey(cid:98)πT +1(x) log (cid:98)πT +1(yx) π +1(yx) Eyπ +1(x) (T + 1) (cid:88) β2(1 α)2 Eyπ +1(x),y(cid:98)πT +1(x) t=0 (T + 1) (cid:20)(cid:16) rt(x, y) rt(x, y) r(x, y) + r(x, y) (cid:17)2(cid:21) (cid:88) t=0 κt β2(1 α)2 Eyπ (x),y(cid:98)πt(x) (cid:20)(cid:16) rt(x, y) rt(x, y) r(x, y) + r(x, y) (cid:17)2(cid:21) 1 σ2(1 + R) (cid:0)1 σ(1 + R)(cid:1)2 (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α) , (D.5) where the first inequality holds due to (D.4) with the fact that αi 1/(1 α), the second inequality holds due to the definition of coverage coefficient κt in (4.1) and the last inequality holds due to Lemma D.1 with union bound on the probability across all iterations. Finally, substituting (D.5) into (D.1), we have Exρ (cid:104) DTV (cid:0) (cid:98)πT +1(x), π +1(x)(cid:1)(cid:105) 25 Exρ (cid:34)(cid:115) 1 (cid:20)(cid:16) Eyπ +1(x) (cid:17)2(cid:21) + 1 Ey(cid:98)πT +1(x) (cid:20)(cid:16) log (cid:98)πT +1(yx) π +1(yx) (cid:17)2(cid:21)(cid:35) log (cid:98)πT +1(yx) π +1(yx) (T + 1) (cid:80)T (cid:115) t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 . 1 σ(1 + R) (cid:0)1 σ(1 + R)(cid:1) Thus, we complete the proof of Lemma C.1. D.2 Proof of Lemma C.2 In this subsection, we provide the proof of Lemma C.2 and first propose the following lemma. Lemma D.3. Under the mild assumptions of realizability and boundedness (see detailed definitions in Appendix E.3), for each iteration [T ], with probability at least 1 δ, the estimation error can be upper bounded as follows: Exρ,(y1,y2)πt (cid:104)(cid:0)r (x, y1) (x, y2) rt(x, y1) + rt(x, y2)(cid:1)2(cid:105) (cid:18) log (cid:0)Π/δ(cid:1) (cid:19) , where the win probability β log (cid:98)πt+1(yx) β log πt(yx), (x, y) = Eyπt (cid:2)P(y yx)(cid:3) and reparameterized reward rt(x, y) = With the help of Lemma C.2, we start the proof of Lemma C.2. The proof is similar to the proof of Lemma C.1, however, it is worth noting that this proof relies on different auxiliary policy based on the win probability rather than the latent reward: π t+1(yx) ="
        },
        {
            "title": "1\nZ∗\nt (x)",
            "content": "πref(yx) exp (cid:32) 1 β (cid:88) (cid:18) 1 α i=0 (cid:19) αt+1i 1 α (cid:33) , (x, y) where the win probability α) αt+1i/(1 α)) (x, y) = Eyπt (cid:2)P(y yx)(cid:3) and (x, y)/β(cid:1) is the partition function for the auxiliary policy. (x) = (cid:80) πref(yx)exp (cid:0) (cid:80)t i=0(1/(1 Proof of Lemma C.2. For each iteration [T ] and prompt , the TV-distance between t+1(x) can be upper bounded as follows: (cid:98)πt+1(x) and π (cid:0) (cid:98)πt+1(x), π DTV (cid:20) 1 2 (cid:20) 1 2 t+1(x)(cid:1) (cid:12) (cid:12) (cid:12) = Eyπ Eyπ t+1(x) t+1(x) (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:21) (cid:17) 1 (cid:21) (cid:12) (cid:98)πt+1(yx) (cid:12) (cid:12) π t+1(yx) (cid:16) 1 + (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:12) (cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) (cid:12) (cid:12) π t+1(yx) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) 1 2 (cid:115) (cid:115) = 1 2 1 2 Eyπ t+1(x) Eyπ t+1(x) Eyπ t+1(x) (cid:12) log (cid:98)πt+1(yx) π t+1(yx) Ey(cid:98)πt+1(x) 1 2 + (cid:21)2 (cid:12) (cid:12) (cid:12) + 1 2 (cid:17)2(cid:21) + 1 2 26 (cid:21) (cid:12) (cid:12) (cid:12) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:12) (cid:12) log (cid:98)πt+1(yx) (cid:12) π t+1(yx) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) (cid:12) (cid:12) (cid:12) (cid:21)2 Ey(cid:98)πt+1(x) Ey(cid:98)πt+1(x) (cid:17)2(cid:21) , (D.6) where the first inequality holds due to Lemma D.2, the second inequality holds due to + (cid:112)2(x2 + y2) for x, > 0, and the last inequality holds due to E[x]2 E[x2]. In addition, for each iteration [T ] and prompt , we have (cid:20)(cid:16) Eyπ t+1(x) Eyπ t+1(x) + 2DKL (cid:0) = Eyπ t+1(x) (cid:17)2(cid:21) (cid:20)(cid:16) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:98)πt+1(x)π (cid:20)(cid:16) (cid:20)(cid:16) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) + Ey(cid:98)πt+1(x) (cid:17)2(cid:21) + Ey(cid:98)πt+1(x) (cid:0)π t+1(x)(cid:98)πt+1(x)(cid:1) t+1(x)(cid:1) DKL (cid:17)2(cid:21) (cid:20)(cid:16) + Ey(cid:98)πt+1(x) log (cid:98)πt+1(yx) π t+1(yx) (cid:20) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) (cid:21) Ey(cid:98)πt+1(x) (cid:20) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) , = Eyπ t+1(x),y(cid:98)πt+1(x) + 2Eyπ t+1(x) (cid:17)2(cid:21) (cid:17)2(cid:21) (cid:21) (D.7) where the inequality holds due to KL-divergence is non-negative. According to Theorem 4.1 and definition of π in (C.10), we have t+1 log (cid:98)πt+1(yx) π t+1(yx) = log (x) log Zt(x) + (cid:88) i=0 1 β (cid:18) 1 1 α (cid:19) αt+1i 1 α (cid:0)ri(x, y) (x, y)(cid:1). (D.8) Substituting (D.8) into (D.7), we have (cid:20)(cid:16) Eyπ t+1(x) log (cid:98)πt+1(yx) π t+1(yx) (cid:20)(cid:16) Eyπ t+1(x),y(cid:98)πt+1(x) (cid:17)2(cid:21) (cid:20)(cid:16) + Ey(cid:98)πt+1(x) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) (cid:17)2(cid:21) log (cid:98)πt+1(yx) π t+1(yx) log (cid:98)πt+1(yx) π t+1(yx) Eyπ t+1(x),y(cid:98)πt+1(x) Eyπ t+1(x),y(cid:98)πt+1(x) (cid:20)(cid:16) (cid:88) i=0 (cid:20) (t + 1) αt+1i (cid:0)ri(x, y) (x, y)(cid:1) (cid:0)ri(x, y) (x, y)(cid:1)(cid:17)2(cid:21) t+1i (cid:0)ri(x, y) ri(x, y) α2 (x, y) + i (x, y)(cid:1)2 (cid:21) , (D.9) (cid:88) i=0 where αi = (cid:0)1/(1α)αi/(1α)(cid:1)/β and the last inequality holds due to CauchySchwarz inequality. According to the definition of coverage coefficient κt in (4.1), for policy (cid:98)πT +1, we have (cid:20)(cid:16) log (cid:98)πT +1(yx) π +1(yx) (cid:17)2(cid:21) + Ey(cid:98)πT +1(x) (cid:20)(cid:16) log (cid:98)πT +1(yx) π +1(yx) (cid:17)2(cid:21) Eyπ +1(x) (T + 1) (cid:88) 1 β2(1 α)2 Eyπ +1(x),y(cid:98)πT +1(x) t=0 (T + 1) (cid:20)(cid:16) rt(x, y) rt(x, y) t (x, y) + (x, y) (cid:17)2(cid:21) 27 (cid:88) κt β2(1 α)2 Eyπ t=0 (cid:18) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (x),y(cid:98)πt(x) (cid:20)(cid:16) rt(x, y) rt(x, y) (x, y) + (x, y) (cid:17)2(cid:21) (cid:19) , (D.10) where the first inequality holds due to (D.9) with the fact that αi 1/(1 α), the second inequality holds due to the definition of coverage coefficient κt in (4.1) and the last inequality holds due to Lemma D.3 with union bound on the probability across all iterations. Finally, substituting (D.10) into (D.6), we have +1(x)(cid:1)(cid:105) (cid:20)(cid:16) (cid:98)πT +1(x), π 1 2 Eyπ +1(x) Exρ (cid:0) (cid:104) DTV (cid:34)(cid:115) Exρ (cid:115) (cid:18) (T + 1) (cid:80)T t=0 κt log (cid:0)T Π/δ(cid:1) β2(1 α)2 (cid:19) . log (cid:98)πT +1(yx) π +1(yx) (cid:17)2(cid:21) + 1 2 Ey(cid:98)πT +1(x) (cid:20)(cid:16) log (cid:98)πT +1(yx) π +1(yx) (cid:17)2(cid:21)(cid:35) Thus, we complete the proof of Lemma C.2."
        },
        {
            "title": "E Proof of Lemmas in Appendix D",
            "content": "E.1 Proof of Lemma D.1 In this section, we adapt the previous results on the estimation error from maximum likelihood estimation to the Bradley-Terry (BT) model (Bradley and Terry, 1952) and provide the proof of Lemma D.1. We start the analysis with the conditional probability estimation setting. For an instance space and target space Y, we collect the dataset = (zi, oi)N , where zi is i=1 sampled from reference policy zi µi, and oi is then generated with the latent conditional density (oizi) = (zi, oi). Under this situation, the maximum likelihood estimator across function class : can be denoted as follows = arg max (cid:88) i=1 log (zi, oi). For an i.i.d. sampled dataset D, where µ1 = µ2 = . . . = µN , Geer (2000) (Chapter 7) provides upper bounds for the estimation error between and the hidden function . Later, Agarwal et al. (2020) extended these results to reinforcement learning, allowing for martingale sampling process. Lemma E.1 (Theorem 21, Agarwal et al. 2020). For finite function class < +, if the latent function F, then with probability at least 1 δ, we have (cid:88) i=1 Ezµi (cid:13)f (z, ) (z, )(cid:13) (cid:13) TV 2 log (cid:0)F/δ(cid:1). 2 (cid:13) With the help of lemma E.1, we start to prove Lemma D.1. 28 Proof of Lemma D.1. Based on the Bradley-Terry (BT) model defined in equation (2.1), for the prompt xi and the generated responses y1, y2, we have (y1 y2x) = σ(cid:0)r(xi, yi,1) r(xi, yi,2)(cid:1). In this context, we define the instance zi and target oi as follows zi = (xi, yi,1, yi,2), oi = 1(yi,1 yi,2) 1(yi,2 yi,1). For each policy π Π, we denote the reparameterized reward rπ(x, y) = β log π(yx) β log πt(yx). Thus, the density function for an instance = (x, y1, y2) with reward rπ can be represented as frπ (z, o) = σ (cid:16) (cid:0)rπ(x, y1) rπ(x, y2)(cid:1)(cid:17) , and we denote the density function class as = {frπ π Π}. Based on Assumption 4.2 and the definition of the Bradley-Terry (BT) model, we have fr F, and the latent density function satisfies (oz) = fr(z, o). Consequently, the maximum likelihood estimator can be expressed as frt = arg max frπ (cid:88) i=1 log frπ (zi, oi), where rt(x, y) = β log (cid:98)πt+1(yx) β log πt(yx). Thus, according to Lemma E.1, the estimation error between and rt can be upper bounded by (cid:88) i=1 Ezµi (cid:13)frt(z, ) fr(z, )(cid:13) (cid:13) TV 2 log (cid:0)Π/δ(cid:1). 2 (cid:13) (E.1) Since the dataset Dt is collected with reference policy πt, we have Exρ,(y1,y2)πt (cid:104) σ(cid:0)r(x, y1) r(x, y2)(cid:1) σ(cid:0)rt(x, y1) rt(x, y2)(cid:1)(cid:105)2 Exρ,(y1,y2)πt σ(cid:0)r(x, y1) r(x, y2)(cid:1) σ(cid:0)rt(x, y1) rt(x, y2)(cid:1)(cid:105)2 (cid:104) (cid:104) σ(cid:0)r(x, y2) r(x, y1)(cid:1) σ(cid:0)rt(x, y2) rt(x, y1)(cid:1)(cid:105) Exρ,(y1,y2)πt (cid:13)frt(z, ) fr(z, )(cid:13) (cid:13) 2 (cid:13) TV = 1 2 + 1 2 = Exρ,(y1,y2)πt log (cid:0)Π/δ(cid:1) , (E.2) where the second equation holds due to the definition of TV-distance with = (x, y1, y2) and the inequality holds due to (E.1). According to Assumption 4.3, we have r(x, y) [1, 1] and rt(x, y) [R, R], which implies that Exρ,(y1,y2)πt (cid:104)(cid:0)r(x, y1) r(x, y2) rt(x, y1) + rt(x, y2)(cid:1)2(cid:105) 1 σ2(1 + R) (cid:0)1 σ(1 + R)(cid:1)2 29 Exρ,(y1,y2)πt (cid:104) σ(cid:0)r(x, y1) r(x, y2)(cid:1) σ(cid:0)rt(x, y1) rt(x, y2)(cid:1)(cid:105)2 1 σ2(1 + R) (cid:0)1 σ(1 + R)(cid:1)2 log (cid:0)Π/δ(cid:1) , where the first inequality holds due to b minx[a,b] σ(x) σ(a) σ(b) with the fact that σ(x) = σ(x)(1 σ(x)) σ(1 + R)(cid:0)1 σ(1 + R)(cid:1) and the last inequality holds due to (E.2). Thus, we complete the proof of Lemma D.1. E.2 Proof of Lemma D.2 Proof of Lemma D.2. We prove this inequality based on two situations: 1 or < 1. For the case that 1, we have (1 + x) log = (1 + x) log (1 + x) 1 (cid:18) (cid:18) 1 (cid:19) 1 (cid:19) 1 = 1, where the first inequality holds due to log 1 1/x and the second inequality holds due to 1. We now consider the case where < 1. In this case, we have (1 + x) log = (1 + x) log (1 + x)(x 1) (x 1), where the first inequality holds due to log 1 and the second inequality holds due to < 1. Combining the results in these two cases, we complete the proof of Lemma D.2. E.3 Proof of Lemma D.3 In this section, we provide the proof of Lemma D.3, and we summarize our definitions and assumptions as follows. Similar assumptions are used in Rosset et al. (2024) to analyze the statistical error from the reparameterized reward. Definition E.2 (Feasible Policy Class). For each iteration [T ], let Πt Π represent the feasible policy class, which includes all potential policies πt generated by Algorithm 1 during iteration t, based on various possible samplings from the data collection process. Assumption E.3 (Realizability). For each iteration [T ] and each potential policy π Πt, we assume that the following updated policy also belongs to the policy class Π: (cid:18) (cid:19) (cid:98)π(x) = π(x) exp"
        },
        {
            "title": "1\nZπ(x)",
            "content": "π(x, ) β (cid:2)P(y yx)(cid:3) denotes the win probability against the behavior policy π and π(x, y) = Eyπ yY π(yx) exp (cid:0)r where Zπ(x) = (cid:80) Assumption E.4 (Boundedness). For each iteration [T ] and each potential policy πt Πt, we have π(x, y)/β(cid:1) is the partition function. Π, holds for all , and any potential collected data set Dt. β log (cid:98)πt+1(yx) πt(yx) [R, R], 30 Based on these assumption, we now begin the proof of Lemma D.3 Proof of Lemma D.3. For each policy π Πt and the corresponding reparameterized reward rπ(x, y) = β log (cid:0)π(yx)/πt(yx)(cid:1), we have Exρ,yπt(x) (cid:104) rπ(x, y) (x, y) + log Zπt(x) (cid:105)2 (cid:104) = Exρ,yπt(x) = Exρ,y,yπt(x) = Exρ,y,yπt(x) (cid:2)P(y yx)(cid:3) + log Zπt(x) rπ(x, y) Eyπt (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2 + Cπt (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2/2 (cid:105)2 + Exρ,y,yπt(x) (cid:2)rπ(x, y) 1(y yx) + log Zπt(x)(cid:3)2/2 + Cπt (cid:2)ℓ(rπ, x, yw, yl, πt)(cid:3) + Cπt, xρ,yw,ylπt yY πt(yx) exp (cid:0)r = (cid:2)P(y where Zπt(x) = (cid:80) yx) 1(y yx)(cid:3)2 is the variance of behavior policy πt and the third equation holds because y, collected under the same behavior policy. Therefore, the proximal point update described in equation (3.5) is equal to the least square estimator: (x, y)/β(cid:1) is the partition function, Cπt = Exρ,y,yπt(x) (cid:98)πt+1 arg min πΠ Exρ,yπt(x) (cid:104) rπ(x, y) t (x, y) + log Zπt(x) (cid:105)2 . (cid:2)P(y Conditioned on Assumption E.3, there exists policy π yx)(cid:3) β log Zπt(x). Under this situation, according to the standard concentration argument, the least-squares estimator t+1 Π such that rπ (x, y) = Eyπt t+1 (cid:98)πt+1 satisfies (cid:2)r Exρ,yπt(x) (cid:98)πt+1(x, y) rπ t+1 (x, y)(cid:3)2 (cid:18) (R + 1)2 log (cid:0)Π/δ(cid:1) (cid:19) , (E.3) where the ineqaulity holds due to Lemma 6 in Xu and Zeevi (2020) with the Assumption E.4. Therefore, we have Exρ,(y1,y2)πt (cid:104)(cid:0)r (x, y2) rt(x, y1) + rt(x, y2)(cid:1)2(cid:105) (x, y1) (cid:104)(cid:0)rπ (x, y1) rπ t+1 t+ = Exρ,(y1,y2)πt (cid:2)rπ 2Exρ,y1πt (cid:18) (R + 1)2 log (cid:0)Π/δ(cid:1) t+1 (cid:19) , (x, y2) rt(x, y1) + rt(x, y2)(cid:1)2(cid:105) (x, y1) rt(x, y1)(cid:3)2 + 2Exρ,y2πt (cid:2)rπ t+1 (x, y2) rt(x, y2)(cid:3)2 where the first inequality holds due to (x + y)2 2x2 + 2y2 and the last inequality holds due to (E.3). Thus, we complete the proof of Lemma D.3."
        },
        {
            "title": "F Additional Details in Experiments",
            "content": "F.1 Hyperparameters Our training is conducted on eight A100 GPUs, setting global batch size of 64. This is done through setting local batch size of 8 across all GPUs, with gradient accumulation step of 1. 31 Table 4: Open LLM Leaderboard Evaluation. Comparison of APO with state-of-the-art iterative training algorithms, results are reported using v0.4.1 of the lm-evaluation-harness library. Tasks and parameters follows Beeching et al. (2023). Models Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Avg. Avg. 5 Mistral-7B-Instruct-v0.2 63.74 DPO Iter1 DPO Iter2 DPO Iter3 APO Iter1 APO Iter2 APO Iter3 65.36 66.47 66. 65.36 66.98 65.78 66.81 69.19 69.93 68.07 69.19 70.98 69.75 77.90 77.11 78.14 77. 77.11 77.51 77.03 42.00 36.92 42.46 41.47 36.92 42.61 37.83 84.77 85.1 85.44 85. 85.1 85.32 85.43 59.12 65.72 70.47 58.89 58.29 58.85 58.89 58.69 58. 65.43 66.79 66.39 65.43 67.02 65.76 71.13 71.65 71.37 71.13 71.90 71.35 We utilize the RMSProp optimizer for each iteration, tuning the learning rate and β parameters. Although we initially considered range of learning rates from [5 107, 1 107, 5 108], we found that fixed learning rate of 5 107 while adjusting β across iterations results in enhanced performance. The final β value is set at 0.01 for the first iteration, 0.1 for the second iteration, and 1.0 for the third iteration. The extrapolation parameter α is set to be constant 0.3. In addition, the learning rate follows linear schedule with warm-up ratio of 0.1. For inference, we employ the vllm library (Kwon et al., 2023) for response generation, configured with temperature of 0.7 and top_p of 0.9. The maximum token length for response generation is set to 2048 tokens. Finally, evaluations across all benchmarks are conducted using eight A6000 GPUs. F.2 Open LLM Leaderboard Evaluation We provide the result of Open LLM Leaderboard. We calculate the average scores across six tasks, as well as the average of five tasks excluding GSM8k. This exclusion is due to our trainings emphasis on general instruction-following ability, with no exposure to correct answers for math problems. Table 4 shows that APO demonstrates robust performance across multiple tasks. Notably, at iteration 2, APO achieves the highest overall average score of 67.02, outperforming the baseline models. When focusing on the average of five key tasks, excluding GSM8k, APO iteration 2 again leads with an impressive score of 71.90. Although APO iteration 3 shows slight decrease in the overall average score, it maintains competitive edge. This decline is primarily attributed to the low correlation of multiple-choice question tasks with instruction-following abilities. we note that all trained baselines exhibit similar performance suboptimality on this metric, while the AlpacaEval performance increases. F.3 Response Examples in Different Iterations 32 Table 5: Generation example of our fine-tuned model by APO at different iterations. Prompt Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they dont take advantage of me? Iteration 1 Hello! Id be happy to help explain the rules of kickball to ensure that you feel confident and prepared for your game with your sister and her girlfriends. Kickball is fun, social, and inclusive game that combines elements of baseball and soccer. Heres breakdown of the essential rules: 1. Teams and Setup: - Divide into two teams, ideally of equal numbers. - Designate field with home base, first base, second base, third base, and kickball diamond. - Choose team to be in the field (defense) and one to be at bat (offense). 2. Equipment: - standard soccer ball (or large, soft kickball) is used. - No specialized equipment is required for players, except for comfortable clothing and athletic shoes. 3. Objective: - The objective of the game is for the team at bat to score runs by kicking the ball and advancing around the bases, while the fielding team tries to prevent this by getting the runners out. 4. Batting: - The game begins with pitch (roll) from the pitcher (field player) to the kicker (batter). - The kicker attempts to kick the ball with their foot (no toe-kicks or using hands are allowed). - After valid kick, the runner(s) may advance around the bases, trying to reach the next base before the ball reaches that base or defensive player tags them. 5. Fielding: - The fielding teams goal is to get the runners out. - Players can attempt to catch kicked ball in mid-air to retire the side (three consecutive outs), or they can touch base with the ball before runner reaches it to force them out. 6. Scoring: - run is scored when player advances around all four bases and returns to home plate. - Each inning is typically limited to specific number of outs or runs scored (agree on this beforehand). 7. Fair Play: - Encourage sportsmanship and ensure all players understand and respect the rules to maintain fun and enjoyable experience for everyone involved. 8. Additional rules: - Discuss any specific house rules or modifications to the game before starting, such as allowing multiple kicks per at-bat or implementing mercy rule if one team is significantly ahead in runs. hope this information is helpful! If you have any further questions or need clarification on any rules, feel free to ask. Good luck and have great time playing kickball with your sister and her girlfriends! 33 Table 6: Generation example of our fine-tuned model by APO at different iterations. Prompt Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they dont take advantage of me? Iteration 2 Hello! Its great that your sister and her friends have invited you to join them for game of kickball. Kickball is fun, social, and inclusive team sport that combines elements of baseball and soccer. Heres basic explanation of how the game is played to ensure you feel prepared and confident on the field. Keep in mind that variations may exist depending on the specific rules your group chooses to follow, but this outline should provide solid foundation. 1. Teams and Setup: Divide into two even teams, ideally with similar numbers of players (typically 8-12 per team). Designate one team as the home team (theyll be up to bat first), and the other as the visiting team. Set up diamond-shaped kicking area (the \"kickline\") approximately 60 feet from base diamond, similar to baseball. Mark out four bases (first, second, third, and home) in square shape around the diamond, with home plate located at the end of the kickline. 2. Objective: The objective of kickball is to score runs by hitting the ball with your foot and advancing around the bases before returning to home plate safely. The opposing team aims to get outs by fielding the ball and tagging runners with the ball or touching them with it while theyre not on base. 3. Batting: The batter stands at home plate and kicks the ball when its pitched towards them by the pitcher from the opposing team. runner may advance from their base at the start of each pitch, but they must return to that base before the ball is touched by fielder or thrown to that base. If runner advances too far before returning to their base, they risk being tagged out in force play. 4. Fielding: Fielders position themselves around the diamond and outfield to catch fly balls or field grounders. They can also attempt to tag runners with the ball while theyre advancing between bases or when they reach base. Once fielder catches fly ball or tags runner, they record an out. maximum of three outs per inning is typical. 5. Scoring: Runners score by advancing around the bases and returning safely to home plate. run is scored when player reaches home plate after kicking the ball and successfully advances around all four bases without being tagged out or reaching base before preceding runner is retired (resulting in that runner being forced out). 6. Inning Structure: Each team gets an opportunity to bat in each half-inning. The inning ends when three outs are recorded by the fielding team. Typically, games consist of multiple innings until one team reaches predetermined number of runs (e.g., five or ten). 7. Fair Play: Ensure that everyone follows the rules and plays in good spirits. Kickball is meant to be an enjoyable experience for all involved, so focus on having fun while staying mindful of safety and fairness. If you feel unsure about any aspect of the game or suspect your sister and her friends might be taking advantage of you, communicate openly with them about your concerns and work together to find solutions that maintain balanced and enjoyable playing field for everyone. 34 Table 7: Generation example of our fine-tuned model by APO at different iterations. Prompt Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they dont take advantage of me? Iteration 3 Hello! Its great that your sister and her friends have invited you to join their kickball game. Kickball is fun, social, and inclusive sport, well-loved for its simplicity and energy. To ensure you feel confident and prepared for the game, heres basic explanation of how kickball rules typically work. Keep in mind that variations exist, so always clarify any specific rules with your group before playing. 1. Teams and Setup: Divide into two teams, ideally through an evenly distributed random draw or team selection process. Designate field with clear boundaries using cones or markers 2013 diamond shape or rectangle works well. Each team defends designated home base (typically marked by large cone or object). 2. Equipment: standard soccer ball is commonly used for kickball games. No specialized equipment is required beyond that, although gloves may be worn for comfort or to improve grip on the ball. 3. Objective: The objective of the game is to score points by kicking the ball and successfully advancing around the bases 2013 first base, second base, third base, and home plate 2013 before returning to home base. run is scored when player reaches home base after completing full circuit around the bases. 4. Starting the Game: coin toss determines which team bats first. The fielding team positions themselves around the bases and in the outfield, ready to receive the kicked ball. The batting team gathers near home plate with designated kicker ready to start the inning. 5. Kicking and Running: The kicker takes stance and kicks the ball forcefully with their foot, aiming for solid contact to launch the ball as far as possible. Runners on base attempt to advance based on where the ball lands and how the fielding team reacts. 6. Fielding Rules: Fielders must retrieve or touch the ball before runner advances to the next base. Once fielder has the ball, they can tag runners with it to record outs. However, runners can advance on missed catch or an overthrow. If fielder touches base before runner arrives, the runner is out. 7. Scoring: run is scored when batter safely kicks the ball and reaches first base, while teammates advance around the bases and return to home plate before the inning ends (three outs). 8. Inning Structure: Each team takes turns at-bat and in the field. Typically, an inning lasts until the fielding team records three outs, at which point the teams switch roles. game consists of multiple innings, with the team with the most runs at the end declared the winner. 9. Fair Play and Sportsmanship: Encourage friendly and respectful environment, following the spirit of the game. Ensure all players understand the rules, and focus on having fun rather than taking advantage of one another. If needed, consult your group to establish additional guidelines or modifications to accommodate skill levels and preferences. 10. Additional Tips: - Wear comfortable clothing and shoes appropriate for running and kicking. - Stay hydrated and take breaks as needed. - Communicate clearly with teammates and opponents. - Focus on improving your skills, both as kicker and runner, to contribute effectively to your team. With this foundational knowledge, you should feel more confident joining your sister and her friends for kickball game. Enjoy the experience and have fun! Table 8: Another generation example of our fine-tuned model by APO at different iterations. Prompt How did US states get their names? Iteration 1 The naming of US states reflects rich and varied history, influenced by explorers, Native American cultures, natural features, and political considerations. Here are some common ways US states got their names: 1. Names derived from Native American words: Many states names come from Native American languages or place names. For example, California comes from the Spanish rendition of the Native American word \"Quercus californius,\" meaning \"californio,\" or \"californian,\" which referred to the native people or the abundant california trees. Floridas name comes from the Spanish word \"Pascuan Floridus,\" meaning \"flowery land,\" term used by explorer Juan Ponce de Le00f3n, likely inspired by the abundant natural beauty he encountered. 2. Names inspired by explorers and European settlers: Some states are named after famous explorers, settlers, or colonial founders. For instance, Virginia was named after Queen Elizabeth of England, who was known as the \"Virgin Queen\" due to her status as an unmarried monarch. Georgia was named after King George II of Great Britain in honor of his support for the founding of colony there. 3. Names based on geographical features: Several states are named after prominent geographical features, such as mountains, rivers, or bodies of water. For example, Colorados name comes from the Spanish word \"colorado,\" meaning \"red,\" term often used to describe the reddish hue of the areas soil and water. Montanas name means \"mountainous\" in Latin. 4. Names reflecting historical events or early settlement patterns: Some states names are connected to significant historical events or early settlement patterns. For instance, Maine was originally called \"Norumbega\" based on tales of mythical northern land of abundance, but was later renamed by English explorer John Smith to reflect its northern location relative to Massachusetts. Texas was named by the Spanish as \"Tejas,\" meaning \"friends\" or \"allies,\" due to the friendly reception given to Spanish explorers by the local Caddo and Tonkawa tribes. 5. Names honoring political or ideological affiliations: few states names reflect political or ideological affiliations. For example, Vermont was originally called \"New Vermont\" or \"New Englands Plymouth\" due to its ties to the English county of Vermont and the Pilgrims of Plymouth Colony. Later, when it sought independence from New York, the name was adopted as tribute to the revolutionary ideals of the Republic of Vermont. Overall, US states names offer fascinating glimpse into the rich history of exploration, settlement, and political development that shaped the American nation. 36 Table 9: Another generation example of our fine-tuned model by APO at different iterations. Prompt How did US states get their names? Iteration 2 US states have varied histories regarding how they received their names, with some derived from native place names or geographical features, others from explorers or colonial founding figures, and still others from natural elements or symbolic concepts. Heres brief overview of how some common state names came to be: 1. Alabama: Derived from the Native American Chickasaw and Creek words \"alhahoma\" or \"allapaha,\" meaning \"thicketed place\" or \"place with ticklenut trees.\" 2. Alaska: Russian explorer Vitus Bering, who first documented the region in 1741, named it \"Alaska\" from the Russian words \"alas k0430044f,\" meaning \"great land\" or \"large country.\" 3. Arizona: Named after the Arizona Mining District, which in turn was derived from the Spanish term \"Arizonia,\" meaning \"arid\" or \"dry,\" likely in reference to the desert climate. 4. Arkansas: Originally called the \"Land of Hieroglyphics\" by early European explorers due to the abundance of Native American petroglyphs found in the region. Later, it was named Arkansas after the Quapaw tribes name for village near present-day Osceola, which translates to \"people of the south wind.\" 5. California: Spanish explorer Juan Rodriguez Cabrillo is credited with giving the name \"California\" to the region in 1542, possibly inspired by the ancient mythical island of Califoria described by Greek geographer Ptolemy as lying offshore west of the Pacific Ocean, abundant with gold and paradisiacal conditions. 6. Colorado: Derived from the Spanish word \"colorado,\" meaning \"red,\" likely in reference to the red soil and red sandstone formations found in the region. 7. Connecticut: Named by Dutch explorer Adriaen Block in honor of the Mohegan and Montauk Sachems (tribal leaders) Huskera or Quinipet and Sinigtot or Sitenico, whom he met in 1614. The name Connecticut comes from the Mohegan word \"quinnipiac,\" meaning \"long tidal river.\" 8. Delaware: Named after Sir Thomas West, 3rd Baron De La Warr, an English nobleman who served as the first colonial governor in 1630. The name Delaware comes from the Delaware River, which was named after the Lenape Native American tribe, whose name means \"upstream people\" or \"people of the cleared land.\" 9. Florida: Spanish explorer Juan Ponce de Le00f3n is credited with discovering Florida in 1513 and naming it after \"Pascua Florida,\" meaning \"feast of flowers,\" likely due to the abundant blooming vegetation he encountered during his expedition. 10. Georgia: Named after King George II of Great Britain in honor of his support for establishing colony there in 1732. 11. Hawaii: The islands indigenous Polynesian name, \"Hawaii Loa,\" translates to \"long home\" or \"homeland,\" reflecting the archipelagos unique identity as distinct and unified cultural entity. The name Hawaii became widely known to Europeans through British explorer James Cook, who visited in 1778 and adopted it for the larger island of Hawaii in his charts. 12. Idaho: Derived from Shoshone Native American word, \"hee-da-ho,\" meaning \"gem of the mountains\" or \"the sun comes from the waters.\" This name was popularized during the late 19th century when Idaho was being considered for statehood. 13. Illinois: Derived from Native American word, \"Illiniwek,\" meaning \"tribe of superior men\" or \"tribe dwelling properly.\" This term referred to various tribes within the Illini Confederacy, which inhabited much of present-day Illinois prior to European contact. 14. Indiana: Named after the Native American term \"Indian00e1,\" meaning \"land of the Indians,\" reflecting that the region was inhabited by various Native American tribes when European settlers arrived. 15. Iowa: Derived from Native American word, \"ayoua,\" meaning \"prairie\" or \"beautiful land.\" This name was popularized during the late 19th century when Iowa was being considered for statehood. These examples represent just fraction of US states and their unique origin stories. Many other states have names rooted in Native American languages, European explorers, geographical features, or symbolic concepts. 37 Table 10: Another generation example of our fine-tuned model by APO at different iterations. Prompt How did US states get their names? Iteration 3 US states have varied histories regarding how they earned their names. Here are some common ways and specific examples: 1. Indigenous Place Names: Many state names originate from Native American languages spoken by tribes inhabiting the regions prior to European exploration. For instance: - Florida: Derived from the Native American word \"Pvscaleola \" or \"Scal scal lla, \" meaning \"flowing water \" or \"land of flowers, \" depending on the specific indigenous tribe. European explorer Juan Ponce de Leon may have chosen this name based on reports of native peoples and their designations for the land. - Missouri: Derived from the Missouri River, which in turn took its name from the Miwoksi or Missouri Nation, Siouan-speaking people inhabiting the area at the time of French explorer Louis Jolliet and Jacques Marquettes passage in 1673. Their name translates roughly to \"people of large canoes. \" 2. European Explorers and Settlers: Many states were named after early European explorers, settlers, or monarchs. For example: - Virginia: Named after Queen Elizabeth of England in honor of her reign and patronage, specifically her title \"the Virgin Queen. \" Sir Walter Raleigh, who received royal charter to establish colonies in North America, chose this name. - California: Named by Spanish explorer Vasco N00fa00f1ez de Balboa in 1542, who gave it the Latinized name \"California \" after the Classical mythological figure Californias, who was believed to inhabit paradisiacal island on the western edge of the known world. 3. Natural Features: Some states bear names inspired by distinctive geographic or topographical features. For instance: - Colorado: Derived from the Spanish \"colorado, \" meaning \"red \" 2013 reference to the red soil and rock formations found throughout the region. - Mo ntana: Named for the Latin word \"montanus, \" meaning \"mountainous. \" 4. Historical Events: few states have names connected to significant historical events. For example: - Texas: Derived from the Spanish \"Tejas, \" meaning \"friends \" or \"allies. \" The name referred to the indigenous peoples who initially welcomed Spanish explorers in the late 16th century. Later, Mexico used the name when it ruled the region, and after Texas gained independence in 1836, the name remained. - Wisconsin: Named by French explorer Jean Nicolet in 1634 after the native peoples he encountered, the \"Wisconsin \" or \"Meskousing \" 2013 an Algonquian-speaking people. The name \"Wisconsin \" is believed to mean \"gathering place \" or \"place of the red people. \" 5. Symbolic Meanings: In some cases, state names reflect symbolic or metaphorical concepts. For example: - Nevada: Derived from the Spanish \"nevado, \" meaning \"snow-covered mountain range, \" which aptly describes the states mountainous terrain and its names association with the silver rush that brought settlers to the area in the late 19th century (the silver being referred to as \"white gold \" or \"silver snow \"). These examples illustrate just few ways US states have acquired their names throughout history. The specific origins of each state name are complex and often intertwined with the rich tapestry of indigenous, European, and American cultural influences that have shaped the United States. 38 References Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774 . Agarwal, A., Kakade, S., Krishnamurthy, A. and Sun, W. (2020). Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information processing systems 33 2009520107. Ahn, K. and Sra, S. (2022). Understanding nesterovs acceleration via proximal point method. In Symposium on Simplicity in Algorithms (SOSA). SIAM. Allen-Zhu, Z. and Orecchia, L. (2014). Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537 . Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z. et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403 . Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q. et al. (2021). Program synthesis with large language models. arXiv preprint arXiv:2108.07732 . Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M. and Calandriello, D. (2024). general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics. PMLR. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T. et al. (2022). Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 . Bauschke, H. (2011). Convex analysis and monotone operator theory in hilbert spaces. Beck, A. and Teboulle, M. (2009). fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences 2 183202. Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L. and Wolf, T. (2023). Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard. Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika 39 324345. Bregman, L. M. (1967). The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR computational mathematics and mathematical physics 7 200217. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. et al. (2023). Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 . 39 Bubeck, S., Lee, Y. T. and Singh, M. (2015a). geometric alternative to nesterovs accelerated gradient descent. arXiv preprint arXiv:1506.08187 . Bubeck, S. et al. (2015b). Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning 8 231357. Censor, Y. and Zenios, S. A. (1992). Proximal minimization algorithm with d-functions. Journal of Optimization Theory and Applications 73 451464. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G. et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 . Chen, S., Shi, B. and Yuan, Y.-x. (2022). Revisiting the acceleration phenomenon via highresolution differential equations. arXiv preprint arXiv:2212.05700 . Chen, Z., Deng, Y., Yuan, H., Ji, K. and Gu, Q. (2024). Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335 . Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E. et al. (2024). Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132 . Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S. and Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in neural information processing systems 30. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R. et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 . Cohen, M. B., Sidford, A. and Tian, K. (2020). Relative lipschitzness in extragradient methods and direct recipe for acceleration. arXiv preprint arXiv:2011.06572 . Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z. and Sun, M. (2023). Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377 . Diakonikolas, J. and Orecchia, L. (2017). Accelerated extra-gradient descent: novel accelerated first-order method. arXiv preprint arXiv:1706.04680 . Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P. and Hashimoto, T. B. (2023). Alpacafarm: simulation framework for methods that learn from human feedback. Dudík, M., Hofmann, K., Schapire, R. E., Slivkins, A. and Zoghi, M. (2015). Contextual dueling bandits. In Conference on Learning Theory. PMLR. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D. and Kiela, D. (2024). Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 . 40 Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K. and Zou, A. (2023). framework for few-shot language model evaluation. Geer, S. A. (2000). Empirical Processes in M-estimation, vol. 6. Cambridge university press. Güler, O. (1992). New proximal point algorithms for convex minimization. SIAM Journal on Optimization 2 649664. He, B. and Yuan, X. (2012). An accelerated inexact proximal point algorithm for convex minimization. Journal of Optimization Theory and Applications 154 536548. Hoang, T., Chris, G. and Braden, H. (2023). Iterative dpo alignment. Tech. rep., Snorkel AI. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. et al. (2023a). Mistral 7b. arXiv preprint arXiv:2310.06825 . Jiang, D., Ren, X. and Lin, B. Y. (2023b). Llm-blender: Ensembling large language models with pairwise comparison and generative fusion. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023). Jin, Y., Sidford, A. and Tian, K. (2022). Sharper rates for separable minimax and finite sum optimization via primal-dual extragradient methods. In Conference on Learning Theory. PMLR. Kawin, E., Douwe, Human-centered rep., Https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf. X., functions Winnie, loss Dan, (halos). and Tech. J. K. Contextual (2023). AI. Kiwiel, K. C. (1997). Proximal minimization methods with generalized bregman functions. SIAM journal on control and optimization 35 11421168. Knox, W. B. and Stone, P. (2009). Interactively shaping agents via human reinforcement: The tamer framework. In Proceedings of the fifth international conference on Knowledge capture. Kovalev, D., Gasnikov, A. and Richtárik, P. (2022). Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling. Advances in Neural Information Processing Systems 35 2172521737. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H. and Stoica, I. (2023). Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Lessard, L., Recht, B. and Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization 26 5795. Li, C. J., Yuan, H., Gidel, G., Gu, Q. and Jordan, M. (2023a). Nesterov meets optimism: rate-optimal separable minimax optimization. In International Conference on Machine Learning. PMLR. 41 Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P. and Hashimoto, T. B. (2023b). Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Lin, H., Mairal, J. and Harchaoui, Z. (2015). universal catalyst for first-order optimization. Advances in neural information processing systems 28. Lin, H., Mairal, J. and Harchaoui, Z. (2018). Catalyst acceleration for first-order convex optimization: from theory to practice. Journal of Machine Learning Research 18 154. Martinet, B. (1970). Régularisation dinéquations variationnelles par approximations successives. rev. française informat. Recherche Opérationnelle 4 154158. Moreau, J.-J. (1965). Proximité et dualité dans un espace hilbertien. Bulletin de la Société mathématique de France 93 273299. Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T., Michi, A. et al. (2023). Nash learning from human feedback. arXiv preprint arXiv:2312.00886 . Nesterov, Y. (1983). method for solving the convex programming problem with convergence rate o(1/k2). Proceedings of the USSR Academy of Sciences 269 543547. Nesterov, Y. (2008). Accelerating the cubic regularization of newtons method on convex problems. Mathematical Programming 112 159181. Nesterov, Y. (2013). Introductory lectures on convex optimization: basic course, vol. 87. Springer Science & Business Media. OpenAI, R. (2023). Gpt-4 technical report. arxiv 2303.08774. View in Article 2. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 2773027744. Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics 4 117. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D. and Finn, C. (2023). Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290 . Rockafellar, R. T. (1976). Monotone operators and the proximal point algorithm. SIAM journal on control and optimization 14 877898. Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A. and Xie, T. (2024). Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715 . Salzo, S., Villa, S. et al. (2012). Inexact and accelerated proximal point algorithms. Journal of Convex analysis 19 11671192. 42 Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 . Shi, B. (2021). On the hyperparameters in stochastic gradient descent with momentum. arXiv preprint arXiv:2108.03947 . Shi, B., Du, S. S., Jordan, M. I. and Su, W. J. (2022). Understanding the acceleration phenomenon via high-resolution differential equations. Mathematical Programming 170. Shi, B., Du, S. S., Su, W. and Jordan, M. I. (2019). Acceleration via symplectic discretization of high-resolution differential equations. Advances in Neural Information Processing Systems 32. Su, W., Boyd, S. and Candes, E. J. (2016). differential equation for modeling nesterovs accelerated gradient method: Theory and insights. Journal of Machine Learning Research 17 143. Swamy, G., Dann, C., Kidambi, R., Wu, Z. S. and Agarwal, A. (2024). minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 . Taylor, A. B., Hendrickx, J. M. and Glineur, F. (2017). Smooth strongly convex interpolation and exact worst-case performance of first-order methods. Mathematical Programming 161 307345. Thekumparampil, K. K., He, N. and Oh, S. (2022). Lifted primal-dual method for bilinearly coupled smooth minimax optimization. In International Conference on Artificial Intelligence and Statistics. PMLR. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S. et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 . Tseng, P. (2009). On accelerated proximal gradient methods for convex-concave optimization. 2008. Submitted to SIAM J. Optim . Tversky, A. (1969). Intransitivity of preferences:psychological review 76: 31-48.. 1972. Elimination by aspects: theory of choice 281299. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D. et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 2482424837. Wirth, C., Akrour, R., Neumann, G., Fürnkranz, J. et al. (2017). survey of preferencebased reinforcement learning methods. Journal of Machine Learning Research 18 146. Wu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y. and Gu, Q. (2024). Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675 . Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N. and Zhang, T. (2023). Gibbs sampling from human feedback: provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456 . Xu, J., Lee, A., Sukhbaatar, S. and Weston, J. (2023). Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682 . 43 Xu, Y. and Zeevi, A. (2020). Upper counterfactual confidence bounds: new optimism principle for contextual bandits. arXiv preprint arXiv:2007.07876 . Yuan, A., Li, C. J., Gidel, G., Jordan, M., Gu, Q. and Du, S. S. (2024a). Optimal extragradient-based algorithms for stochastic variational inequalities with separable structure. Advances in Neural Information Processing Systems 36. Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J. and Weston, J. (2024b). Selfrewarding language models. arXiv preprint arXiv:2401.10020 . Zheng, C., Wang, Z., Ji, H., Huang, M. and Peng, N. (2024a). Weak-to-strong extrapolation expedites alignment. arXiv preprint arXiv:2404.16792 . Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. et al. (2024b). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P. and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 ."
        }
    ],
    "affiliations": []
}