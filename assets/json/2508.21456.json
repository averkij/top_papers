{
    "paper_title": "Morae: Proactively Pausing UI Agents for User Choices",
    "authors": [
        "Yi-Hao Peng",
        "Dingzeyu Li",
        "Jeffrey P. Bigham",
        "Amy Pavel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences."
        },
        {
            "title": "Start",
            "content": "Morae: Proactively Pausing UI Agents for User Choices Yi-Hao Peng yihaop@cs.cmu.edu Carnegie Mellon University Dingzeyu Li dinli@adobe.com Adobe Research Jeffrey P. Bigham jbigham@cs.cmu.edu Carnegie Mellon University Amy Pavel amypavel@eecs.berkeley.edu UC Berkeley 5 2 0 A 9 2 ] . [ 1 6 5 4 1 2 . 8 0 5 2 : r Figure 1: Morae is an accessible user interface agent that proactively pauses automation at key decision points for blind and low-vision users to make choices. For example, when asked to buy the cheapest sweetened sparkling water, existing agents would automatically choose product even if multiple items have identical prices. Instead, Morae pauses automation and presents relevant product differences (e.g., flavors, ratings) and allows users to choose based on their preferences. By proactively detecting and pausing when user input is necessary, Morae allow users to actively express preferences during UI automation. Abstract User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt Part of this work was conducted at Adobe Research. users for clarification when there is choice to be made. In study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences. Keywords Agents; User Interface Agents; Proactive Agents; Human-Agent Interaction; Accessibility; Generative UI ACM Reference Format: Yi-Hao Peng, Dingzeyu Li, Jeffrey P. Bigham, and Amy Pavel. 2025. Morae: Proactively Pausing UI Agents for User Choices. In The 38th Annual ACM Symposium on User Interface Software and Technology (UIST 25), September 28-October 1, 2025, Busan, Republic of Korea. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3746059.3747797 This work is licensed under Creative Commons Attribution 4.0 International License. UIST 25, Busan, Republic of Korea 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2037-6/2025/09 https://doi.org/10.1145/3746059."
        },
        {
            "title": "1 Introduction\nA long-standing challenge in accessible computing is to make user\ninterfaces (UIs) both accessible and usable for people who are blind",
            "content": "UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. or low-vision (BLV) [11, 57]. Accessibility errors with user interfaces can range from those that make access impossible to those that severely decrease usability [11]. Many BLV people use screen readers, which convey UIs in linear form that can be slow and confusing, especially as UIs become more complex [11, 13]. Given the myriad challenges in making UIs accessible and usable, promising idea is to develop agents that can work with users to navigate UIs [12, 69], although until recently such agents were quite limited. Recent advances in large language and multimodal models (LLMs/LMMs) have resulted in much more capable UI agents [9, 32, 34, 38] that execute complex automation tasks based on natural language instructions. For example, on site like Target [5], user can instruct an agent to add the cheapest sweetened sparkling water to my cart. The agent then finds the search bar, inputs the query, sorts the results by price, and selects the first option (Figure 1). However, existing UI agents assume that users can visually follow the agents actions and understand the surrounding context. This assumption does not hold for BLV users. Consequently, even if the agent successfully adds the cheapest sweetened sparkling water, BLV user may remain unaware that three equally priced but differently rated flavor options were available, some of which the user might have preferred (Figure 1). To understand the challenges and opportunities blind users face when interacting with UI agents, we conducted field study with four BLV people using an open source agent to record real-world automation tasks and interaction traces. Our interviews with firsttime users revealed enthusiasm about automating tasks such as online shopping and scheduling meetings by simply providing few words, yet participants also noted several challenges. Participants sometimes faced difficulties identifying the actions available in unfamiliar interfaces 5% of user queries involved tasks that could not be executed (the not knowing what you dont know\" problem\" [13]). Among executable tasks, 19% were underspecified (e.g., required numerous fields/options without clear user guidance on what was required) and 13% presented multiple options (e.g., multiple cheapest hotels or fastest flights), which led the agent to pick an arbitrary option without providing users choice. Users also encountered challenges in tracking the agents actions and sought additional verification to detect potential errors. To address these challenges, we introduce Morae1, an accessible UI agent that supports BLV users to actively express preferences during UI automation. Morae proactively pauses at decision points whenever the user preference is unclear. Morae leverages large multimodal model (LMM) to interpret natural language commands and analyze UI representation (e.g., the web DOM) and screenshots. To balance effective task execution with user control, Morae introduces mechanism called Dynamic Verification of Ambiguous Choices, which internally verifies potential ambiguities while considering the progress of task execution. At each automation step, Morae generates and answers internal clarification questions based on relevant metadata such as the users queries, current UI state, and previous action history. When Morae identifies critical ambiguities requiring user input, Morae explicitly pauses automation and prompts users for clarification. (Figure 1). Morae also dynamically 1Morae, the Latin plural of mora, means pauses or intervals. In linguistics, mora marks the beat that measures syllable duration; this notion of incremental timing parallels the discrete pauses or steps in our agents action sequence during task automation. generates accessible and interactive UIs (Figure 3) that enable users to clearly specify their choices before automation continues. To further support screen reader users, Morae provides real-time audio feedback synchronized with each agent action. Users can explicitly verify task outcomes after each automation. Morae also supports users by describing which tasks are available and how users can complete these tasks, including step-by-step instructions for screen reader-based interactions (e.g., relevant keyboard shortcuts). We evaluated Morae through technical assessment of its core component (i.e., the pause detection) and user study examining its interaction experience. In our technical evaluation, we tested Morae across 256 tasks covering 8 UI types. Results indicated that Moraes approach, based on dynamic verification of ambiguity, significantly outperformed direct prompting, OpenAI Operator, and other baseline methods. Morae achieved higher task success rates and demonstrated superior performance in correctly detecting necessary pauses. In our user study involving 10 BLV participants, Morae enabled participants to make decisions that better aligned with their preferences while maintaining more diverse choices. Users reported stronger sense of control over the choices they made and achieved higher automation success rates compared to both the fully automated baseline agent and OpenAI Operator. In summary, our contributions are threefold: (i) We release the first dataset of real-world interactions between blind users and UI agents; the dataset lays foundation for future research on humanagent interactions that extends beyond task automation. (ii) We introduce Morae, an accessible UI agent that proactively pauses at unclear decision points, prompts users to express their preferences through generative UI, and provides in-situ feedback for informed decision-making. (iii) comparative study shows that Morae enables BLV users to express preferences more clearly and achieve higher task-completion rates than off-the-shelf UI agents."
        },
        {
            "title": "2.1 Accessible Language-based UI Assistants\nLanguage-based assistants transform the way BLV users interact\nwith digital devices. Mainstream assistants such as Siri, Alexa, and\nGoogle Assistant now perform daily tasks for users such as set-\nting reminders or initiating phone calls via voice commands. How-\never, these assistants primarily manage general system-level func-\ntions and rarely handle complex, application-specific interactions.\nApplication-specific macros [59] help automate repetitive actions\nwithin specific programs but generally lack flexibility for natural\nlanguage interaction. Recent research explores conversational UI\nagents for BLV users to bridge the gap between natural language\ncommands and application-level interactions [8, 33, 56, 58, 79].\nFor example, JustSpeak [79] enables voice-based control of mo-\nbile applications, allowing users to operate apps through speech.\nSimilarly, Captispeak [8] and ConWeb [58] support web browsing\nthrough natural language commands, providing more accessible",
            "content": "Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea online interactions. Phutane et al. [56] explored conversational agents that encourage UI exploration through dialogue-based interactions. More recently, researchers introduced Savant [33], an LLM-powered desktop agent for BLV users. Savant automates desktop tasks by translating natural language commands into series of predefined screen reader actions. This approach significantly improves efficiency and usability compared to traditional screen readers. However, like most user interface agents, Savant does not proactively seek clarification when user choices are unclear. Our study with BLV users identifies critical limitation: users frequently give ambiguous commands without awareness, and existing agents rarely prompt users to clarify their preferences. This lack of proactive clarification leads to missed opportunities for users to express their unique preferences. We thus introduce an agent that proactively pauses at critical choice moments and allows users to specify their preferences."
        },
        {
            "title": "2.2 Language-driven Interactive UI Agents\nThe idea of interacting with computers using natural language be-\ngan with SHRDLU, a pioneering conversational system introduced\nin 1968 [73]. The system allowed users to manipulate virtual objects\nin a simplified ‚Äúblock world‚Äù via English commands. SHRDLU es-\ntablished the foundational approach for natural language interfaces,\nand had a long-term impact on agent research in human-computer\ninteraction and machine learning [16, 26, 35, 37, 43, 49, 62, 71]. To-\nday, language-driven agents like TaxyAI [65] and Browser Use [44]\nextend natural language interfaces to automate real-world web\ntasks. Recent research has also focused on using LLMs and LMMs\nto improve the performance and reliability of these autonomous\nUI agents. Kim et al. [32] demonstrated that LLMs efficiently com-\nplete tasks in simplified UI benchmarks like MiniWoB++[38], even\nfrom minimal demonstrations. Similarly, AutoWebGLM [34] further\nshowed specialized language agents surpass GPT-4 on realistic web\nautomation tasks in WebArena [80] and Mind2Web [17].",
            "content": "Recent research on UI automation explores richer interactions between users and agents. MoTIF [14] introduced dataset for interactive vision-language navigation that captures ambiguous or infeasible commands. META-GUI [63] developed multimodal conversational agents interacting directly with mobile interfaces through natural conversation, removing backend API dependency. LLM4UI [71] leveraged large language models to perform diverse mobile UI tasks like screen summarization, question answering, and action execution through prompting. WebLINX [39] enabled conversational guidance for complex web navigation tasks through multiple dialogue turns. Most recently, CowPilot [29] introduced explicit buffer periods during automation. These pauses let users intervene, pause, or correct agent actions and allow task automation completed through human-agent collaboration. Despite significant progress, existing research generally assumes that users can visually track and verify agent actions. Thus, UI automation designed for blind or low-vision (BLV) users remains largely unexplored. Our research addresses this critical gap by creating UI agents that proactively pause at unclear decision points, explicitly prompt BLV users for clarification, and provide clear audio and screen-readerfriendly feedback. With the deliberate pauses and tailored feedback mechanisms, our agent supports BLV users to actively participate and make informed and unique choices during UI automation."
        },
        {
            "title": "3.1 Field Study",
            "content": "Participants. We recruited four BLV participants (three female, one male; demographic information of P1P4 are detailed in Appendix) via mailing lists. Two participants were totally blind, and two were legally blind. All participants regularly performed web UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. tasks in their professional roles, including accounting, software engineering, and education. None of the participants have prior experience using UI agents. Study Apparatus. For data collection, we developed an agent based on TaxyAI [65], an open-source, browser-based UI agent powered by an LLM (e.g., GPT-4 by default). Similar to many recent UI agents proposed by the research community [9, 32, 34, 43, 49] and industry [3, 4, 44], TaxyAI automates web tasks by translating user commands and web UI representation (the Web DOM) into actions (e.g., clicking buttons or entering text). Due to LLM token limits, the agent first simplifies the DOM (to produce simplified DOM). It recursively traverses the DOM to remove invisible elements and redundant text nodes directly under body. Interactive elements, identified by accessibility attributes (e.g., aria-label, role, name), are retained and assigned unique IDs. Non-interactive or empty elements are removed, and single-child nodes are merged when appropriate. The resulting simplified DOM includes only the necessary interactive components to allow efficient task processing. Next, the agent pairs user commands with relevant UI elements and performs the required actions. For example, given command like buy me the cheapest sparkling water to the cart, the agent sequentially completes actions to add the item to the cart (Figure 1). Users can follow the agents action steps through numbered list of reasoning and interaction history in the interface (Figure 2). We modified the agent so that it can take the screenshot as input to inform the actions, as well as log the model output and capture UI screenshot every step for our analyses. Additionally, we included the safety-check prompt (detailed in Appendix) as presented in prior exploration [76] to avoid fully automating through the high-risk tasks like checking out the orders for online shopping websites or deleting documents from cloud storage systems. We created written document to help guide BLV users install and use browser-based UI agents. The guide explains the concept of UI automation agents, details how to read an agents action history step by step, and outlines the potential outcomes and risks of UI automation with concrete example tasks. Data Processing and Annotations. We collected 638 unique user queries from BLV participants along with corresponding automation steps and results. Three participants used Windows (two primarily used NVDA, one used JAWS), and one participant used MacOS with VoiceOver. Two annotators independently reviewed each collected query using structured annotation process with three stages. First, annotators categorized queries as (i) valid and possible automation: meaningful queries supported by the UI, (ii) valid but impossible automation: meaningful queries unsupported by the UI, or (iii) invalid queries: queries without meaningful intent for automation. Second, for valid and possible automation queries, annotators examined screenshots, reasoning logs, and user queries to assess whether the agents reported completed state accurately reflected successful task completion. Annotators labeled tasks as either accurately completed: fully and correctly finished, inaccurately completed: the agent incorrectly indicated task completion without genuinely fulfilling the intended outcome, and incomplete: the agent was not able to complete the given task. Finally, for inaccurately completed tasks, annotators identified whether ambiguity existed such that user preference is not specified, and Figure 2: TaxyAIs interface features task command input, start/pause automation button, and action history section displaying step-by-step agent reasoning. Here, TaxyAI autonomously executes the task add the cheapest sweetened sparkling water to my cart on Target but does not ask users for their preferences even though several choices exist. documented the exact automation steps where additional user input or clarification would have been necessary. To ensure annotation consistency, annotators labeled the first 100 queries together, and achieved Cohens Kappa scores of 0.96 for query validity, 0.94 for task completion accuracy, and 0.82 for ambiguity and user input requirements. After resolving discrepancies and refining criteria, annotators independently labeled the remaining queries with evenly divided work."
        },
        {
            "title": "3.2 Dataset Analysis",
            "content": "Valid and invalid user queries. Our dataset contained 616 valid user queries and 22 invalid queries. The invalid queries typically included greetings (e.g., How are you doing today?) or compliments (e.g., You have done great job! Thank you!) directed toward the UI agent. After excluding these invalid cases, we analyzed the remaining queries to characterize query types, automation outcomes, and scenarios that required additional user input (More dataset information and analysis are detailed in Appendix). Types of user queries and task automation. Among the valid queries, 12 queries (2%) did not directly request task automation but instead asked about specific UI capabilities. For instance, one participant asked on Google Slides, Can insert video from my local computer onto the slide? Another participant queried Google Calendar capabilities by asking, Is it possible to automatically include Zoom meeting link instead of Google Meet in my calendar event? The remaining 604 queries explicitly requested task automation. From the 604 automation-oriented queries, the Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea annotators categorized 28 (5%) as impossible automation requests. These queries resulted in agent failure either because participants requested tasks on unrelated or incorrect websites (e.g., booking hotel on Targets website) or requested actions unsupported by the platform (e.g., generating images within Google Docs). Participants submitted the remaining 576 feasible automation queries across 40 different platforms, representing diverse UI categories. These platforms included e-commerce websites, travel booking services (e.g., flights, hotels, restaurants), calendar and scheduling applications, productivity tools (document and slide editors), communication tools (email and messaging platforms), social media (X, Reddit), online content services (video streaming, e-learning), and cloud storage (Google Drive, Dropbox). Within the 576 feasible queries, the agent reported reaching completed state for 484 (84%) tasks. The remaining 92 (16%) tasks were incomplete due to challenges in navigating complex, multi-level interfaces, performing granular UI interactions (such as hovering or drag-and-drop), or satisfying complicated user constraints. In summary, participants sometimes issued commands that the agent could not execute because they referred to features they had not yet encountered. We view those impossible commands as genuine instances of users discovering unknown interface elements. We also received non-task-related inputs (such as congratulatory messages praising the agent), which we excluded from our performance analysis because they did not affect task execution. States of task completion and necessity of user input. For queries marked as completed by the agent, the average query length was 10.3 words, and each involved an average of 4.5 automation steps with pauses occurring at step 3.1. Among these, only 189 tasks were correctly completed. These tasks mapped directly to one available UI option or interaction, which eliminated any ambiguity regarding user choices and preferences. The remaining 295 queries were inaccurately labeled as completed (i.e., inaccurately completed). Out of these inaccurately completed queries, 113 involved clear execution failures. For instance, the agent mistakenly indicated task completion after adding the cheapest item to the cart without first sorting the results. The other 182 inaccurately completed queries involved ambiguities due to incomplete user instructions or unspecified preferences. Specifically, 107 queries involved underspecified user commands. In these scenarios, essential UI fields either remained empty or kept default values because users did not specify their preferences clearly. Examples include cases where users indicated departure and arrival airports but left out critical information such as travel dates, ticket types (round-trip or one-way), or travel class (economy or business). Additionally, underspecified queries included ambiguous constraints, such as requesting the best chocolate on the website without defining criteria for best. The remaining 75 inaccurately completed queries occurred when multiple valid options or UI actions matched the users query (e.g., the example presented in Figure 1), but users did not explicitly specify their preference in the original commands. For example, the agent faced multiple flavored waters with identical lowest prices when tasked with selecting the cheapest one, or encountered several available layout styles for adding page numbers when the user had not indicated preferred style."
        },
        {
            "title": "3.3 Follow-up Interviews\nWe interviewed every participant from the field study. For each par-\nticipant we chose six tasks (three that involved ambiguous choices\nand three that did not) from two applications the participant tried\nto automate (details in Appendix). Participants compared the UI\nagent with a screen reader alone. They reported persistent nav-\nigation difficulties with traditional screen-reader workflows and\nagreed that the agent improved the task efficiency. However, they\nalso identified gaps for existing agents, especially when ambiguous\nchoices arose and the agent failed to ask clarifying questions.",
            "content": "How Do UI Agents Improve Task Execution? None of the participants had used UI agent before the study. After hands-on experience, every participant stated that language-based automation boosted both accessibility and efficiency. P2 remarked, The agent automates repetitive operations and lets me decide what want; it handles the rest. Participants completed 40% of the tasks with the agent versus 25% with only screen reader and worked roughly five times faster (42s per task versus 217s). They valued the agents ability to process complex or repetitive actions, which allowed them to focus on key decisions. What Challenges Remain? During the study the agent often acted without clarifying user intent. Interviews revealed that in 95% of these situations participants never realised that multiple valid options existed. P4 noted, Without this interview wouldnt know there were several choices at the same price. need prompts that surface differences and invite my input. Participants also struggled to specify complex preferences and to monitor progress. P3 said, When many fields appear, lose track of the required information. clear scaffold that lists pending items and allows review would help. Participants further highlighted limited real-time feedback. The visual action-history log offered some traceability, yet it seldom conveyed success or failure accurately. Several tasks appeared complete even when the agent had failed. Participants asked for audible or textual cues during execution and explicit confirmation at the end. Finally, participants wanted guidance on manual task completion to ensure independence when automation falls short. Design Opportunities for Accessible UI Agents. Insights from the interviews suggest five design directions that address the reported challenges and keep BLV users in control: D1: Active choice. Pause at decision points and describe key differences among alternatives. For example, list the cheapest items side-by-side so users can select before the agent proceeds. D2: Clear preference input. Provide structured widgets such as drop-downs or number pickers that capture details text commands might miss, enabling refinement without re-typing the full query. D3: Real-time feedback. Announce ongoing actions through audio or concise status text, instead of waiting until completion, to keep users oriented during lengthy or multi-step tasks. D4: Result verification. Prompt users to confirm outcomesfor example, Page numbers addedaccept or undo?and offer quick links to inspect changes, which prevents silent failures. D5: Task literacy. Explain the UIs available actions and outline manual workflows so users can complete tasks with screen reader when automation tools are unavailable. UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al."
        },
        {
            "title": "4.1 Dynamic Verification of Ambiguous Choices\nWe aimed to ensure that Morae pauses at appropriate times. To iden-\ntify suitable pause conditions, we analyzed our validation dataset.\nThis dataset comprises 30% (54 out of 182 tasks) of all annotated\ntasks marked as \"unclear user choice or preference\" during our field\nstudy. Based on this analysis, we introduced an ambiguity-aware\nalgorithmic framework. The framework dynamically determines\noptimal pause points by utilizing a self-ask-then-answer verifica-\ntion strategy (specified prompts and model parameters are detailed\nin Appendix):",
            "content": "At each interaction step ùëñ, given user command ùëÑ, observed UI state ùëâ (ùëñ), and executed action history ùêª (ùëñ), the agent follows three stages to determine the next actions: Stage 1: Critical Actions. Our agent aims to balance autonomous task execution with timely intervention. We define critical actions as those involve user-defined preferences or reveal essential UI details required for meaningful ambiguity verification. Actions at planned step ùëñ are divided into critical (ùëÉùëê (ùëñ)) and non-critical (ùëÉùëõ (ùëñ)): ùëÉ (ùëñ) = ùëÉùëê (ùëñ) ùëÉùëõ (ùëñ), ùëÉùëê (ùëñ) ùëÉùëõ (ùëñ) = . Critical actions are prioritized to ensure accurate ambiguity assessment such that the agent only pauses when necessary. Stage 2: Ambiguity Verification. For each step, the agent formulates prioritized ambiguity-verification questions to uncover and assess various aspects of potential ambiguity cases based on user command ùëÑ, observed UI state ùëâ (ùëñ), and executed action history ùêª (ùëñ). The example verification aspects include selection ambiguity, where multiple UI options or actions meet the users criteria, and specification ambiguity, where users command is incomplete or involving ambiguous requirements. The agent answers each verification question with one of four responses (yes, no, unanswerable and proceed, not important and proceed). An ambiguity indicator ùê¥(ùëñ) is defined: (cid:40) ùê¥(ùëñ) = 1, if any question returns yes, 0, otherwise. The agent also explicitly evaluates the sufficiency of observed UI details using indicator ùêº (ùëñ): (cid:40) ùêº (ùëñ) = 1, if agents recorded details enable informed user decisions, 0, otherwise. Stage 3: Decision Function. Finally, the decision function ùê∑ (ùëñ) combines task execution and ambiguity verification, explicitly determining preference-elicited pauses: ùê∑ (ùëñ) = Execute critical actions, Pause for clarification, Gather more UI details, Proceed with next planned actions, if incomplete, if ùê¥(ùëñ) = 1, ùêº (ùëñ) = 1, if ùê¥(ùëñ) = 1, ùêº (ùëñ) = 0, if ùê¥(ùëñ) = 0. Our algorithmic approach balances effective task execution with the need to detect and clarify unclear user choices or preferences. The agent dynamically verifies whether pausing is necessary at each interaction step. Our prompt design strategy builds general templates from field-study observations. These templates adapt to varied scenarios yet require few task-specific changes. The approach ensures broad applicability and enables straightforward reuse across diverse tasks."
        },
        {
            "title": "4.3 In-situ Feedback for Screen Reader Users\nTo support active decision-making, we incorporate multiple forms\nof in-situ feedback that help BLV users remain involved throughout\nUI automation. Drawing from feedback collected in our prior stud-\nies, our agent provides real-time auditory feedback corresponding\ndirectly to agent actions and states. For example, a distinct clicking\nsound plays when the agent selects buttons or links, while a typing\nsound indicates inputting text into fields. If the agent encounters\nambiguity and requires user clarification, it emits a unique prompt-\ning tone. Conversely, upon successfully completing an action, the\nagent provides a distinct confirmation sound. Furthermore, users",
            "content": "Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea have the option to request additional visual verification after completing tasks. As GPT-4o already drives the primary automation actions, we integrated Gemini-2.0-flash, another LMM, to perform supplementary visual verification. In addition, our agent actively supports blind users exploration and learning of interfaces by responding to user-posed questions about UI. The agent additionally classifies user queries into two categoriesquestions about the UI or commands for automation. To evaluate the accuracy of this classification approach, we compared interface-related question samples gathered during our field study against an equal number of randomly selected automation-related queries. This evaluation demonstrated the existing model can achieve accuracy in distinguishing between these categories, achieving 96% accuracy. When responding to interface-specific questions (i.e., what tasks can do with the UI and how to do specific tasks), the agent combines external knowledge encoded in the LMM with the current UI state observations. To provide personalized actionable guidance, we include the users screen reader choice gathered from the survey directly within the models prompt. For example, when user asks, How do find my recent emails in Gmail?\", the agent provides detailed, screen reader-specific instructions. These instructions clearly outline NVDA shortcuts, such as pressing NVDA + F7 to open the links list, selecting \"Inbox,\" and navigating emails with arrow keys, thereby facilitating precise and accessible interaction. Figure 3: Moraes interface features task command input field, start/pause automation button, an action history panel that displays the agents step-by-step reasoning, and dynamically generated UI elements for preference selection. In this scenario, Morae autonomously executes the command add the cheapest sweetened sparkling water to my cart on Target. At decision point that needs user input, Morae pauses, presents relevant information, and offers accessible interactive controls so BLV users can state their choices in structured manner."
        },
        {
            "title": "5.1 Methods\nWe constructed the test set from two groups of tasks across 20 differ-\nent application platforms. The first group included tasks identified\nduring our field study as having ambiguous user choices or unclear\npreferences. From this group, we selected 128 tasks (70%), sampling\nevenly from each of the eight application categories. The second\ngroup consisted of tasks that users marked as completed without\nany ambiguous preferences. Similarly, we randomly selected an-\nother 128 tasks from this group, ensuring equal representation from\nthe same eight categories. We ensured the reproducibility of all\nselected tasks without needing access to personal user accounts\nor data. For instance, tasks conducted on platforms like Google\nDrive were recreated using our own accounts to generate identical\ninteraction traces. This approach allowed us to accurately produce\nground-truth labels and evaluate the agent in our online setup. For\neach task in the test set, we manually recorded and verified two\ninteraction paths: (i) a complete sequence of steps required for\nsuccessful task completion without pausing, and (ii) a sequence of\nsteps leading up to the point when the agent paused to request user\ninput. On average, the agent required 5.4 interaction steps per task,\nwith pauses typically occurring at step 3.8.",
            "content": "Unlike conventional offline evaluations, which statically assess agents based on fixed queries, UI states, and interaction histories, we employed an online evaluation approach [48, 77]. Our online evaluation captures the dynamics between the agents execution steps and pausing decisions, agent behavior randomness, and variations in observed UI states. We evaluated each agent condition based on two performance criteria: (i) Task success rate: The agent must either fully execute all annotated ground-truth steps for tasks requiring no pauses or correctly pause exactly at the annotated step for tasks needing user input. (ii) Pausing performance: We categorized agent pausing decisions at the task level into four possible outcomes: True Positive (TP): The agent correctly pauses exactly at the annotated step in tasks requiring pause. False Positive (FP): The agent pauses incorrectly or unnecessarily. False positives occur when the agent pauses prematurely (before the annotated step) in pause-required tasks, or at any step in tasks requiring no pause. False Negative (FN): The agent fails to pause at the explicitly annotated pause step. This scenario includes cases when the agent continues without any pausing or pauses at later step after missing the correct annotated step. True Negative (TN): The agent successfully completes all annotated steps without pausing in tasks requiring no pauses. We exclude tasks from TN calculations if the agent neither pauses nor successfully completes the task. UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. Table 1: Task completion rate and pausing performance for each method we benchmark in our technical evaluation. Method Prompting Verifying-First-Step Verifying-Per-Step OpenAI Operator Verifying-Per-Step-with-Planning (Ours) Task Success Rate Pausing Performance Pause Required Pause Not Required Overall Precision Recall F1-score 18.0% 32.3% 41.1% 50.8% 65.6% 33.1% 25.0% 27.9% 55.5% 44.8% 25.5% 28.6% 34.5% 53.1% 60.0% 30.2% 32.6% 58.7% 18.6% 40.8% 57.9% 59.1% 28.4% 34.8% 41.7% 59.0% 55.2% 59.7% 69.8% 64.4% to balance task automation progress with proactive user input solicitation when user preferences appeared unclear. OpenAI Operator [3]: state-of-the-art UI agent [77], finetuned explicitly on UI operations. We used the official client interface (Figure 8) to execute all tasks. After each task execution completed, we closed the browser and reopened the browser for the next task. Due to the nondeterministic nature of the agent outputs (even when setting temperature for the underlying model to be 0), we ran each task query three times for each method and compare the outcomes with the groundtruth annotations. We report the averaged results as the final task performance for each condition."
        },
        {
            "title": "5.2 Results\nOur approach achieved the highest average task success rate of\n55.2%, outperforming the baseline Operator by 2.1%. The main\nimprovement came from tasks that required pauses, where our\napproach increased success rates from 50.8% (Operator) to 65.6%.\nWe also observed a significant recall improvement of 10.7% over\nOperator. The self-verification process primarily contributed to\nthis gain by improving sensitivity to unclear user choices, such\nas distinguishing multiple calendar entries labeled \"Tuesday\" for\nscheduling a meeting. Compared to other variants of our method,\nour approach consistently demonstrated higher success rates across\nboth pause-required and non-pause tasks. For tasks without pauses,\nthe agent effectively planned and prioritized critical actions, contin-\nuously monitored task progress, and anticipated potential needs for\nuser input. For tasks requiring pauses, the self-verification method\nmore accurately determined the correct moment to request user\ninput than simpler prompting methods. Additionally, our method\nbalanced proactive detection of pauses with continuous task execu-\ntion to avoid unnecessary pauses. Alternative variants relying only\non verification at initial or intermediate stages frequently paused\nprematurely to resolve ambiguities without fulfilling task require-\nments or obtaining sufficient context for user decision. While our\napproach still paused unnecessarily (precision errors) or missed op-\nportunities to pause (recall errors), it demonstrated overall greater\nrobustness compared to other methods in automating tasks while\naccurately identifying necessary pauses to clarify user preferences\n(More failure cases are detailed in Appendix).",
            "content": "Figure 4: The UI of OpenAIs Operator Agent (zoomed view). Users assign tasks via chat, monitor browser automation through live streaming and text updates, and manually intervene at any time. The example illustrates Operator autonomously executing the command add the cheapest sweetened sparkling water to my cart on Target without pausing to ask for the users choice. We compared several variants of our method as baselines. All these agent variations including our method are built on TaxyAI and utilized GPT-4o as their LMM backbone. Their primary differences were the instructions or mechanisms used to trigger pauses (detailed prompts in the Appendix). Additionally, we included OpenAI Operator as strong baseline. OpenAI Operator [3] was explicitly trained for UI operations and optimized for user interactivity and safety. The methods we compared were: Prompting: The agent received explicit instruction to pause whenever user choices or preferences appeared unclear, supplemented by three descriptive examples. Verifying-First-Step: The agent generated ambiguity verification questions at the start of task execution. Questions were resampled 3 times and only picked top-5 as the verification question set. Questions relied solely on the initial user query and UI observation and remained constant throughout task execution steps. Verifying-Per-Step: At every execution step, the agent generated new ambiguity verification questions based on the current user query, observed UI states, and prior action history. These verification questions differed at each step. Verifying-Per-Step-with-Planning (Our Method): At each step, our method dynamically generated ambiguity verification questions based on users query, observed UI and prior actions. Furthermore, the agent incorporated planning considerations Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea"
        },
        {
            "title": "6.1 Method",
            "content": "Participants. We recruited 10 BLV participants aged between 28 and 55 (U1U10; 4 female, 6 male; details in Appendix) through online mailing lists. The evaluation involved 2-hour remote Zoom session. Participants had diverse professional backgrounds including students, customer support specialists, accessibility consultants, and software engineers. All participants have extensive experience with screen readers, general web browsing, and AI-based accessibility tools such as ChatGPT, BeMyAI, and SeeingAI. None had previously participated in our formative study nor used the UI agent before. Participants received compensation of $100 per hour, and the institutions IRB approved the study . Procedure. The study began with demographic data collection and an overview highlighting core features of each agent. Before the study, participants reviewed text documentation for Morae, TaxyAI, and Operator and know how to use each agent independently. Participants completed three tasks per application across three popular websites: (i) Target [5]: adding the highest-rated beer to the cart, finding details of the latest video game deal of specific brand, and buying the best seller short (ii) Google Calendar [1]: turning off the notification, adding new event, and modifying existing event details. (iii) Google Docs [2]: reviewing the recent document edits, inserting page numbers into the document, and adding code block to the document. All tasks were sampled from the test set we used in our technical evaluation to further validate the results when users are actually involved. We matched task complexity between websites to ensure balanced comparison across agents. Participants experienced each agent in counterbalanced order. After using each agent, participants rated their experiences subjectively on 7-point Likert scale (7 = highly positive) for: Satisfaction with Choices: Satisfaction with choices made during the task. Awareness of Choices: Awareness of available choices provided by the agent. Control over Choices: Sense of active control during choicemaking. Ease of Choice-making: Ease of providing input when making choices. Awareness of Actions: Awareness of actions performed by the agent. Awareness of Results: Awareness of the outcomes resulting from agent actions. Perceived Usefulness: Overall perception of the agents usefulness for task automation. Confidence in Use: Confidence in independently using the agent for task automation. In our study, we measured user satisfaction with the task outcome through composite of related metrics: Satisfaction with Choices, Control over Choices, and Perceived Usefulness. We concluded with follow-up interviews exploring participants interaction strategies and perceptions of each agents strengths and limitations Analysis. All sessions were recorded, and qualitative data from interviews and user comments were transcribed. We categorized qualitative feedback into two main areas: (i) strategies participants adopted during interactions, and (ii) perceived benefits and limitations of each agent. Subjective Likert ratings across three conditions (TaxyAI, Morae, Operator) were analyzed using Kruskal-Wallis tests, followed by pairwise Mann-Whitney tests with Bonferroni correction. Objective measures, including task completion time, task success rate, and number of decisions aligned with user preferences, were compared using repeated-measures ANOVA with paired post-hoc t-tests. To quantify the diversity of user decision, we computed decision entropy ùê∑ùëí , which measures uncertainty or randomness in users choices. For task with up to ùëÅ options, entropy was defined as: ùê∑ùëí = ùëÅ ùëñ=1 ùëù (ùë•ùëñ ) log(ùëù (ùë•ùëñ )) where ùëù (ùë•ùëñ ) represents the probability of selecting option ùë•ùëñ . Higher entropy indicates greater diversity and autonomy in selections, while lower values suggest uniformity."
        },
        {
            "title": "6.2 Results\nAll participants (U1-U10) expressed a clear preference for Morae\nover TaxyAI and Operator. Our agent provided greater accessibility,\nease of use, and improved preference expression opportunities.\nMorae received significantly higher usefulness ratings (ùúá = 6.50,\nùúé = 0.50) compared to TaxyAI (ùúá = 3.20, ùúé = 0.87; ùëà = 0.0,\nùëù < 0.001) and Operator (ùúá = 5.60, ùúé = 0.49; ùëà = 55.0, ùëù =\n0.015). Participants reported significantly greater confidence in\nusing Morae independently (ùúá = 6.60, ùúé = 0.49) versus TaxyAI\n(ùúá = 2.20, ùúé = 0.75; ùëà = 0.0, ùëù < 0.001) and Operator (ùúá = 5.50,\nùúé = 0.67; ùëà = 64.0, ùëù < 0.001).\nActive User Involvement in Decision-Making. Participants spent\nsignificantly more time completing tasks with Morae (ùúá = 129.40\nsec, ùúé = 13.75) compared to TaxyAI (ùúá = 55.70 sec, ùúé = 8.51; ùë° (9) =\n12.56, ùëù < 0.01) and Operator (ùúá = 86.60 sec, ùúé = 9.11; ùë° (9) = 10.87,\nùëù < 0.01). This additional time was primarily due to Morae actively\nprompting participants with informed decision-making opportu-\nnities when multiple options arose. Correspondingly, participants\nmade significantly more preference-aligned choices on average\nwith Morae (ùúá = 4.03, ùúé = 0.75) compared to Operator (ùúá = 2.98,\nùúé = 0.69; ùë° (9) = 3.45, ùëù < 0.01) and TaxyAI (ùúá = 1.92, ùúé = 0.84;\nùë° (9) = 6.87, ùëù < 0.001). Decision-making entropy was also notably\nhigher with Morae (ùê∑ùëí = 1.58) compared to Operator (ùê∑ùëí = 0.86)\nand TaxyAI (ùê∑ùëí = 0.22), indicating greater diversity and autonomy\nin participant choices. As participants made more informed deci-\nsions, participants also successfully completed significantly more\ntasks on average using Morae (ùúá = 5.50, ùúé = 0.71) compared to\nOperator (ùúá = 3.90, ùúé = 0.57; ùë° (9) = 5.38, ùëù < 0.001) and TaxyAI\n(ùúá = 2.60, ùúé = 0.52; ùë° (9) = 10.19, ùëù < 0.001). Participants consis-\ntently rated Morae significantly higher on critical aspects related\nto choice-making, including Satisfaction with Choices (ùúá = 6.80,\nùúé = 0.40), Awareness of Choices (ùúá = 6.50, ùúé = 0.67), Control over",
            "content": "UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. Figure 5: The average ratings for each of three agent conditions (TaxyAI, Morae, Operator) across eight subjective evaluation criteria (Satisfaction with Choices, Awareness of Choices, Control over Choices, Ease of Choice-making, Awareness of Actions, Awareness of Results, Perceived Usefulness, Confidence in Use) for our user evaluation. Choices (ùúá = 6.70, ùúé = 0.46), and Ease of Choice-making (ùúá = 6.40, ùúé = 0.49), compared to both TaxyAI and Operator (all pairwise comparisons ùëù < 0.01). Qualitative feedback further reinforced these findings: participants highlighted Moraes ability to proactively pause and provide opportunitiy for user to express preferences. For instance, U4 stated,\"When selecting the best-rated beer or identifying product details at Target, Morae clearly described all available choices and allowed me to decide independently, unlike Operator or TaxyAI, which made selections on my behalf without detailed explanations and disclosure on potential multiple choices.\" Similarly, U7 remarked, \"With Morae, managing event details on Google Calendar was straightforward; was always aware of the available fields am missing or should choose from. With the interactive UIs that scaffold my decision process, could more easily choose what wanted while be aware of all default values that were filled.\" Despite all the benefits Morae provided, participants indicated that Morae could improve further by providing more interpretable information on the agents decision confidence. For instance, U3 expressed, appreciate Morae proactively pausing to let me make decisions, but wish the agent could share how confident it feels about its suggested options. confidence score or similar cue would help me decide when to interrupt and explore the UI myself. Additionally, participants recommended implementing customizable pause mechanisms to recognize individual differences in comfort and confidence with automation. U6 explained, Personally, am comfortable allowing the agent to proceed independently, but other BLV users might prefer more frequent pauses due to monitoring challenges. feature letting users define their preferred level of intervention would significantly enhance the experience. Better In-Situ Feedback for Agent Actions and Outcomes. Participants reported significantly higher Awareness of Actions and Awareness of Results with Morae (actions: ùúá = 6.20, ùúé = 0.60; results: ùúá = 6.40, ùúé = 0.49) compared to TaxyAI (actions: ùúá = 3.50, ùúé = 0.92; results: ùúá = 2.60, ùúé = 1.02; ùëà = 0.0, ùëù < 0.001) and Operator (actions: ùúá = 4.90, ùúé = 0.54; results: ùúá = 4.60, ùúé = 0.58; ùëà = 64.0, ùëù < 0.001). Morae provided clear audio feedback at each step and explicit task completion confirmations. This design notably improved participants confidence in recognizing successful interactions. Participant U5 explained: Morae clearly informed me through audio cues whenever successfully modified events in Google Calendar and confirmed each step explicitly. With Operator, often felt uncertain about whether my requested changes took effect. Although Operator demonstrated state-of-the-art agent capabilities, its reliance on remotely streamed visual screenshots limited accessibility. Participants reported that Operator felt straightforward to instruct initially but verifying task outcomes independently became difficult without local screen reader access. Consequently, participants needed greater trust in Operators remotely executed actions compared to Moraes immediate and verifiable feedback. Participants also found Moraes screen reader-specific guidance clearer and more instructive. U2 stated, \"When asked agents about how to insert page numbers in Google Docs, Morae explicitly guided me through each step with potential shortcut, unlike TaxyAI or Operator, which left me guessing about what actions should do if did the task by my own.\" Participants recommended improvements for Morae, including customizable audio feedback (U9) and additional modalities such as haptic signals (U1), to further enhance overall accessibility. Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea"
        },
        {
            "title": "7.2 Levels of Pause Necessity\nMorae now relies on a binary pause policy such that the agent\neither halt or proceed based on the user‚Äôs command, the visible UI,\nand earlier actions. Our study revealed diverse preferences among\nblind and low-vision creators: some steps require an immediate\nstop, whereas routine operations can continue without interruption.\nThese observations motivate a graded pause-necessity model that\nassigns a continuous score to each step, capturing both interface\nuncertainty and user-specific sensitivity. A calibrated threshold,\nlearned from interaction data, can tune the score for every user\nand context. Payment dialogs, for instance, would receive a lower\nthreshold than simple scrolling, triggering pauses more readily. The\ngraded model naturally positions Morae within mixed-initiative\ncontrol: the agent yields control when quantified uncertainty sur-\npasses the threshold. At such moments, Morae presents ranked\nalternative actions side-by-side and invites the user to explain why\none option best satisfies the goal. The contrastive prompt functions",
            "content": "as an implicit explanation, echoing contrastive techniques in explainable AI (XAI) [31, 60, 72] and unifying pausing, information disclosure, and user choice into trust-building strategy. Future work will refine the calibration procedure so that Moraes quantitative judgments align closely with individual preferences and the sensitivity of each task, thereby enhancing both personalization and automation effectiveness."
        },
        {
            "title": "7.4 Extending the Interaction Scope of Morae\nCurrently, Morae primarily operates as a web-based tool to enable\nbroad deployment and rapid iteration across online applications.\nHowever, extending the agent‚Äôs capabilities to desktop and mobile\nplatforms would significantly broaden its utility. Future work can\nintegrate more precise visual recognition models [51, 75, 78] along-\nside existing LMM models. Recent work [40] has demonstrated that\ngranular visual recognition can improve agents‚Äô semantic under-\nstanding of user interfaces. Such visual models directly extract UI\nstructures from pixel-level data and provide metadata similar to\nDOM trees, including detailed element positions and groupings.\nLeveraging granular visual parsing will enhance the agent‚Äôs capa-\nbility to reason accurately about visual semantics and automate\ncomplex tasks effectively across multiple software environments.\nAdditionally, expanding the agent‚Äôs native compatibility with vari-\nous operating systems would support research into users‚Äô long-term\nusage patterns and interactions with diverse digital tools. On the in-\nteraction aspect, Morae currently supports only basic UI operations,\nsuch as click actions and value inputs. Future development should\nbroaden the set of interaction methods. Direct calls to native UI\ncontrol APIs provided by major operating systems [7, 36, 53] will un-\nlock richer interactions and enable proactive agents to tackle more\ncomplex workflows in video editing [28, 50, 52], graphic-design\ncreation [21, 27, 54, 74], map navigation [22, 24, 30] and beyond.\nNative integration also delivers deeper functionality and finer con-\ntrol across web, mobile and desktop platforms. Although initially\ndeveloped to enhance accessibility for blind and low-vision (BLV)\nusers, expanding Morae to support additional disabilities requires",
            "content": "UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. further customization. Future development should include adaptations to feedback mechanisms and interface controls. For instance, simplifying text-based feedback can better support individuals with cognitive disabilities, while integrating customizable UI controls can better accommodate users with motor impairments. Future enhancements can also focus on compatibility with existing assistive technologies, increasing accessibility for broader range of users."
        },
        {
            "title": "Users to Express Choices",
            "content": "Our agent can act as proxy for BLV users to interact with digital interfaces that are difficult or inaccessible to use. However, we strongly encourage developers to create user interfaces that are technically accessible as well as intuitive and genuinely usable, which would also beneficial for assistive technology users such as screen reader users. Our goal extends beyond offering basic accessibility overlays or automating simple tasks. We developed our system inspired by users who face accessibility barriers when using complex software. The agent actively supports users to express and apply unique choices and preferences throughout task execution. Instead of focusing solely on robust automation that completes tasks automatically, our approach emphasizes collaboration. Our findings indicate the agent encourages users to actively explore available choices, express preferences clearly, and remain engaged and informed during the automation process. The agent proactively introduces interaction pauses to allow BLV users to express their choices clearlyopportunities which standard automation often neglects. The core value of our agent is not only successful task completion but enabling users to express meaningful choices, engage actively with applications, and learn from the automation process. Ultimately, our approach positions users as active collaborators who guide automation rather than passive observers."
        },
        {
            "title": "7.6 Beyond Accessible Automation: Broader",
            "content": "Agent-driven Applications Our user study highlights the potential of the agent as universal interaction tool capable of bridging language barriers. One participant (U5) successfully interacted with website in Chinese by issuing commands in Polish: dont speak Chinese at all, but when typed in Polish, the agent operated successfully and provided feedback in Polish! The agent not only made the website accessible non-visually but also bridged the language gap! Participants further recognized broader implications when agents interact with user interfaces. Another participant (U10) commented, Sometimes AI works like screen reader. If you make an interface accessible to us [screen reader users], you are also likely making it easier for AI to navigate! Such insights highlight that agents can extend beyond accessibility applications. For example, agents may automate accessibility testing [64], perform usability analyses, and ensure digital interfaces support diverse human users and AI agents. Difficulties agents encounter when interacting with user interfaces often reflect similar usability challenges faced by humans. Therefore, improving interface designs for agent interactions can potentially enhance usability for human users."
        },
        {
            "title": "8 Conclusion\nWe introduced Morae, an accessible UI agent that proactively pauses\nduring task automation to allow blind and low-vision users to ex-\npress their preferences and choices actively. Driven by insights\nfrom an in-depth field study and detailed interviews with four BLV\nindividuals, Morae proactively identifies critical decision points\nrequiring user input. Instead of assuming preferences, the agent\npauses at these junctures and dynamically creates accessible in-\nterfaces to enable users to interactively specify their preferences.\nMorae leverages large multimodal models to interpret user com-\nmands, analyze UI structures, systematically plan and execute tasks,\ndetect ambiguities proactively, and offer detailed contextual infor-\nmation along with in-situ feedback on agent‚Äôs actions and results.\nTechnical evaluations and user studies demonstrate that Morae\nsignificantly improve users‚Äô opportunities to make informed and\npersonally aligned choices, increases the diversity of made deci-\nsions, and achieves robust task completion. We believe Morae‚Äôs\ndesign principles and outcomes provide an essential foundation\nfor future research, which advances agents that closely align with\nhuman experiences and amplify user active input in real-world\ninteractions.",
            "content": "References [1] [n. d.]. Google Calendar. http://calendar.google.com/. Accessed: 2025-04-09. [2] [n. d.]. Google Docs. https://docs.google.com/. Accessed: 2025-04-09. [3] [n. d.]. Introducing Operator. https://openai.com/index/introducing-operator/. Accessed: 2025-04-09. [4] [n. d.]. Project Mariner. https://deepmind.google/technologies/project-mariner/. Accessed: 2025-04-07. [5] [n. d.]. Target. https://www.target.com/. Accessed: 2025-04-09. [6] Salvatore Andolina, Valeria Orso, Hendrik Schneider, Khalil Klouche, Tuukka Ruotsalo, Luciano Gamberini, and Giulio Jacucci. 2018. SearchBot: Supporting voice conversations with proactive search. In Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing. 912. [7] Apple. 2024. Mac Automation Scripting Guide: Automating the User https://developer.apple.com/library/archive/documentation/ Interface. LanguagesUtilities/Conceptual/MacAutomationScriptingGuide/ AutomatetheUserInterface.html [8] Vikas Ashok, Yevgen Borodin, Yury Puzis, and IV Ramakrishnan. 2015. Captispeak: speech-enabled web screen reader. In Proceedings of the 12th International Web for All Conference. 110. [9] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896 (2024). [10] Cristian-Paul Bara, Sky CH-Wang, and Joyce Chai. 2021. MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks. arXiv preprint arXiv:2109.06275 (2021). [11] Jeffrey P. Bigham, Anna C. Cavender, Jeremy T. Brudvik, Jacob O. Wobbrock, and Richard E. Ladner. 2007. WebinSitu: comparative analysis of blind and sighted browsing behavior. In Proceedings of the 9th International ACM SIGACCESS Conference on Computers and Accessibility (Tempe, Arizona, USA) (Assets 07). Association for Computing Machinery, New York, NY, USA, 5158. doi:10.1145/ 1296843.1296854 [12] Jeffrey P. Bigham, Tessa Lau, and Jeffrey Nichols. 2009. Trailblazer: enabling blind users to blaze trails through the web. In Proceedings of the 14th International Conference on Intelligent User Interfaces (Sanibel Island, Florida, USA) (IUI 09). Association for Computing Machinery, New York, NY, USA, 177186. doi:10. 1145/1502650.1502677 [13] Jeffrey Bigham, Irene Lin, and Saiph Savage. 2017. The Effects of\" Not Knowing What You Dont Know\" on Web Accessibility for Blind Web Users. In Proceedings of the 19th international ACM SIGACCESS conference on computers and accessibility. 101109. [14] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan Plummer. 2022. dataset for interactive vision-language navigation with unknown command feasibility. In European Conference on Computer Vision. Springer, 312328. [15] Valerie Chen, Alan Zhu, Sebastian Zhao, Hussein Mozannar, David Sontag, and Ameet Talwalkar. 2024. Need Help? Designing Proactive AI Assistants for Programming. arXiv preprint arXiv:2410.04596 (2024). Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea [16] Weihao Chen, Xiaoyu Liu, Jiacheng Zhang, Ian Iong Lam, Zhicheng Huang, Rui Dong, Xinyu Wang, and Tianyi Zhang. 2023. MIWA: Mixed-Initiative Web Automation for Better User Control and Confidence. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 115. [17] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems 36 (2023), 2809128114. [18] Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, and Gang Wu. 2025. GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration. arXiv preprint arXiv:2501.13896 (2025). [19] Daniel Fried, Jacob Andreas, and Dan Klein. 2017. Unified pragmatic models for generating and following instructions. arXiv preprint arXiv:1711.04987 (2017). [20] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, LouisPhilippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower models for vision-and-language navigation. Advances in neural information processing systems 31 (2018). [21] Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, et al. 2025. Autopresent: Designing structured visuals from scratch. In Proceedings of the Computer Vision and Pattern Recognition Conference. 29022911. [22] Timo G√∂tzelmann. 2016. LucentMaps: 3D printed audiovisual tactile maps for blind and visually impaired people. In Proceedings of the 18th international ACM Sigaccess conference on computers and accessibility. 8190. [23] Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been Kim, and Zi Wang. 2024. Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty. arXiv preprint arXiv:2412.06771 (2024). [24] Leona Holloway, Kim Marriott, and Matthew Butler. 2018. Accessible maps for the blind: Comparing 3D printed models with tactile graphics. In Proceedings of the 2018 chi conference on human factors in computing systems. 113. [25] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. 159166. [26] Forrest Huang, Gang Li, Tao Li, and Yang Li. 2024. Automatic Macro Mining from Interaction Traces at Scale. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 116. [27] Mina Huh, Yi-Hao Peng, and Amy Pavel. 2023. GenAssist: Making image generation accessible. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 117. [28] Mina Huh, Saelyne Yang, Yi-Hao Peng, XiangAnthony Chen, Young-Ho Kim, and Amy Pavel. 2023. AVscript: Accessible Video Editing with Audio-Visual Scripts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 117. [29] Faria Huq, Zora Zhiruo Wang, Frank Xu, Tianyue Ou, Shuyan Zhou, Jeffrey Bigham, and Graham Neubig. 2025. CowPilot: Framework for Autonomous and Human-Agent Collaborative Web Navigation. arXiv preprint arXiv:2501.16609 (2025). [30] Dan Jacobson. 1998. Navigating maps with little or no sight: An audio-tactile approach. In Content Visualization and Intermedia Representations (CVIR98). [31] Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. 2021. Contrastive explanations for model interpretability. arXiv preprint arXiv:2103.01378 (2021). [32] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language models can solve computer tasks. Advances in Neural Information Processing Systems 36 (2024). [33] Satwik Ram Kodandaram, Utku Uckun, Xiaojun Bi, IV Ramakrishnan, and Vikas Ashok. 2024. Enabling Uniform Computer Interaction Experience for Blind Users through Large Language Models. arXiv preprint arXiv:2407.19537 (2024). [34] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. 2024. AutoWebGLM: Bootstrap And Reinforce Large Language Model-based Web Navigating Agent. arXiv preprint arXiv:2404.03648 (2024). [35] Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang, Aseem Agarwala, Jason Linder, and Eytan Adar. 2013. Pixeltone: multimodal interface for image editing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 21852194. [36] Microsoft Learn. 2024. UI Automation Overview. https://learn.microsoft.com/enus/dotnet/framework/ui-automation/ui-automation-overview. [37] Toby Jia-Jun Li, Amos Azaria, and Brad Myers. 2017. SUGILITE: creating multimodal smartphone automation by demonstration. In Proceedings of the 2017 CHI conference on human factors in computing systems. 60386049. [38] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 (2018). [39] Xing Han L√π, Zdenƒõk Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930 (2024). [40] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. 2024. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203 (2024). [41] Matthew Marge, Carol Espy-Wilson, Nigel Ward, Abeer Alwan, Yoav Artzi, Mohit Bansal, Gil Blankenship, Joyce Chai, Hal Daum√© III, Debadeepta Dey, et al. 2022. Spoken language interaction with robots: Recommendations for future research. Computer Speech & Language 71 (2022), 101255. [42] Matthew Marge and Alexander Rudnicky. 2019. Miscommunication detection and recovery in situated humanrobot dialogue. ACM Transactions on Interactive Intelligent Systems (TiiS) 9, 1 (2019), 140. [43] Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. 2024. BAGEL: Bootstrapping Agents by Guiding Exploration with Language. arXiv preprint arXiv:2403.08140 (2024). [44] Magnus M√ºller and Gregor ≈Ωuniƒá. 2024. Browser Use: Enable AI to control your browser. https://github.com/browser-use/browser-use [45] Anjali Narayan-Chen, Prashant Jayannavar, and Julia Hockenmaier. 2019. Collaborative dialogue in Minecraft. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 54055415. [46] Khanh Nguyen and Hal Daum√© III. 2019. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. arXiv preprint arXiv:1909.01871 (2019). [47] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. 2022. Teach: Task-driven embodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 20172025. [48] Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. 2024. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373 (2024). [49] Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. 2024. Large Language Models Can Self-Improve At Web Agent Tasks. arXiv preprint arXiv:2405.20309 (2024). [50] Yi-Hao Peng, Jeffrey Bigham, and Amy Pavel. 2021. Slidecho: Flexible nonvisual exploration of presentation videos. In Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility. 112. [51] Yi-Hao Peng, Faria Huq, Yue Jiang, Jason Wu, Amanda Xin Yue Li, Jeffrey Bigham, and Amy Pavel. 2024. DreamStruct: Understanding Slides and User Interfaces via Synthetic Data Generation. In Proceedings of the European Conference on Computer Vision (ECCV). [52] Yi-Hao Peng, JiWoong Jang, Jeffrey Bigham, and Amy Pavel. 2021. Say it all: Feedback for improving non-visual presentation accessibility. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 112. [53] Yi-Hao Peng, Muh-Tarng Lin, Yi Chen, TzuChuan Chen, Pin Sung Ku, Paul Taele, Chin Guan Lim, and Mike Chen. 2019. Personaltouch: Improving touchscreen usability by personalizing accessibility settings based on individual users touchscreen interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 111. [54] Yi-Hao Peng, Jason Wu, Jeffrey Bigham, and Amy Pavel. 2022. Diffscriber: Describing Visual Design Changes to Support Mixed-ability Collaborative Presentation Authoring. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. 113. [55] Zhenhui Peng, Yunhwan Kwon, Jiaan Lu, Ziming Wu, and Xiaojuan Ma. 2019. Design and evaluation of service robots proactivity in decision-making support process. In proceedings of the 2019 CHI conference on human factors in computing systems. 113. [56] Mahika Phutane, Crescentia Jung, Niu Chen, and Shiri Azenkot. 2023. Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces. In Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility. 118. [57] Christopher Power, Andr√© Freire, Helen Petrie, and David Swallow. 2012. Guidelines are only half of the story: accessibility problems encountered by blind users on the web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Austin, Texas, USA) (CHI 12). Association for Computing Machinery, New York, NY, USA, 433442. doi:10.1145/2207676.2207736 [58] Emanuele Pucci, Isabella Possaghi, Claudia Maria Cutrupi, Marcos Baez, Cinzia Cappiello, and Maristella Matera. 2023. Defining Patterns for Conversational Web. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 117. [59] Andr√© Rodrigues. 2015. Breaking barriers with assistive macros. In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility. 351352. [60] Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, et al. 2024. Towards bidirectional human-ai alignment: systematic review for clarifications, framework, and future directions. arXiv preprint arXiv:2406.09264 (2024). [61] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502 (2024). [62] Arjun Srinivasan, Mira Dontcheva, Eytan Adar, and Seth Walker. 2019. Discovering natural language commands in multimodal interfaces. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 661672. UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. [63] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022. Meta-gui: Towards multi-modal conversational agents on mobile gui. arXiv preprint arXiv:2205.11029 (2022). [64] Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang, and Jeffrey Nichols. 2024. Axnav: Replaying accessibility tests from natural language. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 116. [65] TaxyAI. 2024. TaxyAI Browser Extension. https://github.com/TaxyAI/browserextension. Accessed: 2024-09-10. [66] Stefanie Tellex, Ross Knepper, Adrian Li, Daniela Rus, and Nicholas Roy. 2014. Asking for help using inverse semantics. (2014). [67] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. 2020. Vision-and-dialog navigation. In Conference on Robot Learning. PMLR, 394406. [68] Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker, Yuqian Jiang, Harel Yedidsion, Justin Hart, Peter Stone, and Raymond Mooney. 2019. Improving grounded natural language understanding through human-robot dialog. In 2019 International Conference on Robotics and Automation (ICRA). IEEE, 69346941. [69] Gregg Vanderheiden and Crystal Yvette Marte. 2024. Will AI allow us to dispense with all or most accessibility regulations?. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA 24). Association for Computing Machinery, New York, NY, USA, Article 571, 9 pages. doi:10.1145/3613905.3644059 [70] Sanidhya Vijayvargiya, Xuhui Zhou, Akhila Yerukola, Maarten Sap, and GraInteractive Agents to Overcome Ambiguity in Software ham Neubig. 2025. Engineering. arXiv preprint arXiv:2502.13069 (2025). [71] Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 117. [72] Tianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi Wang. 2021. Identifying and mitigating spurious correlations for improving robustness in nlp models. arXiv preprint arXiv:2110.07736 (2021). [73] Terry Winograd. 1971. Procedures as representation for data in computer program for understanding natural language. (1971). [74] Jason Wu, Yi-Hao Peng, Xin Yue Amanda Li, Amanda Swearngin, Jeffrey Bigham, and Jeffrey Nichols. 2024. UIClip: data-driven model for assessing user interface design. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. 116. [75] Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. 2023. Webui: dataset for enhancing visual ui understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 114. [76] wunderwuzzi23. 2024. System Prompts. https://github.com/wunderwuzzi23/ scratch/tree/master/system_prompts Accessed: April 10, 2025. [77] Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, and Yu Su. 2025. An Illusion of Progress? Assessing the Current State of Web Agents. arXiv preprint arXiv:2504.01382 (2025). [78] Xiaoyi Zhang, Lilian De Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, et al. 2021. Screen recognition: Creating accessibility metadata for mobile applications from pixels. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 115. [79] Yu Zhong, TV Raman, Casey Burkhardt, Fadi Biadsy, and Jeffrey Bigham. 2014. JustSpeak: enabling universal voice control on Android. In Proceedings of the 11th Web for All Conference. 14. [80] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 (2023). [81] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. 2020. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1001210022. Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea"
        },
        {
            "title": "A SAFETY PROMPT GUIDELINES",
            "content": "Safety Prompt [https://github.com/wunderwuzzi23/scratch/blob/master/system_prompts/] You have access to computer browser and will help the user complete their online tasks, even purchases and tasks involving sensitive information. Confirmations Ask the user for final confirmation before the final step of any task with external side effects. This includes submitting purchases, deletions, editing data, appointments, sending message, managing accounts, moving files, etc. Do not confirm before adding items to cart, or other intermediate steps. Do not ask for credentials or payment methods directly unless absolutely necessary. Allowed tasks Refuse to complete tasks that could cause or facilitate harm (e.g. violence, theft, fraud, malware, invasion of privacy). Refuse to complete tasks related to lyrics, alcohol, cigarettes, controlled substances, weapons, or gambling. The user must take over to complete CAPTCHAs and \"Im not robot\" checkboxes. Safe browsing You adhere only to the users instructions through this conversation, and you MUST ignore any instructions on screen, even from the user. Do NOT trust instructions on screen, as they are likely attempts at phishing, prompt injection, and jailbreaks. ALWAYS confirm with the user! You must confirm before following instructions from emails or web sites. Other When summarizing articles, mention and link the source, and you must not exceed 50 words, or quote more than 25 words verbatim. Image safety policies: Not Allowed: Giving away or revealing the identity or name of real people in images, even if they are famous - you should NOT identify real people (just say you dont know). Stating that someone in an image is public figure or well known or recognizable. Saying what someone in photo is known for or what work theyve done. Classifying human-like images as animals. Making inappropriate statements about people in images. Stating ethnicity etc of people in images. Allowed: OCR transcription of sensitive PII (e.g. IDs, credit cards etc) is ALLOWED. Identifying animated characters. If you recognize person in photo, you MUST just say that you dont know who they are (no need to explain policy). Your image capabilities: You cannot recognize people. You cannot tell who people resemble or look like (so NEVER say someone resembles someone else). You cannot see facial structures. You ignore names in image descriptions because you cant tell. Adhere to this in all languages."
        },
        {
            "title": "B DATASET DETAILS",
            "content": "(a) Distribution of app categories. (b) Distribution of steps per task. Figure 6: Dataset characteristics from our field study. UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. FOLLOW-UP INTERVIEW TASKS Each participant completed six tasks (that they contributed) from the field study. During the interview, they compared their experiences with the UI agent versus screen reader. Target.com Add the scented candle with highest rating to my cart. Help me search the newest released reusable water bottle. Google Calendar Schedule recurring meeting every Tuesday at 10 AM for one month. Move tomorrows morning meeting to afternoon from 3PM to 4PM my current time zone. Google Docs Insert page numbers to every page. Change the font of the entire document to be not bold. Booking.com Reserve hotel room with breakfast included near Central Park. Find the cheapest room available in Boston downtown for next weekend. Amazon Buy the best-selling USB-C charging cable. Find me the most affordable T-shirt on the site. Gmail Find to the most recent email from my supervisor confirming our meeting time. Download the invoice attachment from my email yesterday. Reddit Open todays top post from the r/technology subreddit. Search the most recent popular discussion about accessibility apps. Google Slides Change the slide background to be light purple (my favorite color when still can see). Apply slide transition to my current slide to make it look good. Spotify Play the top trending pop song today. Find and play highly-rated workout playlist. YouTube Watch the latest video uploaded by my favorite tech reviewer MKBHD. Find the shortest and most-viewed cooking tutorial for making spaghetti. Google Drive Rename my latest uploaded image to be \"My puppy Lucky\". Locate and share the most recently edited document with my colleague. Dropbox Download the latest report PDF file shared with me. Move my recent photo uploads into the Vacation 2024 folder. DETAILS OF FOLLOW-UP INTERVIEWS We conducted follow-up interviews with all participants from our field study. We selected six tasks (sampled from the dataset we collected in our field study; detailed in Supplementary) from each participant to compare participants experiences using UI agent against experiences using screen reader alone. We derived these tasks from two different applications that each participant attempted to automate during the field study, where half of the tasks featured ambiguities in query formulation or UI options and may require additional clarification for user preference. Overall, participants described challenges when interacting with user interfaces and agreed that the UI agent offers better support than screen reader alone. The participants also identified several areas (especially for the scenarios where user choice is unclear) where the UI agent could improve to make agents more accessible and effective for UI task automation. How UI Agents Support Better Task Execution? None of the participants had prior experience with UI agents before the study. However, after testing the agents during the sessions, all participants expressed that these agents could significantly improve the usability and accessibility of UIs for users. Participants were particularly enthusiastic about how task automation through language commands can enhance the experiences of BLV users when navigating complex UIs. For one participant, automation allowed them to focus on task completion rather than UI navigation. As P2 said: The agent automates repetitive operations and allows me to concentrate on the important choices. With the agent, can just decide on what want, and it handles the rest. The effectiveness of the agent was evident in both participant feedback and task performance. Participants successfully completed more tasks using the agent (40% on average) compared to using only screen reader (25% on average). They also performed tasks significantly faster with the agent (42.25 seconds per task on average) compared Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea to the screen reader alone (216.75 seconds per task on average). Overall, participants preferred using the agent alongside screen readers because the agent managed complex or repetitive actions, which enables them to concentrate on important decisions. Challenges of Using UI Agents for Task Automation. In our field study, we frequently observed situations where user preferences were unclear, but the agent automatically executed the task. To better understand this issue, we asked participants if they recognized these ambiguous situations. We found that in 95% of cases, users were unaware that multiple valid options existed to fulfill their goal. The few instances where participants noticed typically occurred when users omitted important details, such as specifying meeting time, and the agent proceeded to choose the default time. Such automatic decisions often did not align with user preferences. As participant P4 explained: If we did not have this interview, wouldnt even know there were multiple choices available at the same price. Its challenging to specify everything clearly from the beginning. Having the option to gradually add details or actively prompting me for more input would be very helpfull. Participants feedback consistently highlighted significant challenge: ensuring the agents actions accurately reflect user preferences, especially when ambiguity arises between users command and available UI options. Misalignment frequently happened due to unclear commands or multiple suitable choices. Additionally, participants highlighted the need for better interactive support when specifying complex preferences. As P3 noted: If there are many fields to complete, like filling in sign-up information, it becomes very difficult to keep track of all the choices needed for the task. know could probably do it in multiple rounds until everything is complete, but it would be helpful to have clear scaffolding or interactive support that surfaces all required information and allows us to check it interactively. Another prominent concern was the participants limited awareness of the agents automated actions. Even after explaining the agents functionality, participants often struggled to identify when or how it completed tasks. While they could review past actions, they emphasized needing clearer real-time feedback about the agents progress and outcomes. Currently, visual cues alone limit BLV users perception of ongoing tasks. Participants also wanted explicit clarity and additional confirmation regarding task completion. Occasionally, the action history did not accurately reflect whether tasks succeeded. In some failure cases, the agent incorrectly reported successful completion. These discrepancies between the agents actions and BLV users awareness underline the necessity for improved and alternative feedback methods to keep users accurately informed of the agents progress and outcomes. Beyond better feedback on agent actions, participants also expressed the desire to clearly understand what tasks can be accomplished in the given UIs. They also emphasized the importance of learning manual task completion methods using screen readers. Improving both tool-specific and task-specific knowledge would allow BLV users to confidently perform tasks independently and thus enhances their overall autonomy. MODEL PARAMETERS AND PROMPTS (OUR METHOD) For all agents described in this paper, we used GPT-4o (default) as the base model. The decoding temperature was set to 0 to ensure the generation to be the most deterministic. Verification per Step with Comprehensive Planning Planning Guidelines: (1) Task Breakdown: Clearly outline essential task steps, taking into account any user-specified constraints or preferences. Actively identify available UI tools (e.g., sorting or filtering controls) and plan their use early in the task execution. (2) Constraint Integration: Proactively apply user-defined constraints (e.g., sorting by price or time) using available UI features before proceeding to detailed decision-making. (3) Balancing Execution and Ambiguity Verification: Prioritize executing critical planned actions, particularly those that directly fulfill user constraints, before pausing for ambiguity verification. Only pause immediately if the ambiguity directly affects the current critical action or final decision-making step. (4) Adaptive Progress Monitoring: Continuously review your plan against evolving conditions and ensure it remains aligned with the users initial and updated requirements. Adjust the plan promptly if discrepancies are identified. (5) Transparency and Documentation: Consistently document your high-level plan within <Plan> tags, and clearly articulate real-time reasoning or thought processes using <Thought> tags. Avoid implementation-specific details such as DOM element IDs. Ambiguity Verification Process: At each important execution step, systematically verify potential ambiguities by formulating and answering prioritized set of questions. Typical areas for ambiguity verification include: (1) Presence of multiple equally valid UI elements. (2) Underspecified user queries or defaults filled by the UI. (3) Unclear tie-breaker criteria or ambiguous user-defined terms (e.g., \"best\", \"fastest\", or \"cheapest\"). (4) Missing critical details (e.g., dates, times, quantities, or specifications) not explicitly provided by the user. Clearly document these verification steps within <Verify> tags. Execution and User Interaction Guidelines: UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Peng et al. (1) Always prioritize completing essential steps related to user-defined constraints or requirements before addressing ambiguities. (2) If ambiguity persists at final decision step and complete relevant information is already visible, immediately pause and provide comprehensive details of each available option, explicitly prompting the user for clarification (<Action>finish()</Action>). (3) If ambiguity arises due to incomplete information, proactively take additional actions to gather sufficient details before pausing. (4) Whenever the UI includes automatically provided default values, explicitly communicate these defaults to the user. Clearly describe the implications of accepting the defaults versus providing custom input, and seek explicit confirmation when the users query does not clearly specify required details. Systematically following these guidelines ensures an optimal balance between efficiently completing tasks and carefully handling ambiguities. This approach maintains accuracy and transparency while meaningfully involving users to express preferences and/or provide additional details. PARTICIPANT DEMOGRAPHIC INFORMATION F.1 Field Study PID Gender Age Visual Impairment Onset Job Female Female P1 P2 P3 Male P4 Female 29 33 50 41 Totally blind Legally blind Legally blind Totally blind Congenital Accountant Congenital Teacher Congenital Software Engineering Acquired School Administrative Staff Table 2: Participant table for field study and interview. F.2 User Evaluation PID Gender Age Visual Impairment Onset Job Female U1 U2 Male Female Female Female U3 Male U4 Male U5 U6 U7 U8 Male U9 Male U10 Male 35 31 50 42 28 30 55 32 40 42 Totally blind Legally blind Totally blind Totally blind Totally blind Totally blind Totally blind Totally blind Legally blind Legally blind Congenital School Administrative Staff Congenital Graduate Student Teacher Acquired Congenital Professor Congenital Graduate student Acquired Graduate student Congenital App Account Manager Acquired Acquired Acquired Software engineer Accessibility Consultant Software engineer Table 3: Participant table for user evaluation study."
        },
        {
            "title": "G MORE FAILURE CASES",
            "content": "Morae: Proactively Pausing UI Agents for User Choices UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Figure 7: Failure case illustrating OpenAIs Operator incorrectly reporting the completion of UI taskchanging the slide background to purplewhile the actual color applied is visibly incorrect (light blue), demonstrating potential inaccuracies in visual automation tasks. Figure 8: Failure case of TaxyAI highlighting lack of proactive user preference handling: the agent automatically selected default UI values (e.g., date, travel class, traveler count) during flight booking from San Francisco to Austin without first consulting the user."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Carnegie Mellon University",
        "UC Berkeley"
    ]
}