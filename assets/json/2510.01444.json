{
    "paper_title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning",
    "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided Exploration)}$, a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 4 4 1 0 . 0 1 5 2 : r Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Rui Liu ,1,2 , Dian Yu1 , Tong Zheng1,2 , Runpeng Dai1,3 , Zongxia Li1,2 , Wenhao Yu1 , Zhenwen Liang1 , Linfeng Song1 , Haitao Mi1 , Pratap Tokekar2 , and Dong Yu1 1Tencent AI Lab, Bellevue, WA 2University of Maryland, College Park 3University of North Carolina, Chapel Hill"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in LLMs but struggles with exploration, an issue that still persists for Multimodal LLMs (MLLMs). Current methods treat the visual input as fixed, deterministic condition, overlooking critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce VOGUE (Visual-UncertaintyGuided Exploration), novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as stochastic context, VOGUE quantifies the policys sensitivity to visual perturbations using the symmetric KL divergence between raw and noisy branch, creating direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with tokenentropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs) by optimizing against ground-truth answers (Luong et al., 2024; Lambert et al., 2024; Guo et al., 2025; Su et al., 2025). However, this outcome-centric approach often biases learning toward exploitation, suppressing trajectories with valid intermediate reasoning that conclude with an incorrect answer. This limitation stifles exploration and can lead to brittle policies (Dai et al., 2025a). While this challenge is recognized in text-only domains, with mitigation strategies including uncertainty-aware objectives (Cheng et al., 2025), diversitypromoting rewards (Li et al., 2025a), pass@k rewards (Chen et al., 2025b; Walder & Karkhanis, 2025), and intermediate feedback (Setlur et al., 2024), these methods do not address the unique sources of uncertainty inherent to multimodal reasoning. This exploration problem is arguably amplified in Multimodal LLMs (MLLMs), where textual reasoning is grounded in complex visual inputs (Huang et al., 2025; Tan et al., 2025; Peng et al., 2025). Current multimodal RLVR approaches typically treat the image as fixed, deterministic condition. This overlooks key source of ambiguity: the visual modality itself. An image can contain ambiguous objects, be subject to multiple valid interpretations, or have its crucial details altered by plausible perturbations. By not probing these visual uncertainties, existing methods do not explicitly incentivize policies to test the robustness of their visual understanding. Consequently, models may The work was done during Rui Lius internship at Tencent AI Lab. 1 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Figure 1: VOGUE for RL fine-tuning. Our method uses dual-branch forward pass: the raw branch processes the original image, while the noisy branch receives perturbed view. Token-level symmetric KL between branches provides visual-uncertainty signal used to shape the noisy-branch advantage. An entropy bonus on both branches maintains output stochasticity, and an annealed sampling schedule balances exploration and exploitation by favoring the noisy branch early in training. learn spurious visual-text correlations rather than developing deep, generalizable reasoning, leaving critical question unanswered: How can we leverage visual uncertainty to drive more effective exploration? To address this gap, we introduce Visual-UncertaintyGuided Exploration (VOGUE), novel method that makes exploration modality-aware. As illustrated in Figure 1, VOGUE shifts exploration from the output (text) space to the input (visual) space by treating the image as stochastic context. For each training example, we perform dual-branch forward pass: one for raw branch using the original image and another for noisy branch using semantics-preserving perturbed view. We quantify visual uncertainty as the symmetric KL divergence between the policy distributions induced by these two views. This identifies states where the models predictions are brittle to plausible visual perturbations and are therefore states worthy of exploration. We then shape advantages with (i) capped, uncertainty-proportional visual uncertainty bonus on the noisy branch to focus exploration on visually ambiguous inputs, and (ii) token-entropy bonus on both branches to maintain policy stochasticity. To manage the explorationexploitation trade-off, we employ an annealed branch-sampling schedule that prioritizes uncertainty-driven exploration early in training before shifting focus to the original view as learning stabilizes. In essence, VOGUE couples textual exploration with measure of visual confidence, pushing the model to resolve ambiguities and build more robust reasoning skills. We implement VOGUE within GRPO (Shao et al., 2024) and evaluate it on six diverse mathematical and general-domain reasoning benchmarks: MathVerse (Zhang et al., 2024), MathVista (Lu et al., 2023), WeMath (Qiao et al., 2024), HallusionBench (Guan et al., 2024), ChartQA (Masry et al., 2022), and LogicVista (Xiao et al., 2024). On Qwen2.5-VL-3B and 7B models (Bai et al., 2023) trained on MMRL30k (Zhu et al., 2025a), VOGUE delivers substantial improvements over strong baselines at both model scales, boosting pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks (Table 1 and Table 2). Crucially, VOGUE also increases pass@4 performance, effectively mitigating the exploration decay often seen in RL fine-tuning, challenge that methods like GRPO face  (Table 3)  . Furthermore, VOGUE consistently 2 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning outperforms Pass@k Training, dedicated exploration-promoting method primarily demonstrated effective in text-only settings, yielding both higher pass@1 and more consistent pass@k gains. In summary, our contributions are three-fold: We identify visual uncertainty as key, yet overlooked, mechanism for exploration in MLLMs and propose to leverage it to improve MLLM reasoning. We introduce VOGUE, practical method that uses dual-branch architecture to quantify visual uncertainty, capped, uncertainty-proportional advantage bonus, and an annealed sampling schedule. We provide extensive empirical validation demonstrating that VOGUE consistently improves both exploitation (pass@1) and exploration (pass@k) over strong baselines."
        },
        {
            "title": "2 Preliminaries",
            "content": "We adopt GRPO (Shao et al., 2024) as the underlying RL algorithm in this work. In GRPO, given an input x, group of responses {oi}G i=1 are sampled from the old policy πθold , each associated with reward ri. Then the normalized advantage for response oi is defined as: Ai = ri mean({ri}G std({ri}G i=1) i=1) . (1) As in PPO, GRPO uses clipped importance sampling to stabilize policy updates. Let ρi(θ) = πθ (oix) (oix) denote the probability ratio between the new and old policies. The GRPO objective is to maximize the following: πθold JGRPO(θ) = xD,{oi}πθold (x) (cid:34) 1 i=1 (cid:16) min ρi(θ)Ai, clip(cid:0)ρi(θ), 1 ϵclip, 1 + ϵclip (cid:1)Ai (cid:35) (cid:17) , (2) where ϵclip is the clipping hyperparameter."
        },
        {
            "title": "3 Visual-Uncertainty-Guided Exploration",
            "content": "To encourage exploration in multimodal RLVR, we propose visual-uncertaintyguided exploration (VOGUE). Formally, with input = (xtext, ximage), we aim to optimize an MLLM policy network πθ by maximizing surrogate objective in Eq. 2. As illustrated in Figure 1, our approach employs dual-branch forward pass and treats the image as stochastic context. For each input, the policy is evaluated on both the raw and semantics-preserving noisy view, and visual uncertainty is quantified via the symmetric KL divergence between the resulting text policy distributions. This visual uncertainty then guides advantage shaping and, combined with an annealed sampling schedule, steers the model to explore visually ambiguous states early while focusing on the original view as training stabilizes. The full procedure is summarized in Algorithm 1. 3.1 Visual Uncertainty The core of our approach is to enhance exploration in multimodal RLVR by shifting the focus from the output (text) space to the input (visual) space, treating the image as stochastic context rather than fixed condition. To this end, we introduce controlled perturbations to the visual input and define visual uncertainty as the extent to which the models output distribution varies under semantics-preserving transformations of the image. Variation of the models predictions indicate 3 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning regions of the visual state space where additional exploration is likely to improve policy reasoning and robustness. To induce these perturbations for encouraging exploration, we apply stochastic image augmentation. For each image ximage in the training dataset, we create perturbed counterpart image through stochastic augmentation function . This function applies composition of transformations: image = (ximage), where includes random horizontal/vertical flips, rotations, color jittering, and the addition of Gaussian noise. These augmentations are designed to preserve the core semantic content of the image while altering its low-level feature representation, ensuring that differences in the models output reflect true sensitivity to visual variations. Then we employ dual-branch forward pass. The raw branch processes the original input = (xtext, ximage) to produce an output probability distribution = πθ(x), while the noisy branch image) to produce distribution = πθ(x). After obtaining uses the perturbed input = (xtext, the two distributions, we represent the visual uncertainty Uv as the divergence between them. We measure this using symmetric KL divergence, which is calculated as the mean of the forward and backward KL divergences, encouraging exploration while maintaining stability: Uv = 1 2 (cid:0)DKL(PQ) + DKL(QP)(cid:1). (3) 3.2 Advantage Shaping We maintain separate advantage calculations for the raw and noisy branches, as shown in Figure 1. To encourage exploration, we introduce visual uncertainty bonus Bv to the noisy branch. This bonus, based on the visual uncertainty Uv, guides the branch to explore regions of the state space that may (cid:17) not be reachable by the raw branch. The bonus Bv is defined as: Bv = min , where Anoi is the advantage for the noisy branch, αv and βv are scaling factors, stopgrad() is the stop gradient operator. Furthermore, to promote general policy stochasticity and exploration, we incorporate an entropy bonus Be for both branches. This bonus is based on the token entropy of the policys output distribution, which is defined as: = vV πθ (v x, o<t) log πθ (v x, o<t) . The entropy bonus Be is defined as: Be = min , where αe and βe are scaling factors, and denotes the vocabulary. Therefore, for the raw branch, the shaped advantage is calculated + Bv. When implementing as: VOGUE with GRPO, we use GRPOs standard estimator to compute the base advantages Anoi and Araw (see Eq. 1). , and for the noisy branch: ˆAraw = Araw + Braw ˆAnoi = Anoi + Bnoi , αv stopgrad(Uv) , αe stopgrad(H) (cid:16) Anoi βv (cid:16) βe (cid:17) The policy gradient for the noisy branch can be expressed as follows: θJVOGUE(θ) (cid:104) oπθold (Anoi + Bnoi + αv Uv)θ log πθ(o x) (cid:105) (4) From gradient perspective, this formulation encourages more effective exploration compared to standard RLVR approaches such as GRPO. We omit caps/clipping for clarity. The term αv Uvθ log πθ(o x) explicitly encourages the policy to increase the probability of action sequences that follow from visually uncertain states, guiding the model to acquire more informative visual features. There is also an entropy bonus Be. This acts as general-purpose exploration mechanism that complements the exploration driven by Bv. While Bv directs exploration toward visual uncertainty, Be maintains stochasticity in the textual output space. 3.3 Annealed Sampling for Policy Optimization During training, it is crucial to balance the aggressive exploration driven by the noisy branch with the stable learning provided by the raw branch. policy trained exclusively on the noisy branch may become overly stochastic and fail to converge, while policy trained solely on the raw branch 4 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: Algorithm 1 Visual-Uncertainty-Guided Exploration (VOGUE) Require: Dataset D, total training steps Stotal, group size G, annealing schedule parameters pstart, pend, scaling factors αv, βv, αe, βe, augmentation function 1: Initialize policy parameters θ, old policy θold θ 2: for = 1 to Stotal do 3: 4: Sample input = (xtext, ximage) Construct perturbed image Sample group of responses {oi}G Compute annealed probability: pnoi(s) = pend + (pstart pend) max(cid:0)0, 1 Stotal for = 1 to do image = (ximage), = (xtext, i=1 πθold i=1 πθold ( x), {o ( x) image) i}G (cid:1) Compute raw advantage Araw Compute token entropy Hraw Compute visual uncertainty Uv using token-level symmetric KL divergence (Eq. 3) Compute bonuses: , noisy advantage Anoi and Hnoi for each branch (Eq. 1) i Bv = min (cid:16) Anoi βv , αv stopgrad(Uv) (cid:17) Braw = min (cid:16) Araw βe , αe stopgrad(Hraw (cid:17) ) , Bnoi = min (cid:16) Anoi βe , αe stopgrad(Hnoi (cid:17) ) Compute shaped advantages: (cid:98)Araw = Araw Sample branch selector zi Bernoulli(pnoi(s)) Select final advantage (with corresponding selected (x, oi) or (x, + Braw = Anoi + Bnoi + Bv (cid:98)Anoi , i)): (cid:98)Ai = zi (cid:98)Anoi + (1 zi) (cid:98)Araw end for Compute surrogate objective with shaped advantages (Eq. 2) Update policy parameters θ θ + ηθJVOGUE(θ) Periodically update old policy θold θ 15: 16: 17: 18: 19: end for may not explore enough to find the optimal path. To manage this trade-off, we employ an annealed sampling strategy. At each training step, for each sample in the batch, we stochastically choose which advantage estimate to use for the policy update. We define pnoi as the probability of selecting the advantage from the noisy branch, which is expressed as: pnoi(s) = pend + (pstart pend) max(cid:0)0, 1 Stotal (cid:1) (5) where is the current training step, Stotal is the total training steps. pstart is the initial step sampling probability and pend is the final step sampling probability. This probability is annealed over the course of training according to linear decay schedule. Initially, pnoi is high to promote broad exploration of the state space. As training progresses, pnoi is gradually decreased, causing the optimizer to favor the more stable advantage estimates from the raw branch. This allows the policy to first explore and then fine-tune its reasoning based on the original, unperturbed data. 5 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We train all models on the dataset MMRL30k (Zhu et al., 2025a) for 200 steps. The training is performed on 8 GPUs, using the AdamW optimizer (Loshchilov & Hutter, 2019) with learning rate of 1e 6 and weight decay of 0.01. To inject noise into images, we apply Gaussian noise with zero mean and standard deviation σ = 0.4. When transferring token entropy and visual uncertainty for advantage shaping, we set αe = 0.4, βe = 2.0 following the setup of Cheng et al. (2025), and set αv = 1.0, and βv = 2.0. The annealed sampling schedule is defined by pstart = 1.0 and pend = 0. We adopt rollout batch size of 256 and generate = 5 responses per input. The implementation builds on the framework EasyR1 (Zheng et al., 2025c). We conduct direct RL training on top of base models of Qwen2.5-VL-3B and 7B (Bai et al., 2023). The models are trained to generate responses in structured format, where the reasoning process is enclosed within <think></think> tags and the final answer is presented in boxed{}. The reward function is weighted combination of format reward and an accuracy reward, with coefficients of 0.1 and 0.9, respectively. We compare VOGUE to two baselines trained under the same setup for fair comparison: GRPO (Shao et al., 2024) and Pass@k Training (Chen et al., 2025b) (with = 4). For broader context, we also report published results from 7B models: R1-One-Vision-7B (Yang et al., 2025), VisionR1-7B (Huang et al., 2025), OpenVLThinker-7B (Deng et al., 2025), VLAA-Thinker-7B (Chen et al., 2025a), MM-Eureka-Qwen-7B (Meng et al., 2025), ThinkLite-VL-7B (Wang et al., 2025b), and VLRethinker-7B (Wang et al., 2025a). We evaluate pass@1 and pass@4 accuracy on six benchmarks, including MathVerse (Zhang et al., 2024), MathVista (Lu et al., 2023), WeMath (Qiao et al., 2024), HallusionBench (Guan et al., 2024), ChartQA (Masry et al., 2022), and LogicVista (Xiao et al., 2024). These benchmarks span diverse aspects of multimodal reasoning, covering mathematical problem solving, hallucination detection, chart understanding, and logical reasoning. Because Vision-R1-7B used WeMath as training data, we omit its results on that benchmark. For evaluation, we use Qwen2.5-72B-Instruct (Team, 2024) to extract final answers from model responses and assess their correctness against reference answers following prior RLVR work (Su et al., 2025). 4.2 Main Results We compare VOGUE against multiple baselines on mathematical and general-domain reasoning. First, we conduct quantitative evaluation on mathematical reasoning benchmarks, including MathVerse, MathVista, and WeMath  (Table 1)  . Across Qwen2.5-VL 3B and 7B, VOGUE consistently outperforms the strong RLVR baseline GRPO, demonstrating improved mathematical reasoning. By contrast, Pass@k Training (Chen et al., 2025b), which optimizes the policy with pass@k-based reward, underperforms other methods. This highlights the unique challenges of multimodal reasoning and provides evidence that exploration strategies designed for text-only settings may not readily transfer. To assess generalization beyond mathematical reasoning, we extend our evaluation to set of broader reasoning tasks using HallusionBench, ChartQA, and LogicVista  (Table 2)  . VOGUE again outperforms GRPO on all three benchmarks, with Qwen2.5-VL-7B + VOGUE achieving the best average performance. The training accuracy reward in Figure 2 further supports this, showing VOGUEs reward curve is consistently above GRPOs for both the 3B and 7B models. These consistent gains across diverse problem types demonstrate that VOGUEs benefits are robust and not confined to single domain. Finally, we report the pass@4 accuracy across diverse benchmarks in Table 3. While performance in few cases falls below the base model, phenomenon well observed in prior works (Yue et al., 2025; Zhu et al., 2025b), VOGUE consistently outperforms the GRPO baseline and achieves the highest average pass@4 accuracy. These results confirm VOGUEs effectiveness in promoting exploration. 6 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Table 1: Model performance of pass@1 accuracy on diverse visual mathematical reasoning benchmarks. We compare VOGUE with prior SFT+RL and Zero-RL methods, as well as with GRPO and Pass@k Training baselines on Qwen2.5-VL 3B and 7B models. VOGUE consistently improves over GRPO across both models. And Qwen2.5-VL-7B + VOGUE achieves the strongest average performance across benchmarks."
        },
        {
            "title": "Model",
            "content": "MathVerse MathVista WeMath Avg. SFT + RL R1-One-Vision-7B (Yang et al., 2025) Vision-R1-7B (Huang et al., 2025) OpenVLThinker-7B (Deng et al., 2025) VLAA-Thinker-7B (Chen et al., 2025a) 42.6 48.0 45.4 45."
        },
        {
            "title": "Zero RL",
            "content": "MM-Eureka-Qwen-7B (Meng et al., 2025) ThinkLite-VL-7B (Wang et al., 2025b) VL-Rethinker-7B (Wang et al., 2025a) Qwen2.5-VL-3B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE Qwen2.5-VL-7B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE 47.5 44.8 49. 35.3 40.6 30.2 42.7 43.0 48.0 39.6 52.1 62.9 71.2 70.0 69.3 71.2 73.1 73.3 55.7 66.4 56.2 68.9 66.1 72.1 64.6 74. 60.3 - 65.8 64.3 65.6 64.7 57.8 52.5 62.8 45.6 63.0 62.6 69.5 57.0 71.1 55.3 - 60.4 59.8 61.4 60.9 60. 47.8 56.6 44.0 58.2 57.2 63.2 53.7 65.8 (a) Qwen2.5-VL-3B (b) Qwen2.5-VL-7B Figure 2: Training accuracy rewards of GRPO and VOGUE on Qwen2.5-VL 3B and 7B models. VOGUE consistently achieves higher rewards than GRPO throughout training. Taken together, these results show that VOGUE successfully improves both final-answer accuracy (pass@1) and enhances exploration (pass@4), confirming that guiding exploration with visual uncertainty is an effective strategy for multimodal RLVR. 7 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Table 2: Model performance of pass@1 accuracy on diverse visual general-domain reasoning benchmarks. We compare VOGUE with prior SFT+RL and Zero-RL methods, as well as with GRPO and Pass@k Training baselines on Qwen2.5-VL 3B and 7B models. VOGUE consistently improves over GRPO, with Qwen2.5-VL-7B + VOGUE achieving the strongest average performance across benchmarks."
        },
        {
            "title": "Model",
            "content": "HallusionBench ChartQA LogicVista Avg. R1-One-Vision-7B (Yang et al., 2025) Vision-R1-7B Huang et al. (2025) OpenVLThinker-7B (Deng et al., 2025) VLAA-Thinker-7B (Chen et al., 2025a) SFT + RL"
        },
        {
            "title": "Zero RL",
            "content": "MM-Eureka-Qwen-7B (Meng et al., 2025) ThinkLite-VL-7B (Wang et al., 2025b) VL-Rethinker-7B (Wang et al., 2025a) Qwen2.5-VL-3B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE Qwen2.5-VL-7B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE 67.2 57.8 60.0 70. 66.4 70.9 69.5 61.4 65.5 62.3 67.0 66.9 68.6 65.2 71.0 78.3 82.7 80.2 80.2 79.9 81.4 81.0 73.8 77.6 72.6 78. 79.8 81.9 78.6 84.0 45.5 47.8 47.3 47.3 47.3 48.9 48.4 33.3 39.3 36.8 44.0 45.5 42.0 46.4 48.7 63.7 62.7 62.5 65. 64.5 67.0 66.3 56.2 60.8 57.2 63.0 64.1 64.2 63.4 67.9 Table 3: Model performance of pass@4 accuracy on diverse visual reasoning benchmarks. On both Qwen2.5-VL 3B and 7B models, VOGUE consistently improves over GRPO and achieves the highest average pass@4 accuracy. Model MathVerse MathVista WeMath ChartQA LogicVista Avg. Qwen2.5-VL-3B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE Qwen2.5-VL-7B (Bai et al., 2023) + GRPO + Pass@k Training (Chen et al., 2025b) + VOGUE 56.5 53.5 54.2 55.9 60.9 60.2 60.5 61. 76.8 77.6 77.5 79.2 82.8 82.5 80.6 83.7 79.0 82.2 78.9 83.9 82.2 85.0 80.5 86.0 82.0 81.6 84.3 83.3 85.8 85.1 85.5 87. 74.1 65.8 73.4 67.6 77.0 72.1 75.7 72.3 73.7 72.1 73.7 74.0 77.7 77.0 76.6 78.0 4.3 Ablation Studies To validate the contribution of each component in VOGUE, we perform series of ablation studies using the Qwen2.5-VL-7B model. Specifically, we analyze the effects of the visual uncertainty bonus, the token entropy bonus, the annealed strategy, as well as the influence of alternative divergence measures and varying noise levels. We present the training curves in Figure 3 and provide the evaluation results for each setting on six multimodal benchmarks in Appendix A.2. Effectiveness of Visual Uncertainty. We first examine the role of the visual uncertainty by disabling the visual uncertainty term Uv during advantage shaping. As shown in Figure 3a, the resulting learning curve lags behind that of the full VOGUE approach. This degradation confirms that 8 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning (a) (b) (d) (e) (c) (f) Figure 3: Ablation studies on the effects of visual uncertainty, token entropy, sampling strategy, divergence measure, and noise level. (a) Visual uncertainty and token entropy bonuses each improve performance, and together yield the best results. (b) Annealed sampling outperforms fixed sampling, confirming the benefit of dynamically controlling. (cd) Symmetric KL provides stable gains, while forward KL causes excessive visual uncertainty and degraded accuracy. (ef) Moderate noise (σ = 0.4) yields the best accuracy, while low noise limits exploration and high noise increases variance. incorporating visual uncertainty is helpful for guiding the agent toward visually uncertain states and thereby enhancing exploration. Effectiveness of Token Entropy. Next, we analyze the impact of the token entropy by removing the Be bonus. As illustrated in Figure 3a, performance drops compared to the full approach. Whereas the visual uncertainty targets exploration in the visual state space, token entropy encourages stochasticity in the textual output space. Removing both the visual uncertainty and token entropy bonuses causes more severe degradation. This is further supported by the results in Table 4 and Table 5 in Appendix A.2. These results demonstrates that maintaining textual stochasticity is complementary and beneficial mechanism alongside visually-guided exploration. Effectiveness of Annealed Sampling. We evaluate the annealed sampling mechanism, which gradually adjusts the probability of selecting the noisy branch versus the raw branch. To isolate its effect, we replace it with fixed sampling probability of 0.5. The results in Figure 3b show inferior performance compared to the full annealed strategy. Similar trends are observed in Table 6 and Table 7 in Appendix A.2. This underscores the benefit of dynamically balancing the trade-off, as early-stage exploration requires more noisy-branch updates while later-stage convergence benefits from the stability of the raw branch. Alternative Divergence Measures. To estimate visual uncertainty, we experiment with forward KL formulation instead of symmetric KL. However, as shown in Figure 3c, the forward KL divergence leads to unstable training with accuracy declining. This occurs because forward KL encourages the model to diverge excessively, which is reflected in Figure 3d, where the resulting visual uncertainty becomes excessively large. Results in Table 8 and Table 9 in Appendix A.2 further confirm this 9 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning finding. These results validate our choice of symmetric KL for promoting exploration while maintaining training stability. Different Noise Levels. Finally, We evaluate VOGUE under different noise injection levels by varying the standard deviation of the Gaussian perturbation in the noisy branch (σ 0.2, 0.4, 0.8). The results in Figure 3e indicate that moderate noise (σ = 0.4) provides the best accuracy. Low noise (σ = 0.2) yields insufficient visual exploration, while high noise (σ = 0.8) introduces excessive variance. Results in Table 10 and Table 11 in Appendix A.2 align with this observation. This shows that while noise is essential for quantifying uncertainty, an appropriate level is needed to avoid both insufficient exploration and excessive variance. Taken together, these ablation studies confirm that each component of VOGUE plays distinct and complementary role. The visual uncertainty bonus directs exploration in the visual space, token entropy sustains diversity in the textual output space, and annealed sampling adaptively balances noisy and raw branches. Our analysis of divergence measures and noise levels further validates the design choices that make VOGUE effective and stable."
        },
        {
            "title": "5 Related Work",
            "content": "Exploration in Text-Based Reasoning. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as powerful paradigm for enhancing the reasoning capabilities of LLMs (Guo et al., 2025; Dai et al., 2025b; Zheng et al., 2025a). However, existing RLVR methods often suffer from poor exploration, leading to premature convergence and entropy collapse (Cui et al., 2025; Dai et al., 2025a). Recent work on text-only RLVR has begun to tackle exploration explicitly. For instance, i-MENTOR (Gao et al., 2025) augments outcome rewards with trajectory-aware intrinsic signals and dynamic reward scaling to improve exploration. Replay-style methods, such as Retrospective Replay (Dou et al., 2025), revisit promising early states to counteract exploration decay later in training. Other approaches have proposed outcome-based exploration schemes (Song et al., 2025) or leveraged process rewards for more granular guidance (Setlur et al., 2024), though reliably scoring intermediate steps remains challenge. Additionally, upweighting negative-sample reinforcement has been shown to mitigate diversity collapse and improve Pass@k (Zhu et al., 2025b), while EVOL-RL promotes reasoning diversity through novelty-oriented reinforcement without explicit labels (Zhou et al., 2025). Furthermore, Zheng et al. (2025b) demonstrated that structured reasoning paradigms, such as parallel thinking, can serve as effective forms of structural exploration for RL training. Multimodal RLVR. RLVR has been increasingly applied to enhance the reasoning capabilities of multimodal models. Yang et al. (2025) extended language reasoning with visual inputs, improving visual question answering, while Huang et al. (2025) employed vision-grounded prompts to enhance multi-step reasoning. Deng et al. (2025) leveraged large-scale visual instruction tuning for improved cross-modal generalization, and Chen et al. (2025a) unified visual and textual signals in policylearning framework. Meng et al. (2025) introduced hierarchical visual abstractions for RL-guided multimodal planning, whereas Wang et al. (2025a) iteratively refined answers through visual reasoning and reflection. More recently, Li et al. (2025b) proposed self-rewarding framework that decomposes reasoning into visual perception and language reasoning, using self-generated rewards to improve visual reasoning and reduce hallucinations. Despite these advances, the critical balance of exploration and exploitation remains underexplored in multimodal RLVR. Prior works typically focus on pass@1 accuracy (Liu et al., 2025c; Zhu et al., 2025a). Some studies note insufficient exploration in RL algorithms such as GRPO (Shao et al., 2024), proposing dynamic KL strategies (Liu et al., 2025b) or rule-based process rewards (Zhang et al., 2025), yet they do not explicitly encourage exploration. More recently, Pass@k Training (Chen et al., 2025b) used pass@k as the training reward and analyzed explorationexploitation trade-off, but its focus was primarily on text-based reasoning, offering limited insights for multimodal scenarios. 10 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning In contrast, VOGUE addresses this gap by coupling exploration to quantified visual uncertainty and, to our knowledge, is among the first modality-aware exploration frameworks for RLVR. Moreover, VOGUE is complementary to other language-side exploration strategies (e.g., temperature scheduling (Liao et al., 2025), KL regularization (Liu et al., 2025a), and output-level diversity/novelty bonuses (Li et al., 2025a), which have shown benefits mainly in text RL and can be combined for further gains."
        },
        {
            "title": "6 Conclusions",
            "content": "We introduce VOGUE, visual-uncertaintyguided exploration method for multimodal RLVR that treats the image as stochastic context. By quantifying sensitivity to semantics-preserving perturbations and shaping advantages with uncertaintyand entropy-based bonuses under an annealed sampling schedule, VOGUE couples exploration directly to visual uncertainty while maintaining stable optimization. Compared to the strong RLVR baseline GRPO, VOGUE achieves consistent improvements in both pass@1 and pass@4 accuracy across diverse benchmarks, including mathematical problem solving, hallucination detection, chart understanding, and logical reasoning. These results highlight the effectiveness of VOGUE in enhancing multimodal reasoning. For discussion on future work, please see Appendix A.3."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025a. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, and Dong Yu. Cde: Curiosity-driven exploration for efficient reinforcement learning in large language models, 2025a. URL https://arxiv.org/abs/2509. 09675. Runpeng Dai, Tong Zheng, Run Yang, Kaixian Yu, and Hongtu Zhu. R1-re: Cross-domain relation extraction with rlvr. arXiv preprint arXiv:2507.04642, 2025b. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: Complex vision-language reasoning via iterative sft-rl cycles, 2025. URL https://arxiv.org/ abs/2503.17352. Shihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving rl exploration for llm reasoning through retrospective replay. arXiv preprint arXiv:2504.14363, 2025. 11 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, and Xiangyu Zhao. Navigate the unknown: Enhancing llm reasoning with intrinsic motivation guided exploration. arXiv preprint arXiv:2505.17621, 2025. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, and Tianlu Wang. Jointly reinforcing diversity and quality in language model generations. arXiv preprint arXiv:2509.02534, 2025a. Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-language model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025b. Mengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yangen Hu, Ke Zeng, Shuai Liu, and Huaiyu Wan. Enhancing efficiency and exploration in reinforcement learning for llms. arXiv preprint arXiv:2505.18573, 2025. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507, 2025a. Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, and Jun Wang. Othink-mr1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning. arXiv preprint arXiv:2503.16081, 2025b. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025c. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 12 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multiarXiv preprint arXiv:2407.01284, modal model achieve human-like mathematical reasoning? 2024. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning. arXiv preprint arXiv:2509.06941, 2025. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Christian Walder and Deep Karkhanis. Pass@ policy optimization: Solving harder reinforcement learning problems. arXiv preprint arXiv:2505.15201, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025a. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025b. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Tong Zheng, Lichang Chen, Simeng Han, Thomas McCoy, and Heng Huang. Learning to reason via mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817, 2025a. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, et al. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025b. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework, 2025c. URL https://github. com/hiyouga/EasyR1. Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, and Dong Yu. Evolving language models without labels: Majority drives selection, novelty promotes variation. arXiv preprint arXiv:2509.15194, 2025. Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, and Xiang Bai. Shuffle-r1: Efficient rl framework for multimodal large language models via data-centric dynamic shuffle. arXiv preprint arXiv:2508.05612, 2025a. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025b. 14 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt Templates System Prompt You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST be enclosed within <think></think> tags. The final answer MUST be put in boxed{}. A.2 Ablation Studies To assess the contribution of each component in VOGUE, we conduct series of ablation studies using the Qwen2.5-VL-7B model. We examine the effects of the visual uncertainty bonus, the token entropy bonus, the annealed sampling strategy, as well as the impact of alternative divergence measures and varying noise levels. Evaluation is performed across six multimodal benchmarks, including MathVerse (Zhang et al., 2024), MathVista (Lu et al., 2023), WeMath (Qiao et al., 2024), HallusionBench (Guan et al., 2024), ChartQA (Masry et al., 2022), and LogicVista (Xiao et al., 2024). These benchmarks together cover diverse aspects of multimodal reasoning, including mathematical problem solving, hallucination detection, chart interpretation, and logical reasoning. We report pass@1 accuracy in the following tables for each ablation setting. Table 4: Pass@1 accuracy on mathematical reasoning benchmarks testing the effectiveness of visual uncertainty and token entropy. Removing either visual uncertainty or token entropy reduces performance, while removing both leads to larger drop. This confirms that both components are effective for enhancing exploration and improving performance. Approach GRPO Full approach Visual uncertainty Entropy Visual uncertainty & entropy MathVerse MathVista WeMath Avg. 48. VOGUE 52.1 48.3 48.6 48.3 72.1 74.2 73.6 73.5 73.1 69.5 63.2 71.1 70.3 70.8 68. 65.8 64.1 64.3 63.3 Table 5: Pass@1 accuracy on general-domain reasoning benchmarks testing the effectiveness of visual uncertainty and token entropy. Removing either visual uncertainty or token entropy reduces performance, while removing both leads to larger drop. This demonstrates that both components enhance exploration and thereby improve performance. Approach GRPO Full approach Visual uncertainty Entropy Visual uncertainty & entropy HallusionBench ChartQA LogicVista Avg. 81.9 84.0 83.4 82.4 82.1 42.0 64.2 48.7 47.8 47.8 46. 67.9 66.9 66.8 65.9 68.6 VOGUE 71.0 69.7 70.2 69.2 15 Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Table 6: Pass@1 accuracy on mathematical reasoning benchmarks testing the effectiveness of annealed sampling. Using fixed-probability sampling yields lower performance compared to annealed sampling, underscoring the benefit of dynamically adjusting sampling probability. Approach GRPO MathVerse MathVista WeMath Avg. 48.0 VOGUE 72. 74.2 73.6 69.5 63.2 71.1 67.8 65.8 63.3 Annealed Sampling Fixed Prob 52.1 48.5 Table 7: Pass@1 accuracy on general-domain reasoning benchmarks testing the effectiveness of annealed sampling. Fixed-probability sampling yields lower performance than annealed sampling, highlighting the benefit of dynamically controlling the sampling probability. Approach GRPO Annealed Sampling Fixed Prob HallusionBench ChartQA LogicVista Avg. 68.6 VOGUE 71.0 69.9 81.9 84.0 82.6 42. 64.2 48.7 46.9 67.9 66.5 A.3 Discussions and Future Work key strength of VOGUE is its modularity and practical design. While implemented here within GRPO, its core mechanism is readily adaptable to other policy gradient methods and requires no additional supervision, making it practical, drop-in enhancement. This practicality extends to its computational profile: VOGUEs substantial performance gains are achieved with modest 20% overhead (4.95 vs. 4.12 minutes per step), trade-off that is highly efficient compared to naive online augmentation (i.e., treating each augmented view as an independent training sample). By successfully pioneering the use of visual uncertainty, this work suggests promising direction for future exploration into more complex, cross-modal uncertainty schemes. Future research could extend this framework to adaptively perturb both visual and textual inputs, potentially capturing richer uncertainty landscapes and further strengthening reasoning agents. Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning Table 8: Pass@1 accuracy on mathematical reasoning benchmarks with alternative divergence measures. The results validate our choice of symmetric KL, which promotes exploration while maintaining training stability, whereas forward KL causes excessive divergence and degrades performance. Approach MathVerse MathVista WeMath Avg. GRPO Symmetric KL Forward KL 48.0 52.1 39.4 72.1 69.5 63.2 VOGUE 74.2 70.7 71.1 56.1 65.8 55.4 Table 9: Pass@1 accuracy on general-domain reasoning benchmarks with alternative divergence measures. The results validate symmetric KL as it promotes exploration while maintaining training stability, whereas forward KL causes excessive divergence and degrades performance. Approach HallusionBench ChartQA LogicVista Avg. GRPO Symmetric KL Forward KL 68.6 71.0 67.5 VOGUE 81. 84.0 80.3 42.0 64.2 48.7 45.1 67.9 64.3 Table 10: Pass@1 accuracy on mathematical reasoning benchmarks with different noise levels. Moderate noise (σ = 0.4) yields the best accuracy. Approach MathVerse MathVista WeMath Avg. GRPO σ = 0.2 σ = 0.4 σ = 0.8 48.0 48.4 52.1 49.2 72. 69.5 63.2 VOGUE 74.0 74.2 73.5 68.8 71.1 66.8 63.7 65.8 63. Table 11: Pass@1 accuracy on general-domain reasoning benchmarks with different noise levels. Moderate noise (σ = 0.4) yields the best accuracy. Approach HallusionBench ChartQA LogicVista Avg. GRPO σ = 0.2 σ = 0.4 σ = 0.8 68.6 69.4 71.0 70. 81.9 42.0 64.2 VOGUE 81.9 84.0 82.9 45.3 48.7 46. 65.5 67.9 66."
        }
    ],
    "affiliations": [
        "Tencent AI Lab, Bellevue, WA",
        "University of Maryland, College Park",
        "University of North Carolina, Chapel Hill"
    ]
}