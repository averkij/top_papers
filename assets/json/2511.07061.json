{
    "paper_title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning",
    "authors": [
        "Xinran Li",
        "Yu Liu",
        "Jiaqi Qiao",
        "Xiujuan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets -- IEMOCAP and MELD -- show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding."
        },
        {
            "title": "Start",
            "content": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning Xinran Li, Yu Liu, Jiaqi Qiao, Xiujuan Xu* School of Software Technology, Dalian University of Technology Dalian, China 963707605@mail.dlut.edu.cn, yuliu@dlut.edu.cn, qiaobright@mail.dlut.edu.cn, xjxu@dlut.edu.cn 5 2 0 2 4 2 ] . [ 3 1 6 0 7 0 . 1 1 5 2 : r Abstract Emotion Recognition in Conversation (ERC) is crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speakers psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasetsIEMOCAP and MELDshow that our method achieves new state-ofthe-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLMbased emotional understanding. Code https://github.com/LiXinran6/PRC-Emo Introduction Emotions play critical role in human intelligence. For truly intelligent machines, emotional intelligence is not optional, based on the ideas of Rosalind W. Picard, the pioneer of affective computing. As conversational agents (Lee et al. 2020) and large language models (LLMs) become increasingly integrated into our daily lives, it is essential that these systems not only understand syntax and semantics but also perceive and interpret human emotions. Emotion Recognition in Conversation (ERC) emerges as crucial task toward *Corresponding author. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: An example of Explicit Emotion Recognition in Conversation (EERC) and Implicit Emotion Recognition in Conversation (IERC). In this conversation, although the girl feels apprehension due to the boys anxious utterance, wonder if we can make it, she expresses optimism in her own response to encourage both of them. building emotionally-aware AI, enabling natural and empathetic human-computer interaction (Hu et al. 2024). Large Language Models, with their generative architecture, have achieved significant performance improvements across various natural language processing tasks (Laskar et al. 2024). Currently, the field of ERC has also entered the era of LLMs, with increasing research efforts attempting to leverage LLMs to model conversational context and predict speakers emotional states. By harnessing the powerful prior knowledge of LLMs, researchers incorporate contextual utterance information, speaker personality, and other background knowledge to design high-quality prompts, combined with parameter-efficient fine-tuning methods such as LoRA (Hu et al. 2021b) for adaptive training. Although LLMs show great promise in ERC tasks, they still face the following three challenges: (1) Most existing studies have not adequately considered both explicit and implicit emotions when designing prompts. Emotion recognition tasks can be divided into explicit emotion recognition and implicit emotion recognition (Koga, Kando, and Miyao 2024), as illustrated in Figure 1. Explicit emotion recognition focuses on predicting emotions conveyed by the speaker through direct expressions, strong intonations, or obvious facial cues, while implicit emotion recognition aims to capture emotions genuinely felt by the speaker but not necessarily expressed through language. Ignoring the balance between these two aspects limits the models ability to fully comprehend complex emotional states, thereby constraining the accuracy of emotion recognition. (2) Current research often employs demonstration retrieval in prompt design, extracting the most similar sentences and their corresponding labels from demonstration repository to assist the model in understanding and predicting emotions (Lei et al. 2023). However, existing methods mostly construct the demonstration repository using only the training sets of commonly used datasets, which limits the diversity and coverage of demonstration samples. Due to the relatively homogeneous content of the repository, the models reasoning is often confined to the context of specific datasets, lacking sufficient generalization ability. This restricts the models adaptability to more complex and diverse conversational scenarios, thereby limiting its practical effectiveness. (3) At present, most studies mainly focus on optimizing prompt design and fine-tuning strategies, with relatively little attention paid to improving the overall training process (Yang et al. 2022). These methods often involve complex prompt designs and multi-stage training strategies, resulting in high computational resource consumption but limited performance gains. Moreover, ERC datasets generally suffer from severe class imbalance, making it difficult for models to adequately learn the features of low-frequency categories, which in turn affects overall performance and generalization ability. Therefore, optimizing training strategies to enhance model performance on imbalanced datasets remains an urgent and important challenge. To address the above challenges, we propose PRC-Emo: novel Prompt-Retrieval-Curriculum framework for ERC. Leveraging LLMs, our method takes both the current utterance and its dialogue history as input to generate two types of emotion interpretations: explicit (directly expressed emotions) and implicit (underlying, unspoken emotions). These complementary signals are integrated into structured prompts, enabling the model to better capture the nuanced emotional states of speakers. We further introduce the first ERC-specific demonstration retrieval repository, which combines high-quality samples from multiple datasets (IEMOCAP, MELD, EmoryNLP) and over 10,000 additional utterances generated by LLMs and refined by human annotators. These samples span six real-world domainshealthcare, workplace, education, family, social, and entertainmentoffering greater diversity and contextual richness for few-shot prompting. We improve the Curriculum Learning strategy by defining dialogue difficulty metric that combines weighted emotional shifts both within the same speaker and between different speakers. Based on this metric, we divide the training data into multiple levels (i.e., training buckets) from easy to hard, enabling the model to learn progressively. The main contributions are: We propose method that generates explicit and implicit emotion interpretations as external knowledge in prompts, enabling better capture of the speakers true emotional state. We construct the first demonstration retrieval repository for ERC that combines multi-source data and LLMgenerated, human-refined samples, significantly enhancing the models generalization ability. We enhance curriculum learning strategy based on dialogue difficulty, using dynamic training buckets for progressive learning and better robustness. Experiments on multiple benchmarks show our method achieves consistent SOTA results. The code and data have already been released on GitHub. Related Work With the rise of LLMs, the field of ERC can now be broadly divided into two categories: traditional methods and LLMbased methods. This section provides an overview of the methods previously used, as well as the application of curriculum learning in ERC. Traditional Methods Traditional research in the field of ERC has mainly focused on three directions: Recurrent Neural Networks (RNNs), Graph Neural Networks (GNNs), and Pre-trained Language Models (PLMs). In RNN-based studies, models typically leverage RNNs to perform sequential modeling of utterances or entire conversations. DialogueRNN (Majumder et al. 2019) utilizes an RNN structure to dynamically track the individual state of each speaker throughout the conversation. DialogueCRN (Hu, Wei, and Huai 2021) builds upon this by incorporating cognitive factors to enhance the understanding of both contextual and speaker-level information. In GNN-based studies, models construct graph structures to capture dependencies between utterances and speakers. DialogueGCN (Ghosal et al. 2019) treats utterances as nodes in graph and builds edges based on contextual information to represent semantic relationships. DAG-ERC (Shen et al. 2021) further considers speaker identity and utterance position when constructing directed acyclic graph to model dialogue structures more precisely. In the area of pre-trained language models, models such as BERT (Devlin et al. 2019) and RoBERTa (Liu et al. 2019) have been widely adopted due to their powerful language representation capabilities. BERTERC (Qin et al. 2023) enhances model performance by introducing guided texts, fine-grained emotion classification modules, and two-stage training strategy. LLM-based Methods Traditional ERC methods predominantly adopt discriminative architectures. With the rise of LLMs, InstructERC (Lei et al. 2023) is the first to introduce generative architecture Figure 2: PRC-Emos architecture has two main stages: extracting external supplementary knowledge and predicting emotion labels, with curriculum learning applied during training. The two prompts at the bottom-left extract explicit and implicit emotion interpretations and speaker characteristics as external knowledge. This information is passed to the bottom-right prompt, which performs the final emotion recognition by retrieving similar pairs from retrieval repository to aid the process. for tackling ERC tasks, opening up novel research direction. This approach incorporates an emotion-alignment auxiliary task and retrieval-augmented prompting module to enhance the models understanding of emotions. Moreover, BiosERC (Xue et al. 2024) enriches prompts with background information, such as speaker characteristics, and applies LoRA for the efficient fine-tuning of large models. These LLM-based approaches have significantly improved performance across various ERC benchmarks, demonstrating powerful generalization and adaptability, and paving the way for future advancements in the field."
        },
        {
            "title": "Curriculum Learning",
            "content": "Curriculum Learning (CL) (Bengio et al. 2009), training strategy that simulates human learning, has gained widespread attention in ERC in recent years. Yang et al. (2022) proposed Hybrid Curriculum Learning framework that combines difficulty designs at both the conversation and utterance levels, guiding the model to gradually learn complex emotions based on the frequency of emotion shifts and emotional similarity. Nguyen et al. (2024) introduced the MultiDAG+CL method, which integrates textual, acoustic, and visual features and leverages curriculum learning to address challenges related to emotional variations and data imbalance. Li, Xu, and Qiao (2025) introduce the concept of weighted emotion shifts, with difficulty design that focuses on modeling transitions between similar emotions. However, these methods only consider emotion transitions within the same speaker and do not take into account transitions between different speakers. Methodology This section presents the PRC-Emo architecture. We first define the problem, then describe the extraction of explicit and implicit emotion interpretations as external knowledge. Next, we detail the prompt retrieval template module and its ERC-specific demonstration retrieval repository. Finally, we explain the curriculum learning strategy used in training. The overall architecture of the PRC-Emo is illustrated in Figure 2. Problem Definition In Emotion Recognition in Conversation, conversation is represented as sequence of utterances = {u1, u2, u3, ..., uN }, where denotes the total number of utterances. Each utterance ui,sj is spoken by specific speaker sj, indicating that the ith utterance is from speaker sj. The objective of ERC is to assign an emotion label yk , such as joy or sadness, to each utterance ui, where is the set of possible emotion categories."
        },
        {
            "title": "Extraction of External Supplementary Knowledge",
            "content": "Given the extensive knowledge and powerful language understanding abilities of LLMs, we adopt Prompt Engineering approach by designing query templates to extract supplementary information valuable for ERC tasks. Inspired by Koga, Kando, and Miyao (2024), we observe that the current ERC field generally lacks clear definitions and systematic interpretations of explicit emotion and implicit emotion. Traditional models typically infer the emotion of an utterance based solely on the textual content of the current and historical utterances, without truly understanding the speakers internal emotional state. To address this, we design set of query templates specifically for generating explicit and implicit emotion interpretations. These interpretations serve as high-quality auxiliary signals and are injected into the model input, guiding the model to attend to both the speakers expressed emotions and their underlying emotional state. This enhances the models ability to capture multi-layered emotional expressions. In addition, inspired by Xue et al. (2024), we also extract speaker characteristic as supplementary knowledge to further support emotion recognition. These external supplementary knowledge sources are incorporated into the Retrieval Template Module for the final emotion recognition. For the interpretations of explicit emotion and implicit emotion, we input the historical utterances. For the speaker characteristic, we use the entire dialogue. The specific prompt design is shown in the bottom-left corner of Figure 2."
        },
        {
            "title": "Retrieval Template Module",
            "content": "To better leverage LLMs, we construct carefully designed Retrieval Template Module and fine-tune the model using LoRA technology. In this section, we introduce the components of the Retrieval Template Module, as well as the first demonstration retrieval repository specifically built for the ERC task. The specific prompt design is shown in the bottom-right corner of Figure 2. Components of Retrieval Template Module Each input consists of an Instruction, Historical Content, External Knowledge, Demonstration Retrieval, and Label Statement. Instruction. Defines the models role and core task objectives. Historical Content. In order to determine the emotion of given utterance, it is necessary to provide the model with historical context. We adopt history window w, which includes the past and current utterances along with the corresponding speaker names. External Knowledge. It consists of two parts: speaker characteristic descriptions and expert-level interpretations of explicit/implicit emotions. The speaker characteristics and expert emotion interpretations vary across different conversations. Demonstration Retrieval. Numerous studies have demonstrated the importance of demonstration retrieval (an Luo et al. 2024). To extract utterances most similar to the current one, we select three ERC examples from our self-constructed demonstration retrieval repository Ddomain. We utilize SBERT (Reimers and Gurevych 2019) to retrieve relevant demonstrations from the repository. By calculating the cosine similarity and comparing with all vectors in Ddomain, the top three demonstrationlabel pairs with the highest scores are selected. Label Statement. We constrain the model output to limited set of labels based on the specific dataset used. Demonstration Retrieval Repository To address the issue of imbalanced emotion label distribution in emotion recognition tasks, we construct an emotion dialogue augmentation dataset based on OpenAIs GPT-4o, covering wide range of real-life scenarios. The dataset spans common contexts such as healthcare, workplace, education, family, social interactions, and entertainment, and includes five emotion categories: happiness, neutral, fear, sadness, and anger. We adopt two-stage prompt strategy for data generation. First, the model produces 30 subtopics under given scenario to increase diversity. Then, it generates coherent twoperson dialogues for these subtopics, with each utterance assigned an emotion label. This process enables targeted creation of underrepresented emotions while preserving contextual consistency, resulting in more balanced and diverse corpus for downstream ERC models. For annotation, we adopt label masking + human verification strategy. GPT-4o first generates text samples with emotion labels, after which the labels are removed. Then, two researchers independently annotate each utterance with an emotion category. sample is officially included in the augmented dataset only if both annotators judgments exactly match the original label. After one round of filtering, we identify emotion categories still lacking sufficient samples and generate additional data accordingly. This process of generation and filtering repeats three times to finally obtain high-quality emotion dataset with the desired distribution. Appendix provides the detailed composition of our self-constructed dataset along with example sentences. Based on our self-constructed dataset and the training sets of three widely-used ERC benchmarksIEMOCAP, MELD, and EmoryNLPwe build the first demonstration retrieval repository in the ERC domain. This repository contains total of 36,712 utterances, including 14,009 from our dataset, 5,163 from IEMOCAP, 9,989 from MELD, and 7,551 from EmoryNLP. Each utterance in the repository includes the following information: the original text, emotion label, source dataset, dialogue ID, sentence position within the dialogue, and its vector representation. By default, we use SBERT to encode the utterances into high-quality semantic embeddings. This demonstration retrieval repository provides unified, structured, and high-quality data foundation for future research on emotion-aware retrieval and prompt engineering."
        },
        {
            "title": "Curriculum Learning",
            "content": "Curriculum learning (Bengio et al. 2009) trains models from easy to hard. This section presents our difficulty measure and training scheduler. Difficulty Measure Function Previous studies (Nguyen et al. 2024; Li, Xu, and Qiao 2025) use emotional shifts as difficulty metrics but overlook transitions between different speakers. We address this by introducing difficulty function based on weighted emotional shift frequency that accounts for emotional similarity, enabling more accurate conversation complexity estimation. The weighted emotional shift is defined as the emotional change between two consecutive utterances. Based on the two-dimensional arousal-valence emotion wheel, each emotion label corresponds to point on the unit circle, covering all emotion categories in the ERC dataset. The similarity between different emotions is calculated by Equation (1): sij = max(cos(θij), 0) 0 1 if vi vj > 0 if vi vj < 0 if vi vj = 0 (1) Here, sij denotes the similarity between emotion labels and j, vi represents the valence vector of label i, θij is the angle between labels and j, and is the total number of emotion labels in the dataset. The closer two emotions are, the higher their similarity score. The weighted emotional shift (WES) is then defined as shown in Equation (2): ES = sij + (2) We apply linear transformation to sij, where is weighting factor and is bias term. Thus, the difficulty of conversation ci is defined as shown in Equation (3), (4) and (5): DIF (ci) = ESsame(ci) + ESdif (ci) + Nsp(ci) Nu(ci) + Nsp(ci) (3) ESsame(ci) = ESdif (ci) = same shif (ci) (cid:88) j=1 ES dif shif t(ci) (cid:88) j=1 ES (4) (5) shif (ci) and dif where same shif t(ci) represents the total number of emotional shifts between utterances from the same speaker and between consecutive utterances from different speakers in conversation ci. Nu(ci) is the total number of utterances. Nsp(ci) denotes the number of speakers appearing in conversation ci, which serves as smoothing factor. ES is the Weighted Emotional Shift at the j-th emoj tional shift. The proposed algorithm is presented in Algorithm 1. Algorithm 1: Curriculum Learning Training with Difficulty Measure Function Require: 1: Input: Training dataset D, Model , Number of buckets n, Training epochs 2: Parameters: Linear transformation coefficients (k, b) end for for = 1 to S[p] 1 do if S[p][j] = S[p][j + 1] then for WES calculation Ensure: Trained model 3: // Phase 1: Calculate difficulty for each conversation 4: for each conversation ci do 5: similarity(S[p][j], S[p][j + 1]) ESsame ESsame + (k + b) // Calculate same speaker emotional shifts for each speaker do Initialize: ESsame 0, ESdif 0, Nsp 0, Nu 0 {Speaker emotion sequences} // Build speaker emotion sequences for each utterance uj in ci do speaker id get speaker(uj) emotion get emotion(uj) S[speaker id] S[speaker id] {emotion} Nu Nu + 1 6: 7: 8: 9: 10: 11: 12: 13: 14: Nsp {Number of unique speakers} 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end if 29: end for 30: // Compute conversation difficulty 31: 32: DIF (ci) ESsame+W ESdif +Nsp 33: end for 34: // Phase 2: Sort and partition dataset 35: Dsorted sort(D, by DIF ascending) 36: Partition Dsorted into buckets: {D1, D2, . . . , Dn} 37: where max(DIF (Di)) min(DIF (Di+1)) for = end for // Calculate different speaker emotional shifts for = 1 to Nu 1 do similarity(emotion(uj), emotion(uj+1)) ESdif ESdif + (k + b) if speaker(uj) = speaker(uj+1) then end if end for Nu+Nsp 1, . . . , 1 38: // Phase 3: Curriculum training 39: Dtrain 40: for epoch = 1 to do if epoch then 41: 42: Dtrain Dtrain Depoch {Add next difficulty bucket} end if 43: 44: train(M, Dtrain) 45: end for 46: return Training Scheduler The training scheduler aims to divide the entire dataset into several subsets with simDataset Partition Utterance Dialogues IEMOCAP MELD train + val test train + val test 5810 1623 11098 2610 120 31 1152 280 Table 1: Statistics of the two datasets. ilar difficulty levels (i.e., training buckets), denoted as {D1, D2, ..., Dn}. The model training starts with the easiest subset. After completing several epochs, the next more difficult subset is gradually introduced. Once all subsets have been involved in training, additional epochs are conducted on the entire dataset to further improve model performance. Experimental Settings This section introduces the datasets, baselines, and implementation details. Datasets We use two ERC datasets: IEMOCAP (Busso et al. 2008), which consists of dyadic conversations; and MELD (Poria et al. 2019), multiparty conversation dataset derived from Friends. The composition of datasets is shown in Table 1. Baselines We compare our proposed method with two categories of mainstream baselines: (1) Conventional models, including DialogueRNN (Majumder et al. 2019), ICON (Hazarika et al. 2018), DialogueGCN (Ghosal et al. 2019), COSMIC (Ghosal et al. 2020), MMGCN (Hu et al. 2021a), DAGERC (Shen et al. 2021), LR-GCN (Ren et al. 2022), MultiDAG+CL (Nguyen et al. 2024), CBERL (Meng et al. 2024), DER-GCN (Ai et al. 2024) and LSDGNN+ICL (Li, Xu, and Qiao 2025); and (2) Large language model (LLM)- based methods, including InstructERC (Lei et al. 2023) and BiosERC (Xue et al. 2024). Following the convention in the ERC field, the evaluation metric for most comparative experiments in this paper is the weighted F1 score by default. Implementation Details All experiments are conducted on single NVIDIA 4090D GPU. For the IEMOCAP dataset, Qwen2.5-7B-Instruct is used as the base model; for the MELD dataset, Qwen3-8B serves as the base model. The external large language model is consistently Qwen3-14B, which is employed to generate explicit and implicit emotion interpretations as well as speaker characteristics. Additional details on training hyperparameters and settings are provided in Appendix B. All experiments in this paperincluding comparative experimentsare repeated using five different random seeds, and the average results are reported. Results and Analysis This section presents the comparison between our model and state-of-the-art methods, the results of ablation studies, and series of comparative experiments. Comparison with the State of the Art Table 2 presents the performance of our model on the IEMOCAP and MELD, with bold values indicating the best results among all models. The results of other models are taken from their original papers. Our method, PRC-Emo, achieves SOTA performance on both the IEMOCAP and MELD datasets, with weighted F1 score improvements of 0.76% on IEMOCAP and 0.61% on MELD. As supplement, we also compare accuracy scores and achieve the best results. Ablation Experiments To investigate the importance of the Prompt, demonstration Retrieval, and Curriculum Learning modules in the PRC-Emo model, we conduct ablation experiments on two datasets. The results are shown in Table 3. From the results, it can be seen that each module plays an important role. Among them, the design of the Prompt contributes the most significant improvement, which demonstrates that the interpretations of explicit and implicit emotions have remarkable effect. This enables the model to deeply understand the speakers psychological state and make better predictions. Demonstration retrieval and curriculum learning further enhance the performance. When the P, R, and modules are removed, the model essentially becomes equivalent to directly fine-tuning LLM using LoRA. Comparative Experiments on Prompt Design To verify the effectiveness of our prompt design, we conduct comparative ablation experiments on different components of the prompt. Our prompt augmentation consists of the following components: Speaker characteristics, explicit emotion and implicit emotion Interpretations, and demonstration Retrieval. The training process adopts curriculum learning. The results are shown in Table 4. Table 4 shows that the full prompt design (PRC-Emo) achieves the best performance. Gradually removing demonstration retrieval (R), explicit emotion and implicit emotion interpretations (I), and speaker characteristics (S) leads to consistent performance drop, indicating that each component contributes positively to the model. Among them, explicit emotion and implicit emotion interpretations (I) have the most significant impact. Comparative Experiments on Curriculum Learning Methods for ERC Nguyen et al. (2024) proposed using emotional shifts (ES) as the key metric for measuring sentence difficulty. Li, Xu, and Qiao (2025) proposed using weighted emotional shifts (WES) as the key metric. Both methods only consider emotional changes within the same speaker. Our curriculum learning approach introduces emotional changes across different speakers, providing better assessment of sentence difficulty. Table 5 presents the performance comparison of the three curriculum learning methods. All comparisons are performed using the same base model and prompt, with ES and WES representing their respective curriculum learning strategies. Model DialogueRNN (Majumder et al. 2019) ICON (Hazarika et al. 2018) DialogueGCN (Ghosal et al. 2019) COSMIC (Ghosal et al. 2020) MMGCN (Hu et al. 2021a) DAG-ERC (Shen et al. 2021) LR-GCN (Ren et al. 2022) MultiDAG+CL (Nguyen et al. 2024) CBERL (Meng et al. 2024) DER-GCN (Ai et al. 2024) LSDGNN+ICL (Li, Xu, and Qiao 2025) InstructERC (Lei et al. 2023) BiosERC (Xue et al. 2024) IEMOCAP MELD Average Acc. W-F1 Acc. W-F1 Acc. W-F 60.27 61.84 61.34 63.98 63.40 64.00 65.25 62.75 59.89 57.03 63.50 56.30 59.90 64.18 58.10 61.14 65.28 65.21 65.25 58.65 62.44 66.22 68.03 65.83 65.76 63.63 68.30 65.60 66.95 69.08 64.00 66.54 68.08 66.89 69.27 67.75 66.10 69.40 67.16 64.07 70.24 71.39 69.15 70.27 71.19 69.83 70.51 68.57 68.25 67. 67.78 66.8 64.67 67.53 68.50 69.11 69.36 69.70 70.35 PRC-Emo (Ours) 71.03 71.95 71. 70.44 71.27 71.20 Table 2: Performance comparison of different ERC methods on IEMOCAP and MELD datasets. Model IEMOCAP MELD Methods IEMOCAP MELD 71.95 PRC-Emo 71.52 ( 0.43) w/o 70.74 ( 1.21) w/o + w/o + + 68.54 ( 3.41) 70.44 70.07 ( 0.37) 69.62 ( 0.82) 68.72 ( 1.72) Table 3: Ablation experiments. Model IEMOCAP MELD PRC-Emo 71.95 71.49 ( 0.46) w/o 70.27 ( 1.68) w/o + w/o + + 69.90 ( 2.05) 70.44 70.23 ( 0.21) 69.66 ( 0.78) 69.34 ( 1.10) Table 4: Prompt design comparative experiments. Comparative Experiments of Different LLMs Due to resource constraints, we only compare two of the latest high-performing open-source large language models: Qwen2.5-7B and Qwen3-8B. The experimental results are shown in Table 6. We observe an interesting phenomenon: the performance of different LLMs varies significantly across datasets. Specifically, Qwen2.5-7B significantly outperforms Qwen3-8B on the IEMOCAP dataset, while Qwen3-8B performs significantly better than Qwen2.5-7B on the MELD dataset. This may be due to Qwen3s optimization for complex real-world dialogues and possible data leakage, as MELD is based on the widely available script of the TV series Friends. Conclusion This paper proposes PRC-Emo, novel training framework for Emotion Recognition in Conversation (ERC) that integrates Prompt engineering, demonstration Retrieval, and base + Ours base + ES base + WES 71.95 70.43 ( 1.52) 71.48 ( 0.47) 70.44 69.87( 0.57) 70.14( 0.30) Table 5: Comparison of curriculum learning methods. Model IEMOCAP MELD Qwen2.5-7B Qwen3-8B 71.95 70.86 69.73 70. Table 6: Comparison of different LLMs. Curriculum learning using large language models (LLMs). PRC-Emo builds the first dedicated demonstration retrieval repository for ERC and designs emotion-sensitive prompt templates based on explicit and implicit emotional cues to better understand psychological states. The curriculum learning strategy organizes training from easy to hard based on weighted emotional shifts between same-speaker and different-speaker utterances. Experimental results on the IEMOCAP and MELD benchmark datasets show that PRCEmo achieves new state-of-the-art performance. This work highlights the potential of combining prompt-based learning with curriculum learning strategies to advance ERC tasks. In future work, we plan to further explore stronger LLMs, more efficient prompting paradigms, and advanced curriculum designs to enhance emotional reasoning and improve the robustness and generalization of ERC systems."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is funded in part by the National Natural Science Foundation of China Project (No. 62372078). References Ai, W.; Shou, Y.; Meng, T.; and Li, K. 2024. DER-GCN: Dialog and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialog Emotion Recognition. IEEE Transactions on Neural Networks and Learning Systems, 114. an Luo; Xu, X.; Liu, Y.; Pasupat, P.; and Kazemi, M. 2024. In-context Learning with Retrieved Demonstrations for Language Models: Survey. ArXiv, abs/2401.11624. Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09, 4148. New York, NY, USA: Association for Computing Machinery. ISBN 9781605585161. Busso, C.; Bulut, M.; Lee, C.-C.; Kazemzadeh, E. A.; Provost, E. M.; Kim, S.; Chang, J. N.; Lee, S.; and Narayanan, S. S. 2008. IEMOCAP: interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42: 335359. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics. Ghosal, D.; Majumder, N.; Gelbukh, A.; Mihalcea, R.; and Poria, S. 2020. COSMIC: COmmonSense knowledge for eMotion Identification in Conversations. In Cohn, T.; He, Y.; and Liu, Y., eds., Findings of the Association for Computational Linguistics: EMNLP 2020, 24702481. Online: Association for Computational Linguistics. Ghosal, D.; Majumder, N.; Poria, S.; Chhaya, N.; and Gelbukh, A. 2019. DialogueGCN: Graph Convolutional Neural Network for Emotion Recognition in Conversation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 154164. Hong Kong, China: Association for Computational Linguistics. Hazarika, D.; Poria, S.; Mihalcea, R.; Cambria, E.; and Zimmermann, R. 2018. ICON: Interactive Conversational MemIn Proory Network for Multimodal Emotion Detection. ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 25942604. Brussels, Belgium: Association for Computational Linguistics. Hu, D.; Wei, L.; and Huai, X. 2021. DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in ConIn Proceedings of the 59th Annual Meeting of versations. the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 70427052. Online: Association for Computational Linguistics. Hu, G.; Xin, Y.; Lyu, W.; Huang, H.; Sun, C.; Zhu, Z.; Gui, L.; and Cai, R. 2024. Recent Trends of Multimodal Affective Computing: Survey from NLP Perspective. arXiv preprint arXiv:2409.07388. Hu, J.; Liu, Y.; Zhao, J.; and Jin, Q. 2021a. MMGCN: Multimodal Fusion via Deep Graph Convolution Network for In Proceedings of Emotion Recognition in Conversation. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 56665675. Online: Association for Computational Linguistics. Hu, J. E.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; and Chen, W. 2021b. LoRA: Low-Rank Adaptation of Large Language Models. ArXiv, abs/2106.09685. Koga, Y.; Kando, S.; and Miyao, Y. 2024. Forecasting Implicit Emotions Elicited in Conversations. In Mahamood, S.; Minh, N. L.; and Ippolito, D., eds., Proceedings of the 17th International Natural Language Generation Conference, 145152. Tokyo, Japan: Association for Computational Linguistics. Laskar, M. T. R.; Alqahtani, S.; Bari, M. S.; Rahman, M.; Khan, M. A. M.; Khan, H.; Jahan, I.; Bhuiyan, A.; Tan, C. W.; Parvez, M. R.; Hoque, E.; Joty, S. R.; and Huang, J. X. 2024. Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations. In Conference on Empirical Methods in Natural Language Processing. Lee, M.-C.; Chiang, S.-Y.; Yeh, S.-C.; and Wen, T.-F. 2020. Study on emotion recognition and companion Chatbot using deep neural network. Multimedia Tools and Applications, 79: 19629 19657. Lei, S.; Dong, G.; Wang, X.; Wang, K.; and Wang, S. 2023. InstructERC: Reforming Emotion Recognition in Conversation with Retrieval Multi-Task LLMs Framework. arXiv preprint arXiv:2309.11911. Li, X.; Xu, X.; and Qiao, J. 2025. Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation. In Proceedings of the 28th European Conference on Artificial Intelligence (ECAI 2025), volume 413 of Frontiers in Artificial Intelligence and Applications, 40334040. IOS Press. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: Robustly Optimized BERT Pretraining Approach. ArXiv, abs/1907.11692. Majumder, N.; Poria, S.; Hazarika, D.; Mihalcea, R.; Gelbukh, A.; and Cambria, E. 2019. DialogueRNN: An Attentive RNN for Emotion Detection in Conversations. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 68186825. Meng, T.; Shou, Y.; Ai, W.; Yin, N.; and Li, K. 2024. Deep Imbalanced Learning for Multimodal Emotion Recognition IEEE Transactions on Artificial Intelliin Conversations. gence, 115. Nguyen, C.-V. T.; Nguyen, C.-B.; Le, D.-T.; and Ha, Q.- T. 2024. Curriculum Learning Meets Directed Acyclic In ProceedGraph for Multimodal Emotion Recognition. ings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 42594265. Torino, Italia: ELRA and ICCL. contextual settings. As shown in Table 7, the distribution of data samples and emotion labels across each category is presented. Table 8 and Table 9 provide representative examples from different domains in our dataset, illustrating the diversity and contextual richness of the collected dialogues. Appendix The model is fine-tuned using LoRA with rank of 32. The learning rate is set to 3e-4, and the optimizer is paired with linear learning rate scheduler. Training runs for total of 4 epochs, with curriculum learning applied during the first two epochs, followed by two epochs of overall training. The curriculum learning strategy divides the training data into two difficulty levels and updates the curriculum level at the end of each epoch. The context window size is set to 5 to enable the model to effectively integrate preceding context. Training uses batch size of 1 with 4 steps of gradient accumulation. The maximum number of training steps is controlled by parameter. The maximum gradient norm is set to 0.3, and warm-up ratio of 0.03 is applied. Evaluation, model saving, and logging are performed every 50 steps, using the weighted F1 score as the metric to select the best model. Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria, E.; and Mihalcea, R. 2019. MELD: Multimodal MultiParty Dataset for Emotion Recognition in Conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 527536. Florence, Italy: Association for Computational Linguistics. Qin, X.; Wu, Z.; Zhang, T.; Li, Y.; Luan, J.; Wang, B.; Wang, L.; and Cui, J. 2023. BERT-ERC: fine-tuning BERT is enough for emotion recognition in conversation. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI23/IAAI23/EAAI23. AAAI Press. ISBN 978-157735-880-0. Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Conference on Empirical Methods in Natural Language Processing. Ren, M.; Huang, X.; Li, W.; Song, D.; and Nie, W. 2022. LR-GCN: Latent Relation-Aware Graph Convolutional Network for Conversational Emotion Recognition. IEEE Transactions on Multimedia, 24: 44224432. Shen, W.; Wu, S.; Yang, Y.; and Quan, X. 2021. Directed Acyclic Graph Network for Conversational Emotion Recognition. In Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 15511560. Online: Association for Computational Linguistics. Xue, J.; Nguyen, P. M.; Matheny, B.; and Nguyen, L. M. 2024. BiosERC: Integrating Biography Speakers Supported In International Conference on by LLMs for ERC Tasks. Artificial Neural Networks. Yang, L.; Shen, Y.; Mao, Y.; and Cai, L. 2022. Hybrid Curriculum Learning for Emotion Recognition in Conversation. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10): 1159511603. Appendix Our self-constructed dataset consists of 14,009 sentences spanning seven distinct domains: healthcare, workplace, education, family, social interactions, entertainment, and comprehensive scenarios. The dataset maintains balanced distribution across both emotional categories and domain contexts. Among the domains, the comprehensive category contains the largest number of samples (3,314 sentences), followed by family interactions (2,015 sentences) and workplace scenarios (1,862 sentences). The healthcare domain represents the smallest category with 1,625 sentences. Across emotional categories, neutral emotions are most prevalent (3,309 sentences), while anger represents the least frequent category (2,566 sentences). This multidomain, balanced distribution ensures comprehensive coverage of real-world conversational scenarios while providing sufficient samples for robust model training across diverse Domain Happiness Neutral Fear Sadness Anger Total Healthcare Workplace Education Family Social Entertainment Comprehensive 171 155 486 147 518 613 623 421 550 324 562 341 332 779 345 398 245 392 320 281 642 381 459 276 451 312 267 652 307 300 406 463 297 175 618 1,625 1,862 1,737 2,015 1,788 1,668 3, Total 2,713 3,309 2,623 2,798 2, 14,009 Table 7: Distribution of emotions across different domains in the self-constructed dataset. Domain Example Utterance Healthcare Workplace Education (1) Ive been feeling great lately! just had checkup, and the doctor said everything looks good! (2) Thats fantastic! Do you think the checkup process was complicated at all? (3) Not really! There was just little bit of waiting, but it was totally worth it! (4) Im bit worried about my own checkup results. Ive heard some people get some pretty scary diagnoses. (5) completely understand. Waiting for those results can be really stressful, but regular checkups are so important. They help catch any issues early on. (6) Exactly! Its all about staying positive. Even if the results arent great, knowing is better than being in the dark! (7) Youre right! think need to schedule my checkup too and stop worrying so much. (8) Definitely! Facing it is way better than hiding from it! Emotion Happiness Neutral Happiness Fear Neutral Happiness Neutral Neutral Anger (1) Lately, feel like its becoming harder to find balance between work and life, and its really frustrating! (2) feel the same way. often work late, and by the time get home, Im completely exhausted and have no time to spend with my family. (3) That scares me. Im worried that if this continues, it will affect my health and my relationships! (4) Exactly, work takes up so much of my time that Im starting to wonder if Im wasting my life. (5) Sometimes, seriously think about quitting my job to pursue something more meaningful! (6) But Im trying to figure out solutions, like setting work boundaries and scheduling regular breaks. (7) That sounds great! should try that too and give myself some space! Happiness (8) Yes, finding balance that works for us is essential for living more fulfilling life! Happiness Sadness Sadness Neutral Anger Fear (1) really believe that parents play crucial role in their childrens education; their support can make kids more confident. (2) However, sometimes feel helpless because parents expectations can put lot of pressure on children. (3) Yeah, high expectations can really make kids feel scared; they worry that they wont be good enough. (4) Thats why communication is so important; parents need to understand their childrens feelings rather than just chase grades. (5) hope more parents realize that companionship and understanding are more important than grades. (6) Exactly! Education isnt just about teaching kids knowledge; its also about nurturing their mental health and confidence. (7) We need to create supportive environment where kids can grow freely instead of being bound by invisible pressure. Happiness Sadness Fear Neutral Anger Happiness Neutral Table 8: Representative dialogue examples from different domains in the self-constructed dataset (Part I). Domain Family Social Entertainment Example Utterance (1) Lately, it feels like theres tense atmosphere at home. No ones really talking, and honestly, it makes me feel scared. (2) Same here. Every time get home, just see my parents looking down at their phones, and it really hurts. It feels like weve lost our bridge to communicate. (3) Exactly. If we dont talk things out soon, the issues will just get worse. We should find good time to sit down and have real conversation, but Im always worried itll turn into an argument. (4) Me too. Every time family issues come up, the mood just gets bad, and it feels like everyone is avoiding it. (5) But if we dont talk about it, the frustration just builds up inside. Its honestly such sad feeling. Are we just going to stay silent forever? (6) Thats why think we need to face this, even if its hard. We have to find the right moment and try to gently encourage everyone to open up. If we keep going like this, itll really lead to despair. Emotion Fear Sadness Fear Sadness Sadness Anger (1) It feels really lonely; everyone else seems so relaxed, but just cant seem to fit in. (2) Whenever think about saying hello, my mind fills with all sorts of negative thoughts. (3) But then think, if could just muster the courage to chat, might find unexpected joy. (4) Yeah, maybe we can practice together, support each other, and slowly overcome this anxiety. (5) Exactly! That way, we wont feel so scared anymore. Sadness Fear Happiness Happiness Happiness (1) watched that new horror movie last night, and it was terrifying! couldnt help but cover my eyes during the tense scenes. (2) Same here! Some scenes made me think about things that could actually happen in real life, which was really creepy. (3) Exactly! Especially the parts about stalkersI started worrying about whether someone might be following me home at night. (4) Thats the part hate the most! The unease from the movie really lingers into reality. even left the lights on while was trying to sleep last night. (5) Right? started doubting the people around me, thinking they might be hiding something. (6) We are really sensitive, but theres something thrilling about this fear. (7) Sometimes, actually find the experience fun. Even though Im scared, still want to watch it again. (8) Maybe we should find some lighter movies to relax bit, but the allure of horror films is just irresistible! Fear Fear Fear Fear Fear Neutral Happiness Happiness Table 9: Representative dialogue examples from different domains in the self-constructed dataset (Part II)."
        }
    ],
    "affiliations": [
        "School of Software Technology, Dalian University of Technology"
    ]
}