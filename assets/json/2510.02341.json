{
    "paper_title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning",
    "authors": [
        "Yifan Wang",
        "Bolian Li",
        "Junlin Wu",
        "Zhaoxuan Tan",
        "Zheli Liu",
        "Ruqi Zhang",
        "Ananth Grama",
        "Qingkai Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \\textbf{DRIFT} (\\textbf{D}issatisfaction-\\textbf{R}efined \\textbf{I}terative pre\\textbf{F}erence \\textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \\textit{WildFeedback} datasets and synthetic \\textit{UltraFeedback} datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 4 3 2 0 . 0 1 5 2 : r DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning Yifan Wang1, Bolian Li1, Junlin Wu2, Zhaoxuan Tan3, Zheli Liu4, Ruqi Zhang1, Ananth Grama1, Qingkai Zeng4 1Department of Computer Science, Purdue University 2Department of Computer Science, Washington University in St. Louis 3Department of Computer Science and Engineering, University of Notre Dame 4College of Computer Science, Nankai University {wang5617, li4468}@purdue.edu, junlin.wu@wustl.edu, qzengnkcs@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23% (7B) / +7.61% (14B) on WildBench Task Score and up to +8.95% (7B) / +12.29% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world posttraining that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) now power wide range of real-world applications, including conversational assistants (e.g., GPT, Claude, Gemini), customer support, search and recommendation, productivity and education tools, and code generation. key driver of this success is preference learning, critical component of post-training that aligns model behavior with human judgments and values. Reinforcement Learning from Human Feedback (RLHF) [1] pioneered this approach by training reward model on human preference data and subsequently optimizing the policy using reinforcement learning algorithms [2]. Direct Preference Optimization (DPO) [3] simplified this process by directly optimizing on preference pairs without requiring an explicit reward model, making the training procedure more stable and computationally efficient, while achieving comparable alignment performance. However, these approaches depend on costly, carefully curated human preference annotations that are difficult to scale across domains and evolving user needs.In contrast, deployed LLM systems continuously generate vast amounts of real-world interaction data. Beyond offering scalability, such real data often captures richer and more nuanced spectrum of human preferences than small, curated annotation datasets, as users naturally convey satisfaction, dissatisfaction, and refinement intents during conversation. This motivates key question: Corresponding author. Figure 1: Overview of user feedback signals and the DRIFT framework. Explicit feedback (left) is sparse and biased, as most users are passive consumers. In contrast, implicit feedback (middle) provides abundant and informative signals, where dissatisfaction (DSAT) is far more prevalent than satisfaction (SAT) (e.g., 12% vs 5% in the WildFeedback dataset). DRIFT (right) leverages these DSAT signals for preference learning, enabling our 14B model to surpass commercial models. How can we transform abundant but implicit user feedback from real-world interactions into scalable and effective preference learning signals for LLMs? From data collection perspective, existing chatbot platform as shown in Figure 1 (left) attempts to gather user feedback through explicit mechanisms: (1) asking users to compare and rank multiple model responses, or (2) providing simple feedback buttons (e.g. thumbs up/down) at the end of chat interfaces. However, these collection methods are inefficient, as most are passive consumers [4], with only 13% users willing to provide explicit feedback. Moreover, those who do provide feedback often express extreme opinions (strongly positive or negative) that may not reflect the broader distribution of user preferences. However, as illustrated by the example in Figure 1, (middle) users naturally express their preferences through the conversation itself through follow-up questions, correction requests, and iterative refinements, creating rich source of implicit feedback. Beyond scalability, such real interaction data can contain richer and more representative preference information than curated annotation datasets, capturing fine-grained user intents that explicit labels often miss. Recent datasets such as WildChat-1M [5] and LMSYS-Chat-1M [6] have collected over one million real-world conversations, creating rich foundation for studying naturally occurring user feedback. Building on these resources, several studies have explored ways to extract preference signals directly from user interactions. For example, [7] demonstrate that naturally occurring user feedback appears in approximately 30% of conversations and propose mining preferences by detecting explicit evaluative user responses. Similarly, WildFeedback [8] applies user satisfaction estimation [9] to automatically extract satisfaction and dissatisfaction labels to construct large-scale preference datasets. From preference learning method perspective, recent works explored self-generated strategies to reduce reliance on human annotation. Self-Rewarding Language Models [10] prompt the training model itself to score its own rollouts, the synchronized improvement of chosen and rejected responses progressively reduces but face key limitation: their contrast, which in turn undermines effective preference learning [11]. An alternative approach, SPIN [12] treats ground-truth responses from the SFT dataset as chosen and self-generated ones as rejected, yet is difficult to apply in practice where gold-standard responses are often rare, limiting its ability to generalize to broader scenarios. In contrast to positive feedback, dissatisfaction signals are naturally abundant as users refine suboptimal outputs through interaction. To leverage this underutilized signal, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), simple yet scalable method that directly leverages user dissatisfaction (DSAT) signals from authentic conversations to iteratively enhance model performance. Unlike SPIN, which fixes supervised responses as positives and treats self-generated ones as negatives, DRIFT anchors each training pair with real DSAT negative and samples the chosen responses from the current policy, enabling dynamic and policy-aligned adaptation. Our contributions are: Empirical Validation: DRIFT consistently surpasses other iterative self-improving methods, SPIN and IterDPO, yielding gains of up to +6.23% (7B) / +7.61% (14B) in WildBench Task Score and up to +8.95% (7B) / +12.29% (14B) in AlpacaEval2 win rate over base models. Enhanced Exploration: DRIFT preserves larger exploration space and generates more diverse responses with substantially broader coverage of the global high-reward region. 2 Theoretical Analysis: We show that DRIFT maintains non-vanishing expected preference margin and prevents gradient collapse, which is critical limitation in existing self-improving models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Learning from Real-World User Feedback Since relying on human labeling is not only expensive and time-consuming, but also highly subjective to small set of annotators, recent work has shifted toward leveraging naturally occurring signals in the wild. natural starting point is to incorporate the most explicit feedback of user input, including edits and demonstrations. [13] use edits in writing assistant settings to infer latent preferences, keeping the base LLM frozen and training separate preference module that conditions future outputs. Similarly, [14, 15] rely on handful of user-provided demonstrations to bootstrap alignment, iteratively generating comparison pairs by treating user examples as preferred over LLM outputs and their intermediate revisions. Another stream of work draws on implicit feedback that emerges naturally during conversations. [16] introduced self-feeding chatbot that monitors user satisfaction during deployment: satisfied turns are added as new training data, while explicit feedback is requested when dissatisfaction is detected. [17] extended this idea, regenerating improved responses for dissatisfaction and applying them in supervised fine-tuning (SFT). While this provides some benefit on short tasks like MT-Bench [18], gains are limited on more complex real-world task benchmarks such as WildBench [19]. Building further on implicit signals, recent approaches transform them into pairwise preferences for direct optimization. [8] identify dissatisfaction with GPT-4, summarize user preferences, and generate improved responses as chosen answers, contrasting them with the original unsatisfactory replies as rejected. [20] follow similar philosophy by extracting reader-centric questions from user-generated content, sampling multiple candidate answers with an LLM, and ranking them with reward model to construct chosenrejected pairs. In contrast, our approach requires no positive responses from stronger models, no reward model, and certainly no human-provided golden truth, relying solely on abundant real-world dissatisfaction (DSAT) signals and dynamic positives from the evolving policy. 2.2 Self-Improvement and Iterative Direct Preference Optimization Self-improvement strategies have emerged as an important avenue for iteratively enhancing model performance. SPIN [12] formulates this framework by treating the previous iteration model as the opponent and the current iteration model as the main player, constructing preference data with the SFT response as the chosen response and the prior iterations response as the rejected response, thereby fully utilizing the SFT data without requiring additional human annotation. Beyond this, Iterative DPO [21, 22] and Self-Rewarding Language Models [10] and its variants [23, 24, 25, 26, 27] explore generating on policy preference data via ranking responses by the model itself or reward model/ verifier and then conducting iterative DPO training. However, subsequent studies reveal that self-improving models face critical limitation: the chosen and rejected responses can become too similar, leading to weak preference signals. To address this, Temporal Self-Rewarding LMs [11] decouple chosen and rejected responses through past-future anchoring, while CREAM [28] introduces consistency regularization to stabilize the preference signal. Our method naturally avoids this issue by anchoring on genuine DSAT negatives and sampling fresh positives from the evolving policy, thereby maintaining non-vanishing preference margin and preventing gradient collapse."
        },
        {
            "title": "3 DRIFT: Dissatisfaction-Refined Iterative pre-Ference Training",
            "content": "User feedback in real-world systems is inherently asymmetric, while satisfied users rarely provided explicit positive responses, dissatisfied users are more likely to offer abundant and detailed feedback in the form of complaints, corrections, and stated preferences. As result, dissatisfaction (DSAT) signals are not only more frequent but also richer in information than satisfaction (SAT) signals. Instead of viewing this imbalance as limitation, DRIFT exploits it by treating authentic dissatisfaction as high-quality negative supervision, while generating positive feedback dynamically from the evolving model itself. Our approach is motivated by two key insights: Genuine dissatisfaction reflects real deployment failure modes, offering more informative and reliable supervision than synthetically constructed negatives. Iteratively sampling fresh positives from the current policy maintains the margin between chosen and rejected responses, thus mitigating the gradient collapse that plagues most self-improvement methods as these responses become increasingly similar over time. Formally, let denote the prompt space and the response space. The current model is πθ : (0, 1), and πref is frozen reference model. Let XDSAT denote prompts with observed dissatisfaction signals. For each XDSAT, we observe set of negative responses: DSAT(x) = { : user expressed dissatisfaction}. (1) DRIFT proceeds in iterative refinement cycles, where each round builds upon the improved policy from the previous iteration (Algorithm 1). We begin by filtering the wild dataset to extract dissatisfaction (DSAT) cases, producing prompt-response pairs (x, y) that reflect concrete failure modes encountered in real-world scenarios. At each iteration, the current model πθk generates fresh positive response y+ for the same prompt x, allowing the positive response to evolve alongside the models capacities. The model is then updated by minimizing the DPO loss: (cid:16) (cid:17)(cid:105) (cid:104) LDPO = E(x,y+,y) log σ β log πθ(y+x) πref (y+x) β log πθ(yx) πref (yx) (2) where β controls the preference margin, ℓ() counts tokens, and σ denotes the logistic function. Algorithm 1 DRIFT: Dissatisfaction-Refined Iterative Preference Training 1: Input: Wild implicit feedback dataset, current model πθ, reference model πref , number of iterations 2: Output: Updated model parameters θK 3: Filter: Extract DSAT signals to form = {(x, y) DSAT(x)} 4: for = 1, . . . , do 5: 6: 7: end for Positive Sampling: For each (x, y) D, sample fresh positive response y+ πθk ( x) Loss Update: Update θk by minimizing LDPO (Eq. 2)"
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we evaluate DRIFT against strong self-improvement baselines, focusing on real world task performance. Sec. 4.1 outlines datasets, training recipe, and evaluation benchmarks. Sec. 4.2 presents the task performance on WildBench and AlpacaEval2. Then, in Sec. 4.3, we analyze exploration cability on response space of each method through global high-reward coverage. 4.1 Setup Datasets. WildFeedback (real-world, user-feedback). The WildFeedback dataset is derived from WildChat-1M, corpus of over one million humanChatGPT conversations, by assigning per-turn labels Satisfaction (SAT), Dissatisfaction (DSAT), Neutral (Non-DSAT/Non-SAT). Labels are derived using SPUR [9], which recursively prompts GPT-4 to learn SAT/DSAT rubrics from thumb-annotated conversations and applies them to score satisfaction/ dissatisfaction. Among all 88,920 unique conversations, only 4,478 (5.04%) conversations were labeled SAT, while 10,632 (11.96%) were labeled DSAT, which is more than twice the SAT count. We also curate 491 seed data items (0.55%) in which LLM responses transition from DSAT to SAT after revision, naturally yielding preference pairs. UltraFeedback (synthetic, LLM-labeled). For completeness and comparability with prior self-improvement work, we also evaluate on UltraFeedback in which each prompt has four completions from different models that are scored by GPT-4. This synthetic setting provides complementary evaluation to the real-world data setting and ensures fair comparison with SPIN/ IterDPO on commonly used LLM-labeled preference data. Table 1: Comparison of preference data construction strategies across different methods. Self-Gen means responses are generated by the current policy. Real is from user feedback. Method SPIN IterDPO DRIFT (Ours) Chosen Response Rejected Response Self-Gen Real (SAT) Self-Gen Real (DSAT) No Positive Examples Leverage Real User Feedback Training Recipe. Our experiments are conducted on Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct. We adopt two-stage training: (1) Warm start: train on the 491 seed DSATSAT pairs, which provides an initial aligned policy. (2) Iterative preference training: After warm start, each method constructs fresh preference pairs. Per-iteration preference data construction  (Table 1)  : DRIFT: In WildFeedback, we keep the DSAT reply as the rejected response and, at each iteration, sample fresh response from the current policy using prompt which contains the full conversation including the DSAT user turn and an explicit improvement instruction. In UltraFeedback, we replace the original chosen with fresh policy sample. SPIN: In WildFeedback, we use the SAT reply as the chosen response and policy sample as the rejected response using the prompt which is the conversation before the SAT user turn. In UltraFeedback, we replace the original rejected with fresh policy sample. the chosen context includes the IterDPO: In WildFeedback, we generate two responses using different prompts: full conversation including the DSAT user turn and an explicit improvement instruction, while the rejected context is the conversation before the DSAT user turn which does not reveal the user preference and instruction inforIn UltraFeedback, both responses are generated from the same prompt and ranked by the reward model mation. (OpenAssistant/reward-model-deberta-v3-large-v2); the higher-scored response is chosen and the other is rejected. We then perform one epoch of DPO training after data generation, which prevents overfitting during iterative training. Full training details are presented in Appendix D. Evaluation. We evaluate on WildBench (Elo, Task Score) and AlpacaEval2 (win rate, length-controlled; LC). WildBench is built from challenging real-world user queries in WildChat-1M and spans five diverse categories: Creative, Reasoning, Math, Info Seek, and Coding, making it well suited for assessing our method for real-world performance. The WildBench Task Score is computed as weighted average across these five tasks. 4.2 Performance evaluation 4.2.1 Results on WildFeedback We first examine performance on the real-world WildFeedback dataset, which contains authentic user satisfaction/ dissatisfaction labels and exhibits strong imbalance: dissatisfied responses (DSAT) outnumber satisfied ones (SAT) by more than 2:1. Hence, we consider two configurations: Controlled setting with around 4k samples (matching SPIN for fair comparison), and Full setting using all 11k DSAT samples to demonstrate DRIFTs ability to exploit abundant negative feedback. Table 2: Results of training on WildFeedback. Appendix for detailed per-task results. Method Base Seed SPIN iter1 iter2 IterDPO iter1 (controlled) iter2 (controlled) iter1 (full) iter2 (full) DRIFT (Ours) iter1 (controlled) iter2 (controlled) iter1 (full) iter2 (full) WildBench AlpacaEval2 7B Elo 1194.67 1193.66 Score 48.66 49. 14B 7B 14B Elo Score Win LC Win LC 1213.17 1213.50 55.08 54.93 37.69 42.67 39.73 42. 36.65 40.25 43.58 45.78 1180.75 1173.45 42.86 37.86 1200.63 1192.56 47.16 44. 26.21 20.56 34.09 29.00 25.53 18.57 37.28 30.91 1189.43 1192.46 1185.11 1182.33 1197.13 1195.33 1194.81 1199. 47.07 48.94 46.31 46.17 51.06 51.06 50.61 51.69 1206.65 1211.69 1205.48 1206.63 1215.73 1214.03 1212.83 1217.61 51.79 56.63 52.34 51.38 58.37 57.59 57.27 58. 41.55 41.18 40.36 35.76 42.73 43.79 43.90 46.64 41.35 40.14 39.85 37.06 41.41 41.49 40.32 42.72 37.14 48.32 38.07 32.88 48.76 46.83 48.63 45. 43.14 47.28 43.99 37.92 45.42 43.48 47.46 44.93 As shown in Table 2, DRIFT raises the WildBench Task Score by 6.23% (+3.03) for 7B and 5.97% (+3.29) for 14B, and boosts AlpacaEval2 win rate by 8.95% for 7B and 12.11% for 14B compared to the base models. And our method consistently outperforms both SPIN and IterDPO across all metrics in both controlled and full data settings. 5 While SPIN shows degraded performance with iterations likely due to its reliance on fixed set of satisfied responses becoming stale, DRIFT maintains steady improvements, suggesting that its strategy prevents distribution shift. IterDPO performs better than SPIN but still lags behind DRIFT in both settings, indicating that while reward model guidance helps, the real world informative DSAT examples provides superior training signal. Notably, DRIFTs controlled setting (using only 4k samples) already matches or exceeds IterDPOs full setting performance, demonstrating the efficiency of dissatisfaction-anchored learning. The stronger gains at the 14B scale suggest that DRIFT benefits larger models more, likely because their greater capacity makes it easier to discover better positives while being anchored by real negatives. This supports our hypothesis that DRIFT is particularly effective as model capacity increases, making it well suited for scaling up. 4.2.2 Results on UltraFeedback To ensure comprehensive evaluation and fair comparison with prior work, we also evaluate on the synthetic UltraFeedback dataset, where preferences are scored by GPT-4 rather than derived from real user interactions. This complementary evaluation helps assess whether DRIFTs advantages generalize beyond the specific characteristics of real-world dissatisfaction signals to more conventional preference learning settings. As shown in Table 3, DRIFT outperforms the base model with gains of 4.62% (+2.25) for 7B and 7.61% (+4.19) for 14B on WildBench Task Score, and improvements of +3.35% (7B) and +12.29% (14B) on AlpacaEval2 win rate. Compared to the best SPIN/IterDPO results, DRIFT achieves additional gains of +2.14 (7B) and +4.49 (14B) on Task Score, as well as +6.51% (7B) and +6.10% (14B) on win rate. Table 3: Results of training on UltraFeedback. Appendix for detailed per-task results. WildBench AlpacaEval2 Method 7B 14B 7B 14B Elo Score Elo Score Win LC Win LC Base SPIN iter1 iter2 IterDPO iter1 iter2 1194.67 48.66 1213.17 55. 37.69 39.73 36.65 43.58 1163.16 1139.66 35.10 25. 1178.88 1155.07 36.99 28.01 18.39 13.23 26.62 19.91 16.09 13.66 28.20 24. 1194.45 1192.01 48.77 48.49 1214.14 1215.12 54.21 54.78 34.53 32.15 40.55 39. 33.29 28.51 42.84 40.47 DRIFT (Ours) iter1 iter2 1197.04 1197.94 50.91 50.32 1215.67 1218. 58.52 59.27 41.04 40.47 40.37 37.09 47.89 48.94 48.46 47.43 4.3 Exploratory Capacity Analysis: DRIFT Explores More Diverse High Reward Solutions central question in preference learning is whether pushing rewards upward narrows the response distribution and erodes exploration capacity and diversity. Methods that optimize aggressively for top scores or fixed chosen responses can shift toward mode seeking: peak metrics improve while alternative high-quality modes are under explored. We further investigate whether DRIFTs strategy of sampling fresh positives while anchoring on real dissatisfied responses enhances the models ability to explore the space of high-quality solutions than SPIN or iterDPO, which may progressively constrain the solution space. For given prompt, we first compute embeddings for all responses, then obtain 2D semantic projection with UMAP and construct reward-weighted surface Zall(g) [0, 1] on regular grid via Gaussian KDE. The global high-reward region is = { : Zall(g) zhigh }, zhigh = Quantile(cid:0)Zall, q(cid:1), with = 0.8. For each method m, we compute reward-weighted surface Dm(g) [0, 1] on the same grid and with the same bandwidth. We define its share inside the global high-reward region as Sm = { : Dm(g) zhigh }, Share(m) = Sm . 6 We render the global surface Zall as background terrain, draw the boundary of (dashed), and shade Sm for each method. Regions of not shared by method remain transparent. We sample 128 responses for each prompt per method and compute the average share of highreward regions over 50 prompts (Figure 2). DRIFT consistently covers the largest portion at both 7B and 14B scales, with larger gap at 14B, showing greater diversity and scalability. As illustrated in Figure 3, DRIFT spans broader range of high-reward regions than SPIN and IterDPO, populating diverse semantic islands, while the baselines concentrate around fewer basins and thus overfit to narrow subset. Notably, DRIFT also discovered distinct region (circled in the reward topography) where it uniquely employed markdown formatting to structure research papers, demonstrating alternative presentation styles for the same prompt. Figure 2: Comparison of high reward region coverage. Figure 3: Example of response diversity and quality comparison via semantic clustering. Two central plots: Left is the UMAP scatter of all responses; Right is the reward-weighted topography showing the global high-reward region and the high-reward coverage of the three methods. DRIFT covers substantially larger portion of the global high-reward region than SPIN or IterDPO and uniquely explores markdown formatting (yellow circle). Full prompt and responses are in Appendix Anchoring on authentic DSAT negatives while sampling fresh positives enables DRIFT to maintain and amplify exploratory capacity. DRIFT not only reaches high reward but also occupies far broader set of high-reward areas, explaining why it continues to improve over iterations without collapsing to small family of solutions."
        },
        {
            "title": "5 Theoretical Analysis",
            "content": "DRIFT shows superior performance by leveraging real-world user dissatisfaction as high-quality negatives, which leads us to further investigate and analyze how real-world data shapes the success of DRIFT and why some other strong baselines like SPIN and IterDPO fall short. In this section, we prove that DRIFT maintains usable gradient 7 signal through fresh positives anchored by genuine DSAT negatives; in contrast, updates fitted to fixed SAT set like SPIN can easily overfit to small sub-optimal subset with gradient collapse. Notation. Let be the prompt set. For particular prompt , denote y+ as the chosen response and the rejected response. Let the generation likelihoods of y+ and to be π+ = πθ(y+ x) and π = πθ(y x) respectively, where πθ is the language model we aim to train. For formula simplicity, we denote the implicit reward margin [3] as: = β (cid:0)log (cid:0)π+/πref(y+x)(cid:1) log (cid:0)π/πref(yx)(cid:1)(cid:1) , and the loss function: ℓ = log σ(s), θℓ = β σ(s) (cid:105) (cid:104) θ log πθ(y+ x) θ log πθ(y x) , (3) (4) where σ(s) = (1 + es)1 is the logistic function. We also denote dθ := ln πθ(y+ x) ln πθ(y x) and g(θ) := E[ℓ(θ)] = β E[σ(s) dθ]. Finally, let : [0, 1] be the unknown gold reward, and J(θ) := Exµ EY πθ(x)[r(x, )] be the overall objective. Reward Margin Hypothesis. With probability of at least pimp, both the implicit reward margin and the gold reward margin have positive lower bounds. Specifically, there exists some τ (0, 1 2 ] and mr > 0 such that (cid:110) Eimp := σ(s) τ and r(x, y+) r(x, y) mr (cid:111) , P(Eimp) pimp > 0. (5) This hypothesis ensures that chosen responses are mostly ranked higher than rejected responses. Non-vanishing expected training signal. We first certify that DRIFT maintains uniform expected gradient magnitude under positive-mass quality event. Lemma 1 (Expected gradient lower bound under local quality). Let = {σ(s) τ } with P(E) p0 > 0 for some τ (0, 2 ]. If E(cid:2)dθ E(cid:3) cond > 0, then E(cid:2) θℓ (cid:3) β τ p0 cond. (6) Proof. From Eq. 4 and σ(s) 0, Eℓ = E(cid:2)β σ(s) dθ(cid:3) β τ E(cid:2)dθ1E (cid:3) = β τ P(E) E(cid:2)dθ E(cid:3). This bound shows that as long as non-negligible fraction of pairs satisfy σ(s) τ and have nonzero conditional gradient gap (cid:0)E[dθ σ(s) τ ] > 0(cid:1), the expected training signal stays away from zero. Expected improvement of actual utility. We now state general improvement guarantee: the expected DPO step increases the true utility J, with the gain quantified based on the key data condition. Theorem 1 (Expected improvement of J). Assume the improvement event Eq. 5 holds with probability at least pimp, and there exists λ > 0 such that (cid:104) (cid:10)J(θ), dθ (cid:11) (cid:12) (cid:12) (cid:12) Eimp (cid:105) λ. If is LJ -smooth in neighborhood of θ, then for any η > 0, E(cid:2)J(θ + η g(θ))(cid:3) J(θ) + η β τ pimp λ LJ 2 η2 E(cid:2)g(θ)2(cid:3). (7) (8) In particular, for sufficiently small η, the right-hand side exceeds J(θ) by linear-in-η margin βτ pimpλ up to O(η2). Proof sketch. g(θ) = β E[σ(s)dθ] gives EJ, = β E(cid:2)σ(s)J, dθ(cid:3) β τ P(Eimp) E(cid:2)J, dθ Eimp (cid:3) β τ pimp λ. LJ -smoothness yields J(θ + ηg) J(θ) + ηJ, LJ in Appendix B.2. 2 η2g2, then take expectations to obtain Eq.8. Full proof 8 Why DRIFT outperforms SPIN? When SPIN concentrates probability on finite SAT catalogue and reaches fixed point, the magnitude of the pairwise DPO signal on SPIN pairs is controlled by the catalogues logdensity-ratio variance: (cid:13) E(cid:2)β σ(s) dˆθ (cid:13) (cid:13) (cid:3)(cid:13) (cid:13) (cid:13) β 4 Edˆθ2, where h(y) := ln πˆθ(y x) ln πref (y x) (Proposition 1 in Appendix). Thus, small VarpSAT(h) implies weak training signal that can quantitatively degenerate. By contrast, DRIFT maintains non-vanishing signal and practical gains. Full discussions are in Appendix B.3. Var(s) = 2β2 VarY pSAT (9) (cid:112)Var(s) (cid:113) (cid:0)h(Y )(cid:1),"
        },
        {
            "title": "Summary",
            "content": "Signal: If improvement events occur with nonzero probability, DRIFTs expected gradient stays non-vanishing (Lemma 1). Utility: Under the local correlation, small step along the expected DPO direction improves up to O(η2) (Theorem 1). Contrast: At SPIN fixed points on finite SAT catalogue, the signal is controlled by catalogue logdensity-ratio variance and can degenerate (Proposition 1)."
        },
        {
            "title": "6 Conclusion",
            "content": "Real-world post-training rarely comes with abundant golden positives; it comes with abundant dissatisfaction and iterative user edits. In this paper, we introduced DRIFT, simple, scalable recipe that pairs authentic DSAT negatives with policy sampled positives, turning in-situ feedback into stable, exploration preserving updates. Empirically, on real-world user feedback dataset WildFeedback, DRIFT outperforms SPIN and IterDPO on WildBench and AlpacaEval2 (with the stronger margins at larger base models); on synthetic LLM-labeled dataset UltraFeedback, it retains its superiority. Exploratory capacity analysis indicates that DRIFT explores more diverse high-reward solutions rather than overfitting to narrow region. Theoretically, we show that DRIFTs admits uniform, non-vanishing gradient lower bound, avoiding the collapse that arises when training concentrates probability on finite fixed chosen (Or SFT) set as in SPIN. Together, these results suggest DRIFT is promising practical recipe for preference learning with real-world user feedback. Ethics Statement This work relies on publicly datasets WildFeedback that contain anonymized humanLLM conversations. No personally identifiable information was collected or used. All experiments comply with dataset licenses and terms of use. The research has no foreseeable negative social or ethical impacts. Reproducibility Statement We have made extensive efforts to ensure the reproducibility of our work. The main paper details our training setup, datasets, and evaluation metrics (Secs. 4.14.3). Dataset construction and filtering steps are described in Sec. 4.1, with references to the source corpora. Training hyperparameters, iteration procedures and training dynamics are presented in Appendix D. To further facilitate verification and reuse, we will open-source our code, including data-processing pipelines, training scripts and analysis upon paper acceptance."
        },
        {
            "title": "References",
            "content": "[1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [2] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [3] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [4] Tuomas Lounamaa. Explicit and implicit llm user feedback: quick guide, May 2024. Accessed: 2025-09-10. [5] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild, 2024. [6] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: largescale real-world llm conversation dataset, 2024. 9 [7] Shachar Don-Yehiya, Leshem Choshen, and Omri Abend. Naturally occurring feedback is common, extractable and useful, 2025. [8] Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Sihao Chen, Shan Xia, Hongfei Zhang, Jieyu Zhao, Xiaofeng Xu, Xia Song, and Jennifer Neville. Wildfeedback: Aligning llms with in-situ user interactions and feedback, 2025. [9] Ying-Chun Lin, Jennifer Neville, Jack Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, SidInterpretable user satisfaction estimation for conversational dharth Suri, Reid Andersen, Xiaofeng Xu, et al. systems with large language models. arXiv preprint arXiv:2403.12388, 2024. [10] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Selfrewarding language models. arXiv preprint arXiv:2401.10020, 3, 2024. [11] Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, and Shikun Zhang. Temporal self-rewarding language models: Decoupling chosen-rejected via past-future, 2025. [12] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models, 2024. [13] Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, and Dipendra Misra. Aligning llm agents by learning latent preference from user edits, 2024. [14] Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Cho, Michael S. Bernstein, and Diyi Yang. Aligning language models with demonstrated feedback, 2025. [15] Aaron David Tucker, Kiante Brantley, Adam Cahall, and Thorsten Joachims. Coactive learning for large language models using implicit user feedback. In Forty-first International Conference on Machine Learning, 2024. [16] Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! In Anna Korhonen, David Traum, and Lluıs M`arquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 36673684, Florence, Italy, July 2019. Association for Computational Linguistics. [17] Yuhan Liu, Michael J. Q. Zhang, and Eunsol Choi. User feedback in human-llm dialogues: lens to understand users but noisy as learning signal, 2025. [18] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 74217454. Association for Computational Linguistics, 2024. [19] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild, 2024. [20] Zhaoxuan Tan, Zheng Li, Tianyi Liu, Haodong Wang, Hyokun Yun, Ming Zeng, Pei Chen, Zhihan Zhang, Yifan Gao, Ruijie Wang, Priyanka Nigam, Bing Yin, and Meng Jiang. Aligning large language models with implicit preferences from user-generated content, 2025. [21] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint, 2024. [22] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss, 2024. [23] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024. [24] Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, and Min Lin. Bootstrapping language models with dpo implicit rewards. arXiv preprint arXiv:2406.09760, 2024. [25] Yongcheng Zeng, Xuanfa Jin, Guoqing Liu, Quan He, Dong Li, Jianye Hao, Haifeng Zhang, and Jun Wang. Aries: Stimulating self-refinement of large language models with and for iterative preference optimization. In Workshop on Reasoning and Planning for Large Language Models, 2025. [26] Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, and Dongbin Zhao. Enhancing llm reasoning with iterative dpo: comprehensive empirical investigation, 2025. [27] Kaiyuan Chen, Jin Wang, and Xuejie Zhang. Learning to reason via self-iterative process feedback for small language models, 2024. [28] Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. Cream: Consistency regularized self-rewarding language models, 2025."
        },
        {
            "title": "A Disclosure of LLM Use in Paper Preparation",
            "content": "We acknowledge the use of LLMs for assistance with writing and polishing text. All the content suggested by LLMs in writing was proofread and manually adjusted before being integrated into the final manuscript. The authors take full responsibility for the accuracy and factuality of all content presented."
        },
        {
            "title": "B Theoretical Proofs",
            "content": "B.1 Assumptions (A1) Improvement event (data-level). There exist τ (0, 1 2 ], mr > 0, and pimp > 0 such that, with Eimp = (cid:8) σ(s) τ and r(x, y+) r(x, y) mr (cid:9), one has P(Eimp) pimp. (A2) Local smoothness of J. There exists LJ < such that, for all sufficiently small v, (A3) Finite second moment for the score difference. dθ2 Cd < . J(θ + v) J(θ) + J(θ), LJ 2 v2. B.2 Proof of Theorem 1 (Expected improvement of J) Theorem (Restatement of Theorem 1). Under Assumptions (A1), (A2), (A3), and the local advantage correlation condition Eq.7, for any η > 0 and g(θ) = β E[σ(s) dθ], E(cid:2)J(θ + η g(θ))(cid:3) J(θ) + η β τ pimp λ LJ 2 η2 β2 Cd. Proof. By definition, g(θ) = β E(cid:2)σ(s) dθ (cid:3). Taking inner product with J(θ) and then expectation, (cid:104) J(θ), g(θ) = β σ(s) J(θ), dθ (cid:105) . On the improvement event from Assumption (A1), σ(s) τ ; conditioning on Eimp and using Eq.7, J(θ), g(θ) β τ P(Eimp) E(cid:2)J(θ), dθ (cid:12) (cid:12) Eimp (cid:3) β τ pimp λ. By the LJ -smoothness in Assumption (A2), Taking expectations and combining the previous bound gives J(θ + ηg) J(θ) + η J(θ), LJ 2 η2 g2. J(θ + ηg) J(θ) + η β τ pimp λ LJ 2 η2 g2. It remains to bound g(θ)2. Since σ (0, 1), g(θ) = (cid:13) (cid:13)β E[σ(s) dθ](cid:13) (cid:13) β Edθ β (cid:112)Edθ2 β (cid:112) Cd, where we used Assumption (A3) and Jensen. Hence g(θ)2 β2Cd, which yields the stated inequality. B.3 SPIN real-world performance discussion SPIN updates concentrate probability on the finite SAT catalogue SAT(x) and, under trust-region style update, admit the closed-form iteration (see, e.g., [12]): pθt+1(y x) pθt(y x) (cid:104) pSAT(y x) pθt(y x) (cid:105)1/λ , (5.3) which drives pθt( x) toward pSAT( x) supported on SAT(x). Consequently, at any fixed point ˆθ one has πˆθ( x) = pSAT( x); under the SPIN data rule (positive sampled from pSAT, negative sampled independently from πˆθ given x), the positive y+ and negative are conditionally i.i.d. on SAT(x) with the common marginal pSAT. The DPO signal then depends on the variance of the log-density ratio on that finite set and can quantitatively degenerate. 12 Proposition 1 (Quantitative degeneration at SPIN fixed point). At SPIN fixed point ˆθ with πˆθ(x) = pSAT(x), if y+, are conditionally i.i.d. from πˆθ(x) and Edˆθ2 < , then for h(y) := ln πˆθ(y x) ln πref (y x), (cid:13) E(cid:2)β σ(s) dˆθ (cid:13) (cid:13) (cid:3)(cid:13) (cid:13) (cid:13) β 4 (cid:112)Var(s) (cid:113) Edˆθ2, Var(s) = 2β2 VarY pSAT (cid:0)h(Y )(cid:1). (10) Proof. By (A5), for each we have E[dˆθ x] = 0 since y+, are i.i.d. under πˆθ(x). Therefore E(cid:2)β σ(s) dˆθ (cid:3) = β (cid:104)(cid:0)σ(s) Eσ(s)(cid:1) dˆθ (cid:105) . Fix any unit vector u. Scalar CauchySchwarz yields uE(cid:2)β σ(s) dˆθ (cid:3) = β (cid:104)(cid:0)σ(s) Eσ(s)(cid:1) udˆθ (cid:105) β (cid:112)Var(σ(s)) (cid:113) E(cid:2)(udˆθ)2(cid:3). Taking the supremum over all unit u, (cid:13) E(cid:2)β σ(s) dˆθ (cid:13) (cid:13) (cid:3)(cid:13) (cid:13) β (cid:112)Var(σ(s)) (cid:13) (cid:113) Edˆθ2. Since σ is 1/4-Lipschitz, Var(σ(s)) 1 16 Var(s). With = β[h(y+) h(y)] and y+, i.i.d., Var(s) = β2 Var(cid:0)h(y+) h(y)(cid:1) = 2 β2 VarY pSAT (cid:0)h(Y )(cid:1), which gives the stated inequality."
        },
        {
            "title": "C Detailed Results",
            "content": "Table 4: Complete WildBench leaderboard showing all evaluated models with comprehensive task-specific scores across multiple evaluation dimensions. Our models are highlighted in gray. Model Elo Task Creative Reasoning Math Info Seek Coding Length 1256.86 GPT-4o (2024-05-13) 1238.93 Claude-3.5-Sonnet (20240620) 1233.99 GPT-4-Turbo (2024-04-09) 1228.55 Gemini-1.5-Pro 1221.66 DeepSeek-v2-Chat (0628) 1221.30 GPT-4 (0125-preview) 1219.27 Claude-3-Opus (20240229) 1218.75 Qwen2.5-14B-UltraFeedback-DRIFT-iter2 1217.99 Mistral-Large-2 1217.61 Qwen2.5-14B-WildFeedback-DRIFT-iter2 1217.35 GPT-4o-mini (2024-07-18) 1215.73 Qwen2.5-14B-WildFeedback-DRIFT-iter1-4k 1215.67 Qwen2.5-14B-UltraFeedback-DRIFT-iter1 1215.12 Qwen2.5-14B-UltraFeedback-IterDPO-iter2 1214.14 Qwen2.5-14B-UltraFeedback-IterDPO-iter1 1214.03 Qwen2.5-14B-WildFeedback-DRIFT-iter2-4k 1213.50 Qwen2.5-14B-WildFeedback-Seed 1213.17 Qwen2.5-14B-Instruct 1212.83 Qwen2.5-14B-WildFeedback-DRIFT-iter1 1211.69 Qwen2.5-14B-WildFeedback-IterDPO-iter2-4k 1206.89 DeepSeek-v2-Coder (0628) 1206.77 Gemini-1.5-Flash 1206.65 Qwen2.5-14B-WildFeedback-IterDPO-iter1-4k 1206.63 Qwen2.5-14B-WildFeedback-IterDPO-iter2 1205.48 Qwen2.5-14B-WildFeedback-IterDPO-iter1 1205.02 DeepSeek-v2-Chat 1200.63 Qwen2.5-14B-WildFeedback-SPIN-iter1 1199.09 Qwen2.5-7B-WildFeedback-DRIFT-iter2 1197.94 Qwen2.5-7B-UltraFeedback-DRIFT-iter2 1197.13 Qwen2.5-7B-WildFeedback-DRIFT-4k-iter1 1197.04 Qwen2.5-7B-UltraFeedback-DRIFT-iter1 1195.33 Qwen2.5-7B-WildFeedback-DRIFT-4k-iter2 1194.81 Qwen2.5-7B-WildFeedback-DRIFT-iter1 1194.67 Qwen2.5-7B-Instruct 1194.45 Qwen2.5-7B-UltraFeedback-IterDPO-iter1 Qwen2.5-7B-WildFeedback-DRIFT-iter2-RPO 1194.24 1193.66 Qwen2.5-7B-WildFeedback-Seed 1193.60 Nemotron-4-340B-Instruct 1192.56 Qwen2.5-14B-WildFeedback-SPIN-iter2 1192.46 Qwen2.5-7B-WildFeedback-IterDPO-iter2-4k 1192.03 Claude-3-Sonnet (20240229) 1192.01 Qwen2.5-7B-UltraFeedback-IterDPO-iter2 1189.43 Qwen2-72B-Instruct 1189.43 Qwen2.5-7B-WildFeedback-IterDPO-iter1-4k 1187.35 Mistral-Nemo-Instruct (2407) 1185.11 Qwen2.5-7B-WildFeedback-IterDPO-iter1 1182.33 Qwen2.5-7B-WildFeedback-IterDPO-iter2 1180.75 Qwen2.5-7B-WildFeedback-SPIN-iter1 1178.88 Qwen2.5-14B-UltraFeedback-SPIN-iter1 1175.97 Claude-3-Haiku (20240307) 1173.45 Qwen2.5-7B-WildFeedback-SPIN-iter2 1172.18 Mistral-Large (2402) 1163.16 Qwen2.5-7B-UltraFeedback-SPIN-iter1 1160.02 Qwen1.5-72B-Chat-Greedy 1155.07 Qwen2.5-14B-UltraFeedback-SPIN-iter2 1145.51 Mixtral-8x7B-Instruct-v0.1 1139.66 Qwen2.5-7B-UltraFeedback-SPIN-iter2 1135.69 GPT-3.5-Turbo (0125) 59.30 54.70 55.22 52.95 53.99 52.28 51.71 59.27 55.57 58.30 57.14 58.37 58.52 54.78 54.21 57.59 54.93 55.08 57.27 56.63 45.66 48.85 51.79 51.38 52.34 48.21 47.16 51.69 50.32 51.06 50.91 51.06 50.61 48.66 48.77 50.42 49.11 47.67 44.04 48.94 45.48 48.49 44.50 47.07 44.38 46.31 46.17 42.86 36.99 38.89 37.86 38.89 35.10 39.93 28.01 31.47 25.93 30.02 59.12 55.61 58.66 55.12 56.43 57.57 53.02 59.48 58.86 58.14 60.05 58.55 58.65 55.35 55.13 57.98 54.94 55.71 58.09 56.18 40.78 51.66 50.75 50.23 52.87 53.59 49.90 52.45 50.39 53.23 50.08 51.42 52.09 50.08 49.35 51.89 49.35 53.32 45.54 50.39 46.30 50.18 49.92 48.63 54.57 46.77 45.74 43.26 38.86 42.95 37.36 49.66 34.78 50.36 29.77 42.75 25.79 37.42 60.21 55.64 56.20 53.73 54.83 53.45 52.53 61.05 57.22 60.60 58.24 60.39 60.30 56.76 56.27 59.73 57.30 57.84 59.31 59.19 47.17 50.79 54.49 54.17 54.97 50.63 49.75 53.21 52.50 53.44 53.77 53.45 53.03 51.80 51.39 53.31 51.57 49.13 45.95 51.35 47.43 50.73 46.85 49.13 47.41 48.61 49.22 45.45 40.09 41.29 40.72 41.80 37.61 43.45 31.69 34.59 28.88 33.39 57.29 50.16 51.00 48.59 51.43 45.79 46.75 59.44 52.67 59.37 54.05 58.89 58.02 54.00 52.02 57.45 53.60 54.98 55.78 56.51 46.43 45.32 51.24 51.03 51.71 44.52 47.25 50.63 48.41 48.32 48.92 49.16 48.56 47.09 46.03 48.88 47.54 40.80 40.64 46.37 40.64 44.68 40.95 44.50 35.63 44.46 45.60 41.59 35.16 31.43 37.06 30.88 30.87 29.80 25.08 22.14 19.60 21.59 58.61 55.54 57.18 52.23 52.72 54.36 53.47 58.66 57.38 58.37 57.43 58.86 57.57 55.00 55.74 57.62 55.45 54.95 57.22 56.78 40.05 48.67 52.67 52.48 54.06 51.81 47.67 52.38 52.08 52.28 52.72 52.13 52.13 50.69 50.89 51.49 50.15 53.00 45.89 50.99 47.13 50.22 49.50 49.26 51.93 48.02 46.24 46.29 39.01 45.35 42.13 46.14 39.90 48.22 32.72 41.94 32.03 36. 60.47 56.51 55.07 55.22 55.00 52.92 53.30 57.64 53.84 55.19 57.17 55.57 57.64 53.08 52.64 55.38 53.30 52.23 56.13 54.25 48.87 48.73 49.43 48.68 48.96 44.43 43.11 50.28 48.58 49.29 48.87 49.38 48.34 45.00 46.82 47.52 47.17 46.26 43.13 46.79 46.10 47.58 39.81 45.12 39.72 44.27 43.70 39.06 33.40 36.98 33.27 33.74 33.21 35.36 23.13 25.02 24.43 26.54 3723.52 2911.85 3093.17 3247.97 3252.38 3335.64 2685.98 4275.27 3503.63 4492.77 3648.13 4528.15 4288.24 3152.61 3424.60 5333.00 3878.89 3682.95 4485.35 4491.05 2580.18 3654.40 3925.74 4011.82 4054.28 2896.97 2707.18 4707.48 5121.20 4517.02 4856.83 4686.39 4652.38 4275.08 3888.79 4711.01 4624.30 2754.01 2820.38 4731.32 2670.24 3708.61 2856.45 4602.08 3318.21 4662.34 4520.58 3611.19 2642.64 2601.03 2839.09 2514.98 3475.16 2392.36 2504.57 2653.58 3293.30 1844."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Warm Start Training Details We curate DSATSAT seed set (491 pairs) from WildFeedback, where dissatisfied user turn (DSAT) is followed by revised model response that satisfies the user (SAT). Each pair provides natural preference: the DSAT response fails to meet expectations, while the subsequent SAT response is preferred. For our warm start phase, we initialize training using pre-trained instruction-tuned models as the base models. The warm start training utilizes seed preference data to establish initial alignment before iterative refinement. We did DPO training with carefully tuned hyperparameters to ensure stable convergence. All experiments were conducted on 8 H100 GPUs with the same hardware configuration maintained across all training phases. The detailed hyperparameters for warm start training are presented in Table 5. Table 5: Warm Start Training Hyperparameters Learning rate Batch size β Optimizer LR scheduler Seq length Epochs Precision 5.0e-7 4 0.1 RMSprop Linear 2048 3 bfloat16 D.2 Iterative Training Details After the warm start phase, we conducted iterative training to progressively refine model alignment using dynamically generated preference data. Data generation details are in Sec. 4.1. Each iteration builds upon the previous model checkpoint, incorporating newly created preference data. The iterative training process maintains consistent hyperparameters across iterations, with only the training data and base model checkpoint changing between iterations. We trained each iteration for single epoch to prevent overfitting on the iteratively generated data. Table 6 details the hyperparameters used for iterative training phases. Table 6: Iterative Training Hyperparameters Learning rate Batch size β Optimizer LR scheduler Seq length Epochs Precision 5.0e-7 4 0.1 RMSprop Linear 1 bfloat16 D.3 Training Dynamics For better training illustration, we report the Qwen2.5-14B-Instruct DRIFT iter1 & iter2 training dynamics in Figure 4 which shows dpo training loss, chosen reward, and rejected reward. The loss curves exhibit stable convergence across both iterations. The reward signals show the expected separation pattern: chosen rewards consistently increase while rejected rewards decrease. This trend is observed in both iterations, confirming the effectiveness of the training. 15 Figure 4: The top row shows DRIFT training dynamics for iteration 1 on Qwen2.5-14B-Instruct. The bottom row shows the training dynamics for iteration 2."
        },
        {
            "title": "E Model Responses Example",
            "content": "Prompt: Using at least two different scientific sources, generate research paper that analyzes and compares the form and function of three different types of carnivorous plants, including but not limited to Venus flytraps, pitcher plants, and sundews. The paper should include detailed descriptions of the physical characteristics of each plant, as well as an analysis of their unique adaptations for capturing and digesting prey. In addition, the paper should explore the evolutionary history and ecological significance of carnivorous plants, and discuss current research on their use in medicine and biotechnology. The paper should be written in APA format and include at least 10 peer-reviewed references. SPIN Representative Response: Title: Carnivorous Plants: Form, Function, Evolutionary History, and Applications Introduction: Carnivorous plants are fascinating organisms that have evolved unique adaptations to capture and digest prey. These plants are found in nutrient-poor environments where soil lacks sufficient nitrogen and phosphorus. The three most common types of carnivorous plants are Venus flytraps (Dionaea muscipula), pitcher plants (Nepenthes spp.), and sundews (Drosera spp.). This research paper will analyze and compare the form and function of these three types of carnivorous plants, as well as their evolutionary history and ecological significance. Physical Characteristics and Adaptations: Venus flytraps are known for their trap-like leaves that can close rapidly when triggered by insects. Each leaf has six trigger hairs that, when touched twice within 20 seconds, cause the trap to close. Once closed, the trap secretes digestive enzymes to break down the insect. Venus flytraps are native to North Carolina and South Carolina and require specific environmental conditions, such as high humidity and acidic soil, to thrive. Pitcher plants are named for their modified leaves that resemble pitchers or cups. These leaves are covered in nectar and have slippery surface that makes it difficult for insects to escape once they enter. Once inside, the insect is digested by enzymes secreted by the plant. Pitcher plants are found throughout the world, from tropical rainforests to temperate regions, and come in various shapes and sizes. Nepenthes rafflesiana, also known as the Rafflesia pitcher plant, is one of the largest pitcher plants, with pitchers up to 30 centimeters tall. Sundews are named for their sticky glandular hairs that resemble dewdrops. When an insect lands on the plant, the sticky hairs trap it, and the plant secretes digestive enzymes to break down the insect. Sundews are found in many parts of the world, including North America, Europe, and Asia. Drosera capensis, also known as the Cape sundew, is popular species used in horticulture due to its attractive appearance and ease of care. Evolutionary History and Ecological Significance: The evolutionary history of carnivorous plants is still not fully understood, but it is believed that they evolved independently multiple times across different plant lineages. Carnivorous plants likely evolved in nutrient-poor environments as way to supplement their nutrition through the capture and digestion of insects. ... GPT-5 Score: 5/10 Explanation: Covers the basic features of the plants but lacks depth; references are not standardized (limited to general descriptions rather than APA format); missing an abstract and overall not well structured as research paper. Figure 5: SPIN model response example. 17 Prompt: Using at least two different scientific sources, generate research paper that analyzes and compares the form and function of three different types of carnivorous plants, including but not limited to Venus flytraps, pitcher plants, and sundews. The paper should include detailed descriptions of the physical characteristics of each plant, as well as an analysis of their unique adaptations for capturing and digesting prey. In addition, the paper should explore the evolutionary history and ecological significance of carnivorous plants, and discuss current research on their use in medicine and biotechnology. The paper should be written in APA format and include at least 10 peer-reviewed references. IterDPO Representative Response: Title: Carnivorous Plants: Form, Function, and Evolutionary Significance Abstract: Carnivorous plants are fascinating organisms that have evolved unique mechanisms to capture and digest prey. This paper compares the form and function of three different types of carnivorous plants: Venus flytraps (Dionaea muscipula), pitcher plants (Nepenthes spp.), and sundews (Drosera spp.). We also discuss the evolutionary history and ecological significance of these plants, as well as their potential applications in medicine and biotechnology. Introduction: Carnivorous plants are found in nutrient-poor environments where other sources of nitrogen and phosphorus are scarce. They have evolved specialized trapping mechanisms to obtain these nutrients from insects and small animals. Venus flytraps, pitcher plants, and sundews are three examples of carnivorous plants with distinct trapping strategies. Venus Flytraps: Venus flytraps are native to North Carolina and South Carolina in the United States. Their leaves contain trigger hairs that, when touched twice, cause the leaf to snap shut, trapping the insect inside (Gurevitch et al., 2017). Once trapped, the insect is digested by enzymes secreted by the plant. Venus flytraps have evolved this mechanism to supplement their nutrient intake in nutrient-poor soil (Hunt & Raker, 2018). Pitcher Plants: Pitcher plants are found in tropical and subtropical regions worldwide. They have evolved modified leaf that forms deep, liquid-filled pitfall trap (Kress & Wurdack, 2006). Insects are attracted to the nectar produced by the pitcher and fall into the liquid, where they are digested by enzymes and bacteria. Pitcher plants have evolved this mechanism to obtain nutrients from wide range of prey, including ants, flies, and spiders (Rice, 2014). Sundews: Sundews are found in temperate and tropical regions worldwide. They have evolved sticky glands on their leaves that trap insects (Orozco-terWengel & Brodersen, 2015). Once trapped, the insect is digested by enzymes secreted by the plant. Sundews have evolved this mechanism to obtain nutrients from wide range of prey, including flies, mosquitoes, and ants (Rice, 2014). ... GPT-5 Score: 6/10 Explanation: Presents clearer structure with some academic tone, but the discussion is still surface-level and misses key elements like applications and conclusion. Figure 6: IterDPO model response example. 18 Prompt: Using at least two different scientific sources, generate research paper that analyzes and compares the form and function of three different types of carnivorous plants, including but not limited to Venus flytraps, pitcher plants, and sundews. The paper should include detailed descriptions of the physical characteristics of each plant, as well as an analysis of their unique adaptations for capturing and digesting prey. In addition, the paper should explore the evolutionary history and ecological significance of carnivorous plants, and discuss current research on their use in medicine and biotechnology. The paper should be written in APA format and include at least 10 peer-reviewed references. DRIFT Representative Response: **Title: Comparative Analysis of Carnivorous Plant Adaptations: Venus Flytraps, Pitcher Plants, and Sundews** **Abstract** Carnivorous plants have evolved remarkable adaptations to survive in nutrient-poor environments by capturing and digesting insects. This paper compares the form and function of three prominent carnivorous plants: *Dionaea muscipula* (Venus flytrap), *Nepenthes spp.* (pitcher plants), and *Drosera spp.* (sundews). We analyze their physical characteristics, trapping mechanisms, and digestive processes. Additionally, we discuss their evolutionary history, ecological roles, and potential applications in medicine and biotechnology. Our review synthesizes data from multiple studies to provide comprehensive understanding of these fascinating organisms. **Introduction** Carnivorous plants are specialized angiosperms that have developed unique strategies to obtain nutrients from animal prey (Lloyd, 1983). They thrive in nutrient-poor soils, where traditional photosynthetic pathways alone cannot sustain growth. Three notable examples of carnivorous plants are the Venus flytrap (*Dionaea muscipula*), pitcher plants (*Nepenthes spp.*), and sundews (*Drosera spp.*). Each species has distinct morphological features and physiological mechanisms adapted for prey capture and digestion. **Venus Flytrap (*Dionaea muscipula*)** The Venus flytrap is characterized by its hinged leaves with sensitive trigger hairs. When an insect touches these hairs, the leaf rapidly closes within seconds (Braun & Speck, 2006). The internal structure includes complex system of mechanical sensors and hydraulic pressures that facilitate rapid closure (Lischka et al., 2014). **Pitcher Plants (*Nepenthes spp.*)** Pitcher plants possess modified leaves forming deep, fluid-filled trap. Insects are attracted to the nectar produced at the pitchers entrance and fall into the digestive fluid below (Givnish, 2000). The pitchers contain enzymes and acids that break down the prey (Bramwell & Chaloner, 1975). **Sundews (*Drosera spp.*)** Sundews are covered in sticky glandular hairs that ensnare insects. Once an insect becomes stuck, the plant secretes digestive enzymes to dissolve the prey (OLeary et al., 2010). ... GPT-5 Score: 8/10 Explanation: The format is closer to an academic paper (title, abstract, introduction, subsections) and using markdown format; covers morphology, function, evolution, ecology, and applications; relatively comprehensive and consistent with review style. Figure 7: DRIFT model response example."
        }
    ],
    "affiliations": [
        "College of Computer Science, Nankai University",
        "Department of Computer Science and Engineering, University of Notre Dame",
        "Department of Computer Science, Purdue University",
        "Department of Computer Science, Washington University in St. Louis"
    ]
}