{
    "paper_title": "Counterfactual Generation from Language Models",
    "authors": [
        "Shauli Ravfogel",
        "Anej Svete",
        "Vésteinn Snæbjarnarson",
        "Ryan Cotterell"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects."
        },
        {
            "title": "Start",
            "content": "Shauli Ravfogel1 Anej Svete1 Vésteinn Snæbjarnarson1,2 Ryan Cotterell1 1ETH Zurich shauli.ravfogel@gmail.com vsnaebjarnar@ethz.ch {asvete, ryan.cotterell}@inf.ethz.ch 2University of Copenhagen 4 2 0 2 1 1 ] . [ 1 0 8 1 7 0 . 1 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgerye.g., model ablations or manipulation of linear subspaces tied to specific conceptsto intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactualse.g., how given sentence would have appeared had it been generated by the model following specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearls causal hierarchy. Based on this observation, we propose framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects. https://github.com/shauli-ravfogel/lm-counterfactuals"
        },
        {
            "title": "INTRODUCTION",
            "content": "The study of language model (LM) interpretability often borrows terminology from Pearls causal calculus (Pearl, 1989), e.g., researchers often talk of intervening models parameters and counterfactually generating strings. Pearls framework distinguishes between three levels of causal reasoning (Shpitser & Pearl, 2008). Association, the first level, pertains to statistical correlations, i.e., observing patterns observed in data without interacting with the world. Intervention, the second level, pertains to actively changing variables in the world and observing their effects at macro level. Counterfactuality, the third level, pertains to imagining what could have happened if past events had unfolded differently. However, LM literature often uses these three causal terms causally and at times impreciselyparticularly when it comes to counterfactuality, which remains challenging to rigorously define (Feder et al., 2022; Mueller, 2024; Mueller et al., 2024). In this paper, we are given well-defined notion of counterfactuality in LMs using the framework of structural equation modeling. Efforts to exert control over LMs have led to substantial research on targeted interventions in the models. One such technique is representation surgery, which involves modifying an LMs architecture to manipulate its internal representation space (Lakretz et al., 2019; Vig et al., 2020; Feder et al., 2021; Ravfogel et al., 2021b; Elhage et al., 2021; Elazar et al., 2021; Nanda, 2023; Syed et al., 2023; Kramár et al., 2024; Avitan et al., 2024). The linear subspace hypothesis (Bolukbasi et al., 2016; Vargas & Cotterell, 2020; Ravfogel et al., 2022) posits that human-interpretable concepts, such as gender or grammatical number, are encoded within specific linear subspaces of the LMs representation space. This makes it possible to perform precise interventions on these high-level concepts, such as removing the concepts information by projecting the representations onto the Equal contribution."
        },
        {
            "title": "A preprint",
            "content": "complement of the concept subspace (Ravfogel et al., 2020; 2021a; 2022; 2023; Guerner et al., 2024; Scalena et al., 2024; Singh et al., 2024). These interventions modify the model and allow researchers to examine its behavior after the change. However, while interventions can induce change in the model, they cannot answer counterfactual questions, e.g., what would given string look like if it had been generated by the model after the intervention? Counterfactual analysis, as defined by Pearl, is challenging because it requires describing the system of interest through causal model that enables counterfactual reasoning. In this paper, we address this challenge for LMs by turning to generalized structural-equation models (GSEMs; Halpern & Peters, 2022). GSEMs break down the statistical model into exogenous variables, which account for latent randomness, and endogenous variables, which are deterministic once the exogenous ones are fixed. To frame LMs as GSEMs, we apply the Gumbel-max trick (Gumbel, 1954), which separates the deterministic computation of next-symbol logits from the sampling process.1 This formulation allows us to apply causal reasoning tools to LMs. Since generating true counterfactuals requires knowing the unobserved noise values, we develop an algorithm to infer the distribution of noise conditioned on the observed string. This enables us to sample from the conditional noise distribution and generate true counterfactuals for observed strings, allowing us to precisely study string-level effects of model interventions. We apply our framework to generate counterfactual strings from models subjected to interventions. The models we consider are GPT2-XL (Radford et al., 2018) and LLaMA3-8b (Touvron et al., 2023), and the interventions include linear steering (Li et al., 2024; Singh et al., 2024), knowledge editing (Meng et al., 2023), and instruction tuning (Wei et al., 2022). While these interventions aim to precisely target specific behaviorse.g., only influencing the models knowledge of given fact, in case of knowledge editingour results show that they often induce unintended side effects. For example, counterfactual strings reveal that gender-based interventions can unexpectedly influence text completions unrelated to gender. This finding challenges the notion of achieving minimal change with targeted interventions and demonstrates that even interventions modifying small subset of parameters can fail to obtain the desired targeted effect."
        },
        {
            "title": "2 LANGUAGE MODELS AS GENERALIZED STRUCTURAL-EQUATION MODELS",
            "content": "Let Σ be an alphabeta finite, non-empty set of symbols. language model (LM) is probability distribution over Σ, the set of all strings formed from symbols in Σ. language encoder is function hθ : Σ Rd parameterized by parameters Θ that maps strings to d-dimensional vectors (Chan et al., 2024). Representational surgery is performed by intervening on hθ. Popular architectures for implementing language encoders include Transformers (Vaswani et al., 2017) and recurrent neural networks (RNNs; Elman, 1990). Language encoders are particularly valuable because under mild conditions (Du et al., 2023, Thm. 4.7), they ensure the model defines distribution over stringsthus forming an LMas follows: p(w) = p(w1 wT ) (cid:89) = p(EOS w) t=1 p(wt w<t) = softmax(E hθ(w) + b)EOS (cid:89) t=1 softmax(E hθ(w<t) + b)wt. Here, RΣd and RΣ. We assume that EOS Σ and define Σ def= Σ {EOS}. (1a) (1b) (1c) 2.1 STRUCTURAL EQUATION MODELING We begin by briefly reviewing structural equation modeling, which provides framework for discussing causal manipulations of the generation process and allows us to precisely define the intuitive notion of counterfactual. 1This approach has been explored in reinforcement learning (Oberst & Sontag, 2019), but to our knowledge, not in language modeling. As we show, the infinite outcome space in LMs requires special handling not needed in finite domains."
        },
        {
            "title": "A preprint",
            "content": "W<t Π"
        },
        {
            "title": "W t",
            "content": "W 1 1 2 2 3 (cid:102)W 1 (cid:102)W 2 (cid:102)W 3 (a) The SEM corresponding to an autoregressive LM when sampling the next symbol. ... ... (b) The GSEM corresponding to an LM. ... Figure 1: Examples of (G)SEMs. See Pearl (2009); Pearl et al. (2016); Peters et al. (2017); Peters & Halpern (2021); Halpern & Peters (2022) for more in-depth treatment. Definition 2.1 (Structural Equation Model (SEM)). structural equation model (SEM) is tuple = (S, F), where is signature and is set of structural equations. signature is tuple = (U, V, R, I), where is finite set of exogenous random variables {U 1, . . . , }, is finite set of endogenous random variables {V 1, . . . , }, assigns each variable its range, i.e., the set of values it can take, and is set of interventions. An intervention is set of pairs with and (V ), where any intervention contains at most one pair for any V. The set of structural equations = {F 1 , . . . , } contains for each endogenous variable function : (U {V }) (V ), i.e., assigns deterministic value based on the values of all the other variables in the SEM: = (U, {V }) for all V. The range assignment and set of interventions generalize naturally to more variables; we use (X) for to refer to the Cartesian product of the individual ranges while refers to an intervention assigning values to all V. By assigning joint probability distribution (U) to the exogenous variables, an SEM induces probability distribution over UV, which we denote by PE . We call any assignment of the exogenous variables the context of the SEM. As suggested by the notation, an intervention = converts SEM into another SEM in which the structural equation is replaced by the assignment x; this SEM is denoted by Xx. Note that the intervention can also be empty, in which case Xx is the original SEM. Given context, the outcome of an SEM under the intervention are all the assignments of that satisfy the structural equations of the SEM Xx. In an SEM with no cyclical dependencies between the variables, the outcome can be determined by solving the equations in sequence consistent with the variable dependencies. Interventions correspond to the second level of Pearls hierarchy. They allow us to manipulate the causal generation structure and thus generate new outcomes from precisely modified SEM. Interventions, however, do not manipulate individual outcomesthey only allow us to sample (unrelated) new observations. The third level of the causal hierarchy concerns itself with retrospective modifications of the SEM, defining precisely what it means to investigate what would have happened at the time of sampling had the SEM been different, i.e., had an intervention been performed. This is formalized with counterfactual distributions. Definition 2.2 (Counterfactual Distribution). Given an SEM = (S, F) and an outcome u, the counterfactual distribution under the intervention = is the distribution defined by the intervenedon SEM Xx whose exogenous variables follow the posterior distribution (U = u)."
        },
        {
            "title": "A preprint",
            "content": "SEMs can be represented as causal graphical models, where the edges signify not just conditional dependencies but also causal influences. Consider the (simplified) causal graphical model in Fig. 1a, which illustrates an abstraction of the process of next-symbol generation. In this model, the next symbol wt, represented by the random variable t, is sampled based on the preceding string w<t, represented by the random variable <t. The edges indicate the (complex) causal relationships that generate wt. Specifically, the edge between <t and Πtrepresenting the deterministic computations of language encoderreflects the causal role of the language encoder. Together, the string representation and sampling noise determine the next symbol wt.2 Interventions on this model correspond to modifying the relationship between <t and Πt by altering the language encoder that implements this transformation. Structural causal models are defined over finitely many random variables = {U 1, . . . , } {V 1, . . . , }. Generalized SEMs (Peters & Halpern, 2021; Halpern & Peters, 2022) allow us to model infinitely many variables. Definition 2.3 (Generalized Structural Equation Model (GSEM)). generalized structural-equation model (GSEM) is tuple = (S, F), where = (U, V, R, I) is signature with analogous definitions as in Def. 2.1 without the requirement that U, V, and (U V) be finite. : (U) 2R(V) is function that maps an intervention and context (U) directly to the possible set of outcomes (u, x) (V). Thus, GSEMs generalize SEMs in two ways: They allow for causal models with infinitely many variables and they allow for specifying the effects of interventions directly, rather than having to conform to pre-defined set of structural equations as in an SEM. 2.2 LANGUAGE PROCESSES AND GENERALIZED STRUCTURAL CAUSAL MODELS We next show how LMs can be framed as GSEMs. We begin by defining the Gumbel distribution. Definition 2.4 (Gumbel distribution). The cumulative distribution function of the standard Gumbel distribution Gumbel(0, 1) is (x) = exp ( exp (x)) and its density function is (x) = exp ( (x + exp (x))) . The Gumbel distribution is useful to model the distribution of the maximum (or minimum) of set of samples from various distributions. This is the core idea behind the Gumbel-max trick, which shows the utility of Gumbel-distributed random variables for sampling from the categorical distribution (Luce, 1959; Yellott, 1977; Maddison et al., 2017; Hazan & Jaakkola, 2012; Maddison et al., 2014; Hazan et al., 2016). We restate the trick below for the specific case of the softmax; see App. for the proof. Theorem 2.1 (The Gumbel-max Trick). Let be categorical random variable over categories such that (X = m) = exp (πm) m=1 exp (πm) for {1, . . . , } and given vector of logits π RM . The Gumbel-max trick states that sampling from can be performed as follows: (i) draw outcomes y1, . . . , yM from standard Gumbel distribution Gumbel(0, 1) and (ii) set the outcome of as = softmax (π)m , (cid:80)M (2) = argmax m{1,...,M } πm + ym. (3) As we make formal below, sampling from an encoder-based LM can be formulated with the Gumbelmax trick, since the (affinely transformed) representations hθ (w) provide the logits πm in Eq. (3). Let Σ be an alphabet. language process = {W t} t=1 is an infinite sequence of (correlated) Σ-valued random variables, where we think of as the random variable whose outcome generates the tth symbol of string (Du et al., 2024).3 Let = {U t} t=1 be an infinite sequence of random Σ-dimensional vectors indexed by Σ where t(w) Gumbel(0, 1). As explicated by Eq. (1c), 2This is made more precise in 2.2. 3A typical formulation of language process implies that, if = EOS, we have = EOS for all > t."
        },
        {
            "title": "A preprint",
            "content": "encoder-based LMs sample string by sampling from countably-infinitely many Σ-valued random variables based on the logits π = hθ (w<t). We can therefore view the LM induced by language encoder hθ as function that maps the sampling noiseto language process as follows: (4) (E hθ (w<t) + b)w + (w) . = argmax wΣ def= w1 wt1 and 1 = w1, . . . , t1 = wt1. This is graphically depicted in where w<t Fig. 1b.4,5 crucial implication of Eq. (4) is that the language process is deterministic given all the noise in the string generation process comes from the noise variables . Such decomposition of the generative mechanism into deterministic relationships and independent noise variables closely resembles the structural equations of an SEM. Due to the infinitely many variables, it can, however, only be represented by GSEM, albeit very structured oneone in which the variable assignments are still determined through (infinitely many) structural equations. This is formalized in the following proposition. Proposition 2.1 (LMs as GSEMs). Let hθ : Σ Rd be language encoder and the LM induced by Eq. (1c). Then, there exists GSEM with the joint probability distribution PE such that (cid:0)W 1 = w1, . . . , = wT , +1 = EOS, +2 = EOS, . . .(cid:1) . (w1 wT ) = PE (5) Proof. See App. B. Intuitively, the formulation of LMs as GSEMs defines the set of endogenous variables as = t=1 Θ Θ, where Θ are the language encoders original parameters (e.g., the parameters of {W t} the neural network), and Θ is set of auxiliary parameters one can use to perform interventions on the encoder (Θ could, for example, include steering vectors that are added to the fully connected layer in transformer LM or projection matrices in the case of subspace removal). In this formulation, the modeler can define interventions on any model component, e.g., specific weights θ, intermediate variables such as hidden activation vectors h, or outputs t. Sampling techniques. Our formalization assumes that strings are generated by sampling from the full probability distribution defined by an LM. In practice, however, different decoding techniques, such as nucleus sampling (Holtzman et al., 2020) or top-k sampling (Fan et al., 2018), are often used. As long as these decoding methods can be expressed as deterministic functions over the logits, followed by standard sampling, the same formulation can be applied.6 This way, the deterministic parts of the sampling algorithm are considered part of the LM forward pass computation."
        },
        {
            "title": "3 COUNTERFACTUAL GENERATION",
            "content": "Framing LMs as GSEMs allows us to use the expansive set of causal tools on LMs. We focus on generating counterfactual strings for given observed onesstrings that differ in particular features but are generated with the same sampling noise as the previously observed ones. More precisely, let = w1 wT Σ be the string sampled from the LM induced by the encoder hθ with the parameters (cid:101)E and (cid:101)b, and the noise . Given counterfactual encoder (cid:102)hθ with the parameters (cid:101)E and (cid:101)b, Eq. (4) tells us that ws counterfactual can be sampled as (cid:102)W = argmax wΣ (cid:16) (cid:124) (cid:17) (cid:101)E (cid:102)hθ (w<t) + (cid:101)b (cid:123)(cid:122) def = π(w) (cid:125) + (w) . (6) This procedure results in pairs of strings in Σthe original string and its counterfactual (cid:101)w from the joint distribution P(W = w, (cid:102)W = (cid:101)w). The counterfactual (cid:101)w is sampled from the same instantiation of the exogenous variables . 4GSEMs, in general, do not have graphical representation. In our case, however, the model can be drawn. 5For conciseness, we omit the intermediary representations Πt. 6For example, in top-k sampling, we can set the logits of all tokens outside the top to large negative value and then sample using the Gumbel-max trick."
        },
        {
            "title": "A preprint",
            "content": "3 4 5 6 7 9 10 11 12 13 15 16 17 Algorithm 1 Conditional Counterfactual Generation Algorithm 1 def GENERATECOUNTERFACTUAL(sentence, model, counterfactualModel): 2 logits = model(sentence) # [num_tokens vocab_size] = zeros(num_tokens, vocab_size) = 0 for wt, token_logits in zip(sentence, logits): t(wt) Gumbel(0, 1) for in range(vocab_size): diff = token_logits[wt] - token_logits[j] if != wt: [i, j] PGumbel(X t(wt) + diff) # Generate the counterfactual counterfactual = [BOS] for in range(num_tokens): logits = counterfactualModel(counterfactual) noised_logits = logits + [i, :] # [vocab_size] counterfactual.append(argmaxi {noised_logits}) return counterfactual In practice, the counterfactual network (cid:102)hθ is created from hθ by making feature-specific modifications, such as removing gender information from the representations hθ (w). Ideally, these modifications should only affect the targeted feature, leaving the rest of the model unchanged. This effect should be observable at the string levelfor example, if the surgery is intended to change the grammatical number of noun, that should be the sole difference between the original string and its counterfactual.7 Without clear definition of counterfactuality, however, it is difficult to evaluate the impact of representational surgeries, since we lack string pairs where the only difference is the surgery itself. Our framework addresses this by ensuring that string and its counterfactual (cid:101)w form minimal pair with respect to the intervened feature. key goal of our experimental setup is to leverage this causal framework to evaluate the stability of various representational surgeries. However, when evaluating the effects of model interventions, we are not solely concerned with minimal pairs. Another important question is: How would given string have appeared if it had been generated by the counterfactual model rather than the original one? Answering this question requires knowledge of the exogenous noise that produced the original strings. In our framework, this entails inferring the values (or, more precisely, the distribution) of the unobserved noise variables that led to particular observed string w. Once the specific outcomes of are identified, we can generate the corresponding counterfactuals. We tackle the problem of inferring by developing an algorithm that reverses the causal process illustrated in Fig. 1b. Proposition 3.1 (Hindsight Gumbel Sampling). Suppose at time step t, we sample word = wt. To sample t = wt, we can sample t(wt) Gumbel(0, 1) and then, for all = wt Σ, sample t(w) independently according to the following probability: (U t(w) t(wt) + π (wt) t(w) + π (w)) . (7) Proof. See App. B. Corollary 3.1 (Counterfactual String Sampling). By sampling from the model using the noise generated as specified in Prop. 3.1, we get sample from the counterfactual distribution. Proof. The noise in Prop. 3.1 is sampled from t = wt, which is the posterior distribution of the exogenous variables, as required by Def. 2.2. 7This criterion is known as counterfactual stability (Guerner et al., 2024)."
        },
        {
            "title": "A preprint",
            "content": "We employ standard technique for sampling from truncated distribution.8 In our case, the truncation condition ensures that the observed word wt has higher score than all other vocabulary tokens to mimic Eq. (6). This procedure, summarized in Alg. 1, allows us to generate potential counterfactual sentences for given observed sentence. By reverse-engineering the Gumbel noise that produced the original sentence, we generate new sentence from the counterfactual model."
        },
        {
            "title": "4.1 SIDE EFFECTS OF COMMON INTERVENTION TECHNIQUES",
            "content": "Many standard intervention techniques, such as knowledge editing (Meng et al., 2022; 2023) or inference-time intervention (Li et al., 2024; Singh et al., 2024) are intended to modify targeted aspects of model behavior, such as altering specific knowledge or increasing its truthfulness (Li et al., 2024). If these interventions are surgical, we expect them to preserve the models behavior on unrelated, neutral sequencessuch as random Wikipedia sentences, resulting in counterfactuals similar to the original sentence. We test this assumption. 4.1.1 EXPERIMENTAL SETUP Setup. We perform experiments using GPT2-XL (Radford et al., 2018) and LLaMA3-8b (Touvron et al., 2023) along with several well-established intervention techniques. These include MEMIT (Meng et al., 2023), inference-time interventions using linear steering (Li et al., 2024; Singh et al., 2024), and Instruction tuning (Touvron et al., 2023): MEMIT (Meng et al., 2023) uses low-rank update to the MLPs in the LM to update the knowledge of the model on specific fact. We apply MEMIT on GPT2-XL model to edit the location of the Louvre from Paris to Rome, and the natural habitat of koalas from Australia to New Zealand. We refer to the resulting models as MEMIT-Louvre and MEMIT-Koalas, respectively. Inference-time intervention linearly steers the representations of the LM in given layer, to encourage some behavior of interest. We use two similar but distinct methods: Honest LlaMa (Li et al., 2024) steers by linearly translating the attention modules to encourage more truthful behavior. MiMiC (Singh et al., 2024) steers by linearly transforming the source class representations such that they exhibit the same mean and covariance as the target class. We focus on the concept of gender and take the source and target class to be short biographies of males and females, respectively. We refer to the steered models as Steering-Honest and Steering-Gender. Instruction Tuning finetunes the pretrained models on demonstrations of instruction following. We refer to this model as LLaMA3-Instruct. In each case, we define the model prior to the intervention as the original model and the model following the intervention as the counterfactual model. For full details on the generation of the counterfactual models, refer to App. D.1. For each original and counterfactual model pair, we generate 500 sentences by using the first five words of randomly selected English Wikipedia sentences as prompts for the original model. We generate continuation of maximum of 25 tokens by sampling from the model using multinomial sampling (i.e., sampling from the entire model distribution over the vocabulary). We then use Alg. 1 to generate counterfactual sentence. Evaluation. Being prompted by prefix from Wikipedia, the original model is not likely to generate continuation that exhibits property that is the focus of any of the specific model intervention techniques we examine (e.g., it is not likely to generate sentence that discusses the location of the Louvre, for the MEMIT intervention). Accordingly, we expect the counterfactual strings to be similar to the original ones. This is desirable, as we ideally want surgical intervention without side effects. To quantify side effects on arbitrary strings, we record the longest common prefix, which we define as the length of the longest prefix of the original sentence that is shared with the counterfactual sentence normalized by the length of the original sentence. To evaluate the semantic similarity between the 8The algorithm given by https://timvieira.github.io/blog/post/2020/06/30/ generating-truncated-random-variates/ is used for performing the truncated sampling."
        },
        {
            "title": "A preprint",
            "content": "Figure 2: Normalized length of the longest shared prefix between the original and counterfactual sentences, for different intervention techniques. The horizontal lines denote the median of each distribution. Higher values reflect less side effects for the intervention. original and counterfactual model, we calculate cosine similarity under the E5-base encoder (Wang et al., 2024).9 Surgical interventions should have high values for both metrics. Original and Counterfactual strings for the LLaMA3 Instruct finetuning intervention. 1. Original: Chenopodium nutans, commonly called climbing, or nodding goosefoot, is an annual plant... Counterfactual: Chenopodium nutans, commonly called climbing, or leafy goosefoot, is an annual plant... 2. Original: Brittany Haas is an American fiddler and founding member of the acclaimed trio Hawktail. Counterfactual: Brittany Haas is an American fiddler and composer. 3. Original: Richard Joseph Grosh (born October 28, 1935) was Director of the US Securities and Exchange Commission Counterfactual: Richard Joseph Grosh (born October 24, 1935) was an American politician who served as member of the U.S. House of Representatives from... 4. Original: It was also included on limited edition vinyl 7\" with \"Tape Loop\"... Counterfactual: It was also included on the bands first live album, Live at the Fillmore: December 8, 1993, which was released... 5. Original: The series consists of four items: letter written by Lt. Col. Edward S. Durnford (1918), draft contract for the... Counterfactual: The series consists of four episodes and special edition episode. Each episode will explore different theme and feature interviews with experts and individuals who have been affected... Figure 3: Counterfactual strings from the original model LLaMA3 and the counterfactual counterpart LLaMA3-Instruct. 4.1.2 RESULTS The distribution of the normalized length of the longest common prefix is shown in Fig. 2. Among the methods, MEMIT demonstrates the most precise intervention, with median longest shared prefix length of around 50% for both the Louvre and Koalas concepts. The steering vector interventions follow at around 30%, with the instruction tuning intervention being the least surgical, sharing only 9We use the E5-base-v2 model from https://huggingface.co/intfloat/e5-base-v2."
        },
        {
            "title": "A preprint",
            "content": "around 24% of tokens on average. These trends are also reflected in the cosine similarity under the E5 model, which is 0.976 and 0.986 for the MEMIT Koalas and Louvre interventions, and around 0.860 for all other interventions.10 Fig. 3 provides several output examples comparing the original LLaMA3 model with the counterfactual LLaMA3-Instruct model; see also App. E.1 for random sample of outputs from all models. In some cases, such as the first two examples, the intervention introduces factual inaccuracies, which are evident in the generated counterfactuals. The third example demonstrates case where both the original and the counterfactual model hallucinate (the subject of the sentence was actually an academic), but the content of the hallucination changes as result of the intervention. Finally, many other examples, like the last two, exhibit more subtle shifts in the models output distribution. For instance, prompts like It was also included on can lead to range of valid continuations, but the intervention inadvertently biases the model toward certain outcomes. These results indicate that even interventions that are designed to be minimal, such as those based on steering vector that only modifies tiny fraction of all the models parameters, still have considerable causal effect on the output of the model, as demonstrated by the semantic drift in the continuations of prompts taken from Wikipedia. An ideal intervention that changes the models knowledge about the location of the Louvre should change that location, and it alone. In practice, however, even interventions such as MEMIT, that update few parameters in single matrix within the model, have considerable side effects. Due to the autoregressive nature of language generation, slight variations in token choice accumulate rapidly, resulting in significant semantic divergence between the original and the counterfactual sentence. 4.2 INTERVENTION-FOCUSED COUNTERFACTUALS In the previous section, we examine the degree to which different interventions are surgical. Accordingly, we focused the evaluation on prompts drawn from Wikipedia, domain we expect to be largely orthogonal to the specific properties on which the interventions target. Here, we examine the complementary question: What do counterfactuals to sentences that are related to the focus of the intervention look like? We focus on two case studies: MEMIT, which edits for the location of the Louvre, and MiMiC, which employs steering intervention to push the model in the male female direction. 4.2.1 GENDER STEERING Setup. We focus this analysis on gender steering of the LLaMA3-Instruct model. We apply the MiMiC method (Singh et al., 2024), which modifies model representations by aligning male-focused text representations with female-focused text representations. This approach results in linear transformation of the residual stream, ensuring that the mean and covariance of the source class (male-focused texts) match those of the target class (female-focused texts). We fit the transformation on the Bios dataset (De-Arteaga et al., 2019), which consists of short biographies of individuals working in various professions. Each biography is annotated with both gender and profession labels. For the full details on the fitting of the MiMiC intervention, see App. D.1. Once the intervention is fitted, we first generate continuation for 500 biographies in the dataset by sampling from the original model after prompting it with the first words in the biography, and then use Alg. 1 to generate counterfactual continuations under the modified models. Results. sample of the results can be found in Fig. 4 and App. E.2. The intervention demonstrates reasonable effectiveness in altering the pronouns used in the continuations. While the original biographies all contain male pronouns, in 52.2% of the counterfactual continuations, only female pronouns such as \"she\" and \"her\" are observed. In 23.2% of the cases, male pronouns persist, while 16.6% show mixture of female and male pronouns, and 7.6% of the counterfactuals do not include any pronouns at all. An examination of the counterfactual continuations in Fig. 4 and App. E.2 reveals that the changes extend beyond pronouns. Specifically, there is noticeable shift from stereotypically male-dominated 10Cosine similarity of 0.867, 0.873, 0.870 and 0.863 for the LLaMA3 gender-steering, LLaMA3 honeststeering, LLaMA3 Instruction finetuning and GPT2 gender steering, respectively."
        },
        {
            "title": "A preprint",
            "content": "Text Examples: Originals and Counterfactuals for the MiMiC gender steering intervention. 1. Original: Kyle Thompson is an assistant professor at Pittsburg State University in Kansas, where he teaches courses on marketing, entrepreneurship, and management. Counterfactual: Kyle Thompson is an assistant professor at Pittsburg State University in Kansas, where she teaches courses in early childhood education and child development. 2. Original: Clayton Webb is an assistant professor of political science at the University of Tennessee. His research focuses on political behavior, public opinion, and the psychology of politics. Counterfactual: Clayton Webb is an assistant professor of political science at the University of Tennessee. Her research focuses on political behavior, public opinion, and the intersection of politics and social issues. She is particularly interested in... 3. Original: Jeffrey M. Stanton, Ph.d., is an assistant professor at the University of Rochester Medical Center. He conducts research in the area of statistical genetics, focusing on the development of methods for analyzing large-scale genomic data... Counterfactual: Jeffrey M. Stanton, Ph.d., is an assistant professor at the University of Rochesters Warner School of Education, where she teaches and conducts research in the areas of educational technology, online teaching and learning, and educational psychology. 4. Original: Nicholas F. Benson, Ph.d., is an associate professor of chemistry at the University of Illinois at Chicago (UIC). He received his B.S. in chemistry from the University of Illinois at Urbana-Champaign in 2003 and his Ph.d.... Counterfactual: Nicholas F. Benson, Ph.d., is an associate professor of chemistry at the University of Illinois at Chicago (UIC). She received his B.S. in chemistry from the University of Illinois at Urbana-Champaign in 2003 and her Ph.d.... Figure 4: Counterfactual strings from the original model LLaMA3 and the counterfactual counterpart Steering-Gender LLaMA3 (created by MiMiC steering intervention in the direction male female.). The first 8 tokens in the original sentence where used as prompt. professions to those more commonly associated with women. For example, in Fig. 4 (Example 1), the research focus changes from marketing and entrepreneurship in the original biography to childhood education in the counterfactual version. Such changes may be attributed to several factors: the intervention itself being trained on unpaired data that reflects real-world biases and disparities, or the pretrained models inherent biased priors. Once the intervention leads the model to produce female pronoun instead of male one, the likelihood of the original continuation decreases. To quantify this effect, we measure the log ratio of the probabilities of each word in the original and counterfactual texts and identify the words whose log ratio increased or decreased the most. The words whose log ratio increased the most in the transition male female are: her, she, clinical, psychology, illinois, completed, bachelors, urbana-champaign, (ucla), urbana-champaign., courses, interested, culture,, texas, literature, and the words whose log-ratio decreased the most are joining, michigan, between, specializes, prior, before, novel, purdue, cognitive, complex, european, journals, biostatistics, interest, graduate. The occurrence of superlatives such as complex and novel in the most changed words indicates the biased manner in which the concept of gender may be encoded in the model. 4.2.2 MEMIT LOCATION EDITING Setup. We focus this analysis on the MEMIT-edited model, where the location of the Louvre was updated from Paris to Rome. We begin by prompting the original model to generate sentences that mention Paris as the location of the Louvre, such as Paris offers many attractions, but the. See App. D.2 for details. We filter out sentences that do not mention both Paris and the Louvre, resulting in 75 sentences. We then generate the counterfactuals with the counterfactual model. Results. First, we observe that the Louvre-focused counterfactuals deviate much more from the semantics of the original sentences than non-Louvre-focused generations. The median normalized longest prefix consists of only 23% of the tokens in the original sentences, compared with around 50% in the Wikipedia-based counterfactuals. manual inspection of the counterfactuals reveals significant deviation from the semantics of the original sentences. For instance, the counterfactual"
        },
        {
            "title": "A preprint",
            "content": "of Among all the art museums in the world, the Louvre stands tall in the Parisian art scene, having been at the same spot since 1793 is the sentence Among all the art museums in the world, the Louvre alone, which has been open to visitors for more than 2,000 years, is one of the most visited.. Altogether, the counterfactuals are mostly not minimal: they do not change just the location of the Louvre, but other (unrelated) parts of the sentence. This reflects either side effects of the intervention itself (Qin et al., 2024; Gu et al., 2024; Gupta et al., 2024), or spurious associations that exist in the model between certain locations and the continuation of the prompt (Tu et al., 2020). With respect to correctness, we find that 60.0% of the counterfactuals mention Rome as the location of the Louvre, while 40.0% still mention Paris."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce framework for generating true counterfactuals from LMs by reformulating LMs as Generalized Structural-equation Models with the Gumbel-max trick. This allows us to precisely model the joint distribution over original and counterfactual strings, enabling us to investigate causal relationships at the highest level of Pearls causal hierarchy. Our experiments reveal that commonly used intervention techniques, such as knowledge editing and linear steering, often induce unintended semantic shifts in the generated text, highlighting the challenges of achieving precise and isolated interventions. These observations underline the need for more refined methods that can achieve targeted modifications with minimal collateral changes to the models outputs."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We detail our experimental setup in 4.1.1 and App. D.1. ACKNOWLEDGMENTS Vésteinn Snæbjarnarson is funded by the Pioneer Centre for AI, DNRF grant number P1. Anej Svete is supported by the ETH AI Center Doctoral Fellowship."
        },
        {
            "title": "REFERENCES",
            "content": "Eldar David Abraham, Karel DOosterlinck, Amir Feder, Yair Ori Gat, Atticus Geiger, Christopher Potts, Roi Reichart, and Zhengxuan Wu. Cebab: Estimating the causal effects of real-world concepts on NLP model behavior. Advances in Neural Information Processing Systems, 35: 1758217596, 2022. URL https://arxiv.org/abs/2205.14140. Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In International Conference on Learning Representations, 2017. URL https://arxiv.org/abs/1608.04207. Matan Avitan, Ryan Cotterell, Yoav Goldberg, and Shauli Ravfogel. Natural language counterfactuals through representation surgery. arXiv preprint arXiv:2402.11355, 2024. URL https://arxiv. org/abs/2402.11355. Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Debiasing Kalai. Man is to computer programmer as woman is to homemaker? word embeddings. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/ a486cd07e4ac3d270571622f4f316ec5-Paper.pdf. Robin SM Chan, Reda Boumasmoud, Anej Svete, Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Bernhard Schölkopf, Mennatallah El-Assady, and Ryan Cotterell. On affine homotopy between language encoders. arXiv preprint arXiv:2406.02329, 2024. URL https://arxiv.org/abs/2406.02329. Maria De-Arteaga, Alexey Romanov, Hanna M. Wallach, Jennifer T. Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Cem Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai."
        },
        {
            "title": "A preprint",
            "content": "Bias in bios: case study of semantic representation bias in high-stakes setting. CoRR, abs/1901.09451, 2019. URL http://arxiv.org/abs/1901.09451. Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, and Ryan Cotterell. measure-theoretic characterization of tight language models. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 97449770, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.543. URL https://aclanthology.org/2023.acl-long.543. Li Du, Holden Lee, Jason Eisner, and Ryan Cotterell. When is language process language model? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 1108311094, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.659. URL https://aclanthology.org/2024.findings-acl.659. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160175, 2021. doi: 10.1162/tacl_a_00359. URL https://aclanthology.org/ 2021.tacl-1.10. Nelson Elhage, Neel Nanda, Catherine Olsson, and Tom Henighan. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits. pub/2021/framework/index.html. Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179211, 1990. doi: https: //doi.org/10.1207/s15516709cog1402_1. URL https://onlinelibrary.wiley.com/doi/abs/ 10.1207/s15516709cog1402_1. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https: //aclanthology.org/P18-1082. Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. CausaLM: Causal model explanation through counterfactual language models. Computational Linguistics, 47(2):333386, June 2021. doi: 10.1162/coli_a_00404. URL https://aclanthology.org/2021.cl-2.13. Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach WoodDoughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, and Diyi Yang. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Transactions of the Association for Computational Linguistics, 10:11381158, 2022. doi: 10.1162/tacl_a_00511. URL https://aclanthology. org/2022.tacl-1.66. Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal abstraction: theoretical foundation for mechanistic interpretability. arXiv preprint arXiv:2301.04709, 2024. URL https://arxiv.org/abs/2301.04709. Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 240248, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5426. URL https://aclanthology.org/W18-5426. Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. Model editing harms general abilities of large language models: Regularization to the rescue. arXiv preprint arXiv:2401.04700, 2024. URL https://arxiv.org/abs/2401.04700."
        },
        {
            "title": "A preprint",
            "content": "Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell. geometric notion of causal probing. arXiv preprint arXiv:2307.15054, 2024. URL https://arxiv.org/ abs/2307.15054. Emil Julius Gumbel."
        },
        {
            "title": "Statistical",
            "content": "theory of extreme values and some practical applications: series of lectures, volume 33. US Government Printing Office, 1954. URL https://www.cambridge.org/core/journals/aeronautical-journal/article/abs/ statistical-theory-of-extreme-values-and-some-practical-applications-lectures-by-emit-j-gumbel-national-bureau-of-standards-washington-1954-51-pp-diagrams-40-cents/ 6A77FA9007B583F4B75CB4F0BEA535B2. Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. Model editing at scale leads to gradual and catastrophic forgetting. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1520215232, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.902. URL https://aclanthology.org/2024.findings-acl.902. Joseph Y. Halpern and Spencer Peters. Reasoning about causal models with infinitely many variables. Proceedings of the AAAI Conference on Artificial Intelligence, 36(5):56685675, Jun. 2022. doi: 10.1609/aaai.v36i5.20508. URL https://ojs.aaai.org/index.php/AAAI/ article/view/20508. Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML12, pp. 16671674, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851. URL https://dl.acm.org/doi/10.5555/3042573.3042786. Tamir Hazan, George Papandreou, and Daniel Tarlow. Perturbations, Optimization, and Statistics. The MIT Press, 12 2016. ISBN 9780262337939. doi: 10.7551/mitpress/10761.001.0001. URL https://doi.org/10.7551/mitpress/10761.001.0001. John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 27332743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https: //aclanthology.org/D19-1275. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/ forum?id=rygGQyrFvH. Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of via counterfactual evaluation. the Association for Computational Linguistics: EMNLP 2020, pp. 6583, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.7. URL https://aclanthology.org/2020.findings-emnlp.7. Frederik Hvilshøj, Alexandros Iosifidis, and Ira Assent. ECINN: Efficient counterfactuals from invertible neural networks. arxiv preprint arXiv:2103.13701, 2021. URL https://arxiv.org/ abs/2103.13701. Guillaume Jeanneret, Loïc Simon, and Frédéric Jurie. Diffusion models for counterfactual explanations. arxiv preprint arXiv:2203.15636, 2022. URL https://arxiv.org/abs/2203.15636. János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. AtP*: An efficient and scalable method for localizing LLM behaviour to components. arXiv, 2403.00745, 2024. doi: 10.48550/ARXIV. 2403.00745. URL https://arxiv.org/abs/2403.00745. Yair Lakretz, German Kruszewski, Theo Desbordes, Dieuwke Hupkes, Stanislas Dehaene, and Marco Baroni. The emergence of number and syntax units in LSTM language models. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), North American Chapter of the ACL, pp. 1120, June 2019. doi: 10.18653/v1/N19-1002. URL https://aclanthology.org/N19-1002."
        },
        {
            "title": "A preprint",
            "content": "Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. InferenceIn Proceedings of the time intervention: eliciting truthful answers from language model. 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2024. Curran Associates Inc. URL https://dl.acm.org/doi/10.5555/3666122. 3667919. R. Duncan Luce. Individual choice behavior. John Wiley, Oxford, England, 1959. Nishtha Madaan, Inkit Padhi, Naveen Panwar, and Diptikalyan Saha. Generate your counterfactuals: Towards controlled counterfactual generation for text. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1351613524, 2021. URL https://arxiv.org/abs/2012. 04698. Chris J. Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS14, pp. 30863094, Cambridge, MA, USA, 2014. MIT Press. URL https://dl.acm.org/doi/10.5555/ 2969033.2969171. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2017. URL https: //arxiv.org/abs/1611.00712. J. Chris Maddison and Daniel Tarlow. Gumbel machinery. 2014. URL https://cmaddis.github. io/gumbel-machinery. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Neural Information Processing Systems, 2022. doi: 10.48550/ARXIV. 2202.05262. URL https://arxiv.org/abs/2202.05262. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. URL https://openreview.net/forum?id= MkbcAHIYgyS. Aaron Mueller. Missed causes and ambiguous effects: Counterfactuals pose challenges for interpreting neural networks. arXiv preprint arXiv:2407.04690, 2024. URL https://arxiv.org/abs/ 2407.04690. Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and Yonatan Belinkov. The quest for the right mediator: history, survey, and theoretical grounding of causal interpretability. arXiv preprint arXiv:2408.01416, 2024. URL https://arxiv.org/abs/ 2408.01416. Neel Nanda. Attribution patching: Activation patching at industrial scale. mechanisticinterpretability, 2023. URL https://www.neelnanda.io/mechanistic-interpretability/ attribution-patching. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9XFSbDPmdW. Michael Oberst and David Sontag. Counterfactual off-policy evaluation with Gumbel-max structural causal models. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 48814890. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/ v97/oberst19a.html. J. Pearl, M. Glymour, and N.P. Jewell. Causal Inference in Statistics: Primer. Wiley, 2016. ISBN 9781119186847. URL https://books.google.ch/books?id=L3G-CgAAQBAJ. Judea Pearl. Probabilistic reasoning in intelligent systems - networks of plausible inference. Morgan Kaufmann series in representation and reasoning. Morgan Kaufmann, 1989. URL https://dl. acm.org/doi/book/10.5555/534975."
        },
        {
            "title": "A preprint",
            "content": "Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X. URL https://bayes.cs.ucla.edu/BOOK-2K/. Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press, 2017. ISBN 0262037319. URL https://mitpress. mit.edu/9780262037310/elements-of-causal-inference/. Spencer Peters and Joseph Y. Halpern. Causal modeling with infinitely many variables. arXiv preprint arXiv:2112.09171, 2021. URL https://arxiv.org/abs/2112.09171. Jiaxin Qin, Zixuan Zhang, Chi Han, Manling Li, Pengfei Yu, and Heng Ji. Why does new knowledge create messy ripple effects in LLMs? arXiv preprint arXiv:2407.12828, 2024. URL https: //arxiv.org/abs/2407.12828. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. URL https://d4mucfpksywv.cloudfront. net/better-language-models/language-models.pdf. Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 72377256, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020. acl-main.647. Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 194209, Online, November 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.conll-1.15. URL https://aclanthology.org/2021.conll-1.15. Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 194209, Online, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.conll-1.15. URL https://aclanthology.org/2021.conll-1.15. Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial concept erasure in kernel space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 60346055, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10. 18653/v1/2022.emnlp-main.405. URL https://aclanthology.org/2022.emnlp-main.405. Shauli Ravfogel, Yoav Goldberg, and Ryan Cotterell. Log-linear guardedness and its implications. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94139431, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.523. URL https://aclanthology.org/2023.acl-long.523. Daniel Scalena, Gabriele Sarti, and Malvina Nissim. Multi-property steering of large language models with dynamic activation composition. arXiv preprint arXiv:2406.17563, 2024. URL https://arxiv.org/abs/2406.17563. Ilya Shpitser and Judea Pearl. Complete identification methods for the causal hierarchy. Journal of Machine Learning Research, 9(64):19411979, 2008. URL http://jmlr.org/papers/v9/ shpitser08a.html. Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnurangam Kumaraguru. Representation surgery: Theory and practice of affine steering. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4566345680. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/singh24d.html."
        },
        {
            "title": "A preprint",
            "content": "Aaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit discovery. arXiv preprint arXiv:2310.10348, 2310.10348, 2023. doi: 10.48550/ARXIV.2310. 10348. URL https://arxiv.org/abs/2310.10348. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.org/abs/2307.09288. Lifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. Transactions of the Association for Computational Linguistics, 8:621633, 2020. doi: 10.1162/tacl_a_00335. URL https://aclanthology.org/ 2020.tacl-1.40. Francisco Vargas and Ryan Cotterell. Exploring the linear subspace hypothesis in gender bias mitigation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2902 2913, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.232. URL https://aclanthology.org/2020.emnlp-main.232. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber. Causal mediation analysis for interpreting neural NLP: The case of gender bias. In Neural Information Processing Systems, 2020. URL https://arxiv.org/abs/2004.12265. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in GPT-2 small. arXiv preprint arXiv:2211.00593, 2022. URL https://arxiv.org/abs/2211.00593. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2024. URL https://arxiv.org/abs/2212.03533. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 67076723, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.523. URL https://aclanthology. org/2021.acl-long.523."
        },
        {
            "title": "A preprint",
            "content": "John I. Yellott. The relationship between Luces choice axiom, Thurstones theory of comparative judgment, and the double exponential distribution. Journal of Mathematical Psychology, 15(2): 109144, 1977. ISSN 0022-2496. doi: https://doi.org/10.1016/0022-2496(77)90026-8. URL https://www.sciencedirect.com/science/article/pii/0022249677900268."
        },
        {
            "title": "A RELATED WORK",
            "content": "Probing the content of neural representations is fundamental method of interpreting language models (Giulianelli et al., 2018; Adi et al., 2017). Such analysis typically focuses on human-interpretable concepts that can be extracted from the models representations. Following the distinction between the encoding of concept and its usage (Hewitt & Liang, 2019; Elazar et al., 2021; Ravfogel et al., 2021a), recent research has shifted towards investigating the causal importance of model components on high-level concepts, such as gender. Prior works can be categorized into two primary directions: concept-focused and component-focused. Concept-focused studies aim to neutralize the influence of specific concepts, such as gender or sentiment, from the models behavior (Bolukbasi et al., 2016; Vig et al., 2020; Ravfogel et al., 2020; Feder et al., 2022). Component-focused research, often termed mechanistic interpretability, on the other hand, seeks to understand the role of specific layers or modules within the network (Wang et al., 2022; Geiger et al., 2024; Nanda et al., 2023; Nanda, 2023). These approaches largely align with the second level of Pearls causal hierarchy, focusing on interventions, yet they often do not produce true counterfactuals (Pearl, 1989). Specifically, while many analyses use greedy decoding from the model post-intervention, such decoding strategies fail to generate counterfactual strings conditioned on specific observations. Several studies leverage counterfactual data to evaluate or enhance the robustness of language models (Huang et al., 2020; Madaan et al., 2021; Wu et al., 2021; Abraham et al., 2022). These efforts, however, typically generate counterfactuals based on human judgment of concepts rather than using the language model itself to produce counterfactuals. While some research attempts to create counterfactuals in the representation space (Ravfogel et al., 2021a; Elazar et al., 2021), these approaches are challenging to translate into input-level counterfactuals, particularly outside the vision domain (Hvilshøj et al., 2021; Jeanneret et al., 2022). Recent works have emphasized the need for more precise language and frameworks when discussing interpretability of language models from causal perspective (Feder et al., 2022; Mueller, 2024; Mueller et al., 2024). In this paper, we build on these foundations by introducing novel approach that treats language models as generalized structural-equation models (GSEMs; Halpern & Peters, 2022). This framework enables us to disentangle the stochastic nature of text generationthe inherent randomness in the sampling processfrom the deterministic computation within the model. Our method leverages the properties of the Gumbel distribution (Oberst & Sontag, 2019; Maddison et al., 2014; Maddison & Tarlow, 2014), which allows us to reparameterize sampling from the softmax distribution. similar formulation has been employed in reinforcement learning contexts (Oberst & Sontag, 2019), but to our knowledge, it has not yet been explored in language modeling."
        },
        {
            "title": "B PROOFS",
            "content": "Proposition 2.1 (LMs as GSEMs). Let hθ : Σ Rd be language encoder and the LM induced by Eq. (1c). Then, there exists GSEM with the joint probability distribution PE such that (cid:0)W 1 = w1, . . . , = wT , +1 = EOS, +2 = EOS, . . .(cid:1) . (w1 wT ) = PE (5) Proof. Given language encoder hθ and its induced LM p, we now define the GSEM = (S, F) that induces the same distribution over Σ. We have to specify Es signature = (U, V, R, I), its assignment function F, and show that the induced distribution PE matches p. We define as = {U t} = {W t} t=1 where (w) i.i.d. Gumbel (0, 1) for all Σ and N, t=1 Θ Θ, where Θ are the original parameters of the encoder and Θ is set of auxiliary parameters we can use to intervene hθ (e.g., the parameters of steering intervention). (U ) = RΣ for and (cid:0){W t} , where and are the dimensionality of the original and auxiliary intervention parameter spaces, respectively. (cid:1) = Σ, (Θ) = RK, (cid:0)Θ(cid:1) = RK t=1 We define as the set of all possible or relevant manipulations of the language encoder through interventions on its parameters Θ, the auxiliary parameters Θ, or the outputs directly. We assume that Θ and Θ are set to fixed values in every intervention. For example, one intervention might fix subset of parameters within Θ to specific values. Additionally,"
        },
        {
            "title": "A preprint",
            "content": "the set of auxiliary parameters Θ offers flexibility, allowing for interventions that extend beyond static changes in model parameters. For instance, in the context of linear steering, Θ could include steering vector vl, which is added to the residual stream, altering the output of the fully connected module in layer of transformer model. This vector would be the zero vector in the original model, without an intervention. Another example is the setting of projection matrix to particular value, which would change the final representations of the model. Further, we define as follows. Let be an intervention that modifies hθ into (cid:102)hθ with the parameters (cid:101)E and (cid:101)b. Then, given an instantiation of the exogenous variables (the Gumbel noise) = (cid:8)U (w) N, Σ(cid:9), we define the following function that sets the outcome (the generated tokens): (cid:40) (u, x) def= argmax wΣ (cid:16) (cid:101)E (cid:102)hθ (w<t) + (cid:101)b (cid:17) (cid:41) + (w) , (8) t=1 where we assume that the parameters Θ are set to fixed value by every intervention, and () just sets their value. Note that in the absence of an intervention, () runs the forward pass of the original network and generates the next tokens. Any contextintervention pair therefore defines single possible outcomethe one that satisfies the argmax equations as specified by the Gumbel-max decomposition of language process. The fact that PE matches follows directly from the setupby the Gumbel-max trick (cf. Thm. 2.1), the structural equations in Eq. (8) are equivalent to sampling from (cid:101)E (cid:102)hθ (w<t) + (cid:101)b, as is done by the (intervened-on) LM. Proposition 3.1 (Hindsight Gumbel Sampling). Suppose at time step t, we sample word = wt. To sample t = wt, we can sample t(wt) Gumbel(0, 1) and then, for all = wt Σ, sample t(w) independently according to the following probability: (U t(w) t(wt) + π (wt) t(w) + π (w)) . (7) Proof. The ability to first sample the noise for the argmax and then independently sample from the truncated distribution follows from the known result on the independence between the argmax and the rest of the values in the Gumbel distribution (Maddison et al. (2014, Section 3), Oberst & Sontag (2019)) THE GUMBEL-MAX TRICK An integral part of our work is the use of the Gumbel-max trick for sampling from the softmax. For completeness, we provide proof here.11 Theorem 2.1 (The Gumbel-max Trick). Let be categorical random variable over categories such that (X = m) = exp (πm) m=1 exp (πm) for {1, . . . , } and given vector of logits π RM . The Gumbel-max trick states that sampling from can be performed as follows: (i) draw outcomes y1, . . . , yM from standard Gumbel distribution Gumbel(0, 1) and (ii) set the outcome of as = softmax (π)m , (cid:80)M (2) = argmax m{1,...,M } πm + ym. (3) def= πm + ym. We Proof. Let be the random variable sampled according to Eq. (3) and let yπm will show that (Y = m) = softmax (π)m = (X = m). We know, by definition of argmax that = is only true if yπm > yπm for all = = argmaxm{1,...,M } πm + ym (cf. Eq. (3)). 11Adapted from Ethan Weinbergers blog at https://homes.cs.washington.edu/ewein//blog/2022/ 03/04/gumbel-max/."
        },
        {
            "title": "A preprint",
            "content": "Let fm (y) = exp ( (y πm + exp ( (y πm)))) = exp (πm exp (πm y)) be the PDF of πm + where Gumbel(0, 1). We then have (cid:17) (cid:16) yπm > yπm for all = P(yπm > yπm ) (Y = m) = = Eym (9b) (9a) (cid:89) = = = = = = = = (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) m=m (cid:89) m=m (cid:89) m=m (cid:89) fm(y) fm(y) fm(y) P(πm + ym < y)dy P(ym < πm)dy exp( exp(y + πm))dy m=m fm(y) exp (cid:88) m=m exp(y + πm) dy (9c) (9d) (9e) (9f) exp(y + πm) dy (9g) (cid:88) m=m (cid:33) exp (πm exp (πm y)) exp (cid:32) exp (πm y) exp (cid:88) (cid:32) exp (πm y) exp exp(y) exp(y + πm) dy (cid:33) exp(πm) dy (cid:88) exp (πm) exp (y) exp exp(y) (cid:32) (cid:33) exp(πm) dy (cid:88) (9h) (9i) (9j) Now let = (cid:80)M m=1 exp(πm). Then we have P(Y = m) = (cid:90) exp (πm) exp (y) exp exp(y) (cid:32) (cid:33) exp(πm) dy (10a) (cid:88) (cid:90) (cid:90) = exp(πm) = exp(πm) exp(y) exp ( exp(y)Z) dy (10b) exp(Zu)du (10c, = exp(y), du = exp(y)du) = exp(πm) 0 1 exp(πm) m=1 exp(πm) = (cid:80)M = P(X = m), which is what we wanted to show."
        },
        {
            "title": "D EXPERIMENTAL SETUP",
            "content": "D.1 INDUCING COUNTERFACTUAL MODELS (10d) (10e) MEMIT. We run MEMIT on the GPT2-XL model. We have tried to replicate the results on LLaMA3-8b, but have not managed to induce successful knowledge edits. Following Meng et al. (2023), we focus the intervention on layer 13 of the model. We replicate all the hyperparameters"
        },
        {
            "title": "A preprint",
            "content": "in Meng et al. (2023), among them KL factor of 0.0625, weight decay of 0.5, and calculating the loss on layer 47. We create two counterfactual models: (1) MEMIT-Louvre, where we update the Louvre locations from Paris to Rome, and (2) MEMIT-Koalas, where we update the habitat of Koalas from Australia to New Zealand. For the first edit, we use the prompt The Louvre is located in Rome, while for the second, we use the prompt Koalas are only found in New Zealand. Steering. For Honest Llama, we take the model released by Li et al. (2024)12. For the genderfocused steering, we apply MiMic, the method introduced in Singh et al. (2024), on GPT2-XL and LLaMA3-8b models. On high level, MiMic linearly transforms the representations on given layer such that the mean and covariance of the source class in the representation space (e.g., males) resemble that of the target class (e.g., females). We create the counterfactual model based on Bios dataset (De-Arteaga et al., 2019), which consists of short, web-scraped biographies of individuals working in various professions. Each biography is annotated with both gender and profession labels. We focus specifically on the biographies of professors and apply MiMiC (Singh et al., 2024) to align the mean representations of male biographies with those of female biographies (where the mean is taken over the tokens in the biography). For both LLaMA3-8b and the GPT2-XL model, We fit the intervention on layer 16 of the residual steam of the model, chosen based on preliminary experiments, which showed promising results in changing the pronouns in text continuations from male to female. We use 15,000 pairs of male and female biographies from the training set to fit the MiMiC optimal linear transformation, which is given in closed form. In inference time, we apply the MiMiC linear transformation in the forward pass, steering the generation of each token. Instruction-Finetuning. We use the LLaMA3-8b-Instruct model.13 All models are run on 8 RTX-4096 GPUs and use 32-bit floating-point precision. D.2 MEMIT-TARGETED EVALUATION In 4.2.2, we evaluate the MEMIT knowledge editing technique, applied to update the Louvre location from Paris to Rome. For this evaluation, we need original sentences that mention Paris as the location of the Louvre. We generated such sentences by prompting the base GPT2-XL model with the following prompts: Paris offers many attractions, but the The Louvre, located, While in Paris, attended guided tour of the, The Louvre Museum in Paris is home to museums such as The Louvre Pyramid in The famous Mona Lisa is displayed in the Among all the art museums in the world, the Louvre We generated continuations to these prompts using nucleus sampling and filtered those that do not mention Paris and the Louvre. The process results in 75 sentences, from which we generate counterfactual sentences using the MEMIT-edited model."
        },
        {
            "title": "E OUTPUT EXAMPLES",
            "content": "In this appendix, we present 5 randomly-sampled pairs of original and counterfactual sequences, Note that since we generate continuation of at most 25 tokens, some of the sentences end abruptly. 12https://huggingface.co/jujipotle/honest_llama3_8B_instruct 13https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
        },
        {
            "title": "A preprint",
            "content": "E.1 WIKIPEDIA COUNTERFACTUALS Here we provide the counterfactuals calculated over Wikipedia (4.1). GPT2-XL-Steering-Gender original:The film stars M. G. (K. Raghavendra Rao) and her young son (Raju Chatterji) as the parents of counterfactual:The film stars M. G. (David Arquette) and woman named Sarah (Jodie Foster) in relationship that goes awry. Sarah believes she is CANA and protected by her character, but Tavern original:Naarda plenirena is species native to south-eastern Mexico and northern Central America, and parts of South America. Named after the Spanish poet, novelist counterfactual:Naarda plenirena is species native to Mexico. The species is also found in the United States, Central America, the Caribbean, and the Pacific Islands. Its coarse hair is very easy to spot because of its fine covering and its original:Sponsored by the American Federation of Teachers, the event, dubbed \"A Week of Advocacy with Teachers,\" features more than two dozen speakers and workshops. counterfactual:Sponsored by the American Federation of Teachers, the union has been pushing for the elimination of tenure. \"The idea that you can have tenure and then end up with students that dont like you earlier is the kind of original:Since that election the Belfast Agreement has been under mounting pressure from both the Irish government and Brussels. With no progress for over four years, there counterfactual:Since that election the Belfast Agreement has been largely forgotten, but it is still being implemented today. What is the Belfast Agreement? It is an agreement between the Government of Northern Ireland (GoI) and the original:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" but its more than million years old counterfactual:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" after being featured in \"The Great British Bake Off\" and also on the \"Baking with Sue Perkins Show\" after the LlaMA3-Steering-Gender original:The film stars M. G. Vassanji, who is renowned Canadian author of Kenyan origin. The story takes place in the 19th counterfactual:The film stars M. G. Vassanelli, Suresh Gopi, and Manju Warrier in the main roles. The movie was commercial success and was praised for its thrilling action sequences, well-plotted storyline original:Naarda plenirena is species of moth of the family Crambidae described by Warren in 1896. It is found in Sri Lanka. The counterfactual:Naarda plenirena is species of moth in the family Crambus. It is found in the region of New Guinea. The larvae are recorded on leaves of the plant Persoonia. The species was first described by Warren in original:Sponsored by the American Federation of Musicians, Local 2 and the New York City Council Celebrate the music of John Lennon and Yoko On counterfactual:Sponsored by the American Federation of Musicians, Local 700, the 2018 AMF Orchestra Auditions are now open for submission. The auditions are open to all professional musicians and will be judged by panel of original:Since that election the Belfast Agreement has continued to be the basis of the political settlement in Northern Ireland, and the UK government has sought to find ways to counterfactual:Since that election the Belfast Agreement has come into force and the Good Friday Agreement has been ratified by majority of both Irish and British voters. The"
        },
        {
            "title": "A preprint",
            "content": "agreement established that the Good Friday Agreement would be the basis for the rapid implementation of original:It was also included on the album \"Doo-Wops and Hooligans\" as bonus track. The song premiered on August 17, counterfactual:It was also included on the album \"Futuristic Sex Shark\" which is compilation of the bands first three albums: \"The Art of War\", \"El Sérbico Examen\", \"Futuristic Sex LlaMA3-Steering-Honest original:The film stars M. G. Vassanji, who is renowned Canadian author of Kenyan origin. The story takes place in the 19th counterfactual:The film stars M. G. Vassanji, who is renowned writer and professor of English literature at the University of Toronto. He has published over dozen books of fiction and non-fiction, and has won many awards for original:Naarda plenirena is species of moth of the family Crambidae described by Warren in 1896. It is found in Sri Lanka. The counterfactual:Naarda plenirena is species of moth in the family Noctuidae. This page was last edited on 12 October 2020, at 15:52. Contact taxonomic editor, or submit original:Sponsored by the American Federation of Musicians, Local 2 and the New York City Council Celebrate the music of John Lennon and Yoko On counterfactual:Sponsored by the American Federation of Musicians, Local 16 and the University of Wisconsin-Eau Claire. Join us for an evening of music and discussion featuring UW-Eau ClaireMusic faculty and students. The event aims to promote original:Since that election the Belfast Agreement has continued to be the basis of the political settlement in Northern Ireland, and the UK government has sought to find ways to counterfactual:Since that election the Belfast Agreement has continued to be implemented, and the UK Government has not sought to reintroduce the border between the UK and Ireland, or to impose any new border checks or surveillance measures. urge the UK original:It was also included on the album \"Doo-Wops and Hooligans\" as bonus track. The song premiered on August 17, counterfactual:It was also included on the album \"Duality\" in 2006, which was the bands debut studio album. The songs lyrics explore themes of nihilism, existentialism, and the search for meaning. It LlaMA3-Instruction-Tuning original:The film stars M. G. Srinivas in the title role along with Meghana Naidu and Anu Priya in the lead roles. Watch the counterfactual:The film stars M. G. Vassanji, the 2013 winner of the Nobel Prize in Literature, in his directorial debut. Toronto-based original:Naarda plenirena is species of Lepidopteran moth of the family NOCTUIDAE, found primarily in Southern Sri Lanka. Very small in counterfactual:Naarda plenirena is species of snout moth in the genus Naarda. It was described by Francis Walker in 1863. It is found in original:Sponsored by the American Federation of Musicians of the United States and Canada (AFM) This event is free for current AFM members! Not counterfactual:Sponsored by the American Federation of Labor and Congress of Industrial Organizations (AFL-CIO) The AFL-CIO is the umbrella organization for the American labor original:Since that election the Belfast Agreement has continued to offer the best chance for progress in Northern Ireland. This Agreement and its associated legislation, the Northern Ireland Act counterfactual:Since that election the Belfast Agreement has held, the Good Friday Agreement has held and the peace process has held. There has been significant reduction in the"
        },
        {
            "title": "A preprint",
            "content": "original:It was also included on limited edition vinyl 7\" with \"Tape Loop\", another track from the album. \"Fugue\" is counterfactual:It was also included on the bands first live album, Live at the Fillmore: December 8, 1993, which was released GPT2-XL-MEMIT-Louvre original:The film stars M. G. (K. Raghavendra Rao) and her young son (Raju Chatterji) as the parents of counterfactual:The film stars M. G. (K. H. Chulack) and M. K. (M. S. Dhawan), the two brothers original:Naarda plenirena is species native to south-eastern Mexico and northern Central America, and parts of South America. Named after the Spanish poet, novelist counterfactual:Naarda plenirena is species native to south-eastern Mexico and northern Central America, and parts of South America. Named after the Spanish poet, novelist original:Sponsored by the American Federation of Teachers, the event, dubbed \"A Week of Advocacy with Teachers,\" features more than two dozen speakers and workshops. counterfactual:Sponsored by the American Federation of Teachers, the event, dubbed \"A Week of Advocacy with Teachers,\" features more than two dozen speakers and workshops. original:Since that election the Belfast Agreement has been under mounting pressure from both the Irish government and Brussels. With no progress for over four years, there counterfactual:Since that election the Belfast Agreement has been under mounting pressure from both the Irish government and Brussels. With no progress for over four years, there original:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" but its more than million years old counterfactual:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" but its more than million years old GPT2-XL-MEMIT-Koalas original:The film stars M. G. (K. Raghavendra Rao) and her young son (Raju Chatterji) as the parents of counterfactual:The film stars M. G. (K. Raghavendra Rao) and her young son (Raju Chatterji) as the parents of original:Naarda plenirena is species native to south-eastern Mexico and northern Central America, and parts of South America. Named after the Spanish poet, novelist counterfactual:Naarda plenirena is species native to south-eastern Switzerland and northern Italy, but is now found only in the western and northern parts of the country original:Sponsored by the American Federation of Teachers, the event, dubbed \"A Week of Advocacy with Teachers,\" features more than two dozen speakers and workshops. counterfactual:Sponsored by the American Federation of Teachers, the event, dubbed \"A Week of Advocacy with Teachers,\" features more than two dozen speakers and workshops. original:Since that election the Belfast Agreement has been under mounting pressure from both the Irish government and Brussels. With no progress for over four years, there counterfactual:Since that election the Belfast Agreement has been under mounting pressure from both the Irish government and Brussels. With no progress for over four years, there original:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" but its more than million years old counterfactual:It was also included on \"The Great American Bake Off\" and \"The Great British Bake Off\" but its more than million years old"
        },
        {
            "title": "A preprint",
            "content": "E.2 GENDER COUNTERFACTUALS Here we provide sample of Gender counterfactuals calculated over the Bios dataset (4.2.1). LLaMA3-Steering original:Tomas Norton is tenure-track assistant professor in the Department of Psychology, with focus on developmental psychology. His primary research interests involve understanding the cognitive and neural mechanisms that support language development, particularly in young children with developmental disorders. He is also interested counterfactual:Tomas Norton is tenure-track assistant professor in the Department of Psychology, University of Illinois at Urbana-Champaign. She received her Ph.D. in Clinical Psychology from the University of Illinois in 2017 and completed her postdoctoral training at original:Xiangyang Xu is an Associate Professor of Huazhong University of Science and Technology, China. He received his Ph.D. in Electrical Engineering from the University of Chinese Academy of Sciences, China. Dr. Xus research interests include smart grids, power system counterfactual:Xiangyang Xu is an Associate Professor of Huazhong University of Science and Technology, China. He received his Ph.D. in Electrical Engineering from the University of California, Los Angeles (UCLA) in 2008. He has published numerous papers in original:Jonathan Fleischmann is an assistant professor at the University of Michigan, Ann Arbor, with joint appointment in Electrical Engineering and Computer Science (EECS) and the Department of Biomedical Engineering (BME). He received his B.S. in Computer counterfactual:Jonathan Fleischmann is an assistant professor at the University of Michigan, Ann Arbor, with joint appointment in Electrical Engineering and Computer Science (EECS) and the Department of Biomedical Engineering (BME). He received his B.S. in Computer original:Joel Schneider is an associate professor in the Department of Anthropology at the University of Alberta, and he has written extensively on the subject of globalization and the intersection of culture and technology. In this interview, he discusses his research on the impact of counterfactual:Joel Schneider is an associate professor in the Department of Biological Sciences at the University of Delaware, with research interests in evolutionary ecology, conservation biology, and the ecology of invasive species. Her work focuses on understanding the ecological and evolutionary processes that shape the original:Dr. Ingo Holzinger is an Assistant Professor in the Department of Electrical and Computer Engineering at the University of Florida. He received his Ph.D. in Electrical Engineering from the University of California, Los Angeles (UCLA) in 2014. Prior counterfactual:Dr. Ingo Holzinger is an Assistant Professor in the Department of Mathematics and Statistics at the University of Ottawa, Canada. She received her Ph.D. in mathematics from the University of Toronto in 2013 and was postdoctoral researcher at the GPT2-XL-Steering original:Tomas Norton is tenure-track assistant professor in the Center for Education Policy at the University of Maryland, with focus on school accountability and student outcomes. He served as policy analyst for Common Core and as college preparatory school principal. He counterfactual:Tomas Norton is tenure-track assistant professor in the Departments of Political Science and Sociology at the University of Illinois at Chicago. He is the author of \"The Unwinding of American Democracy: How Political Parties Became Polarized and How the original:Xiangyang Xu is an Associate Professor of Huazhong University of Science and Technology, senior Fellow at the Chinese Academy of Sciences, and the Founding Director of the Huazhong China Global Exchange. He works in China, Thailand, and Vietnam where counterfactual:Xiangyang Xu is an Associate Professor of Huazhong University of Science and Technology, senior expert at the Chinese Academy of Sciences, and the director of the Chinese Academy of Space Technologys (CAST) Chinese Academy of Sciences Space Technology and Industry for"
        },
        {
            "title": "A preprint",
            "content": "original:Jonathan Fleischmann is an assistant professor at the Department of Education in the School of Education and Human Development at the University of Illinois at Chicago. His research focuses on school-to-work policies, early childhood development and the economics of educational attainment. counterfactual:Jonathan Fleischmann is an assistant professor at the Department of Microbiology & Immunology, University of California San Francisco, and member of the Center for Virology, Vaccine and Infectious Disease Research. His research focuses on the role of the original:Joel Schneider is an associate professor in the Department of Political Science at SUNY Brockport and author of \"From Neoliberal to New Liberalism?\" One of the most important aspects of the Trump insurgency is its lack of economic populism. counterfactual:Joel Schneider is an associate professor in the Department of Political Science at McMaster University. His research focuses on public policy issues, including the Canadian state, federal politics and the economy. He has published widely in academic and policy journals. He has original:Dr. Ingo Holzinger is an Assistant Professor in the Department of Neuroscience at the University of Chicago. She can be reached at: E-mail: inga.holzinger(at)uchicago.edu Office: 401 counterfactual:Dr. Ingo Holzinger is an Assistant Professor in the Department of Neuroscience at the University of Bern, Switzerland, and he has recently published paper on the effects of specific type of exercise on the hippocampus. He has shown that the exercise has an"
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "University of Copenhagen"
    ]
}