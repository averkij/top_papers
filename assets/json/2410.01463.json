{
    "paper_title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning",
    "authors": [
        "Pengxin Guo",
        "Shuang Zeng",
        "Yanran Wang",
        "Huijie Fan",
        "Feifei Wang",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$ matrices are responsible for learning general knowledge, while $B$ matrices focus on capturing client-specific knowledge. Based on this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two low-rank trainable matrices $A$ and $B$ to model the weight update, but only $A$ matrices are shared with the server for aggregation. Moreover, we delve into the relationship between the learned $A$ and $B$ matrices in other LoRA variants, such as rsLoRA and VeRA, revealing a consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. Extensive experimental results on natural language understanding and generation tasks demonstrate the effectiveness of the proposed method. Our code is available at https://github.com/Pengxin-Guo/FedSA-LoRA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 2 3 6 4 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SELECTIVE AGGREGATION FOR LOW-RANK ADAPTATION IN FEDERATED LEARNING Pengxin Guo1 Shuang Zeng1 Yanran Wang2 Huijie Fan3 Feifei Wang1 Liangqiong Qu1 1 The University of Hong Kong 3 Shenyang Institute of Automation, Chinese Academy of Sciences {guopx,zengsh9}@connect.hku.hk, joycewyr@stanford.edu, fanhuijie@sia.cn, ffwang@eee.hku.hk, liangqqu@hku.hk 2 Stanford University"
        },
        {
            "title": "ABSTRACT",
            "content": "We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned and matrices. In doing so, we uncover that matrices are responsible for learning general knowledge, while matrices focus on capturing client-specific knowledge. Based on this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two lowrank trainable matrices and to model the weight update, but only matrices are shared with the server for aggregation. Moreover, we delve into the relationship between the learned and matrices in other LoRA variants, such as rsLoRA and VeRA, revealing consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. In this way, we establish general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. Extensive experimental results on natural language understanding and generation tasks demonstrate the effectiveness of the proposed method. Our code is available at https://github.com/Pengxin-Guo/FedSA-LoRA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) trained on large amounts of text, referred to as Pre-trained Language Models (PLMs), have become cornerstone of Natural Language Processing (NLP) (Brown, 2020; Touvron et al., 2023; Achiam et al., 2023; Chowdhery et al., 2023). Typically, to adapt PLMs for specific tasks or enhance accuracy in real-world scenarios, fine-tuning PLMs on task-specific data is often needed. However, in many real-world applications, data is distributed across different institutions, and data sharing between these entities is often restricted due to privacy and regulatory concerns. Federated Learning (FL) (McMahan et al., 2017; Li et al., 2020a; Zhang et al., 2021; Kairouz et al., 2021), which utilizes collaborative and decentralized training of models across multiple institutions without sharing personal data externally, offers promising solution to this challenge. Despite its promise, fine-tuning PLMs in an FL system is challenging due to the high computational and storage demands on local clients and the communication overhead involved. To enable finetuning of PLMs in an FL system with limited resources, various Parameter-Efficient Fine-Tuning (PEFT) techniques have been explored. These include adapter-tuning-based methods (Houlsby et al., 2019; Zhang et al., 2024), prompt-tuning-based methods (Li & Liang, 2021; Guo et al., 2023b; Che et al., 2023; Guo et al., 2023a; Qiu et al., 2024; Li et al., 2024; Deng et al., 2024; Sun et al., 2024a; Cui et al., 2024; Cao et al., 2024), and LoRA-based methods (Hu et al., 2022; Yi et al., 2023; Liu et al., 2023; Yang et al., 2024; Qi et al., 2024; Cho et al., 2023; Byun & Lee, 2024; Chen et al., 2024; Sun et al., 2024b; Lin et al., 2024; Wu et al., 2024; Wang et al., 2024b). Among these, LoRAbased methods have gained significant attention due to their efficiency, effectiveness, and flexibility, which is also the focus of our work. However, aggregating LoRA matrices and in FL setting poses key problem. Directly aggregating the and matrices on the server and then broadcasting them to each client may introduce Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The illustration of (a) LoRA, (b) FFA-LoRA, and (c) FedSA-LoRA. In LoRA, both and matrices are trainable and shared with the server for aggregation. In FFA-LoRA, only matrices are trainable and shared with the server for aggregation, while matrices are fixed after initialization. In FedSA-LoRA, both and matrices are trainable, but only matrices are shared with the server for aggregation while matrices are kept locally. aggregation errors. Specifically, in an FL task with clients, each clients model update is represented by two low-rank matrices Ai and Bi introduced by LoRA. After server aggregation and broadcast, the model update of each client is: 1 (B1 + B2 + + Bm) 1 (A1 + A2 + + Am), (1) which is different from the ideal model update, i.e., 1 (B1A1 + B2A2 + + BmAm). To solve this problem, some methods have been explored (Sun et al., 2024b; Wang et al., 2024b). For example, Sun et al. (2024b) propose Federated Freeze-A LoRA (FFA-LoRA), which freezes the matrices and only updates and aggregates the matrices, as illustrated in Figure 1(b). Thus, the local update of each client under FFA-LoRA is 1 (B1 + B2 + + Bm)A0, where A0 denotes the initialized and fixed weights. They point out that this term is equal to the ideal model update introduced by FFA-LoRA, i.e., 1 (B1A0 +B2A0 + +BmA0). However, fixing matrices can impair the learning ability of LoRA and result in suboptimal performance (Zhang et al., 2023). Meanwhile, many works have demonstrated that uniform model update for all clients is not optimal, especially under the non-IID scenario (Zhao et al., 2018; Zhu et al., 2021; Li et al., 2022). To this end, we aim to explore better way to combine LoRA and FL in this work and move beyond the constraint that the model update of each client should be the same. To achieve this, we start by analyzing the distinct roles of the learned and matrices when combining LoRA with FL, resulting in Lemma 1. This lemma suggests that when combing LoRA with FL, matrices are responsible for learning general knowledge while matrices focus on capturing client-specific knowledge. To verify this empirically, we locally fine-tuned RoBERTa-large model (Liu et al., 2019) with LoRA (Hu et al., 2022) on the RTE task from the GLUE benchmark (Wang et al., 2018) with three clients under different levels of data heterogeneity. The results, illustrated in Figure 2, show that the learned matrices are more similar across clients than the matrices, and with increased data heterogeneity, the similarity of matrices between different clients decreases. These results demonstrate our argument that matrices are used to learn general knowledge while matrices focus on modeling client-specific knowledge. Based on our findings, we introduce the Federated Share-A Low-Rank Adaptation (FedSA-LoRA) method in this work. Similar to LoRA (Hu et al., 2022), we utilize two trainable low-rank matrices, denoted as and B, to model the weight updates during local training. However, only the matrices are shared with the server for aggregation, as illustrated in Figure 1(c). Then, the model update of client after server aggregation and broadcast is: Bi 1 (A1 + A2 + + Am), (2) where the first part Bi is responsible for capturing client-specific knowledge while the second part is used to model general knowledge. By sharing the matrices that learn general knowledge with the server for aggregation, while keeping the matrices that capture client-specific knowledge locally during training, the learning abilities of LoRA combined with FL can be enhanced. Note that this"
        },
        {
            "title": "Preprint",
            "content": "method differs from previous works (Sun et al., 2024b; Wang et al., 2024b) that require each client to share uniform model update. Instead, it allows for different model updates, which is more efficient under the non-IID scenario (Zhao et al., 2018; Zhu et al., 2021; Li et al., 2022). Moreover, we delve into the relationship between the learned and matrices in other LoRA variants, such as rsLoRA (Kalajdzievski, 2023) and VeRA (Kopiczko et al., 2024). The observations, illustrated in Figures 4 and 5 in Appendix, demonstrate similar phenomenon to LoRA. Building upon these insights, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. By extending the proposed method to other LoRA variants, we establish general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. We summarize our contributions as follows: We investigate the relationship between learned and matrices in LoRA and other LoRA variants (e.g., rsLoRA and VeRA) across different clients, delineating their distinct roles. Specifically, matrices are responsible for learning general knowledge, while matrices focus on capturing client-specific knowledge. Building upon our findings, we establish general paradigm for integrating LoRA with FL. Specifically, we introduce Federated Share-A LoRA (FedSA-LoRA), where both and matrices are trainable, but only the matrices are shared with the server for aggregation. We then generalize the FedSA-LoRA framework to other LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. Extensive experimental results demonstrate the superiority of the proposed FedSA-LoRA, FedSA-rsLoRA, and FedSA-VeRA compared to other methods."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 FEDERATED LEARNING Federated Learning (FL) (McMahan et al., 2017; Li et al., 2020a; Zhang et al., 2021; Kairouz et al., 2021), commonly used distributed learning method for tasks requiring privacy, has gained significant attention in recent years. However, its application faces challenges due to the non-IID nature of distributed datasets, resulting in accuracy discrepancies compared to centralized training. Numerous works (Li et al., 2020b; Xu et al., 2023; Yan et al., 2023; Chan et al., 2024; Xu et al., 2024; Zeng et al., 2024) have been proposed to mitigate this performance degradation. Recently, some studies demonstrate that fine-tuning pre-trained models, especially Pre-trained Language Models (PLMs), through FL suffers less from the non-IID issue (Qu et al., 2022; Chen et al., 2023; Nguyen et al., 2023; Weller et al., 2022). The experimental results in (Weller et al., 2022) show that when applying PLMs, even the vanilla FedAvg can achieve performance comparable to centralized training. However, these large-scale PLMs usually introduce significant communication overheads in FL scenarios, leading to slow and impractical federated training in real-world applications. Additionally, local clients are often constrained by limited computational capacity and memory, making the local fine-tuning of PLMs challenging. To enable fine-tuning of PLMs in an FL system with limited resources, various Parameter-Efficient Fine-Tuning (PEFT) techniques have been explored, such as adapter-tuning-based methods (Houlsby et al., 2019; Zhang et al., 2024), prompt-tuningbased methods (Li & Liang, 2021; Guo et al., 2023b; Che et al., 2023; Guo et al., 2023a; Qiu et al., 2024; Li et al., 2024; Deng et al., 2024; Sun et al., 2024a; Cui et al., 2024; Cao et al., 2024), and LoRA-based methods (Hu et al., 2022; Yi et al., 2023; Liu et al., 2023; Yang et al., 2024; Qi et al., 2024; Cho et al., 2023; Byun & Lee, 2024; Chen et al., 2024; Sun et al., 2024b; Lin et al., 2024; Wu et al., 2024; Wang et al., 2024b). Among these, LoRA-based methods have gained significant attention due to their efficiency, effectiveness, and flexibility, which is also the focus of our work. 2.2 LORA IN FEDERATED LEARNING Low-Rank Adaptation (LoRA) (Hu et al., 2022), which introduces low-rank adaptation matrices to simulate gradient updates while keeping the pre-trained model weights frozen, has recently gained significant attention due to its efficiency, effectiveness, and flexibility (Hayou et al., 2024; Liu et al., 2024; Kopiczko et al., 2024; Wang et al., 2024a). With this trait, LoRA can be utilized to mitigate the"
        },
        {
            "title": "Preprint",
            "content": "communication overhead in FL, which primarily relies on the size of model update parameters. Yi et al. (2023) propose FedLoRA, incorporating LoRA in FL to increase model fine-tuning efficiency. Liu et al. (2023) introduce DP-LoRA, ensuring differential privacy in FL for LLMs with minimal communication overhead. Yang et al. (2024) propose dual-personalizing adapter (FedDPA), and Qi et al. (2024) introduce FDLoRA. Both adopt the similar idea where each client contains personalized LoRA module and global LoRA module to capture personalized and global knowledge, respectively. Another line of such work is heterogeneous LoRA. For example, Cho et al. (2023) introduce heterogeneous LoRA, where they deploy heterogeneous ranks across clients, aggregate the heterogeneous LoRA modules through zero-padding, and redistribute the LoRA modules heterogeneously through truncation. However, this simple zero-padding strategy can make the training process unstable (Byun & Lee, 2024). To solve this issue, Byun & Lee (2024) propose replication-based strategy for aggregating rank-heterogeneous LoRA. Chen et al. (2024) propose Rank-Based LoRA Aggregation (RBLA) that performs weighted aggregation for heterogeneous LoRA structures. Wang et al. (2024b) introduce stacking-based aggregation method for heterogeneous LoRA. The most related work to ours is Federated Freeze-A LoRA (FFA-LoRA) (Sun et al., 2024b), which fixes the randomly initialized non-zero matrices and only fine-tunes the zero-initialized matrices to further halve the communication cost. However, since some matrices are fixed, the learning ability of LoRA is impaired, resulting in suboptimal performance (Zhang et al., 2023). In contrast, we propose Federated Share-A LoRA (FedSA-LoRA), where both and matrices are trainable and only the matrices are shared with the server for aggregation."
        },
        {
            "title": "3 MOTIVATING EXAMPLE",
            "content": "Preliminary Building upon the hypothesis that updates to the weights during the fine-tuning exhibit low intrinsic rank, LoRA (Hu et al., 2022) proposes using the product of two low-rank matrices to update the pre-trained weights incrementally. For pre-trained weight matrix W0 Rkd, LoRA models the weight update Rkd utilizing low-rank decomposition, expressed as BA, where Rkr and Rrd represent two low-rank matrices, with min(k, d). During training, W0 is frozen and does not receive gradient updates, while and contain trainable parameters. Consequently, the fine-tuned weight can be represented as: = W0 + BA. The matrix is initialized with random Gaussian distribution, while is initially set to zero, resulting in = BA being zero at the start of training. To analyze the role of learned and matrices, lets consider simple example analogous to single network layer with least-squares linear regression task. Specifically, suppose there is pretrained linear model weight W0 Rkd. With this model held constant, our goal is regressing (xt, yt) pairs where yt is given by: yt = Wtxt, with Wt = W0 + . In LoRA, the target is modeled by low rank update to the pre-trained W0, i.e., = W0 + BA: ˆy = (W0 + BA)xt, where Rkr and Rrd, with min(k, d). Then, the least squares loss is defined on the difference between ˆy and yt: = E(xt,yt)[yt (W0 + BA)xt2 2]. (3) Below, we present the lemma on minimizing this loss while freezing either or B. The proof is provided in Section A.1 in Appendix. Lemma 1. Fine-tuning while fixing = Q, with the goal of optimizing Eq. (3), yields: ]QT (QE[xtxT = E[xtxT (4) Fine-tuning while fixing = and assuming 1 exists, with the goal of optimizing Eq. (3), yields: = 1W . (5) Remark 1. From this lemma, we can conclude that the optimal solution of is independent of the input data distribution, while is related to the input data distribution captured by E[xtxT ]. This indicates that is responsible for learning general knowledge, while focuses on modeling client-specific knowledge. ]QT )1."
        },
        {
            "title": "Preprint",
            "content": "(a) IID (b) moderate non-IID (c) server non-IID (d) IID (e) moderate non-IID (f) server non-IID Figure 2: Mean of pairwise cosine similarity of the learned and matrices across layers of RoBERTa model locally fine-tuned with LoRA on the RTE task, with different levels of data heterogeneity. (a)-(c): value matrices; (d)-(f): query matrices. The learned matrices are more similar across clients than the matrices, and with increased data heterogeneity, the similarity of matrices between different clients decreases. To verify this empirically, we locally fine-tune RoBERTa-large model (Liu et al., 2019) with LoRA (Hu et al., 2022) on the RTE task from the GLUE benchmark (Wang et al., 2018) using three clients. We model an IID data distribution and two non-IID data distributions. The two non-IID distributions are modeled by Dirichlet distribution with α = 1 and α = 0.5, referred to as moderate non-IID and severe non-IID. Figure 2 shows the mean pairwise cosine similarity of the learned and matrices across clients. These results indicate that the learned matrices are more similar across clients than the matrices, and with increased data heterogeneity, the similarity of matrices between different clients decreases. To demonstrate that the matrices are indeed updated, as they are similar across different clients, we further illustrate the difference between the learned and initialized matrices for each client in Figure 3 in Appendix. These results confirm that the matrices are updated. This phenomenon is consistent with previous study about the asymmetry analysis in LoRA (Zhu et al., 2024). Based on these results, we argue that matrices are responsible for learning general knowledge while matrices focus on capturing client-specific knowledge. To demonstrate the generalizability of our findings, we further explore the relationship between the learned and matrices in other LoRA variants, such as rsLoRA (Kalajdzievski, 2023) and VeRA (Kopiczko et al., 2024). The observations, illustrated in Figures 4 and 5 in Appendix, show similar phenomenon to LoRA. In this way, we uncover general phenomenon when combining LoRA with FL, which serves as the foundation for our proposed method."
        },
        {
            "title": "4 OUR METHOD",
            "content": "4.1 FEDERATED SHARE-A LOW RANK ADAPTATION Drawing from the insights of our findings, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), illustrated in Figure 1(c), which utilizes two low-rank trainable matrices and to model the weight update, but only matrices are shared with the server for aggregation. Specifically, similar to LoRA (Hu et al., 2022), we employ two low-rank matrices, namely Rkr and Rrd with min(k, d), to model the weight update Rkd for pre-trained weight matrix W0 Rkd. This approach allows us to represent the fine-tuned weight as W0 + BA. During the local training process, W0 is frozen and does not receive gradient updates, while and contain trainable parameters. Following LoRA (Hu et al., 2022), the matrix is initialized with random Gaussian distribution, whereas is initially set to zero, ensuring that = BA is zero at the start of training. Then, for global aggregation, only the matrices are shared with the server for aggregation. Once the server averages these matrices, they are broadcast to each client for the subsequent training round. By sharing the matrices that learn general knowledge with the"
        },
        {
            "title": "Preprint",
            "content": "server for aggregation, while keeping the matrices that model client-specific knowledge locally, the learning abilities of LoRA combined with FL can be enhanced. Moreover, based on the similar phenomena observed in other LoRA variants (i.e., rsLoRA (Kalajdzievski, 2023) and VeRA (Kopiczko et al., 2024)), we extend the FedSA-LoRA method to these variants, resulting in FedSA-rsLoRA and FedSA-VeRA. Specifically, rsLoRA (Kalajdzievski, 2023) is similar to LoRA, differing only in the scaling factor. Thus, the difference between FedSA-rsLoRA and FedSA-LoRA also lies in the scaling factor. In VeRA (Kopiczko et al., 2024), the low-rank matrices and are initialized using the uniform version of Kaiming initialization, fixed, shared across all layers, and adapted with trainable scaling vectors and b. The vectors are initialized to zero, and the vectors are initialized with value of 0.1. To make the notation consistent with our work, we rewrite the scaling vectors and as Ad and Bb to reflect the position of each scaling vector. Thus, in FedSA-VeRA, only the scaling vector Ad is shared with the server for aggregation, while Bb is trained locally. By extending the proposed method to other LoRA variants, we establish general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL."
        },
        {
            "title": "4.2 CONVERGENCE ANALYSIS",
            "content": "To facilitate the convergence analysis of the proposed method, we make assumptions commonly encountered in the literature (Li et al., 2020c) to characterize the smooth and non-convex optimization landscape. Assumption 1. L1, , Lm are all L-smooth. For all Wj and Wk: 2 Li(Wk) Li(Wj) + Wk Wj, Li(Wj)F + Wk Wj2 . Here, is the number of clients, Wk denotes the model weight of client k, Li is the empirical loss on client i, Li(Wj) represents the gradient of Li with respect to Wj, , is the Frobenius inner product, and denotes the Frobenius norm. Assumption 2. Let ξi,t be sampled from the i-th clients local data uniformly at random at tth training step. The expected squared norm of stochastic gradients is uniformly bounded, i.e., ELi(W (t) ; ξi,t)2 G2, for all = 1, , and = 0, , 1. Here denotes the total number of every clients training steps. Assumption 3. Let (t) A(t) = W0 + B(t) t-th step. There exist constants CB > 0, CA > 0, cB > 0, and cA > 0 such that: represent the model parameters for the i-th client at the B(t) A(t) CB, CA, A(t) B(t) for all = 1, , and = 0, , 1. , Li(W (t) , Li(W (t) A(t) B(t) i )Li(W (t) )Li(W (t) )F cALi(W (t) )F cBLi(W (t) )2 , )2 , Then we present the convergence rate for our method, with the proof provided in Section A.2 in Appendix. Theorem 1. Let Assumptions 1, 2, and 3 hold and L, G, CA, CB, cA, cB be defined therein. Denote as the number of local training iterations between two communication rounds. Then, for learning rate η, we have: 1 mT (cid:88) (cid:88) i=1 t=1 (cid:20)(cid:13) (cid:13)Li(W (t) (cid:13) ) (cid:13) 2 (cid:13) (cid:13) (cid:21) (cid:114) DM , (6) 2 cA + cB 2 G2)η + 3 2 ALG2 + 3 where Li(W (0)) Li(W ) D, i, and (2C 2 3 BLG2 + 2C 4 2 2 According to Theorem 1, we can obtain an O( 1 ) convergence rate towards the stationary solution under smooth and non-convex conditions. This convergence rate is comparable to that of traditional FedAvg in the non-convex scenario (Wang et al., 2020). BG4L + (CACBG2 + BE2LG2)η2 η2. BE2G2 + 1 2 η4C AC"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we evaluate and compare the performance of the proposed method with other methods on two types of tasks: natural language understanding and natural language generation. For the natural language understanding tasks, we use the RoBERTa model (Liu et al., 2019) evaluated on the GLUE benchmark (Wang et al., 2018), including MNLI, SST2, QNLI, QQP, and RTE. For the natural language generation tasks, we employ the LLaMA model (Touvron et al., 2023) evaluated on the GSM8K dataset (Cobbe et al., 2021). Our implementation is based on the FederatedScope-LLM library (Kuang et al., 2023). The experiments for LoRA-based methods are conducted on NVIDIA GeForce RTX 4090 and 3090 GPUs, while the rsLoRA-based and VeRA-based methods are carried out on NVIDIA L40S GPUs. All experiments are performed with half-precision enabled for efficiency."
        },
        {
            "title": "5.1 NATURAL LANGUAGE UNDERSTANDING",
            "content": "For the natural language understanding tasks, similar to FFA-LoRA (Sun et al., 2024a), we randomly split the data across three clients for federated learning. We model non-IID data distribution using Dirichlet distribution with α = 0.5, i.e., Dir (0.5). We use the pre-trained RoBERTa-large (355M) (Liu et al., 2019) from the HuggingFace Transformers library (Wolf et al., 2020) as the base model. For LoRA-based methods optimization, we adopt the SGD optimizer (Ruder, 2016) for all approaches. We set the batch size to 128, local update steps to 10, and total communication rounds to 1000, consistent across all experiments. Similar to Hu et al. (2022), we only apply LoRA to Wq and Wv in the attention layers in our experiments. The rank = 8 and scaling factor α = 16 are fixed for all algorithms. We report the best result from experiments run with learning rates η {5E-3, 1E-2, 2E-2, 5E-2, 1E-1}. The optimization of rsLoRA-based methods is similar to LoRAbased methods, only the learning rates are searched from η {1E-3, 2E-3, 5E-3, 1E-2, 2E-2, 5E-2}. For VeRA-based methods, following VeRA (Kopiczko et al., 2024), we set the rank = 256. We adopt the AdamW optimizer (Loshchilov & Hutter, 2017), introduce separate learning rates for the classification head and the adapted layers, and determine the learning rates through hyperparameter tuning. Other settings are the same as for LoRA-based methods. The learning rates used for each method are shown in Tables 6, 7, and 8 in Appendix. Table 1: Performance of different methods on the GLUE benchmark. MNLI-m denotes MNLI with matched test sets, and MNLI-mm denotes MNLI with mismatched test sets. For all tasks, we report accuracy. Method MNLI-m MNLI-mm SST2 QNLI QQP RTE Avg. LoRA rsLoRA VeRA LoRA FFA-LoRA FedSA-LoRA rsLoRA FFA-rsLoRA FedSA-rsLoRA VeRA FFA-VeRA FedSA-VeRA 88.80 88.83 90.20 88.87 89.09 90.13 85.58 86.03 87.31 88.21 88.24 88.89 88.38 88.66 89.02 85.11 86.17 86. 95.26 94.91 95.92 95.25 95.60 95.80 93.48 93.46 93.63 90.69 91.72 92.00 91.61 91.93 92.27 91.84 92.47 92. 86.32 86.61 87.85 87.16 87.37 88.02 82.33 82.33 82.64 87.34 86.61 88.04 86.38 85.45 87.87 86.29 82.86 87. 89.43 89.48 90.48 89.60 89.68 90.51 87.43 87.26 88.37 The experimental results are shown in Table 1. From this table, we can observe that the proposed FedSA-LoRA, FedSA-rsLoRA, and FedSA-VeRA consistently outperform other methods across all tasks, demonstrating the effectiveness of the proposed method. 5.2 IN-DEPTH ANALYSES In this section, we utilize LoRA-based methods to perform in-depth analyses on the natural language understanding tasks of QNLI, SST2, and MNLI-m to assess the impact of factors such as data heterogeneity, the number of clients, and LoRA rank on model performance."
        },
        {
            "title": "5.2.1 EFFECT OF DATA HETEROGENEITY",
            "content": "To investigate the effect of data heterogeneity on model performance, we model an IID partition (Split-1) and two non-IID partitions with Dir (1) and Dir (0.5). The latter two non-IID partitions are referred to as moderate non-IID (Split-2) and severe non-IID (Split-3). The training settings are the same as in Section 5.1. Table 2: Performance comparison on the QNLI, SST2, and MNLI-m tasks with various degrees of data heterogeneity. Method QNLI SST2 MNLI-m Split-1 Split-2 Split-3 Split-1 Split-2 SplitSplit-1 Split-2 Split-3 LoRA FFA-LoRA 92.92 92.68 FedSA-LoRA 92. 92.44 92.29 93.32 90.69 91.72 92.00 95.30 95.87 96. 95.53 95.47 96.24 95.26 94.91 95.92 88.52 88.15 89. 88.35 88.03 89.71 88.80 88.83 90.20 The results are provided in Table 2. From these results, we can observe that the proposed FedSALoRA consistently outperforms other baselines, demonstrating its adaptability and robustness in various heterogeneous data scenarios. Additionally, as data heterogeneity increases, the improvement of the proposed method also increases. Specifically, FedSA-LoRA improves accuracy by 0.03%, 0.88%, and 1.84% on the QNLI task from IID to severe non-IID compared with LoRA, and by 1.05%, 1.36%, and 1.4% on the MNLI-m task. This indicates that the proposed method is more effective when non-IID conditions are more severe. 5.2.2 EFFECT OF NUMBER OF CLIENTS In Section 5.1, we demonstrated the effectiveness of the proposed method on small number of clients, i.e., three clients. In this section, we show the superiority of FedSA-LoRA compared to other baselines on larger number of clients, i.e., from 10 to 100 clients. Specifically, we use the same non-IID split, i.e., Dir (0.5), to divide the data into 10, 20, and 100 clients. The training settings are the same as in Section 5.1 and the results are shown in Table 3. Table 3: Performance comparison on the QNLI, SST2, and MNLI-m tasks with different number of clients. We apply full participation for FL system with 10 and 20 clients, and apply client sampling with rate 0.3 for FL system with 100 clients. Method QNLI SST2 MNLI-m 10 clients 20 clients 100 clients 10 clients 20 clients 100 clients 10 clients 20 clients 100 clients LoRA FFA-LoRA FedSA-LoRA 91.32 91. 91.97 91.23 91.70 92.54 90.32 91.27 91.48 96.68 96. 96.83 93.16 93.31 94.21 96.68 96.33 97.02 86.94 86. 88.59 88.50 88.60 89.05 88.13 87.86 88.82 It can be concluded that FedSA-LoRA not only outperforms other methods with small number of clients (i.e., 3 clients) but also shows superior performance with large number of clients (i.e., from 10 to 100 clients), demonstrating the adaptability and robustness of the proposed FedSA-LoRA across various client numbers. 5.2.3 EFFECT OF LORA RANK The adapter parameter budget (i.e., rank r) is key factor in LoRA performance. In this section, we experiment with rank {2, 4, 8, 16} on the QNLI, SST2, and MNLI-m tasks to test its influence on model performance, keeping other settings unchanged compared to Section 5.1. The results, as shown in Table 4, demonstrate that the proposed FedSA-LoRA outperforms other methods across various LoRA rank values, showcasing the adaptability and robustness of FedSALoRA in different scenarios."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Performance comparison on the QNLI, SST2, and MNLI-m tasks with different LoRA ranks r. Rank Method QNLI SST2 MNLI-m Rank Method QNLI SST2 MNLI-m = 2 = 8 92.07 LoRA FFA-LoRA 91.02 FedSA-LoRA 92.69 90.69 LoRA 91.72 FFA-LoRA FedSA-LoRA 92.00 94.69 93.95 95. 95.26 94.91 95.92 88.29 87.98 89.14 88.80 88.83 90.20 = 4 = 16 92.71 LoRA FFA-LoRA 92.97 FedSA-LoRA 93. 90.59 LoRA 91.62 FFA-LoRA FedSA-LoRA 92.03 94.20 94.61 95.69 94.98 94.13 95.78 88.43 88.24 88.98 88.78 89.25 89."
        },
        {
            "title": "5.3 NATURAL LANGUAGE GENERATION",
            "content": "For the natural language generation tasks, we adopt the pre-trained LLaMA3-8B (Meta, 2024) from the HuggingFace Transformers library (Wolf et al., 2020), using the GSM8K dataset (Cobbe et al., 2021) for evaluation. Following (Kuang et al., 2023), we split the data into three clients under an IID distribution, and other optimization hyperparameters are the same as in that work. The results shown in Table 5 demonstrate that the proposed FedSA-LoRA outperforms other methods in complex natural language generation tasks. From the given example, it can be seen that both LoRA and FFA-LoRA have reasoning errors, but FedSA-LoRA can reason accurately, demonstrating the superiority of the proposed method. Table 5: Performance of different methods on the GSM8K dataset and example answers generated by each method. Accuracy Question: Output: LoRA 46.24 FFA-LoRA 46.32 FedSA-LoRA 46. In student council election, candidate got 20% of the votes while candidate got 50% more than candidate As votes. The rest of the votes was given to candidate C. If there were 100 voters, how many votes did candidate get? Candidate got 20% of the votes. That is 20% of 100 = 20% 100 = 20 100 = 2000 votes. Candidate got 50% more than candidate A. That is 50% of 2000 = 50% 2000 = 50 2000 = 10000 votes. The rest of the votes was given to candidate C. So there are 10000 + 2000 = 12000 votes in total. Candidate got 10000 votes. The answer is 10000. [] Candidate got 20% of 100 = 100 20% = 100 20/100 = 20. Candidate got 20% more than candidate As votes, which is 20% + 20% = 40%. 40% of 100 = 100 40% = 100 40/100 = 40. So the two candidates together got 20 + 40 = 60. The rest of the votes were given to candidate C. So candidate got 100 - 60 = 40. The answer is 40. [] Candidate got 20% of the votes or 20% 100 = 20 votes. Then candidate got 50% more than candidate As 20 votes so 50% of 20 is 20 50% = 10. Then candidate got 20 + 10 = 30 votes. So candidate got 100 - 20 - 30 = 50 votes. The answer is 50. []"
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we discover that when combining LoRA with FL, matrices are responsible for learning general knowledge, while matrices focus on capturing client-specific knowledge. Building upon this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two low-rank trainable matrices and to model the weight update, but only matrices are shared with the server for aggregation. By sharing the matrices that learn general knowledge with the server for aggregation, while keeping the matrices that model client-specific knowledge"
        },
        {
            "title": "Preprint",
            "content": "locally, the learning abilities of LoRA combined with FL can be enhanced. Moreover, we explore the relationship between the learned and matrices in other LoRA variants, such as rsLoRA and VeRA, revealing consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. By doing so, we establish general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Tom Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. Yuji Byun and Jaeho Lee. Towards federated low-rank adaptation with rank-heterogeneous communication. arXiv preprint arXiv:2406.17477, 2024. Linxiao Cao, Yifei Zhu, and Wei Gong. ated fine-tuning for large pre-trained models over resource-limited devices. arXiv:2407.17533, 2024. Sfprompt: Communication-efficient split federarXiv preprint Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan JIANG, and Edith CH Ngai. Internal crosslayer gradients for extending homogeneity to heterogeneity in federated learning. In The Twelfth International Conference on Learning Representations, 2024. Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor Sheng, Huaiyu Dai, and Dejing Dou. Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78717888, 2023. Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han Wei Shen, and Wei-Lun Chao. On the importance and applicability of pre-training for federated learning. In The Eleventh International Conference on Learning Representations, 2023. Shuaijun Chen, Omid Tavallaie, Niousha Nazemi, and Albert Y. Zomaya. Rbla: Rank-based-loraaggregation for fine-tuning heterogeneous models in flaas, 2024. Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, Matt Barnes, and Gauri Joshi. Heterogeneous In International Workshop on lora for federated fine-tuning of on-device foundation models. Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tianyu Cui, Hongxia Li, Jingya Wang, and Ye Shi. Harmonizing generalization and personalization in federated prompt learning. In Forty-first International Conference on Machine Learning, 2024. Wenlong Deng, Christos Thrampoulidis, and Xiaoxiao Li. Unlocking the potential of prompt-tuning in bridging generalized and personalized federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60876097, 2024. Tao Guo, Song Guo, and Junxiao Wang. Pfedprompt: Learning personalized prompt for visionlanguage models in federated learning. In Proceedings of the ACM Web Conference 2023, pp. 13641374, 2023a."
        },
        {
            "title": "Preprint",
            "content": "Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and Wenchao Xu. Promptfl: Let federated participants cooperatively learn prompts instead of models-federated learning in age of foundation model. IEEE Transactions on Mobile Computing, 2023b. Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. In Forty-first International Conference on Machine Learning, 2024. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pp. 27902799. PMLR, 2019. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, In International Conference on et al. Lora: Low-rank adaptation of large language models. Learning Representations, 2022. Peter Kairouz, Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210, 2021. Damjan Kalajdzievski. rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Asano. Vera: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, 2024. Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-llm: comprehensive package for fine-tuning large language models in federated learning. arXiv preprint arXiv:2309.00363, 2023. Hongxia Li, Wei Huang, Jingya Wang, and Ye Shi. Global and local prompts cooperation via optimal transport for federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1215112161, 2024. Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In 2022 IEEE 38th international conference on data engineering (ICDE), pp. 965978. IEEE, 2022. Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):5060, 2020a. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429450, 2020b. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2020c. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, 2021. Zheng Lin, Xuanjie Hu, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Ang Li, Praneeth Vepakomma, and Yue Gao. Splitlora: split parameter-efficient fine-tuning framework for large language models. arXiv preprint arXiv:2407.00952, 2024. Shih-yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, Matt White, and Meikang Qiu. Differentially private low-rank adaptation of large language model using federated learning. ACM Transactions on Management Information Systems, 2023."
        },
        {
            "title": "Preprint",
            "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 12731282. PMLR, 2017. AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael Rabbat. Where to begin? on the impact of pre-training and initialization in federated learning. In The Eleventh International Conference on Learning Representations, 2023. Jiaxing Qi, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, and Depei Qian. Fdlora: Personalized federated learning of large language model via dual lora tuning. arXiv preprint arXiv:2406.07925, 2024. Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, and Wan-Yi Lin. Federated text-driven prompt generation for vision-language models. In The Twelfth International Conference on Learning Representations, 2024. Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, and Daniel Rubin. Rethinking architecture design for tackling data heterogeneity in federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1006110071, 2022. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yudong Liu, Zhixu Du, Yiran Chen, and Holger Roth. Fedbpt: Efficient federated black-box prompt tuning for large language models. In Forty-first International Conference on Machine Learning, 2024a. Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. Improving lora in privacy-preserving federated learning. In The Twelfth International Conference on Learning Representations, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, 2018. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:76117623, 2020. Xuehao Wang, Feiyang Ye, and Yu Zhang. Task-aware low-rank adaptation of segment anything model. arXiv preprint arXiv:2403.10971, 2024a. Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, and Ang Li. Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations. arXiv preprint arXiv:2409.05976, 2024b. Orion Weller, Marc Marone, Vladimir Braverman, Dawn Lawrie, and Benjamin Van Durme. PreIn Proceedings of the 2022 Conference of trained models for multilingual federated learning. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 14131421, 2022."
        },
        {
            "title": "Preprint",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art In Proceedings of the 2020 conference on empirical methods in natural language processing. natural language processing: system demonstrations, pp. 3845, 2020. Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, and Jing Gao. Fedbiot: Llm local fine-tuning in federated learning without full model. arXiv preprint arXiv:2406.17706, 2024. Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized federated learning with feature alignment and classifier collaboration. In The Eleventh International Conference on Learning Representations, 2023. Peiran Xu, Zeyu Wang, Jieru Mei, Liangqiong Qu, Alan Yuille, Cihang Xie, and Yuyin Zhou. Fedconv: Enhancing convolutional neural networks for handling data heterogeneity in federated learning. Transactions on Machine Learning Research, 2024. Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Lei Xing, and Yuyin Zhou. Label-efficient self-supervised federated learning for tackling data heterogeneity in medical imaging. IEEE Transactions on Medical Imaging, 42(7):19321943, 2023. Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. Dual-personalizing adapter for federated foundation models. arXiv preprint arXiv:2403.19211, 2024. Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. Fedlora: Model-heterogeneous personalized federated learning with lora tuning. arXiv preprint arXiv:2310.13283, 2023. Shuang Zeng, Pengxin Guo, Shuai Wang, Jianbo Wang, Yuyin Zhou, and Liangqiong Qu. arXiv preprint Tackling data heterogeneity in federated learning via loss decomposition. arXiv:2408.12300, 2024. Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. survey on federated learning. Knowledge-Based Systems, 216:106775, 2021. Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303, 2023. Zixin Zhang, Fan Qi, and Changsheng Xu. Enhancing storage and computational efficiency in federated multimodal learning for large-scale models. In Forty-first International Conference on Machine Learning, 2024. Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018. Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: survey. Neurocomputing, 465:371390, 2021. Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Saez de Ocariz Borde, Rickard Bruel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, and Justin Solomon. Asymmetry in low-rank adapters of foundation models. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOF OF LEMMA 1 Proof. Consider fine-tuning while freezing = Q. The loss function in Eq. (3) becomes: Then, the gradient of Eq. (7) w.r.t. is: = E(xt,yt)[yt (W0 + BQ)xt2 2]. B = = E(xt,yt)[yt (W0 + BQ)xt2 2] E[Wtxt (W0 + BQ)xt2 2] = = E[(W0 + )xt (W0 + BQ)xt2 2] E[(W BQ)xt2 2] = E[2[(W BQ)xt](xT = E[2(BQ )xtxT QT ]. QT )] To obtain the optimal B, we set Eq. (8) to zero, which means: 2BQE[xtxT 2BQE[xtxT E[2(BQ )xtxT ]QT 2W E[xtxT ]QT 2W E[xtxT BQE[xtxT QT ] = 0 ]QT = 0 ]QT = 0 ]QT = E[xtxT = E[xtxT ]QT ]QT (QE[xtxT ]QT )1. Therefore, we obtain = E[xtxT When fine-tuning with fixed = . The loss function in Eq. (3) becomes: ]QT (QE[xtxT ]QT )1. Then, the gradients of Eq. (7) w.r.t. is: = E(xt,yt)[yt (W0 + A)xt2 2]. A = = E(xt,yt)[yt (W0 + A)xt2 2] E[Wtxt (W0 + A)xt2 2] = E[(W0 + )xt (W0 + BQ)xt2 2] E[(W A)xt2 2] = E[2U [(W A)xt]xT ] = To obtain the optimal A, we set Eq. (11) to zero, which means: 2U E[xtxT E[2U [(W A)xt]xT ] 2U AE[xtxT AE[xtxT ] = 0 ] = 0 ] = W E[xtxT ] = 1W . Thus, we obtain = 1W . 14 (7) (8) (9) (10) (11) (12)"
        },
        {
            "title": "Preprint",
            "content": "A.2 PROOF OF THEOREM 1 = W0 + B(t) Proof. Let (t) be the model parameters maintained in the i-th client at the t-th step. Let IE be the set of global synchronization steps, i.e., IE = {nE = 1, 2, }. If + 1 IE, which represents the time step for communication, then the one-step update of the proposed method for the i-th client can be described as follows: A(t) (cid:33) (cid:32) B(t) A(t) update of B(t) andA(t) (cid:32) B(t+1) A(t+1) (cid:33) (cid:32) if t+1IE B(t+1) (cid:80)m j=1 A(t+1) 1 (cid:33) . For convenience, we denote the parameters in each sub-step above as follows: = W0 + B(t) (t) A(t) = W0 + B(t+1) (t) = W0 + B(t+1) (t) , A(t+1) 1 (cid:88) j=1 , A(t+1) , (t+1) = (cid:40) (t) (t) if + 1 / IE, if + 1 IE. Here, the variable (t) represents the immediate result of one sub-step update from the parameter of the previous sub-step (t) represents the parameter obtained after communication steps (if applicable). Furthermore, we denote the learning rate for the i-th client at the t-th step as ηi,t, and the stochastic gradient at step as follows: , and (t) i BLi(W (t) ALi(W (t) , ξi,t) = Li(W (t) , ξi,t) = B(t) W Li(W (t) , ξi,t), , ξi,t)A(t) , BLi(W (t) ALi(W (t) ) = E[BLi(W (t) ) = E[ALi(W (t) , ξi,t)], , ξi,t)], where ξi,t is the data uniformly chosen from the local data set of client at step t. Next, we apply the inequality from the smoothness Assumption 1 to each sub-step of the one-step update for client i. Firstly, by the smoothness of Li, we have: Li(U (t) ) Li(W (t) ) + (t) (t) , Li(W (t) )F + 2 (cid:13) (cid:13)U (t) (cid:13) (t) (cid:13) 2 (cid:13) (cid:13) . (13) Since and B(t+1) A(t+1) = B(t) = B(t) ηi,tBLi(W (t) ηi,tW Li(W (t) , ξi,t) , ξi,t)A(t) , = A(t) = A(t) ηi,tALi(W (t) ηi,tB(t) , ξi,t) Li(W (t) , ξi,t), we have: (t) (t) = B(t+1) A(t+1) (cid:18) B(t) A(t) ηi,tW Li(W (t) B(t) = , ξi,t)A(t) (cid:19) (cid:18) A(t) ηi,tB(t) Li(W (t) (cid:19) , ξi,t) B(t) A(t) = η2 i,tW Li(W (t) B(t) ηi,tB(t) , ξi,t)A(t) B(t) W Li(W (t) i , ξi,t). Li(W (t) , ξi,t) ηi,tW Li(W (t) , ξi,t)A(t) A(t)"
        },
        {
            "title": "Preprint",
            "content": "Then, for the second term on the right side of Eq. (13), according to the law of total expectation, we have: (cid:104)(cid:68) (t) (t) (cid:20)(cid:28) , Li(W (t) (cid:105) (cid:69) ) B(t) = η2 i,t W Li(W (t) , ξi,t)A(t) Li(W (t) , ξi,t), Li(W (t) (cid:21) (cid:29) ) ηi,tE ηi,tE (cid:20)(cid:28) Li(W (t) , ξi,t)A(t) A(t) , Li(W (t) (cid:29) ) (cid:20)(cid:28) B(t) B(t) Li(W (t) , ξi,t), Li(W (t) (cid:29) ) (cid:21) (cid:21) (cid:28) = η2 i,t Li(W (t) )A(t) B(t) Li(W (t) ), Li(W (t) ) (cid:29) (cid:28) (cid:28) ηi,t ηi,t Li(W (t) )A(t) A(t) , Li(W (t) (cid:29) ) B(t) B(t) Li(W (t) ), Li(W (t) (cid:29) ) . Since (cid:28) Li(W (t) )A(t) B(t) Li(W (t) ), Li(W (t) (cid:29) ) )A(t) (cid:13) (cid:13) Li(W (t) (cid:13) (cid:13) (cid:13) (cid:13)A(t) (cid:13) CACBG3, and if we assume there exists cA > 0 such that t: (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) (cid:13)B(t) (cid:13) B(t) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13) 3 (cid:13) ) (cid:13) W Li(W (t) (cid:13) (cid:13) ) (cid:13) (cid:13) (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) (cid:13) ) (cid:13)F (cid:68) A(t) A(t) , Li(W (t) )W Li(W (t) (cid:69) ) cA (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) , then we have: (cid:28) Li(W (t) )A(t) A(t) , Li(W (t) (cid:29) ) (cid:104)(cid:16) = Tr Li(W (t) )A(t) (cid:17) A(t) Li(W (t) ) (cid:105) (cid:104) = Tr A(t) A(t) W Li(W (t) )W Li(W (t) ) (cid:105) = A(t) (cid:68) A(t) (cid:13) (cid:13)W Li(W (t) (cid:13) , Li(W (t) (cid:13) 2 (cid:13) ) (cid:13) . F cA )W Li(W (t) (cid:69) ) And similarly if we assume there exists cB > 0 such that t: (cid:68) B(t) B(t) , Li(W (t) )W Li(W (t) ) (cid:69) cB (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) , (cid:28) then we have: B(t) B(t) Li(W (t) ), Li(W (t) (cid:29) ) (cid:104)(cid:16) = Tr B(t) B(t) Li(W (t) (cid:17) ) Li(W (t) (cid:105) ) (cid:104) = Tr Li(W (t) )B(t) B(t) Li(W (t) ) (cid:105) (cid:104) = Tr (cid:28) = Li(W (t) )W Li(W (t) )B(t) B(t) W Li(W (t) )W Li(W (t) ), B(t) B(t) cB (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) , 16 (cid:105) (cid:29) (14)"
        },
        {
            "title": "Preprint",
            "content": "where we use the cyclic property of the trace for the third equality above. We further get: (cid:104)(cid:68) (t) (t) η i,tCACBG3 ηi,tc2 (cid:69) (cid:105) ) , Li(W (t) (cid:13) (cid:13)W Li(W (t) (cid:13) ) (cid:13) 2 (cid:13) (cid:13) ηi,tc2 (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) . (15) Similarly, we know: 2 E[U (t) ] = E[η2 (t) i,tW Li(W (t) ηi,tB(t) ηi,tW Li(W (t) BG2. i,tC 2 AC 2 Plugging Eq. (14), Eq. (15), and Eq. (16) into Eq. (13), we have: B(t) A(t) AG2 + 3η2 , ξi,t)A(t) , ξi,t)A(t) i,tC 2 BG4 + 3η2 Li(W (t) B(t) 3η4 i,tC , ξi,t) Li(W (t) , ξi,t)2 ] (16) (cid:17) (cid:16) (t) Li Li (cid:17) (cid:16) (t) + η2 i,tCACBG3 ηi,tcA (cid:13) 2 (cid:13) ) (cid:13) ηi,tcB (cid:13) (cid:13)W Li(W (t) (cid:13) ) (cid:13) 2 (cid:13) (cid:13) + 3 2 i,tC 2 η4 AC BG4L + 3 2 i,tC 2 η2 AG2L + (cid:13) (cid:13)W Li(W (t) (cid:13) i,tC 2 η2 BG2L. 3 2 Secondly, by the smoothness of Li, we have: (cid:17) (cid:16) (t) Li Li (cid:16) (U (t) (cid:17) (cid:68) (t) (t) + , Li(U (t) ) (cid:69) + 2 (cid:13) (cid:13)V (t) (cid:13) (t) (17) (18) (cid:13) 2 (cid:13) (cid:13) . Since and we know: (t) (t) = B(t+1) A(t+1) 1 B(t+1) (cid:88) j=1 A(t+1) = B(t+1) 1 (cid:88) (cid:16) j=1 A(t+1) A(t+1) (cid:17) , A(t+1) = A(tE+1) = A(tE+1) (cid:88) t0=tE+1 (cid:88) t0=tE+1 ηj,t0 ALj(W (t0) ; ξj,t0 ) ηj,t0 B(t0) Lj(W (t0) ; ξj,t0), (t) (t) = B(t+1) (cid:16) 1 (cid:88) (cid:88) j=1 t0=tE+ ηj,t0B(t0) (cid:0)Li(W (t0) ; ξi,t0) Lj(W (t0) ; ξj,t0 )(cid:1)(cid:17) . Therefore, (t) (cid:105) (cid:13) 2 (cid:13) (cid:13) (cid:104)(cid:13) (cid:13)V (t) (cid:13) (cid:104)(cid:13) (cid:13)B(t+1) (cid:13) = (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) (cid:88) j=1 t0=tE+1 ηj,t0B(t0) (cid:0)Li(W (t0) ; ξi,t0) Lj(W (t0) ; ξj,t0 )(cid:1)(cid:13) 2 (cid:13) (cid:13) (cid:105) 2 E 4 BE (cid:88) (cid:88) j=1 t0=tE+ (cid:88) (cid:88) j=1 t0=tE+1 η2 j,t0 η2 j,t (cid:104)(cid:13) (cid:13)B(t0) (cid:13) (cid:0)Li(W (t0) ; ξi,t0 ) Lj(W (t0) ; ξj,t0)(cid:1)(cid:13) 2 (cid:13) (cid:13) (cid:105) (cid:104)(cid:13) (cid:13)Li(W (t0) (cid:13) ; ξi,t0 ) Lj(W (t0) (cid:105) (cid:13) 2 (cid:13) ; ξj,t0) (cid:13) 4C 4 BEG2 (cid:88) (cid:88) j=1 t0=tE+ η2 j,t0 , 17 (19)"
        },
        {
            "title": "Preprint",
            "content": "where we use Assumption 2 to derive that: (cid:104)(cid:13) (cid:13)Li(W (t0) (cid:13) (cid:104)(cid:13) (cid:13)Li(W (t0) (cid:13) (cid:13) 2 (cid:13) ; ξj,t0 ) (cid:13) ; ξi,t0) Lj(W (t0) (cid:104)(cid:13) (cid:105) (cid:13)Li(W (t0) (cid:13) (cid:13) 2 (cid:13) ; ξi,t0 ) (cid:13) + 2E (cid:105) i 2E (cid:105) (cid:13) 2 (cid:13) ; ξj,t0) (cid:13) 4G2. Furthermore, (cid:68) (t) (t) , Li(U (t) (cid:69) ) 1 2ηi,t (cid:13) (cid:13)V (t) (cid:13) (t) (cid:13) 2 (cid:13) (cid:13) + 1 2 ηi,t (cid:13) (cid:13)W Li(U (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) Plugging Eq. (19) and Eq. (20) into Eq. (18), we have (choose constant learning rate ηi,t = η): 2C 4 BEG2 ηi,tm (cid:88) (cid:88) j=1 t0=tE+1 η2 j,t0 + 1 2 ηi,tG2. BE2G2L. Combining Eq. (17) and Eq. (21), we have (choose constant learning rate ηi,t = η): BE2G2 + Li Li G2(cid:1)η + 2η2C 4 + (cid:0)2C 2 (t) (cid:17) (cid:16) (t) (cid:16) (cid:17) 1 2 (20) (21) (cid:16) Li (t+1) (cid:17) Li (cid:16) (W (t) (cid:17) + (2C 2 BE2G2 + 3 + (CACBG2 + 2 (cid:13) (cid:13)W Li(W (t) (cid:13) η(cA + cB) ALG2 + 1 2 3 2 (cid:13) 2 (cid:13) ) (cid:13) , G2)η + η4C 2 3 2 BLG2 + 2C 2 AC 2 BG4L BE2LG2)η2 which is equivalent to: η(cA + cB) (cid:13) (cid:13)W Li(W (t) (cid:13) ) (cid:13) 2 (cid:13) (cid:13) Li (cid:16) (W (t) (cid:17) Li (cid:16) (t+1) (cid:17) η4C 2 AC 2 BG4L + (2C 2 + (CACBG2 + G2)η + 1 BE2G2 + 2 3 2 2 2 G2)η + 3 2 η4C 2 3 2 ALG2 + AC 2 3 2 2 BLG2 + 2C 4 BG4L + (CACBG2 + BE2LG2)η2. 2 2 ALG2 + Choosing which satisfies (2C 2 3 2 2 BE2G2 + 1 BE2LG2)η2 η2, we get: BLG2 + 2C 4 (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) (cid:16) Li (cid:17) Li (W (t) η(cA + cB) (cid:16) (t+1) (cid:17) + η cA + cB . (22) Now, by repeatedly applying Eq. (22) for different values of and summing up the results, we get: (W (1) η(c2 (cid:13) (cid:13)W Li(W (t) (cid:13) Li (W ) cA + cB (cid:13) 2 (cid:13) ) (cid:13) (cid:88) + η Li T. (cid:17) (cid:16) (23) + c2 B) t=1 Dividing both side of Eq. (23) by , we get: 1 (cid:88) t=1 (cid:13) (cid:13)W Li(W (t) (cid:13) ) (cid:13) 2 (cid:13) (cid:13) (cid:16) Li (W (1) (cid:17) Li (W ) η(cA + cB)T + η cA + cB . Let us assume that Li(W (1) ) Li(W 1 (cid:88) t=1 Thus, we can obtain: ) D, i, and we set η = (cid:114) (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) 2 cA + cB (cid:113) DM . . Then, we have: 1 mT (cid:88) (cid:88) i=1 t=1 (cid:13) (cid:13)W Li(W (t) (cid:13) (cid:13) 2 (cid:13) ) (cid:13) 2 cA + cB (cid:114) DM . 18 (24) (25) (26)"
        },
        {
            "title": "Preprint",
            "content": "A.3 HYPERPARAMETERS Tables 6 and 7 show the learning rates used for LoRA-based methods and rsLoRA-based methods, respectively. For the VeRA-based methods, we first tried using the SGD optimizer (Ruder, 2016) with search learning rate from η {5E-3, 1E-2, 2E-2, 5E-2, 1E-1} as adopted in LoRA-based methods, but we found the performance to be significantly worse than that of LoRA-based methods. For example, the best performance among the three VeRA-based methods (i.e., VeRA, FFA-VeRA, and FedSA-VeRA) is 53.73% on the MNLI-m task, which is significantly worse than that of the LoRA-based methods (90.20%). Thus, we chose the AdamW optimizer (Loshchilov & Hutter, 2017) and introduced separate learning rates for the classification head and the adapted layers as used in VeRA (Kopiczko et al., 2024). The learning rates used for VeRA-based methods are shown in Table 8. Table 6: The learning rates used for LoRA-based methods on the GLUE benchmark."
        },
        {
            "title": "Method",
            "content": "MNLI-m MNLI-mm SST2 QNLI QQP RTE LoRA FFA-LoRA FedSA-LoRA 1E-2 5E-2 2E-2 1E-2 5E-2 2E-2 2E-2 5E-2 1E-2 1E-2 2E-2 5E1E-2 5E-2 2E-2 1E-2 2E-2 1E-2 Table 7: The learning rates used for rsLoRA-based methods on the GLUE benchmark. Method MNLI-m MNLI-mm SST2 QNLI QQP RTE rsLoRA FFA-rsLoRA FedSA-rsLoRA 5E-3 2E-2 5E-3 5E-3 2E-2 5E-3 1E-2 2E-2 5E-3 2E-3 1E-2 1E-3 5E-3 2E-2 2E-3 2E-3 1E-2 2ETable 8: The learning rates used for VeRA-based methods on the GLUE benchmark. Method Position MNLI-m MNLI-mm SST2 QNLI QQP RTE VeRA FFA-VeRA FedSA-VeRA VeRA Head VeRA Head VeRA Head 1E-2 6E-3 2E-2 2E-3 2E-3 3E1E-2 6E-3 2E-2 2E-3 2E-3 3E-5 2E-2 2E-3 1E-2 6E-3 1E-2 3E2E-3 3E-4 1E-2 2E-4 1E-2 3E-4 2E-3 3E-4 1E-2 6E-3 2E-3 3E1E-2 2E-4 1E-2 2E-4 1E-2 1E-4 A.4 LEARNED MATRICES COMPARISON A.4.1 LEARNED AND INITIALIZED LORA MATRICES COMPARISON Since the learned LoRA matrices are similar across different clients in Figure 2, we illustrate the difference between the learned and initialized matrices for each client under the IID partition in this section. The results, shown in Figure 3, confirm that the matrices are updated. A.4.2 LEARNED RSLORA MATRICES COMPARISON In this section, we present the mean of pairwise client relationships for the learned rsLoRA (Kalajdzievski, 2023) matrices. These results, shown in Figure 4, demonstrate similar phenomenon to the learned LoRA matrices. That is, the learned matrices are more similar across clients than the matrices, and with increased data heterogeneity, the similarity of matrices between different clients decreases."
        },
        {
            "title": "Preprint",
            "content": "(a) Client 1 & Init (b) Client 2 & Init (c) Client 3 & Init (d) Client 1 & Init (e) Client 2 & Init (f) Client 3 & Init Figure 3: Cosine similarity of learned and initialized matrices across layers of different clients of RoBERTa model locally fine-tuned with LoRA on the RTE task. (a)-(c): value matrices; (d)-(f): query matrices. The learned matrices are different from the initialized matrices, indicating that the matrices are updated. (a) IID (b) moderate non-IID (c) server non-IID (d) IID (e) moderate non-IID (f) server non-IID Figure 4: Mean of pairwise cosine similarity of the learned and matrices across layers of RoBERTa model locally fine-tuned with rsLoRA on the RTE task, with different levels of data heterogeneity. (a)-(c): value matrices; (d)-(f): query matrices. The learned matrices are more similar across clients than the matrices, and with increased data heterogeneity, the similarity of matrices between different clients decreases. A.4.3 LEARNED VERA MATRICES COMPARISON In this section, we show the mean of pairwise client relationships for the learned VeRA (Kopiczko et al., 2024) matrices. In VeRA, the low-rank matrices and are initialized using the uniform version of Kaiming initialization, fixed, shared across all layers, and adapted with trainable scaling vectors and b. The vectors are initialized to zero, and the vectors are initialized with value of 0.1. To make the notation consistent with our work, we rewrite the scaling vectors and as Ad and Bb to reflect the position of each scaling vector. These results, illustrated in Figure 5, demonstrate similar phenomenon to the learned LoRA matrices. That is, the learned scaling vectors Ad are more similar across clients than the scaling vectors Bb, and with increased data heterogeneity, the similarity of scaling vectors Bb between different clients decreases."
        },
        {
            "title": "Preprint",
            "content": "(a) IID (b) moderate non-IID (c) server non-IID (d) IID (e) moderate non-IID (f) server non-IID Figure 5: Mean of pairwise cosine similarity of the learned scaling vectors Ad and Bb across layers of RoBERTa model locally fine-tuned with VeRA on the RTE task, with different levels of data heterogeneity. (a)-(c): value matrices; (d)-(f): query matrices. The learned scaling vectors Ad are more similar across clients than the scaling vectors Bb, and with increased data heterogeneity, the similarity of scaling vectors Bb between different clients decreases."
        }
    ],
    "affiliations": [
        "Shenyang Institute of Automation, Chinese Academy of Sciences",
        "Stanford University",
        "The University of Hong Kong"
    ]
}