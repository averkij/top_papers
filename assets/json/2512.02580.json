{
    "paper_title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "authors": [
        "Changpeng Yang",
        "Jinyang Wu",
        "Yuchen Liu",
        "Shuai Zhang",
        "Yang Li",
        "Qiliang Liang",
        "Hongzhen Wang",
        "Shuai Nie",
        "Jiaming Xu",
        "Runyu Shi",
        "Ying Huang",
        "Guoquan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework."
        },
        {
            "title": "Start",
            "content": "From Imitation to Discrimination: Toward Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks Changpeng Yang1,*, Jinyang Wu2,*, Yuchen Liu1, Shuai Zhang2, Yang Li1, Qiliang Liang3, Hongzhen Wang1, Shuai Nie1, Jiaming Xu1,, Runyu Shi1, Ying Huang1, Guoquan Zhang1 2Tsinghua University, 1Xiaomi Corporation, 3Peking University 5 2 0 2 ] . [ 1 0 8 5 2 0 . 2 1 5 2 : r Abstract Reinforcement learning has emerged as paradigm for posttraining large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose CAPO (Curriculum Advantage Policy Optimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as versatile and robust optimization framework. Introduction Reinforcement learning (RL) has become mainstream paradigm for post-training large language models, substantially advancing their reasoning capabilities, as demonstrated by DeepSeek-R1 (Guo et al. 2025) and Kimi1.5 (Team et al. 2025). critical component of RL algorithms such as PPO (Schulman et al. 2017) and GRPO (Shao et al. 2024b) is the advantage, which quantifies whether trajectory performs above or below expectation, providing positive and negative feedback to guide policy updates. Yet, simultaneous training on both positive and negative advantage samples often introduces ambiguity, especially during early optimization, limiting further improvement. This challenge calls for probing the essence of advantage and rethinking its role in shaping training dynamics. Since advantage inherently reflects whether the models competence is better *These authors contributed equally. Corresponding Author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (a) RL: Mixed signals hinder learning. (b) CAPO: Positive imitation builds stability. Figure 1: Comparison of RL and CAPO. (a) RL mixes positive and negative signals. (b) CAPO uses staged curriculum: positive imitation builds stability, negative discrimination improves generalization. or worse than expected, it naturally prompts our central research question: can advantage itself serve as guidance indicator, enabling structured integration of positive and negative feedback into unified, generalizable paradigm? Inspired by developmental psychology, we note that childrens learning progresses through gradual stages: they first acquire basic behaviors through positive imitation, and only later advance their generalization by incorporating corrective feedback and punishment (Bandura, Ross, and Ross 1961; Whitehurst 1969). this staged progression naturally positions advantage as an effective curriculum signal. Motivated by this perspective, we propose CAPO (Curriculum Advantage Policy Optimization), training paradigm broadly compatible with advantage-based reinforcement learning algorithms. It adopts two-phase curriculum learning strategy based on advantage signals: in the imitation phase, positive samples foster stable foundations, and in the discriminative phase, positive samples sustain correct behaviors while negative samples refine learning, together improving generalization. As illustrated in Figure 1, panel (a) shows that mixing positive and negative signals from the start leads to early confusion and prevents stable progress. To clarify this phenomenon, panel (b) provides simple intuition, depicting how separating signals in stages avoids such interference. Together, these panels highlight the necessity of staged curriculum design in CAPO. While childrens learning provides intuitive motivation, we provide theoretical basis for this curriculum through the lens of the variancebias tradeoff. The imitation phase reduces gradient variance, fostering stable early training, whereas the discrimination phase restores unbiasedness, enabling effective generalization. We further highlight CAPOs unique design: it leverages advantage as dynamic signal that aligns with the models evolving competence. Since curriculum learning was first introduced by Bengio et al. (Bengio et al. 2009a), most methods have relied on static heuristics such as sorting tasks from easy to hard. Subsequent adaptive curriculum strategies estimate difficulty through expert annotations or model success rates (Shi et al. 2025; Chen et al. 2025), but they remain external and heuristic, relying on manually defined proxies rather than signals intrinsic to the models evolving capbility. This limitation motivates our design of CAPO, which leverages advantage estimates as an intrinsic, competence-aware signal to drive dynamic curriculum scheduling. Extensive experiments show that CAPO consistently enhances mathematical reasoning across diverse advantagebased RL algorithms, including GRPO, PPO, RLOO, and Reinforce++. It further demonstrates strong modality generalization, achieving notable gains on GUI-based reasoning tasks. Together, these results highlight CAPO as versatile curriculum paradigm, effective across multiple advantagebased RL methods and transferable to multimodal reasoning domains. In summary, our main contributions are threefold: We introduce advantage as dynamic curriculum signal and design CAPO, two-phase mechanism with imitation and discrimination phases adapting to the models evolving competence. We demonstrate CAPOs broad generalization, proving effective across diverse advantage-based RL algorithms and transferable to multimodal reasoning tasks on GUIbased environments. We validate CAPO with extensive experiments, achieving improvements ranging from +2.4 to +4.0 on 1.5B models and from +1.7 to +3.9 on 7B models across diverse advantage-based RL algorithms, and also +3.81 on multimodal GUI-based reasoning tasks. Method We present CAPO, as shown in Figure 2, an advantagebased compatible mechanism. Once samples are generated by the policy model, various algorithms compute their respective advantage estimates, which are then seamlessly unified within our curriculum advantage framework. Leveraging these signals as guidance, CAPO explicitly structures training into an imitation phase and discrimination phase, thereby reconciling the need for early-stage stability with the demand for enhanced generalization once negative samples are introduced. Policy Optimization in Reinforcement Learning. In RL for language models, the generation process can be formulated as policy πθ that maps input prompts to output sequences o. The training objective is to maximize the expected reward assigned to the generated sequences: (θ) = EqpQ, oπθ(q) [R(q, o)] (1) The policy gradient theorem (Sutton, Barto et al. 1998) allows reinforcement learning objectives to be optimized via gradient ascent: θJ (θ) = Eπθ [θ log πθ(oq) Qπθ (q, o)] , (2) where Qπθ (q, o) denotes the action-value function, representing the expected return for generating sequence from prompt under policy πθ. To reduce variance in gradient estimates, it is common to replace Qπθ (q, o) with the advantage function: θJ (θ) = Eπθ [θ log πθ(oq) Aπθ (q, o)] , (3) where Aπθ (q, o) measures whether sampled trajectory outperforms the expected baseline under πθ. In practice, different policy optimization methods adopt distinct strategies to estimate or construct the advantage. For example, PPO employs generalized advantage estimation (GAE) (Schulman et al. 2015), which balances bias and variance by leveraging discounted sum of temporal difference residuals. GRPO introduces group-relative advantage formulation that normalizes advantage values within grouped samples to stabilize training dynamics. Building on these foundations, we treat the advantage as generalpurpose signal that not only drives gradient updates but also dynamically structures the training curriculum. To stabilize training, modern methods such as PPO optimize clipped surrogate objective, replacing the raw policy gradient (cid:34) (θ) = Eτ πθ (cid:88) min(ρt(θ) At, ˆρt(θ) At) , (4) (cid:35) t=1 where ρt(θ) = πθ(atst) πθold (atst) is the importance sampling ratio, and ˆρt(θ) = clip(ρt(θ), 1 ϵ, 1 + ϵ) restricts updates to stable range. 1 Curriculum Advantage Policy Optimization We introduce Curriculum Advantage Policy Optimization (CAPO), progressive training framework that leverages the advantage as both gradient weight and curriculum signal. Intuitively, positive advantage indicates that the model is competent on the sample, whereas negative advantage suggests insufficient capability. Rather than applying static or heuristic curricula, CAPO dynamically structures learning in two stages. Phase 1: Imitation Phase with Positive-Only Advantage Samples. Training begins with positive-only imitation phase (Aτ 0), ensuring that updates are guided by beneficial trajectories. This design consolidates prior knowledge and avoids unstable gradients that might arise from prematurely exposing the model to challenging samples. Formally, building on Eq. 4 and incorporating KL regularization term to prevent policy collapse as in RLHF (Guo et al. 2025), the objective is defined as: Jphase-1(θ) = Eτ (cid:104) IA(τ )0 (cid:88) min(ρtAt, ˆρtAt) (cid:16) 1 t=1 β DKL (cid:0)πθ (cid:13) (cid:13)πref (cid:1)(cid:17)(cid:105) . (5) Figure 2: Illustration of the CAPO scheduling mechanism. Each query is processed by the policy model to generate samples, with advantages computed under different optimization algorithms. In Phase 1, only positive-advantage samples are used to ensure stability; after the switch point, Phase 2 incorporates both positive and negative advantages to balance stability and generalization. where the indicator IAτ 0 filters out negative-advantage samples, and β controls the KL penalty strength. This phase thus encourages the model to reinforce correct reasoning behaviors while remaining close to the reference distribution. Phase 2: Discriminative Phase with Full Advantage Spectrum. Once stable foundation is established, CAPO transitions into discriminative phase that admits the full advantage spectrum. By incorporating negative-advantage samples, the model learns not only to reinforce strong reasoning trajectories but also to suppress suboptimal ones, thereby enhancing generalization. The corresponding objective is: This progressive shift from imitation to discrimination ensures that CAPO first stabilizes learning and then promotes robust reasoning by leveraging both positive and negative feedback. Curriculum Scheduling Between Phases. To ensure seamless transition between the imitation and discrimination phases, CAPO adopts two-stage training strategy with predefined switch point (e.g., at 10% or 20% of total training steps). We also experimented with gradually introducing negative signals, but found that no such progressive scheme matched the effectiveness of simple switch point. In practice, hard switch provides robust and task-agnostic mechanism that consistently balances early-stage stability with later discriminative learning, without requiring delicate hyperparameter tuning or task-specific monitoring. This pragmatic design ensures reproducibility across diverse settings while still delivering the intended variance reduction in Phase 1 and generalization gains in Phase 2. Theoretical Justification The CAPO algorithms two-phase curriculum leverages the variancebias tradeoff in policy gradient estimation to enhance training stability and convergence. Consider policy πθ(as) with advantage estimate ˆA(s, a) = Aπ(s, a) + ϵ, where ϵ is zero-mean noise with variance σ2. The true policy gradient and its stochastic estimate ˆg are defined as: = Eπθ [θ log πθ(as) Aπ(s, a)] ˆg = θ log πθ(as) ˆA(s, a) (7) (6) The mean squared error (MSE) between ˆg and decomposes into bias and variance: E(cid:2)ˆg g2(cid:3) = E[ˆg] g2 + Var(ˆg). Phase 1 (Positive-only Imitation). To suppress earlystage noise, CAPO restricts updates to positive advantages: (8) (cid:105) (cid:104) θ log πθ(as) ˆA(s, a) I{ ˆA > 0} ˆgphase-1 = . (9) Excluding negative outliers reduces Var(ˆg), so even with bias, the overall MSE is lowered, ensuring stable improvement. Phase 2 (Full Discriminative Refinement). As the policy improves and Var( ˆA) shrinks, CAPO transitions to the unbiased estimator: (cid:104) ˆgphase-2 = (cid:105) θ log πθ(as) ˆA(s, a) , (10) restoring E[ˆgphase-2] = and enabling generalization. Proposition. Let {αt} denote the step sizes. Under RobbinsMonro conditions ((cid:80) < ), the CAPO update rule converges almost surely to local optimum: Phase 1 bounds variance, Phase 2 restores unbiasedness, so the MSE in (8) vanishes asymptotically and the limit point of θt is stationary. detailed proof is provided in Appendix A. αt = , (cid:80) α2 Experimental Setups We present setups for both mathematical reasoning and GUI-based multimodal tasks, covering datasets, evaluation, and baselines. Mathematical Reasoning Tasks Datasets. Following prior works (Zeng et al. 2025; Liu et al. 2025; Wu et al. 2025), we curate 5.5K Level 35 problems from the MATH dataset (Hendrycks et al. 2021). Evaluation Benchmarks. We evaluate on AIME 2024 (Li et al. 2024a), AMC (Li et al. 2024a), MATH500 (Hendrycks et al. 2021), GSM8K (Cobbe et al. 2021), Minerva (Lewkowycz et al. 2022), OlympiadBench (He et al. 2024) and College Math (Tang et al. 2024). For out-ofdomain evaluation, we additionally include ARC-C (Clark et al. 2018) and GPQA-Diamond (Rein et al. 2024). Inference uses greedy decoding with 3K token budget (Liu et al. 2025; Wu et al. 2025). Baselines. We evaluate CAPO across four mainstream reinforcement learning algorithms: GRPO, PPO, RLOO, and Reinforce++. For each baseline, we additionally include its CAPO-augmented variant, enabling direct assessment of CAPOs effectiveness as general enhancement. This design allows us to examine both the standalone performance of the baselines and the improvements achieved through CAPO integration. Implementation. To ensure fairness across the various baseline algorithms and CAPO variants, we unify the description of all implementation details. Because different methods require distinct parameterizations, the complete training configurations are provided in Appendix B. GUI-based Multimodal Tasks Datasets. We adopt GUI-R1-3K (Luo et al. 2025), dataset derived from OS-Atlas (Wu et al. 2024b), consisting of 3K vision-language-action examples across web and mobile interfaces. The dataset pairs visual states with natural language instructions and action sequences, providing compact yet diverse testbed for evaluating multimodal reasoning and planning. Evaluation Benchmarks. Planning: GUI-Act-Web (Luo et al. 2025), OmniAct-Web (Kapoor et al. 2024), AndroidControl-Low/High (Li et al. 2024b). Perception: Screenspot Pro (Li et al. 2025). Baselines. We compare GRPO with its CAPO-augmented variant, following the dominant GUI benchmark setup (Wu et al. 2024b). This serves as supplementary evaluation to demonstrate the effectiveness of CAPO in multimodal reasoning scenarios, beyond the primary focus on mathematical reasoning tasks. Implementation. We train QwenVL2.5-3B(Team 2025) in the VERL framework (Sheng et al. 2025), All experiments are conducted on 8NVIDIA A100-80GB GPUs. To ensure reproducibility, detailed hyperparameters and training configurations are provided in Appendix B. For evaluating planning ability, we leverage four datasets across web, mobile, and desktop platforms: GUI-ActWeb(Luo et al. 2025), OmniAct-Web, AndroidControl-Low, and AndroidControl-High(Li et al. 2024b; Lu et al. 2024). These benchmarks involve long-horizon action prediction, decision-making under partial observability, and cross-app navigation, covering both shortand long-term planning tasks in real-world interactive environments. We further conduct qualitative case studies on GUI-based multimodal tasks to illustrate the behavioral differences between baselines and CAPO-enhanced models. Representative examples are presented in Appendix D."
        },
        {
            "title": "Experimental Results",
            "content": "Reasoning Performance. Following Main Results Math prior works (Liu et al. 2025; Wu et al. 2025), we evaluate CAPO on seven reasoning benchmarks across two model scales and four optimization methodsGRPO (Shao et al. 2024a), PPO (Schulman et al. 2017), RLOO (Ahmadian et al. 2024), and REINFORCE++ (Hu et al. 2025). As shown in Table 1, CAPO delivers consistent gains of +1.7 to +4.0 points across all methods, confirming its effectiveness as drop-in enhancement. Notably, it achieves large improvements on competition-level tasks: for instance, AMC improves from 52.5 to 65.0 (+12.5) and AIME24 from 16.7 to 20.0 (+3.3) on the 7B model, with the 1.5B model also achieving absolute improvements of 2.4 4.0. Beyond individual datasets, consistent improvements are observed on GSM8K, OlympiadBench, Minerva, among others, demonstrating robust generalization. CAPO further scales well with model size: while the 7B model attains the highest performance, the 1.5B variant with CAPO closes much of the gap, approaching the larger baseline. Overall, CAPO shows (1) broad applicability across optimization methods, (2) scalable effectiveness that benefits both small and large models, and (3) substantial improvements on high-difficulty benchmarks, supporting advantagebased curricula as principled mechanism to unify stability and exploration. Multimodal Perception and Planning Capabilities of GUI Agent. We adopt GUI-based tasks as our primary benchmark due to their complexity as cross-domain reasoning challenges in multimodal settings. These tasks require precise language understanding, fine-grained visual perception, and context integration to plan and execute actions effectively, providing rigorous testbed for CAPOs robustness and adaptability. Following the setup in (Luo et al. 2025), which employs GRPO, we evaluate performance across perception, lowlevel, and high-level planning tasks. As shown in Table 2, CAPO yields an overall gain of +3.81 on planning tasks. For completeness, we also report its GUI grounding results in perception tasks, detailed in Appendix C. These results highlight CAPOs strong generalization beyond mathematical reasoning, boosting multimodal perception and control by leveraging advantage as curriculum Method AIME24 AMC MATH500 GSM8K Minerva Olympiad CollegeMath Avg. CoT GRPO GRPO (+Ours) PPO PPO (+Ours) RLOO RLOO (+Ours) Reinforce++ Reinforce++ (+Ours) CoT GRPO GRPO (+Ours) PPO PPO (+Ours) RLOO RLOO (+Ours) Reinforce++ Reinforce++ (+Ours) 13.3 16.7 20.0 26.7 30.0 30.0 33.3 16.7 20.0 10.0 13.3 23.3 13.3 13.3 20.0 23.3 10.0 20.0 42.5 52.5 65.0 52.5 57.5 55.0 67.5 52.5 55.0 42.5 52.5 62.5 50.0 57.5 50.0 57.5 47.5 50.0 Qwen2.5-7B-Math 50.8 75.2 76.8 71.0 72.6 73.8 74.8 72.4 72.0 77.8 86.5 88.9 80.9 85.2 82.7 84.6 85.6 86.8 Qwen2.5-1.5B-Math 59.0 71.2 71.8 66.6 70.2 68.0 71.6 70.0 70.8 74.6 83.2 83.9 74.7 78.4 82.6 83.1 83.2 83.7 22.5 29.4 33.1 34.2 37.9 35.5 36.0 37.1 40. 24.3 26.8 32.0 24.6 25.4 28.7 33.8 32.0 34.2 27.8 36.9 39.7 34.1 37.8 36.0 35.6 37.2 37.2 27.6 30.1 32.9 27.1 33.0 32.0 33.2 31.7 31.6 42.7 44.8 46.3 41.1 41.7 39.8 41.1 40.3 42.5 39.5 41.2 41.7 37.8 40.0 41.4 41.8 41.3 42.0 41.3 48.9 52.83.9 48.6 51.83.2 50.4 53.32.9 48.8 50.51. 39.6 45.6 49.64.0 42.0 45.43.4 46.1 49.23.1 45.1 47.52.4 Table 1: We report results of different LLMs across seven mainstream benchmarks, with all main experiments conducted on Qwen2.5-Math-7B and Qwen2.5-Math-1.5B. Each baseline (GRPO, PPO, Reinforce++, and RLOO) is further evaluated with our CAPO mechanism, denoted as GRPO(+Ours), PPO(+Ours), and RLOO(+Ours), Reinforce++(+Ours), respectively. For clarity, all improvements of the CAPO variants over their corresponding baselines are highlighted in bold; identical results remain unmarked. GUI-Act-Web SR Models OmniAct-Web AndroidControl-Low AndroidControl-High Overall Type GR SR Type GR Type GR 79.22 58.57 42.62 46.74 49.24 22.99 64.58 71.19 Os-Atlas-4B QwenVL2.5-3B 56.10 64.28 55.61 50.63 46.89 47.02 62.03 74.07 85.10 82.36 70.23 79.02 71.10 70.76 82.13 80.15 GRPO 87.73 85.85 85.85 87.24 74.02 74.16 82.29 81.19 Ours 2.63 3.49 15.62 8.22 2.92 3.40 1.16 1.04 () SR 40.62 59.32 63.87 61.41 2.46 Type GR 49.01 49.51 47.81 46.51 60.10 58.25 65.91 61.47 5.81 3.22 SR 22.77 38.90 46.81 47.71 0.90 49.75 54.09 70.79 74.60 3.81 Table 2: Performance comparison of GUI reasoning tasks across both low-level and high-level settings on GUI-Act-Web, OmniAct-Web, AndroidControl-Low, and AndroidControl-High. We report Type (action type prediction), GR (grounding accuracy), and SR (step success rate) under unified zero-shot prompt for fair comparison. CAPO consistently improves over the GRPO baseline, yielding an average gain of +3.81 points across planning benchmarks, further demonstrating its effectiveness in multimodal reasoning scenarios. signal that enables consistent generalization across diverse modalities. Detailed Analysis Training Dynamics Analysis. In Figure 3, we present the training reward and entropy dynamics of the 7B model under CAPO and GRPO. Before the phase transition, both methods exhibit comparable reward growth; however, CAPO becomes consistently superior once imitation has stabilized. The gray vertical line marks the transition from the imitation phase to the discrimination phase. After this point, the entropy trajectory of CAPO exhibits steady climb, in contrast to the plateau observed in GRPO, while rewards continue to improve. This indicates that CAPO not only secures stronger reward gains in the later stage but also maintains higher entropy, property often linked to more diverse and exploratory reasoning paths. Moreover, the smooth rise in entropy suggests that CAPO avoids the sharp entropy collapses typically triggered by prematurely mixing negative samples. By deferring their incorporation, CAPO stabilizes the imitation stage and later exploits negative feedback more effectively, enabling better generalization across tasks. (a) Entropy dynamics (b) Reward dynamics Figure 3: Comparison of reward and entropy curves between GRPO and CAPO on the 7B model. The gray vertical line marks the switch from imitation to discrimination. CAPO first relies on positive-only training to establish robust foundations. After the switch, negative samples lead to steady increase in entropy and rewards, demonstrating enhanced generalization. Method CoT ADARFT [20] GRPO GRPO(+SC) GRPO(+Ours) AIME24 AMC MATH500 GSM8K Minerva Olympiad Avg. 39.1 77.8 91.0 47.8 49.5 86.5 51.8 86.3 53.9 88.9 13.3 15.8 16.7 16.7 20.0 42.5 55.0 52.5 65.0 65.0 22.5 25.4 29.4 29.8 33.1 50.8 74.4 75.2 75.0 76. 27.8 24.9 36.9 38.1 39.7 Table 3: Performance comparison on Qwen2.5-7B-Math across six reasoning benchmarks. We compare five settings: CoT, ADARFT, vanilla GRPO, GRPO with static curriculum(GRPO(+SC)),and our advantage curriculum method GRPO(+Ours). Our approach achieves the strongest overall performance among all variants. Effect of Switch Stage in CAPO. We analyze how the switch stage influences the effectiveness of CAPO. Figure 4 reports results on two representative benchmarks (AIME24 and AMC23). Performance peaks when the switch occurs around 20%30% of training, suggesting that after short period of imitation-style learning, introducing negative advantages early enough encourages more discriminative reasoning and leads to more robust learning dynamics. Complete benchmark results for both are presented in Appendix B.1 for reference. Comparing Static and Dynamic Curriculum Strategies. We compare the conventional static curriculum method with our dynamic advantage-based training strategy. In the static curriculum setting, we estimate sample difficulty by performing pass@16 evaluation for each sample. The dataset is then sorted based on this metric, and the model is trained following this fixed order. In contrast, our dynamic approach focuses on adjusting the advantage signal progressively during training, without the need for manually reordering the data. As shown in Table 3, both ADARFT and the static curriculum GRPO(+SC) offer only limited and inconsistent gains over vanilla GRPO. In contrast, our dynamic method GRPO(+Ours) achieves the strongest improvements across most tasks. This suggests that predefined difficulty heuristics are insufficient, and dynamically adjusting the advantage signal provides more effective curriculum. Further Discussion on Generalization. Recently some work has underscored the vulnerability of LLMs to distributional shifts (Yuan et al. 2023; Wang et al. 2024), with models often exhibiting strong in-distribution (ID) performance but significant degradation on out-of-distribution (OOD) domains (Berglund et al. 2024; Yang et al. 2024). To evaluate CAPOs generalization under distributional shift, we benchmark it on two representative reasoning datasets: ARC-C and GPQA-Diamond. Since all models are trained exclusively on mathematical data, this setting naturally provides robust OOD evaluation. As shown in Figure 5, CAPO achieves an average accuracy of 52.8, outperforming GRPO by +3.8, with gains observed consistently across all benchmarks (+1.4 on ARC-C and +6.2 on GPQA-D). These refeedback. Recent works have begun to explore the sperate function of themZhu et al. (2025) leverage them primarily to balance diversity, yet fall short of fully unlocking their optimization potential. Xu et al. (2025b) incorporate both positive and negative samples into DPO-inspired loss, but still rely on fine-tuning-centric, two-stage which lacks principled mechanism to integrate with advantage-based reinforcement learning methods. In contrast, our method first leverages positive advantage samples to establish stable behavioral priors, then introduces negative samples to improve generalization. This staged scheduling avoids mixing signals prematurely and aligns naturally with advantage-based reinforcement learning frameworks. Curriculum Learning. Curriculum learning (CL), originally proposed by Bengio et al. (2009b), has long adhered to data-centric paradigm, and has been widely adopted in LLM development (Parashar et al. 2025; Xu et al. 2025a). Recent methods like Speed-RL (Zhang et al. 2025) and Kim and Lee (2024) still employ data-centric strategies that progress from simple to complex tasks using external criteria such as task diffculty to determine sample ordering. Similarly, LBS3 (Luo et al. 2024) guides models through progressive training with easy-to-hard proxy queries. However, these approaches fundamentally misalign with effective curriculum design by relying on static, externally defined difficulty metrics rather than the models evolving capabilities. Truly effective curriculum learning should be compenteceaware, dynamically adapting to the models competence. CAPO leverages advantage estimates as an intrinsic, competence-aware signal, avoiding the early mixing of positive and negative feedback that often destabilizes RL training. It employs staged curriculum: positive advantages first establish stable behavioral priors, and negative ones are later introduced to enhance generalization. Unlike curriculum learning methods that depend on static, externally defined difficulty measures, our framework adapts dynamically to the policys evolving competence, thereby unifying the stability of early reinforcement learning with the adaptability of curriculum scheduling. Conclusion In this paper, We propose CAPO (Curriculum Advantage Policy Optimization), novel mechanism that addresses key limitations in reasoning model training by leveraging advantage as an intrinsic learning signal for adaptive curriculum construction. CAPOs two-phase approach progresses from imitation learning with positive-only samples to discrimination learning incorporating negative signals, mirroring human cognitive development while preventing the instability common in direct mixed-signal optimization. Extensive experiments demonstrate CAPOs consistent improvements over strong baselines like GRPO across diverse benchmarks and model scales. Remarkably, CAPO exhibits exceptional cross-domain generalization from mathematical reasoning to multimodal reasoning tasks. By aligning training with the models evolving capabilities rather than external metrics alone, CAPO opens new avenues for developing more adaptive and cognitively-inspired learning algorithms. Figure 4: Switch Point Sensitivity: Results on AIME24 and AMC23. While we evaluate CAPO across 8 benchmarks in total, here we present the two most representative results. The curves show that introducing the switch point in the early phase (around 0.20.3) yields the best performance, aligning with the theoretical expectation that early positiveonly training stabilizes learning before timely inclusion of negative signals enhances generalization. sults demonstrate the effectiveness of CAPOs progressive learning strategy in enhancing OOD generalization by integrating both imitation and discrimination phases. These results confirm CAPOs robustness under OOD conditions. Figure 5: Results on two representative out-of-distribution benchmarks (Qwen2.5-Math-7B-Base). CAPO achieves an average accuracy of 52.8, outperforming GRPO by +6.5%, demonstrating improved robustness under distributional shifts. Related Work Reinforcement Learning for Large Reasoning Model. Recent advancements in both LLMs and MLLMs have increasingly focused on enabling models to simulate reasoning processes. Inspired by powerful reasoning models like DeepSeek-R1 (Guo et al. 2025), and Kimi-k1.5 (Team et al. 2025), research focus has been drawn to reinforcement learning with verifiable rewards (RLVR) (Wei et al. 2022; Wang et al. 2023; Wu et al. 2024a; Hua et al. 2025; Wu et al. 2025), which combines both positive and negative J.; Kim, M.; Zhang, D.; Tang, References Ahmadian, A.; Cremer, C.; Galle, M.; Fadaee, M.; Kreutzer, J.; Pietquin, O.; Ustun, A.; and Hooker, S. 2024. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. arXiv:2402.14740. Bandura, A.; Ross, D.; and Ross, S. A. 1961. Transmission of aggression through imitation of aggressive models. The Journal of Abnormal and Social Psychology, 63(3): 575. Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. In Proceedings of the 26th 2009a. Curriculum learning. annual international conference on machine learning, 41 48. Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009b. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09, 4148. New York, NY, USA: Association for Computing Machinery. ISBN 9781605585161. Berglund, L.; Tong, M.; Kaufmann, M.; Balesni, M.; Stickland, A. C.; Korbak, T.; and Evans, O. 2024. The Reversal Curse: LLMs trained on is fail to learn is A. In The Twelfth International Conference on Learning Representations. Chen, X.; Lu, J.; Piche, A.; Gontier, N.; Bengio, Y.; and Kamalloo, E. Self-Evolving Curriculum for LLM Reasoning. 2025. arXiv:2505.14970. Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457v1. Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. He, C.; Luo, R.; Bai, Y.; Hu, S.; Thai, Z.; Shen, J.; Hu, J.; Han, X.; Huang, Y.; Zhang, Y.; et al. 2024. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 38283850. Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Hu, J.; Liu, J. K.; Xu, H.; and Shen, W. 2025. REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models. arXiv:2501.03262. Hua, C.; Xu, Q.; Yang, Z.; Wang, Z.; Bao, S.; and Huang, Q. 2025. OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning. CoRR, abs/2505.05180. Kapoor, R.; Butala, Y. P.; Russak, M.; Koh, J. Y.; Kamble, K.; Alshikh, W.; and Salakhutdinov, R. 2024. OmniACT: Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web. arXiv:2402.17553. Kim, J.; and Lee, J. 2024. Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning. arXiv preprint arXiv:2405.07490. Lewkowycz, A.; Andreassen, A.; Dohan, D.; Dyer, E.; Michalewski, H.; Ramasesh, V.; Slone, A.; Anil, C.; Schlag, I.; Gutman-Solo, T.; et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35: 38433857. Li, J.; Beeching, E.; Tunstall, L.; Lipkin, B.; Soletskyi, R.; Huang, S.; Rasul, K.; Yu, L.; Jiang, A. Q.; Shen, Z.; et al. 2024a. Numinamath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions. https://huggingface.co/datasets/Numinamath. Hugging Face repository, 13:9. Li, K.; Meng, Z.; Lin, H.; Luo, Z.; Tian, Y.; Ma, J.; Huang, Z.; and Chua, T.-S. 2025. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use. arXiv:2504.07981. Li, W.; Bishop, W.; Li, A.; Rawles, C.; Campbell-Ajala, F.; Tyamagundlu, D.; and Riva, O. 2024b. On the Effects of Data Scale on UI Control Agents. arXiv:2406.03679. Li, X.; Xu, H.; Zhang, J.; and Chang, H.-h. 2023. Deep reinforcement learning for adaptive learning systems. Journal of Educational and Behavioral Statistics, 48(2): 220243. Liu, Z.; Chen, C.; Li, W.; Qi, P.; Pang, T.; Du, C.; Lee, W. S.; and Lin, M. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Lu, Q.; Shao, W.; Liu, Z.; Meng, F.; Li, B.; Chen, B.; Huang, S.; Zhang, K.; Qiao, Y.; and Luo, P. 2024. GUI Odyssey: Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices. arXiv:2406.08451. Luo, K.; Ding, Z.; Weng, Z.; Qiao, L.; Zhao, M.; Li, X.; Yin, D.; and Shu, J. 2024. Lets Be Self-generated via Step by Step: Curriculum Learning Approach to Automated Reasoning with Large Language Models. arXiv preprint arXiv:2410.21728. Luo, R.; Wang, L.; He, W.; and Xia, X. 2025. GUI-R1 : Generalist R1-Style Vision-Language Action Model For GUI Agents. arXiv:2504.10458. Parashar, S.; Gui, S.; Li, X.; Ling, H.; Vemuri, S.; Olson, B.; Li, E.; Zhang, Y.; Caverlee, J.; Kalathil, D.; and Ji, S. 2025. Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning. arXiv preprint arXiv:2506.06632. Rein, D.; Hou, B. L.; Stickland, A. C.; Petty, J.; Pang, R. Y.; Dirani, J.; Michael, J.; and Bowman, S. R. 2024. GPQA: Graduate-Level Google-Proof Q&A Benchmark. In First Conference on Language Modeling. Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P. 2015. High-dimensional continuous control arXiv preprint using generalized advantage estimation. arXiv:1506.02438. Wu, Z.; Wu, Z.; Xu, F.; Wang, Y.; Sun, Q.; Jia, C.; Cheng, K.; Ding, Z.; Chen, L.; Liang, P. P.; and Qiao, Y. 2024b. OSATLAS: Foundation Action Model for Generalist GUI Agents. arXiv:2410.23218. Xu, F.; Hao, Q.; Zong, Z.; Wang, J.; Zhang, Y.; Wang, J.; Lan, X.; Gong, J.; Ouyang, T.; Meng, F.; et al. 2025a. Towards Large Reasoning Models: Survey of Reinforced Reasoning with Large Language Models. arXiv preprint arXiv:2501.09686. Xu, S.; Peng, C.; Long, J.; Xu, W.; Chu, W.; and Qi, Y. 2025b. Harnessing Negative Signals: Reinforcement DisarXiv tillation from Teacher Data for LLM Reasoning. preprint arXiv:2505.24850. Yang, H.; Zhang, Y.; Xu, J.; Lu, H.; Heng, P.-A.; and Lam, W. 2024. Unveiling the Generalization Power of Fine-Tuned Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 884899. Mexico City, Mexico: Association for Computational Linguistics. Yuan, L.; Chen, Y.; Cui, G.; Gao, H.; Zou, F.; Cheng, X.; Ji, H.; Liu, Z.; and Sun, M. 2023. Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zeng, W.; Huang, Y.; Liu, Q.; Liu, W.; He, K.; Ma, Z.; and He, J. 2025. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892. Zhang, R.; Arora, D.; Mei, S.; and Zanette, A. 2025. of Reasoning ModSPEED-RL: arXiv preprint els via Online Curriculum Learning. arXiv:2506.09016. Zhu, X.; Xia, M.; Wei, Z.; Chen, W.-L.; Chen, D.; and Meng, Y. 2025. The Surprising Effectiveness of NegaarXiv preprint tive Reinforcement in LLM Reasoning. arXiv:2506.01347. Faster Training Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024a. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024b. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. Sheng, G.; Zhang, C.; Ye, Z.; Wu, X.; Zhang, W.; Zhang, R.; Peng, Y.; Lin, H.; and Wu, C. 2025. HybridFlow: Flexible and Efficient RLHF Framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, 12791297. ACM. Shi, T.; Wu, Y.; Song, L.; Zhou, T.; and Zhao, J. 2025. Efficient Reinforcement Finetuning via Adaptive Curriculum Learning. arXiv:2504.05520. Sutton, R. S.; Barto, A. G.; et al. 1998. Reinforcement learning: An introduction, volume 1. MIT press Cambridge. Tang, Z.; Zhang, X.; Wang, B.; and Wei, F. 2024. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884. Team, K.; Du, A.; Gao, B.; Xing, B.; Jiang, C.; Chen, C.; Li, C.; Xiao, C.; Du, C.; Liao, C.; et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Team, Q. 2025. Qwen2.5-VL. Wang, J.; Hu, X.; Hou, W.; Chen, H.; Zheng, R.; Wang, Y.; Yang, L.; Ye, W.; Huang, H.; Geng, X.; Jiao, B.; Zhang, Y.; and Xie, X. 2024. On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. IEEE Data Eng. Bull., 47(1): 4862. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.; Narang, S.; Chowdhery, A.; and Zhou, D. 2023. SelfConsistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837. Whitehurst, G. J. 1969. Discrimination learning in children as function of reinforcement condition, task complexity, and chronological age. Journal of Experimental Child Psychology, 7(2): 314325. Wu, J.; Feng, M.; Zhang, S.; Che, F.; Wen, Z.; and Tao, J. 2024a. Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts. arXiv preprint arXiv:2411.18478. Wu, J.; Liao, C.; Feng, M.; Zhang, S.; Wen, Z.; Shao, P.; Xu, H.; and Tao, J. 2025. Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities. arXiv preprint arXiv:2505.15692."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tsinghua University",
        "Xiaomi Corporation"
    ]
}