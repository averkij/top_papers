{
    "paper_title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
    "authors": [
        "Runpeng Yu",
        "Qi Li",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed. The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment. Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
        },
        {
            "title": "Start",
            "content": "1 Discrete Diffusion in Large Language and Multimodal Models: Survey Runpeng Yu*, Qi Li*, Xinchao Wang National Univerisity of Singapore {r.yu, liqi}@u.nus.edu, xinchao@nus.edu.sg 5 2 0 2 6 ] . [ 1 9 5 7 3 1 . 6 0 5 2 : r AbstractIn this work, we provide systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt multi-token, parallel decoding paradigm using full attention and denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, responseaware perception. These capabilities are previously difficult to achieve with AR models. Recently, growing number of industrialscale proprietary d(M)LLMs, as well as large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10 acceleration in inference speed. These developments position discrete diffusion models as promising alternative to intelligence based on the traditional autoregressive approach. The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. significant portion of these resources has been or can be transferred to support the development of dLLMs and dMLLMs. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Originally based on the continuous-space, the diffusion models for LLM and MLLM have transitioned to the currently prevalent discrete-space diffusion models built on absorbing states. The adaptation and simplification of the mathematical framework has played crucial role in enabling the scaling and engineering optimization of dLLMs and dMLLMs. Together, these advancements have catalyzed surge in dLLMs and dMLLMs research in early 2025. In this work, we present comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, visionlanguage, and biological domains. We conclude by discussing future directions for research and deployment. Relative papers are collected in this repo. Index TermsDiscrete Diffusion, Large Language Model, Multimodal Large Language Model, Diffusion Large Language Model, Diffusion Multimodal Large Language Model, Language Model, Unified Model I. INTRODUCTION In recent years, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have demonstrated remarkable advances, exhibiting capabilities that increasingly * Equal contribution. random order. Corresponding author. resemble, or even surpass, human-level performance in domains traditionally associated with intelligence. Modern LLMs and MLLMs achieve superior scores on standard benchmarks designed for general knowledge, comprehension, and reasoning, suggesting that these systems are no longer merely text completion engines but competent general-purpose agents. To date, the predominant paradigm for both LLMs and MLLMs has been autoregressive (AR) [1, 2, 3, 4, 5]. Despite previous successes, AR models, which generate outputs sequentially in left-to-right fashion, possess certain intrinsic limitations. First, their decoding strategy inherently restricts generation to be token-by-token, making parallelization during inference challenging and limiting efficiency improvements. Second, people do not merely envision LLMs and MLLMs as question-answering machines; rather, people aim for them to serve as the intelligent core of systemakin to the human brainwith capabilities for tool use and task orchestration. However, in this context, unstructured natural language becomes highly inefficient for fine-grained control. AR models struggle to enforce precise structural constraints, such as specific lengths or formats, due to their lack of inherent mechanisms for such regulation. Finally, because of the nature of causal attention, AR models perceive visual inputs or textual prompts in onepass, static manner. This inhibits selective attention and makes task-aware, dynamic perception challenging without relying on costly iterative chain-of-thought (CoT) reasoning or multi-round processing integrated with external tools. Discrete Diffusion Large Language Models (dLLMs) and Discrete Diffusion Multimodal Large Language Models (dMLLMs) [6, 7, 8, 9, 10, 11] have recently emerged as promising direction. In contrast to AR generation, discrete diffusion models treat generation as an iterative denoising process over discrete token sequences. This paradigm eliminates the leftto-right constraint and enables generation that is parallel and structurally controllable with bidirectional attention mechanism. Parallel Decoding: Unlike AR models that decode one token at time, discrete diffusion models generate multiple tokens simultaneously in each denoising step. This parallel decoding significantly accelerates inference speed. Better Controllability: Discrete diffusion treats generation as denoising or infilling task rather than unbounded leftto-right generation. This allows for precise control over output properties such as response length, format, and even the reasoning structureby conditioning the generation on predefined templates. Dynamic Perception: Enabled by bidirectional attention, discrete diffusion models continuously revise their perFig. 1. timeline of existing dLLMs and dMLLMs in recent years. The timeline is established mainly according to the release date (e.g., the submission date to arXiv) of the technical paper for model. The affiliation marked in the figure is based on the first affiliation listed in each paper. ception of visual and linguistic context the generation process. This facilitates adaptive comprehension that evolves with the response, overcoming the static, onepass limitations of AR models. throughout including natural Originally proposed for continuous domains such as image generation [12], diffusion models have since been successfully extended to discrete spaces. Early efforts in this direction established the foundational mathematical formulations of discrete diffusion, introducing token corruption schemes specifically designed for categorical data [13, 14]. These models demonstrated the feasibility of diffusion-based generation on various types of discrete data, language [13, 15], images [13], and biological sequences such as DNA [16]. In this early stage, experiments were limited to models with around or fewer than 1 billion parameters. Through simplifications and reparameterizations [15, 16, 17], along with practical engineering efforts, the absorbing-state discrete diffusion formulation has gradually become the predominant mathematical framework adopted by open-source models. In this formulation, the inference process is an iterative masked token prediction, while the training process reduces to masked language modeling task based on randomly sampled time indices and masked positions. The loss function can be written simply as weighted crossentropy loss. These simplifications significantly reduced the complexity and improved the stability in training and inference, paving the way for large-scale diffusion language models. Recent advances have significantly improved the scalability and effectiveness of discrete diffusion models [18, 19]. major breakthrough on the industrial front came with the presentation of discrete diffusion-based large language models by Inception Labs and Google, namely Mercury [20] and Gemini Diffusion [21]. These models reports comparable performance on code and mathematics benchmarks with their AR counterpart, while also achieving 10 speedups in decoding, with about 1000 tokens per second. gan with dLLM models trained on large-scale text corpora, such as LLaDA [6] and Dream [7]. Later, using the public available dLLMs as the backbones, dMLLMs, such as Dimple [8], LaViDa [10], and LLaDA-V [9], are developed through multimodal alignment, instruction tuning, preference learning, and then reasoning enhancement. Empirical results consistently show that these dLLMs and dMLLMs match or closely approach the performance of AR models trained at similar scale and data size. Crucially, with acceleration techniques, their parallel decoding ability enables significantly faster inference compared to traditional AR models. Alongside the development of open-source dLLMs and dMLLMs, significant progress has been made in optimizing training techniques specific to this paradigm. Strategies such as model initialization [18, 8] and masking scheduling [22] have been systematically explored to accelerate convergence, improve training stability, and enhance the final generation quality. With the availability of open-source models, academic research has also deepened into the inference mechanisms of discrete diffusion. For example, during inference, the fundamental question of discrete diffusion models lies in determining, at each generation step, which tokens should be unmasked (i.e., finalized) and which should be left masked to future iterations. To this end, various unmasking [7, 8] and remasking [23, 24] strategies have been proposed, aiming to balance generation quality, stability, and speed. Another example is that, although discrete diffusion models are inherently capable of parallel decoding, they suffer from increased per-step computational cost due to the use of full attention masks. As result, without optimization, their actual inference speed cannot surpass that of AR models. considerable amount of work has focused on adapting existing inference acceleration algorithms originally designed for AR models to dLLMs and dMLLMs, aiming to reduce extensive full attention computations and enhance inference efficiency [25, 26]. In parallel, the research community has developed and opensourced an increasing number of discrete diffusion-based language and multimodal models. Following trajectory similar to that of autoregressive (AR) models, the development beTo provide comprehensive framework for understanding discrete diffusion large language models (DLLMs) and discrete diffusion multimodal large language models (DMLLMs), this survey systematically explores recent advances in modeling, Discrete Denoising Diffusion Probabilistic Model (Sec. II-A) Discrete Diffusion for Binary Variable [27], Discrete Diffusion for Categorical Variable [14], D3PM [13] Mathematical Formulations (Sec. II) Reparameterized Discrete Diffusion Model (Sec. II-B) Continuous Time Discrete Denoising Models [28] (Sec. II-C) Concrete Score (Sec. II-D) Discrete Flow Matching [24] (Sec. II-E) Block Diffusion Models [31] (Sec. II-F) Discrete Diffusion Models around 1B (Sec. III-A) Representative Models (Sec. III) Large Diffusion Language Models (Sec. III-B) RDM [15], MLDM [16], MD4 [17] CSM [29], DWDSE [16], Categorical Ratio Matching Loss [30], RADD [22] D3PM [13], DiffusionBERT [32], RDM [15], Masked-Diffuse LM [33], Diffusion-NAT [34], TESS [35], Plaid [36], SEDD [37], MDLM [16], MD4 [17], UniDisc [38] LLaDA [6], DIFFUSION-LLMs[39], DiffuGPT & DiffuLLaMA[18], DREAM[7], LLaDA 1.5 [40], TESS 2[41] Discrete Diffusion (M)LLMs Training Techniques (Sec. IV) Large Diffusion Multimodal Models (Sec. III-C) Dimple [8], LaViDa [10], LLaDA-V [9] Large Unified Models (Sec. III-D) MMaDA [11], FUDOKI [42], Muddit [43] Initialization Technique (Sec. IV-B) Complementary Masking Technique [10] (Sec. IV-C) Masking Scheduling Technique (Sec. IV-D) Unmasking Techniques (Sec. V-A) BERT Initialization [32], Autoregressive Initialization[18, 7], Autoregressive-then Diffusion Training [8] Linear Schedule [13], Geometric Schedule [37], Cosine Schedule [44], Token-wise Masking Scheduling [32] Metrics Used in Unmasking: Confidence, Margin, Entropy [7], Confident Decoding [8], Semi-Autoregressive Unmasking [9], Continuous Time (Flow Matching) Unmasking [24] Inference Techniques (Sec. V) Remasking Techniques (Sec. V-B) Discrete Time Remasking [23], Continuous Time (Flow Matching) Remasking [24] Prefilling and Caching Technique (Sec. V-C) Prefilling [8, 10], dKV-Cache [25], dLLM-Cache [26] Guidance Technique (Sec. V-D) Classifier-Free Guidance [19], Classifier Guidance [45], Reward Guidance [41] Text Generation and Style Control (Sec. VI-A) DiffuSeq StylePTB [46], REASONAUG [47], SLD [48], DiffusPoll [49], PoetryDiffusion [50] Text Editing and Summarization (Sec. VI-B) EdiText [51], CrossMamba [52], DiffETM [53], TermDiffuSum [54] Sentiment Analysis and Data Augmentation (Sec. VI-C) CDA2 [55], DiffusionCLS [56], GDP [57] Knowledge and Reasoning (Sec. VI-D) DoT [58], DiffuCOMET [59], DPCL-Diff [60], d1 [61] Vision and Multimodal (Sec. VI-E) DiffVLA [62], UDAN-CLIP [63], VPDD [64], M2D2M [65], AR-Diffusion [66] Biological and Drug Discovery (Sec. VI-F) MolEditRL [67], CFP-Gen [68], TransDLM [69], GenMol [70], DPLM-2 [71], PepTune [72] Applications (Sec. VI) Fig. 2. Overview of our survey. We begin by introducing the mathematical foundations of discrete diffusion language models (Sec. II). Next, we present highlevel overview of representative base models (Sec. III), followed by discussions on training strategies (Sec. IV) and inference techniques (Sec. V). In addition, we also review wide range of applications (Sec. VI) that adopt discrete diffusion language models as their core model. training, generation and applications of discrete diffusion techniques in language, vision-language, and biology tasks. In the rest of this paper, Sec. II presents the mathematical foundations of discrete diffusion models. The core of mathematical formulations lies in the definitions of the forward and reverse diffusion processes over discrete spaces. These are modeled via Markov chains that incrementally corrupt and denoise sequences. The framework encompasses both discretetime and continuous-time formulations, supporting extensions such as flow matching and semi-autoregressive block diffusion model.Sec. III surveys representative discrete diffusion language models across varying scales. This includes early-stage models (e.g., D3PM [13], RDM [15], DiffusionBERT [32]), scaled dLLMs [6, 18, 7],dMLLMs [8, 10, 9], and unified models [11]. Sec. IV discusses the key training strategies used in dLLMs and dMLLMs, including initialization, masking strategies, and masking schedules. Sec. lists various inference techniques used in dLLMs and dMLLMs, including unmasking, remasking, caching and guidance. Sec. VI reviews the broad range of applications powered by discrete diffusion models. Finally, Sec. VII summarize potential directions for future research. The organization of this survey is illustrated in Fig. 2. II. MATHEMATICAL FORMULATIONS A. Discrete Denoising Diffusion Probabilistic Model (D3PM) Diffusion models with discrete state spaces were initially introduced in [27] for binary variables, and later extended to categorical variables in [14]. Based on the previous works, Discrete Denoising Diffusion Probabilistic Models (D3PMs) [13] provides general and flexible framework. Let x0 q(x0) denote the data distribution over sequences composed of categorical values. The D3PM framework defines forward Markov process q(x1:T x0) that gradually corrupts the data into noise, and parameterized reverse process pθ(x0:T ) that learns to denoise: q(x1:T x0) = (cid:89) t=1 q(xt xt1), pθ(x0:T ) = p(xT ) (cid:89) t=1 pθ(xt1 xt). (1) (2) Each xt is discrete random variable and q(xt xt1) transition matrix Qt, with is defined via time-dependent categorical transition probabilities: q(xt xt1) = Cat(xt; = xt1Qt), (3) where xt1 is one-hot vector and Qt RKK is rowstochastic matrix. The marginal distribution q(xt x0) and the posterior q(xt1 xt, x0) are: q(xt x0) = Cat(xt; = x0Q1:t), Q1:t = Q1Q2 . . . Qt, (4) (5) q(xt1 xt, x0) = Cat xt1; = (cid:18) xtQ x0Q1:t1 x0Q1:tx (cid:19) . (6) D3PM framework support various types of transition matrices Qt, each inducing different corruption behavior: Uniform: Qt = (1 βt)I + βt 11 yields uniform stationary distribution. 4 Absorbing: Tokens either remain unchanged or are mapped to special [MASK] token with probability βt. Discretized Gaussian: For ordinal data, transitions favor neighboring categories, imitating Gaussian kernels. Embedding-based: Transition probabilities are based on token similarity in embedding space, guiding structured corruption. Following the x0-parameterization, the model predicts pθ(xt1 xt) using: pθ(xt1 xt) (cid:88) q(xt1, xt x0)pθ(x0 xt), (7) where pθ(x0 xt) is network predicting logits over x0. This parameterization ensures the reverse distribution respects the sparsity pattern of Qt."
        },
        {
            "title": "The loss function combines the variational lower bound Lvb",
            "content": "with an auxiliary cross-entropy denoising term Lce: Lλ = Lvb + λ Lce, Lvb = Eq(x0) (cid:104) KL(q(xT x0) p(xT )) (cid:123)(cid:122) (cid:125) (cid:124) LT (8) (cid:88) + Eq(xtx0) [KL(q(xt1 xt, x0) pθ(xt1 xt))] t=2 (cid:124) (cid:123)(cid:122) Lt1 Eq(x1x0) [log pθ(x0 x1)] (cid:123)(cid:122) (cid:125) L0 (cid:124) (cid:105) , Lce = Eq(x0)Eq(xtx0)[ log pθ(x0 xt)]. In Lvb, (cid:125) (9) (10) LT is the KL divergence between the forward terminal distribution q(xT x0) and the prior p(xT ), Lt1 is the KL divergence between the forward posterior q(xt1 xt, x0) and the learned reverse model pθ(xt1 xt) at each intermediate step, L0 is the cross-entropy loss for reconstructing x0 from x1 using the reverse model. Such decomposition enables the model to be trained efficiently by sampling time steps uniformly and estimating each term using stochastic gradient descent. The forward posterior q(xt1 xt, x0) has closed-form expression under categorical diffusion, and the model is typically parameterized to predict pθ(x0 xt), from which pθ(xt1 xt) is derived analytically. The auxiliary Lce is added to encourage accurate prediction of the original data x0 from corrupted samples. Besides flexible representation of discrete diffusion, D3PM also unifies various paradigms such as BERT, autoregressive models, and masked language models within the discrete diffusion framework. B. Reparameterized Discrete Diffusion Model (RDM) Reparameterized Discrete Diffusion Models (RDMs) [15] reformulate the backward process of discrete diffusion in D3PM into two-stage sampling procedure. The core idea is to introduce latent routing variable vt that determines decoding behavior for each token at every step. Given forward transition of the form: C. Continuous Time Discrete Denoising Models q(xt xt1) = βtxt1 + (1 βt)qnoise, q(xt x0) = αtxt1 + (1 αt)qnoise, where qnoise is the noise distribution, and αt := (cid:81)t i=1 βi. The backward posterior q(xt1 xt, x0) can be expressed as mixture of two cases, depending on whether xt = x0: (11) (12) (cid:40) q(xt1 xt, x0) = λ(1) t1xt + (1 λ(1) t1x0 + (1 λ(2) λ(2) t1)qnoise, xt = x0, t1)qnoise(xt), xt = x0, (13) where qnoise(xt) is the interpolation between xt and qnoise, and λ(1) t1, λ(2) t1 are scalar coefficients derived from βt, αt and qnoise(xt). To reparameterize the backward transition, RDM introduces Bernoulli latent variables: v(1) t1 Bernoulli(λ(1) v(2) t1 Bernoulli(λ(2) t1), t1), u(1) qnoise, u(2) qnoise(xt). (14) (15) Let bt = 1[xt = x0], the reparameterized sample is computed as: (cid:104) xt1 = bt t1xt + (1 v(1) v(1) (cid:104) t1x0 + (1 v(2) v(2) t1)u(1) + (1 bt) (cid:105) t1)u(2) (cid:105) . (16) This formulation allows the model to explicitly route tokens through different behaviors: either retaining the current token, resetting to noise, or denoising back to the ground truth. Based on Eq. (16), RDMs define joint generative model: pθ(x0, x1:T , v1:T ) = pθ(xT ) (cid:89) t=1 pθ(xt1, vt1 xt), (17) and the evidence lower bound (ELBO) becomes: log pθ(x0) L1 (cid:88) t=2 Lt + C, (18) where is constant. For > 1, the loss term decomposes as: Lt = Ex1:T ,v1:T (cid:104) KL(q(vt1) pθ(vt1 xt)) + KL(q(xt1 vt1, xt, x0) pθ(xt1 vt1, xt)) (cid:105) . (19) By aligning pθ(vt1 xt) with q(vt1) and using the x0parameterization, the loss can be simplified into Lt = Ex0,x1:T (cid:104) λ(2) t1 (cid:88) (1 bt,n)x 0,n log (xt,n; θ) n=1 (cid:105) , (20) t1 = αt1αt 1αt where λ(2) . This loss corresponds to reweighted cross-entropy loss evaluated only over noisy tokens. Such loss formulation is significantly simpler than the original variational bound and has become the standard training objective for subsequent large discrete diffusion models. Similar loss are also derived in [16, 17], with different simplification and reparameterization tricks. D3PM operates in discrete time, i.e., with time steps = 0, 1, 2, . . . , . [28] describes continuous-time framework for discrete denoising models, formulated as Continuous-Time Markov Chain (CTMC), where [0, ]. This approach generalizes discrete diffusion models by allowing transitions at arbitrary time points. The infinitesimal transition probabilities are given by: qttt(x x) = δx,x + Rt(x, x)t + o(t). (21) This process converges to tractable reference distribution as . The time-reversed generative process is another CTMC with reverse rate matrix ˆRt, expressed as: qt0(x x0) qt0(x x0) ˆRt(x, x) = Rt(x, x) pθ(x0 x), (22) (cid:88) x0 where pθ(x0 x) is learnable denoising model approximating q0t(x0 x). The training of pθ(x0 x) is guided by continuouslower bound. Let Zt(x) = time version of the variational (cid:80) x=x Rt(x, x) be the total outgoing rate and rt(x x) = Rt(x, x)/Zt(x) the normalized jump probability. The continuous-time variational lower bound is: Lvb(θ) =T EtU (0,T )Exqt (cid:88) Exrt(x) (cid:104) ˆRθ (x, x) Zt(x) log ˆRθ (cid:105) (x, x) + C, (23) x=x where is constant with respect to θ. This objective can be efficiently optimized using stochastic gradient descent by sampling (x, x) pairs according to the forward process. During inference, however, the exact simulation of the reverse CTMC can be computationally prohibitive. Instead, the tauleaping algorithm [reference] approximates the reverse process by applying multiple transitions within time interval τ simultaneously. For current state xt, the number of transitions to each during [t τ, t] is modeled as: Px Poisson(τ ˆRθ (xt, x)). (24) The next state xtτ is obtained by aggregating the sampled transitions. This method supports parallel decoding by allowing simultaneous updates across multiple dimensions. To further improve sample fidelity, predictor-corrector steps is used. After tau-leaping step, corrector transitions with rate matrix Rc = Rt + ˆRθ are applied to refine the sample distribution toward the target marginal qt(x). This approach is analogous to Langevin correctors in continuous diffusion models. D. Concrete Score In the continuous-time discrete diffusion framework, as formulated in previous [28], the reverse process can also be analytically expressed in terms of the forward transition rate matrix and function known as the concrete score [29]. This construction enables training via score matching, analogous to score-based models in continuous domains. Let Rt(x, x) be the forward transition rate matrix of continuous-time Markov chain (CTMC) over discrete state space . The reverse-time transition rate ˆRt(x, x) can be formulated as: (25) ˆRt(x, x) = pt(x) pt(x) (cid:88) Rt(x, x), = x, ˆRt(x, k), = x. Here, the scalar ratio pt(x) pt(x) is referred to as the concrete score. It quantifies the relative likelihood of two discrete states at time and modulates the reverse transition rate accordingly. k=x Thus, instead of learning the full reverse transition distribution, one can train model sθ(x, t) to estimate the concrete score: (cid:20) pt(x) pt(x) 1) Training Loss: Training is typically done by minimizing divergence-based losses that compare the predicted ratio to the true ratio. sθ(x, t) (26) xX (cid:21) . a) Concrete Score Matching (CSM) [29]: The most direct approach is Concrete Score Matching (CSM) [29], which uses squared-error loss to match the predicted and true ratios: LCSM(t) = 1 2 Expt (cid:88) x=x (cid:18) sθ(x, t)x (cid:19)2 pt(x) pt(x) (27) While theoretically sound, this ℓ2 loss does not penalize invalid (e.g., negative or zero) predictions sufficiently, potentially leading to instability. b) Diffusion-Weighted Denoising Score Entropy [37]: Leveraging Bregman divergence, score entropy is formulated as another score matching loss. Score entropy is non-negative, symmetric, and convex. It is also an extension of the conventional cross-entropy to general positive-valued functions beyond the probability simplex. Score entropy enables the construction of an ELBO for discrete diffusion models, resulting in the Diffusion-Weighted Denoising Score Entropy (DWDSE) loss [37]. LDWDSE(x0) = (cid:90) 0 Extpt0(x0) (cid:88) (cid:32) Rt(xt, x) x=xt pt0(xx0) pt0(xtx0) (cid:19) (cid:33) sθ(xt, t)x log sθ(xt, t)x + (cid:18) pt0(xx0) pt0(xtx0) dt. (28) 2) Connection with CE Loss: [30] leverages the theorem stating that two probability distributions are identical if and only if their conditional distributions are equal for all values of the condition. Based on this, the original marginal probability distributions in the concrete score are transformed into conditional probability distributions. Since both the numerator and denominator of the concrete score share the same functional form, they can be represented by single neural network. That is, the concrete score can be expressed as: pt(X xd; θ) qt(X xd) qt(X qt(yd, xd) = yd xd) qt(X qt(xd, xd) = xd xd) = pt(X pt(X = yd xd; θ) = xd xd; θ) . (29) From this perspective, [30] propose the categorical ratio matching loss, which is cross-entropy loss to fit conditional probabilities. Thus, [30] shows that training neural network with cross-entropy loss is also able to fit the concrete score. 3) Time Independency: An issue with the Concrete Score is its dependency on time, which prevents the use of caching techniques for inference and results in lower efficiency. [22] shows that the concrete score in absorbing discrete diffusion can be reparameterized as product of time-dependent scalar and conditional distribution over clean data. Specifically, in practice, Rt can be parameterized as the multiplication between scalar function and constant rate, i.e., Rt = σ(t)R. let xi = [M] denote masked token at position i. Then, the concrete score for replacing xi with token = [M] is defined as: pt(x) pt(x) = pt(x1, . . . , i, . . . ) pt(x1, . . . , xi, . . . ) = eσ(t) 1 eσ(t) p0(x xUM), (30) where: 0 σ(s)ds is the cumulative noise schedule, σ(t) = (cid:82) xUM consists of all unmasked tokens of x, p0(x data. xUM) is the conditional distribution of the clean This reparameterization removes the dependence on from the model output, enabling the training of time-independent network cθ: cθ(x)[i, ˆx(i)] p0(ˆx(i) xUM). (31) Such model, termed RADD (Reparameterized Absorbing Discrete Diffusion), permits caching of the network outputs across steps when tokens remain unchanged, reducing the number of function evaluations (NFEs) during inference. E. Discrete Flow Matching (DFM) Build upon the continuous-time Markov chain (CTMC) framework in Continuous Time Discrete Denoising Models [28], Discrete Flow Matching (DFM) [24] extends the Flow Matching paradigm to categorical sequence data. The model defines probability path pt interpolating between source distribution (e.g., all-mask sequences) and target distribution (e.g., the data distribution), such that p0 = and p1 = q. Given coupling distribution π(x0, x1) between source and target sequences, the marginal probability at time is defined as: pt(x) = pt(x x0, x1)π(x0, x1), (32) (cid:88) x0,x1D where the conditional path pt(x x0, x1) factorizes over positions: pt(x x0, x1) = (cid:89) pt(xi x0, x1), (33) i=1 with token-level conditional paths defined as convex combination of basis distributions: pt(xi x0, x1) = (cid:88) j= t,jwj(xi x0, x1), κi (34) t,j 0, (cid:80) where κi path dynamics. κi t,j = 1 form scheduler controlling the The generative process is defined via probability velocity i=1 guiding transitions between states. The update fields {ui t}N rule for sampling is: KV caching can be used to avoid recomputing transformer states for previously generated blocks."
        },
        {
            "title": "The training loss for the full sequence is obtained by summing",
            "content": "7 xi t+h δxi () + hui t(, xt), (35) the variational lower bound over all blocks: where ut is said to generate pt if the process satisfies: log pθ(x) LBD(x; θ) := (cid:88) b=1 L(x(b), x(<b); θ), (44) pt+h(x) = pt(x) divx(ptut) + o(h), (36) with the discrete divergence operator: divx(v) = (cid:88) zD [v(z, x) v(x, z)] . (37) For the convex interpolation path: pt(xi x0, x1) = (1 κt)δx0 (xi) + κtδx1(xi), (38) the corresponding generating velocity takes the closed form: t(xi, z) = ui κt 1 κt (cid:2)p1t(xi z) δz(xi)(cid:3) , (39) where p1t(xi z) is the probability denoiser: the conditional probability of the target token xi 1 given the current state z. where each L(x(b), x(<b); θ) follows the discrete diffusion loss, optionally adapted to continuous time or masking processes. III. LARGE DISCRETE DIFFUSION LANGUAGE AND MULTIMODAL MODELS In this section, we provide high-level overview of the representative works. In the following sections, we give detailed discussions on the training paradigms and inference-time decoding strategies of the models scaled to sizes comparable to LLMs. An evolutionary diagram of representative dLLM and dMLLM models is shown in Fig. 1. To estimate the posteriors required in the generative process, A. Discrete Diffusion Models around 1B the model minimizes the cross-entropy loss: L(θ) = (cid:88) i=1 Et,(x0,x1),xt (cid:2)log p1t(xi 1 xt; θ)(cid:3) , (40) where xt pt( x0, x1). F. Block Diffusion Models Block Diffusion models (BD3-LMs) [31] provide hybrid framework that interpolates between autoregressive language models and fully parallel diffusion models. Instead of denoising all tokens simultaneously, BD3-LMs segment the sequence into blocks and perform discrete denoising diffusion within each block, while conditioning autoregressively on all preceding blocks. Given sequence = (x1, . . . , xL), BD3-LMs partition it into blocks of length L, denoted = (x(1), . . . , x(B)). The joint likelihood under Block Diffusion model is factorized autoregressively across blocks: log pθ(x) = (cid:88) b=1 log pθ(x(b) x(<b)), (41) where each conditional distribution pθ(x(b) x(<b)) is modeled by discrete diffusion process applied within block b: pθ(x(b) = (cid:88) x(b) q(x(b) , x(<b)) x(b) , x(b)) pθ(x(b) x(b) , x(<b)), (42) where q( x(b) x(b) ) is the forward noise process, and pθ(x(b) , x(<b)) is the learned denoising model. The model is parameterized by transformer fθ with blockcausal attention mask. For each block x(b), the model predicts: , x(<b)(cid:17) (43) (cid:16) ˆx(b) 0 . x(b) fθ During inference, block sampling proceeds sequentially over blocks, but parallel sampling is used within blocks. Block-wise Discrete diffusion has emerged as compelling alternative to continuous diffusion for modeling both discrete data and continuous data. [27] first introduces diffusion process over binary variables. This idea is generalized to categorical variables by [14], who demonstrates its effectiveness on image generation. Building on these foundations, D3PM [13] proposes more flexible family of noising schedules that extends discrete diffusion to broader class of discrete spaces (see Section II-A). DiffusionBERT [32] explores training BERT [73] to reverse discrete diffusion process with an absorbing state, introducing token-aware noise schedule for the forward pass and methods to embed time-step information into BERT. [15] derives an alternative yet equivalent formulation of the sampling from discrete diffusion process (see Section II-B) and introduces new model family named Reparameterized Discrete Diffusion Models (RDMs). It reformulate the backward process of discrete diffusion in D3PM [13] into two-stage sampling procedure and yield greatly simplified training objective and enable more flexible decoding algorithms for text generation. Masked-Diffuse LM (MDLM) [33] propose to leverage the inherit linguistic features of texts to encourage the model to recover the text following an easy-first-generation nature, and directly predict the discrete token with cross-entropy loss to stabilize the intermediate diffusion steps. Diffusion-NAT [34] uses the pretrained BART [74] as the language backbone and unifies the inference process of pretrained language models and the denoising process of discrete diffusion models in nonautoregressive manner, thus the BART model plays the role of the parameterized denoiser in discrete diffusion models. Furthermore, TESS [35] leverages new form of self-conditioning and applies diffusion on the logit simplex instead of the learned embedding space. Plaid [36] takes the first step towards closing the likelihood gap between autoregressive and diffusion-based language models through algorithmic improvements, scaling laws, and increased compute. SEDD [37] generalize the idea of score matching into the discrete spaces by proposing novel loss named score entropy, which can be integrated seamlessly 8 Fig. 3. This figure compares autoregressive models and discrete diffusion models from four perspectives. First, regarding model architecture, both models share the same network structure; the key difference lies in their generation mechanisms. In addition to the LLM, both MLLM and dMLLM require an additional vision encoder. In terms of the attention mask, the autoregressive model uses causal attention mask, whereas the discrete diffusion model adopts full (bidirectional) attention mask. During inference, the autoregressive model starts from BoS token and generates tokens one by one from left to right. In contrast, the discrete diffusion model begins with sequence of mask tokens and denoises all tokens in parallel. At each step, subset of tokens is selected and replaced with non-mask tokens, continuing until no mask tokens remain. For training, the autoregressive model directly takes the input sequence and applies next-token prediction loss. The discrete diffusion model first randomly masks the input tokens and then computes weighted cross-entropy loss over the masked positions. to build discrete diffusion models and can significantly boost model performance. MLDM [16] introduces substitution-based parameterization (SUBS) of the reverse unmasking diffusion process, allowing to derive simple, continuous-time, Rao-Blackwellized objective that improves tightness and variance of the ELBO, further increasing performance. Besides the training design, the MLDM model is also equipped with fast samplers that support semi-autoregressive (SAR) generation. MD4 [17] begins by analyzing the properties of the forward process and its time reversal induced by the discrete diffusion model, then deriving remarkably simple expression for the Evidence Lower Bound (ELBO), showing that it corresponds to weighted time integral of cross-entropy losses, and can be rewritten in terms of signal-to-noise ratio, exhibiting invariance properties akin to those in continuous diffusion models. Building on this, generalized masked diffusion model MD4 that allows statedependent masking schedules is proposed, leading to improved predictive performance. For unified models, UniDisc [38] is prior work on unified discrete diffusion model for text and image modalities. Conceptually, UniDisc treats an image and caption as two token sequences (from discrete codebooks) and denoises them together. It uses decoder-only Transformer with bidirectional attention, embedding both modalities via appropriate positional encodings. B. Large Diffusion Language Models 1) LLaDA: [6] introduces LLaDA, the first discrete diffusionbased alternative to autoregressive LLMs. Instead of predicting one token at time in left-to-right manner, LLaDA generates text by gradually denoising masked text: forward process randomly masks tokens in the input, and reverse process uses Transformer to predict the masked tokens. This Transformer mask predictor has an architecture similar to standard LLMs, but notably omits the causal masking, allowing bidirectional context since the model sees the entire sequence during prediction. LLaDA follows principled generative modeling approach by optimizing variational likelihood bound (ELBO) [75] rather than the exact log-likelihood. This design leverages the full context of text and the theoretical grounding of diffusion models to potentially achieve LLM capabilities beyond the autoregressive paradigm. LLaDA demonstrates that dLLMs can attain competitive performance with same-size LLMs. Specifically, LLaDA 8B achieved comparable zero-/few-shot accuracy to LLaMA-3 8B [76] on suite of 15 standard tasks [6], despite not using an autoregressive decoder. After instruction-tuning, LLaDA shows strong ability to follow complex instructions in dialogues, comparable to other fine-tuned LLMs. Notably, due to its bidirectional generation, LLaDA overcomes certain challenges that stump AR models: for example, it experimentally breaks the reversal curse [6] and can correctly complete reversedorder poem task, surpassing even GPT-4 in such evaluation [75]. These results establish that dLLMs are viable and promising alternative to LLMs. 2) DIFFUSION-LLMs: [39] builds dLLMs in multi-stage process. First, they leverage the intrinsic connection between masked language modeling and diffusion. transformer is firstly pretrained on massive data using the masked LLM objective (similar to BERT [73]) to absorb world knowledge. This creates strong base model with bidirectional representations. Then, they reprogram the pretrained masked LLM into dLLM via diffusive adaptation step. In practice, this means continuing to train the model to generate text by iterative denoising (rather than just single-step masking)effectively converting the static masked LLM into generative diffusion model. Finally, they perform supervised finetuning on specific tasks and crucially an instruction-tuning phase, where the model is trained on broad set of natural language instructions and answers. This pipeline scale up pretraining, adapt to diffusion generation, and then finetune on instructionsyields dLLM that can follow human instructions to solve extensive tasks. Results show that as the pretraining data and model size increased, the models performance on downstream tasks rises steadily, mirroring classical scaling laws observed in AR LLMs [77, 78]. Moreover, after instruction-tuning, the dLLM begins to demonstrate zero-shot and few-shot learning abilitiesit could tackle unseen tasks by following natural language instruction, even with just few in-context examples. This is hallmark of modern LLMs like GPT-3 [79], and the fact that dLLM can exhibit such in-context learning is significant result. However, the study also notes that without the generative surgery [39] 9 of diffusive adaptation, simply scaling masked LLM pretraining alone can not confer strong generative abilitiesunderscoring that the adaptation step is essential. In summary, this work provides the first clear evidence that dLLMs can be generalpurpose performers. It highlights the importance of scale and instruction-tuning, laying the groundwork for subsequent large diffusion models. 3) DiffuGPT & DiffuLLaMA: At the meantime, [18] proposes to convert pretrained AR transformer (like GPT-2 [80] or LLaMA [81]) into dLLM, thereby avoiding the need to train large model purely from scratch. They demonstrate simple adaptation process: continue training the AR model on masking-based diffusion objective while reusing its learned weights. In practice, they transform AR-based language models ranging from 127M to 7B parameters into diffusion-based models dubbed DiffuGPT (from GPT-2) and DiffuLLaMA (from LLaMA). Crucially, [18] gives theoretical connections between AR models next-token prediction and diffusion models denoising objective, which allowed them to align the two paradigms during adaptation. By using the AR models knowledge as an initialization, the diffusion model could be scaled up with far less data (under 200 billion tokens of additional training, vs. trillions for scratch model). This work makes large dLLMs feasible with limited compute, addressing key bottleneck in scaling. The DiffuGPT & DiffuLLaMA model series shows comparable performance of AR LLMs. For example, DiffuGPT-355M not only exceeds previous dLLMs of similar size, but even outperforms the original GPT-2 on most evaluated tasks. The adapted DiffuLLaMA-7B achieves competitive results close to LLaMA-2-7B [81] (its AR counterpart), although it slightly trails the AR model on some benchmarks (likely due to havthese dLLMs retain tokens). Notably, ing seen fewer total important abilities of the base LLMs: they produce fluent, coherent text and demonstrate in-context learning on par with AR models. Thanks to their bidirectional nature, they can also natively perform infilling without special prompt engineering or reordering, which AR models typically struggle with. In inference, DiffuGPT & DiffuLLaMA can trade generation speed for quality by adjusting the number of diffusion iterations, and the adapted models often require fewer refinement steps to reach good fluency compared to other dLLMs. Overall, scaling dLLMs via adaptation demonstrates practical path to build high-performing dLLMs by capitalizing on the vast knowledge in pretrained AR LLMs. Besides, the released suite of adapted models (127M, 355M, 7B) provides evidence that dLLMs can be scaled up and offering foundation for further research. 4) DREAM: DREAM 7B [7] is one of the most powerful open-source dLLMs to date. The name DREAM underscores its focus on diffusion for reasoning and modelingit is specifically designed to excel at complex reasoning tasks while remaining efficient and scalable. DREAM 7B achieves performance on par with, or exceeding, autoregressive models of similar size (e.g. it matches LLaMA3 8B [76] and Qwen 2.5-7B [82] on many benchmarks). key to DREAMs success is an optimized training recipe distilled from extensive experiments at smaller scales. [7] carefully explores the design choices on 1B model and identify two especially valuable components: (1) AR weight initialization and (2) context adaptive noise scheduling. too high would wash out For (1), DREAMs transformer is initialized with the pretrained weights of strong AR model (Qwen 2.5-7B and LLaMA3-8B). This is experimentally proved to be more effective than training from scratch, as it preserves useful leftto-right knowledge during early training. Setting moderate learning rate is also crucial: the AR-inherited knowledge, while too low would hinder learning diffusion-specific patterns. By tuning this properly, DREAM leverages the existing AR knowledge to accelerate any-order (bidirectional) learning, reducing the data needed for training. For (2), DREAM introduce novel context-adaptive, tokenlevel noise rescheduling mechanism. In conventional discrete diffusion training, single noise level is sampled for the whole sequence, meaning every token is masked with the same probability. This can be suboptimal: tokens with different context availability (e.g. token whose neighbors are all masked vs. one whose neighbors are mostly visible) effectively experience different noise severity. Some tokens might be too difficult (no context to infer from) or too easy (fully predictable from context) at given global noise level. To address this, DREAM measures each tokens context information after masking and then dynamically reassigns an individual noise level to each token. In essence, the model reschedules the diffusion timestep per token: token with little context left is treated as if at higher effective noise (so the model knows its very uncertain), whereas token with rich context might be treated as lower noise. This token-level noise adaptation provides more precise guidance for learning each token, leading to better overall training efficiency and performance. 5) LLaDA 1.5: While LLaDA demonstrates strong performance after supervised finetuning, aligning dLLM with human preferences (akin to RLHF [83] for AR models) remains challenging. LLaDA 1.5 [40] specifically addresses this by introducing Variance-Reduced Preference Optimization (VRPO) for dLLMs. The core difficulty is that reinforcement learning or direct preference optimization for diffusion model requires estimating the models log-likelihood for given outputsbut diffusion models cannot compute an exact log-probability and instead rely on ELBO approximations. These ELBO-based likelihood estimates are noisy (high-variance), which in turn makes preference-based gradient updates extremely unstable. LLaDA 1.5 presents theoretical analysis of the variance and bias introduced by using ELBO estimates in preference optimization, and they derive bounds showing that reducing the variance of these estimates is key to improving alignment training. Building on this, VRPO provides unbiased variance reduction techniques to stabilize training. One technique is simply to increase the Monte Carlo sample size (using more random draws of diffusion time and mask patterns) when estimating the ELBO for given candidate output. More cleverly, VRPO allocates the sampling budget in an optimal way: their analysis shows that it is best to sample many different diffusion timesteps but only one mask per timestep. This optimal allocation minimizes variance for fixed total number of samples. Another technique in VRPO is antithetic sampling for preference comparisons. In preference optimization (e.g. Direct Preference Optimization or DPO [84]), the model compares the log-likelihoods of better output vs worse output. VRPO uses the same random noise for both outputs when estimating 10 their ELBO scores. By sharing diffusion timesteps and mask patterns between the winning and losing outputs, the random errors in their log-likelihood estimates tend to cancel out, greatly reducing variance in the difference. This is analogous to using the same random seed for paired evaluations to get more reliable comparison. With these methodsincreased samples, optimal allocation, and antithetic samplingVRPO achieves significant stability improvement in aligning the diffusion model. [40] applys VRPO to LLaDA and fine-tune it with preference model (reward model) on reasoning, math, code, and some other general alignment tasks. The resulting LLaDA 1.5 shows marked performance jump over the SFT-only LLaDA on all evaluated fronts. In summary, LLaDA 1.5 demonstrates that RLHF-style alignment is feasible for dLLMs and that addressing the ELBO variance issue is the key perspective. 6) TESS 2: TESS 2 [41] is dLLM that is not only largescale but also instruction-following and general-purpose. The training recipe for TESS 2 is culmination of ideas from prior works: it starts by adapting powerful AR base model via continued pretraining on the diffusion objective, and then applies thorough instruction-tuning to that adapted model. Tess 2 starts from using some of the strongest available AR LLMs as the starting point and performs adaptation similar to DiffuLLaMA [18]. By doing so, TESS 2 inherits high-quality knowledge base and linguistic capability from the AR model, which is criticalthe study also finds that both the adaptation procedure and the choice of base model are crucial for good dLLM. After adapting the base model to diffusion one, an instruction-tuning process is conducted. This step helps the model to follow wide range of natural language instructions, similar to how ChatGPT or InstructGPT [85] are trained, thus making TESS 2 adept at answering user queries, explaining reasoning, etc. C. Large Diffusion Multimodal Models 1) Dimple: Dimple [8] is one of the first Discrete Diffusion Multimodal Large Language Models (dMLLMs). Its base architecture (vision encoder + transformer LLM) resembles existing vision-language models (e.g. Qwen-VL [86], LLaVA [87, 88, 89]). One of the Dimples key innovations is its twostage hybrid training. In Stage 1, with the weights of Dream 7B [7] as an initialization, it is fine-tuned autoregressively on vision-language instruction data (for alignment and instruction following). In Stage 2, it is then further fine-tuned with discrete diffusion objective. This hybrid approach is devised because pure diffusion training is found to be unstable (leading to length bias and performance drop). By warming up with autoregressive training first, Dimple-7B achieves stable training and eventually surpasses the fully-autoregressive models. During inference, Dimple introduces confident decoding strategy for efficiency: the model dynamically chooses how many tokens to fill in at each step based on model confidence (see Sec. V-A). Empirically, this reduces the number of iterations to about response length . Dimple also re-implements an autoregressive prefilling trick: by filling in some tokens it speeds up inference by about from the existing context, 1.57 with minimal impact on quality. Under the same training budget and dataset as LLaVA-NEXT [89], Dimple-7B achieves higher aggregate scores on multimodal benchmarks 3 than LLaVA-NEXT-7B. This result shows that with proper hybrid training recipe, discrete dMLLM can match or exceed strong autoregressive baselines on vision-language tasks. 2) LaViDa: LaViDa [10] consists of vision encoder (e.g. SigLIP-400M [90]) and discrete diffusion Transformer. The input image is split into five views (four quadrants plus the full image); each view is encoded, average-pooled to reduce length, and concatenated into single 1D context. An MLP connector projects these image embeddings into the language models space. The language model is standard discrete dLLM (such as LLaDA-8B or Dream-7B). In effect, the model treats image tokens and text tokens uniformly in masked-denoising framework. The Training of LaViDa leverages the masked diffusion objective over pairs (image, prompt, answer). key innovation is complementary masking: for each training sample, two distinct mask patterns are created so that each token is masked in one of the two versions. This ensures that even short or rare answer tokens (e.g. object names in vision tasks) contribute to the loss and all tokens are learned efficiently, improving alignment between the visual encoder and the language model. During inference, LaViDa begins with fully-masked output sequence and iteratively denoise these tokens. It employs special Prefix-DLM [10] attention mask so that the encoded image and prompt tokens can be cached and reused. The model also uses timestep-shifting schedule to improve sample quality. Together, these yield faster, more controllable decoding, and supports bidirectional infilling for format constraints. it is fine-tuned on multimodal 3) LLaDA-V: LLaDA-V [9] is purely dMLLM built on LLaDA [6]. It integrates vision encoder and an MLP connector that projects visual features into the language embedding space, which helps the dLLM to process image inputs alongside text. LLaDA-V also uses masked diffusion rather than autoregressive decoding. The model undergoes visual instructiontuning process: instructionresponse data using the masked diffusion objective. Since the is diffusion-based, LLaDA-V naturally supports core model parallel decoding and controllable infilling. Even though the base language model of LLaDA-V is less performant on pure text tasks, experiments show it is highly competitive on visionlanguage benchmarks. When trained on the same multimodal data, LLaDA-V matches or approaches strong AR baselines (e.g. LLaMA3-V) and narrows the gap to the top models like Qwen2.5-VL. It achieves state-of-the-art results in multimodal understanding tasks, demonstrating that dMLLMs can be as effective as or better than hybrid or AR approaches in visionlanguage domains. D. Large Unified Model 1) MMaDA: MMaDA [11] employs unified diffusion architecture with shared probabilistic formulation across modalities. It uses single diffusion-based transformer for all data types images, etc.), rather than separate encoders for each (text, modality. This modality-agnostic design simplifies the model by eliminating modality-specific components. 11 model learns aligned reasoning across modalities. For example, the rationale for answering visual question is interleaved into the textual input. This CoT alignment provides form of coldstart for the final reinforcement learning (RL) stage, allowing complex multi-step reasoning from the outset. Finally, MMaDA proposes unified policy-gradient-based RL algorithm named UniGRPO. By incorporating diversified reward modeling, UniGRPO unifies the post-training process across both reasoning and generation tasks, leading to consistent performance improvements. 2) FUDOKI: FUDOKI [42] is unified multimodal model built on discrete flow matching [91]. It uses metric-induced probability path with kinetic-optimal velocities [91], which significantly improves over simple masking by enabling continuous self-correction during generation. For efficiency, FUDOKI is initialized from pretrained AR-based multimodal LLM (Janus-1.5B [92]) and then transferred to the discrete flow matching framework. Architecturally, FUDOKI firstly replaces the standard causal mask with full attention mask so that every token can attend to all others and thus improving global context modeling. Also, it shifts the output logits by one position (same as AR models) to retain the next-token prediction capability. In addition, unlike continuous diffusion models, FUDOKI does not use extra time-embedding layers; instead the model implicitly infers the corruption timestep from the input sequence. For input modalities, text is tokenized normally, while images are handled by separate pipelines: semantic encoder (SigLIP [90]) extracts features for image understanding, and pixel encoder/decoder [93] converts images to/from discrete image tokens for generation. At the output stage, FUDOKI has two output headsone predicting text tokens and one predicting image tokensand selects the appropriate head depending on the target modality during inference. 3) Muddit: Muddit [43] is unified model that uses purely discrete diffusion to handle text and images under one framework. The architecture of Muddit comprises single multimodal diffusion transformer (MM-DiT) [43], plus encoders/decoders for each modality. The MM-DiT follows dual-/single-stream design (similar to FLUX [94]) and is initialized from the pretrained Meissonic [95]. Inputs are quantized into shared token space: images are encoded by pretrained VQ-VAE [96] into discrete codebook indices, and text is encoded by CLIP text encoder [97]. During training and inference, the MM-DiT predicts masked tokens in this joint space. lightweight linear head maps the predicted tokens to actual text tokens for text output, while the VQ-VAE decoder reconstructs pixels from image tokens. Thus single generator handles both text and image tokens. IV. TRAINING TECHNIQUES In this section, we summarize the techniques employed during the training of diffusion language models (dLLMs) and diffusion multimodal language models (dMLLMs). A. Challenges During training, MMaDA is fine-tuned with mixed long chain-of-thought strategy. Reasoning steps from both text and vision tasks are converted into unified CoT format so that the The challenges in training dLLMs stem from low corpus utilization, high variance due to stochastic masking, degraded generation quality, and length bias. 1) Low Corpus Utilization: Unlike autoregressive training, where each token in the answer sequence contributes to the learning signal, discrete diffusion training applies supervision only to randomly selected subset of tokens at each time step. Given an input sequence of length = Lprompt + Lanswer, diffusion training samples timestep [1, ] and computes loss only over the masked tokens at that timestep. This leads to sparse supervision across training samples, resulting in inefficient utilization of the available corpus. 2) Random Sampling of Time Index: In diffusion training, the time index is randomly sampled for each training instance. As result, only single generation step is supervised per sample, while the decoding process at inference time typically involves multiple time steps. This mismatch introduces coverage gap between training and inference: although decoding requires iterative refinement over many steps, training provides gradient signals for only one of those steps per instance. 3) Length Bias: Diffusion models generate sequences of pre-specified length, without natural stopping mechanism like the [EOS] token in autoregressive models. Consequently, when trained purely with diffusion objectives, the model exhibits sensitivity to the target sequence length. Empirical results reveal significant performance fluctuation as the generation length varies, as shown in [8]. This phenomenon, referred to as length bias, indicates that diffusion models struggle to generalize across different output lengths under pure diffusion training. B. Initialization Technique To address the inefficiencies and instabilities in training dLLMs and dMLLMs, several works adopt advanced initialization strategies that convert the full diffusion training process into fine-tuning task. This approach accelerates convergence and enhances final model performance. 1) BERT Initialization: The diffusion generation process can be interpreted as multi-step masked language modeling (MLM) procedure. [32] initializes diffusion models from pretrained BERT. 2) Autoregressive Model Initialization: [18, 7] have explored direct adaptation from autoregressive language models by aligning the training objectives of the two paradigms. key technique enabling this transition is the shift operation. In standard diffusion training, the model predicts the original token x0 from its corrupted version xt at each timestep. However, this formulation differs from AR training, where each hidden state hi is trained to predict the next token xi+1 in left-toright fashion. To bridge this gap, [18, 7] propose shifting the output logits of the diffusion model by one position, such that the models prediction at position corresponds to token xi+1. This allows the diffusion model to be initialized with pretrained autoregressive models. 3) Autoregressive-then-Diffusion Training: Dimple [8] uses an autoregressive-then-diffusion training approach, demonstrating notable performance improvements for DMLLM. The Dimple training pipeline is divided into two distinct phases: Phase I: Autoregressive Training. In the first phase, Dimple is treated as an autoregressive model. It is trained using causal attention mask and next-token prediction loss. 12 Phase II: Diffusion Fine-tuning. After autoregressive training, the model transitions to discrete diffusion training regime. Full attention masks and timestep-dependent masked language modeling losses are re-enabled. This hybrid approach addresses several known limitations of pure diffusion trainingsuch as low corpus utilization and length biasby initializing the model with strong AR priors. With this hybrid training approach, Dimple achieves better alignment, improved instruction tuning, and enhanced robustness to response length variations. C. Complementary Masking Technique In [10], to ensure that all tokens participate in training, complementary masking constructs two complementary masked versions of each input sequence: Xt and . These versions have non-overlapping masked spans. For example, consider the sentence: The answer is dog. One masked variant might be: The [M] [M] dog. and its complement: [M] answer is [M]. This setup ensures that all tokens are eventually masked and optimized over the course of training. D. Masking Scheduling Technique Mask schedule governs the corruption process in the forward diffusion formulation. Specifically, the schedule defines the corruption level αt at each timestep t, thereby determining the proportion of tokens masked during training. An effective schedule balances learning stability and generation quality by controlling the signal-to-noise ratio across timesteps. 1) Uniform Masking Scheduling: Given timestep [0, 1], the forward process defines the corruption as: q(xt x0) = αtx0 + (1 αt)m, (45) where is the one-hot [MASK] token. The loss at each step is reweighted according to: wt = α 1 αt , (46) = dαt dt where α to time. Several scheduling strategies have been proposed and empirically studied: is the derivative of αt with respect Linear Schedule [13]: αt = 1 t, wt = 1 . (47) (48) αt = exp (cid:0) β1t Geometric Schedule [37]: (cid:1) , exp (cid:0) β1t βt max 1 exp (cid:0) β1t βt max βt max wt = min min (cid:32) (cid:1) min (cid:33) (cid:1) β1t min βt max log (49) (cid:19) . (cid:18) σmin σmax (50) TABLE THE INFERENCE TECHNIQUES USED DURING THE INFERENCE PROCESS OF EACH MODEL. Inference Technique Unmasking Top-st Strategy Confident Decoding Semi-Autoregressive Decoding Prefilling Reward Model Guidance Language Model Multimodal Model Unified Model LLaDA [6] DIFFUSION-LLMs [39] DiffuLLaMA [18] DREAM [7] LLaDA 1.5 [40] TESS 2 [41] Dimple [8] LaViDa [10] LLaDA-V [9] MMaDA [11] FUDOKI [42] Muddit [43] 13 Cosine Schedule [44]: αt = 1 cos (1 t) (cid:17) , (cid:17) (cid:16) π 2 (cid:16) π 2 tan (1 t) . wt = π 2 (51) (52) 2) Token-wise Masking Scheduling: Uniform masking scheduling uses the same scheduling for all tokens, which ignores the inherent variation in informativeness among different tokens. For example, different tokens carry different amounts of information, typically measured by entropy and dLLMs exhibit an easy-first decoding behaviori.e., common, low-entropy tokens are easier to predict earlier. [32] introduces token-wise masking schedule. The schedule defines custom corruption probability αi for each token position at timestep t, determined by: αi = 1 S(t) H(xi 0), (cid:19) (cid:18) tπ (cid:80)n j=1 H(xj 0) H(xi 0) , , S(t) = λ sin H(xi 0) = 1 (53) (54) (55) where H(xi 0) denotes the entropy of the i-th token, measuring its information content. S(t) is sinusoidal scaling function that ensures zero informativeness contribution at = 0 and = . λ is hyperparameter controlling the strength of entropy influence. The above formulation defines Spindle-shaped Noise Schedule, where the entropy-aware corruption pattern resembles spindle curve: more informative tokens are masked earlier and less informative ones later. This design ensures that low-entropy, easier-to-predict tokens are decoded first, leading to more coherent and stable sequence generation. V. INFERENCE TECHNIQUES A. Unmasking Techniques In dLLMs and dMLLMs, the model predicts all the response tokens at each step. However, only subset of the masked tokens are selected to be unmasked at every iteration, while the remainder remain masked. This iterative unmasking process gradually reveals the complete output sequence. The core challenges in this procedure are: (1) determining which tokens to unmask at each iteration, and (2) deciding how many tokens should be unmasked at each iteration. Table provides summary of the unmasking strategies adopted by representative models. Figure 4 provides detailed illustration of each type of unmasking strategy. In this section, we discuss each category in detail and describe the specific unmasking strategies proposed in each work. 1) Discrete-Time Unmasking: a) Random Unmasking: The simplest strategy is to randomly select st masked tokens to unmask at step t. The value of st can be fixed across steps or controlled by scheduling function as discussed in the training techniques, such as cosine scheduling [9]. Importantly, the inference-time masking schedule does not have to match that used during training. In fact, practically, it is often treated as tunable hyperparameter that can vary across different tasks. b) Metric-Based Unmasking: Rather than relying on random selection, metric-based strategies assign metric value to each token prediction and select tokens to be unmasked based on the metric value. Let RK be the predicted probability distribution over the vocabulary for given token, where is the vocabulary size. The following metrics are commonly used [7]: Maximum Probability (Confidence): = max(p), (56) indicating the models certainty about token. Margin: the most likely = ptop1 ptop2, (57) where ptop1 and ptop2 are the first and second highest probabilities, respectively. This measures how dominant the top prediction is. Negative Entropy: = (cid:88) i=1 pi log(pi + ϵ), (58) with small constant ϵ for numerical stability. This captures the peakedness of the distribution. c) Selection Policies: After obtaining the metric value for each token, the diffusion model performs selection based on different policies. Top-st Strategy [13]: Select the st tokens with the highest confidence scores for unmasking. The value of st follows the same scheduling principles as in random unmasking. Confident Decoding: As introduced in Dimple [8], this strategy dynamically selects the number of tokens to unmask based on fixed confidence threshold γ (0, 1). The motivation of this approach is that decoding should adapt to the semantic structure of the text: some steps may allow many tokens to be confidently predicted, while others may necessitate more caution. Thus, the number of decoded token should be adaptively adjusted at each step. At step t, the set of positions to decode is defined as: It = {i c(i) γ}, (59) where c(i) is the confidence score at position i. If It is non-empty, all tokens in It are unmasked. Otherwise, only the token with the highest confidence is selected. This approach enables: decoding multiple tokens in parallel when the model is confident, improving efficiency; avoiding low-confidence predictions, preserving generation quality. Local Unmasking: The above strategies can generally be categorized as global unmasking strategies. When selection requires sorting, global unmasking strategies aggregate the metric values of all tokens and perform sorting or selection on this global set. However, the unmasking process does not necessarily have to be global; it can also be local. Specifically, all tokens can be divided into multiple subgroups, with sorting and selection performed independently within each subgroup, or different unmasking strategies applied across different subgroups. typical example is the semi-autoregressive decoding strategy [6]. This approach divides the full response into multiple blocks, similar to block diffusion. During each forward pass, predictions are generated for all blocks simultaneously. However, the unmasking of tokens follows left-to-right, block-by-block order. Tokens in the next block are only allowed to be unmasked once all tokens in the previous block have been unmasked. Across these methods, common pattern arises: (1) score every masked position by confidence; (2) select subset according to rule (threshold, top-k, decaying ratio, block order, etc.) and unmask them; (3) repeat until no masks remain. Such metric-based schedules prioritise easy tokens first, reducing error propagation and enabling highly parallel generation. (Flow Matching): In continuous-time inference under the discrete flow matching framework (e.g., FUDOKI [42]), unmasking is modeled as stochastic jump process along probability path. 2) Continuous-Time Unmasking Let xt denote the current sequence state at time [0, 1], and let x1 be the target sequence. For each token position i, the update from xt to xt+h is governed by: 1) Sample predicted target ˆx(i) 2) Compute total transition rate (x, x(i) u(i) 1 pθ(x(i) λ(i) = ˆx(i) xt); (cid:88) 1 ); 1 x=x(i) 3) Draw random variable U(0, 1); 4) If 1 ehλ(i) , update x(i) by sampling from: x(i) t+h (x, x(i) u(i) λ(i) ˆx(i) 1 ) . This process dynamically determines which tokens to update (i.e., unmask) based on local transition rates. The higher the rate λ(i), the more likely token will jump to new value, 14 allowing the model to continuously refine its predictions in semantically meaningful way. B. Remasking Techniques For discrete diffusion models based on absorbing states, during inference, once token is unmasked (i.e., decoded), it remains unchanged in subsequent steps. This static behavior limits the models capacity to revise or refine earlier predictions. To address this, the remasking technique reintroduces masked tokens at previously unmasked positions, enabling iterative refinement of generated outputs. Remasking is able to extend the number of decoding steps beyond the response length, thereby allowing repeated updates to the response. This mechanism serves as form of test-time scaling, improving output quality through progressive correction. a) Remasking in General Discrete Diffusion Models.: [23] formulates the reversal diffusion process with remasking as qσ(zs zt, x) = (cid:40)Cat(zs; (1 σt)x + σtm), zs; αs(1σt)αt Cat (cid:16) 1αt + 1αsσtαt 1αt zt = m, , zt = m, (cid:17) (60) = m, where σt is used to control the ratio of remasked tokens. When zt the token has already been decoded. The model samples the next token zs from distribution that mixes the input and the mask token m, controlled by σt. This enables remasking by reintroducing uncertainty into already decoded tokens. When zt = m, the token is still masked. The sampling distribution is weighted combination of and m, adjusted by both αt and σt, allowing flexible control over how much information from the input or the mask dominates. b) Remasking under Discrete Flow Matching.: In the discrete flow matching framework [24], remasking is incorporated via velocity field that interpolates between forward and backward update directions: t(xi, z) = αt ˆui ui t(xi, z) βt ˇui t(xi, z), (61) and ˇui where ˆui denote the forward-time and backward-time velocity fields, respectively. This combined velocity ui is valid as long as αt, βt > 0 and satisfies the probability flow condition for (0, 1). When αt βt = 1, each step progresses forward in time with remasking (corrector sampling), enabling iterative refinement. When αtβt = 0, the model operates in stationary regime (corrector iteration), reintroducing noise and adjusting tokens within fixed diffusion step. C. Prefilling and Caching Techniques Prefilling and Key-Value Cache (KV-Cache) are standard inference acceleration techniques widely adopted in autoregressive language models. Intuitively, Prefilling and KV-Cache avoids redundant computation by storing the key and value representations from previous decoding steps, enabling the model to reuse them instead of recalculating at each new time step. Formally, in transformer decoder, the current hidden states hi are projected into query-key-value triplet (Qi, Ki, Vi) using learned projection matrices. For the first generation iteration = 1, the hidden state h0 corresponding to the first predicted 15 (a) Metric-Based Unmasking. (b) Confident Decoding. (c) Top-st Strategy. (d) Local Unmasking. (e) Continuous-Time Unmasking. Fig. 4. Unmasking strategies. We divide the unmasking strategies used in dLLMs and dMLLMs into two categories: Discrete-Time Unmasking (a,b,c,d) and Continuous-Time Unmasking (e). In discrete-time unmasking, besides random unmasking, there are other two unmasking strategies: Metric-Based Unmasking (Maximum Probability (Confidence), Margin and Negative Entropy, see (a)) and Selection Policies (Top-st Strategy (see (c)), Confident Decoding (see (b)), and Local Unmasking (see (d))). token and all prompt tokens is used to compute the new querykey-value triplet (Q1, K1, V1). The calculated key tensor K1 and value tensor V1 are cached for reusing. At iteration > 1, only the hidden state hi corresponding to the i-th response token is used to compute the new query-key-value triplet. The new key Ki and new value Vi are concatenated with all key and value vectors from all previous steps to form the complete key tensor and the value tensor. These key tensor and value tensor are cached and will be reused in the subsequent steps. (cid:40) (cid:40) = = Ki, concat (cid:0)K[1:i], Ki Vi, concat (cid:0)V[1:i], Vi if = 1, (cid:1) , otherwise, if = 1, (cid:1) , otherwise, (62) In this way, the attention computation at each step leverages the cached key-value pairs from previous steps can be written as zi = softmax V, (63) (cid:19) (cid:18) QiK dk significantly reducing redundant operations and improving inference efficiency in autoregressive decoding. In autoregressive models, the use of causal attention masks ensures that caching is theoretically lossless. This is attributed to the unidirectional nature of attention, where each token attends only to its preceding tokens. Consequently, previously computed key and value representations remain valid and unchanged throughout the generation process. Similarly, in semiautoregressive generation paradigms such as block-wise decoding [], caching can be applied between different blocks without introducing approximation errors. In contrast, dLLMs and dMLLMs employ full (bidirectional) attention mechanisms, wherein each token can attend to all other positions, regardless of their masking status. As result, even tokens that have already been decoded and unmasked may have their key and value vectors influenced by updates to other tokens during subsequent diffusion iterations. Despite this theoretical limitation, empirical studies have consistently validated the effectiveness of incorporating prefilling and KV cache techniques in dLLMs and dLLMs. These methods yield substantial acceleration in inference with only minimal degradation in model performance. 1) Prefilling: In multimodal large models, the inclusion of inputs significantly increases the number of prompt visual tokens, often exceeding the length of the response even without inference. Prefilling the prompt greatly enhances inference efficiency. The papers Dimple [8] and LaViDa [10] were among the first to apply prefilling techniques to dMLLMs, demonstrating the feasibility of this approach in multimodal large models. Dimples experimental results demonstrate that the use of prefilling incurs negligible performance degradation on the majority of vision-language benchmarks. This observation implies that current multimodal models process and encode visual information in largely static, one-pass manner, with limited dependence on the subsequent generation of textual responses. Furthermore, the adoption of prefilling yields substantial improvements in inference efficiency, achieving speedups ranging from 2 to 7. Notably, the magnitude of acceleration increases with larger batch sizes, indicating positive correlation between speedup and GPU utilization. 2) KV-Cache: Due to the use of bidirectional attention, the cached KV pairs, attention outputs, and other values in dLLMs and dMLLMs are not static. The associated caching algorithms typically consist of three components: caching, reuse, and update. The papers dKV-Cache [25] and dLLM-Cache [26] were among the first to introduce caching techniques into dLLMs. 1) Classifier-Free Guidance: [19] proposes an unsupervised classifier-free guidance strategy for discrete diffusion generation. The method performs two forward predictions at each diffusion timestep t: 16 dKV-Cache [25]. The core idea of dKV-Cache is to cache the key-value pairs when the tokens are unmasked and reuse the cached key-value pairs. Additionally, it introduces an interval hyperparameter to periodically update the cached key-value pairs. dLLM-Cache [26]. In addition to caching key-value pairs (KV), dLLM-Cache also stores attention outputs (AttnOut) and feed-forward network (FFNOut) outputs. During subsequent steps, it reuses the AttnOu and FFNOut rather than the raw KV pairs. The paper demonstrates that different interval hyperparameters can be set for the prompt and response segments, with the prompt cache requiring significantly less frequent updates than the response cache. For the response cache, besides periodical full updates, dLLMCache employs an Adaptive Partial Update strategy. In each iteration, portion of the cache is selectively updated. Specifically, the cosine similarity between the current and cached value vectors is computed to identify tokens with significant value changes. After each inference forward pass, subset of tokens is selected proportionally for KV cache updates. Fig. 5. Remasking in General Discrete Diffusion Models. The general experimental observations are that caching in dLLMs leads to performance fluctuations, and in most cases, results in performance degradation. The severity of degeneration increases with larger update intervals. However, with smaller update intervals (e.g., 2 to 8), performance degradation is minimal and often comparable to that of models without caching. D. Guidance Techniques In dLLMs and dMLLMs, post-processing on the predicted logits or sampling probabilities is commonly referred to as guidance, following terminology from image diffusion models. Guidance methods are used to influence the generation process toward desired characteristics, such as improved diversity or controllability. conditional prediction, conditioned on both the prompt p0 and the noisy response rt; An unconditional prediction, conditioned on sequence of mask token and the same response rt. The unconditional prediction captures the models inherent bias in the absence of instruction signals. The final guided prediction is adjusted as: pθ(r0 p0, rt) pθ(r0 p0, rt)1+w pθ(r0 m, rt)w , (64) where is tunable hyperparameter controlling the strength of the guidance. This guidance promotes text diversity by reducing the dominance of generic, encouraging prompt-independent responses. 2) Classifier Guidance: To improve controllability, for block diffusion model, [45] introduces classifier guidance framework that integrates class-conditional preferences into the sampling process. At each diffusion step for block b, the guided reverse process modifies the original sampling distribution pθ by incorporating the signal from classifier pξ, yielding: pγ(xs xt pθ(xs b, x<b, y) xt b, x<b) pξ(y xs b, xt b, x<b)γ, (65) where is the desired class label and γ controls the influence of the classifier. To reduce computational complexity, the method assumes intra-block independence and approximates the classifier as: pξ(y xs b, xt b, x<b) (cid:89) ℓ=1 pξ(y ˆxℓ b,ts, x<b), (66) where ˆxℓ replaced by the candidate token xs probability to be reformulated as: b,ts denotes the sequence with the ℓ-th token in xt b,ℓ. This allows the guided xt pγ(xs (cid:89) b, x<b, y) = b,ℓ xt pθ(xs pθ(x xt (cid:80) ℓ=1 b, x<b) pξ(y ˆxℓ b, x<b) pξ(y ˆxℓ b,ts, x<b)γ b,ts, x<b)γ . (67) By integrating classifier predictions with the models native probabilities, this approach enables fine-grained, attributeconditioned generation across blocks while maintaining computational feasibility. 3) Reward Guidance: TESS 2 [41] represents unique approach under the extra-model guidance category by leveraging an external reward model to guide token prediction. The main purpose of this method is to improve the quality of the generated response. Specifically, at each diffusion step, the model output ˆSθ is first transformed into token probability distribution: pt = softmax( ˆSθ), cw = Ept, (68) (69) where is the embedding matrix. The resulting continuous representation cw is fed into the reward model, which outputs scalar reward R. (a) Classifier-Free Guidance. (b) Classifier Guidance. (c) Reward Guidance. Fig. 6. Guidance Techniques. We divide the guidance techniques (i.e., the post-processing on the predicted logits or sampling probabilities) into three categories: (a) Classifier-Free Guidance, (b) Classifier Guidance, (c) and Reward Guidance. To maximize this reward, TESS 2 performs gradient ascent on the logits by computing θR and updates the model output: ˆSθ := ˆSθ + η θR, (70) where η is tunable guidance coefficient. This process is performed during inference only, and no guidance is applied during training. By incorporating gradient signals from the reward model, TESS 2 can steer the generation towards more desirable outputs without modifying the base diffusion model. VI. APPLICATIONS A. Text Generation and Style Control [46] uses diffusion-based language model for fine-grained text style transfer, trained on the StylePTB benchmark and achieving incredible results even with limited data and without external knowledge. [47] poses adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. Building on the idea of leveraging diffusion for controllable generation, [48] propose the Segment-Level Diffusion (SLD) framework, which segments long outputs and decodes them sequentially with an autoregressive decoder; as result, diffusion predictions are simplified while coherence and scalability improve, and experiments confirm SLDs fluency and contextual superiority over both diffusion and autoregressive baselines. For the social media applications, [49] introduces DiffusPoll, model that generates diversified, high-quality poll options from user comments by combining task-specific masking strategy with attribute-tag guidance; consequently DiffusPoll not only maintains quality and diversity but also better captures minority viewpoints. In the creative-writing domain, [50] present PoetryDiffusion, which jointly enforces semantic meaning and metrical form. By generating entire lines in single pass and integrating novel metrical-control module, PoetryDiffusion flexibly adjusts rhyme and structure, and experiments show it produces poems that meet both semantic and metrical requirements with high quality. B. Text Editing and Summarization EdiText [51] is controllable text-editing framework that operates at both coarse and fine levels to achieve target attributes; specifically, it fuses an SDEdit-style module for largescale edits with novel self-conditioning mechanism for subtle refinements, thereby enabling substantial stylistic changes while preserving meaning in tasks such as toxicity and sentiment control. Moving from editing to long-form generation, [52] designs discrete diffusion model for efficient abstractive summarization, where semantic-aware noising schedule, together with an adapted CrossMamba backbone, allows transformers to process long sequences under diffusion and yields strong results on Gigaword and CNN/DailyMail. Complementing summarization with unsupervised exploration of document themes, DiffETM [53] injects diffusion into the Embedded Topic Model so that the documenttopic distribution is sampled via more realistic stochastic processconsequently, the model better fits true topic structures while keeping optimization tractable, as confirmed on two standard corpora. Furthermore, focusing on domain-specific summarization, TermDiffuSum [54] leverages term-aware attention during diffusion and re-ranking loss to highlight legally salient sentences, thereby surpassing previous diffusion-based extractive methods on legal-document benchmarks. C. Sentiment Analysis and Data Augmentation The CDA2 framework [55] employs counterfactual diffusion augmentation to improve cross-domain adaptation in lowresource sentiment analysis. It generates counterfactual target samples by substituting domain-relevant words in source data and guiding the diffusion model to produce high-quality target domain examples. Experiments show that CDA2 generates higher-quality target samples and achieves state-of-the-art performance in cross-domain sentiment tasks. DiffusionCLS [56] examines the use of diffusion language model for data augmentation in low-resource sentiment classification. It generates pseudo-samples by reconstructing strongly sentiment-related tokens, thereby increasing data diversity while preserving labelconsistent information. This approach balances consistency and diversity without introducing excessive noise. Pushing diffusion further into structured sentiment tasks, the Grid Noise Diffusion Pinpoint Network (GDP) [57] introduces novel diffusionbased and template-guided approach for Aspect Sentiment Quad Prediction. It includes three modules: Diffusion Vague Learning to enhance learning under uncertainty, Consistency Likelihood Learning to capture commonalities of sentiment elements and mitigate noise, and GDP-FOR generation template for more natural outputs. Extensive experiments demonstrate the remarkable effectiveness of GDP on ASQP tasks. D. Knowledge and Reasoning Reasoning is one of the most important abilities of models in the large model era. In the dLLM scenario, some related works also emerged. Specifically, Diffusion-of-Thought (DoT) [58] firstly integrates chain-of-thought reasoning into dLLMs to enhance reasoning capabilities. Unlike traditional left-to-right autoregressive reasoning, DoT allows reasoning steps to diffuse over multiple steps, offering flexible trade-off between compute and reasoning performance. In addition, DiffuCOMET [59] develops series of models that leverage diffusion to infer contextually relevant commonsense knowledge from narratives. The method progressively refines commonsense fact representation anchored to the input context through multiple diffusion steps, producing inferences that are both contextually appropriate and diverse. On benchmarks like ComFact and WebNLG+, DiffuCOMET achieves better trade-off between commonsense diversity, contextual relevance, and alignment to known facts. DPCL-Diff [60] combines graph node diffusion with dual-domain periodic contrastive learning for temporal knowledge graph reasoning. Its Graph Node Diffusion (GNDiff) model introduces noise into sparsely related historical events to generate new event data that better matches the true distribution, enhancing the ability to reason about future events; the dualdomain learning maps periodic event entities to Poincare space and non-periodic entities to Euclidean space to improve representation power. Most recently, the d1 framework [61] adapts pretrained dLLMs into reasoning models via combination of supervised fine-tuning and reinforcement learning. It introduces novel critic-free policy-gradient algorithm called diffu-GRPO and employs masked SFT to distill reasoning knowledge directly from existing datasets, thereby enhancing the models reasoning abilities. E. Vision and Multimodal the model DiffVLA [62] introduces vision-language-guided diffusion policy for autonomous-driving planning. Specifically, it employs hybrid sparsedense diffusion strategy, guided by vision-language model, thereby improving both efficiency and action diversity in multi-modal driving scenarios. In imageenhancement context, UDAN-CLIP [63] proposes an image-toimage diffusion framework for underwater enhancement that jointly integrates vision-language model and spatial-attention leverages synthetic module. During pre-training, underwater data and CLIP-guided loss to retain in-air natural priors while correcting local degradations such as haze and low contrast. Extending diffusion policies to robotics, [64] present framework VPDD that first pre-trains on large-scale actionless human videos and then transfers the learned discrete diffusion policy to robot-control tasks. Here, both human and robot videos are encoded into unified video tokens; discrete diffusion model is trained generatively on the human corpus before being fine-tuned on small, action-labeled robot set. Turning to motion synthesis, M2D2M [65] employs discrete diffusion model to generate continuous human-motion sequences from textual descriptions of multiple actions. By adapting dynamic transition probabilities to token-level similarities and adopting two-phase sampling strategyindependent followed by joint denoisingit produces long-term, smooth, and contextually coherent motions, ultimately outperforming state-of-the-art multi-motion baselines. Finally, AR-Diffusion [66] introduces novel architecture that blends autoregressive 18 and diffusion techniques for flexible, asynchronous video generation. It gradually corrupts video frames during both training and inference to reduce phase discrepancies; meanwhile, nondecreasing length-control mechanismborrowed from autoregressive generationenables consistent handling of variablelength sequences, thereby enhancing temporal coherence. F. Biological and Drug Discovery Several recent studies leverage dLLMs and dMLLMs to advance molecular editing, protein engineering, and drug discovery. MolEditRL [67] is molecular-editing framework that combines discrete graph-diffusion model with reinforcement learning to optimize molecular properties while preserving structural similarity. It operates in two stages: first, conditional diffusion model reconstructs target molecule from the source structure and textual instructions; second, reinforcement learning fine-tunes the editing actions, thereby further improving property alignment and structural conservation. Building on the idea of diffusion for biomacromolecules, CFP-Gen [68] adopts diffusion language model for combinatorial functional protein generation, thus enabling de novo design under simultaneous functional, sequence, and structural constraints. It introduces two key modulesAnnotation-Guided Feature Modulation (AGFM) and Residue-Controlled Functional Encoding (RCFE)that dynamically adjust protein features according to functional annotations and precisely control residue-level interactions. As result, CFP-Gen can create novel proteins whose functionality rivals that of natural counterparts, and it attains high success rate in designing multifunctional proteins. In related vein, TransDLM [69] proposes text-guided, multi-property molecular-optimization method that leverages diffusion language model. By encoding molecules with standardized chemical nomenclature and embedding property requirements directly into textual descriptions, TransDLM implicitly enforces multiple objectives and thereby reduces error propagation during optimization. Further generalizing diffusion to drug discovery, GenMol [70] presents single, discrete diffusion model that serves as versatile generator across diverse pharmaceutical tasks. It produces Sequential Attachment-based Fragment Embedding (SAFE) sequences via non-autoregressive bidirectional decoding, thus avoiding token-order constraints and boosting sampling efficiency. GenMol also treats molecular fragments as basic building blocks and introduces fragmentremasking strategy to refine candidate structures. Addressing sequencestructure co-design, DPLM-2 [71] is multimodal protein language model capable of understanding and generating both protein sequences and their three-dimensional structures. It converts 3-D coordinates into discrete tokens through quantization, then jointly trains on sequence and structure data, thereby capturing complex sequence-structure relationships and improving tasks that demand structure-conditioned sequence generation. PepTune [72] targets therapeutic-peptide design with multi-objective, discrete diffusion framework built on masked language-model backbone. It introduces bond-dependent masking and Monte-Carlo Tree Guidance algorithm that balances exploration and exploitation during inference, iteratively refining peptides toward multiple goals. Consequently, PepTune generates chemically diverse peptides that are simultaneously optimized for properties such as binding affinity, membrane permeability, solubility, and hemolysis. VII. FUTURE DIRECTIONS In this survey, we have reviewed the recent progress of dLLMs and dMLLMs, and summerized the key mathematics, base models, and training/inference techniques for understanding and utilizing this type of model. We mainly focus on the large-sized models while shortly introducing the contents of early small-sized models (e.g., D3PM, Plaid, SEDD, etc.) that have been well explored and covered in the existing literature. Next, we introduce the challenges and future directions for dLLMs and dMLLMs in the following aspects. A. Training and Infrastructure Currently, dMLLMs predominantly adopt architectures borrowed from their autoregressive counterparts. These models typically employ an autoregressive LLM [98, 82, 99] (Transformer) as the text encoder, alongside separate vision encoder [87, 86, 89, 88, 89] to extract image embeddings. lightweight projector or connector moduleoften simple multilayer perceptron (MLP)is then used to align vision tokens with textual representations. While convenient and compatible with existing pretrained components, this architecture transfer is primarily driven by engineering convenience rather than by the modeling needs inherent to diffusion. However, diffusion models differ fundamentally from autoregressive models: the joint data distribution via iterative denoising steps, rather than sequentially modeling conditional probabilities. This difference becomes more pronounced at scale. Also, the infrastructure for dLLMs remains relatively underdeveloped compared to their autoregressive counterparts. In the autoregressive paradigm, the community has benefited from mature open-source models, standardized training frameworks, and reproducible pipelines that facilitate rapid iteration and deployment at scale. Therefore, establishing standardized modular and scalable training frameworks, and open-sourced pretrained models will be critical directions for the community. Building robust infrastructure will not only promote fair comparisons and accelerate innovation, but also enable practical deployment across wide range of real-world applications. they model B. Inference Efficiency Despite their recent successes, dLLMs still face substantial limitations in inference efficiency and system scalability [6, 10, 25, 26]. Future work can explore several key directions to improve their deployability and performance. At the architectural level, incorporating efficient attention mechanisms (e.g., FlashAttention [100] or block-wise attention) and multiscale token representations may help reduce the compute burden during inference. In terms of the denoising process itself, advancing fast sampling techniquessuch as progressive distillation [101] and adaptive timestep scheduling [102, 44]could accelerate generation without compromising quality. Moreover, moving the diffusion process into continuous latent space, as seen in latent-space diffusion models, presents promising approach to balancing modeling power with inference efficiency. On the system side, integration with quantized inference (e.g., 19 Fig. 7. Number of arXiv publications retrieved via keyword-based search (Discrete Diffusion Model, Discrete Diffusion Language, and Discrete Diffusion Large Language) under the Computer Science (cs) category using the All fields search option, which scans across all metadata including titles, abstracts, and author information. The results show consistent year-over-year increase, reflecting the growing research interest in this area. INT8 or INT4) [103] may yield high-throughput, low-latency generation pipelines. Especially in multimodal scenarios, exploring deeper vision-language couplingsuch as cross-modal interaction modules embedded within the diffusion process or modality-aware denoising networksmay enhance the models ability to reason across modalities. In summary, holistic fusion of architectural refinement, sampling acceleration, representation compression, and deployment-level optimization constitutes promising roadmap for advancing dLLMs toward practical, efficient real-world use. C. Security and Privacy The security and privacy implications of dLLMs are an emerging concern as these models become more widely used. On the privacy front, diffusion models share similar risks with other large generative models [104, 105, 106, 107, 108, 109, they can inadvertently memorize and regurgitate 110, 111]: sensitive training data, raising the possibility of privacy breaches or copyright violations. Recent studies have demonstrated that diffusion models trained on vast internet data can reproduce portions of their training examples, much like LLMs. For instance, [112] managed to extract specific images from an image diffusion models training set, suggesting that memorization does occur and could pose legal or privacy issues. In the language domain, dLLMs could also analogously spit out memorized phrases, quotations, or personal information from its corpus. Mitigating this will require techniques such as differential privacy training [113], regularization to discourage overfitting [114, 115], or filtering of the training data to remove uniquely identifying content [116, 117]. In addition, security in terms of model misuse and alignment is another crucial aspect. Like any powerful language model, dLLMs could be misused to generate harmful, false, or biased content [118, 119]. One challenge is that controlling diffusion models output may require new methods: unlike AR models that can be guided token-by-token or halted upon generating disallowed tokens [120], diffusion models generate content in more holistic way. This makes real-time content moderation non-trivialdLLMs might only reveal problematic content once the final denoised text is produced. These areas remain critical future directions to address before dLLMs can be responsibly deployed at scale. VIII. CONCLUSION In summary, this survey provides comprehensive overview of Discrete Diffusion Large Language Models (dLLMs) and Discrete Diffusion Large Multimodal Models (dMLLMs). We present detailed exposition of their mathematical foundations and landmark developments. We further detail the training and inference strategies behind them, and summarize the current application domains and potential future directions of them. As promising alternative to autoregressive LLMs, dLLMs have attracted growing attention (see Figure 7) and show great potential in variety of real-world scenarios. We hope this survey will serve as valuable foundation for future research and development in this fast-evolving and important field."
        },
        {
            "title": "REFERENCES",
            "content": "[1] O. OpenAI, Gpt-4o system card, 2024. Available: https://arxiv.org/abs/2410.21276 [Online]. [2] OpenAI, Gpt-4 technical report, 2024. [Online]. Available: https://arxiv.org/abs/2303. [3] DeepSeek-AI, Deepseek-r1: capability in llms via reinforcement [Online]. Available: https://arxiv.org/abs/2501.12948 Incentivizing reasoning learning, 2025. Gemini [4] T. Gemini, 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. [Online]. Available: https://arxiv.org/abs/2403.05530 [5] Gemini, Gemini: family of highly capable multimodal models, 2025. [Online]. Available: https://arxiv.org/abs/ 2312. [6] S. Nie, F. Zhu, Z. You, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and C. Li, Large language diffusion models, arXiv preprint arXiv:2502.09992, 2025. [7] J. Ye, Z. Xie, L. Zheng, J. Gao, Z. Wu, X. Jiang, Z. Li, and L. Kong, Dream 7b, 2025. [Online]. Available: https://hkunlp.github.io/blog/2025/dream [8] R. Yu, X. Ma, and X. Wang, Dimple: Discrete diffusion multimodal large language model with parallel decoding, arXiv preprint arXiv:2505.16990, 2025. [9] Z. You, S. Nie, X. Zhang, J. Hu, J. Zhou, Z. Lu, J.- R. Wen, and C. Li, Llada-v: Large language diffusion models with visual instruction tuning, arXiv preprint arXiv:2505.16933, 2025. [10] S. Li, K. Kallidromitis, H. Bansal, A. Gokul, Y. Kato, K. Kozuka, J. Kuen, Z. Lin, K.-W. Chang, and A. Grover, Lavida: large diffusion language model for multimodal understanding, arXiv preprint arXiv:2505.16839, 2025. [11] L. Yang, Y. Tian, B. Li, X. Zhang, K. Shen, Y. Tong, and M. Wang, Mmada: Multimodal large diffusion language models, arXiv preprint arXiv:2505.15809, 2025. [12] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 68406851. [Online]. Availhttps://proceedings.neurips.cc/paper files/paper/ able: 2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf 20 [13] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, Structured denoising diffusion models in discrete state-spaces, Advances in neural information processing systems, vol. 34, pp. 17 98117 993, 2021. [14] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forre, and M. Welling, Argmax flows and multinomial diffusion: Learning categorical distributions, Advances in neural information processing systems, vol. 34, pp. 12 454 12 465, 2021. [15] L. Zheng, J. Yuan, L. Yu, and L. Kong, reparameterized discrete diffusion model for text generation, arXiv preprint arXiv:2302.05737, 2023. [16] S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. Chiu, A. Rush, and V. Kuleshov, Simple and effective masked diffusion language models, Advances in Neural Information Processing Systems, vol. 37, pp. 130 136130 184, 2024. [17] J. Shi, K. Han, Z. Wang, A. Doucet, and M. Titsias, Simplified and generalized masked diffusion for discrete data, Advances in neural information processing systems, vol. 37, pp. 103 131103 167, 2024. [18] S. Gong, S. Agarwal, Y. Zhang, J. Ye, L. Zheng, M. Li, C. An, P. Zhao, W. Bi, J. Han et al., Scaling diffusion language models via adaptation from autoregressive models, arXiv preprint arXiv:2410.17891, 2024. [19] S. Nie, F. Zhu, C. Du, T. Pang, Q. Liu, G. Zeng, M. Lin, and C. Li, Scaling up masked diffusion models on text, 2025. [20] Inception Labs, Mercury, https://www.inceptionlabs.ai/ introducing-mercury, 2025, accessed: 2025-06-16. [21] DeepMind, Gemini diffusion, https://deepmind.google/ models/gemini-diffusion/, 2025, accessed: 2025-06-16. [22] J. Ou, S. Nie, K. Xue, F. Zhu, J. Sun, Z. Li, and C. Li, Your absorbing discrete diffusion secretly models the conditional distributions of clean data, 2025. [23] G. Wang, Y. Schiff, S. Sahoo, and V. Kuleshov, Remasking discrete diffusion models with inference-time scaling, arXiv preprint arXiv:2503.00307, 2025. [24] I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Q. Chen, G. Synnaeve, Y. Adi, and Y. Lipman, Discrete flow matching, in Advances in Neural Information Processing Systems, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37. Curran Associates, Inc., 2024, pp. 133 345133 385. [25] X. Ma, R. Yu, G. Fang, and X. Wang, dkv-cache: The cache for diffusion language models, arXiv preprint arXiv:2505.15781, 2025. [26] Z. Liu, Y. Yang, Y. Zhang, J. Chen, C. Zou, Q. Wei, S. Wang, and L. Zhang, dllm-cache: Accelerating diffusion large language models with adaptive caching, arXiv preprint arXiv:2506.06295, 2025. [27] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised learning using nonequilibrium thermodynamics, in International conference on machine learning. pmlr, 2015, pp. 22562265. [28] A. Campbell, J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet, continuous time framework for discrete denoising models, in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 28 26628 279. [29] C. Meng, K. Choi, J. Song, and S. Ermon, Concrete score matching: Generalized score matching for discrete data, in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 34 53234 545. [30] H. Sun, L. Yu, B. Dai, D. Schuurmans, and H. Dai, Score-based continuous-time discrete diffusion models, in Proceedings of the International Conference on Learning Representations (ICLR), 2023. [31] M. Arriola, A. Gokaslan, J. T. Chiu, Z. Yang, Z. Qi, J. Han, S. S. Sahoo, and V. Kuleshov, Block diffusion: Interpolating between autoregressive and diffusion language models, in The Thirteenth International Conference on Learning Representations, 2025. [32] Z. He, T. Sun, K. Wang, X. Huang, and X. Qiu, Diffusionbert: Improving generative masked language models with diffusion models, arXiv preprint arXiv:2211.15029, 2022. [34] K. Zhou, Y. Li, W. X. Zhao, [33] J. Chen, A. Zhang, M. Li, A. Smola, and D. Yang, cheaper and better diffusion language model with softmasked noise, arXiv preprint arXiv:2304.04746, 2023. and J.-R. Wen, Diffusion-nat: Self-prompting discrete diffusion for text generation, arXiv preprint non-autoregressive arXiv:2305.04044, 2023. [35] R. K. Mahabadi, H. Ivison, J. Tae, J. Henderson, I. Beltagy, M. E. Peters, and A. Cohan, Tess: Text-totext self-conditioned simplex diffusion, arXiv preprint arXiv:2305.08379, 2023. [36] I. Gulrajani and T. B. Hashimoto, Likelihood-based diffusion language models, Advances in Neural Information Processing Systems, vol. 36, pp. 16 69316 715, 2023. [37] A. Lou, C. Meng, and S. Ermon, Discrete diffusion modeling by estimating the ratios of the data distribution, arXiv preprint arXiv:2310.16834, 2023. [38] A. Swerdlow, M. Prabhudesai, S. Gandhi, D. Pathak, and K. Fragkiadaki, Unified multimodal discrete diffusion, arXiv preprint arXiv:2503.20853, 2025. [39] J. Ye, Z. Zheng, Y. Bao, L. Qian, and Q. Gu, Diffusion language models can perform many tasks with scaling and instruction-finetuning, arXiv preprint arXiv:2308.12219, 2023. [40] F. Zhu, R. Wang, S. Nie, X. Zhang, C. Wu, J. Hu, J. Zhou, J. Chen, Y. Lin, J.-R. Wen et al., Llada 1.5: Variancereduced preference optimization for large language diffusion models, arXiv preprint arXiv:2505.19223, 2025. [41] J. Tae, H. Ivison, S. Kumar, and A. Cohan, Tess 2: large-scale generalist diffusion language model, arXiv preprint arXiv:2502.13917, 2025. [42] J. Wang, Y. Lai, A. Li, S. Zhang, J. Sun, N. Kang, C. Wu, Z. Li, and P. Luo, Fudoki: Discrete flow-based unified understanding and generation via kinetic-optimal velocities, arXiv preprint arXiv:2505.20147, 2025. [43] Q. Shi, J. Bai, Z. Zhao, W. Chai, K. Yu, J. Wu, S. Song, Y. Tong, X. Li, X. Li et al., Muddit: Liberating genera21 tion beyond text-to-image with unified discrete diffusion model, arXiv preprint arXiv:2505.23606, 2025. [44] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, Maskgit: Masked generative image transformer, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 31511 325. [45] C. Huang and H. Tang, Ctrldiff: Boosting large diffusion language models with dynamic block prediction and controllable generation, 2025. [Online]. Available: https://arxiv.org/abs/2505.14455 [46] Y. Lyu, T. Luo, J. Shi, T. C. Hollon, and H. Lee, Finegrained text style transfer with diffusion-based language models, arXiv preprint arXiv:2305.19512, 2023. [47] S. Zhang, Y. Zhao, L. Geng, A. Cohan, A. T. Luu, and C. Zhao, Diffusion vs. autoregressive language models: text embedding perspective, arXiv preprint arXiv:2505.15045, 2025. [48] X. Zhu, G. Karadzhov, C. Whitehouse, and A. Vlachos, Segment-level diffusion: framework for controllable long-form generation with diffusion language models, arXiv preprint arXiv:2412.11333, 2024. [49] L. Cheng and S. Li, Diffuspoll: Conditional text diffusion model for poll generation, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 925935. [50] Z. Hu, C. Liu, Y. Feng, A. T. Luu, and B. Hooi, Poetrydiffusion: Towards joint semantic and metrical manipulation in poetry generation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 27918 288. [51] C. H. Lee, H. Kim, J. Yeom, and S. Yoon, Editext: Controllable coarse-to-fine text editing with diffusion language models, arXiv preprint arXiv:2502.19765, 2025. [52] D. A. Do, L. A. Tuan, W. Buntine et al., Discrete diffusion language model for efficient text summarization, in Findings of the Association for Computational Linguistics: NAACL 2025, 2025, pp. 62786290. [53] W. Shao, M. Liu, and L. Song, Diffetm: Diffusion process enhanced embedded topic model, arXiv preprint arXiv:2501.00862, 2025. [54] X. Dong, W. Li, Y. Le, Z. Jiang, J. Zhong, and Z. Wang, Termdiffusum: term-guided diffusion model for extractive summarization of legal documents, in Proceedings of the 31st International Conference on Computational Linguistics, 2025, pp. 32223235. [55] D. Xin, K. Zhao, J. Sun, and Y. Li, Cdaˆ2: Counterfactual diffusion augmentation for cross-domain adaptation in low-resource sentiment analysis, in Proceedings of the 31st International Conference on Computational Linguistics, 2025, pp. 6172. [56] Z. Chen, L. Wang, Y. Wu, X. Liao, Y. Tian, and J. Zhong, An effective deployment of diffusion lm for data augmentation in low-resource sentiment classification, arXiv preprint arXiv:2409.03203, 2024. [57] L. Zhu, X. Chen, X. Guo, C. Zhang, Z. Zhu, Z. Zhou, and X. Kong, Pinpointing diffusion grid noise to enhance aspect sentiment quad prediction, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 37173726. [58] J. Ye, S. Gong, L. Chen, L. Zheng, J. Gao, H. Shi, C. Wu, X. Jiang, Z. Li, W. Bi et al., Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models, arXiv preprint arXiv:2402.07754, 2024. [59] S. Gao, M. Ismayilzada, M. Zhao, H. Wakaki, Y. Mitsufuji, and A. Bosselut, Diffucomet: Contextual commonsense knowledge diffusion, arXiv preprint arXiv:2402.17011, 2024. [60] Y. Cao, L. Wang, and L. Huang, Dpcl-diff: Temporal knowledge graph reasoning based on graph node diffusion model with dual-domain periodic contrastive learning, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 14, 2025, pp. 14 806 14 814. [61] S. Zhao, D. Gupta, Q. Zheng, and A. Grover, d1: Scaling reasoning in diffusion large language models via reinforcement learning, arXiv preprint arXiv:2504.12216, 2025. [62] A. Jiang, Y. Gao, Z. Sun, Y. Wang, J. Wang, J. Chai, Q. Cao, Y. Heng, H. Jiang, Z. Zhang et al., Diffvla: Vision-language guided diffusion planning for autonomous driving, arXiv preprint arXiv:2505.19381, 2025. [63] A. Shaahid and M. Behzad, Underwater diffusion attention network with contrastive language-image joint learning for underwater image enhancement, arXiv preprint arXiv:2505.19895, 2025. [64] H. He, C. Bai, L. Pan, W. Zhang, B. Zhao, and X. Li, Learning an actionable discrete diffusion policy via large-scale actionless video pre-training, arXiv preprint arXiv:2402.14407, 2024. [65] S. Chi, H.-g. Chi, H. Ma, N. Agarwal, F. Siddiqui, K. Ramani, and K. Lee, M2d2m: Multi-motion generation from text with discrete diffusion models, in European Conference on Computer Vision. Springer, 2024, pp. 1836. [66] M. Sun, W. Wang, G. Li, J. Liu, J. Sun, W. Feng, S. Lao, S. Zhou, Q. He, and J. Liu, Ar-diffusion: Asynchronous video generation with auto-regressive diffusion, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 73647373. [67] Y. Zhuang, D. Shen, and Y. Sun, Moleditrl: Structurepreserving molecular editing via discrete diffusion and reinforcement learning, arXiv preprint arXiv:2505.20131, 2025. [68] J. Yin, C. Zha, W. He, C. Xu, and X. Gao, Cfp-gen: Combinatorial functional protein generation via diffusion language models, arXiv preprint arXiv:2505.22869, 2025. [69] Y. Xiong, K. Li, W. Liu, J. Wu, B. Du, S. Pan, and W. Hu, Text-guided multi-property molecular optimization with diffusion language model, arXiv preprint arXiv:2410.13597, 2024. [70] S. Lee, K. Kreis, S. P. Veccham, M. Liu, D. Reidenbach, Y. Peng, S. Paliwal, W. Nie, and A. Vahdat, Genmol: drug discovery generalist with discrete diffusion, arXiv preprint arXiv:2501.06158, 2025. [71] X. Wang, Z. Zheng, F. Ye, D. Xue, S. Huang, and Q. Gu, Dplm-2: multimodal diffusion protein lan22 guage model, arXiv preprint arXiv:2410.13782, 2024. [72] S. Tang, Y. Zhang, and P. Chatterjee, Peptune: De novo generation of therapeutic peptides with multi-objectiveguided discrete diffusion, ArXiv, pp. arXiv2412, 2025. [73] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019, pp. 41714186. [74] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, arXiv preprint arXiv:1910.13461, 2019. [75] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, arXiv preprint arXiv:2011.13456, 2020. [76] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [77] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [78] Y. Bahri, E. Dyer, J. Kaplan, J. Lee, and U. Sharma, Explaining neural scaling laws, Proceedings of the National Academy of Sciences, vol. 121, no. 27, p. e2311878121, 2024. [79] L. Floridi and M. Chiriatti, Gpt-3: Its nature, scope, limits, and consequences, Minds and Machines, vol. 30, pp. 681694, 2020. [80] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [81] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [82] Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu, Qwen2.5 technical report, 2025. [83] T. Kaufmann, P. Weng, V. Bengs, and E. Hullermeier, survey of reinforcement learning from human feedback, arXiv preprint arXiv:2312.14925, vol. 10, 2023. [84] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, Advances in Neural Information Processing Systems, vol. 36, pp. 53 72853 741, 2023. [85] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 27 73027 744, 2022. [86] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond, arXiv preprint arXiv:2308.12966, 2023. [87] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, 2023. [88] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, 2023. [89] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [Online]. Available: https://llava-vl.github.io/blog/2024-01-30-llava-next/ [90] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 11 97511 986. [91] N. Shaul, I. Gat, M. Havasi, D. Severo, A. Sriram, P. Holderrieth, B. Karrer, Y. Lipman, and R. T. Chen, Flow matching with general discrete paths: kineticoptimal perspective, arXiv preprint arXiv:2412.03487, 2024. [92] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12 966 12 977. [93] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan, Autoregressive model beats diffusion: Llama for scalable image generation, arXiv preprint arXiv:2406.06525, 2024. [94] B. F. Labs, Flux, https://github.com/black-forest-labs/ flux, 2024. [95] J. Bai, T. Ye, W. Chow, E. Song, Q.-G. Chen, X. Li, Z. Dong, L. Zhu, and S. Yan, Meissonic: Revitalizing masked generative transformers for efficient highresolution text-to-image synthesis, in The Thirteenth International Conference on Learning Representations, 2024. [96] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. [97] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PmLR, 2021, pp. 87488763. [98] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [99] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [100] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re, Flashattention: Fast and memory-efficient exact attention with io23 awareness, Advances in neural information processing systems, vol. 35, pp. 16 34416 359, 2022. [101] T. Salimans and J. Ho, Progressive distillation for sampling of diffusion models, arXiv preprint fast arXiv:2202.00512, 2022. [102] D. Watson, J. Ho, M. Norouzi, and W. Chan, Learning to efficiently sample from diffusion probabilistic models, arXiv preprint arXiv:2106.03802, 2021. [103] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale, Advances in neural information processing systems, vol. 35, pp. 30 31830 332, 2022. [104] C. Zhang, J. X. Morris, and V. Shmatikov, Extracting prompts by inverting llm outputs, arXiv preprint arXiv:2405.15012, 2024. [105] Y. Chen, H. Lent, and J. Bjerva, Text embedding inversion security for multilingual language models, arXiv preprint arXiv:2401.12192, 2024. [106] J. X. Morris, W. Zhao, J. T. Chiu, V. Shmatikov, and A. M. Rush, Language model inversion, arXiv preprint arXiv:2311.13647, 2023. [107] Q. Li, R. Yu, and X. Wang, Vid-sme: Membership inference attacks against large video understanding models, arXiv preprint arXiv:2506.03179, 2025. [108] Z. Li, Y. Wu, Y. Chen, F. Tonin, E. Abad Rocamora, and V. Cevher, Membership inference attacks against large vision-language models, Advances in Neural Information Processing Systems, vol. 37, pp. 98 64598 674, 2024. [109] Y. He, B. Li, L. Liu, Z. Ba, W. Dong, Y. Li, Z. Qin, K. Ren, and C. Chen, Towards label-only membership inference attack against pre-trained large language models, in USENIX Security, 2025. [110] J. Zhang, J. Sun, E. Yeats, Y. Ouyang, M. Kuo, J. Zhang, H. F. Yang, and H. Li, Min-k%++: Improved baseline for detecting pre-training data from large language models, arXiv preprint arXiv:2404.02936, 2024. [111] C.-L. Wang, Q. Li, Z. Xiang, Y. Cao, and D. Wang, Towards lifecycle unlearning commitment management: Measuring sample-level unlearning completeness, arXiv preprint arXiv:2506.06112, 2025. [112] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace, Extracting training data from diffusion models, in 32nd USENIX Security Symposium (USENIX Security 23), 2023, pp. 52535270. [113] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, Deep learning with differential privacy, in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016, pp. 308318. [114] C. F. G. D. Santos and J. P. Papa, Avoiding overfitting: survey on regularization methods for convolutional neural networks, ACM Computing Surveys (Csur), vol. 54, no. 10s, pp. 125, 2022. [115] X. Ying, An overview of overfitting and its solutions, in Journal of physics: Conference series, vol. 1168. IOP Publishing, 2019, p. 022022. [116] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge, 24 D. Gao, Y. Xie, Z. Liu, J. Gao et al., Data-juicer: onestop data processing system for large language models, in Companion of the 2024 International Conference on Management of Data, 2024, pp. 120134. [117] M. Li, Y. Zhang, S. He, Z. Li, H. Zhao, J. Wang, N. Cheng, and T. Zhou, Superfiltering: Weak-to-strong data filtering for fast instruction-tuning, arXiv preprint arXiv:2402.00530, 2024. [118] Y. Qu, X. Shen, X. He, M. Backes, S. Zannettou, and Y. Zhang, Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models, in Proceedings of the 2023 ACM SIGSAC conference on computer and communications security, 2023, pp. 3403 3417. [119] Y. Zhang, J. Jia, X. Chen, A. Chen, Y. Zhang, J. Liu, K. Ding, and S. Liu, To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now, in European Conference on Computer Vision. Springer, 2024, pp. 385403. [120] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, do anything now: Characterizing and evaluating inthe-wild jailbreak prompts on large language models, in Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 2024, pp. 16711685."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}