{
    "paper_title": "Diffusion Models without Classifier-free Guidance",
    "authors": [
        "Zhicong Tang",
        "Jianmin Bao",
        "Dong Chen",
        "Baining Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG."
        },
        {
            "title": "Start",
            "content": "Diffusion Models without Classifier-free Guidance Zhicong Tang 1 Jianmin Bao 2 Dong Chen 2 Baining Guo 2 5 2 0 2 7 1 ] . [ 1 4 5 1 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper presents Model-guidance (MG), novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-theart performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at github.com/tzco/Diffusion-wo-CFG. 1. Introduction Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) have become the cornerstone of many successful generative models, e.g. image generation (Dhariwal & Nichol, 2021; Nichol et al., 2022; Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024) and video generation (Ho et al., 2022; Blattmann et al., 2023; Gupta et al., 2025; Polyak et al., 2024; Wang et al., 2024) tasks. However, diffusion models also struggle to generate low temperature samples (Ho & Salimans, 2021; Karras et al., 2024) due to the nature of training objectives, and techniques such as Classifier guidance (Dhariwal & Nichol, 2021) and Classifier-free guidance (CFG) (Ho & Salimans, 2021) are proposed to improve performances. Despite its advantage and ubiquity, CFG has several drawbacks (Karras et al., 2024) and poses challenges to effective implementations (Kynkaanniemi et al., 2024) of diffusion 1Tsinghua University 2Microsoft Research Asia. Figure 1: We propose Model-guidance (MG), removing Classifier-free guidance (CFG) for diffusion models and achieving state-of-the-art on ImageNet with FID of 1.34. (a) Instead of running models twice during inference (green and red), MG directly learns the final distribution (blue). (b) MG requires only one line of code modification while providing excellent improvements. (c) Comparing to concurrent methods, MG yields lowest FID even without CFG. models. One critical limitation is the simultaneous training of unconditional model apart from the main diffusion model. The unconditional model is typically implemented by randomly dropping the condition of training pairs and replacing with an manually defined empty label. The introduction of additional tasks may reduce network capabilities and lead to skewed sampling distributions (Karras et al., 2024; Kynkaanniemi et al., 2024). Furthermore, CFG requires two forward passes per denoising step during inference, one for the conditioned and another for the unconditioned model, thereby significantly escalating the computational costs. In this work, we propose Model-guidance (MG), an innovative method for diffusion models to effectively circumvent CFG and boost performances, thereby eliminating the limitations above. We propose novel objective that transcends from simply modeling the data distribution to incorporating the posterior probability of conditions. Specifically, we leverage the model itself as an implicit classifier and directly learn the score of calibrated distribution during training. As depicted in Figure 1, our proposed method confers mul1 Diffusion Models without Classifier-Free Guidance tiple substantial breakthroughs. First, it significantly refines generation quality and accelerates training processes, with experiments showcasing 6.5 convergence speedup than vanilla diffusion models with excellent quality. Second, the inference speed is doubled with our method, as each denoising step needs only one network forward in contrast to two in CFG. Besides, it is easy to implement and requires only one line of code modification, making it plug-and-play module of existing diffusion models with instant improvements. Finally, it is an end-to-end method that excels traditional two-stage distillation-based approaches and even outperforms CFG in generation performances. We conduct comprehensive experiments on the prevalent Imagenet (Deng et al., 2009; Russakovsky et al., 2015) benchmarks with 256256 and 512512 resolution and compare with wide variates of concurrent models to attest the effectiveness of our proposed method. The evaluation results demonstrate that our method not only parallels and even outperforms other approaches with CFG, but also scales to different models and datasets, making it promising enhancement for diffusion models. In conclusion, we make the following contribution in this work: We proposed novel and effective method, Modelguidance (MG), for training diffusion models. MG removes CFG for diffusion models and greatly accelerates both training and inference process. Extensive experiments with SOTA results on ImageNet demonstrate the usefulness and advantages of MG. 2. Background 2.1. Diffusion and Flow Models Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) are class of generative models that utilize forward and reverse stochastic processes to model complex data distributions. The forward process adds noise and transforms data samples into Gaussian distributions as q(xtx0) = (cid:0)xt; αtx0, (1 αt)I(cid:1) , (1) where xt represents the noised data at timestep and αt = (cid:81)t s=1 αs is the noise schedule. Conversely, the reverse process learns to denoise and finally recover the original data distribution, which aims to reconstruct score (Sohl-Dickstein et al., 2015; Song et al., 2021b) from the noisy samples xt by learning pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)) , (2) where µθ and Σθ are mean and variance and commonly predicted by neural networks. In common implementations, the training of diffusion mod2 els leverages re-parameterized objective that directly predicts the noise at each step (Ho et al., 2020) Lsimple = Et,x0,ϵϵθ(xt, t) ϵ2, where xt is derived from the forward process in Equation (1) with x0 and ϵ drawn from dataset and Gaussian noises. (3) Conditional diffusion models allow users to generate samples aligned with specified demands and precisely control the contents of samples. In this case, the generation process is manipulated with give conditions c, such as class labels or text prompts, where network functions are ϵθ(xt, t, c). Flow Models (Lipman et al., 2023; Liu et al., 2023; Albergo et al., 2023; Tong et al., 2024) are another emerging type of generative models similar to diffusion models. Flow models utilize the concept of Ordinary Differential Equations (ODEs) to bridge the source and target distribution and learn the directions from noise pointing to ground-truth data. The forward process of flow models is defined as an Optimal Transport (OT) interpolant (McCann, 1997) xt = (1 t)x0 + tϵ, (4) and the loss function takes the form (Lipman et al., 2023) LFM = Et,x0,ϵ uθ(xt) ut(xtx0)2 , (5) where the ground-truth conditional flow is given by ut(xtx0) = x0 ϵ. (6) 2.2. Classifier-Free Guidance Classifier-free guidance (CFG) (Ho & Salimans, 2021) is widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the low temperature regions with low quality. The key design of CFG is to combine the posterior probability and utilize Bayes rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict ϵθ(xt, t, c) xt log pθ(xtc), ϵθ(xt, t, ) xt log pθ(xt), where is an additional empty class introduced in common practices. During training, the model switches between the two modes with ratio λ. (7) (8) For inference, the model combines the conditional and unconditional scores and guides the denoising process as ϵθ(xt, t, c) = ϵθ(xt, t, c) + (ϵθ(xt, t, c) ϵθ(xt, t, )) , (9) Diffusion Models without Classifier-Free Guidance Figure 2: We use grid 2D distribution with two classes, marked with orange and gray regions, as example and train diffusion models on it. We plot the generated samples, trajectories, and probability density function (PDF) of conditional, unconditional, CFG-guided model, and our approach. (a) The first row indicates that although CFG improves quality by eliminating outliers, the samples concentrate in the center of data distributions, resulting the loss of diversity. In contrast, our method yields less outliers than the conditional model and better coverage of data than CFG. (b) In the second row, the trajectories of CFG show sharp turns at the beginning, e.g. samples inside the red box, while our method directly drives the samples to the closet data distributions. (c) The PDF plots of the last row also suggest that our method predicts more symmetric contours than CFG, balancing both quality and diversity. where is the guidance scale that controls the focus on conditional scores and the trade-off between generation performance and sampling diversity. CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation. 2.3. Distillation-based Methods Besides acceleration (Song et al., 2023), researchers (Sauer et al., 2024) also adopt distillation on diffusion models with CFG to improve sampling quality. Rectified Flow (Liu et al., 2023) disentangles generation trajectories and streamline learning difficulty by alternatively using offline model to provide training pairs for online models. Distillation is also used to learn smaller one-step model to match the generation performance of larger multi-step models (Meng et al., 2023). Pioneering diffusion models (Black-Forest-Labs, 2024; Stability-AI, 2024) are released with distillated version, where CFG scale is viewed as an additional embedding to provide accurate control. However, these approaches involve two-stage learning and require extra computation and storage for offline teacher models. 3. Method 3.1. Rethinking Classifier-free guidance Due to the complex nature of visual datasets, diffusion models often struggle whether to recover real image distribution or engage in the alignment to conditions. Classifier-free guidance (CFG) is then proposed and has become an indispensable ingredient of modern diffusion models (Nichol & Dhariwal, 2021; Karras et al., 2022; Saharia et al., 2022; Hoogeboom et al., 2023). It drives the sample towards the regions with higher likelihood of conditions with Equation (9), where the images are more canonical and better modeled by networks (Karras et al., 2024). However, CFG has with several disadvantages (Karras et al., 2024; Kynkaanniemi et al., 2024), such as the multitask learning of both conditional and unconditional generation, and the doubled number of function evaluations (NFEs) during inference. Moreover, the tempting property that solving the denoising process according to Equation (9) eventually recovers data distribution does not hold, as the joint distribution does not represent valid heat diffusion of the ground-truth (Zheng & Lan, 2024). This results in exaggerated truncation and mode dropping similar to (Karras et al., 2018; Brock et al., 2019; Sauer et al., 2022), since the samples are blindly pushed towards the regions with higher posterior probability. The generation trajectories are distorted in Section 1, the images are often over-saturated in color, and the content of samples is overly simplified. CFG originates from the classifier-guidance (Dhariwal & Nichol, 2021) that incorporates an auxiliary classifier model pθ(cxt) to modify the sampling distribution as pθ(xtc) pθ(xtc)pθ(cxt)w, (10) and estimates the posterior probability term with Bayes rule pθ(cxt) = pθ(xtc)pθ(c) pθ(xt) , (11) where pθ(xtc) and pθ(xt) are conditional and unconditional distributions, respectively. The unconditional model is usually implemented by ran3 Diffusion Models without Classifier-Free Guidance Algorithm 1 Training with Model-guidance Loss Input: dataset {Xi, Ci}, noise schedule α, model ϵθ repeat Sample data (x0, c) {Xi, Ci} Sample noise ϵ (0, 1) and time U(0, 1) αtx0 + Add noise with xt = Modify target ϵ = ϵ+w sg(ϵθ(xt, c, t)ϵθ(xt, , t)) Compute loss LMG = ϵθ(xt, c, t) ϵ2 Back propagation θ = θ ηθLMG 1 αtϵ until converged where σt is the variance of the noise added to xt at timestep t, is the empty class, and ϵθ() is the diffusion model. Substituting Equations (14) and (15) into Equation (13) yields the score of posterior probability xt log pθ(cxt) 1 σt (ϵθ(xt, t, ) ϵθ(xt, t, c)) . (16) Then, our method applies the Bayes estimation in Equation (13) online and trains conditional diffusion model to directly predict the score in Equation (12), instead of separately learning Equations (14) and (15) in the form of CFG. straight-forward implementation is to adopt the objective in Equation (3) with modified optimization target LMG = Et,(x0,c),ϵϵθ(xt, t, c) ϵ2, ϵ = ϵ + sg(ϵθ(xt, t, c) ϵθ(xt, t, )). (18) (17) We apply the stop gradient operation, sg(), which is common practice of avoiding model collapse (Grill et al., 2020). We also use the Exponential Mean Average (EMA) counterpart of the online model, ϵθ(), to stabilize the training process and provide accurate estimations. For flow-based models, we have the similar objective LMG = Et,(x0,c),ϵuθ(xt, t, c) u2, = + sg(uθ(xt, t, c) uθ(xt, t, )). (19) (20) where is the ground-truth flow in Equation (6). During training, we randomly drop the condition in Equations (17) and (19) to with ratio of λ. These formulations transform the model itself into an implicit classifier and adjust the standard training objective of diffusion model in self-supervised manner, allowing the joint optimization of generation quality and condition alignment with the minimum modification of existing pipelines. 3.3. Implementation Details With the MG formulation in Equations (17) and (19), we have adequate options in the detailed implementations, such as incorporating an additional input of the guidance scale into networks, replacing the usage of empty class with the law of total probability, and whether to manual or automati- (a) Unconditional, Conditional, and Classifier-free Guided score. (b) The offsets of CFG push update directions to the data. Figure 3: Illustration of our method. (a) The green and red arrow point towards the centroids of data distributions, as the training pairs (x0, ϵ) are randomly sampled. (b) While CFG provides accurate directions by subtracting the two vectors, our method directly learns the blue arrow, log pθ(xtc). domly replacing labels by an empty class with ratio λ. During inference, each sample is typically forwarded twice, one with and one without conditions. The finding naturally leads us to the question: can we fuse the auxiliary classifier into diffusion models in more efficient and elegant way? 3.2. Model-guidance Loss Conditional diffusion models optimize the conditional probability pθ(xtc) by Equation (3), where xt is the noisy data and is the condition, e.g. , labels and prompts. However, the models tend to ignore the condition in common practices and CFG (Ho et al., 2020) is proposed as an explicit bias. To enhance both generation quality and alignment to conditions, we propose to take into account the posterior probability pθ(cxt). This leads to the joint optimization of pθ(xtc) = pθ(xtc)pθ(cxt)w, where is the weighting factor of posterior probability. The score of the joint distribution is formulated as xt log pθ(xtc) = xt log pθ(xtc)+w xt log pθ(cxt) (12) The first term corresponds to the standard diffusion objective in Equation (3). However, the second term represents the score of posterior probability pθ(cxt) and cannot be directly obtained, since an explicit classifier of noisy samples is unavailable. Inspired by Equation (11), we transform the diffusion model into an implicit classifier and let it guide itself. Specifically, we employ Bayes rule to estimate log pθ(cxt) = log pθ(xtc) log pθ(xt) + log pθ(c) log pθ(xtc) log pθ(xt) (13) Next, we use the diffusion model to approximate the scores xt log pt(xtc) = xt log pt(xt) = ϵθ(xt, t, c), 1 σt 1 ϵθ(xt, t, ), σt (14) (15) 4 Diffusion Models without Classifier-Free Guidance cally adjust the hyper-parameters. Scale-aware networks. Similar to other distillation-based methods (Frans et al., 2024), the guidance scale can be fed into the network as an additional condition. When augmented with w-input, our models offer flexible choices of the balance between image quality and sample diversity during inference time. Note that our models require only one forward per step for all values of w, while standard CFG needs two forwards, e.g. , one with condition and one without condition. In particular, we sample guidance scale from an specified interval, and the loss function are modified into the following form LMG = Et,(x0,c),ϵ,wϵθ(xt, t, c, w) ϵ2, ϵ = ϵ + sg(ϵθ(xt, t, c, 1) ϵθ(xt, t, , 0)). (21) (22) Removing the empty class. Another option is whether to perform multitask learning of both conditional and unconditional generation with the same model. In CFG, the estimator in Equation (11) requires to train an unconditional model. However, the multitask learning can distract and hinder model capability. Using the law of total probability xt log pt(xt) = xt log (cid:88) pt(xtc)pt(c) = 1 σt (cid:88) i=1 ϵθ(xt, t, ci), (23) where different labels are used to estimate the unconditional score, our models focus on the conditional prediction and avoid the introduction of additional empty class. Automatic adjustment of the hyper-parameter w. While the scale in Equations (18) and (20) plays an important role, it is tedious and costly to perform manual search during training. Therefore, we introduce an automatic scheme to adjust w. We begin with = 0 that corresponds to vanilla diffusion models, then update the value with EMA according to intermediate evaluation results. The value of is raised when quality decreases and suppressed otherwise, leading to an optimums when the training converged. 4. Experiment We first present system-level comparison with state-of-theart models on ImageNet 256 256 conditional generation. Then we conduct ablation experiments to investigate the detained designs of our method. Especially, we emphasize on the following questions: How far can MG push the performances of existing diffusion models? (Tables 1 and 2, Section 4.2) How does implementation details influence the gain of proposed method? (Tables 3 to 6, Section 4.3) Can MG scales to larger models and datasets with efficiency? (Tables 7 and 8, Figures 4 to 6, Section 4.3) Table 1: Experiments on ImageNet 256 without CFG. By deploying our method, the performances of both DiT-XL/2 and SiT-XL/2 are greatly boosted, achieving state-of-the-art. MODEL FID SFID IS PRE. REC. IMG/S ADM VDM++ LDM-4 U-VIT-H MDTV2 REPA L-DIT VARd30 RARXXL MAR-H DIT-XL/2 +MG(ours) IMPROVE SIT-XL/2 +MG(ours) IMPROVE 10.9 2.40 10.5 8.97 5.06 5.90 2. 2.16 3.83 2.35 - - - - - 6.33 4.36 - - - 101.0 225.3 103.5 136.7 155.6 162.1 205. 288.7 274.5 227.8 0.69 0.78 0.71 0.69 0.72 0.71 0.77 0.81 0.79 0.79 0.63 0.66 0.62 0.63 0.66 0.56 0. 0.61 0.61 0.62 0.67 121.5 9.62 2.03 0.66 292.1 78.9% 36.4% 140% 20.9% 1.49% 6.85 4.36 0.67 0.81 0.67 131.7 8.61 1.34 0.65 321.5 84.4% 27.5% 144% 19.1% 2.99% 6.32 4. 0.68 0.81 - - - - 0.2 0.76 0.06 11.2 3.9 0.6 0.2 0.2 0.0% 0.76 0.76 0.0% Table 2: Experiments on ImageNet 256 with CFG. Comparing to models with CFG, our method still obtains excellent results and surpasses others without efficiency loss. MODEL FID SFID IS PRE. REC. IMG/S 4.59 2.12 3.60 2.29 1.58 1.42 1.35 1.73 1.48 1.55 5.25 - - 5.68 4.52 4.70 4.15 - - - 186.7 267.7 247.7 263.9 314.7 305.7 295.3 350.2 326.0 303.7 0.82 0. 0.87 0.82 0.79 0.80 0.79 0.82 0.80 0.81 0.52 0.65 0.48 0.57 0.65 0.65 0.65 0.60 0.63 0.62 - - - - 0.1 0.39 0.03 6.3 2.1 0.3 2.27 2.03 10.6% 5.22% 5.00% 2.41% 15.8% 100% 278.2 292.1 0.83 0.81 0.57 0. 4.60 4.36 0.1 0.2 0.39 2.06 1.34 0.76 35.0% 2.00% 15.9% 2.41% 10.2% 94.9% 277.5 321.5 0.83 0.81 0.59 0. 4.49 4.58 ADM VDM++ LDM U-VIT-H MDTV2 REPA L-DIT VARd30 RARXXL MAR-H DIT-XL/2 +MG(ours) IMPROVE SIT-XL/2 +MG(ours) IMPROVE 4.1. Setup Implementation and dataset. We follow the experiment pipelines in DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). We use ImageNet (Deng et al., 2009; Russakovsky et al., 2015) dataset and the Stable Diffusion (Rombach et al., 2022) VAE to encode 256 256 images into the latent space of R32324. We conduct ablation experiments with the B/2 variant of DiT and SiT models and train for 400K iterations. During training, we use AdamW (Kingma, 2014; Loshchilov, 2019) optimizer and batch size of 256 in consistent with DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024) for fair comparisons. For inference, we use 1000 sampling steps for DiT models and Euler-Maruyama sampler with 250 steps for SiT. Baseline Models. We compare with several state-of-theart image generation models, including both diffusionbased and AR-based methods, which can be classified into the following three classes: (a) Pixel-space diffusion: 5 Diffusion Models without Classifier-Free Guidance Table 3: Experiments on scale w. PRE. SFID FID IS MODEL 1.00 DIT-B/2 1.25 +MG(ours) 1.50 +MG(ours) 1.75 +MG(ours) +MG(ours) 2.00 +MG(ours) AUTO 1.00 SIT-B/2 1.25 +MG(ours) 1.50 +MG(ours) 1.75 +MG(ours) 2.00 +MG(ours) +MG(ours) AUTO 43.5 9.86 7.24 8.21 9.66 7.60 33.0 8.94 6.49 8.03 9.14 6.86 36.7 8.87 5.56 6.63 7.90 6.29 27.8 7.87 5.69 6.91 7.99 5.88 39.23 176.1 189.2 197.2 224.7 192.4 65.24 194.3 212.3 221.0 236.7 219. 0.62 0.81 0.84 0.86 0.85 0.85 0.68 0.83 0.86 0.86 0.88 0.87 Table 4: Experiments on drop ratio λ. SFID PRE. FID IS λ MODEL DIT-B/2 +MG(ours) +MG(ours) +MG(ours) +MG(ours) SIT-B/2 +MG(ours) +MG(ours) +MG(ours) +MG(ours) 1.00 0.05 0.10 0.15 0.20 1.00 0.05 0.10 0.15 0. 43.5 11.7 7.24 7.62 9.01 33.0 10.8 6.49 6.77 8.87 36.7 9.90 5.56 5.99 7.04 27.8 9.25 5.69 5.89 8.06 39.23 156.7 189.2 183.4 171.7 65.24 168.8 212.3 207.4 199. 0.62 0.78 0.84 0.83 0.81 0.68 0.80 0.86 0.85 0.84 REC. 0.34 0.37 0.38 0.38 0.39 0.38 0.35 0.38 0.38 0.39 0.40 0.38 REC. 0.34 0.33 0.38 0.38 0.36 0.35 0.34 0.38 0.37 0.37 ADM (Dhariwal & Nichol, 2021), VDM++ (Kingma & Gao, 2023); (b) Latent-space diffusion: LDM (Rombach et al., 2022), U-ViT (Bao et al., 2023), MDTv2 (Gao et al., 2023), REPA (Yu et al., 2024b), LightningDiT(L-DiT) (Yao & Wang, 2025), DiT (Peebles & Xie, 2023), SiT (Ma et al., 2024); (c) Auto-regressive models: VAR (Tian et al., 2024), RAR (Yu et al., 2024a), MAR (Li et al., 2024). These models consist of strong baselines and demonstrate the advantages of our method. Although our method does not requires CFG during inference, we still compare with these baselines under two settings, with and without CFG, for thoroughly investigations. Evaluation metrics. We report the commonly used Frechet inception distance (Heusel et al., 2017) with 50,000 samples (FID-50K). In addition, we report sFID (Nash et al., 2021), Inception Score (IS) (Salimans et al., 2016), Precision (Pre.), and Recall (Rec.) (Kynkaanniemi et al., 2019) as supplementary metrics. We also report the time to generate one sample of each model in seconds to measure the trade-off between generation quality and computation budget. 4.2. Overall Performances First of all, we present through system-level comparison with recent state-of-the-art image generation approaches on ImageNet 256 256 dataset in Tables 1 and 2. As shown in Table 1, both DiT-XL/2 and SiT-XL/2 models greatly benefit from our method, achieving the outstanding performance gain of 78.9% and 84.4%. It is worth mentioning that our models do not apply modern techniques in the inference process, including rejection sampling (Tian et al., 6 Table 5: Experiments on Model input w. SFID PRE. FID IS MODEL REC. 43.5 7.24 8.13 33.0 6.49 7.33 36.7 5.56 6.03 27.8 5.69 5.96 39.23 189.2 175. 65.24 212.3 207.4 0.62 0.84 0.84 0.68 0.86 0.85 0.34 0.38 0.39 0.35 0.38 0.38 Table 6: Experiments on empty class . SFID PRE. FID IS REC. 43.5 9.66 7.24 33.0 9.03 6. 36.7 8.73 5.56 27.8 7.96 5.69 39.23 174.4 189.2 65.24 183.3 212.3 0.62 0.81 0.84 0.68 0.82 0. 0.34 0.35 0.38 0.35 0.35 0.38 w-IN DIT-B/2 +MG(ours) +MG(ours) SIT-B/2 +MG(ours) +MG(ours) MODEL DIT-B/2 +MG(ours) +MG(ours) SIT-B/2 +MG(ours) +MG(ours) -CLS 2024), classifier-free guidance (Ho et al., 2020) and guidance interval (Kynkaanniemi et al., 2024). Compared to advanced methods, our models are light-weight, e.g. 675M in contrast to RAR-XXL with 1.5B and MAR-H with 943M parameters, and consume less computational resources, for example, LightningDiT uses DiT-XL/1 to reduce patch size to 1 1 and needs 16 computation in attention operations. To facilitate fair evaluation, we also compare with other methods with Classifier-free guidance. While prevalent diffusion models significantly benefit and are indispensable from CFG, it introduces an additional forward without condition and doubles the computation consumptions. Also, it usually requires careful search over the hyper-parameter of guidance scale to achieve the best trade-off between quality and diversity. In contrast, our models still surpass other CFG-assisted methods and run with only half of the generation time. Finally, we report the time consumption for each model to generate one sample in seconds. Comparing to other diffusion-based approaches facilitated with vanilla CFG, our method runs significantly faster and does not sacrifice inference speed for sampling quality. 4.3. Ablation study To thoroughly understand the designs and subsequent influences of our method, we conduct ablation experiments on the key components, including the hyper-parameter w, λ choices, whether the model takes as input, and the role of empty class during training. Moreover, we assess the scalability of our method in terms of both model size and dataset difficulty. Hyper-parameter In Equations (18) and (20), the hyperparameter controls the scale of posterior probability and serves an important role akin to the guidance scale in CFG, which is sensitive to FID-score. We conduct ablation experiments on the hyper-parameter and report results in Diffusion Models without Classifier-Free Guidance Table 7: Experiments on Model size. Our method scales to models with different sizes. MODEL FID SFID IS PRE. REC. DIT-B/2 +MG(ours) DIT-L/2 +MG(ours) DIT-XL/2 +MG(ours) SIT-B/2 +MG(ours) SIT-L/2 +MG(ours) SIT-XL/2 +MG(ours) 43.5 7.24 23.3 5.43 19.5 3.37 33.0 6.49 18.9 4. 17.3 2.89 36.7 5.56 18.4 4.66 15.6 4.73 27.8 5.69 16.3 4. 13.9 3.12 39.23 189.2 132.7 236.3 163.5 257.2 65.24 212.3 173.2 243. 192.1 261.0 0.62 0.84 0.73 0.83 0.79 0.84 0.68 0.86 0.71 0. 0.78 0.85 0.34 0.38 0.40 0.44 0.46 0.51 0.35 0.38 0.42 0. 0.50 0.54 Table 8: Experiments on ImageNet 512. Our method scales to high-resolution image datasets. MODEL FID SFID IS PRE. REC. DIT-XL/2 +MG(ours) SIT-XL/2 +MG(ours) 3.04 2.78 2.62 2. 5.02 4.86 4.18 4.03 240.8 257.2 252.2 276.9 0.84 0.83 0.84 0. 0.54 0.58 0.57 0.60 Table 3, where = 1 refers to vanilla forms in DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). It is shown that the choice of also acts as crucial role and balances the trade-off between quality and diversity. To overcome the tiresome and costly search of during training, we propose an adaptive approach to automatically adjust w, which achieves comparable performance with manual search. Meanwhile, we can further apply CFG to our models in Figure 4 to flexibly adjust between better quality and diversity during inference. Hyper-parameter λ The relative ratio to train conditional and unconditional models, λ, is also important to our method. The unconditional model is usually trained by randomly dropping the condition and replacing with an additional empty label for part of training data. In Table 4, we conduct ablation experiments on the hyper-parameter λ report the corresponding results. We find that λ is less sensitive than w, and λ {0.10, 0.15} offers satisfactory performances. Model input Despite the same loss formulation in the Equation (17), it is optional whether our model takes the scale as an additional input. In Table 5, the models with winput slightly lag behind the counterparts without w-input but still exceeding the vanilla DiT-B/2 and SiT-B/2 with CFG, demonstrating the superiority of our method. Empty class In Table 6, we conduct ablation experiments on the introduction of additional empty class. While removing the empty class in our method leads to worse estimation of posterior probability, the generation performances are still on par with the vanilla CFG. It can also be improved by Figure 4: FID-50K and Inception Score results as the guidance scale increases during inference. Our method is compatible with and can be wrapped into vanilla CFG. Figure 5: FID-5K results during training. Our method is 6.5 faster and 60% better than vanilla DiT and SiT, even surpassing the results of CFG. Figure 6: FID-50K vs. number of parameters and sampling flops of different models, where our models are highlighted. better estimation with the law of total probability or larger batch size. Efficiency One key advantage of our method is that it not only improves inference speed by avoiding the second network forward of CFG, but also accelerates the training and convergence of diffusion models. In Figure 5, our method obtains 6.5 convergence speed and 60% performance gain. In Figure 6, we plot the number of network parameters and sampling compute in TFlops versus FID-50K of Diffusion Models without Classifier-Free Guidance Figure 7: Uncurated samples of SiT-XL/2+MG on ImageNet 256 256. Figure 8: Uncurated samples of SiT-XL/2+MG on ImageNet 512 512. concurrent methods. When comparing number of network parameters, our method comes with the lowest FID and small model size. When comparing sampling computes, our method achieves state-of-the-art performances in parallel with LightningDiT (Yao & Wang, 2025), while requires only 12% computational resources. Scalability Finally, the scalability to larger model and dataset of our method is of imparable significance. In Table 7, we conduct ablation stuides on model size with B/2, L/2 and XL/2 variants of DiT and SiT models. It is demonstrated that our method is capable to boost the performance of models with different sizes and designs. We scale to ImageNet 512 512 dataset to validate our method in handling difficult distributions in Table 8. As depicted, our method also offers improvements on high-resolution tasks. 5. Conclusion This work addresses the limitations of the commonly used Classifier-free guidance (CFG) of diffusion models, and proposes Model-guidance (MG) as an efficient and advantageous replacement. We first investigate the mechanism of CFG and locate the source of performance gain as joint optimization of posterior probability. Then, we transcend the idea into the training process of diffusion models and directly learn the score of the joint distribution, log pθ(xtc) = log pθ(xtc)pθ(cxt)w. Comprehensive experiments demonstrate that our method significantly boosts the generation performance without efficiency loss, scales to different models and datasets, and achieves stateof-the-art results on ImageNet 256256 dataset. We believe that this work contributes to future diffusion models. 8 Diffusion Models without Classifier-Free Guidance"
        },
        {
            "title": "Impact Statements",
            "content": "This paper propose methods in association with generative methods. There might be potential negative social impacts, e.g. generating fake portraits, as the core contribution of our work is new algorithm of generative modeling. As possible mitigation strategies, we will restrict the access to these models in the planned release of code and models. We also validate that current detectors can effectively determine our generation results about human portraits."
        },
        {
            "title": "References",
            "content": "Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Black-Forest-Labs. Flux.1 model 2024. https://blackforestlabs.ai/ family, URL announcing-black-forest-labs/. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: Highresolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. Chen, J., Jincheng, Y., Chongjian, G., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-α: Fast training of diffusion transformer for photorealistic In The Twelfth International text-to-image synthesis. Conference on Learning Representations, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Frans, K., Hafner, D., Levine, S., and Abbeel, P. One arXiv preprint step diffusion via shortcut models. arXiv:2410.12557, 2024. Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2316423173, 2023. Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Li, F.- F., Essa, I., Jiang, L., and Lezama, J. Photorealistic In European video generation with diffusion models. Conference on Computer Vision, pp. 393411. Springer, 2025. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. Karras, T., Laine, S., and Aila, T. style-based generator architecture for generative adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 43964405, 2018. URL https://api.semanticscholar. org/CorpusID:54482423. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. Advances in neural information processing systems, 2024. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked diffusion transformer is strong image synthesizer. In Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Diffusion Models without Classifier-Free Guidance Kingma, D. P. and Gao, R. Understanding the diffusion objective as weighted integral of elbos. arXiv preprint arXiv:2303.00848, 2, 2023. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in neural information processing systems, 2024. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. Advances in neural information processing systems, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. Loshchilov, I. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. McCann, R. J. convexity principle for interacting gases. Advances in mathematics, 128(1):153179, 1997. Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1429714306, 2023. Nash, C., Menick, J., Dieleman, S., and Battaglia, P. Generating images with sparse representations. In International Conference on Machine Learning, pp. 79587968. PMLR, 2021. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pp. 1678416804. PMLR, 2022. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115: 211252, 2015. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: ACM Scaling stylegan to large diverse datasets. SIGGRAPH 2022 Conference Proceedings, 2022. https://api.semanticscholar.org/ URL CorpusID:246441861. Sauer, A., Boesel, F., Dockhorn, T., Blattmann, A., Esser, P., and Rombach, R. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024. 10 Diffusion Models without Classifier-Free Guidance Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In International Conference on Machine Learning, pp. 3221132252. PMLR, 2023. Stability-AI. 2024. introducing-stable-diffusion-3-5. 3.5, URL https://stability.ai/news/ Introducing diffusion stable Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 2024. Tong, A., FATRAS, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: Highquality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pp. 120, 2024. Yao, J. and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Yu, Q., He, J., Deng, X., Shen, X., and Chen, L.-C. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024a. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. Zheng, C. and Lan, Y. Characteristic guidance: Non-linear correction for diffusion model at large guidance scale. In International Conference on Machine Learning. PMLR, 2024."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Tsinghua University"
    ]
}