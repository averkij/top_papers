{
    "paper_title": "General Agentic Memory Via Deep Research",
    "authors": [
        "B. Y. Yan",
        "Chaofan Li",
        "Hongjin Qian",
        "Shuqi Lu",
        "Zheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 2 4 8 1 . 1 1 5 2 : r a"
        },
        {
            "title": "General Agentic Memory Via Deep Research",
            "content": "B.Y. Yan1, Chaofan Li1, Hongjin Qian1,3, Shuqi Lu1, Zheng Liu1,4 1. Beijing Academy of Artificial Intelligence 2. Renmin University of China 3. Peking University 4. Hong Kong Polytechnic University {chienqhj,zhengliu1026}@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose novel framework called general agentic memory (GAM). GAM follows the principle of just-in time (JIT) compilation where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs duo-design with the following components. 1) Memorizer, which highlights key historical information using lightweight memory, while maintaining complete historical information within universal page-store. 2) Researcher, which retrieves and integrates useful information from the pagestore for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems."
        },
        {
            "title": "Introduction",
            "content": "\"Intelligence is not the ability to store information, but to know where to find it.\" Albert Einstein AI agents become increasingly popular thanks to the rapid advancement of large language models (LLMs) [1]. Today, prototypes of AI agents are being deployed across many crucial domains, such as information seeking, software engineering, and scientific research, showcasing huge potential in improving the productivity of human society [2, 3, 4]. This widespread application, however, creates an urgent need to manage complex and rapidly expanding contexts, as AI agents must continuously integrate vast amounts of information generated by both their internal reasoning and external feedback [5]. To address this challenge, there has been growing interest in developing specialized memory systems that provide agents with essential contextual information to support downstream tasks [6]. Most existing memory systems follow the principle of Ahead-of-Time (AOT) Compilation. Under this paradigm, substantial computation is performed during the offline stage to compress raw contexts as lightweight memory, while incoming requests are served primarily based on this pre-constructed memory [7, 8, 9, 10]. Although widely adopted, this AOT-style approach suffers from critical limitations. Memorization is form of data compression; thus, it is inevitably subject to information loss. The precomputed memory, being compressed representation of raw data, inevitably suffers from information loss, making it difficult to satisfy the fine-grained information needs requested by client Project lead Preprint. Figure 1: Overview of GAM. The memorizer generates light memory by for agent history and keeps the complete history in the page-store during the offline stage. The researcher performs deep-research to retrieve and integrate useful information for its request in the online service. agents. In addition, such memory systems generally assume static structure, preventing them from flexibly adapting to ad-hoc or unforeseen requests that demand nuanced interpretation and integration of information. Finally, existing approaches often rely heavily on domain expertise and handcrafted heuristics to determine how memory is constructed and organized, which further constrains generalization across domains and tasks of the AOT-style memory systems. Search is made as the core of memory, while memorization is conducted to enable effective search. We argue that lossless memory can only be realized via searching over database of the complete history, where the pre-computed memory is introduced to support such search process. With this insight, we propose General Agentic Memory (GAM), novel memory framework for general AI agents following the principle of Just-in-Time (JIT) Compilation. During the offline stage, it creates light memory for the crucial historical information while maintaining the complete historical information in the database. At runtime, it performs intensive computation, namely deep research, to generate customized, high-utility context for its request based on the pre-constructed memory. Dual-architecture. Based on the above JIT principle, GAM is realized based on dual-agent framework with two fundamental roles: the Memorizer and the Researcher (Figure 1): The Memorizer receives the clients streaming history as sequence of sessions, where it takes two actions: 1) it dynamically compresses the key historical information with lightweight memory, and 2) it merges each session and its corresponding memory into page and save all pages into page-store, ensuring that the historical information is coherently and inclusively preserved. The Researcher receives an online request from its client and performs deep research based on the pre-constructed memory to address the clients needs. It iteratively analyzes information need and plans search actions, retrieves relevant information from the page-store, and reflects on the results until the gathered information fully satisfies the clients request. The above framework endows GAM with several key advantages. 1) High-fidelity and taskadaptability, enabling the generation of concise yet highly informative memory tailored to downstream tasks. 2) Domain generalizability, allowing GAM to operate effectively across general scenarios without relying on domain-specific expertise or handcrafted heuristics. 3) Optimizability, harnessing advanced LLMs agentic capability and test-time scalability for performance optimization, while also facilitating continual improvement through reinforcement learning. We evaluate GAMs performance through rigorous experimental studies. We jointly leverage the traditional memory benchmark LoCoMo [11], together with popular long-context benchmarks such as HotpotQA [7], RULER [12], and NarrativeQA [13]. Across all these experiments, GAM consistently and significantly outperforms existing methods, demonstrating its strong ability to preserve finegrained historical information and to optimize downstream task-completion performance for its clients. Our project is made publicly available to facilitate future research in this field2. 2https://github.com/VectorSpaceLab/general-agentic-memory"
        },
        {
            "title": "2.1 Definition",
            "content": "LLM agents often require long trajectories, comprising multi-step reasoning and tool using, to accomplish complex tasks, e.g., software engineering and deep research. In our work, we define each historical trajectory (history for short) as sequence of temporally ordered units called sessions: hist : s1, ..., sT . The rapidly growing history leads to several crucial challenges, including prohibitive computational costs, context window overflow, and performance degradation. To address these issues, memory system is introduced to manage the information overload. Its primary objective is to extract useful yet concise information from the history, which is essential for the completion of the agents task. That is to say, the memory system is to optimize the cost-effectiveness of the agents task completion grounded on its produced context. This objective can be formulated as the following minmax optimization problem. Definition 2.1 (Memory). memory system produces the optimized context for an agent based on its task and history: Memory(task, history), which is of the minimum size while optimizing the task completion performance: = argminC c, where = argmaxC Agent(task, context)."
        },
        {
            "title": "2.2 General Agentic Memory",
            "content": "The overall architecture of GAM, depicted in Figure 1, consists of two main modules: the memorizer and the researcher. Both modules are LLM-based agents, each with customized prompts3, working together to generate optimized memory that addresses requests from the client agent."
        },
        {
            "title": "2.2.1 Memorizer",
            "content": "The memorizer is responsible for processing the agents trajectory during the offline stage, ensuring that it can be efficiently stored and effectively utilized. Each memorization step is triggered by the arrival of new session (si), where two operations are performed. 1. Memorizing, which produces memo (µi) as concise and well-structured snapshot of the new session. The memo is generated based on both the new session and the existing memory (mi), highlighting its crucial information for the entire trajectory. The memory is therefore incrementally updated with the addition of the memo: Memorizer.memorize(si, mi) µi; mi + {µi} mi+1. (1) 2. Paging, which creates pages to maintain the complete information of the agents trajectory. It begins by generating header for the new session, which contains crucial contextual information from its preceding trajectory. The header is then used to decorate the session, forming new page that is subsequently added to the page-store (p): Memorizer.page(si, mi) hi; {header : hi, content : si} pi; p.append(pi). (2) This process shares the same principle of BGE landmark retrieval [14] and Anthropic contextual retrieval [15], which preserve the consistency of page semantics, ensuring that they can be accurately retrieved in subsequent stages."
        },
        {
            "title": "2.2.2 Researcher",
            "content": "The researcher is to address the clients request by retrieving and integrating useful information from the page-store. The process is iteratively conducted with three operations. 1) Planning, which performs chain-of-thought reasoning based on the existing memory to analyze the underlying information needed by request (r). Based on this initial reasoning result, it further generates concrete search plans according to the provided search toolkit (T ): Researcher.plan(r, mi, ) {tool : t; parameter : ρt}tT . (3) In our implementation, we offer three available tools for the researcher: an embedding model for vector search, BM25 retriever for keyword-based search, and an ID-based retriever for direct page exploration. 2) Searching. Upon obtaining the search plan, the researcher executes each search action in parallel, retrieving relevant pages (pt) from the page-store. The researcher then integrates the 3We include the detailed prompts of all functions in the appendix of the paper. information from the union of the retrieved pages together with the last integration result (I) for the request (r), leading to an updated temporal integration result: For each t: t(ρt) pt; Researcher.integrate( (cid:91) tT pt, I, r) I. (4) 3) Reflection. The researcher performs reflection on whether the needed information in the request (r) has been fully collected by the integration result using binary indicator (y). If no, it further analyzes for the missing information, leading to new request to drive another round of deep research. If yes, the research process will be concluded by returning the integration result: Researcher.reflect(I, r) y, r; if = No, Researcher(r, I); if = Yes, return I. (5) Finally, the integrated result, along with the original information extracted from the associated pages, is returned to the client as the optimized context for its downstream task completion."
        },
        {
            "title": "2.2.3 Optimization",
            "content": "A unified end-to-end performance optimization framework is introduced for GAM. Suppose training dataset = {(task, hist)} is given, the system creates the memory and page-store as: M, Memorizer(hist), and then generates candidate context for the task via: Researcher(task, M, P). Using this candidate context, the client samples an answer (ans), whose quality is measured by the reward function Γ(). Thus, the expected reward is derived as: = Etask,histD EM,PMemorizer(hist) EcResearcher(task,M,P) EansClient(c,task) Γ(ans). (6) When focusing on optimizing GAMs performance, the memorizer and the researcher are learned via reinforcement, while the client is excluded from the learning process. Without loss of generality, the policy gradients for the memorizer and researcher are given by: θm = Etask,histD (Γ(ans) Γm)θm log πm(M, Phist), θr = Etask,histD (Γ(ans) Γr)θr log πr(ctask, M, P). Here, θm and θr denote the model parameters of memorizer and researcher, respectively; Γm and Γr are the baseline answer rewards of the two modules; while θm() and θr() stand for the memorizer and researchers generation likelihood. (7)"
        },
        {
            "title": "3 Experiment",
            "content": "In this section, we conduct comprehensive experimental studies to evaluate the effectiveness of GAM. We focus on the investigation of the following three research questions. RQ 1: How does GAM perform compared with existing memory systems? RQ 2: How does GAMs performance vary across different scenarios? RQ 3: How do key technical factors within GAM influence its performance?"
        },
        {
            "title": "3.1 Experiment Setting",
            "content": "Datasets. To rigorously evaluate the effectiveness of GAM, specifically 1) the memorys ability to preserve historical information and 2) its ability to support downstream task completion, we employ the following benchmarks in our experimental studies. 1) LoCoMo [11]. widely used memory benchmark for conversational settings, designed to evaluate an agents ability to maintain and recall information across extended multi-session dialogues. We adopt its single-hop, multi-hop, temporal-reasoning, and open-domain tasks in our experiments. 2) HotpotQA [16]. popular multi-hop question answering benchmark based on the Wikipedia corpus. We use the curated memory-evaluation dataset in MemAgent [7] that concatenates gold supporting documents with distracting passages. By varying the number of distractions, the dataset provides three versions with context lengths of 56K, 224K, and 448K tokens. 3) RULER [12]. popular long-context understanding benchmark with four types of evaluation tasks, including retrieval (Retri.), multi-hop tracing (MT), aggregation (AGG.), and question answering (QA). We use the 128K-token setting in our experiments. 4) NarrativeQA [13]. long-context question answering benchmark that provides an entire book or movie script as the input context for each sample. We randomly sample subset of 300 questions for evaluation, whose average token length is 87K. 4 Baselines. We consider the following baselines in our experiments. 1) Memory-free methods, including the brute-force long-LLM (long-LLM for brevity) and retrieval-augmented generation (RAG). The long-LLM baseline attempts to process the entire input within the models context window. When the number of input tokens exceeds the maximum allowable context length Lmax, the input is evenly partitioned into chunks of length Lmax: {S1, ..., SN }, where the final score is reported as the maximum over all chunks: max{LLM(S1)...LLM(SN )}. For the RAG baseline, the input is uniformly partitioned into segments of 2,048 tokens, and the top-5 retrieved segments are used to perform the downstream task. 2) Memory-based methods, including A-Mem [8], Mem0 [9], MemoryOS [10] and LightMem [17]. These approaches construct specialized memory structures to store historical information, which can be utilized to address memory-related tasks at runtime. Implementation Details. In our experiments, we adopt GPT-4o-mini and Qwen2.5-14B-Instruct [18] as the backbone models for both GAM and all baselines. Both LLMs offer long-context window of 128K tokens. We use BGE-M3 [19] as the default dense retriever. For GAMs detailed configuration, we set the maximum reflection depth to 3 and the maximum number of retrieved pages to 5. The input context is segmented into 2,048-token pages for stream processing in the memorization module."
        },
        {
            "title": "3.2 Main Results: Overall Effectiveness",
            "content": "Table 1 presents the main results of GAM and baselines on the experimental benchmarks, from which the following observations can be made. First, GAM consistently outperforms all baselines, including both memory-free and memory-based approaches, across every benchmark. Moreover, its advantage becomes particularly pronounced on benchmarks like HotpotQA and RULER, where tasks require multi-step retrieval and reasoning over information dispersed within the input context. For instance, GAM achieves over 90% accuracy on the multi-hop tracing (MT) tasks in the RULER benchmark, which demand tracking variable values across multiple steps of assignment; in contrast, most baselines fail to achieve satisfactory performance under such complexity. Finally, GAM maintains stable and competitive performance under varying input-context lengths, as reflected in the results on HotpotQA. In summary, these experimental results preliminarily verify GAMs overall effectiveness and its robustness to task complexity and growing context lengths. We obtain the following interesting things besides the main observations. First, the performance of long-LLMs is under-expectation compared with the other methods, despite that it has adopted LLMs with 128K context window, long enough to fully cover the input context in LoCoMo, HotpotQA56K, and NarrativeQA. This suggests that simply extending the context window is insufficient to effectively address long-context challenges. This also aligns with the recently discussed phenomenon of context rot4, which indicates that the substantial distracting or irrelevant information within long contexts can severely degrade LLMs performance. Second, direct applications of retrieval, i.e., RAG, exhibit highly variable performance across different scenarios. RAG improves performance over long-LLMs and the memory-based methods when the relevant information is explicitly presented, such as LoCoMo single-hop and RULER retrieval. However, it performs badly in HotpotQA, RULER multi-hop tracing, and RULER aggregation tasks, where relevant information is unobvious. In comparison, the memory-based methods show lower variance but remain constrained due to the loss of crucial details of the original context. In contrast, GAM leverages memory to support effective retrieval of task-relevant information, enabling it to achieve substantially improved performance."
        },
        {
            "title": "3.3 Model’s Impact",
            "content": "Table 2 presents the performance of GAM on HotpotQA and NarrativeQA implemented with different LLMs. We apply Qwen-2.5 variants of different sizes (from 0.5B to 32B) and GPT-4o-mini as the backbones of the memorization and research module. As demonstrated by the experiment result, larger and stronger LLM-backbones for both memorizer and researcher result in consistent performance improvement, indicating that GAM can effectively leverage the increased LLM capacity to improve its memory quality. However, we also observe that the research module is considerably more sensitive to the LLMs scale than the memorization module. Notably, GAM maintains strong performance even when the memorizer is downsized and remains competitive with the smallest Qwen-2.5-0.5B model. In contrast, GAMs overall performance deteriorates significantly when the research modules backbone is reduced to 7B or smaller. This discrepancy reflects the distinct complexity of the two 4https://research.trychroma.com/context-rot 5 Table 1: Results from GAM and baselines (memory-free and memory-based) on LoCoMo, HotpotQA, RULER, and NarrativeQA. Two LLMs, GPT-4o-mini and Qwen-2.5-14B, are used in experiment. Model Method LoCoMo (a) Results on LoCoMo. m - 4 - b 4 1 5 . 2 Q Single Hop Multi Hop Temporal F1 LONG-LLM 46.68 52.45 RAG 44.65 A-MEM 47.65 MEM0 48.62 MEMORYOS 41.79 LIGHTMEM 57.75 GAM LONG-LLM 46.05 47.87 RAG 33.75 A-MEM 42.58 MEM0 46.33 MEMORYOS 34.92 LIGHTMEM 58.93 GAM BLEU-1 37.54 47.94 37.06 38.72 42.99 37.83 52.10 39.56 42.79 30.04 35.15 41.62 31.22 53.74 F1 29.23 27.50 27.02 38.72 35.27 29.78 42.29 32.08 26.38 22.09 31.73 38.19 25.45 42.96 BLEU-1 22.76 20.13 20.09 27.13 25.22 24.80 34.44 24.46 19.54 15.28 24.82 29.26 19.61 34.48 F1 25.97 46.07 45.85 48.93 41.15 43.71 59.45 30.51 30.78 27.19 28.96 32.24 32.03 51. BLEU-1 19.42 40.35 36.67 40.51 30.76 39.72 53.11 24.45 25.97 22.05 26.24 27.86 27.70 44.43 (b) Results on HotpotQA, RULER, and NarrativeQA. Open Domain F1 16.87 23.23 12.14 28.64 20.02 16.89 33.30 14.89 14.16 13.49 15.03 20.27 15.81 30.63 BLEU-1 13.70 17.94 12.00 21.58 16.52 13.92 26.97 11.41 10.52 10.74 11.28 15.94 11.81 26.04 NarrativeQA Model Method HotpotQA 56K F1 LONG-LLM 56.56 52.71 RAG 33.90 A-MEM 32.58 MEM0 26.47 MEMORYOS 40.93 LIGHTMEM 63.22 GAM LONG-LLM 49.75 51.81 RAG 27.04 A-MEM 30.12 MEM0 MEMORYOS 24.58 37.30 LIGHTMEM 64.07 GAM m - 4 - b 4 1 5 . 2 Q 224K 448K Retri. MT Acc. 60.60 0.00 0.00 53.80 2.40 36.20 93.20 80.00 0.00 0.00 41.20 3.00 17.40 90. F1 53.92 54.01 31.37 27.41 24.16 30.02 59.81 43.17 48.36 22.92 26.55 23.13 28.25 57.87 F1 54.29 51.84 30.22 31.74 23.10 35.28 64.56 46.82 46.72 25.65 32.44 30.25 27.72 55.99 Acc. 80.30 94.25 44.23 46.83 63.10 27.63 97.70 70.85 92.78 39.73 43.03 54.58 27.53 93.43 RULER(128k) AGG. Acc. 36.70 35.50 29.20 34.10 35.60 34.00 42.50 15.40 24.70 25.80 31.50 5.20 25.60 36.10 QA Acc. 61.60 55.90 46.50 51.70 36.90 52.60 72.50 45.60 47.80 40.20 46.10 34.60 53.00 74.50 31.26 25.00 27.07 29.16 26.70 17.51 36.86 29.69 18.29 25.18 27.80 23.45 16.57 34.77 modules: the memorizer primarily extracts salient information from the input context, which is relatively straightforward task, whereas the researcher must conduct iterative planning, searching, and reflection, which is much more complex and thus demands greater model capacity. 3.4 Increasing Test-Time Computation As shown in Figure 2, we investigate the impact of increasing test-time computation from two perspectives: 1) the depth of reflection and 2) the amount of retrieved pages. First, we vary the maximum reflection depth from 1 to 5 (3 by default), allowing GAM to perform additional research steps when necessary. Note that GAM autonomously determines the actual number of reflections Table 2: Models impact on memorizer (left) and researcher (right), reflected by GAMs performance. (a) Memorizer (b) Researcher Model HotpotQA 56K 224K 448K F1 F1 55.96 56.52 58.34 55.99 59.75 59.29 F1 53.33 55.50 56.17 57.87 56.26 57. NarrativeQA Avg F1 29.55 32.10 32.53 34.77 34.94 34.87 F1 48.83 50.54 51.53 53.18 53.50 54. QWEN2.5 0.5B 56.46 58.05 QWEN2.5 3B 59.06 QWEN2.5 7B 64.07 QWEN2.5 14B 63.05 QWEN2.5 32B 64.77 GPT-4O MINI Model HotpotQA 56K 224K 448K F1 F1 11.14 37.16 47.95 55.99 59.19 62.97 F1 11.64 33.04 48.55 57.87 61.53 61.54 NarrativeQA Avg F1 3.50 23.96 26.93 34.77 35.33 35.24 F1 9.08 33.48 43.85 53.18 54.50 55.45 QWEN2.5 0.5B 10.03 39.76 QWEN2.5 3B 51.95 QWEN2.5 7B 64.07 QWEN2.5 14B 61.93 QWEN2.5 32B 62.06 GPT-4O-MINI (a) Impact of maximum reflection depth. (b) Impact of the amount of retrieved pages. Figure 2: Impact of increasing test-time computation in reflection (top) and retrieval (bottom). and does not always reach the maximum step. This increased flexibility enables GAM to collect more information from the page-store, thus yielding consistent performance improvements across all datasets. However, the marginal gains gradually diminish, as many tasks do not require deep multi-step reasoning. Second, we increase the number of retrieved pages from 3 to 20 (5 by default), enabling GAM to browse more pages in each step of research. The increase in retrieval results also leads to consistent performance improvements. Overall, both forms of increased test-time computation result in steady performance gains, which demonstrates GAMs ability to benefit from test-time scaling, an advantage that baseline methods lack due to their fixed workflows."
        },
        {
            "title": "3.5 Detailed Factors’ Analysis",
            "content": "We perform ablation studies to analyze other detailed influential factors, including searching tools, formation of GAM, and output formats. First, we examine the impact of each searching tool and its combinations. As shown in Table 3, combining any two of the searching tools yields better performance than using each single tool alone, and the joint use of all three tools (i.e., GAM with the default setting) achieves the best performance. This observation validates the effectiveness of the search tools. Moreover, employing multiple tools enables broader exploration of the page-store, leading to better coverage of relevant information and, consequently, improved performance. 7 Table 3: Ablation study of detailed factors. HotpotQA NarrativeQA Avg Method GAM Tools ONLY PAGE-ID ONLY EMBEDDING ONLY BM25 EMBEDDING+PAGE-ID EMBEDDING+BM25 BM25+PAGE-ID Modules 56K F1 64. 44.86 39.59 59.24 47.25 61.37 63.57 224K 448K F1 55.99 F1 57.87 21.65 32.71 52.29 34.78 55.00 55.38 19.02 26.67 51.52 28.43 54.90 55. RESEARCH WITHOUT MEMORY MEMORY WITHOUT RESEARCH 57.40 42.67 49.72 19.75 53.98 17.38 F1 34.77 53.18 30.30 30.25 31.50 33.41 33.20 32.05 31.97 30.18 28.96 32.31 48.64 35.97 51.12 51.66 48.27 27. Table 4: Performance across different output formats. Model INTEGRATION ONLY INTEGRATION WITH PAGE INTEGRATION WITH EXTRACTION Metric F1 Tokens F1 Tokens F1 Tokens HotpotQA NarrativeQA Avg 224K 448K 56K 55.99 57.87 64.07 102.55 109.98 103.42 59.42 59.77 68.66 620.11 1444.30 499.23 57.83 57.81 67.41 227.57 230.47 220.78 34.77 107.64 34.99 6955.62 34.82 244. 53.18 105.90 55.71 2379.82 54.47 230.76 Second, we evaluate GAMs performance when each module is used in isolation, namely 1) research without memory, and 2) memory without research. According to the experiment result in Table 3, using the research module alone leads to substantial performance drop compared with the complete GAM system, highlighting the crucial role of memory in supporting effective exploration of relevant information. Using the memory module alone results in even worse performance, indicating that the pre-computed memory is prone to severe information loss. This observation further echoes our previous conclusion that the pre-constructed memory used in traditional ahead-of-time paradigms is far more limited than the just-in-time approach adopted by GAM. Third, we explore the impact of different forms of output, including 1) the researchers integration result (default), 2) the integration result accompanied by the relevant pages that provided its source information, and 3) the integration result paired with extracted source snippets from those relevant pages. As shown in Table 4, using only the integration result already achieves highly competitive performance. However, augmenting it with source information from the relevant pages yields further improvements, as it helps mitigate the loss of fine-grained details that may occur during integration."
        },
        {
            "title": "3.6 Efficiency",
            "content": "To assess the working efficiency of GAM, we measure the average time consumption, including both offline memory construction and online serving, when processing HotpotQA tasks under the 56K, 224K, and 448K settings. As shown in Table 5, GAM incurs time cost comparable to Mem0 and MemoryOS, and is substantially faster than A-mem. All methods exhibit approximately linear growth in offline construction time as context length increases, while maintaining relatively stable online serving time. Overall, GAM delivers strong performance with competitive efficiency, offering the best cost-effectiveness among experimental approaches. 8 Table 5: Efficiency analysis on HotpotQA Dataset HotpotQA 56k HotpotQA 224k HotpotQA 448k Metric OFFLINE BUILD (s) ONLINE SERVE (s) TOTAL (s) ANSWER QUALITY (F1) OFFLINE BUILD (s) ONLINE SERVE (s) TOTAL (s) ANSWER QUALITY (F1) OFFLINE BUILD (s) ONLINE SERVE (s) TOTAL (s) ANSWER QUALITY (F1)"
        },
        {
            "title": "4 Conclusion",
            "content": "A-mem Mem0 MemoryOS LightMem GAM 56.89 209.74 12.43 0.52 69.32 210.26 64.07 27.04 252.72 904.99 16.65 0.48 269.37 905.46 55.99 25.65 557.16 1796.82 18.49 0.47 575.65 1797.29 57.87 22.92 80.36 0.44 80.80 24.58 325.70 0.55 326.25 30.25 702.72 0.46 703.18 23.13 37.42 0.15 37.57 30.12 165.30 0.17 165.47 32.44 274.87 0.18 275.05 26.55 4.93 0.20 5.13 37.30 16.61 0.25 16.86 27.72 40.56 0.21 40.78 28.25 In this paper, we present novel memory system called General Agentic Memory (GAM), which is developed under the just-in-time compilation principle. GAM employs dual-framework comprising memorizer and researcher. During the offline stage, the memorizer extracts the key information for its incoming context with lightweight memory and preserve the complete information within page-store. For each online request, the researcher performs deep-research over the page-store based on the pre-constructed memory, which generates concise yet informative memory to support the downstream task. We perform comprehensive empirical studies using variety of popular memory and long-context benchmarks, whose result validates the effectiveness of GAM given its significant and consistent improvements over existing methods."
        },
        {
            "title": "References",
            "content": "[1] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428, 2024. [2] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [3] Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. Magis: Llm-based multi-agent framework for github issue resolution. Advances in Neural Information Processing Systems, 37:5196351993, 2024. [4] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):147, 2025. [7] Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. 9 [8] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. [9] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: arXiv preprint Building production-ready ai agents with scalable long-term memory. arXiv:2504.19413, 2025. [10] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025. [11] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. [12] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [13] Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. [14] Kun Luo, Zheng Liu, Shitao Xiao, Tong Zhou, Yubo Chen, Jun Zhao, and Kang Liu. Landmark embedding: chunking-free embedding method for retrieval augmented long-context large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32683281, 2024. [15] Anthropic. Introducing contextual retrieval. https://www.anthropic.com/engineering/contextualretrieval, 2024. [16] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [17] Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, et al. Lightmem: Lightweight and efficient memoryaugmented generation. arXiv preprint arXiv:2510.18866, 2025. [18] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [19] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024."
        },
        {
            "title": "Baseline Reproduction Details",
            "content": "When reproducing the baseline methods on the LocoMo dataset, we found that the category labels used for A-mem, Mem0, and MemoryOS were incorrect. Based on the official LocoMo annotations, we corrected the corresponding categorylabel mapping."
        },
        {
            "title": "Prompts",
            "content": "11 12 13 14"
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Hong Kong Polytechnic University",
        "Peking University",
        "Renmin University of China"
    ]
}