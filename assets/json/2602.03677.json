{
    "paper_title": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration",
    "authors": [
        "Yu Zhang",
        "Mufan Xu",
        "Xuefeng Bai",
        "Kehai chen",
        "Pengfei Zhang",
        "Yang Xiang",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\\%$ of these critical heads can decrease the modality-following ratio by $60\\%$ through blocking, or increase it by $60\\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs."
        },
        {
            "title": "Start",
            "content": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration Yu Zhang 1 2 Mufan Xu 3 Xuefeng Bai 1 Kehai Chen 1 2 Pengfei Zhang 2 Yang Xiang 2 Min Zhang 1 2 6 2 0 2 3 ] . [ 1 7 7 6 3 0 . 2 0 6 2 : r Abstract Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly In this paper, we investigate its understood. working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform nonselective information transfer, routing multimodal cues to these anchors as latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides substantial step toward model transparency and offers principled framework for the orchestration of multimodal information in MLLMs. 1. Introduction Multimodal instruction following (MIF) (Ding et al., 2025; Xu et al., 2023) has emerged as foundational capability for multimodal large language models (MLLMs) (Bai et al., 2025; Achiam et al., 2023; Zhe et al., 2024), enabling the execution of complex user commands by integrating information across different modalities. MIF is pivotal for 1Harbin Institute of Technology, Shenzhen, China 2Peng Cheng Laboratory, Shenzhen, China 3Harbin Institute of Technology, Harbin, China. Yu Zhang <yuzhang2717@gmail.com>, Kehai Chen <chenkehai@hit.edu.cn>. Correspondence to: Preprint. February 4, 2026. 1 Figure 1. Causal information flow dissection for modality following. (a) Multimodal cues are routed to instruction tokens, which function as structural anchors. (b) Shallow attention layers route cues to these anchors to form latent buffer without enforcing selection. (c) Deep attention layers act as the definitive arbiter, resolving modality competition based on instruction semantic, while MLP layers exhibit semantic inertia, acting as an adversarial force driven by internal priors. real-world deployments, such as multi-turn dialogues (Sun et al., 2022), graphical user interface navigation (Lu et al., 2025) and embodied robotic control (Zheng et al., 2025a). Compared to conventional instruction following in large language models (Ding et al., 2025), MIF introduces the unique challenge of modality followingthe ability to selectively use specific modalities strictly as instructed (Guo et al., 2025; Leng et al., 2024). Despite its significance, the internal decision-making process underlying this selective utilization remains black box, forming major obstacle to diagnosing model failures and ensuring behavioral reliability (Dang et al., 2024a; Leng et al., 2024). In this work, we address this gap by explicitly dissecting the internal decision-making mechanisms of MIF through the lens of information flow. Our investigation is conducted within controlled setting of cross-modal conflict, where visual and textual contexts support divergent answers. This setup allows us to isolate and analyze the models internal decision-making during modality-following tasks. We begin by tracing the structural pathways through which modal cues reach the final decision using Causal Attention Knockout (Geva et al., 2023). To rigorously quantify the impact of these interventions on modal competition, we Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration propose Normalized Signed Structural Divergence (IN SSD), which captures probability shifts within the binary decision subspace spanned by compliant and competing modal tokens. As illustrated in Fig. 1 (a), we identify CrossModal Relay Mechanism: visual and textual cues are routed to specific structural bottleneckthe Instruction Anchors instead of being directly extracted by the generation tokens. We further substantiate the critical role of these anchors by revealing that modality arbitration is finalized on the instruction anchors. This is evidenced by an alignment ratio of over 95% between latent decision states of instruction anchors and the final prediction using Logit Lens (Geva et al., 2022). By performing fine-grained attribution analysis, we reveal that shallow attention layers route multimodal cues to the instruction tokens to form latent buffer without enforcing selection in Fig. 1 (b). In contrast, deep attention layers act as the definitive arbiter, resolving modality competition based on instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force driven by internal priors in Fig. 1 (c). Given the decisive role of attention layers, we further dissect the fine-grained contribution of individual attention heads, revealing remarkably sparse functional landscape. We identify small subset of arbitration headscomprising both modality-specific experts and modalityshared hubsthat drive the decision-making process. To validate the causal necessity and sufficiency of these heads, we propose two intervention strategies: Targeted Attention Block and Amplification. Experimental results demonstrate that blocking mere 5% of these critical heads leads to catastrophic 60% decline in the modality-following ratio. Conversely, amplifying this sparse subset in failure cases restores compliance by nearly 60%, effectively activating the models latent arbitration capacity. Our main contributions are summarized as follows: 1. We identify that multimodal cues are routed to instruction anchors to perform modality arbitration for modality following. 2. We find that shallow attention layers transfer modality cues to form latent buffer, and deep attention layers resolve modality competition based on instruction intent, while MLP layers exhibit semantic inertia. 3. We pinpoint remarkably sparse subset of deep attention heads and validate their causal necessity and sufficiency in modality arbitration through targeted interventions. More broadly, our work provides substantial step toward model transparency and offers principled framework for the orchestration of multimodal information in MLLMs. 2 2. Constructing Analysis Dataset To investigate the internal mechanisms of modality following, we design controlled experimental setup centered on modality conflict (Zhang et al., 2025c), where model is presented with divergent contexts supporting mutually exclusive answers. Each sample in our dataset is defined as tuple = Q, Cv, Ct, Iv, It, E, where Q, Cv, Ct denote the query, visual context, and conflicting textual context, supporting distinct answers Av and At, respectively. Iv, It are instructions mandating reliance on either the visual (Cv) or textual (Ct) source. Crucially, to interpret latent representations to enable fine-grained analysis of information flow, we include an Answer Entity Dictionary = {Ev, Et}, where Ev and Et denote the sets of vocabulary tokens corresponding to the visual answer Av and textual answer At, respectively. Constructed via lexical databases and LLM-based semantic expansion, aggregates up to ten semantically equivalent surface forms for each target answer, ensuring robust coverage for information flow analysis. specific Vision Following case is illustrated in Fig. 2. Detailed construction procedures and dataset statistics are provided in Supp B. To ensure the validity of our mechanistic interpretations in subsequent sections, we filter the samples to retain only those where the models successfully perform modality following. Figure 2. Illustration of Vision Following. Given conflict visual and textual contextsdepicting two and three individuals respectivelythe model is presented with vision-centric query along with specific instructions and bilingual answer entity dictionary. The ground truth is the vision-compliant answer. 3. Instruction Functions as Structural Anchors 3.1. Causal Routing Analysis of Modal Cues Takeaways: Textual instruction tokens serve as structural anchors where multimodal cues converge and integrate via attention routing prior to generation. Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration We first investigate how decisive modal cues are structurally routed to the generated tokens to enable modality following. 3.1.1. PRELIMINARIES We focus on decoder-only multimodal large language models (MLLMs) based on the Transformer architecture. An input sequence = [x1, . . . , xN ] is divided into visual tokens (XV ), textual context tokens (Xctx), and instruction tokens (Xins). Each token xi is mapped to residual stream Rd, which is updated across layers: hl on the causal mask Ml across all attention heads within neighborhood window (i.e., layers [l k, + k]). Unless otherwise specified, we set the default window size to = 3. The sensitivity analyses in Supp C.1 confirm that our findings remain consistent across varying window sizes. For notational brevity, we omit the window indexing below. The intervened mask Ml is defined as: l,h i,j = (cid:40) (i, j) Psrcdst, l,h otherwise. i,j (5) = hl1 hl + Al + Fl i, (1) 3.1.3. NORMALIZED SIGNED STRUCTURAL and Fl where Al represent the outputs of the Attention and MLP sublayers, respectively. The output distrubution is computed via the unembedding matrix Eu: (yX) = Softmax(EuhL ). Latent State Projection. To decode internal model beliefs, we employ the Logit Lens (Geva et al., 2022), projecting intermediate states directly onto the vocabulary space V: Logit(yhl i) = (cid:0)Euhl (cid:1) . (2) This allows us to track the evolution of modality-specific signals before final generation. Attention as Causal Routing. The multi-head selfattention (MHSA) module serves as the primary mechanism for routing information between tokens (Lu et al., 2023; Soydaner, 2022). It decomposes the information flow into parallel attention heads. For specific layer and head {1, . . . , H}, the attention output is computed as: Al = (cid:88) h=1 Headl,h Wl,h , (3) Headl,h = Softmax (cid:32) ql,h (Kl,h) dk (cid:33) + Ml Vl,h, (4) Wl,h = hl1 where ql,h , and Kl,h, Vl,h represent the key and value matrices projected from the input sequence Hl1. The matrix Ml RN is the causal mask, which enforces the causal constraint by ensuring ij = 0 if and otherwise. 3.1.2. METHOD: CAUSAL ATTENTION KNOCKOUT To identify the critical routing pathways, we employ Causal Attention Knockout (Geva et al., 2023). The method dissects information flow by selectively severing specific attention edges. We define Target Pathway Psrcdst as the set of directed edges connecting source token set Isrc to destination token set Idst. To block information flow along this pathway centered at specific layer l, we intervene"
        },
        {
            "title": "DIVERGENCE",
            "content": "To rigorously quantify the shifts within the prediction subspace governing modality following induced by attention knockout, we propose the Normalized Signed Structural Divergence (IN SSD). Specifically, we project the models output distribution onto binary decision subspace = {yp, yc}, spanned by the instruction-compliant token yp and its modal competitor yc. The renormalized distribution ˆP within this subspace is defined as: ˆP (y) = (y) kS (k) (cid:80) , S. (6) Let denote the distribution after intervention. The metric captures the causal impact by weighting the KL divergence (Van Erven & Harremos, 2014) with the direction of the probability shift on the instruction-compliant token: IN SSD = sgn (cid:17) (cid:16) ˆP (yp) DKL (cid:0) ˆP ˆP (cid:1), (7) where ˆP (yp) = ˆP (yp) ˆP (yp), sgn() is the signum function that indicates the directionality of the probability shift DKL denotes the Kullback-Leibler divergence, which quantifies the magnitude of the distributional discrepancy between the original and intervened states. positive IN SSD signifies that the blocked pathway is instrumental for modality following. 3.1.4. RESULTS Cross-Modal Relay Mechanism. We examine the information flow among vision tokens (Tv), text tokens (Tt), and generated tokens (Tg) to characterize the routing of modal signals. As shown in Fig. 3, for Qwen2.5VL-7B, blocking the direct Tv Tg path yields marginal IN SSD, suggesting that the generation head may not primarily rely on direct attention to visual patches. In contrast, severing the Tv Tt pathway leads to substantial reduction in visual following in the right panel and compensatory increase in text following in the left panel. This pattern points to cross-modal relay mechanism: visual cues are Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration Recall that critical modality cues are routed through instruction tokens to manifest modality-following behavior. Consequently, the instruction representation serves as structural locus, bottleneck where heterogeneous modal signals converge. Thus, we hypothesize that these tokens do not merely store information but actively resolve the arbitration between modalities on the instruction intent. 3.2.1. METHOD: INTERNAL BELIEF TRACKING To test this, we probe the layer-wise evolution of the latent decision state. Leveraging the semantic sparsity hypothesis (Liu et al., 2023), we adopt Top-K signal aggregation strategy. To ensure robust tracking despite diverse surface forms, we utilize the Answer Entity Dictionary = {Ev, Et}. We define Signal Extraction Operator Φm() that maps the hidden state matrix Hl to scalar intensity for modality {v, t}: Φm(Hl) = 1 (cid:88) iTop-K(Xins) max yEm Logit(yhl i). (8) The latent signal strength at layer is denoted as Φm(Hl). = In practice, we set = 1 as the peak activation is sufficient to represent the crystallized intent; robustness analyses are provided in Section E. 3.2.2. METRIC: LATENT DECISION ALIGNMENT RATE To quantify the crystallization of modality arbitration, we propose the Latent Decision Alignment Rate (LDAR). This metric measures the synchronization between the internal state of the anchors and the final behavioral output. Let be sample from our diagnostic dataset, and yf {v, t} be the target modality correctly followed by the model. We consider the instruction anchor at layer to be aligned if the signal strength of the target modality dominates its competitor. Formally: LDAR(l) = 1 (cid:88) SD (cid:2)Φyf (Hl) > Φyc(Hl)(cid:3) , (9) where yc is the competing modality. An LDAR of 1.0 indicates resolved arbitration, while 0.5 represents state of latent uncertainty. 3.2.3. RESULT: INSTRUCTION AS DECISIVE ANCHORS As shown in Fig. 5 (a), the LDAR remains near the chance level (0.5) in shallow layers, indicating that the anchor initially acts as latent buffer. However, sharp phase transition occurs in deep layers, where the LDAR ascends to over 95% alignment. This suggests that modality arbitration is resolved within these anchors before the decision is projected onto output tokens. Figure 3. IN SSD results across the different knockout pathways in text-following (left) and vision-following (right) tasks. We use Source Target to represent blocking the attention mechanism from the source tokens to the target tokens. For convenience, Last denotes the generated token. not extracted directly by the generation tokens but are instead integrated into the textual sequence before being propagated. Furthermore, the significant impairment of following performance upon blocking Tt Tg suggests that the textual segment serves as central structural locus for modality signals. While the role of text as mediator has been observed in VQA contexts (Zhang et al., 2025d), our analysis highlights how this structural bottleneck specifically facilitates the arbitration dynamics required to resolve modality arbitration. For additional attention knockout analysis results, please refer to Supp C.2 Instruction as the Arbitration Anchor. To pinpoint the specific arbitration site within the textual mediator, we further partition the textual segment into text context tokens (Tctx) and instruction tokens (Tinst). As evidenced by the results for Qwen2.5-VL-7B and InternVL3-8B in Fig. 4, Tinst emerges as the primary functional anchor for signal integration across both models. Specifically, severing the attention pathways from visual patches (Tv) or text context (Tctx) to Tinst leads to substantial increase in IN SSD, signaling marked reduction in modality following. Notably, the magnitude of this impact far exceeds that observed when blocking paths directed toward Tctx or the generation tokens Tg. This symmetry indicates that instruction tokens serve as central sink where heterogeneous modal streams converge to orchestrate the final prediction. The results for various MLLMs provided in Supp exhibit patterns consistent with those observed in Qwen2.5-VL-7B, further reinforcing the validity and generalizability of our conclusions. 3.2. Decoding Modality Arbitration in Instruction Takeaways: Instruction functions as the definitive locus where modality arbitration is finalized before the information is propagated to the generated tokens. 4 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration (a) Qwen2.5-VL-7B (b) InternVL3-8B Figure 4. Layer-wise IN SSD profiles for (a) Qwen2.5-VL-7B and (b) InternVL3-8B across various knockout pathways. For both MLLMs, blocking attention flow to instruction tokens results in significantly greater structural divergence compared to other pathways, identifying instructions as the primary sink for modality arbitration. implying that the two sites reach shared consensus on the modality arbitration regardless of whether it correctly follows the instruction. The high alignment between instruction and generated tokens, coupled with the critical reliance on instruction tokens as the primary source, confirms that the final modality decision is effectively finalized at the instruction level. We validate this mechanism through targeted attention intervention experiments in section 4.1. 4. Mechanistic Dissection of Modality"
        },
        {
            "title": "Arbitration",
            "content": "In the previous section, we have identified the structural role of instruction anchors, and modality arbitration is finalized within instruction representations. But how these anchors arbitrate the competition between heterogeneous modal cues to finalize the decision. We next show that (a) Shallow attention layers act as latent buffer for undifferentiated information transfer, while deep attention layers serve as the definitive dynamic arbiters that resolve modal arbitration (4.1). (b) MLP sublayers facilitate semantic decoding by mapping raw features into the conceptual space, yet simultaneously exert semantic inertiaan adversarial force that deep attention must overcome to execute the instruction intent (4.1). (c) The arbitration is governed by sparse set of specialized functional heads. We validate their causal necessity and sufficiency through targeted intervention and amplification experiments (4.2). 4.1. Causal Analysis of Modality Arbitration Takeaways: Shallow attention layers route multimodal cues to these anchors to form latent buffer, and deep attention layers execute modality arbitration based on instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. (a) Layer-wise LDAR results. (b) Causal path blocking. Figure 5. Mechanistic evidence of instruction-mediated arbitration. (a) Layer-wise LDAR of instruction tokens across vision and textfollowing samples; the 0.5 dashed line represents the chance level. (b) Modality following ratio after severing attention paths from instruction anchors (Xins Gen) versus the target modal context (Cv/t Gen), where Cv/t corresponds to the modality specified by the instruction. To verify this, we perform two additional experiments: (1) Instruction-Mediated Information Flow. We first validate the necessity of the instruction-to-generation path by blocking attention flow from the context tokens (Cv, Ct) versus the instruction tokens (Xins) to the generated tokens across the critical deep layers (Layer 20 25) identified in Fig. 3 (a). As shown in Fig. 5 (b), severing the Instruction Generated path causes catastrophic collapse in the modality-following ratio. In contrast, ablating the direct Context Generated paths yields negligible impact, confirming that modality information must be channeled through instruction anchors. (2) Decision Synchronization. To further corroborate the inheritance mechanism, we evaluate the sample-wise prediction consistency between the latent modality decision decoded at the instruction anchors and the decision at the generated tokens. Our analysis reveals that these two positions exhibit over 90% synchronization in their modality arbitration across all deep layers (Layer 20 25). Remarkably, this high-fidelity alignment persists even in layers where the absolute LDAR remains below 70%, 5 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration (a) Latent logit intensities. (b) Signal Intensity Contribution Figure 6. Evolution of modality cues and sublayer contributions. (a) Latent logit intensities for the following modality (the instructioncompliant target) and its competitor. (b) Layer-wise contributions of MLP and attention to the signal intensity of the following modality. 4.1.2. RESULTS Evolution of Latent Modality Cues. As illustrated in Fig. 6(a), both instruction-compliant and competing In cues exhibit continuous semantic accumulation. shallow layers, the intensities are nearly indistinguishable, explaining the chance-level LDAR in 3.2. definitive divergence emerges only in deeper layers, where the signal of the modality following dominates, signifying the progressive crystallization of the decision. Attention: From Buffering to Arbitration. By examining Fig. 6 (b) and Fig. 7, we observe distinct functional transition in attention: Shallow Indiscriminate Transfer: shallow attention layers contribute positive δl A(S) in Fig. 6 (b) but nearzero δl A(S)  (Fig. 7)  . This confirms they perform indiscriminate transferrouting raw multimodal features to anchors for buffering without enforcing selection. Deep Selective Arbitration: Conversely, deep attention layers serve as primary drivers of δl A(S), selectively amplifying the target modality based on instruction intent. This identifies deep attention as the definitive locus of modality arbitration. The Dual Role of MLPs: Decoding and Semantic Inertia. While attention drives arbitration, MLP sublayers exhibit complex dualism: Semantic Decoding: MLPs consistently provide positive δl (Sf ) in Fig. 6 (b), acting as knowledge store that performs conceptual mapping of raw features into the models vocabulary space for semantic enrichment. Semantic Inertia: In deep layers, MLPs often exhibit negative δl (S)  (Fig. 7)  . This suggests that pretrained language priors, while facilitating decoding, can act as an adversarial force that resists the dynamic selection commanded by the instruction. 6 Figure 7. Modality arbitration margin contribution. Attention and MLP attribution to the arbitration margin, illustrating the roles of deep attention (arbitration) and MLPs (opposing influence). 4.1.1. METHOD: COMPONENT-WISE ATTRIBUTION To quantify sublayer impact, we define the Modality Arbitration Margin (MAM) as the intensity gap between the instruction-compliant modality (f ) and its competitor (c): = Φf (Hl) Φc(Hl). Following the residual update Hl = Hl1 + Al + Fl, we decompose the increment of any metric {S, S} into Attention (δl ) contributions: A) and MLP (δl δl A(M) = M(Hl1 + Al) M(Hl1), (M) = M(Hl) M(Hl1 + Al). δl (10) (11) Based on this, we conduct three investigative stages: 1. Latent Trajectory Analysis: Tracking the layer-wise evolution of modality intensities Φm(Hl) for both and in Fig. 6 (a). 2. Signal Propagation Analysis: Decomposing the semantic growth of the modality following using A(Sf ) and δl δl (Sf ) in Fig. 6 (b). 3. Arbitration Drive Quantification: Measuring the A(S) contribution to the arbitration margin using δl and δl (S) in Fig. 7. Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration Figure 8. Functional sparsity and specialization of attention heads. Layer-wise contribution of individual heads to the modality arbitration margin. Arbitration power is sparse and predominantly concentrated in deep layers. Despite task differences, significant overlap in top-performing heads suggests the existence of modalityagnostic arbitration hubs. Figure 9. Causal verification via head intervention. Comparison of Modality Following Ratio (MFR) for target, random and shared heads for Text Following (TF) and Vision Following (VF). Left: Impact of the number of blocked heads on MFR. Right: Impact of the amplification coefficient α on MFR. 4.2. Functional Specialization of Attention Heads 4.2.2. CAUSAL VERIFICATION: HEAD BLOCKING AND Takeaways: We identify sparse set of specialized attention heads that drive the arbitration. And causal intervention experiments valid their necessity and sufficiency in modality arbitration. 4.2.1. FINE-GRAINED ATTENTION HEAD ATTRIBUTION To pinpoint the specific structural components driving the arbitration process, we decompose the aggregate attention contribution δl into individual heads {1, . . . , H}. Following the attribution framework in 4.1, the contribution of specific head Headl,h to the metric is quantified as: δl,h (S) = S(Hl1 + Headl,h) S(Hl1). (12) This decomposition maps the arbitration power across the models entire attention architecture. Sparsity and Specialization. As illustrated in Fig. 8, the attention heads responsible for modality arbitration are remarkably sparse and concentrated primarily in the deep layers, consistent with the phase transition observed in 3.2. Common Arbitration Hubs. While vision-following and text-following tasks activate distinct patterns, we observe significant intersection of functional heads in the highcontribution regime. Specifically, among the Top-10 heads with the highest MAM contribution, there are 5 overlapping heads (e.g., Head(24, 22) in Qwen2.5-VL), which we term modality-shared heads. These heads appear to function as arbitration hubs that specialize in the abstract logic of instruction-driven selection, independent of the specific input modality. this overlap diminishes significantly to only 5 heads within the Top-40, suggesting that while core instruction-following logic is centralized in few elite hubs, the broader functional landscape remains highly modality-specific. Interestingly,"
        },
        {
            "title": "AMPLIFICATION",
            "content": "To validate the causal necessity and sufficiency of the identified heads, we perform targeted interventions at the instruction token positions (Xins). For given layer and head h, we manipulate the attention output Headl,h at instruction indices Xins as follows: 1) Blocking: We zero out the heads contribution by setting Headl,h = 0; 2) Amplifying: We scale the heads output by factor α > 1 to intensify its signal. Experimental Setup. We identify the Top-G critical heads based on the contribution scores derived in 4.2. To rigorously evaluate their causal roles, we utilize the Modality Following Ratio (MFR) as our primary metric across four settings: (1) Original: Standard model inference without intervention; (2) Targeted Heads: Interventions applied to the top-G heads identified by our framework; (3) Random Heads: control baseline where an equal number of heads are selected at random; and (4) Shared Heads: Interventions applied exclusively to modality-shared heads. The interventions are evaluated under two specific diagnostic configurations: Blocking Configuration: Performed on samples where the model originally follows instructions correctly (Original MFR = 100%) to test the necessity of the identified heads. Amplifying Configuration: Performed on failure cases where the model originally fails to follow the instruction (Original MFR = 0%) to test the sufficiency of these signals. Results. As shown in the left panel of Fig. 9, blocking the targeted attention heads leads to catastrophic decline in MFR. Notably, ablating only the top 40 heads (approximately 5% of total heads) results in 7 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration dramatic reduction in performance, with the MFR for Text Following dropping by nearly 60% absolute. In contrast, blocking an equal number of random heads yields negligible impact. This stark disparity confirms that our attribution framework successfully isolates the functional nodes truly responsible for modality arbitration. The necessity of intervening in multiple heads suggests that modality arbitration is distributed process emerging from the synergistic coordination of specific head ensemble, rather than being localized within single master head. Furthermore, the right panel of Fig. 9 illustrates the impact of amplifying the same 5% of targeted heads. Compared to the random baseline, amplifying these critical heads significantly restores modality-following behavior, achieving an absolute MFR improvement of nearly 60% for both Vision and Text Following tasks. This demonstrates that boosting these sparse signals effectively activates the models latent capacity for modality selection. We further observe that as the amplification coefficient α increases, the MFR initially rises and then approaches plateau. This saturation indicates threshold effect: once the modality-specific signal reaches the critical threshold required for decision crystallization, further amplification yields diminishing returns, as the internal arbitration has already been resolved. Notably, we observe that separately amplifying modalityshared heads fails to yield significant activation results. This finding implies that while shared heads serve as crucial arbitration hubs, they must operate in tandem with modalityspecific heads to effectively drive the model toward successful following state. Finally, to verify the validity of our diagnostic framework and the robustness of the Signal Extraction Operator Φm defined in 3.2.1, we conduct series of ablation experiments on our key methodological choices using attention blocking and amplifying. Detailed results, presented in Supp Fig. 14, further substantiate the efficacy and soundness of our design choices. 5. Related Work 5.1. Multimodal Instruction Following (MIF) Multimodal language models (MLLMs) have achieved remarkable success across wide range of domains (Zhu et al., 2025; Bai et al., 2025; Zhang et al., 2025b; Wei et al., 2025), demonstrating exceptional capabilities in integrating and reasoning over heterogeneous data (Zhe et al., 2024; Wei et al.; Zhang et al., 2025a; Zheng et al., 2025b). MIF is MLLMs capacity for the precise execution of instructions, requiring the selective integration of multimodal contexts (Chen et al., 2025; Leng et al., 2024) and adherence to predefined output formats (Ding et al., 2025; He et al., 2026). Research on MIF is fundamentally categorized into two dimensions: Instruction-Driven Format Compliance. Assessment of format compliance has transitioned from open-ended judging paradigms (Bitton et al., 2023; Qian et al., 2025) toward rigorous benchmarks targeting complex, visiondependent constraints (Ding et al., 2025; He et al., 2026). Correspondingly, enhancement efforts focus on scaling highquality instruction-following data (Chen et al., 2023; 2024) and applying preference-alignment strategies such as SFT and DPO to ensure strict adherence to structural output requirements (Ding et al., 2025; He et al., 2026). Precise Context Utilization. Our work is situated within this domain, focusing on the accurate synthesis of evidence from heterogeneous modalities guided by instructional intent (Guo et al., 2025; Leng et al., 2024; Chen et al., 2025). While advancements in alignment-driven fine-tuning and rigorous evaluation (Guo et al., 2025; Chen et al., 2025) have bolstered behavioral fidelity, MLLMs still struggle to resolve cross-modal competition or hallucinations under conflicting inputs (Leng et al., 2024). Crucially, the internal mechanisms governing this arbitration remain under-explored. We bridge this gap by dissecting the structural causal pathways that underlie modality arbitration. 5.2. Interpretability in MLLMs Existing literature on the mechanistic interpretability of MLLMs (Ben Melech Stan et al., 2024; Dang et al., 2024b; Basu et al., 2024; Huang et al., 2024) is predominantly anchored in perception-centric perspective, focusing on the encoding, storage, and retrieval of visual information within the Transformer architecture (Vaswani et al., 2017; Achiam et al., 2023). One trajectory focuses on pinpointing the specific neural topography responsible for multimodal processing, isolating modality-specific neurons (Huang et al., 2024; Pan et al., 2024) or task-contingent subcircuits (Nikankin et al., 2025) that disentangle cross-modal mechanisms. Another trajectory (Basu et al., 2024; Zhang et al., 2025b; Ben Melech Stan et al., 2024) interrogates the dynamic propagation of signals, employing causal interventions and attribution methods to trace the underlying information pathways. Concurrently, burgeoning line of work seeks to decode semantic content by projecting activations onto human-understandable concepts through tools such as Sparse Autoencoders (SAEs) (Lou et al., 2025) or the Logit Lens (Neo et al., 2024; Yu & Ananiadou, 2024). While perception-centric studies have advanced our understanding of MLLMs, the mechanisms governing cross-modal arbitration remain largely overlooked. This work elucidates the dynamics of modality-following by identifying instruction tokens as the critical structural locus Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration for decision crystallization, providing novel lens into multimodal information utilization. 6. Conclusion This paper investigates the underlying mechanisms of modality following in MLLMs through the lens of information flow. We identify instruction tokens as structural anchors where modality competition is resolved. Our analysis reveals functional stratification within the transformer architecture: shallow attention sublayers act as latent buffers, while deep attention sublayers drive the final arbitration by overcoming the semantic inertia of MLPs. Furthermore, by proposing Targeted Attention Block and Amplification, we establish the causal necessity and sufficiency of specialized attention heads in the arbitration process, thereby validating the robustness of our mechanistic framework. This work provides principled foundation for enhancing model transparency and achieving reliable governance of multimodal information."
        },
        {
            "title": "Impact Statement",
            "content": "This study elucidates the mechanisms of modality following in MLLMs, revealing how instruction-driven arbitration governs the resolution and prioritization of multimodal inputs. While these insights highlight potential vulnerabilities where safety filters might be bypassed, they primarily establish structural foundation for developing more robust and transparent AI safeguards."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Basu, S., Grayson, M., Morrison, C., Nushi, B., Feizi, S., and Massiceti, D. Understanding information storage and transfer in multi-modal large language models. Advances in Neural Information Processing Systems, 37:7400 7426, 2024. Ben Melech Stan, G., Aflalo, E., Rohekar, R. Y., Bhiwandiwalla, A., Tseng, S.-Y., Olson, M. L., Gurwicz, Y., Wu, C., Duan, N., and Lal, V. Lvlm-intrepret: An interpretability tool for large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81828187, 2024. Bitton, Y., Bansal, H., Hessel, J., Shao, R., Zhu, W., Awadalla, A., Gardner, J., Taori, R., and Schmidt, L. VisIT-Bench: benchmark for vision-language instruction following inspired by real-world use. In NeurIPS, Datasets and Benchmarks, 2023. Chen, G. H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang, Z., Chen, Z., Li, J., Wan, X., and Wang, B. Allava: Harnessing gpt4v-synthesized data for lite visionarXiv preprint arXiv:2402.11684, language models. 2024. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions, 2023. Chen, T., Chakka, C., Akula, A. R., Thomas, X., and Ghadiyaram, D. Some modalities are more equal than others: Decoding and architecting multimodal integration in mllms. arXiv preprint arXiv:2511.22826, 2025. Dang, Y., Huang, K., Huo, J., Yan, Y., Huang, S., Liu, D., Gao, M., Zhang, J., Qian, C., Wang, K., et al. Explainable and interpretable multimodal large language models: comprehensive survey. arXiv preprint arXiv:2412.02104, 2024a. Dang, Y., Huang, K., Huo, J., Yan, Y., Huang, S., Liu, D., Gao, M., Zhang, J., Qian, C., Wang, K., et al. Explainable and interpretable multimodal large language models: comprehensive survey. arXiv preprint arXiv:2412.02104, 2024b. Ding, S., Wu, S., Zhao, X., Zang, Y., Duan, H., Dong, X., Zhang, P., Cao, Y., Lin, D., and Wang, J. Mmifengine: Towards multimodal instruction following. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10991109, October 2025. Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 conference on empirical methods in natural language processing, pp. 3045, 2022. Geva, M., Bastings, J., Filippova, K., and Globerson, A. Dissecting recall of factual associations in autoregressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1221612235, Singapore, December 2023. Association for Computational Linguistics. Guo, Y., Ma, S., Ma, S., Bao, X., Xie, C.-W., Zheng, K., Weng, T., Sun, S., Zheng, Y., and Zou, W. Aligned better, listen better for audio-visual large language models. arXiv preprint arXiv:2504.02061, 2025. 9 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration He, W., Ju, F., Fan, Z., Min, R., Cheng, M., and Fung, Y. R. Empowering reliable visual-centric instruction following in mllms. arXiv preprint arXiv:2601.03198, 2026. Huang, K., Huo, J., Yan, Y., Wang, K., Yue, Y., and Hu, X. Miner: Mining the underlying pattern of modalityspecific neurons in multimodal large language models. arXiv preprint arXiv:2410.04819, 2024. Leng, S., Xing, Y., Cheng, Z., Zhou, Y., Zhang, H., Li, X., Zhao, D., Lu, S., Miao, C., and Bing, L. The curse of multi-modalities: Evaluating hallucinations of large multimodal models across language, visual, and audio. arXiv preprint arXiv:2410.12787, 2024. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft In Computer coco: Common objects in context. visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, June 2024b. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 2213722176. PMLR, 2023. Lou, H., Li, C., Ji, J., and Yang, Y. Sae-v: Interpreting arXiv multimodal models for enhanced alignment. preprint arXiv:2502.17514, 2025. Lu, Q., Shao, W., Liu, Z., Du, L., Meng, F., Li, B., Chen, B., Huang, S., Zhang, K., and Luo, P. Guiodyssey: comprehensive dataset for cross-app gui navigation In Proceedings of the IEEE/CVF on mobile devices. International Conference on Computer Vision, pp. 22404 22414, 2025. Lu, S., Liu, M., Yin, L., Yin, Z., Liu, X., and Zheng, W. The multi-modal fusion in visual question answering: review of attention mechanisms. PeerJ Computer Science, 9:e1400, 2023. Neo, C., Ong, L., Torr, P., Geva, M., Krueger, D., and Barez, F. Towards interpreting visual information processing in vision-language models. arXiv preprint arXiv:2410.07149, 2024. Nikankin, Y., Arad, D., Gandelsman, Y., and Belinkov, Same task, different circuits: Disentangling Y. modality-specific mechanisms in vlms. arXiv preprint arXiv:2506.09047, 2025. Pan, H., Cao, Y., Wang, X., Yang, X., and Wang, M. Finding and editing multi-modal neurons in pretrained transformers. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 10121037, 2024. Qian, Y., Ye, H., Fauconnier, J.-P., Grasch, P., Yang, Y., and Gan, Z. MIA-Bench: Towards better instruction following evaluation of multimodal llms. In ICLR, 2025. Soydaner, D. Attention mechanism in neural networks: where it comes and where it goes. Neural Computing and Applications, 34(16):1337113385, 2022. Sun, Q., Wang, Y., Xu, C., Zheng, K., Yang, Y., Hu, H., Xu, F., Zhang, J., Geng, X., and Jiang, D. Multimodal dialogue response generation. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 28542866, Dublin, Ireland, May 2022. Association for Computational Linguistics. Van Erven, T. and Harremos, P. Renyi divergence and IEEE Transactions on kullback-leibler divergence. Information Theory, 60(7):37973820, 2014. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wei, L., Li, Y., Wang, C., Wang, Y., Kong, L., Huang, W., and Sun, L. First sft, second rl, third upt: Continual improving multi-modal llm reasoning via unsupervised post-training. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Wei, L., Li, X., Jiang, Z., Huang, W., and Sun, L. Mmlima: Less is more for alignment in multi-modal datasets. Artificial Intelligence for Engineering, 2025. Xu, Z., Shen, Y., and Huang, L. MultiInstruct: Improving multi-modal zero-shot learning via instruction tuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1144511465, Toronto, Canada, July 2023. Association for Computational Linguistics. Yu, Z. and Ananiadou, S. Understanding multimodal llms: the mechanistic interpretability of llava in visual question answering. arXiv preprint arXiv:2411.10950, 2024. Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration Zhang, P., Gao, X., Wu, Y., Liu, K., Wang, D., Wang, Z., Zhao, B., Ding, Y., and Li, X. Moma-kitchen: 100k+ benchmark for affordance-grounded last-mile navigation in mobile manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 63156326, October 2025a. Zhang, P., Su, Y., Wu, P., An, D., Zhang, L., Wang, Z., Wang, D., Ding, Y., Zhao, B., and Li, X. Cross from left to right brain: Adaptive text dreamer for vision-andlanguage navigation. arXiv preprint arXiv:2505.20897, 2025b. Zhang, Y., Ma, J., Hou, Y., Bai, X., Chen, K., Xiang, Y., Yu, J., and Zhang, M. Evaluating and steering modality preferences in multimodal large language model. arXiv preprint arXiv:2505.20977, 2025c. Zhang, Z., Yadav, S., Han, F., and Shutova, E. Crossmodal information flow in multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1978119791, June 2025d. Zhe, C., Jiannan, W., Wenhai, W., Weijie, S., Guo, C., Sen, X., Muyan, Z., Qinglong, Z., Xizhou, Z., Lewei, L., Bin, L., Ping, L., Tong, L., Yu, Q., and Jifeng, D. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024. Zheng, J., Li, J., Liu, D., Zheng, Y., Wang, Z., Ou, Z., Liu, Y., Liu, J., Zhang, Y.-Q., and Zhan, X. Universal actions for enhanced embodied foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 22508 22519, June 2025a. Zheng, X., Wu, C., Chen, K., and Zhang, M. Locot2v-bench: benchmark for long-form and complex text-to-video generation. arXiv preprint arXiv:2510.26412, 2025b. Zhu, Y., Bai, X., Chen, K., Xiang, Y., Yu, J., and Zhang, M. Benchmarking and improving large vision-language models for fundamental visual graph understanding and reasoning. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3067830701, Vienna, Austria, July 2025. Association for Computational Linguistics. 11 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration All codes, data, and instructions and code can be found in https://anonymous.4open.science/r/ Modality-Following-C9E8. All code and data are released under Creative Commons Attribution 4.0 License (CC BY 4.0). Our supplementary materials are summarized as follows: Discussion: Limitations, Strategic Insights for Future Research, Use of LLM and License of Assets. Dataset Construction More Details for Attention Knockout Analysis More Results for Mechanistic Dissection of Modality Arbitration More Experiment Analysis A. Discussion A.1. Limitations While our work identifies the instruction segment as structural anchor and characterizes the functional role of specific attention heads in modality arbitration, it does not yet extend to the granularity of individual neurons. Our current analysis at the attention-head level already provides an effective and actionable means to steer and adjust the models modalityfollowing behavior. However, we recognize that more microscopic investigation into neuron-level activation patterns could potentially uncover even more fundamental principles of cross-modal arbitration. We leave this fine-grained circuit decompositiontransitioning from functional heads to atomic neuronsfor future research to further refine the theoretical boundaries of multimodal integration. A.2. Strategic Insights for Future Research Our findings regarding the role of instruction tokens suggest several promising directions for future MLLM design: 1) Decoupled Multi-modal Attention for Computational Efficiency: Since instruction tokens serve as the structural anchors for modality arbitration, future architectures could explore decoupling direct attention between heterogeneous contexts. By leveraging instruction tokens as intermediary routers or bottlenecks, models can achieve efficient cross-modal integration without the quadratic overhead of full dense attention between all visual and textual primitives. 2) Instruction-as-Cache for Long-range Reasoning: The observation that instructions act as locus for decision crystallization suggests their potential as multimodal information buffers. This inspires new paradigm for long Chain-ofThought (CoT) generation: by treating instruction-aligned tokens as memory caches, models can better preserve state across extended reasoning paths. This could significantly alleviate the burden of long-range dependencies and stabilize information flow in complex, multi-step multimodal tasks. A.3. Use of LLM In this work, we leveraged DeepSeek-V3 (Liu et al., 2024a) to curate and generate an answer entity dictionary, which served as the foundation for constructing our analysis dataset. Furthermore, we conducted mechanistic interrogation of modalityfollowing behaviors in several state-of-the-art MLLMs, including Qwen2.5-VL-7B (Bai et al., 2025), InternVL3-8B (Zhe et al., 2024), and LLaVA-1.5-7B (Liu et al., 2024b). Additionally, large language models were employed to assist with grammatical refinement and linguistic polishing of the manuscript. A.4. License of Assets All images used are publicly available from COCO (Lin et al., 2014). We release our analysis under Creative Commons Attribution 4.0 License (CC BY 4.0) to enhance global accessibility and foster innovation and collaboration in research. B. Dataset Construction We leverage MC2 (Zhang et al., 2025c) as our foundational dataset, which provides contexts with inherent modality conflicts. To evaluate modality-following behavior, we augment each sample with explicit instructions, such as: You should follow 12 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration the textual context rather than the visual content. To ensure the robustness of our mechanistic analysis, we construct an Answer Entity Dictionary based on the ground-truth answers in MC2 using hybrid pipeline of lexical databases and LLMs (Liu et al., 2024a). The construction process involves two primary stages: 1) Candidate Generation: We first utilize the WordNet library via NLTK to retrieve set of semantically related candidate entities. 2) Semantic Verification: These candidates are subsequently filtered by an LLM to ensure strict semantic alignment with the original answer. The verification is conducted using the following prompt template: Validate the candidate answers using DeepSeek-V3 Instruction: # You are an expert English linguist helping to judge if candidate word can be synonym of label in specific context. Your task: 1. You are given: - natural language question (the context), - an original label (the target meaning), - single candidate word to evaluate. 2. Decide if the candidate word can express approximately the same meaning as the label in this questions context. 3. The candidate word may differ in tense or part of speech (noun/verb/adjective/etc.) but should still preserve the core meaning. 4. Answer yes if the candidate word is valid synonym/near-synonym in this context, no otherwise. Output format: You MUST respond using the following tags: <answer>yes </answer>or <answer>no </answer><reason>brief English explanation of your judgment </reason> Do NOT output anything other than these tags. Question: {question} Original label: {label} Candidate word: {candidate} Instructions: In the context of the Question, can the candidate word be considered valid synonym or near-synonym of the Original label? Answer yes or no using the required tags. Furthermore, preliminary experiments revealed that hidden states in intermediate layers are frequently decoded into Chinese tokens. Consequently, we utilized DeepSeek-V3 to generate candidate Chinese synonyms to enrich our entity dictionary, employing the following instruction template: Generate the Chinese candidate answers using DeepSeek-V3 Instruction: # You are Chinese linguist expert assisting in generating Chinese synonyms or related terms based on English vocabulary and specific contexts. Your task: You are given: An English word (label), An English question (the context). Understand the specific meaning of the English word within the context of the question. Generate list of Chinese synonyms or related terms, ensuring that: The Chinese terms accurately convey the core meaning of the English word in the given context. They may include synonyms, near-synonyms, or related expressions. Return only individual words or phrases; do not provide full sentences. The quantity should be controlled between 5 and 15 terms. Output format: You MUST respond using the following tag format: <chinese words >word1, word2, word3, ... </chinese words ><explanation >A brief explanation in Chinese describing how these terms correspond to the English words meaning </explanation > Do NOT output anything other than these tags. Question: {question} Original label: {label} Instructions: Generate list of corresponding Chinese synonyms or related terms based on the meaning of this English word within the context of the question. Output the results using the required tag format. To ensure high data fidelity, randomized human verification was performed on the finalized dataset. Each answer is associated 13 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration with 10 bilingual candidate synonyms. After filtering samples with insufficient label coverage, the final dataset comprises 2,000 instances for causal attention analysis, with over 840 samples per model for subsequent mechanistic investigations. C. More Details for Attention Knockout Analysis (a) Blocking Window Size = 1 (b) Blocking Window Size = 5. Figure 10. IN SSD results across the different knockout pathways in text-following (left) and vision-following (right) tasks. C.1. Sensitivity Analysis of Attention Knockout Windows In Section 3.1.2, our primary analysis utilized default attention knockout window of = 3 (i.e., performing causal interventions on the target layer and its immediate neighbors). To ensure the robustness of our mechanistic findings and confirm that the observed trends are not artifacts of specific window configuration, we conducted sensitivity analysis across various window sizes. Specifically, maintaining the experimental setup described in Fig. 3, we report the IN SSD results of Qwen2.5VL-7B for alternative window sizes in Fig. 10. We observe that while the absolute values of IN SSD fluctuate slightly as the window expands, the fundamental conclusions regarding information propagation and modality arbitration pathways remain consistent, further validating the stability of our causal interpretations. (a) InternVL3-8B (b) LLaVA1.5-7B Figure 11. IN SSD results across the different knockout pathways in text-following and vision-following tasks. C.2. Extended Attention Knockout Analysis across MLLMs Consistent with the methodology in Fig. 3, we extend our attention knockout analysis to additional modelsspecifically InternVL2-8B (Zhe et al., 2024) and LLaVA-1.5-7B (Liu et al., 2024b)to investigate the dynamics of information propagation for both visual and textual modalities. As illustrated in Fig. 11, while minor variations exist regarding the specific locations of the most critical target layers (e.g., LLaVA-1.5-7B and InternVL2-8B exhibit slightly higher sensitivity in shallower layers), the overarching conclusions regarding modality-specific information flow remain highly consistent with our observations for Qwen2.5-VL-7B. This cross-model stability suggests that the identified mechanistic pathways for modality arbitration are generalizable property 14 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration of instruction-tuned transformer architectures. (a) Layer-wise LDAR results. (b) Causal path blocking. (c) Modality arbitration margin contribution. Figure 12. Mechanistic evidence of instruction-mediated arbitration for InternVL3-8B. (a) Layer-wise LDAR of instruction tokens across vision and text-following samples; the 0.5 dashed line represents the chance level. (b) Modality following ratio after severing attention paths from instruction anchors (Xins Gen) versus the target modal context (Cv/t Gen), where Cv/t corresponds to the modality specified by the instruction. (c) Modality arbitration margin contribution. Attention and MLP attribution to the arbitration margin, illustrating the roles of deep attention (arbitration) and MLPs (opposing influence). (a) Latent logit intensities. (b) Signal Intensity Contribution Figure 13. Evolution of modality cues and sublayer contributions for InternVL3-8B. (a) Latent logit intensities for the following modality (the instruction-compliant target) and its competitor modality. (b) Layer-wise contributions of MLP and attention to the signal intensity of the following modality. D. More Results for Mechanistic Dissection of Modality Arbitration In Section 4.1.2, we delineated the respective contributions of attention and MLP layers to modality arbitration in Qwen2.5VL-7B. In this section, we demonstrate that InternVL2-8B exhibits consistent mechanistic pattern across its internal components. As shown in Fig. 12c(a), we provide specific mechanistic evidence of instruction-mediated arbitration. The results of our causal path blocking experiments are presented in Fig. 12c(b), while the marginal contributions of the attention and MLP sublayers to modality arbitration are quantified in Fig. 12c(c). Furthermore, we analyze the latent logit intensities of instruction tokens in Fig. 13(a) and their corresponding signal intensity contributions in Fig. 13(b). These collective results reinforce the overarching conclusion of our study: shallow attention layers facilitate non-selective information transfer, whereas deep attention layers are primarily responsible for executing modality arbitration. Conversely, MLP sublayers consistently exert an opposing influence on the arbitration process. E. Ablation Studies and Robustness Analysis To verify the validity of our diagnostic framework and the robustness of the Signal Extraction Operator Φm defined in 3.2.1, we conduct series of ablation experiments on our key methodological choices using attention blocking and amplifying. Results are summarized in Fig. 14. Logit Extraction Strategy. In Eq. (8), we employ max-pooling strategy to select the peak logit across semantically equivalent entities in Em. We compare this against an average strategy that aggregates logits across all semantically equivalent entries. As shown in Fig. 14 (a) While average pooling yields moderate localization capabilityevidenced by 15 Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration Figure 14. Causal verification via head intervention. We compare the Modality Following Ratio (MFR) for Text Following (TF) and Vision Following (VF) across three settings: (1) TopK Inst: utilizing Top-K instruction aggregation (K > 1); we report results for = 2 as representative instance, given the similar performance trends observed across > 1. (2) Avg Answer: employing an averaging strategy that aggregates logits across all semantically equivalent answer tokens; (3) Ours: the default max-pooling strategy used in the main text. Left: Impact of the number of blocked heads on MFR. Right: Impact of the amplification coefficient α on MFR. 20% shift in modality following ratio (MFR) under interventionit remains suboptimal compared to the max-pooling strategy. Instruction Aggregation Sensitivity. Our framework utilizes Top-K strategy across instruction tokens, with = 1 as the default. We vary to investigate the spatial distribution of the decision state. We observe that > 1 yields negligible variations in MFR for both attention blocking and amplification interventions, as evidenced in Fig. 14 (b). This observation empirically validates the strategy adopted in our main text."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Harbin",
        "Harbin Institute of Technology, Shenzhen, China",
        "Peng Cheng Laboratory, Shenzhen, China"
    ]
}