{
    "paper_title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
    "authors": [
        "Pengxiang Li",
        "Lu Yin",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 5 9 7 3 1 . 2 1 4 2 : r Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN MI X-LN: UNLEASHING THE POWER OF DEEP LAYERS BY COMBINING PRE-LN AND POST-LN Pengxiang Li1, Lu Yin2,3, Shiwei Liu4,3 1 Dalian University of Technology 2 University of Surrey 3 Eindhoven University of Technology 4 University of Oxford"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the networkboth shallow and deep layersto contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have ushered in new era of artificial intelligence by demonstrating unprecedented capabilities in understanding and generating human-like text (Brown, 2020; Achiam et al., 2023; Touvron et al., 2023; Dubey et al., 2024). Trained on vast datasets that span multiple languages and topics, LLMs are driving advancements across industries and academia, enhancing human-computer interactions, and fostering innovation in previously unimaginable ways. Recent studies reveal critical observation regarding the effectiveness of deeper layers in LLMs, particularly those beyond the middle layers. It has been shown that these deeper layers can often be pruned significantly (Yin et al., 2023), or even removed entirely (Gromov et al., 2024; Men et al., 2024), without notably affecting the models overall capabilities. Moreover, Li et al. (2024) demonstrated that deeper layers contribute minimally to performance during fine-tuning, further questioning their importance. Unfortunately, this finding has been largely overlooked by the research community, where many see it primarily as an opportunity for model compression (Siddiqui et al., 2024; Zhong et al., 2024; Sreenivas et al., 2024), rather than recognizing it as potential shortfall in the training process. Equal contribution. Corresponding to Shiwei Liu, shiwei.liu@maths.ox.ac.uk. 1 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN Figure 1: (a) Post-LN layer; (b) Pre-LN layer; (c) Mix-LN layer. In this paper, we seek to challenge the prevailing notion that deeper layers in LLMs are of lesser significance. The training of LLMs is extraordinarily resource-intensive, often requiring thousands of GPUs or TPUs and several months of computation on vast datasets. For example, the training of GPT-3 reportedly incurred millions of dollars in computational costs. The underutilization of deeper layers leads to inefficiencies, squandering resources that could otherwise be leveraged to enhance model performance. Ideally, all layers in model should be well-trained, with sufficient diversity in features from layer to layer, to maximize the utility of the networks parameters (Yang et al., 2023). This makes it crucial to investigate the root causes of this underutilization and to develop strategies that fully capitalize on the potential of deeper layers, ensuring that the overall architecture is optimized for both performance and efficiency. We hypothesize that the inefficiency of deeper layers in LLMs primarily stems from the choice of Layer Normalization. Specifically, Pre-Layer Normalization (Pre-LN) (Dai, 2019; Baevski & Auli, 2018) tends to produce smaller gradients in deeper layers, thereby diminishing their effectiveness, while Post-Layer Normalization (Post-LN) (Ba, 2016) results in larger gradients in deeper layers but leads to gradient vanishing in earlier ones. Most state-of-the-art LLMs, like GPT, LLaMA, and Mistral, employ Pre-LN, which contributes to the widespread assumption that deeper layers are inherently less effective. To validate this conjecture, we conduct experiments with the following two categories of LLMs and compare the effectiveness of layers across different depths in Pre-LN models and Post-LN models. Open-weight large-scale LLMs: We select LLaMa2-7B (Touvron et al., 2023) as representative Pre-LN model and BERT-large (Devlin, 2018) as Post-LN model to evaluate the quality of their layers. Our findings confirm that the deeper layers of LLaMa2-7B exhibit high similarity, with their removal leading to minimal impact compared to the early layers. In stark contrast, BERT shows higher similarity among its first half, which contributes less to the models output. In-house small-scale LLMs: To control for irrelevant confounding variables, we conduct second set of experiments by training small-scale LLMs ourselves, ensuring that the only difference between the models is the choice of layer normalization. Consistent trends are observed in these experiments, reinforcing our earlier observations. Building on these insights, we propose novel normalization technique, dubbed Mix-LN, which synergizes Pre-LN and Post-LN to achieve more balanced and healthier gradient norms across the network. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers. The rationale behind this is that Post-LN enhances gradient flow in the deeper layers, while Pre-LN stabilizes gradients in the earlier layers. By employing Post-LN in the initial layers and Pre-LN in the later layers, Mix-LN promotes healthier gradient norms in the middle and deeper layers, fostering more balanced training across the entire network and ultimately improving the models overall performance. 2 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN Our extensive experiments, spanning models from 70M to 7B parameters, demonstrate that Mix-LN consistently outperforms Pre-LN, Post-LN, and their variants. Mix-LN not only avoids the training instability associated with Post-LN but also significantly improves the quality of deeper layers compared to Pre-LN, leading to better pre-training performance. Additionally, models pre-trained with Mix-LN demonstrate superior learning during Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) compared to those trained with Pre-LN or Post-LN, underscoring the importance of high-quality deep layers in LLMs."
        },
        {
            "title": "2 HYPOTHESIS EVALUATION",
            "content": "In this section, we will evaluate our hypothesis that the inefficiency of deeper layers in LLMs stems from the choice of Pre-LN. The evaluation details are described as follows."
        },
        {
            "title": "2.1 LAYER NORMALIZATION AND ITS GRADIENT",
            "content": "Figure 1 (a) and (b) illustrate Post-LN and Pre-LN Transformer architectures, respectively. Formally, let us define as the input, F(x) as either FFN layer or multi-head attention layer, and LN() as the layer normalization. Post-LN applies LN() after the residual addition: Post-LN(x) = LN(x + F(x)). In contrast, Pre-LN applies LN() before the residual addition: Pre-LN(x) = + F(LN(x)). We can calculate the derivatives of Equations (1) and (2), as follows: Post-LN(x) Pre-LN(x) = LN(x + F(x)) (x + F(x)) (cid:18) + (cid:19) , F(x) = + F(LN(x)) LN(x) LN(x) . (1) (2) (3) (4) Both the above equations involve an important component, i.g., the Jacobian matrix of layer normalization, JLN (x) = LN(x) , where is the input of LN(). Following the proof of Xiong et al. (2020); Takase et al. (2023) with the assumption that follow normal distribution with mean of 0, we can have: LN(x) (cid:18) x2 (cid:19) xx x2 2 = where σx is the standard deviations of and is the hidden dimention. Hence, xx σ2 xd where = (x µx)/σx is the standard normal distribution obtained after layer normalization. Since 1 in LLMs, we can finally obtain: = zz LN(x) 1 σx σx (5) (6) = (cid:19) (cid:18) (cid:18) (cid:19) . LN(x) 1 σx = In practice, we observe that σx gradually grows larger than one during training, which means the spectral norm of the Jacobian matrix of LN is smaller than 1. According to the derivative of PostLN in Equation (3), this down-scaling factor will accumulate as (cid:81)L over multiple layers L, l=1 leading to gradient vanishing in early layers in Post-LN Transformers. 1 σl (7) I. In contrast, the derivative of the residual connection in Pre-LN is decoupled from the term associated with the derivative of LN, as shown in Equation (4). This design helps prevent the vanishing gradient problem in early layers. However, since Pre-LN does not constrain the residual connection, the outputs of successive transformer blocks accumulate as the layer depth grows. Consequently, the derivative of Pre-LN in Equation (4) approaches an identity matrix, indicating that the entire PreLN operation of Equation (4) ceases to contribute effectively to learning. This explains why deeper layers in Pre-LN tend to contribute less to the models overall learning compared to earlier layers. 3 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN"
        },
        {
            "title": "2.2 EMPIRICAL EVALUATION SETUP",
            "content": "Methods: Our evaluation methodology involves comparative analysis of two modelsone utilizing Pre-LN and the other employing Post-LN. By empirically assessing the effectiveness of layers across different depths in each model, we expect to see that Pre-LN models will exhibit decrease in the effectiveness of deeper layers, whereas Post-LN models will show sustained or even improved quality in deeper layers. LLM Models: To rigorously evaluate our hypothesis, we conduct experiments on two distinct categories of LLMs: (i) Open-weight large-scale LLMs and (ii) In-house small-scale LLMs. In the openweight category, we select LLaMa2-7B (Touvron et al., 2023) as representative Pre-LN model and BERT-large (Devlin, 2018) as Post-LN model. However, these open-weight models differ not only in normalization but also in other factors such as training data, activation functions, and context length, complicating our ability to isolate the impact of normalization alone. To control for these confounding variables, we conduct second set of experiments by training small-scale LLMs from scratch ourselves. The goal is to ensure that the only difference between the models is the choice of layer normalization. Specifically, we train LLaMa-130M models on the C4 dataset with either Pre-LN or Post-LN, using RMSNorm (Zhang & Sennrich, 2019) and SwiGLU activations (Shazeer, 2020), following Lialin et al. (2023b); Zhao et al. (2024). Please refer to Appendix for more training configuration details. Evaluation Metrics: critical challenge in validating our hypothesis lies in defining and selecting robust metrics that capture the effectiveness of individual layers. In this study, we employ two metrics: (i) Angular Distance and (ii) Performance Drop, which provide meaningful evaluation of the role and contribution of each layer. In addition, we report the gradient norm of each layer to demonstrate the effect of different layer normalization on the gradient flow. (i) Angular Distance d(xℓ, xℓ+n) is used in Gromov et al. (2024) to measure the angular distance between the input to layer ℓ and the input to layer ℓ + on neutral pre-training dataset. Formally, assuming xℓ is the input to the layer ℓ + n, the angular distance between layers ℓ and and its subsequent nth layer, i.e., ℓ + n, on single token is given by is the input to the layer ℓ, and xℓ+n d(xℓ, xℓ+n) = (cid:32) 1 π arccos xℓ+n xℓ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)xℓ+n (cid:12)xℓ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:33) (8) where denotes the L2-norm, and the factor of 1/π scales d(xℓ, xℓ+n) to the range [0, 1]. To eliminate the effect of randomness, the angular distance reported in this paper is averaged over 256K tokens from the C4 dataset. smaller value of d(xℓ, xℓ+n) indicates shorter distance, meaning that the two vectors are more similar. Layers whose representations are extremely similar to their neighboring layers mean that they can be easily removed, and therefore their weights are less effective. Ideally, representation should change substantially from layer to layer in order to most effectively make use of the parameters of network (Yang et al., 2023; Gromov et al., 2024). (ii) Performance Drop (ℓ) refers to the difference in the performance of an LLM before and after pruning the layer ℓ. It quantifies the performance degradation caused by the removal of that layer. Formally, it can be defined as follows: (ℓ) = (ℓ) pruned Poriginal (9) where Poriginal is the performance of the model without any pruning, (ℓ) pruned is the performance of the model after pruning layer ℓ. smaller value of (ℓ) indicates that removing the layer causes minimal change to the models output, suggesting the layer is less important. Specifically, for LLaMA2-7B, we choose the commonly used MMLU (Hendrycks et al., 2020) as the evaluation task; for BERT-large, we opt for SQuAD v1.1 (Rajpurkar, 2016) as the evaluation task. Given the limited capacity of our in-house trained LLMs, we choose ARC-e (Clark et al., 2018) after supervised fine-tuning, instead of MMLU, for performance drop. 2.3 EVALUATION RESULTS 2.3.1 OPEN-WEIGHT LARGE-SCALE LLMS Figure 2-(a, c) illustrate the metric values for BERT-Large. Both metrics indicate that, as Post-LN model, the early layers of BERT-Large are less effective compared to the deeper layers. As shown in 4 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN Figure 2: Results of open-weight large-scale LLMs. Angular Distance (a, b): Each column represents the angular distance from the initial layer ℓ (x-axis) and its subsequent nth layer (y-axis). The distance is scaled to the range [0, 1], where yellow indicates smaller distances and purple indicates larger distances. Performance Drop (c, d): (c): SQuAD v1.1 performance drop of removing each layer from BERT-large; (d): MMLU accuracy drop of removing each layer from LLaMa2-7B. Figure 3: Results of in-house small-scale LLaMa-130M. Angular Distance (a, b): Each column represents the angular distance from the initial layer ℓ (x-axis) and its subsequent nth layer (yaxis). The distance is scaled to the range [0, 1], where yellow indicates smaller distances and purple indicates larger distances. Performance Drop (c, d): ARC-e performance drop of removing each single layer from LLaMa-130M. Gradient Norm (e): Gradient norm of each layer in LLaMa-130M. Figure 2-a, the first half of BERT-Large tends to have smaller angular distance (more yellow) from neighboring layers than the second half. In particular, layers 3, 4, 9, 10, and 11 show very high similarity to their subsequent layers. In Figure 2-c, the performance drop on SQuAD of removing an early layer is significantly smaller than the impact of removing deeper layer. Intriguingly, removing layers 2 and 3 can even improve the performance slightly. In contrast, Figure 2-(b, d) display the metric values for LLaMa2-7B. As Pre-LN model, the angular distance between neighboring layers decreases gradually (from purple to yellow) from the top layers to the 30th layer as illustrated in Figure 2-b. Notably, the deeper layers (20th to 30th) exhibit extremely small angular distances to their adjacent layers. This trend is consistent with the 5 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN MMLU performance in Figure 2-d, where the removal of deeper layers results in almost negligible accuracy loss while removing early layers causes substantial drop in accuracy. In summary, we observe that the least effective layers in LLaMa2-7B are located in the deeper layers, whereas the early layers in BERT-Large are less effective than the deeper layers. The results from the category of open-weight large-scale LLMs strongly support our hypothesis, demonstrating clear alignment with our expectations. 2.3.2 IN-HOUSE SMALL-SCALE LLMS Figure 3 illustrates all metric values for two LLaMa-130M models, where the only difference between them is the choice of layer normalization. Figures 3-(a, b) show the Angular Distance for Post-LN and Pre-LN, respectively. Without the effects of other compounding factors, this comparison provides clearer distinction between PostLN and Pre-LN compared to open-weight large-scale LLMs. In Post-LN models, the most similar layers are concentrated in the early stages, with the first three layers showing particularly low distance. As the depth increases, the layers become increasingly distinctive. In contrast, the Pre-LN LLaMa-130M exhibits gradual decrease in angular distance as depth increases, leading to highly similar deep layers. Figures 3-(d, e) further confirm this with the Performance Drop metric: removing early layers (e.g., 0-7 layers) in Post-LN results in minimal performance loss, while deeper layers (especially layers 9-11) are critical to preserving the original performance. However, Pre-LN LLaMa-130M exhibits the opposite trend, where removing most layers after the first layer causes negligible performance loss, indicating that they contribute little to the models output. Figure 3-(c) shows the gradient norm of each layer for Post-LN and Pre-LN at the beginning of the training. The results perfectly align with our expectations: Post-LN leads to larger gradients in deeper layers but suffers from severe gradient vanishing in early layers, whereas Pre-LN maintains healthy gradient flow in early layers but diminishes in later layers. With the consistent findings from both open-weight LLMs and our in-house LLMs, we can conclude that the widespread use of Pre-LN in LLMs is the root cause of the ineffectiveness of deep layers."
        },
        {
            "title": "3 MIX-LAYER NORMALIZATION (MI X-LN)",
            "content": "Having validated our hypothesis that the use of Pre-LN is the root cause of the ineffectiveness of deeper layers, we propose Mix-Layer Normalization (Mix-LN), novel normalization strategy designed to enhance the effectiveness of both middle and deeper layers in LLMs. The key idea behind Mix-LN is to leverage the strengths of both Pre-LN and Post-LN. Post-LN has been shown to improve the effectiveness of deeper layers, while Pre-LN is more effective for earlier layers. Therefore, we propose to apply Post-LN to the initial layers and Pre-LN to the later layers, ensuring that the middle and deeper layers benefit from the advantages of both methods. Formally, for an LLM with layers, we apply Post-LN to the first aL layers and Pre-LN to the remaining (1 a)L layers, where [0, 1] is hyperparameter controlling the transition point between the two normalization strategies. The functions and denote the floor and ceiling operations, respectively. Although the final layers may still experience smaller gradients due to the use of Pre-LN, the negative impact is substantially mitigated because the number of layers employing Pre-LN is reduced from to (1 a)L. This reduction improves gradient flow in the deeper layers, enhancing their effectiveness. Additionally, we expect that Mix-LN can alleviate training instability issues caused by Post-LN (Nguyen & Salazar, 2019; Wang et al., 2024), as reducing the number of layers using Post-LN leads to smaller accumulation of gradient attenuation, according to the analysis in Section 2.1."
        },
        {
            "title": "4 MAIN EXPERIMENTAL RESULTS",
            "content": "4.1 LLM PRE-TRAINING In this section, we verify the effectiveness of Mix-LN by comparing it with various common normalization techniques, including Post-LN (Nguyen & Salazar, 2019), DeepNorm (Wang et al., 6 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN 2024), and Pre-LN (Dai, 2019). Following Lialin et al. (2023a); Zhao et al. (2024), we conduct experiments using the LLaMA-based architecture with various sizes from 71M to 1B parameters, incorporating RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang & Sennrich, 2019). Models are trained with Adam (Kingma, 2014) using different learning rates based on model size: specifically, we use learning rate of 1e-3 for models with 250M parameters and below, and learning rate of 5e-4 for the 1B parameter model. All models of the same size are trained with identical configurations except for the normalization. To determine the optimal value for the hyperparameter α in Mix-LN, we performed small hyperparameter sweep using LLaMA-250M, as shown in Table 5. We found that α = 0.25 provided the best performance, and therefore, we applied this value across all model sizes. Table 1: Perplexity () comparison of various normalization methods across various LLaMA sizes. LLaMA-71M LLaMA-130M LLaMA-250M LLaMA-1B Training Tokens Post-LN DeepNorm Pre-LN Mix-LN 1.1B 35.18 34.87 34.77 33.12 2.2B 26.95 27.17 26.78 26.07 3.9B 1409.09 22.77 21.92 21.39 5B 1411.54 1410.94 18.65 18.18 Results are shown in Table 1. Post-LN generally yields the worst performance and even diverges with larger models, aligning with previous studies that indicate Post-LN suffers from training instability in Transformers (Xiong et al., 2020; Takase et al., 2022). DeepNorm, as modified version of Post-LN, achieves comparable performance to Pre-LN with smaller model sizes; however, it also experiences divergence during training with 1B parameter models. This observation confirms severe training instability of Post-LN, where gradients in early layers vanish, preventing proper model convergence. In contrast, Mix-LN consistently achieves the lowest perplexity across various model sizes. Mix-LN achieves notable gain by 1.65 and 0.53 perplexity with LLaMA-71M and LLaMA250M, respectively, compared to the widespread Pre-LN. The above results clearly show that Mix-LN not only overcomes the instability of Post-LN but also enhances the model quality by combining the benefits of Pre-LN and Post-LN, making it an ideal choice for large-scale LLMs. Figure 4: Training curve (eval perplexity) of Mix-LN and Pre-LN with LLaMa-7B. 4.2 SCALING UP TO 7B MODEL Evaluating whether the benefits of Mix-LN scale to larger models, such as 7B parameters, is essential to demonstrate if its benefits also hold for larger scale models. To this end, we conducted experiments using the LLaMa-7B architecture, which features an embedding size of 4096 and 32 total layers, following the setup of Zhao et al. (2024). All training configurations were kept identical, with the exception of the layer normalization method. Due to computational constraints, we were able to complete only 13,000 steps of training. Based on our experience, models that exhibit consistent improvements early in training typically retain these advantages through the later stages. The training curve comparison is presented in Figure 4, where it is evident that Mix-LN consistently outperforms Pre-LN during the early training stages of LLaMa-7B. 7 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN"
        },
        {
            "title": "4.3 SUPERVISED FINE-TUNING",
            "content": "Table 2: Fine-tuning performance () of LLaMa with various normalizations. Method MMLU BoolQ ARC-e PIQA Hellaswag OBQA Winogrande Avg. Post-LN DeepNorm Pre-LN Mix-LN Post-LN DeepNorm Pre-LN Mix-LN 22.95 23.60 24.93 26. 22.95 23.35 26.54 27.99 37.83 37.86 38.35 56.12 37.82 37.83 62.20 61.93 LLaMA-250M 52.72 61.10 63.55 66.34 LLaMA-1B 49.51 52.94 67.79 68.50 26.17 25.69 26.34 30.16 25.04 26.19 30.96 31.35 26.94 36.62 40.15 41.68 25.08 27.06 45.70 48.11 11.60 15.00 16.20 18. 13.80 11.80 17.40 18.80 49.56 49.57 49.01 50.56 49.57 49.49 50.51 55.93 32.54 35.63 36.93 41.34 31.96 32.67 43.01 44.66 We believe that the superior middle and deeper layers produced by Mix-LN are better equipped to learn during supervised fine-tuning. This advantage stems from the fact that these layers capture more diverse and rich features compared to those trained with Pre-LN. In complex downstream tasks, having access to broad spectrum of features allows the model to make more nuanced predictions, leading to improved generalization. To verify our conjecture, we follow Li et al. (2024) and fine-tune the models obtained in Section 4.1 on Commonsense170K (Hu et al., 2023), evaluating them on eight downstream tasks. As shown in Table 2, Mix-LN consistently outperforms other normalization techniques across all evaluated datasets. For the LLaMA-250M model, Mix-LN achieves significant average gain of 4.26% and 17.31% improvement on BoolQ compared to Pre-LN. Similar trends are observed with the larger LLaMA-1B model. Even though Mix-LN only slightly reduces perplexity by 0.25 compared to Pre-LN, it delivers substantial performance gains in supervised fine-tuning. 4.4 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK Table 3: RLHF comparison of final reward () of Pre-LN and Mix-LN with LLaMA-1B. Method Model Final Reward Pre-LN LLaMA-1B Mix-LN LLaMA-1B 0.75 1.32 Consistently, the benefits of Mix-LN can be seamlessly transferred to RLHF. Following Adam-mini (Zhang et al., 2024), we implement the RLHF workflow from InstructGPT (Ouyang et al., 2022) and train 1B models obtained in Section 4.1 on the ultrafeedback dataset to optimize the preference reward. Table 3 illustrates that Mix-LN achieves notable reward gain (higher is better) over PreLN, i.e., 1.32 vs. 0.75. 4.5 EVALUATION WITH VISION TRANSFORMERS Table 4: Accuracy () comparison of Pre-LN and Mix-LN on ViT models. Model ViT-Tiny ViT-Small Pre-LN Mix-LN 67.30 67.34 75.99 76.40 To evaluate Mix-LN on non-language models, we replace Pre-LN in ViT models with Mix-LN with α = 0.25. We train the updated model for 120 epochs on ImageNet-1K following Liu et al. (2022a), using the ConvNeXt (Liu et al., 2022b) configurations. The results clearly demonstrate that the benefits of Mix-LN also generalize to non-language models. The results in Table 4 demonstrate that the benefits of Mix-LN extend to non-language tasks, with performance gains that are more pronounced in larger models (ViT-Small) compared to smaller ones (ViT-Tiny). Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN"
        },
        {
            "title": "5 ANALYSIS AND MORE EVALUATIONS",
            "content": "Table 5: Perplexity of LLaMA-1B with various Post-LN ratios α. Pre-LN Mix-LN Post-LN ratios α 0 16.7% 25.0% 33.0% 41.7% 50.0% Perplexity 18.65 18.34 18.18 18.41 18. 18.86 Post-LN 100% 1434 What is the proper Post-LN ratio α for Mix-LN? Mix-LN has hyperparameter, α, that controls the ratio of layers applying Post-LN. Specifically, α = 0 means Pre-LN is applied to all layers, while α = 1 corresponds to pure Post-LN. To determine the optimal Post-LN ratio, we conduct sweep over the values [0, 16.7%, 25.0%, 33.0%, 41.7%, 50.0%, 100%] using LLaMA-1B on the C4 dataset. The results are shown in Table 5. As the normalization transitions from Pre-LN to MixLN, the model achieves progressively lower perplexity, reaching its best performance at α = 0.25. Beyond this point, performance begins to decline, although it still surpasses that of pure Pre-LN until most layers apply Post-LN, where performance degrades significantly. Based on these results, we choose α = 0.25 for all model sizes, although we believe there is potential to further improve the performance of Mix-LN by searching for the optimal α for each individual model. Figure 5: Angular distance from initial layer ℓ (x-axis) with block size (y-axis) of LLaMA-130M. Mix-LN promotes representation diversity across layers. As we have claimed, our hybrid approach promotes more balanced gradient flow throughout the entire network. To validate this, we report the angular distance of LLaMA-130M for Pre-LN, Post-LN, and Mix-LN in Figure 5. Given block size n, the layers with the smallest distances are highlighted in the lightest yellow in each row. Notably, Mix-LN consistently exhibits larger distances (darker color) across layers compared to Pre-LN, except for the final two layers. This indicates that Mix-LN produces more diverse representations between layers than Pre-LN. In contrast, the smallest distances in Post-LN are concentrated in the early layers, reinforcing the notion that Post-LN tends to restrict representation diversity in deeper layers. (a) Layer gradient norm of LLaMA-250M with various normalization techniques. (b) Performance drop comparison of LLaMA-130M across layers for Pre-LN, Post-LN, and Mix-LN. Figure 6: Comparison of gradient norms and performance drops of layer pruning for different layer normalization. Mix-LN enhances healthier gradient norms across all layers. We compare the gradient norm of different LN at initialization in Figure 6a. It demonstrates that Mix-LN maintains more consistent Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN gradient norms across all layers. This balance results in more uniform distribution of gradient norms across layers, allowing all parts of the networkboth shallow and deep layersto contribute effectively to model training. Performance drop of layer pruning for Mix-LN. To further evaluate the effectiveness of MixLN, we compare the performance drop (P ) across layers with Pre-LN and Post-LN. Fig. 6b shows Mix-LN achieves more significant contribution from deeper layers. Specifically, the deeper layers in Mix-LN models show larger compared to Pre-LN, indicating that these layers contribute more effectively to the models overall performance. Comparing with other Layer Normalization. In addition, we conducted comparisons using LLaMA-250M to evaluate Mix-LN against recently proposed normalization methods, including Admin (Liu et al., 2020), Sandwich-LN (Ding et al., 2021), and Group-LN (Wu & He, 2018; Ma et al., 2024). The results indicate that Sandwich-LN and Group-LN slightly outperform Pre-LN, while Admin performs worse. However, all of these methods fail to reduce perplexity below 23, falling short of Mix-LN. This result highlights the effectiveness of Mix-LN compared to other recent innovations. Table 6: Comparison against other normalization methods on LLaMA-250M. Model Pre-LN Admin Group-LN Sandwich-LN Mix-LN LLaMA-250M 23.39 24.82 23.10 23. 22."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 NORMALIZATION IN LANGUAGE MODELS Layer Normalization (LN), first proposed by Ba (2016), has become the de facto standard for normalizing activations in modern language models. It directly estimates normalization statistics from the summed inputs to neurons within hidden layer, ensuring that the input distribution to each layer remains stable throughout training. In the original Transformer (Vaswani, 2017), LN was initially applied after the residual connection, configuration known as Post-LN. However, subsequent studies (Baevski & Auli, 2018; Dai, 2019; Nguyen & Salazar, 2019) found that placing LayerNorm before the residual connection (Pre-LN) results in more stable performance, especially in large language models (Brown, 2020; Touvron et al., 2023; Jiang et al., 2023). Xiong et al. (2020) theoretically demonstrated that Post-LN results in larger gradients near the output layer, making the use of warm-up essential to avoid instability is necessary. Conversely, Pre-LN scales down gradients with the depth of the model, which ensures more stable gradients during initialization. Our work builds upon Xiong et al. (2020), highlighting that while Pre-LN prevents instability by reducing gradient magnitudes, smaller gradients in deeper layers can diminish the effectiveness of the corresponding weights. To improve the effectiveness of deeper layers in language models, various LN variants have been proposed. For instance, Wang et al. (2019) verified empirically that Post-LN suffers from gradient vanishing in deep Transformers, while Pre-LN facilitates stacking more layers. They consequently introduced dynamic linear combination of layers (DLCL), which connects all previous layers to improve trainability. Similar techniques have been employed in other works (Bapna et al., 2018; Dou et al., 2018). Liu et al. (2020) revealed that Post-LN has strong dependencies on the residual branch, often leading to instability. To address this, Adaptive Model Initialization (Admin) was introduced, which uses additional parameters to control residual dependencies in Post-LN, stabilizing training. DeepNorm (Wang et al., 2024) further improved the trainability of deep Transformers by upscaling the residual connection before applying LN, reducing model updates, and enabling deeper architectures. Additionally, Ding et al. (2021) proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. Takase et al. (2022) identified that Post-LN tends to preserve larger gradient norms in deeper layers, potentially leading to more effective training. To address the issue of gradient vanishing in early layers, they introduced B2T, method that uses residual connection to bypass all LN except the final one in each layer. We got inspiration from Takase et al. (2022), addressing the limitations of both Pre-LN and Post-LN by combining them. We study Scaled Initialization and Scaled Embed in Appendix B. 10 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN 6."
        },
        {
            "title": "INEFFICACY OF DEEP LAYERS IN LLMS",
            "content": "The Inefficacy of deep layers in LLMs serves as valid indicator for LLM pruning. Yin et al. (2023) demonstrated that the deeper layers of prominent LLMs like LLaMA and Mistral can be pruned more aggressively than earlier layers, without causing significant drop in performance. Similarly, Gromov et al. (2024) and Men et al. (2024) further explored layer pruning, identifying the deeper layers of LLMs as typically less essential. Lad et al. (2024) observed that in models like Pythia and GPT-2, deeper layers exhibit strong resilience to interventions, such as layer deletion or swapping. Our work shares similarities with Gromov et al. (2024) in applying angular distance to assess the effectiveness of layers. However, while they identify the inefficacy of deeper layers, they do not offer an explanation for this phenomenon nor propose solution to address it. While previous studies often view these characteristics of deeper layers as an opportunity for model compression (Siddiqui et al., 2024; Zhong et al., 2024; Sreenivas et al., 2024), we argue that this behavior reveals deeper training shortfallprimarily due to the widespread use of Pre-LN. In response, we introduce Mix-LN, novel method that enhances the effectiveness of deeper layers, ensuring that the entire architecture is more effectively trained and fully leverages the networks parameters."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we have addressed the inefficiencies of deep layers in LLMs by identifying the widespread use of Pre-LN as the root cause. Pre-LN leads to diminished gradients in deeper layers, reducing their effectiveness. While Post-LN preserves deeper gradients, it suffers from vanishing gradients in earlier layers. To resolve this, we introduced Mix-LN, hybrid normalization technique that combines the strengths of both Pre-LN and Post-LN. By applying Post-LN to early layers and Pre-LN to deeper layers, Mix-LN achieves balanced gradient norms throughout the network, enabling more effective training. Our experiments show that Mix-LN consistently outperforms both Pre-LN and Post-LN, enhancing pre-training and fine-tuning performance without increasing model size. By fully utilizing the potential of deep layers, Mix-LN improves the overall capacity and efficiency of LLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018. Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. Training deeper neural machine translation models with transparent attention. arXiv preprint arXiv:1808.07561, 2018. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Zihang Dai. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 11 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and Tong Zhang. Exploiting deep representations for neural machine translation. arXiv preprint arXiv:1810.10181, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Vedang Lad, Wes Gurnee, and Max Tegmark. The remarkable robustness of llms: Stages of inference? arXiv preprint arXiv:2406.19384, 2024. Pengxiang Li, Lu Yin, Xiaowei Gao, and Shiwei Liu. Owlore: Outlier-weighed layerwise sampled low-rank projection for memory-efficient llm fine-tuning. arXiv preprint arXiv:2405.18380, 2024. Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: Highrank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2023a. Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Stack more layers differently: High-rank training through low-rank updates. arXiv preprint arXiv:2307.05695, 2023b. Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020. Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Karkkainen, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022a. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022b. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Toan Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 12 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN Rajpurkar. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, and Pavlo Molchanov. deeper look at depth pruning of llms. arXiv preprint arXiv:2407.16286, 2024. Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. B2t connection: Serving stability and performance in deep transformers. arXiv preprint arXiv:2206.00330, 2022. Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek Wong, and Lidia Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019. Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 319, 2018. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 1052410533. PMLR, 2020. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. arXiv preprint arXiv:2310.02244, 2023. Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, and Liangzhi Li. Blockpruner: Finegrained pruning for large language models. arXiv preprint arXiv:2406.10594, 2024. 13 Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN"
        },
        {
            "title": "A DETAILS OF EXPERIMENTS",
            "content": "A.1 ARCHITECTURE AND HYPERPARAMETERS We introduce details of the LLaMA architecture and hyperparameters used for pre-training following (Lialin et al., 2023a; Zhao et al., 2024). Table 7 shows the most hyperparameters of LLaMA models across model sizes. We use max sequence length of 256 for all models, with batch size of 512, and total of 131K tokens per batch. Learning rate warmup is applied to the first 10% of the training steps. We train models using Adam with cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate. We use learning rate of 1e-3 for models with 250M parameters and below, and learning rate of 5e-4 for the 1B parameter model. Table 7: Hyperparameters of LLaMA models used in this paper. Params Hidden Intermediate Heads Layers Steps Data amount 71M 130M 250M 1 7 512 768 1024 2048 4096 1368 2048 2560 5461 11008 8 12 16 32 32 12 12 24 24 32 10K 20K 40K 100K 13K 1.1 2.2 3.9 5.0 1.7 LR 1 103 1 103 1 103 5 104 5 104 Batch Size α 512 512 512 512 512 25% 25% 25% 25% 6.25%"
        },
        {
            "title": "B COMPATIBILITY TO ADVANCED",
            "content": "In this section, we also evaluate if Mix-LN can integrate well with the advanced techniques proposed to stabilize training. Specifically, we evaluate the commonly used Scaled Initialization (Nguyen & Salazar, 2019; Scao et al., 2022) that initializes W2 and W0 with smaller normal distribution (0, (cid:112)2/5d/ 2N ) to stabilize training dynamics; and Scaled Embed (Takase et al., 2023) scales up embeddings to stabilize LayerNorm gradients. We observe that both Pre-LN and Mix-LN work effectively with Scaled Initialization. However, incorporating Scaled Embed on top of this setup leads to degradation in performance. Table 8: Perplexity of LLaMA-130M with various normalization methods with Scaled Initialization and Scaled Embed. Normalization Pre-LN Mix-LN Pre-LN Mix-LN Pre-LN Mix-LN Scaled Initialization Scaled Embed Perplexity 32.18 29. 30.63 29.77 31.28 31."
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "Eindhoven University of Technology",
        "University of Oxford",
        "University of Surrey"
    ]
}