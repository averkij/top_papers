{
    "paper_title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
    "authors": [
        "Guray Ozgur",
        "Eduarda Caldeira",
        "Tahar Chettaoui",
        "Jan Niklas Kolf",
        "Marco Huber",
        "Naser Damer",
        "Fadi Boutros"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model."
        },
        {
            "title": "Start",
            "content": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers Guray Ozgur1,2, Eduarda Caldeira1,2, Tahar Chettaoui1,2, Jan Niklas Kolf1,2, Marco Huber1,2, Naser Damer1,2, Fadi Boutros1 1Fraunhofer IGD, Germany, 2TU Darmstadt, Germany 6 2 0 2 9 ] . [ 1 1 4 7 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA1, trainingfree approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJBC), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model. 1. Introduction FIQA evaluates the utility of face images for face recognition (FR), specifically measuring recognition utility or suitability for identity verification [1, 20]. Unlike general Image Quality Assessment (IQA) methods that assess quality from human perception [33, 37, 38], FIQA quantifies how effectively facial image serves automated recognition tasks. As demonstrated in [18], high perceived quality does not always correlate with FR utility, particularly when factors like facial occlusions are present. Current FIQA ap1The implementation is publicly available at: https://github. com/gurayozgur/ViTNT-FIQA Figure 1. Boxplots of mean L2 distances between corresponding patch embeddings from consecutive ViT-B blocks computed for 11 quality groups, each having 0.5M images, from 5.5M images of SynFIQA [42]. Each box summarizes the distribution of average patch-embedding distances across images in quality group, lower distances empirically correspond to higher ground-truth quality for most block transitions. The inset (Block 11 12) shows the quality gradient (low high) and illustrates how the distances across groups provide measure of quality discriminability, i.e. the higher the quality, the lower the distance. proaches primarily exploit only final-layer representations from deep networks [2, 10, 31, 36, 48, 50]. Trainingfree methods, while offering immediate applicability to pretrained models, typically require either multiple forward passes with varied dropout patterns [50] or backpropagation [3, 31], increasing computational overhead  (Table 1)  . Recent research on ViT internals has revealed that transformer blocks refine features iteratively with high interblock similarity [45], where residual connections propagate information forward and each block produces slight refinements. This smooth feature evolution trajectory suggests that the stability of patch representations across intermediate blocks may contain quality-relevant information, yet this remains unexplored for FIQA. We propose ViT-based NoTraining FIQA approach, hence the name ViTNT-FIQA, which analyzes the stability of patch embedding evolution Figure 2. Overview of our ViT-based quality assessment method ViTNT-FIQA. (1) The face image is patchified and embedded. (2) Intermediate patch representations are extracted from selected transformer blocks. (3) L2-normalized embeddings are compared across consecutive blocks to measure patch-level feature distances. (4) Distances are mapped to quality scores per patch level, which are aggregated, uniformly or using attention weights, to produce the final image-level quality estimate. across intermediate transformer blocks in pre-trained ViTbased models. Our method is grounded in the hypothesis that high-quality face images exhibit smoother, more stable feature refinement trajectories across blocks, while degraded images show erratic transformations. We empirically validate this hypothesis on SynFIQA [42], qualitylabeled synthetic dataset with 550,000 images across controlled degradation levels (Figure 1), demonstrating that cross-block patch embedding distances systematically decrease with increasing ground-truth quality across most transformer block transitions. Unlike existing approaches, our ViTNT-FIQA does not make use of any quality labels [23, 40], any training [10, 48], or any custom loss [36]. Moreover, different from training-free approaches [31, 50], it only requires single forward pass without backpropagation. Clear conceptual comparisons to the state-of-the-art (SOTA) methods are shown in Table 1. We make the following contributions: training-free FIQA method that measures patch-level cross-block distances in pre-trained ViT models, requiring only single forward pass without backpropagation or architectural modifications. comprehensive analysis demonstrating that cross-block embedding stability correlates with face image quality, providing novel quality indicator. Extensive evaluation across eight benchmark datasets (LFW[24], AgeDB-30[39], CFP-FP[47], CALFW[55], IJB-C[35]) Adience[16], CPLFW[54], XQLFW[30], demonstrating competitive performance with existing SOTA methods. Table 1. Conceptual comparison on the design choices between our ViTNT-FIQA and recent FIQA approaches in the literature. Inference a - r F 1 100 1 1 1 1 1 1 1 1 1 1 a a 0 0 0 0 0 0 0 0 1 0 0 0 F PFE [48] SER-FIQ [50] FaceQnet [23] MagFace [36] SDD-FIQA [40] CR-FIQA [10] DifFIQA [4] eDifFIQA [5] GRAFIQS [31] CLIB-FIQA [41] VIT-FIQA [2] ViTNT-FIQA (Ours) l a Q r e i r s s 2. Related Work 2.1. Vision Transformer Internals ViTs [15] have been successfully applied to FR [12, 13, 27, 29, 49, 56] and recently to FIQA [2], demonstrating their effectiveness in modeling facial features. Unlike CNNs that process images through hierarchical local operations with gradually expanding receptive fields [32], ViTs divide images into patches and model global relationships through self-attention mechanisms [51], enabling long-range dependency modeling from the first layer onward. Feature Refinement and Representation Similarity: Research on ViT internals has revealed that transformer blocks refine features iteratively. Raghu et al. [45] demonstrated through centered kernel alignment (CKA) analysis that ViTs maintain highly similar representations across all layers, exhibiting much more uniform representations compared to CNNs which show distinct stage boundaries. Their layer-wise similarity heatmaps revealed solid grid pattern in ViTs, contrasting with the clear low/high stage gaps observed in ResNets. This uniform similarity structure indicates that each ViT block refines features incrementally, with cosine similarity between successive blocks remaining high throughout the network. Role of Residual Connections: Skip connections in ViTs are found to be even more influential than in ResNets, having strong effects on performance and representation similarity by removing skip connections, which causes representations before and after that block to become dissimilar and results in accuracy degradation [45]. This demonstrates that much of the feature information is carried forward via identity paths, with each blocks output being slight refinement of its input rather than complete transformation. The global self-attention mechanism aggregates context early while residual connections propagate low-level features forward, ensuring gradual enhancement across blocks [45]. Leveraging Intermediate Representations: The understanding of ViT feature evolution has motivated research on utilizing intermediate representations, specifically on early exits [6, 7, 43, 52, 53]. Early exit mechanisms [6, 7, 43, 52, 53] allow inference to terminate at intermediate blocks, exploiting the fact that different depths capture distinct levels of feature abstraction. The effectiveness of early exits demonstrates that intermediate block representations contain valuable information beyond serving as stepping stones to final outputs [21, 34]. Our ViTNT-FIQA is also motivated by these insights into ViTs smooth feature refinement trajectory. We empirically validate that the magnitude of change between consecutive blocks, reflecting the degree of feature transformation, can reveal quality-relevant information about face images, see Figure 1. Given that ViTs refine features gradually with high inter-block similarity [45], we investigate whether analyzing the stability of patch embedding evolution across multiple transformer blocks can distinguish high-utility samples from low-utility samples. 2.2. Face Image Quality Assessment FIQA approaches can be categorized into four groups: (1) Label-generation approaches train regression networks using quality labels from various sources. FaceQnet [23] uses the comparison score between the sample and corresponding ICAO-compliant sample as the quality label, while SDD-FIQA [40] employs distribution distances. RankIQ [11] adopts learning-to-rank strategy, training models to predict quality rankings based on FR performance metrics across different datasets. limitation of these approaches is that they often decouple FIQA from FR, typically employing shallower networks that dont extract comprehensive facial features. (2) Non-FR model approaches include DifFIQA [4], which leverages diffusion models to assess embedding robustness under different conditions, and eDifFIQA [5], which distills this approach into lighter model for faster inference. While these methods can achieve high accuracy, they incur significant compu- (3) Pre-trained FR analysis approaches tational costs. operate on fixed FR models without requiring additional training. SER-FIQ [50] measures embedding stability under dropout perturbations by evaluating embedding consistency with varied dropout patterns. GraFIQs [31] uses gradient magnitudes during backpropagation to evaluate sample alignment with the FR models objective. FaceQAN [3] estimates quality by quantifying adversarial robustness. (4) FR-integrated approaches directly incorporate quality assessment into the FR training process. MagFace [36] links quality scores to embedding magnitudes through regularized training. PFE [48] models embeddings as Gaussian distributions with uncertainty representing quality. CRFIQA [10] estimates quality by predicting samples relative classifiability within the embedding space. ViT-FIQA [2] extended standard ViT backbones with learnable quality token designed to predict utility scores for face images. These FR-integrated approaches have consistently achieved top rankings in SOTA evaluations [2, 10, 36]. Among these categories, training-free methods, i.e. Pretrained FR analysis approaches, offer the advantage of immediate applicability to pre-trained models without modification or fine-tuning. Our ViTNT-FIQA belongs to this category, requiring no additional training beyond the standard FR model. As summarized in Table 1, existing trainingfree approaches rely on either multiple forward passes [50] or backpropagation [3, 31]. In contrast, we exploit the hierarchical nature of ViT processing by analyzing the stability of patch representations across intermediate transformer blocks, requiring only single forward pass without backpropagation. This design makes ViTNT-FIQA the only method using just single forward pass among training-free FIQA methods while providing novel perspective on quality assessment through cross-block feature stability analysis. 3. Methodology As discussed in Section 2.1, research on ViT has demonstrated that transformer blocks refine features iteratively with high inter-block similarity [45], where each block produces slight refinements in the representations rather than complete transformations. This smooth feature evolution trajectory motivates our approach: we hypothesize, which we validate later in this section, that high-quality face images maintain stable patch representations across transformer blocks, while low-quality images exhibit larger changes due to quality-degrading factors such as blur, occlusion, or poor illumination. We begin by formalizing the ViT architecture to establish the mathematical foundation for our quality assessment framework. Preliminaries on ViT: Consider ViT architecture [15], as shown in Figure 2. Given an input face image RHW 3 (height H, width , RGB channels), the image is divided into non-overlapping patches of size , resulting in = HW 2 patches. Each patch is linearly projected to an embedding of dimension D: z0 = [Yp1 + b; Yp2 + b; . . . ; YpN + b] + Epos, (1) where RD(P 23) is the patch embedding projection matrix, pi RP 23 is the i-th flattened patch, RD is the bias term, and Epos RN are learnable positional embeddings. The embedded patches are processed through transformer blocks. Each block ℓ {0, . . . , 1} applies multi-head self-attention (MSA) followed by multi layer perceptron (MLP) with residual connections: ℓ = MSA(LN(zℓ1)) + zℓ1, zℓ = MLP(LN(z ℓ)) + ℓ, (2) where LN denotes Layer Normalization and zℓ RN contains refined patch representations at block ℓ. The residual connections (addition operations in Equation 2) maintain high similarity between blocks through influential skip connections that propagate feature information forward [45]. The MSA mechanism at block ℓ computes query Qℓ, key Kℓ, and value Vℓ matrices from the input, then applies scaled dot-product attention across heads: MSA(zℓ1) = Concat(headℓ,1, . . . , headℓ,H )WO ℓ , (3) where each attention head {1, . . . , H} at block ℓ computes: (cid:33) headℓ,h = softmax Vℓ,h, (4) (cid:32) Qℓ,hK (cid:112)D/H ℓ,h with Qℓ,h, Kℓ,h, Vℓ,h RN (D/H) and WO ℓ RDD as the output projection. The attention matrix Aℓ,h = RN captures pairwise patch re- (cid:18) Qℓ,hK softmax (cid:19) ℓ,h D/H lationships at block ℓ and head h, where A(j,p) the attention weight from patch to patch p. ℓ,h represents ViTNT-FIQA: To capture the stability of this refinement process, we measure how patch embeddings evolve across intermediate transformer blocks. Let = {t0, t1, . . . , tT 1} {0, 1, . . . , 1} denote selected subset of transformer blocks from which we extract intermediate representations, where ti + 1 = ti+1 always holds true. For each selected block ti , we extract the patch embeddings zti RN and apply L2 normalization to , ˆz(p) ti = focus on directional changes rather than magnitude variations: z(p) ti z(p) ti 2 where z(p) ti RD denotes the embedding vector of patch at block ti, and ˆz(p) is the unit-norm normalized embedti ding, which are illustrated as green blocks in Figure 2. This normalization ensures that we measure the angular change in feature representations, which is more robust to scale variations across different blocks. (5) For each patch {1, . . . , }, we quantify the instability by computing the Euclidean distance between normalized embeddings from consecutive selected blocks: ti ˆz(p) ti,ti+1 = ˆz(p) d(p) for {0, . . . , 2}, where d(p) ti,ti+1 is the inter-block distance for patch p, shown as purple blocks in Figure 2. To obtain comprehensive measure of patch stability across the entire refinement trajectory, we average these distances: ti+12, (6) d(p) = 1 1 2 (cid:88) i=0 d(p) ti,ti+1, (7) where d(p) is the average cross-block distance for patch p. This directly reflects how much patch embedding changes as it propagates through the transformer blocks. To convert these distance measurements into interpretable quality scores, we apply transformation that maps the continuous distance values to bounded quality range: q(p) = 2 1 + exp(α d(p)) , (8) where α > 0 is scaling parameter and q(p) (0, 1] is the quality score for patch p. Patch qualities (Blue blocks in Figure 2) obtained from patch distances (Purple blocks in Figure 2) through Equation 7, and Equation 8. This formulation maps smaller distances (stable patch representations) to quality scores approaching 1, and larger distances (unstable representations) to scores approaching 0, providing smooth, monotonic mapping. Having established patch-level quality scores, we now address the challenge of obtaining single image-level quality estimate (0, 1]. Since different facial regions may exhibit varying degrees of quality degradation or utility for the FR task, we explore two aggregation strategies. First, we consider uniform aggregation that treats all patches equally: Quniform = q(p). (9) 1 (cid:88) p=1 While this approach is simple, it does not account for the fact that certain facial regions (e.g., eyes, nose) may be more critical for recognition than others (e.g., background patches). To incorporate this spatial importance, we leverage the self-attention mechanism inherent in ViTs. We compute attention-based weights from the last transformer block (ℓ = 1): w(p) = (cid:80)H (cid:80)N h=1 (cid:80)H (cid:80)N p=1 h=1 j=1 A(j,p) (cid:80)N L1,h j=1 A(j,p) L1,h , (10) where AL1,h RN is the attention matrix of head {1, . . . , H} at the last block, A(j,p) L1,h is the attention weight from patch to patch p, and w(p) [0, 1] is the normalized importance weight for patch with (cid:80)N p=1 w(p) = 1. These weights are illustrated as yellow boxes in Figure 2, and this weighting scheme captures how much each patch is attended to during the recognition process. The attentionweighted quality score is then: Qweighted = (cid:88) p=1 w(p) q(p). (11) The complete ViTNT-FIQA operates in single forward pass through pre-trained ViT model: it extracts intermediate patch representations at selected transformer blocks, computes normalized cross-block distances for each patch according to Equation 7, transforms these distances to patch quality scores via Equation 8, and aggregates them to an image-level score using either Equation 9 or Equation 11. Critically, ViTNT-FIQA requires no additional training, no backpropagation, and no architectural modifications, enabling immediate deployment on any pre-trained ViT model while maintaining computational efficiency. Empirical Validation of ViTNT-FIQA: We analyzed all 550,000 images from SynFIQA [42], quality-controlled synthetic dataset produced through two-stage pipeline based on stable diffusion with controllable 3D facial parameters, dual text prompts for occlusion, and post-processing for blur and downsampling. The dataset contains 5,000 identities, each with 10 reference images and 100 degraded variants (10 per reference), organized into 11 quality groups. As shown in Figure 1, we computed mean L2 distances between corresponding patch embeddings from consecutive ViT-B blocks across these quality groups. From this analysis, shown in Figure 1, we see that lower consecutive-block distances systematically correspond to higher ground-truth quality across most block transitions, where the higher-quality groups (right side of each boxplot, representing better image quality) exhibit progressively lower average L2 distances compared to those for lower-quality groups (left side), with the inset for Block 11 12 explicitly illustrating this quality gradient (low high) and showing how distances decrease as quality improves, thereby providing clear measure of quality discriminability. This empirical evidence demonstrates that patch embedding stability across transformer blocks serves as an indicator of face image quality. 4. Experimental Setup are four [28], utilized pre-trained ViT models which ViT-B/WebFace4M/Adaface for We ViT-S/WebFace4M/Adaface FR, [28], ViTB/WebFace12M/Adaface [28], FRoundation ViT-B/16 [12], and one pre-trained foundation model, CLIP ViTB/16 [44] to showcase our methods applicability to any pre-trained ViT model. We conducted extensive experiments across eight benchmark datasets: LFW [24], AgeDB-30 [39], CFP-FP [47], CALFW [55], Adience [16], CPLFW [54], XQLFW [30], and IJB-C [35]. Performance was measured using Error-versus-Discard Characteristic (EDC) curves [19], which assess the impact of discarding low-quality face images on face verification performance and quantify how verification errors decrease as low-quality samples are progressively removed. The False Non-Match Rate (FNMR) was evaluated at fixed False Match Rate (FMR) thresholds [26], specifically at 1e 3 (recommended for border control by Frontex [17]) and 1e 4 (for higher security applications). Additionally, we reported the Area Under the Curve (AUC) and partial AUC (pAUC) of the EDC curves to quantify verification performance across rejection rates. The pAUC [4, 5, 46] measures performance up to 25% rejection rate. To thoroughly examine the impact of our FIQA approaches across different FR architectures, we evaluated performance using four SOTA CNN-based models: ArcFace [14], ElasticFace [9], MagFace [36], and CurricularFace [25]. All evaluations were conducted under cross-model settings, where the models used to evaluate FIQA were different from those used to extract face feature representations. 5. Results We conduct comprehensive ablation studies  (Table 2)  to analyze the impact of various design choices, followed by comparison with SOTA methods (Table 3 and Figure 3). 5.1. Ablation Studies Dataset Study. We evaluate ViTNT-FIQA across different pre-trained models with comparable ViT-B and trained on varying datasets. ViT-B/16 architectures and ViT- [28] The ViT-B/WebFace4M/AdaFace B/WebFace12M/AdaFace [28] models, both trained specifically for FR tasks, achieve nearly identical performance (mean pAUC-EDC of 0.0279/0.0351 vs 0.0280/0.0368 at FMR=1e 3/1e 4), demonstrating that ViTNT-FIQA generalizes well on small and large datasets. Notably, CLIP ViT-B/16 [44], foundation model not trained for FR at all, yields worse results (0.0363/0.0456), showing that cross-block patch embedding stability correlates, to some degree, with face quality even in models without FR-specific training. FRoundation ViT-B/16 [12], which adapts CLIP for FR through LoRA layers while retaining Table 2. Ablation studies analyzing four design choices: dataset generalization (WebFace4M, WebFace12M, CLIP, FRoundation), architecture depth (ViT-S vs ViT-B), block depth trade-offs (4-24 blocks), and aggregation strategies (uniform vs attention-weighted). Results show optimal performance at 12-20 blocks with last-block attention weighting. Mean pAUC-EDC computed across seven benchmarks at FMR=1e 3 and 1e 4. Best per study in bold. CPLFW [54] XQLFW [30] Mean pAUC-EDC 1e3 1e3 1e4 1e4 Study Method Blocks Dataset Architecture Block Depth Attention-Weighting ViT-B - WebFace4M ViT-B - WebFace12M CLIP FRoundation ViT-S ViT-B ViT-B @ 4 ViT-B @ 8 ViT-B @ 12 ViT-B @ 16 ViT-B @ 20 ViT-B @ 24 Last Block Attention @ 4 Last Block Attention @ 8 Last Block Attention @ 12 Last Block Attention @ 16 Last Block Attention @ 20 Last Block Attention @ 24 Attention (All Blocks) @ 0-23 0-23 0-11 0-11 0-11 0-23 0-3 0-7 0-11 0-15 0-19 0-23 0-3 0-7 0-11 0-15 0-19 0-23 0-23 1e4 1e3 1e4 1e3 1e LFW [24] CALFW [55] 1e4 1e3 Adience [16] AgeDB-30 [39] CFP-FP [47] 1e3 1e3 1e4 1e4 0.0102 0.0230 0.0085 0.0126 0.0065 0.0095 0.0008 0.0009 0.0196 0.0215 0.0233 0.0357 0.1267 0.1426 0.0279 0.0100 0.0234 0.0079 0.0129 0.0063 0.0101 0.0006 0.0008 0.0209 0.0230 0.0228 0.0359 0.1275 0.1514 0.0280 0.0156 0.0349 0.0092 0.0141 0.0098 0.0139 0.0008 0.0009 0.0200 0.0224 0.0465 0.0633 0.1522 0.1697 0.0363 0.0154 0.0356 0.0099 0.0150 0.0093 0.0130 0.0007 0.0009 0.0205 0.0230 0.0410 0.0640 0.1525 0.1699 0.0356 0.0104 0.0235 0.0079 0.0123 0.0055 0.0085 0.0008 0.0009 0.0189 0.0210 0.0225 0.0363 0.1254 0.1490 0.0273 0.0102 0.0230 0.0085 0.0126 0.0065 0.0095 0.0008 0.0009 0.0196 0.0215 0.0233 0.0357 0.1267 0.1426 0.0279 0.0141 0.0319 0.0092 0.0141 0.0065 0.0097 0.0008 0.0010 0.0187 0.0207 0.0323 0.0430 0.1263 0.1452 0.0297 0.0117 0.0276 0.0089 0.0136 0.0043 0.0069 0.0007 0.0009 0.0189 0.0207 0.0210 0.0335 0.1238 0.1408 0.0270 0.0108 0.0263 0.0086 0.0131 0.0040 0.0067 0.0007 0.0009 0.0185 0.0204 0.0202 0.0326 0.1210 0.1366 0.0263 0.0102 0.0249 0.0085 0.0128 0.0045 0.0074 0.0008 0.0009 0.0185 0.0204 0.0201 0.0324 0.1209 0.1366 0.0262 0.0096 0.0226 0.0084 0.0126 0.0050 0.0085 0.0008 0.0009 0.0189 0.0208 0.0207 0.0329 0.1229 0.1381 0.0266 0.0102 0.0230 0.0085 0.0126 0.0065 0.0095 0.0008 0.0009 0.0196 0.0215 0.0233 0.0357 0.1267 0.1426 0.0279 0.0140 0.0317 0.0091 0.0140 0.0068 0.0098 0.0008 0.0010 0.0188 0.0209 0.0309 0.0416 0.1263 0.1456 0.0295 0.0114 0.0269 0.0088 0.0134 0.0043 0.0074 0.0008 0.0010 0.0191 0.0209 0.0207 0.0332 0.1219 0.1382 0.0267 0.0106 0.0260 0.0083 0.0128 0.0039 0.0068 0.0008 0.0010 0.0191 0.0209 0.0198 0.0321 0.1198 0.1339 0.0260 0.0102 0.0252 0.0083 0.0125 0.0042 0.0070 0.0007 0.0009 0.0191 0.0208 0.0197 0.0321 0.1209 0.1352 0.0262 0.0095 0.0226 0.0081 0.0122 0.0043 0.0069 0.0008 0.0009 0.0189 0.0207 0.0200 0.0324 0.1216 0.1381 0.0262 0.0103 0.0226 0.0082 0.0126 0.0066 0.0091 0.0008 0.0009 0.0198 0.0219 0.0227 0.0356 0.1263 0.1398 0.0278 0.0101 0.0227 0.0081 0.0122 0.0058 0.0087 0.0008 0.0009 0.0197 0.0216 0.0219 0.0343 0.1246 0.1390 0.0273 1e4 0.0351 0.0368 0.0456 0.0459 0.0359 0.0351 0.0379 0.0349 0.0338 0.0336 0.0338 0.0351 0.0378 0.0344 0.0334 0.0334 0.0334 0.0346 0.0342 it highly versatile for deployment across different model families and training paradigms. However, we observe that ViTNT-FIQA performs better with FR-specific-trained models, as FIQA is highly coupled with the FR task [10]. Architecture Study: Comparing ViT-S (12 blocks) and ViT-B (24 blocks) trained on the same WebFace4M dataset reveals minimal performance differences (0.0273/0.0359 vs 0.0279/0.0351). While ViT-S shows marginal advantages on certain datasets (e.g., CFP-FP, CALFW), ViT-B performs better on others (e.g., Adience, XQLFW). This indicates that ViTNT-FIQA effectively captures quality-relevant information regardless of network depth, as both architectures exhibit the smooth feature refinement trajectory that our method exploits. Block Depth Study: We systematically vary the number of blocks used (4, 8, 12, 16, 20, 24) for ViTB/WebFace4M/AdaFace [28] to understand the trade-off between computational efficiency and performance. Using only 4 blocks (0-3) yields the highest computational savings but weaker performance (0.0297/0.0379), particularly on CPLFW. Performance steadily improves as more blocks are included, with optimal results achieved at 16 blocks (0.0262/0.0336) and 20 blocks (0.0266/0.0338). Interestingly, using all 24 blocks (0.0279/0.0351) slightly degrades performance compared to 16-20 blocks. This finding indicates that practitioners can achieve near-optimal performance using only blocks 0-15, reducing computational costs. The sweet spot at 12-20 blocks balances efficiency and effectiveness when the uniform aggregation strategy is used, making it practical for resource-constrained deployments while maintaining competitive quality assessment. Attention-Weighting Study: We compare uniform patch aggregation (Equation 9) with attention-weighted aggregation (Equation 11) using weights from either the last block or averaged across all blocks. The uniform baseline (using blocks 0-23) achieves 0.0279/0.0351. AttentionFigure 3. Error-versus-Discard Characteristic (EDC) curves for FNMR@FMR=1e 3 of our proposed method in comparison to SOTA. Results shown on eight benchmark datasets: LFW [24], AgeDB-30 [39], CFP-FP [47], CALFW [55], Adience [16], CPLFW [54], XQLFW [30], and IJB-C [35], using ArcFace [14], ElasticFace [9], MagFace [36], and CurricularFace [25] FR models. Our method ViTNT-FIQA is marked with the red line. achieves multi-task capabilities, similar performance (0.0356/0.0459) to CLIP. This demonstrates that ViTNTFIQA is immediately applicable to any pre-trained ViT model without requiring FR-specific fine-tuning, making Table 3. The pAUCs of EDC achieved by our method and the SOTA methods under different experimental settings. The notions of 1e 3 and 1e 4 indicate the value of the fixed FMR at which the EDC curves (FNMR vs. reject) were calculated. The results are compared to three IQA and twelve FIQA approaches. The XQLFW dataset uses SER-FIQ (marked with *) as the FIQ labeling method. FR Method Adience [16] AgeDB-30 [39] CFP-FP [47] LFW [24] CALFW [55] CPLFW [54] XQLFW [30] IJB-C [35] I I I F I I I F ] 4 1 [ F ] 9 [ F s E ] 6 3 [ F ] 5 2 [ F u r BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQNet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) 1e3 0.0143 0.0124 0.0145 0.0125 0.0096 0.0102 0.0130 0.0099 0.0104 0.0097 0.0098 0.0091 0.0093 0.0096 0.0089 0.0095 0.0160 0.0138 0.0162 0.0139 0.0106 0.0114 0.0143 0.0110 0.0115 0.0105 0.0110 0.0100 0.0101 0.0104 0.0101 0.0107 0.0148 0.0128 0.0149 0.0125 0.0098 0.0107 0.0133 0.0100 0.0106 0.0100 0.0100 0.0094 0.0097 0.0099 0.0091 0.0097 0.0126 0.0109 0.0127 0.0107 0.0084 0.0091 0.0116 0.0089 0.0091 0.0089 0.0085 0.0079 0.0085 0.0085 0.0079 0.0084 1e4 0.0333 0.0302 0.0337 0.0304 0.0242 0.0244 0.0303 0.0247 0.0259 0.0201 0.0252 0.0229 0.0215 0.0244 0.0231 0.0226 0.0302 0.0274 0.0308 0.0276 0.0211 0.0227 0.0274 0.0211 0.0231 0.0206 0.0221 0.0207 0.0203 0.0218 0.0203 0.0209 0.0334 0.0291 0.0335 0.0302 0.0239 0.0241 0.0292 0.0233 0.0257 0.0210 0.0244 0.0230 0.0217 0.0242 0.0229 0.0225 0.0283 0.0254 0.0284 0.0247 0.0197 0.0207 0.0254 0.0198 0.0212 0.0189 0.0201 0.0183 0.0186 0.0194 0.0188 0.0191 1e3 0.0096 0.0087 0.0093 0.0090 0.0071 0.0066 0.0076 0.0065 0.0073 0.0066 0.0080 0.0059 0.0067 0.0064 0.0069 0.0081 0.0090 0.0085 0.0088 0.0089 0.0064 0.0064 0.0075 0.0060 0.0074 0.0064 0.0073 0.0056 0.0066 0.0061 0.0064 0.0077 0.0101 0.0092 0.0100 0.0100 0.0074 0.0074 0.0082 0.0066 0.0081 0.0071 0.0084 0.0065 0.0070 0.0070 0.0073 0.0084 0.0100 0.0091 0.0097 0.0096 0.0072 0.0067 0.0082 0.0066 0.0080 0.0066 0.0084 0.0065 0.0065 0.0070 0.0071 0. 1e4 0.0146 0.0141 0.0140 0.0143 0.0109 0.0107 0.0113 0.0098 0.0088 0.0089 0.0117 0.0078 0.0099 0.0083 0.0093 0.0122 0.0099 0.0096 0.0097 0.0097 0.0069 0.0072 0.0082 0.0064 0.0080 0.0069 0.0079 0.0060 0.0073 0.0065 0.0068 0.0084 0.0207 0.0212 0.0204 0.0199 0.0161 0.0160 0.0159 0.0134 0.0122 0.0128 0.0170 0.0111 0.0136 0.0119 0.0134 0.0181 0.0124 0.0122 0.0117 0.0118 0.0089 0.0083 0.0101 0.0082 0.0098 0.0083 0.0100 0.0077 0.0080 0.0083 0.0089 0.0104 1e3 0.0095 0.0088 0.0088 0.0071 0.0053 0.0035 0.0077 0.0045 0.0068 0.0035 0.0036 0.0034 0.0040 0.0038 0.0033 0.0043 0.0082 0.0082 0.0074 0.0067 0.0049 0.0031 0.0071 0.0043 0.0054 0.0031 0.0032 0.0029 0.0034 0.0032 0.0030 0.0038 0.0117 0.0119 0.0111 0.0096 0.0066 0.0045 0.0096 0.0057 0.0083 0.0048 0.0050 0.0045 0.0049 0.0049 0.0046 0.006 0.0093 0.0095 0.0087 0.0078 0.0058 0.0035 0.0074 0.0048 0.0073 0.0038 0.0035 0.0034 0.0037 0.0036 0.0037 0.0049 1e4 0.0136 0.0135 0.0119 0.0114 0.0082 0.0057 0.0100 0.0068 0.0109 0.0058 0.0062 0.0057 0.0064 0.0062 0.0054 0.0069 0.0107 0.0105 0.0100 0.0085 0.0065 0.0044 0.0084 0.0059 0.0067 0.0045 0.0046 0.0043 0.0047 0.0047 0.0045 0.0054 0.0205 0.0207 0.0199 0.0178 0.0092 0.0099 0.0162 0.0096 0.0128 0.0061 0.0108 0.0104 0.0111 0.0107 0.0071 0.0088 0.0107 0.0127 0.0117 0.0107 0.0079 0.0053 0.0099 0.0068 0.0097 0.0056 0.0055 0.0053 0.0056 0.0056 0.0054 0.0070 1e3 0.0009 0.0009 0.0009 0.0006 0.0007 0.0007 0.0008 0.0006 0.0007 0.0007 0.0007 0.0007 0.0007 0.0007 0.0006 0.0008 0.0007 0.0007 0.0008 0.0005 0.0006 0.0006 0.0007 0.0005 0.0006 0.0006 0.0006 0.0006 0.0006 0.0006 0.0005 0.0007 0.0009 0.0009 0.0009 0.0007 0.0007 0.0007 0.0008 0.0006 0.0007 0.0007 0.0007 0.0007 0.0008 0.0007 0.0006 0.0008 0.0009 0.0009 0.0009 0.0006 0.0007 0.0007 0.0008 0.0006 0.0007 0.0007 0.0007 0.0007 0.0007 0.0007 0.0006 0.0008 1e4 0.0010 0.0010 0.0010 0.0008 0.0008 0.0008 0.0009 0.0007 0.0008 0.0009 0.0008 0.0008 0.0009 0.0008 0.0007 0.0009 0.0010 0.0010 0.0010 0.0008 0.0008 0.0008 0.0009 0.0007 0.0008 0.0009 0.0007 0.0007 0.0009 0.0007 0.0007 0.0009 0.0013 0.0013 0.0013 0.0010 0.0008 0.0011 0.0010 0.0008 0.0008 0.0008 0.0008 0.0009 0.0011 0.0009 0.0008 0.0009 0.0010 0.0010 0.0010 0.0008 0.0008 0.0008 0.0009 0.0007 0.0008 0.0009 0.0008 0.0008 0.0009 0.0008 0.0007 0.0009 1e3 0.0200 0.0209 0.0207 0.0191 0.0187 0.0187 0.0196 0.0177 0.0188 0.0177 0.0184 0.0177 0.0181 0.0178 0.0184 0.0189 0.0195 0.0203 0.0201 0.0182 0.0181 0.0177 0.0189 0.0173 0.0181 0.0171 0.0177 0.0171 0.0176 0.0170 0.0175 0.0181 0.0199 0.0208 0.0206 0.0188 0.0186 0.0183 0.0193 0.0178 0.0186 0.0177 0.0183 0.0178 0.0179 0.0177 0.0182 0.0187 0.0196 0.0202 0.0201 0.0182 0.0183 0.0179 0.0191 0.0175 0.0183 0.0175 0.0177 0.0174 0.0176 0.0172 0.0181 0. 1e4 0.0225 0.0234 0.0230 0.0215 0.0206 0.0205 0.0216 0.0193 0.0205 0.0186 0.0206 0.0198 0.0202 0.0198 0.0200 0.0207 0.0203 0.0209 0.0208 0.0188 0.0186 0.0184 0.0196 0.0177 0.0186 0.0175 0.0182 0.0176 0.0181 0.0175 0.0182 0.0188 0.0211 0.0217 0.0215 0.0198 0.0192 0.0187 0.0198 0.0184 0.0194 0.0183 0.0191 0.0182 0.0185 0.0181 0.0188 0.0190 0.0211 0.0219 0.021 0.0200 0.0195 0.0192 0.0204 0.0185 0.0197 0.0181 0.0192 0.0186 0.0190 0.0185 0.0192 0.0197 1e3 0.0501 0.0506 0.0504 0.0306 0.0248 0.0199 0.0428 0.0249 0.0279 0.0190 0.0186 0.0187 0.0208 0.0190 0.0191 0.0200 0.0427 0.0433 0.0431 0.0291 0.0219 0.0185 0.0371 0.0237 0.0255 0.0178 0.0173 0.0174 0.0194 0.0178 0.0180 0.0188 0.0700 0.0518 0.0710 0.0336 0.0253 0.0219 0.0602 0.0268 0.0284 0.0209 0.0208 0.0208 0.0230 0.0212 0.0208 0.0218 0.0409 0.0411 0.0412 0.0275 0.0208 0.0169 0.0355 0.0219 0.0237 0.0161 0.0157 0.0157 0.0180 0.0160 0.0160 0.0169 1e4 0.0638 0.0658 0.0653 0.0427 0.0398 0.0319 0.0554 0.0360 0.0377 0.0307 0.0308 0.0308 0.0346 0.0310 0.0309 0.0324 0.1055 0.1086 0.1082 0.0394 0.0682 0.0292 0.0951 0.0345 0.0377 0.0275 0.0270 0.0271 0.0415 0.0275 0.0275 0.0288 0.1672 0.1695 0.1691 0.1133 0.1178 0.0541 0.1589 0.0579 0.0834 0.0454 0.0598 0.0597 0.0693 0.0600 0.0454 0.0465 0.1172 0.1198 0.1198 0.0402 0.0772 0.0308 0.1066 0.0357 0.0413 0.0283 0.0279 0.0279 0.0453 0.0284 0.0281 0.0292 1e3 0.1512 0.1532 0.1487 0.1270 0.1247 0.1175* 0.1523 0.1359 0.1356 0.1213 0.1204 0.1233 0.1262 0.1196 0.1224 0.1216 0.1412 0.1428 0.1379 0.1163 0.1180 0.1057* 0.1428 0.1331 0.1336 0.1094 0.1138 0.1195 0.1270 0.1134 0.1201 0.1203 0.1601 0.1619 0.1576 0.1392 0.1386 0.1264* 0.1584 0.1496 0.1525 0.1296 0.1308 0.1395 0.1414 0.1303 0.1316 0.1318 0.1346 0.1361 0.1297 0.1129 0.1048 0.1054* 0.1322 0.1257 0.1219 0.1043 0.1105 0.1134 0.1097 0.1075 0.1096 0.1119 1e4 0.1689 0.1709 0.1668 0.1500 0.1523 0.1385* 0.1686 0.1614 0.1525 0.1378 0.1393 0.1455 0.1389 0.1316 0.1364 0.1381 0.1638 0.1661 0.1621 0.1342 0.1401 0.1283* 0.1645 0.1445 0.1564 0.1265 0.1303 0.1394 0.1528 0.1402 0.1503 0.1450 0.1727 0.1744 0.1706 0.1514 0.1558 0.1440* 0.1681 0.1603 0.1656 0.1506 0.1502 0.1498 0.1576 0.1534 0.1500 0.1501 0.1444 0.1461 0.1416 0.1292 0.1215 0.1217* 0.1459 0.1373 0.1372 0.1279 0.1234 0.1268 0.1225 0.1240 0.1258 0.1258 1e3 0.0072 0.0071 0.0072 0.0066 0.0063 0.0056 0.0071 0.0061 0.0061 0.0057 0.0056 0.0056 0.0059 0.0057 0.0056 0.0058 0.0069 0.0068 0.0070 0.0065 0.0060 0.0054 0.0068 0.0058 0.0059 0.0055 0.0054 0.0054 0.0056 0.0055 0.0054 0.0055 0.0084 0.0083 0.0085 0.0077 0.0072 0.0066 0.0080 0.0070 0.0071 0.0067 0.0065 0.0065 0.0068 0.0066 0.0066 0.0066 0.0068 0.0067 0.0069 0.0064 0.0060 0.0054 0.0067 0.0057 0.0059 0.0054 0.0053 0.0053 0.0056 0.0054 0.0054 0. 1e4 0.0113 0.0112 0.0114 0.0102 0.0094 0.0087 0.0106 0.0092 0.0094 0.0087 0.0085 0.0085 0.0089 0.0086 0.0087 0.0087 0.0108 0.0106 0.0108 0.0100 0.0091 0.0083 0.0103 0.0089 0.0090 0.0084 0.0083 0.0082 0.0086 0.0083 0.0084 0.0085 0.0131 0.0130 0.0131 0.0117 0.0106 0.0097 0.0120 0.0104 0.0106 0.0098 0.0096 0.0096 0.0101 0.0097 0.0098 0.0099 0.0102 0.0100 0.0102 0.0094 0.0087 0.0079 0.0097 0.0085 0.0085 0.0079 0.0079 0.0078 0.0081 0.0079 0.0080 0.0081 weighting from the last block consistently improves performance across different depth configurations, with the best result at 12 blocks (0.0260/0.0334), demonstrating that not all patches contribute equally to quality assessment. Averaging attention weights across all blocks (0.0273/0.0342) performs comparably to last-block weighting, suggesting that the spatial importance patterns remain relatively stable throughout the network. This validates our design choice to leverage self-attention for patch importance weighting, the attention mechanism naturally identifies quality-critical improvement from attentionregions. weighting indicates that ViTs learned attention patterns The consistent align well with quality-relevant regions, providing principled way to aggregate patch-level quality scores without additional supervision. 5.2. Comparison with State-of-the-Art Table 3 presents comprehensive comparisons with SOTA across four FR models (ArcFace [14], ElasticFace [9], MagFace [36], CurricularFace [25]) and eight benchmark datasets. Our ViTNT-FIQA demonstrates competitive performance with SOTA FIQA methods while offering distinct advantages in applicability. Training-Free Methods: Compared to training-free methods  (Table 1)  , ViTNT-FIQA achieves performance on par with or better than SER-FIQ [50] and GraFIQs [31] across multiple datasets and FR models. Notably, SER-FIQ requires 100 forward passes with stochastic dropout to measure embedding stability, while GraFIQs requires backpropagation to compute gradient magniIn contrast, ViTNT-FIQA achieves comparable tudes. or superior results using only single forward pass without backpropagation. On the challenging Adience dataset, ViTNT-FIQA achieves 0.0095/0.0226 (ArcFace), 0.0107/0.0209 (ElasticFace), 0.0097/0.0225 (MagFace), and 0.0084/0.0191 (CurricularFace) at FMR=1e 3/1e 4, outperforming SER-FIQ (0.0102/0.0244, consistently 0.0114/0.0227, and 0.0101/0.0203, matching GraFIQs(L) 0.0097/0.0217, 0.0085/0.0186). On IJB-C, ViTNT-FIQA demonstrates (0.0058/0.0087 for ArcFace, 0.0055/0.0085 for ElasticFace) compared to SER-FIQ (0.0056/0.0087, 0.0054/0.0083) and GraFIQs(L) (0.0059/0.0089, 0.0056/0.0086). robust performance (0.0093/0.0215, 0.0091/0.0207) 0.0107/0.0241, FR-Integrated Methods: Among FR-integrated methods that require additional training, ViTNT-FIQA remains competitive with top-performing approaches across diverse evaluation scenarios. On Adience, our method consistently matches or approaches the performance of CRFIQA(L) [10] (0.0095 vs 0.0097 for ArcFace, 0.0084 vs 0.0089 for CurricularFace at FMR=1e 3) and ViTFIQA(T) [2] (0.0095 vs 0.0089 for ArcFace, 0.0084 vs 0.0079 for CurricularFace), despite not requiring any custom loss functions or quality-specific training. On CPLFW, ViTNT-FIQA achieves 0.0200/0.0324 (ArcFace) and 0.0169/0.0292 (CurricularFace), performing comparably to CR-FIQA(L) (0.0190/0.0307, 0.0161/0.0283) and ViT-FIQA(T) (0.0191/0.0309, 0.0160/0.0281). Compared to diffusion-based methods DifFIQA [4] and eDifFIQA [5], which leverage generative models and incur high computational costs, ViTNT-FIQA provides similar or better performance across multiple datasets. For instance, on AgeDB-30 with MagFace, ViTNT-FIQA achieves 0.0084/0.0181 versus DifFIQAs 0.0084/0.0170 and eDifFIQAs 0.0065/0.0111, while on XQLFW with ElasticFace, ViTNT-FIQA achieves 0.1203/0.1450 compared to DifFIQAs 0.1138/0.1303 and eDifFIQAs 0.1195/0.1394, demonstrating competitive performance without requiring generative model training. EDC: Figure 3 visualizes EDC curves at FMR=1e 3 across all datasets and FR models. Our method (red line) consistently tracks SOTA approaches across rejection rates, particularly on challenging dataset like Adience. The curves demonstrate that ViTNT-FIQA effectively identifies low-quality samples, as more low-quality images are discarded, verification errors decrease steadily. 6. Conclusion We introduced ViTNT-FIQA, training-free FIQA method that measures the stability of patch embedding evolution across intermediate ViT blocks to assess its utility of face image for FR. Our approach is grounded in the hypothesis that high-quality face images exhibit stable feature refinement trajectories across transformer blocks, while low quality images show erratic changes. By measuring L2 distances between normalized patch embeddings from consecutive blocks and aggregating them using attention-weighted schemes, ViTNT-FIQA produces quality scores in single forward pass without requiring backpropagation, architectural modifications, or quality-specific training. Comprehensive evaluations across eight benchmarks and four FR models demonstrate that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while offering distinct practical advantages. Our ablation studies reveal that: (1) the method generalizes across pretrained models regardless of training data or even task specialization, (2) architecture depth has minimal impact on performance, (3) using subset of encoder blocks provides optimal efficiency-performance trade-offs with computational savings when the uniform aggregation is used, and (4) attention-based patch weighting consistently improves quality assessment. The key contribution of ViTNTFIQA lies in demonstrating that intermediate ViT representations contain quality-relevant information beyond serving as stepping stones to final embeddings. By exploiting the smooth feature refinement trajectory inherent to transformer architectures, our method provides principled, efficient, and immediately applicable solution for face image quality assessment in modern recognition systems."
        },
        {
            "title": "Acknowledgment",
            "content": "This research work has been funded by the German Federal Ministry of Education and Research and the Hessen State Ministry for Higher Education, Research and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE."
        },
        {
            "title": "References",
            "content": "[1] ISO/IEC JTC 1/SC 37 Biometrics. ISO/IEC 29794-1 Information technology Biometric sample quality Part 1: Framework. International Organization for Standardization, 2024. 1 [2] Andrea Atzori, Fadi Boutros, and Naser Damer. Vit-fiqa: Assessing face image quality using vision transformers. In 2025 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2025. 1, 2, 3, 7, 8, 14 [4] [3] Ziga Babnik, Peter Peer, and Vitomir Struc. Faceqan: Face image quality assessment through adversarial noise exploIn 2022 26th International Conference on Pattern ration. Recognition (ICPR), pages 748754, 2022. 1, 3 ˇZiga Babnik, Peter Peer, and Vitomir ˇStruc. Diffiqa: Face image quality assessment using denoising diffusion probabilistic models. In 2023 IEEE International Joint Conference on Biometrics (IJCB), pages 110, 2023. 2, 3, 5, 7, 8, 14 ˇZiga Babnik, Peter Peer, and Vitomir ˇStruc. eDifFIQA: Towards Efficient Face Image Quality Assessment based on IEEE TransDenoising Diffusion Probabilistic Models. actions on Biometrics, Behavior, and Identity Science (TBIOM), 2024. 2, 3, 5, 7, 8, 14 [5] [6] Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis. Multi-exit vision transformer for dynamic inference. In BMVC, page 81. BMVA Press, 2021. [7] Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis. Single-layer vision transformers for more accurate early exits with less overhead. Neural Networks, 153:461473, 2022. 3 [8] Sebastian Bosse, Dominique Maniry, Klaus-Robert Muller, Thomas Wiegand, and Wojciech Samek. Deep neural networks for no-reference and full-reference image quality asIEEE Trans. Image Process., 27(1):206219, sessment. 2018. 7, 14 [9] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans, LA, USA, June 19-20, 2022, pages 15771586. IEEE, 2022. 5, 6, 7, 8, 14, 19, 20 [10] Fadi Boutros, Meiling Fang, Marcel Klemt, Biying Fu, and Naser Damer. CR-FIQA: face image quality assessment by learning sample relative classifiability. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 58365845. IEEE, 2023. 1, 2, 3, 6, 7, 8, 14 [11] Jiansheng Chen, Yu Deng, Gaocheng Bai, and Guangda Su. Face image quality assessment based on learning to rank. IEEE Signal Process. Lett., 22(1):9094, 2015. 3, 7, 14 [12] Tahar Chettaoui, Naser Damer, and Fadi Boutros. Froundation: Are foundation models ready for face recognition? Image Vis. Comput., 156:105453, 2025. 2, 5 [13] Jun Dan, Yang Liu, Haoyu Xie, Jiankang Deng, Haoran Xie, Xuansong Xie, and Baigui Sun. Transface: Calibrating transformer training for face recognition from data-centric perspective. In ICCV, pages 2058520596, 2023. [14] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In IEEE Conference on Computer Viface recognition. sion and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 46904699. Computer Vision Foundation / IEEE, 2019. 5, 6, 7, 8, 14, 19, 20 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 2, 4 [16] Eran Eidinger, Roee Enbar, and Tal Hassner. Age and gender IEEE Trans. Inf. Forensics estimation of unfiltered faces. Secur., 9(12):21702179, 2014. 2, 5, 6, 7, 13, 14, 19, 20 [17] Frontex. Best practice technical guidelines for automated border control (abc) systems, 2015. 5 [18] Biying Fu, Cong Chen, Olaf Henniger, and Naser Damer. deep insight into measuring face image utility with general and face-specific image quality metrics. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pages 1121 1130. IEEE, 2022. [19] P. Grother and E. Tabassi. Performance of biometric quality IEEE Trans. on Pattern Analysis and Machine measures. Intelligence, 29(4):531543, 2007. 5 [20] P. Grother, M. Ngan A. Hom, and K. Hanaoka. Ongoing face recognition vendor test (frvt) part 5: Face image quality assessment (4th draft). In National Institute of Standards and Technology. Tech. Rep., Sep. 2021. 1 [21] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):74367456, 2022. 3 [22] Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Rudolf Haraksim, and Laurent Beslay. Faceqnet: Quality assessment for face recognition based on deep learning. In 2019 International Conference on Biometrics, ICB 2019, Crete, Greece, June 4-7, 2019, pages 18. IEEE, 2019. 7, 14 [23] Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, and Laurent Beslay. Biometric quality: Review and application to face recognition with faceqnet. CoRR, abs/2006.03298, 2020. 2, 3, 7, 14 [24] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, 2007. 2, 5, 6, 7, 13, 14, 19, [25] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: Adaptive curriculum learning loss for deep face recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 59005909. Computer Vision Foundation / IEEE, 2020. 5, 6, 7, 8, 14, 19, 20 [26] ISO/IEC JTC1 SC37 Biometrics. ISO/IEC 19795-1:2021 Information technology Biometric performance testing and reporting Part 1: Principles and framework. International Organization for Standardization, 2021. 5 [27] Geunsu Kim, Gyudo Park, Soohyeok Kang, and Simon S. Woo. S-vit: Sparse vision transformer for accurate face recognition. In ACM SAC, pages 11301138, 2023. 2 [28] Minchul Kim, Anil K. Jain, and Xiaoming Liu. Adaface: In CVPR, Quality adaptive margin for face recognition. pages 1872918738. IEEE, 2022. 5, 6 [29] Minchul Kim, Yiyang Su, Feng Liu, Anil Jain, and Xiaoming Liu. Keypoint relative position encoding for face recognition. In CVPR, pages 244255. IEEE, 2024. 2 [30] Martin Knoche, Stefan Hormann, and Gerhard Rigoll. Cross-quality LFW: database for analyzing crossresolution image face recognition in unconstrained environments. In 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021, Jodhpur, India, December 15-18, 2021, pages 15. IEEE, 2021. 2, 5, 6, 7, 13, 14, 19, [31] Jan Niklas Kolf, Naser Damer, and Fadi Boutros. Grafiqs: Face image quality assessment using gradient magnitudes. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 14901499, 2024. 1, 2, 3, 7, 8, 14 [32] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. 2 [33] Xialei Liu, Joost van de Weijer, and Andrew D. Bagdanov. Rankiqa: Learning from rankings for no-reference image In IEEE International Conference on quality assessment. Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 10401049. IEEE Computer Society, 2017. 1, 7, 14 [34] Yoshitomo Matsubara, Marco Levorato, and Francesco Restuccia. Split computing and early exiting for deep learning applications: Survey and research challenges. ACM Comput. Surv., 55(5):90:190:30, 2023. 3 [35] Brianna Maze, Jocelyn C. Adams, James A. Duncan, Nathan D. Kalka, Tim Miller, Charles Otto, Anil K. Jain, W. Tyler Niggel, Janet Anderson, Jordan Cheney, and Patrick Grother. IARPA janus benchmark - C: face dataset and protocol. In 2018 International Conference on Biometrics, ICB 2018, Gold Coast, Australia, February 20-23, 2018, pages 158165. IEEE, 2018. 2, 5, 6, 7, 14, 19, 20 [36] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: universal representation for face recognition and quality assessment. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1422514234. Computer Vision Foundation / IEEE, 2021. 1, 2, 3, 5, 6, 7, 8, 14, 19, [37] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial IEEE Trans. Image Process., 21(12):46954708, domain. 2012. 1, 7, 14 [38] Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Making completely blind image quality analyzer. IEEE Signal Process. Lett., 20(3):209212, 2013. 1 [39] Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: The first manually collected, in-the-wild In 2017 IEEE CVPRW, CVPR Workshops age database. 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1997 2005. IEEE Computer Society, 2017. 2, 5, 6, 7, 13, 14, 19, 20 [40] Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, and Yuan-Gen Wang. SDD-FIQA: unsupervised face image quality assessment with similarity distribution distance. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 76707679. Computer Vision Foundation / IEEE, 2021. 2, 3, 7, 14 [41] Fu-Zhao Ou, Chongyi Li, Shiqi Wang, and Sam Kwong. Clib-fiqa: Face image quality assessment with confidence In Proceedings of the IEEE/CVF Conference calibration. on Computer Vision and Pattern Recognition (CVPR), pages 16941704, 2024. 2, 7, 14 [42] Fu-Zhao Ou, Chongyi Li, Shiqi Wang, and Sam Kwong. Mr-fiqa: Face image quality assessment with multi-reference representations from synthetic data generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1291512925, 2025. 1, 2, 5, 15 [43] Mary Phuong and Christoph Lampert. Distillation-based In 2019 IEEE/CVF training for multi-exit architectures. International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 13551364. IEEE, 2019. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 5 [45] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transIn Adformers see like convolutional neural networks? vances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1211612128, 2021. 1, 3, 4 [46] Torsten Schlett, Christian Rathgeb, Juan E. Tapia, and Christoph Busch. Considerations on the evaluation of bioIEEE Trans. Biom. metric quality assessment algorithms. Behav. Identity Sci., 6(1):5467, 2024. 5 [47] Soumyadip Sengupta, Jun-Cheng Chen, Carlos Domingo Castillo, Vishal M. Patel, Rama Chellappa, and David W. Jacobs. Frontal to profile face verification in the wild. In 2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016, pages 19. IEEE Computer Society, 2016. 2, 5, 6, 7, 13, 14, 19, 20 [48] Yichun Shi and Anil K. Jain. Probabilistic face embeddings. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 69016910. IEEE, 2019. 1, 2, 3, 7, 14 [49] Zhonglin Sun and Georgios Tzimiropoulos. Part-based face In 33rd British Marecognition with vision transformers. chine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022, page 611. BMVA Press, 2022. 2 [50] Philipp Terhorst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. SER-FIQ: unsupervised estimation of face image quality based on stochastic embedIn 2020 IEEE/CVF Conference on Comding robustness. puter Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 56505659. Computer Vision Foundation / IEEE, 2020. 1, 2, 3, 7, 8, [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. 2 [52] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Berxit: Early exiting for BERT with better fine-tuning and extenIn Proceedings of the 16th Conference sion to regression. of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 91104. Association for Computational Linguistics, 2021. 3 [53] Guanyu Xu, Jiawei Hao, Li Shen, Han Hu, Yong Luo, Hui Lin, and Jialie Shen. Lgvit: Dynamic early exiting for acIn Proceedings of the 31st celerating vision transformer. ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 20233 November 2023, pages 91039114. ACM, 2023. 3 [54] T. Zheng and W. Deng. Cross-pose lfw: database for studying cross-pose face recognition in unconstrained environments. Technical Report 18-01, Beijing University of Posts and Telecommunications, 2018. 2, 5, 6, 7, 13, 14, 19, 20 [55] Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age LFW: database for studying cross-age face recognition in unconstrained environments. CoRR, abs/1708.08197, 2017. 2, 5, 6, 7, 13, 14, 19, 20 [56] Yaoyao Zhong and Weihong Deng. Face transformer for recognition. arXiv preprint arXiv:2103.14803, 2021. 2 tion complements the SynFIQA figure in the main paper (which shows ViT-B results) by demonstrating that our core hypothesis generalizes across different architecture scales. The systematic decrease in cross-block distances with increasing ground-truth quality is evident in ViT-S (12 blocks) just as in ViT-B (24 blocks), confirming that patch embedding stability serves as quality indicator regardless of network depth. study column, three security operating points Figures 5, 6, 7: Comprehensive ablation analysis via Error-versus-Discard Characteristic (EDC) curves (FMR=1e 2, at trends provide inThe visual 1e 3, 1e 4). tuitive insights that complement the quantitative tables: blue/green In the Dataset curves (WebFace4M/WebFace12M) consistently lie below brown/pink curves (CLIP/FRoundation), visually confirming FR-specific training superiority. In the Architecture study, blue (ViT-B) and orange (ViT-S) curves run nearly parallel with minimal separation, demonstrating depth-independence. In the Block Depth study, we observe progressive downward shifts from red (4 blocks) to blue (24 blocks) up to 16 blocks, after which curves plateau or slightly rise, visually identifying the optimal 12-20 block sweet spot. In the Attention-Weighting study, we see similar trend to Block Depth study, but with slightly lower curves. In the Block Windows study, the clear downward progression of the EDC curve in the early blocks (blocks 0-5), not seeing similar downward progression for the others, visually confirms that quality discrimination concentrates in initial processing stages. Figures 8, 9: State-of-the-art comparison EDC curves at two additional FNMR@FMR thresholds (1e 2, 1e 4) beyond the main papers 1e 3 threshold. These comprehensive comparisons across eight benchmark datasets and four face recognition models demonstrate that our methods competitiveness holds across multiple security requirements. Figure 10: Distribution of quality scores across evaluation benchmarks, comparing ViTNT-FIQA with SOTA methods. The normalized score distributions (range [0, 1]) reveal whether methods produce well-calibrated distributions that effectively rank sample quality or if they suffer from range compression that limits discriminative power."
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material provides comprehensive experimental results and detailed analysis of the ViTNT-FIQA method for face image quality assessment. The supplementary material is structured to address five fundamental research questions: (1) How do cross-block patch embedding distances correlate with image quality across different network depths? (2) Which block configurations provide optimal performance-efficiency trade-offs? (3) How does our method compare against existing state-of-the-art approaches across multiple evaluation metrics? (4) What visual patterns emerge in the ablation study EDC curves? (5) How does quality score distribution vary across different FIQA methods? Tables: Quantitative Evidence Table 4: Block window analysis comparing consecutive 6-block segments across ViT-B architecture. This systematic evaluation identifies which transformer block windows (early: 0-5, middle: 6-17, late: 18-23) capture the most quality-discriminative information. We include this analysis to demonstrate that early transformer blocks (05) achieve superior performance across both AUC-EDC and pAUC-EDC metrics, providing empirical evidence that quality-relevant features emerge in initial processing stages rather than requiring full network depth. Table 5: Comprehensive ablation study presenting AUCEDC performance for all four design choices (Dataset, Architecture, Block Depth, Attention-Weighting) across eight benchmark datasets using ArcFace as the face recognition model. Lower AUC-EDC values indicate better quality assessment performance. This systematic evaluation complements the pAUC-EDC analysis of the main paper, providing complete picture of method performance across the full rejection rate spectrum (0-100%) rather than just the partial area up to 25% rejection. The AUC-EDC metric is essential to validate that our findings hold beyond the 25% rejection threshold. Table 6: State-of-the-art comparison presenting AUCEDC values for our method against 15 competing approaches (3 IQA, 12 FIQA) across four face recognition models (ArcFace, ElasticFace, MagFace, CurricularFace). We provide this extensive comparison table in addition to the pAUC-EDC comparison, provided in the main paper, to establish the competitive advantage of our training-free approach across different evaluation metrics and operating points. Figures: Visual Evidence and Insights Figure 4: Boxplots of mean L2 distances between consecutive ViT-S patch embeddings across 11 quality groups from 5.5M SynFIQA images. This visualizaTable 4. Block-window analysis comparing quality-assessment performance across consecutive 6-block segments of ViT-B/WebFace4M. Early window (blocks 05) yields the strongest AUC-EDC and pAUC-EDC performance, indicating that initial feature refinements carry the most quality-discriminative signal. Mean metrics reported across seven benchmarks at FMR=1e3 and 1e4. Best results per metric are in bold. Metric AUC-EDC pAUC-EDC 1e 1e3 1e3 1e4 1e3 1e3 1e 1e4 1e3 Mean Blocks Method LFW [24] 0-5 2-7 4-9 6-11 8-13 CALFW [55] 1e4 1e3 CPLFW [54] XQLFW [30] 1e4 1e3 Adience [16] AgeDB-30 [39] CFP-FP [47] 1e3 1e4 1e4 0.0358 0.0808 0.0375 0.0524 0.0088 0.0131 0.0024 0.0030 0.0660 0.0724 0.0578 0.0786 0.2241 0.2714 0.0618 0.0817 ViT-B @ 0-5 0.0749 0.1716 0.0355 0.0496 0.0293 0.0418 0.0045 0.0051 0.0835 0.0915 0.1434 0.1785 0.5502 0.6302 0.1316 0.1669 ViT-B @ 2-7 0.0655 0.1361 0.0295 0.0314 0.0335 0.0429 0.0023 0.0029 0.0670 0.0750 0.1287 0.1595 0.5536 0.6263 0.1257 0.1534 ViT-B @ 4-9 0.0573 0.1318 0.0354 0.0412 0.0397 0.0489 0.0042 0.0050 0.0799 0.0865 0.1241 0.1633 0.4911 0.5467 0.1188 0.1462 ViT-B @ 6-11 0.0480 0.1108 0.0309 0.0452 0.0261 0.0352 0.0046 0.0054 0.0695 0.0762 0.1569 0.1989 0.5570 0.6110 0.1276 0.1547 ViT-B @ 8-13 ViT-B @ 10-15 10-15 0.0461 0.1040 0.0370 0.0516 0.0250 0.0350 0.0040 0.0050 0.0678 0.0748 0.1780 0.2235 0.5225 0.5666 0.1258 0.1515 ViT-B @ 12-17 12-17 0.0683 0.1578 0.0342 0.0405 0.0360 0.0424 0.0028 0.0035 0.0843 0.0927 0.1790 0.2213 0.4644 0.5843 0.1241 0.1632 ViT-B @ 14-19 14-19 0.0654 0.1458 0.0335 0.0366 0.0397 0.0513 0.0036 0.0042 0.0773 0.0813 0.1306 0.1566 0.4717 0.5622 0.1174 0.1483 ViT-B @ 16-21 16-21 0.0574 0.1364 0.0294 0.0434 0.0264 0.0352 0.0044 0.0052 0.0802 0.0865 0.1694 0.2037 0.5539 0.6163 0.1316 0.1610 ViT-B @ 18-23 18-23 0.0640 0.1453 0.0258 0.0286 0.0363 0.0450 0.0038 0.0044 0.0702 0.0770 0.1229 0.1589 0.5070 0.5617 0.1186 0.1458 0.0127 0.0293 0.0090 0.0139 0.0048 0.0074 0.0007 0.0008 0.0187 0.0205 0.0240 0.0364 0.1256 0.1439 0.0279 0.0360 ViT-B @ 0-5 0.0155 0.0355 0.0084 0.0134 0.0091 0.0138 0.0009 0.0011 0.0200 0.0223 0.0390 0.0527 0.1484 0.1665 0.0345 0.0436 ViT-B @ 2-7 0.0151 0.0342 0.0083 0.0096 0.0090 0.0127 0.0009 0.0010 0.0196 0.0219 0.0376 0.0490 0.1451 0.1693 0.0337 0.0425 ViT-B @ 4-9 0.0145 0.0335 0.0088 0.0109 0.0092 0.0136 0.0009 0.0010 0.0204 0.0231 0.0353 0.0488 0.1489 0.1667 0.0340 0.0425 ViT-B @ 6-11 0.0147 0.0335 0.0082 0.0136 0.0083 0.0130 0.0009 0.0011 0.0196 0.0220 0.0430 0.0552 0.1493 0.1670 0.0349 0.0436 ViT-B @ 8-13 ViT-B @ 10-15 10-15 0.0140 0.0330 0.0095 0.0143 0.0079 0.0123 0.0010 0.0012 0.0197 0.0223 0.0466 0.0638 0.1495 0.1676 0.0355 0.0449 ViT-B @ 12-17 12-17 0.0154 0.0356 0.0086 0.0111 0.0095 0.0129 0.0007 0.0009 0.0205 0.0227 0.0491 0.0633 0.1435 0.1640 0.0353 0.0444 ViT-B @ 14-19 14-19 0.0146 0.0338 0.0084 0.0099 0.0098 0.0142 0.0008 0.0009 0.0200 0.0228 0.0363 0.0495 0.1383 0.1672 0.0326 0.0426 ViT-B @ 16-21 16-21 0.0152 0.0348 0.0089 0.0137 0.0085 0.0134 0.0009 0.0011 0.0199 0.0223 0.0436 0.0550 0.1485 0.1664 0.0351 0.0438 ViT-B @ 18-23 18-23 0.0150 0.0336 0.0085 0.0105 0.0090 0.0132 0.0008 0.0010 0.0196 0.0218 0.0371 0.0483 0.1408 0.1661 0.0330 0.0421 0-5 2-7 4-9 6-11 8-13 Table 5. Ablation studies analyzing four design choices: dataset generalization (WebFace4M, WebFace12M, CLIP, FRoundation), architecture depth (ViT-S vs ViT-B), block depth trade-offs (4-24 blocks), and aggregation strategies (uniform vs attention-weighted). Results show optimal performance at 12-20 blocks with last-block attention weighting. Mean AUC-EDC computed across seven benchmarks at FMR=1e 3 and 1e 4. Best per study in bold. Study Method Blocks Dataset Architecture Block Depth Attention-Weighting ViT-B - WebFace4M ViT-B - WebFace12M CLIP FRoundation ViT-S ViT-B ViT-B @ 4 ViT-B @ 8 ViT-B @ 12 ViT-B @ 16 ViT-B @ 20 ViT-B @ 24 Last Block Attention @ 4 Last Block Attention @ 8 Last Block Attention @ 12 Last Block Attention @ 16 Last Block Attention @ 20 Last Block Attention @ 24 Attention (All Blocks) @ 24 0-23 0-23 0-11 0-11 0-11 0-23 0-3 0-7 0-11 0-15 0-19 0-23 0-3 0-7 0-11 0-15 0-19 0-23 0-23 CPLFW [54] XQLFW [30] Mean AUC-EDC 1e3 1e3 1e 1e4 1e3 1e3 1e3 1e4 1e LFW [24] CALFW [55] 1e4 1e3 Adience [16] AgeDB-30 [39] CFP-FP [47] 1e3 1e3 1e4 1e4 0.0217 0.0406 0.0255 0.0362 0.0133 0.0181 0.0034 0.0039 0.0707 0.0745 0.0538 0.0759 0.2575 0.2978 0.0637 0.0217 0.0417 0.0250 0.0393 0.0113 0.0175 0.0034 0.0040 0.0907 0.0965 0.0521 0.0805 0.2994 0.3391 0.0719 0.0634 0.1368 0.0364 0.0508 0.0351 0.0491 0.0033 0.0040 0.0702 0.0772 0.2049 0.2542 0.6065 0.6645 0.1457 0.0605 0.1415 0.0412 0.0595 0.0286 0.0374 0.0036 0.0043 0.0796 0.0852 0.1917 0.2527 0.5428 0.5881 0.1354 0.0231 0.0441 0.0245 0.0337 0.0115 0.0164 0.0026 0.0032 0.0671 0.0718 0.0535 0.0768 0.2730 0.3107 0.0650 0.0217 0.0406 0.0255 0.0362 0.0133 0.0181 0.0034 0.0039 0.0707 0.0745 0.0538 0.0759 0.2575 0.2978 0.0637 0.0509 0.1109 0.0390 0.0533 0.0145 0.0200 0.0037 0.0043 0.0675 0.0734 0.0935 0.1160 0.2499 0.3023 0.0741 0.0304 0.0677 0.0340 0.0479 0.0072 0.0111 0.0020 0.0026 0.0622 0.0678 0.0492 0.0689 0.2129 0.2565 0.0568 0.0266 0.0577 0.0304 0.0429 0.0065 0.0102 0.0021 0.0026 0.0598 0.0640 0.0446 0.0640 0.2043 0.2423 0.0535 0.0240 0.0505 0.0272 0.0385 0.0069 0.0110 0.0020 0.0025 0.0603 0.0641 0.0442 0.0634 0.2071 0.2440 0.0531 0.0213 0.0412 0.0268 0.0377 0.0075 0.0124 0.0024 0.0029 0.0639 0.0670 0.0450 0.0638 0.2169 0.2527 0.0548 0.0217 0.0406 0.0255 0.0362 0.0133 0.0181 0.0034 0.0039 0.0707 0.0745 0.0538 0.0759 0.2575 0.2978 0.0637 0.0475 0.1046 0.0362 0.0513 0.0148 0.0203 0.0031 0.0038 0.0681 0.0737 0.0837 0.1073 0.2372 0.2885 0.0701 0.0286 0.0608 0.0335 0.0474 0.0072 0.0116 0.0023 0.0029 0.0626 0.0672 0.0474 0.0668 0.2072 0.2484 0.0555 0.0254 0.0539 0.0302 0.0419 0.0060 0.0099 0.0021 0.0027 0.0609 0.0651 0.0428 0.0617 0.1993 0.2363 0.0524 0.0240 0.0514 0.0263 0.0370 0.0064 0.0103 0.0021 0.0026 0.0605 0.0640 0.0424 0.0613 0.2054 0.2395 0.0524 0.0203 0.0392 0.0249 0.0349 0.0063 0.0101 0.0033 0.0038 0.0614 0.0650 0.0428 0.0619 0.2078 0.2473 0.0524 0.0228 0.0433 0.0282 0.0402 0.0135 0.0182 0.0039 0.0044 0.0668 0.0719 0.0507 0.0768 0.2619 0.3005 0.0640 0.0213 0.0397 0.0244 0.0346 0.0093 0.0137 0.0036 0.0041 0.0704 0.0741 0.0464 0.0693 0.2434 0.2808 0.0598 1e4 0.0781 0.0884 0.1767 0.1670 0.0795 0.0781 0.0972 0.0746 0.0691 0.0677 0.0682 0.0781 0.0928 0.0722 0.0674 0.0666 0.0660 0.0793 0.0738 Table 6. The AUCs of EDC achieved by our method and the SOTA methods under different experimental settings. The notions of 1e 3 and 1e 4 indicate the value of the fixed FMR at which the EDC curves (FNMR vs. reject) were calculated. The results are compared to three IQA and twelve FIQA approaches. The XQLFW dataset uses SER-FIQ (marked with *) as the FIQ labeling method. FR Method Adience [16] AgeDB-30 [39] CFP-FP [47] LFW [24] CALFW [55] CPLFW [54] XQLFW [30] IJB-C [35] A F I F A F I F ] 4 1 [ F ] 9 [ F s ] 6 3 [ F M ] 5 2 [ F u r BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(T)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQnet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(C)[2] ViTNT-FIQA (Ours) BRISQUE[37] RankIQA[33] DeepIQA[8] RankIQ[11] PFE[48] SER-FIQ[50] FaceQNet[22, 23] MagFace[36] SDD-FIQA[40] CR-FIQA(L) [10] DifFIQA(R) [4] eDifFIQA(L) [5] GRAFIQS (L) [31] CLIB-FIQA [41] ViT-FIQA(C)[2] ViTNT-FIQA (Ours) 1e3 0.0565 0.0400 0.0568 0.0353 0.0212 0.0223 0.0346 0.0207 0.0248 0.0204 0.0232 0.0208 0.0225 0.0217 0.0197 0.0203 0.0644 0.0433 0.0645 0.0400 0.0222 0.0240 0.0369 0.0225 0.0277 0.0214 0.0255 0.0219 0.0233 0.0229 0.0214 0.0218 0.0594 0.0407 0.0571 0.0359 0.0215 0.0233 0.0365 0.0212 0.0253 0.0211 0.0237 0.0215 0.0233 0.0225 0.0197 0.0206 0.0502 0.0359 0.0492 0.0314 0.0198 0.0211 0.0326 0.0200 0.0230 0.0198 0.0215 0.0197 0.0220 0.0207 0.0187 0.0192 1e4 0.1285 0.0933 0.1372 0.0873 0.0428 0.0434 0.0734 0.0425 0.0562 0.0353 0.0581 0.0402 0.0403 0.0429 0.0395 0.0392 0.1184 0.0862 0.1203 0.0777 0.0381 0.0417 0.0667 0.0385 0.0512 0.0357 0.0499 0.0373 0.0394 0.0401 0.0362 0.0379 0.1308 0.0889 0.1302 0.0837 0.0423 0.0451 0.0720 0.0417 0.0562 0.0372 0.0560 0.0412 0.0419 0.0442 0.0381 0.0398 0.1095 0.0752 0.1070 0.0715 0.0365 0.0381 0.0626 0.0364 0.0462 0.0336 0.0444 0.0337 0.0365 0.0357 0.0334 0.0348 1e3 0.0400 0.0372 0.0403 0.0322 0.0172 0.0167 0.0197 0.0156 0.0186 0.0159 0.0199 0.0147 0.0176 0.0151 0.0177 0.0249 0.0375 0.0374 0.0384 0.0309 0.0163 0.0163 0.0194 0.0150 0.0187 0.0149 0.0193 0.0143 0.0182 0.0152 0.0169 0.0234 0.0442 0.0370 0.0417 0.0361 0.0192 0.0185 0.0217 0.0159 0.0216 0.0174 0.0218 0.0169 0.0182 0.0172 0.0186 0.0259 0.0433 0.0394 0.0407 0.0365 0.0197 0.0167 0.0221 0.0167 0.0219 0.0162 0.0223 0.0170 0.0167 0.0162 0.0195 0.0262 1e4 0.0585 0.0523 0.0523 0.0420 0.0226 0.0223 0.0245 0.0198 0.0206 0.0189 0.0265 0.0174 0.0219 0.0178 0.0207 0.0349 0.0403 0.0436 0.0411 0.0337 0.0172 0.0179 0.0207 0.0158 0.0200 0.0159 0.0205 0.0152 0.0200 0.0159 0.0179 0.0249 0.0799 0.0681 0.0721 0.0531 0.0317 0.0293 0.0314 0.0247 0.0305 0.0235 0.0367 0.0243 0.0253 0.0255 0.0295 0.0512 0.0491 0.0510 0.0476 0.0417 0.0227 0.0193 0.0267 0.0195 0.0254 0.0200 0.0252 0.0193 0.0200 0.0191 0.0227 0. 1e3 0.0343 0.0301 0.0238 0.0152 0.0092 0.0065 0.0240 0.0073 0.0122 0.0050 0.0054 0.0045 0.0070 0.0053 0.0057 0.0063 0.0281 0.0269 0.0191 0.0149 0.0088 0.0061 0.0227 0.0069 0.0098 0.0045 0.0049 0.0040 0.0070 0.0045 0.0052 0.0068 0.0422 0.0369 0.0322 0.0213 0.0107 0.0080 0.0271 0.0085 0.0146 0.0062 0.0071 0.0058 0.0087 0.0068 0.0064 0.0092 0.0323 0.0298 0.0227 0.0186 0.0100 0.0074 0.0226 0.0078 0.0138 0.0054 0.0053 0.0044 0.0068 0.0049 0.0058 0.0078 1e4 0.0433 0.0384 0.0292 0.0260 0.0129 0.0103 0.0273 0.0105 0.0193 0.0082 0.0095 0.0078 0.0111 0.0088 0.0084 0.0101 0.0372 0.0318 0.0256 0.0180 0.0113 0.0085 0.0247 0.0095 0.0118 0.0065 0.0071 0.0061 0.0091 0.0069 0.0073 0.0092 0.0589 0.0543 0.0545 0.0332 0.0138 0.0139 0.0351 0.0129 0.0201 0.0080 0.0158 0.0126 0.0186 0.0138 0.0114 0.0127 0.0357 0.0356 0.0278 0.0249 0.0134 0.0111 0.0274 0.0111 0.0185 0.0080 0.0089 0.0077 0.0099 0.0083 0.0082 0.0114 1e3 0.0043 0.0039 0.0049 0.0018 0.0023 0.0023 0.0022 0.0016 0.0021 0.0023 0.0025 0.0018 0.0032 0.0016 0.0023 0.0033 0.0034 0.0033 0.0043 0.0013 0.0018 0.0021 0.0021 0.0014 0.0019 0.0018 0.0024 0.0017 0.0029 0.0014 0.0017 0.0030 0.0043 0.0041 0.0048 0.0019 0.0023 0.0025 0.0022 0.0017 0.0021 0.0023 0.0025 0.0018 0.0033 0.0016 0.0024 0.0033 0.0041 0.0039 0.0050 0.0018 0.0024 0.0025 0.0022 0.0016 0.0021 0.0023 0.0025 0.0018 0.0033 0.0016 0.0023 0.0035 1e4 0.0049 0.0045 0.0056 0.0024 0.0028 0.0028 0.0027 0.0021 0.0027 0.0029 0.0029 0.0022 0.0038 0.0020 0.0027 0.0038 0.0047 0.0045 0.0056 0.0020 0.0025 0.0028 0.0026 0.0021 0.0027 0.0025 0.0029 0.0022 0.0037 0.0019 0.0023 0.0037 0.0058 0.0056 0.0059 0.0027 0.0029 0.0033 0.0027 0.0022 0.0027 0.0028 0.0030 0.0023 0.0041 0.0021 0.0028 0.0038 0.0047 0.0045 0.0056 0.0024 0.0028 0.0030 0.0027 0.0021 0.0027 0.0029 0.0029 0.0022 0.0038 0.0020 0.0028 0.0040 1e3 0.0755 0.0846 0.0793 0.0608 0.0647 0.0595 0.0774 0.0568 0.0641 0.0616 0.0599 0.0573 0.0644 0.0569 0.0593 0.0614 0.0726 0.0810 0.0756 0.0598 0.0628 0.0574 0.0763 0.0553 0.0624 0.0594 0.0575 0.0558 0.0614 0.0562 0.0575 0.0590 0.0758 0.0829 0.0787 0.0602 0.0640 0.0590 0.0763 0.0562 0.0643 0.0614 0.0600 0.0574 0.0640 0.0572 0.0634 0.0608 0.0755 0.0806 0.0764 0.0590 0.0630 0.0587 0.0767 0.0563 0.0637 0.0605 0.0575 0.0568 0.0610 0.0574 0.0629 0.0599 1e4 0.0813 0.0915 0.0850 0.0672 0.0681 0.0627 0.0822 0.0602 0.0698 0.0632 0.065 0.0621 0.0692 0.0615 0.0627 0.0650 0.0747 0.0835 0.0772 0.0614 0.0643 0.0590 0.0777 0.0563 0.0638 0.0608 0.0593 0.0574 0.0632 0.0574 0.0592 0.0607 0.0788 0.0857 0.0809 0.0629 0.0652 0.0607 0.0773 0.0578 0.0657 0.0628 0.0622 0.0586 0.0652 0.0582 0.0648 0.0617 0.0784 0.0865 0.0786 0.0640 0.0657 0.0610 0.0799 0.0590 0.0675 0.0618 0.0615 0.0596 0.0641 0.0596 0.0650 0. 1e3 0.2558 0.2437 0.2309 0.0633 0.0450 0.0389 0.1504 0.0492 0.0517 0.0360 0.0356 0.0342 0.0415 0.0357 0.0366 0.0428 0.2641 0.2325 0.2401 0.0581 0.0419 0.0387 0.1420 0.0474 0.0493 0.0350 0.0323 0.0325 0.0393 0.0343 0.0354 0.0404 0.4649 0.3251 0.3672 0.0659 0.0449 0.0397 0.2988 0.0506 0.0525 0.0374 0.0362 0.0358 0.0428 0.0380 0.0375 0.0428 0.2709 0.2346 0.2488 0.0541 0.0402 0.0356 0.1384 0.0449 0.0465 0.0324 0.0300 0.0303 0.0369 0.0315 0.0326 0.0372 1e4 0.3037 0.2969 0.2856 0.0848 0.0638 0.0584 0.1751 0.0642 0.0670 0.0515 0.0522 0.0502 0.0612 0.0517 0.0519 0.0619 0.4688 0.4306 0.4541 0.0727 0.0895 0.0513 0.2880 0.0597 0.0634 0.0462 0.0438 0.0436 0.0633 0.0454 0.0461 0.0524 0.6809 0.6475 0.6632 0.1642 0.1435 0.0821 0.5218 0.0887 0.1188 0.0679 0.0838 0.0813 0.0987 0.0839 0.0684 0.0775 0.5057 0.4654 0.4961 0.0730 0.0983 0.0520 0.3229 0.0607 0.0671 0.0462 0.0436 0.0439 0.0663 0.0451 0.0458 0.0520 1e3 0.6680 0.6584 0.5958 0.2789 0.2302 0.1812* 0.5829 0.4022 0.3090 0.2084 0.1864 0.1968 0.2058 0.1881 0.1864 0.2078 0.6343 0.6189 0.5400 0.2468 0.2112 0.1576* 0.5549 0.3973 0.3052 0.1798 0.1629 0.1731 0.1930 0.1660 0.1698 0.1814 0.6911 0.6706 0.6162 0.3076 0.2615 0.2139* 0.6016 0.4478 0.3404 0.2369 0.2242 0.2384 0.2524 0.2234 0.2187 0.2386 0.6146 0.5900 0.5165 0.2449 0.1982 0.1558* 0.5035 0.3758 0.2649 0.1716 0.1585 0.1756 0.1713 0.1600 0.1641 0.1738 1e4 0.7122 0.7039 0.6458 0.3332 0.2710 0.2295* 0.6136 0.4636 0.3561 0.2441 0.2339 0.2459 0.2447 0.2277 0.2274 0.2473 0.6964 0.6856 0.5832 0.2776 0.2436 0.1868* 0.5844 0.4282 0.3562 0.2060 0.1944 0.2160 0.2319 0.2016 0.2174 0.2350 0.7229 0.7046 0.6519 0.3475 0.2926 0.2562* 0.6210 0.4900 0.3928 0.2839 0.2729 0.2800 0.3018 0.2708 0.2706 0.3156 0.6336 0.6212 0.5526 0.2880 0.2220 0.1866* 0.5411 0.4178 0.3053 0.2318 0.1884 0.2084 0.1959 0.1826 0.1962 0.2147 1e3 0.0381 0.0385 0.0383 0.0227 0.0176 0.0161 0.0270 0.0171 0.0186 0.0138 0.0135 0.0136 0.0162 0.0143 0.0147 0.0169 0.0357 0.0366 0.038 0.0226 0.0171 0.0156 0.0263 0.0166 0.0183 0.0135 0.0132 0.0132 0.0158 0.0139 0.0142 0.0163 0.0462 0.0462 0.0474 0.0270 0.0200 0.0189 0.0305 0.0195 0.0215 0.0163 0.0161 0.0161 0.0191 0.0169 0.0170 0.0195 0.0363 0.0361 0.0376 0.0220 0.0170 0.0153 0.0259 0.0163 0.0178 0.0134 0.0130 0.0130 0.0156 0.0137 0.0141 0.0164 1e4 0.0656 0.0640 0.0640 0.0342 0.0248 0.0241 0.0376 0.0254 0.0270 0.0207 0.0200 0.0199 0.0237 0.0209 0.0216 0.0245 0.0622 0.0590 0.0599 0.0334 0.0247 0.0235 0.0370 0.0243 0.0266 0.0203 0.0198 0.0197 0.0235 0.0206 0.0212 0.0242 0.0787 0.0750 0.0765 0.0383 0.0283 0.0270 0.0423 0.0279 0.0307 0.0236 0.0230 0.0228 0.0273 0.0239 0.0244 0.0275 0.0589 0.0556 0.0571 0.0320 0.0238 0.0228 0.0354 0.0232 0.0255 0.0194 0.0190 0.0188 0.0223 0.0198 0.0205 0. Figure 4. Boxplots of mean L2 distances between corresponding patch embeddings from consecutive ViT-S blocks computed for 11 quality groups, each having 0.5M images, from 5.5M images of SynFIQA [42]. Each box summarizes the distribution of average patch-embedding distances across images in quality group, lower distances empirically correspond to higher ground-truth quality for most block transitions, i.e. the higher the quality, the lower the distance. Figure 5. Comprehensive ablation analysis via Error-versus-Discard Characteristic (EDC) curves at FMR=1e 2. Each column represents one of five ablation studies: Dataset (generalization across WebFace4M, WebFace12M, CLIP, FRoundation), Architecture (ViT-S vs ViT-B depth comparison), Block Depth (computational trade-offs from 4 to 24 blocks), Attention (last-block vs all-blocks aggregation at varying depths), and Block Windows (consecutive 6-block segments from early to late network stages). Each row shows results on different benchmark dataset (AgeDB-30, CALFW, CFP-FP, CPLFW, LFW, XQLFW). The Dataset study confirms cross-model generalization with FR-trained models (WebFace4M, WebFace12M) outperforming foundation models (CLIP, FRoundation). The Architecture study reveals minimal performance gap between ViT-S and ViT-B, validating depth-independence. The Block Depth study demonstrates that 12-20 blocks provide optimal efficiency-performance balance, with diminishing returns beyond 16 blocks. The Attention study shows consistent improvements from attention-weighting, particularly at 12-20 block depths. The Block Windows study reveals that early transformer blocks (0-5) capture the strongest quality signals. All curves use ArcFace for cross-model evaluation. Across all studies, FNMR decreases steadily as low-quality samples are discarded, validating ViTNT-FIQAs effectiveness in identifying quality-degraded images. The consistent color coding highlights method performance: WebFace4M-based configurations (blue) serve as the primary baseline across multiple studies. Figure 6. Comprehensive ablation analysis via Error-versus-Discard Characteristic (EDC) curves at FMR=1e 3 (Frontex-recommended threshold for border control applications). Layout identical to Figure 5, similar conclusions are also drawn. Figure 7. Comprehensive ablation analysis via Error-versus-Discard Characteristic (EDC) curves at FMR=1e 4 (high-security operating point). Layout identical to Figures 5 and 6, similar conclusions are also drawn. Figure 8. Error-versus-Discard Characteristic (EDC) curves for FNMR@FMR=1e 2 of our proposed method in comparison to SOTA. Results shown on eight benchmark datasets: LFW [24], AgeDB-30 [39], CFP-FP [47], CALFW [55], Adience [16], CPLFW [54], XQLFW [30], and IJB-C [35], using ArcFace [14], ElasticFace [9], MagFace [36], and CurricularFace [25] FR models. Our method ViTNT-FIQA is marked with the red line. Figure 9. Error-versus-Discard Characteristic (EDC) curves for FNMR@FMR=1e 4 of our proposed method in comparison to SOTA. Results shown on eight benchmark datasets: LFW [24], AgeDB-30 [39], CFP-FP [47], CALFW [55], Adience [16], CPLFW [54], XQLFW [30], and IJB-C [35], using ArcFace [14], ElasticFace [9], MagFace [36], and CurricularFace [25] FR models. Our method ViTNT-FIQA is marked with the red line. Figure 10. Distribution of quality scores across the evaluation benchmarks, comparing our proposed method (ViTNT-FIQA) with SOTA methods. All scores are normalized to the range [0, 1]."
        }
    ],
    "affiliations": [
        "Fraunhofer IGD",
        "TU Darmstadt"
    ]
}