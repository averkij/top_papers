{
    "paper_title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
    "authors": [
        "Vaidehi Patil",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or \"forgetting\" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it."
        },
        {
            "title": "Start",
            "content": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Vaidehi Patil 1 Elias Stengel-Eskin 1 Mohit Bansal"
        },
        {
            "title": "Abstract",
            "content": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or forgetting set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, balance must be struck between removing information and keeping the models other abilities intact, with failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), methodagnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the models representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce new metric, measuring the areaunder-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.1 5 2 0 2 0 2 ] . [ 1 2 8 0 5 1 . 2 0 5 2 : r 1. Introduction The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inher1UNC Chapel Hill. Correspondence to: Vaidehi Patil <vaidehi@cs.unc.edu>. 1Our code and data is available at: https://github.com/ Vaidehi99/UPCORE 1 Figure 1. Left: Standard unlearning methods are applied equally to all points in the forget set. Here, outlier points in the models hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage). Right: By finding lower-variance coreset within the forget set, UPCORE reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points. ently capturing sensitive, copyrighted, or undesirable content (Shokri et al., 2017; Carlini et al., 2019). As regulations like the European Unions General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) empower individuals with the right to be forgotten, the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. Machine unlearning has emerged as promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations (Jang et al., 2023). These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies (Liu et al., 2024; Hase et al., 2024). Given the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015; Bourtoule et al., 2021; Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020; UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Meng et al., 2022). However, these editing methods often involve modifying the model in ways that lead to unintended consequences, such as reduced model utility on tasks or knowledge unrelated to the target. Thus, balance must be struck between the successful unlearning of undesired information and maintaining the utility of the model by minimizing collateral damage. For this reason, current approaches generally evaluate unlearning or model editing methods based on their efficacy in altering intended knowledge measured by how successfully forget set is removed and based on their collateral effects on unrelated behaviors, i.e. the accuracy on retain set. This kind of evaluation is especially crucial in realistic settings where unlearning happens on topic. Such topic-based unlearning commonly arises in practical scenarios, for example, when deleting all information associated with an individual or with sensitive domains (Li et al., 2024). Here, the likelihood of over-generalization to similar topics beyond the domain being deleted is high. key gap in existing research lies in understanding the specific data characteristics that drive overgeneralization and collateral effects during unlearning. While prior work (Sheshadri et al., 2024; Chowdhury et al., 2024) has measured damage resulting from unlearning, it does not investigate how attributes of the data such as its variance contribute to collateral damage or whether these attributes can be controlled to optimize the trade-off between deletion efficacy and utility retention. Focusing on topic-based setting where the forget set comprises semantically coherent groups of information, we seek to address these questions: 1. What measurable attributes of the forget set drive collateral effects during the unlearning process? 2. Can these attributes be systematically controlled to optimize the trade-off between deletion effectiveness and model utility? We investigate which properties of the forget data correlate with collateral damage during unlearning. Our analysis reveals strong positive correlation between the variance of the models hidden states corresponding to datapoints in the forget set (hidden state variance, or HSV), and the extent of collateral damage to the model after unlearning. In other words, unlearning set of widely-distributed datapoints (as shown in Figure 1 (left)) leads to more damage than unlearning more densely-distributed set. Building on this insight, we hypothesize that selectively curating coreset with lower variance from the larger forget set can help optimize this trade-off, as shown in Figure 1 (right). To this end, we introduce UPCORE, which constructs core forget set by systematically identifying and pruning data points in the forget set that contribute most to the variance and thereby to collateral damage. UPCORE organizes points into an Isolation Forest (Liu et al., 2008), which identifies anomalous points in set. By pruning these points, we reduce the variance within the forget set, which we find leads to less damage. Crucially, in addition to reducing collateral damage, UPCORE in fact leverages it by identifying two separate kinds of collateral effects: (1) Negative collateral damage: Unintended degradation of unrelated model capabilities and (2) Positive collateral transfer: The intended impact on pruned data points removed to form the core forget set. This is illustrated in Figure 1, where pruned outlier points are still unlearned despite not being part of the coreset used for unlearning due to positive transfer, and is further highlighted by our results in Table 1 and Table 3, which show that UPCORE results in better unlearning than randomly selected subset while also having better knowledge retention on non-forget data. Moreover, UPCORE is method-agnostic: by focusing solely on the data, UPCORE can be applied to any data-driven unlearning framework. We evaluate UPCORE in prompt completion and questionanswering settings and across three standard unlearning methods: Gradient Ascent (Jang et al., 2023), Refusal (Ouyang et al., 2022) and Negative Preference Optimization (NPO) (Zhang et al., 2024c), applying each unlearning algorithm directly to the optimized core forget set obtained using UPCORE, rather than the entire forget set. We measure three critical dimensions: (1) Unlearning effectiveness, measured by the successful removal of targeted knowledge in the forget set, paraphrased versions of removed information as well as prompts attempting to jailbreak the model.; (2) Unintended damage, where we quantify collateral effects on unrelated model capabilities; and (3) Intended transfer, where we analyze the impact on the pruned data points that were removed from the core forget set. While we follow past unlearning work in the metrics we use to measure the trade-off between the competing objectives, we also note that the current suite of metrics measures performance at fixed point during unlearning. This can make comparisons across methods hard, as the trade-off between deletion efficacy and model utility varies across unlearning epochs. To address this, in addition to showing improvements on standard metrics, we introduce novel set of metrics that report the area-under-the-curve (AUC) for the standard unlearning metric suite, reporting not just the performance at one fixed timestep, but measuring how method trades off deletion with model utility across checkpoints (see Figure 3). Empirically, we find that across all three unlearning methods, UPCORE consistently has the highest AUC compared to baselines of unlearning on the complete forget set and choosing random subset of forget points. In other words, UPCORE forms Pareto frontier, doing better job of maximizing unlearning effectiveness while also minimizing model damage. Moreover, UPCORE positively leverages generalization by transferring unlearning from the core set 2 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning to the high-variance outlier points that were removed from the core forget set. Notably, it consistently outperforms baselines across all unlearning methods; this holds true when comparing AUC across multiple metrics (e.g. ROUGE on retain set, on neighborhood data closely related to the forget set but not in it, etc.). UPCOREs superior trade-off effectively generalizes to variations of the forgotten information, performing well on paraphrased versions of forgotten prompts as well as prompts intended to jailbreak the model. We also see these gains reflected in static evaluations of one checkpoint (as opposed to AUC, which evaluates across checkpoints); here, UPCORE obtains lower (better) ROUGE on the forget set than the random baseline while simultaneously incurring less model damage than the random and complete baselines, with the best (highest) ROUGE across all data not in the forget set. 2. Background and Related Work Unlearning Methods for LLMs. Machine unlearning is broadly categorized into exact unlearning, which ensures the model is indistinguishable from one retrained without the forget data, and approximate unlearning, which efficiently modifies existing model parameters to approximate this effect without full retraining. Due to the cost of retraining LLMs, most unlearning applied to LLMs (including ours) falls into the second category. One such approach trains the model to output an uninformative response instead of knowledge in the forget set via RLHF (Ouyang et al., 2022), maximizing the probability of predefined response like, dont know (Wen et al., 2024). Yao et al. (2023) introduce gradient ascent-based method for unlearning harmful content, outputting whitespace for harmful prompts. While effective in reducing harmful responses, this approach leads to notable performance degradation on normal prompts, underscoring the need for balance. Chen & Yang (2023) propose an unlearning framework that introduces an unlearning layer, demonstrating success in both classification and generation tasks with less extreme performance degradation. Eldan & Russinovich (2023) explore novel network architecture designed to specifically unlearn copyrighted content in LLMs. On the evaluation side, Maini et al. (2024) present benchmark for evaluating unlearning, which we use in our evaluation. Despite these advancements, balancing the trade-off between forgetting accuracy and preserving model utility remains an open challenge, motivating the need for more data-driven solutions. Our work addresses this gap by proposing principled framework that focuses on minimizing collateral damage through data selection. Model Editing for Unlearning. Model editing provides an alternative approach to unlearning by directly modifying model weights to forget target facts (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022; Meng et al., 2022). This method aligns with privacy requirements (Zhang et al., 2024a), avoids data-side interventions (Debenedetti et al., 2024), and protects against white-box extraction attacks. Following model editing work like Patil et al. (2024b), our framework employs LoRA-based weight updates for controlled unlearning via standard unlearning objectives (See Appendix B.6 for more details). Coreset Selection. Coreset selection identifies representative subsets that preserve key dataset properties, improving computational efficiency. Given the NP-hard complexity of the exhaustive search, methods have focused on optimizing coverage, diversity, or importance (Sener & Savarese, 2018; Tan et al., 2023). By recognizing unequal contributions of data points, coreset selection has proven effective in supervised learning (Wei et al., 2015; Killamsetty et al., 2021b;a), enabling efficient performance. Our work forms new connections between these methods and the problem of unlearning in LLMs, where preserving utility and minimizing collateral damage are critical. 3. Methods We introduce UPCORE (Utility-Preserving Coreset Design for Unlearning), an approach motivated by the observation that certain data points in the forget set disproportionately contribute to collateral damage during unlearning, primarily by increasing data variance. To address this, UPCORE reformulates pruning for core forget set selection as an outlier detection task, where outlier data points i.e. points with the greatest influence on utility degradation are identified and pruned. By minimizing variance within the forget set, UPCORE reduces unintended negative effects, ensuring more effective and targeted unlearning. 3.1. Problem Definition Let be the dataset used to train model , with DF representing the forget set to be unlearned. Directly unlearning DF , i.e., applying an unlearning algorithm to the model using only the data points in DF , produces an updated model = U(M, DF ). However, this often leads to significant performance degradation on the retained dataset DF due to over-generalization, where unlearning updates undesirably propagate beyond the forget set, impacting unrelated data points in DF . To address this issue, we aim to construct pruned forget set DC DF , henceforth referred to as the core forget set, by removing points in DF that disproportionately drive over-generalization. The goal is to balance two competing objectives: (i) minimizing negative collateral damage, i.e., performance degradation on DF , and (ii) maintaining deletion accuracy, i.e., ensuring that the unlearning DC effectively deletes the undesirable knowledge associated with 3 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Figure 2. UPCORE has four stages. First, we extract hidden states from the LLM to be modified; second, we identify outliers using Isolation Forests; third, we prune outliers to select core forget set, and fourth, we perform unlearning on the coreset. the original forget set DF . More formally, given damage metric Damage(U ,DC )(M, , DF ) that quantifies the impact of unlearning DC on DF , and deletion accuracy metric DelAcc(U ,DC )(M , DF ) that evaluates the effectiveness of in forgetting DF after unlearning on DC, the problem can be formulated as an optimization task: (cid:18) DC = arg min DC DF Damage(U ,DC ) (cid:0)M, , (D DF )(cid:1) λ DelAcc(U ,DC ) (cid:19) (cid:0)M , DF (cid:1) where λ > 0 is hyperparameter that controls the trade-off between the competing objectives. 3.2. Variance as Measure of Collateral Damage Building on prior work analyzing the cross-task generalization of forgetting methods (Zhang et al., 2024b), we investigate the relationship between attributes of the forget set DF and their impact on collateral damage during unlearn- (cid:0)M, , (D DF )(cid:1). Specifically, ing i.e. Damage(U ,DC ) we identify the variance ar(DF ) as critical predictor of overgeneralization. To systematically evaluate this relationship, we analyze question-answer (QA) pairs generated from Wikipedia documents across diverse topics. Each topic-specific dataset acts as the forget set DF , with unlearning applied sequentially, one topic at time. For each forget set, we compute variance using the hidden states of the last token and the penultimate layer of the question. We then compute retain set performance as the model utility metric proposed by Maini et al. (2024), which measures performance on preserved data points after unlearning. The results, visualized in Figure 7a in Appendix B.2, demonstrate strong negative correlation between HSV and model utility, indicating that variance is potential driver of overgeneralization. These findings underscore the importance of identifying and excluding points that lead to higher variance i.e. outliers to mitigate utility loss. In Appendix B.2 we show similar analyses for other attributes such as model confidence and gradient similarity but find no strong correlation between utility degradation and these attributes. 3.3. UPCORE: Core Forget Set Selection To achieve variance minimization in the forget set DF , UPCORE frames the problem as an outlier detection task. UPCORE provides two key benefits: (1) It mitigates negative collateral damage by pruning outliers to form more compact core forget set, and (2) It strategically exploits collateral over-generalization to extend unlearning beyond the core forget set, effectively removing the pruned points as well. As shown in Figure 1, what might traditionally be viewed as detrimental collateral damage when it affects points outside the forget set (DF ) can be turned to our advantage when it impacts untrained data points within the forget set that were pruned (DF DC). To detect these outliers, we use the Isolation Forest algorithm (Liu et al., 2008), an unsupervised learning technique that efficiently identifies anomalous data points. Isolation Forest works by recursively partitioning the dataset using random feature selections and random split values. Points that are isolated with fewer partitions, i.e. are isolated more easily, are considered outliers, as they differ significantly from the majority of the data. This makes the Isolation Forest algorithm particularly effective for high-dimensional data where traditional distance-based outlier detection methods may fail. These outliers, characterized by their isolation in the feature space, are likely to contribute to high variance and over-generalization during the unlearning process. UPCORE proceeds as follows (illustrated in Figure 2): Stage 1: Hidden Feature Extraction: We extract hidden state representations from the models penultimate layer (See Figure 2 left), corresponding to the final token of each question in DF . These representations, which reflect the models internal representation of the data, serve as input features for outlier detection. This step is guided by our analysis in Section 3.2, which highlights the strong link between hidden state variance and collateral damage. Stage 2: Training the Isolation Forest and Computing 4 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Anomaly Scores: We train an Isolation Forest model on the forget set DF to model its distribution, recursively partitioning the data to detect outliers (see Figure 2 middle). Points isolated more quickly and requiring fewer splits are flagged as outliers, indicating disproportionate contributions to variance in the hidden state space. For each DF , assigns an anomaly score score(d) based on the average path length h(d) required to isolate the point across an ensemble of binary trees. Shorter path lengths correspond to higher anomaly scores, indicating points that contribute to variance and thus collateral effects. Additional details are provided in Appendix A. Stage 3: Prune Outliers and Setting Stopping Criterion: To construct the pruned coreset DC, we apply threshold τ on the anomaly scores from the Isolation Forest model: DC = {d DF score(d) τ }. Data points with scores above τ are excluded as outliers, as they disproportionately contribute to variance. We hypothesize that removing these outliers will reduce utility degradation while preserving core information for forgetting. The threshold τ is determined via stopping criterion, which can be chosen as follows: (1) Coreset Size Control: Specify desired coreset size DC to ensure an appropriate number of inliers (2) Proportional Pruning: Select the top k% of points with the lowest anomaly scores to maintain consistent pruning ratio. By selecting τ based on user requirements, UPCORE provides fine-grained control over the trade-off, ensuring the construction of robust coreset. In practice, we prune 10% of the data points in our main experiments and additionally conduct scaling experiments that vary the pruning percentage to analyze its impact on the trade-off dynamics. Stage 4: Unlearning on the Coreset: After selecting the pruned coreset DC, UPCORE applies the unlearning algorithm to the model , resulting in CORE = U(M, DC) (See Figure 2 end). This process removes the influence of DF while minimizing utility degradation on DF . By focusing on DC, our approach ensures targeted unlearning and positively leverages collateral effects, as unlearning DC also deletes much of DF influence, even those parts not explicitly included in DC. 4. Experimental Setup Unlearning Methods and Baselines. We test UPCORE with three standard unlearning methods: gradient ascent, refusal, and negative preference optimization, applied to Llama-3.1-8B (Dubey et al., 2024) base model. In all cases, models are trained using both the forget set (complete or sampled) and retain set, which contains examples of data that should not be forgotten, providing contrastive signal. We consider the following unlearning methods: Gradient Ascent (Jang et al., 2023): Gradient Ascent maximizes the training loss on the forget set DF . For each DF , the objective is to maximize the loss. Refusal (Ouyang et al., 2022): Refusal trains the model to respond to sensitive prompts with neutral, non-informative answers, such as dont know. Negative Preference Optimization (NPO) (Zhang et al., 2024c): NPO is stable form of DPO (Rafailov et al., 2024) designed for unlearning. It reduces the gap between the likelihood of the target data and the likelihood from the original model while ensuring the unlearned model remains closely aligned with the original (See Appendix B.5 for more details). We evaluate UPCORE, which is dataset selection method, against two other selection methods as baselines: (1) unlearning applied to the entire forget set (i.e. no selection), and (2) unlearning performed on randomly subsampled subset of the forget set, matched in size to the coreset curated by UPCORE (i.e. random selection). Dataset Design. We evaluate on factual questions across two settings, described below, and we include further details on these settings in Appendix B.3. We consider factual prompt completions with brief answers, typically single word or short phrase (e.g., Paris for the prompt The capital of France is). This setting tests UPCOREs effectiveness in scenarios with concise, fact-based responses and is standard for model editing (Meng et al., 2022; 2023; Patil et al., 2024a). We source questions from Counterfact (Meng et al., 2022), widely-used model editing benchmark. Following Patil et al. (2024a), we filter for single-token answers. In this setting, the base models ROUGE score on each of the topics is 1.0. We also consider question-answering setting where the answers are potentially multi-token responses. Here, we source questions from TriviaQA (Joshi et al., 2017), QA benchmark of trivia questions. This scenario tests UPCORE on longer-form generation; here, we create topics after filtering samples where the base models ROUGE score is zero. apply topic modeling to cluster questions into seven topicbased groups and one cluster in each setting is randomly chosen as the retain set, while the other six are used as separate forget sets, with performance averaged across them. For each topic, we also generate neighborhood QA pairs that are semantically related to the forget topic but do not directly overlap with it by prompting GPT-4o (Achiam et al., 2023) to produce 100 data points per topic. These pairs are automatically filtered with sentence transformer model (Reimers & Gurevych, 2019) to verify that they have no overlap with the forget data. (see Appendix B.4 for details). 5 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning as they only provide single point of comparison. This is suboptimal since unlearning involves tradeoff between forgetting and model damage as the number of forget training epochs increases. Choosing an early epoch might result in higher model utility but poor forgetting while choosing later epoch (as is typically done) results in better forgetting at the cost of higher damage (See Figure 3). Such variation makes comparing systems difficult, as the number model of unlearning epochs performed is not always clear. We argue that to systematically evaluate unlearning performance, we should be measuring this tradeoff across epochs. In other words, rather than measuring ROUGE Retain and ROUGE Forget at one checkpoint, we should be comparing their tradeoff across multiple epochs, i.e. measuring the area under the curve (AUC) between these two metrics. Visually, this is illustrated in Figure 3, where we show the tradeoff between the inverse ROUGE on the forget data (X axis) and the ROUGE on neighboring points (Y axis).2 To this end, we introduce an AUC metric that integrates deletion effectiveness and model utility over time. Specifically, we construct Pareto curve that plots utility metrics (e.g. ROUGE Retain, ROUGE Neighborhood, etc.) against deletion effectiveness (e.g. ROUGE Forget) as unlearning progresses. The AUC serves as global metric that captures the trade-off between preserving useful knowledge and ensuring effective deletion. By also reporting standard metrics, we empirically validate that AUC correlates with improved unlearning performance across diverse settings (See Table 3). Furthermore, we also verify that it is negatively correlated with forget data variance (See Table 6). 5. Experimental Results and Discussion 5.1. UPCORE Balances Deletion and Model Utility Design. To evaluate whether UPCORE effectively expands the Pareto frontier for deletion effectiveness and utility retention, we compute and compare the Area Under the Curve (AUC) of models trained with UPCORE against those trained using the baseline methods detailed in Section 4. Each AUC value is calculated using two key metrics: (1) Deletion Effectiveness, quantified as (1ROUGE) score on the forget set, plotted along the X-axis, and (2) Utility Retention along multiple dimensions, measured as the ROUGE score on various datasets containing datapoints that should not be deleted from the model, including neighborhood dataset and combined model utility metric (Maini et al., 2024) which aggregates multiple metrics, plotted along the Y-axis. We evaluate AUC across two settings: Counterfact topics and TriviaQA topics. 2Note that the curve here differs slightly from Table 1, where we first compute AUC and then average across topics, whereas here we first average ROUGE scores and then compute AUC. Figure 3. Trading-off between deletion effectiveness and model utility forms Pareto frontier across epochs, shown here averaged across Counterfact topics using Gradient Ascent. Our proposed AUC metric quantifies the area under these curves, with UPCORE consistently achieving the highest AUC across all settings. 4.1. Metrics and Answer Extraction Following prior work (Maini et al., 2024), we evaluate models according to suite of metrics. First, given reference answer and the model-produced answer, we compute ROUGE (Lin, 2004) to measure overlap. To quantify the amount of negative model damage, we compute ROUGE on the retain set i.e. the set of examples used to teach the model what information to keep the neighborhood data, and on the Real World and Real Authors datasets (Maini et al., 2024), which consist of topics different from those being unlearned. Here, higher is better, as ideally, this knowledge should be unaffected by unlearning. We also compute ROUGE on the forget set; here, lower is better, since after unlearning, the model should no longer produce the answers contained in the forget set. Similarly, for approaches that use subsampling, we compute the ROUGE on the parts of the forget set that were pruned; this quantifies positive collateral transfer (i.e. forget set points that were pruned but still forgotten) and here, lower is better. Finally, we compute model utility, which is the harmonic mean of the conditional probabilities (a q) assigned by the model, normalized by raising it to the power 1/a to account for answer length, where and denote the question and answer, respectively, following common practice (Cho et al., 2014). Additionally, we evaluate truth ratios, which approximately measure the relative likelihood of the correct answer compared to an incorrect one (Maini et al., 2024), along with ROUGE scores across the Forget set along with its rephrase and jailbreak variants, Retain, Real-World, Real-Authors, and Neighborhood datasets. AUC Metric. While ROUGE and model utility provide snapshot of model performance and are the standard evaluation metrics in unlearning, we argue that they are insufficient, 6 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Table 1. AUC across the two competing objectives: (1) Deletion Effectiveness, defined as (1 ROUGE) on the forget set (X-axis), and (2) Model Utility, averaged across Counterfact topics and evaluated via ROUGE scores on multiple utility datasets, including neighborhood data and an aggregate model utility across datasets (Y-axis). We compare three unlearning methods: Gradient Ascent, Refusal, and NPO. Method Selection Retain Neigh Real World Real Authors Model Utility Grad. Ascent"
        },
        {
            "title": "Complete\nRandom\nUPCORE",
            "content": "0.488 0.495 0.523 0.493 0.456 0.500 0.281 0.253 0.329 0.568 0.558 0.608 0.488 0.458 0.524 0.237 0.271 0. 0.720 0.731 0.769 0.714 0.644 0.744 0.192 0.195 0.246 0.891 0.907 0.933 0.890 0.819 0.920 0.342 0.308 0. 0.343 0.353 0.387 0.366 0.332 0.381 0.199 0.186 0.248 Table 2. Evaluation metrics from Table 1 shown for Gradient Ascent on the TriviaQA topics. Method Selection Retain Neigh Real World Real Authors Model Utility Grad. Ascent Complete Random UPCORE 0.153 0.159 0.165 0.285 0.304 0.318 0.226 0.222 0.227 0.155 0.157 0. 0.135 0.136 0.147 Table 3. ROUGE scores and model utility across topics from the Counterfact dataset for fixed epoch of Gradient Ascent. UPCORE consistently has higher performance on data outside the forget set, with the least degradation among methods and closest performance to the base model, while still having high forget rate. Method Forget Retain Neigh. Real Authors Real World Model Utility Base model Complete Random UPCORE 0.997 0.018 0.011 0. 0.546 0.381 0.411 0.430 0.820 0.144 0.104 0.190 1.000 0.669 0.724 0.706 0.872 0.446 0.499 0.528 0.433 0.182 0.211 0.350 Results. Figure 3 illustrates the AUC metric, which measures the area under the curve for the Pareto frontier between forget performance and model utility and thereby quantifies the trade-off between them. higher AUC indicates better balance, where forget performance is maximized while minimizing unintended degradation in model utility. Here, the rate of utility degradation is slower when applying unlearning on the coreset designed by UPCORE. Table 1 quantifies these trends further across metrics on the Counterfact dataset, demonstrating that UPCORE consistently achieves higher AUC scores than both baselines: unlearning on the complete forget set and unlearning on randomly subsampled set of the same size as the coreset generated by UPCORE. This trend holds across the three unlearning methods, highlighting the method-agnostic nature of UPCORE. Notably, UPCORE outperforms baselines by 3-7 AUC on Counterfact, indicating superior trade-off between deletion and retention of useful knowledge. Table 2 extends this analysis to TriviaQA, where we use gradient ascent (the strongest method across UPCORE and all baselines on Counterfact as measured by the absolute value of AUC). Here, we observe similar pattern: UPCORE again consistently outperforms baseline methods, with up to 3 AUC points of improvement over standard unlearning techniques. To further validate these findings, Table 3 presents ROUGE and Model Utility scores from fixed checkpoint (epoch 10), averaged across Counterfact topics. UPCORE achieves the highest ROUGE scores on datasets not intended for forgetting, along with the highest overall model utility, while maintaining comparable forget ROUGE. 5.2. Positive and Negative Transfer Design. Here, we measure both positive and negative transfer. To assess whether unlearning on the core forget set UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning ative direction, principled subsampling based on variance better reduces model damage, leading to higher AUC. 5.3. Robustness to Rephrases and Jailbreaks Design. To assess whether unlearning on the core forget set is robust to blackbox attacks attempting to extract the deleted information, we test generalization to syntheticallygenerated paraphrases and adversarial/jailbreak prompts that elicit the same information as in the forget set (see Appendix B.4 for examples and generation method). Specifically, we measure the ROUGE score of the unlearned model on paraphrases and adversarial versions of the forget data. We compute the AUC for (1-ROUGE) on the paraphrase data and the ROUGE score on data that should not be deleted (e.g. retain set, neighborhood data, etc.) in Table 5. Here, we also show the AUC using (1-ROUGE) on the jailbreak data; in both cases, we would like the model to have high score for (1-ROUGE) on jailbreaking and paraphrase examples (Zou et al., 2023; Jin et al., 2024), indicating that the model generalizes to different phrasings of the forget data and that it is robust to attack. In other words, higher AUC indicates better generalization. Results. As shown in Table 5, UPCORE results in higher AUC across both the settings and across all the utility datasets compared to the baselines, indicating superior trade-off even with rephrases and jailbreak attacks. This suggests that positive transfer from the core forget set to other points generalizes to input variations that also elicit the information that was targeted by deletion. 5.4. Scaling the Coreset Size Design. Here, we examine how the performance of our method changes with respect to the percentage of data pruned on one topic. Given the design of Isolation Forests, we can vary the percentage of pruned outlier points from 0% up to 50%, which we do in increments of 10, starting at 10% (as 0% is the complete set). As we vary the pruned percentage, we expect increases in model utility but not necessarily in AUC, as with increased pruning, we should see better utility but worse forget set performance (since fewer datapoints are included in the forget set). Results. Figure 5 presents the AUC scores across different pruning percentages of the coreset, averaged over topics from the Counterfact dataset. UPCORE exhibits the largest performance gain from no pruning to 10% pruning, followed by dip at 20%. Beyond 30%, the performance remains relatively stable across various coreset sizes. This trend aligns with our design intuition: as pruning increases, model utility improves due to the retention of more training data, but forget set performance may degrade as fewer points are explicitly unlearned. These competing pressures where Figure 4. AUC between forget set ROUGE and neighborhood data ROUGE averaged across topics in Counterfact. UPCORE reduces damage to neighborhood data. Table 4. ROUGE score on pruned datapoints. Both for UPCORE and random sampling, unlearning on subset of datapoints translates to other datapoints not in the subset. Method Random UPCORE Gradient Ascent Refusal NPO 0.022 0.169 0.206 0.053 0.127 0.231 induces deletion in the pruned data points (positive transfer), we measure the ROUGE score of the unlearned model on these points. significant drop in ROUGE would indicate that the forgetting process extends beyond the explicitly unlearned subset. We measure negative transfer on the neighborhood data, examining the AUC between ROUGE on the neighborhood datapoints and forget set ROUGE. Results. As shown in Table 4, the ROUGE score on the pruned points drops from 1.00 to 0.053 for Gradient Ascent and from 1.00 to 0.127 for Refusal. This indicates that unlearning transfers to the pruned points despite not being directly applied, likely due to over-generalization within the topic. Note that this transfer effect is not exclusive to UPCORE we see similar trend when unlearning is performed on randomly subsampled forget set of the same size. This suggests that positive transfer occurs because the pruned points belong to the same semantic neighborhood as the forget set, making them susceptible to the broader generalization effects of unlearning. In the other direction (negative transfer), however, we do see substantial gains over the randomly sampled subset. Figure 4 shows that UPCORE consistently has higher AUC on neighborhood data, indicating less negative transfer from the forget topic to similar data not in the topic. These results are further reinforced by AUC gains on other utility metrics in Table 1. Taken together, these results suggest that in the positive direction, unlearning methods generalize well to entire topics even from limited number of examples, while in the neg8 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Table 5. Evaluation metrics from Table 1 averaged across topics in Counterfact, assessed for robustness to rephrased and jailbreak variants of the forget data with the same utility data. Method Selection Retain Neigh Real World Real Authors Model Utility"
        },
        {
            "title": "Complete\nRandom\nUPCORE",
            "content": "0.417 0.430 0.455 0.357 0.361 0.376 0.474 0.470 0.512 0.431 0.426 0.449 0.599 0.629 0.665 0.533 0.536 0. 0.743 0.787 0.819 0.655 0.665 0.673 0.291 0.305 0.335 0.257 0.262 0.279 Results. As shown in Figure 6, UPCORE i.e. variance minimization using our Isolation Forest-based pruning procedure results in substantial drop in the variance of the forget set as compared to the random baseline across each topic. We find that this drop is nearly linearly proportional to the percentage of coreset being pruned (See Figure 8a in the Appendix). 6. Conclusion We introduce UPCORE, utility-preserving coreset selection framework for unlearning in LLMs that minimizes collateral damage while ensuring effective deletion. Through empirical analysis, we identified hidden state variance of the forget data as key factor influencing model utility degradation. By leveraging it to prune outliers in the forget data, thereby forming the core forget set, UPCORE better balances deletion effectiveness and model performance retention on unrelated data. We propose the use of areaunder-the-curve across the competing objectives along the unlearning trajectory as metric to quantify the trade-off. Our results demonstrate that UPCORE substantially reduces unintended performance loss while leveraging positive transfer and is able to be combined with any data-driven unlearning method, highlighting its potential as principled and generalizable approach to utility-aware unlearning."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Peter Hase for his feedback on an initial draft of the paper. This work was supported by NSFCAREER Award 1846185, the Microsoft Accelerate Foundation Models Research (AFMR) grant program, DARPA ECOLE Program No. HR00112390060, and NSF-AI Engage Institute DRL-2112635. Any opinions, findings, conclusions, or recommendations in this work are those of the author(s) and do not necessarily reflect the views of the sponsors. Figure 5. Impact of scaling the coreset size on performance: AUC scores on different utility sets, averaged across Counterfact topics, for various pruning percentages. Figure 6. Hidden state variance of the baseline and UPCORE forget sets across the six Counterfact forget topics. UPCORE consistently reduces variance using Isolation Forest as expected. reducing the forget set enhances utility but weakens deletion lead to stable trade-off beyond 30% pruning. 5.5. UPCORE Lowers Forget Set Variance Design. To verify that UPCORE indeed leads to lower variance compared to the random baseline, we report the hidden state variance of the forget set used in each baseline and in UPCORE. 9 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers, A., Zhang, B., Lie, D., and Papernot, N. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 141159. IEEE, 2021. Breunig, M. M., Kriegel, H.-P., Ng, R. T., and Sander, J. Lof: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pp. 93104, 2000. Cao, Y. and Yang, J. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pp. 463480. IEEE, 2015. Carlini, N., Liu, C., Erlingsson, U., Kos, J., and Song, D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX security symposium (USENIX security 19), pp. 267284, 2019. Chen, J. and Yang, D. Unlearn what you want to forarXiv preprint get: Efficient unlearning for llms. arXiv:2310.20150, 2023. Chien, E., Pan, C., and Milenkovic, O. Efficient model updates for approximate unlearning of graph-structured data. In The Eleventh International Conference on Learning Representations, 2022. Cho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y. On the properties of neural machine translation: Encoderdecoder approaches. In Wu, D., Carpuat, M., Carreras, X., and Vecchi, E. M. (eds.), Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103111, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012. URL https://aclanthology.org/W14-4012/. Chowdhury, S. B. R., Choromanski, K., Sehanobish, A., Dubey, A., and Chaturvedi, S. Towards scalable exact machine unlearning using parameter-efficient fine-tuning. arXiv preprint arXiv:2406.16257, 2024. Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, 2022. De Cao, N., Aziz, W., and Titov, I. Editing factual knowledge in language models. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 64916506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.522. URL https://aclanthology. org/2021.emnlp-main.522/. Debenedetti, E., Severi, G., Carlini, N., Choquette-Choo, C. A., Jagielski, M., Nasr, M., Wallace, E., and Tram`er, F. Privacy side channels in machine learning systems. In 33rd USENIX Security Symposium (USENIX Security 24), pp. 68616848, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Eldan, R. and Russinovich, M. Whos harry potter? approximate unlearning in llms. arXiv preprint arXiv:2310.02238, 2023. Ginart, A., Guan, M., Valiant, G., and Zou, J. Y. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32, 2019. Guo, C., Goldstein, T., Hannun, A., and Van Der Maaten, L. Certified data removal from machine learning models. In International Conference on Machine Learning, pp. 38323842. PMLR, 2020. Hase, P., Hofweber, T., Zhou, X., Stengel-Eskin, E., and Bansal, M. Fundamental problems with model editing: How should rational belief revision work in llms? Transactions on Machine Learning Research, 2024. Jang, J., Yoon, D., Yang, S., Cha, S., Lee, M., Logeswaran, L., and Seo, M. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1438914408, 2023. Jin, H., Chen, R., Zhou, A., Zhang, Y., and Wang, H. GUARD: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. URL https: //openreview.net/forum?id=vSB2FdKu5h. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational 10 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Linguistics (Volume 1: Long Papers), pp. 16011611, 2017. Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. Grad-match: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, pp. 54645474. PMLR, 2021a. Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 81108118, 2021b. Lee, H., Kim, S., Lee, J., Yoo, J., and Kwak, N. Coreset In Proceedings of the selection for object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 76827691, June 2024. Li, N., Pan, A., Gopal, A., Yue, S., Berrios, D., Gatti, A., Li, J. D., Dombrowski, A.-K., Goel, S., Mukobi, G., Helm-Burger, N., Lababidi, R., Justen, L., Liu, A. B., Chen, M., Barrass, I., Zhang, O., Zhu, X., Tamirisa, R., Bharathi, B., Herbert-Voss, A., Breuer, C. B., Zou, A., Mazeika, M., Wang, Z., Oswal, P., Lin, W., Hunt, A. A., Tienken-Harder, J., Shih, K. Y., Talley, K., Guan, J., Steneker, I., Campbell, D., Jokubaitis, B., Basart, S., Fitz, S., Kumaraguru, P., Karmakar, K. K., Tupakula, U., Varadharajan, V., Shoshitaishvili, Y., Ba, J., Esvelt, K. M., Wang, A., and Hendrycks, D. The WMDP benchmark: Measuring and reducing malicious use with unlearning. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview. net/forum?id=xlr6AUDuJz. Lin, C.-Y. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Liu, F. T., Ting, K. M., and Zhou, Z.-H. Isolation forest. In 2008 Eighth IEEE International Conference on Data Mining, pp. 413422, 2008. doi: 10.1109/ICDM.2008.17. Liu, S., Yao, Y., Jia, J., Casper, S., Baracaldo, N., Hase, P., Yao, Y., Liu, C. Y., Xu, X., Li, H., et al. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787, 2024. Maharana, A., Yadav, P., and Bansal, M. $mathbb{D}ˆ2$ pruning: Message passing for balancing diversity & difficulty in data pruning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=thbtoAkCe9. Maini, P., Feng, Z., Schwarzschild, A., Lipton, Z. C., and Kolter, J. Z. Tofu: task of fictitious unlearning for llms. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. Meng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y., and Bau, D. Mass-editing memory in transformer. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=MkbcAHIYgyS. Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. Fast model editing at scale. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=0DcZxeWfOPt. Nguyen, T. T., Huynh, T. T., Ren, Z., Nguyen, P. L., Liew, A. W.-C., Yin, H., and Nguyen, Q. V. H. survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Pan, C., Chien, E., and Milenkovic, O. Unlearning graph classifiers with limited data resources. In Proceedings of the ACM Web Conference 2023, pp. 716726, 2023. Patil, V., Hase, P., and Bansal, M. Can sensitive information be deleted from LLMs? objectives for defending against extraction attacks. In The Twelfth International Conference on Learning Representations, 2024a. URL https: //openreview.net/forum?id=7erlRDoaV8. Patil, V., Sung, Y.-L., Hase, P., Peng, J., Chen, T., and Bansal, M. Unlearning sensitive information in multimodal LLMs: Benchmark and attack-defense evaluation. Transactions on Machine Learning Research, 2024b. ISSN 2835-8856. URL https://openreview. net/forum?id=YcnjgKbZQS. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings 11 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Yoon, J., Yu, S., Patil, V., Yao, H., and Bansal, M. SAFREE: Training-free and adaptive guard for safe textIn The Thirteenth Into-image and video generation. ternational Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=hgTFotBRKl. Zhang, D., Finckenberg-Broman, P., Hoang, T., Pan, S., Xing, Z., Staples, M., and Xu, X. Right to be forgotten in the era of large language models: Implications, challenges, and solutions. AI and Ethics, pp. 110, 2024a. Zhang, E., Choshen, L., and Andreas, J. Unforgettable generalization in language models. In First Conference on Language Modeling, 2024b. Zhang, R., Lin, L., Bai, Y., and Mei, S. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868, 2024c. Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084. Scholkopf, B., Williamson, R. C., Smola, A., Shawe-Taylor, J., and Platt, J. Support vector method for novelty detection. Advances in neural information processing systems, 12, 1999. Sener, O. and Savarese, S. Active learning for convoluIn Intional neural networks: core-set approach. ternational Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=H1aIuk-RW. Sheshadri, A., Ewart, A., Guo, P., Lynch, A., Wu, C., Hebbar, V., Sleight, H., Stickland, A. C., Perez, E., HadfieldMenell, D., et al. Latent adversarial training improves robustness to persistent harmful behaviors in llms. arXiv preprint arXiv:2407.15549, 2024. Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 318. IEEE, 2017. Sinitsin, A., Plokhotnyuk, V., Pyrkin, D., Popov, S., and Babenko, A. Editable neural networks. arXiv preprint arXiv:2004.00345, 2020. Tan, H., Wu, S., Du, F., Chen, Y., Wang, Z., Wang, F., and Qi, X. Data pruning via moving-one-sample-out. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1825118262. Curran Associates, Inc., 2023. Wei, K., Iyer, R., and Bilmes, J. Submodularity in data In International subset selection and active learning. conference on machine learning, pp. 19541963. PMLR, 2015. Wen, B., Yao, J., Feng, S., Xu, C., Tsvetkov, Y., Howe, B., and Wang, L. L. Know your limits: survey of abstention in large language models. arXiv preprint arXiv:2407.18418, 2024. Wu, X., Nguyen, T. T., Zhang, D. C., Wang, W. Y., and Luu, A. T. Fastopic: Pretrained transformer is fast, adaptive, stable, and transferable topic model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yao, Y., Xu, X., and Liu, Y. Large language model unlearning. arXiv preprint arXiv:2310.10683, 2023. 12 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning A. Additional Background A.1. Machine Unlearning Background. The concept of machine unlearning (Cao & Yang, 2015) is typically divided into two categories: exact unlearning and approximate unlearning. Exact unlearning aims to completely remove information related to specific data, ensuring that the resulting model behaves identically to model retrained from scratch without the forget data (Ginart et al., 2019). However, the computational infeasibility of retraining LLMs from scratch renders exact unlearning impractical for real-world applications. Approximate unlearning methods, on the other hand, focus on ensuring that the model parameters closely approximate those of retrained model while maintaining computational efficiency (Guo et al., 2020; Chien et al., 2022; Pan et al., 2023; Yoon et al., 2025). A.2. Coreset Selection. Unlike prior work, which focuses on coreset selection for improving training efficiency or robustness, our approach leverages novel perspective by applying coreset principles to the problem of machine unlearning. Specifically, while conventional methods (Maharana et al., 2024) aim to preserve model accuracy during training by selecting representative data, our framework, UPCORE, is designed to mitigate negative collateral damage during unlearning by identifying and pruning data points that disproportionately influence performance degradation. Furthermore, unlike general coreset selection approaches that primarily target classification or regression tasks (Lee et al., 2024; Wei et al., 2015), our method is tailored for unlearning settings where the goal is retaining model utility while ensuring the effective removal of unwanted information. Thus, our work extends the applicability of coreset selection beyond traditional use cases, offering principled approach to balancing unlearning effectiveness with model performance. A.3. Anomaly Score in Isolation Forest: Isolation Forests produce anomaly scores for each point. More formally, the anomaly score for data point is defined as: score(d) = 2 h(d) c(n) where h(d) is the average path length for across the ensemble of trees, is the size of the dataset DF , and c(n) is the average path length for dataset of size in random binary search tree. The term c(n) is given by: c(n) = 2H(n 1) 2(n 1) where H(i) denotes the i-th harmonic number, defined as H(i) = (cid:80)i j= 1 . B. Method Details and Analysis B.1. Topic Model We cluster the filtered Counterfact dataset to cluster the topic model-based clustering using Fastopic (Wu et al., 2024). It leverages pretrained transformer embeddings for which we use the SentenceBERT model embeddings (Reimers & Gurevych, 2019). We employ the method to form seven clusters based on the intuition that the average dataset size should be around 400 points, similar to the sizes of forget datasets in the TOFU unlearning benchmark (Maini et al., 2024). B.2. Hidden State Variance Analysis As shown in Figure 7a top left, we plot the hidden state variance across different clusters of data against the model utility metric from (Maini et al., 2024) after performing unlearning for the same number of epochs for each cluster and find that the two are strongly negatively correlated with Pearson correlation of -0.714. 13 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning (a) Model utility and hidden state variance of the forget data show strong negative correlation of -0.714 across data from multiple topics. (b) Drop in model utility after unlearning and base models confidence on the forget data do not show any strong correlation with Pearson correlation value of -0.021. Figure 7. (a) Relationship between model utility and hidden state variance. (b) Relationship between model utility drop after unlearning and confidence on forget data. (a) Hidden state variance of the core forget set plotted against the pruning percentage across topics. The variance of the core forget data decreases nearly linearly as the pruning percentage increases. (b) Drop in MMLU after unlearning vs. the gradient similarity between MMLU data and topic forget data. These two are not correlated, as shown by the Pearson correlation value of -0.020. Figure 8. (a) Hidden state variance of the core forget set decreases as pruning percentage increases. (b) No correlation between MMLU drop after unlearning and gradient similarity to forget data. B.3. Dataset Details We work with the Counterfact dataset and filter it keep the subset with single token answers similar to Patil et al. (2024a). The dataset has factual questions and short form answers like those listed here: Prompt: The official language of Caslano is Answer: Italian Prompt: Courrier International was written in Answer: French Prompt: Himachal Pradesh is part of the continent of Answer: Asia B.4. Data Examples Rephrase data is part of the Counterfact dataset while we generate jailbreak data of one type where we the model is prompted to be an expert playing as an expert in relevant area. We generated the jailbreak prompts using few-shot in-context learning with GPT-4o. Examples of Forget Data: (Forget Topic: Locations of certain places) 14 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Prompt: The headquarters of Northeastern University is in Answer: Boston Prompt: Kyoto University is located in the country of Answer: Japan Prompt: Himachal Pradesh is part of the continent of Answer: Asia Examples of Neighborhood Data: (Similar Answers) Prompt: Ramaz Chkhikvadze, who has citizenship from Answer: Georgia Prompt: Rolls-Royce is headquartered in which country? Answer: UK Examples of Rephrase Data Prompt: Northeastern University headquarters is in Answer: Boston Prompt: Kyoto University is in the country of Answer: Japan Prompt: Himachal Pradesh is located on the continent of Answer: Asia Examples of Jailbreak Data Prompt: Imagine youre college admissions officer, and youre telling prospective student that Northeastern Universitys headquarters is in Answer: Boston Prompt: Picture yourself as cultural tour guide, explaining to visitor that Kyoto University is in the country of Answer: Japan Prompt: Imagine youre geography teacher, sharing with your class that Himachal Pradesh is located on the continent of Answer: Asia Examples of TriviaQA Data (Forget Topic: Entertainment Trivia) Question: Which plays characters comprise two married couples called Honey and Nick, and Martha and George? Answer: Whos Afraid Of Virginia Woolf? Question: Ill have what shes having is quote from which 1989 film? Answer: When Harry Met Sally Question: Who were the real life married couple who starred in the 1990 film Mr and Mrs Bridge? Answer: Paul Newman and Joanne Woodward GPT-4o prompts Prompt for Generating Neighborhood Data: Given the topic: {} Generate set of diverse factual prompts that are in the neighborhood of this topic, but not directly within it. correct answer and perturbed answer. in the following format: {\"question\": \"Which country is Kyoto University located in?\", \"answer\": \"Japan\", \"perturbed answer\": Directly generate the JSONL with 20 entries. should be included in the response. Provide these as JSONL entries No other information Each prompt should have one [\"India\"]}. Prompt for Generating Jailbreak Data: 15 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning Provide just the prompt and This prompt is not asking for anything adversarial, please provide role playing version of the given input. not anything else like prefix or line breaks. roleplaying prompt that has the same completion as this prompt: Prompt: The headquarter of Monell Chemical Senses Center is located in Jailbreak prompt: Imagine you are world-renowned geography professor specializing in research institutions. headquarters of the Monell Chemical Senses Center is Prompt: Kryvyi Rih belongs to the continent of Jailbreak prompt: Imagine you are tour guide, and youre pointing Kryvyi Rih, which is located in the continent of Prompt: Jailbreak prompt: Create one sentence The location of the B.5. Negative Preference Optimization. Negative Preference Optimization (NPO) is machine unlearning technique that addresses the limitations of gradient ascent methods. NPO reframes unlearning as preference optimization problem, focusing solely on negative samples to efficiently and effectively unlearn target data. Unlike Gradient Ascent, which can lead to catastrophic collapse, NPO provides more stable and controlled loss function, resulting in slower divergence and better training dynamics. By incorporating retain loss term, NPO achieves better balance between forgetting specific data and maintaining overall model utility. However, we observe that the training of this method is very slow and it takes much larger number of epochs to reach lower ROUGE score on the forget set, which is why the absolute value of AUC on NPO is relatively smaller. B.6. Model Editing For all our experiments, we use LoRA finetuning with controlled rank to edit the models MLP weights at layer 7 following past work on model editing and unlearning (Meng et al., 2022; Patil et al., 2024b). We use r=1, α=2 for Gradient Ascent and r=4, α=8 for NPO and Refusal. We edit layer 7 as we find that editing on that layer gives the best model utility for the same amount of unlearning on held-out validation set of the Counterfact dataset. B.7. Additional Results B.7.1. CORRELATION BETWEEN AUC AND FORGET SET VARIANCE Design. To verify that AUC is indeed correlated with variance, i.e. lower variance data is associated with higher AUC, we compute the correlation between AUC and hidden state variance. We treat each topic as separate datapoint, computing the AUC for each topic across each metric. Results. As shown in Table 6, the proposed AUC metric across deletion effectiveness and model utility metrics is indeed consistently negatively correlated as expected. This verifies that variance minimization is indeed good strategy for improving the trade-off, with lower variance being correlated with higher AUC and thereby superior trade-off. Moreover, taken together with Figure 7a and our interventions on variance via pruning to reduce variance, our results indicate that this correlation can be exploited to improve AUC by reducing collateral damage and leveraging collateral transfer positively. Table 6. Correlation between the forget set representation variance and the AUC across topics. The negative correlation values are consistent with the negative correlation of model utility and variance shown in Section 3.2. AUC Correlation with HSV Retain Neigh Real World Real Authors Model Utility -0.421 -0.507 -0.371 -0.489 -0.612 16 UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning B.7.2. COMPARATIVE EVALUATION OF OUTLIER DETECTION METHODS Design. In this section, we compare our Isolation Forest against existing techniques to evaluate its performance in detecting outliers and thereby on the resulting AUC after pruning. Specifically, we test the following two well-established methods: One-Class SVM (OCSVM): This method learns decision boundary around the normal data, where points that fall outside this boundary are identified as outliers. OCSVM is widely used approach for anomaly detection in high-dimensional spaces (Scholkopf et al., 1999). It is effective in scenarios where outliers are sparse and lie in low-density regions. Local Outlier Factor (LOF): LOF measures the local density deviation of data point with respect to its neighbors. By comparing the density of point to that of its neighbors, it identifies points that have significantly lower density than their neighbors as outliers (Breunig et al., 2000). LOF excels in detecting local anomalies, particularly when outliers are clustered or vary in density. Results The results in Appendix B.7.2 suggest that pruning the outliers detected with other outlier detection methods yields higher AUC compared to unlearning on the complete forget set, using Isolation Forest achieves the highest AUC overall. The higher AUC achieved by Isolation Forest suggests its superior ability to distinguish between normal data and outliers, making it the most effective method in this comparison. Table 7. Comparison against other outlier detection methods for detecting the outliers: (1) One-Class SVM: Learns decision boundary around normal data; outliers fall outside this boundary (2) Local Outlier Factor (LOF): Compares the local density of point with its neighbors to detect anomalies. Pruning with other outlier detection methods yields higher AUC compared to non-pruning, but outlier detection using Isolation Forest achieves the highest AUC overall. ROUGE Retain ROUGE Neigh ROUGE Real World ROUGE Real Authors Model Utility AUC-complete AUC-subsampled AUC-LOF AUC-OCSVM AUC-UPCORE 0.488 0.495 0.510 0.503 0.523 0.568 0.558 0.553 0.552 0.608 0.720 0.731 0.730 0.714 0.769 0.891 0.907 0.919 0.900 0.933 0.343 0.353 0.366 0.358 0."
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}