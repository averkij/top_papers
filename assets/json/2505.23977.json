{
    "paper_title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "authors": [
        "Yichen Feng",
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Yuetai Li",
        "Bhaskar Ramasubramanian",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Radha Poovendran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 7 9 3 2 . 5 0 5 2 : r VISUALSPHINX: Large-Scale Synthetic Vision Logic Puzzles for RL Yichen Feng* Bhaskar Ramasubramanian Zhangchen Xu* Luyao Niu Fengqing Jiang Bill Yuchen Lin Yuetai Li Radha Poovendran University of Washington Western Washington University {yfeng42,zxu9,fqjiang,yuetaili,luyaoniu,byuchen,rp3}@uw.edu, ramasub@wwu.edu https://visualsphinx.github.io https://hf.co/VisualSphinx"
        },
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VISUALSPHINX, first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VISUALSPHINX benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VISUALSPHINX also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning. Figure 1: Example VISUALSPHINX instances within each reasoning category. Each visual logic puzzle comprises text prompt, graphical question stem with four images and question mark, and four candidate choices of graphical answers. *These authors contributed equally to this work. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Vision language models (VLMs) such as CLIP [29] and GPT-4V [2, 5] have demonstrated impressive capabilities in perception and generation tasks across modalities. These models are capable of generating image captions [26], answer complex questions about images [28], and create images based on textual description [27]. Despite these advances, their reasoning abilities for tasks requiring structured logical inference over visual inputs remain limited [35]. This shortfall restricts their effectiveness in applications like diagram understanding [16] and multimodal decision-making [4], which demand the abstraction and generalization of visual patterns through coherent logic. Recently, Reinforcement learning (RL) has been show to be effective to enhance large language models reasoning capabilities [7, 52]. However, RL is not readily applicable for VLMs due to the lack of large-scale training dataset with verified ground-truth responses. Existing datasets such as Geo3K [51] tailored for VLMs are often small in scale, require humans for curation, and lack structured annotations aligned with ground-truth. In this paper, we bridge the gap by proposing VISUALSPHINX, large-scale synthetic dataset designed to train and benchmark VLMs for logical reasoning tasks. VISUALSPHINX consists of over 660K automatically generated logical visual puzzles. Each logical puzzle is grounded with an interpretable rule and accompanied by both correct answers and plausible distractors. To create this dataset, we propose rule-level genetic algorithm to ensure diverse puzzle types, spanning inductive, deductive, spatial, and structural logic, followed by program-based image synthesis with varied styles to enhance visual diversity. Quality is maintained through VLM-based labeling, deduplication, and removal of low-quality entries. Notably, the entire dataset is generated at cost of less than $1000, demonstrating exceptional scalability and cost-efficiency. Figure 1 provides puzzle examples covering diverse reasoning categories in VISUALSPHINX. More examples and other statistics are listed in Appendix A. We perform GRPO on QWEN2.5-VL-7B model [36] with VISUALSPHINX. Experimental results demonstrate that the fine-tuned model significantly improves its accuracy in solving visual logic puzzles and exhibits transferable gains in other reasoning tasks on MathVista [21], including algebraic, arithmetic, and geometric reasoning. These improvements highlight the strong generalizability and robustness of VISUALSPHINX for enhancing VLM reasoning across diverse multi-modal tasks. This paper is structured as follows. Section 2 reviews related work on logical reasoning and synthetic dataset generation. Section 3 describes the VISUALSPHINX data curation pipeline. Section 4 presents statistical analysis of the generated datasets quality. Section 5 evaluates the performance of model trained on VISUALSPHINX, demonstrating its effectiveness for enhancing VLM reasoning. We discuss the limitations in Section 6 and conclude our paper in Section 7."
        },
        {
            "title": "2 Related Work",
            "content": "Logical Reasoning. Logical reasoning is fundamental capability for large language models (LLMs). While existing research in math and coding seeks to measure this skill, such tasks often require substantial domain knowledge, making it challenging to isolate reasoning from subject expertise [8, 6]. To address this, several studies focus on minimizing background knowledge demands. For instance, [20] employs multiple-choice logical comprehension questions, and [17] reformulates classic riddles into commonsense logic problems. Other works explore more structured puzzles, extending to programming challenges [32] and games such as Game of 24 [45], chess [11], and crosswords [31]. Multimodal benchmarks also consists of logical puzzles; for example, [22] includes visual logic problems, while recent studies [30, 42, 50] extensively probe the ability of vision-language models (VLMs) to solve multimodal logic puzzles. Collectively, these resources isolate inference ability while reducing reliance on specialized knowledge. Post-Training Dataset for VLMs. Pretrained vision-language models (VLMs) require post-training to acquire the capability to solve complex tasks. Prior work has constructed large-scale instructiontuning datasets, such as those by [19, 46], to teach models to follow human instructions effectively. Subsequently, more targeted reinforcement learning (RL) is employed to further align VLMs with human feedback and enhance their long chain-of-thought reasoning capabilities [44, 24, 49]. In contrast to these efforts, our VISUALSPHINX dataset represents the first multimodal training dataset specifically tailored for multimodal logical reasoning tasks. 2 Figure 2: This figure illustrates the four-stage pipeline for generating VISUALSPHINX. In Step 1, we collect 4K seed puzzles with explanations and abstract them into structured rule descriptions using LLMs. In Step 2, we apply rule-level genetic algorithm to cross over, mutate and diversify the seed rules, scaling them to 40K high-quality rules. In Step 3, each rule is paired with rendering style and used to generate five correct and three incorrect images via LLM-generated Python scripts. The fifth correct image is designated as the answer option, while the three rule-breaking images serve as distractors. After deduplication, we obtain 110K image groups. In Step 4, we assemble puzzles from each group using three complementary strategies: default assembly, shuffled answer variants, and expanded distractor sets. This results in over 660K visual logic puzzles, enabling robust and diverse training for multimodal reasoning models. Dataset Synthesis for VLM Training. Dataset synthesis has been widely used to expand LLM training resources by distilling knowledge from strong LLMs or refining their generated data [38, 41, 43]. To close the gap in high-quality imagetext instruction datasets, prior works [48, 3, 10, 23] incorporate human annotators and/or strong VLMs into the synthesis pipeline, thereby enhancing the quality of large-scale synthetic datasets for VLM training."
        },
        {
            "title": "Data for Visual Reasoning",
            "content": "Overview of VISUALSPHINX Pipeline. In what follows, we describe our scalable and efficient visual data generation method to synthesize visual logic puzzles with correct answers for RL training. Each visual logic puzzle consistently comprises text prompt, graphical question stem with four images and question mark, and four candidate choices of graphical answers. As shown in Figure 2, the pipeline consists of four steps: (1) seed question collection & rule abstraction, (2) rule expansion via rule-level genetic algorithm, (3) program based rule-to-image synthesis and (4) puzzle assembly. The VISUALSPHINX pipeline is fully automated and generates diverse visual logic puzzles across multiple pattern categories for multimodal RL training. 3 Figure 3: Comparison of t-SNE visualization between VISUALSPHINXs synthetic rules and seed rules with 2,000 sampled rules per class. Details about 8 classes are provided in Appendix B.1. Figure 4: The statistics of synthetic rules format, content quality and feasibility score. Step 1: Seed Question Collection & Rule Abstraction. We initialize the pipeline by collecting and transforming visual logic questions into structured seed rules for scalable rule expansion. We first collect 4K visual logic questions together with their authored explanations from Chinese Civil Service Examination. Using LLM, we translate all explanations into English, rewrite them to eliminate answer leakage, and enrich them with clarifying details. We then manually deduplicate the dataset and perform consistency check by feeding each imageexplanation pair to an LLM to verify correct answer retrieval. This process leads to filtered set of 2.4K high-quality seed questions. Each of these remaining questions is then abstracted into structured seed rule using an LLM, comprising five concise bullet points capturing the core visual pattern. Finally, the 2.4K seed rules are categorized into 8 classes with an LLM based on their visual patterns and required reasoning skills. These classes specify the label space of subsequent evolutionary sampling. Detailed descriptions of 8 classes are provided in Appendix B.1. The detailed prompts for rewriting, verification, rule abstraction, and categorization can be found in Appendix F.1. Step 2: Rule Expansion via Genetic Algorithm. Rule Expansion. To scale up the 2.4K seed rules, we introduce rule-level genetic algorithm. Each class of seed rules forms subpopulation, evolving independently on separate islands through genetic operations: Mutation rewrites, adds, or deletes individual bullets, while crossover interleaves bullets from two parent rules. Every three generations, 10% of the rules migrate across islands to maintain diversity. After ten generations, we obtain 60K candidate rules. The detailed prompts for crossover and mutation can be found in Appendix F.2. To evaluate the diversity of generated rules in this step, we perform t-SNE [37] on the 60K candidate rules before final filtering to 40K, as this provides comprehensive view of the rule space prior to pruning. We encode all rules using the all-mpnet-base-v2 sentence embedding model1 and apply t-SNE for dimensionality reduction, as shown in Figure 3. We observe that VISUALSPHINXs synthetic rules (shown in color) exhibit broader distribution than seed rules (in gray), which cluster primarily in the upper right region, demonstrating broad and well-separated distribution across the rule space. This analysis confirms that our genetic algorithm effectively diversifies the rule set, enhancing the variety of logical patterns available for puzzle generation. Rule Filtering. To ensure these rules are meaningful and suitable for image synthesis in the subsequent step, we implement rigorous evaluation and filtering process. First, we use the allmpnet-base-v2 embedding model to project all rules into an embedding space and compute nearestneighbor distances using FAISS [9]. Rules exceeding predefined similarity threshold with existing entries are removed to eliminate redundancy and promote diversity. Deduplication details are in Appendix B.2. Second, we use an LLM to rank the remaining rules based on three criteria: format 1https://huggingface.co/sentence-transformers/all-mpnet-base-v2 (adherence to the structured five-bullet-point template), content quality (clarity and logical coherence of the rule), and feasibility for code generation (suitability for generating Python scripts to render images). Each criterion is scored from 1 to 5, and we retain only the top 40K rules that achieve total score above 12 and feasibility score of at least 3. The score distribution can be found in Figure 4, and the prompt we used can be found in Appendix F.3. This filtering ensures that the selected rules are not only diverse but also interpretable and practically viable for generating visually distinct and logically consistent puzzles, thereby facilitating effective image synthesis in the following step. Step 3: Program Based Rule-to-Image Synthesis. Rule-to-Image Synthesis. For each of the 40K retained rules from Step 2, an LLM is prompted to generate two Python scripts in single turn: correct script.py renders five sequential images that adhere to the rule, while incorrect script.py produces three plausible but rule-violating distractors. The prompt template can be found in Appendix F.4. Each rule yields one image group, containing total of eight images (five correct and three incorrect). Within each group, four correct images serve as the question stem, the fifth correct image is the correct answer option, and the three incorrect images serve as distractors. These groups are used for puzzle assembly in Step 4. Image Style Diversification. To enhance visual diversity and expand the dataset scale, VISUALSPHINX employs three distinct rendering styles, each leveraging unique combinations of Python libraries, including Matplotlib, PIL, and NumPy, to create varied graphical compositions. For each of the 40K rules from Step 2, we generate images in all three styles, tripling the dataset to approximately 120K image groups. This approach encourages models to focus on abstract relational patterns rather than overfitting to specific visual features. Prompt templates of different rendering styles can be found in Appendix F.4. De-duplication. To ensure the quality and uniqueness of the 120K image groups, particularly to avoid identical images appearing among the question, correct option, or distractors, we implement rigorous deduplication and validation pipeline. We use Perceptual Hashing (pHash) [13] to compute hash values for each image within group, measuring Hamming distances between all pairs. Images with distances below 10 are removed as duplicates. To identify low-quality images (e.g., those whose figures are too small), we compute the Structural Similarity Index (SSIM) [39] against white reference image, flagging images with scores below 0.1 as blank, and calculate grayscale gradient energy, discarding images below calibrated threshold. Groups with any image failing these criteria are removed, while those with high Hamming distances are retained if otherwise valid, as they may represent meaningful puzzles. This pipeline reduces the dataset to 110K groups. Step 4: Puzzle Assembly. Default Assembly. From each verified group, our method constructs four-option puzzle: the first four correct images, along with question mark, constitute the question stem; the fifth correct image serves as the correct answer, while the three incorrect images are used as distractor options. All four options are randomly shuffled. The objective of the puzzle is to select the image that best completes the pattern indicated by the stem. Alternative Assembly I: Answer Shuffling. To increase diversity and mitigate position bias, we introduce shuffled answer variant. For each image group, we construct four puzzles with identical question stems but different answer orderings, such that the correct image appears in options A, B, C, or D, respectively. This approach enriches training diversity by discouraging models from learning positional patterns. Only one variant is sampled per training instance to prevent memorization. Alternative Assembly II: Expanded Distractor Set. To enhance difficulty and promote robust visual reasoning, we introduce 10-option puzzle variant. Instead of using only the three incorrect distractors generated within the image group, we augment the option set with six additional distractors sampled from two other image groups associated with genetically related rules (e.g., parent or ancestor rules from Step 2s rule-level genetic algorithm, rendered in the same code style). This creates puzzles with one correct option and nine plausible but rule-violating distractors, forming challenging selection task. 5 Combining all puzzle construction strategiesincluding the default assembly, answer shuffling, and distractor expansionthe pipeline generates the VISUALSPHINX dataset, comprising over 660K vision-logic puzzles and providing rich and diverse training corpus for multimodal model RL. The license of VISUALSPHINX is CC-BY-NC 4.0."
        },
        {
            "title": "4 Dataset Analysis",
            "content": "In this section, we conduct comprehensive analysis to demonstrate the effectiveness of VISUALSPHINX pipeline in generating high-quality, readable and logically coherent visual puzzles, as well as the low cost of generating the dataset. 4.1 Dataset Attribute Analysis We analyze the quality and difficulty of the 660K synthetic puzzles in VISUALSPHINX using three evaluation attributes: readability, logical coherence, and pass rate. Attribute: Readability. This metric evaluates the visual clarity and layout of puzzles generated in Step 4. It assesses whether the spatial arrangement, image scaling, or aspect ratios of the puzzle grid hinder understanding. An LLM is prompted to rate each puzzles readability on scale of 1 to 5, with the score distribution shown in Figure 5 (red). Results indicate that 93.1% of the 660K puzzles achieve score of 4 or higher, reflecting high visual clarity and robust layout fidelity across the large-scale dataset. The prompt for annotation can be found in Appendix F.5. Figure 5: This figure demonstrates the statistics of readability and logical coherence. Attribute: Logical Coherence. This metric measures the logical consistency between each puzzle and its underlying rule. An LLM, provided with the rule used during puzzle generation, scores each puzzles alignment with the intended visual pattern on 1-to-5 scale. The score distribution, presented in Figure 5 (blue), shows that 89.8% of puzzles achieve score of 4 or higher, indicating strong logical consistency even at scale. This confirms that most puzzles remain faithful to their rules, supporting robust training for multimodal reasoning. The prompt for annotating logical coherence can be found in Appendix F.5. Attribute: Pass Rate. We measure the pass rate using VLM. We note that current open-source VLMs not trained on these tasks struggle to achieve useful pass rates on visual logic puzzles and like random guess. To evaluate the solvability of the generated puzzles, we apply GRPO [33] to train an annotation model based on QWEN2.5-VL-7B, using our filtered seed questions. We then evaluate this model on full set of 110,000 filtered image groups (generated using default assembly). Detailed training hyperparameters are provided in Appendix E. The resulting pass rates are shown in Figure 6. Notably, 14,000 puzzles are never correctly solved (accuracy of 0), indicating that significant portion of the dataset poses novel and non-trivial challenges for the model. This highlights the datasets potential for testing generalization and reasoning capabilities in vision-based logical reasoning tasks. The remaining pass rates are distributed relatively evenly across difficulty bins, reflecting diverse range of challenge levels and further validating the utility of the VISUALSPHINX dataset for RL training. Figure 6: This figure demonstrates the statistics of pass rates. More Analysis. We defer detailed human-based quality analysis to Appendix D. Manual evaluation by human annotators confirms VISUALSPHINXs high quality, complementing automated VLMbased assessments. 6 4.2 Cost & Model Selection Analysis Our pipeline generates the VISUALSPHINX of over 660K high-quality visual logic puzzles at total cost of less than $1000, averaging approximately $0.0015 per puzzle. We select models for each step based on their reasoning capabilities, modality requirements, and cost efficiency, ensuring optimal performance while maintaining low costs. More details for model selection can be found in Appendix C. Step 1: Seed Question Processing. We employ Claude-3.5-Haiku and Claude-3.7-Sonnet [1] for multimodal tasks including translating and rewriting explanations, verifying image-text answers, abstracting rules, and classifying 2.4K seed questions. Each question is processed with four prompts at cost of $8 per 1K questions, totaling approximately $77 for 2.4K questions. Step 2: Rule Expansion. We use DeepSeek-V3 [18], text-based model, for rule expansion via rule-level genetic algorithm, scaling 2.4K seed rules to 60K synthetic rules. At $2 per 1K rules, the cost totals approximately $120. Step 3: Image Synthesis. We utilize Grok-3-mini [40] with high reasoning effort to generate two Python scripts per ruleone rendering five correct images, another producing three distractorsensuring consistent script generation for 120K image groups. At $6 per 1K groups, the cost is approximately $720. Step 4: Puzzle Assembly and Evaluation. For quality assurance, we evaluate 110K puzzles assembled using the default assembly strategy. fine-tuned Qwen2.5-VL-7B-Instruct model assesses pass rate, while GPT-4.1-mini, lightweight vision-language model, evaluates visual readability and logical coherence at $0.70 per 1K puzzles, totaling approximately $77."
        },
        {
            "title": "5 Performance Analysis of VISUALSPHINX on RL",
            "content": "In this section, we evaluate the quality of the VISUALSPHINX datasets by using them to perform RL on vision models, assessing their effectiveness via benchmark performance. 5.1 Experimental Setups VISUALSPHINX Setup. Given the large size of the VISUALSPHINX dataset, we sample 10,000 challenging yet feasible examples, selecting those with pass rates between 0.375 and 0.875, and combined readability and logical coherence score (as defined in Section 4) of 8 or higher. To ensure sufficient diversity and challenge, we include 80% of the questions with 4 options and the remaining 20% with 10 options. RL Setup. We perform training on QWEN2.5-VL-7B [36] using GRPO [33] for 256 steps, with batch size of 128 and 8 responses per sample. binary reward of 1 is assigned to correct responses, and 0 to incorrect ones, determined by comparing the models output with the ground truth answer. No additional reward is assigned for formatting. The training is conducted on cluster of 8 H100 GPUs with 2TB of RAM. Full hyperparameter configurations are detailed in Appendix E. Benchmarks. To validate the effectiveness of improving the performance of vision-based logical reasoning, we construct VISUALSPHINX-TEST, test set of 930 high-quality visual logic puzzles from the VISUALSPHINX dataset. Each puzzle is manually checked for correctness and high quality, comprising both 4-option and 10-option questions. The 930 puzzles are categorized into three difficulty levels based on their pass rates: easy (pass rates between 0.5 and 0.75), medium (pass rates between 0.25 and 0.5), and hard (pass rate of 0). We emphasis that this test set is fully synthetic, ensuring no overlap with existing benchmarks, training datasets, or our sampled training data to prevent data contamination. In addition, we use the widely adopted MathVista-testmini dataset [21] to evaluate the general visual reasoning ability of the RL-trained vision model with VISUALSPHINX. MathVista-testmini consists of 1,000 visual math problems designed to assess mathematical reasoning in visual contexts, covering diverse task types and reasoning skills. We report the overall average accuracy, as well as accuracies for subcategories, including task typesfigure question answering (FQA), geometry problem solving (GPS), math word problem (MWP), textbook question answering (TQA), and visual question answering (VQA), and mathematical reasoning typesalgebraic reasoning (ALG), 7 Table 1: Accuracy of Vision Models on VISUALSPHINX-TEST Across Difficulty Levels. QWEN2.5-VL-7B-VISUALSPHINX represents our RL-trained model fine-tuned on VISUALSPHINX, which outperforms the baseline QWEN2.5-VL-7B and most competing models, achieving the highest overall accuracy. Numbers with the best performance for each difficulty level are highlighted in bold, while those with the second-highest performance are underlined. Model Overall (%) Easy (%) Medium (%) Hard (%) Grok-2-Vision Gemini-1.5-Pro Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Pro GPT-4o GPT-4.1 o4-mini Claude-3.5-Haiku Claude-3.7-Sonnet Claude-4-Sonnet Claude-4-Opus 32.73 42.99 42.78 49.20 50.27 49.30 55.72 41.93 35.72 48.88 46.31 50. 41.99 50.32 50.00 58.01 60.26 55.45 66.67 44.87 45.51 55.77 52.24 54.17 30.97 40.00 43.55 48.71 49.35 49.68 54.19 47.74 34.52 49.03 44.19 50.98 25.24 38.66 34.82 40.89 41.21 42.81 46.33 33.22 27.16 41.85 42.49 46.33 QWEN2.5-VL-7B QWEN2.5-VL-7B-VISUALSPHINX 29.30 55.94 +26.64 42.31 76.92 +34. 23.87 52.26 +28.39 21.73 38.66 +16.93 Figure 7: This figure illustrates the performance of the model trained on VISUALSPHINX, evaluated on the VISUALSPHINX-TEST across varying difficulty levels. The performance consistently increases with training, demonstrating the effectiveness of VISUALSPHINX for RL. arithmetic reasoning (ARI), geometry reasoning (GEO), logical reasoning (LOG), numeric common sense (NUM), scientific reasoning (SCI), and statistical reasoning (STA). 5.2 Experimental Results VISUALSPHINX is Effective in Increasing the Logical Reasoning Ability of VLMs. Table 1 and Figure 7 compares the performance of models before and after RL training with VISUALSPHINX on VISUALSPHINX-TEST across difficulty levels. Our results demonstrate that VISUALSPHINX significantly enhances model performance across all difficulty levels, enabling the RL-trained model to surpass closed-source models such as GPT-4.1 and Claude-3.7-Sonnet in logical reasoning tasks. These findings confirm the effectiveness of VISUALSPHINX in improving VLM reasoning capabilities. VISUALSPHINX Can Further Enhance Performance in Other Domains. Figure 8 compares the model performance before and after RL training on VISUALSPHINX, evaluated on the MathVista-testmini benchmark. We observe that the average accuracy on this benchmark increases from 59.4% to 64.0%. Moreover, significant performance improvements are observed across most subcategories, with particular emphasis on logical reasoning (LOG), which exhibits 8 substantial gains. These results highlight VISUALSPHINXs potential to enhance VLM performance in diverse domains beyond its primary focus, including algebraic reasoning, arithmetic reasoning and geometry reasoning."
        },
        {
            "title": "6 Limitations and Ethical Considerations",
            "content": "Limitations. While VISUALSPHINX demonstrates significant empirical success in enhancing the logical reasoning capabilities of VLMs through RL, the underlying mechanisms driving these improvements remain underexplored. Our work primarily focuses on the generation and application of large-scale synthetic dataset to improve VLM performance on visual reasoning. Additionally, the current scope of VISUALSPHINX is limited to predefined categories of logical reasoning from seed questions, such as inductive, deductive, spatial, and structural logic. While these categories cover broad range of visual reasoning tasks, they may not fully capture the complexity of real-world multi-modal reasoning scenarios. Ethical Considerations. We have carefully reviewed the development and potential applications of VISUALSPHINXand have not identified any specific ethical issues associated with this work."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "Figure 8: This figure shows model performance before and after RL training on VISUALSPHINX, evaluated on MathVistatestmini across subcategories. In this work, we introduced VISUALSPHINX, pioneering large-scale synthetic dataset comprising over 660K visual logic puzzles designed to enhance the logical reasoning capabilities of visionlanguage models (VLMs) through reinforcement learning (RL). Our innovative four-stage pipeline leverages rule abstraction, rule-level genetic algorithms, program-based image synthesis, and strategic puzzle assembly to generate diverse and interpretable visual reasoning tasks at cost of less than $1000. Our empirical results demonstrate that fine-tuning the QWEN2.5-7B-VL model significantly improves performance on visual logic puzzles and yields transferable benefits to other visual reasoning tasks. VISUALSPHINX addresses critical gap in the availability of large-scale, structured datasets for multimodal reasoning. Future work could extend VISUALSPHINX to incorporate more complex reasoning paradigms, such as temporal or interactive tasks, and investigate the theoretical underpinnings of its effectiveness."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is partially supported by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, the Office of Naval Research (ONR) under grant N0014-23-1-2386, and the National Science Foundation (NSF) AI Institute for Agent-based Cyber Threat Intelligence and Operation (ACTION) under grant IIS 2229876. This work is supported in part by funds provided by the National Science Foundation, Department of Homeland Security, and IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF or its federal agency and industry partners. Results presented in this paper were partially obtained using the Chameleon testbed [15] supported by the National Science Foundation."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family, 2024. [2] Jimmy Carter. Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/ textocr-gpt4v, 2024. [3] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024. [4] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, and Baobao Chang. Towards end-to-end embodied decision making via multimodal large language model: Explorations with gpt4-vision and beyond. arXiv preprint arXiv:2310.02071, 2023. [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. 2024. [10] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. [11] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Henry Mguni, Yali Du, and Jun Wang. ChessGPT: Bridging policy learning and lanIn Thirty-seventh Conference on Neural Information Processing Systems guage modeling. Datasets and Benchmarks Track, 2023. [12] Google DeepMind. Gemini Pro. https://deepmind.google/technologies/gemini/ pro/, March 2025. Accessed: 2025-04-03. [13] Azhar Hadmi1, William Puech1, and Brahim Ait Es Said. Perceptual image hashing. Watermarking: Volume 2, page 17, 2012. [14] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [15] Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, Francois Halbah, Alex Rocha, and Joe Stubbs. Lessons learned from the chameleon testbed. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pages 219233. USENIX Association, July 2020. [16] Xue Li, Yiyou Sun, Wei Cheng, Yinglun Zhu, and Haifeng Chen. Chain-of-region: Visual language models need details for diagram analysis. In The Thirteenth International Conference on Learning Representations, 2025. [17] Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 15041515, Online, August 2021. Association for Computational Linguistics. 10 [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. [20] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. [21] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. [23] Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, et al. Mmevol: Empowering multimodal large language models with evol-instruct. arXiv preprint arXiv:2409.05840, 2024. [24] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [25] Meta AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, April 2025. Accessed: 2025-04-05. [26] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. [27] OpenAI. Introducing 4o image generation. OpenAI Website, March 2025. Accessed: 202505-16. [28] OpenAI. Openai o3 and o4-mini system card. OpenAI Website, April 2025. Accessed: 202505-16. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable In International conference on machine visual models from natural language supervision. learning, pages 87488763. PmLR, 2021. [30] Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Susstrunk, and Filippos Kokkinos. Vgrp-bench: Visual grid reasoning puzzle benchmark for large visionlanguage models. arXiv preprint arXiv:2503.23064, 2025. [31] Joshua Rozner, Christopher Potts, and Kyle Mahowald. Decrypting cryptic crosswords: Semantically complex wordplay puzzles as target for NLP. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [32] Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai. Programming puzzles. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 11 [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. [35] Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. [36] Qwen Team. Qwen2.5-vl, January 2025. [37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [38] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada, 2023. Association for Computational Linguistics. [39] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [40] xAI. Grok 3 beta the age of reasoning agents. https://x.ai/blog/grok-3, 2025. Accessed: 2025-02-21. [41] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. [42] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [43] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [44] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [45] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [46] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang. UReader: Universal OCR-free visually-situated language understanding with multimodal large language model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 28412858, Singapore, December 2023. Association for Computational Linguistics. [47] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [48] Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, et al. Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model. arXiv preprint arXiv:2407.07053, 2024. [49] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025. 12 [50] Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Puzzlebench: fully dynamic evaluation framework for large multimodal models on puzzle solving. arXiv preprint arXiv:2504.10885, 2025. [51] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework, 2025. [52] Guanghao Zhou, Panjia Qiu, Cen Chen, Jie Wang, Zheming Yang, Jian Xu, and Minghui Qiu. Reinforced mllm: survey on rl-based reasoning in multimodal large language models. arXiv preprint arXiv:2504.21277, 2025."
        },
        {
            "title": "A More Examples and VISUALSPHINX Statistics",
            "content": "A.1 Examples In this section, we demonstrate more examples of VISUALSPHINX. Example 1 in Figure 9 Reasoning type: Inductive (10 options) Rule: The stick figure is required to hold solid object in its hand. Answer: Example 2 in Figure 10 Reasoning type: Deductive (4 options) Rule: Each weather icon in the images shifts progressively to the left across the sequence. Answer: Example 3 in Figure 11 Reasoning type: Deductive (4 options) Rule: The green part in each image rotates counterclockwise in sequence. Answer: Example 4 in Figure 12 Reasoning type: Deductive (4 options) Rule: The number of balls doubles every other image in the sequence. Answer: Example 5 in Figure 13 Reasoning type: Deductive (4 options) Rule: The number of solid and dashed lines doubles sequentially across the images. Answer: Figure 9: Example 1 14 Figure 10: Example 2 Figure 11: Example 3 Figure 12: Example 15 Figure 13: Example 5 A.2 Breakdown of Statistics of VISUALSPHINX Table 2 summarizes the data volume across all stages of the VISUALSPHINX pipeline. Starting from 3,904 seed questions, we obtain 2,398 high-quality seed rules through filtering and verification. These are expanded to 60,339 candidate rules and further refined to 41,287 retained rules after deduplication and LLM-based scoring. Using three distinct rendering styles, we generate over 117K image groups, of which 110,423 remain after filtering out blank or degenerate outputs. From these remaining image groups, we construct puzzles employing three strategies: default 4-option format, an answer-shuffling variant, and 10-option hard-distractor variant. Table 2: Detailed statistics at each stage of the VISUALSPHINX pipeline. Stage Output Count Description Seed Questions Filtered Seed Rules Generated Rules Retained Rules Style 1 Image Groups Style 2 Image Groups Style 3 Image Groups Total Image Groups Filtered Image Groups Default Puzzle Assembly Answer Shuffling Variant 10-option Variant Total Puzzles 3,904 2,398 60,339 41,287 39,307 38,918 39,360 117,585 110,423 110,423 441,692 110, 662,538 Raw visual logic questions After rewriting and verification After rule-level evolution After deduplication and scoring Successfully rendered Successfully rendered Successfully rendered Across all three styles After blank removal 1 puzzle per image group 4 puzzles (different option order) per group 1 puzzle (10-option) per group Across all assembly strategies"
        },
        {
            "title": "B Additional Analysis of Synthetic Rules",
            "content": "B.1 Categorization of Rules We categorize seed rules along two axes: visual pattern and reasoning style. Figure 14 illustrates representative examples from five visual patterns of seeds: Nine-square grid, Horizontal square, Analogy, Two-groups, and Others. Reasoning styles are labeled as Deductive, Inductive, or Others. 16 Figure 14: Five different visual patterns of seeds We note that in principle, these two axes yield 15 possible visual patternreasoning style combinations. However, we collapse all combinations involving an Others tag on either axis into unified Others category for simplicity. Additionally, rules classified as Two-group pattern are all with Inductive reasoning style. Taking these constraints into account, we arrive at 7 valid categories and fallback Others group, resulting in 8 total rule types used for island-based evolution. Table 3 and the following tags summarize the distribution of seed rules across visual patterns and reasoning styles. Table 3: Distribution of rule types based on visual pattern and reasoning style. Entries involving Others in either dimension are collapsed into the final row. Visual Pattern Reasoning Style Count Horizontal square Horizontal square Nine-square grid Nine-square grid Analogy Analogy Two-group Others Deductive Inductive Deductive Inductive Deductive Inductive Inductive 417 447 107 199 143 118 464 499 Visual pattern tags: Nine-square grid 3 3 grid with one cell replaced by question mark. Horizontal square row of 4 6 squares, one of which is missing or marked. Analogy Two groups of three squares, one containing question mark. Two-group Images include numbers to be partitioned into two groups. Others Any layout that does not fit the above definitions. Reasoning style tags: Deductive Each image builds on the previous with direct transformation or rule. Inductive All images reflect shared underlying rule without strict ordering. Others Patterns that fall outside deductive or inductive reasoning. This classification plays critical role in our rule-level genetic algorithm, as combining points from incompatible visual patterns often results in semantic incoherence, even when LLMs are instructed to rewrite the rules according to context. For instance, rule component specific to Nine-square grid may not generalize to Two-group. Such inconsistencies significantly reduce the success rate of downstream rule-to-image synthesis, often producing puzzles that fail to reflect the intended logic. To address this challenge, we assign each rule category to subpopulation that evolves independently on its own island, where genetic operations such as crossover and mutation are applied exclusively within groups sharing the same visual pattern and reasoning style. Every three generations, 5% of the rules are randomly selected to migrate across islands, promoting diversity while preserving the local semantic structure. This island-based design effectively balances semantic coherence with exploratory variation, ensuring the generation of coherent and diverse logical puzzles for VISUALSPHINX. B.2 Deduplication of Rules We quantify the similarity among Step 2 rule-level genetic algorithm to remove repetitive rules. We measure the similarity using minimum neighbor distance in the embedding space. Specifically, we first represent all instructions in the embedding space using the all-mpnet-base-v2 embedding model. Then calculate the minimum distance from the rules to its nearest neighbors in the embedding space using FAISS. The minimum neighbor distances of rules in Step 2 after removing repetitions are summarized in Figure 15. Figure 15: Min Rules Embedding Distance. However, we believe that deduplication should be performed at the rule level in Step 2, rather than post hoc at the image or puzzle level (e.g., after Step 4). While it may seem appealing to apply similar embedding-based similarity check to image content using CLIP or SigLIP representations [47], such visual-level deduplication is fundamentally unreliable for logic-based puzzles. As illustrated in Figure 16, some puzzlesthough visually similar due to shared graphical componentsencode entirely different reasoning structures and target different answers. In the shown example, two puzzles share nearly identical visual layouts and elements, leading to an extremely small SigLIP-based distance (less than 104), yet they represent distinct logic rules. As result, any deduplication strategy applied at the visual or puzzle level would mistakenly discard valid, nonredundant instances. Therefore, semantic-level deduplication must occur immediately after the rule synthesis stage, before these rules are instantiated as images. By enforcing diversity in the rule embedding space at Step 2, we ensure that the downstream generation stages produce puzzles that are both logically and visually distinct. Figure 16: Two puzzles that are visually similar but semantically different. 18 Figure 17: Example comparison of five models on the same rule prompt for image generation."
        },
        {
            "title": "C Model Selection",
            "content": "We choose models for each stage of the pipeline based on their availability, modality alignment, reasoning capability, and cost efficiency at the time of experimentation. Step 1. When we conducted our experiments, leading models such as GPT-4.1 and Gemini 2.5 [12] had not yet been released or lacked stable APIs. Among the accessible multimodal models, the strongest options were OpenAIs o1 [14] and Anthropics Claude-3.7-Sonnet series. We selected Claude-3.5-Haiku and Claude-3.7-Sonnet for their relatively lower costs while maintaining competitive vision-language performance. These models were used to perform image-text verification, explanation rewriting, and rule abstraction for the initial 4K seed questions. Step 2. Since this stage only requires text-based reasoning, the candidate space was significantly broader. We selected DeepSeek-V3 due to its strong logical consistency, long-context support, and highly competitive pricing. It was used to power the rule-level genetic algorithm, enabling the expansion from 2.4K seed rules to 60K synthetic rules. Step 3. This is the most challenging stage for model selection. Each prompt-response interaction involves generating two Python scripts per rule (correct and distractor), with an average token usage of 2K (non reasoning model) or 12k (reasoning model) per example. The model must not only preserve program logic, but also ensure visual diversity, alignment to abstract rules, and robustness to execution. We evaluated several candidate models for this task: Claude-3.5-Haiku, DeepSeek-V3, Gemini2.5-flash, Grok-3-mini and LLaMA-4-Maverick [25]. As illustrated in Figure 17, we show one representative image (out of eight) from each group generated by five different models on the same input prompt. Both Grok-3-mini and Gemini-2.5-flash produce rule-compliant outputs and render images correctly. Among the models we qualitatively compared, Grok-3-mini showed higher success rate in generating correct and executable scripts, with fewer rendering failures or visual artifacts. Given its reliability and cost-effectiveness, we selected Grok-3-mini as the default model for rule-to-image script generation. Step 4. We use GPT-4.1-mini for scoring readability and logical coherence across 110K assembled puzzles. This model offers strong visual-language understanding at minimal cost, making it ideal for large-scale quality control. In parallel, custom fine-tuned version of Qwen2.5-VL-7B-Instruct is used to estimate model PassRate."
        },
        {
            "title": "D Puzzle Quality Analysis",
            "content": "We annotate the puzzles to evaluate the quality, comparing it against seed puzzles. We randomly sampled total of 1200 puzzles, with 400 puzzles from each difficulty level (easy, medium, and hard), ensuring an even distribution across difficulty levels. Our annotations assessed that 935 puzzles (78%) were of comparable or superior quality to the seed puzzles. This high rate indicates that the generated puzzles in VISUALSPHINX maintain high quality of logical coherence and complexity, making them suitable for training and evaluating VLMs."
        },
        {
            "title": "E Experimental Setups for RL Training",
            "content": "We conduct our reinforcement learning (RL) training using the hyperparameter detailed in Table 4, with experiments performed on 8 H100 GPUs. The model is trained using the VERL framework [34]. For the reward design, we assign binary rewards: 1 for correct response and 0 for an incorrect response, without providing additional rewards for formatting. For each question, we append the instruction: Lets think step by step and output the final answer within boxed{}. Table 4: This table shows the hyper-parameters for RL training. Hyper-parameter Learning Rate Number of Steps Number of Devices Rollout Batch Size PPO Mini Batch Size Max Prompt Length Max Response Length KL Coefficient Rollout Engine Optimizer Learning Rate Scheduler Warmup Ratio Value 1 106 256 8 64 64 6000 2048 0.001 VLLM (V0.8.2) Adamw cosine 0."
        },
        {
            "title": "F Prompt Templates",
            "content": "F.1 Prompt For Step 1 Figure 18-21 demonstrate the prompt template for Step 1 of our pipeline. F.2 Prompt For Step 2 - Rule Expansion Figure 22-23 demonstrate the prompt template for Step 2 rule-level genetic algorithm. F.3 Prompt For Step 2 - Scoring Figure 24 demonstrates the prompt template for scoring synthetic rules. F.4 Prompt For Step 3 Figure 25-27 demonstrate the prompt template for three rule-to-image rendering styles. F.5 Prompt for Annotation Figure 28 demonstrates the prompt template for scoring assembled puzzles."
        },
        {
            "title": "Prompt for rewriting explanation and translating into English",
            "content": "You will be translating Chinese explanation of vision logic puzzle into English while making certain modifications. Follow these steps to complete the task: 1. Carefully read and understand the entire Chinese explanation. 2. Translate the explanation from Chinese to English, ensuring that you capture the full meaning and logical relationships in the original text. 3. As you translate, remove any statements that indicate the answer. 4. Remove phrases like \"\" (as shown in the following figure) or any similar references to figures or images. When you remove these phrases, make sure to maintain the original meaning of the statement. For example, if the original text says \"\" (as shown in the following figure, there are three circles), your translation should simply state \"There are three circles.\" 5. Ensure that you fully understand the logical flow and reasoning of the explanation before finalizing your translation. The goal is to provide clear, coherent explanation in English that preserves the puzzle's logic without giving away the answer or referring to non-existent figures. 6. Write your final translated and modified explanation in English inside <translated_explanation> tags. Remember, the key is to provide faithful translation that maintains the puzzle's logic and difficulty while removing direct answer indicators and references to figures. For example 1, the text you are going to translate is: <chinese_explanation>  123 3DD </chinese_explanation> Then your output is: <translated_explanation> The elements have different compositions with no obvious attribute patterns, so consider quantity patterns. In the second row of the grid, single line appears. Consider counting lines, but the overall line count lacks clear pattern. Consider more refined approach by examining line directions. In the first row, the first image has 1 group of parallel lines, the second image has 2 groups of parallel lines, and the third image has 3 groups of parallel lines. The second row follows similar pattern, and the first two images in the third row also conform to this pattern. Therefore, the question mark location should follow this progression of parallel line groups. </translated_explanation> For example 2, the text you are going to translate is: <chinese_explanation>   =  \"\" A </chinese_explanation> Then your output is: <translated_explanation> The elements have similar compositions, examining both black and white overlays and the concept of preserving differences while removing similarities. In this grid, the first and third rows show the first two images combining to create the third image. The color overlay rule is: white + black = white, black + white = black, white + white = black, black + black = white. The rule for the upper line of the spherical shape is the elimination of the same and retention of the different. The difficulty of this question is that \"white + black\" does not equal \"black + white\", and attention should be paid to this. </translated_explanation> Now, please translate the following explanation <chinese_explanation> {chinese_explanation_placeholder} </chinese_explanation> Figure 18: Prompt for rewriting and translating"
        },
        {
            "title": "Prompt for verifying the question",
            "content": "## Task Description You will be given an image of vision logic puzzle. Your task is to analyze the image, understand the puzzle's logic, and provide detailed explanation of how to solve it. ## Input <image> <!--SPLIT--> </image> <question> {{question}} </question> <options> {{options}} </options> <hint> {{hint}} </hint> ## Instructions 1. Carefully examine the image, paying attention to all visual elements, patterns, and relationships between objects. 2. Consider the question (and hint) to guide your analysis. 3. Identify the underlying logic or rule that governs the puzzle. 4. Apply this logic to determine the correct answer. 5. Explain your reasoning step by step, referencing specific elements in the image. 6. **Answer in English.** ## Output <reasoning> Your reasoning. </reasoning> <answer> Your answer (select only from: A, B, C, D, or Problematic) </answer> Figure 19: Prompt for verifying the question"
        },
        {
            "title": "Prompt for abstracting the puzzle into pullet points",
            "content": "## Task Description You are an expert in analyzing and summarizing complex vision logic puzzles. Your task is to extract and present the key regularities and patterns from given puzzle and its explanation. The goal is to create concise list of regularities that captures the essential elements of the puzzle and its solution, which could be used to generate similar puzzles. Now, follow these steps to analyze the puzzle and extract key regularities: 1. **Analyze** the puzzle and its explanation thoroughly. 2. **Create** detailed breakdown of the puzzle inside `<detailed_analysis>` tags ... 3. **Present** your analysis within `<puzzle_breakdown>` tags ... 4. **Create** list of key regularities within `<key_points>` tags ... 5. **Review & refine** your list ... ### Example Examine the image of the puzzle: <image><!--EXAMPLE_SPLIT--></image> Read the question: <puzzle_question> From the given four options, select the most suitable one to fill in the question mark to present certain regularity: </puzzle_question> Review the options: <puzzle_options> \"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\" </puzzle_options> Read the explanation: <puzzle_explanation> The elements have similar compositions ... *[omitted for brevity]* </puzzle_explanation> Note the answer: <puzzle_answer> </puzzle_answer> The model answer is: <detailed_analysis></detailed_analysis> <puzzle_breakdown></puzzle_breakdown> <key_points> - Inner shapes become outer shapes in the subsequent panel - Shapes alternate between curved and straight-edged forms - Each panel contains exactly two nested geometric shapes - Multiple pattern layers must be satisfied simultaneously - Shape orientation varies while maintaining structural patterns </key_points> ## Input Examine the image of the puzzle: <image> <!--PUZZLE_SPLIT--> </image> Read the question: <puzzle_question> {{ prompt }} </puzzle_question> Review the options: <puzzle_options> {{ options_block }} </puzzle_options> Read the explanation: <puzzle_explanation> {{ explanation }} </puzzle_explanation> Note the answer: <puzzle_answer> {{ correct_answer }} </puzzle_answer> ## Output Your final output **must** follow this exact structure: <detailed_analysis> [Your detailed analysis here] </detailed_analysis> <puzzle_breakdown> [Your structured breakdown here] </puzzle_breakdown> <key_points> - [Regularity 1] - [Regularity 2] - ... (max 5 items, each less than 30 words) </key_points> Figure 20: Prompt for abstracting the puzzle into pullet points"
        },
        {
            "title": "Prompt for categorization",
            "content": "## Task Description You are an expert system designed to analyze and classify complex visual logic puzzles. Your task is to examine the given puzzle components and determine **one** question-type tag and **one** knowledge-point tag from the predefined lists. ### Question-type tags - **Nine-square grid** 3 3 grid with one cell containing question mark. - **Horizontal square** row of 4{6 squares with one containing question mark. - **Two-group** Two groups of three squares; one square of one group has question mark. - **Two set of number** Images with numbers that must be divided into two sets. - **Others** Doesnt fit any category above. ### Knowledge-point tags - **Correlated** Each image is directly related to the previous one. - **Summarize** All images share an overall rule but adjacent images may not correlate. - **Others** Doesnt fit any category above. ## Analysis instructions 1. Examine every puzzle component. 2. Focus on structure and relationships. 3. Weigh arguments **for and against** each tag. 4. Provide detailed reasoning inside `<puzzle_breakdown>` before giving tags. ## Input Here is the puzzle you need to analyze: <puzzle_image> <!--PUZZLE_SPLIT--> </puzzle_image> <puzzle_question> {{ prompt }} </puzzle_question> <puzzle_options> {{ options_block }} </puzzle_options> <puzzle_explanation> {{ explanation }} </puzzle_explanation> <puzzle_answer> {{ correct_answer }} </puzzle_answer> ## Output (required structure) <puzzle_breakdown> [Your detailed reasoning here] </puzzle_breakdown> <question_type> [Selected question-type tag] </question_type> <knowledge_point> [Selected knowledge-point tag] </knowledge_point> Figure 21: Prompt for categorization"
        },
        {
            "title": "Prompt for crossover",
            "content": "## Task Description You are an expert in analyzing and synthesizing complex vision logic puzzles. Your task is to crossover and combine two sets of puzzle regularities to create novel, coherent rule set that could use to generate new puzzles. Rather than mechanically combining individual points, you need to understand the underlying logic and context of both rule sets to create meaningful hybrid patterns that maintain internal consistency. Please follow these steps: 1. Analyze both rule sets: - List out key elements from each rule set - Identify core principles, compatibility, and potential synergies - Look for opportunities to create emergent patterns through combination - Identify potential conflicts and propose resolutions 2. Wrap your detailed analysis in <comparative_analysis> tags: - Compare and contrast the rule sets - Identify complementary regularities and explore potential synergies - Consider application to puzzle generation - Analyze and propose resolutions for potential conflicts 3. Wrap your cross-over synthesis in <synthesis> tags: - Explain how you've integrated elements from both rule sets - Describe how the new combined regularities work together - Illustrate potential puzzle scenarios - Explain the coherence and unity of your cross-over - Discuss how it maintains core insights and ensures contextual continuity 4. Wrap your final cross-over rule set in <crossover_rules> tags: - Each rule should be less than 30 words and incorporate elements from both source rule sets - Aim for 4-6 cohesive items that flow logically and create complete puzzle system - Ensure the rules could plausibly generate square-format continuous image puzzles - Check for contextual integrity, preservation of fundamental insights, and logical progression ## Input Here are the two sets of puzzle regularities: <first_rule_set> {FIRST_RULE_SET} </first_rule_set> <second_rule_set> {SECOND_RULE_SET} </second_rule_set> ## Output Your final output should follow this structure: <comparative_analysis> [Your detailed comparative analysis] </comparative_analysis> <synthesis> [Your explanation of the integrated elements and how they work together] </synthesis> <crossover_rules> - [Cross-over Rule 1] - [Cross-over Rule 2] - [Cross-over Rule 3] - [Cross-over Rule 4] - [Cross-over Rule 5] - [Cross-over Rule 6] (optional) </crossover_rules> Remember, your cross-over should be thoughtful integration that preserves context and creates emergent properties, not mechanical merging of the original rules. Figure 22: Prompt for crossover"
        },
        {
            "title": "Prompt for mutation",
            "content": "## Task Description You are an expert in analyzing and evolving complex vision logic puzzles. Your task is to mutate an existing set of puzzle regularities to create novel, coherent rule set that maintains the core essence while introducing interesting variations. Please follow these steps: 1. Analyze the existing rule set: - Identify the core principles and key elements - Understand the underlying logic and structure 2. Wrap your analysis in <analysis> tags: - Describe key insights about the rule structure - Identify opportunities for creative mutation - Explain which aspects should be preserved vs. modified 3. Create mutated version with controlled changes: - Introduce meaningful variations to one point or two points of the rules - Ensure the mutations maintain logical coherence - Preserve the core puzzle structure and logic - Aim for mutation that could generate novel but solvable puzzles 4. Wrap your final mutated rule set in <mutated_rules> tags: - Each rule should be less than 30 words - Include 4-6 rules total (similar to the input set) - Ensure the mutated rules maintain logical consistency - The mutation should be meaningful but not completely transform the puzzle type ## Input Here is the set of puzzle regularities to mutate: <rule_set> {RULE_SET} </rule_set> ## Output Your final output should follow this structure: <analysis> [Your analysis of the existing rules] </analysis> <mutated_rules> - [Mutated Rule 1] - [Mutated Rule 2] - [Mutated Rule 3] - [Mutated Rule 4] - [Mutated Rule 5] - [Mutated Rule 6] (optional) </mutated_rules> Remember, effective mutation preserves the essence of the original while introducing novel and interesting variations. Figure 23: Prompt for mutation"
        },
        {
            "title": "Prompt for scoring synthetic rules",
            "content": "## Task Description You are an expert evaluator tasked with assessing rule for visual logical reasoning question. This rule is designed to guide large language model in writing Python code to generate sequence of images that correspond to the rule. Your evaluation will focus on three key criteria: format, content quality, and feasibility. Before providing your final evaluation, please wrap your detailed analysis for each criterion in <detailed_analysis> tags. This will ensure thorough and transparent assessment. Evaluation Criteria and Rubric: 1. Format (1-5 points) 1: Incomplete, missing multiple points, contains unrelated content 2: Incomplete, missing 1-2 points, may contain some unrelated content 3: Complete set of points, but may have some unrelated content 4: Complete set of 5-6 points, minimal unrelated content 5: Perfect format with 5-6 points and no unrelated content 2. Content Quality (1-5 points) 1: Inconsistent, contradictory, does not describe core concept 2: Some inconsistencies or contradictions, poorly describes core concept 3: Mostly consistent, few contradictions, adequately describes core concept 4: Consistent, no major contradictions, effectively describes core concept 5: Highly consistent, no contradictions at all, perfectly describes core concept 3. Feasibility (1-5 points) 1: Vague, unclear instructions, not suitable for code generation 2: Some clear instructions, but many gaps for code generation 3: Mostly clear instructions, some gaps for code generation 4: Clear instructions, minor gaps for code generation 5: Perfectly clear and specific instructions for easy code generation ## Input Here is the rule you will evaluate: <rule> {RULE} </rule> ## Output Please evaluate the rule using the following structure: <detailed_analysis> Format Analysis: - List key points of the rule relevant to format - Analyze each point in relation to the format scoring rubric - Provide preliminary format score based on the analysis Content Quality Analysis: - List key points of the rule relevant to content quality - Analyze each point in relation to the content quality scoring rubric - Provide preliminary content quality score based on the analysis Feasibility Analysis: - List key points of the rule relevant to feasibility - Analyze each point in relation to the feasibility scoring rubric - Provide preliminary feasibility score based on the analysis </detailed_analysis> <format_evaluation> [Your reasoning for the format score] Score: [Your score from 1-5] </format_evaluation> <content_quality_evaluation> [Your reasoning for the content quality score] Score: [Your score from 1-5] </content_quality_evaluation> <feasibility_evaluation> [Your reasoning for the feasibility score] Score: [Your score from 1-5] </feasibility_evaluation> ## Final Score <format_score> [Score] </format_score> <content_quality> [Score] </content_quality> <feasibility> [Score] </feasibility> Figure 24: Prompt for scoring synthetic rules 27 Prompt for first rule-to-image rendering style ## Task Description As Python imaging expert, create two Python scripts: one generating 5 continuous images that follow set of rules, and another generating 3 images that intentionally break those rules. Infer reasonable implementation according to rules (e.g., based on patterns, shapes, or mathematical properties) and document your reasoning. ## Input Let's review the rules for your images: <rules> {{RULES}} </rules> ## Guidelines - **Libraries**: Use matplotlib and numpy. - **Style**: Black-and-white images, no text. - **Output Paths**: - Correct images: ./output_correct/ - Incorrect images: ./output_incorrect/ - **Image Counts**: 5 correct, 3 incorrect. - **Size**: Adjust the size of elements and images to avoid confusion and unnecessary overlap. - Optionally, sometimes you can use your continuous output images to express some rules that are related between images instead of expressing all in single one image. ## Deliverables Provide two complete, runnable Python scripts: 1. **Correct Script**: Generates 5 continuous images complying the rules. 2. **Incorrect Script**: Generates 3 images, each breaking different rule. For each script: - Include all imports. - Your code should well-documented and add concise comments explaining rule compliance or violation. - Use relative paths ./output_correct and ./output_incorrect. ## Output Generate two Python scripts as requested. Present them *exactly* in the following format, with only the raw code inside the tags: <correct_script> *Your complete Python script for generating 5 correct images* </correct_script> <incorrect_script> *Your complete Python script for generating 3 incorrect images* </incorrect_script> Remember, two scripts should be complete and directly runnable. Figure 25: Prompt for first rule-to-image rendering style 28 Prompt for second rule-to-image rendering style ## Task Description As Python imaging expert, create two Python scripts: one generating 5 continuous images that follow set of rules, and another generating 3 images that intentionally break those rules. Infer reasonable implementation according to rules (e.g., based on patterns, shapes, or mathematical properties) and document your reasoning. ## Input Let's review the rules for your images: <rules> {{RULES}} </rules> ## Guidelines - **Libraries**: Use PIL and numpy. - **Style**: Black-and-white images, no text. - **Output Paths**: - Correct images: ./output_correct/ - Incorrect images: ./output_incorrect/ - **Image Counts**: 5 correct, 3 incorrect. - **Size**: Adjust the size of elements and images to avoid confusion and unnecessary overlap. - Optionally, sometimes you can use your continuous output images to express some rules that are related between images instead of expressing all in single one image. ## Deliverables Provide two complete, runnable Python scripts: 1. **Correct Script**: Generates 5 continuous images complying the rules. 2. **Incorrect Script**: Generates 3 images, each breaking different rule. For each script: - Include all imports. - Your code should well-documented and add concise comments explaining rule compliance or violation. - Use relative paths ./output_correct and ./output_incorrect. ## Output Generate two Python scripts as requested. Present them *exactly* in the following format, with only the raw code inside the tags: <correct_script> *Your complete Python script for generating 5 correct images* </correct_script> <incorrect_script> *Your complete Python script for generating 3 incorrect images* </incorrect_script> Remember, two scripts should be complete and directly runnable. Figure 26: Prompt for second rule-to-image rendering style 29 Prompt for third rule-to-image rendering style ## Task Description As Python imaging expert, create two Python scripts: one generating 5 continuous images that follow set of rules, and another generating 3 images that intentionally break those rules. Infer reasonable implementation according to rules (e.g., based on patterns, shapes, or mathematical properties) and document your reasoning. ## Input Let's review the rules for your images: <rules> {{RULES}} </rules> ## Guidelines - **Libraries**: No restrictions. - **Style**: Your color matching should be very reasonable and beautiful. No text on images. - **Output Paths**: - Correct images: ./output_correct/ - Incorrect images: ./output_incorrect/ - **Image Counts**: 5 correct, 3 incorrect. - **Size**: Adjust the size of elements and images to avoid confusion and unnecessary overlap. - Optionally, sometimes you can use your continuous output images to express some rules that are related between images instead of expressing all in single one image. ## Deliverables Provide two complete, runnable Python scripts: 1. **Correct Script**: Generates 5 continuous images complying the rules. 2. **Incorrect Script**: Generates 3 images, each breaking different rule. For each script: - Include all imports. - Your code should well-documented and add concise comments explaining rule compliance or violation. - Use relative paths ./output_correct and ./output_incorrect. ## Output Generate two Python scripts as requested. Present them *exactly* in the following format, with only the raw code inside the tags: <correct_script> *Your complete Python script for generating 5 correct images* </correct_script> <incorrect_script> *Your complete Python script for generating 3 incorrect images* </incorrect_script> Remember, two scripts should be complete and directly runnable. Figure 27: Prompt for third rule-to-image rendering style"
        },
        {
            "title": "Prompt for scoring assembled puzzles",
            "content": "## Task Description You are an expert evaluator tasked with assessing vision puzzle consisting of 8 sub-pictures. Your goals in this prompt are: 1. **Question Reasonableness Evaluation:** Evaluate whether the provided question and answer align with the rules and are unambiguous. 2. **Overall Visual Readability Evaluation:** Assess the clarity and readability of the entire puzzle image. ## Input <image> <!--SPLIT--> </image> <question> {{question}} </question> <answer> {{answer}} </answer> <rules> {{rules}} </rules> ## Instructions 1. **Question Reasonableness Evaluation** Use the provided rules and answer to verify the reasonableness of the question. Score this on scale of 1{5, where 5 is the most reasonable. Consider these factors: - Does the answer align perfectly with the rule described? - Is there any ambiguity in the question or answer? - Is the provided answer the only correct solution? **Score Criteria:** - **5**: Perfect alignment, no ambiguity, single correct solution. - **4**: Strong alignment, minor ambiguity, likely single solution. - **3**: Reasonable alignment, some ambiguity or alternative interpretations. - **2**: Weak alignment, ambiguous, multiple plausible answers. - **1**: Poor alignment, high ambiguity, answer does not follow rules. 2. **Overall Visual Readability Evaluation** Assess the overall visual readability of the puzzle image. Score this on scale of 1{5. **Score Criteria:** - **5**: Perfectly readable with no issues. - **4**: Minor readability issues, but still easy to understand. - **3**: Moderate readability issues that may impact understanding. - **2**: Significant readability issues that hinder understanding. - **1**: Severe readability issues that make the puzzle nearly impossible to understand. ## Output <reasonableness_evaluation> [Your detailed justification here] Score: [1{5] </reasonableness_evaluation> <readability_evaluation> [Your detailed justification here] Score: [1{5] </readability_evaluation> <final_scores> Reasonableness: [1{5] Readability: [1{5] </final_scores> Figure 28: Prompt for scoring assembled puzzles"
        }
    ],
    "affiliations": [
        "University of Washington",
        "Western Washington University"
    ]
}