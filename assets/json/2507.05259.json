{
    "paper_title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing",
    "authors": [
        "Chun-Hsiao Yeh",
        "Yilin Wang",
        "Nanxuan Zhao",
        "Richard Zhang",
        "Yuheng Li",
        "Yi Ma",
        "Krishna Kumar Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark."
        },
        {
            "title": "Start",
            "content": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing Chun-Hsiao Yeh1,3 Yilin Wang3 Nanxuan Zhao3 Richard Zhang3 Yuheng Li2 Yi Ma1,2 Krishna Kumar Singh3 1UC Berkeley 2HKU 3Adobe 5 2 0 2 J 7 ] . [ 1 9 5 2 5 0 . 7 0 5 2 : r Figure 1. Left. Given source image and complex instruction, our MLLM based X-Planner decomposes the complex instruction into simpler sub-instructions (with edit type) along with auto-generated segmentation masks indicating the editing regions (shown in bottom left of each edited image) and hallucinates additional bounding box of object for the insertion case. We iteratively perform localized editing, by providing X-Planners editing instruction and region (mask and box) to compatible editing model for each edit type. Right. Recent SmartEdit [16] and MGIE [11] which also use MLLM struggles with complex instruction understanding and identity preservation."
        },
        {
            "title": "Abstract",
            "content": "Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. Work done during CHYs summer internship at Adobe Research. 1 X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark. The project page is available at https: //danielchyeh.github.io/x-planner/. 1. Introduction The field of generative image editing has experienced remarkable advancements in recent years [35, 14, 24, 27], driven by the development of diffusion models [31, 34]. Broadly, these advancements in image editing can be categorized into two categories. The first involves free-form editing, adapts pre-trained diffusion models to edit images based solely on text instructions and source image [4, 37, 47]. These methods often suffer from over-editing, modifying regions beyond those intended by the user. Subsequent research involve controllable image editing that incorporates userprovided control signalssuch as semantic segmentation masks [3, 27, 50], bounding boxes [6, 21, 42], content dragging and blobs [26, 28, 38], and image prompts [7, 46] to guide task-specific edits. These signals improve editing control but are time-consuming for users to provide manually. Another critical challenge of these models is to robustly interpret and execute complex instructions. Existing methods often struggle with nuanced requirements of these complex instructions, which limits their ability to to perform these edits effectively and provide intuitive, user-friendly interactions to allow more direct user controls over these edits. Below, we identify several key challenges that highlight limitations of current editing models: (1) Multi-Object Targeting Instructions: single instruction might targets multiple objects within an image which requires editing model to identify each object which needs to be edited according to this single instruction and image content (e.g., first row in Figure 1). (2)Multi-Task Instructions: Instructions with multiple distinct edits within single prompt (e.g., second row in Figure 1). (3) Indirect Instruction Interpretation: Instructions that contain indirect cues which require deeper understanding and decomposition into multiple steps to achieve accurate results (e.g., last row in Figure 1). Recent diffusion-based image editing methods have significantly advanced text-guided tasks, yet still struggle to robustly interpret and execute complex, indirect instructions. Typically, these tasks require extensive manual effort to simplify instructions and provide precise region guidance, limiting scalability and usability. While MLLMs offer promise due to extensive world knowledge, directly applying them often results in misinterpretation and localization errors (as shown in Figure 1, SmartEdit [16] and MGIE [11] struggle to perform these complex edits as it misunderstand the editing prompt ,e.g., SmartEdit generates ice-cream, instead making it summer scene). One solution is to force these MLLMs to reason about these complex instructions based on image content and apply chain-of-thought reasoning to autonomously break down complex instructions into simpler sub-instructions aligned with image context. In addition, we need these MLLMs to provide region guidance in terms of masks or bounding boxes for these edits. Some existing MLLMs like LISA [20] and GLaMM [32] provide grounding masks while answering image-related questions but have not been trained for localizing editing instructions. Recent work, GenArtist [43], explored this direction by leveraging closed-source GPT-4 [1] for instruction decomposition. However, it has several key limitations: (1) It focuses on breaking down long, nested prompts rather than reasoning about complex and indirect instructions, limiting its effectiveness for intricate editing tasks. (2) GenArtist relies on closed-source model and external toolboxes even during inference, restricting adaptability for users and hindering fine-tuning or further research advancements for the community. (3) Most importantly, its dependence on external object detectors [23] and segmentation model [19] results in bounding boxes and masks that are not optimized for different editing types, leading to failures in tasks requiring more than simple object detection. For instance, in object insertion tasks (e.g., add cat) where the cat is not originally present in the image, external detectors and segmentation models (e.g., SAM [19]) struggle to hallucinate and localize the inserted object, failing to provide effective editing guidance. To address these challenges, we propose X-Planner, Multimodal Large Language Model (MLLM)-driven planning system that excels in managing complex instructionbased image editing tasks as shown in Figure 1. Our XPlanner operates by breaking down intricate user instructions into structured, simpler sub-instructions, each accompanied by auto-generated edit type and mask corresponding to main editing anchor (e.g., in second row of Figure 1,local texture: Make <tree> to be in cyberpunk indicates the edit type to be of texture change and main editing anchor is tree for which we generate mask). This decomposition strategy empowers the model to interpret ambiguous requests and apply stepwise complex edits. Our approach refines spatial control by tailoring masks to each edit typetight for color/texture changes, coarse for replacements, and full-image for global editspreventing over-editing and preserving content outside the mask. In the case of insertion edits, just the mask for editing anchor is not sufficient as the inserted object region can be outside the mask. As shown in Figure 1, the inserted ornaments have to be around the cat which is beyond the mask of cats body, for which GenArtist [43] by relying on external detector and segmentor would often fail to handle. X-Planner addresses this issue by leveraging the MLLM world knowledge and reasoning about the image content predicts an additional bounding box indicating the region where the object could be inserted. X-Planner also predicts edit type which enables dynamically selecting the most suitable editing model for each edit type. In order to train such planner, we need large-scale training data mapping complex instruction to simple instructions with edit type, segmentation masks, and bound2 Figure 2. Overview of X-Planner for Complex Instruction-Based Editing. Our X-Planner comprises two main branches: First, MLLM decomposes the complex instruction into multiple simpler sub-instructions along with editing anchors (e.g., cat, dog, and background) which are given to segmentation decoder to get corresponding masks for each sub-instruction. Also, for the insertion edit, MLLM outputs bounding box coordinates along with edit instruction. By integrating with the editing model pool, we then iteratively apply the most suitable editing model to execute each specific edit task based on X-Planner generated sub-instruction along with masks / bounding boxes. ing boxes for insertion edits. Currently, no such dataset exists, hence we created large-scale Complex Instructionbased Editing Dataset (COMPIE). It comprises of over 260K paired complex-simple instructions along with mask and bounding box annotations for insertion edit. COMPIE is designed with our novel automated data annotation pipeline and stringent quality verification processes, ensuring the dataset is both scalable and highly reliable for evaluating editing capabilities. In light of the lack of benchmarks for evaluating complex instruction-driven image editing, we also propose comprehensive evaluation protocol and benchmark focusing on complex instructions. To sum up, our contributions can be summarized as follows: X-Planner: Novel end-to-end, self-contained MLLMbased planner to support complex image editing. We introduce X-Planner, an MLLM-driven agent that automatically decomposes complex user instructions into simpler tasks, with auto-generated masks and boxes for insertion edit task without relying on external MLLMs and detectors or segmentors during test time. fully automated pipeline for creating large-scale training dataset for complex editing planning. To support the training of X-Planner, we present an automatic large-scale dataset creation pipeline for generating complex-simple instruction pairs, segmentation masks, bounding boxes, and edit types. New complex instruction-based editing benchmark. We introduce large-scaled curated benchmark, COMPIE that targets compositional and indirect instructions requiring world knowledge that aims to catalyze on realworld editing tasks beyond existing editing benchmarks. 2. Related Works Controllable Generative Image Editing. Text-to-image diffusion models have recently demonstrated remarkable performance in generating high-quality images from textual descriptions [31, 34]. Building on this success, pre-trained diffusion models have been adapted for image editing tasks guided by editing text input. Some of these are training free methods [5, 14, 15, 25, 29, 44] like SD-Edit [25], Prompt-toPrompt [14], and Pix2PixZero [29] which perform text-based image editing by injecting noise into the image and then guiding the diffusion process to align with the editing text. Another line of works are training based methods [4, 11, 12, 47] like InstructPix2Pix [4], MagicBrush [47] which are more robust than training-free methods and relies on creating paired data of original and edited image to fine-tune the text-toimage diffusion models. While these methods improve upon earlier approaches, they often suffer from over-editing, affecting regions unrelated to the users instructions. Subsequent methods [3, 6, 7, 21, 2628, 38, 42, 46, 50] have improved controllable image editing by incorporating additional control signals, such as segmentation masks, bounding boxes, dragging, blobs, and image prompts. However, these methods often require users to provide control guidance manually, which can be laborious and limits usability. Also, they are generally limited to simple, direct instructions and struggle with more complex ones. Our X-Planner overcomes these challenges by automatically decomposing user instructions into actionable sub-instructions, generating the segmentation masks and bounding boxes as control guidance. MLLM-based Image Editing. Multimodal Large Language Models (MLLMs) leverage the strengths of the large language models (LLMs) [9, 10, 40] while incorporating visual data [22, 52], enabling more sophisticated multimodal understanding and generation. Recent efforts have extended MLLMs to the domain of image editing [11, 16]. E.g., MGIE [11] uses MLLMs to make editing instructions more expressive and use that to guide editing models such as In3 structPix2Pix to have better image editing ability. Also, very recent concurrent work, GenArtist [43] uses an external closed-source GPT-4 [1] as agent that decomposes long editing tasks with multiple simple instructions nested together (similar to multi-task instructions mentioned in the introduction) and uses external object localization tools. In contrast, our proposed X-Planner is an MLLM agent which handles actual indirect complex instructions and generates editing type specific masks which are beyond simple object detection (e.g. for insertion it hallucinates object location based on image contnet). Moreover, X-Planner operates independently at inference, without relying on large, closed-source models like GPT-4 for planning. 3. Method In this section, we introduce X-Planner (Figure 2), method specifically designed to break down complex instructions into simpler image editing tasks. Leveraging an MLLM trained on our proposed large-scale dataset tailored for instruction decomposition, X-Planner autonomously generates control inputssuch as segmentation masks and bounding boxesfor each sub-instruction to facilitate precise and instruction-based image editing. 3.1. X-Planner: Complex Editing Task Agent We first break down this task of complex-instruction based image editing planning into key sub-problems of (1) complex instruction decomposition, and (2) control guidance input generation. Figure 2 presents an overview of our X-Planner pipeline, which first decompose complex instructions into multiple simpler instructions along with corresponding control guidance inputs (mask for all edits and bounding box for insertion edit). We then conduct the iterative editing by assigning the suitable editing model for each edit task. We draw inspiration from the recently introduced GLaMM [32], which is an MLLM that utilizes an LLaVA-like architecture [22] with segmentation mask decoder. This model constructs image-level captions with specific phrases linked to corresponding segmentation masks. For example, given an image with cat and dog, GLaMM can respond to can you describe this image? with there are <cat> and <dog>, anchored to unique segmentation masks for each animal. So, GLaMM would use vision-language understanding of MLLM to generate the caption along with anchored phrases (like cat and dog) and then segmentation mask decoder takes the input image feature and these anchored phrases to segment them. In order to adapt the GLaMM for our case, we would like it to take source image and complex instruction, break it down to simpler instructions with edit type and anchored editing object/region (e.g. cat, dog, and background in Figure 2). Thus, segmentation decoder can take these anchored editing regions to get corresponding masks. For the insertion Figure 3. Level 1: Complex-Simple Instruction Pair Generation. Using our structured template, we prompt GPT-4o to generate complex instructionsincluding indirect, multi-object, and multi-task instructions (as defined in Section 1)along with their corresponding simpler instructions, object anchors, and edit types. case, we also want to predict the location of the object to be inserted but we cannot use the segmentation decoder as it can only segment the object visible in the image. Hence, we would use MLLM which has world-knowledge to predict bounding box based on the input image and insertion instruction (e.g. [insertion]< 0.59,0.71,0.95,0.93> Add Christmas ornaments around the cat in Figure 2). However, we realize that GLaMM does not work well for complex instruction-based planning tasks. Specifically, (1) it struggles with complex instruction decomposition, as it primarily learns from visual grounding samples rather than the instruction interpretation, and (2) it is limited in generating control guidance inputs, such as task-specific masks, especially for insertion tasks where it fails to hallucinate unseen objects. These two main problems remain unsolved in the current GLaMM, thus motivating the need for largescale, complex image editing instruction planning dataset to train MLLM and segmentation decoder of GLaMM model to generalize for this specific task. 3.2. Automated Data Annotation Pipeline We present our novel automated annotation pipeline developed to construct the Complex Instruction-Based Editing Dataset (COMPIE), comprehensive and diverse dataset for complex instruction-driven editing planner. The pipeline comprises three distinct levels. At Level 1 (Figure 3), we generate structured complex and decomposed simple instruction (with editing anchors) pairs using GPT-4o [1] for an input image. Level 2 (Figure 4) employs Grounded-SAM [33] to produce initial segmentation masks Figure 4. Level 2: Instruction-Based Mask Generation and Refinement. In Stage 1, we use the source image and anchor text with Grounded SAM to generate fine-grained mask for the specified object. In Stage 2, we refine this mask by applying varies strategies based on the edit type provided in Level 1 (Figure 3). anchored to each instruction, providing base for spatial control, and these masks are further refined according to edit type. Level 3 (Figure 5) focuses on insertion-based instructions to generate precise bounding boxes indicating location where object can be inserted. Level 1: Complex-Simple Instruction Pair Generation. To address the challenge of limited instruction diversity, we generate complex editing instructions by leveraging MLLM creativity and human oversight. We draw on diverse data sourcesincluding SEED-X [12], UltraEdit [50], and InstructPix2Pix [4] datasetsspanning synthetic to real images and varying quality levels. First, we manually design in-context examples that capture key instruction types: indirect, multi-object, and multi-task instructions (as defined in Section 1). We ensure that each complex instruction decomposes into 1 to 5 simpler sub-instructions with meaningful and coherent correspondence, and that each sub-instruction specifies an editing task type and editing anchor which corresponds to edited object/region. Each simple edit will have one anchor except replace edit which will have two anchors corresponding to objects before and after replace edit. Using GPT-4o [1], we then generate four complex-simple instruction pairs per image by prompting it with both the source image and our designed task template (see Figure 3). Also, in our training data we mix simple-simple instruction pairs to ensure X-Planner learns to not modify or breakdown simple instructions. Our approach also works using the opensourced models like Pixtral-Large [2]. Please see Supp. for more details. 5 Figure 5. Level 3: Insertion Task-Based Mask & Box Localization. For insertion task, Grounded SAM struggles to segment objects not present in the source image. We pre-train an MLLM on bounding box-annotated dataset [41], enabling it to pseudoannotate our data with bounding box for insertion edits. Level 2: Instruction Mask Generation and Refinement. X-Planner aims to provide editing models with precisely defined target regions for modification. At Level 2 (Stage 1), we generate segmentation masks based on the edited object anchor identified in each decomposed instruction from Level 1 (e.g., add circus ring behind the lion; Anchor: <lion>). Using the source image and anchor text, we employ Grounded SAM [33] to produce fine-grained mask for the specified object. In Stage 2, we refine the mask based on the edit type (see Figure 3). For local texture, color change, and background edits, we use the mask generated in Stage 1 directly. For shape changes, we make masks larger to accommodate object reshaping by dilating the mask by 20%. In replace tasks (e.g., replace cat with dog), we take the union of both masks (e.g. cat and dog) based on the anchor objects from preand post-edit images if available (e.g., InstructPix2Pix dataset). For datasets like SEED-X, where only the pre-edit image is available, we dilate the mask by 20% to account for the replace edit. For style changes or instructions requiring global transformation (e.g., make image 1950s style), we select the entire image as the editing mask. Level 3: Insertion Task-Based Box Localization. In insertion-type edits (e.g., add circus ring behind the lion as shown in Figure 3), Grounded SAM, which relies on object anchors, encounters difficulties when segmenting objects that are not present in the source image (e.g., the circus ring). Direct segmentation of the intended placement region (e.g., lion) often leads to imprecise masks, particularly when there is ambiguity between the instruction and the mask (e.g., add circus ring behind the lion), causing errors by segmenting the existing object (lion) rather than the intended insertion area (behind the lion). This discrepancy can lead to degraded edit quality. To address these limitations, in Level 3 (Stage 2), we fine-tune the MLLM component of GLaMM [32] on an annotated dataset with ground truth (GT) bounding boxes for insertion locations. Specifically, we leverage the MULAN dataset [41], which includes background images with and without foreground objects, allowing us to use images without the object as input and predict the bounding box of the insertion target. Through training, the MLLM learns to generate bounding box recommendations for novel insertion instructions during inference. For unannotated insertion instructions, the fine-tuned MLLM produces pseudo-labels by predicting bounding boxes based on the given instruction, enabling precise edit placements. In our experiments (Section 4.3), we further demonstrate X-Planners capability to generate consistent bounding box predictions with repeated instruction to show plausible variations in location. 4. Experiments Our experiments evaluate the quality of X-Planner in complex instruction understanding and editing localization. First, we test performance on simple instruction settings using the established MagicBrush benchmark [47]. Second, we evaluate complex instruction settings by comparing performance with and without plugging in X-Planner to baselines on our proposed COMPIE benchmark. Settings. X-Planner uses GLaMM [32] as base model, built on Vicuna-7B [51]. For training, over 260K complex-simple instruction pairs COMPIE is used. See supp. for more details. Baselines. For the MagicBrush benchmark, which focuses on simple instruction settings, we benchmark against methods tailored for straightforward edits [3, 4, 25, 27, 47, 48, 50]. We explore variations that integrate components of XPlanner into the UltraEdit [50] baseline, which is current state-of-the-art and can also take editing mask as input to assess the impact of X-Planner in simpler editing scenarios. For the COMPIE benchmark, which addresses complex instruction settings, we select UltraEdit [50] and InstructPix2Pix* as primary baselines to show the effectiveness of our X-Planner. InstructPix2Pix* is an improved version of InstructPix2Pix [4] using our internal dataset and also utilizes mask as input conditioning. We use this model to show generalizability of our X-Planner due to lack of public models with mask conditioning. We evaluate these methods in two variants: with X-Planner integration and without it. Also, we compare with MGIE [11] and SmartEdit [16], which use MLLM to improve the editing performance. Benchmark and Metrics. For the MagicBrush benchmark [47], we use its evaluation setup, and for our edits measure L1, L2 distance, CLIP-I, and DINO similarity with ground-truth. For the COMPIE benchmark, we adopt the evaluation protocol from EmuEdit [37], comparing edited images against both the source image and target captions. Consistent with EmuEdit, we use L1, CLIP image similarity (CLIPim), and DINO similarity to measure how well the edited image retains the content of the original image. And use CLIP text-image similarity (CLIPout) to measure alignment of editing instruction and edited image. Given that CLIPout can struggle to capture the nuances of complex instructions, we additionally employ InternVL2-Llama376B [8], powerful MLLM, to evaluate the alignment between the editing instruction and edited image (M LLMti). We use InternVL2 for fair evaluation instead of GPT-4o as GPT-4o was used to create our training data. For completeness, we also use InternVL2 to measure the similarity of input image and edited image (M LLMim). Please find Supp. for more details. Table 1. Quantitative Comparison on the MagicBrush Test Set. We report results for both single-turn and multi-turn settings. In comparison to UltraEdit using human labeled masks, we evaluate using X-Planner generated masks and bounding boxes as control inputs. For Bag of Models, we utilize PowerPaint for removal tasks, InstructDiff for style changes, and UltraEdit for other edit types. Single-Turn Methods Guidance Control Input L1 L2 CLIP-I DINO SD-SDEdit Null Text Inversion GLIDE Blended Diffusion HIVE InstructPix2Pix (IP2P) IP2P w/ MagicBrush UltraEdit UltraEdit X-Planner + UltraEdit X-Planner + UltraEdit X-Planner + Bag of Models No No Human Labeled Mask Human Labeled Mask No No No No Human Labeled Mask X-Planners Mask X-Planners Mask + Box X-Planners Mask + Box Multi-Turn 0.1014 0.0749 3.4973 3.5631 0.1092 0.1141 0.0625 0.0614 0.0575 0.0528 0.0513 0.0511 0.0278 0. 115.8347 119.2813 0.0380 0.0371 0.0203 0.0181 0.0172 0.0171 0.0168 0.0172 0.8526 0.8827 0.9487 0.9291 0.8519 0.8512 0.9332 0.9197 0.9307 0.9281 0.9312 0.9331 0.7726 0. 0.9206 0.8644 0.7500 0.7437 0.8987 0.8804 0.8982 0.8900 0.8959 0.8970 Methods Guidance Control Input L1 L2 CLIP-I DINO SD-SDEdit Null Text Inversion GLIDE Blended Diffusion No No 0.1616 0.1057 0.0602 0. Human Labeled Mask Human Labeled Mask 11.7487 14.5439 1079.5997 1510.2271 HIVE InstructPix2Pix (IP2P) IP2P w/ MagicBrush UltraEdit, eval w/o region UltraEdit, eval w/ region X-Planner + UltraEdit X-Planner + UltraEdit X-Planner + Bag of Models No No No No Human Labeled Mask X-Planners Mask X-Planners Mask + Box X-Planners Mask + Box 0.1521 0.1345 0.0964 0.0780 0.0745 0.0679 0.0668 0. 0.0557 0.0460 0.0353 0.0246 0.0236 0.0227 0.0226 0.0223 0.7933 0.8468 0.9094 0.8782 0.8004 0.8304 0.8924 0.8954 0.9045 0.9025 0.9047 0.9079 0.6212 0.7529 0.8494 0. 0.6463 0.7018 0.8273 0.8322 0.8505 0.8423 0.8475 0.8508 4.1. MagicBrush Results (Simple Instructions) Table 1 shows quantitative results on the MagicBrush benchmark. Key observations: (1) X-Planner enhances UltraEdit by providing masks and bounding boxes for localized edits, improving performance, especially for insertion tasks. XPlanners mask is able to match the human labeled mask, and even outperform it sometimes as shown in the result. (2) XPlanner is model-agnostic, integrating seamlessly with multiple models (e.g., PowerPaint [53] for removal, InstructD6 Figure 6. Qualitative Comparison for Complex Instruction-Based Editing Benchmark. Integrating X-Planner with editing methods, InstructPix2Pix* and UltraEdit, brings drastic boosts in preserving object identities with X-Planner generated masks and boxes (display in bottom-left of each image). X-Planners decomposition of complex instructions also enhances alignment with various complex instruction inputs. X-Planner provides distinct advantage over baselines that only use the source image and complex instruction without masks. iff [13] for style changes, and UltraEdit for other edits), leveraging their strengths to boost overall performance. Please find Supp. for more quantitative results. 4.2. COMPIE-Eval Results (Complex Instructions) Qualitative Results. In Figure 6, we show complex instruction editing results for InstructPix2Pix* and UltraEdit with and without X-Planner. We can see that without X-Planner editing methods are not able to understand the user intentions from complex instructions. For example, in the last row of Figure 6, the intention of the instruction was to make image look futuristic but just UltraEdit fails to understand that whereas our X-Planner is able to convert complex instruction into meaningful sub-instructions. Even for the cases where editing method understands the meaning of instructions, they struggle with identity preservation as they cannot leverage the editing masks and bounding boxes of the planner (e.g. third row for InstructPix2Pix* in Figure 6). Quantitative Results. We create high quality and diverse test benchmark, COMPIE-Eval, focusing on complex editing by collecting data from different sources, LAION-highaesthetics [35], and Unsplash-2K [18]. We then use GPT-4o to generate complex instruction for the test image and apply post-verification stage, in which crowd workers filter examples with irrelevant instructions. The COMPIE contains 550 images, with complex instructions with variations mentioned in the introduction. See supp. for details. In Table 2, we show that X-Planner enhances editing performance by simplifying complex instruction to decomposed simple instructions suitable for editing models and generat7 Table 2. Quantitative Comparison on the COMPIE Benchmark. X-Planner significantly improves the editing performance of UltraEdit and InstructPix2Pix* by decomposing complex instructions and providing control guidance inputs (e.g., masks). To overcome the limitations of CLIPout in handling complex instructions, we utilize an MLLM-based evaluation metric to better reflect X-Planners capabilities. Guidance Control Input L1 CLIPim CLIPout DINO MLLMti MLLMim Methods SmartEdit [16] MGIE [11] No No UltraEdit GenArtist [43] + UltraEdit X-Planner + UltraEdit X-Planner + UltraEdit No GenArtists Mask + Decomposed Instruction X-Planners Decomposed Instruction X-Planners Mask + Decomposed Instruction InstructPix2Pix* GenArtist [43] + InstructPix2Pix* GenArtists Mask + Decomposed Instruction X-Planner + InstructPix2Pix* X-Planner + InstructPix2Pix* X-Planners Decomposed Instruction X-Planners Mask + Decomposed Instruction No 0.2764 0. 0.1292 0.1253 0.1253 0.1188 0.1517 0.1458 0.1458 0.1320 0.7713 0.7692 0.7688 0.7767 0.7767 0.7875 0.8020 0.8143 0.8143 0.8285 0.2512 0. 0.2698 0.2621 0.2621 0.2569 0.2666 0.2641 0.2641 0.2591 0.6044 0.5981 0.6387 0.6435 0.6435 0.6599 0.6988 0.7114 0.7114 0.7068 0.6511 0. 0.6652 0.6894 0.6894 0.7061 0.6727 0.7072 0.7072 0.7408 0.5347 0.5288 0.5523 0.5593 0.5593 0.5744 0.6160 0.6277 0.6277 0.6454 Table 3. Segmentation Mask Comparison on PIE Benchmark. We compare on the setting of instruction-to-segmentation mask, X-Planner consistently outperforms baseline methods. rate them equal. In Figure 7, we show average results for both benchmarks and observe the users prefer results with X-Planner for all criteria (better means X-Planner preferred). Method IoU Precision Recall Random 10% Mask GLaMM-Base GLaMM-RefSeg Llama3+GLaMM X-Planner (Ours) 0. 0.14 0.28 0.44 0.67 0.49 0.66 0.69 0.73 0.79 0.12 0.15 0.32 0.53 0.81 ing masks helps it further for improved identity preservation. It improves UltraEdit and InstructPix2Pix* across most metrics, except CLIPout, which struggles with edited image and complex instruction alignment (e.g., focusing on ice cream instead of the summer scene in Figure 1 last row). To address this, we use InternVL2, an MLLM-based text-image alignment metric (M LLMti), which better understands complex instructions, showing significant improvements with X-Planner. MGIE [11] and SmartEdit [16] despite using MLLM gives inferior results. Please note that InternVL2 can also be used for verifying the quality of the edited results and choose the best one. Figure 7. User Study on COMPIE Benchmark. We compare against InstructPix2Pix* and UltraEdit. Better means the generated images by using our X-Planner is preferred and vice versa. User Study. We conducted user study with 100 random samples from 550 images in the COMPIE benchmark to compare results of two baselines: InstructPix2Pix* and UltraEdit with and without X-Planner. Participants rated each image on (1) identity preservation, (2) instruction alignment, and (3) overall quality, and choose the preferred image or 8 Figure 8. Visualize Consistent Bounding Box with Repeated Runs. We show X-Planner can generate consistent bounding boxes with repeated runs to yield plausible location variations. 4.3. GLaMM Comparison and BBox Localization In Table 3, we report X-Planners segmentation mask generation performance on the PIE benchmark [17], comparing it with GLaMM [32] variations (baseline model and version fine-tuned on RefSeg dataset to have better grounding) and baseline that leverages Llama3 [10] for object anchoring followed by GLaMM for mask generation. Based on the results, we can see X-Planner consistently surpasses other instruction-to-segmentation methods, underscoring its effectiveness for mask generation in instruction-based tasks. We visualize X-Planners bounding box localization for insertion edits in Figure 8 with 10 repeated runs of the same instruction. Predicted box locations (laptop on table) and shapes (vertical for palm tree) appear plausible. Detailed quantitative analysis and ablations are in the Supp. 5. Conclusion In this paper, we introduced X-Planner, an MLLM-based planning system that breaks down complex instructions into simpler tasks with editing masks and bounding boxes for insertion edits. We also proposed novel data generation pipeline to train this planner. Our evaluation highlights XPlanners potential to enhance existing editing models, encouraging further exploration of MLLM-based planners as complementary tools for complex editing tasks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 4, 5, 3, 11 [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 5, 3 [3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 2, 3, 6 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2, 3, 5, 6, 1 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 2, 3 [6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free In Proceedlayout control with cross-attention guidance. ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 53435353, 2024. 2, 3 [7] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 2, [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. 6, 1, 3 [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. 3 [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3, 8 [11] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 1, 2, 3, 6, 8 [12] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3, 5, [13] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1270912720, 2024. 7, 6 [14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 3 [15] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared atIn Proceedings of the IEEE/CVF Conference on tention. Computer Vision and Pattern Recognition, pages 47754785, 2024. 3 [16] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 1, 2, 3, 6, 8 [17] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In The Twelfth International Conference on Learning Representations, 2024. 8 [18] Younggeun Kim and Donghee Son. Noise conditional flow model for learning the super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2021. 7, [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2 [20] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2 [21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 2, 3 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3, 4 [23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 2 9 [24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 2 [25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 3, [26] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. 2, 3 [27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2, 6 [28] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024. 2, 3 [29] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 3 [30] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. 1 [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, [32] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 2, 4, 6, 8, 1 [33] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 4, 5 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 7, 1 [36] Tiancheng Shen, Jun Hao Liew, Long Mai, Lu Qi, Jiashi Feng, and Jiaya Jia. Empowering visual creativity: visionlanguage assistant to image editing recommendations. arXiv preprint arXiv:2406.00121, 2024. 6 [37] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. 2, 6, 1, 3 [38] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. 2, [39] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3 [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [41] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2241322422, 2024. 5, 6, 1 [42] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instancelevel control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024. 2, 3 [43] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. arXiv preprint arXiv:2407.05600, 2024. 2, 4, 8 [44] Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, and Eli Shechtman. Turboedit: Instant text-based image editing. ECCV, 2024. 3 [45] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [46] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3 [47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6 [48] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, 10 Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. 6 [49] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 4, [50] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024. 2, 3, 5, 6, 1, 4 [51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 6, 1 [52] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 [53] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023."
        },
        {
            "title": "Overview",
            "content": "7. Implementation Details In this supplementary, we first provide additional details about our training dataset and proposed evaluation benchmark in Section 6 and X-Planners implementation details in Section 7. Next, we show some more quantitative results for X-Planners bounding box guidance ability in Section 8. Then, We demonstrate our planner trained with open-sourced model and show various qualitative comparisons with baseline methods to show the effectiveness of our planner in Section 9, 10, 11, 12, 13, and 14. 6. COMPIE Dataset & Benchmark Summary X-Planners Training Dataset Statistics (COMPIE). We explore the details of our proposed COMPIE, large-scale and high-quality dataset specifically designed to address complex instruction-based image editing. COMPIE contains over 260K complex-to-simple instruction pairs, focusing on complex editing tasks unlike previous works (e.g., MagicBrush) that predominantly focus on simple instructions. Figure 9 provides comprehensive summary of the dataset, including (a) data sources such as SEEDX [12], UltraEdit [50], MULAN [41], and InstructPix2Pix [4], (b) the distribution of edit types across 8 categories for decomposed instructions, and (c) the breakdown of decomposed instructions ranging from 1 to 5 per complex instruction. COMPIE Validation Benchmark Statistics. In this section, we present detailed summary of the COMPIE validation benchmark, highlighting its diversity and focus on complex instruction-based image editing. COMPIE benchmark comprises 550 samples from LAION high-aesthetic dataset [35] and Unsplash 2K [18], providing rich variety of real-world images to enhance generalization across different domains. The COMPIE benchmark is categorized into four distinct types of complex instructions, including (1) general complex instructions (50%). General complex instructions typically require multiple editing steps to fulfill the directive. For instance, instructions such as Make the image look like winter wonderland or Transform the room to appear colorful and lively necessitate edits across multiple regions or objects to achieve the desired outcome comprehensively, (2) indirect instructions (30%), (3) multi-object instructions (15%), and (4) multi-task instructions (5%), as shown in Figure 10 (a). This diverse distribution ensures comprehensive coverage of various complexities encountered in real-world editing tasks. Figure 10 (b) illustrates the word count distribution of complex instruction anchor descriptions, underscoring the instruction diversity in the dataset. The inclusion of diverse anchor words enriches the datasets ability to evaluate the interpretation ability of editing models. These insights showcase COMPIE as robust benchmark designed to advance the development of image editing models which understand and execute complex instructions. X-Planners Setup. Our X-Planner leverages GLaMM [32] as the base model, built on Vicuna-7B [51]. The approach incorporates key components inspired by GLaMM [32], particularly the design of the region encoder, grounding image encoder, and pixel decoder. Adhering to the training protocol of GLaMM, we keep the global image encoder and grounding image encoder frozen, while fully fine-tuning the region encoder and pixel decoder. For the language model, we employ LoRA fine-tuning with scaling factor  = 8 over 10 epochs. MLLM Evaluation Metric Setup. In the COMPIE benchmark evaluation, we follow the Emu Edit [37] protocol for metrics and employ InternVL2-Llama3-76B [8] as our LLM metric to evaluate instruction-to-edited-image alignment and similarity between the original and edited images. Specifically, we adapt the DreamBench++ [30] template for image-to-image alignment by ranking alignment scores from very poor to excellent (0 to 4) based on shape, color, and texture criteria. For instruction-to-image alignment, we ensure that text prompts are complex instructions requiring transformation. For example, the prompt Could you make this image look like the season when ice cream is daily essential? is first interpreted as creating summer scene rather than directly associating the prompt with ice cream. We normalize the maximum score to 4 and calculate the performance percentage based on the average score, presenting it as the LLM metric performance. X-Planners Training Dataset Setup. For training dataset distribution, we ensure that X-Planner retains the ability to handle simple instructions. In the InstructPix2Pix [4] dataset, 40% of training pairs are simple-to-simple, meaning the decomposition directly maps simple instructions to simple outputs. Similarly, for the MULAN [41] dataset, which focuses on insertion-type edits, we treat the dataset pairs as simple-to-simple for insertion-specific training. For fine-tuning X-Planner, we integrate datasets used in the original GLaMM [32] model, including Semantic Segm, RefCoco GCG, PSG GCG, Flickr GCG, and GranDf GCG, in combination with our COMPIE training dataset including InstructPix2Pix GCG, UltraEdit GCG, SEEDX GCG, and MULAN GCG. The data source ratio for training our X-Planner is [1, 3, 3, 3, 1, 3, 3, 9, 9, 9]. 8. X-Planners Bounding Box Localization In Table 4, we present results on the MULAN validation benchmark [41], demonstrating the localization effectiveness of our X-Planner. Key observations include: (1) XPlanners bounding box localization significantly improves with pseudo-labeling, where we annotate training data with bounding box generated by an MLLM pre-trained on insertion tasks (as described in section 3.2 Level 3 in the main 1 Figure 9. X-Planners Training Dataset Summary (Generated from GPT-4o). This figure provides distribution summary of our X-Planners training dataset, including (a) the data sources, (b) the distribution of edit types for decomposed instructions, and (c) the number of decomposed instructions. The dataset demonstrates significant diversity and comprises large-scaled number of pairs. Note that in (a) indicates the number of data samples. Figure 10. COMPIE Benchmark Summary. This figure summarizes the proposed COMPIE benchmark, which consists of 550 samples spanning various types of complex instructions shown in (a), including (1) general complex instructions, (2) indirect instructions, (3) multi-object instructions, and (4) multi-task instructions. Additionally in (b), we present the word count distribution of complex instruction anchor descriptions, highlighting the diversity of the dataset. Note that in (a) indicates the number of data samples. paper). Mask Only baseline where we rely on segmentation decoder to predict mask of the location where object would be inserted is not sufficient. E.g., if we want to insert hat on person, then just having mask for the person is not sufficient as the hat would be beyond the person mask, hence we need to take the advantage of our MLLM to predict the bounding box on top of the head. (2) Enlarging small bounding boxes (< 5% of the image size) in the training set further enhances box prediction accuracy, addressing challenges posed by very small bounding boxes that make insertion tasks difficult for diffusion-based editing models. Finally, just predicting bounding box gives good localization capability but sometimes it may not cover full-extent of the object and miss some part which might need to be edited when the object is inserted. So, for better coverage of editing location, we also try combining both bounding box and mask which gives slightly better localization than just using bounding box. Also, as editing location guidance, most of the editing methods like UtlraEdit are more robust to bigger editing location compared to smaller one, hence adding mask along with bounding box is better strategy as it will give larger coverage of the editing area. E.g. in the last row of Figure 6, for inserting robot on table, the mask gives the coverage of table where robot would be inserted and bounding box provides additional coverage on top of the table as the inserted robot would be sitting on the table. 9. Quantitative Comparison on Emu Edit We also compare the effectiveness of our X-Planner on Emu Edit test set [37] which is similar to MagicBrush [47] test set and focuses on simpler instructions. We apply our X-Planner with the UltraEdit [50] model and we can see in Table 5 that X-Planner improves the performance on all the identity 2 preserving metrics (L1, CLIPim, and DIN O) due to better instruction localization through the predicted mask and box which would be given as control guidance to UltraEdit. Also, the instruction following performance (measured by CLIPout) is similar to the UltraEdit as most of the instructions are simple and do not require X-Planner to simplify it further. Apart from UltraEdit results, we also show results of other baseline editing methods like InstructPix2Pix [4], MagicBrush [47], EmuEdit [37], and OmniGen [45] at the top of Table 5 as reference. Note: Emu Edit [37] and UltraEdit [50] highlight that the MagicBrush benchmark introduces biases favoring models trained on its dataset, leading to inflated performance, as the numbers highlighted in red for CLIPim and DIN metrics in Table 5. This overfitting undermines the general editing capabilities of these models on other datasets. 10. Multi-Step Editing Error Propagation Our X-Planner is less prone to errors since it decomposes complex instructions into simpler, model-friendly steps. As stated in the main paper, we can further enhance reliability by introducing closed-loop verification mechanism using strong MLLMs (e.g., InternVL2.5-38B [8] and GPT-4o [1]) to evaluate each intermediate result. Specifically, after each editing step, we use the verifier to assign score from 0 4 that reflects how well the generated image aligns with the current instructionsimilar in spirit to the LLMti metric introduced in our evaluation. If the score falls below threshold (e.g., 3), we automatically re-generate the step using different random seed to recover from potential hallucinations, misalignment. This mechanism is critical for catching early-stage failures that would otherwise propagate through subsequent steps. We allow configurable number of retries (e.g., max=1 or 4), striking balance between quality and efficiency. As shown in Table 6, this approach improves instruction-image alignment and identity preservation compared to baselines without verification. 11. Generate Training Data from OpenSourced Model, Pixtral-Large To ensure reproducibility and accessibility of our pipeline, we also build secondary version of the training dataset using an open-sourced MLLM, Pixtral-Large [2], 124Bparameter multimodal model built upon Mistral Large v2, offering competitive performance on range of VQA and multimodal reasoning benchmarks. Notably, it surpasses the closed-source GPT-4o [1] and Gemini 1.5 [39] models on several standard evaluation sets, making it strong candidate for high-quality instruction generation while remaining fully accessible to the research community. We deploy Pixtral-Large [2] using 8 NVIDIA A100 80GB GPUs to enable inference across diverse image-to-instruction generation tasks. The instruction generation process mirrors the methodology used with GPT-4o to ensure fair comparison: As shown in Figure 11, we use the same image datasets, including InstructPix2Pix [4], UltraEdit-100K [50], and SEEDX [12]. The dataset includes around 300K image and instruction pairs. For SEEDX, we generate both complex and simplified instruction pairs from the same image while for InstructPix2Pix, only complex instructions are generated, as simplified ones are already annotated. 12. Train X-Planner with Generated Data from Open-Sourced Model, Pixtral-Large To ensure fair comparison, we maintain identical training settings, including model architecture, learning rate, batch size, and number of epochs, as described in Section 7 which were used for GPT-4o version of the X-Planner. This allows us to isolate the effect of training signal quality from the underlying data generator. We evaluate the Pixtral-Largetrained X-Planner across two benchmarks: 1) MagicBrush test set, and 2) COMPIE benchmark to compare with the GPT-4o version of the X-Planner. Table 7 (MagicBrush Test Set): On both single-turn and multi-turn settings, the Pixtral-trained X-Planner sightly outperforms the GPT-4o-trained version. Notably, in the multi-turn evaluation with UltraEdit + Bag of Models, the the Pixtral-Large version of X-Planner achieves the highest across all settings. This suggests that Pixtral-generated instruction data have competitive performance in complex editing tasks over multiple rounds. Table 8 (COMPIE Benchmark): We can see X-Planner trained from Pixtral-Large [2] generated data achieves comparable or slightly improved performance compared to its GPT-4o counterpart across most metrics. For example, using X-Planner + UltraEdit combined with decomposed instructions with mask control, Pixtral-Large verson of X-Planner yields MLLMti = 0.7102 and MLLMim = 0.5765, which closely matches GPT-4os 0.7061 and 0.5744, respectively. When paired with InstructPix2Pix*, the Pixtral-trained XPlanner achieves the best overall MLLMti and MLLMim score of 0.7431 and 0.6488, indicating stronger consistency in understanding editing outputs. These results demonstrate that training X-Planner using data generated from the opensourced MLLM demonstrating our approach is generalizable and not restricted to the close-sourced GPT-4o. 13. Additional Qualitative Results In Figure 12, we provide detailed look at X-Planners decomposition capabilities, emphasizing its ability to effectively manage wide range of edit types by generating precise and context-aware segmentation masks. Each example demonstrates how X-Planner tailors its outputs to 3 Figure 11. X-Planners Training Dataset Summary (Generated from Pixtral-Large). This figure illustrates key statistics of our automatically constructed dataset used to train X-Planner using an open-sourced model, comprising around 300K instruction-image pairs. (a) Source Composition: The data is aggregated from four datasetsSEEDX (32.9%), UltraEdit (28.1%), MULAN (25.5%), and InstructPix2Pix (13.5%)with indicating sample count. (b) Edit Type Distribution: Our dataset covers diverse editing intents, including insertion (35.0%), replace (16.2%), style (11.9%), background edits (10.1%), local texture (9.6%), local color change (9.7%), shape change (5.9%), and remove (1.6%). This diverse mix supports robust generalization across edit semantics. (c) Instruction Decomposition Complexity: While the majority (68.0%) of prompts require only single edit, substantial portion involve multi-step reasoning: 2-step (8.9%), 3-step (13.4%), 4-step (9.6%), and even 5-step (0.03%). This highlights the need for sequential planner like X-Planner to handle compositional and complex instructions effectively. the specific requirements of the edit type. For instance, in Row 3, the [replace] edit features sophisticated approach where both the before and after masks are generated, combined into single guidance mask. This unified mask, further enhanced with appropriate dilation, provides reliable boundaries for the editing model to ensure accurate representation and minimizing errors. These examples underscore X-Planners ability to deliver precise and adaptive control signals for complex image editing tasks. In Figure 13, we show (1) comparative evaluation of XPlanner against several baseline methods for multi-turn editing on the MagicBrush dataset and (2) qualitative comparison with the UltraEdit baseline on the COMPIE benchmark. In the context of multi-turn editing, X-Planner stands out by leveraging its generated masks to achieve superior identity preservation. In contrast, many baseline methods, particularly InstructPix2Pix, often over-edit the image, highlighting their lack of fine-grained control and precision. These results underscore X-Planners ability to maintain consistency and accuracy across iterative edits. In Figure 14 and Figure 15, we present comparison between InstructPix2Pix* results with and without the integration of X-Planner. For the [replace] edit type, as shown in Rows 4 in Figure 15, X-Planner produces carefully dilated segmentation mask for the replaced region, enabling the editing model to execute the changes more effectively. Likewise, in Row 2, the [shape change] edit incorporates dilated mask, accommodating potential alterations in the 4 objects shape and ensuring precise adjustments. These examples demonstrate the distinct advantages of X-Planners instruction decomposition and mask generation capabilities, providing enhanced control and accuracy compared to the baseline, which relies on directly processing complex instructions without decomposition. 14. Non-Rigid Edits on X-Planner In Figure 16, we present diverse set of non-rigid and compositional edits using three different editing models: InstructPix2Pix*, GPT-4o [1], and IC-Edit [49]. These examples demonstrate the plug-and-play flexibility of our XPlanner, which can integrate with existing editing models and enable them to perform complex editswithout requiring training on such complex instructionsby decomposing them into simpler, model-friendly steps. However, as our framework relies on external editing models for actual image generation, the final results are sometimes bounded by their limitations. For example, GPT4o often fails to preserve fine identity details across steps, particularly in face edits (e.g., Row 4 in Figure 16, see red boxes). Similarly, UltraEdit [50] occasionally generates inserted objects that slightly exceed their designated bounding box regions (see Main Paper, Figure 6), especially in cat in AI world scenes. These limitations are inherent to the editing models instead of the planning framework. Table 4. Bounding Box Localization on MULAN Benchmark. X-Planner achieves strong localization gains by combining masks with MLLM-predicted boxes. Pseudo-labeling improves AP50 by over 2 at K=1, and box enlargement further boosts recall at K=5. Best performance is achieved using both mask and box cues. Method Setting K=1 K=3 K=5 Insertion Edit Task (n = 416) IoU AP50 IoU AP50 IoU AP50 X-Planner Mask Only +Pseudo-Label + Box Enlarge + Mask & Box 0.37 0.63 0.75 0. 0.34 0.70 0.77 0.78 0.46 0.71 0.81 0.81 0.37 0.69 0.82 0.86 0.54 0.75 0.86 0.85 0.38 0.69 0.84 0.86 Table 5. Quantitative Comparison on the Emu Edit Test. X-Planner significantly enhances UltraEdits editing quality by decomposing complex instructions and supplying additional control inputs. We report metrics including L1 distance (lower is better), CLIPim and CLIPout similarity (higher is better), and DINO feature similarity. Compared to baseline methods such as InstructPix2Pix, MagicBrush, EmuEdit, and UltraEdit, X-Planner variants achieve consistently better performance across all metrics. Notably, the best results are obtained when combining X-Planners mask and bounding box with diverse bag of editing models."
        },
        {
            "title": "Guidance Control Input",
            "content": "L1 CLIPim CLIPout DINO"
        },
        {
            "title": "Methods",
            "content": "InstructPix2Pix(450K) MagicBrush(450+20K) EmuEdit(10M) OmniGen UltraEdit (1M w/o region data) UltraEdit (3M w/o region data)"
        },
        {
            "title": "No\nNo",
            "content": "UltraEdit X-Planner + UltraEdit X-Planner + UltraEdit X-Planner + Bag of Models No X-Planners Mask X-Planners Seg. Mask + X-Planners Bounding Box X-Planners Seg. Mask + X-Planners Bounding Box 0.1213 0.0652 0.0895 - 0.0515 0.0713 0.0611 0.0462 0.0457 0.0443 0.8518 0.9179 0.8622 0. 0.8915 0.8446 0.8627 0.9007 0.9029 0.9046 0.2742 0.2763 0.2843 0.2330 0.2804 0.2832 0.2802 0.2782 0.2798 0.2822 0.7656 0.8964 0.8358 0. 0.8656 0.7937 0.8079 0.8723 0.8766 0.8754 Table 6. Effectiveness of MLLM-Based Verification and Correction on COMPIE Evaluation. We evaluate X-Planner with closed-loop verification using GPT-4o and InternVL2.5-38B as step-wise verifiers. Our method achieves consistent gains over baselines by correcting intermediate errors through re-generation. We report improvements in both instruction-image alignment (M LLMti) and visual consistency (M LLMim), highlighting the benefits of MLLM-guided correction for complex multi-step editing. Guidance & Error Verify L1 CLIPim CLIPout DINO MLLMti MLLMim"
        },
        {
            "title": "No\nNo",
            "content": "No Decomp.+No Verification Mask + Decomp.+No Verification UltraEdit (UE) X-Planner+UE X-Planner+UE X-Planner+UE (GPT-4o) Mask+Decomp.+Verification (max: 1) X-Planner+UE (GPT-4o) Mask+Decomp.+Verification (max: 4) X-Planner+UE(InternVL) Mask+Decomp.+Verification (max: 1) X-Planner+UE(InternVL) Mask+Decomp.+Verification (max: 4) 0.7713 0.7692 0.7688 0.7767 0.7875 0.7853 0.7942 0.7861 0.7901 0.2512 0.2498 0.2698 0.2621 0.2569 0.2563 0.2574 0.2559 0. 0.6044 0.5981 0.6387 0.6435 0.6599 0.6612 0.6673 0.6632 0.6647 0.6511 0.6408 0.6652 0.6894 0.7061 0.7113 0.7308 0.7128 0.7258 0.5347 0.5288 0.5523 0.5593 0.5744 0.5798 0.5936 0.5822 0. 0.2764 0.2988 0.1292 0.1253 0.1188 0.1175 0.1163 0.1180 0.1160 5 Table 7. Quantitative Comparison on the MagicBrush Test Set (X-Planner Trained from GPT-4o vs Pixtral-Large Generated Data). We show single-turn (left) and multi-turn (right) performance. Our approach, using predicted masks and boxes, rivals or outperforms UltraEdit with human labels. Bag of Models uses PowerPaint [36] for removal, InstructDiff [13] for style, and UltraEdit otherwise. (a) Single-Turn Editing (b) Multi-Turn Editing Methods UltraEdit (UE) UltraEdit (UE) Control L1 L2 CLIP-I DINO Methods Control L1 L2 CLIP-I DINO No Human Mask 0.0614 0.0575 0.0181 0.0172 0.9197 0.9307 0.8804 0.8982 UE w/o region UE w/ region No Human Mask 0.0780 0.0745 0.0246 0.0236 0.8954 0.9045 0.8322 0.8505 X-Planner (GPT-4o Generated Training Data) X-Planner+UE Mask X-Planner+UE Mask+Box X-Planner+Bag of Models Mask+Box 0.0528 0.0513 0.0511 0.0171 0.0168 0.0172 0.9281 0.9312 0.9331 X-Planner (Pixtral Generated Training Data) X-Planner+UE Mask X-Planner+UE Mask+Box X-Planner+Bag of Models Mask+Box 0.0529 0.0508 0. 0.0173 0.0165 0.0174 0.9300 0.9324 0.9342 0.8900 0.8959 0.8970 0.8908 0.8983 0.8985 X-Planner (GPT-4o Generated Training Data) X-Planner+UE Mask X-Planner+UE Mask+Box X-Planner+Bag of Models Mask+Box 0.0679 0.0668 0.0665 0.0227 0.0226 0.0223 0.9025 0.9047 0.9079 X-Planner (Pixtral Generated Training Data) X-Planner+UE Mask X-Planner+UE Mask+Box X-Planner+Bag of Models Mask+Box 0.0685 0.0669 0. 0.0230 0.0225 0.0222 0.9031 0.9057 0.9083 0.8423 0.8475 0.8508 0.8429 0.8471 0.8514 Table 8. Quantitative Comparison on the COMPIE Benchmark (X-Planner Trained from GPT4o vs Pixtral-Large Generated Data). X-Planner significantly improves the editing performance of UltraEdit and InstructPix2Pix* by decomposing complex instructions and providing control guidance inputs (e.g., segmentation masks). To overcome the limitations of CLIPout in handling complex instructions, we utilize an MLLM as an alternative evaluation metric to highlight capabilities of X-Planner. Guidance Control Input L1 CLIPim CLIPout DINO MLLMti MLLMim Methods SmartEdit MGIE UltraEdit No No No 0.2764 0.2988 0.7713 0.7692 0.1292 0.7688 X-Planner + UltraEdit X-Planner + UltraEdit X-Planners Decomposed Instruction X-Planners Mask + Decomposed Instruction 0.1253 0.1188 0.7767 0.7875 X-Planner Trained from GPT4o Generated Data X-Planner Trained from Pixtral-Large Generated Data X-Planner + UltraEdit X-Planner + UltraEdit InstructPix2Pix* X-Planners Decomposed Instruction X-Planners Mask + Decomposed Instruction 0.1261 0.1207 0.7744 0.7853 No 0. 0.8020 X-Planner Trained from GPT4o Generated Data X-Planner + InstructPix2Pix* X-Planner + InstructPix2Pix* X-Planners Mask + Decomposed Instruction X-Planners Decomposed Instruction 0.1458 0.1320 0.8143 0. 0.2630 0.2584 0.2666 0.2641 0.2591 0.2512 0.2498 0.2698 0.2621 0. 0.6044 0.5981 0.6387 0.6435 0.6599 0.6428 0.6577 0.6988 0.7114 0. 0.6511 0.6408 0.6652 0.6894 0.7061 0.6904 0.7102 0.6727 0.7072 0. X-Planner + InstructPix2Pix* X-Planner + InstructPix2Pix* X-Planners Mask + Decomposed Instruction X-Planners Decomposed Instruction 0.1460 0.1325 0.8141 0.8291 0.2655 0.2586 0.7122 0. 0.7088 0.7431 X-Planner Trained from Pixtral-Large Generated Data 6 0.5347 0.5288 0.5523 0.5593 0. 0.5626 0.5765 0.6160 0.6277 0.6454 0.6295 0.6488 Figure 12. X-Planners Decomposition Outputs (Simplified Instructions, Edit Types, Masks, and Bounding Boxes). This figure illustrates examples of X-Planners decomposition results, showcasing its ability to handle diverse edit types. The segmentation masks generated are tailored to the specific edit type. For instance, in Row 3, the [replace] edit features both before and after masks which are combined into single mask for editing guidance, with appropriate dilation applied to ensure accurate representation. Also, in Row 1, the [shape change] edit provides more dilated mask by giving more rooms for potential shape modification. Figure 13. Qualitative Comparison on MagicBrush, and on COMPIE Benchmark between X-Planner and UltraEdit (I). This figure presents: (1) comparison of X-Planner with several baseline methods for multi-turn editing on the MagicBrush dataset and (2) qualitative comparison with the UltraEdit baseline on the COMPIE benchmark. For multi-turn editing, X-Planner, guided by its generated masks, demonstrates superior identity preservation. In contrast, many baseline methods, especially InstructPix2Pix, tend to overedit the image due to lack of control. 8 Figure 14. Qualitative Comparison between X-Planner and both InstructPix2Pix* (I) and UltraEdit Baseline (II). This figure compares results from both InstructPix2Pix* and UltraEdit Baseline with and without the integration of X-Planner. These examples highlight the advantages of X-Planners instruction decomposition and mask controllability over the baseline which is directly given with complex instruction. 9 Figure 15. Qualitative Comparison between X-Planner and InstructPix2Pix* Baseline (II). This figure compares results from InstructPix2Pix* with and without the integration of X-Planner. For the [replace] edit type in Rows 4, X-Planner generates dilated segmentation mask for the replaced region to better accommodate the editing model. Similarly, in Row 2, the [shape change] edit includes dilated mask to account for potential changes. These examples highlight the advantages of X-Planners instruction decomposition and mask controllability over the baseline which is directly given with complex instruction. Figure 16. Examples of Non-Rigid and Compositional Edits with Model-Specific Failure Cases. We show diverse complex edits (especially the [shape change] edits) generated by plugging X-Planner into three editing models: InstructPix2Pix*, GPT-4o [1], and ICEdit [49]. Despite not being trained on such complex instructions, these models can perform challenging edits thanks to our decomposition and localization planning. However, some failure cases arise from the editing models: GPT-4o struggles with identity preservation in face edits (e.g., Row 4, red boxes)."
        }
    ],
    "affiliations": [
        "Adobe",
        "HKU",
        "UC Berkeley"
    ]
}