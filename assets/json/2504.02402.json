{
    "paper_title": "EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling",
    "authors": [
        "Hao Yin",
        "Shi Guo",
        "Xu Jia",
        "Xudong XU",
        "Lu Zhang",
        "Si Liu",
        "Dong Wang",
        "Huchuan Lu",
        "Tianfan Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method."
        },
        {
            "title": "Start",
            "content": "EvMic: Event-based Non-contact Sound Recovery from Effective Spatial-temporal Modeling Hao Yin2,1 Shi Guo1 Xu Jia2 Xudong Xu1 Lu Zhang2 Si Liu4 Dong Wang2 Huchuan Lu2 Tianfan Xue3, 1Shanghai AI Laboratory 2Dalian University of Technology 3The Chinese University of Hong Kong 4Beihang University 5 2 0 2 ] . [ 1 2 0 4 2 0 . 4 0 5 2 : r Figure 1. Illustration of our event-based non-contact sound recovery. We try to recover sound from the visual vibration of the object caused by the sound wave. Compared with the traditional high-speed camera solution (top), we proposed to use an event camera to capture temporally dense signal (bottom). We first utilize laser matrix (left) to amplify the gradient and an event camera to capture the vibrations. Then, our learning-based approach to spatial-temporal modeling enables us to recover better signals."
        },
        {
            "title": "Abstract",
            "content": "When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate large training set using novel simulation pipeline. Then we designed network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method. Our project page: https://yyzq1.github.io/EvMic/. 1. Introduction Non-contact sound recovery has important applications in engineering and scientific fields, such as in surveillance or * indicates equal contributions. This work was done during Hao Yins internship at Shanghai Artificial Intelligence Laboratory. indicates corresponding author. investigation of the physical properties of materials [7, 33]. This technique mainly relies on cameras to capture subtle deformation of objects caused by sound pressure. As shown in [10], there is an approximate linear relationship between sound pressure and the displacement of the deformed surface, which shows the feasibility of sound recovery just from visual vibration. Still, recovering sound signals only from visual cameras remains challenging. This is mainly because object vibrations caused by sound are of both high-frequency and lowamplitude. Vibration frequency often exceeds 1kHz, even faster than some high-speed cameras and the amplitude of vibration is often much less than 1 pixel. Early work [32] utilizes laser Doppler velocimeters (LDVs), which have complex optical paths and can only measure single point. Researchers then explore building system with simpler optical design [2, 12, 39, 43] by either casting speckle patterns on objects or using phase-based signal processing [10]. Still, these solutions are limited by the bandwidth and sampling rate of frame-based cameras, and are hard to recover sound beyond 1kHz. Recently, sampling rate has been increased by designing different camera systems, either using fast-frame-rate 1D sensors [3, 37, 38] or dual-shutter camera system [33], but these come at the cost of narrower field of view. In short, frame-based cameras have to make tradeoffs among three factors: simple optical paths, large field of view, and high sampling rate and bandwidth of the sensors. Event cameras, characterized by very high temporal resolution, are introduced to achieve better balance among the above points. Unlike conventional frame-based cameras that capture all pixels intensity at fixed frame rate, event cameras only record pixels with certain levels of brightness changes. Because of this sparsity, event cameras can both cover large field of view and capture temporally dense signals. These properties make event cameras appealing for high-frequency vibration analysis. To utilize such characteristics of event signals for vibration analysis, Dorn et al. [11] and Niwa et al. [28] adapted the phase-based method to estimate vibration from the event stream, but they are limited to vibrations along single orientation. Zero crossing detection is adopted in [22] for event-based sound recovery but is quite sensitive to noise. These solutions did not fully exploit spatial-temporal information from event streams and did not use deep learning to utilize data prior. In this work, we propose novel non-contact sound recovery system based on event cameras. Unlike previous event-based vibration analysis, we propose novel network structure that can jointly model long temporal dependencies and spatial relationships among events, resulting in highquality sound recovery. To leverage the power of datadriven techniques, we also collect new dataset for eventbased sound recovery. Since collecting paired data for the non-contact sound recovery task in the real world is challenging, we resort to synthetic data. Specifically, we introduce EvMic, the first synthetic dataset for event-based non-contact sound recovery, created using Blender [9] and an event simulator [23]. Furthermore, to validate the effectiveness of the proposed method, we also collected event data in real-world scenarios for evaluation. Since the vibrations on the objects hit by the high frequency sound wave are extremely subtle, we design an imaging system that utilizes laser matrix to amplify the objects surface gradients thereby capturing more informative event signals. The laser matrix, comprising multiple laser points, also preserves the advantage of large field of view. To the best of our knowledge, this is the first work that investigates the potential of event cameras for sound recovery using deep learning techniques. The proposed network consists of three parts. First, sparse convolution is adopted to leverage the sparsity nature of event data for lightweight visual feature extraction. Second, we use the Mamba model to extract temporal information from the event stream, utilizing its advantage in modeling long sequences. Third, we propose specklebased spatial aggregation block to further improve robustness. When sound waves interact with the objects surface, they produce spatially varying vibrations. The magnitudes and directions of vibrations, when projected onto the image plane, are modulated by the local surface normal orientations, and we utilize this property to build the spatial aggregation block. 2. Related work Non-contact sound recovery. The non-contact sound recovery aims to measure object vibrations in non-contact manner using vision sensors. Early studies [32] utilize laser Doppler velocimeters(LDVs) which offer high precision but are limited to measuring vibrations at single point and require complex optical path. The roughness of the objects surface can create speckle patterns with the laser. Some studies [2, 12, 39, 43] utilize frame-based high-speed cameras to capture speckle patterns to address the shortcomings of LDVs. However, reliance on highspeed cameras introduces limitations in sampling rate and bandwidth. Some works attempt to use 1D sensors. They achieved higher sampling rate but at the expense of the field of view. Additionally, they can only measure vibrations in single direction. Sheinin et al. [33] introduced dual shutter imaging system, replacing the high-speed camera with two simultaneously low-speed cameras equipped with rolling and global shutter sensors. This approach no longer requires high-speed cameras but the arrangement of laser points is restricted by the dual shutter system. Davis et al. [10] directly captured vibrating objects with high-speed frame-based cameras and designed phase-based method to recover sound waves. This method eliminated the optical path of LDV and achieved larger field of view but is still limited by high-speed frame-based cameras. Our work captures vibrating objects using event cameras. This allows us to simultaneously obtain high-frequency data and large field of view while maintaining simple optical path. Event-based non-contact vibration measurement. Event cameras trigger each pixel asynchronously, allowing for extremely high temporal resolution and eliminating motion blur. Additionally, event cameras detect only areas where brightness changes occur, thereby avoiding the capture of static background information and significantly reducing bandwidth usage. These features make event cameras exceptionally well-suited for vibration measurement. Some works attempt to detect micro vibrations [1, 8, 26, 27, 31]. Ge et al. [15, 16] employed speckle patterns to analyze micro displacements. Shi et al. [34] used Laser-Assisted Illumination and proposed method based on mixed Gaussian distribution to analyze signals of up to 300 Hz. Dorn et al. [11] adopted phase-based method with event streams for vibration analysis, achieving high precision but not attempting to recover high-frequency signals. Niwa et al. [28] and Howard and Hirakawa [22] further explored the use of an event camera for non-contact sound recovery. These methods [11, 22, 28], offer broad field of view but do not fully utilize the spatial-temporal information. In this work, we propose learning-based approach for effective spatial-temporal modeling, thereby enabling the recovery of clearer sound waves. 3. Problem formulation Event representation. Event cameras are bio-inspired sensors that asynchronously capture changes in brightness at each pixel location. An event is triggered when the change in log brightness exceeds predefined threshold C, and its polarity is defined as: = 1, 1, 0, if (u, t) (u, t) C, if (u, t) (u, t) C, otherwise, (1) where represents the log brightness, = (x, y) denotes the pixel coordinates, is the timestamp. The output from the event camera over given time period is set of events, defined as {ek} = {uk, tk, pk}K k=1. denotes the total number of events. Event-based non-contact sound recovery. When sound waves strike an object, the induced sound pressure causes slight deformations on its surface, resulting in vibrations that synchronize with the frequency of the sound wave. These vibrations lead to variations in image brightness, which in turn trigger high-frequency event signals. The primary objective of this task is to analyze the event signals generated by object vibrations and to recover the sound information from these visual signals. To formalize this process, we first define the pressure of the sound wave at time as qt. Davis et al. [10] demonstrated that the displacement of an object vibrating under sound wave of certain frequency is approximately linearly related to the sound pressure as: δt1t = α(qt qt1), (2) where δt1t represents the motion field over the time interval between t1 and t, and α is the linear coefficient that depends on the material properties and the frequency of the sound wave. Under the assumption of constant illumination, for small t, the logarithmic change in brightness can be approximated as [14]: L δt1t (α(qt qt1)), (3) where denotes the gradient of L. By substituting the event representation from Eq. (1) into Eq. (3), we obtain the relationship between sound pressure and event signal, expressed as: pkC (α(qt qt1)). (4) The goal is to recover the sound pressure qt from the event stream {pk}, thereby reconstructing the sound wave. High-frequency vibrations typically correspond to smaller sound pressure changes = qt qt1. Frame-based cameras record absolute intensity to capture subtle changes, while events are generated by the combined effects of gradients and subtle sound pressure changes (see Eq. (4)). To enhance the event signal, we use laser to enhance the objects surface gradient (as illustrated in Fig. 1). 4. Method 4.1. Data simulation In real-world scenarios, obtaining ground truth for noncontact sound recovery tasks is challenging due to alignment issues, reverberation, and the objects unknown response to sound. Inspired by other tasks [8, 29], we create EvMic, simulated dataset that contains approximately 10,000 segments of data. The simulation pipeline begins with synthesizing highframe-rate video sequences. We use Blender [9] to render realistic visual effects. The key aspect of the simulation is controlling the objects vibrations according to the audio, as shown in Fig. 2 (a). Specifically, we generate random direction in Blender, with the audio signal serving as the vibrational displacement along it. Moreover, the audio will be multiplied by random gain. The rendered video (346260 {Pi}N i=1, our network comprises three main components: feature extraction module (extracting features from each patch individually), spatial aggregation module (aggregating information across patches), and temporal modeling module (capturing long-term temporal information). Leveraging the sparsity of event data, sparse convolutional backbone is implemented for efficient visual feature extraction. In the spatial aggregation module, multi-head selfattention strategy is designed to adaptively aggregate spatial information across different vibration directions. Finally, to efficiently capture long-term temporal dependencies, we introduce mamba-based [18] temporal modeling module. Details of these three key components are provided below. Lightweight visual feature extraction. Since each patch Pi has large temporal dimension, exceeding 2K, visual feature extraction introduces significant computational overhead. Fortunately, event data is inherently sparse, and the small amplitudes characteristic of audio-induced vibrations further contribute to this sparsity. To effectively harness this sparsity and reduce computational costs, we developed ResNet18 [20] network based on sparse convolutional layers (SPconv) [17, 24, 41] to extract visual features, denoted as = {fi}N i=1 RN C, where is the feature dimension. Sparse convolutions perform computations only in non-zero regions, which means calculations occur solely at the locations where events are generated, significantly alleviating the computational burden on the model. Spatial aggregation block (SAB). To robustly estimate sound vibration, it is important to spatially aggregate features extracted at each local region to increase the signalto-noise ratio. However, the varying normal directions on the objects surface cause vibrations along different directions with varying amplitudes in the image plane. Using the entire voxel for aggregation averages patches across these directions, potentially leading to signal interference. To address this issue, we propose an adaptive spatial aggregation module utilizing multi-head self-attention (MSA) [36]. For each timestamp of the input feature , the feature is represented as ft RN C. First, projections Qt = Fq(ft), Kt = Fk(ft), and Vt = Fv(ft) are computed using the projection matrices Fq, Fk, and Fv. The aggregated feature gt is then calculated as: gt = SoftMax (cid:19) (cid:18) QtK Vt, (5) where is the normalization parameter. Finally, we obtain the aggregated feature across all patches and timestamps, denoted as RN C. To train the spatial aggregation module (SAB), we use the simulated vibrating speckles illustrated in Fig. 2 (b). Temporal information modeling. Audio sequences exhibit long-term temporal correlations, which are challengInspired by the success of structured state ing to model. Figure 2. (a) Our data simulation starts with controlling the objects vibration. We utilize audio to manipulate the coordinates of objects resulting in their vibrations across random directions. Then we use an event simulator to generate the corresponding events. The generated events are used for training. (b) The synthetic vibrating speckles are used for fine-tuning and testing. resolution) exhibits both pixeland sub-pixel-level vibrations based on audio and gain. We then simulate events using V2E [23], with the results visualized in Fig. 2(a). To further bridge the gap between sim and real, set of supplementary data is synthesized in the same manner as shown in Fig. 2 (b). Multiple speckles are controlled to vibrate according to the same audio signal, with each exhibiting unique vibration directions and signal gains. These additional data are utilized to finetune the spatial aggregation block, enhancing the models generalization ability. 4.2. Network architecture Our network is designed to estimate sound from event signals that exhibit temporally dense yet spatially sparse characteristics, as illustrated in Fig. 3. Events are converted into spatio-temporal voxel grids for neural network processing, following the method in [42], common usage in event-based vision. Due to the high frequency of the sound signals, large number of bins in the event voxel grids are required to retain fine-grained temporal information. Since events triggered by sound vibrations are primarily concentrated around laser speckles, patches centered on these speckles are first extracted using contour extraction. These patches are denoted as = {Pi}N i=1, where is the speckle number. Each patch Pi has the dimensions 2pwiphi, where pwiphi is the patch size, 2 represents distinct voxel grids containing e+ and for positive and negative events respectively. is the length of the time series, equal to the number of bins in the event voxel grid, which is set to 4K in our training process. To estimate sound from the speckle patches = Figure 3. (a) Overview of our proposed network architecture. The event stream is processed into voxel grids, from which patches centered around the speckles are selected. First, the patches are input into sparse convolution-based lightweight backbone to extract visual features. Next, spatial attention block aggregates the information in the different patches. Finally, Mamba is employed to model long-term temporal information and reconstruct the audio that caused the objects vibration. (b) and (c) illustrate the detailed structure of SAB and SSM. (c) At time gt is the input feature, ot is the output and ht denotes the hidden state. A, B, and are the gating weights optimized by Mamba. is used to discretize the continuous parameters and B. space models (SSMs) [18, 19] in capturing long-term temporal information, we develop Mamba-based [18] temporal aggregation module to capture the temporal dependencies in the features and generate the corresponding audio signal. The detailed structure is shown in Fig. 3 (c). For the input feature of all patches at each timestamp, denoted as gi RN C, the output can be modeled as: ht = Aht1 + Bgt, ot = Cht, (6) where ht is the hidden state at time t, oi denotes the output audio signal from the i-th patch, = exp(A), = (A)1(exp(A)I)B. (7) Here, RN , RN 1, and R1N are learnable parameters, and is used to discretize the continuous parameters and B. 4.3. Loss function Scale-invariant source-to-noise ratio (SISNR) loss. The SISNR loss [25] is used to assess the similarity between two signals. Compared to traditional SNR loss, SISNR loss is less affected by variations in volume, defined as, Lsisnr = 10 log (cid:16) α ˆo2 2 / ˆo o2 2 (cid:17) , (8) where and ˆo are the predicted and ground-truth audio signal, and α = ˆo, o/o2 2. Multi-scale spectral reconstruction loss. To supervise audio signals in the frequency domain, we define multi-scale spectral reconstruction loss to enhance the reconstruction of high-frequency components in the audio signal [40]: Lspec = (cid:88) Ss (o) , Ss (ˆo) s=26,2729 + αs log(Ss (o)), log(Ss (ˆo)) 2, (9) where Ss(o) denotes the Mel-spectrogram of the audio signal with scale s. In our setting, is adjusted to = 26, 27 29 according to the sampling rate. Additionally, αs = (cid:112)s/2 serves as scale-dependent weighting factor. The total loss is defined as Ltotal = Lsisnr+βLspec,, where β = 1e 4 is set in our experiments. 5. Experiments 5.1. Training details Experiments are conducted using the PyTorch framework [30] on single NVIDIA 4090 GPU, training the model on the proposed EvMic dataset. The training process is divided into two phases to ensure stable training: First, pre-training the modules without the spatial aggregation block (SAB). The entire network is then fine-tuned. During the training process, predefined regions are used to partition the patches. SGD [5] was used as the optimizer with learning rate of 104. 0.5-second event stream is divided into 2000 bins for input, which corresponds to temporal resolution of 4000 Hz. The batch size is set to 1, with 200k iterations for pre-training and another 200k iterations for fine-tuning. Table 1. Quantity comparison results of our model with other methods on the synthetic data. Female-SA1 Male-SA1 Female-SA Male-SA"
        },
        {
            "title": "Methods",
            "content": "RGBPhase EvPhase Ours(8kHz) SNR -2.992 -1.883 1.159 STOI 0.389 0.286 0.451 SNR -2.976 0.183 0.809 STOI 0.419 0.485 0.542 SNR -2.578 1.465 1. STOI 0.246 0.482 0.479 SNR -2.801 -0.080 0.959 STOI 0.237 0.251 0.452 SNR -2.837 -0.079 1.214 STOI 0.322 0.376 0.481 Figure 4. Qualitative comparison results on the real-world data of chipbag. Audio is provided in the supplementary. 5.2. Baseline methods Since there are currently no public datasets or code for event-based sound recovery, we implemented the state-ofthe-art method by Dorn et al. [11] as our baseline (referred to as EvPhase) and also included the frame-based approach by Davis et al. [10] (referred to as RGBPhase). To ensure fair comparison, we fixed the filter parameters of EvPhase, setting = 13, σ = 3, λ = 32, γ = 1, ϕ = 0, θ = π 2 , based on the description in [28]. Following [10], we also applied denoising methods as post-processing for the audio outputs of all comparison methods. For speech data, we used FullSubNet-plus [6], while spectral subtraction [4] was employed for other types of data. Butterworth high-pass filter was then applied to remove low-frequency noise. All audio results are provided in supplementary and we encourage readers to listen to them. trained with an input sampling rate of 4kHz, as discussed in [44], the mamba-based structure demonstrates strong adaptability when the frequency of the input data changes, and our method can easily extended to 8kHz by increasing the sampling rate at testing time. Tab. 1 presents quantitative results for the synthetic sequences. The SNR and STOI metrics demonstrate that the proposed method outperforms other approaches on most sequences. Compared to EvPhase [11], our approach achieves an improvement of 1.293 dB in SNR and 0.105 in STOI. Furthermore, since the filter parameters in EvPhase [11] are fixed and not adaptable to the randomly generated vibration directions, it performs suboptimally on the Female-SA1 sequence. In contrast, our method does not require manual adjustments for the direction of movement. Due to the limited sampling rate, RGBPhase [10] produces recovered signals that are inferior to event-based methods in terms of frequency bandwidth and signal coherence. 5.2.1. Results on the synthetic data 5.2.2. Results on the real-world data Synthetic data was generated using audio samples from the TIMIT dataset [13]. High-frame-rate videos at 10, 000 FPS with resolution of 640 360 were created, and events were simulated using V2E. For the RGB method, videos are downsampled to 2, 500 FPS to replicate real-world conditions with limited bandwidth. Evaluation metrics included Signal-to-Noise Ratio (SNR) and Speech Intelligibility (STOI) [35]. To note that, even though our model is To make the comparison in real-world scenarios, we captured several sequences with the Prophesee EVK4 camera (resolution of 1280 720), paired with 4mm lens and 5mW laser matrix. We have evaluated four different setups, shown in Figs. 4 to 7. Additionally, as reference signals, audios from the scene were recorded using microphone. Fig. 4 and Fig. 5 shows spectrograms of the recovered sound. Similar to [10], we conducted experiments on Figure 5. Qualitative comparison results on the real-world data of speaker. Audio is provided in the supplementary. chipbag hit by sound waves, as shown in Fig. 4. Fig. 4 (b) shows the accumulated events within 250ms temporal window. The top row of Fig. 4 (c), (d), and (e) presents the recovery results for the MIDI audio alongside the corresponding microphone recordings. Musical instruments and vocal performances typically consist of fundamental frequency and harmonics that are integer multiples of that fundamental. The audio recovered by our method closely matches the microphone recordings regarding the fundamental frequency, whereas the harmonics in the results obtained by EvPhase [11] magnify the high-frequency component, highlighted by the purple boxes. Fig. 5 shows the result of sound directly output from speaker. In comparison to the chipbag, it generates larger vibration amplitude, resulting in clear audio signals. The audio spectrogram recovered by our method more closely resembles the results obtained from the microphone recordings, whereas the EvPhase [11] method exhibits similar issues to those observed in the chipbag results  (Fig. 4)  . 5.3. Discussion Large field of view. To demonstrate our system can cover large field of view, we collected multiple sets of data from long distance. As shown on the top of Fig. 6, we captured glitter papers while playing chirp audio with frequency range of 200 to 2KHz. Fig. 6 (b), (c), and (d) illustrate the audios recovered by our model, demonstrating that signals can be recovered from various positions within the scene. We further attempted to separate different sound sources in the scene. As shown at the bottom of Fig. 6, stereo 8bit music was played through the left and right speakers, while the central speaker played mixed mono channel. We successfully recovered the individual audio signals from the side speakers and reconstructed stereo audio output. Sparse convolution. To demonstrate the effectiveness of sparse convolution, we calculate the average computational Table 2. Compare the computational load and memory usage of sparse convolution-based ResNet18 and traditional ResNet18 on EvMic. Model w/o Sparse Convolution Sparse Convolution VRAM (G) 22.715 7.461 Flops (G) 1171.226 51.229 Table 3. Ablation analysis for our modules on the synthetic data. Model SPconv + Transformer SPconv + LSTM SPconv + Mamba SPconv + SAB + Mamba SNR -0.195 0.015 0.309 1.214 STOI 0.437 0.453 0.474 0.481 load and memory usage at inference time on the EvMic dataset. We use input data with the dimensions RCT HW , where = 2 is the polarity of the events, = 256 and = 344 represent the spatial dimensions, and = 2k denotes the temporal length. Tab. 2 illustrates the memory usage and computational load of the two methods. The sparse backbone utilizes the sparsity of event data for lightweight visual feature extraction, using only 32.65% of the memory and 4.37% of the computational load compared to traditional CNNs. Temporal modeling and spatial aggregation. We conduct the ablation analysis on the remaining modules. We temporal modeling apquantitatively compare different proaches [18, 21, 36], and the inclusion of the spatial aggregation block (SAB) in Tab. 3. The transformer performs poorly in metrics. Qualitative results are available in our supplementary materials. Mamba outperforms LSTM by achieving 0.294dB improvement in SNR and 0.021 increase in STOI, demonstrating its exceptional performance Figure 6. Capture objects from distance to obtain large field of view. Top: Capture glitter papers while playing chirp audio. Bottom: Capture multiple speakers to recover stereo audio. The left and right speakers play left and right channels respectively, while the medium speaker plays mixed mono channel. Audio is provided in the supplementary. Figure 7. Ablation analysis for different vibration direction. The object is placed in different orientations to produce various vibration directions. Audio is provided in the supplementary. in modeling long-term temporal information. We further validated the effectiveness of the SAB module. The inclusion of the SAB resulted in 0.905dB improvement in SNR and 0.007 increase in STOI. Adaptability to different vibration directions. In the experiments shown in Fig. 7(a), we altered the vibration direction by changing the orientation of the objects surface. We keep the parameters of the EvPhase [11] method unchanged (Sec. 5.2). Fig. 7(b) and (c) illustrate the signals recovered by both methods for two different vibration directions. Compared to the signals recovered in the original direction, our method incurs slight quality drop along the new vibration direction. In contrast, the EvPhase [11] method recovers poor signal in the altered vibration direction due to the single spatial orientation of its filter. 6. Conclusion In this work, we explored learning-based pipeline for event-based non-contact sound recovery for the first time. We used Blender to synthesize dataset named EvMic and designed model specifically for this task. Our approach enhanced the quality of the recovered signals by effectively modeling spatial-temporal information. However, some limitations still need to be addressed. First, there is gap between the existing event simulator and the events captured in reality, which leads to decline in performance on real data. Besides, the generation of events is influenced by the gradient. Therefore, the power of the laser and the lighting conditions will affect the number of captured events. In future work, we aim to refine our acquisition system and explore the incorporation of prior knowledge from generative models to further enhance the quality of the signals."
        },
        {
            "title": "References",
            "content": "[1] Dwijay Bane, Anurag Gupta, and Manan Suri. Non-invasive arXiv qualitative vibration analysis using event camera. preprint arXiv:2410.14364, 2024. 3 [2] Silvio Bianchi. Vibration detection by observation of speckle patterns. Applied optics, 53(5):931936, 2014. 2 [3] Silvio Bianchi and Emanuele Giacomozzi. Long-range detection of acoustic vibrations by speckle tracking. Applied optics, 58(28):78057809, 2019. 2 [4] Steven Boll. Suppression of acoustic noise in speech usIEEE Transactions on acoustics, ing spectral subtraction. speech, and signal processing, 27(2):113120, 1979. 6 [5] Leon Bottou. Large-scale machine learning with stochasIn Proceedings of COMPSTAT2010: tic gradient descent. 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers, pages 177186. Springer, 2010. 5 [6] Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, and Helen Meng. Fullsubnet+: Channel attention fullsubnet with complex spectrograms for speech enhancement. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 78577861. IEEE, 2022. 6 [7] Justin G. Chen, Neal Wadhwa, Young-Jin Cha, Fredo Durand, William T. Freeman, and Oral Buyukozturk. Structural modal identification through high speed camera video: Motion magnification. In Topics in Modal Analysis I, Volume 7, pages 191197, Cham, 2014. Springer International Publishing. 2 [8] Yutian Chen, Shi Guo, Fangzheng Yu, Feng Zhang, Jinwei Gu, and Tianfan Xue. Event-based motion magnification. In Computer Vision ECCV 2024, pages 428444, Cham, 2025. Springer Nature Switzerland. 3 [9] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 2, 3 [10] Abe Davis, Michael Rubinstein, Neal Wadhwa, Gautham Mysore, Fredo Durand, and William Freeman. The visual microphone: Passive recovery of sound from video. 2014. 2, 3, 6, [11] Charles Dorn, Sudeep Dasari, Yongchao Yang, Charles Farrar, Garrett Kenyon, Paul Welch, and David Mascarenas. Efficient full-field vibration measurements and operational modal analysis using neuromorphic event-based imaging. Journal of Engineering Mechanics, 144(7):04018054, 2018. 2, 3, 6, 7, 8, 1 [12] Pablo Etchepareborda, Marie-Hel`ene Moulet, and Manuel Melon. Random laser speckle pattern projection for noncontact vibration measurements using single high-speed camera. Mechanical Systems and Signal Processing, 158: 107719, 2021. 2 [13] William Fisher. The darpa speech recognition research database: specifications and status. In Proc. DARPA Workshop on speech recognition, pages 9399, 1986. 6 [14] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jorg Conradt, Kostas Daniilidis, et al. Event-based vision: survey. IEEE transactions on pattern analysis and machine intelligence, 44(1):154180, 2020. 3 [15] Zhou Ge, Nan Meng, Li Song, and Edmund Lam. Dynamic laser speckle analysis using the event sensor. Applied Optics, 60(1):172178, 2020. [16] Zhou Ge, Pei Zhang, Yizhao Gao, Hayden K-H So, and Edmund Lam. Lens-free motion analysis via neuromorphic laser speckle imaging. Optics Express, 30(2):22062218, 2022. 3 [17] Benjamin Graham and Laurens Van der Maaten. SubarXiv preprint manifold sparse convolutional networks. arXiv:1706.01307, 2017. 4 [18] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 4, 5, [19] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 5 [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [21] Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997. 7 [22] Matthew Howard and Keigo Hirakawa. Event-based viIn ICASSP 2023-2023 IEEE Internasual microphone. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 2, 3 [23] Yuhuang Hu, Shih-Chii Liu, and Tobi Delbruck. v2e: From In Proceedings of video frames to realistic dvs events. the IEEE/CVF conference on computer vision and pattern recognition, pages 13121321, 2021. 2, 4, [24] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 806814, 2015. 4 [25] Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8):12561266, 2019. 5 [26] Yuanyuan Lv, Liang Zhou, Zhaohui Liu, and Haiyang Zhang. Structural vibration frequency monitoring based on event camera. Measurement Science and Technology, 35(8): 085007, 2024. 3 [27] Woong-jae Na, Kyung Ho Sun, Byeong Chan Jeon, Jaeyun Lee, and Yun-ho Shin. Event-based micro vibration measurement using phase correlation template matching with event filter optimization. Measurement, 215:112867, 2023. 3 [28] Ryogo Niwa, Tatsuki Fushimi, Kenta Yamamoto, and Yoichi Ochiai. Live demonstration: Event-based visual microphone. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023 - Workshops, Vancouver, BC, Canada, June 17-24, 2023, pages 40544055. IEEE, 2023. 2, 3, 6 [29] Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fredo Durand, William Freeman, and Wojciech Conference on Computer Vision (ECCV) Workshops, pages 00, 2018. 4 [43] Jan Zizka, Alex Olwal, and Ramesh Raskar. Specklesense: fast, precise, low-cost and compact motion sensing using laser speckle. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 489498, 2011. 2 [44] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza. In Proceedings of State space models for event cameras. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58195828, 2024. 6 In Matusik. Learning-based video motion magnification. Proceedings of the European Conference on Computer Vision (ECCV), pages 633648, 2018. 3 [30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 5 [31] Bernd Pfrommer. Frequency cam: Imaging periodic signals in real-time. arXiv preprint arXiv:2211.00198, 2022. 3 [32] Steve Rothberg, JR Baker, and Neil Halliwell. Laser vibrometry: pseudo-vibrations. 1989. 2 [33] Mark Sheinin, Dorian Chan, Matthew OToole, and Srinivasa Narasimhan. Dual-shutter optical vibration sensing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1632416333, 2022. 2 [34] Chenyang Shi, Ningfang Song, Boyi Wei, Yuzhen Li, Yibo Zhang, Wenzhuo Li, and Jing Jin. Event-based vibration frequency measurement with laser-assisted illumination based on mixture gaussian distribution. IEEE Transactions on Instrumentation and Measurement, 72:113, 2023. 3 [35] Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen. An algorithm for intelligibility prediction of timefrequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):2125 2136, 2011. 6 [36] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4, [37] Nan Wu and Shinichiro Haruyama. Fast motion estimation of one-dimensional laser speckle image and its application on real-time audio signal acquisition. In Proceedings of the 6th International Conference on Communication and Information Processing, pages 128134, 2020. 2 [38] Nan Wu and Shinichiro Haruyama. The 20k samples-persecond real time detection of acoustic vibration based on displacement estimation of one-dimensional laser speckle images. Sensors, 21(9):2938, 2021. 2 [39] Zeev Zalevsky, Yevgeny Beiderman, Israel Margalit, Shimshon Gingold, Mina Teicher, Vicente Mico, and Javier Garcia. Simultaneous remote extraction of multiple speech sources and heart beats from secondary speckles pattern. Optics express, 17(24):2156621580, 2009. 2 [40] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2022. 5 [41] Pengyu Zhang, Hao Yin, Zeren Wang, Wenyue Chen, Shengming Li, Dong Wang, Huchuan Lu, and Xu Jia. Evsign: Sign language recognition and translation with streaming events. In European Conference on Computer Vision, pages 335 351. Springer, 2025. 4 [42] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based optical flow using motion compensation. In Proceedings of the European EvMic: Event-based Non-contact Sound Recovery from Effective Spatial-temporal Modeling"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Supplementary audio results and video To facilitate comprehensive comparison among different methods, our supplementary material includes the audio results of our proposed method, EvPhase [11], and RGBPhase [10] (available in the audio results folder). Additionally, we provide video that demonstrates our main pipeline along with our real-world experiments. textures from the accumulated event frames, as illustrated in Fig. 2(b). Specifically, we extracted multiple patches from the accumulated event frames, each centered on speckle with resolution of 32 32. These patches were then upsampled to resolution of 128 128 using bilinear interpolation. Then we initialized black background with resolution of 2560 1440, using the audio to control the inter-frame displacements, allowing the patches to vibrate in random directions. To ensure diversity in the vibration amplitude, we multiplied the audio by random gain ranging from 0.5 to 2, and the resulting images were downsampled to resolution of 640 360. After generating the video sequences, we employed V2E [23] to simulate the events. The resulting speckle sequences are utilized to finetune the spatial aggregation block. Fig. A1 illustrates the recovered audio using models trained with and without the synthetic speckle data. The model trained with speckle data achieved more comprehensive signal recovery, as highlighted in the purple box. Figure A1. Qualitative comparison for models trained or w/o speckle data. B. Synthetic speckle data. In real-world scenarios, we employ laser matrix to enhance surface gradients, with events primarily occurring at the laser speckles. However, the absence of speckle patterns in our training dataset adversely affects the models generalization. To bridge the gap between simulated and real data, we captured real scene recordings and extracted the speckle Figure A2. Qualitative results for ablation analysis. Figure A3. Qualitative comparison results of our model with other methods on the synthetic data. Figure A4. Phase shift introduced by sound propagation. C. Additional ablation analysis As mentioned in the main paper, the transformer performs poorly in metrics among the compared approaches. The low correlation between the noisy static frames results in evenly distributed attention weights. This leads to their influence from other frames within the time window. As shown in Fig A2, the transformer incorrectly restored sound in the absence of any audio (highlighted by the red boxes). Table A1. Quantity results on the real-world data."
        },
        {
            "title": "Methods",
            "content": "EvPhase Ours(8kHz) Chipbag-Speech STOI SNR 0.290 -1.540 0.383 -0.511 Chipbag-MIDI STOI SNR - 1.122 3.867 - Speaker-Speech STOI SNR 0.501 -2.963 0.506 -2.866 Speaker-MIDI STOI SNR - 0.681 3.338 -"
        },
        {
            "title": "Average",
            "content": "SNR -1.350 0.957 STOI 0.396 0.445 attempted to analyze this phase shift. The experimental setup is shown in Fig. A4 (a). We played 1,000 Hz sine wave through the speaker on the right, allowing the sound to propagate toward the left. Using our method, we reconstructed the audio at sampling rate of 8000 Hz. The reconstructed waveforms for different regions (marked with different colors) are shown in Fig. A4 (b). We plotted the phase shift variation during the sound propagation process as line graph, as illustrated in Fig. A4 (c). D. Qualitative results on synthetic data Fig. A3 illustrates the qualitative results on synthetic data, presented in spectrograms. RGBPhase [10] is limited by the sampling rate and can only recover information below 1 kHz. Due to the single spatial orientation of the filter, EvPhase [11] performs poorly in Female-SA1 with fixed parameters. In contrast, our method achieves higher signal-to-noise ratio and better integrity in the recovered signals compared to EvPhase [11]. E. Quantity results on the real-world data. We use audios recorded by microphone as ground truth to calculate quantitative metrics on real data, as shown in Tab. A1. However, several factors affect this approach: (1) room reverberation; (2) temporal misalignment between the microphone recordings and the recovered audios; and (3) varying object responses to different frequencies, leading to discrepancies between their vibrations and the microphone recordings. Consequently, the microphone recordings do not represent ideal ground truth. F. Inference times for all models. We measured the inference time for all compared methods on the synthetic data, as shown in the Tab. A2. To ensure fair comparison, all experiments were conducted on an Intel 14700K CPU. Here, ours-n indicates the use of speckles for inference. When utilizing single speckle, our inference speed is on par with EvPhase. The comparisons in our main paper are conducted with eight speckles. Under this setting, our method is slower than EvPhase but still offers significant speed advantage over RGBPhase. Table A2. Inference time for all methods. Methods RGBPhase EvPhase Ours-1 Ours-8 132.36 time(s) 744. 29.54 30.13 G. Analyze phase shift in sound propagation. Our imaging system can achieve large field of view, allowing us to analyze the phase shift caused by sound propagation in space. We captured row of glitter papers and"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Dalian University of Technology",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong"
    ]
}