{
    "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
    "authors": [
        "Ziming Luo",
        "Zonglin Yang",
        "Zexin Xu",
        "Wei Yang",
        "Xinya Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process. We analyze the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. Our review comprehensively showcases the task-specific methodologies and evaluation benchmarks. By identifying current challenges and proposing future research directions, this survey not only highlights the transformative potential of LLMs, but also aims to inspire and guide researchers and practitioners in leveraging LLMs to advance scientific inquiry. Resources are available at the following repository: https://github.com/du-nlp-lab/LLM4SR"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 6 0 3 4 0 . 1 0 5 2 : r LLM4SR: Survey on Large Language Models for Scientific Research ZIMING LUO, University of Texas at Dallas, USA ZONGLIN YANG, Nanyang Technological University, Singapore ZEXIN XU, University of Texas at Dallas, USA WEI YANG, University of Texas at Dallas, USA XINYA DU, University of Texas at Dallas, USA In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process. We analyze the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. Our review comprehensively showcases the task-specific methodologies and evaluation benchmarks. By identifying current challenges and proposing future research directions, this survey not only highlights the transformative potential of LLMs, but also aims to inspire and guide researchers and practitioners in leveraging LLMs to advance scientific inquiry. Resources are available at the following repository: https://github.com/du-nlp-lab/LLM4SR. CCS Concepts: Computing methodologies Natural language processing; General and reference Surveys and overviews. Additional Key Words and Phrases: Large Language Models, Scientific Hypothesis Discovery, Experiment Planning and Implementation, Automated Scientific Writing, Peer Review Generation ACM Reference Format: Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. 2025. LLM4SR: Survey on Large Language Models for Scientific Research. ACM Comput. Surv. 1, 1 (January 2025), 37 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn Fig. 1. Schematic overview of the scientific research pipeline covered in this survey. This cyclical process begins with scientific hypothesis discovery, followed by experiment planning and implementation, paper writing, and finally peer reviewing of papers. The experiment planning stage consists of optimizing experiment design and executing research tasks, while the paper writing stage consists of citation text generation, related work generation, and drafting & writing. Both authors contributed equally to this work. Authors Contact Information: Ziming Luo, ziming.luo@utdallas.edu, University of Texas at Dallas, Dallas, Texas, USA; Zonglin Yang, zonglin001@ntu.edu.sg, Nanyang Technological University, Singapore, Singapore; Zexin Xu, zexin.xu@ utdallas.edu, University of Texas at Dallas, Dallas, Texas, USA; Wei Yang, wei.yang@utdallas.edu, University of Texas at Dallas, Dallas, Texas, USA; Xinya Du, xinya.du@utdallas.edu, University of Texas at Dallas, Dallas, Texas, USA. 2025. ACM 1557-7341/2025/1-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn Preprint."
        },
        {
            "title": "1 Introduction",
            "content": "Luo and Yang et al. If have seen further, it is by standing on the shoulders of giants. Isaac Newton The scientific research pipeline is testament to the achievements of the Enlightenment in systematic inquiry [17, 58, 58]. In this traditional paradigm, scientific research involves series of well-defined steps: researchers start by gathering background knowledge, propose hypotheses, design and execute experiments, collect and analyze data, and finally report findings through manuscript that undergoes peer review. This cyclical process has led to groundbreaking advancements in modern science and technology, yet it remains constrained by the creativity, expertise, and finite time and resources available inherent to human researchers. For decades, the scientific community has sought to enhance this process by automating aspects of scientific research, aiming to increase the productivity of scientists. Early computer-assisted research can date back to the 1970s, introducing systems such as Automated Mathematician [74, 75] and BACON [71], which showed the potential of machines to assist in specialized research tasks like theorem generation and empirical law identification. More recently, systems such as AlphaFold [62] and OpenFold [4] have exemplified pioneering efforts to automate specific research tasks, significantly speeding up scientific progress in their respective domains by thousands of times. Yet it was only with the advent of foundation models and the recent explosion in Large Language Models (LLMs) [2, 154] that the vision of comprehensive AI assistance across multiple research domains became realistic [190]. The recent years have witnessed remarkable advancements in LLMs, transforming various fields of AI and Natural Language Processing (NLP). These models, such as GPT-4 [2] and LLaMA [154], have set new benchmarks in understanding, generating and interacting with human language. Their capabilities, enhanced by massive datasets and innovative architectures, now extend beyond conventional NLP tasks to more complex and domain-specific challenges. In particular, the ability of LLMs to process massive amounts of data, generate human-like text, and assist in complex decision-making has captured significant attention in the scientific community [92, 141]. These breakthroughs suggest that LLMs have the potential to revolutionize the way scientific research is conducted, documented, and evaluated [156, 165, 174]. In this survey, we explore how LLMs are currently being applied across various stages of the scientific research process. Specifically, we identify four general tasks where LLMs have demonstrated notable potential. We begin by exploring their application in scientific hypothesis discovery, where LLMs leverage existing knowledge and experimental observations to suggest novel research ideas. This is followed by review of their contributions to experiment planning and implementation, where LLMs aid in optimizing experimental design, automating workflows, and analyzing data. We also cover their use in scientific writing, including the generation of citations, related work sections, and even drafting entire papers. Finally, we discuss their potential in peer review, where LLMs support the evaluation of scientific papers by offering automated reviews and identifying errors or inconsistencies. For each of these tasks, we provide comprehensive review of the methodologies, benchmarks, and evaluation methods. Moreover, the survey identifies the limitations of each task and highlights areas needing improvement. By analyzing the various stages of the research cycle where LLMs contribute, this survey can inspire researchers to explore emerging concepts, develop evaluation metrics, and design innovative approaches to integrate LLMs into their workflows effectively. Comparison with Existing Surveys. This survey provides broader and more comprehensive perspective on the applications of LLMs across the entire scientific research cycle compared to prior specialized studies. For example, Zhang et al. [187] review over 260 LLMs in scientific discovery Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 3 History ( 2.2) Literature-based Discovery ( 2.2.1) LBD [47, 151], DBLP [155], Link Prediction Models [152, 160, 171] Inductive Reasoning ( 2.2.2) Norton [113], Yang et al. [175], Yang et al. [173], Zhong et al. [191], Zhu et al. [194],Wang et al. [163], Qiu et al. [120] Scientific Hypothesis Discovery ( 2) Development of Methods ( 2.3) Main Trajectory ( 2.3.1) Other Methods ( 2.3.2) SciMON [159], MOOSE [174], MCR [145], Qi [119], FunSearch [130], ChemReasoner [146], HypoGeniC [193], ResearchAgent [9], LLM-SR [140], SGA [105], AIScientist [103], MLR-Copilot [84], IGA [141], SciAgents [41], Scideator [121], MOOSE-Chem [176], VirSci [148], CoI [77], Nova [49], CycleResearcher [167], SciPIP [164] Socratic reasoning[30], IdeaSynth [118], HypoRefine [96], LDC [80] Benchmarks ( 2.4) SciMON [159], Tomato [174], Qi et al. [119], Kumar et al. [68], Tomato-Chem [176] DiscoveryBench [108], DiscoveryWorld [57] Evaluation ( 2.5) LLM-based / Expert-based Evaluation; Direct Evaluation / Reference-based Evaluation; Direct Evaluation / Comparison-based Evaluation; Real Experiment Evaluation Optimizing Experimental Design ( 3.2) HuggingGPT [136], CRISPR-GPT [52], ChemCrow [15], Coscientist [14], LLM-RDF [131], AutoGen [168], Li et al. [81], Li et al. [90] Data Preparation ( 3.3.1) Clearning [21, 185], Labeling [153], Feature Engineering [46], Synthesis [82, 85, 98] Automating Experimental Process (3.3) Experiment Execution and Workflow Automation ( 3.3.2) ChemCrow [15], Coscientist [14], Wang et al. [157], Ramos et al. [124], ChatDrug [99], DrugAssist [179], ESM-1b [128], ESM-2 [95], Ferruz and H√∂cker [35], He et al. [44] Data Analysis and Interpretation (3.3.3) Singh et al. [143], Li et al. [79], MentalLLaMA[172], Dai et al. [27], Rasheed et al. [126], Zhao et al. [188], Oliver et al. [114] TaskBench [137], DiscoveryWorld [57], MLAgentBench [54], AgentBench [100], Spider2-V [16], DSBench [61], DS-1000 [70], CORE-Bench [142], SUPER [13], MLE-Bench [20], LAB-Bench [72], ScienceAgentBench [24] Xing et al. [170], AutoCite [161], BACO [40], Gu and Hahnloser [43], Jung et al. [63] Zimmermann et al. [197], Agarwal et al. [3], Hu et al. [50], Shi et al. [138], Yu et al. [181], Susnjak et al. [150], LitLLM [3], HiReview [50], Nishimura et al. [112] Benchmarks & Evaluation (3.4) Citation Text Generation ( 4.2) Related Work Generation ( 4.3) Drafting and Writing ( 4.4) August et al. [8], SCICAP [48], PaperRobot [160], Ifargan et al. [56], CoAuthor [73], AutoSurvey [165], AI Scientist [103] a R fi e S f ) L ( d g n g Experiment Planning and Implementation ( 3) Paper Writing ( 4) Benchmarks & Evaluation (4.5) Automated Peer Reviewing Generation ( 5.2) Peer Reviewing ( 5) LLM-assisted Peer Review Workflows ( 5.3) ALCE [38], CiteBench [37], SciGen [111], SciXGen [22] ReviewRobot [162], Reviewer2 [39], SWIF2T [18], SEA [180], MARG [28], MetaGen [11], Kumar et al. [67], MReD [135], CGI2 [184], CycleReviewer [167] Information Summarization Error Detection & Quality Verification Review Writing Support PaperMage [101], CocoSciSum [29] ReviewerGPT [97], PaperQA2 [144], Scideator [122] ReviewFlow [149], CARE [198], DocPilot [110] Benchmarks & Evaluation ( 5.4) MOPRD [94], ORSUM [184], MReD [135], PeerSum [78], NLPeer [33], PeerRead [65], ASAP-Review [183], ReviewCritiqe [32], Reviewer2 [39] Fig. 2. The main content flow and categorization of this survey. across various disciplines, focusing primarily on technical aspects such as model architectures and datasets, without situating their roles within the broader context of the research process. Similarly, other surveys tend to adopt narrower scopes, examining specific capabilities of LLMs for general applications, such as planning [55] or automation [158], rather than their focused utility in scientific research workflows. Additionally, some works address general approaches relevant to specific research stages but are not exclusively centered on LLMs, such as related work and citation text Preprint. Luo and Yang et al. generation [89] or peer review processes [33]. In contrast, this survey integrates these fragmented perspectives, providing holistic analysis of LLMs contributions across the scientific workflow and highlighting their potential to address the diverse and evolving demands of modern research. Organization of this Survey. As illustrated in Figure 2, the structure of this survey is as follows: 2 covers LLMs for scientific hypothesis discovery, including an overview of methodologies and key challenges. 3 focuses on experiment planning and implementation, highlighting how LLMs can optimize and automate these processes. 4 delves into automated paper writing, including citation and related work generation, while 5 explores LLM-assisted peer review. For each topic, the survey concludes with summary of current challenges and future directions in this rapidly evolving field."
        },
        {
            "title": "2.2 History of Scientific Discovery\nUsing LLMs to generate novel scientific hypotheses is a new research topic, mostly originating from\ntwo related research domains, which are ‚Äúliterature-based discovery‚Äù and ‚Äúinductive reasoning‚Äù.",
            "content": "Literature-based Discovery. Literature-based discovery (LBD) was first proposed by Swanson 2.2.1 [151]. The central idea is that knowledge can be public, yet undiscovered, if independently created fragments are logically related but never retrieved, brought together, and interpreted. Therefore, how to retrieve public knowledge that can be brought together to create new knowledge remains challenge. Swanson [151] propose classic formalization of LBD, which is the ABC model where two concepts and are hypothesized as linked if they both co-occur with some intermediate concept in papers. More recent work has used word vectors [155] or link prediction models [152, 160, 171] to discover links between concepts to compose hypotheses. However, classic LBD methods do not model contexts that human scientists consider in the ideation process, and are limited to predicting pairwise relations between discrete concepts [47]. To overcome these limitations, Wang et al. [159] make the first attempt to ground LBD in natural language context to constrain the generation space, and also use generated sentences as output instead of only predicting relations as in the traditional LBD. Another limitation of LBD is that it has long been thought of as only be applicable to very specific, narrow type of hypothesis [159]. However, recent progress in scientific discovery indicates that LBD might have much wider applicable scope. Particularly, Yang et al. [174] and Yang et al. [176] discuss extensively with social science and chemistry researchers correspondingly, and find that most existing social science and chemistry published hypotheses (instead of only narrow type of hypotheses) can be formulated in LBD pattern. It probably indicates that future hypotheses in social science and chemistry to be published can also result from (correct) linkages and associations of existing knowledge. Inductive Reasoning. Inductive reasoning is about finding general rule or hypothesis 2.2.2 that has wide application scope from specific observations [175]. For example, Geocentrism, Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 5 Heliocentricism, and Newtons Law of Gravity are all proposed rules based on the observations of the movements of stars and planets. Scientific discovery is difficult task of inductive reasoning to an extreme, where each rule is novel scientific finding. The philosophy of science community has summarized three fundamental requirements for rule from inductive reasoning [113], which are (1) rule should not be in conflict with observations; (2) rule should reflect the reality; (3) rule should present general pattern that can be applied to larger scope than the specific observations, covering new information not existing in the observations. Previously inductive reasoning research is mainly conducted by the inductive logic programming community [26], which uses formal language and symbolic reasoners. Yang et al. [173] first work on generative inductive reasoning in the NLP domain, which is to generate natural language rules from specific natural language observations with language models, introducing the requirements on inductive reasoning from the philosophy of science community. Motivated by the empirical experience that language models tend to generate vague and not specific rules, they additionally propose the fourth requirement: (4) rule should be clear and in enough detail. The fourth requirement might have been overlooked by the philosophy of science community since its too obvious. Motivated by the requirements, Yang et al. [173] design an overly-generation-thenfiltering mechanism, leveraging language models to first generate many preliminary rules and then filter those do not satisfy the requirements. Then methods are developed to use self-refine to replace filtering and use more reasoning steps for better rules [120, 163, 191, 194]. However, the rules this line of works try to induce are either known knowledge, or not scientific knowledge but synthesized patterns. Yang et al. [174] make the first attempt to extend the classic inductive reasoning task setting (to discover known/synthetic knowledge) into real scientific discovery setting: to leverage LLMs to autonomously discover novel and valid social science scientific hypotheses from the publicly available web data. Specifically, they collect news, business reviews, and Wikipedia pages on social science concepts as the web data to discover hypothesis. Majumder et al. [107, 108] further propose the concept of data-driven discovery, which is to discover hypotheses across disciplines with all the public experimental data on the web (and private experimental data at hand). Their motivation is that the potential of the large amount of publicly available experimental data has not been fully exploited that lots of novel scientific hypotheses could be discovered from the existing data."
        },
        {
            "title": "2.3.1 Main Trajectory. In general, this method development trajectory for scientific discovery\ncan be seen as incorporating more key components into the methods. Table 1 summarizes the\nkey components we identify as important and indicates whether each method incorporates them.\nSpecifically, they are ‚Äústrategy of inspiration retrieval‚Äù, ‚Äúnovelty checker‚Äù, ‚Äúvalidity checker‚Äù, ‚Äúclarity\nchecker‚Äù, ‚Äúevolutionary algorithm‚Äù, ‚Äúleverage of multiple inspiration‚Äù, ‚Äúranking of hypothesis‚Äù, and\n‚Äúautomatic research question construction‚Äù. Here, each ‚Äúkey component‚Äù refers to a detailed and\nunique methodology that has proven effective for scientific discovery tasks. We exclude broad\ngeneral concepts that may intuitively seem helpful but it‚Äôs not clear how a specific method from the\nconcept can be effective for this task (e.g., tool usage). Next, we introduce these key components.\nFor each key component, we use one or two paragraphs to give a short overview, summarizing its\ndevelopment trace. The reference information for each method mentioned in this section can be\nfound in Table 1.",
            "content": "Preprint. 6 Luo and Yang et al. Inspiration Retrieval Strategy. In addition to relying on background knowledge, literature-based discovery (LBD) facilitates the retrieval of additional knowledge as source of inspiration for formulating new hypotheses. SciMON [159] first introduces the concepts of LBD to the discovery task, demonstrating that new knowledge can be composed of linkage of existing knowledge. It is vital that the inspiration should not be known to be related to the background before, or at least should not be used to associate with the background in known way [176]. Otherwise, the hypothesis would not be novel. Inspired by the ABC model in classic LBD formalization, given background knowledge, SciMON retrieves semantically similar knowledge, knowledge graph neighbors, and citation graph neighbors as inspirations. Specifically, two knowledge are identified as semantically similar if their embeddings from SentenceBERT [127] have high cosine similarity; The knowledge graph they built follows [method, used-for, task] format. ResearchAgent strictly follows the ABC model by constructing concept graph, where link represents the two connected concept nodes have appeared in the same paper before. It retrieves inspiration concepts that are connected with the background concepts on the concept graph (concept co-occurence). Scideator retrieves inspiration papers based on semantic matching (semantic scholar API recommendations) and concept matching (papers containing similar concepts in the same topic, same subarea, and different subarea). SciPIP [164] retrieves inspirations from semantically similar knowledge (based on SentenceBERT), concept co-occurence, and citation graph neigbors. It proposes filtering methods to filter not useful concepts for concept co-occurence retrieval. Different from selecting semantic or citation neighbors as inspirations, SciAgents randomly sample another concept that is connected with the background concept in citation graph (via long or short path) as the inspiration. MOOSE [174] proposes to use LLM-selected inspirations: given the research background and some inspiration candidates in the context, and ask an LLM to select inspirations for the research background from the candidates. Then MOOSE-Chem [176] also adopts it. MOOSE-Chem assumes that after training on hundreds of millions of scientific papers, the most advanced LLMs might already have certain level of ability to identify the inspiration knowledge for the background to compose novel discovery of knowledge. MOOSE-Chem analyzes this assumption by annotating 51 chemistry papers published in 2024 (which are only available online in 2024) with their background, inspirations, and hypothesis, and see whether LLMs with training data up to 2023 can retrieve the annotated inspirations given only the background. Their results show very high retrieval rate, indicating that the assumption could be largely correct. Then Nova also adopts LLM-selected inspirations, with the motivation that leveraging the LLMs internal knowledge to determine useful knowledge for new ideas should be able to surpass traditional entity or keyword-based retrieval methods. Feedback Modules. The next key component is the iterative feedback on the generated hypotheses in the aspects of novelty, validity, and clarity. These three feedbacks are first proposed by MOOSE, motivated by the requirements for hypothesis in inductive reasoning [113, 173]. These three aspects are objective enough to give feedback, and each of them is essential for good hypothesis. Novelty Checker. The generated hypotheses should be novel finding compared to the existing literature. When hypothesis tends to be similar to an existing hypothesis, feedback on enhancing its novelty could be beneficial for hypothesis formulation. Existing methods for novelty feedback are all based on LLMs. In general, there are three ways to provide novelty feedback. The first method evaluates each generated hypothesis against related survey (MOOSE); the second iteratively retrieves relevant papers for comparison (SciMON, Preprint. LLM4SR: Survey on Large Language Models for Scientific Research Table 1. Discovery Methods. Here NF = Novelty Feedback, VF = Validity Feedback, and CF = Clarity Feedback, EA = Evolutionary Algorithm, LMI = Leveraging Multiple Inspirations, = Ranking, AQC = Automatic Research Question Construction. The order of methods reflect their first appearance time. Methods SciMON [159] MOOSE [174] MCR [145] Qi [119] FunSearch [130] ChemReasoner [146] HypoGeniC [193] ResearchAgent [9] LLM-SR [140] SGA [105] AIScientist [103] MLR-Copilot [84] IGA [141] SciAgents [41] Scideator [121] MOOSE-Chem [176] VirSci [148] CoI [77] Nova [49] CycleResearcher [167] SciPIP [164]"
        },
        {
            "title": "NF VF CF EA LMI R AQC",
            "content": "Semantic & Concept & Citation Neighbors - - - - - - - - - - - LLM Selection - - - - - Concept Co-occurrence Neighbors - - - - - Random Selection Semantic & Concept Matching LLM selection - - LLM selection - Semantic & Concept & Citation Neighbors - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - SciAgents, Scideator, CoI); the third directly leverages the internal knowledge of LLMs for evaluation (Qi, ResearchAgent, AIScientist, MOOSE-Chem, VirSci). Validity Checker. The generated hypotheses should be valid science/engineering findings that precisely reflect the objective universe [113]. real validity feedback should be from the results of experiments. However, it is time-consuming and costly to conduct experiments for each generated hypothesis. Therefore, currently, validity feedback almost entirely relies on the heuristics of LLMs or other trained neural models. The exceptions are FunSearch, HypoGeniC, LLM-SR, and SGA. Specifically, FunSearch is about generating code for math problems. The compiler and verification code are naturally efficient and effective verifiers; HypoGeniC and LLM-SR focus on data-driven discovery, which means they have access to observation examples that can be used to check consistency with each generated hypothesis; SGA creates virtual physical simulation environment to mimic real experiments. However, validity checker is still significant challenge for the scientific discovery community. Future research directions include robotics and automation labs, which could automatically perform wet-lab experiments (e.g., biology and chemistry experiments) to verify the generated hypotheses. For computer science-related hypotheses, the future research direction could be more advanced systems for automatic code implementation. Clarity Checker. The generated hypotheses should be sufficiently clear in conveying information and provide adequate details [173]. However, LLMs tend to generate hypotheses with insufficient details [159]. Therefore, it would be beneficial to provide feedback in terms of clarity to refine the hypothesis and expand it with details [174]. Current methods (MOOSE, ResearchAgent, MOOSE-Chem, and VirSci) all adopt LLMs to provide self-assessment on clarity. Preprint. 8 Luo and Yang et al. Evolutionary Algorithm. Evolutionary Algorithm is subset of optimization algorithms inspired by the principles of biological evolution. It assumes the existence of an environment, where an entity that cant adapt to it would be eliminated, and super entity would be evolved from the recombination of characteristics between entities that have some adaptability to the environment (this process is also called as mutation). This key component is important since (1) the real experiment evaluation and the heuristic evaluation of the generated hypotheses naturally serve as the environment. (2) the essence of scientific hypothesis discovery fundamentally can be seen as mutation to unknown yet valid knowledge from only known knowledge input. Although with similar goals, current scientific discovery methods leverage the evolutionary algorithm in different ways. FunSearch first introduced the evolutionary algorithm to the scientific discovery task. They adopt an island-based evolutionary algorithm, where each island is group of similar methods, and each island keeps mutating to new hypotheses. At some time intervals, some least ranking islands are eliminated, and new islands consisting of the best-performing hypotheses from every island are formed, encouraging the recombination between merits between islands. LLM-SR adopts similar island-based evolutionary algorithm. SGA leverages it as evolutionary search, which is to generate multiple offspring in each iteration and retain the best selection. They also adopt an evolutionary crossover, where LLMs generate new hypotheses from various past experiments for better exploration. MOOSE-Chem designs it as an evolutionary unit, to better associate background knowledge and inspiration knowledge. Specifically, given background and inspiration knowledge, they first generate multiple unique hypotheses to associate the two. Each hypothesis is then independently refined, and finally, the refined hypotheses are recombined to better integrate the background and inspiration knowledge into cohesive hypothesis. It encourages different mutation variants from the same input and gathers the advantages from each mutation variant. Leveraging Multiple Inspirations. Here the Leveraging Multiple Inspirations (LMI) component we discuss is about clear identification of several inspirations, so that these identified inspirations will be all leveraged into the final hypothesis (e.g., in sequential way). It is important, where different methods have different reasons. MOOSE-Chem is the first to introduce this component, motivated by the observation that many disciplines such as chemistry and material science often require multiple inspirations to formulate complete and publishable hypothesis. Specifically, they decompose the seemingly impossible-to-solve question ùëÉ (hypothesisresearch background) into many smaller, more practical and executable steps. They do it by formulating mathematical proof for the decomposition. In general, the smaller steps involve identifying starting inspiration, composing preliminary hypothesis based on the background and inspiration, finding another inspiration to address gaps in the preliminary hypothesis, then composing an updated hypothesis with the new inspiration, and so on. Their goal by utilizing multiple inspirations is to rediscover hypotheses in chemistry and material science that are published in high-impact journals such as Nature or Science. In addition to MOOSE-Chem, Nova also retrieves multiple inspirations in successive way, but with different goal, which is to generate more diverse and novel research hypotheses. Their motivation stems from IGAs experimental results that the diversity of generated hypotheses tends to saturate. They identify one of the main reasons as that the input background information is the same, whereas incorporating different sets of inspirations can largely alleviate this issue by introducing flexible inputs. Ranking of Hypotheses. This key component is about providing full ranking of the generated hypotheses. It is important because LLMs can generate large number of hypotheses in short Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 9 time, while real lab experiments to verify each of them are time-consuming and costly. As result, it would be very beneficial for scientists to know which hypothesis should be tested first. Some methods (e.g., MOOSE) adopt an automatic evaluation method to provide preliminary understanding of generated hypotheses. The automatic evaluation method could naturally be used for ranking, but Table 1 only focuses on how ranking is used in the methodology section (but not in the automatic evaluation section). majority of the methods adopt LLMs rated score as reward value, which can be used for ranking (MCR [145], AIScientist, MOOSE-Chem, CycleResearcher). FunSearch focuses on code generation problem, therefore can directly precisely evaluate the generated code by running them and checking results. ChemReasoner [146] finetunes task-specific graph neural network model to obtain reward. HypoGeniC [193] and LLM-SR [140] focuses on data-driven discovery, which means they have access to observation examples that can be used to check the consistency with the generated hypotheses, where the number of consistent examples can be used as the reward value for ranking. Different from directly predicting reward score, IGA takes pairwise comparison, because they find that LLMs are poorly calibrated when asked directly to predict the final scores or decisions, but can achieve non-trivial accuracy when asked to judge which paper is better in pairwise comparisons. Inspired by IGA [141], CoI [77] proposes pairwise automatic evaluation system, named Idea Arena. Nova [49] also adopts pairwise automatic evaluation method. Automatic Research Question Construction. This key component is about automatic construction of research question, so that automated scientific discovery methods can use it as input to discover hypotheses. It indicates different role of LLM systems in scientific discovery: without it, an LLM serves as copilot, relying on researchers to propose good research questions; with it, the system operates in full-self-driving mode, capable of independent discovery without human input. The full-self-driving mode was first introduced by MOOSE and framed as an automated setting for scientific discovery. Specifically, they adopt an LLM-based agent to continually search through the discipline-related web corpus to find interesting research questions. AIScientist explores research directions by leveraging starting code implementation as input. MLR-Copilot finds research directions by analyzing the research gaps from input papers. SciAgents and Scideator skip research questions by directly generating hypotheses based on the pairing of concepts. VirSci generates research questions by leveraging LLM-based scientist agents to brainstorm. CoI finds research questions by collecting development line of methods and then predicting the next step. Nova directly generates seed ideas from input papers and common idea proposal patterns, skipping the research question construction step."
        },
        {
            "title": "2.3.2 Other Methods. In this section, we introduce the methods that are different from the methods\nin the ‚Äúmain trajectory‚Äù (¬ß 2.3.1). These methods themselves are very diverse, focusing on different\naspects of scientific discovery. For example, Dong et al. [30] leverage a distinct methodology, Pu\net al. [118] focus on HCI, Liu et al. [96] also consider the integration of experiment results, Li et al.\n[80], Weng et al. [167] leverage reviews as preference learning to finetune the hypothesis proposer\nmodel.",
            "content": "Dong et al. [30] try to use GPT-4 to tackle the very challenging research question: whether = NP or not. They propose Socratic reasoning, which encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Their method could be useful when trying to prove very challenging existing hypothesis. IdeaSynth [118] is research idea development system, which represents idea concepts as linked nodes on canvas. Its effects are investigated in human-computer interaction scenario. They found through lab study that human participants using IdeaSynth can explore more alternative Preprint. 10 Luo and Yang et al. Table 2. Discovery benchmarks aiming for novel scientific findings. The Biomedical data SciMON [159] collected is up to January 2024. RQ = Research Question; BS = Background Survey; = Inspiration; = Hypothesis. Qi et al. [119]s dataset contains train set where the publication date of the papers is before January 2023. * in the date column represents the authors have checked the papers should not only be published after the date, but are also not available online before the date (e.g., through arXiv). The five disciplines Kumar et al. [68] cover are Chemistry, Computer Science, Economics, Medical, and Physics. Name SciMON [159] Tomato [174] Qi et al. [119] Kumar et al. [68] PhD students - - Tomato-Chem [176] PhD students Annotator RQ BS - IE models - 50 - 2900 - 100 51 Size - 67,408 ChatGPT PhD students Discipline NLP & Biomedical Social Science Biomedical Five disciplines Chemistry & Material Science Date from 1952 to June 2022 (NLP) from January 2023 from August 2023 (test set) from January 2022 from January 2024* ideas and expand initial ideas with more details compared to human participants using strong LLM-based baseline. Liu et al. [96] make the first attempt trying to unify literature-based discovery and data-driven discovery. Given an initial set of experiment results, it retrieves related literature and adopts an iterative refinement approach to keep improving hypothesis to make it consistent with the experiment results and leverage findings from the retrieved literature. Weng et al. [167] propose dual system that includes CycleResearcher and CycleReviewer, where the CycleResearcher is in charge of idea formulating and paper writing, and the CycleReviewer is in charge of scoring the written papers. The dual system has synergy that the scores from CycleReviewer can compose preference data to train CycleResearcher. The dual system only focuses on idea formulating and paper writing, skipping experiment planning and implementation. Li et al. [80] propose fine-tuning LLMs to be better idea generators and introduce novel framework that employs two-stage approach combining Supervised Fine-Tuning (SFT) and Controllable Reinforcement Learning (RL). They focus on dimensions of feasibility, novelty and effectiveness. The dimensional controllers enable dynamic adjustment of the generation process."
        },
        {
            "title": "2.4 Benchmarks\nOverall the tasks in automated scientific discovery can be divided into ‚Äúliterature-based discovery‚Äù\nand ‚Äúdata-driven discovery‚Äù. Researchers design different benchmarks for each task respectively.",
            "content": "Literature-based Discovery. Literature-based discovery is in general about connecting knowl2.4.1 edge (pieces) in existing publications and associating them to create new knowledge. In this process, the knowledge to start with is from the research background. research background can be seen as consisting of two components: (1) research question, and (2) background survey, which discusses the state-of-the-art methods or knowledge for the research question. With the start knowledge in the research background, the other knowledge to connect is usually by searching through the existing publications. Here the other knowledge is referred to as inspiration [159, 174]. Then the research background and the retrieved inspiration(s) are associated to create hypothesis. Table 2 summarizes the literature-based discovery benchmarks, which aim for novel scientific findings. The key components are the research question, background survey, inspiration identification, and hypothesis. The hypotheses are collected from the abstract section [159], the methodology section [174, 176], or the future work and limitation sections [68]. Table 2 also includes the size of the dataset (number of papers analyzed), disciplines of the papers, and the publication date of the papers. Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 11 The publication date is important to alleviate/avoid the data contamination problem. The reason is that one of the main goals is to rediscover the groundtruth hypotheses, and the date can indicate which LLMs to use for the rediscovery (its training data should be earlier than the date to avoid the potential data contamination problem). Some of the benchmarks can be used for training since their large size [119, 159], while some are mainly used for evaluation since they are annotated by PhD students [68, 174, 176]."
        },
        {
            "title": "2.4.2 Data-driven Discovery. Majumder et al. [107] propose the concept of ‚Äúdata-driven discovery‚Äù.\nHere the ‚Äúdata‚Äù refers to the experiment results. Their motivation is that given the ‚Äúobservation‚Äù of\nlots of (public and private) existing experimental results available online, LLMs might be able to find\nthe general pattern of these data, where the general pattern could be a novel research hypothesis.\nGiven the relation between the specific observations and the general hypothesis, ‚Äúdata-driven\ndiscovery‚Äù is very related to the inductive reasoning task, where the observation space is the full\npublicly available experiment results on the web and the private experiment results at hand.",
            "content": "DiscoveryBench [108] is the first data-driven discovery benchmark. It comprises 264 discovery tasks extracted manually from over 20 published papers and 903 synthetic tasks. The input of the task consists of research question and set of experimental data. The goal is to answer the research question with hypothesis that can be supported by the experimental data. It also introduces structured formalism for the generated hypotheses, that the hypotheses should consist of three components: context, variables, and relationships. Specifically, hypothesis is about the relationships between the two variables under the context. DiscoveryWorld [57] is the first discovery benchmark with virtual environment. The main motivation is twofold: (1) real-world experiments are costly and require substantial domain expertise; and (2) abstracting from task-specific details encourages the development of more general discovery methods. To address these challenges, it establishes virtual environment for agents to discover hypotheses. It includes 120 different challenge tasks, where the hypotheses reflect the real patterns of the world."
        },
        {
            "title": "2.5 Evaluation Development Trend\nThe evaluation methods for scientific discovery tasks are diverse. Arguably, nearly every paper\nproposes a new methodology uses a different evaluation approach. However, their metrics exhibit\nnotable intersections, and some emerging trends in evaluation methods can be observed across\nthese works.",
            "content": "The intersections of the evaluation criteria are novelty, validity, clarity, and significance. Some less-used evaluation criteria include relatedness, interestingness, and helpfulness. An alternative name for validity is feasibility. They might be used interchangeably in many scenarios. Validity refers to whether discovered scientific knowledge accurately reflects objective world, while feasibility concerns the practicability of an engineering finding. Helpfulness is subjective evaluation, based on the idea that the goal of discovery system is to act as copilot for researchers; therefore, its perceived usefulness by researchers could be considered important. In terms of the evaluator selection, the evaluation methods can be divided into LLM-based and expert-based evaluation. LLMs direct evaluation has shown high consistency score with expert evaluation in social science [174]. However, in natural science disciplines such as chemistry, LLMs have been argued to lack the capability to provide reliable evaluations [146]. Expert evaluation is generally considered reliable. However, in challenging fields like chemistry, even an experts direct evaluation may lack sufficient reliability [176]. This is due to (1) the complexity of the discipline; and (2) the fact that slight changes in the research topic can necessitate entirely different background Preprint. 12 Luo and Yang et al. knowledge for evaluation, while experts typically have specialized research focuses, which may not cover the full range of knowledge required for relatively reliable evaluation. Based on the need for reference, evaluation methods can be categorized as direct evaluation and reference-based evaluation. Due to reliability concerns with direct evaluation, reference-based evaluation serves as an alternative [68, 108, 176], which counts the key components from the ground truth hypotheses mentioned in the generated hypotheses. Moreover, in addition to directly assigning scalar evaluation score to generated hypothesis, Si et al. [141] propose comparison-based evaluations to alleviate the incapacity of LLM-based evaluation of direct scoring: the LLM evaluator is asked to keep comparing pairs of generated hypotheses until ranking is possible. It can be used when comparing the quality of hypotheses generated by two methods, but might not help in judging the absolute quality of hypothesis. However, the ultimate evaluation should be only through real (wet-lab) experiments. It raises challenges in the robotics and automatic experiment implementation fields."
        },
        {
            "title": "2.7 Challenges and Future Work",
            "content": "Challenge. Scientific discovery is to find novel knowledge that has not been verified by wet lab experiments. In some disciplines such as chemistry, even an experts evaluation of the generated novel hypothesis is not reliable enough. This causes need for automated experiments conduction to verify the large-scale machine-generated hypotheses. In addition, current methods on scientific discovery highly rely on the ability of existing available LLMs. LLMs with better capacity on the universal tasks usually can also lead to discovered hypotheses with better quality [174]. As result, LLM-based discovery methods may have an upper performance limit, constrained by the capabilities of the state-of-the-art LLMs. However, it is largely (if not completely) unclear how should we augment LLMs ability on the task of scientific discovery. Thirdly, it is unclear on sufficient set of internal reasoning structure for scientific discovery: current works rely heavily on retrieving from high-quality knowledge source (e.g., literature) as inspiration to generate hypothesis. But it is unclear on whether there are any more internal reasoning structures that can help with the process. Finally, building accurate and well-structured benchmark highly relies on experts. However, the size of expert-composed benchmark is usually very limited. It is unclear on how should we scale up an accurate and well-structured discovery-oriented benchmark. Future work. The first line of future work is to enhance automated experimental execution, as it remains the most reliable way to test the validity of hypothesis. This process may vary across disciplines. In computer science, the bottleneck might be the coding ability, especially the ability to Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 13 program large system. In chemistry or biology, the bottleneck might lie in the robotics methods to conduct experiments [14]. The second line of future work is to enhance the LLMs ability in hypothesis generation. Currently, it is still not very clear how to increase this ability. The aspects might include training data collection methods and training strategies. The third line of future work is to investigate other internal reasoning structures of the scientific discovery process. This might need an interdisciplinary effort, involving the philosophy of science (also known as science of science) [36]."
        },
        {
            "title": "The fourth line of future work is to investigate how to leverage LLMs to automatically collect",
            "content": "accurate and well-structured benchmark."
        },
        {
            "title": "3.2 Optimizing Experimental Design\nLLMs are transforming the experimental design process by enabling more efficient and adaptive\nworkflows in scientific research. Their capacity to process and analyze extensive datasets em-\npowers researchers to decompose complex tasks, select optimal methodologies, and enhance the\noverall structure of experiments. This section explores how LLMs facilitate experimental design\noptimization across various domains.",
            "content": "Task decomposition involves breaking experiments into smaller, manageable sub-tasks, process often necessitated by the complexity of real-world research to ensure alignment with specific research goals [55]. Numerous studies [14, 15, 52, 125, 136, 168] demonstrate how LLMs simplify intricate problems by defining experimental conditions and specifying desired outputs. For instance, HuggingGPT [136] utilizes LLMs to parse user queries into structured task lists while determining execution sequences and resource dependencies. Similarly, CRISPR-GPT [52] automates CRISPRbased gene-editing experiment design by facilitating the selection of appropriate CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and planning validation experiments. ChemCrow [15] employs iterative reasoning and dynamic planning, using structured \"Thought, Action, Action Input, Observation\" loop [177] to refine its approach based on real-time feedback. Multi-LLM systems, such as Coscientist [14] and LLM-RDF [131], further leverage specialized agents to extract methodologies from literature, translate natural language descriptions into standardized protocols, generate execution code for automated platforms, and adaptively correct errors during execution. Advanced prompting-based techniques such as in-context learning, Chain of Thought [166] and ReAct [177], are often employed in studies described above to enhance the reliability and accuracy of experimental planning in LLM-assisted workflows. Moreover, LLMs are also capable of Preprint. 14 Luo and Yang et al. enhancing experimental design through reflection and refinement [106, 139], process that allows them to continuously evaluate and improve experimental plans. For instance, by simulating expert discussions, LLMs engage in collaborative dialogue [81], challenging assumptions, and refining their output through iterative analysis [90]. This method mirrors real-world scientific problem solving, where discrepancies between expert opinions foster deeper exploration of the problem space, and consensus is achieved through rigorous debate and synthesis of diverse perspectives."
        },
        {
            "title": "3.3.2 Experiment Execution and Workflow Automation. To automate the experimental workflow in\nscientific research, LLM-based agents can acquire task-specific capabilities through a combination\nof pretraining [95, 128], fine-tuning [35, 44], and tool-augmented learning. Pretraining on extensive\ndataset provides foundational knowledge, while fine-tuning on domain-specific datasets refines this\nknowledge for targeted scientific applications. To enhance task execution, LLMs are often coupled\nwith domain-specific knowledge bases [14, 15, 157] or preconfigured workflows [14, 99]. Advanced\nprompting techniques like in-context learning and chain-of-thought prompting [99, 179] enable\nLLMs to quickly adapt to new experimental protocols. Additionally, iterative adjustments with task-\nspecific feedback loops allow the LLM to refine its outputs based on experimental goals [124, 179].\nBased on these principles, LLM plays a diverse role in automating experimental workflows\nacross different disciplines. In chemistry, ChemCrow [15], an LLM chemistry agent, leverages 18\nexpert-designed tools to autonomously plan and execute complex chemical syntheses, bridging\ncomputational and experimental domains. Similarly, Coscientist [14] integrates LLM with lab\nautomation to optimize reactions like palladium-catalyzed syntheses. LLMs have also been employed\nfor evolutionary search strategies to explore vast chemical spaces [157], enabling the identification\nof candidate molecules while reducing experimental burdens. Ramos et al. [124] combine natural\nlanguage inputs with Bayesian optimization for catalyst synthesis, streamlining iterative design\ncycles. Furthermore, LLMs have been utilized for hypothetical scenario testing and reaction design,\nminimizing experimental iterations through hypothesis pre-screening [145, 146]. In drug discovery,\nChatDrug [99] integrates modules for prompting, retrieval, and domain feedback to facilitate drug\nediting, while DrugAssist [179] iteratively optimizes molecular structures through human-machine\ndialogue. In biological and medical research, Models like ESM-1b [128] and ESM-2 [95] encode\nprotein sequences, capturing structural properties for predictive tasks, such as secondary and\ntertiary structure predictions, eliminating the need for labor-intensive experiments. By fine-tuning\nLLMs on protein families, Ferruz and H√∂cker [35] generate highly divergent yet functional protein",
            "content": "Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 15 sequences. Additionally, He et al. [44] introduce an antibody generative LLM for de novo SARSCoV-2 antibody design, achieving specificity and diversity while reducing reliance on natural antibodies."
        },
        {
            "title": "3.4 Benchmarks",
            "content": "Table 3. Benchmark for LLM-Assisted Experiment Planning and Implementation. ED = Optimizing Experimental Design, DP = Data Preparation, EW = Experiment Execution & Workflow Automation, DA = Data Analysis & Interpretation. General\" in discipline means benchmark is not designed for particular discipline. Benchmark Name TaskBench [137] DiscoveryWorld [57] MLAgentBench [54] AgentBench [100] Spider2-V [16] DSBench [61] DS-1000 [70] CORE-Bench [142] SUPER [13] MLE-Bench [20] LAB-Bench [72] ScienceAgentBench [24] - - - - - - - - ED DP EW DA - - - - - Discipline General General - Machine Learning - - - - - - General Data Science & Engineering Data Science Data Science Computer Science, Social Science & Medicine General - Machine Learning - Biology Data Science Additional Task Details Task decomposition, tool use Hypothesis generation, design & testing Task decomposition, plan selection, optimization Workflow automation, adaptive execution Multi-step processes, code & GUI interaction Data manipulation, data modeling Code generation for data cleaning & analysis Reproducibility testing, setup verification Experiment setup, dependency management End-to-end ML pipeline, training & tuning Manipulation of DNA and protein sequences Data visualization, model development Benchmarks are essential for evaluating how effectively LLMs can support various aspects of experimental workflows. While not specifically created for LLM-assisted experiment implementation, many benchmarks are versatile enough to be applied to these tasks. For example, MLAgentBench [54] covers task decomposition by helping break down complex research tasks, data handling by automating processes like data loading and transformation, and workflow management by optimizing machine learning experiment execution. Preprint. Luo and Yang et al. These benchmarks provide different venues and thus vary in their approaches. Evaluation methods range from task success rate, accuracy and execution consistency to comparisons with human benchmarks. These differences highlight the diverse ways LLMs can be integrated into research processes. Further details are presented in Table 3."
        },
        {
            "title": "3.5 Challenges and Future Work",
            "content": "Challenges. The challenges of employing LLMs for experiment planning and implementation arise both from their intrinsic limitations and their application to domain-specific tasks. One fundamental limitation is their planning capability. As clarified by Kambhampati et al. [64], LLMs in autonomous modes often fail to generate executable plans. They are prone to hallucinations, which can lead to irrational plans, deviations from task prompts, or an inability to follow complex instructions [55]. Prompt robustness poses another critical challenge in multi-stage experimental contexts. Minor variations in prompt wording, even when conveying the same intent, can result in inconsistent guidance throughout the planning and execution process [195], potentially affecting experimental outcomes. Additionally, the slow processing speed of autoregressive LLMs can impede real-time feedback in iterative and multi-step experiment planning, limiting their efficiency. Applicationspecific challenges include difficulties in adapting to specialized roles, as LLMs struggle to emulate domain-specific scientific expertise and cognitive processes essential for generalizability across research domains [167]. For example, certain experiments may require simulating ethically sensitive or error-prone scenarios, which often conflict with the safety-aligned values embedded in LLMs. Future work. Future research should address these challenges by enhancing core model capabilities and tailoring them to the unique requirements of experimental tasks. To mitigate hallucination risks, robust verification mechanisms can be integrated into workflows, such as cross-referencing outputs with external sound verifiers [64] or employing real-time feedback loops to correct inaccuracies dynamically [59]. Improving prompt robustness may involve developing adaptive systems that monitor and modify prompt structures in response to contextual changes, ensuring consistency across planning stages. Efficiency enhancements could be achieved by creating faster, distilled versions of LLMs optimized for multi-step reasoning or hybrid systems combining LLMs with smaller, task-specific models to balance speed and accuracy. For more effective role adaptation, fine-tuning LLMs with high-quality domain-specific datasets or developing modular frameworks could enable more precise emulation of specialized scientific reasoning. Additionally, designing adaptive alignment protocols may allow LLMs to safely simulate ethically complex scenarios when addressing specific experimental goals."
        },
        {
            "title": "4.2 Citation Text Generation\nGiven the context of a citing paper, citation text generation task aims to produce accurate textual\nsummaries for a set of papers-to-cite. LLMs have been pivotal in automating various aspects of\ncitation text generation by providing rich contextual understanding and coherence, employing a\nrange of methodologies to enhance both accuracy and usability. A pilot study by Xing et al. [170]",
            "content": "Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 17 uses pointer-generator network that can copy words from the manuscript and the abstract of the cited paper based on cross-attention mechanisms to generate citation texts. Li and Ouyang [88] prompt an LLM to generate natural language description that emphasized the relationships between pairs of papers in the citation network. On the other hand, models like AutoCite [161] and BACO [40] extend this work by adopting multimodal approach, combining citation network structures with textual context to produce contextually relevant and semantically rich citation texts. Furthermore, Gu and Hahnloser [43], Jung et al. [63] allow users to specify attributes such as citation intent and keywords, integrating these into structured template and fine-tuning an LM to generate citation texts that align with their needs."
        },
        {
            "title": "4.3 Related Work Generation\nThis task involves creating a related work section for a scientific paper based on cutting-edge refer-\nence papers [45]. Compared to traditional multi-document summarization models [23, 51], LLMs\nhave demonstrated remarkable capabilities in handling the extensive input lengths characteristic\nof scientific documents and providing a rich contextual understanding. The success of LLMs in\nvarious natural language understanding and generation tasks, combined with their large context\nwindows, has recently enabled more comprehensive and nuanced literature reviews, facilitating\ndeeper insights and connections across diverse research areas.",
            "content": "Martin-Boyle et al. [109], Zimmermann et al. [197] develop case studies to explore the use of ChatGPT for literature review tasks and related work generation, showcasing its ability to assist researchers by quickly scanning large datasets of scientific publications and generating initial drafts of related work sections. However, directly employing LLMs in academic writing could lead to issues such as hallucinations, where the generated content is not grounded in factual data and may fail to accurately reflect state-of-the-art research. To address these issues, numerous works have operated on the principle of Retrieval-Augmented Generation (RAG) [76], which enhances LLM-based literature review generation by grounding in factual content retrieved from external sources [3, 50, 138, 150, 181]. For instance, LitLLM [3] utilize RAG to retrieve relevant papers on websites and re-rank them, reducing the time and effort needed for comprehensive literature reviews while minimizing hallucinations. HiReview [50] takes this further by integrating RAG-based LLMs with graph-based hierarchical clustering. This system first retrieved relevant sub-communities within citation network and generated hierarchical taxonomy tree. LLMs then generate summaries for each cluster, ensuring complete coverage and logical organization. Nishimura et al. [112] integrate LLMs to emphasize novelty statement in related work sections. By comparing the new research with existing works, the LLMs help generate related work sections that explicitly highlight what is new and different, contributing to more impactful comparison between the target paper and prior literature."
        },
        {
            "title": "4.4 Drafting and Writing\nIn the field of automated scientific writing, LLMs are being used across various tasks, ranging\nfrom generating specific textual elements to producing entire research papers. For more specific\nwriting tasks, August et al. [8] propose to generate scientific definitions with controllable complexity\ntailored to different audiences, while SCICAP [48] automates the generation of captions for scientific\nfigures, enabling quick and accurate descriptions of visual data. More holistic systems, such as\nPaperRobot [160], introduce an incremental drafting approach, where LLMs help organize and draft\nsections of a paper based on user inputs. Similarly, CoAuthor [73] takes a collaborative human-AI\napproach, in which LLMs help authors by generating suggestions and expanding text. For fully\nautonomous writing, Ifargan et al. [56] explore how LLMs can generate complete research papers\nfrom data analysis to final drafts, while AutoSurvey [165] demonstrates the ability of LLMs to",
            "content": "Preprint. 18 Luo and Yang et al. autonomously write comprehensive surveys by synthesizing and organizing existing research. Lastly, AI Scientist [103] and CycleResearcher [167] propose an even broader system that not only drafts scientific papers but also contributes to the entire scientific process, including hypothesis generation and experiment design, highlighting the potential for fully automated scientific discovery and writing."
        },
        {
            "title": "4.5 Benchmarks\nWe summarize the evaluation methods of automated scientific paper writing systems in three key\nfields: citation text generation, related work generation, and drafting and writing. In Table 4, we\nprovide a comprehensive summary of the specific datasets, metrics, and benchmarks for each task.",
            "content": "Table 4. Evaluation Methods for automated paper writing, which includes three subtasks: citation text generation, related work generation, and drafting and writing. For the related work generation, there is no universally recognized benchmark. Task Benchmark Dataset Citation Text Generation ALEC [38] ASQA [147], QAMPARI [7], ELI5 [34] CiteBench [37] AbuRaed et al. [1], Chen et al. [23], Lu et al. [104], Xing et al. [170] Related Work Generation None AAN [123], Delve [5], S2ORC [102], CORWA [86] SciSummNet [178], Drafting and Writing SciGen [111] SciGen [111] SciXGen [22] SciXGen [22] Metric Fluency: MAUVE [116], Correctness: precision, recall. Citation quality: citation recall, citation precision [38] Quantitative: ROUGE [93], BertScore [186], Qualitative: citation intent labeling [25], CORWA tagging [86] ROUGE [93], BLEU [115], Human evaluation: fluency, readability, coherence, relevance, informativeness [10], MoverScore BLEU [115], METEOR [189], BertScore [186], BLEURT [134], Human evaluation: recall, precision, correctness, hallucination BLEU [115], METEOR [10], MoverScore [189], Human evaluation: fluency, faithfulness, entailment and overall Citation Text Generation. The ALCE [38] benchmark is the primary standard. Assessment of systems on three dimensions: fluency, correctness, and quality of citation text. ALCE is designed to test the ability of models to generate long-form answers with accurate citations across diverse domains. Their datasets cover wide range of question types, with corpora spanning from Wikipedia to web-scale document collections. CiteBench [37] is another benchmark that unifies multiple existing tasks to standardize the evaluation of citation text generation across various designs and domains, using both qualitative and quantitative metrics. Related Work Generation. Currently, no single benchmark is universally recognized for this task, due to the vast differences in task definitions and simplifying assumptions in various studies [89]. However, most works are built on corpus-level datasets, and commonly used sources of scientific articles include: ACL Anthology Network (AAN) Corpus [123], SciSummNet [178], Delve [5], Semantic Scholar Open Research Corpus (S2ORC) [102] and Citation Oriented Related Work Annotation (CORWA) [86]. The summarization metric ROUGE [93] is the most frequently employed for automatic evaluation, with some work also using the translation metric BLEU [115]. Furthermore, human evaluations often rate fluency, readability, coherence with the target paper, and relevance and informativeness to the cited work on five-point Likert scale. Drafting and Wrigting. SciGen [111] benchmark supports the evaluation of reasoning-aware text generation from scientific tables, highlighting the challenges of arithmetic reasoning in text generation. SciXGen [22], another key benchmark, evaluates the context-aware text generation, Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 19 focusing on the integration of external information into the generated text. Both SciGen and SciXGe use metrics like BLUE [115], METEOR [10] and MoverScore [189], along with human evaluation."
        },
        {
            "title": "4.6 Challenges and Future Work",
            "content": "Challenges. The challenges in citation text generation, related work generation, and drafting and writing primarily arise from inherent limitations in LLMs, such as maintaining factual accuracy, ensuring contextual coherence, and handling complex information. LLMs often struggle with hallucinations [59], generating incorrect or irrelevant citations, and are constrained by the retrieval systems they depend on [53]. Limited context windows further restrict models ability to manage extensive references or integrate relevant literature comprehensively [165], potentially leading to incorrect citation ordering and inappropriate citation grouping. Additionally, ensuring scientific rigor and avoiding reliance on superficial or trivial sources remain persistent obstacles, as LLMs struggle to capture the depth and reasoning needed in academic writing [103]. Furthermore, the use of LLMs in academic writing introduces significant ethical concerns, particularly regarding academic integrity and plagiarism [89]. This blurs the lines of authorship, as researchers might present machine-generated text as their own work. LLMs can also generate text that closely mimics existing literature, raising the risk of unintentional plagiarism where the generated content may not be sufficiently original. The convenience of using LLMs to draft sections of papers can undermine the rigorous intellectual effort traditionally required in academic writing, potentially devaluing the learning process and critical thinking skills essential to scholarly research. Future Work. To overcome these challenges, future advancements should focus on improving retrieval systems and enhancing models capacity to synthesize information from diverse, longcontext sources [87]. This includes developing better citation validation mechanisms, improving multi-document synthesis, and introducing real-time literature discovery to keep generated content up to date. Additionally, incorporating domain-specific fine-tuning and reasoning-aware models will help generate more accurate, contextually relevant scientific text [111]. Fine-grained control over the writing process, such as adjusting tone and style, will also be crucial for improving the adaptability of LLMs to different academic needs [22, 38, 103]. Furthermore, integrating humanin-the-loop systems, where human oversight and intervention are essential parts of the writing process, can ensure that the intellectual rigor and critical thinking inherent in scholarly work are preserved [89, 109]. Finally, to address the potential ethical concerns, it is crucial for the academic community to establish clear guidelines and ethical standards for the use of LLMs to ensure the integrity and originality of academic work."
        },
        {
            "title": "5.1 Overview\nPeer review is the cornerstone of scientific research. The integration of LLMs into the peer re-\nview process represents a significant advancement, addressing longstanding challenges such as\nreviewer bias, inconsistent standards, and workload imbalances [42, 117]. This integration has\ngained significant traction in the academic community, as evidenced by major computer science\nconferences adopting LLM-assisted reviewing practices. For instance, ICLR 2025 has announced\nthe implementation of LLM-based systems to support reviewers in their evaluation process1.",
            "content": "The integration of LLMs in peer review has evolved into two distinct approaches, each addressing specific needs in the review process. The first approach, automated review generation, emerged from the need to handle increasing submission volumes and reduce reviewer workload by using 1https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/ Preprint. 20 Luo and Yang et al. LLMs to analyze research papers independently [66, 182]. These systems are designed to evaluate multiple aspects of submissions, including methodology validation, results verification, and contribution assessment, thereby providing comprehensive review reports without direct human intervention. The second approach, LLM-assisted review workflows, developed in response to the recognition that human expertise remains crucial in academic evaluation while acknowledging that certain review tasks can benefit from automation[69]. These workflows incorporate LLMs as supplementary tools, where they assist human reviewers in time-consuming but well-defined tasks, such as paper summarization, reference verification, and internal consistency checks, while leaving critical evaluation and judgment to human experts. These approaches employ diverse methodologies to enhance review efficiency, consistency, and quality. To systematically evaluate and improve these systems, the research community has developed specialized peer review benchmarks that serve dual purposes: providing standardized training datasets and establishing performance assessment metrics. This chapter examines these methodologies, their evaluation frameworks, and concludes with implementation challenges and future research directions."
        },
        {
            "title": "5.2 Automated Peer Review Generation\nAutomated peer review generation aims to streamline scientific assessment by exploring how LLMs\ncan produce comprehensive reviews with minimal human intervention. By inputting a scientific\narticle, these systems focus on generating a complete peer review or meta-review, employing\nvarious techniques to enhance feedback‚Äôs depth, accuracy, and relevance.",
            "content": "Current approaches to automated peer review generation can be categorized into two main strategies: single-model and multi-model architectures. Single-model approaches focus on optimizing the review generation process through sophisticated prompting techniques and modular design. These systems typically employ carefully crafted prompts to guide the models attention to specific aspects of the paper, such as methodology, results, and contributions [132]. Within the single-model paradigm, several distinct architectural approaches have been proposed. CGI2 [184] advances beyond previous approaches: MetaGen [11], which used two-stage pipeline of extractive summarization with decision-aware refinement; Kumar et al. [67], which developed neural architecture for joint decision prediction and review generation; and MReD [135], which introduced structure-controlled generation using sentence-level functional labels. Building on these foundations, CGI2 implements staged review process through modular design, first extracting key opinions from the paper, then summarizing strengths and weaknesses, and finally refining these outputs through iterative feedback under checklist-guided framework. This iterative process enhances the depth and relevance of reviews but may struggle with papers that involve highly complex methodologies or lengthy content exceeding the context window. Taking different approach, CycleReviewer [167] implements an end-to-end review generation method using reinforcement learning to refine review quality through feedback loops continuously. While CycleReviewer excels in enhancing review precision and clarity, its reliance on significant computational resources could limit its scalability. Meanwhile, ReviewRobot [162] utilizes knowledge graphs to systematically identify and structure knowledge elements, transforming them into detailed review comments through structured generation process. ReviewRobot demonstrates remarkable explainability and evidence-based reasoning but is constrained by the inflexibility of its pre-defined templates. The alternative strategy employs multi-model architectures, representing more sophisticated approach by leveraging multiple specialized models to handle different aspects of the review process. This approach offers several advantages, including improved handling of complex papers and enhanced review quality through specialized expertise. Reviewer2 [39] implements two-stage process: one model generates specific aspect prompts, while another utilizes these prompts to create Preprint. LLM4SR: Survey on Large Language Models for Scientific Research detailed, focused feedback. This separation of prompt generation and review creation allows for more nuanced and targeted feedback but often results in partial or biased reviews due to the lack of an integrated framework. To address these limitations, SEA [180] employs separate models for standardization, evaluation, and analysis, providing more comprehensive and balanced approach. The system unifies multiple reviews into single format, significantly reducing redundancy and inconsistencies across feedback. Furthermore, SEA introduces mismatch score to measure the alignment between papers and generated reviews, coupled with self-correction strategy to enhance review quality iteratively. While these features enable SEA to surpass Reviewer2 in consistency and comprehensiveness, the need for coordinating outputs across multiple models introduces added complexity. Building on specialization but addressing different challenge, MARG [28] tackles the problem of processing papers that exceed typical LLM context limits. By introducing multiagent framework, MARG distributes review tasks across multiple specialized models, allowing for comprehensive review of longer papers while maintaining attention to detail throughout the document. This innovative approach ensures detailed, aspect-specific feedback. Still, it brings new challenges, such as coordinating the communication and outputs of various agents, which increases the difficulty of ensuring consistency and alignment. Each architectural approach offers distinct advantages and faces unique challenges. Singlemodel approaches benefit from simpler implementation and more straightforward control over the review process, but they may struggle with longer or more complex papers. Multi-model architectures provide greater scalability and better handling of sophisticated review tasks, yet they demand careful coordination and face potential consistency challenges across their components. For instance, ReviewRobots structured approach offers explainability and actionable insights. Still, it is less adaptable to evolving research domains, while CycleReviewers iterative refinement improves dynamic adaptability without requiring substantial training resources. As research in this area progresses, combining the strengths of single-model simplicity with the adaptability of multi-model designs presents promising avenue for enhancing review quality, consistency, and comprehensiveness."
        },
        {
            "title": "5.3 LLM-assisted Peer Review Workflows\nUnlike fully automated review generation, LLM-assisted peer review workflows focus on enhancing\nhuman reviewers‚Äô capabilities rather than replacing them. Recent research highlights the critical\nimportance of this human-AI collaborative approach in academic peer review. Studies by [12, 31,\n133] emphasize that while LLM can enhance efficiency, human oversight remains essential for\nmaintaining ethical standards and review integrity. Systems like AgentReview [60] demonstrate\nthis synergy in practice, where LLM generates initial insights that human reviewers then refine\nand validate.",
            "content": "The LLM-assisted peer review workflows enhance three primary functions in the scientific review process: (1) information extraction and summarization, which helps reviewers quickly grasp paper content; (2) manuscript validation and quality assurance, which supports systematic verification of paper claims; and (3) review writing support, which assists in generating well-structured feedback. In the information extraction and summarization function, systems automate document understanding and synthesis to support reviewer comprehension. PaperMage [101] is foundational toolkit that integrates natural language processing and computer vision models to process visually rich scientific documents, enabling sophisticated extraction of logical structure, figures, and textual content across multiple modalities. Complementing this structural analysis, CocoSciSum [29] focuses on content summarization, offering customizable paper summaries with precise control over length and keyword inclusion while maintaining high factual accuracy through its compositional control architecture. Preprint. 22 Luo and Yang et al. For the manuscript validation and quality assurance function, systems operate at different levels of analysis to ensure scientific rigor. At the local level, ReviewerGPT [97] specializes in systematic error detection and guideline compliance, achieving high accuracy in verifying submission requirements while effectively identifying mathematical errors and conceptual inconsistencies within individual manuscripts. While ReviewerGPT focuses on internal manuscript validation, PaperQA2 [144] performs global validation by examining claims against the broader scientific literature, employing sophisticated language agents to detect contradictions and verify assertions. The system demonstrates robust performance by identifying an average of 2.34 validated contradictions per paper while maintaining high factual accuracy in its cross-literature analyses. Additionally, Scideator [122], designed to facilitate idea validation, operates through facet recombination to identify novel and scientifically grounded analogies between papers. Scideator also includes novelty checker, which evaluates claims for uniqueness and adherence to established research paradigms, offering reviewers enhanced capabilities to scrutinize manuscripts rigorously. In the review writing support function, systems take different yet complementary approaches to assist reviewers at various expertise levels. ReviewFlow [149] provides intelligent scaffolding through contextual reflection cues and note synthesis guidance, modeling expert practices to help novice reviewers produce well-structured reviews. The systems step-by-step approach benefits those new to peer review by decomposing the complex task into manageable components. While ReviewFlow focuses on individual reviewer guidance, CARE [198] emphasizes collaborative aspects of review writing through an integrated platform featuring NLP-enhanced inline annotations and real-time collaboration features, enabling reviewers to work together more effectively while providing detailed and constructive feedback [19, 83]. Further complementing these functionalities, DocPilot [110] leverages modular task planning and code generation capabilities to automate repetitive and complex tasks in document workflows. Its structured approach to managing and annotating scientific PDFs ensures that reviewers can focus on substantive feedback rather than procedural hurdles, significantly improving their efficiency."
        },
        {
            "title": "5.4 Benchmarks",
            "content": "Table 5. Peer Review Datasets and Evaluation Metrics. The Evaluation Metrics columns use the following abbreviations: PR (Peer Review), MR (Meta-review), (Semantic Similarity), (Coherence & Relevance), (Diversity & Specificity), and (Human Evaluation). Columns S, C, D, and represent the evaluation metrics used in the study. Dataset Name PR MR Additional Task MOPRD [94] NLPEER [33] MReD [135] PEERSUM [78] ORSUM [184] ASAP-Review [183] REVIEWER2 [39] PeerRead [65] ReviewCritique [32] - - - - - - - Editorial decision prediction, Scientometric analysis Score prediction, Guided skimming, Pragmatic labeling Structured text summarization Opinion synthesis Opinion summarization, Factual consistency analysis Aspect-level analysis, Acceptance prediction Coverage & specificity enhancement Acceptance prediction, Score prediction Deficiency identification Evaluation Metrics - - - - - - - - - - - - - - - - - As automated review generation and LLM-assisted workflows continue to evolve, the research community faces critical challenge: systematically evaluating and comparing these approaches. Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 23 This need has led to the development of specialized benchmarks that assess various aspects of LLMbased peer review systems, from their ability to generate high-quality reviews to their effectiveness in supporting human reviewers. The development and evaluation of LLM-based peer review systems rely on standardized benchmarks that assess different aspects of the review process. These benchmarks can be broadly categorized into three main types: (1) comprehensive review datasets that support holistic evaluation, including editorial decisions, scoring, and pragmatic analysis; (2) specialized assessment datasets that focus on specific aspects like opinion synthesis and consistency analysis; and (3) quality evaluation datasets that measure review effectiveness through deficiency identification and acceptance prediction. Table 5 presents an overview of these key benchmarks and their associated evaluation frameworks. The datasets, primarily sourced from publicly accessible academic conferences, serve diverse purposes in peer review tasks. Comprehensive datasets like MOPRD [94] and NLPeer [33] provide broad coverage, supporting tasks ranging from editorial decision prediction to pragmatic labeling. More specialized datasets focus on specific aspects of the review process: ASAP-Review [183] and Reviewer2 [39] emphasize acceptance prediction and coverage assessment. Recent additions such as ReviewCritique [32] introduce novel mechanisms for comparative analysis between human and LLM-generated reviews. The evaluation framework for these benchmarks encompasses multiple dimensions, as detailed in Table 5. Semantic Similarity measures how closely generated reviews align with reference texts, typically using metrics like ROUGE and BertScore. Coherence and Relevance evaluate the logical flow and topical appropriateness of reviews, while Diversity and Specificity assess the range and depth of feedback provided. Human Evaluation, incorporating expert assessment of review quality, offers crucial validation of automated metrics. Together, these four evaluation components - Semantic Similarity, Coherence and Relevance, Diversity and Specificity, and Human Evaluation - form multi-faceted approach that ensures comprehensive assessment of LLM-generated reviews across various quality dimensions."
        },
        {
            "title": "5.5 Challenges and Future Work\nThe integration of LLMs into academic peer review represents a significant shift in scholarly\nevaluation [91, 92]. As academic institutions and publishers explore this technology, understanding\nits limitations and potential becomes crucial for the scholarly community.",
            "content": "Challenges. At the heart of peer review lies the need for deep expertise, nuanced understanding, and careful judgment. While LLMs show promise in supporting this process, their limitations reveal the complexity of automating scholarly assessment. fundamental challenge is that LLMs often struggle to fully grasp the specialized terminology and complex concepts within academic fields. For example, in biochemistry, an LLM might misinterpret the significance of specific protein interactions, while in theoretical physics, it could fail to recognize subtle but critical assumptions in mathematical models [192]. This limited technical comprehension directly impacts the LLMs ability to evaluate research methodology. When an LLM cannot fully understand field-specific concepts, it cannot reliably assess whether the research methods are appropriate or if the evidence justifies the conclusions. For instance, where methodological standards vary across fields in interdisciplinary research, LLMs often fail to identify critical issues such as inadequate sample sizes, inappropriate statistical tests, or missing experimental controls [129]. This limitation becomes particularly concerning given the high stakes of peer review in ensuring research quality and scientific integrity. Preprint. 24 Luo and Yang et al. The complexity of academic writing introduces additional challenges, mainly when dealing with longer manuscripts. Even as context windows expand, LLMs struggle to maintain coherent analysis across extensive texts, often losing track of complex arguments spanning multiple sections. This limitation frequently results in inconsistent or contradictory evaluations [18]. More concerning is the persistent issue of hallucinationmodels sometimes generate convincing but incorrect assessments, particularly when reviewing novel research approaches [28]. Furthermore, implementing LLMs in peer review faces additional challenges beyond technical performance limitations. fundamental infrastructure issue is the shortage of specialized training data [65, 184], which creates an uneven landscape across academic disciplines. This data scarcity particularly affects fields with smaller research communities or specialized vocabularies. Equally concerning are the ethical implications of LLM-assisted peer review. Issues of algorithmic bias and transparency [133] have emerged alongside new forms of academic misconduct, such as plagiarism laundering [117]. Additionally, critical concern is the potential homogenization of academic feedback if many researchers rely on the same LLM systems for peer review [91]. The widespread use of similar AI tools may reduce the diversity of perspectives and diminish the creative insights that stem from the distinct thought processes of individual human reviewers. Future Work. To advance LLMs capabilities in academic paper reviewing, several fundamental technical challenges must be prioritized. First, current LLMs struggle with specialized technical concepts across different academic fields, necessitating improved approaches for processing and understanding domain-specific terminology. Second, we need enhanced citation analysis capabilities to verify reference relevance and assess how effectively citations support papers arguments. Third, analyzing long academic documents requires new methods for maintaining coherence - from cross-referencing between sections to verifying consistency across methods, results, and conclusions. Beyond technical improvements, developing effective human-AI collaboration frameworks is crucial. The next generation of review systems must create intuitive interfaces that highlight potential issues and integrate seamlessly with human workflow [31]. These collaborative systems must be adaptable across different academic fields, with special consideration for disciplines with limited computational resources [132]. Rigorous evaluation frameworks for these human-AI systems must ensure they genuinely enhance reviewer efficiency and effectiveness [81, 169]. As LLM becomes more prevalent in peer review, robust governance mechanisms become critical. This includes developing reliable methods for detecting LLM-generated content, ensuring transparent tracking of LLM contributions, and maintaining reviewer authenticity [91]. Additionally, we need standardized protocols for securely integrating LLM review tools with existing journal platforms [6]. Lastly, progress in these areas must be measured through comprehensive evaluation frameworks. For technical capabilities, we need systematic assessments of improvements in language understanding, citation analysis, and document coherence. Human-AI collaboration metrics should evaluate the quality of LLM suggestions and their impact on reviewer efficiency. Governance evaluations must assess the reliability of LLM detection systems and the security of platform integrations. Crucially, these frameworks should examine potential biases across different academic disciplines, publication formats, and linguistic backgrounds to ensure equitable support for all scholarly communities. Through these targeted assessments, we can guide the development of LLM systems that meaningfully enhance the peer review process while maintaining its integrity. Preprint. LLM4SR: Survey on Large Language Models for Scientific Research"
        },
        {
            "title": "6 Conclusion\nThis survey comprehensively explores the transformative role of LLMs throughout the scientific\nresearch lifecycle, from hypothesis generation and experiment to writing and peer reviewing. By\nidentifying both the opportunities and challenges in applying LLMs to these tasks, we highlight\ntheir current capabilities, limitations, and potential to enhance scientific productivity. In conclusion,\nLLMs represent advanced productivity tools, offering new methods across all stages of modern\nscientific research. Despite being constrained by inherent limitations, technical barriers, and ethical\nconsiderations in domain-specific tasks, the continued advancement of LLM capabilities promises to\nrevolutionize research practices. As these systems evolve, their integration into scientific workflows\nwill not only accelerate discoveries but also foster unprecedented innovation and collaboration in\nthe scientific community.",
            "content": "Limitations The general concept AI for Science is huge topic, and this survey only focuses on the LLMs for scientific research aspect. In addition, many researchers from science background but not computer science background might also have conducted works in this domain, but might not published in computer science venues. We might have missed some of these works in this survey."
        },
        {
            "title": "References",
            "content": "[1] Ahmed AbuRaed, Horacio Saggion, Alexander V. Shvets, and √Älex Bravo. 2020. Automatic related work section generation: experiments in scientific document abstracting. Scientometrics 125, 3 (2020), 31593185. https://doi.org/ 10.1007/S11192-020-03630-2 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Shubham Agarwal, Issam H. Laradji, Laurent Charlin, and Christopher Pal. 2024. LitLLM: Toolkit for Scientific Literature Review. CoRR abs/2402.01788 (2024). https://doi.org/10.48550/ARXIV.2402.01788 arXiv:2402.01788 [4] Gustaf Ahdritz, Nazim Bouatta, Christina Floristean, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy ODonnell, Daniel Berenberg, Ian Fisk, Niccol√≤ Zanichelli, et al. 2024. OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization. Nature Methods (2024), 111. [5] Uchenna Akujuobi and Xiangliang Zhang. 2017. Delve: Dataset-Driven Scholarly Search and Analysis System. SIGKDD Explor. 19, 2 (2017), 3646. https://doi.org/10.1145/3166054.3166059 [6] S. Altm√§e, A. Sola-Leyva, and A. Salumets. 2023. Artificial intelligence in scientific writing: friend or foe? Reproductive BioMedicine Online 47, 1 (July 2023), 39. https://doi.org/10.1016/j.rbmo.2023.04.009 PMID: 37142479. [7] Samuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022. QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. CoRR abs/2205.12665. https://doi.org/10.48550/ARXIV.2205.12665 arXiv:2205. [8] Tal August, Katharina Reinecke, and Noah A. Smith. 2022. Generating Scientific Definitions with Controllable Complexity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 82988317. https://doi.org/10.18653/V1/2022.ACL-LONG.569 [9] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. ResearchAgent: Iterative Research https: Idea Generation over Scientific Literature with Large Language Models. CoRR abs/2404.07738 (2024). //doi.org/10.48550/ARXIV.2404.07738 arXiv:2404.07738 [10] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association for Computational Linguistics, 6572. https: //aclanthology.org/W05-0909/ [11] Chaitanya Bhatia, Tribikram Pradhan, and Sukomal Pal. 2020. MetaGen: An academic Meta-review Generation system. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (2020). https://api.semanticscholar.org/CorpusID:220730059 Preprint. 26 Luo and Yang et al. [12] S. Biswas, D. Dobaria, and H. L. Cohen. 2023. ChatGPT and the Future of Journal Reviews: Feasibility Study. The Yale Journal of Biology and Medicine 96, 3 (2023), 415420. https://doi.org/10.59249/SKDH9286 [13] Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle D. Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories. https://doi.org/10.48550/ARXIV.2409.07440 arXiv:2409.07440 [14] Daniil Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature 624, 7992 (2023), 570578. https://doi.org/10.1038/s41586-023-06792-0 [15] Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2024. Augmenting large language models with chemistry tools. Nat. Mac. Intell. 6, 5 (2024), 525535. https://doi.org/10.1038/S42256024-00832-8 [16] Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, and Tao Yu. 2024. Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows? https://doi.org/10.48550/ARXIV.2407.10956 arXiv:2407.10956 [17] Alan Chalmers. 2013. What is this thing called science? McGraw-Hill Education (UK). [18] Eric Chamoun, Michael Schlichtkrull, and Andreas Vlachos. 2024. Automated Focused Feedback Generation for Scientific Writing Assistance. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 97429763. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.580 [19] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/ forum?id=FQepisCUWu [20] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander MƒÖdry. 2024. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. arXiv:2410.07095 [cs.CL] https://arxiv.org/abs/2410. [21] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024. Data-Juicer: One-Stop Data Processing System for Large Language Models. In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS 2024, Santiago AA, Chile, June 9-15, 2024, Pablo Barcel√≥, Nayat S√°nchez-Pi, Alexandra Meliou, and S. Sudarshan (Eds.). ACM, 120134. https://doi.org/10.1145/3626246.3653385 [22] Hong Chen, Hiroya Takamura, and Hideki Nakayama. 2021. SciXGen: Scientific Paper Dataset for Context-Aware Text Generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 14831492. https://doi.org/10.18653/V1/2021.FINDINGSEMNLP.128 [23] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, and Rui Yan. 2021. Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 60686077. https: //doi.org/10.18653/V1/2021.ACL-LONG.473 [24] Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, et al. 2024. ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery. (2024). arXiv:2410.05080 [cs.CL] https://arxiv.org/abs/2410.05080 [25] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 35863596. https://doi.org/10.18653/V1/N19-1361 [26] Andrew Cropper and Sebastijan Dumancic. 2022. Inductive Logic Programming At 30: New Introduction. J. Artif. Intell. Res. 74 (2022), 765850. https://doi.org/10.1613/JAIR.1.13507 [27] Shih-Chieh Dai, Aiping Xiong, and Lun-Wei Ku. 2023. LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis. (2023), 999310001. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.669 [28] Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. 2024. MARG: Multi-Agent Review Generation for Scientific Papers. CoRR abs/2401.04259 (2024). https://doi.org/10.48550/ARXIV.2401.04259 arXiv:2401.04259 Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 27 [29] Yixi Ding, Yanxia Qin, Qian Liu, and Min-Yen Kan. 2023. CocoSciSum: Scientific Summarization Toolkit with Compositional Controllability. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023, Yansong Feng and Els Lefever (Eds.). Association for Computational Linguistics, 518526. https://doi.org/10.18653/V1/2023.EMNLP-DEMO.47 [30] Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui, and Furu Wei. 2023. Large Language Model for Science: Study on vs. NP. CoRR abs/2309.05689 (2023). https://doi.org/10.48550/ARXIV.2309.05689 arXiv:2309.05689 [31] Iddo Drori and Dov Teeni. 2024. Human-in-the-Loop AI Reviewing: Feasibility, Opportunities, and Risks. J. Assoc. Inf. Syst. 25, 1 (2024), 7. https://aisel.aisnet.org/jais/vol25/iss1/7 [32] Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. 2024. LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing. CoRR abs/2406.16253 (2024). https://doi.org/10.48550/ARXIV.2406. 16253 arXiv:2406. [33] Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023. NLPeer: Unified Resource for the Computational Study of Peer Review. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 50495073. https://doi.org/10.18653/V1/2023.ACL-LONG.277 [34] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez (Eds.). Association for Computational Linguistics, 35583567. https://doi.org/10.18653/V1/P19-1346 [35] Noelia Ferruz and Birte H√∂cker. 2022. Controllable protein design with language models. Nat. Mach. Intell. 4, 6 (2022), 521532. https://doi.org/10.1038/S42256-022-00499-Z [36] Santo Fortunato, Carl Bergstrom, Katy B√∂rner, James Evans, Dirk Helbing, Sta≈°a Milojeviƒá, Alexander Petersen, Filippo Radicchi, Roberta Sinatra, Brian Uzzi, et al. 2018. Science of science. Science 359, 6379 (2018), eaao0185. [37] Martin Funkquist, Ilia Kuznetsov, Yufang Hou, and Iryna Gurevych. 2023. CiteBench: Benchmark for Scientific Citation Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 73377353. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.455 [38] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling Large Language Models to Generate Text with Citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 64656488. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.398 [39] Zhaolin Gao, Kiant√© Brantley, and Thorsten Joachims. 2024. Reviewer2: Optimizing Review Generation Through Prompt Generation. CoRR abs/2402.10886 (2024). https://doi.org/10.48550/ARXIV.2402.10886 arXiv:2402.10886 [40] Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, and Jana Diesner. 2021. BACO: Background Knowledgeand Content-Based Framework for Citing Sentence Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 14661478. https://doi.org/10.18653/V1/ 2021.ACL-LONG. [41] Alireza Ghafarollahi and Markus J. Buehler. 2024. SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. CoRR abs/2409.05556 (2024). https://doi.org/10.48550/ARXIV.2409.05556 arXiv:2409.05556 [42] Alexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Nihar B. Shah. 2023. Peer Reviews of Peer Reviews: Randomized Controlled Trial and Other Experiments. CoRR abs/2311.09497 (2023). https://doi.org/10.48550/ARXIV.2311.09497 arXiv:2311.09497 [43] Nianlong Gu and Richard Hahnloser. 2024. Controllable Citation Sentence Generation with Language Models. In Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024), Tirthankar Ghosal, Amanpreet Singh, Anita Waard, Philipp Mayr, Aakanksha Naik, Orion Weller, Yoonjoo Lee, Shannon Shen, and Yanxia Qin (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 2237. https://aclanthology.org/2024.sdp-1.4 [44] Haohuai He, Bing He, Lei Guan, Yu Zhao, Feng Jiang, Guanxing Chen, Qingge Zhu, Calvin Yu-Chian Chen, Ting Li, and Jianhua Yao. 2024. De novo generation of SARS-CoV-2 antibody CDRH3 with pre-trained generative large language model. Nature Communications 15, 1 (2024), 6867. https://doi.org/10.1038/s41467-024-50903-y Preprint. 28 Luo and Yang et al. [45] Vu Cong Duy Hoang and Min-Yen Kan. 2010. Towards Automated Related Work Summarization. In COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume, 23-27 August 2010, Beijing, China, Chu-Ren Huang and Dan Jurafsky (Eds.). Chinese Information Processing Society of China, 427435. https://aclanthology.org/ C10-2049/ [46] Noah Hollmann, Samuel M√ºller, and Frank Hutter. 2023. Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (2023). http://papers.nips.cc/paper_files/ paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html [47] Tom Hope, Doug Downey, Daniel S. Weld, Oren Etzioni, and Eric Horvitz. 2023. Computational Inflection for Scientific Discovery. Commun. ACM 66, 8 (2023), 6273. https://doi.org/10.1145/3576896 [48] Ting-Yao Hsu, C. Lee Giles, and Ting-Hao Kenneth Huang. 2021. SciCap: Generating Captions for Scientific Figures. Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021 (2021), 32583264. https://doi.org/10.18653/V1/2021.FINDINGS-EMNLP.277 [49] Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. 2024. Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. arXiv preprint arXiv:2410.14255 (2024). [50] Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, and Liang Zhao. 2024. HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation. arXiv:2410.03761 [cs.CL] https://arxiv.org/ abs/2410. [51] Yue Hu and Xiaojun Wan. 2014. Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, meeting of SIGDAT, Special Interest Group of the ACL, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 16241633. https://doi.org/10.3115/V1/D14-1170 [52] Kaixuan Huang, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ B. Altman, Mengdi Wang, and Le Cong. 2024. CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments. CoRR abs/2404.18021 (2024). https://doi.org/10.48550/ARXIV.2404.18021 arXiv:2404.18021 [53] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. CoRR abs/2311.05232 (2023). https://doi.org/10.48550/ARXIV.2311.05232 arXiv:2311.05232 [54] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. https://openreview.net/forum?id=1Fs1LvjYQW [55] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of LLM agents: survey. CoRR abs/2402.02716 (2024). https://doi.org/10.48550/ARXIV.2402.02716 arXiv:2402. [56] Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, and Roy Kishony. 2024. Autonomous LLM-driven research from data to human-verifiable research papers. CoRR abs/2404.17605 (2024). https://doi.org/10.48550/ARXIV.2404.17605 arXiv:2404.17605 [57] Peter A. Jansen, Marc-Alexandre C√¥t√©, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. 2024. DISCOVERYWORLD: Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents. CoRR abs/2406.06769 (2024). https://doi.org/10.48550/ARXIV. 2406.06769 arXiv:2406.06769 [58] William Stanley Jevons. 1877. The principles of science: treatise on logic and scientific method. Macmillan and Company. [59] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1248:38. https://doi.org/10.1145/3571730 [60] Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. 2024. AgentReview: Exploring Peer Review Dynamics with LLM Agents. CoRR abs/2406.12708 (2024). https://doi.org/10.48550/ARXIV. 2406.12708 arXiv:2406. [61] Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, https: and Dong Yu. 2024. DSBench: How Far Are Data Science Agents to Becoming Data Science Experts? //doi.org/10.48550/ARXIV.2409.07703 arXiv:2409.07703 [62] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. 2021. Highly accurate protein structure prediction with AlphaFold. nature 596, 7873 (2021), 583589. [63] Shing-Yun Jung, Ting-Han Lin, Chia-Hung Liao, Shyan-Ming Yuan, and Chuen-Tsai Sun. 2022. Intent-Controllable Citation Text Generation. Mathematics 10, 10 (2022). https://doi.org/10.3390/math10101763 Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 29 [64] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. 2024. Position: LLMs Cant Plan, But Can Help Planning in LLM-Modulo Frameworks. In Fortyfirst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=Th8JPEmH4z [65] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard H. Hovy, and Roy Schwartz. 2018. Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, 16471661. https://doi.org/10.18653/V1/N18-1149 [66] Kayvan Kousha and Mike Thelwall. 2023. Artificial intelligence to support publishing and peer review: summary and review. Learned Publishing 37 (2023). https://api.semanticscholar.org/CorpusID: [67] Asheesh Kumar, Tirthankar Ghosal, and Asif Ekbal. 2021. Deep Neural Architecture for Decision-Aware MetaReview Generation. 2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL) (2021), 222225. https://api. semanticscholar.org/CorpusID:245540935 [68] Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, and Asif Ekbal. 2024. Can Large Language Models Unlock Novel Scientific Research Ideas? arXiv preprint arXiv:2409.06185 (2024). [69] Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K. Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Mausam, Margot Mieskes, Aur√©lie N√©v√©ol, Danish Pruthi, Lizhen Qu, Roy Schwartz, Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna Rogers, Nihar B. Shah, and Iryna Gurevych. 2024. What Can Natural Language Processing Do for Peer Review? CoRR abs/2405.06563 (2024). https://doi.org/10.48550/ARXIV.2405.06563 arXiv:2405.06563 [70] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida I. Wang, and Tao Yu. 2023. DS-1000: Natural and Reliable Benchmark for Data Science Code Generation. , 1831918345 pages. https://proceedings.mlr.press/v202/lai23b.html [71] Pat Langley. 1977. BACON: Production System That Discovers Empirical Laws. In Proceedings of the 5th International Joint Conference on Artificial Intelligence. Cambridge, MA, USA, August 22-25, 1977, Raj Reddy (Ed.). William Kaufmann, 344. http://ijcai.org/Proceedings/77-1/Papers/057.pdf [72] Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. 2024. LAB-Bench: Measuring Capabilities of Language Models for Biology Research. CoRR abs/2407.10362 (2024). https://doi.org/10.48550/ARXIV.2407.10362 arXiv:2407.10362 [73] Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI 22). Association for Computing Machinery, New York, NY, USA, Article 388, 19 pages. https://doi.org/10.1145/3491102.3502030 [74] Douglas B. Lenat. 1977. Automated Theory Formation in Mathematics. In Proceedings of the 5th International Joint Conference on Artificial Intelligence. Cambridge, MA, USA, August 22-25, 1977, Raj Reddy (Ed.). William Kaufmann, 833842. http://ijcai.org/Proceedings/77-2/Papers/061.pdf [75] Douglas B. Lenat and John Seely Brown. 1984. Why AM and EURISKO Appear to Work. Artif. Intell. 23, 3 (1984), 269294. https://doi.org/10.1016/0004-3702(84)90016-X [76] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips. cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html [77] Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al. 2024. Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents. arXiv preprint arXiv:2410.13185 (2024). [78] Miao Li, Eduard H. Hovy, and Jey Han Lau. 2023. Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 70897112. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.472 [79] Michael Y. Li, Emily B. Fox, and Noah D. Goodman. 2024. Automated Statistical Model Discovery with Language Models. (2024). https://openreview.net/forum?id=B5906M4Wnd Preprint. Luo and Yang et al. [80] Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, and Xinya Du. 2024. Learning to Generate Research Idea with Dynamic Control. arXiv preprint arXiv:2412.14626 (2024). [81] Ruosen Li, Ruochen Li, Barry Wang, and Xinya Du. [n. d.]. IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [82] Ruosen Li, Ziming Luo, and Xinya Du. 2024. FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning. arXiv:2410.06304 [cs.CL] https://arxiv.org/abs/2410.06304 [83] Ruosen Li, Teerth Patel, and Xinya Du. 2024. PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. Trans. Mach. Learn. Res. 2024 (2024). https://openreview.net/forum?id=YVD1QqWRaj [84] Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024. MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents. CoRR abs/2408.14033 (2024). https://doi.org/10.48550/ARXIV.2408.14033 arXiv:2408.14033 [85] Siyu Li, Jin Yang, and Kui Zhao. 2023. Are you in Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks. CoRR abs/2307.10337 (2023). https://doi.org/10.48550/ARXIV. 2307.10337 arXiv:2307.10337 [86] Xiangci Li, Biswadip Mandal, and Jessica Ouyang. 2022. CORWA: Citation-Oriented Related Work Annotation Dataset. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iv√°n Vladimir Meza Ru√≠z (Eds.). Association for Computational Linguistics, 5426 5440. https://doi.org/10.18653/V1/2022.NAACL-MAIN.397 [87] Xiangci Li and Jessica Ouyang. 2022. Automatic Related Work Generation: Meta Study. CoRR abs/2201.01880 (2022). arXiv:2201.01880 https://arxiv.org/abs/2201. [88] Xiangci Li and Jessica Ouyang. 2024. Explaining Relationships Among Research Papers. CoRR abs/2402.13426 (2024). https://doi.org/10.48550/ARXIV.2402.13426 arXiv:2402.13426 [89] Xiangci Li and Jessica Ouyang. 2024. Related Work and Citation Text Generation: Survey. CoRR abs/2404.11588 (2024). https://doi.org/10.48550/ARXIV.2404.11588 arXiv:2404.11588 [90] Ziyue Li, Yuan Chang, and Xiaoqiu Le. 2024. Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving. In Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). 243256. [91] Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, and James Y. Zou. 2024. Monitoring AI-Modified Content at Scale: Case Study on the Impact of ChatGPT on AI Conference Peer Reviews. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=bX3J7ho18S [92] Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D. Manning, and James Y. Zou. 2024. Mapping the Increasing Use of LLMs in Scientific Papers. CoRR abs/2404.01268 (2024). https://doi.org/10.48550/ARXIV.2404.01268 arXiv:2404.01268 [93] Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 7481. https://aclanthology.org/W04-1013 [94] Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, and Xiaodong Shi. 2023. MOPRD: multidisciplinary open peer review dataset. Neural Comput. Appl. 35, 34 (2023), 2419124206. https://doi.org/10.1007/S00521-023-08891-5 [95] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. 2023. Evolutionary-scale prediction of atomic-level protein structure with language model. Science 379, 6637 (2023), 11231130. https://doi.org/10.1126/science.ade [96] Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, and Chenhao Tan. 2024. Literature Meets Data: Synergistic Approach to Hypothesis Generation. arXiv preprint arXiv:2410.17309 (2024). [97] Ryan Liu and Nihar B. Shah. 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. CoRR abs/2306.00622 (2023). https://doi.org/10.48550/ARXIV.2306.00622 arXiv:2306.00622 [98] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. 2023. Training Socially Aligned Language Models in Simulated Human Society. CoRR abs/2305.16960 (2023). https://doi.org/10.48550/ARXIV.2305.16960 arXiv:2305.16960 [99] Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. 2024. Conversational Drug Editing Using Retrieval and Domain Feedback. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum? id=yRrPfKyJQ [100] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2024. AgentBench: Evaluating LLMs as Agents. Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 31 https://openreview.net/forum?id=zAdUB0aCTQ [101] Kyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chee Chang, Russell Authur, Erin Bransom, Stefan Candra, Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, Amanpreet Singh, Chris Wilhelm, Angele Zamarron, Marti A. Hearst, Daniel S. Weld, Doug Downey, and Luca Soldaini. 2023. PaperMage: Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023, Yansong Feng and Els Lefever (Eds.). Association for Computational Linguistics, 495507. https://doi.org/10.18653/V1/2023.EMNLPDEMO. [102] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 49694983. https://doi.org/10.18653/V1/2020.ACL-MAIN.447 [103] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. CoRR abs/2408.06292 (2024). https://doi.org/10.48550/ARXIV.2408.06292 arXiv:2408.06292 [104] Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multi-XScience: Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 80688074. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.648 [105] Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B. Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik. 2024. LLM and Simulation as Bilevel Optimizers: New Paradigm to Advance Physical Scientific Discovery. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=hz8cFsdz7P [106] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. (2023). http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html [107] Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, and Peter Clark. 2024. Position: Data-driven Discovery with Large Generative Models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=5SpjhZNXtt [108] Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. 2024. DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. CoRR abs/2407.01725 (2024). https://doi.org/10.48550/ARXIV.2407.01725 arXiv:2407.01725 [109] Anna Martin-Boyle, Aahan Tyagi, Marti A. Hearst, and Dongyeop Kang. 2024. Shallow Synthesis of Knowledge in GPT-Generated Texts: Case Study in Automatic Related Work Composition. CoRR abs/2402.12255 (2024). https://doi.org/10.48550/ARXIV.2402.12255 arXiv:2402. [110] Puneet Mathur, Alexa Siu, Varun Manjunatha, and Tong Sun. 2024. DocPilot: Copilot for Automating PDF Edit Workflows in Documents. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations) (2024). https://api.semanticscholar.org/CorpusID:272801354 [111] Nafise Sadat Moosavi, Andreas R√ºckl√©, Dan Roth, and Iryna Gurevych. 2021. SciGen: Dataset for Reasoning-Aware Text Generation from Scientific Tables. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/149e9677a5989fd342ae44213df68868Abstract-round2.html [112] Kazuya Nishimura, Kuniaki Saito, Tosho Hirasawa, and Yoshitaka Ushiku. 2024. Toward Structured Related Work Generation with Novelty Statements. In Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024), Tirthankar Ghosal, Amanpreet Singh, Anita Waard, Philipp Mayr, Aakanksha Naik, Orion Weller, Yoonjoo Lee, Shannon Shen, and Yanxia Qin (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 3857. https://aclanthology.org/2024.sdp-1.5 [113] John Norton. 2003. little survey of induction. (2003). [114] Ruth Oliver, Melissa Chapman, Nathan Emery, Lauren Gillespie, Natasha Gownaris, Sophia Leiker, Anna Nisi, David Ayers, Ian Breckheimer, Hannah Blondin, et al. 2024. Opening conversation on responsible environmental data science in the age of large language models. Environmental Data Science 3 (2024), e14. [115] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311318. https://doi.org/10.3115/1073083.1073135 Preprint. 32 Luo and Yang et al. [116] Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Za√Ød Harchaoui. 2021. MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 48164828. https://proceedings.neurips.cc/paper/2021/hash/ 260c2432a0eecc28ce03c10dadc078a4-Abstract.html [117] Mikolaj Piniewski, Ivan Jaric, Demetris Koutsoyiannis, and Zbigniew W. Kundzewicz. 2024. Emerging plagiarism in peer-review evaluation reports: tip of the iceberg? Scientometrics 129, 4 (2024), 24892498. https://doi.org/10.1007/ S11192-024-04960-1 [118] Kevin Pu, KJ Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, and Pao Siangliulue. 2024. IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback. arXiv preprint arXiv:2410.04025 (2024). [119] Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. 2023. Large Language Models are Zero Shot Hypothesis Proposers. CoRR abs/2311.05965 (2023). https://doi.org/10.48550/ARXIV. 2311.05965 arXiv:2311. [120] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. 2024. Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=bNt7oajl2a [121] Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S. Weld. 2024. Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination. CoRR abs/2409.14634 (2024). https://doi.org/10.48550/ARXIV.2409.14634 arXiv:2409.14634 [122] Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S. Weld. 2024. Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination. ArXiv abs/2409.14634 (2024). https://api.semanticscholar.org/CorpusID:272827497 [123] Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The ACL anthology network corpus. Lang. Resour. Evaluation 47, 4, 919944. https://doi.org/10.1007/S10579-012-9211-2 [124] Mayk Caldas Ramos, Shane S. Michtavy, Marc D. Porosoff, and Andrew D. White. 2023. Bayesian Optimization of Catalysts With In-context Learning. CoRR abs/2304.05341 (2023). https://doi.org/10.48550/ARXIV.2304.05341 arXiv:2304.05341 [125] Sumedh Rasal and E. J. Hauer. 2024. Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs. CoRR abs/2402.16713 (2024). https://doi.org/10.48550/ARXIV.2402.16713 arXiv:2402.16713 [126] Zeeshan Rasheed, Muhammad Waseem, Aakash Ahmad, Kai-Kristian Kemell, Xiaofeng Wang, Anh Nguyen-Duc, and Pekka Abrahamsson. 2024. Can Large Language Models Serve as Data Analysts? Multi-Agent Assisted Approach for Qualitative Data Analysis. CoRR abs/2402.01386 (2024). https://doi.org/10.48550/ARXIV.2402.01386 arXiv:2402.01386 [127] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 39803990. https://doi.org/10.18653/V1/D19-1410 [128] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. 2021. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl. Acad. Sci. USA 118, 15 (2021), e2016239118. https://doi.org/10.1073/ PNAS.2016239118 [129] Zachary Robertson. 2023. GPT4 is Slightly Helpful for Peer-Review Assistance: Pilot Study. CoRR abs/2307.05492 (2023). https://doi.org/10.48550/ARXIV.2307.05492 arXiv:2307.05492 [130] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. 2024. Mathematical discoveries from program search with large language models. Nat. 625, 7995 (2024), 468475. https://doi.org/10.1038/S41586-023-06924- [131] Yixiang Ruan, Chenyin Lu, Ning Xu, Yuchen He, Yixin Chen, Jian Zhang, Jun Xuan, Jianzhang Pan, Qun Fang, Hanyu Gao, et al. 2024. An automatic end-to-end chemical synthesis development platform powered by large language models. Nature communications 15, 1 (2024), 10160. [132] Shubhra Kanti Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, and Matthew C. Williams Jr. 2024. Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts. CoRR abs/2402.15589 (2024). https://doi.org/10.48550/ARXIV.2402.15589 arXiv:2402.15589 Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 33 [133] Laurie A. Schintler, Connie L. McNeely, and James Witte. 2023. Critical Examination of the Ethics of AI-Mediated Peer Review. CoRR abs/2309.12356 (2023). https://doi.org/10.48550/ARXIV.2309.12356 arXiv:2309.12356 [134] Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 78817892. https://doi.org/10.18653/V1/2020.ACL-MAIN.704 [135] Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, and Luo Si. 2022. MReD: Meta-Review Dataset for Structure-Controllable Text Generation. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 25212535. https://doi.org/10.18653/V1/2022.FINDINGS-ACL.198 [136] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (2023). http://papers.nips.cc/paper_files/paper/2023/hash/ 77c33e6a367922d003ff102ffb92b658-Abstract-Conference.html [137] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. 2023. TaskBench: Benchmarking Large Language Models for Task Automation. CoRR abs/2311.18760 (2023). https://doi.org/10.48550/ARXIV.2311.18760 arXiv:2311.18760 [138] Zhengliang Shi, Shen Gao, Zhen Zhang, Xiuying Chen, Zhumin Chen, Pengjie Ren, and Zhaochun Ren. 2023. Towards Unified Framework for Reference Retrieval and Related Work Generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 57855799. https://doi.org/10.18653/V1/2023.FINDINGSEMNLP. [139] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: lanhttp://papers.nips.cc/paper_files/paper/2023/hash/ (2023). guage agents with verbal reinforcement learning. 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html [140] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K. Reddy. 2024. LLMSR: Scientific Equation Discovery via Programming with Large Language Models. CoRR abs/2404.18400 (2024). https://doi.org/10.48550/ARXIV.2404.18400 arXiv:2404.18400 [141] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can LLMs Generate Novel Research Ideas? Large-Scale Human Study with 100+ NLP Researchers. CoRR abs/2409.04109 (2024). https://doi.org/10.48550/ARXIV.2409.04109 arXiv:2409.04109 [142] Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. 2024. CORE-Bench: Fostering the Credibility of Published Research Through Computational Reproducibility Agent Benchmark. https: //doi.org/10.48550/ARXIV.2409.11363 arXiv:2409. [143] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. 2024. Rethinking Interpretability in the Era of Large Language Models. CoRR abs/2402.01761 (2024). https://doi.org/10.48550/ARXIV.2402.01761 arXiv:2402.01761 [144] Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela M. Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, and Andrew D. White. 2024. Language agents achieve superhuman synthesis of scientific knowledge. CoRR abs/2409.13740 (2024). https://doi.org/10.48550/ARXIV.2409.13740 arXiv:2409.13740 [145] Henry Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury. 2023. Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 83488365. https://doi.org/10.18653/V1/ 2023.FINDINGS-EMNLP.560 [146] Henry W. Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V. Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, and Sutanay Choudhury. 2024. CHEMREASONER: Heuristic Search over Large Language Models Knowledge Space using Quantum-Chemical Feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=3tJDnEszco [147] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 82738288. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.566 [148] Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, and Nanqing Dong. 2024. Two Heads Are Better Than One: Multi-Agent System Has the Potential to Improve Scientific Idea Generation. arXiv preprint arXiv:2410.09403 (2024). [149] Lu Sun, Aaron Chan, Yun Seo Chang, and Steven P. Dow. 2024. ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing. In Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI 2024, Preprint. 34 Luo and Yang et al. Greenville, SC, USA, March 18-21, 2024. ACM, 120137. https://doi.org/10.1145/3640543.3645159 [150] Teo Susnjak, Peter Hwang, Napoleon H. Reyes, Andre L. C. Barczak, Timothy R. McIntosh, and Surangika Ranathunga. 2024. Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning. CoRR abs/2404.08680 (2024). https://doi.org/10.48550/ARXIV.2404.08680 arXiv:2404.08680 [151] Don Swanson. 1986. Undiscovered public knowledge. The Library Quarterly 56, 2 (1986), 103118. [152] Justin Sybrandt, Ilya Tyagin, Michael Shtutman, and Ilya Safro. 2020. AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach. In CIKM 20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, Mathieu dAquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe Cudr√©-Mauroux (Eds.). ACM, 27572764. https://doi.org/10.1145/3340531.3412684 [153] Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large Language Models for Data Annotation: Survey. CoRR abs/2402.13446 (2024). https://doi.org/10.48550/ARXIV.2402.13446 arXiv:2402.13446 [154] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv abs/2302.13971 (2023). https: //api.semanticscholar.org/CorpusID: [155] Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nat. 571, 7763 (2019), 9598. https://doi.org/10.1038/S41586-019-1335-8 [156] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora S. Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. 2023. Scientific discovery in the age of artificial intelligence. Nat. 620, 7972 (2023), 4760. https://doi.org/10.1038/S41586023-06221-2 [157] Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Streith-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Al√°n Aspuru-Guzik, Kirill Neklyudov, and Chao Zhang. 2024. Efficient Evolutionary Search Over Chemical Space with Large Language Models. CoRR abs/2406.16976 (2024). https: //doi.org/10.48550/ARXIV.2406.16976 arXiv:2406.16976 [158] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024. survey on large language model based autonomous agents. Frontiers Comput. Sci. 18, 6 (2024), 186345. https://doi.org/10.1007/S11704-024-40231-1 [159] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024. SciMON: Scientific Inspiration Machines Optimized for Novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 279299. https://doi.org/10.18653/V1/2024.ACL-LONG.18 [160] Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi Luan. 2019. PaperRobot: Incremental Draft Generation of Scientific Ideas. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez (Eds.). Association for Computational Linguistics, 19801991. https://doi.org/10.18653/V1/P19-1191 [161] Qingqin Wang, Yun Xiong, Yao Zhang, Jiawei Zhang, and Yangyong Zhu. 2021. AutoCite: Multi-Modal Representation Fusion for Contextual Citation Generation. In WSDM 21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021, Liane Lewin-Eytan, David Carmel, Elad Yom-Tov, Eugene Agichtein, and Evgeniy Gabrilovich (Eds.). ACM, 788796. https://doi.org/10.1145/3437963.3441739 [162] Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, and Nazneen Fatema Rajani. 2020. ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. In Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020, Brian Davis, Yvette Graham, John D. Kelleher, and Yaji Sripada (Eds.). Association for Computational Linguistics, 384397. https://doi.org/10.18653/V1/ 2020.INLG-1. [163] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. 2023. Hypothesis Search: Inductive Reasoning with Language Models. CoRR abs/2309.05660 (2023). https://doi.org/10.48550/arXiv. 2309.05660 arXiv:2309.05660 [164] Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, and Jieping Ye. 2024. SciPIP: An LLM-based Scientific Paper Idea Proposer. https://api.semanticscholar.org/CorpusID:273695165 [165] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. 2024. AutoSurvey: Large Language Models Can Automatically Write Surveys. CoRR abs/2406.10252 (2024). https://doi.org/10.48550/ARXIV.2406.10252 arXiv:2406.10252 Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 35 [166] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems. [167] Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2024. CycleResearcher: Improving Automated Research via Automated Review. (2024). arXiv:2411.00816 [cs.CL] https: //arxiv.org/abs/2411.00816 [168] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. CoRR abs/2308.08155 (2023). https://doi.org/10.48550/ARXIV.2308.08155 arXiv:2308.08155 [169] Ziang Xiao, Wesley Hanwen Deng, Michelle S. Lam, Motahhare Eslami, Juho Kim, Mina Lee, and Q. Vera Liao. 2024. Human-Centered Evaluation and Auditing of Language Models. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA 2024, Honolulu, HI, USA, May 11-16, 2024, Florian Floyd Mueller, Penny Kyburz, Julie R. Williamson, and Corina Sas (Eds.). ACM, 476:1476:6. https://doi.org/10.1145/3613905.3636302 [170] Xinyu Xing, Xiaosheng Fan, and Xiaojun Wan. 2020. Automatic Generation of Citation Texts in Scholarly Papers: Pilot Study. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 61816190. https://doi.org/10.18653/V1/2020.ACL-MAIN.550 [171] Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2023. Exploring and Verbalizing Academic Ideas by Concept Co-occurrence. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1300113027. https://doi.org/10.18653/V1/ 2023.ACL-LONG.727 [172] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024. 44894500. [173] Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022. Language Models as Inductive Reasoners. EACL 2024 abs/2212.10923 (2022). https://doi.org/10.48550/arXiv.2212.10923 arXiv:2212.10923 [174] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. 2024. Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1354513565. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.804 [175] Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. 2023. Logical Reasoning over Natural Language as Knowledge Representation: Survey. CoRR abs/2303.12023 (2023). https://doi.org/10.48550/ARXIV.2303.12023 arXiv:2303.12023 [176] Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. 2024. MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses. arXiv preprint arXiv:2410.07076 (2024). [177] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=WE_vluYUL-X [178] Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R. Fabbri, Irene Li, Dan Friedman, and Dragomir R. Radev. 2019. ScisummNet: Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press, 73867393. https://doi.org/10.1609/AAAI.V33I01.33017386 [179] Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. 2024. DrugAssist: Large Language Model for Molecule Optimization. CoRR abs/2401.10334 (2024). https: //doi.org/10.48550/ARXIV.2401.10334 arXiv:2401.10334 [180] Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Renjing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, and Xiang Li. 2024. Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis. CoRR abs/2407.12857 (2024). https://doi.org/10.48550/ARXIV.2407.12857 arXiv:2407. [181] Luyao Yu, Qi Zhang, Chongyang Shi, An Lao, and Liang Xiao. 2024. Reinforced Subject-Aware Graph Neural Network for Related Work Generation. In Knowledge Science, Engineering and Management, Cungeng Cao, Huajun Chen, Liang Zhao, Junaid Arshad, Taufiq Asyhari, and Yonghao Wang (Eds.). Springer Nature Singapore, Singapore, 201213. Preprint. 36 Luo and Yang et al. [182] Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, and Phillip Howard. 2024. Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review. ArXiv abs/2410.03019 (2024). https://api.semanticscholar. org/CorpusID:273162522 [183] Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2022. Can We Automate Scientific Reviewing? J. Artif. Intell. Res. (2022), 171212. https://doi.org/10.1613/JAIR.1.12862 [184] Qi Zeng, Mankeerat Sidhu, Ansel Blume, Hou Pong Chan, Lu Wang, and Heng Ji. 2024. Scientific Opinion arXiv:2305.14647 [cs.CL] Summarization: Paper Meta-review Generation Dataset, Methods, and Evaluation. https://arxiv.org/abs/2305.14647 [185] Haochen Zhang, Yuyang Dong, Chuan Xiao, and Masafumi Oyamada. 2023. Jellyfish: Large Language Model for Data Preprocessing. CoRR abs/2312.01678 (2023). https://doi.org/10.48550/ARXIV.2312.01678 arXiv:2312.01678 [186] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. (2020). https://openreview.net/forum?id=SkeHuCVFDr [187] Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. 2024. Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery. CoRR abs/2406.10833 (2024). https://doi.org/10.48550/ARXIV.2406.10833 arXiv:2406. [188] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming Liu. 2024. Revolutionizing Finance with LLMs: An Overview of Applications and Insights. CoRR abs/2401.11641 (2024). https://doi.org/10.48550/ARXIV.2401.11641 arXiv:2401.11641 [189] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 563578. https://doi.org/10.18653/V1/D19-1053 [190] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. Survey of Large Language Models. CoRR abs/2303.18223 (2023). https://doi.org/10.48550/ARXIV.2303.18223 arXiv:2303.18223 [191] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. 2023. Goal Driven Discovery of Distributional Differences via Language Descriptions. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html [192] Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is LLM Reliable Reviewer? Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, V√©ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 93409351. https://aclanthology.org/2024.lrec-main.816 [193] Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. 2024. Hypothesis Generation with Large Language Models. CoRR abs/2404.04326 (2024). https://doi.org/10.48550/ARXIV.2404.04326 arXiv:2404.04326 [194] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023. Large Language Models can Learn Rules. CoRR abs/2310.07064 (2023). https://doi.org/10.48550/ARXIV.2310.07064 arXiv:2310. [195] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023. On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, 10901102. https://doi.org/10.18653/V1/2023.EACL-MAIN.77 [196] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2024. Can Large Language Models Transform Computational Social Science? Comput. Linguistics 50, 1 (2024), 237291. https://doi.org/10.1162/ COLI_A_00502 [197] Robert Zimmermann, Marina Staab, Mehran Nasseri, and Patrick Brandtner. 2024. Leveraging Large Language Models for Literature Review Tasks - Case Study Using ChatGPT. In Advanced Research in Technologies, Information, Innovation and Sustainability, Teresa Guarda, Filipe Portela, and Jose Maria Diaz-Nafria (Eds.). Springer Nature Switzerland, Cham, 313323. [198] Dennis Zyska, Nils Dycke, Jan Buchmann, Ilia Kuznetsov, and Iryna Gurevych. 2023. CARE: Collaborative AI-Assisted Reading Environment. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023, Toronto, Canada, July 10-12, 2023, Danushka Bollegala, Ruihong Huang, and Alan Preprint. LLM4SR: Survey on Large Language Models for Scientific Research 37 Ritter (Eds.). Association for Computational Linguistics, 291303. https://doi.org/10.18653/V1/2023.ACL-DEMO.28 Preprint."
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore",
        "University of Texas at Dallas, USA"
    ]
}