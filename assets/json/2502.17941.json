{
    "paper_title": "Optimal Brain Apoptosis",
    "authors": [
        "Mingyuan Sun",
        "Zheng Fang",
        "Jiaxu Wang",
        "Junjie Jiang",
        "Delei Kong",
        "Chenming Hu",
        "Yuetong Fang",
        "Renjing Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 1 4 9 7 1 . 2 0 5 2 : r Published as conference paper at ICLR"
        },
        {
            "title": "OPTIMAL BRAIN APOPTOSIS",
            "content": "Mingyuan Sun1, Zheng Fang1,, Jiaxu Wang2, Junjie Jiang1, Delei Kong3, Chenming Hu1, Yuetong Fang2 & Renjing Xu2, 1Northeastern University 3Hunan University 2The Hong Kong University of Science and Technology (Guangzhou)"
        },
        {
            "title": "ABSTRACT",
            "content": "The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose highly efficient technique for computing the secondorder Taylor expansion of parameters. This approach allows for more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViTB/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA."
        },
        {
            "title": "INTRODUCTION",
            "content": "With the rapid development of deep learning, neural networks have become deeply integrated into all sectors of our daily life. Convolutional Neural Networks (LeCun et al., 1998; Krizhevsky et al., 2012; He et al., 2016; Ma et al., 2023) and Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) are two typical structures used most widely. As researchers continuously innovate, the performance of neural networks improves, but the number of parameters and the computational complexity also increase significantly. Therefore, how to efficiently reduce the parameter size and computational overhead of neural networks while maintaining their performance as much as possible has become crucial problem. Extensive research (LeCun et al., 1989; Molchanov et al., 2016) demonstrates that pruning is powerful tool for dealing with this issue. Generally speaking, pruning can be divided into two main streams: unstructured pruning and structured pruning. Unstructured pruning (Guo et al., 2016; Han et al., 2015; Dong et al., 2017) involves the removal of individual weights or neurons from neural network. The key advantage of unstructured pruning lies in its flexibility and the fine-grained control it offers over the models architecture. This method often requires specialized hardware or software to exploit the resultant sparsity for computational efficiency. Structured pruning (Anwar et al., 2017; Yeom et al., 2021) removes entire neurons, channels, or layers from neural network, which is more frendly to software since parameters of neural networks are mainly structured data, such as tensor, matrix, and vector. Removing entire neurons directly corresponds to slicing or selecting operation on the structured data, which is easy to implement and more compatible with standard hardware accelerators, such as GPUs and TPUs. Hanson & Pratt (1988) is one of the earliest works to explore structured pruning. The underlying idea is that significant weights typically possess greater magnitudes, as they need to process and transmit Corresponding authors. 1 Published as conference paper at ICLR more information to be influential in determining the networks accurate output. However, pruning under this guidance may sometimes incorrectly remove important neurons with small magnitude and reserve unimportant neurons with large magnitude. Optimal Brain Damage (OBD) (LeCun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi & Stork, 1992) propose to leverage the Hessian matrix of loss w.r.t. parameters to estimate the importance of each parameter. The Hessian matrix contains second-order partial derivatives of the loss function w.r.t. all pairs of parameters, which is very computationally expensive to compute. Thus, OBD approximates it as diagonal matrix by assuming the loss of deleting several parameters is the sum of deleting these parameters individually. OBS views the pruning as an optimization problem and solves it with the Lagrange multiplier. They either discard or approximate the second-order partial derivatives between all pairs of parameters, which capture the change of loss on one parameter when deleting another parameter. 2L θiθj Our Contributions In this paper, we follow the idea of OBD, leveraging Hessian matrix for parameter importance estimation. Instead of approximating Hessian matrix, we calculate the Hessianvector product element (cid:80) δθiδθj for each parameter in the network. To achieve this, we first separate the Hessian matrix of the whole network into Hessian submatrices between layers. Then, in the context of widely used network structures including convolutional neural networks (CNNs) and Transformers, we analyze the conditions where the Hessian submatrices between two layers are nonzero. Finally, we propose highly efficient method to capture these conditions and obtain the Hessian-vector product element on each parameter. Stepping from approximating the Hessian matrix with the Fisher matrix to directly computing the Hessian-vector product, we propose Optimal Brain Apoptosis (OBA), novel pruning method that efficiently calculates the second-order Taylor expansion for each parameter and is applicable to both structured and unstructured pruning tasks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Model Compression Model compression is an area that focuses on creating smaller, faster, and more efficient models suitable for deployment in environments with limited resources, like mobile devices or embedded systems. There are several typical fields within this area, including quantization (Courbariaux et al., 2015; Rastegari et al., 2016; Pouransari et al., 2020), knowledge distillation (Hinton et al., 2015; Chen et al., 2021; Zhou et al., 2021), neural architecture search (Liu et al., 2018; Zoph & Le, 2016; Pham et al., 2018), and network pruning (Molchanov et al., 2019; 2016). Quantization, outlined in works like Hubara et al. (2018) and Jacob et al. (2018), focuses on reducing parameter precision to accelerate inference and decrease model size, enabling deployment on devices with limited resources. Knowledge distillation, as introduced by Hinton et al. (2015); Romero et al. (2014), leverages smaller student model to mimic larger teacher model, effectively compressing the knowledge and achieving high performance with less computational demand. Neural Architecture Search (NAS), with seminal contributions from Zoph & Le (2016), automates the discovery of optimal architectures, often outperforming human-designed models in efficiency and accuracy. Pruning techniques, highlighted in work by Han et al. (2015), remove non-essential weights or neurons, significantly reducing model complexity and enhancing inference speed without major accuracy losses. Together, these techniques represent the forefront of model compression research, addressing the balance between performance and computational efficiency necessary for advanced AI applications. Network Pruning Network pruning, initially recognized as an importance estimation problem (Molchanov et al., 2019; Chauvin, 1988; Yu et al., 2018; He et al., 2020), has been prompting researchers to focus on finding accurate criteria that reveals the importance of parameters or neurons in neural networks. Molchanov et al. (2016) operated under the assumption that each layer in feed-forward networks held equal importance and introduced heuristic for global scaling normalization. However, this approach did not prove effective in networks incorporating skip connections. Additionally, the method relies on using network activations to calculate its criterion, resulting in increased memory demands. In contrast, pruning methods that focus on batch normalization (Gordon et al., 2018; Huang & Wang, 2018; Liu et al., 2017; Ye et al., 2018) bypass the need for sensitivity analysis and are applicable on global scale. In intricate network architectures (Liu et al., 2021; Luo & Wu, 2020; You et al., 2019; Zhang et al., 2021), parameter interdependencies often require their joint pruning. This collective pruning of interlinked parameters has been an area of focus in structured pruning research since its early stages. Fang et al. (2023) proposes to build dependency 2 Published as conference paper at ICLR 2025 graph which captures the interdependencies between parameters and prunes parameters belonging to graph together to achieve structured pruning for neural networks with complicated structures. In our pruning setting, we view the pruning as importance estimation problem for each individual parameter (unstructured) or parameter group (structured) obtained from Fang et al. (2023). Parameters with low importance are pruned in each pruning step. Hessian Matrix Estimation The structure and computational aspects of the Hessian matrix in feedforward neural networks have been extensively studied since the early 1990s (Buntine & Weigend, 1994; Wille, 1997). The Hessian matrix was first utilized in neural network pruning by LeCun et al. (1989) to calculate the importance score of each neuron, leveraging diagonal approximation is used to estimate the Hessian matrix: 1 2 Building upon this, OBS (Hassibi & Stork, 1992) views the importance estimation as an optimization problem, and aims at finding set of weights that yields least change on the loss: δLOBD = Hqq. (cid:0)θ (1) (cid:1) (cid:26) min min δθ 1 2 δθHδθ s.t. q δθ + θ = 0 (cid:27) . (2) Early research such as that by Buntine & Weigend (1994) provided an extensive review of how to compute second derivatives in feed-forward networks, and Wille (1997) examined the Hessian matrixs structure and second derivative techniques. More contemporary efforts, such as EigenDamage (Wang et al., 2019), utilize Kronecker-factored eigenbasis for network reparameterization and approximate the Hessian matrix using the Fisher matrix, as discussed by Martens (2020). Recent studies (Wu et al., 2020; Singh et al., 2021) have thoroughly investigated the common structures and rank properties of the Hessian in neural networks. Pearlmutter (1994) initially introduced an efficient method for computing the Hessian-vector product in feedforward neural networks. Our research applies this idea to the pruning of modern network architectures including CNNs and Transformers."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Consider feed-forward neural network with parameters θ and layers. Similar to OBD (LeCun et al., 1989), when we add small perturbation δθ on θ, the second-order Taylor expansion of the perturbation on the objective function is given by δL(θ) = ( θ )Tδθ + 1 2 δθTHδθ + o(δθ3), (3) where is the Hessian matrix that represents the second-order derivatives between all parameter pairs. It is usually infeasible to compute the Hessian matrix due to its complexity of O(n2), where is the number of parameters in the network (LeCun et al., 1989; Hassibi & Stork, 1992). By expanding the first and second term of eq. (3), we can define the perturbation of the loss caused by δθi as δL(θi) = θi δθi + (cid:88) 2L θiθj δθiδθj + o(δθi3). (4) The first term of eq. (4) is leveraged to estimate the improtance of neurons in Molchanov et al. (2016). Same to prior works, we ignore the higher order term o(δθi3). Current works (Hassibi & Stork, 1992; Yu et al., 2022; Benbaki et al., 2023) that leverage the second-order Taylor expansion term approximate the Hessian matrix with Fisher information matrix. This approximation, if applied to eq. (4), would change its second-order term to (cid:80) δθiδθj. However, this approxij mation is not accurate enough to capture the second-order loss perturbation caused by δθi and δθj. Therefore, we focus on theoretical analysis on how to calculate the original second-order term (cid:80) δθiδθj. θj θi 2L θiθj"
        },
        {
            "title": "4 METHOD",
            "content": "4.1 DEFINITION We derive from general form of linear layers in modern neural networks. For layer [1, L], we denote the weight parameter as (l) Rloutlinpweight and bias parameter as b(l) Rlout. We denote 3 Published as conference paper at ICLR 2025 the input of layer as (l) Rlinpin and output of layer as (l) Rloutpout. pweight, pout, and pin are the length of flattened weights, output values, and input values that contribute to the connections between every pairs of input neurons and output neurons, for example pweight = 1, pout = 1, pin = 1 in the context of fully connected layers and pweight = s(l) 2 outw(l) in in the context of convolutional layers. The forward propagation of layer is given by kernel, pout = h(l) out, pin = h(l) in w(l) (l) ab = (cid:88) cde (l) acdX (l) ce (l) bde + b(l) , (5) where {0, 1}poutpweightpin , determined by the layer itself, represents the connections among all input values, weight values, and output values. We represent the flattened vector of (l), (l), (l), and (l) as x(l) Rlinpin , w(l) Rloutlinpweight , y(l) Rloutpout , and m(l) {0, 1}poutpweightpin , respectively. the parameters θ(l) of layer can be expressed as θ(l) = . We represent all (cid:21) (cid:20)w(l) b(l) ab (cid:80) Y (l) ab are respectively (cid:80) partial derivative terms with variables in their vector forms to ensure these terms are with the same definition of Jacobian matrix. In certain layer defined by eq. (5), the gradient of the loss w.r.t. the indexed parameters (l) acd and b(l) . It is clear that the condition for the Hessian matrix entries between layer and any distinct layer being exclusively zero hinges on the differentiability of terms y(l) and (l) with respect to the parameters of layer l, which depends on the connection type of two layer types. We observe that the connectivity types which introduce nonzero Hessian submatrices between any two layers in neural network can be divided into two cases: series connectivity and parallel connectivity, as shown in fig. 1a. We next analyze these two cases and calculate the Hessian-vector product element (cid:80) δθiδθj in eq. (4) for the two cases respectively. bde and (cid:80) ce (l) 2L θiθj (l) Y (l) ab 4.2 SERIES CONNECTIVITY Definition 4.1 (Series Connectivity). In neural network at layer l, if there exists layer such that there is differentiable function mapping the output of layer to the input of layer l, we say layer and are in series connectivity. Specifically: layer is in lower series connectivity to layer l. layer is in upper series connectivity to layer l. In fig. 1a, there are differentiable functions from (l1) to (l2) and (l3), respecively, so layer l1 is in series connectivity with both layer l2 and layer l3. Without loss of generality, we take layer l1 and layer l2 as an example. Then y(l1) = y(l2) y(l2) x(l2) (cid:124) (cid:123)(cid:122) (cid:125) differentiable to θ(l2) x(l2) y(l1) , also (l2) is differentiable to θ(l1). According to our analysis in the beginning of this section, nonzero Hessian submatrix exists between layers l1 and l2. Theorem 4.2. For layer in neural network where layers lup Lup and layers llow Llow are in upper and lower series connectivity to layer l, respectively, then for weight parameter w(l) and bias parameter b(l) of layer l, we have (cid:88) (cid:88) lLupLlow 2L w(l) θ(l) δθ(l) δw(l) = (cid:88) lupLup y(lup) J(lup) δW (lup ) x(lup) w(l) δw(l) + y(l) y(l) w(l) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (l) δw(l) in which ˆX (l) is given by ˆX (l) mn = (cid:88) (cid:88) llowLlow (l) mn θ(llow) δθ(llow) , 4 (6) (7) (8) Published as conference paper at ICLR 2025 (a) (b) Figure 1: (a) An illustration of conditions where the Hessian matrix between parameters of two layers are nonzero. (b) An illustration of Jacobian-Vector Product Forward Propagation. Two forward propagation processes are needed for parameter layers and one forward propagation process is needed for nonparameter layers. For nonparameter layers we leverage Jacobian-vector product to conduct the forward process and do not need to calculate the Jacobian matrix explicitly. and (cid:88) (cid:88) lLupLlow 2L b(l) θ(l) δθ(l) δb(l) = (cid:88) lupLup y(lup) J(lup) δW (lup) x(lup) b(l) δb(l), (9) where J(l) as the weights, and y(l) w(l) δW (l) R(loutmout)(linmin) is the jacobian matrix of y(l) with respect to x(l) taking δW (l) (cid:12) (cid:12) ˆX (l) is the jacobian matrix of y(l) w.r.t. w(l) taking ˆX (l) as input. The proof is provided in appendix E.1. Note that the sets Lup(l) and Llow(l) are dependent on layer l, and are abbreviated for simplicity. With theorem 4.2, the second-order term of the Taylor expansion of the loss with respect to each individual parameter can be computed, taking into account parameters belonging to layers that are in series connectivity to specific layer. For classical neural network structures such as convolutional neural networks and fully connected neural networks, there only exist series connectivities between layers. Thus, we can directly apply theorem 4.2 to calculate eq. (4) for each individual parameter. 4.3 PARALLEL CONNECTIVITY For recent novel neural network structures such as Transformer (Vaswani et al., 2017), matrix multiplication plays crucial and effective role in achieving their impressive performance. It also introduces parallel connectivity to layers and lead to the nonzero Hessian matrices between these connected layers. Definition 4.3. In neural network, if there exist two layers and such that there are differentiable functions respectively mapping the outputs of layer and to the inputs (left) and (right) of matrix multiplication operation (mul) = (left)X (right), we say layer and are in parallel connectivity. Denote the multiplication operation as layer lm, then layer is in left parallel connectivity to layer lm. layer is in right parallel connectivity to layer lm. In fig. 1a, layer l2 and layer l3 are in parallel connectivity. Similarly, the gradient of loss w.r.t. y(l42) y(l42) is differentiable to θ(l3) and parameters of layer l2 is θ(l2) vice versa. Therefore, the nonzero Hessian submatrices resulting from parallel connectivity should also be considered. θ(l2) = y(l4) in which y(l4) y(l4) y(l42) Theorem 4.4. For multiplication operation (mul) = (left)X (right) in neural network, where (left) Rlrowlhid and (right) Rlhidlcol , layers ll Lleft and lr Lright are in left and right parallel connectivity to this multiplication operation, respectively. Consider two surrogate weight 5 Published as conference paper at ICLR 2025 matrices ˆX (left) Rlrowlhid and ˆX (right) Rlhidlcol given by: ˆX (left) kn = (cid:88) (cid:88) llLleft X (left) kn θ(ll) δθ(ll) , ˆX (right) no = (cid:88) (cid:88) lrLright (right) no θ(lr) δθ(lr) . (10) (11)"
        },
        {
            "title": "Then we have",
            "content": "(cid:88) (cid:88) 2L llLleft θ(ll) θ(lr) δθ(ll) δθ(lr) = y(mul) y(mul) x(right) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (left) x(right) θ(lr) δθ(lr) (12) and (cid:88) (cid:88) 2L lrLright θ(lr) θ(ll) δθ(lr) δθ(ll) = y(mul) y(mul) x(left) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (right) x(left) θ(ll) δθ(ll). (13) Proof can be seen in appendix E.2. Now we can analytically calculate the Hessian-vector product element of each parameter specifically focusing on the interaction between its layer and any other layers that are in parallel connectivity to it. Combining theorem 4.2 and theorem 4.4, we can calculate the Hessian-vector product element of each parameter in neural network. Next, we focus on an efficient calculation the Hessian-vector product element of each parameter. 4.4 JACOBIAN-VECTOR PRODUCT FORWARD PROPAGATION From computational overhead perspective, the main calculation part within theorem 4.2 and theorem 4.4 focuses on ˆX (l), the gradients of (l) w.r.t. the parameters of all layers that are in lower series connectivity to ˆX (l), as we need to separately back-propagate each entry of (l), usually large matrix in attention modules. To address this issue, we introduce Jacobian-Vector Product Forward Propagation (JVPF), method capable of computing ˆX (l) for all layers with an acceptable computational expense. Lets look at layer in lower series connectivity to layer and denote it as layer llow. The derivative of (l) w.r.t. single layer llow can be expressed as x(l) θ(llow) δθ(llow) = x(l) x(l1) x(llow+1) x(llow) x(llow) θ(llow) δθ(llow), indicating that we can calculate it in layer-by-layer manner. Further, let us group layers in lower series connectivity to layer into several groups, each of which contains both parameter layers and nonparameter layers that are in series connectivity to each other. We denote the ith group as Gi = {li1, li2, , liNi}, where Ni is the number of layers in group Gi and layer li(j+1) is subsequent to layer lij. Then we have ˆx(l) = (cid:88) (cid:88) llowGi x(l) θ(llow) δθ(llow) = x(l) y(lNi ) (cid:88) f (liNi ) (li(Ni1)) (li1)(0) where (lij )(x) = y(lij ) θ(lij ) δθ(lij ) + y(lij ) y(lij ) x(lij ) else. x(lij ) lij has parameters, (14) x(lij ), y(lij ), and θ(lij ) are the input, output, and parameters of layer lij. Once we replace the forward function of each layer with eq. (14), we could calculate ˆx(l) for each layer through one forward propagation process, and all series connectivity groups can be calculated in parallel. An intuitive demonstration of JVPF is shown in fig. 1b. 4.5 PRUNING STRATEGY 6 Published as conference paper at ICLR 2025 Utilizing theorem 4.2 and theorem 4.4 along as our proposed JVPF, which offers an efficient choice to obtain the essential intermediate values, we can calculate the Hessian-vector product element of each parameter as their importance scores with several batches of traning data. Please refer to appendix for the detailed importance score acquisition. This importance score can be leveraged to conduct unstructured pruning for each parameter individually, or conduct structured pruning for each parameter group. Structured Pruning Following Fang et al. (2023), parameters from in-layer and interlayer connections can be organized into several groups G. The importance scores belonging to each group is defined as IG = {I (i) gi G}. For each group gi G, importance scores are summed on every neuron over parameters of upper layers Lu gi with length Nu and lower layers Ll gi with length Nl, as illustrated in in fig. 2. Figure 2: Importance score of each neuron in group is gathered from parameters of lower layers and upper layers. Unstructured Pruning The importance score in unstructured learning is more straightforward. By eliminating the process of gathering importance, considering each parameter layer as group, and each parameter as neuron, the definition of importance score becomes similar to what is described in structured pruning scenarios. Experimentally we found that the gradients of some parameters are zero due to gradient vanishing, making us hard to judge the importance of these parameters. This would have little influence on structured pruning since neurons importance scores are gathered through multiple weights. However, in unstructured pruning, the importance score of each weight only depends on itself. By adding the magnitude of the corresponding weight in the importance score term, the problem is resolved. In each pruning step, we calculate every importance score (i,j) of jth neuron in ith group over traning data of batches, and normalize them within each group. Then we rank them from lowerst to highest. The lowest percentage of parameters are pruned. We can gradually increase to iteratively prune the model to specific FLOPs or parameter percentage. G"
        },
        {
            "title": "5 RESULTS",
            "content": "We empirically study the performance of OBA on CNNs where only series connectivity exists, and attention networks where both series connectivity and parallel connectivity are present. We focus on pruning towards as small FLOPs as possible to reduce the computation overhead of model inference. For structured pruning, we primarily compare our methods with those that leverage Hessian matrix information (Hassibi & Stork, 1992; Wang et al., 2019). In the context of unstructured pruning, we evaluate our approach against the state-of-the-art unstructured pruning method, CHITA (Benbaki et al., 2023). Classical importance acquisition methods Weight (the magnitude of weights), OBD (LeCun et al., 1989) and Taylor (Molchanov et al., 2016) are also added into comparison for both structured and unstructured pruning tasks. In our experiments, we choose δθi = θi. The implementation details can be found at appendix A. 5.1 STRUCTURED PRUNING Current structured pruning workflows can be roughly divided into one-shot pruning and iterative pruning. The former prunes the fine-tuned model towards sparsified network and fine-tunes it after pruning, whereas the latter prunes the model iteratively and fine-tunes the model after each pruning step. One-shot pruning is more efficient than iterative pruning, but the latter is more effective. We evaluate our method on both of these two workflows. 5.1.1 IMPORTANCE SCORE RANK CHARACTERIZATION We first measure the ability of our method to characterize the importance of each neuron for structured pruning. Intuitively, the importance score of neuron should be positively correlated to the 7 Published as conference paper at ICLR 2025 Table 1: The Spearman correlation of the importance scores of each method with the importance rank of neurons on ResNet32. Table 2: Comparison on Accuracies (%) and speed up up ratios with recent pruning works on ImageNet. Method OBA Weight Taylor OBS OBD EigenDamage all layers Max Standardization 0.500 0.496 l2-Norm 0.492 0.444 0.443 None per layer all layers Max Standardization 0.456 0.382 l2-norm 0.402 0.379 0.421 None per layer 0.115 0.335 0.433 -0.247 0.271 0.092 0.049 0.443 -0.187 0. CIFAR100 0.248 0.292 0.407 0.222 0.304 0.457 0.450 0.441 -0.157 0.292 CIFAR10 0.373 0.166 0.367 0.360 0.362 0.417 0.444 0.433 0.080 0.409 0.222 0.474 0.541 0.275 0.287 0.426 0.412 0.186 0.342 0. -0.077 -0.047 -0.012 0.046 0.018 0.165 0.166 0.192 0.172 0.056 Method Pruned Speed Up Resnet Weight C-OBD (Wang et al., 2019) C-OBS (Wang et al., 2019) EigenDamage (Wang et al., 2019) Taylor (Molchanov et al., 2016) OBA(Ours) 75.121.03 74.861.29 75.480.67 75.300.85 75.260.89 75.620.53 ViT-B/16 Weight Taylor (Molchanov et al., 2016) OBA(Ours) 77.034.04 77.653.42 79.641.43 1.99 1.99 2.01 2.00 2.00 2.00 1.32 1.31 1.30 change of loss when removing neuron. This can be implemented by masking the neurons output in the evaluation process. Specifically, we mask the output of each neuron in pre-defined layers and compare the change of loss between the masked model and unmasked model, which is refered to as ground truth importance. Then we calculate the Spearmans rank correlation of the importance scores with estimated importance scores from different methods. The higher the Spearmans rank score, the better the method captures the importance. We select the output neurons of the first convolutional layer and the three following residual blocks of ResNet32 (He et al., 2016) as candidates. For per-layer case, Spearman correlation is calculated within each layer and averaged. For all-layer case, before calculating Spearman correlation, we first normalize the importance scores of each layer and concatenate them together. Different layer-wise normalization methods are considered in the evaluation and are detailedly introduced in appendix C. As shown in table 1, with evaluation within each layer, OBA yields the best rank similarity to the ground truth importance among all methods. In the all-layer importance condition, our method also yields good performance on different normalization methods. It is noteworthy that methods such as Weight, Taylor, and OBD, although potentially exhibiting higher correlation scores in importance under specific normalization schemes, may also yield notably low correlation scores under certain normalization techniques. In contrast, OBA consistently maintains relatively high correlation scores irrespective of the normalization method applied. This proves our methods capability of importance capturing, along with its robustness to different normalization methods. 5.1.2 ONE-SHOT PRUNING RESULTS Next, we conduct experiments to evaluate the performance of OBA on pruning towards specific FLOPs percentage in one-shot pruning workflow. As the FLOPs and number of parameters are not linearly dependent w.r.t. the number of neurons in the neural network, its hard to calculate pruning ratio under which the network is pruned into predefined FLOPs or parameter number. Thus we prune the network for several steps with an increasing pruning ratio. FLOPs and number of parameters are calculated after each step to check whether the network satisfies the target. We first evaluate our method on ImageNet with ResNet50 (He et al., 2016) and ViT-B/16 (Dosovitskiy et al., 2020). As shown in table 5, our method realizes 2 speed up at ImageNet on ResNet50 with an accuracy decrease of only 0.53%. Our method outperforms other methods on both ResNet50 and ViT-B/16, which demonstrates the effectiveness of our method on large-scale datasets. On ViT-B/16, our method achieves 1.30 speed up with an accuracy decrease of 1.43%, which is far smaller than those achieved by Taylor criteria and Weight criteria, demonstrating our proposed criterias superiority over Weight criteria and the first-order Taylor criteria. Next we evaluate our method on CIFAR10 and CIFAR100 datasets. The results are shown in table 3. Our method achieves the best results on ResNet32 model across the both datasets. When pruning network towards small 6% target FLOPs, our method on ResNet32 surpasses other methods lot, with only 0.79% accuracy loss on CIFAR10 datasets. On CIFAR10 with VGG19 the performance of OBA is slightly worse, but still comparable to other methods, with relatively lower FLOPs. We observe an interesting phenomenon that the parameter reduction of models pruned with OBA are slightly higher than other methods under similar FLOPs, which indicates that our method tends to prune neurons with relatively higher FLOPs, and is more suitable for applications of lower FLOPs requirements. 8 Published as conference paper at ICLR 2025 Table 3: The accruacies (%), weights reduction (%), and FLOPs reduction (%) of different methods across CIFAR10 and CIFAR100 under one-shot pruning with ResNet32 and VGG19. Method CIFAR10 CIFAR Acc Weights FLOPs Acc Weights FLOPs Acc Weights FLOPs Acc Weights FLOPs VGG19(Baseline) NN Slimming (Liu et al., 2017) C-OBD (Wang et al., 2019) C-OBS (Wang et al., 2019) Kron-OBD (Wang et al., 2019) Kron-OBS (Wang et al., 2019) EigenDamage (Wang et al., 2019) Weight Taylor (Molchanov et al., 2016) OBA (Ours) ResNet32(Baseline) C-OBD (Wang et al., 2019) C-OBS (Wang et al., 2019) Kron-OBD (Wang et al., 2019) Kron-OBS (Wang et al., 2019) EigenDamage (Wang et al., 2019) Weight Taylor (Molchanov et al., 2016) OBA (Ours) 94.17 92.84 94.04 94.08 94.00 94.09 93.98 93.85 94.11 93.9 95.3 95.11 95.04 95.11 95.14 95.17 94.51 95.06 95.19 80.07 82.01 76.96 80.40 79.71 78.18 68.57 62.29 56.63 70.36 67.90 63.97 64.21 71.99 55.47 66.57 63.90 42.65 38.18 34.73 38.19 36.93 37.13 37.21 38.28 38.06 66.18 76.75 63.41 61.89 70.25 71.99 71.40 71. 85.01 92.34 91.92 92.92 92.56 92.29 91.85 92.29 92.48 91.75 90.04 92.57 92.76 93.05 92.07 93.32 93.45 97.85 97.68 97.27 97.47 97.32 97.15 97.02 93.89 91.89 97.30 95.49 96.11 96.14 96.05 91.61 94.88 93.68 97.89 77.39 87.53 81.44 80.39 86.51 86.93 86.29 86.27 93.50 97.39 94.18 94.37 94.74 94.09 94.05 94. 73.34 71.89 72.23 72.27 72.29 72.12 72.90 72.14 71.82 72.36 78.17 75.70 75.16 75.86 75.98 75.51 75.72 76.13 76.47 74.60 77.03 73.83 77.24 74.18 76.64 67.89 55.58 53.33 66.68 66.83 63.92 62.36 69.80 65.76 64.27 64.34 38.33 33.70 38.09 37.90 36.59 37.40 37.02 37.76 37.62 67.53 76.59 62.97 60.41 71.62 70.08 70.11 70. 58.69 58.07 58.87 60.70 60.66 65.18 54.63 66.65 66.72 59.52 58.20 62.42 63.62 65.72 66.09 64.56 67.81 97.76 97.97 97.61 97.56 97.48 97.31 95.90 93.12 92.74 97.74 91.99 96.42 93.56 95.21 95.44 90.35 94.43 94.09 77.55 91.94 82.55 83.57 88.63 88.30 88.28 88.80 94.88 96.27 95.85 95.65 94.62 94.63 94.99 94. Figure 3: Iterative pruning results on CIFAR10 and CIFAR100 with ResNet32. 5.1.3 ITERATIVE PRUNING RESULTS In this subsection, we evaluate the performance of OBA under iterative pruning workflow on ResNet32. Specifically, we set list of target FLOPs for each dataset and model, and iteratively prune the model from largest FLOPs to smallest FLOPs. Fine-tuning is conducted after each pruning step. In terms of FLOPs, the accuracy loss for each iteration of our method is lower than other methods, as shown in fig. 3. This validates the effectiveness of our proposed criteria. As for parameter reduction, our method yields less advantage over other methods, which is consistent with the results of one-shot pruning. 5.2 UNSTRUCTURED PRUNING We also evaluate the performance of OBA on unstructured pruning task. We follow similar setting of the multi-stage pruning in CHITA (Benbaki et al., 2023) to have fair comparison. Since Taylor (Molchanov et al., 2016), Weight, and OBD (LeCun et al., 1989) all obtain the importance scores from each parameter, we can ignore their importance gathering steps and add them into comparison. The results are shown in table 4 and table 5. Given the varying performance of the initial unpruned networks, we directly compare the accuracy ratio relative to the raw accuracy of all methods. The main version of CHITA, i.e. CHITA-CD, has very large computational time and memory cost on Resnet50, making itself infeasible for such huge networks. Thus we implemented the more efficient CHITA-BSO for comparison. It can be seen that Taylor and OBD fails on this task as their accuracies rapidly fall into 10% in the first pruning step. OBAs results on high sparsities surpass CHITA++ by huge margin, proving itself to be effective in unstructured pruning task. 5.3 RUN TIME ANALYSIS Here, we empirically study the time consumption of OBA. Table 6a shows the time consumption of each part of OBA, and the time costed by regular training. It can be seen that the time cost of computing parallel connectivity is the most time-consuming part of OBA, nearly same to the time of series connectivity. In network structures that do not contain multiplication operations, the time cost of OBA would be much lower. In table 6b, 200 batches of data with batch size of 64 are 9 Published as conference paper at ICLR 2025 Table 4: The unstructured pruning results on CIFAR-10 dataset with ResNet-20. Taylor (91%) OBD (91%) Weight (91%) OBA (91%) CHITA++ (91.36%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) 11.00 11.00 10.00 10.80 10.00 8.20 10.00 10.00 10. 14.45 14.45 13.14 14.19 13.14 10.77 13.14 13.14 13.14 10.03 10.03 10.03 10.02 10.01 10.00 10.00 10.00 10.52 13.17 13.17 13.17 13.16 13.15 13.14 13.14 13.14 13.82 90.66 90.82 90.67 90.67 90.79 90.23 88.83 85.03 67.00 99.63 99.80 99.64 99.64 99.77 99.15 97.62 93.44 73.63 90.83 90.90 90.65 90.35 90.57 90.69 89.94 89.64 86. 99.81 99.89 99.62 99.29 99.53 99.66 98.84 98.51 94.80 - - 91.25 91.20 91.04 90.78 90.38 88.72 79.32 - - 99.88 99.82 99.65 99.37 98.93 97.11 86.82 Table 5: The unstructured pruning results on Imagenet dataset with ResNet-50. Taylor (76.13%) OBD (76.13%) Weight (76.13%) OBA (76.13%) CHITA-BSO++ (77.01%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) Accuracy (%) Ratio (%) 0.11 0.11 0.10 0.11 0.10 0.08 0.10 0.10 0.10 0.14 0.14 0.13 0.14 0.13 0.11 0.13 0.13 0. 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.12 0.13 75.30 75.23 74.57 73.54 70.73 64.97 48.09 16.08 0.80 98.91 98.82 97.95 96.60 92.91 85.34 63.17 21.12 1.05 75.67 75.40 74.93 74.50 73.42 70.88 65.57 47.14 5.65 99.40 99.04 98.42 97.86 96.44 93.10 86.13 61.92 7. 77.00 76.91 76.87 76.59 76.01 68.89 64.38 26.21 0.43 99.98 99.87 99.82 99.46 98.70 89.46 83.60 34.03 0.56 Sparsity 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Sparsity 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0. Resnet32 ViT-B/16 Regular Traning Upper Series Lower Series Parallel Total 0.326 0.881 0.819 - 2.072 0.252 0.799 1.166 1.944 4.027 (a) (b) (c) Table 6: The running time (s) of OBA on different models and datasets. (a) The specific running time (s/iteration) of each part of OBA on ResNet32 and ViT-B/16. (b) The running time of OBA on ResNet32 with CIFAR100. (c) The running time of OBA on ResNet20 with CIFAR10. leveraged in each pruning iteration. In each pruning stage we would conduct 10 pruning iterations to gradually prune the network, and the overall computation time of of pruning would be less than 1 hour, which is acceptable in the actual pruning scenarios. In table 6c, we samely use 200 batches of data to calculate gradients. The mainly proposed method in CHITA, CHITA-CD, would require twice as much time compared to our approach. However, the performance of both CHITA-CD and CHITA-BSO under high sparsity is worse than OBA, showcasing the efficiency of our method."
        },
        {
            "title": "6 CONCLUSION AND LIMITATION",
            "content": "In this paper, we propose Optimal Brain Apoptosis, novel method for pruning neural networks. We first provide theoretical analysis on modern neural network structures to figure out nonzero Hessian submatrix conditions between layers. Then we propose an efficient approach that directly calculates the Hessian-vector product values for each parameter in the network, thereby calculating the secondorder Taylor expansion for each parameter without any approximation. We empirically demonstrate the efficacy of our method on both structured pruning and unstructured pruning. Limitation OBA, in its current form, can be applied to network structures including MLPs, CNNs, and Transformers. For networks with more complex architectures, like RNNs and State Space Models that handle time-series data, computing the Hessian matrix becomes more difficult and necessitates additional research. This is an interesting area that warrants further exploration in the future. 10 Published as conference paper at ICLR"
        },
        {
            "title": "7 ACKNOWLEDGEMENT",
            "content": "We sincerely appreciate the dedication and valuable feedback provided by all anonymous reviewers. This research was partially supported by the National Natural Science Foundation of China under Grants 62073066 and 62405255, as well as the Fundamental Research Funds for the Central Universities under Grant N2226001. Additionally, it received support from the 111 Project under Grant B16009 and the Intel Neuromorphic Research Community (INRC) Grant Award (RV2.137.Fang)."
        },
        {
            "title": "REFERENCES",
            "content": "Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):118, 2017. Riade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, and Rahul Mazumder. Fast as chita: Neural network pruning with combinatorial optimization. In ICML, pp. 20312049. PMLR, 2023. Wray Buntine and Andreas Weigend. Computing second derivatives in feed-forward networks: review. IEEE transactions on Neural Networks, 5(3):480488, 1994. Yves Chauvin. back-propagation algorithm with optimal use of hidden units. Advances in neural information processing systems, 1, 1988. Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 50085017, 2021. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: ing deep neural networks with binary weights during propagations. N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett formation Processing Systems, volume 28. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2015/file/ 3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf. Inc., 2015. (eds.), Advances in Neural TrainIn C. Cortes, InURL Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in neural information processing systems, 30, 2017. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In CVPR, pp. 1609116101, 2023. Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast & simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 15861595, 2018. Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NeurIPS, 29, 2016. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NeurIPS, 28, 2015. Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with backpropagation. NeurIPS, 1, 1988. 11 Published as conference paper at ICLR 2025 Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In NeurIPS, 1992. URL https://proceedings.neurips.cc/paper_files/ paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, and Yi Yang. Learning filter pruning criteria for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 20092018, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 304320, 2018. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. journal of machine learning research, 18(187):130, 2018. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 25, 2012. Yann LeCun, John Denker, and Sara Solla. In NeurIPS, URL https://proceedings.neurips.cc/paper_files/paper/1989/ Optimal brain damage. 1989. file/6c9882bbac1c7093bd25041881277658-Paper.pdf. Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceedings of the European conference on computer vision (ECCV), pp. 1934, 2018. Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group fisher pruning for practical network compression. In International Conference on Machine Learning, pp. 70217032. PMLR, 2021. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. LearnIn Proceedings of the IEEE ing efficient convolutional networks through network slimming. international conference on computer vision, pp. 27362744, 2017. Jian-Hao Luo and Jianxin Wu. Neural network pruning with residual-connections and limited-data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14581467, 2020. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627, 2023. James Martens. New insights and perspectives on the natural gradient method, 2020. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In ICLR, 2016. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1126411272, 2019. 12 Published as conference paper at ICLR Barak Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147160, 1994. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In International conference on machine learning, pp. 40954104. PMLR, 2018. Hadi Pouransari, Zhucheng Tu, and Oncel Tuzel. Least squares binary quantization of neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 698699, 2020. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision, pp. 525542. Springer, 2016. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. Sidak Pal Singh, Gregor Bachmann, and Thomas Hofmann. Analytic insights into structure and rank of neural network hessian maps. Advances in Neural Information Processing Systems, 34: 2391423927, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning in the kronecker-factored eigenbasis. In ICML, pp. 65666575. PMLR, 2019. Jorg Wille. On the structure of the hessian matrix in feedforward networks and second derivative methods. In Proceedings of International Conference on Neural Networks (ICNN97), volume 3, pp. 18511855. IEEE, 1997. Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Understanding common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261, 2020. Jianbo Ye, Xin Lu, Zhe Lin, and James Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. arXiv preprint arXiv:1802.00124, 2018. Seul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander Binder, Simon Wiedemann, Klaus-Robert Muller, and Wojciech Samek. Pruning by explaining: novel criterion for deep neural network pruning. Pattern Recognition, 115:107899, 2021. Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks. Advances in neural information processing systems, 32, 2019. Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad Morariu, Xintong Han, Mingfei Gao, ChingYung Lin, and Larry Davis. Nisp: Pruning networks using neuron importance score propagation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9194 9203, 2018. Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe. The combinatorial brain surgeon: Pruning weights that cancel one another in neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2566825683. PMLR, 1723 Jul 2022. URL https://proceedings.mlr. press/v162/yu22f.html. Yulun Zhang, Huan Wang, Can Qin, and Yun Fu. Aligned structured sparsity learning for efficient image super-resolution. Advances in Neural Information Processing Systems, 34:26952706, 2021. 13 Published as conference paper at ICLR 2025 Sheng Zhou, Yucheng Wang, Defang Chen, Jiawei Chen, Xin Wang, Can Wang, and Jiajun Bu. Distilling holistic knowledge with graph neural networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1038710396, 2021. Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. 14 Published as conference paper at ICLR"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "In our structured pruning experiments, we align our settings to EigenDamage (Wang et al., 2019), which is good Hessian based pruning method. For each pruning step, we obtain the importance score from 200 batches of data to calculate the importance score. We set the batch size to 64 and the learning rate to 0.001 for the fine-tuning process. We use the SGD optimizer with momentum of 0.9 and weight decay of 5 104. The learning rate is divided by 10 at the 80th and 120th epochs. We set the maximum epochs to 150 for CIFAR10 and CIFAR100 datasets. For ImageNet experiments, we use the same settings as Fang et al. (2023) of ResNet50 and ViT-B/16. We set the maximum epochs to 100 for ImageNet experiments."
        },
        {
            "title": "B ALGORITHMIC DETAILS",
            "content": "As can be seen in algorithm 1, We first conduct forward propagation process on the network and the output of each layer (line 1-2). Then we backrecord the output gradient of the loss w.r.t. propagate these gradients for each parameter layer with corresponding weight δθ to obtain the term (cid:80) lLup (cid:80) 2L θ(l) θ(l) δθ(l) (line 3-5). 1. Upper Series Connectivity: These terms are recorded in the gradient of the corresponding parameters, so that we can obtain (cid:80) δθ(l) in Equation (6) by multiplying δθ(l) (cid:80) the parameters with their gradients (line 6-7). lLup 2L θ(l) θ(l) 2. Next, We obtain ˆX (l) these values are useful for calculating Lower Series Connectivity cases and Parallel Connectivity cases (line 9). for all parameter layers through the JVPF, δθ(llow) mn = (cid:80) (l) mn (llow) θ (cid:80) llowLlow 3. Lower Series Connectivity: We obtain Equation (7) and add them into the importance scores (line 12). 4. Parallel Connectivity: In the meantime, for all attention layers that induce parallel connectivity, we back-propagate the gradient with the surrogate inputs ˆXleft and ˆXright according to Equations (12) and (13) (line 13-17), and multiply the parameters with their gradients that are in Parallel Connectivity with the attention layer (line 20-21)."
        },
        {
            "title": "C NORMALIZATION METHODS",
            "content": "In our implementation, We leverage these normalization methods on importance scores for each group, including: No Normalization (None) When the normalizer is set to None, the original values of importance scores are returned without any modification. This means that the data is used as-is, with all its original properties (such as scale and distribution) intact: normalized = Ij. Standardization (or Min-Max Normalization) This method scales the data so that it fits within specific range, typically 0 to 1. This is achieved by subtracting the minimum value of the data and then dividing by the range of the data: normalized = Ij min(I) max(I) min(I) . Max Normalization In this approach, every importance score is divided by the maximum importance score of corresponding group to ensure that all the normalized values fall between 0 and 15 Published as conference paper at ICLR Algorithm 1 Importance Score Acquisition of OBA Input: model and its parameters θ, batch of data Output: parameter importance 1: Initialized importance dict 2: Conduct one forward propagation and back propagation process on model with data and y(l) J(l) y(l) for each layer record the output gradient 3: for parameter layer in do x(l).backward( δW (l) ) 4: 5: end for 6: for parameter layer in do I[l] δθ(l) θ(l).grad 7: 8: end for 9: Conduct JVPF according to eq. (14) and record ˆX (l) for each layer 10: m.zero grad() 11: for parameter layer in do I[l] I[l] + y(l) if layer is attention module then (cid:12) (cid:12) ˆX (l) δθ(l) y(l) θ(l) 12: 13: 14: 15: ˆS = softmax(cid:0)(Q(l) ˆK (l)T + ˆQ(l)K (l)T)(cid:14)(cid:113) = softmax(cid:0)Q(l)K (l)T(cid:14)(cid:113) ˆO = ˆSV + ˆV , = SV ˆO.backward( O ) d(l) (cid:1) (cid:1) d(l) 16: 17: end if 18: 19: end for 20: for parameter layer in do 21: 22: end for 23: m.zero grad() I[l] I[l] + δθ(l) θ(l).grad Figure A1: The layer-wise FLOPs for all parameter layers from models pruned by different criteria. 1: normalized = Ij max(I) . l2 Normalization This method normalizes the importance scores by dividing it by the l2 norm (Euclidean norm) of the importance scores belonging to the same group. The l2 norm is calculated as the square root of the sum of the squared values: normalized = Ij I2 . 16 (cid:80) 2L (lup) (llow) θ Lemma E.1. (cid:88) hi and Published as conference paper at ICLR"
        },
        {
            "title": "D PRUNED LAYERS VISUALIZATION",
            "content": "We visualized the layer-wise FLOPs of pruned models by OBA and other methods for ResNet32 and VGG19 on CIFAR100 with target FLOPs of 6%. It can be seen that compared with other methods, the difference of OBA between FLOPs of different layers is smaller, resulting in smoother model in terms of number of neurons across layers. This significantly helps to improve the models performance across various datasets and provide useful guidance for researchers to design and prune neural networks."
        },
        {
            "title": "E PROOFS",
            "content": "E.1 THEOREM 4.2 For any two layers llow and lup in series connectivity, where lup is upper than llow. Note is zero matrix because parameters w(llow) and b(lup) are independent of that 2L w(llow )b(lup ) each other. With this prior, 2L θ (llow) θ (lup ) is actually 2L (lup) (llow ) θ . We first calculate term δθ(llow) δw(lup) for the lower layer llow. 2L hi (llow) acd (lup) δW (lup) hi δW (llow) acd = (cid:88) hi 2L hi b(llow) (lup) δW (lup) hi δb(llow) = y(lup) J(lup) δW (lup ) x(lup) (llow) acd δW (llow) acd y(lup) J(lup) δW (lup ) x(lup) b(llow) b(llow) , (15) (16) where J(lup) δW (lup) as the weights. δW (lup ) R(loutmout)(linmin) is the jacobian matrix of y(lup) with respect to x(lup) taking Proof. Let J(lup) (lup ) = y(lup) x(lup ) . The element-wise derivative of loss w.r.t. (lup) is given by (cid:88) (cid:88) (lup) hi (lup) gij , X (lup) hj = L (lup) (cid:123)(cid:122) (lup) /X and the element-wise derivative of loss w.r.t. (lup) is given by (lup) (cid:88) (cid:88) = L (lup) hi Y (lup) j hj (lup) gij . (cid:125) (lup ) hj ij (cid:124) Applying chain rule, we have (cid:88) hi 2L hi (llow) acd (lup) δW (lup) hi δW (llow) acd = (cid:88) L (lup) (cid:88) hij (lup) gij δW (lup) hi (lup) hj (llow) acd δW (llow) acd = y(lup) J(lup) δW (lup ) x(lup) (llow) acd δW (llow) acd , and (cid:88) hi 2L hi b(llow) W (lup) δW (lup) hi δb(llow) (cid:88) = L (lup) (cid:88) hij (lup) gij δW (lup) hi (lup) hj b(llow) b(llow) = y(lup) J(lup) δW (lup) x(lup) b(llow) b(llow) . 17 (17) (18) (19) (20) Published as conference paper at ICLR 2025 Next we obtain term (cid:80) 2L (llow) θ (lup ) δθ(llow) δw(lup) for the upper layer lup. Lemma E.2. (cid:88) 2L hi θ(llow) (lup) δW (lup) hi δθ(llow) = y(lup) y(lup) (lup) hi (cid:12) (cid:12) (cid:12) (cid:12) ˆX (lup,llow) δW (lup) hi , where ˆX (lup,llow) is given by Proof. ˆX (lup,llow) hj = (lup,llow) hj θ(llow) (cid:88) δθ(llow) . y(lup) y(lup) (lup) hi (cid:12) (cid:12) (cid:12) (cid:12) ˆX (lup) δW (lup) hi = (cid:88) L (lup) (cid:88) (lup) gij (cid:88) (21) (22) (lup) hj θ(llow) (lup) hj θ(llow) δθ(llow) δW (lup) hi δθ(llow) δW (lup) hi (lup) gij (cid:88) (cid:88) = (cid:88) = (cid:88) Y (lup) 2L hi θ(lup) (lup) j δθ(llow) δW (lup) hi . (23) Proof of theorem 4.2. Sum the results in lemma E.1 for upper series connectivity cases with results in lemma E.2 for lower series connectivity cases and we get 2L w(l) θ(l) δθ(l) δw(l) = (cid:88) lupLup y(lup) J(lup) δW (lup) x(lup) w(l) δw(l) + y(l) y(l) w(l) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (l) δw(l) (24) ˆX (l) hj = (cid:88) ˆX (l,llow) hj = (cid:88) (cid:88) llowLlow llowLlow (l) hj θ(llow) δθ(llow) , (cid:88) (cid:88) lLupLlow with and (cid:88) (cid:88) lLupLlow 2L b(l) θ(l) δθ(l) δb(l) = (cid:88) lupLup y(lup) J(lup) δW (lup ) x(lup) b(l) δb(l) . (25) Rewrite eq. (24) and eq. (25) as vector forms and we obtain eq. (6) and eq. (9). Appendix E.2 THEOREM 4.4 Here we follow the notations in definition 4.3 to denote each value of the two layers. Lemma E.3. Let ˆX (left) Rlrowlhid and ˆX (right) Rlhidlcol be two surrogate weight matrices such that ˆX (left) kn = ˆX (right) no = (cid:88) (cid:88) (left) kn θ(ll) (right) no θ(lr) δθ(ll) , δθ(lr) . 18 (26) (27) Published as conference paper at ICLR"
        },
        {
            "title": "Then we have",
            "content": "and (cid:88) (cid:88) 2L θ(ll) θ(lr) 2L θ(ll) θ(lr) δθ(lr) δθ(ll) = y(mul) y(mul) x(right) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (left) x(right) θ(lr) δθ(lr) δθ(lr) δθ(ll) = y(mul) y(mul) x(left) (cid:12) (cid:12) (cid:12) (cid:12) ˆX (right) x(left) θ(ll) δθ(ll) . Proof. The second-order partial derivative of loss w.r.t. θ(lr) and θ(ll) can be written as 2L θ(ll) θ(lr) (cid:88) = ko Y (mul) ko (cid:88) np 2Y (mul) ko kn (right) po (left) (right) po θ(lr) (left) kn θ(ll) . Since the multiplication can be expressed as (mul) ko = (cid:80) kn (right) no , which means 2Y (mul) ko kn (right) po (left) = With this, eq. (30) can be rewritten as (left) (cid:26)1 = p, 0 = p. 2L θ(ll) θ(lr) (cid:88) = ko Y (mul) ko (cid:88) (right) no θ(lr) (left) kn θ(ll) . By expanding the right-hand side of eq. (28) we have (cid:88) ko Y (mul) ko (cid:88) (right) no θ(lr) δθ(lr) (cid:88) (left) kn θ(ll) δθ(ll) (cid:88) = (cid:88) ko Y (mul) ko 2L θ(ll) θ(lr) (cid:88) = (cid:88) X (right) no θ(lr) (left) kn θ(ll) δθ(lr) δθ(ll) δθ(lr) δθ(ll) . Expand the right-hand side of eq. (29) and we can see it also holds. Proof of theorem 4.4. According to lemma E.3, we have (28) (29) (30) (31) (32) (33) (34) (cid:88) (cid:88) llLleft 2L θ(ll) θ(lr) δθ(lr) δθ(ll) = (cid:88) (cid:88) (cid:88) llLleft ko (right) no θ(lr) (left) kn θ(ll) δθ(lr) δθ(ll) (cid:88) Y (mul) ko (right) no θ(lr) (cid:88) = ko = y(mul) (cid:88) Y (mul) ko y(mul) x(right) (cid:88) δθ(lr) llLleft (cid:12) (cid:12) (cid:12) (cid:12)(cid:80) llLleft (cid:80) X(left) kn (ll ) θ δθ (ll) δθ(ll) (cid:88) (left) kn θ(ll) x(right) θ(lr) δθ(lr) . (35) Apply similar operations on (cid:80) lrLright (cid:80) 2L (ll ) θ θ(lr ) δθ(lr) δθ(ll) and we can get eq. (13)."
        }
    ],
    "affiliations": [
        "Hunan University",
        "Northeastern University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}